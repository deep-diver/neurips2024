[{"type": "text", "text": "On the Role of Information Structure in Reinforcement Learning for Partially-Observable Sequential Teams and Games ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Awni Altabaa Zhuoran Yang Department of Statistics & Data Science Department of Statistics & Data Science Yale University Yale University awni.altabaa@yale.edu zhuoran.yang@yale.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In sequential decision-making problems, the information structure describes the causal dependencies between system variables, encompassing the dynamics of the environment and the agents\u2019 actions. Classical models of reinforcement learning (e.g., MDPs, POMDPs) assume a restricted and highly regular information structure, while more general models like predictive state representations do not explicitly model the information structure. By contrast, real-world sequential decisionmaking problems typically involve a complex and time-varying interdependence of system variables, requiring a rich and flexible representation of information structure. In this paper, we formalize a novel reinforcement learning model which explicitly represents the information structure. We then use this model to carry out an information-structural analysis of the statistical complexity of general sequential decision-making problems, obtaining a characterization via a graph-theoretic quantity of the DAG representation of the information structure. We prove an upper bound on the sample complexity of learning a general sequential decision-making problem in terms of its information structure by exhibiting an algorithm achieving the upper bound. This recovers known tractability results and gives a novel perspective on reinforcement learning in general sequential decision-making problems, providing a systematic way of identifying new tractable classes of problems. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The information structure of a sequential decision-making problem is a description of the causal dependencies between system variables. In particular, the information structure specifies the subset of past events that causally influence the present state. This includes information affecting system dynamics and information available to each agent when choosing an action. The control community has long recognized the importance of information structure, leading to the development of the celebrated Witsenhausen intrinsic model [1], and extensive study since the 1970s [e.g., 2-14]. ", "page_idx": 0}, {"type": "text", "text": "In general, sequential decision-making problems with arbitrary causal dependencies between system variables can be computationally and statistically intractable in both control and learning settings [3, 15]. In response, early research has identified classes of tractable information structures. For example, Markov Decision Processes (MDP) and Markov teams/games assume an observable Markovian state, which serves as a sufficient statistic for the system's evolution. These structural assumptions enable practical planning and learning algorithms, for example based on dynamic programming [16, 17]. Although such restricted model classes can enable more fruitful analysis, they lack the expressiveness needed to naturally capture the causal structures of complex real-world sequential decision-making problems, where each event in the system may depend arbitrarily on past events. ", "page_idx": 0}, {"type": "text", "text": "The concept of information structure is also fundamental to studying the phenomenon of partial observability. In general, partial observability refers to situations where a system's evolution depends on a potentially large number of sequential events, but only a subset of these events is observable by the learning agent. However, commonly studied models capture only a restrictive form of this phenomenon. For example, in a Partially-Observable Markov Decision Process (POMDP)the standard model of partial observability \u2014it is assumed that a Markovian state exists and that each observation is a noisy measurement of the current state. This assumption is often unrealistic, as general systems may not have clearly defined \u201cstates\", and observations may be generated by more complex dependencies. Information structure provides a more powerful framework for understanding partial observability, capturing a general notion in which system variables evolve with an arbitrary causal structure (not necessarily Markovian), and the set of observables is any subset of all system variables. ", "page_idx": 1}, {"type": "text", "text": "The highly-regular information structures of classical models make analysis more tractable, enabling favorable theoretical results [e.g., 18-22] and driving notable empirical success across a wide range of domains [e.g., 23-28]. Despite this empirical success, a general theory addressing the role of information structure in the statistical aspects of reinforcement learning is missing. We argue for the perspective that information structure is an important component of analyzing and solving reinforcement learning problems. A rich and fexible representation of information structure is essential to faithfully capture real-world sequential decision-making problems, where the system evolves according to a complex and time-varying dependence on the past, and different agents have different information available to them at different points in time. ", "page_idx": 1}, {"type": "text", "text": "In this work, we formulate a general model of sequential decision-making with an explicit representation of information structure, and study the role of information structure in the statistical complexity of reinforcement learning. Explicitly modeling information structure allows us to identify a broader class of tractable decision-making problems and ultimately develop more tailored reinforcement learning approaches that leverage this structure. ", "page_idx": 1}, {"type": "text", "text": "Summary of Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A model for representing information structure. We propose partially-observable sequential teams (POsT) and partiallyobservable sequential games (POsG) as general models that contain an explicit representation of information structure as part of the problem specification. This forms a unifying framework that enables an analysis of the role of information structure in RL and captures commonly studied RL models as special cases, including MDPs, Markov teams/games, POMDPs, and Dec-POMDPs/POMGs (Figure 1). The models introduced in this work draw inspiration from Witsenhausen's intrinsic model [1] and its variants from the control literature [29, 30], with added elements to model partial observability in the context of reinforcement learning. ", "page_idx": 1}, {"type": "text", "text": "Theoretical analysis of sequential decision-making through information structure. We characterize the complexity of the observable dynamics of any sequential decision-making problem through a graph-theoretic analysis of the information structure. Moreover, we propose a generalization of predictive state representations\u2014which may be of independent interestand construct such a representation for POSTs/POSGs by exploiting information structure. This provides a robust and efficient parameterization amenable to learning. ", "page_idx": 1}, {"type": "image", "img_path": "QgMC8ftbNd/tmp/74038d7bf37635f3d1c6153023568013040e9cbf2a67e913a5fcb38deb01ac64.jpg", "img_caption": ["Figure 1:A depiction of the generality of our proposed models. POSTs and POSGs capture MDPs, POMDPs, Dec-POMDPs, and POMGs as special cases. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Learning theory & information-structural analysis of statistical complexity. We prove an upper bound on the sample complexity of learning general sequential decision-making problems, expressed as a function of information structure. In particular, the dependence is on an interpretable quantity derived from a graphical representation that can be thought of as an effective \u201cinformation-structural state\", generalizing the typical notion of a Markovian state. We prove this result by exhibiting an algorithm achieving this upper bound, making use of the generalized predictive state representation constructed earlier which provides a robust representation amenable to learning. In doing so, we identify a larger class of statistically tractable reinforcement learning problems. ", "page_idx": 1}, {"type": "text", "text": "Related Work. We provide a detailed discussion of related work in Appendix B. ", "page_idx": 1}, {"type": "text", "text": "2  Background & Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "What follows is an abridged description of the relevant background and preliminaries. We refer the reader toAppendix $\\mathrm{C}$ for a more detailed treatment, and to Appendix A for a summary of notations. ", "page_idx": 2}, {"type": "text", "text": "2.1  Generic Sequential Decision Making Problems ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider a controlled stochastic process $(X_{1},\\allowbreak\\cdot\\cdot,X_{H})$ , where $X_{h}$ is a random variable corresponding to the variable at time $h$ . At each time $h\\in[H]$ , the variable $X_{h}$ may be either an \u2018observation' (i.e., observable system variable) or an ^action'. The dynamics of this stochastic process are described by a tuple $(\\dot{H},\\{\\mathbb{X}_{h}\\}_{h},\\mathcal{O},\\mathcal{A},\\mathbb{P})$ ,where $H$ is the time horizon, $\\mathbb{X}_{h}$ is the variable space at time $\\dot{h_{}}\\,\\dot{O}\\,\\,\\dot{\\subset}\\,\\,[\\dot{H}]$ is the index set of observations (i.e., $X_{h}$ is an observation if $h\\,\\in\\,\\bar{\\mathcal{O}},$ $\\mathcal{A}\\subset[H]$ is the index set of actions, and $\\mathbb{P}~=~\\{\\mathbb{P}_{h}\\}_{h\\in{\\mathcal{O}}}$ is a set of probability kernels which describes the probability of any trajectory $x_{1},\\ldots,x_{H}$ given the actions, $\\mathbb{P}\\left[\\{x_{s}:s\\in{\\mathcal{O}}\\}\\mid\\{x_{s}:s\\in A\\}\\right]=$ $\\begin{array}{r}{\\prod_{h\\in\\mathcal{O}}\\mathbb{P}_{h}\\left[x_{h}\\;\\middle|\\;x_{1},\\dotsc,x_{h-1}\\right]}\\end{array}$ . A choice of policy $\\pi$ induces a probability distribution $\\mathbb{P}^{\\pi}$ on ${\\mathbb X}_{1}\\times$ $\\cdots\\bar{\\times}\\mathbb{X}_{H}$ The objective of the agent(s) is to choose a policy which maximizes their expected reward $V^{R}(\\pi)=\\mathbb{E}^{\\pi}\\left[R(\\bar{X}_{1},\\ldots,X_{H})\\right]$ ", "page_idx": 2}, {"type": "text", "text": "Let $\\begin{array}{r}{\\mathbb{H}_{h}=\\prod_{s\\in1:h}\\mathbb{X}_{s}}\\end{array}$ denote the space of histories at time $h$ and $\\begin{array}{r}{\\mathbb{F}_{h}=\\prod_{s\\in h+1:H}\\mathbb{X}_{s}}\\end{array}$ denote the space futures at time $h$ . Similarly, let $\\begin{array}{r}{\\mathbb{H}_{h}^{o}=\\mathsf{o b s}(\\mathbb{H}_{h})=\\prod_{s\\in\\mathcal{O}_{1:h}}\\mathbb{X}_{s}}\\end{array}$ denote the observation component of histories and let $\\begin{array}{r}{\\mathbb{H}_{h}^{a}=\\mathsf{a c t}(\\mathbb{H}_{h})=\\prod_{s\\in\\mathcal{A}_{1:h}}\\mathbb{X}_{s}}\\end{array}$ denote the action component. The observation and action components of the futures, $\\mathbb{F}_{h}^{o}$ and $\\mathbb{F}_{h}^{a}$ respectively, are defined similarly. Here, $\\circ{\\tt b s}(\\cdot)$ and $\\mathtt{a c t}(\\cdot)$ extract the observation and action components, respectively, of any trajectory. ", "page_idx": 2}, {"type": "text", "text": "We define the system dynamics matrix $D_{h}\\,\\in\\,\\mathbb{R}^{|\\mathbb{H}_{h}|\\,\\times\\,|\\mathbb{F}_{h}|}$ as the matrix giving the probability of each possible pair of history and future at time $h$ given the execution of the actions, $\\overline{{[D_{h}]_{\\tau_{h},\\omega_{h}}}}:=$ $\\overline{{\\mathbb{P}}}\\left[\\tau_{h},\\omega_{h}\\right]\\equiv\\mathbb{P}\\left[\\tau_{h}^{o},\\omega_{h}^{o}\\ |\\ \\mathrm{do}(\\tau_{h}^{a},\\omega_{h}^{a})\\right]$ where $\\tau_{h}$ is the history, $\\omega_{h}$ is the future, and $\\tau_{h}^{o},\\tau_{h}^{a},\\omega_{h}^{o},\\omega_{h}^{a}$ separate the history and future into observation components and action components. ", "page_idx": 2}, {"type": "text", "text": "The rank of the dynamics of a sequential decision-making problem is a measure of its complexity. It is defined as the maximal rank of its dynamics matrices. ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Rank of dynamics). The rank of the dynamics $\\{D_{h}\\}_{h\\in[H]}$ $r=\\operatorname*{max}_{h\\in[H]}$ rank $\\left(D_{h}\\right)$ ", "page_idx": 2}, {"type": "text", "text": "2.2  Generalized Predictive State Representations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Predictive state representations (PSR) [31, 32] are a representation of dynamical systems and sequential decision-making problems based on predicting future observations given the past, without explicitly modeling a latent state. In this section, we propose and formalize a generalization of standard PSRs which allows for an arbitrary order of observations and actions. This generalization is necessary to capture POSTs/POSGs (introduced in Section 3), but may also be of independent interest. ", "page_idx": 2}, {"type": "text", "text": "The \u201cPSR rank\u201d of a sequential decision-making problem coincides with the rank of its dynamics (Definition 1). Denote $r_{h}\\sp{\\underline{{\\ \u3001}}}:=\\mathrm{rank}(D_{h})$ . At the heart of predictive state representations is the concept of \u201ccore test sets.\" A core test set at time $h$ is a set of futures such that the vector of probabilities of those futures conditioned on the past encodes all the information that the past contains about the future. ", "page_idx": 2}, {"type": "text", "text": "Definition 2 (Core test sets). $A$ core test set at time $h$ is a subset of $d_{h}~\\geq~r_{h}$ futures, $\\mathbb{Q}_{h}\\;:=$ $\\{q_{h}^{1},\\ldots,q_{h}^{d_{h}}\\}\\subset\\mathbb{F}_{h}$ , such that the submatrix $D_{h}[\\mathbb{Q}_{h}]\\in\\mathbb{R}^{|\\mathbb{H}_{h}|\\times d_{h}}$ is full-rank. ", "page_idx": 2}, {"type": "text", "text": "The $d_{h}$ dimensional vector $\\psi_{h}(\\tau_{h}):=(\\overline{{\\mathbb{P}}}[\\tau_{h},q_{h}^{1}],\\dots,\\overline{{\\mathbb{P}}}[\\tau_{h},q_{h}^{d_{h}}])$ serves as a sufficient statistic for computing the probability of any future conditioned on $\\tau_{h}$ . Intuitively, core test sets relate the rank of the dynamics to a representation based on predicting future outcomes. A PSR parameterization of the system dynamics represents the probability of any trajectory using the sufficient statistics $\\psi_{h}$ via aset of operators that iteratively update the predictive representations at each step as new observations come in. We proceed to formally define our generalized predictive state representation. ", "page_idx": 2}, {"type": "text", "text": "Definition 3 (Generalized Predictive State Representations). Consider a sequential decision-making problem $(X_{h})_{h\\in[H]}$ where $A,{\\mathcal{O}}$ partition $[H]$ into actions and observations, respectively. Then, a generalized predictivestaterepresentationforthissequential decision-makingproblemis a tuple $\\bar{(\\{\\mathbb{Q}_{h}\\}_{0\\le h\\le H-1},\\phi_{H},\\{M_{h}\\}_{1\\le h\\le H-1},\\psi_{0})}$ satisfying ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\mathbb{P}}}\\left[x_{1},\\ldots,x_{H}\\right]=\\phi_{H}(x_{H})^{\\top}M_{H-1}(x_{H-1})\\cdot\\cdot\\cdot M_{1}(x_{1})\\psi_{0}}\\\\ &{\\psi_{h}(x_{1},\\ldots,x_{h})=M_{h}(x_{h})\\cdot\\cdot\\cdot M_{1}(x_{1})\\psi_{0},\\,\\forall h,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "An important condition for the learnability of PSR models, which was used in prior work [including 33-35], is the so-called \u201cwell-conditioning assumption\". We state the analogous assumption for our generalized PSR model below. For a core test set $\\mathbb{Q}_{h}$ , let $\\mathbb{Q}_{h}^{A}=\\mathsf{a c t}(\\mathbb{Q}_{h})$ be the set of core action sequences, let $Q_{A}=\\operatorname*{max}_{h}|\\mathbb{Q}_{h}^{A}|$ be the number of core action sequences, and let $d=\\operatorname*{max}_{h}d_{h}$ ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 ( $\\gamma$ -well-conditioned generalized PSR). A generalized PSR model is said to be $\\gamma$ -well conditioned for $\\gamma>0$ if, for any $h\\in[H]$ and any $\\pi$ it satisfies ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\substack{z\\in\\mathbb R^{d_{h}}}}\\sum_{\\substack{1\\leq1\\,\\omega_{h}\\in\\mathbb R_{h}}}\\pi(\\omega_{h})\\left|m_{h}(\\omega_{h})^{\\top}z\\right|\\leq\\frac{1}{\\gamma},\\quad\\operatorname*{max}_{\\substack{z\\in\\mathbb R^{d_{h}}}}\\sum_{\\substack{x_{h}\\in\\mathbb X_{h}}}\\|M_{h}(x_{h})z\\|_{1}\\,\\pi(x_{h})\\leq\\frac{\\left|\\mathbb Q_{h+1}^{A}\\right|}{\\gamma},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{m_{h}(\\omega_{h})^{\\top}=\\phi_{H}(x_{H})^{\\top}M_{H-1}(x_{H-1})\\cdots M_{h+1}(x_{h+1})\\,a n d\\,\\forall\\omega_{h}^{a},\\,\\sum_{\\omega_{h}^{o}}\\pi(\\omega_{h}^{o},\\omega_{h}^{a})=1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To understand this condition, recall that $m_{h}(\\omega_{h})^{\\top}\\psi_{h}(\\tau_{h})=\\overline{{\\mathbb{P}}}\\left[\\tau_{h},\\omega_{h}\\right]$ . Thus, we can interpret $z$ as the error in estimating the prediction features $\\psi_{h}$ . This condition ensures that the error in estimating the probability of system trajectories remains controlled when the estimation error of $\\psi_{h}$ is small. ", "page_idx": 3}, {"type": "text", "text": "Although we focus on finite spaces $\\mathbb{X}_{h}$ in this work, we briefly discuss possible extensions to continuous spaces. In finite settings, the core tests $q\\in\\mathbb{Q}_{h}$ are represented by future trajectories $q=(x_{h+1},...,x_{H})\\in\\mathbb{F}_{h}$ . In continuous spaces, the core tests become trajectories over subsets of the underlying continuous space: $q=(\\bar{\\chi_{h+1}},\\bar{.}\\,.\\,.\\,,\\bar{\\chi_{H}})\\subset\\mathbb{F}_{h}$ ,where each $\\mathcal{X}_{k}\\subset\\mathbb{X}_{k}$ is a measureable subset. We point to [33] for more discussion on how to extend (standard) PSRs to continuous spaces. ", "page_idx": 3}, {"type": "text", "text": "3  Information Structure ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The \u201cinformation structure\u201d of a sequential decision-making problem defines the causal dependencies between events in the system occurring at different points time, whether those events are observable by the learning agent or not. In this section, we will introduce a novel reinforcement learning model that explicitly represents information structure. We demonstrate that this enables a rich analysis of the system's dynamics and is crucial for characterizing the statistical complexity of general RL problems. ", "page_idx": 3}, {"type": "text", "text": "3.1   Partially-Observable Sequential Teams ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We propose a model of sequential decision-making problems that includes an explicit representation of information structure, which we call partially-observable sequential teams (POST). A POST is a controlled stochastic process consisting of a sequence of variables, where each variable is either a \"system variable or an \u201caction variable'\". POSTs also model the observability of each system variable with respect to the learning algorithm (i.e., which system variables are available to the learning algorithm). The information structure of a POsT describes the causal dependence between these variables. The information set of each system variable describes the subset of past variables that are coupled to it in the dynamics. The information set of an action variable describes the information available to the agent when choosing an action, hence defining the policy class they optimize over. ", "page_idx": 3}, {"type": "text", "text": "Definition 4 (POsT). A partially-observable sequential team is a controlled stochastic process that specifies the joint distribution of $T$ variables $(X_{t})_{t\\in[T]}$ , together with a specification of the observability of eachvariable.Here each $X_{t}$ is either a system variable or an action variable, and is either observable by the learning agent or not. A POST is specified by the following components. ", "page_idx": 3}, {"type": "text", "text": "1. Variable Structures. The variables $\\{X_{t}\\}_{t\\in[T]}$ are partitioned into two disjoint subsets \u2014 system variables and action variables. $\\mathcal S\\subset[T]$ indexes system variables and $A\\subset[T]$ indexes action variables, with $s\\cap A=\\emptyset$ \uff0c $S\\cup A=[T]$   \n2. Information Structure. For $t\\in[T]$ the \u201cinformation set\" $\\mathcal{T}_{t}\\subset[t-1]$ of the variable $X_{t}$ is the set of past variables that are coupled to $X_{t}$ in the dynamics. That is, the transition to $X_{t}$ depends on the value of $I_{t}:=(X_{s}:s\\in\\mathcal{Z}_{t})$ . We call $I_{t}$ the\u201cinformation variable\u201d at time $t$ and call $\\begin{array}{r}{\\mathbb{I}_{t}=\\prod_{s\\in\\mathcal{Z}_{t}}\\mathbb{X}_{s}}\\end{array}$ the \u201cinformation space\".   \n3. System Kernels. For any $t\\ \\in\\ S$ $\\mathcal{T}_{t}\\;\\in\\;\\mathcal{P}(\\mathbb{X}_{t}|\\mathbb{I}_{t})$ is kernel from $\\mathbb{I}_{t}$ to ${\\mathbb X}_{t}$ that specifies the conditional distribution of a systemvariable $X_{t}$ given the information variable $I_{t}$   \n4. Decision Kernels.Each agent chooses a decision kernel $\\pi_{t}\\ :\\ \\mathbb{I}_{t}\\ \\to\\ \\mathcal{P}(\\mathbb{X}_{t})$ specifying a (potentiallyrandomized) policyfor choosing anaction at time $t\\in\\mathcal{A}$ ", "page_idx": 3}, {"type": "text", "text": "5. Observability.We denote the observable system variables by $\\mathcal{O}\\subset\\mathcal{S}$ We require that the information sets of the action variables are observable to the learning algorithm: $\\bar{O}\\supset\\cup_{t\\in A}(\\mathcal{T}_{t}\\cap$ $\\boldsymbol{S}$ ).We define $\\mathcal{U}:=\\mathcal{O}\\cup\\mathcal{A}$ ,andlet $H:=|\\mathcal{U}|$ be the time-horizon of the observable variables. 6. Reward Function. At the end of an episode, the team receives a reward determined by the function $\\begin{array}{r}{R:\\prod_{s\\in\\mathcal{U}}\\mathbb{X}_{s}\\to[0,1]}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "With the above components, any set of decision kernels (i.e., joint policy) $\\pi=(\\pi_{t}:t\\in{\\mathcal{A}})$ induces a unique probability measure over $\\mathbb{X}_{1}\\times\\cdot\\cdot\\cdot\\times\\mathbb{X}_{T}$ , which is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{P}^{\\pi}\\left[x_{1},\\ldots,x_{T}\\right]=\\prod_{t\\in S}{\\mathcal{T}}_{t}(x_{t}|\\left\\{x_{s}:s\\in{\\mathcal{Z}}_{t}\\right\\})\\prod_{t\\in A}\\pi_{t}(x_{t}|\\left\\{x_{s}:s\\in{\\mathcal{Z}}_{t}\\right\\}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We will be interested in modeling the observable dynamics of the POsT. We index the observable variables by their position among the observables $h\\in[H]$ rather than their position among all variables, as follows: $(X_{t})\\mathbf{\\dot{\\iota}}_{t\\in\\mathcal{U}}=(X_{t(1),\\cdot\\cdot\\cdot\\cdot,X_{t(H)}})$ where $t\\ \\dot{:}\\ [\\dot{H}]\\rightarrow\\mathcal{U}\\subset[T]$ maps the index over observables to the index over all variables. The distribution of the observables is obtained by marginalizing over the unobservable variables, $\\begin{array}{r}{\\mathbb{P}^{\\pi}\\left[x_{t(1)},\\cdot\\cdot\\cdot,x_{t(H)}\\right]=\\sum_{s\\in\\mathcal{O}^{\\complement}}\\sum_{x_{s}\\in\\mathbb{X}_{s}}\\mathbb{P}^{\\pi}\\left[X_{1}\\stackrel{<}{=}x_{1},\\stackrel{<}{\\sim}...\\,X_{T}\\stackrel{<}{=}x_{T}\\right]}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "The value of a policy is given by its expected reward $V(\\pi):=\\mathbb{E}^{\\pi}\\left[R(X_{t(1)},\\ldots,X_{t(H)})\\right]$ ,where $\\mathbb{E}^{\\pi}$ is the expectation associated with the probability measure $\\mathbb{P}^{\\pi}$ . The objective of a POsT is to learn a policy $\\bar{\\pi}=(\\pi_{t})_{t\\in\\mathcal{A}}$ which maximizes the expected reward, $\\operatorname{sup}_{\\pi}V(\\mathbf{\\bar{\\pi}})$ . When the variable spaces ${\\mathbb X}_{t}$ are finite, this supremum is attained by a deterministic policy, $\\pi_{t}\\colon\\ensuremath{\\mathbb{I}_{t}}\\to\\ensuremath{\\mathbb{X}_{t}},t\\in\\mathcal{A}$ ", "page_idx": 4}, {"type": "text", "text": "Representation of the information structure as a directed acyclic graph. The information structure of a POST can be naturally represented as a labeled directed acyclic graph (DAG). Given the variable structure and information structure of a POsT, $(S,\\mathcal{A},\\{\\mathcal{T}_{t}\\}_{t})$ , its DAG representation is given by $\\mathcal{G}(\\mathcal{V},\\mathcal{E},\\mathcal{L})$ . The nodes of the graph are the set of variables, $\\dot{\\mathcal{V}}=[T]=S\\cup\\bar{\\mathcal{A}}$ . The edges $\\mathcal{E}\\in\\mathcal{V}\\times\\dot{\\mathcal{V}}$ of the DAG are given by $\\bar{\\mathcal{E}}=\\bar{\\{(i,t):t\\in[T],i\\in\\mathcal{I}_{t}\\}}$ . That is, there exists an edge from $i$ to $t$ if $i$ is in the information set of $t$ . Finally, $\\mathcal{L}$ contains labels for each node as being a system variable (in $\\cal{S}$ or an action variable (in $\\boldsymbol{\\mathcal{A}}$ ). Further, the observability of system variables (in $\\scriptscriptstyle\\mathcal{O}$ ) is also labeled. ", "page_idx": 4}, {"type": "text", "text": "This DAG represents a directed graphical model for the POST. In particular, the probability distribution on $\\mathbb{X}_{1}\\times\\Bar{\\cdot}\\cdot\\cdot\\times\\mathbb{X}_{T}$ factors according to $\\mathcal{G}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[X_{1},\\ldots,X_{T}\\right]=\\prod_{t\\in\\mathcal{V}}\\mathbb{P}\\left[X_{t}\\mid\\operatorname{pa}(X_{t})\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\operatorname{pa}(X_{t})$ is the set of parents of $X_{t}$ in $\\mathcal{G}$ (which are $\\mathcal{T}_{t}$ ). This representation of the information structure as a DAG will be crucial for our analysis of the dynamics of sequential teams in Section 3.2. ", "page_idx": 4}, {"type": "text", "text": "Partially observable sequential games. We define partially-observable sequential games (POSGs) in a similar manner. The main difference is that, in the game setting, there exists an expanded reward structure with $N$ different reward objectives $R^{1},...,\\bar{R}^{N}$ , with different agents pursuing different objectives. The action index set is partitioned into $N$ subsets $\\mathcal{A}=\\mathcal{A}^{1}\\cup\\dot{\\cdot}\\cdot\\cdot\\mathcal{A}^{\\dot{N}}$ ,where $\\mathcal{A}^{i}\\subset\\mathcal{A}$ denotes the action index set associated with the agent(s) optimizing for objective $R^{i}$ . The extension to the game setting is treated in detail in Appendix F. ", "page_idx": 4}, {"type": "text", "text": "3.2 Information Structure Determines the Rank of POSTs/POSGs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "POSTs and POSGs form a highly general framework that captures any sequential decision-making problem that can be described by a controlled stochastic process, subsuming classical models such as MDPs, POMDPs, Dec-POMDPs, and POMGs. By introducing a model with an explicit representation of information structure, we gain the ability to perform a richer analysis of sequential decision-making problems. In particular, we will show that the rank of the dynamics, and ultimately the statistical complexity of reinforcement learning, can be characterized as a function of the information structure. ", "page_idx": 4}, {"type": "text", "text": "We begin by defining the central quantity in our analysis, which we call the \u201cinformation-structural state\", hinting at the role it will play. The information structural state is defined for each point in time as a subset of the past (whether observed or latent) which forms a sufficient statistic for the future. ", "page_idx": 4}, {"type": "text", "text": "Definition 5 (Information-structural state). Let $\\mathcal{G}^{\\dagger}$ be the DAG obtained from $\\mathcal{G}$ by removing all edges directed towards actions. That is, it consists of the edges $\\mathcal{E}^{\\dagger}:=\\mathcal{E}\\setminus\\left\\{(x,a):x\\in\\mathcal{N},a\\in\\right.$ ${\\mathcal{A}}\\}$ . For each $h\\,\\in\\,[H],$ let $\\mathcal{T}_{h}^{\\dagger}\\subset[t(h)]$ be the minimal set of past variables (observed or unobserved) which $d_{\\cdot}$ separates the past observations $\\left(X_{t(1)},\\ldots,X_{t(h)}\\right)$ from the future observations $\\left(X_{t(h+1)},\\ldots,X_{t(H)}\\right)$ in the $D A G\\,\\mathcal{G}^{\\dagger}$ Define $\\begin{array}{r}{\\mathbb{I}_{h}^{\\dagger}:=\\prod_{s\\in\\mathbb{Z}_{h}^{\\dagger}}\\mathbb{X}_{s}}\\end{array}$ as the joint space of those variables. ", "page_idx": 4}, {"type": "text", "text": "The following theorem, whose proof is given in Appendix H, states that the rank of the observable system dynamicsof POSTs and POSGs is bounded by the cardiality of $\\mathbb{I}_{h}^{\\dagger}$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. The rank of the observable system dynamics of a POST or POSG is bounded in terms of its information structureby $r\\leq\\mathrm{max}_{h\\in[H]}|\\mathbb{I}_{h}^{\\dag}|$ ", "page_idx": 5}, {"type": "text", "text": "This result shows that the complexity of the observable system dynamics is characterized by the information structure through $\\mathbb{I}_{h}^{\\dag}$ . In particular, $i_{h}^{\\dagger}\\in\\mathbb{I}_{h}^{\\dagger}$ describes a set of system variables, either observable or latent, which provide a sufficient statistic of the past at time $h$ for predicting future observations- $\\boldsymbol{I}_{h}^{\\dagger}$ \"separates\"\u2019 the past from the future. Hence, the quantity $|\\mathbb{I}_{h}^{\\dagger}|$ admits an interpretation as the size an effective state space at time $h$ . This is a generalization of the standard notion of a latent state. For example, in the case of POMDPs or Dec-POMDPs, the information-structural state indeed coincides with the latent Markovian state (Figure 2a). We emphasize that $\\mathbb{I}_{h}^{\\dagger}$ may contain observable variables as well as unobservable system variables. In fact, unobservable system variables can introduce crucial structure that simplifies the observable system dynamics. ", "page_idx": 5}, {"type": "text", "text": "3.3Examples of Information Structures and their Rank ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, to provide some intuition, we present examples of information structures and apply Theorem 1 to obtain a bound the rank of their observable system dynamics. We see that classical models such as MDPs, POMDPs, and POMGs are special cases of the POST/POSG framework, and known results about their rank [31] are recovered by the generalized graph-theoretic analysis of their information structure. For notational convenience, we adopt a modified notation in this section where informationsets $\\mathcal{T}$ are indexed by the symbol of the variable rather than its time-index. For example, in an MDP, we write $\\mathcal{T}(s_{t})=\\{\\dot{s}_{t-1},\\dot{a_{t-1}}\\}$ for the information set of the state variable $s_{t}$ ", "page_idx": 5}, {"type": "text", "text": "Decentralized POMDPs and POMGs. At each time $t$ , the system variables of a decentralized POMDP (or POMG) consists of a latent state $s_{t}$ , observations for each agent $o_{t}^{1},\\ldots,o_{t}^{N}$ , and actions of each agent $a_{t}^{1},\\ldots,a_{t}^{N}$ Thelte satrasitonareMakviantant action. The observations are sampled via a kernel conditioned on the latent state. Each agent can use their own history of observations to choose an action. Thus, the information structure is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{Z}(s_{t})=\\left\\{s_{t-1},a_{t-1}^{1},\\ldots,a_{t-1}^{N}\\right\\},\\mathbb{Z}(o_{t}^{i})=\\left\\{s_{t}\\right\\},\\mathbb{Z}(a_{t}^{i})=\\left\\{o_{1:t-1}^{i},a_{1:t-1}^{i}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, the observable variables are $\\mathcal{U}=\\{o_{1:T}^{i},a_{1:T}^{i},i\\in[N]\\}$ . By Theorem 1, we have $\\mathbb{Z}^{\\dagger}(o_{t}^{i})=$ $\\mathcal{T}^{\\dagger}(a_{t}^{i})=\\{s_{t}\\}$ \uff0c $\\forall t,i$ , as shown in Figure 2a. Thus, the rank of a Dec-POMDP is bounded by $|\\mathbb{S}|$ where $\\mathbb{S}$ is the state space. Note that in the case of models with a true latent state (e.g., POMDPs, DecPOMDPs, and POMGs), the information-structural state coincides with the Markovian latent state. ", "page_idx": 5}, {"type": "text", "text": "Point-to-Point Real-Time Communication with Feedback. Consider the following model of realtime communication with feedback. Let $x_{t}$ be a Markov source. At time $t$ , the encoder receives the source $x_{t}\\in\\mathbb{X}$ and sends an encoded symbol $z_{t}\\,\\in\\,\\mathbb{Z}$ . The symbol is sent through a memoryless noisy channel which outputs $y_{t}$ to the receiver. The decoder produces the estimate ${\\widehat{x}}_{t}$ . The output of the noisy channel is also fed back to the encoder. The encoder and decoder have full memory of their observations and previous \u201cactions\". The observation variables are $\\mathcal{O}=\\{x_{1:T},\\,y_{1:T}\\}$ and the \"actions\"\u201d are $\\pmb{\\mathcal{A}}=\\{z_{1:T},\\,\\widehat{x}_{1:T}\\}$ . Hence, the information structure is given by the following, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{Z}({x_{t}})=\\left\\{{x_{t-1}}\\right\\},\\;\\mathbb{Z}({z_{t}})=\\left\\{{x_{1:t}},{y_{1:t-1}},{z_{1:t-1}}\\right\\},\\;\\mathbb{Z}({y_{t}})=\\left\\{{z_{t}}\\right\\},\\;\\mathbb{Z}(\\widehat{x}_{t})=\\left\\{{y_{1:t}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "By Theorem 1, we have that, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}^{\\dagger}(x_{t})=\\left\\{x_{t}\\right\\},\\,\\mathcal{Z}^{\\dagger}(z_{t})=\\left\\{x_{t}\\right\\},\\,\\mathcal{Z}^{\\dagger}(y_{t})=\\left\\{x_{t},z_{t}\\right\\},\\,\\mathcal{Z}^{\\dagger}(\\widehat{x}_{t})=\\left\\{x_{t}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Hence, the rank is bounded by $|\\mathbb{X}||\\mathbb{Z}|$ . This is depicted in Figure 2c. ", "page_idx": 5}, {"type": "text", "text": "Limited-memory information structures. Consider a sequential decision making problem with variables $o_{t},a_{t},t\\in[T]$ and an information structure with length- $^{m}$ \"'memory\". That is, observations do not directly depend on variables more than $m$ steps in the past. That is, the information structure is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}(o_{t})=\\left\\{o_{t-m:t-1},a_{t-m:t-1}\\right\\},\\mathcal{Z}(a_{t})=\\left\\{o_{1:t},a_{1:t-1}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The observables consist of all observations and actions, $\\mathcal{U}=\\left\\{o_{1:T},a_{1:T}\\right\\}$ . By Theorem 1 we have that $\\mathcal{T}^{\\dagger}(o_{t})\\,=\\,\\bigl\\{o_{t-m:t-1},a_{t-m:t-1}\\bigr\\}$ , as shown in Figure 2d. Hence, the rank of this sequential decision-making problem is bounded by $|\\mathbb{O}|^{m}|\\mathbb{A}|^{m}$ ", "page_idx": 5}, {"type": "image", "img_path": "QgMC8ftbNd/tmp/1048a09b58cdc5d7e8e54fd2e63227db2ffaa101f15c4674d2a82a04868eb90b.jpg", "img_caption": ["(a) Decentralized POMDP/POMG information-structure. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "QgMC8ftbNd/tmp/34a5f40d80c0e204540a8fcefc68f538c1c67f34a6307654562c4a030c90c547.jpg", "img_caption": ["(d) Limited-memory $(m\\,=\\,2)$ information (e) Fully connected information structure. structures. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 2: DAG representation of various information structures. Solid edges indicate the edges in $\\mathcal{E}^{\\dagger}$ and light edges indicate the information sets of action variables. Grey nodes represent unobservable variables, blue nodes represent past observable variables, green nodes represent future observable variables, and red nodes represent $\\mathcal{T}_{h}^{\\dagger}$ To find $\\mathcal{T}_{h}^{\\dagger}$ , as per Theorem 1, we first remove the incoming edges into the action variables, then we find the minimal set among all past variables (both observable and unobservable)which $d$ -separates the past observations from the future observations. ", "page_idx": 6}, {"type": "text", "text": "The examples above show that the complexity of the dynamics of a sequential decision-making problem depends directly on its information structure. An expanded version of this discussion is provided in Appendix D. Next, we use an information-structural analysis to construct a generalized PSR representation for a class of POSTs/POSGs, which we will ultimately use to prove an upper bound on the statistical complexity of reinforcement learning as a function of information structure. ", "page_idx": 6}, {"type": "text", "text": "4 Constructing a PSR Parameterization for POSTs and POSGs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "A key challenge in reinforcement learning is constructing representations which enable robustly and efficiently modeling probabilities of system trajectories (i.e., probabilities of the form $\\mathbb{P}$ [future | history]). In this section, we will construct a generalized predictive state representation for a class of POSTs/POSGs, ultimately enabling sample-efficient reinforcement learning. ", "page_idx": 6}, {"type": "text", "text": "4.1Core test sets for POSTs/POSGs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Recall that a core test set is a set of futures such that the probabilities of those futures given the past encode all the information that the past contains about the future. For systems with a simple and regular information structure, a core test set may be simple to obtain. For example, undercomplete POMDPs with a full-rank emission matrix admit the 1-step observation futures as core test sets. ", "page_idx": 6}, {"type": "image", "img_path": "QgMC8ftbNd/tmp/17e4cf604212b9505cd248c43a9a1d9723848fe31e1a8447ace7e90fbf43074f.jpg", "img_caption": ["Figure 3: A depiction of the construction of a generalized generalized predictive state representation forPOST/POSG models. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "For POSTs/POSGs with arbitrary information structures, obtaining a core test set is much more challenging without knowing the system dynamics. In this section, we identify a condition in terms of the information structure under which $m$ -step futures are a core test set for POSTs/POSGs. Let us denote the candidae core test set o m-step futuresat time hby Q =IIseuh+t-miaotm We define the matrix $G_{h}$ as encoding the probability of observing each $m$ -step future conditioned on the information-structural state $i_{h}^{\\dagger}\\in\\mathbb{I}_{h}^{\\dagger}$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{G_{h}:=\\left[\\overline{{\\mathbb{P}}}\\big[q\\ \\lvert\\ i_{h}^{\\dagger}\\big]\\right]_{q,i_{h}^{\\dagger}}\\in\\mathbb{R}^{\\lvert\\mathbb{Q}_{h}^{m}\\lvert\\times\\rvert\\mathbb{I}_{h}^{\\dagger}\\rvert},\\ q\\in\\mathbb{Q}_{h}^{m},\\ i_{h}^{\\dagger}\\in\\mathbb{I}_{h}^{\\dagger}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The operational meaning of $G_{h}$ is depicted in Figure 3. Next, we formulate a condition in terms of information structure that we will show implies that the $m$ -step futures are core test sets. ", "page_idx": 7}, {"type": "text", "text": "Definition 6 ( $m$ -step $\\mathcal{T}^{\\dagger}$ -weakly revealing). We say that a sequential team is $m$ -step $\\mathcal{T}^{\\dagger}$ -weakly revealing if for all $h\\in[H]$ $\\operatorname{rank}(G_{h})=|\\mathbb{I}_{h}^{\\dagger}|$ . Furthermore, we say that the sequential team is $\\alpha$ robustly $m$ -step $\\mathcal{T}^{\\dagger}$ -weakly revealing if forall $h\\in[H-m+1],$ $\\sigma_{|\\mathbb{I}_{h}^{\\dagger}|}(\\pmb{G}_{h})\\geq\\alpha$ ", "page_idx": 7}, {"type": "text", "text": "The $\\mathcal{T}^{\\dagger}$ -weakly revealing condition is essentially a statistical identifiability condition. If a POST/POSG is $\\mathcal{T}^{\\dagger}$ -weakly revealing, then, for any two mixtures of the information-structural state, the distributions of the $m$ -step futures are distinct. Formally, for any $\\nu_{1},\\nu_{2}\\,\\in\\,\\mathcal{P}(\\mathbb{I}_{h}^{\\dagger})$ with $\\operatorname{supp}(\\nu_{1})\\cap\\operatorname{supp}(\\nu_{2})=\\emptyset$ ,we have ${\\bf G}_{h}\\nu_{1}\\ne G_{h}\\nu_{2}$ . That is, the future observations contain information that can distinguish between mixtures of the latent information-structural state. The $\\alpha$ -robust version of the $\\mathcal{T}^{\\dagger}$ -weakly revealing condition requires that $G_{h}$ is not only full rank, but that its $|\\mathbb{I}_{h}^{\\dag}|$ -th singular value is bounded away from zero, so that $\\|G_{h}\\nu_{1}-G_{h}\\nu_{2}\\|\\geq\\dot{\\alpha}\\|\\nu_{1}-\\nu_{2}\\|$ ", "page_idx": 7}, {"type": "text", "text": "The condition holds whenever there exists a sequence of actions within the $m$ -step futures such that executing these actions results in a sequence of observations that is informative about the informationstructural stae $i_{h}^{\\dagger}\\in\\mathbb{I}_{h}^{\\dagger}$ . In general, this condition will be harder to satisfy when $\\mathbb{I}_{h}^{\\dagger}$ is large since it would require the $m$ -step future observations to encode more information. In particular, $G_{h}$ cannot be full rank when $|\\mathbb{Q}_{h}^{m}|<|\\mathbb{I}_{h}^{\\dag}|$ . As a heuristic, when we don't have prior knowledge about the dynamics (e.g., in the learning setting), we can choose $m$ such that $|\\mathbb{Q}_{h}^{m}|\\geq|\\mathbb{I}_{h}^{\\dagger}|$ . In general, it will be possible to find a smaller core test set when the $d\\!.$ separating set $\\mathcal{T}_{h}^{\\dagger}$ is small. This happens when the system dynamics contain state-like variables that are low-dimensional. ", "page_idx": 7}, {"type": "text", "text": "The $\\mathcal{T}^{\\dagger}$ -weakly-revealing condition is a generalization of the \u201cweakly-revealing\u201d condition for POMDPs introduced by Liu et al. [36]. Liu et al. [37] proposed an algorithm for learning weaklyrevealing POMGs. Our analysis here recovers weakly-revealing POMGs as a special case and enables learning a much more general class of problems. We note that such an identifiability condition is necessary, and is reflective of the fundamental difficulty of the partially-observable setting. For example, in the case of POMDPs, there exist hardness results that state that if an analogous condition does not hold, the statistical complexity can scale exponentially with the relevant quantities [36]. ", "page_idx": 7}, {"type": "text", "text": "We now proceed to show that under the $\\mathcal{T}^{\\dagger}$ -weakly-revealing condition, $m$ -step futures are core test sets for POSTs/POSGs which can be used to construct a generalized PSR representation amenable to learning. Recall that the vector of core test set probabilities for the history $\\tau_{h}$ is given by the mappings $\\psi_{h},\\overline{{\\psi}}_{h}:\\mathbb{H}_{h}\\rightarrow\\mathbb{R}^{|\\mathbb{Q}_{h}^{m}|}$ ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\n\\psi_{h}(\\tau_{h})=[{\\mathbb{P}}\\left[q^{o},\\tau_{h}^{o}\\;|\\;\\mathrm{d}\\boldsymbol{\\mathrm{o}}(\\tau_{h}^{a}),\\;\\mathrm{d}\\boldsymbol{\\mathrm{o}}(q^{a})\\right]]_{q\\in\\mathbb{Q}_{h}^{m}},\\;\\;\\;\\overline{{\\psi}}_{h}(\\tau_{h})=[{\\mathbb{P}}\\left[q^{o}\\;|\\;\\tau_{h}^{o};\\;\\mathrm{d}\\boldsymbol{\\mathrm{o}}(\\tau_{h}^{a}),\\;\\mathrm{d}\\boldsymbol{\\mathrm{o}}(q^{a})\\right]]_{q\\in\\mathbb{Q}_{h}^{m}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Define the mapping $m_{h}:\\mathbb{F}_{h}\\rightarrow\\mathbb{R}^{|\\mathbb{Q}_{h}^{m}|}$ as, ", "page_idx": 8}, {"type": "equation", "text": "$$\nm_{h}(\\omega_{h}):=(\\mathbf{G}_{h}^{\\dagger})^{\\top}\\,\\left[\\overline{{\\mathbb{P}}}[\\omega_{h}\\,|\\,i_{h}^{\\dagger}]\\right]_{i_{h}^{\\dagger}\\in\\mathbb{I}_{h}^{\\dagger}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The following lemma, whose proof is given in Appendix I, shows that the $m$ -step futures $\\mathbb{Q}_{h}^{m}$ are core test sets for any $m$ -step $\\mathcal{T}^{\\dagger}$ -weakly revealing POST/POSG. In particular, given any future $\\omega_{h}\\in\\mathbb{F}_{h}$ and history $\\tau_{h}\\in\\mathbb{H}_{h}$ , the conditional probability $\\overline{{\\mathbb{P}}}\\left[\\omega_{h}\\mathrm{~}|\\mathrm{~}\\tau_{h}\\right]$ can be written as a linear combination of the core test probabilities $\\overline{{\\psi}}_{h}(\\tau_{h})$ , with weights given by $m_{h}(\\omega_{h})$ , depending only on the future $\\omega_{h}$ ", "page_idx": 8}, {"type": "text", "text": "Lemma 1 (Core test set for POSTs). Suppose that the POST is $m$ -step $\\mathcal{T}^{\\dagger}$ -weaklyrevealing.Then, $\\mathbb{Q}_{h}^{m}$ is a core test set for all $h\\in[H]$ . Furthermore, we have $\\overline{{\\mathbb{P}}}\\left[\\tau_{h},\\omega_{h}\\right]=\\langle m_{h}(\\omega_{h}),\\psi_{h}(\\tau_{h})\\rangle$ and $\\overline{{\\mathbb{P}}}\\left[\\omega_{h}\\ |\\ \\tau_{h}\\right]=\\left\\langle m_{h}(\\omega_{h}),\\overline{{\\psi}}_{h}(\\tau_{h})\\right\\rangle$ ", "page_idx": 8}, {"type": "text", "text": "4.2  Generalized PSR parameterization of POST/POSG ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Consider a POST/POSG which is $m$ -step $\\mathcal{T}^{\\dagger}$ -weakly revealing. Lemma 1 shows that the $m$ -step futures $\\mathbb{Q}_{h}^{m}$ are core test sets. In this section, we will explicitly construct a generalized PSR parameterization for this class of sequential decision-making problems. Moreover, we will show that this generalized PSR representation is well-conditioned when the weakly revealing condition is robust. ", "page_idx": 8}, {"type": "text", "text": "Let $d_{h}:=|\\mathbb{Q}_{h}^{m}|$ . The key observation that allows us to construct the generalized PSR representation is that the vector mappings $m_{h}:\\mathbb{F}_{h}\\rightarrow\\mathbb{R}^{d_{h}}$ and $\\psi_{h}:\\mathbb{H}_{h}\\to\\mathbb{R}^{d_{h}}$ can be used to derive a recursive form of the dynamics of the POST/POSG. We define the operator mapping $M_{h}:\\mathbb{X}_{t(h)}\\to\\mathbb{R}^{d_{h}\\times d_{h-1}}$ by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\left[M_{h}\\big(x_{t(h)}\\big)\\right]_{q,\\cdot}=m_{h-1}\\big(x_{t(h)},q\\big)^{\\top},\\,q\\in\\mathbb{Q}_{h}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "That is, $M_{h}\\bigl(\\v{x}_{t(h)}\\bigr)$ is the matrix whose rows are indexed by the core tests at the $h$ -th observable step, where the row corresponding to each $q\\in\\mathbb{Q}_{h}$ is the weights returned by the mapping $m_{h-1}$ when applied to the future consisting of $x_{t(h)}$ followed by core test $q$ . The operator map $M_{h}$ allows ustudattprabiliftretetfivngaddionaleva $x_{t(h)}$ $\\psi_{h}(x_{t(1)},...,x_{t(h)})\\;=\\;M_{h}(x_{t(h)})\\psi_{h-1}(x_{t(1)},...,x_{t(h-1)}).$ The following result, whose proof is given in Appendix I, states that the set of operators $\\{M_{h}\\}_{h}$ forms a generalized PSR. ", "page_idx": 8}, {"type": "text", "text": "Theorem 2. Consider an $m$ step $\\mathcal{T}^{\\dagger}$ -weakly revealing POST/POSG. Let $\\{M_{h}\\}_{h\\in[H-1]}$ be defined as above and let $\\psi_{0}=\\left[\\overline{{\\mathbb{P}}}\\left[q\\right]\\right]_{q\\in\\mathbb{Q}_{0}^{m}}$ \uff0c $\\phi_{H}\\big(x_{t(H)}\\big)=e_{x_{t(H)}}$ Then, $\\big(\\{\\mathbb{Q}_{h}^{m}\\}_{h},\\phi_{H},\\{M_{h}\\}_{h\\in[H-1]},\\psi_{0}\\big)$ forms a generalized predictive state representation. Moreover, if the weakly-revealing property is $\\alpha$ -robust,then thisPSR is $\\gamma$ -well-conditionedwith $\\gamma=\\alpha/\\operatorname*{max}_{h}\\lvert\\mathbb{I}_{h}^{\\dag}\\rvert^{1/2}$ ", "page_idx": 8}, {"type": "text", "text": "Thus, we have constructed a robust parameterization of the sequential decision-making problem, making use of its information structure, that will enable us to design an efficient learning algorithm. ", "page_idx": 8}, {"type": "text", "text": "5   Characterizing the Statistical Complexity of General Reinforcement Problems via Information Structure ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we establish an upper bound on the achievable sample complexity of general reinforcement learning problems in terms of their information structure. In plain language, this is a result that roughly says \u201cany sequential decision-making problem with an information structure $\\mathcal{T}$ canbe learned with a sample complexity at most $f(\\mathcal{T})^{\\ast}$ . This identifies a class of sequential decision-making problems that are statistically tractable via conditions on the information structure, expanding the set of known-tractable problems while recovering existing tractability results as a special case. ", "page_idx": 8}, {"type": "text", "text": "We will prove this result by exhibiting an algorithm that achieves this upper bound. Our approach will be to use the generalized predictive state representation constructed for POSTs/POSGs in Section 4, which provides a robust representation amenable to learning. We will introduce an algorithm for learning generalized PSRs and prove a corresponding sample complexity result, which will in turn imply a bound on the sample complexity of learning POST/POsG models via the informationstructural characterization of the rank of observable dynamics established in Section 3.2. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "There exist several works in the literature which study learning in PSRs [e.g. 33-35, 38, 39]. Using the technical tools developed in this paper, most of these algorithms can be directly extended to our generalization of PSRs. With such an algorithm, Theorems 1 and 2 then imply a bound on the achievable sample complexity for learning general sequential decision-making problems in terms of their information structure. In this work, we will adapt the model-based UCB-type algorithm of Huang et al. [35], extending it to generalized PSRs to obtain a bound on the achievable sample complexity for POSTs/POSGs. We will defer the details of the algorithm to Appendix E and formally verify the proof in Appendix J. Here, we will focus on discussing the role of information structure in determining the statistical complexity of reinforcement learning. ", "page_idx": 9}, {"type": "text", "text": "The following result states that the size of the information-structural state, $|\\mathbb{I}_{h}^{\\dag}|$ , characterizes an upper bound on the statistical complexity of learning a sequential decision-making problem. ", "page_idx": 9}, {"type": "text", "text": "Theorem 3. Suppose a sequential decision-making problem described by a POST is $\\alpha$ -robustly mstep $\\mathcal{T}^{\\dagger}$ -weakly revealing. Let $Q_{m}:=\\operatorname*{max}_{h}|\\mathbb{Q}_{h}^{m}|$ be the size of the $m$ -step observabletrajectories, and let $A=\\operatorname*{max}_{s\\in\\mathcal{A}}\\lvert\\mathbb{X}_{s}\\rvert$ be the size of largest action space. Then, there exists an algorithm that can learn an $\\epsilon$ -optimal policy with a sample complexity (omitting log factors) bounded by ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\frac{1}{\\epsilon^{2}}\\times\\mathrm{poly}\\left(\\frac{1}{\\alpha},\\operatorname*{max}_{h}\\left|\\mathbb{I}_{h}^{\\dagger}\\right|,Q_{m},A,H\\right).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Underthegame setting,thesameassumptionimpliestheexistenceofaself-playalgorithmthat learns an $\\epsilon$ (Nash or coarse-correlated) equilibrium with the same sample complexity. ", "page_idx": 9}, {"type": "text", "text": "This result identifies $\\mathcal{T}^{\\dagger}$ -weakly revealing POSTs/POSGs as a class of statistically tractable sequential decision-making problems. The sample complexity is polynomial in the size of the informationstructural state space $\\operatorname*{max}_{h}|\\mathbb{I}_{h}^{\\dag}|$ , the size of the action space $A$ , and the time horizon $H$ . Further, it depends on $Q_{m}$ , the size of $m$ -step observable trajectories, and the robustness parameter $\\alpha^{-1}$ , which corresponds to the $m$ -step $\\mathcal{T}^{\\dagger}$ -weakly revealing identifiability condition. We note that the algorithm constructed to prove Theorem 3 only needs to know the parameters of the $\\mathcal{T}^{\\dagger}$ -weakly revealing condition (i.e., $m$ and $\\alpha$ ), and does not need to know the full information structure. ", "page_idx": 9}, {"type": "text", "text": "This result shows that the size of the information-structural state is a fundamental measure of the statistical complexity of reinforcement learning. As a result, learning is tractable when $\\operatorname*{max}_{h}\\vert\\mathbb{I}_{h}^{\\dag}\\vert$ is of modest size, and the information structural state is strongly coupled to the observable system variables. ", "page_idx": 9}, {"type": "text", "text": "One notable special case of POSTs/POSGs is POMDPs. Learning in POMDPs has been studied extensively in the literature. Theorem 3 implies a poly $\\intercal(S,O,A,H,\\Bar{\\alpha}^{-1})\\cdot\\epsilon^{-2}$ bound on the sample complexity of learning in $\\alpha$ weakly POMDPs and $\\mathrm{poly}(S,(O A)^{m},H,\\alpha^{-1})\\cdot\\epsilon^{-2}$ for learning $m$ step weakly revealing POMDPs. This recovers a similar sample complexity as was shown by more specialized analysis tailored to POMDPs [e.g., 36, 40]. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Summary. This paper examines the role of information structure in general reinforcement learning problems. We introduced novel models that explicitly represent information structure, and proved an upper bound on the sample complexity of general reinforcement learning problems in terms of information structure. The central quantity in this upper bound is derived from a graphical representation of the information structure, and admits an interpretation as an effective informationstructural state, generalizing the typical notion of a Markovian state. ", "page_idx": 9}, {"type": "text", "text": "Limitations. The results of this paper are theoretical in nature, and the algorithm proposed to prove our main result is not computationally practical. Current SOTA algorithms for partially-observable algorithms are based on recurrent neural networks or other deep learning models, which create internal representations of latent states based on the history, akin to belief states. Our theory may offer insights into the design of improved architectures in such deep learning-based algorithms. For example, the information structure can be incorporated as an inductive bias of the neural network. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  Hans S Witsenhausen. \u201cThe Intrinsic Model for Discrete Stochastic Control: Some Open ProblemsIn:ControlTheory,Numerical Methods and Computer Systems Modelling.Ed. by M. Beckmann, H. P. Kunzi, A. Bensoussan, and J. L. Lions. Berlin, Heidelberg: Springer Berlin Heidelberg, 1975 (cited on pages 1, 2, 17).   \n[2]   Hans S Witsenhausen. \u201c\"On information structures, feedback and causality)\". In: SIAM Journal on Control (1971) (cited on page 1).   \n[3]   Y. Ho and K. Chu. \u201cTeam decision theory and information structures in optimal control problems-Part IP\". In: IEEE Transactions on Automatic Control (1972) (cited on pages 1, 17).   \n[4]  Y. Ho and K. Chu. \u201c\"On the Equivalence of Information Structures in Static and Dynamic Teams\". In: IEEE Transactions on Automatic Control (1973) (cited on page 1).   \n[5]  Tsuneo Yoshikawa. \u201c\"Decomposition of dynamic team decision problems\". In: IEEE Transactions on Automatic Control (1978) (cited on page 1).   \n[6] Hans S Witsenhausen. \u201cEquivalent Stochastic Control Problems\". In: Mathematics of Control, Signals, and Systems (1988) (cited on page 1).   \n[7]  Mark S Andersland and Demosthenis Teneketzis. \u201cInformation structures, causality, and nonsequential stochastic control I: Design-independent properties\". In: SIAM journal on control and optimization (1992) (cited on page 1).   \n[8] Demosthenis Teneketzis. \u201c\"On Information Structures and Nonsequential Stochastic Control'. In: CWI Quarterly (1996) (cited on page 1).   \n[9]  Aditya Mahajan and Sekhar Tatikonda. \"A Graphical Modeling Approach to Simplifying Sequential Teams\". In: 2009 7th International Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks. 2009 (cited on page 1).   \n[10]   Ashutosh Nayyar, Aditya Mahajan, and Demosthenis Teneketzis. \u201cOptimal Control Strategies in Delayed Sharing Information Structures\"'. In: IEEE Transactions on Automatic Control (2011) (cited on page 1).   \n[11]  Aditya Mahajan, Nuno C. Martins, Michael C. Rotkowitz, and Serdar Yuksel. \u201cInformation Structures in Optimal Decentralized Control\". In: 2012 IEEE 51st IEEE Conference on Decision and Control (CDC). Maui, HI, USA: IEEE, 2012 (cited on pages 1, 17).   \n[12]   Ashutosh Nayyar, Aditya Mahajan, and Demosthenis Teneketzis. \u201c\\*The Common-Information Approach to Decentralized Stochastic Control\". In: Information and Control in Networks. Ed. by Giacomo Como, Bo Bernhardson, and Anders Rantzer. Cham: Springer International Publishing, 2014 (cited on pages 1, 18).   \n[13] Andreas A. Malikopoulos. \u201cOn Team Decision Problems with Nonclassical Information Structures\". 2022. arXiv: 2101. 10992 [math] (cited on page 1).   \n[14]  Naci Saldi and Serdar Yuiksel. \u201cGeometry of Information Structures, Strategic Measures and Associated Stochastic Control Topologies\". In: Probability Surveys (2022) (cited on page 1).   \n[15] Hans S Witsenhausen. \u201cSeparation of estimation and control for discrete time systems\". In: Proceedings of the IEEE (1971) (cited on pages 1, 17).   \n[16]  Richard Bellman.\u201cDynamic programming and stochastic control processes\". In: Information and control (1958) (cited on page 1).   \n[17] Christopher John Cornish Hellaby Watkins. \u201cLearning from delayed rewards\". In: (1989) (cited on page 1).   \n[18]  Satinder Singh, Tommi Jakola, Michael L Littman, and Csaba Szepesvari. \u201cConvergence results for single-step on-policy reinforcement-learning algorithms\". In: Machine learning (2000) (cited on page 2).   \n[19]   Richard S Sutton, Hamid Maei, and Csaba Szepesvari. \u201cA convergent $o(n)$ temporal-difference algorithm for off-policy learning with linear function approximation\". In: Advances in neural information processing systems (2008) (cited on page 2).   \n[20]  R\u00e9mi Munos and Csaba Szepesvari. \u201cFinite-Time Bounds for Fitted Value Iteration.\" In: Journal of Machine Learning Research (2008) (cited on page 2).   \n[21]   Yasin Abbasi- Yadkori and Csaba Szepesvari. \u201c'Regret bounds for the adaptive control of linear quadratic systems\". In: Proceedings of the 24th Annual Conference on Learning Theory. JMLR Workshop and Conference Proceedings. 2011 (cited on page 2).   \n[22] Tor Lattimore and Marcus Hutter. \u201cPAC bounds for discounted MDPs\". In: Algorithmic Learning Theory: 23rd International Conference, ALT 2012, Lyon, France, October 29-31, 2012. Proceedings 23. Springer. 2012 (cited on page 2).   \n[23]  Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, loannis Antonoglou, Daan Wierstra, and Martin Riedmiller. \u201cPlaying Atari with Deep Reinforcement Learning\". 2013. arXiv: 1312.5602 [cs] (cited on page 2).   \n[24]  Jens Kober, J. Andrew Bagnell, and Jan Peters. \\*Reinforcement Learning in Robotics: A Survey\". In: The International Journal of Robotics Research (2013) (cited on pages 2, 18).   \n[25]   Volodymyr Mnih et al. \u201cHuman-Level Control through Deep Reinforcement Learning\". In: Nature (2015) (cited on page 2).   \n[26]  David Silver et al. \"Mastering the Game of Go with Deep Neural Networks and Tree Search\". In: Nature (2016) (cited on pages 2, 18).   \n[27]  Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. \u201cSafe, Multi-Agent, Reinforcement Learning for Autonomous Driving\". 2016. arXiv: 1610.03295 [cs, stat] (cited on pages 2, 18).   \n[28]  Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wojciech M. Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, and Richard Powell.\"Alphastar: Mastering the Real-Time Strategy Game Starcraft Ii'. In: DeepMind blog (2019) (cited on pages 2, 18).   \n[29]   Aditya Mahajan and Sekhar Tatikonda. \u201cAn Axiomatic Approach for Simplification of Sequential Teams\". In: (2011) (cited on page 2).   \n[30]   Aditya Mahajan and Sekhar Tatikonda. \u201cAn Algorithmic Approach to Identify Irelevant Information in Sequential Teams\". In: Automatica (2015) (cited on page 2).   \n[31]  Michael Littman and Richard S Sutton. \u201cPredictive representations of state\". In: Advances in neural information processing systems (2001) (cited on pages 3, 6, 17, 19, 20).   \n[32] Herbert Jaeger. \u201cObservable Operator Models for Discrete Stochastic Time Series\". In: Neural Computation (2000) (cited on pages 3, 17, 19).   \n[33] Qinghua Liu, Praneeth Netrapalli, Csaba Szepesvari, and Chi Jin. \u201cOptimistic MLE - A Generic Model-based Algorithm for Partially Observable Sequential Decision Making\u201d. 2022. arXiv: 2209.14997 [cs, stat] (cited on pages 4, 10, 17,18, 21, 25,30, 39).   \n[34]  Masatoshi Uehara, Ayush Sekhari, Jason D. Lee, Nathan Kalus, and Wen Sun.\u201cProvably Efficient Reinforcement Learning in Partially Observable Dynamical Systems\"'. 2022. arXiv: 2206.12020 [cs, math, stat] (cited on pages 4, 10, 17).   \n[35]  Ruiquan Huang, Yingbin Liang, and Jing Yang.\u201cProvably Efficient UCB-type Algorithms For Learning Predictive State Representations\". 2023. arXiv: 2307. 00405 [cs, stat] (cited on pages 4, 10, 17, 18, 21, 24, 26, 27, 38, 40, 41, 47, 53, 54).   \n[36] Qinghua Liu, Alan Chung, Csaba Szepesvari, and Chi Jin. \\*\"When Is Partially Observable Reinforcement Learning Not Scary? 2022. arXiv: 2204. 08967 [cs, eess, stat] (cited on pages 8, 10, 17, 18, 25, 27).   \n[37] Qinghua Liu, Csaba Szepesvari, and Chi Jin. \u201cSample-Efficient Reinforcement Learning of Partially Observable Markov Games\". 2022. arXiv: 2206 . 01315 [cs, stat] (cited on pages 8, 18, 27).   \n[38] Fan Chen, Yu Bai, and Song Mei. \u201cPartially Observable RL with B-Stability: Unified Structural Condition and Sharp Sample-Eficient Algorithms\". 2022. arXiv: 2209 . 14990 [cs, math, stat] (cited on pages 10, 17, 18, 25, 48).   \n[39] Wenhao Zhan, Masatoshi Uehara, Wen Sun, and Jason D. Lee. \u201cPAC Reinforcement Learning for Predictive State Representations\". 2022. arXiv: 2207. 05738 [cs] (cited on pages 10, 17, 18).   \n[40]  Chi Jin, Sham M. Kakade, Akshay Krishnamurthy, and Qinghua Liu. \u201c\"Sample-Efficient Reinforcement Learning of Undercomplete POMDPs\". 2020. arXiv: 2006.12484 [cs, math, stat] (cited on pages 10, 17).   \n[41] Christos H. Papadimitriou and John N. Tsitsiklis. \u201c\"The Complexity of Markov Decision Processes\". In: Mathematics of operations research (1987) (cited on page 17).   \n[42]  Ashutosh Nayyar, Aditya Mahajan, and Demosthenis Teneketzis. \u201cOptimal control strategies in delayed sharing information structures\"'. In: IEEE Transactions on Automatic Control (2010) (cited on page 17).   \n[43]  Ashutosh Nayyar, Abhishek Gupta, Cedric Langbort, and Tamer Basar. \u201cCommon information based Markov perfect equilibria for stochastic games with asymmetric information: Finite games\". In: IEEE Transactions on Automatic Control (2013) (cited on page 17).   \n[44]   Ashutosh Nayyar, Aditya Mahajan, and Demosthenis Teneketzis. \u201c\"Decentralized stochastic control with partial history sharing: A common information approach\". In: IEEE Transactions on Automatic Control (2013) (cited on page 17).   \n[45]  Yi Ouyang, Hamidreza Tavafoghi, and Demosthenis Teneketzis. \u201c\"Dynamic games with asymmetric information: Common information based perfect bayesian equilibria and sequential decomposition\". In: IEEE Transactions on Automatic Control (2016) (cited on page 17).   \n[46]  Aditya Dave, Nishanth Venkatesh, and Andreas A Malikopoulos. \u201cDecentralized Control of TwoAgents with Nested Accessible Information\". In: 2022 American Control Conference (ACC) IEEE. 2022 (cited on page 17).   \n[47]   Yue Guan, Mohammad Afshari, and Panagiotis Tsiotras. \u201c\"Zero-Sum Games between MeanField Teams: A Common Information and Reachability based Analysis\". 2023. arXiv: 2303. 12243 [eess. SY] (cited on page 17).   \n[48]   Serdar Yuksel and Tamer Basar. \u201cStochastic Teams, Games and Control under Information Constraints\". Springer, 2023 (cited on page 17).   \n[49]  Joseph A Tatman and Ross D Shachter. \u201cDynamic programming and influence diagrams\". In: IEEE transactions on systems, man, and cybernetics (i990) (cited on page 17).   \n[50]  Daphne Kollr and Brian Milch. \u201cMulti-agent infuence diagrams for representing and solving games\". In: Games and economic behavior (2003) (cited on page 17).   \n[51]  Ian Osband and Benjamin Van Roy. \u201cNear-optimal rinforcement learning in factored mdps\". In: Advances in Neural Information Processing Systems (2014) (cited on page 17).   \n[52]  Peter Auer, Thomas Jaksch, and Ronald Ortner. \u201cNear-Optimal Regret Bounds for Reinforcement Learning\". In: Advances in neural information processing systems (2008) (cited on page 17).   \n[53] Shipra Agrawal and Randy Jia. \u201cOptimistic Posterior Sampling for Reinforcement Learning: Worst-Case Regret Bounds\". In: Advances in Neural Information Processing Systems (2017) (cited on page 17).   \n[54]  Mohammad Gheshlaghi Azar, Ian Osband, and R\u00e9mi Munos. \u201cMinimax Regret Bounds for Reinforcement Learning\"'. In: International Conference on Machine Learning. PMLR, 2017 (cited on page 17).   \n[55] Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. \u201cBridging Offine Reinforcement Learning and Imitation Learning: A Tale of Pessimism\". In: Advances in Neural Information Processing Systems (2021) (cited on page 17).   \n[56]  Martin Mundhenk, Judy Goldsmith, Christopher Lusena, and Eric Allender. \u201cComplexity of Finite-Horizon Markov Decision Process Problems\". In: Journal of the ACM (JACM) (2000) (cited on page 17).   \n[57]  Nikos Vlass, Michael L. Littman, and David Barber. \u201cOn the Computational Complexity of Stochastic Controller Optimization in POMDPs\". In: ACM Transactions on Computation Theory (TOCT) (2012) (cited on page 17).   \n[58]  Elchanan Mossel and Sebastien Roch. \u201cLearning Nonsingular Phylogenies and Hidden Markov Models\" In: Proceedings of the Thirty-Seventh Annual ACM Symposium on Theory of Computing. 2005 (cited on page 17).   \n[59]   Akshay Krishnamurthy, Alekh Agarwal, and John Langford. \u201cPAC Reinforcement Learning with Rich Observations\". In: Advances in Neural Information Processing Systems (2016) (cited on page 17).   \n[60]  Yonathan Efroni, Chi Jin, Akshay Krishnamurthy, and Sobhan Miryoosefi. \u201cProvable Reinforcement Learning with a Short-Term Memory\". In: International Conference on Machine Learning. PMLR, 2022 (cited on page 17).   \n[61]  Jiacheng Guo, Zihao Li, Huazheng Wang, Mengdi Wang, Zhuoran Yang, and Xuezhou Zhang. \u201cProvablyEffcient RepresentationLearming with Tractable Planning inLow-Rank POMDP 2023. arXiv: 2306 .12356 [cs . LG] (cited on page 17).   \n[62]  Hongming Zhang, Tongzheng Ren, Chenjun Xiao, Dale Schuurmans, and Bo Dai. \u201cProvable Representation with Efficient Planning for Partially Observable Reinforcement Learning\". 2023. arXiv: 2311. 12244 [cs . LG] (cited on page 17).   \n[63]  Noah Golowich, Ankur Moitra, and Dhruv Rohatgi.\u201cLearning in oservable pomdps, without computationally intractable oracles\". In: Advances in Neural Information Processing Systems (2022) (cited on page 17).   \n[64]  Noah Golowich, Ankur Moitra, and Dhruv Rohatgi. \u201cPlanning in Observable POMDPs in Quasipolynomial Time\". 2022. arXiv: 2201. 04735 [cs .LG] (cited on page 17).   \n[65] Noah Golowich, Ankur Moitra, and Dhruv Rohatgi. \u201cPlanning and Learning in Partially Observable Systems viaFilter Stability\u201d In: Proceedings of the 55th Annual ACM Symposium on Theory of Computing. 2023 (cited on page 17).   \n[66]  Qi Cai, Zhuoran Yang, and Zhaoran Wang. \u201cReinforcement learning from partial observation: Linear function approximation with provable sample effciency\". In: International Conference on Machine Learning. PMLR. 2022 (cited on page 17).   \n[67] Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. \u201cEmbed to Control Partially Observed Systems: Representation Learning with Provable Sample Efficiency\"'. 2022. arXiv: 2205.13476 [cs, eess, stat] (cited on page 17).   \n[68]   Satinder P. Singh, Michael L. Littman, Nicholas K. Jong, David Pardoe, and Peter Stone. \"Learning Predictive State Representations\" In: Proceedings of the 2Oth International Conference on Machine Learning (ICML-03). 2003 (cited on page 17).   \n[69]  Satinder Singh, Michael R James, and Matthew R Rudary. \u201cPredictive State Representations: A New Theory for Modeling Dynamical Systems\". In: Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence (UA12004) (2004) (cited on page 17).   \n[70]  Michael R. James, Satinder Singh, and Michael L. Littman. \u201c'Planning with Predictive State Representations\". In: 2004 International Conference on Machine Learning and Applications, 2004. Proceedings. IEEE, 2004 (cited on page 17).   \n[71]  Peter McCracken and Michael Bowling. \u201cOnline Discovery and Learning of Predictive State Representations\". In: Advances in neural information processing systems (2005) (cited on page 17).   \n[72]  Byron Boots, Sajid M. Siddiqi, and Geofrey J. Gordon.\u201cClosing the Learning-Planning Loop with Predictive State Representations\". In: The International Journal of Robotics Research (2011) (cited on page 17).   \n[73] Nan Jiang, Alex Kulesza, and Satinder Singh. \u201cCompleting State Representations Using Spectral Learning\". In: Advances in Neural Information Processing Systems (2018) (cited on page 17).   \n[74]  Zhi Zhang, Zhuoran Yang, Han Liu, Pratap Tokekar, and Furong Huang. \u201cReinforcement Learning under a Multi-agent Predictive State Representation Model: Method and Theory\". In: International Conference on Learning Representations. 2021 (cited on page 17).   \n[75]  Ahmed Hefny, Carlton Downey, and Geoffrey J. Gordon. \"Supervised Learning for Dynamical System Learning\". In: Advances in neural information processing systems (2015) (cited on page 17).   \n[76]  Han Zhong, Wei Xiong, Sirui Zheng, Liwei Wang, Zhaoran Wang, Zhuoran Yang, and Tong Zhang. \u201cGEC: A Unified Framework for Interactive Decision Making in MDP, POMDP, and Beyond\". 2023. arXiv: 2211. 01962 [cs, math, stat] (cited on pages 17, 18, 48).   \n[77]  Shuang Qiu, Ziyu Dai, Han Zhong, Zhaoran Wang, Zhuoran Yang, and Tong Zhang.\u201cPosterior Sampling for Competitive RL: Function Approximation and Partial Observation\". 2023 arXiv: 2310. 19861 [cs .LG] (cited on pages 17, 18, 48).   \n[78] Zhihan Liu, Miao Lu, Wei Xiong, Han Zhong, Hao Hu, Shenao Zhang, Sirui Zheng, Zhuoran Yang, and Zhaoran Wang.\u201cMaximize to explore: One objective function fusing estimation, planning, and exploration\". In: Advances in Neural Information Processing Systems (2024) (cited on pages 17, 18).   \n[79]  Richard S Sutton and Andrew G Barto. \\*Reinforcement learning: An introduction\". MIT press, 2018 (cited on page 18).   \n[80]  Noam Brown and Tuomas Sandholm. \"Superhuman AI for Multiplayer Poker\". In: Science (2019) (cited on page 18).   \n[81]  Ronen I. Brafman and Moshe Tennenholtz. \u201c\"R-Max-a General Polynomial Time Algorithm for near-Optimal Reinforcement Learning\". In: Journal of Machine Learning Research (2002) (cited on page 18).   \n[82]  Yu Bai, Chi Jin, and Tiancheng Yu. \u201cNear-Optimal Reinforcement Learning with Self-Play\". In:Advances in Neural Information Processing Systems. Ed.by H. Larochell, M. Ranzato, R. Hadsell,M.F Balcan, and H. Lin. Curran Associates, Inc., 2020 (cited on page 18).   \n[83]  Ziang Song, Song Mei, and Yu Bai. \u201c\"When Can We Learn General-Sum Markov Games with a Large Number of Players Sample-Efficiently?\" In: (2021). arXiv: 2110 . 04184 (cited on page 18).   \n[84]  Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. \u201cRegret Minimization in Games with Incomplete Information\". In: Advances in neural information processing systems (2007) (cited on page 18).   \n[85]  Tadashi Kozuno, Pierre M\u00e9nard, R\u00e9mi Munos, and Michal Valko. \u201cModel-Free Learning for Two-Player Zero-Sum Partially Observable Markov Games with Perfect Recall'. In: (2021). arXiv: 2106. 06279 (cited on page 18).   \n[86]  Gabriele Farina and Tuomas Sandholm. \"Model-Free Online Learning in Unknown Sequential Decision Making Problems and Games\". In: Proceedings of the AAAl Conference on Artificial Intelligence. 2021 (cited on page 18).   \n[87]Xiangyu Liu and Kaiqing Zhang. \u201cPartially Observable Multi-agent RL with (Quasi)Efficiency: The Blessing of Information Sharing\". 2024. arXiv: 2308 . 08705 [cs .LG] (cited on page 18).   \n[88]  Jayakumar Subramanian, Amit Sinha, Raihan Seraj, and Aditya Mahajan. \u201cApproximate information state for approximate planning and reinforcement learning in partially observed systems\". In: The Journal of Machine Learning Research (2022) (cited on page 18).   \n[89] Weichao Mao, Kaiqing Zhang, Erik Miehling, and Tamer Basar. \u201cInformation state embedding in partially observable cooperative multi-agent reinforcement learning\". In: 2020 59th IEEE Conference on Decision and Control (CDC). IEEE. 2020 (cited on page 18).   \n[90]  Ali Kara and Serdar Yuksel. Near optimality of finite memory feedback policies in partially observed markov decision processes\". In: Journal of Machine Learning Research (2022) (cited on page 18).   \n[91]   Hsu Kao and Vijay Subramanian. \u201cCommon information based approximate state representations in multi-agent reinforcement learning\"'. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2022 (cited on page 18).   \n[92] Dengwang Tang, Ashutosh Nayyar, and Rahul Jain. \u201cA novel point-based algorithm for multiagent control using the common information approach\". In: 2023 62nd IEEE Conference on Decision and Control (CDC). IEEE. 2023 (cited on page 18).   \n[93] John Nash. \u201cNon-cooperative games\". In: Annals of mathematics (1951) (cited on page 28).   \n[94]   Sara van de Geer. \u201c\"Rates of Convergence for Maximum Likelihood Estimators\". In: Applications of Empirical Process Theory. Reprint. Cambridge Series on Statistical and Probabilistic Mathematics. Cambridge: Cambridge Univ. Pr, 2006 (cited on pages 33, 40).   \n[95] Thomas Verma and Judea Pearl. \u201c\"Causal networks: Semantics and expressiveness\". In: Machine intelligence and pattern recognition. Elsevier, 1990 (cited on page 35).   \n[96]  Varsha Dani, Thomas P Hayes, and Sham M Kakade. \u201c\"Stochastic linear optimization under bandit feedback\". In: (2008) (cited on pages 53, 54).   \n[97]  Yasin Abbasi- Yadkori, David Pal, and Csaba Szepesvari. \u201cImproved algorithms for linear stochastic bandits\". In: Advances in neural information processing systems (2011) (cited on pages 53, 54).   \n[98]  Alexandra Carpentier, Claire Vernade, and Yasin Abbasi- Yadkori. \u201c\"The elliptical potential lemma revisited\". In: (2020). arXiv: 2010 . 10182 [stat . ML] (cited on pages 53, 54). ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Summary of Notation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Generic Sequential Decision-Making Problems ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "QgMC8ftbNd/tmp/392d715a596748552e31b2eb26ed70a4b58fea6d2a78e9115e61d74f364a3162.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "POSTs and POSGs ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "QgMC8ftbNd/tmp/7b29f68b7dd960612f1ba4df0ac0003ac063e8772a4c3d6ee2c0f05112d14bda.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Generalized PSRs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "$\\mathbb{Q}_{h}$ Core test set at time $h$ . Let $d_{h}:=|\\mathbb{Q}_{h}|$ and $d=\\operatorname*{max}_{h}d_{h}$   \n$\\mathbb{Q}_{h}^{A}$ Action component of core test set at time $h$ $\\mathbb{Q}_{h}^{A}=\\mathsf{a c t}(\\mathbb{Q}_{h})$   \n$Q_{A}$ Maximum size of the action component of core test sets. $Q_{A}:=\\operatorname*{max}_{h}|\\mathbb{Q}_{h}^{A}|$   \n$M_{h}$ Observable operators of PSR representation mapping $M_{h}:\\mathbb{X}_{h}\\rightarrow\\mathbb{R}^{d_{h}\\times d_{h-1}}$   \n$\\psi_{h}$ Prediction features. $\\begin{array}{r c l}{\\psi_{h}(\\tau_{h})}&{:=}&{\\left(\\overline{{\\mathbb{P}}}\\left[\\tau_{h},q\\right]\\right)_{q\\in\\mathbb{Q}_{h}}}\\end{array}$ . In PSR, $\\begin{array}{r l}{\\psi_{h}(x_{1},\\dots,x_{h})}&{{}=}\\end{array}$ $M_{h}(x_{h})\\cdot\\cdot\\cdot M_{1}(x_{1})\\psi_{0}$   \n$m_{h}$ Prediction coeffcients. $m_{h}(\\omega_{h})^{\\top}:=\\phi_{H}(x_{H})^{\\top}M_{H-1}(x_{H-1})\\cdot\\cdot\\cdot M_{h+1}(x_{h+1})$ $\\psi_{h}$ Normalized predictionfeatres. $\\begin{array}{r}{\\overline{{\\psi}}_{h}(\\tau_{h}):=\\psi_{h}(\\tau_{h})/\\overline{{\\mathbb{P}}}\\left[\\tau_{h}\\right]=\\left(\\overline{{\\mathbb{P}}}\\left[q\\mid\\tau_{h}\\right]\\right)_{q\\in\\mathbb{Q}_{h}}.}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "General Mathematical Notation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "$\\overline{{\\mathsf{D}_{\\mathrm{TV}}\\left(p,q\\right)}}$ Total variation distance. $\\begin{array}{r}{\\overline{{\\mathsf{D}_{\\mathrm{TV}}\\left(p,q\\right):=\\sum_{x\\in\\mathbb{X}}\\!\\left|p(x)-q(x)\\right|}}}\\end{array}$   \n$\\mathsf{D}_{\\mathtt{H}}^{2}\\left(p,q\\right)$ Hellinger squared distance. $\\begin{array}{r}{\\mathbb{D}_{\\mathtt{H}}^{2}\\left(p,q\\right):=\\frac{1}{2}\\sum_{x\\in\\mathbb{X}}(\\sqrt{p(x)}-\\sqrt{q(x)})^{2}}\\end{array}$   \n$\\sigma_{k}(A)$ $k$ -th largest singular value of the matrix $A$   \n$\\left\\|A\\right\\|_{p}$ The matrx $p$ norm. $\\left\\|A\\right\\|_{p}:=\\operatorname*{max}_{\\left\\|x\\right\\|_{p}=1}\\left\\|A x\\right\\|_{p}$   \n$\\|x\\|_{A}$ The vector norm induced by the positive semi-definite matrix $A$ $\\left\\|x\\right\\|_{A}:={\\sqrt{x^{\\top}A x}}$ $A^{\\dagger}$ Moore-Penrose pseudoinverse.   \n$\\mathcal{P}(\\mathbb{A})$ Space of probability distributions over A. ${\\mathcal{P}}({\\mathbb{B}}|{\\mathbb{A}})$ is the space of kernels from $\\mathbb{A}$ to $\\mathbb{B}$ $\\mathcal{N}_{i:j}$ For an index set $\\mathcal{N}\\subset[H],\\mathcal{N}_{i:j}=\\mathcal{N}\\cap\\{i,\\ldots,j\\}$ ", "page_idx": 15}, {"type": "text", "text": "B Related Work ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The study of information structure in the control literature. In stochastic control, the construct of the \u201cinformation structure\u201d is used to model the structural properties of a system which may restrict the flow, storage, and processing of information. The role of information structure in decentralized control has been extensively studied since Witsenhausen [15] and Ho and Chu [3] began investigating information structures in the context of team decision theory. For example, early work showed that the information structure can determine the tractability of optimal decentralized control problems [15, 41]. More generally, information structure plays an important role in the analysis of multi-agent team decision problems and games, as well as in the design of efficient algorithms, especially in the decentralized setting. See, e.g., Mahajan et al. [11], Nayyar et al. [42-44], Ouyang et al. [45], Dave et al. [46], and Guan et al. [47] and the references therein. We also refer the reader to Yuksel and Basar [48] for a comprehensive overview of the interaction between information and control, including recent progress in the field. The models we propose in this paper are closely related to Witsenhausen's intrinsic model [1], but with some added elements to model partial-observability in the context of reinforcement learning. Related to this is the framework of multi-agent influence diagrams [e.g., 49, 50] which allow for an explicit representation of dependence relations among variables in games. Whereas the above-mentioned work studies the role of information structure in planning and control, we study the the role of information structure in reinforcement learning (i.e., statistical estimation and sample complexity). We note that specific types of structural assumptions related to information structure have been studied in the learning setting as well. For example, previous work has studied factored MDPs [51], which assume an information structure where the state transitions and the reward function are factored, enabling improved learning results. In the present work, we study the role of information structure in reinforcement learning in greater generality, focusing particularly on the partially-observable setting. ", "page_idx": 16}, {"type": "text", "text": "Learning under partial observations. In an MDP, where the system dynamics obey a Markovian property and are fully observable, reinforcement learning has been shown to be both computationally and statistically effcient [e.g., 52-55]. However, under partial observability, reinforcement learning can be computationally and statistically intractable in the worst case, even when assuming a Markovian latent state. Such worst-case hardness results are well-known. For example, [41, 56, 57] show that planning is computational intractable and [58, 59] show that learning is statistically intractable, in the worst-case. In these worst cases, the hardness comes from instances where the observations reveal little information about the latent state, which causes errors in learned representations to be uncontrollable. Accordingly, sub-classes of POMDPs have been identified in recent work where added structural conditions make efficient learning possible. One such condition is decodability [see e.g., 59-62], which assumes that the latent state can be decoded from the current observation (i.e., Block MDP), or an $m$ -step history of observations. Another set of conditions is the \u201cobservability\u201d condition [63-65] and its cousin the \u201cweakly revealing\u201d condition [36, 40] which require different belief states to induce distinguishable distributions over observations. These conditions are further extended to POMDP models in the function approximation setting, where the state or observation spaces are large and function approximators (e.g., linear functions) are used to represent the model. See, e.g., Uehara et al. [34], Guo et al. [61], Zhang et al. [62], Cai et al. [66], and Wang et al. [67]. In our work, we build on the above by identifying a class of POSTs/POSGs which can be learned efficiently. This is significant since POSTs/POsGs are much more general than POMDPs and do not assume the existence of a latent state. ", "page_idx": 16}, {"type": "text", "text": "Predictive state representations. Predictive state representations were introduced by Littman and Sutton [31] building on prior work on observable operator models by Jaeger [32] which proposed the idea of predictive representations as an alternative to belief states for modeling HMMs and POMDPs [see also 68-71]. PSRs are a way to represent the dynamics of a sequential decision-making problem by modeling the (conditional) probabilities of a small set of future trajectories, typically called \u201ccore tests\". In a PSR, the probability of any future trajectory is a deterministic function of the conditional probabilities of the core tests. That is, the probabilities of the core tests encode all the information that the past contains about the future. Littman and Sutton [31] showed that any POMDP can be represented as a PSR. Various reinforcement learning methods for PSRs have been proposed under the assumption that data distribution is explorative, including spectral algorithms [72-74] and supervised learning approaches [75]. In addition, when it comes to the online setting where the algorithm needs to explore, there is a line of work that extends the theory and algorithms for online POMDP learning to PSRs. Moreover, some of these works propose generic theory and algorithms that can be applied to a large class of models including MDPs, POMDPs, and two-player zero-sum dynamic games with partial observability. See, e.g., Liu et al. [33], Huang et al. [35], Liu et al. [36], ", "page_idx": 16}, {"type": "text", "text": "Chen et al. [38], Zhan et al. [39], Zhong et al. [76], Qiu et al. [77], and Liu et al. [78]. Our work is particularly related to the works that combine the idea of optimism in the face of uncertainty [79] and maximum likelihood model estimation [33, 35, 36, 38, 39]. Specifically, our algorithm extends the UCB-type algorithm proposed by Huang et al. [35] for standard PSRs to learn a generalization of PSRs which captures POSTs/POSGs. ", "page_idx": 17}, {"type": "text", "text": "Learning in multi-agent systems. Most applications of interest in reinforcement learning involve the participation of multiple agents in the same environment. Empirical research has achieved striking success in several domains, including for example in the games of Go [26], Starcraft [28], and Poker [80], as well as in robotic control [24] and autonomous driving [27]. There also exists a growing literature of theoretical work. For example, Brafman and Tennenholtz [81], Bai et al. [82], and Song et al. [83] tackle learning in Markov games (MGs)\u2014a generalization of single-agent MDPs that assumes the existence of a Markovian state which is observable by all agents. Another model which has been explored in the literature is imperfect-information extensive-form games (IIEFG), which assumes tree-structured transitions and deterministic emission, and can be viewed as a subclass of partially-observable Markov games (POMGs). Learning under this model has been studied in Zinkevich et al. [84], Kozuno et al. [85], and Farina and Sandholm [86]. More recently, Liu et al. [37] studied reinforcement learning in POMGs using an MLE-based algorithm building on their previous work in the single-agent setting [36]. To address the computational intractability of the planning step for such model-based algorithms, Liu and Zhang [87] proposed a quasi-efficient algorithm for multi-agent POMGs that runs in quasi-polynomial time with quasi-polynomial sample complexity. Their proposed algorithm leverages the common information approach [see 12] to construct an approximate Markov game where the state space of this new game corresponds to the space of approximate common information among agents. The idea of leveraging an informationsharing structure in multi-agent reinforcement learning has also appeared in Subramanian et al. [88], Mao et al. [89], Kara and Yuksel [90], Kao and Subramanian [91], and Tang et al. [92]. ", "page_idx": 17}, {"type": "text", "text": "In each of the above-mentioned models (e.g., MDP, POMDP, MG, IEFG, POMG, etc.), a particular fixed information structure is assumed. We emphasize that the POST/POSG model proposed in our work allows the information structure to be specified arbitrarily and hence captures these models as special cases within a unifying framework. Moreover, our analysis and proposed algorithm significantly expand the class of multi-agent sequential decision-making problems that can be efficiently learned. ", "page_idx": 17}, {"type": "text", "text": "C Preliminaries ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1  Generic Sequential Decision-Making Problems ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Consider a controlled stochastic process $(X_{1},\\allowbreak\\cdot\\cdot,X_{H})$ , where $X_{h}$ is a random variable corresponding to the variable at time $h$ . At each time $h\\in[H]$ , the variable $X_{h}$ may be either an \u2018observation' (i.e., observable system variable) or an ^action'. The dynamics of this stochastic process are described by a tuple $(H,\\{\\mathbb{X}_{h}\\}_{h},\\mathcal{O},\\mathcal{A},\\mathbb{P})$ , where $H$ is the time horizon, $\\mathbb{X}_{h}$ is the variable space at time $h$ (i.e., $X_{h}\\in\\mathbb{X}_{h})$ \uff0c $\\mathcal O\\subset[H]$ is the index set of observations (i.e., $X_{h}$ is an observation if $h\\in{\\mathcal{O}}_{.}$ $\\mathcal{A}\\subset[H]$ is the index set of actions, and $\\mathbb{P}=\\{\\mathbb{P}_{h}\\}_{h\\in\\mathcal{O}}$ is a set of probability kernels which describes the the probability of any trajectory $x_{1},\\ldots,x_{H}$ given that the actions are executed, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left\\{x_{s}:s\\in{\\mathcal{O}}\\right\\}\\mid\\{x_{s}:s\\in{\\mathcal{A}}\\}\\right]=\\prod_{h\\in{\\mathcal{O}}}\\mathbb{P}_{h}\\left[x_{h}\\mid x_{1},\\ldots,x_{h-1}\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A choice of policy $\\pi=\\{\\pi_{h}\\}_{h\\in\\mathcal{A}}$ induces a probability distribution on $\\mathbb{X}_{1}\\times\\cdot\\cdot\\cdot\\times\\mathbb{X}_{H}$ as follows ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}^{\\pi}\\left(x_{1},\\ldots,x_{H}\\right)=\\prod_{h\\in\\mathcal{O}}\\mathbb{P}_{h}\\left(x_{h}\\mid x_{1},\\ldots,x_{h-1}\\right)\\cdot\\prod_{h\\in\\mathcal{A}}\\pi_{h}\\left(x_{h}\\mid x_{1},\\ldots,x_{h-1}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We now define some notation. Let $\\begin{array}{r}{\\mathbb{H}_{h}=\\prod_{s\\in1:h}\\mathbb{X}_{s}}\\end{array}$ denote the space of histories at time $h$ and $\\begin{array}{r}{\\mathbb{F}_{h}=\\prod_{s\\in h+1:H}\\mathbb{X}_{s}}\\end{array}$ denote the space futures at time $h$ . Similarly, let $\\begin{array}{r}{\\mathbb{H}_{h}^{o}=\\mathsf{o b s}(\\mathbb{H}_{h})=\\prod_{s\\in\\mathcal{O}_{1:h}}\\mathbb{X}_{s}}\\end{array}$ denote the observation component of histories and let $\\begin{array}{r}{\\mathbb{H}_{h}^{a}=\\mathsf{a c t}(\\mathbb{H}_{h})=\\prod_{s\\in\\mathcal{A}_{1:h}}\\mathbb{X}_{s}}\\end{array}$ denote the action component. Here, $\\mathscr{O}_{i:j}$ denotes ${\\mathcal{O}}\\cap\\{i,\\ldots,j\\}$ , and similarly for $\\mathscr{A}_{i:j}$ . The observation and action components of the futures, $\\mathbb{F}_{h}^{o}$ and $\\mathbb{F}_{h}^{a}$ respectively, are defined similarly. ", "page_idx": 17}, {"type": "text", "text": "We define the system dynamics matrix $D_{h}\\in\\mathbb{R}^{|\\mathbb{H}_{h}|\\times|\\mathbb{F}_{h}|}$ as the matrix giving the probability of each possible pair of history and future at time $h$ given the execution of the actions, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left[\\boldsymbol{D}_{h}\\right]_{\\tau_{h},\\omega_{h}}=\\overline{{\\mathbb{P}}}\\left[\\tau_{h},\\omega_{h}\\right]=\\mathbb{P}\\left[\\tau_{h}^{o},\\omega_{h}^{o}\\mid\\mathrm{do}(\\tau_{h}^{a},\\omega_{h}^{a})\\right],\\quad\\tau_{h}\\in\\mathbb{H}_{h},\\omega_{h}\\in\\mathbb{F}_{h},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\omega_{h}^{o}=\\mathsf{o b s}(\\omega_{h})$ are is the observation component of the future $\\omega_{h}$ \uff0c $\\omega_{h}^{a}=\\mathsf{a c t}(\\omega_{h})$ is the action component, and similarly for $\\tau_{h}^{o},\\tau_{h}^{a}$ Note that the actions are actively executed via the do-operation. Hence, the system dynamics matrices are independent of any action-selection criteria. Note that $D_{H}\\in\\mathbb{R}^{|\\mathbb{H}_{H}|\\times1}$ is defined as $[D_{H}]_{\\tau_{H}}=\\overline{{\\mathbb{P}}}\\left[\\tau_{H}\\right]$ and $D_{0}=D_{H}^{\\top}$ ", "page_idx": 18}, {"type": "text", "text": "We introduce the notion of the rank of the dynamics. The rank of such a controlled stochastic process is the maximal rank of its dynamics matrices. This is a measure of the complexity of the dynamics. ", "page_idx": 18}, {"type": "text", "text": "Definition (Rank of dynamics; Definition 1). The rank of the dynamics {Dn)he[H] is r = $\\operatorname*{max}_{h\\in[H]}\\operatorname{rank}(D_{h})$ ", "page_idx": 18}, {"type": "text", "text": "This defines the dynamics of the system. A sequential decision-making problem is such a controlled stochastic process together with an objective. The objective is defined by a reward function $R:$ $\\mathbb{X}_{1}\\,\\times\\,\\cdot\\,\\cdot\\,\\times\\,\\mathbb{X}_{H}\\,\\rightarrow\\,[0,1]$ mapping a trajectory to a reward in $[0,1]$ . The agent(s) can affect the dynamics of the system through their choice of actions or policies. Each action $X_{h},h\\in A$ may be chosen by either a single agent or one of several agents (e.g., a team). The policy at time $h\\in{\\mathcal{A}}$ is a mapping $\\pi_{h}:\\mathbb{H}_{h-1}\\stackrel{-}{\\rightarrow}\\mathcal{P}(\\mathbb{X}_{h})$ from previous observations to an action (or a distribution over actions, if randomized). The collection of policies at all time steps is denoted $\\pi=(\\pi_{h}:h\\in{\\mathcal{A}})$ , and induces a probability distribution over trajectories, denoted $\\mathbb{P}^{\\pi}$ . Then, the value of a policy $\\pi$ is the expected value of the reward under the measure $\\mathbb{P}^{\\pi}$ \uff0c $V^{R}(\\pmb{\\pi}):=\\mathbb{E}^{\\pmb{\\pi}}\\left[R(X_{1},\\ldots,X_{H})\\right]$ , where $\\mathbb{E}^{\\pi}$ is the expectation associated with $\\mathbb{P}^{\\pi}$ ", "page_idx": 18}, {"type": "text", "text": "The formalism of sequential decision-making problems introduced in this section is highly generic, but does not explicitly model the information structure. In the next section, we introduce the models of partially observable sequential teams/games, which explicitly represent information structures. We then show that the information structure characterizes the rank of a sequential decision-making problem as per Definition 1. ", "page_idx": 18}, {"type": "text", "text": "C.2  (Generalized) Predictive State Representations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Predictive state representations (PSR) [31, 32] are a model of dynamical systems and sequential decision-making problems based on predicting future observations given the past, without explicitly modeling a latent state. In this section, we propose and formalize a generalization of standard PSRs. ", "page_idx": 18}, {"type": "text", "text": "In the standard formulation of sequential decision-making and predictive state representations, the sequence of variables is such that observations and actions always occur in an alternating manner (i.e, $o_{h},a_{h},o_{h+1},a_{h+1},\\ldots)$ . The POST/POSG models we will propose are more general, and hence require a more flexible formalization of PSRs which allows for arbitrary order of observations and actions as well as arbitrary variable spaces at each time point. This generalization of PSRs will be used in our reinforcement learning algorithms. ", "page_idx": 18}, {"type": "text", "text": "The \u201cPSR rank\u2019 of a sequential decision-making problem coincides with the rank of its dynamics, as defined in Definition 1. Recall that the system dynamics matrix $D_{h}\\in\\mathbb{R}^{|\\mathbb{H}_{h}|\\times|\\mathbb{F}_{h}|}$ is indexed by all possible observable histories $\\tau_{h}$ and futures $\\omega_{h}$ . Denote the rank of the system dynamics at time $h$ by $\\bar{r}_{h}:=\\mathrm{rank}(D_{h})$ ", "page_idx": 18}, {"type": "text", "text": "Consider a sequential decision-making problem as defined in Section 2.1 (i.e., with an arbitrary order of observations and actions, and arbitrary variable spaces). At the heart of predictive state representations is the concept of \u201ccore test sets.\"\u201d A core test set at time $h$ is a set of futures such that the set of probabilities of those futures conditioned on the past encodes all the information that the past contains about the future. This is formalized in the definition below as a set of futures such that the submatrix of the full dynamics matrix restricted to those futures is full rank. ", "page_idx": 18}, {"type": "text", "text": "Definition (Core test sets). $A$ core test set at time $h$ is a subset of $d_{h}~\\geq~r_{h}$ futures, $\\mathbb{Q}_{h}\\;:=$ $\\left\\{q_{h}^{1},\\ldots,q_{h}^{d_{h}}\\right\\}\\subset\\mathbb{F}_{h}$ such that the submatrx $D_{h}[\\mathbb{Q}_{h}]\\in\\mathbb{R}^{|\\mathbb{H}_{h}|\\times d_{h}}$ is fl-ran, $\\operatorname{rank}(D_{h}[\\mathbb{Q}_{h}])=$ $\\operatorname{rank}(D_{h})=r_{h}$ ", "page_idx": 18}, {"type": "text", "text": "A core test set implies the existence of a matrix $W_{h}\\in\\mathbb{R}^{|\\mathbb{F}_{h}|\\times d_{h}}$ such that $D_{h}=D_{h}[\\mathbb{Q}_{h}]\\cdot W_{h}^{\\top}$ ", "page_idx": 18}, {"type": "text", "text": "Denote the $\\tau_{h}$ -th row of $D_{h}[\\mathbb{Q}_{h}]$ by, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\psi_{h}(\\tau_{h}):=\\left(\\overline{{\\mathbb{P}}}\\left[\\tau_{h},q_{h}^{1}\\right],\\ldots,\\overline{{\\mathbb{P}}}\\left[\\tau_{h},q_{h}^{d_{h}}\\right]\\right)\\in\\mathbb{R}^{d_{h}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The vector $\\psi_{h}(\\tau_{h})$ is a sufficient statistic for the history $\\tau_{h}$ in predicting the probabilities of all futures conditioned on $\\tau_{h}$ . This is sometimes called the prediction features of a history $\\tau_{h}$ ", "page_idx": 19}, {"type": "text", "text": "For any integer $d_{h}~\\geq~r_{h}$ , there exists a core test set of size $d_{h}$ . In particular, for any low-rank sequential decision-making problem, there exists a minimal core test set of size $r_{h}$ at each $h$ .However, the minimal core test set depends on the system dynamics matrix $D_{h}$ , which is unknown in the learning setting. In the literature on reinforcement learning in PSRs, it is typically assumed that a core test set is known. We address the problem of constructing a PSR representation for POSTs/POSGs in Section 4. ", "page_idx": 19}, {"type": "text", "text": "For a core test set $\\mathbb{Q}_{h}$ , let $\\mathbb{Q}_{h}^{A}=\\{\\operatorname{act}(q):q\\in\\mathbb{Q}_{h}\\}$ where $\\operatorname{act}(q)$ denotes the action components of the test $q\\in\\mathbb{Q}_{h}$ . Let $Q_{A}=\\operatorname*{max}_{h}\\left|\\mathbb{Q}_{h}^{A}\\right|$ and $d=\\operatorname*{max}_{h}d_{h}$ ", "page_idx": 19}, {"type": "text", "text": "With core test sets defined, we are now ready to present the definition of a generalized predictive state representation. The essential element in a PSR is a set of operators $M_{h}\\ \\stackrel{*}{:}\\mathbb{X}_{h}\\rightarrow\\mathbb{R}^{d_{h}\\stackrel{*}{\\times}\\times d_{h-1}}$ for each time point $h\\in[H]$ . Given the prediction features at time $h-1$ $\\psi_{h-1}(x_{1},\\dots,x_{h-1})\\in\\mathbb{R}^{d_{h-1}}$ the linear map $M_{h}(x_{h})$ computes the prediction features at time $h$ , incorporating the additional observation $x_{h}$ . That is, $\\psi_{h}(\\dot{x_{1}},\\dot{}...\\,,x_{h})\\dot{}=M_{h}(x_{h})\\psi_{h-1}(x_{1},\\dots,x_{h-1})$ . The full definition is given below. ", "page_idx": 19}, {"type": "text", "text": "Definition (Generalized Predictive State Representations; Definition 3). Consider a sequential decision-making problem $(X_{h}\\in\\mathbb{X}_{h})$ where ${\\bar{\\mathcal{A}}},{\\mathcal{O}}$ partition $[H]$ into actions and observations, respectively. Then, a predictive state representation of this sequential decision-making problem is a tuple $\\theta=(\\{\\mathbb{Q}_{h}\\}_{0\\leq h\\leq H-1},\\phi_{H},M,\\bar{\\psi_{0}})$ given by ", "page_idx": 19}, {"type": "text", "text": "1. $\\{\\mathbb{Q}_{h}\\}_{0\\leq h\\leq H-1}$ are core test sts, incldin $h=0$ where $\\mathbb{Q}_{0}\\,=\\,\\{q_{0}^{1},\\dots,q_{0}^{d_{0}}\\}\\,\\subset\\,\\mathbb{F}_{0}$ are core tests before the system begins.   \n2. $\\psi_{0}\\in\\mathbb{R}^{d_{0}}$ is the vector $\\psi(\\emptyset)=(\\overline{{\\mathbb{P}}}[q_{0}^{1}],\\dots,\\overline{{\\mathbb{P}}}[q_{0}^{d_{0}}])$   \n3. ${\\cal M}\\,=\\,\\{{\\cal M}_{h}\\}_{1\\le h\\le H-1}$ is a set of mappings $M_{h}\\,:\\,\\mathbb{X}_{h}\\,\\rightarrow\\,\\mathbb{R}^{d_{h}\\,\\times\\,d_{h-1}}$ , from an observation/action to a matrix of size $d_{h}\\times d_{h-1}$ $\\phi_{H}:\\mathbb{X}_{H}\\to\\mathbb{R}^{d_{H-1}}$ ic a mannina from tha fnal ahcomvation to a $d_{H-1}$ dimoncional voctor ", "page_idx": 19}, {"type": "text", "text": "This tuple satisfies, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\mathbb{P}}}\\left[x_{1},\\ldots,x_{H}\\right]=\\phi_{H}(x_{H})^{\\top}M_{H-1}(x_{H-1})\\cdot\\cdot\\cdot M_{1}(x_{1})\\psi_{0}}\\\\ &{\\psi_{h}(x_{1},\\ldots,x_{h})=M_{h}(x_{h})\\cdot\\cdot\\cdot M_{1}(x_{1})\\psi_{0},\\,\\forall h}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To obtain a probability for a trajectory $\\begin{array}{r l r}{\\tau_{h}}&{{}=}&{(x_{1},\\ldots,x_{h})}\\end{array}$ \uff0cwith $\\textit{b}<\\textit{H}$ \uff0c note that $\\begin{array}{r}{\\sum_{\\omega_{h}\\in\\mathbb{F}_{h}}\\overline{{\\mathbb{P}}}\\left[\\tau_{h},\\omega_{h}\\right]=|\\mathbb{F}_{h}^{a}|\\,\\overline{{\\mathbb{P}}}\\left[\\tau_{h}\\right]\\!.}\\end{array}$ Hence ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\mathbb{P}}}\\left[\\tau_{h}\\right]=\\frac{1}{|\\mathbb{F}_{h}^{a}|}\\displaystyle\\sum_{\\omega_{h}}\\overline{{\\mathbb{P}}}\\left[\\tau_{h},\\omega_{h}\\right]}\\\\ &{\\qquad=\\frac{1}{\\prod_{s\\in h+1:H}\\left|\\mathbb{X}_{s}\\right|^{1\\{s\\in A\\}}}\\displaystyle\\sum_{x_{H}}\\cdots\\sum_{x_{h+1}}\\phi_{H}^{\\top}M_{H}(x_{H})\\cdot\\cdot\\cdot M_{h+1}(x_{h+1})\\psi_{h}(\\tau_{h}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, if we recursively define $\\phi_{h}$ \uff0c $h<H$ via ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{\\left|\\mathbb{X}_{h}\\right|^{1\\{h\\in A\\}}}\\sum_{x_{h}}\\phi_{h}^{\\top}M_{h}(x_{h})=\\phi_{h-1}^{\\top},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with $\\phi_{H}$ as the terminating condition, then, we can obtain $\\overline{{\\mathbb{P}}}\\left[\\tau_{h}\\right]$ for any $h<H$ , via an inner product between $\\phi_{h}$ and $\\psi_{h}(\\tau_{h})$ \uff0c ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\overline{{{\\mathbb{P}}}}\\,[\\tau_{h}]=\\phi_{h}^{\\top}\\psi_{h}(\\tau_{h}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Finally, if we define $\\overline{{\\psi}}_{h}(\\tau_{h})\\,=\\,\\psi_{h}(\\tau_{h})/\\overline{{\\mathbb{P}}}[\\tau_{h}]$ , then we obtain the conditional probability of the core tests given the history, $\\overline{{\\psi}}_{h}(\\tau_{h})=(\\overline{{\\mathbb{P}}}[q_{h}^{1}\\,|\\,\\tau_{h}],\\,.\\,.\\,.\\,.\\,,\\,\\overline{{\\mathbb{P}}}[q_{h}^{d_{h}}\\,|\\,\\tau_{h}])\\in\\mathbb{R}^{d_{h}}$ $\\overline{{\\psi}}_{h}(\\tau_{h})$ is known as the (normalized) prediction feature of the history $\\tau_{h}$ [31]. ", "page_idx": 19}, {"type": "text", "text": "Remark 1 (Generality and difference from standard PSRs). In standard PSRs, observations and actions are assumed to occurin an alternatingmanner,and hence observable operators are defined onpairs of observations andactions(i.e., $M_{h}(o_{h},a_{h}))$ .Thisstructureleadstoasomewhatsimpler description compared to the above. However, our formulation is more general, as it allows each variable to be treated independently,and allows for an arbitrary sequence of variables with arbitrary spaces.This generality will be needed when modeling problems with an explicit representation of informationstructure. ", "page_idx": 20}, {"type": "text", "text": "An important condition for the learnability of PSR models, which was used in prior work [including 33, 35], is the so-called \u201cwell-conditioning assumption\". We state the analogous assumption for our generalized PSR model below. ", "page_idx": 20}, {"type": "text", "text": "Assumption $\\gamma$ -well-conditioned generalized PSR; Assumption 1). A PSR model $\\theta\\quad=$ $\\left(\\left\\{\\mathbb{Q}_{h}\\right\\}_{0\\leq h\\leq H-1},\\phi_{H},M,\\psi_{0}\\right)$ as defned inenii issad $\\gamma$ well ondioned for $\\gamma>0$ if it satisfies ", "page_idx": 20}, {"type": "text", "text": "1. For any $h\\in[H]$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\boldsymbol{z}\\in\\mathbb{R}^{d_{h}}}\\operatorname*{max}_{\\boldsymbol{\\pi}}\\sum_{\\omega_{h}\\in\\mathbb{F}_{h}}\\pi(\\omega_{h}|\\boldsymbol{\\tau}_{h})\\left|m_{h}(\\omega_{h})^{\\top}\\boldsymbol{z}\\right|\\leq\\frac{1}{\\gamma},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "2. For any $h\\in[H-1]$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\substack{||z||_{1}\\leq1}}\\sum_{x_{h}\\in\\mathbb{X}_{h}}\\|M_{h}(x_{h})z\\|_{1}\\,\\pi(x_{h})\\leq\\frac{\\left|\\mathbb{Q}_{h+1}^{A}\\right|}{\\gamma},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "To understand this condition, recall that $m_{h}(\\omega_{h})^{\\top}\\psi_{h}(\\tau_{h})=\\overline{{\\mathbb{P}}}\\left[\\tau_{h},\\omega_{h}\\right]$ . We may think of $z$ in Assumption 1 as representing the error in estimating $\\psi_{h}(\\tau_{h})$ , the probabilities of core tests at time $h$ given the history $\\tau_{h}$ . The $\\gamma$ -well-conditioned assumption ensures that the error in estimating the overall PSR (i.e., the probability of a particular trajectory) does not blow up when the estimation error of $\\psi_{h}(\\tau_{h})$ is small. ", "page_idx": 20}, {"type": "text", "text": "The following result states that any_ sequential decision-making problem of the form described in Section 2.1 admits a generalized PSR representation. The proof and explicit construction are given in Appendix G. ", "page_idx": 20}, {"type": "text", "text": "Proposition 1. Let $(X_{1},\\allowbreak\\cdot\\cdot,X_{H})$ be any sequential decision-making problem with observation index set $\\scriptscriptstyle\\mathcal{O}$ action index et $\\boldsymbol{\\mathcal{A}}$ and variable spaces $\\{\\mathbb{X}_{h}\\}_{h\\in[H]}$ Let $r_{h}=\\mathrm{rank}(D_{h}),$ where $D_{h},h\\in[H]$ are the system dynamics matrices. Then, there exists a PSR representation $\\psi_{0}$ $\\phi_{H}:\\mathbb{X}_{H}\\to\\mathbb{R}^{r_{H-1}}$ $M_{h}:\\mathbb{X}_{h}\\to\\mathbb{R}^{r_{\\hat{h}+1}\\times r_{h}},h\\in[H-1].$ satisfying Definition 3. ", "page_idx": 20}, {"type": "text", "text": "Proof. The proof is given in Appendix G. ", "page_idx": 20}, {"type": "text", "text": "D   Examples of Information Structures and their Rank ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The analysis in Section 3 and Theorem 1 characterizes the rank of any sequential decision-making problem as a function of its information structure. In this section, we illustrate this on several sequential decision-making problems, characterizing the information-structural complexity of their dynamics. The procedure is as follows: 1) formulate the sequential decision-making problem as a POST/POsG; 2) represent the information structure as a labeled directed acyclic graph $\\mathcal{G};\\mathfrak{z}$ remove incoming edges into the action variables to produce $\\mathcal{G}^{\\dagger};\\mathbf{4})$ apply Theorem 1 to find the information structural state at each point in time through a $d_{\\cdot}$ -separation analysis. ", "page_idx": 20}, {"type": "text", "text": "Illustration: translating to the POST/POSG framework. We begin by illustrating how an arbitrary sequential decision-making problem can be formulated in the POST/POsG framework. Consider ", "page_idx": 20}, {"type": "image", "img_path": "QgMC8ftbNd/tmp/6b4a07c0b4a48b10ae0b3968fb8428d6d64f5ad7f8dac5ac97a042ea46138651.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 4: An illustrative example of the information-structural state for POMDPs. Left. The DAG representation of the information structure $\\mathcal{G}$ .Right. The DAG $\\mathcal{G}^{\\dagger}$ is depicted by drawing the edges corresponding to the information sets of the action variables with dotted lines. The informationstructural state coincides with the Markovian state $s_{t}$ , and is depicted in red. Future observables are drawn in green, and past observables are drawn in blue. ", "page_idx": 21}, {"type": "text", "text": "a POMDP with variables $\\left({{s_{1}},{o_{1}},{a_{1}},{s_{2}},{o_{2}},{a_{2}},\\ldots}\\right)$ . This can be formulated as a POST/POSG by a simple relabelling of variables as follows. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c c c c c c c c c c c c c c}{s_{1}}&{o_{1}}&{a_{1}}&{s_{2}}&{o_{2}}&{a_{2}}&{}&{s_{t}}&{o_{t}}&{a_{t}}&{}\\\\ {\\downarrow}&{\\downarrow}&{\\downarrow}&{\\downarrow}&{\\downarrow}&{\\downarrow}&{\\downarrow}&{\\downarrow}&{\\downarrow}&{\\downarrow}&{\\downarrow}&{\\cdots}\\\\ {x_{1}}&{x_{2}}&{x_{3}}&{x_{4}}&{x_{5}}&{x_{6}}&{}&{x_{3t-2}}&{x_{3t-1}}&{x_{3t}}&{}\\end{array}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Here, the system variables $\\boldsymbol{S}$ are the $s_{\\mathrm{{}}}$ -type and $o$ -type variables, with system index set ${\\boldsymbol{S}}\\,=$ $\\{1,2,4,5,7,8,\\ldots\\}$ , and the action variables are the $a$ -type variables with action index set ${\\mathcal{A}}=$ $\\{3,6,9,\\ldots\\}$ . The observable system variables are the $o$ -type variables only, with index set ${\\mathcal{O}}=$ $\\{2,5,8,\\ldots\\}\\subset{\\mathcal{S}}$ . This can be done for any sequential decision-making problem. ", "page_idx": 21}, {"type": "text", "text": "To ease notation, let us not explicitly write the indices in this section, but rather use the original notation for the variables in the problem formulation. For example, we'll write ${\\cal S}=\\{s_{t},o_{t},t\\in\\bar{[T]}\\}$ Similarly, we use the notation $\\overline{{\\mathcal{I}(x)}}$ to mean the information set corresponding to the variable $x$ Similarly, ${\\mathcal{T}}^{\\dagger}(x)$ denotes the information-structural state at the time when $x$ occurs. For example, in a POMDP $\\mathbb{\\bar{\\cal T}}(s_{t})=\\{s_{t-1},a_{t-1}\\}$ $\\mathcal{T}(o_{t})=\\{s_{t}\\}$ , and $\\mathcal{T}(a_{t})=\\{o_{1:t},a_{1:t}\\}$ ", "page_idx": 21}, {"type": "text", "text": "Below, we will consider several examples of sequential decision-making problems, and apply the information-structural analysis of Theorem 1 to obtain a bound on the rank of the observable dynamics (which in turn implies a bound on the sample complexity, by Theorem 3). ", "page_idx": 21}, {"type": "text", "text": "Decentralized POMDPs and POMGs. At each time $t$ , the system variables of a decentralized POMDP (or POMG) consist of a latent state $s_{t}$ . observations for each agent $o_{t}^{1},\\ldots,o_{t}^{N}$ , and actions ofeachagent $a_{t}^{1},\\ldots,a_{t}^{N}$ ThlatstrMavidnts action. The observations are sampled via a kernel conditional on the latent state. Each agent can use their own history of observations to choose an action. Thus, the information structure is given by, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{Z}(s_{t})=\\left\\{s_{t-1},a_{t-1}^{1},\\ldots,a_{t-1}^{N}\\right\\},\\mathbb{Z}(o_{t}^{i})=\\left\\{s_{t}\\right\\},\\mathbb{Z}(a_{t}^{i})=\\left\\{o_{1:t-1}^{i},a_{1:t-1}^{i}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Here, the observable variables are $\\mathcal{U}=\\left\\{o_{1:T}^{i},a_{1:T}^{i},\\,i\\in[N]\\right\\}$ .By Theorem 1, we have $\\mathbb{Z}^{\\dagger}(o_{t}^{i})=$ $\\{s_{t}\\}\\,,\\,\\forall t,i.$ . as shown in Figure 5a. Thus, the rank of a Dec-POMDP is bounded by $|\\mathbb{S}|$ ,where $\\mathbb{S}$ is the state space. Note that in the case of models with a true latent state (e.g., POMDPs, Dec-POMDPs, and POMGs), the information-structural state coincides with the true latent state. ", "page_idx": 21}, {"type": "text", "text": "Limited-memory information structures. Consider a sequential decision making problem with variables $o_{t},a_{t},t\\in[T]$ and an information structure with $m$ -length memory. That is, observations can only depend directly on at most $m$ of the most recent observations and actions. That is, the information structure is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}(o_{t})=\\left\\{o_{t-m:t-1},a_{t-m:t-1}\\right\\},\\,\\mathcal{Z}(a_{t})=\\left\\{o_{1:t},a_{1:t-1}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The observables are all observations and actions, $\\mathcal{U}=\\left\\{o_{1:T},a_{1:T}\\right\\}$ . By Theorem 1 we have that $\\mathcal{T}^{\\dagger}(o_{t})=\\left\\{o_{t-m:t-1},a_{t-m:t-1}\\right\\}$ as shown in Figure 5d. Hence,the rank of this sequential decisionmaking process is bounded by $\\left|\\mathbb{O}\\right|^{m}\\left|\\mathbb{A}\\right|^{m}$ ", "page_idx": 21}, {"type": "image", "img_path": "QgMC8ftbNd/tmp/a42ad47420fd2a46e1df198d6f5b71627b432febe060d38d9e4ccc0b2dc1eb92.jpg", "img_caption": ["(a) Decentralized POMDP/POMG information-structure. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "QgMC8ftbNd/tmp/0f44a3ceb2fe9cb591ecb936400826a6f28580885ca381c7faf1e68629597e20.jpg", "img_caption": ["(b)\u201cMean-field\" information structure. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "QgMC8ftbNd/tmp/3033f94ee98025fb1f3e9238a5cb4383121317849cc6a04bb11ed0a4901a3b1f.jpg", "img_caption": ["(c) Point-to-point real-time communication with feedback information structure. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "QgMC8ftbNd/tmp/3385bbebf9a0e871cbddccaa94f1af1cdde0808f3a8241c1961a3ebed384b75f.jpg", "img_caption": ["(d) Limited-memory $(m\\,=\\,2)$ information (e) Fully connected information structure. structures. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 5: DAG representation of various information structures. Solid edges indicate the edges in $\\mathcal{E}^{\\dagger}$ and light edges indicate the information sets of action variables. Grey nodes represent unobservable variables, blue nodes represent past observable variables, green nodes represent future observable variables, and red nodes represent the information structural state $\\mathcal{T}_{h}^{\\dagger}$ . To find $\\mathcal{T}_{h}^{\\dagger}$ Ih, as per Theorem 1, we first remove the incoming edges into the action variables, then we find the minimal set among all past variables (both observable and unobservable) which $d$ -separates the past observations from the future observations. ", "page_idx": 22}, {"type": "text", "text": "Symmetric / \u201cMean-field\u2019 Information Structures. Consider a sequential decision-making problem with $N$ agents. Each agent has their own local state, $s_{t}^{i}\\in\\mathbb{S}_{\\mathrm{local}}$ . Similarly, at each time point, each agent take an action $a_{i}^{t}\\in\\mathbb{A}_{\\mathrm{loc}}$ The global state $s_{t}=(s_{t}^{1},\\ldots,s_{t}^{N})\\in\\mathbb{S}_{\\mathrm{loc}}^{N}=:\\mathbb{S}$ is composedby oflaa $\\mathbb{A}:=\\mathbb{A}_{\\mathrm{loc}}^{N}$ Consdera symetie information structure where the evolution of each agent's local state depends only on a symmetric aggregation of all agents\u2019 states and actions, rather than on the local state/action of any particular agent. That is, the identity of who is in what state or takes which action does not matter\u2014only the distribution of states and actions. This is often referred to as a \u201cmean-field\" setting (in the limit). Here, the transition depends only on the distribution of local states and actions, defined as smf = dist(st) :=s,at $\\begin{array}{r}{a_{t}^{\\mathrm{mf}}^{\\star}{=}\\,\\mathrm{dist}(\\dot{a_{t}}):=\\frac{1}{N}\\delta_{a_{t}^{i}}}\\end{array}$ for $s_{t}\\in\\mathbb{S},a_{t}\\in\\mathbb{A}$ Diferent agents can have different transition kernels for their local state. Hence, by introducing $\\mathrm{dist}(s_{t}),\\mathrm{dist}(a_{t})$ as auxiliary unobserved variables at each time $t$ , we obtain the following information structure, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}(s_{t}^{i})=\\{\\mathrm{dist}(s_{t-1}),\\mathrm{dist}(a_{t-1})\\}\\,,\\;\\mathcal{Z}(a_{t}^{i})=\\{s_{t}^{i}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and an application of Theorem 1 bounds the rank by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\mathbb{I}^{\\dagger}(s_{t}^{i})\\right|<|\\mathbb{S}_{\\mathrm{loc}}||\\mathbb{A}_{\\mathrm{loc}}|\\left(\\frac{N}{|\\mathbb{S}_{\\mathrm{loc}}|-1}+1\\right)^{|\\mathbb{S}_{\\mathrm{loc}}|-1}\\left(\\frac{N}{|\\mathbb{A}_{\\mathrm{loc}}|-1}+1\\right)^{|\\mathbb{A}_{\\mathrm{loc}}|-1}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This is compared to $|\\mathbb{S}_{\\mathrm{loc}}|^{N}\\cdot|\\mathbb{A}_{\\mathrm{loc}}|^{N}$ (e.g., if we modeled this as an MDP with the state $s_{t}$ ),which is much larger when the number of agents is large. The information structure and $d$ -separation decomposition are depicted in Figure 5b. ", "page_idx": 23}, {"type": "text", "text": "Point-to-Point Real-Time Communication with Feedback. Consider the following model of realtime communication with feedback. Let $x_{t}$ be the Markov source. At time $t$ , the encoder receives the source $x_{t}\\in\\mathbb{X}$ and encodes sending a symbol $z_{t}\\in\\mathbb{Z}$ . The symbol is sent through a memoryless noisy channel which outputs $y_{t}$ to the receiver. The decoder produces the estimate ${\\widehat{x}}_{t}$ . The output of the noisy channel is also fed back to the encoder. The encoder and decoder have full memory of their observations and previous \u201cactions\". The observation variables are $\\mathcal{O}=\\{x_{1:T},\\,y_{1:T}\\}$ and the \"actions\" are $\\pmb{\\mathcal{A}}=\\{z_{1:T},\\,\\hat{x}_{1:T}\\}$ . Hence, the information structure is given by the following, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{Z}({x_{t}})=\\left\\{{x_{t-1}}\\right\\},\\;\\mathbb{Z}({z_{t}})=\\left\\{{x_{1:t}},{y_{1:t-1}},{z_{1:t-1}}\\right\\},\\;\\mathbb{Z}({y_{t}})=\\left\\{{z_{t}}\\right\\},\\;\\mathbb{Z}(\\widehat{x}_{t})=\\left\\{{y_{1:t}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By Proposition 1, we have that, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}^{\\dagger}(x_{t})=\\left\\{x_{t}\\right\\},\\,\\mathcal{Z}^{\\dagger}(z_{t})=\\left\\{x_{t}\\right\\},\\,\\mathcal{Z}^{\\dagger}(y_{t})=\\left\\{x_{t},z_{t}\\right\\},\\,\\mathcal{Z}^{\\dagger}(\\widehat{x}_{t})=\\left\\{x_{t}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence, the rank is bounded by $|\\mathbb{X}||\\mathbb{Z}|$ . This is depicted in Figure 5c. ", "page_idx": 23}, {"type": "text", "text": "Fully-Connected Information Structures. Consider a sequential decision making problem with variables $o_{t},a_{t},t\\in[T]$ and a fully-connected information structure. That is, each observation directly depends on the entire history of observations and actions. Thus, the information structure is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}(o_{t})=\\left\\{o_{1:t-1},a_{1:t-1}\\right\\},\\;\\mathcal{Z}(a_{t})=\\left\\{o_{1:t},a_{1:t-1}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The observables are all observations and actions, $\\mathcal{U}=\\left\\{o_{1:T},a_{1:T}\\right\\}$ . By Theorem 1 we have that $\\mathcal{T}^{\\dagger}(o_{t})=\\left\\{o_{1:t-1},a_{1:t-1}\\right\\}$ ashown in Figure 5e. Hence, the rank of tis sequential decision-making process can be exponential in the time horizon. ", "page_idx": 23}, {"type": "text", "text": "The examples above show that the tractability of a sequential decision-making problem in terms of the complexity of its dynamics depends directly on its information structure. This gives an interpretation of why certain models, like POMDPs, are more tractable than those with arbitrary information structures. Previous work primarily considers particular problem classes with fixed and highly regular information structures. In this work we argue for the importance of explicitly modeling the information structure of a sequential decision-making problem. ", "page_idx": 23}, {"type": "text", "text": "Remark 2 (Necessity of generalized PSRs). The formalization of generalized PSRs in Section 2.2 wasnecessarytoenablethestudyofinformationstructurethroughPOsTs/POsGs.Analternative (naive) solution to construct PSR representations for models with non-alternating observations and actions is to aggregate consecutive observations and actions to force them to obey the standard formulation of PsRs. This approach results in a loss of \u201cresolution\" in the information structure. That is, when you aggregate consecutive system variables, you also aggregate the DAG which represents the information structure, losing potentially important structure. In particular, in the worst case, such aggregation could result in an exponential increase in the rank of the dynamics. The examples given above elucidate this. Consider for example the\u201cmean-field\"information structure. If we aggregated local states and actions into a combined global state and joint action,thePSRrankwould indeed be $|\\mathbb{S}_{\\mathrm{loc}}|^{N}|\\mathbb{A}_{\\mathrm{loc}}|^{N}$ . By comparison, by considering each local state separately without aggregation, we areabletoobtainadecompositionwithamuchsmallerPSRrank. ", "page_idx": 23}, {"type": "text", "text": "E  UCB-Type Reinforcement Learning Algorithm for Generalized PSRs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We will adapt the model-based UCB-type algorithm of Huang et al. [35], extending it to generalized PSRs, including those representing POSTs. The algorithm involves the estimation of an upper confidence bound which captures the uncertainty in the estimated model and drives exploration so as to minimize this uncertainty. The UCB-based approach has the advantage of providing a last-iterate guarantee and requiring a weaker notion of planning oracle (a standard planning oracle instead of an optimistic planning oracle as required by similar algorithms). The technical contribution of this section is to extend the algorithm and its theoretical guarantees to generalized PSRs. The tools developed in doing so can be used to directly extend any other PSR-based algorithm to generalized PSRs. ", "page_idx": 23}, {"type": "text", "text": "When learning generalized PSRs, we suppose that the core test sets $\\{\\mathbb{Q}_{h}\\}_{0\\le h\\le H-1}$ are known by the algorithm. For example, if the sequential decision-making problem is a POST, Section 4 provides conditions under which $m$ -step futures form core test sets. Let $\\Theta$ be the set of $\\gamma$ -well-conditioned generalized PSR representations with $\\{\\mathbb{Q}_{h}\\}_{0\\leq h\\leq H-1}$ as core test sets. Denote by $\\overline{{\\Theta}}_{\\epsilon}$ an optimistic $\\epsilon$ -cover of $\\Theta$ (defined formally in Appendix $\\mathrm{G}$ ) ", "page_idx": 24}, {"type": "text", "text": "Recall that $d_{h}:=|\\mathbb{Q}_{h}|$ and $d=\\operatorname*{max}_{h}d_{h}$ . Moreover, $\\mathbb{Q}_{h}^{A}:={\\mathsf{a c t}}(\\mathbb{Q}_{h})$ are the action components of the core test sets and $Q_{A}:=\\operatorname*{max}_{h}\\left|\\mathbb{Q}_{h}^{A}\\right|$ is the maximal size of those action components. We define the exploration action sequences at time $h$ to be $\\mathbb{Q}_{h-1}^{\\mathrm{exp}}=\\mathsf{a c t}(\\mathbb{X}_{h}\\times\\mathbb{Q}_{h}\\cup\\mathbb{Q}_{h-1})$ . Moreover, we define $\\mathtt{u}_{h-1}^{\\mathtt{e x p}}$ as the policy, defned from time $h-1$ onwards, in which each selection of action sequences in $\\mathbb{Q}_{h-1}^{\\mathrm{exp}}$ are chosen uniformly at random. For a model $\\theta$ and reward function $R$ , we define the value of a policy under this model and reward as $\\begin{array}{r}{V_{\\theta}^{R}(\\pi):=\\sum_{\\tau_{H}}R(\\tau_{H})\\mathbb{P}_{\\theta}^{\\pi}(\\tau_{H})}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "The algorithmic description is given in Algorithm 1. At each iteration $k$ , the learner collects a trajectory $\\tau_{H}^{k,h}$ for each time index $h\\in[H]$ byusing aparticular policy that drives explorationso as to better estimate the parameters associated with the $h$ -th time step. To collect the trajectory $\\tau_{H}^{k,h}$ , the learner executes the policy at the previous iteration, $\\pi^{k-1}$ , until time $h-1$ collecting the trajectory Th-1 then executes $\\mathtt{u}_{h-1}^{\\mathtt{e x p}}$ which samples action sequences from $\\mathbb{Q}_{h-1}^{\\exp}$ uniformly. The particularchoiceof thexploratryactinequns $\\mathbb{Q}_{h-1}^{\\mathrm{exp}}$ comes out of the proof (see proof of Lemma 5 in the appendix). Intuitively, $\\mathsf{a c t}(\\mathbb{Q}_{h-1})$ allows us to estimate the prediction features $\\overline{{\\psi}}^{\\ast}(\\tau_{h-1}^{k,h})=[\\overline{{\\mathbb{P}}}(q\\,|\\,\\tau_{h-1}^{k,h})]_{q\\in\\mathbb{Q}_{h-1}}$ and $\\mathsf{a c t}(\\mathbb{X}_{h}\\times\\mathbb{Q}_{h})$ allows us to estimate $M_{h}^{*}(x_{h})\\overline{{\\psi}}^{*}(\\tau_{h-1}^{k,h})$ ", "page_idx": 24}, {"type": "text", "text": "The collected trajectories are added to the dataset, together with the policies used to collect them. The next step is model estimation via (constrained) maximum likelihood estimation. The algorithm estimates a model $\\widehat{\\theta}^{k}$ by selecting any model in a constrained set $B^{k}$ defined as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Theta_{\\operatorname*{min}}^{k}=\\left\\{\\theta\\in\\Theta:\\forall h,\\,(\\tau_{h},\\pi)\\in\\mathcal{D}_{h}^{k},\\,\\mathbb{P}_{\\theta}^{\\pi}(\\tau_{h})\\geq p_{\\operatorname*{min}}\\right\\},}\\\\ &{\\mathcal{B}^{k}=\\left\\{\\theta\\in\\Theta_{\\operatorname*{min}}^{k}:\\displaystyle\\sum_{(\\tau_{H},\\pi)\\in\\mathcal{D}^{k}}\\log\\mathbb{P}_{\\theta}^{\\pi}(\\tau_{H})\\geq\\displaystyle\\operatorname*{max}_{\\theta^{\\prime}\\in\\Theta_{\\operatorname*{min}}^{k}}\\sum_{(\\tau_{H},\\pi)\\in\\mathcal{D}^{k}}\\log\\mathbb{P}_{\\theta^{\\prime}}^{\\pi}(\\tau_{H})-\\beta\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The introduction of $\\Theta_{\\mathrm{min}}^{k}$ ensures that $\\mathbb{P}_{\\theta^{*}}^{\\pi^{k-1}}(\\tau_{h-1}^{k,h})$ is not to small sotha thestmates of the prediction features $\\overline{{\\psi}}^{\\ast}(\\tau_{h-1}^{k,h})\\,=\\,[\\overline{{\\mathbb{P}}}(q\\,|\\,\\tau_{h-1}^{k,h})]_{q\\in\\mathbb{Q}_{h-1}}$ are accurate. This design differs from other MLE-based estimators [e.g., 33, 36, 38] due to the estimation of parameters capturing conditional probabilities. ", "page_idx": 24}, {"type": "text", "text": "Next, the algorithm chooses a policy which drives the algorithm to trajectories $\\tau_{h}$ whoseprediction features have so far been unexplored. To do this, Algorithm 1 constructs an upper confidence bound on the total variation distance between the estimated model and the true model. This is done via a bonusfunction $\\widehat{b}^{k}(\\tau_{H})$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{b}^{k}(\\tau_{H})=\\operatorname*{min}\\left\\{\\alpha\\sqrt{\\displaystyle\\sum_{h=0}^{H-1}\\left\\lVert\\widehat{\\overline{{\\psi}}}(\\tau_{h})\\right\\rVert_{(\\widehat{U}_{h}^{k})^{-1}}^{2}},1\\right\\},\\quad\\mathrm{where},}\\\\ &{\\qquad\\widehat{U}_{h}^{k}=\\lambda I+\\displaystyle{\\sum_{\\tau_{h}\\in\\mathcal{D}_{h}^{k}}\\widehat{\\overline{{\\psi}}}^{k}}(\\tau_{h})\\widehat{\\overline{{\\psi}}}^{k}(\\tau_{h})^{\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\lambda$ and $\\alpha$ are pre-specified parameters to the algorithm. Thus, the bonus function captures the degree of uncertainty in the estimated prediction features $\\widehat{\\overline{{\\psi}}}(\\tau_{h})$ . In particular, the bonus $\\widehat{b}(\\tau_{H})$ will be large for trajectories whose prediction feature $\\widehat{\\overline{{\\psi}}}(\\tau_{h})$ lie far away from the empirical distribution of prediction features sampled in the dataset $\\mathcal{D}_{h}^{k}$ . This is captured by computing the norm with respect to the covariance $\\widehat{U}_{h}^{k}$ ", "page_idx": 24}, {"type": "text", "text": "The algorithm then chooses an exploration policy for the next iteration which maximizes this upper confidence bound, hence collecting trajectories that have high uncertainty in their prediction features. When the estimated model is sufficiently accurate on all trajectories, the algorithm terminates and returns the optimal policy with respect to the reward function $R$ under the estimated model. ", "page_idx": 24}, {"type": "text", "text": "for k < 1, ... , K do for h \u2190 1, ... , H do Collect TH $\\tau_{H}^{k,\\dot{h}}=(\\omega_{h-1}^{k,h},\\tau_{h-1}^{k,h})$ using $\\nu(\\pi^{k-1},\\mathsf{u}_{h-1}^{\\mathsf{e x p}})$ $\\mathcal{D}_{h-1}^{k}\\leftarrow\\mathcal{D}_{h-1}^{k-1}\\cup\\left\\{\\left(\\tau_{H}^{k,h},\\nu\\left(\\pi^{k-1},\\mathfrak{u}_{h-1}^{\\tt e x p}\\right)\\right)\\right\\}$ $\\begin{array}{r}{\\mathcal{D}^{k}=\\left\\{\\mathcal{D}_{h}^{k}\\right\\}_{h=0_{\\star}}^{H-1}}\\end{array}$ ${\\widehat{\\theta\\,}}\\in B^{k}$ $\\begin{array}{r l}&{\\Theta_{\\operatorname*{min}}^{k}=\\left\\{\\theta:\\forall h,(\\tau_{h},\\pi)\\in\\mathcal{D}_{h}^{k},\\mathbb{P}_{\\theta}^{\\pi}(\\tau_{h})\\geq p_{\\operatorname*{min}}\\right\\},}\\\\ &{\\mathcal{B}^{k}=\\left\\{\\theta\\in\\Theta_{\\operatorname*{min}}^{k}:\\displaystyle\\sum_{(\\tau_{H},\\pi)\\in\\mathcal{D}^{k}}\\log\\mathbb{P}_{\\theta}^{\\pi}(\\tau_{H})\\geq\\displaystyle\\operatorname*{max}_{\\theta^{\\prime}\\in\\Theta_{\\operatorname*{min}}^{k}}\\sum_{(\\tau_{H},\\pi)\\in\\mathcal{D}^{k}}\\log\\mathbb{P}_{\\theta^{\\prime}}^{\\pi}(\\tau_{H})-\\beta\\right\\}.}\\end{array}$ $\\begin{array}{r l}&{\\mathrm{Define~the~bonus~function},\\widehat{b}^{k}(\\tau_{H})=\\operatorname*{min}\\Bigg\\{\\alpha\\sqrt{\\sum_{h=0}^{H-1}\\left\\lVert\\widehat{\\overline{{\\psi}}}(\\tau_{h})\\right\\rVert_{(\\widehat{U}_{h}^{k})^{-1}}^{2}},1\\Bigg\\},\\mathrm{w}}\\\\ &{\\widehat{U}_{h}^{k}=\\lambda I+\\sum_{\\tau_{h}\\in\\mathcal{D}_{h}}\\widehat{\\overline{{\\psi}}}^{k}(\\tau_{h})\\widehat{\\overline{{\\psi}}}^{k}(\\tau_{h})^{\\top}.}\\end{array}$ here Solve hain bizebs $\\pi^{k}=\\arg\\operatorname*{max}_{\\pi}V_{\\widehat{\\theta}^{k}}^{\\widehat{b}^{k}}(\\pi)$ \uff0c $V_{\\widehat{\\theta}^{k}}^{\\widehat{b}^{k}}(\\pi^{k})\\leq\\epsilon/2$ then ${\\widehat{\\theta}}^{\\epsilon}={\\widehat{\\theta}}^{k}$ . break. end   \nend ", "page_idx": 25}, {"type": "text", "text": "We extend Huang et al.'s theoretical guarantees to show that Algorithm 1 enjoys polynomial sample complexity for generalized PSRs (Definition 3). ", "page_idx": 25}, {"type": "text", "text": "Theorem 4. Suppose Assumption 1 holds. Suppose the parameters $p_{\\operatorname*{min}},\\lambda,\\alpha,\\beta$ are chosen appropriately.In particular,let ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{\\operatorname*{min}}\\le\\frac{\\delta}{K H\\prod_{h=1}^{H}\\vert\\mathbb{X}_{h}\\vert},\\;\\lambda=\\frac{\\gamma\\operatorname*{max}_{s\\in\\mathcal{A}}\\vert\\mathbb{X}_{s}\\vert^{2}Q_{\\mathcal{A}}\\beta\\operatorname*{max}\\{\\sqrt{r},Q_{\\mathcal{A}}\\sqrt{H}/\\gamma\\}}{\\sqrt{d H}},}\\\\ {\\alpha=O\\left(\\frac{Q_{\\mathcal{A}}\\sqrt{d H\\lambda}}{\\gamma^{2}}+\\frac{\\operatorname*{max}_{s\\in\\mathcal{A}}\\vert\\mathbb{X}_{s}\\vert\\,Q_{\\mathcal{A}}\\sqrt{\\beta}}{\\gamma}\\right),\\;\\beta=O\\left(\\log\\left\\vert\\overline{{\\Theta}}_{\\varepsilon}\\right\\vert\\right),\\;\\varepsilon\\le\\frac{p_{\\operatorname*{min}}}{K H}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then, with probability at least $1-\\delta$ Algorithm $^{l}$ returns a model $\\theta^{\\epsilon}$ and a policy $\\pi$ that satisfy ", "page_idx": 25}, {"type": "equation", "text": "$$\nV_{\\theta^{\\epsilon}}^{R}(\\pi^{*})-V_{\\theta^{\\epsilon}}^{R}(\\pi)\\leq\\varepsilon,\\,a n d\\,\\forall\\tilde{\\pi},\\,\\mathsf{D}_{\\mathsf{T V}}\\left(\\mathbb{P}_{\\theta^{\\epsilon}}^{\\tilde{\\pi}}(\\tau_{H}),\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau_{H})\\right)\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In addition, the algorithm terminates with a sample complexity of, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tilde{O}\\left(\\left(r+\\frac{Q_{A}^{2}H}{\\gamma^{2}}\\right)\\cdot\\frac{r d H^{3}\\cdot\\operatorname*{max}_{s\\in\\mathcal{A}}|\\mathbb{X}_{s}|^{2}\\cdot Q_{A}^{4}\\beta}{\\gamma^{4}\\epsilon^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. The proof is given in Appendix J. ", "page_idx": 25}, {"type": "text", "text": "This result shows that the sample complexity of learning a generalized PSR depends on the problem size through a few key quantities. In particular, the sample complexity scales polynomially in the underlying rank $r$ , the dimension of the PSR parameterization $d$ , the size of the action component of the core tests $Q_{A}$ , the time horizon $H$ , the conditioning number $\\gamma^{-1}$ , the size of the action spaces $\\operatorname*{max}_{s\\in A}|\\mathbb{X}_{s}|$ , the log covering number $\\log\\left|\\overline{{\\Theta}}_{\\epsilon}\\right|$ , and the desired suboptimality error $\\epsilon$ . Note that $\\tilde{O}$ omits logarithmic dependence. ", "page_idx": 25}, {"type": "text", "text": "To apply this algorithm to a POST, we can use the generalized PSR parameterization constructed in Section 4. By Theorem 1 the PSR rank is bounded by $r\\,\\leq\\,\\operatorname*{max}_{h}\\lvert\\mathbb{I}_{h}^{\\dagger}\\rvert$ . If this POST is $\\alpha$ robustly $\\mathcal{T}^{\\dagger}$ -weakly revealing, then by Theorem 2 it admits a $\\gamma$ -well-conditioned generalized PSR parameterization with $\\gamma=\\alpha/\\operatorname*{max}_{h}\\lvert\\mathbb{I}_{h}^{\\dag}\\rvert^{1/2}$ and the $m$ -step futures as core test sets. Moreover, we have $d\\,=\\,\\operatorname*{max}_{h}d_{h}\\,=\\,\\operatorname*{max}_{h}|\\mathbb{Q}_{h}^{m}|$ The following corollary states that Algorithm 1 can learn a partially-observable sequential team with a sample complexity which is polynomial in the size of the information-structural state space $\\operatorname*{max}_{h}\\vert\\mathbb{I}_{h}^{\\dag}\\vert$ ", "page_idx": 26}, {"type": "text", "text": "Corollary 1. Suppose a partially-observable sequential team is $m$ step $\\alpha$ -robustly $\\mathcal{T}^{\\dagger}$ -weakly revealing asperDefinition $^{6}$ \uff1aApplyingAlgorithm $I$ to this $P S R$ representation,withparameters $p_{\\operatorname*{min}},\\lambda,\\alpha,\\beta$ chosen as in Theorem 4, returns a $\\varepsilon$ -optimal policy with a sample complexity of, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Tilde{O}\\left(\\left(1+\\frac{Q_{A}^{2}H}{\\alpha^{2}}\\right)\\frac{\\operatorname*{max}_{h}\\lvert\\mathbb{I}_{h}^{\\dagger}\\rvert^{7}\\cdot\\operatorname*{max}_{h}\\lvert\\mathbb{Q}_{h}^{m}\\rvert\\cdot H^{5}\\cdot\\operatorname*{max}_{s\\in A}\\lvert\\mathbb{X}_{s}\\rvert^{2}\\cdot\\operatorname*{max}_{s\\in\\mathcal{U}}\\lvert\\mathbb{X}_{s}\\rvert\\cdot Q_{A}^{4}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We can interpret this result as saying that the information structure of a sequential decision-making problem, through the quantity $\\bar{\\mathrm{max}_{h}}|\\mathbb{I}_{h}^{\\dagger}|$ , is fundamentally a measure of the complexity of the dynamics which need to be modeled. As a result, learning is tractable when $\\operatorname*{max}_{h}\\vert\\mathbb{I}_{h}^{\\dag}\\vert$ is of modest size, and intractable otherwise. Recall that $\\operatorname*{max}_{h}|\\mathbb{I}_{h}^{\\dag}|$ is small when there exists \u201cstate-like\u201d variables, whether they are observable or unobservable. In this sense, $\\operatorname*{max}_{h}|\\mathbb{I}_{h}^{\\dag}|$ is a fundamental quantity which generalizes the notion of a \u201cstate'\". For example, in the case of an $m$ -step $\\alpha$ -weakly revealing POMDP, our algorithm has a sample complexity of $\\mathrm{poly}(S,(O A)^{m},H,\\alpha^{-1})\\cdot\\dot{\\epsilon}^{-2}$ , where $S$ is the size of the state space, $O$ is the size of the observation space, and $A$ is the size of the action space. This is similar to the sample complexity of [36, 37], which designed an algorithm tailored specifically for weaklyrevealing POMDPs. Our algorithms, together with the POST/POSG models, enable sample-efficient reinforcement learning for a much broader class of models all within a unified framework. ", "page_idx": 26}, {"type": "text", "text": "In this section, we extended the algorithm in Huang et al. [35] to generalized PSRs, enabling sample. efficient learning_ of POSTs. We emphasize that other PSR-based algorithms can be extended in a similar manner. In the next section, we tackle the problem of learning in the game setting where different agents have different objectives. ", "page_idx": 26}, {"type": "text", "text": "F  Extension to the game setting ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "F.1  Partially Observable Sequential Games ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In a POST, all agents share the same objective. In the game setting, different agents may have different objectives which compete with each other in interesting ways. Information structures play a crucial role in the study of games. The information available to one agent when making its decisions, compared to the information available to competing agents, determines how well it can achieve its objective. In particular, the information structure of a problem determines the set of equilibria it admits. There has been a plethora of work in the game theory community studying such problems. ", "page_idx": 26}, {"type": "text", "text": "Analogously to partially-observable sequential teams, we define partially-observable sequential games (POSGs). The dynamics of a POSG are identical to a POST, with the same formalization of variable structure, variable spaces, information structure, system kernels, and decision kernels. In contrast to a POST, agents in a POSG may have different objectives. In a POsG, there exists $N$ agents, with agent $i\\in[N]$ deciding the actions at times $t\\in A^{i}$ , where $A^{i}\\subset A$ . Each agent has its own objective defined by a reward function $R^{i}$ . This is defined formally below. ", "page_idx": 26}, {"type": "text", "text": "Definition 7 (Partially-Observable Sequential Game Model). A partially-observable sequential game (PosG) is a controlled stochastic process consisting of the following components: variable structure,variable spaces,information structure,systemkernels,decisionkernels,and observability. ThesearedefinedinanidenticalmannertoDefinition 4.Additionally,POsGsdefinea reward structureasfollows.Let $N$ be the number of agents.Each agent may act several times.Denoteby ", "page_idx": 26}, {"type": "text", "text": "$A^{i}\\subset A$ the index of action variables associated to agent $i\\in[N]$ .Each agent has a reward function $\\begin{array}{r}{R^{i}:\\prod_{t\\in\\mathcal{U}}\\mathbb{X}_{t}\\to[0,1]}\\end{array}$ which they aim to maximize. ", "page_idx": 26}, {"type": "text", "text": "Denote by $\\pi^{i}=(\\pi_{t}:t\\in\\mathcal{A}^{i})$ the collection of decision kernels belonging to agent $i$ , one for each action they take. Denote by $\\pmb{\\pi}=(\\pi^{1},\\cdot\\cdot\\cdot,\\pi^{N})$ the collection of all agents policies. Fixing $\\pi$ induces a probability distribution over $\\mathbb{X}_{1}\\times\\cdot\\cdot\\mathbb{X}_{T}$ in the same way as in the team setting. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}^{\\pi}\\left[X_{1}=x_{1},\\ldots X_{T}=x_{t}\\right]=\\prod_{t\\in\\mathcal{S}}\\mathcal{T}_{t}(x_{t}|\\left\\{x_{s}:s\\in\\mathbb{Z}_{t}\\right\\})\\prod_{t\\in\\mathcal{A}}\\pi_{t}(x_{t}|\\left\\{x_{s}:s\\in\\mathbb{Z}_{t}\\right\\}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The value of a policy $\\pi$ for agent $i\\in[N]$ is defined as the expected value of their reward $R^{i}$ under $\\mathbb{P}^{\\pi}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\nV^{i}(\\pi)\\equiv V^{i}(\\pi^{i},\\pi^{-i}):=\\mathbb{E}^{\\pi}\\left[R^{i}(X_{t(1)},\\dots,X_{t(H)})\\right],\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\pi^{-i}=(\\pi^{j}:j\\neq i)$ ", "page_idx": 27}, {"type": "text", "text": "The nature of randomization in agents? policies is crucial to the analysis of solution concepts in the game setting. To model randomized policies, which are potentially correlated, we introduce a random seed $\\omega\\in{\\bar{\\Omega}}$ which is sampled at the beginning of an episode. Then, the policy at time $t\\in\\mathcal{A}$ can be modeled as a deterministic function mapping the seed $\\omega$ and information variable $i_{t}\\in\\mathbb{I}_{t}$ to an action ${\\mathbb X}_{t}$ . That is, $\\pi_{t}:\\Omega\\times\\mathbb{I}_{t}\\to\\mathbb{X}_{t}$ . To model independently randomized policies with each agent having private randomness, we consider the special case where the seed has the product structure $\\omega\\,=\\,\\bar{(\\omega_{1},\\ldots,\\omega_{N})}\\,\\in\\,\\Omega_{1}\\,\\times\\,\\cdot\\,\\cdot\\,\\times\\,\\Omega_{N}$ , and $\\omega_{i}$ is the seed belonging to agent $i\\,\\,\\in\\,[N]$ . Then, for $t\\in A^{i}$ \uff0c $\\pi_{t}:\\Omega_{i}\\times\\mathbb{I}_{t}\\to\\mathbb{X}_{t}$ . For each agent $i\\in[N]$ , define the three policy spaces, ", "page_idx": 27}, {"type": "text", "text": "Deterministic policies, $\\Gamma_{\\mathrm{det}}^{i}=\\left\\{\\pi^{i}:\\pi^{i}=\\left(\\pi_{t}:\\mathbb{I}_{t}\\to\\mathbb{X}_{t},t\\in\\mathcal{A}^{i}\\right)\\right\\}$ ", "page_idx": 27}, {"type": "text", "text": "2. Independently-randomized policies, $\\Gamma_{\\mathrm{ind}}^{i}=\\bigl\\{\\pi^{i}:\\pi^{i}=\\big(\\pi_{t}:\\Omega_{i}\\times\\mathbb{I}_{t}\\rightarrow\\mathbb{X}_{t},t\\in\\mathcal{A}^{i}\\big)\\bigr\\},$   \n3. Correlated randomized policies, $\\Gamma_{\\mathrm{cor}}^{i}=\\{\\pi^{i}:\\pi^{i}=\\left(\\pi_{t}:\\Omega\\times\\mathbb{I}_{t}\\rightarrow\\mathbb{X}_{t},t\\in\\mathcal{A}^{i}\\right)\\}.$ ", "page_idx": 27}, {"type": "text", "text": "Define the jint detministic poliy spa, as $\\mathbf{\\deltaD}_{\\mathrm{det}}\\,=\\,\\Gamma_{\\mathrm{det}}^{1}\\,\\times\\,\\cdot\\,\\cdot\\,\\times\\,\\Gamma_{\\mathrm{det}}^{N}$ and silarly for the independently-randomized policy space $\\Gamma_{\\mathrm{ind}}$ , and the correlated randomized policy space $\\mathbf{\\deltaT}_{\\mathrm{cor}}$ ", "page_idx": 27}, {"type": "text", "text": "When studying games, a common question is to find an equilibrium within a particular policy space. At a high-level, an equilibrium is a joint policy where no agent can do better by deviating from their policy when the other agents keep their policies fixed. We will consider several notions of equilibrium. We begin by defining the notion of a best-response. Suppose that agent $i$ 'spolicy space is $\\Gamma^{i}$ (e.g., $\\Gamma_{\\mathrm{det}}^{i}$ $\\bar{\\Gamma}_{\\mathrm{ind}}^{i}$ or $\\Gamma_{\\mathrm{cor.}}^{i}$ .Then,we saythat agent $i$ 'splicy $\\dot{\\pi}^{i}$ is a best response to $\\overline{{\\pi}}^{\\bar{-}i}$ if tere is no policy in $\\Gamma^{i}$ which achieves a higher value. This is formalized in the definition below. ", "page_idx": 27}, {"type": "text", "text": "Definition 8 (Best response). For a joint policy $\\pi$ $\\pi^{i}$ is said to be a best-response to $\\pi^{-i}$ in the policy space $\\Gamma^{i}$ (e.g.,. $\\bar{\\Gamma}_{\\mathrm{det}}^{i}$ $\\Gamma_{\\mathrm{ind}}^{i}$ or $\\Gamma_{\\mathrm{{cor}\\,}}^{i},$ \uff0c $\\bar{i}f V^{i}(\\pi^{i},\\pi^{-i})=\\operatorname*{max}_{\\tilde{\\pi}^{i}\\in\\Gamma^{i}}V^{i}(\\tilde{\\pi}^{i},\\bar{\\pi}^{-i})=:V^{i,\\dagger}(\\pi^{-i})$ ", "page_idx": 27}, {"type": "text", "text": "This leads to the definition of two notions of equilibria. A Nash Equilibrium (NE) is a joint policy where all agents are best-responding in the space of independently-randomized policies. A Coarse Correlated Equilibrium (CCE) is a joint policy where all agents are best-responding in the space of correlated randomized policies. The difference between NE and CCE is that the randomness in the joint policy must be independent in an NE but can be correlated in a CCE. Since $\\Gamma_{\\mathrm{ind}}\\subset\\Gamma_{\\mathrm{cor}}$ ,coarse correlated equilibria are a generalization of Nash equilibria. We define them formally below. ", "page_idx": 27}, {"type": "text", "text": "Definition 9 (Nash Equilibrium). A joint policy $\\pi\\in\\Gamma_{\\mathrm{{ind}}}$ is said to be a Nash equilibrium if for all agents $i\\in[N]$ $\\begin{array}{r}{V^{i}(\\pmb{\\pi})=\\operatorname*{max}_{\\tilde{\\pi}^{i}\\in\\Gamma_{\\mathrm{ind}}^{i}}\\bar{V}^{i}\\big(\\tilde{\\pi}^{i},\\dot{\\pi^{-i}})=:V^{i,\\dagger}(\\pmb{\\pi}^{-i})}\\end{array}$ A joint policy $\\pi\\in\\Gamma_{\\mathrm{{ind}}}$ is said to an $\\varepsilon$ -approximateNashequilibriumif $^{:}V^{i}({\\pmb\\pi})\\geq V^{i,\\dagger}({\\pmb\\pi}^{-i})-\\varepsilon$ for all $i\\in[N]$ ", "page_idx": 27}, {"type": "text", "text": "Definition 10 (Coarse Correlated Equilibrium). A joint policy $\\pi\\,\\in\\,\\Gamma_{\\mathrm{{cor}}}$ is said to be a coarse correlated equilibrium if for allagents $i\\in[N],\\,V^{i}(\\pmb{\\pi})=\\dot{\\mathrm{max}}_{\\tilde{\\pi}^{i}\\in\\Gamma_{\\mathrm{cor}}^{i}}\\,V^{i}(\\tilde{\\pi}^{i},\\pmb{\\pi}^{-i})=:V^{i,\\dagger}(\\pmb{\\pi}^{-i})$ .A joint policy $\\pi\\in\\Gamma_{\\mathrm{{cor}}}$ is said to an $\\varepsilon$ -approximateNashequilibriumif $V^{i}(\\pmb{\\pi})\\geq V^{i,\\dagger}(\\pmb{\\pi}^{-i})-\\varepsilon$ for all $i\\in[N]$ ", "page_idx": 27}, {"type": "text", "text": "Since we consider finite-space sequential games, an equilibrium is guaranteed to exist [93]. ", "page_idx": 27}, {"type": "text", "text": "Remark 3 (Notion of equilibrium can be represented through information structure). The policy classes defined above(i.e.,deterministic,independently-randomized,correlated randomized) can be directly modeled by the information structure.For example,to represent correlated randomized policies,therandomseed $\\omega\\in\\Omega$ canbemodeled as anobservablevariable at time $t=0$ whichis in all agents\u2019information sets.Similarly,independently randomized policies can be represented through a different random seed for each agent at time $t=0$ and including theappropriaterandomseed in each action's information set.Hence,the information structure itself can decide which equilibrium notion we are interested in. Moreover, this allows us to consider additional notions of equilibrium where, for example,only subsets of agents can be correlated with each other (e.g., this may be useful in modelingmulti-teamproblems).Notethat addingrandomseedsinorderto modelrandomized policies does not affect the information-structural state $\\mathcal{T}_{h}^{\\dagger}$ sincetheseedsdon't appear in $\\mathcal{G}^{\\dagger}$ .For concreteness,wefocus onNE and CCEin ourpresentation. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "F.2  Characterizing the Sample Complexity of Learning an Equilibrium via a Self-Play Algorithm as a function of the Information Structure ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We now introduce a sample-efficient reinforcement learning algorithm for learning well-conditioned generalized predictive state representations in the game setting with each agent having their own objective. In particular, since partially-observable sequential games with a $\\mathcal{T}^{\\dagger}$ -weaklyrevealing information structure admit a well-conditioned generalized PSR representation, they can also be learned sample-efficiently by this algorithm. ", "page_idx": 28}, {"type": "text", "text": "The algorithm we propose is a self-play algorithm for learning an equilibrium of the dynamic game problem. That is, the algorithm specifies the policies of all agents during the learning phase, collecting the trajectory of observables at each episode to improve its estimate of the system dynamics. This can be thought of as a centralized agent playing against itself. We will propose an algorithm which can find a Nash equilibrium or coarse correlated equilibrium in a sample-efficient manner. We begin with some preliminaries. ", "page_idx": 28}, {"type": "text", "text": "Game setting. Recall that a sequential decision-making problem falls within the game setting if each agent has their own objective. Following Section 2.1, we consider a sequential decisionmaking problem $(X_{1},\\hdots,X_{H})$ where $\\scriptscriptstyle\\mathcal{O}$ denotes the index set of (observable) system variables and $\\boldsymbol{\\mathcal{A}}$ denotes the set of action variables. We suppose the game involves $N$ agents, and denote the action index set of each agent by $A^{i}\\subset A$ , where $\\{{\\dot{A}}^{i}\\}_{i\\in[N]}$ partitions $\\boldsymbol{\\mathcal{A}}$ . Each agent has their own reward function $R^{i}(X_{1},\\ldots,X_{H})$ . Note that POSGs as defined in Definition 7 are structured models which fall within this framework. ", "page_idx": 28}, {"type": "text", "text": "Equilibria and policy classes. Recall that in the game setting, the type of randomization in each agent's policy affects the set of equilibria in the game. In Appendix F.1, we formalized this randomization by introducing a random seed $\\omega\\in{\\Omega}$ and allowing each agent's policy to be a function of their information set and this seed. If the seed has a product structure $\\boldsymbol{\\omega}=(\\omega_{1},\\dots,\\omega_{N})$ with each agent observing their own seed, this results in independently-randomized policies, denoted by $\\Gamma_{\\mathrm{ind}}^{i}$ If all agents use the same seed, this results in correlated randomized policies, which we denote by $\\Gamma_{\\mathrm{cor}}^{i}$ AnequilriumamngindependenlyranomizedpolcieiscalledaNahequilbriumandan equilibrium among correlated randomized policies is called a coarse correlated equilibrium. ", "page_idx": 28}, {"type": "text", "text": "Estimating probabilities in the planner. The probability of any trajectory under a joint policy $\\pi$ is given by $\\begin{array}{r}{\\mathbb{P}^{\\pmb{\\pi}}(\\tau_{H})\\,=\\,\\sum_{\\omega}\\overline{{\\mathbb{P}}}\\left[\\tau_{H}\\right]\\pmb{\\pi}(\\tau_{H}|\\omega)\\mathbb{P}\\left[\\omega\\right]}\\end{array}$ , where $\\overline{{\\mathbb{P}}}\\left[\\tau_{H}\\right]\\,=\\,\\mathbb{P}\\left[\\tau_{H}^{o}\\ |\\ \\tau_{H}^{a}\\right]$ as before, and $\\begin{array}{r}{\\pi(\\tau_{H}\\,|\\,\\omega)=\\prod_{h\\in\\mathcal{A}}\\mathbf{1}\\{x_{h}=\\pi_{h}(\\tau_{h-1},\\omega)\\}}\\end{array}$ . Recall that the probabilities $\\overline{{\\mathbb{P}}}\\left[\\tau_{H}\\right]$ are estimated by the generalized PSR model $\\widehat{\\theta}$ We assume that the planner has knowledge of the randomization, $\\mathbb{P}\\left[\\omega\\right]$ Hence, the planner in the self-play algorithm is able to compute the probability of any trajectory for each choice of policy. ", "page_idx": 28}, {"type": "text", "text": "Algorithm. The algorithmic description is presented in Algorithm 2. In the first stage of the algorithm, the centralized learning agent has a unified goal: to explore the environment. This is done by executing policies which maximize the bonus function $\\widehat{b}^{k}(\\tau_{H})$ by visiting trajectories with imprecise estimates of their probability, as measured by the upper confidence bound on the total variation distance. This part is identical to Algorithm 1. Once the algorithm is sufficiently confident about the estimated probabilities of all trajectories, it computes the equilibrium using the estimated model directly. That is, ComputeEquilibrium computes either NE or CCE. The only difference in the exploration stage of the algorithm compared to Algorithm 1 is that the termination condition involves $\\varepsilon/4$ rather than $\\varepsilon/2$ in order to guarantee an $\\varepsilon$ -approximate equilibrium under the added complications of the game setting. ", "page_idx": 28}, {"type": "text", "text": "Theorem 5. Suppose Assumption. $^{l}$ holds. Suppose the parameters $p_{\\operatorname*{min}},\\lambda,\\alpha,\\beta$ are chosen as in Theorem 4. Then, with probability at least $1-\\delta$ Algorithm2returnsa model $\\theta^{\\epsilon}$ and a policy $\\pi$ whichisan $\\varepsilon$ -approximateequilibrium(eitherNEor $C C E,$ 0.That is, ", "page_idx": 28}, {"type": "equation", "text": "$$\nV_{\\theta^{*}}^{i}(\\pi)\\geq V_{\\theta^{*}}^{i,\\dag}(\\pi^{-i})-\\varepsilon,\\,\\forall i\\in[N].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}^{k}=\\left\\{\\mathcal{D}_{h}^{k}\\right\\}_{h=0_{\\star}}^{H-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Theta_{\\operatorname*{min}}^{k}=\\left\\{\\theta:\\forall h,(\\tau_{h},\\pi)\\in\\mathcal{D}_{h}^{k},\\mathbb{P}_{\\theta}^{\\pi}(\\tau_{h})\\geq p_{\\operatorname*{min}}\\right\\},}\\\\ &{\\mathcal{B}^{k}=\\left\\{\\theta\\in\\Theta_{\\operatorname*{min}}^{k}:\\displaystyle\\sum_{(\\tau_{H},\\pi)\\in\\mathcal{D}^{k}}\\log\\mathbb{P}_{\\theta}^{\\pi}(\\tau_{H})\\geq\\displaystyle\\operatorname*{max}_{\\theta^{\\prime}\\in\\Theta_{\\operatorname*{min}}^{k}}\\sum_{(\\tau_{H},\\pi)\\in\\mathcal{D}^{k}}\\log\\mathbb{P}_{\\theta^{\\prime}}^{\\pi}(\\tau_{H})-\\beta\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\pi=\\mathsf{C o m p u t e E q u i l i b r i u m}(\\theta^{\\epsilon},\\left\\{R^{1},\\boldsymbol{\\cdot}\\boldsymbol{\\cdot},R^{N}\\right\\})\n$$return ", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In addition, the algorithm terminates with a sample complexity of, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\tilde{O}\\left(\\left(r+\\frac{Q_{A}^{2}H}{\\gamma^{2}}\\right)\\cdot\\frac{r d H^{3}\\cdot\\operatorname*{max}_{s\\in\\mathcal{A}}|\\mathbb{X}_{s}|^{2}\\cdot Q_{A}^{4}\\beta}{\\gamma^{4}\\epsilon^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. The proof is given in Appendix K ", "page_idx": 29}, {"type": "text", "text": "To apply this algorithm to a partially-observable sequential game, we can use the generalized PSR parameterization constructed in Section 4. ", "page_idx": 29}, {"type": "text", "text": "Corollary 2. Suppose a partially-observable sequential game is $m$ step $\\alpha$ -robustly $\\mathcal{T}^{\\dagger}$ -weakly revealing as per Definition 6. Applying Algorithm 2 to this PSR representation, with parameters $p_{\\operatorname*{min}},\\lambda,\\alpha,\\beta$ chosenas inTheorem5,returns a $\\varepsilon$ -approximate equilibrium $\\pi$ with a sample complexity of, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\tilde{\\cal O}\\left(\\left(1+\\frac{Q_{A}^{2}H}{\\alpha^{2}}\\right)\\cdot\\frac{\\operatorname*{max}_{h}|\\mathbb{I}_{h}^{\\dagger}|^{\\tau}\\cdot\\operatorname*{max}_{h}|\\mathbb{Q}_{h}^{m}|\\cdot H^{5}\\cdot\\operatorname*{max}_{s\\in\\mathcal{A}}|\\mathbb{X}_{s}|^{2}\\cdot\\operatorname*{max}_{s\\in\\mathcal{U}}|\\mathbb{X}_{s}|\\cdot Q_{A}^{4}}{\\alpha^{4}\\epsilon^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "G  Existence of Generalized PSR representations and their covering number ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section we show that any rank- $^r$ sequential decision-making problem (as per Section 2.1) can be represented via a rank- ${\\bf\\nabla}r$ generalized PSR (Definition 3). Next, we bound the covering number of the class of rank $r$ PSRs, which will be important for our MLE analysis. Similar results have been established in previous work for sequential decision-making problems with alternating observations and actions [e.g., 33]. Recall that our formulation of the generic sequential decision-making problem and generalized PSRs is more general than the standard formulation since it allows for an arbitrary sequence of variables. Here, we follow a similar procedure to prove a slightly generalized result. ", "page_idx": 29}, {"type": "text", "text": "Proposition 2 (Existence of Generalized PSR representation). Consider a sequential decision-making problemwith $\\operatorname{rank}(D_{h})=r_{h}$ $h\\in0:H-1$ .There exists a generalized PSR representation (i.e, observableoperatormodel) $b_{0},\\{B_{h}(x_{h})\\}_{h\\in[H],x_{h}\\in\\mathbb{X}_{h}}\\,,\\{v_{h}\\}_{h\\in0}$ such that, ", "page_idx": 30}, {"type": "text", "text": "1. $B_{h}(x_{h})\\in\\mathbb{R}^{r_{h}\\times r_{h-1}}$ and $\\|B_{h}(\\boldsymbol{x}_{h})\\|_{2}\\leq1$ for any $x_{h}$   \n2. $|b_{0}|\\leq\\sqrt{|\\mathbb{H}_{H}^{a}|}$   \n3. $\\|v_{h}\\|_{2}\\leq\\sqrt{|\\mathbb{F}_{h}^{o}|\\,/\\,|\\mathbb{F}_{h}^{a}|}$   \n4. For any l $\\begin{array}{r}{\\iota,\\;\\frac{1}{|\\mathbb{X}_{h}|^{\\mathbf{1}\\{h\\in\\mathcal{A}\\}}}v_{h}^{\\top}\\sum_{x_{h}\\in\\mathbb{X}_{h}}B_{h}(x_{h})=v_{h-1}^{\\top}.}\\end{array}$   \n5. For any $\\tau_{h}\\in\\mathbb{H}_{h},\\,\\overline{{\\mathbb{P}}}\\,[\\tau_{h}]=v_{h}^{\\top}B_{h}(x_{h})\\cdot\\cdot\\cdot B_{1}(x_{1})b_{0}.$ ", "page_idx": 30}, {"type": "text", "text": "Proof. We construct the representation via the singular value decomposition of the matrix ${D}_{h}^{\\top}$ .Let $U_{h}\\,\\in\\,\\mathbb{R}^{|\\mathbb{F}_{h}|\\times r_{h}},\\Sigma_{h}\\,\\in\\,\\mathbb{R}^{r_{h}\\times r_{h}},V_{h}^{\\top}\\,\\in\\,\\mathbb{R}^{r_{h}\\times|\\mathbb{H}_{h}|}$ bethe S VD such that $\\pmb{D}_{h}^{\\top}=U_{h}\\pmb{\\Sigma}_{h}V_{h}^{\\top}$ .Define $b_{0},B_{h},v_{h}^{\\top}$ asfollows, ", "page_idx": 30}, {"type": "equation", "text": "$$\nb_{0}=\\|D_{0}\\|_{2}\\,,\\;\\;B_{h}(x_{h})=U_{h}^{\\top}\\left[U_{h-1}\\right]_{(x_{h},\\mathbb{F}_{h}),:},\\;\\;v_{h}^{\\top}=\\frac{1}{|\\mathbb{F}_{h}^{a}|}\\mathbf{1}^{\\top}U_{h}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Here, $[U_{h-1}]_{(x_{h},\\Omega_{h}),}$ : denotes an $\\left|\\mathbb{F}_{h}\\right|$ by $r_{h-1}$ submatrix of $U_{h-1}$ consisting of the rows $(x_{h},\\omega_{h}),\\,\\omega_{h}\\in\\mathbb{F}_{h}$ (i.e., the set of futures where the variable at time $h$ .s $x_{h}$ ). Note that $|\\mathbb{F}_{H}^{a}|=1$ by convention, a product over an empty set. We verify each property in turn. ", "page_idx": 30}, {"type": "text", "text": "First, $\\|B_{h}(x_{h})\\|_{2}=\\|U_{h}^{\\top}[U_{h-1}]_{(x_{h},\\mathbb{F}_{h}),:}\\|_{2}\\le1$ since $U_{h},U_{h-1}$ are unitary matrices. Second, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle|b_{0}|=||D_{0}||_{2}=\\sqrt{\\sum_{\\tau_{H}}\\overline{{\\mathbb{P}}}\\left[\\tau_{H}\\right]^{2}}}\\\\ {\\displaystyle\\leq\\sqrt{\\sum_{\\tau_{H}}\\overline{{\\mathbb{P}}}\\left[\\tau_{H}\\right]}=\\sqrt{\\sum_{\\tau_{H}^{a}}\\sum_{\\tau_{H}^{o}}\\mathbb{P}\\left[\\tau_{H}^{o}\\mid\\tau_{H}^{a}\\right]}=\\sqrt{\\sum_{\\tau_{H}^{a}}1}=\\sqrt{\\prod_{s\\in A}|\\mathbb{X}_{s}|},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the inequality is since $\\overline{{\\mathbb{P}}}\\left[\\tau_{H}\\right]\\in\\left[0,1\\right]$ . For property 3, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left\\|{\\boldsymbol v}_{h}\\right\\|_{2}=\\frac{1}{\\left|\\mathbb{F}_{h}^{a}\\right|}\\left\\|{\\bf1}^{\\top}{\\boldsymbol U}_{h}\\right\\|_{2}}\\\\ {\\ \\ \\ \\leq\\frac{1}{\\left|\\mathbb{F}_{h}^{a}\\right|}\\left\\|{\\bf1}\\right\\|_{2}=\\frac{\\sqrt{\\left|\\mathbb{F}_{h}\\right|}}{\\left|\\mathbb{F}_{h}^{a}\\right|}=\\sqrt{\\left|\\mathbb{F}_{h}^{o}\\right|/\\left|\\mathbb{F}_{h}^{a}\\right|},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the inequality is since $U_{h}$ is unitary, and the final equality is since $|\\mathbb{F}_{h}|=|\\mathbb{F}_{h}^{o}|\\,|\\mathbb{F}_{h}^{a}|$ ", "page_idx": 30}, {"type": "text", "text": "Next, to prove properties 4 and 5, we first show the following claim ", "page_idx": 30}, {"type": "text", "text": "Claim. For any history $\\tau_{h}\\,=\\,\\bigl(x_{1},\\ldots,x_{h}\\bigr)\\,\\in\\,\\ensuremath{\\mathbb{H}}_{h}$ $h\\,\\in\\,0\\,:\\,H$ , we have $B_{h}(x_{h})\\cdot\\cdot\\cdot B_{1}(x_{1})b_{0}\\,=$ UT [DH]:,Tn: ", "page_idx": 30}, {"type": "text", "text": "Proof of claim. We prove the claim by induction. In the base case, $h=0,D_{0}^{\\top}$ is a vector in $\\mathbb{R}^{\\mathbb{F}_{0}}$ (note that $\\mathbb{F}_{0}=\\mathbb{H}_{H}\\right)$ 0. Hence, $U_{0}$ is simply the normalized vector $U_{0}=D_{0}^{\\top}/||D_{0}^{\\top}||_{2}$ , and hence $U_{0}^{\\top}D_{0}^{\\top}=D_{0}D_{0}^{\\top}/\\|D_{0}\\|_{2}=\\|D_{0}\\|_{2}=b_{0}$ . Proceeding by induction, suppose the claim holds for $h-1$ Then, we have, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{h}(x_{h})\\cdot\\cdot\\cdot B_{1}(x_{1})b_{0}=B_{h}(x_{h})U_{h-1}^{\\top}\\left[D_{h-1}^{\\top}\\right]_{\\{,\\tau_{h-1}}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=U_{h}^{\\top}\\left[U_{h-1}\\right]_{(x_{h},\\mathbb{R}_{h}),:}U_{h-1}^{\\top}\\left[D_{h-1}^{\\top}\\right]_{\\{,\\tau_{h-1}}}}\\\\ &{\\qquad\\qquad\\quad=U_{h}^{\\top}\\left[U_{h-1}U_{h-1}^{\\top}D_{h-1}^{\\top}\\right]_{(x_{h},\\mathbb{R}_{h}),\\tau_{h-1}}}\\\\ &{\\qquad\\qquad\\quad=U_{h}^{\\top}\\left[D_{h-1}^{\\top}\\right]_{(x_{h},\\mathbb{R}_{h}),\\tau_{h-1}}}\\\\ &{\\qquad\\qquad\\quad=U_{h}^{\\top}\\left[D_{h}^{\\top}\\right]_{\\{,\\tau_{h}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Where the fia equalityis because $\\begin{array}{r l r}{\\left[D_{h-1}^{\\top}\\right]_{(x_{h},\\omega_{h}),\\tau_{h-1}}}&{=}&{\\overline{{\\mathbb{P}}}\\left[\\tau_{h-1},x_{h},\\omega_{h}\\right]\\ =\\ \\overline{{\\mathbb{P}}}\\left[\\tau_{h},\\omega_{h}\\right]\\ =\\ }\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\prod_{h=1}^{h}\\left(\\tau_{h},\\omega_{h}\\right)\\ \\mathrm{~e~r~}}\\end{array}$ [D]wh,T \u53e3 ", "page_idx": 31}, {"type": "text", "text": "Using this fact, we can now show property 5 as follows, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{h}^{\\top}B_{h}(x_{h})\\cdots B_{1}(x_{1})b_{0}=\\frac{1}{\\left|\\mathbb{R}_{h}^{d}\\right|}{\\mathbf1}^{\\top}U_{h}U_{h}^{\\top}\\left[{\\mathbf D}_{h}^{\\top}\\right]_{:,\\tau_{h}}=\\frac{1}{\\left|\\mathbb{R}_{h}^{d}\\right|}{\\mathbf1}^{\\top}\\left[{\\mathbf D}_{h}^{\\top}\\right]_{:,\\tau_{h}}}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\frac{1}{\\left|\\mathbb{R}_{h}^{d}\\right|}\\sum_{w_{h}\\in\\mathbb{R}_{h}}\\frac{\\mathbb{P}}{\\left|\\mathbb{P}_{h}\\right|}\\left[\\tau_{h},\\omega_{h}\\right]=\\frac{1}{\\left|\\mathbb{R}_{h}^{d}\\right|}\\sum_{\\omega_{h}^{\\top}\\in\\mathbb{R}_{h}^{d}}\\sum_{\\omega_{h}^{\\top}\\in\\mathbb{R}_{h}^{d}}\\mathbb{P}\\left[\\tau_{h}^{\\theta},\\omega_{h}^{\\theta}\\right|\\tau_{h}^{a},\\omega_{h}^{a}\\right]}\\\\ &{\\qquad\\qquad\\quad=\\frac{1}{\\left|\\mathbb{R}_{h}^{d}\\right|}\\mathbb{P}\\left[\\tau_{h}^{a}\\mid\\tau_{h}^{a}\\right]\\sum_{w_{h}^{a}\\in\\mathbb{R}_{h}^{d}}\\sum_{\\omega_{h}^{\\top}\\in\\mathbb{R}_{h}^{d}}\\mathbb{P}\\left[\\omega_{h}^{a}\\mid\\omega_{h}^{a},\\tau_{h}^{a},\\tau_{h}^{a}\\right]}\\\\ &{\\qquad\\qquad\\quad=\\frac{1}{\\left|\\mathbb{R}_{h}^{d}\\right|}\\mathbb{P}\\left[\\tau_{h}^{a}\\mid\\tau_{h}^{a}\\right]\\sum_{w_{h}^{a}\\in\\mathbb{R}_{h}^{d}}1}\\\\ &{\\qquad\\qquad\\quad=\\mathbb{P}\\left[\\tau_{h}^{a}\\mid\\tau_{h}^{a}\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Finally, it remains to show property 4. Consider the linear equation $x^{\\top}U_{h}^{\\top}D_{h}^{\\top}=|\\mathbb{F}_{h}^{a}|^{-1}\\mathbf{1}^{\\top}D_{h}^{\\top}$ Note that $U_{h}^{\\top}D_{h}^{\\top}\\in\\mathbb{R}^{r_{h}\\times|\\mathbb{H}_{h}|}$ is rank $r_{h}$ Thus, this equation ha a unique solution. Our srategy is to show that $v_{h}^{\\top}$ and $\\begin{array}{r}{v_{h+1}^{\\top}\\sum_{x_{h+1}}B_{h+1}(x_{h})}\\end{array}$ are both solutions totis inear quation, and hence $\\begin{array}{r}{v_{h}^{\\top}\\,=\\,v_{h+1}^{\\top}\\sum_{x_{h+1}}B_{h+1}(x_{h})}\\end{array}$ That $v_{h}^{\\top}$ is a solutio is lear by defnition of $v_{h}$ \uff0c $v_{h}^{\\top}U_{h}^{\\top}D_{h}^{\\top}=$ $|\\mathbb{F}_{h}^{a}|^{-1}\\mathbf{1}^{\\top}U_{h}U_{h}^{\\top}D_{h}^{\\top}=|\\mathbb{F}_{h}^{a}|^{-1}\\mathbf{1}^{\\top}D_{h}^{\\top}$ . First, recall by the calculation above that $|\\mathbb{F}_{h}^{a}|^{-1}\\mathbf{1}^{\\top}D_{h}^{\\top}$ is a vector in $\\mathbb{R}^{\\mathbb{H}_{h}}$ where the $\\tau_{h}$ -th entry is $\\mathbb{P}\\left[\\tau_{h}^{o}\\mid\\tau_{h}^{a}\\right]$ . We will calculate the $\\tau_{h}$ -th entry of the vector $x^{\\top}U_{h}^{\\top}D_{h}$ when $\\begin{array}{r}{\\boldsymbol{x}^{\\top}=\\boldsymbol{v}_{h+1}^{\\top}\\sum_{\\boldsymbol{x}_{h+1}}B_{h+1}(\\boldsymbol{x}_{h+1}^{\\top})}\\end{array}$ \uff0c ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\overset{\\sum_{i+1}}{\\sum_{i+1}}\\sum_{h_{k+1}}(x_{k+1})\\Bigg)[U_{h}^{\\top}D_{h}^{\\top}]_{\\leq,h}=\\frac{1}{|\\mathcal{E}_{h}|}\\sum_{h_{k+1}}[\\mathcal{T}_{h_{k+1}}U_{h_{k+1}}^{\\top}[U_{h_{k+1}},y_{h_{k+1}}],[U_{h}^{\\top}D_{h}^{\\top}]_{\\leq,h}]}\\\\ &{=\\frac{1}{|\\mathcal{E}_{h}|}\\sum_{i=1}^{N}{\\Gamma}U_{h_{k+1}}U_{h_{k+1}}^{\\top}[U_{h}U_{h}^{\\top}D_{h}^{\\top}]_{\\{h_{k+1},h_{k+1}\\},\\ldots,h_{k+1}}}\\\\ &{=\\frac{1}{|\\mathcal{E}_{h}|}\\Bigg[\\frac{1}{\\int_{\\mathbb{R}_{k+1}}}\\Bigg\\]\\Bigg[\\Gamma\\int_{h_{k+1}}^{N}[\\sigma_{\\mathbb{R}_{k+1},h_{k+1}},\\sigma_{\\mathbb{R}_{k+1}}}\\\\ &{=\\frac{1}{|\\mathcal{E}_{h}|+1}\\Bigg]\\Bigg[\\frac{1}{|\\mathcal{E}_{h}|}\\Bigg[\\sigma_{\\mathbb{R}_{k+1},h_{k+1}}^{\\top},\\sigma_{\\mathbb{R}_{k+1},h_{k+1}}\\Bigg]-\\frac{1}{|\\mathcal{E}_{h}|+1}\\Bigg]\\Bigg[\\frac{1}{|\\mathcal{E}_{h}|}\\Bigg[\\sigma_{k+1}^{\\top}}\\\\ &{=\\frac{1}{|\\mathcal{E}_{h}|}\\Bigg]\\Bigg[\\Gamma\\int_{h_{k+1}}^{\\top}[\\sigma_{k+1}^{\\top}]\\sum_{i<1}\\frac{1}{|\\mathcal{E}_{h}|}\\Bigg[\\sigma_{k}^{\\top}]_{\\mathcal{E}_{h}}\\tilde{u}_{\\mathcal{E}_{h}}\\Bigg]}\\\\ &{=\\frac{1}{|\\mathcal{E}_{h}|}\\Bigg[\\frac{1}{|\\mathcal{E}_{h}|}\\Bigg[\\frac{1}{|\\mathcal{E}_{h}|}\\Bigg[\\sigma_{k+1}^{\\top}]\\sum_{i=1}^{N}1[\\sigma_{k+1}^{\\top}]\\left[\\frac{1}{|\\mathcal \n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the fnal inequality is since $\\begin{array}{r}{|\\mathbb{F}_{h}^{a}|=\\prod_{s\\in h+1:H}(|\\mathbb{X}_{s}|^{\\mathbf{1}\\left\\{s\\in A\\right\\}})}\\end{array}$ ", "page_idx": 31}, {"type": "text", "text": "Corollary 3. Consider a sequential decision-making problem with $\\operatorname{rank}(D_{h})\\leq r$ Then, there exists a generalized PSR $b_{0}\\in\\mathbb{R}^{r}$ $\\{B_{h}(x_{h})\\}_{h\\in[H],x_{h}\\in\\mathbb{X}_{h}}\\!\\stackrel{}{\\subset}\\mathbb{R}^{r\\times r}$ $v_{H}\\in\\mathbb{R}^{r}$ suchthat, ", "page_idx": 31}, {"type": "text", "text": "Proof. In Proposition 2 we constructed such a representation with dimensions in terms of $r_{h}$ instead of $r$ Since $r_{h}\\le r$ , we can pad this representation with dummy columns and/or rows filled with zeros to obtain a representation with dimensions in terms of $r$ \u53e3 ", "page_idx": 32}, {"type": "text", "text": "An important part of maximum likelihood analysis is the notion of a \u201cbracketing number\u2019 which controls the complexity of the model class $\\Theta$ [e.g., 94]. In our analysis, the model class is the set of generalized PSRs of a given rank. As shown in the results above, rank- $^r$ generalized PSRs can represent any rank- $r$ sequential decision-making problem, with operators whose norm is bounded. In the next result, we will consider a closely related notion to the bracketing number which crucially incorporates optimism. $\\overline{{\\Theta}}_{\\varepsilon}$ is said to be an \u201coptimistic $\\varepsilon$ -cover\"for $\\Theta$ if for each $\\theta\\in\\Theta$ , there exists $\\widehat{\\theta}\\in\\bar{\\Theta}_{\\varepsilon}$ with an associated probability measure $\\overline{{\\mathbb{P}}}_{\\widehat{\\theta}}^{\\varepsilon}$ such that, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall h,\\tau_{h},\\,\\overline{{\\mathbb{P}}}_{\\widehat{\\theta}}^{\\varepsilon}(\\tau_{h})\\geq\\overline{{\\mathbb{P}}}_{\\theta}\\left[\\tau_{h}\\right],}\\\\ {\\forall h,\\tau_{h},\\,\\displaystyle\\sum_{\\tau_{h}}\\left|\\overline{{\\mathbb{P}}}_{\\widehat{\\theta}}^{\\varepsilon}(\\tau_{h})-\\overline{{\\mathbb{P}}}_{\\theta}\\left[\\tau_{h}\\right]\\right|\\leq\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The first condition ensures optimism and the second condition ensures that $\\overline{{\\Theta}}_{\\varepsilon}$ $\\varepsilon$ -covers $\\Theta$ , in the sense that the probability of any trajectory is approximated within an error $\\varepsilon$ . Recall that the parameter $\\beta$ in Algorithms 1 and 2, which appears in the sample complexity results in Theorems 4 and 5, is defined in terms of $|\\overline{{\\Theta}}_{\\varepsilon}|$ . The next proposition bounds the size of $|\\overline{{\\Theta}}_{\\varepsilon}|$ ", "page_idx": 32}, {"type": "text", "text": "Proposition 3 (Optimistic cover of sequential decision making problems). Let Smt be the set of allrank- $r$ sequential decision-makingproblemswithahorizonoflength $H$ observationindex set $\\mathcal O\\subset[H]$ . action index set $\\mathcal{A}\\subset[H]$ andvariablespaces $\\mathbb{X}_{1},\\ldots,\\mathbb{X}_{H}$ .Then, there exists an optimistic $\\varepsilon$ -cover $\\overline{{\\Theta}}_{\\varepsilon}$ of $\\Theta$ withcardinalityboundedby, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left.\\log\\left|\\overline{{\\Theta}}_{\\varepsilon}\\right|\\leq O\\left(r^{2}\\operatorname*{max}_{h}\\left|\\mathbb{X}_{h}\\right|H^{2}\\log\\left(\\frac{\\operatorname*{max}_{h}\\left|\\mathbb{X}_{h}\\right|}{\\epsilon}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. Define the set of generalized PSR representations constructed in Corollary 3, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\Theta:=\\left\\{b_{0}\\in\\mathbb{R}^{r},\\{B_{h}({\\boldsymbol{x}}_{h})\\}_{h,x_{h}},v_{H}\\in\\mathbb{R}^{r}:\\ \\|B_{h}({\\boldsymbol{x}}_{h})\\|_{2}\\leq1,\\forall h,x_{h},\\ \\|b_{0}\\|_{2}\\leq\\sqrt{|\\mathbb{H}_{H}^{a}|},\\ \\|v_{H}\\|_{2}\\leq\\frac{r(r_{0})}{r}\\in\\mathcal{S}\\right\\}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $m$ is a sequential decision making problem in 9 f ", "page_idx": 32}, {"type": "text", "text": "Let $\\mathcal{C}_{\\delta}$ be a $\\delta$ cover of the above set with respect to the $\\ell_{\\infty}$ -norm. For $\\widehat{\\theta}=\\left(b_{0},\\left\\{B_{h}(x_{h})\\right\\},v_{H}\\right)\\in\\mathcal{C}_{\\delta},$ define the $\\varepsilon$ -optimistic probabilities as, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\overline{{\\mathbb{P}}}_{\\widehat{\\theta}}^{\\varepsilon}(\\tau_{H}):=v_{H}^{\\top}B_{H}(x_{h})\\cdot\\cdot\\cdot B_{1}(x_{1})b_{0}+\\varepsilon/2\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We will show that for an appropriate choice of $\\delta,\\mathcal{C}_{\\delta}$ is an optimistic $\\varepsilon$ -cover. In particular, for each $\\theta\\in\\Theta$ , there exists ${\\widehat{\\theta\\,}}\\in{\\mathcal{C}}_{\\delta}$ such that, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall h,\\tau_{h},\\,\\overline{{\\mathbb{P}}}_{\\widehat{\\theta}}^{\\varepsilon}(\\tau_{h})\\geq\\overline{{\\mathbb{P}}}_{\\theta}\\left[\\tau_{h}\\right],}\\\\ {\\forall h,\\tau_{h},\\,\\displaystyle\\sum_{\\tau_{h}}\\left|\\overline{{\\mathbb{P}}}_{\\widehat{\\theta}}^{\\varepsilon}(\\tau_{h})-\\overline{{\\mathbb{P}}}_{\\theta}\\left[\\tau_{h}\\right]\\right|\\leq\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "To choose the value of $\\delta$ for which the above holds, observe that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\tau\\varkappa}\\left|\\hat{v}_{H}^{\\top}\\hat{B}_{H}(x_{H})\\cdots B_{1}(x_{1})\\hat{b}_{0}-v_{H}^{\\top}B_{H}(x_{H})\\cdots B_{1}(x_{1})b_{0}\\right|}\\\\ &{\\le\\displaystyle\\sum_{h=1}^{H}\\sum_{\\tau\\varkappa}\\Big|\\hat{v}_{H}^{\\top}\\hat{B}_{H}(x_{H})\\cdots\\hat{B}_{h+1}(x_{h+1})(\\hat{B}_{h}(x_{h})-B_{h}(x_{h}))B_{h-1}(x_{h-1})\\cdots B_{1}(x_{1})b_{0}}\\\\ &{\\quad+\\displaystyle\\sum_{\\tau\\varkappa}\\Big|\\hat{v}_{H}^{\\top}B_{H}(x_{H})\\cdots B_{1}(x_{1})(\\hat{b}_{0}-b_{0})\\Big|}\\\\ &{\\le\\displaystyle\\sum_{h}\\sum_{\\tau\\varkappa}r\\left\\|\\hat{B}_{h}(x_{h})-B_{h}(x_{h})\\right\\|_{\\operatorname*{max}}\\sqrt{\\|\\mathbb{H}_{H}^{\\alpha}\\|}+\\displaystyle\\sum_{\\tau\\varkappa}\\sqrt{r}\\left\\|\\hat{b}_{0}-b_{0}\\right\\|_{\\infty}}\\\\ &{\\le H\\displaystyle\\operatorname*{max}|\\mathbb{X}_{h}|^{H+|A|/2}r\\delta+\\operatorname*{max}|\\mathbb{X}_{h}|^{H}\\sqrt{r}\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the second inequality uses $\\|\\widehat{v}_{H}\\|_{2}=\\|v_{H}\\|_{2}=\\|B_{h}(x_{h})\\|_{2}=1$ \uff0c $\\|\\widehat{B}_{h}(x_{h})-B_{h}(x_{h})\\|_{2}\\le$ $r\\|\\widehat{B}_{h}(x_{h})-B_{h}(x_{h})\\|_{\\operatorname*{max}}\\leq r\\delta,\\|b_{0}\\|_{2}\\leq\\sqrt{\\|\\mathbb{H}_{H}^{a}\\|},$ and $\\|\\widehat{b}_{0}-b_{0}\\|_{2}\\leq\\sqrt{r}\\|\\widehat{b}_{0}-b_{0}\\|_{\\infty}\\leq\\sqrt{r}\\delta$ Hence, choosing $\\delta:=\\varepsilon\\cdot\\operatorname*{max}_{h}{|\\mathbb{X}_{h}|}^{-c H}$ for $c$ an absolute constant large enough achieves a $\\varepsilon$ -optimistic covering of $\\Theta$ . Hence, we let $\\overline{{\\Theta}}_{\\varepsilon}=\\mathcal{C}_{\\delta}$ , with $\\delta=\\varepsilon\\cdot\\operatorname*{max}_{h}\\cdot|{\\mathbb X}_{h}|^{-c H}$ It remains to bound the size of $|\\overline{{\\Theta}}_{\\varepsilon}|$ ", "page_idx": 33}, {"type": "text", "text": "Recall that $\\|\\cdot\\|_{\\infty}\\leq\\|\\cdot\\|_{2}$ and that an interval $[-x,x]$ in $\\mathbb{R}$ admits a $\\delta$ -cover of size bounded by $2x/\\delta$ Now, observe that r $\\mathrm{m}\\ddot{\\mathrm{x}}_{i j}\\,|[B_{h}(x_{h})]_{i j}|\\leq\\|B_{h}\\dot{(}x_{h})\\|_{2}\\leq1$ . Hence, for a fixed $h$ $\\{B_{h}(x_{h})\\}_{x_{h}}$ admits a cover of size bounded by $(2/\\delta)^{r^{2}|\\mathbb{X}_{h}|}$ . Considering all $h$ the cover is bounded by $(2/\\delta)^{r^{2}}\\sum_{h}\\lvert\\mathbb{X}_{h}\\rvert\\le$ $(2/\\delta)^{r^{2}\\operatorname*{max}_{h}|\\mathbb{X}_{h}|H}$ For, $b_{0}$ , we have $\\|b_{0}\\|_{\\infty}\\,\\leq\\,\\|b_{0}\\|_{2}\\,\\leq\\,\\sqrt{\\|\\mathbb{H}_{H}^{a}\\|},$ hence the covering number is bounded by $(2\\sqrt{|\\mathbb{H}_{H}^{a}|}/\\delta)^{r}$ . Finally for $v_{H}$ , we have $\\|v_{H}\\|_{\\infty}\\,\\leq\\,\\|v_{H}\\|_{2}\\,\\leq\\,1$ , hence the covering number is bounded by $(2/\\delta)^{r}$ . Thus, we have, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\log\\left|\\overline{{\\Theta}}_{\\varepsilon}\\right|\\leq O\\left(r^{2}\\operatorname*{max}_{h}\\left|\\mathbb{X}_{h}\\right|H\\log\\left(\\frac{1}{\\delta}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Recalling that $\\delta=\\varepsilon\\operatorname*{max}_{s}|\\mathbb{X}_{s}|^{-c H}$ , we obtain that, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\log\\left|\\overline{{\\Theta}}_{\\varepsilon}\\right|\\leq O\\left(r^{2}\\operatorname*{max}_{h}\\left|\\mathbb{X}_{h}\\right|H^{2}\\log\\left(\\frac{\\operatorname*{max}_{h}\\left|\\mathbb{X}_{h}\\right|}{\\epsilon}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "HProofs of Section 3.2 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Theorem (Restatement of Theorem 1). The rank of the observable system dynamics of a POST or POSG isboundedby ", "page_idx": 33}, {"type": "equation", "text": "$$\nr\\leq\\operatorname*{max}_{h\\in[H]}\\left|\\mathbb{I}_{h}^{\\dagger}\\right|.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. We have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left.D_{h}\\right\\vert_{T_{h},\\omega_{h}}=\\mathbb{P}\\left[\\left.\\tau_{h}^{\\beta},\\,\\omega_{h}^{\\beta}\\right.\\right]\\,\\mathrm{do}(\\tau_{h}^{\\alpha},\\tau_{h}^{\\alpha})\\right\\vert}&{}\\\\ {\\quad}&{=\\mathbb{P}\\left[\\left.\\tau_{h}^{\\alpha}\\;|\\;\\mathrm{do}\\left(\\tau_{h}^{\\alpha}\\right)\\right]\\mathbb{P}\\left[\\left.\\omega_{h}^{\\beta}\\right\\vert\\;\\tau_{h}^{\\alpha};\\,\\mathrm{do}\\left(\\tau_{h}^{\\alpha},\\,\\omega_{h}^{\\alpha}\\right)\\right]}\\\\ {\\stackrel{(a)}{=}\\mathbb{P}\\left[\\left.\\tau_{h}^{\\beta}\\;|\\;\\mathrm{do}\\left(\\tau_{h}^{\\alpha}\\right)\\right]\\sum_{s_{k}\\in\\mathbb{R}_{k}^{\\ell}}\\mathbb{P}\\left[\\left.\\left\\{x_{k},\\,k\\in{\\cal T}_{h}^{1}\\right\\}\\;\\right\\vert\\;\\tau_{h}^{\\alpha};\\,\\mathrm{do}\\left(\\tau_{h}^{\\alpha},\\,\\omega_{h}^{\\alpha}\\right)\\right]\\mathbb{P}\\left[\\omega_{h}^{\\beta}\\right]\\left\\vert\\;\\left\\{x_{k},\\,k\\in{\\cal T}_{h}^{1}\\right\\},\\tau_{h}^{\\alpha};}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{do}\\left(\\tau_{h}^{\\alpha}\\right)}\\\\ {\\frac{(b)}{\\omega_{h}}\\sum_{s_{k}\\in\\mathbb{R}_{k}^{\\ell}}\\mathbb{P}\\left[\\left.\\tau_{h}^{\\beta}\\;|\\;\\mathrm{do}\\left(\\tau_{h}^{\\alpha}\\right)\\right\\vert\\mathbb{P}\\left[\\left\\{x_{k},\\,k\\in{\\cal T}_{h}^{1}\\right\\}\\;\\right\\vert\\;\\tau_{h}^{\\alpha};\\,\\mathrm{do}\\left(\\tau_{h}^{\\alpha}\\right)\\right]\\mathbb{P}\\left[\\omega_{h}^{\\beta}\\right]\\left\\vert\\;\\left\\{x_{k},\\,k\\in{\\cal T}_{h}^{1}\\right\\},\\tau_{h}^{\\alpha};\\,\\mathrm{do}\\left(\\tau_{h}^{\\alpha}\\right)\\right\\}}\\\\ {\\qquad k\\in\\mathbb{Z}_{h}^{1}}&{}\\\\ {\\stackrel{(c)}{=}\\sum_{s_{k}\\in\\mathbb{K}_{k}^{\\ell}}\\mathbb{P}\\left[\\left.\\tau\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where step (a) is simply the law of total probability, step (b) is that $\\{x_{k},k\\in\\mathcal{L}_{h}^{\\dagger}\\}$ is conditionally independent of $\\mathrm{do}(\\bar{\\omega_{h}^{a}})$ (future actions) given $(\\tau_{h}^{o};\\,\\mathrm{do}(\\bar{\\tau}_{h}^{a}))$ (the past), and step (c) is that $\\omega_{h}^{o}$ is conditionally independent of $(\\tau_{h}^{o};\\,\\mathrm{do}(\\tau_{h}^{a}))$ given $\\{x_{k},\\,k\\in\\mathcal{L}_{h}^{\\dagger}\\}$ . This is due to a result by Verma and Pearl [95] which states: for three sets of variables $A,B,C$ in a directed graphical model, if $A$ and $B$ are $d_{\\cdot}$ -separated by $C$ , then $A\\perp B\\,|\\,C$ Recall that $\\mathcal{T}_{h}^{\\dagger}$ is defined as the minimal set which $d_{\\cdot}$ -separates $\\bar{(X_{t(1)},\\bar{\\mathbf{\\alpha}}.\\cdot.\\cdot,X_{t(h)})}$ from $(X_{t(h+1)},\\ldots,X_{t(H)})$ ", "page_idx": 34}, {"type": "text", "text": "As a technical remark, note that $i_{h}^{\\dagger}=(x_{k},k\\in\\mathcal{T}_{h}^{\\dagger})$ may include actions and hence, $>\\left[\\left\\{x_{k},\\,k\\in{\\mathbb{Z}}_{h}^{\\dagger}\\right\\}\\;\\middle|\\;\\tau_{h}^{o};\\,\\mathrm{do}\\left(\\tau_{h}^{a}\\right)\\right]=\\mathbb{P}\\left[\\left\\{x_{k},\\,k\\in{\\mathbb{Z}}_{h}^{\\dagger}\\cap{\\mathcal{S}}\\right\\}\\;\\middle|\\;\\tau_{h}^{o};\\,\\mathrm{do}\\left(\\tau_{h}^{a}\\right)\\right]\\mathbf{1}\\left\\{\\left(x_{k},\\,k\\in{\\mathbb{Z}}_{h}^{\\dagger}\\cap{\\mathcal{A}}\\right)\\;\\mathrm{matc}\\right\\}.$ hes T%} since the action components of $i_{h}^{\\dagger}$ are contained in the history $\\tau_{h}$ Now define two matrices ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{\\bar{\\Pi}}_{h,1}:=\\left[\\mathbb{P}\\left[\\tau_{h}^{o}\\;|\\;\\mathrm{do}\\left(\\tau_{h}^{a}\\right)\\right]\\mathbb{P}\\left[\\left\\{x_{k},\\;k\\in\\mathbb{Z}_{h}^{\\dagger}\\right\\}\\;\\Big|\\;\\tau_{h}^{o};\\;\\mathrm{do}\\left(\\tau_{h}^{a}\\right)\\right]\\right]_{\\tau_{h},i_{h}^{\\dagger}},\\quad\\tau_{h}\\in\\mathbb{H}_{h},\\;i_{h}^{\\dagger}\\equiv\\Big(x_{k},\\;k\\in\\mathbb{Z}_{h}^{\\dagger}\\Big)\\in\\mathbb{Z}}\\\\ &{\\mathsf{\\bar{\\Pi}}_{h,2}:=\\left[\\mathbb{P}\\left[\\omega_{h}^{o}\\;\\Big|\\;\\Big\\{x_{k},\\;k\\in\\mathbb{Z}_{h}^{\\dagger}\\Big\\};\\;\\mathrm{do}\\left(\\omega_{h}^{a}\\right)\\right]\\right]_{i_{h}^{\\dagger},\\;\\omega_{h}},\\quad i_{h}^{\\dagger}\\equiv\\Big(x_{k},\\;k\\in\\mathbb{Z}_{h}^{\\dagger}\\Big)\\in\\mathbb{I}_{h}^{\\dagger},\\;\\omega_{h}\\in\\mathbb{F}_{h}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We have that $D_{h}=D_{h,1}D_{h,2}$ , where both $D_{h,1}$ and $D_{h,2}$ have rank upper bounded by $|\\mathbb{I}_{h}^{\\dagger}|=$ $\\prod_{s\\in{\\mathbb{Z}}_{h}^{\\dagger}}|{\\mathbb{X}}_{s}|$ .Hence, $\\mathrm{rank}(D_{h})\\leq|\\mathbb{I}_{h}^{\\dagger}|$ , and the result follows. \u53e3 ", "page_idx": 34}, {"type": "text", "text": "Proofs of Section 4 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Lemma (Restatement of Lemma 1). Suppose that the POST/POSG is $m$ -step $\\mathcal{T}^{\\dagger}$ -weakly revealing. Then, $\\mathbb{Q}_{h}^{m}$ is a core test set for all $h\\in[H]$ .Furthermore, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{\\mathbb{P}}}\\left[\\tau_{h},\\omega_{h}\\right]=\\left\\langle m_{h}(\\omega_{h}),\\psi_{h}(\\tau_{h})\\right\\rangle,\\;a n d\\;\\;\\overline{{\\mathbb{P}}}\\left[\\omega_{h}\\;\\middle|\\;\\tau_{h}\\right]=\\left\\langle m_{h}(\\omega_{h}),\\overline{{\\psi}}_{h}(\\tau_{h})\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. Let $\\tau_{h}\\in\\mathbb{H}_{h},\\,\\omega_{h}\\in\\mathbb{F}_{h}$ be any history and future, respectively. By Theorem 1, recall that we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\overline{{\\mathbb{P}}}[\\omega_{h}\\,\\vert\\,\\tau_{h}]=\\sum_{i_{h}^{\\dagger}\\in\\mathbb{I}_{h}^{\\dagger}}\\overline{{\\mathbb{P}}}\\left[\\omega_{h}\\ \\Big\\vert\\ i_{h}^{\\dagger}\\right]\\mathbb{P}\\left[i_{h}^{\\dagger}\\ \\Big\\vert\\ \\tau_{h}\\right].\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Recall that $i_{h}^{\\dagger}$ may overlap with $\\tau_{h}$ . In particular, the action component of $i_{h}^{\\dagger}$ is contained in $\\tau_{h}$ Thus, $\\mathbb{P}[i_{h}^{\\dagger}\\,|\\,\\tau_{h}]\\,=\\,\\mathbb{P}[\\{x_{k},\\,k\\,\\in\\,\\mathbb{Z}_{h}^{\\dagger}\\,\\backslash\\,\\mathcal{U}_{1:h}\\}\\,|\\,\\tau_{h}]\\cdot{\\mathbf1}\\{(x_{k},\\,k\\,\\in\\,\\mathbb{Z}_{h}^{\\dagger}\\cap\\mathcal{U}_{1:h})\\,\\}$ matches $\\tau_{h}\\}$ . Note that $\\mathcal{T}_{h}^{\\dag}\\setminus\\mathcal{U}_{1:h}\\subset\\mathcal{S}$ does not contain any actions. Hence, the summation over $\\mathbb{I}_{h}^{\\dagger}$ is equivalent to summing over its unobservable components with the restriction that its observable components match $\\tau_{h}$ ", "page_idx": 34}, {"type": "text", "text": "Define the mappings $\\tilde{m}_{h}\\colon\\ensuremath{\\mathbb{F}}_{h}\\to\\ensuremath{\\mathbb{R}}^{|\\mathbb{I}_{h}^{\\dagger}|}$ and $p_{h}\\colon\\ensuremath{\\mathbb{H}}_{h}\\to\\ensuremath{\\mathbb{R}}^{|\\mathbb{I}_{h}^{\\dagger}|}$ by ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{m}_{h}(\\omega_{h})=\\left[\\overline{{\\mathbb{P}}}\\left[\\omega_{h}\\ \\middle|\\ \\dot{i}_{h}^{\\dagger}\\right]\\right]_{i_{h}^{\\dagger}\\in\\mathbb{I}_{h}^{\\dagger}},\\quad p_{h}(\\tau_{h})=\\left[\\mathbb{P}\\left[i_{h}^{\\dagger}\\ \\middle|\\ \\tau_{h}\\right]\\right]_{i_{h}^{\\dagger}\\in\\mathbb{I}_{h}^{\\dagger}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then, we have that the conditional probability_of the future $\\omega_{h}$ given the past $\\tau_{h}$ is given by the inner product of the above mappings, $\\overline{{\\mathbb{P}}}\\left[\\omega_{h}\\ |\\ \\tau_{h}\\right]\\ =\\ \\langle\\tilde{m}_{h}(\\omega_{h}),p_{h}(\\tau_{h})\\rangle$ Recall that the vector of (conditional) core test set probabilities for the history $\\tau_{h}$ is given by $\\overline{{{\\psi}}}_{h}(\\tau_{h})\\;=\\;$ $\\left[\\mathbb{P}\\left[q^{o}\\mid\\tau_{h}^{o};\\,\\mathrm{do}(\\tau_{h}^{a}),\\,\\mathrm{do}(q^{a})\\right]\\right]_{q\\in\\mathbb{Q}_{h}^{m}}\\in\\,\\mathbb{R}^{|\\mathbb{Q}_{h}^{m}|}$ . By the definition of $G_{h}$ and Equation (22),we have $G_{h}\\,p_{h}(\\tau_{h})=\\psi_{h}(\\tau_{h})$ , since, for $q\\in\\mathbb{Q}_{h}^{m}$ \uff0c ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(G_{h}\\,p_{h}(\\tau_{h})\\right)_{q}=\\displaystyle\\sum_{i_{h}^{\\dagger}}\\left(G_{h}\\right)_{q,i_{h}^{\\dagger}}\\left(p_{h}(\\tau_{h})\\right)_{i_{h}^{\\dagger}}}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\sum_{i_{h}^{\\dagger}}\\mathbb{P}\\left[q^{o}\\Bigm|i_{h}^{\\dagger};\\,\\mathrm{d}\\sigma(q^{a})\\right]\\mathbb{P}\\left[i_{h}^{\\dagger}\\Bigm|\\tau_{h}\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{P}\\left[q^{o}\\Bigm|\\tau_{h}^{o};\\,\\mathrm{d}\\sigma(\\tau_{h}^{a}),\\mathrm{d}\\sigma(q^{a})\\right]}\\\\ &{\\qquad\\qquad\\qquad=:\\left[\\overline{{\\psi}}_{h}(\\tau_{h})\\right]_{q}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Since by assumption $\\operatorname{rank}(G_{h})=\\left|\\mathbb{I}_{h}^{\\dagger}\\right|$ , its pseudo-inverse $G_{h}^{\\dagger}$ is a leftinverse of $G_{h}$ (i.e., $G_{h}^{\\dagger}G_{h}=$ $I$ .Hence, multiplying on the leftby $G_{h}^{\\dagger}$ , we obtain $p_{h}(\\tau_{h})=G_{h}^{\\dagger}\\overline{{{\\psi}}}_{h}(\\tau_{h})$ Hence, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\mathbb{P}}}\\left[\\omega_{h}\\ \\middle\\vert\\ \\tau_{h}\\right]=\\left<\\tilde{m}_{h}(\\omega_{h}),G_{h}^{\\dagger}\\overline{{\\psi}}_{h}(\\tau_{h})\\right>}\\\\ &{\\qquad\\qquad=\\left<\\underbrace{\\left(G_{h}^{\\dagger}\\right)^{\\top}\\tilde{m}_{h}(\\omega_{h})}_{m_{h}(\\omega_{h})},\\overline{{\\psi}}_{h}(\\tau_{h})\\right>.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "That $\\overline{{\\mathbb{P}}}\\left[\\tau_{h},\\omega_{h}\\right]\\ =\\ \\langle m_{h}(\\omega_{h}),\\psi_{h}(\\tau_{h})\\rangle$ follows directly by noting the definition of $\\overline{{\\psi}}_{h}(\\tau_{h})\\;\\;:=\\;\\;$ $\\psi_{h}(\\tau_{h})/\\overline{{\\mathbb{P}}}\\left[\\tau_{h}\\right]$ ", "page_idx": 35}, {"type": "text", "text": "Hence, we have shown that for the test set $\\mathbb{Q}_{h}^{m}$ , the probability of each future $\\omega_{h}$ given a history $\\tau_{h}$ is a linear combination of the probabilities of each test in the core test set with weights $m_{h}(\\omega_{h}):=$ $(\\mathbf{G}_{h}^{\\dagger})^{\\top}\\tilde{m}_{h}(\\omega_{h})\\in\\mathbb{R}^{|\\mathbb{Q}_{h}^{m}|}$ depending only on the future and not the history. \u53e3 ", "page_idx": 35}, {"type": "text", "text": "Theorem (Restatement of Theorem 2). Suppose a POST/POSG is $\\alpha$ -robustly $m$ step $\\mathcal{T}^{\\dagger}$ -weakly revealing. Then, the corresponding generalized PSR as constructed in Section $^{4}$ .s $\\gamma$ -well-conditioned with $\\gamma=\\alpha/\\operatorname*{max}_{h}|\\mathbb{I}_{h}^{\\dagger}|^{1/2}$ ", "page_idx": 35}, {"type": "text", "text": "We will frst show that $\\big(\\{\\mathbb{Q}_{h}^{m}\\}_{h},\\phi_{H},\\{M_{h}\\}_{h},\\psi_{0}\\big)$ indeed forms a PSR through a series of simple calculations. ", "page_idx": 35}, {"type": "text", "text": "A direct corollary of Lemma 1 is the following. ", "page_idx": 35}, {"type": "text", "text": "Lemma 2. For any $h\\in[H]$ \uff0c $\\tau_{h-1}\\in\\mathbb{H}_{h-1}$ \uff0c $x_{t(h)}\\in\\mathbb{X}_{t(h)}$ \uff0c $\\omega_{h}\\in\\mathbb{F}_{h}$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{\\mathbb{P}}}\\left[\\tau_{h-1},x_{t(h)},\\omega_{h}\\right]=\\left\\langle m_{h-1}(x_{t(h)},\\omega_{h}),\\psi_{h}(\\tau_{h-1})\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Hence, given a history $\\tau_{h-1}\\,=\\,\\big(x_{t(1)},\\cdot\\cdot\\cdot,x_{t(h-1)}\\big)$ , having observed another variable $x_{t(h)}$ we can update our predictions of the future and obtain the probability of trajectories of the form $\\bar{(\\tau_{h-1},x_{t(h)},\\omega_{h})}$ for any future $\\omega_{h}\\in\\mathbb{F}_{h}$ . Note that $x_{t(h)}$ may be either an observation or an action. Hence, we can update our prediction of the future after deciding an action, and before receiving the next observation. This is in contrast to the standard PSR formulation where predictions of the future can only be updated with a pair of observation and action. Our formulation provides additional flexibility, which is crucial for the general information structures modeled by POSTs and POSGs. ", "page_idx": 35}, {"type": "text", "text": "This means that, ater observing. $x_{t(h)}$ we can use the $m_{h}\\,:\\,\\mathbb{F}_{h}\\,\\rightarrow\\,\\mathbb{R}^{d_{h}}$ mapping constructed in Lemma 1 to update the probability of any candidate future $\\omega_{h}$ . We are particularly interested in updating the probabilities of the futures corresponding to the core test set at the next time point, since this provides a sufficient statistic of the past. Thus, we define the matrix mapping $\\boldsymbol{M_{h}}:\\mathbb{X}_{t(h)}\\rightarrow$ $\\mathbb{R}^{d_{h}\\times d_{h-1}}$ bv\u3002 ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left[M_{h}\\big(x_{t(h)}\\big)\\right]_{q,\\cdot}=m_{h-1}\\big(x_{t(h)},q\\big)^{\\top},\\,q\\in\\mathbb{Q}_{h}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "That is, $M_{h}\\big({x_{t(h)}}\\big)$ is the matrix whose rows are indexed by the core tests at the $h$ -th observable step, where the $q\\in\\mathbb{Q}_{h}^{\\prime}$ row is the weights given by the $m_{h-1}$ mapping for the future consisting of $x_{t(h)}$ followed by $q$ . This mapping enables us to update the probabilities of the core test sets. ", "page_idx": 35}, {"type": "text", "text": "Lemma 3. For any $h\\in[H-1]$ \uff0c $\\tau_{h}\\in\\mathbb{H}_{h}$ \uff0c $x_{t(h+1)}\\in\\mathbb{X}_{t(h+1)}.$ we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\psi_{h}(\\tau_{h-1},x_{t(h)})=M_{h}(x_{t(h)})\\psi_{h-1}(\\tau_{h-1}).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Hence, for a history $\\tau_{h}=\\left(x_{t(1)},\\ldots,x_{t(h)}\\right)\\in\\mathbb{H}_{h}$ we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\psi_{h}(\\tau_{h})=M_{h}(x_{t(h)})M_{h-1}(x_{t(h-1)})\\cdot\\cdot\\cdot M_{1}(x_{t(1)})\\psi_{0},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $\\psi_{0}=\\psi_{0}(\\emptyset)$ ", "page_idx": 35}, {"type": "text", "text": "Finally, observe that $\\mathbb{Q}_{H-1}^{m}=\\mathbb{X}_{t(H)}$ Hence, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\psi_{H-1}(\\tau_{H-1})=\\left(\\mathbb{\\overline{{\\mathbb{P}}}}\\left[\\tau_{h-1},x_{t(H)}\\right]\\right)_{x_{t(H)}\\in\\mathbb{X}_{t(H)}}\\in\\mathbb{R}^{\\left|\\mathbb{X}_{t(H)}\\right|}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Thus, letting $\\phi_{H}:\\mathbb{X}_{t(H)}\\to\\mathbb{R}^{|\\mathbb{X}_{t}(H)|}$ be $\\phi_{H}\\big(x_{t(H)}\\big)=e_{x_{t(H)}}$ (the canonical basis vector), yiels ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{\\mathbb{P}}}\\left[x_{t(h)}:\\;h\\in[H]\\right]=\\phi_{H}(x_{t(H)})^{\\top}M_{H-1}(x_{t(H-1)})\\cdot\\cdot\\cdot M_{1}(x_{t(1)})\\psi_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Hence, Equation (27) together with Equation (26) imply that $(M,\\phi_{H},\\psi_{0})$ is a valid generalized PSR representation for the POST/POSG (as per Definition 3). ", "page_idx": 36}, {"type": "text", "text": "What remains is to show that if the POST/POSG is $\\alpha$ -robustly $\\mathcal{T}^{\\dagger}$ -weakly revealing, then the PSR constructed above is well-conditioned. ", "page_idx": 36}, {"type": "text", "text": "Theorem (Restatement of Theorem 2). Suppose a POST/POSG is $\\alpha$ -robustly $m$ -step $\\mathcal{T}^{\\dagger}$ -weakly revealing. Then, the corresponding generalized PSR as constructed in Section $^{4}$ is $\\gamma$ -well-conditioned with $\\gamma=\\alpha/\\operatorname*{max}_{h}|\\mathbb{I}_{h}^{\\dagger}|^{1/2}$ ", "page_idx": 36}, {"type": "text", "text": "Proof. We first show condition (1) in Assumption 1. Suppose $h>H-m$ and hence the core tests are the full futures, which have length smaller than $m$ .Then for any $\\boldsymbol{x}\\in\\mathbb{R}^{d_{h}}$ $\\begin{array}{r}{d_{h}=\\prod_{s=h}^{H}|{\\mathbb X}_{s}|}\\end{array}$ wehave ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}\\sum_{\\omega_{h}}\\left|m_{h}(\\omega_{h})^{\\top}x\\right|\\cdot\\pi(\\omega_{h})=\\operatorname*{max}_{\\pi}\\sum_{\\omega_{h}}\\left|x[\\omega_{h}]\\right|\\pi(\\omega_{h})\\leq\\left\\|x\\right\\|_{1},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $x[\\omega_{h}]$ indexes the component of the vector $x$ corresponding to the future $\\omega_{h}$ ", "page_idx": 36}, {"type": "text", "text": "Now suppose $h\\leq H-m$ (and hence the core tests consist of $m$ -step futures). Then, we have, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\pi}{\\operatorname*{max}}\\displaystyle\\sum_{\\omega_{h}}\\big\\vert m_{h}(\\omega_{h})^{\\top}x\\big\\vert\\,\\pi(\\omega_{h})=\\underset{\\pi}{\\operatorname*{max}}\\displaystyle\\sum_{\\omega_{h}}\\Big\\vert m(\\omega_{h})^{\\top}G_{h}G_{h}^{\\dag}x\\Big\\vert\\cdot\\pi(\\omega_{h})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\underset{\\pi}{\\operatorname*{max}}\\displaystyle\\sum_{\\omega_{h}}\\sum_{i\\dag}\\big\\vert m(\\omega_{h})^{\\top}G_{h}e_{i^{\\dag}}\\big\\vert\\,\\Big\\vert e_{i^{\\dag}}^{\\top}G_{h}^{\\dag}x\\Big\\vert\\cdot\\pi(\\omega_{h}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Now observe that for any policy $\\pi$ and any $i^{\\dagger}\\in\\mathbb{I}_{h}^{\\dagger}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\omega_{h}}\\left|m(\\omega_{h})^{\\top}G_{h}e_{i^{\\dag}}\\right|\\cdot\\pi(\\omega_{h})=\\displaystyle\\sum_{\\omega_{h}}\\left|\\tilde{m}(\\omega_{h})^{\\top}G_{h}^{\\dag}G_{h}e_{i^{\\dag}}\\right|\\cdot\\pi(\\omega_{h})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{\\omega_{h}}\\overline{{\\mathbb{P}}}\\left[\\omega_{h}\\ \\middle\\vert\\ i^{\\dag}\\right]\\pi(\\omega_{h})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{\\omega_{h}}\\mathbb{P}^{\\pi}\\left[\\omega_{h}\\ \\middle\\vert\\ i^{\\dag}\\right]=1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where we used the definition of $m_{h}(\\omega_{h}):=\\tilde{m}_{h}(\\omega_{h})^{\\top}G_{h}^{\\dag}$ ,and $[\\tilde{m}_{h}(\\omega_{h})]_{i^{\\dagger}}:=\\overline{{{\\mathbb{P}}}}[\\omega_{h}\\,|\\,i^{\\dagger}]$ . Recall that $\\pi(\\omega_{h})$ isuhaqf $\\omega_{h}^{o}$ $\\begin{array}{r}{\\sum_{\\omega_{h}^{a}}\\pi(\\omega_{h}^{o},\\omega_{h}^{a})=1}\\end{array}$ ", "page_idx": 36}, {"type": "text", "text": "Putting this observation together with the preceding inequality yields ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\operatorname*{max}_{\\pi}\\sum_{\\omega_{h}}\\left|m_{h}(\\omega_{h})^{\\top}x\\right|\\pi(\\omega_{h})}}\\\\ &{\\leq\\sum_{i^{\\dag}\\in\\mathbb{I}_{h}^{\\dag}}\\left|e_{i^{\\dag}}^{\\top}G_{h}^{\\dag}x\\right|}\\\\ &{=\\left\\|G_{h}^{\\dag}x\\right\\|_{1}\\leq\\left\\|G_{h}^{\\dag}\\right\\|_{1}\\cdot\\left\\|x\\right\\|_{1}}\\\\ &{\\leq\\frac{\\sqrt{\\left|\\mathbb{I}_{h}^{\\dag}\\right|}}{\\alpha}\\left\\|x\\right\\|_{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the final inequality is from the relation between the one-norm and two-norm $\\left\\Vert\\mathbf{G}_{h}^{\\dagger}\\right\\Vert_{1}\\leq$ $\\sqrt{\\left|\\mathbb{I}_{h}^{\\dagger}\\right|}\\left\\|\\mathbf{G}_{h}^{\\dagger}\\right\\|_{2}$ and $\\begin{array}{r}{\\left\\|\\mathbf{G}_{h}^{\\dagger}\\right\\|_{2}\\leq\\frac{1}{\\alpha}}\\end{array}$ , by the assumpton on is singlar values. ", "page_idx": 36}, {"type": "text", "text": "Now we show condition (2) in Assumption 1. For ease of notation, we denote $x_{t(h)}$ by $x_{h}$ . When $h\\,>\\,H\\,-\\,m$ noe that $\\left[M_{h}(x_{h})\\right]_{q_{h+1},q_{h}}={\\bf1}\\{q_{h}=(x_{h},q_{h+1})\\}$ , for all $q_{h}\\,\\in\\,\\mathbb{Q}_{h},q_{h+1}\\,\\in\\,\\mathbb{Q}_{h+1}$ Hence,we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\boldsymbol{\\pi}}\\sum_{\\boldsymbol{x}_{h}}\\|M_{h}(\\boldsymbol{x}_{h})\\boldsymbol{z}\\|_{1}\\,\\pi(\\boldsymbol{x}_{h})=\\|\\boldsymbol{z}\\|_{1}\\,.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Now, when $h\\leq H-m$ , by a similar line of reasoning to the proof for condition (1), we have, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum\\Vert M_{h}(x_{h})z\\Vert_{1}\\,\\pi(x_{h}|\\tau_{h-1})\\leq\\operatorname*{max}_{\\pi}\\displaystyle\\sum_{(x_{h},q_{h+1})\\in\\mathbb X_{h}\\times\\mathbb Q_{h+1}\\,i\\in\\mathbb T_{h}^{\\uparrow}}\\left|e_{q_{h+1}}^{\\top}M_{h}(x_{h})G_{h}e_{i\\uparrow}\\right|\\cdot\\left|e_{i\\uparrow}\\,G_{h}^{\\dagger}z\\right|\\pi(x_{h}|\\tau_{h-1})\\right|\\cdot\\left|\\prod_{i=1}^{h}\\pi(x_{h+1})G_{h}^{\\dagger}z\\right|\\pi(x_{h+1})G_{h}e_{i\\uparrow}\\right|\\cdot\\left|e_{i\\uparrow}\\,G_{h}^{\\dagger}z\\right|\\pi(x_{h+1})G_{h}\\pi(x_{h+1})G_{h}e_{i\\uparrow}\\right|\\cdot\\left|e_{i\\uparrow}\\,G_{h}^{\\dagger}z\\right|\\pi(x_{h-1})G_{h}\\pi(x_{h+1})G_{h}^{\\dagger}z\\right|\\cdot\\left|\\prod_{i=1}^{h}\\pi(x_{h+1})G_{h}^{\\dagger}z\\right|\\pi(x_{h+1})G_{h}^{\\dagger}z\\right|\\cdot\\left|\\prod_{i=1}^{h}G_{h}^{\\dagger}z\\right|\\pi(x_{h+1})G_{h}^{\\dagger}z\\right|\\cdot\\left|\\prod_{i=1}^{h}G_{h}^{\\dagger}z\\right|\\pi(x_{h+1})G_{h}^{\\dagger}z\\right|\\cdot\\left|\\prod_{i=1}^{h}G_{h}^{\\dagger}z\\right|\\pi(x_{h+1})G_{h}^{\\dagger}z\\right|.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where step (a) uses the definition of $M_{h}$ and step (b) uses the definition of $m_{h}(\\omega_{h})^{\\top}:=\\tilde{m}_{h}(\\omega_{h})^{\\top}G_{h}^{\\dag}$ and $[\\tilde{m}_{h}(\\omega_{h})]_{i^{\\dagger}}:=\\overline{{{\\mathbb{P}}}}[\\omega_{h}\\,|\\,i^{\\dagger}]$ . Now note that, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{x_{h},q_{h+1}\\atop\\mathbf{\\zeta}}\\overline{{\\mathbb{P}\\left[x_{h},q_{h+1}\\;\\middle|\\;i^{\\dagger}\\right]\\pi(x_{h}|\\tau_{h-1})}}=\\displaystyle\\sum_{x_{h}}\\sum_{\\mathbf{act}(q_{h+1})\\;\\mathbf{obs}(q_{h+1})}\\overline{{\\mathbb{P}\\left[x_{h},\\mathbf{obs}(q_{h+1})\\;\\middle|\\;i^{\\dagger},\\mathbf{act}(q_{h+1})\\right]\\pi(x_{h-1})}}\\overline{{\\mathbb{P}\\left[x_{h},\\mathbf{obs}(q_{h+1})\\;\\middle|\\;i^{\\dagger}\\right]\\pi(x_{h-1})}}}\\\\ {=\\displaystyle\\sum_{\\mathbf{act}(q_{h+1})}1}\\\\ &{=\\left|\\mathbb{Q}_{h+1}^{A}\\right|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the second line is since for any fixed action sequence, the sum over the probabilities of all observation sequences is 1. ", "page_idx": 37}, {"type": "text", "text": "Thus, putting this together, we obtain the following, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{max}_{\\pi}\\sum_{k}\\left\\|M_{h}(x_{h})z\\right\\|_{1}\\pi(x_{h}|\\tau_{h-1})\\le\\left|\\mathbb{Q}_{h+1}^{A}\\right|\\cdot\\left\\|G_{h}^{\\dagger}z\\right\\|_{1}}}\\\\ &{}&{\\le\\frac{\\sqrt{\\left|\\mathbb{I}_{h}^{\\dagger}\\right|}\\,\\left|\\mathbb{Q}_{h+1}^{A}\\right|}{\\alpha}\\left\\|z\\right\\|_{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the last line again follows by the assumption on the singular values of $G_{h}$ ", "page_idx": 37}, {"type": "text", "text": "J  Proof of Theorem 4: UCB Algorithm for Generalized PSRs (Team Setting) ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In this section, we prove Theorem 4 which states that Algorithm 1 returns a near-optimal policy in a polynomial number of iterations. The proof is adapted from [35] and generalized to our setting with generalized PSRs (Definition 3). The proof is organized into several subsections. In Appendix J.1, we show that the total variation distance between trajectories under the true model and the estimated model can be bounded in terms of the estimation error of the observable operators $\\{M_{h}\\}_{h}$ . In Appendix J.2 we state some general results on maximum likelihood estimation which show that the MLE model has small error on the collected dataset. In Appendix J.3 we prove that the bonus term is an upper confidence bound for the total variation distance. In Appendix J.4 we show that the estimation error is sublinear in the number of iterations (i.e., $O(\\sqrt{K}))$ . Finally, in Appendix J.5 we put this all together to prove the theorem. ", "page_idx": 37}, {"type": "text", "text": "J.1  Properties of Generalized PSRs ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Recall that a PSR model $\\theta=(M,\\psi_{0},\\phi_{H})$ consists of operators $\\boldsymbol{M}=\\{M_{h}\\}_{h=1}^{H-1}$ \uff0c $\\boldsymbol{M_{h}}:\\mathbb{X}_{h}\\rightarrow$ $\\mathbb{R}^{d_{h}\\times d_{h-1}}$ \uff0c $\\phi_{H}:\\mathbb{X}_{H}\\to\\mathbb{R}^{d_{H-1}}$ (assumed to be the identity mapping), and $\\psi_{0}$ (assumed to be known ", "page_idx": 37}, {"type": "text", "text": "for the purposes of presentation). Recall that, for any trajectory $\\tau_{h-1}=\\left(x_{1},.\\,.\\,.\\,,x_{h-1}\\right)$ ,undermodel $\\theta$ ,wehave ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M_{h}(x_{h})\\overline{{\\psi}}_{h-1}(\\tau_{h-1})=\\frac{\\psi_{h}(\\tau_{h})}{\\overline{{\\mathbb{P}}}_{\\theta}\\;(\\tau_{h-1})}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{\\psi_{h}(\\tau_{h})}{\\overline{{\\mathbb{P}}}_{\\theta}\\;(x_{h}\\;\\vert\\;\\tau_{h-1})\\,\\overline{{\\mathbb{P}}}_{\\theta}\\;(\\tau_{h-1})}\\overline{{\\mathbb{P}}}_{\\theta}\\;(x_{h}\\;\\vert\\;\\tau_{h-1})}\\\\ &{\\qquad\\qquad\\qquad=\\overline{{\\psi}}_{h}(\\tau_{h})\\overline{{\\mathbb{P}}}_{\\theta}\\;(x_{h}\\;\\vert\\;\\tau_{h-1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Here, the notation $\\mathbb{P}_{\\theta}\\left(x_{h}\\mid\\tau_{h-1}\\right)$ means the probability of $x_{h}$ conditioned on the history $\\tau_{h-1}$ \uff0c with all actions executed. In particular, if $x_{h}$ is an action, then $\\overline{{{\\mathbb{P}}}}_{\\theta}\\left(x_{h}\\mid\\tau_{h-1}\\right)~=~1$ and $M_{h}(x_{h})\\overline{{\\psi}}_{h-1}(\\tau_{h-1})=\\overline{{\\psi}}_{h}(\\tau_{h})$ ", "page_idx": 38}, {"type": "text", "text": "The following proposition shows that the total variation distance between the distribution of trajectories of two PSR models can be bounded in terms of the difference in their observable operators. ", "page_idx": 38}, {"type": "text", "text": "Proposition 4. For any policy $\\pi$ and $\\theta,{\\widehat{\\theta\\,}}\\in\\Theta$ we have, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{D}_{\\mathsf{T V}}\\left(\\mathbb{P}_{\\widehat\\theta}^{\\pi},\\,\\mathbb{P}_{\\theta}^{\\pi}\\right)\\le\\displaystyle\\sum_{h=1}^{H}\\sum_{\\tau_{H}\\in\\mathbb{H}_{H}}\\pi(\\tau_{h})\\left|\\widehat m_{h}(\\omega_{h})^{\\top}\\left(\\widehat M_{h}(x_{h})-M_{h}(x_{h})\\right)\\psi_{h-1}(\\tau_{h-1})\\right|,}\\\\ &{\\mathsf{D}_{\\mathsf{T V}}\\left(\\mathbb{P}_{\\widehat\\theta}^{\\pi},\\,\\mathbb{P}_{\\theta}^{\\pi}\\right)\\le\\displaystyle\\sum_{h=1}^{H}\\sum_{\\tau_{H}\\in\\mathbb{H}_{H}}\\pi(\\tau_{h})\\left|m_{h}(\\omega_{h})^{\\top}\\left(\\widehat M_{h}(x_{h})-M_{h}(x_{h})\\right)\\widehat\\psi_{h-1}(\\tau_{h-1})\\right|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. The probability of any trajectory $\\tau_{H}=(x_{1},\\dots,x_{H})$ can be written in terms of products of the observable operators $M_{h}(x_{h})$ of a PSR model (Equation (1). Hence, we have, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{D}_{\\mathsf{T V}}\\left(\\mathbb{P}_{\\hat{\\theta}}^{\\pi},\\mathbb{P}_{\\theta}^{\\pi}\\right)=\\displaystyle\\frac{1}{2}\\sum_{\\tau_{H}}\\left|\\mathbb{P}_{\\hat{\\theta}}^{\\pi}(\\tau_{H})-\\mathbb{P}_{\\theta}^{\\pi}(\\tau_{H})\\right|}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\frac{1}{2}\\sum_{\\tau_{H}}\\pi(\\tau_{H})\\cdot\\left|\\left(\\prod_{h=1}^{H}\\widehat{M}_{h}(x_{h})\\right)\\psi_{0}-\\left(\\prod_{h=1}^{H}M_{h}(x_{h})\\right)\\psi_{0}\\right|}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{1}{2}\\sum_{\\tau_{H}}\\pi(\\tau_{H})\\sum_{h=1}^{H}\\left|\\widehat{m}_{h}(x_{h+1:H})^{\\top}\\left(\\widehat{M}_{h}(x_{h})-M_{h}(x_{h})\\right)\\psi_{h-1}(\\tau_{h-1})\\right|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the second line follows by the triangle inequality after noting that for any trajectory $\\tau_{H}=$ $x_{1:H}\\in\\mathbb{H}_{H}$ , the following holds for any $h=1,\\ldots,H$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\small\\left(\\prod_{h=1}^{H}\\widehat{M_{h}}(x_{h})\\right)\\psi_{0}-\\left(\\prod_{h=1}^{H}M_{h}(x_{h})\\right)\\psi_{0}=\\widehat{m}_{h}(x_{h+1:H})^{\\top}\\widehat{M}_{h}(x_{h})\\widehat{\\psi}_{h-1}(x_{1:h-1})-m_{h}(x_{h+1:H})^{\\top}M_{h-1:H}(x_{h+1:H}),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "By the same argument, we obtain the second inequality. ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{D}_{\\mathsf{T V}}\\left(\\mathbb{P}_{\\hat{\\theta}}^{\\pi},\\mathbb{P}_{\\theta}^{\\pi}\\right)=\\displaystyle\\frac{1}{2}\\sum_{\\tau_{H}}\\Big|\\mathbb{P}_{\\hat{\\theta}}^{\\pi}(\\tau_{H})-\\mathbb{P}_{\\theta}^{\\pi}(\\tau_{H})\\Big|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\le\\displaystyle\\frac{1}{2}\\sum_{\\tau_{H}}\\pi(\\tau_{H})\\sum_{h=1}^{H}\\Big|m_{h}(x_{h+1:H})^{\\top}\\left(\\widehat{M}_{h}(x_{h})-M_{h}(x_{h})\\right)\\widehat{\\psi}_{h-1}(\\tau_{h-1})\\Big|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In this result, recall that we assume $\\psi_{0}$ is known to the agent, to simplify the presentation. If $\\psi_{0}$ was not known, there would be another term due to the estimation as $\\widehat{\\psi}_{0}$ [see 33, Lemma C.3]. Note that the sample complexity of estimating $\\psi_{0}$ is small compared to learning the other parameters. ", "page_idx": 38}, {"type": "text", "text": "J.2  General Results on MLE ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In this section, we state some general results on maximum likelihood estimation which ultimately guarantee that the estimated model produced by the procedure in Algorithm 1 has a small estimation error. The results are stated without proof. The proofs are given in [35] and use standard techniques on MLE analysis [94]. This ultimately leads us to a lemma which states that the estimation error of the MLE model is small on the collected data. ", "page_idx": 39}, {"type": "text", "text": "The first proposition states that the log-likelihood of the true model $\\theta^{*}$ is large compared to any other model. ", "page_idx": 39}, {"type": "text", "text": "Proposition 5 (Proposition 4 of [351). Fi $\\begin{array}{r}{x\\in<\\frac{1}{K H}}\\end{array}$ With probabilitya least $1-\\delta,$ for any $\\overline{{\\theta}}\\in\\overline{{\\Theta}}_{\\varepsilon}$ andany $k\\in[K]$ ,the followingholds: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall\\overline{{\\theta}}\\in\\overline{{\\Theta}}_{\\varepsilon},\\displaystyle\\sum_{h}\\sum_{(\\tau_{h},\\pi)\\in\\mathcal{D}_{h}}\\log\\mathbb{P}_{\\overline{{\\theta}}}^{\\pi}(\\tau_{h})-3\\log\\frac{K\\left|\\overline{{\\Theta}}_{\\varepsilon}\\right|}{\\delta}\\leq\\displaystyle\\sum_{h}\\sum_{(\\tau_{h},\\pi)\\in\\mathcal{D}_{h}^{k}}\\log\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau_{h})}\\\\ &{\\forall\\overline{{\\theta}}\\in\\overline{{\\Theta}}_{\\varepsilon},\\displaystyle\\sum_{(\\tau_{H},\\pi)\\in\\mathcal{D}^{k}}\\log\\mathbb{P}_{\\overline{{\\theta}}}^{\\pi}(\\tau_{H})-3\\log\\frac{K\\left|\\overline{{\\Theta}}_{\\varepsilon}\\right|}{\\delta}\\leq\\displaystyle\\sum_{(\\tau_{h},\\pi)\\in\\mathcal{D}_{h}^{k}}\\log\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau_{h})}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "The second proposition provides an upper bound on the total variation distance between the distributions of futures given histories on the empirical history of trajectories. This result ensures that the model estimated by Algorithm 1 is accurate on the sampled trajectories. ", "page_idx": 39}, {"type": "text", "text": "Proposition 6 (Proposition 5 in [35]). Fix $p_{\\mathrm{{min}}}$ and $\\begin{array}{r}{\\varepsilon\\le\\frac{p_{\\mathrm{min}}}{K H}}\\end{array}$ . Let ", "page_idx": 39}, {"type": "text", "text": "$\\Theta_{\\mathrm{min}}^{k}\\:=\\:\\big\\{\\theta:\\forall h,\\,(\\tau_{h},\\pi)\\in\\mathcal{D}_{h}^{k}$ $\\mathbb{P}_{\\theta}^{\\pi}(\\tau_{h})\\geq p_{\\operatorname*{min}}\\right\\}$ . Then, with probabit a least $1-\\delta_{i}$ for any kE[K],0 E Omin we have, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{h\\ (\\tau_{h},\\pi)\\in{\\cal D}_{h}^{k}}\\mathrm{D}_{\\mathrm{TV}}^{2}\\left(\\mathbb{P}_{\\theta}^{\\pi}(\\omega_{h}\\vert\\tau_{h}),\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\omega_{h}\\vert\\tau_{h})\\right)\\le6\\sum_{h\\ (\\ (\\tau_{h},\\pi)\\in{\\cal D}_{h}^{k}}\\log\\frac{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau_{H})}{\\mathbb{P}_{\\theta}^{\\pi}(\\tau_{H})}+31\\log\\frac{K\\left\\vert\\overline{{\\Theta}}_{\\varepsilon}\\right\\vert}{\\delta}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "The next proposition is standard in the analysis of maximum likelihood estimation. $\\mathbb{D}_{\\mathrm{H}}$ denotesthe Hellinger distance. ", "page_idx": 39}, {"type": "text", "text": "Proposition7Proposition6of [35). Le $\\begin{array}{r}{\\varepsilon<\\frac{1}{K^{2}H^{2}}}\\end{array}$ .Then, with probabiliy at least $1-\\delta$ the following holds for all and $k\\in[K]$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{\\pi\\in\\mathcal{D}^{k}}\\mathsf{D}_{\\mathsf{H}}^{2}\\left(\\mathbb{P}_{\\theta}^{\\pi}(\\tau_{H}),\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau_{H})\\right)\\le\\frac{1}{2}\\sum_{(\\tau_{H},\\pi)\\in\\mathcal{D}^{k}}\\log\\frac{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau_{H})}{\\mathbb{P}_{\\theta}^{\\pi}(\\tau_{H})}+2\\log\\frac{K\\left|\\overline{{\\Theta}}_{\\varepsilon}\\right|}{\\delta}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "The final proposition of this section states that when $p_{\\mathrm{min}}$ is chosen as in Theorem 4, the true model $\\theta^{*}$ lies in the constraint $\\Theta_{\\mathrm{min}}^{k}$ with high probability. ", "page_idx": 39}, {"type": "text", "text": "oposition 8.Fir $\\begin{array}{r}{p_{\\mathrm{min}}\\,\\leq\\,\\frac{\\delta}{K H\\prod_{h=1}^{H}|\\mathbb{X}_{h}|}}\\end{array}$ $1-\\delta$ we have $\\theta^{*}\\in\\Theta_{\\operatorname*{min}}^{k}$ $\\forall k$ ", "page_idx": 39}, {"type": "text", "text": "Proof For each $k\\in[K]$ we have $\\theta^{*}\\in\\Theta_{\\operatorname*{min}}^{k}$ $\\mathbb{P}_{\\theta^{*}}^{\\pi^{k}}(\\tau_{h}^{k})\\geq p_{\\operatorname*{min}}$ for all $h\\in[H],\\,(\\tau_{h}^{k},\\pi^{k})\\in\\mathcal{D}_{h}^{k}$ Consider the probability of $\\theta^{*}$ violating this constraint for some trajectory in the dataset. For each $k,h,(\\tau_{h}^{k},\\pi^{k})$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[\\mathbb{P}_{\\theta^{*}}^{\\pi^{k}}(\\tau_{h}^{k})<p_{\\operatorname*{min}}\\right]=\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\mathbb{P}\\left[\\mathbb{P}_{\\theta^{*}}^{\\pi^{k}}(\\tau_{h}^{k})<p_{\\operatorname*{min}}\\;\\Big|\\;\\pi^{k}=\\pi\\right]\\right]}\\\\ &{\\phantom{\\quad}=\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\displaystyle\\sum_{\\tau_{h}\\in\\mathbb{H}_{h}}\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau_{h}^{k}=\\tau_{h})\\mathbf{1}\\{\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau_{h})<p_{\\operatorname*{min}}\\}\\right]}\\\\ &{\\phantom{\\quad}<\\displaystyle\\sum_{\\tau_{h}\\in\\mathbb{H}_{h}}p_{\\operatorname*{min}}}\\\\ &{\\phantom{\\quad}=|\\mathbb{H}_{h}|\\,p_{\\operatorname*{min}}}\\\\ &{\\phantom{\\quad}=\\displaystyle\\frac{\\delta}{K H}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "In the above, the first line is by the law of total probability, where the expectation is over the policy $\\pi^{k}$ used while collecting the $(\\boldsymbol{\\dot{h}},\\boldsymbol{k})$ -th trajectory, and the inner probability is over trajectories $\\tau_{h}^{k}$ . The second line calculates the probabilityof the event $\\{\\mathbb{P}_{\\theta^{*}}^{\\pi^{k}}(\\tau_{h}^{k})<p_{\\operatorname*{min}}\\}.$ Taking a union bound over $k\\in[K]$ $h\\in[H]$ , and $(\\tau_{h},\\pi)\\in\\mathcal{D}_{h}$ implies that $\\mathbb{P}\\left[\\theta^{*}\\in\\Theta_{\\operatorname*{min}}^{k}\\right]\\geq1-\\delta$ \u53e3 ", "page_idx": 40}, {"type": "text", "text": "In what follows, let $\\mathcal{E}_{\\omega},\\mathcal{E}_{\\pi},\\mathcal{E}_{\\operatorname*{min}}$ be the events in Propositions 6 to 8, respectively. Let $\\mathcal{E}=\\mathcal{E}_{\\omega}$ n $\\mathcal{E}_{\\pi}\\cap\\mathcal{E}_{\\operatorname*{min}}$ be the intersection of all events. Propositions 6 to 8 guarantee the event $\\mathcal{E}$ occurs with high probability, $\\mathbb{P}\\left[\\mathcal{E}\\right]\\geq1-3\\delta$ , by a union bound. ", "page_idx": 40}, {"type": "text", "text": "The following result states that the estimated model is accurate on the past exploration policies and dataset of collected trajectories. This holds for both the conditional probabilities of futures given past trajectories in the dataset as well as over full trajectories. The result follows from the MLE analysis in Propositions 6 to 8. ", "page_idx": 40}, {"type": "text", "text": "Lemma 4. Let \u03b2 = 31log $\\begin{array}{r}{\\varepsilon\\leq\\frac{\\delta}{K^{2}H^{2}\\prod_{h}|\\mathbb{X}_{h}|}}\\end{array}$ where $\\overline{{\\Theta}}_{\\varepsilon}$ is the optinisie $\\varepsilon$ in Proposition 3. Then, under event $\\mathcal{E}$ the following holds, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{h}\\sum_{(\\tau_{h},\\pi)\\in\\mathcal{D}_{h}^{k}}\\mathrm{D}_{\\mathrm{TV}}^{2}\\left(\\mathbb{P}_{\\widehat{\\theta}^{k}}^{\\pi}(\\omega_{h}|\\tau_{h}),\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\omega_{h}|\\tau_{h})\\right)\\leq7\\beta,a n d}\\\\ &{\\displaystyle\\sum_{\\pi\\in\\mathcal{D}^{k}}\\mathrm{D}_{\\mathrm{H}}^{2}\\left(\\mathbb{P}_{\\widehat{\\theta}^{k}}^{\\pi}(\\tau_{H}),\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau_{H})\\right)\\leq7\\beta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof. The proof follows by Propositions 6 to 8. The argument is direct and is identical to Lemma 1 of [35] . \u53e3 ", "page_idx": 40}, {"type": "text", "text": "J.3UCB for Total Variation Distance ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Notation. Let $m^{*}$ \uff0c $\\{M_{h}^{*}\\}_{h}$ be the observable operators of the true PSR $\\theta^{*}$ , and let $\\{\\widehat{M}_{h}^{k}\\}_{h}$ be the algorithm's estimates of the observable operators corresponding to $\\widehat{\\theta}^{k}$ ", "page_idx": 40}, {"type": "text", "text": "Recall that Proposition 4 shows that the total variation distance between the distribution over trajectories of two PSRs is bounded by the estimation error of the observable operators $M_{h}$ .The following result constructs a bound on the estimation error of the observable operators $M_{h}(x_{h})$ .The proof is adapted from [35, Lemma 2] to our setting with generalized PSRs. ", "page_idx": 40}, {"type": "text", "text": "Lemma 5. Under event $\\mathcal{E}$ , for any policy $\\pi$ and $k\\in[K]$ , we have, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sum_{\\tau_{H}}\\left|m^{\\star}(\\omega_{h})^{\\top}\\left(\\widehat{M}_{h}^{k}(x_{h})-M_{h}^{\\star}(x_{h})\\right)\\widehat{\\psi}_{h-1}^{k}(\\tau_{h-1})\\right|\\boldsymbol{\\pi}(\\tau_{H})\\leq\\mathbb{E}_{\\tau_{h-1}\\sim\\mathbb{P}_{\\hat{\\delta}^{k}}}^{\\pi}\\left[\\alpha_{h-1}^{k}\\left\\|\\widehat{\\psi}_{h-1}^{k}(\\tau_{h-1})\\right\\|_{(\\hat{U}_{h-1}^{k})}\\right]\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\hat{\\gamma}_{h-1}^{k}=\\lambda I+\\sum_{\\tau_{h-1}\\in\\mathcal{D}_{h-1}^{k}}\\left[\\widehat{\\overline{{\\psi}}}_{h}^{k}(\\tau_{h-1})\\widehat{\\overline{{\\psi}}}_{h}^{k}(\\tau_{h-1})^{\\top}\\right]}}\\\\ {{\\displaystyle-1\\right)^{2}=\\frac{4\\lambda Q_{A}^{2}d}{\\gamma^{4}}+\\frac{4\\operatorname*{max}_{s\\in A}\\left|\\mathbb{X}_{s}\\right|^{2}Q_{A}^{2}}{\\gamma^{2}}\\sum_{\\tau_{h-1}\\in\\mathcal{D}_{h-1}^{k}}\\mathbb{D}_{\\mathrm{TV}}^{2}\\left(\\mathbb{P}_{\\hat{\\theta}^{k}}^{\\mathrm{way}}\\left(\\left|\\mathbf{\\theta}_{h-1}^{\\alpha}\\right|\\tau_{h-1},\\omega_{h-1}^{a}\\right),\\mathbb{P}_{\\theta^{k}}^{\\mathrm{way}}\\left(\\left|\\mathbf{\\theta}_{h-1}^{\\alpha}\\right|\\right)\\right)=\\displaystyle\\hat{\\delta}_{h-1}\\left[\\mathbf{\\theta}_{h-1}^{k}\\left(\\tau_{h-1}^{k}\\right)\\left(\\tau_{h-1}^{k}\\right)\\left(\\tau_{h-1}^{k}\\right)\\right],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof. To ase notation, we index the future trajectories $\\omega_{h-1}=(x_{h},\\ldots,x_{H})\\in\\mathbb{F}_{h-1}$ by $i$ and his", "page_idx": 40}, {"type": "text", "text": "tory trajectories $\\tau_{h-1}=(x_{1},\\dots,x_{h-1})\\in\\mathbb{H}_{h-1}$ by $j$ .We denote $m^{\\star}(\\omega_{h})^{\\top}\\left(\\widehat{M}_{h}^{k}(x_{h})-M_{h}^{\\star}(x_{h})\\right)$ as w,n(Th-1) as xj, and \u03c0(wh-1Th-1) as Til. ", "page_idx": 40}, {"type": "text", "text": "The following bound follows from the Cauchy-Schwarz inequality, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{n_{i}=1}^{\\infty}\\left|m^{*}(x_{i})\\right|^{3}\\left(\\widehat{M_{h}}(x_{i})-M_{h}^{*}(x_{h})\\right)\\widehat{\\psi}_{h-1}^{*}(\\tau_{h-1})\\left|\\ \\pi(\\tau_{h})\\right.}\\\\ &{\\left.\\Theta\\displaystyle\\sum_{i=1}^{\\infty}\\sum_{j=1}^{n_{i}}|m^{*}(x_{i})|^{2}\\left(\\widehat{M_{h}}^{*}(x_{h})-M_{h}^{*}(x_{h})\\right)\\widehat{\\psi}_{h}^{*}(x_{h-1})\\right|^{2}(\\alpha(h_{-1})|\\mathbb{P}_{h}^{*}(x_{h-1})|)\\varphi_{h}^{*}(\\tau_{h-1})}\\\\ &{=\\displaystyle\\sum_{k_{i}=1}^{\\infty}\\left|w_{i}^{*}\\mathbb{E}_{j}\\left|\\pi_{k_{i}}^{*}\\mathbb{E}_{j}\\frac{\\pi_{k_{i}}^{*}}{\\mu_{k_{i}}}(y)\\right.}\\\\ &{\\displaystyle=\\sum_{k_{i}=1}^{\\infty}\\sum_{j=1}^{n_{i}}\\left(\\pi_{k_{i}}\\cdot\\mathrm{sign}(w_{i}^{*}x_{j})\\right)^{\\pi_{k_{i}}}\\chi_{j}\\cdot\\mathbb{P}_{h}^{*}(j)}\\\\ &{\\displaystyle=\\sum_{j=1}^{\\infty}\\left(\\sum_{i=1}^{n_{i}}\\left(\\sum_{i=1}^{n_{i}}\\mathrm{sign}(w_{i}^{*}x_{j})\\right)w_{i}\\right)^{\\top}x_{j}\\cdot\\mathbb{P}_{h}^{*}(j)}\\\\ &{\\displaystyle=\\mathbb{E}_{j\\sim\\mathcal{P}_{h}^{*}}\\left[\\left(\\sum_{i=1}^{n_{i}}z_{i}\\cdot\\mathrm{sign}(w_{i}^{*}x_{j})\\log_{1}\\right)^{\\top}x_{j}\\right]}\\\\ &{\\displaystyle\\stackrel{(a)}{\\le}\\mathbb{E}_{j\\sim\\mathcal{P}_{h}^{*}}\\left[\\left(\\prod_{j=1,1}^{n_{i}}|\\phi_{(j),-1}^{*}\\right)^{\\top}\\left|\\sum_{i=1}^{n_{i}}z_{i}\\cdot\\mathrm{sign}(w_{i\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Step (a) follows from the fact that $\\widehat{\\psi}_{h-1}^{k}(\\tau_{h-1})=\\widehat{\\overline{{{\\psi}}}}_{h}^{k}(\\tau_{h-1})\\cdot(\\widehat{\\phi}_{h-1}^{k})^{\\top}\\widehat{\\psi}_{h-1}^{k}(\\tau_{h-1})=\\widehat{\\overline{{{\\psi}}}}_{h}^{k}(\\tau_{h-1})\\;.$ $\\overline{{\\mathbb{P}}}_{\\widehat{\\theta^{k}}}\\left[\\tau_{h-1}\\right]$ and $\\begin{array}{r}{\\overline{{\\mathbb{P}}}_{\\widehat{\\theta}^{k}}\\left[\\tau_{h-1}\\right]\\cdot\\pi(\\tau_{H})=\\pi(\\omega_{h-1}|\\tau_{h-1})\\cdot\\mathbb{P}_{\\widehat{\\theta}^{k}}^{\\pi}(\\tau_{h-1})}\\end{array}$ . Step (b) is the Cauchy-Schwarz inequality. ", "page_idx": 41}, {"type": "text", "text": "Fix $\\tau_{h-1}\\,=\\,j_{0}$ Let $\\begin{array}{r}{I_{1}\\,:=\\,\\,\\big\\|\\textstyle\\sum_{i}\\pi_{i|j_{0}}\\cdot\\mathrm{sign}(w_{i}^{\\top}x_{j_{0}})\\cdot w_{i}\\big\\|_{\\widehat{U}_{h-1}^{k}}^{2}}\\end{array}$ whih we bound nxt By the definition of $\\widehat{U}_{h-1}^{k}$ , we partition this term into two parts, ", "page_idx": 41}, {"type": "image", "img_path": "QgMC8ftbNd/tmp/3e38ed9ad97c9b0e1b10fe9c4a0441662e7ad3f78b0069b58a456b9b318cf539.jpg", "img_caption": [], "img_footnote": [], "page_idx": 41}, {"type": "text", "text": "We bound $I_{2}$ and $I_{3}$ separately. By the triangle inequality, $\\sqrt{I_{2}}$ is bound by a sum of two terms, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{I_{2}}=\\sqrt{\\lambda}\\underset{z\\in\\mathbb{R}^{d_{h-1}}\\cup\\underline{{h}}_{n-1}}{\\operatorname*{max}}\\left|\\sum_{i}\\pi_{i/j_{0}}\\cdot\\mathrm{sign}(w_{i}^{\\top}x_{j_{0}})\\cdot w_{i}^{\\top}z\\right|}\\\\ &{\\phantom{\\sqrt{I_{2}}}\\overset{\\mathrm{max}}{\\leq}\\sqrt{\\lambda}\\underset{\\|\\varepsilon\\|_{2}=1}{\\operatorname*{max}}\\sum_{i_{\\omega\\hbar-1}}\\left|m^{\\star}(\\omega_{h}^{\\top})\\left(\\widehat{M}_{h}^{k}(x_{h})-M_{h}^{\\star}(x_{h})\\right)z\\right|\\pi(\\omega_{h-1}\\vert j_{0})}\\\\ &{\\overset{(b)}{\\leq}\\sqrt{\\lambda}\\underset{\\|\\varepsilon\\|_{2}=1}{\\operatorname*{max}}\\sum_{i_{\\omega\\hbar-1}}\\left|m^{\\star}(\\omega_{h})^{\\top}\\widehat{M}_{h}^{k}(x_{h})z\\right|\\pi(\\omega_{h-1}\\vert j_{0})}\\\\ &{\\phantom{\\overset{(b)}{\\leq}}+\\sqrt{\\lambda}\\underset{\\|\\varepsilon\\|_{2}=1}{\\operatorname*{max}}\\sum_{i_{\\omega\\hbar-1}}\\left|m^{\\star}(\\omega_{h})^{\\top}M_{h}^{\\star}(x_{h})z\\right|\\pi(\\omega_{h-1}\\vert j_{0}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where step (a) is by the definition of $w_{i}^{\\top},\\pi_{i|j_{0}}$ and the triangle inequality, and step (b) is by the triangle inequality. ", "page_idx": 41}, {"type": "text", "text": "Consider the first term. It can be bound via the definition of $\\gamma.$ -well-conditioning as follows, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{|\\mathbb{R}|=|\\mathbb{I}|_{0}}{\\operatorname*{max}}\\sum_{i=1}^{N}\\left|m^{\\star}(\\boldsymbol{\\omega}_{h})^{\\top}\\widehat{M}_{h}^{k}(x_{h})z\\right|\\pi(\\omega_{h-1}|j_{0})}\\\\ &{\\qquad\\qquad=\\underset{|\\mathbb{I}|_{0}=1}{\\operatorname*{max}}\\sum_{\\boldsymbol{x}_{h}}\\left(\\sum_{w_{h}}\\left|m^{\\star}(\\boldsymbol{\\omega}_{h})^{\\top}\\widehat{M}_{h}^{k}(x_{h})z\\right|\\pi(\\omega_{h}|j_{0},\\boldsymbol{x}_{h})\\right)\\pi(x_{h}|j_{0})}\\\\ &{\\qquad\\overset{(a)}{\\le}\\frac{1}{\\gamma}\\underset{|\\mathbb{I}|_{0}=1}{\\operatorname*{max}}\\sum_{\\boldsymbol{x}_{h}}\\left\\|\\widehat{M}_{h}^{k}(x_{h})z\\right\\|_{1}\\pi(x_{h}|j_{0})}\\\\ &{\\qquad\\overset{(b)}{\\le}\\frac{1}{\\gamma}\\underset{|\\mathbb{I}|_{0}=1}{\\operatorname*{max}}\\frac{|\\mathbb{I}|_{0}^{A}|}{\\gamma}|\\mathbb{I}|_{1}}\\\\ &{\\qquad\\overset{(c)}{\\le}\\frac{\\sqrt{d}Q_{{a}}}{\\gamma}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where step (a) is by the first condition in Assumption 1, step (b) is by the second condition of Assumption 1,anstpcisby thefat $\\begin{array}{r l}&{\\operatorname*{max}_{z\\in\\mathbb{R}^{d_{h-1}}:\\|z\\|_{2}=1}\\|z\\|_{1}=\\sqrt{d_{h-1}}\\leq\\sqrt{d}}\\end{array}$ and $\\left|\\mathbb{Q}_{h+1}^{A}\\right|\\leq Q_{A}$ In the above, note that we used the $\\gamma$ -well-conditioning _of PSR $\\widehat{\\theta}^{k}$ in step (a) and the $\\gamma$ wellconditioning of PSR $\\theta^{*}$ in step (b). The second term in $\\sqrt{I_{2}}$ admits an identical bound, simply by using the well-conditioning of the PSR $\\theta^{*}$ in both steps. Hence, we have that ", "page_idx": 42}, {"type": "equation", "text": "$$\nI_{2}\\leq4\\frac{\\lambda d Q_{A}^{2}}{\\gamma^{4}}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Now we upper bound $I_{3}$ \uff0c", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{\\Omega}\\leq\\sum_{n_{1}\\leq P_{h-1}^{k}}\\left(\\sum_{\\alpha_{h-1}}\\left|m^{*}(\\omega_{h})^{\\top}\\left(\\widehat{M}_{h}^{k}(x_{h})-M_{h}^{*}(x_{h})\\right)\\widehat{\\psi}^{k}(\\gamma_{h-1})\\right|\\pi(\\omega_{h-1}|j_{0})\\right)^{2}}\\ \\\\ &{\\leq\\underbrace{\\sum_{n_{1}\\leq P_{h-1}^{k}}\\left(\\sum_{\\omega_{h-1}}\\left|m^{*}(\\omega_{h})^{\\top}\\left(\\widehat{M}_{h}^{k}(x_{h})\\widehat{\\psi}^{k}(\\gamma_{h-1})-M_{h}^{*}(x_{h})\\widehat{\\psi}^{*}(\\gamma_{h-1})\\right)\\right|\\pi(\\omega_{h-1}|j_{0})\\right.}_{I_{1}}}\\\\ &{+\\underbrace{\\sum_{n_{1}\\leq P_{h-1}^{k}}\\left|m^{*}(\\omega_{h})^{\\top}M_{h}^{*}(x_{h})\\left(\\widehat{\\psi}^{k}(\\gamma_{h-1})-\\widehat{\\psi}^{*}(\\gamma_{h-1})\\right)\\left|\\pi(\\omega_{h-1}|j_{0})\\right)^{2}}_{I_{1}}}\\\\ &{=:\\sum_{n_{1}\\leq P_{h-1}^{k}}\\left(I_{d}+I_{5}\\right)^{2}}\\\\ &{=:\\sum_{n_{1}\\leq P_{h-1}^{k}}(I_{d}+I_{5})^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the second equality follows from the triangle inequality by adding and subtracting $m^{*}(\\omega_{h})^{\\top}M_{h}^{*}(x_{h})\\overline{{\\psi}}^{*}(\\tau_{h-1})$ inside the absolute value. We now bound each of $I_{4}$ and $I_{5}$ ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{a}:=\\displaystyle\\sum_{\\omega_{h-1}}\\left|m^{\\star}(\\omega_{h})^{\\top}\\left(\\widehat{H}_{h}^{k}(x_{h})\\widehat{\\psi}^{k}(\\tau_{h-1})-M_{h}^{\\star}(x_{h})\\widehat{\\psi}^{\\star}(\\tau_{h-1})\\right)\\right|\\pi(\\omega_{h-1}|j_{0})}\\\\ &{\\overset{(a)}{=}\\displaystyle\\sum_{\\omega_{h-1}}\\left|m^{\\star}(\\omega_{h})^{\\top}\\left(\\overline{{\\mathbb{P}}}_{\\hat{\\theta}^{h}}\\left[x_{h}\\ |\\ \\tau_{h-1}\\right]\\widehat{\\psi}_{h}(\\tau_{h})-\\overline{{\\mathbb{P}}}_{\\theta^{h}}\\left[x_{h}\\ |\\ \\tau_{h-1}\\right]\\widehat{\\psi}_{h}^{\\star}(\\tau_{h})\\right)\\right|\\pi(\\omega_{h-1}|j_{0})}\\\\ &{\\overset{(b)}{=}\\displaystyle\\sum_{x_{h}}\\left(\\sum_{\\omega_{h}}\\left|m^{\\star}(\\omega_{h})^{\\top}\\left(\\overline{{\\mathbb{P}}}_{\\hat{\\theta}^{h}}\\left[x_{h}\\ |\\ \\tau_{h-1}\\right]\\widehat{\\psi}_{h}(\\tau_{h})-\\overline{{\\mathbb{P}}}_{\\theta^{h}}\\left[x_{h}\\ |\\ \\tau_{h-1}\\right]\\widehat{\\psi}_{h}(\\tau_{h})\\right)\\right|\\pi(\\omega_{h})|j_{0},x_{h}\\right)\\right)\\pi(x_{h}|j_{0})}\\\\ &{\\overset{(c)}{\\leq}\\displaystyle\\frac{1}{\\gamma}\\sum_{x_{h}}\\left|\\overline{{\\mathbb{P}}}_{\\hat{\\theta}^{h}}\\left[x_{h}\\ |\\ \\tau_{h-1}\\right]\\widehat{\\psi}_{h}(\\tau_{h})-\\overline{{\\mathbb{P}}}_{\\theta^{*}}\\left[x_{h}\\ |\\ \\tau_{h-1}\\right]\\widehat{\\psi}_{h}^{\\star}(\\tau_{h})\\right|_{1}\\pi(x_{h}|j_{0})}\\\\ &{\\overset{(d)}{=}\\displaystyle\\frac{1}{\\gamma}\\sum_{x_{h}}\\sum_{y_{h}}\\ \\frac{|\\overline{{\\mathbb{P}}}_{\\hat{\\theta}^{h}}\\left[x_{h}\\ |\\ \\tau_{h-1}\\right]-\\overline{{\\\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where step (a) is by the fact that $M_{h}(x_{h})\\overline{{\\psi}}_{h-1}(\\tau_{h-1})\\,=\\,\\overline{{\\mathbb{P}}}\\left[x_{h}\\ |\\ \\tau_{h-1}\\right]\\overline{{\\psi}}(\\tau_{h})$ , as shown in Equation (28), step (b) uses $\\omega_{h-1}=(x_{h},\\omega_{h})$ and $\\pi(\\omega_{h-1}|j_{0})=\\pi(x_{h}|j_{0})\\pi(\\omega_{h}|j_{0},x_{h})$ , step (c) is by Assumption 1, and step (d) follows by the definition $\\overline{{\\psi}}_{h}$ \uff0c $\\left[\\overline{{\\psi}}_{h}(\\tau_{h})\\right]_{l}=\\overline{{\\mathbb{P}}}_{\\theta}\\left[q_{h}^{l}\\mid\\tau_{h}\\right]$ ", "page_idx": 43}, {"type": "text", "text": "Now, we turn to bound the $I_{5}$ term. We have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{5}=\\displaystyle\\sum_{\\omega_{h}}\\sum_{x_{h}}\\left|m_{h}^{\\star}(\\omega_{h})^{\\top}M_{h}^{\\star}(x_{h})\\left(\\frac{\\widehat{\\psi}^{k}}{\\widehat{\\psi}}\\left(\\tau_{h-1}\\right)-\\overline{{\\psi}}^{\\star}(\\tau_{h-1})\\right)\\right|\\pi(\\omega_{h}|j_{0},x_{h})\\pi(x_{h}|j_{0})}\\\\ &{\\overset{(a)}{=}\\displaystyle\\sum_{\\omega_{h-1}}\\left|m_{h-1}^{\\star}(\\omega_{h-1})^{\\top}\\left(\\frac{\\widehat{\\psi}^{k}}{\\widehat{\\psi}}\\left(\\tau_{h-1}\\right)-\\overline{{\\psi}}^{\\star}(\\tau_{h-1})\\right)\\right|\\pi(\\omega_{h-1}|j_{0})}\\\\ &{\\overset{(a)}{\\leq}\\displaystyle\\frac{1}{\\gamma}\\left\\|\\widehat{\\tilde{\\psi}}^{k}\\left(\\tau_{h-1}\\right)-\\overline{{\\psi}}^{\\star}(\\tau_{h-1})\\right\\|_{1}}\\\\ &{=\\displaystyle\\frac{1}{\\gamma}\\sum_{q_{h-1}\\in Q_{h-1}}\\left|\\overline{{\\tilde{\\psi}}}_{\\hat{\\theta}^{k}}\\left[q_{h-1}\\mid\\tau_{h-1}\\right]-\\overline{{\\mathbb{P}}}_{\\theta^{\\star}}\\left[q_{h-1}\\mid\\tau_{h-1}\\right]\\right|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where step (a) is since $m_{h}^{*}(\\omega_{h})^{\\top}M_{h}^{*}(x_{h})=m_{h-1}^{*}(\\omega_{h-1})^{\\top}$ , step (b) is by the frst condition of Assumption 1, and the final equality is again by the definition of $\\overline{{\\psi}}$ ", "page_idx": 43}, {"type": "text", "text": "Combining the above, we have that, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{3}\\leq\\displaystyle\\sum_{\\gamma_{h-1}\\in\\mathbb{P}_{h-1}^{k}}(I_{4}+I_{5})^{2}}\\\\ &{\\quad\\leq\\displaystyle\\sum_{\\eta_{h-1}\\in\\mathbb{P}_{h-1}^{k}}\\left(\\frac{1}{\\gamma}\\sum_{x_{k}\\in\\mathbb{K}_{h}}\\sum_{\\phi_{h}\\in\\mathbb{Q}_{h}}\\left|\\overline{{\\tilde{p}}}_{\\phi_{h}}\\left[\\mathbf{z}_{h},q_{h}\\right|\\ \\tau_{h-1}\\right]-\\overline{{\\mathbb{P}}}_{\\phi}\\left\\{x_{h},q_{h}\\ |\\ \\tau_{h-1}\\right]\\right|\\pi(x_{h}|\\tau_{h-1})}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\frac{1}{\\gamma}\\sum_{\\phi_{h-1}\\in\\mathbb{Q}_{h-1}^{k}}\\left|\\overline{{\\tilde{p}}}_{\\phi_{h}}\\left[q_{h-1}\\ |\\ \\eta_{h-1}\\right]-\\overline{{\\mathbb{P}}}_{\\phi}\\left\\{\\mathbf{z}_{h-1}\\ |\\ \\tau_{h-1}\\ |\\right\\}\\right|^{2}}\\\\ &{\\quad\\leq\\displaystyle\\frac{1}{\\gamma^{2}}\\cdot\\sum_{\\eta_{h-1}\\in\\mathbb{P}_{h-1}^{k}}\\left(\\sum_{\\alpha_{h},\\eta_{h}\\in\\mathbb{K}_{h}\\times\\mathbb{Q}_{h}}\\left|\\overline{{\\mathbb{P}}}_{\\phi_{h}}\\left[\\mathbf{z}_{h},q_{h}\\ |\\ \\tau_{h-1}\\right]-\\overline{{\\mathbb{P}}}_{\\phi}\\left\\{x_{h},q_{h}\\ |\\ \\tau_{h-1}\\right]\\right|\\pi(x_{h}|\\tau_{h-1})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\sum_{\\eta_{h-1}\\in\\mathbb{Q}_{h-1}^{k}}\\left|\\overline{{\\mathbb{P}}}_{\\phi}\\left[q_{h-1}\\ |\\ \\tau_{h-1}\\right]-\\overline{{\\mathbb{P}}}_{\\phi}\\left\\{\\mathbf{z}_{h-1}\\ |\\ \\tau_{h-1}\\ |\\right\\}\\right|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Now, we decompose the summations above over ${\\mathbb{X}}_{h}\\times{\\mathbb{Q}}_{h}$ and $\\mathbb{Q}_{h-1}$ into separate summations over observation futures and action futures. That is, $\\left(x_{h},q_{h}\\right)$ is decomposed into $(\\omega_{h-1}^{a},\\omega_{h-1}^{o})$ ,where $\\omega_{h-1}^{a}=\\mathsf{a c t}(x_{h},q_{h})$ and $\\omega_{h-1}^{o}=\\mathsf{o b s}(x_{h},q_{h})$ , and the summations are over $\\omega_{h-1}^{a}\\in\\mathsf{a c t}(\\mathbb{X}_{h}\\times\\mathbb{Q}_{h})$ ", "page_idx": 43}, {"type": "text", "text": "and $\\omega_{h-1}^{o}\\in\\mathsf{o b s}(\\mathbb{X}_{h}\\times\\mathbb{Q}_{h})$ . Similarly, $q_{h-1}$ can be decomposed into $(q_{h-1}^{o},q_{h-1}^{a})\\in\\mathsf{o b s}(\\mathbb{Q}_{h-1})\\times$ $\\mathsf{a c t}(\\mathbb{Q}_{h-1})$ . Hence, the bound on $I_{3}$ can be written as, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{3}\\leq\\displaystyle\\frac{1}{\\gamma^{2}}\\cdot\\sum_{\\tau_{h-1}\\in\\mathcal{D}_{h-1}^{k}}\\left(\\sum_{\\omega_{h-1}^{*}\\cup\\tau_{h-1}^{*}}\\left|\\overline{{\\mathbb{P}}}_{\\hat{\\theta}^{k}}\\left[\\omega_{h-1}^{o}\\right.\\right]\\tau_{h-1},\\omega_{h-1}^{a}\\right]-\\overline{{\\mathbb{P}}}_{\\theta^{k}}\\left[\\omega_{h-1}^{o}\\right.\\left|\\ \\tau_{h-1},\\omega_{h}^{a}\\right|\\right]\\pi(x_{h}|\\tau_{h-1})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\sum_{\\eta_{h-1}^{*}\\in\\mathcal{D}_{h-1}^{k}}\\left|\\overline{{\\mathbb{P}}}_{\\hat{\\theta}^{k}}\\left[q_{h-1}^{o}\\right.\\left|\\ \\tau_{h-1},q_{h-1}^{a}\\right]-\\overline{{\\mathbb{P}}}_{\\theta^{k}}\\left[q_{h-1}^{o}\\right.\\left|\\ \\tau_{h-1},q_{h-1}^{a}\\right]\\right|\\right)^{2}}\\\\ &{\\leq\\displaystyle\\frac{1}{\\gamma^{2}}\\cdot\\sum_{\\tau_{h-1}\\in\\mathcal{D}_{h-1}^{k}}\\left(\\sum_{\\omega_{h-1}^{*}\\in\\mathcal{Q}_{h-1}^{\\infty}}\\sum_{\\omega_{h-1}^{*}}\\left|\\overline{{\\mathbb{P}}}_{\\hat{\\theta}^{k}}\\left[\\omega_{h-1}^{o}\\right.\\left|\\ \\tau_{h-1},\\omega_{h-1}^{a}\\right]-\\overline{{\\mathbb{P}}}_{\\theta^{k}}\\left[\\omega_{h-1}^{o}\\right.\\left|\\ \\tau_{h-1},\\omega_{h-1}^{a}\\right]\\right|\\right)^{2}}\\\\ &{=\\displaystyle\\frac{1}{\\gamma^{2}}\\left|\\mathbb{Q}_{h-1}^{\\mathrm{cosp}}\\right|^{2}\\cdot\\sum_{\\tau_{h-1}\\in\\mathcal{P}_{h-1}^{k}}\\sum_{\\tau_{h-1}^{*}\\in\\mathcal{P}_{h-1}^{k}}\\left(\\mathbb{P}_{\\hat{\\theta}_{h}^{k}}^{\\mathrm{u,n}}\\left(\\omega_{h-1}^{o}\\right.\\left|\\ \\tau_{h-1},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Where the second inequality is by the definition of $\\mathbb{Q}_{h-1}^{\\mathtt{e x p}}\\,=\\,\\mathsf{a c t}\\,(\\mathbb{X}_{h}\\times\\mathbb{Q}_{h}\\cup\\mathbb{Q}_{h-1})$ . Here, the second summation is over $\\omega_{h-1}^{o}\\in\\mathsf{o b s}\\left(\\mathbb{X}_{h}\\times\\mathbb{Q}_{h}\\cup\\mathbb{Q}_{h-1}\\right)$ . The final equality uses the fact the under the policy $\\mathtt{u}_{h-1}^{\\mathtt{e x p}}$ the probability of each action sequence $\\omega_{h-1}^{o}$ is $1/\\left|\\mathbb{Q}_{h-1}^{\\tt e x p}\\right|$ . Note that $\\left|\\mathbb{Q}_{h-1}^{\\mathsf{e x p}}\\right|\\leq$ $|\\mathsf{a c t}\\left(\\mathbb{X}_{h}\\times\\mathbb{Q}_{h}\\right)|+|\\mathsf{a c t}\\left(\\mathbb{Q}_{h-1}\\right)|$ , and hence we have $\\begin{array}{r}{\\left|\\mathbb{Q}_{h-1}^{\\mathtt{e x p}}\\right|\\le2\\operatorname*{max}_{s\\in\\mathcal{A}}\\left|\\mathbb{X}_{s}\\right|Q_{A}}\\end{array}$ for all $h$ . Hence, we have, ", "page_idx": 44}, {"type": "equation", "text": "$$\nl_{3}\\le4\\operatorname*{max}_{s\\in A}|\\mathbb{X}_{s}|^{2}\\,Q_{A}^{2}\\frac{1}{\\gamma^{2}}\\sum_{\\tau_{h-1}\\in\\mathcal{D}_{h-1}^{k}}\\mathsf{D}_{\\tau\\mathbb{V}}^{2}\\left(\\mathbb{P}_{\\hat{\\theta}^{k}}^{\\mathfrak{u}_{h-1}^{\\mathrm{op}}}\\left(\\omega_{h-1}^{o}\\mid\\tau_{h-1},\\omega_{h-1}^{a}\\right),\\mathbb{P}_{\\theta^{*}}^{\\mathfrak{u}_{h-1}^{\\mathrm{op}}}\\left(\\omega_{h-1}^{o}\\mid\\tau_{h-1},\\omega_{h}^{a}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Putting this together with the bounds on $I_{2}$ and $I_{3}$ , we get that, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathrm{~\\dot{\\Phi}_1\\le\\frac{4\\lambda Q_{A}^{2}d}{\\lambda^{4}}+4\\operatorname*{max}_{s\\in\\cal A}|\\mathfrak{X}_{s}|^{2}\\,Q_{A}^{2}\\frac{1}{\\gamma^{2}}\\sum_{\\tau_{h-1}\\in\\mathcal{D}_{h-1}^{k}}\\mathfrak{D}_{\\mathrm{TV}}^{2}\\left(\\mathbb{P}_{\\hat{\\theta}^{h}}^{\\mathfrak{u}_{h-1}^{\\alpha\\gamma}}\\left(\\omega_{h-1}^{o}\\mid\\tau_{h-1},\\omega_{h-1}^{a}\\right),\\mathbb{P}_{\\theta^{s}}^{\\mathfrak{u}_{h-1}^{\\alpha\\gamma}}\\left(\\omega_{h-1}^{o}\\mid\\tau_{h-1},\\omega_{h-1}^{a}\\right)\\right),}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{=:\\left(\\alpha_{h-1}^{k}\\right)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "completing the proof. ", "page_idx": 44}, {"type": "text", "text": "Using the above bound on the difference between the observable operators of the true model and the estimated model, we now bound the total variation distance between the distributions of trajectories through Proposition 4. ", "page_idx": 44}, {"type": "text", "text": "Lemma 6. Under even $\\mathcal{E}$ the total variation distance between the estimated model at iteration $k,\\,{\\widehat{\\theta}}^{k}$ and thetruemodel $\\theta^{*}$ ,isboundedby, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{D}_{\\mathsf{T V}}\\left(\\mathbb{P}_{\\widehat{\\theta}^{k}}^{\\pi}(\\tau_{H}),\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau_{H})\\right)\\leq\\alpha\\cdot\\mathbb{E}_{\\tau_{H}\\sim\\mathbb{P}_{\\widehat{\\theta}^{k}}^{\\pi}}\\left[\\sqrt{\\displaystyle\\sum_{h=0}^{H-1}\\left\\|\\widehat{\\overline{{\\psi}}}^{k}(\\tau_{h})\\right\\|_{(\\widehat{U}_{h}^{k})^{-1}}^{2}}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "for any policy $\\pi$ ,where ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\alpha^{2}=\\frac{4\\lambda H Q_{A}^{2}d}{\\gamma^{4}}+28\\operatorname*{max}_{s\\in A}\\vert\\mathbb{X}_{s}\\vert^{2}\\,Q_{A}^{2}\\frac{1}{\\gamma^{2}}\\beta\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Proof. Consider $\\alpha_{h-1}^{k}$ in the previous lemma. We have that. ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\bigtriangleup^{-\\cdots\\ n-1/\\ }}{\\imath=1}}\\\\ &{=\\frac{4\\lambda H Q_{A}^{2}d}{\\gamma^{4}}+4\\operatorname*{max}_{s\\in A}\\vert\\mathbb{X}_{s}\\vert^{2}Q_{A}^{2}\\frac{1}{\\gamma^{2}}\\sum_{h=1}^{H}\\sum_{\\tau_{h-1}\\in\\mathcal{D}_{h-1}^{k}}\\mathsf{D}_{\\mathsf{T V}}^{2}\\left(\\mathbb{P}_{\\hat{\\theta}^{k-1}}^{\\mathbf{\\upmu}_{h-1}^{\\mathrm{sgp}}}\\left(\\omega_{h-1}^{o}\\mid\\tau_{h-1},\\omega_{h-1}^{a}\\right),\\mathbb{P}_{\\theta^{*}}^{{\\mathbf{\\upmu}_{h-1}^{\\mathrm{sgp}}}}\\left(\\omega_{h-1}^{o}\\mid\\tau_{h-1},\\omega_{h-1}^{a}\\right)\\right)}\\\\ &{\\leq\\frac{4\\lambda H Q_{A}^{2}d}{\\gamma^{4}}+4\\operatorname*{max}_{s\\in A}\\vert\\mathbb{X}_{s}\\vert^{2}Q_{A}^{2}\\frac{1}{\\gamma^{2}}\\7\\beta=:\\alpha^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the inequality is by the bound on the total variation distance established in Lemma 4. ", "page_idx": 45}, {"type": "text", "text": "Now, by Proposition 4, the total variation distance is bounded by the estimation error: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{D}_{\\tau\\forall}\\left(\\mathbb{P}_{\\widehat{\\theta}^{k}}^{\\pi}\\left(\\tau_{H}\\right),\\mathbb{P}_{\\theta^{k}}^{\\pi}\\left(\\tau_{H}\\right)\\right)}\\\\ &{\\stackrel{(a)}\\le\\displaystyle\\sum_{h=1}^{H}\\sum_{\\tau\\mu}\\left|m^{\\star}(\\omega_{h})^{\\top}\\left(\\widehat{M}_{h}^{k}(x_{h})-M_{h}^{\\star}(x_{h})\\right)\\widehat{\\psi}_{h-1}^{k}(\\tau_{h-1})\\right|\\pi(\\tau_{H})}\\\\ &{\\stackrel{(b)}\\le\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}_{\\tau_{h-1}\\sim\\mathbb{P}_{\\widehat{\\theta}^{k}}}^{\\pi}\\left[\\alpha_{h-1}^{k}\\left\\|\\widehat{\\psi}_{h-1}^{k}(\\tau_{h-1})\\right\\|_{(\\widehat{U}_{h-1}^{k})^{-1}}\\right]}\\\\ &{\\stackrel{(c)}\\le\\alpha\\cdot\\mathbb{E}_{\\tau_{H}\\sim\\mathbb{P}_{\\widehat{\\theta}^{k}}^{\\pi}}\\left[\\sqrt{\\displaystyle\\sum_{h=0}^{H-1}\\left\\|\\widehat{\\psi}^{k}(\\tau_{h})\\right\\|_{(\\widehat{U}_{h}^{k})^{-1}}^{2}}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where step (a) is by Proposition 4, step (b) is by Lemma 5, and step (c) is by the Cauchy-Schwarz inequality and the calculation above bounding $\\sum_{h}\\left(\\alpha_{h-1}^{k}\\right)^{2}$ \u53e3 ", "page_idx": 45}, {"type": "text", "text": "A direct corollary is the following bound on the error in the estimated value function, which establishes that the bonus term $\\widehat{b}^{k}$ gives an upper confidence bound. ", "page_idx": 45}, {"type": "text", "text": "Corollary 4 (Upper confidence bound). Under the event $\\mathcal{E}$ forany $k\\in[K]$ anyreward function $\\begin{array}{r}{R:\\prod_{h\\in[H]}\\vec{\\mathbb{X}}_{h}\\overset{\\bullet}{\\rightarrow}[0,1],}\\end{array}$ and anypolicy $\\pi$ wehave, ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|V_{\\widehat{\\theta}^{k}}^{R}(\\pi)-V_{\\theta^{*}}^{R}(\\pi)\\right|\\leq V_{\\widehat{\\theta}^{k}}^{\\widehat{b}^{k}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Where $\\begin{array}{r}{\\widehat{b}^{k}(\\tau_{H})=\\operatorname*{min}\\Bigg\\{\\alpha\\sqrt{\\sum_{h}\\left\\|\\widehat{\\overline{{\\psi}}}^{k}(\\tau_{h})\\right\\|_{(\\widehat{U}_{h}^{k})^{-1}}^{2}},1\\Bigg\\}.}\\end{array}$ ", "page_idx": 45}, {"type": "text", "text": "Proof. By a direct calculation, ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|V_{\\theta^{k}}^{R}(\\pi)-V_{\\theta^{k}}^{R}(\\pi)\\right|=\\left|\\displaystyle\\sum_{\\gamma=R}R(\\pi)\\mathbb{P}_{\\hat{\\theta}^{k}}^{\\pi}(\\pi\\mu)-\\sum_{\\eta=R}R(\\pi\\mu)\\mathbb{P}_{\\hat{\\theta}^{k}}^{\\pi}(\\tau\\mu)\\right|}&{}\\\\ {\\overset{(a)}{\\le}\\displaystyle\\sum_{\\gamma\\pi}\\left|\\mathbb{P}_{\\hat{\\theta}^{k}}^{\\pi}(\\tau\\mu)-\\mathbb{P}_{\\theta^{k}}^{\\pi}(\\pi\\mu)\\right|}&{}\\\\ {=\\displaystyle\\mathbb{P}_{\\mathbb{P}}\\left(\\mathbb{P}_{\\theta}^{\\pi}(\\tau\\mu),\\mathbb{P}_{\\theta^{k}}^{\\pi}(\\tau\\mu)\\right)}&{}\\\\ {\\overset{(b)}{\\le}\\alpha\\cdot\\mathbb{E}_{\\pi\\sim\\mathbb{P}_{\\theta^{k}}^{\\pi}}\\left[\\sqrt{\\displaystyle\\sum_{h=0}^{\\eta-1}\\left\\|\\hat{\\tilde{\\psi}}^{k}(\\tau_{h})\\right\\|_{(\\hat{\\mathcal{V}}_{h}^{h-1})}^{2}}\\right]}&{}\\\\ {\\overset{(c)}{\\le}\\alpha\\displaystyle\\sum_{\\gamma\\pi}\\hat{\\tilde{\\nu}}^{k}(\\pi\\mu)\\mathbb{P}_{\\theta^{k}}^{\\pi}(\\tau\\mu)}&{}\\\\ {=:V_{\\pi}^{R}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where step (a) is by the triangle inequality and the fact that $R(\\tau_{H})\\in[0,1]$ , step (b) is by Lemma 6, and step (c) is by the definition of bk. \u53e3 ", "page_idx": 45}, {"type": "text", "text": "J.4 $\\sum_{k=1}^{K}V_{\\widehat{\\theta}^{k}}^{\\widehat{b}^{k}}$ Vb is sublinear ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "The next step is to prove that k=1 Vb\\* $\\begin{array}{r}{\\sum_{k=1}^{K}V_{\\widehat{\\theta}^{k}}^{\\widehat{b}^{k}}=O(\\sqrt{K})}\\end{array}$ . To do that, we first prove that the estimated prediction features and the ground-truth prediction features can be related through the total-variation distance between the estimated model and the true model. ", "page_idx": 45}, {"type": "text", "text": "Lemma 7. Under event $\\mathcal{E}$ for any $k\\in[K],$ we have: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\upzeta}_{\\tau_{H}\\sim\\mathbb{P}_{\\theta^{*}}^{\\pi}}\\left[\\sqrt{\\displaystyle\\sum_{h=0}^{H-1}\\left\\lVert\\widehat{\\psi^{k}}(\\tau_{h})\\right\\rVert_{(\\widehat{U}_{h}^{k})^{-1}}^{2}}\\right]}\\\\ &{\\le\\displaystyle\\frac{2H Q_{A}}{\\sqrt{\\lambda}}\\mathsf{D}_{\\mathsf{T V}}\\left(\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau_{h}),\\mathbb{P}_{\\theta^{k}}^{\\pi}(\\tau_{h})\\right)+\\left(1+\\frac{2\\operatorname*{max}_{s\\in A}\\left\\lvert\\mathbb{X}_{s}\\right\\rvert Q_{A}\\sqrt{7r\\beta}}{\\sqrt{\\lambda}}\\right)\\sum_{h=0}^{H-1}\\mathbb{E}_{\\tau_{h}\\sim\\mathbb{P}_{\\theta^{*}}^{\\pi}}\\left\\lVert\\overline{{\\psi^{*}}}(\\tau_{h})\\right\\rVert_{(U_{h}^{k})^{-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Proof. First, we recall the definition of $\\widehat{U}_{h}^{k}$ , and we define its ground-truth counterpart replacing estimated features with true features, ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{}}&{{\\widehat{U}_{h}^{k}=\\lambda I+\\displaystyle\\sum_{\\tau\\in\\mathcal{D}_{h}^{k}}\\widehat{\\psi^{k}}(\\tau_{h})\\widehat{\\psi^{k}}(\\tau_{h})^{\\top},}}\\\\ {{}}&{{}}\\\\ {{U_{h}^{k}=\\lambda I+\\displaystyle\\sum_{\\tau\\in\\mathcal{D}_{h}^{k}}\\overline{{\\psi^{*}}}(\\tau_{h})\\overline{{\\psi^{*}}}(\\tau_{h})^{\\top}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "For any trajectory $\\tau_{H}\\in\\mathbb{H}_{H}$ , we have, ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sqrt{\\sum_{h=0}^{H-1}\\left\\|\\widehat{\\psi^{k}}(\\tau_{h})\\right\\|_{(\\widehat{U}_{h}^{k})^{-1}}^{2}}\\stackrel{(a)}{\\leq}\\sum_{h=0}^{H-1}\\left\\|\\widehat{\\psi^{k}}(\\tau_{h})\\right\\|_{(\\widehat{U}_{h}^{k})^{-1}}}}\\\\ &{}&{\\leq\\frac{1}{\\sqrt{\\lambda}}\\sum_{h=0}^{H-1}\\left\\|\\widehat{\\psi^{k}}(\\tau_{h})-\\overline{{\\psi^{*}}}(\\tau_{h})\\right\\|_{2}+\\sum_{h=0}^{H-1}\\left(1+\\frac{\\sqrt{r}\\sqrt{\\sum_{\\tau_{h}\\in\\mathcal{D}_{h}^{k}}\\left\\|\\widehat{\\psi^{k}}(\\tau_{h})-\\overline{{\\psi^{*}}}(\\tau_{h})\\right\\|_{2}}}{\\sqrt{\\lambda}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where step (a) is simply using $\\left\\|x\\right\\|_{2}\\leq\\left\\|x\\right\\|_{1}$ and step (b) is by the identity [35, Lemma 13]. Note that $r$ is the rank of the PSR and $r\\geq\\mathrm{rank}(\\{\\widehat{\\psi^{k}}(\\tau_{h}):\\tau_{h}\\in\\mathbb{H}_{h}\\}$ ),rank({b\\*(Th) : Th E Hn}). ", "page_idx": 46}, {"type": "text", "text": "Moreover, we have, ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\vert\\boxed{\\widehat{\\psi^{k}}}(\\tau_{h})-\\overline{{\\psi^{*}}}(\\tau_{h})\\right\\vert\\right\\vert_{2}\\leq\\left\\vert\\left\\vert\\widehat{\\psi^{k}}(\\tau_{h})-\\overline{{\\psi^{*}}}(\\tau_{h})\\right\\vert\\right\\vert_{1}}\\\\ &{\\overset{(a)}{=}\\displaystyle\\sum_{q_{h}\\in\\mathbb{Q}_{h}}\\left\\vert\\overline{{\\mathbb{P}}}_{\\widehat{\\theta^{k}}}\\left[q_{h}^{o}\\mid\\tau_{h},q_{h}^{a}\\right]-\\overline{{\\mathbb{P}}}_{\\theta^{*}}\\left[q_{h}^{o}\\mid\\tau_{h},q_{h}^{a}\\right]\\right\\vert}\\\\ &{\\overset{(b)}{\\leq}2\\displaystyle\\operatorname*{max}_{s\\in\\cal A}\\left\\vert\\mathbb{X}_{s}\\right\\vert Q_{A}\\mathrm{D}_{\\sf T V}\\left(\\mathbb{P}_{\\widehat{\\theta^{k}}}^{{\\mathbf{s}}_{h-1}^{\\mathrm{sp}}}(\\cdot|\\tau_{h}),\\mathbb{P}_{\\theta^{*}}^{{\\mathbf{u}}_{h-1}^{\\mathrm{sp}}}(\\cdot|\\tau_{h})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where we used the definition of $\\overline{{\\psi}}$ in (a) and the definition of the $\\mathtt{u}_{h-1}^{\\mathtt{e x p}}$ in (b). ", "page_idx": 46}, {"type": "text", "text": "Now, by Lemma 4, we have, ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\overline{{\\left\\|\\widehat{\\psi^{k}}(\\tau_{h})\\right\\|_{(\\hat{U}_{h}^{k})^{-1}}^{2}}}\\leq\\frac{1}{\\sqrt{\\lambda}}\\displaystyle\\sum_{h=0}^{H-1}\\left\\|\\widehat{\\psi^{k}}(\\tau_{h})-\\overline{{\\psi^{*}}}(\\tau_{h})\\right\\|_{2}+\\displaystyle\\sum_{h=0}^{H-1}\\left(1+\\frac{\\sqrt{r}\\sqrt{\\sum_{\\tau_{h}\\in\\mathcal{D}_{h}^{k}}\\left\\|\\widehat{\\psi^{k}}(\\tau_{h})-\\overline{{\\psi^{*}}}(\\tau_{h})\\right\\|_{2}^{2}}}{\\sqrt{\\lambda}}\\right)}\\\\ &{}&{\\leq\\displaystyle\\frac{1}{\\sqrt{\\lambda}}\\displaystyle\\sum_{h=0}^{H-1}\\left\\|\\widehat{\\psi^{k}}(\\tau_{h})-\\overline{{\\psi^{*}}}(\\tau_{h})\\right\\|_{2}+\\left(1+\\frac{2\\operatorname*{max}_{s\\in A}|\\mathbb{X}_{s}|Q_{A}\\sqrt{\\mathcal{T}\\mathcal{P}}}{\\sqrt{\\lambda}}\\right)\\displaystyle\\sum_{h=0}^{H-1}\\left\\|\\overline{{\\psi^{*}}}(\\tau_{h})\\right\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where the first line is combining the calculations above and the second line is by the estimation guarantee of Lemma 4. ", "page_idx": 46}, {"type": "text", "text": "The first term can be bounded in expectation under $\\mathbb{P}_{\\theta^{*}}^{\\pi}$ for any $\\pi$ as follows, ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left[\\left\\lVert\\widehat{\\mathbf{v}}^{k}(\\tau_{h})-\\widehat{\\mathbf{v}}^{k}(\\tau_{h})\\right\\rVert_{2}\\right]\\leq\\frac{M-1}{k_{\\operatorname*{max}}}\\mathbf{1}_{\\displaystyle\\geq0}\\mathbf{1}_{\\mathcal{S}_{n}\\sim\\mathbb{R}_{n}^{n_{k}}}\\left[\\left\\lVert\\widehat{\\mathbf{v}}^{k}(\\tau_{h})-\\widehat{\\mathbf{v}}^{k}(\\tau_{h})\\right\\rVert_{1}\\right]}&{}\\\\ {\\leq\\frac{M-1}{k_{\\operatorname*{max}}}\\mathbf{1}_{\\mathcal{S}_{n}}\\left[\\widehat{\\mathbf{v}}^{k}(\\tau_{h})\\left(\\mathbb{E}_{n}^{n_{k}}\\left(\\tau_{h})-\\mathbb{P}_{\\hat{\\mathbf{e}}}^{n_{k}}(\\tau_{h})\\right)+\\widehat{\\mathbf{v}}^{k}(\\tau_{h})\\mathbb{E}_{n_{k}}^{n_{k}}(\\tau_{h})-\\overline{{\\mathbf{v}}^{k}(\\tau_{h})}\\mathbb{E}_{n}^{n_{k}}(\\tau_{h})\\right.}\\\\ &{\\left.\\underset{\\mathrm{~\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\}}}{\\leq\\frac{M}{k_{\\operatorname*{max}}}}\\left\\lVert\\widehat{\\mathbf{v}}^{k}(\\tau_{h})\\right\\rVert_{n}\\left\\lVert\\mathbb{P}_{\\hat{\\mathbf{e}}^{k}}^{n_{k}}(\\tau_{h})-\\mathbb{P}_{\\hat{\\mathbf{e}}}^{n_{k}}(\\tau_{h})\\right\\rVert+\\left\\lVert\\widehat{\\mathbf{v}}^{k}(\\tau_{h})\\mathbb{P}_{\\hat{\\mathbf{e}}}^{n_{k}}(\\tau_{h})-\\overline{{\\mathbf{v}}^{k}(\\tau_{h})}\\mathbb{P}_{\\hat{\\mathbf{e}}}^{n_{k}}(\\tau_{h})\\right\\rVert_{n}}\\\\ {\\overset{(b)}{\\leq}\\left.\\underset{\\mathrm{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\}}}{\\sum_{k=0}^{lfloor n_{k}\\rfloor}\\sum_{\\gamma_{k}}\\left(\\left\\lVert\\widehat{\\mathbf{v}}^{k}(\\tau_{h})\\right\\rVert_{n}\\left\\lVert\\mathbb{P}_{\\hat{\\mathbf{e}}^{\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where step (a) is the triangle inequality, step (b) is the definition of $\\overline{{\\psi}}(\\tau_{h})$ , step (c) is since $\\begin{array}{r l}&{\\left\\|\\widehat{\\overline{{\\psi^{k}}}}(\\tau_{h})\\right\\|_{1}^{\\mathrm{~\\ldots~}}\\leq\\;\\left|\\mathbb{Q}_{h}^{A}\\right|\\;\\leq\\;\\stackrel{\\vee}{Q}_{A}\\mathrm{~for~any~}\\tau_{h}\\mathrm{~and~the~}}\\\\ &{\\mathsf{D}_{\\mathsf{T V}}\\left(\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau_{h}),\\mathbb{P}_{\\widehat{\\theta^{k}}}^{\\pi}(\\tau_{h})\\right)\\geq\\mathsf{D}_{\\mathsf{T V}}\\left(\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau_{h}),\\mathbb{P}_{\\widehat{\\theta^{k}}}^{\\pi}(\\tau_{h})\\right)\\!.}\\end{array}$ defnion of $\\psi(\\tau_{h})$ , and step (d is simply ", "page_idx": 47}, {"type": "text", "text": "Putting this together concludes the proof, ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\upzeta}_{\\tau_{H}\\sim\\mathbb{P}_{\\theta^{*}}^{\\pi}}\\left[\\sqrt{\\displaystyle\\sum_{h=0}^{H-1}\\left\\lVert\\widehat{\\psi^{k}}(\\tau_{h})\\right\\rVert_{(\\widehat{U}_{h}^{k})^{-1}}^{2}}\\right]}\\\\ &{\\le\\displaystyle\\frac{2H Q_{A}}{\\sqrt{\\lambda}}\\mathsf{D}_{\\mathsf{T V}}\\left(\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau_{h}),\\mathbb{P}_{\\theta^{k}}^{\\pi}(\\tau_{h})\\right)+\\left(1+\\frac{2\\operatorname*{max}_{s\\in A}\\left\\lvert\\mathbb{X}_{s}\\right\\rvert Q_{A}\\sqrt{7r\\beta}}{\\sqrt{\\lambda}}\\right)\\sum_{h=0}^{H-1}\\mathbb{E}_{\\tau_{h}\\sim\\mathbb{P}_{\\theta^{*}}^{\\pi}}\\left\\lVert\\overline{{\\psi^{*}}}(\\tau_{h})\\right\\rVert_{(U_{h}^{k})^{-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "The following lemma bounds the cumulative estimation error of the probability of trajectories. It can be proved via an $\\ell_{2}$ Eluder argument [38, 76, 77]. A significant portion of the proof is very similar to that of Proposition 4, involving an exchange of $\\widehat{(\\cdot)}$ and $(\\cdot)^{\\ast}$ . We include the proof for completeness. Lemma 8. Under event $\\mathcal{E}$ for any $h\\in\\{0,\\ldots,H-1\\}$ ,wehave ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\sum_{k}\\mathsf{D}_{\\mathsf{T V}}\\left(\\mathbb{P}_{\\theta^{\\star}}^{\\pi^{k}}(\\tau_{H}),\\mathbb{P}_{\\hat{\\theta}^{k}}^{\\pi^{k}}(\\tau_{H})\\right)\\lesssim\\frac{\\operatorname*{max}_{s\\in\\mathcal{A}}|\\mathbb{X}_{s}|\\,Q_{A}\\sqrt{\\beta}}{\\gamma}\\sqrt{r H K\\log\\left(1+\\frac{d Q_{A}K}{\\gamma^{4}}\\right)}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Here, $a\\lesssim b$ indicates that there is an absolute positive constant c s.t. $a\\leq c\\cdot b$ ", "page_idx": 47}, {"type": "text", "text": "Proof. Recall that, by the first inequality in Proposition 4, we have: ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathsf{D}_{\\mathsf{T V}}\\left(\\mathbb{P}_{\\theta^{\\star}}^{\\pi^{k}}(\\tau_{H}),\\mathbb{P}_{\\hat{\\theta}^{k}}^{\\pi^{k}}(\\tau_{H})\\right)\\le\\sum_{h=1}^{H}\\sum_{\\tau_{H}}\\left|\\widehat{m}^{k}(\\omega_{h})^{\\top}\\left(\\widehat{M}_{h}^{k}(x_{h})-M_{h}^{\\star}(x_{h})\\right)\\psi^{\\star}(\\tau_{h-1})\\right|\\pi^{k}(\\tau_{H})\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "This is very similar to the inequality in Lemma 5, with the difference being that the quantities associated with the estimated model and the true model are exchanged. Since both correspond to a PSR, the analysis follows a similar series of steps. We will use analogous notation to Lemma 5. We index the future trajectory $\\omega_{h-1}=(x_{h},\\ldots,x\\bar{_{H}})$ by $i$ and history trajectory $\\tau_{h-1}=\\left(x_{1},\\ldots,x_{h-1}\\right)$ by $j$ Wedenote $\\widehat{m}^{k}(\\omega_{h})^{\\top}\\left(\\widehat{M}_{h}^{k}(x_{h})-M_{h}^{\\star}(x_{h})\\right)$ as $w_{i},\\overline{{\\psi}}^{\\star}(\\tau_{h-1})$ $x_{j}$ and $\\pi(\\omega_{h-1}|\\tau_{h-1})$ as $\\pi_{i|j}$ ", "page_idx": 47}, {"type": "text", "text": "Define the matrix, ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\Lambda_{h}^{k}=\\lambda_{0}I+\\sum_{t<k}\\mathbb{E}_{j\\sim\\mathbb{P}_{\\theta^{\\star}}^{\\pi^{t}}}\\left[x_{j}x_{j}^{\\top}\\right]\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where $\\lambda_{0}$ is a constant to be determined later. ", "page_idx": 48}, {"type": "text", "text": "For any policy $\\pi$ , using a similar calculation as in Lemma 5, we have, ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\tau,u}\\left|\\widehat{m}^{k}(\\omega_{h})^{\\top}\\left(\\widehat{M}_{h}^{k}(x_{h})-M_{h}^{*}(x_{h})\\right)\\psi^{*}(\\tau_{h-1})\\right|\\pi^{k}(\\tau_{H})}\\\\ &{=\\mathbb{E}_{j\\sim\\mathbb{P}_{\\theta^{k}}^{n}}\\left[\\displaystyle\\sum_{i}\\pi_{i j}\\left|w_{i}^{\\top}x_{j}\\right|\\right]}\\\\ &{=\\mathbb{E}_{j\\sim\\mathbb{P}_{\\theta^{k}}^{n}}\\left[\\left(\\sum_{i}\\pi_{i j}\\mathrm{sign}(w_{i}^{\\top}x_{j})w_{i}\\right)^{\\top}x_{j}\\right]}\\\\ &{\\le\\mathbb{E}_{j\\sim\\mathbb{P}_{\\theta^{k}}^{n}}\\left[\\|x_{j}\\|_{{\\boldsymbol A}_{h}^{*}}\\left\\|\\displaystyle\\sum_{i}\\pi_{i j}\\mathrm{sign}(w_{i}^{\\top}x_{j})w_{i}\\right\\|_{{\\boldsymbol A}_{h}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where the last line is the Cauchy-Schwarz inequality. ", "page_idx": 48}, {"type": "text", "text": "Fix $j\\:=\\:j_{0}$ and consider the term: $\\begin{array}{r}{\\left\\|\\sum_{i}\\pi_{i|j_{0}}\\mathrm{sign}(\\boldsymbol{w}_{i}^{\\top}\\boldsymbol{x}_{j_{0}})\\boldsymbol{w}_{i}\\right\\|_{\\Lambda_{h}}}\\end{array}$ in the above. This term can be partitioned in the same manner as in Lemma 5 by simply using the definition of $\\Lambda_{h}$ and expanding, ", "page_idx": 48}, {"type": "text", "text": "We bound each term separately. The process is nearly identical to the proof of Lemma 5, but we show it for completeness. ", "page_idx": 48}, {"type": "text", "text": "$\\sqrt{I_{1}}$ is bounded by the sum of two terms, ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{I_{1}}=\\sqrt{\\lambda_{0}}\\underset{z\\in\\mathbb{R}^{d_{h-1}^{\\mathbf{n}}}\\cup\\underline{{s}}_{h-1}}{\\operatorname*{max}}\\left|\\sum_{i}\\pi_{i j_{0}}\\cdot\\mathrm{sign}(w_{i}^{\\top}x_{j_{0}})\\cdot w_{i}^{\\top}z\\right|}\\\\ &{\\quad\\overset{(a)}{\\leq}\\sqrt{\\lambda_{0}}\\underset{\\|z\\|_{2}=1}{\\operatorname*{max}}\\sum_{i_{\\alpha h-1}}\\left|\\widehat{m}^{k}(\\omega_{h})^{\\top}\\left(\\widehat{M}_{h}^{k}(x_{h})-M_{h}^{\\star}(x_{h})\\right)z\\right|\\pi(\\omega_{h-1}|j_{0})}\\\\ &{\\quad\\overset{(b)}{\\leq}\\sqrt{\\lambda_{0}}\\underset{\\|z\\|_{2}=1}{\\operatorname*{max}}\\sum_{i_{\\alpha h-1}}\\left|\\widehat{m}^{k}(\\omega_{h})^{\\top}\\widehat{M}_{h}^{k}(x_{h})z\\right|\\pi(\\omega_{h-1}|j_{0})}\\\\ &{\\quad\\quad+\\sqrt{\\lambda_{0}}\\underset{\\|z\\|_{2}=1}{\\operatorname*{max}}\\sum_{i_{\\alpha h-1}}\\left|\\widehat{m}^{k}(\\omega_{h})^{\\top}M_{h}^{\\star}(x_{h})z\\right|\\pi(\\omega_{h-1}|j_{0}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where step (a) is the definition of $w_{i},\\pi_{i|j_{0}}$ , and the triangle inequality, and step (b) is the triangle inequality. ", "page_idx": 48}, {"type": "text", "text": "Both terms can be bounded by the $\\gamma.$ -well-conditioning assumption on $\\widehat{\\theta}^{k}$ and $\\theta^{*}$ . Consider the first term, ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{|\\geq||\\geq|2}{\\operatorname*{max}}\\sum_{i=1}^{N}\\left|\\hat{m}^{k}(\\boldsymbol{u}_{h})^{\\top}\\widehat{M}_{h}^{k}(\\boldsymbol{x}_{h})\\boldsymbol{z}\\right|\\pi(\\omega_{h-1}|j_{0})}\\\\ &{\\qquad\\quad=\\underset{|\\geq||\\underline{{\\mathcal{S}}}||_{2}=1}{\\operatorname*{max}}\\sum_{\\boldsymbol{h}_{h}}\\left(\\sum_{\\omega_{h}}\\left|\\hat{m}^{k}(\\omega_{h})^{\\top}\\widehat{M}_{h}^{k}(\\boldsymbol{x}_{h})\\boldsymbol{z}\\right|\\pi(\\omega_{h}|j_{0},\\boldsymbol{x}_{h})\\right)\\pi(\\boldsymbol{x}_{h}|j_{0})}\\\\ &{\\qquad\\stackrel{(a)}{\\leq}\\underset{|\\geq|\\underline{{\\mathcal{S}}}||_{2}=1}{\\operatorname*{max}}\\sum_{\\boldsymbol{h}_{h}}\\frac{1}{\\gamma}\\left\\|\\widehat{M}_{h}^{k}(\\boldsymbol{x}_{h})\\boldsymbol{z}\\right\\|_{1}\\pi(\\boldsymbol{x}_{h}|j_{0})}\\\\ &{\\qquad\\stackrel{(b)}{\\leq}\\frac{1}{\\gamma}\\underset{|\\geq|\\underline{{\\mathcal{S}}}||_{2}=1}{\\operatorname*{max}}\\frac{|\\mathbb{Q}_{h+1}^{A}||\\mathbb{Q}||_{1}}{\\gamma}}\\\\ &{\\qquad\\stackrel{(c)}{\\leq}\\frac{\\sqrt{d}Q_{{4}}}{\\gamma^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where step (a) is by the first condition in Assumption 1, step (b) is by the second condition of Assumption 1, an steisbyth fact that $\\begin{array}{r l}&{\\operatorname*{max}_{z\\in\\mathbb{R}^{d_{h-1}}:\\|z\\|_{2}=1}\\|z\\|_{1}=\\sqrt{d_{h-1}}\\leq\\sqrt{d}}\\end{array}$ and $\\left|\\mathbb{Q}_{h+1}^{A}\\right|\\leq Q_{A}$ In the above, note that we used the $\\gamma$ -well-conditioning of PSR $\\widehat{\\theta}^{k}$ in both step (a) and step (b). The second term in $\\sqrt{I_{1}}$ admits an identical bound, simply by using the well-conditioning of the PSR $\\widehat{\\theta}^{k}$ in the first step and $\\theta^{*}$ in the second step. Hence, we have that ", "page_idx": 49}, {"type": "equation", "text": "$$\nI_{1}\\leq4\\frac{\\lambda_{0}d Q_{A}^{2}}{\\gamma^{4}}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Now, we consider the term $I_{2}$ ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{2}\\le\\displaystyle\\sum_{t<k}\\mathbb{E}_{\\tau_{h-1}\\sim\\mathbb{P}_{\\theta^{k}}^{\\pi^{k}}}\\left[\\left(\\sum_{\\omega_{h-1}}\\left|\\widehat{m}^{k}(\\omega_{h})^{\\top}\\left(\\widehat{M}_{h}^{k}(x_{h})-M_{h}^{\\star}(x_{h})\\right)\\overline{{\\psi}}^{\\ast}(\\tau_{h-1})\\right|\\pi(\\omega_{h-1}|j_{0})\\right)^{2}\\right]}\\\\ &{\\quad\\le\\displaystyle\\sum_{t<k}\\mathbb{E}_{j\\sim\\mathbb{P}_{\\theta^{k}}^{\\pi^{k}}}\\left[\\left(\\sum_{\\omega_{h-1}}\\left|\\widehat{m}^{k}(\\omega_{h})^{\\top}\\widehat{M}_{h}(x_{h})\\left(\\overline{{\\psi}}^{\\star}(\\tau_{h-1})-\\widehat{\\overline{{\\psi}}}^{k}(\\tau_{h-1})\\right)\\right|\\pi(\\omega_{h-1}|j_{0})\\right.\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\left.-\\frac{1}{\\epsilon}\\right)^{k}(\\tau_{h-1}|j_{0})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "equation", "text": "$$\n+\\sum_{\\omega_{h-1}}\\left|\\widehat{m}^{k}(\\omega_{h})^{\\top}\\left(\\widehat{M}_{h}^{k}(x_{h})\\widehat{\\overline{{\\psi}}}^{k}(\\tau_{h-1})-M_{h}^{\\star}(x_{h})\\overline{{\\psi}}^{\\star}(\\tau_{h-1})\\right)\\right|\\pi(\\omega_{h-1}|j_{0})\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 49}, {"type": "equation", "text": "$$\n=:\\sum_{t<k}\\mathbb{E}_{j\\sim\\mathbb{P}_{\\theta^{\\star}}^{\\pi^{k}}}(I_{3}+I_{4})^{2}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where the line follows by the fact that $x\\leq|x|$ and the line follows from the triangle inequality by adding and subtracting $\\widehat{m}_{h}(\\omega_{h})\\widehat{M}_{h}(x_{h})\\widehat{\\overline{{{\\psi}}}}^{k}(\\tau_{h-1})$ inside the absolute value. We now bound each of $I_{3}$ and $I_{4}$ ", "page_idx": 49}, {"type": "text", "text": "First, we bound $I_{3}$ as follows, ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{3}=\\displaystyle\\sum_{\\omega_{h-1}}\\left|\\widehat{m}_{h}^{k}(\\omega_{h})^{\\top}\\widehat{M_{h}}(x_{h})\\left(\\overline{{\\psi}}^{\\star}(\\tau_{h-1})-\\widehat{\\overline{{\\psi}}}^{k}(\\tau_{h-1})\\right)\\right|\\pi(\\omega_{h-1}|j_{0})}\\\\ &{\\overset{(a)}{=}\\displaystyle\\sum_{\\omega_{h-1}}\\left|\\widehat{m}_{h-1}^{k}(\\omega_{h-1})^{\\top}\\left(\\overline{{\\psi}}^{\\star}(\\tau_{h-1})-\\widehat{\\overline{{\\psi}}}^{k}\\left(\\tau_{h-1}\\right)\\right)\\right|\\pi(\\omega_{h-1}|j_{0})}\\\\ &{\\overset{(b)}{\\leq}\\displaystyle\\frac{1}{\\gamma}\\left\\|\\widehat{\\overline{{\\psi}}}^{k}(\\tau_{h-1})-\\overline{{\\psi}}^{\\star}(\\tau_{h-1})\\right\\|_{1}}\\\\ &{=\\displaystyle\\frac{1}{\\gamma}\\sum_{q_{h-1}\\in\\mathbb{Q}_{h-1}}\\left|\\overline{{\\mathbb{P}}}_{\\hat{\\theta}^{h}}\\left[q_{h-1}\\mid\\tau_{h-1}\\right]-\\overline{{\\mathbb{P}}}_{\\theta^{\\star}}\\left[q_{h-1}\\mid\\tau_{h-1}\\right]\\right|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where step (a) is since $\\widehat{m}(\\omega_{h})^{\\top}\\widehat{M}_{h}(x_{h})=\\widehat{m}(\\omega_{h-1})^{\\top}$ , step (b) is by Assumption 1, and the final equality is by the definition of $\\overline{{\\psi}}$ ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{4=\\sum_{n=1}^{\\infty}\\left|\\hat{m}^{k}(\\omega_{h})^{\\top}\\left(\\widehat{H}_{h}^{k}(x_{h})\\widehat{\\psi}^{k}(\\tau_{h-1})-M_{h}^{*}(x_{h})\\widehat{\\psi}^{k}(\\tau_{h-1})\\right)\\right|\\pi(\\omega_{h-1}|j_{0})}}\\\\ &{\\overset{(a)}{=}\\sum_{n=h-1}^{\\infty}\\left|\\hat{m}^{k}(\\omega_{h})^{\\top}\\left(\\overline{{\\mathbb{P}}}_{\\hat{\\theta}^{k}}\\left[x_{h}\\ |\\ \\tau_{h-1}\\right]\\widehat{\\psi}_{h}(\\tau_{h})-\\overline{{\\mathbb{P}}}_{\\theta^{k}}\\left[x_{h}\\ |\\ \\tau_{h-1}\\right]\\overline{{\\psi}}_{h}^{*}(\\tau_{h})\\right)\\right|\\pi(\\omega_{h-1}|j_{0})}\\\\ &{=\\sum_{\\underline{{\\nu}}_{h}}\\left(\\sum_{\\omega_{h}}\\left|\\hat{m}^{k}(\\omega_{h})^{\\top}\\left(\\overline{{\\mathbb{P}}}_{\\hat{\\theta}^{k}}\\left[x_{h}\\ |\\ \\tau_{h-1}\\right]\\widehat{\\psi}_{h}(\\tau_{h})-\\overline{{\\mathbb{P}}}_{\\theta^{*}}\\left[x_{h}\\ |\\ \\tau_{h-1}\\right]\\overline{{\\psi}}_{h}^{*}(\\tau_{h})\\right)\\right|\\pi(\\omega_{h})|j_{0},x_{h}\\right)\\right)\\pi(x_{h}|j_{0})}\\\\ &{\\overset{(b)}{\\le}\\frac{1}{\\gamma}\\sum_{\\underline{{\\nu}}_{h}}\\left\\|\\overline{{\\mathbb{P}}}_{\\hat{\\theta}^{k}}\\left[x_{h}\\ |\\ \\tau_{h-1}\\right]\\widehat{\\psi}_{h}(\\tau_{h})-\\overline{{\\mathbb{P}}}_{\\theta^{*}}\\left[x_{h}\\ |\\ \\tau_{h-1}\\right]\\overline{{\\psi}}_{h}^{*}(\\tau_{h})\\right\\|_{1}\\pi(x_{h}|j_{0})}\\\\ &{\\overset{(c)}{=}\\frac{1}{\\gamma}\\sum_{\\nu_{h}}\\underbrace{\\int_{0}^{\\infty}\\left|\\overline{{\\mathbb{P}}}_{\\hat{\\theta}^{k}}\\left[x_{h \n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where step (a) is by the fact that $M_{h}(x_{h})\\overline{{\\psi}}_{h-1}(\\tau_{h-1})\\,=\\,\\overline{{\\mathbb{P}}}\\left[x_{h}\\ |\\ \\tau_{h-1}\\right]\\overline{{\\psi}}(\\tau_{h})$ , as shown in Equation (28), step (b) is by Assumption 1, and step (c) is since $\\left[\\overline{{\\psi}}_{h}(\\tau_{h})\\right]_{l}=\\overline{{\\mathbb{P}}}_{\\theta}\\left[q_{h}^{l}\\ |\\ \\tau_{h}\\right]$ ", "page_idx": 50}, {"type": "text", "text": "Combining the above, we have that, ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\underset{\\mathbf{S}}{\\sum}\\frac{\\mathbf{E}}{\\nu}\\frac{\\partial\\mathbf{E}}{\\partial\\alpha_{2}}r^{2}(\\mathbf{\\hat{s}}+L_{2})^{2}}\\\\ &{\\leq\\underset{\\mathbf{S}}{\\sum}\\frac{\\mathbf{E}}{\\nu}\\frac{\\partial\\mathbf{E}}{\\partial\\alpha_{2}}r^{2}\\Bigg[\\left(\\underset{\\mathcal{F}_{\\eta_{2}}\\in\\mathcal{F}_{\\eta_{2}}}{\\underbrace{\\left(\\mathbf{\\hat{s}}_{2}-\\mathbf{\\hat{e}}_{\\eta_{2}}\\right)^{2}+\\left(\\mathbf{\\hat{s}}_{2}-\\mathbf{\\hat{e}}_{\\eta_{2}}\\right)(\\mathbf{\\hat{s}}_{2}-\\mathbf{\\hat{e}}_{\\eta_{2}}\\left)-\\mathbf{\\hat{e}}_{\\eta_{2}}\\left(\\mathbf{\\hat{s}}_{2}-\\mathbf{\\hat{e}}_{\\eta_{2}}\\right)(\\mathbf{\\hat{s}}_{2}-\\mathbf{\\hat{e}}_{\\eta_{2}}\\left)}\\right)}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.+\\frac{1}{2}\\underset{\\mathbf{S}}{\\sum}\\frac{\\mathbf{E}}{\\nu}\\underset{\\mathbf{S}}{\\sum}\\frac{\\mathbf{E}}{\\nu}\\Bigg[\\mathcal{F}_{\\eta_{2}}\\left[\\mathcal{F}_{\\eta_{2}}\\left[\\ln\\left[\\alpha_{2}\\right]\\right]\\left(\\underset{\\mathcal{F}_{\\eta_{2}}\\left(\\mathbf{\\hat{s}}_{2}-\\mathbf{\\hat{e}}_{\\eta_{2}}\\left)+\\mathbf{\\hat{e}}_{\\eta_{2}}\\left(\\mathbf{\\hat{s}}_{2}-\\mathbf{\\hat{e}}_{\\eta_{2}}\\left)+\\mathbf{\\hat{e}}_{\\eta_{2}}\\left(\\mathbf{\\hat{s}}_{2}\\right)\\right]}\\right)\\right]}\\\\ &{=\\frac{1}{\\eta_{2}}\\sum_{\\alpha_{2}}^{\\alpha_{2}}\\underset{\\mathbf{S}}{\\sum}\\alpha_{2}r^{2}\\Bigg[\\left(\\underset{\\mathcal{F}_{\\eta_{2}}\\in\\mathcal{F}_{\\eta_{2}}}{\\underbrace{\\left(\\mathbf{\\hat{s}}_{2}-\\mathbf{\\hat{e}}_{\\eta_{2}}\\left)^{2}+\\left(\\mathbf{\\hat{s}}_{2}-\\mathbf{\\hat{e}}_{\\eta_{2}}\\left)(\\mathbf{\\hat{s}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where step (a) follows from the definition of $\\mathbb{Q}_{h-1}^{\\mathrm{exp}}$ (same as Lemma 5), and step (b) is because the Hellinger distance bounds the total ariationdistance and since $\\begin{array}{r}{\\left|\\mathbb{Q}_{h-1}^{\\mathtt{e x p}}\\right|\\le2\\operatorname*{max}_{s\\in\\mathcal{A}}\\left|\\mathbb{X}_{s}\\right|Q_{A}}\\end{array}$ Hence, we have, ", "page_idx": 50}, {"type": "equation", "text": "$$\nI_{2}\\leq4\\operatorname*{max}_{s\\in A}\\left|\\mathbb{X}_{s}\\right|^{2}Q_{A}^{2}\\frac{1}{\\gamma^{2}}\\sum_{t<k}\\mathsf{D}_{\\mathbb H}^{2}\\left(\\mathbb{P}_{\\widehat{\\theta}^{k}}^{\\nu_{h}(\\pi^{t},u_{\\mathsf{Q}_{h-1}^{\\mathrm{exp}}})}\\left(\\tau_{H}\\right),\\mathbb{P}_{\\theta^{*}}^{\\nu_{h}(\\pi^{t},u_{\\mathsf{Q}_{h-1}^{\\mathrm{exp}}})}\\left(\\tau_{H}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Now, combining the bound on $I_{1}$ and $I_{2}$ allows us to finally bound $\\begin{array}{r}{\\left\\|\\sum_{i}\\pi_{i|j}\\mathrm{sign}(w_{i}^{\\top}x_{j})w_{i}\\right\\|_{\\Lambda_{h}}^{2}}\\end{array}$ as follows, ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\displaystyle\\sum_{i}\\pi_{i|j}\\mathrm{sign}(w_{i}^{\\top}x_{j})w_{i}\\right\\|_{\\Lambda_{h}}^{2}}\\\\ &{\\le\\displaystyle\\frac{4\\lambda_{0}Q_{A}^{2}d}{\\gamma^{4}}+\\frac{4\\operatorname*{max}_{s\\in A}|\\mathbb{X}_{s}|^{2}\\,Q_{A}^{2}}{\\gamma^{2}}\\cdot\\displaystyle\\sum_{t<k}\\mathsf{D}_{\\mathtt{H}}^{2}\\left(\\mathbb{P}_{\\hat{\\theta}^{k}}^{\\nu_{h}(\\pi^{t},\\mathfrak{u}_{h-1}^{\\mathrm{sp}})}\\left(\\tau_{H}\\right),\\mathbb{P}_{\\theta^{*}}^{\\nu_{h}(\\pi^{t},\\mathfrak{u}_{h-1}^{\\mathrm{sp}})}\\left(\\tau_{H}\\right)\\right)}\\\\ &{=:\\left(\\tilde{\\alpha}_{h-1}^{k}\\right)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "We choose $\\begin{array}{r}{\\lambda_{0}=\\frac{\\gamma^{4}}{4Q_{A}^{2}d}}\\end{array}$ , and bound $\\begin{array}{r}{\\tilde{\\alpha}^{2}:=\\sum_{h}\\left(\\tilde{\\alpha}_{h-1}^{k}\\right)^{2}}\\end{array}$ as follows, ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{h}\\left(\\tilde{\\alpha}_{h-1}^{k}\\right)^{2}=H+\\frac{4\\operatorname*{max}_{s\\in\\mathcal{A}}\\left|\\mathbb{X}_{s}\\right|^{2}Q_{\\mathcal{A}}^{2}\\beta}{\\gamma^{2}}\\sum_{\\pi\\in\\mathcal{D}^{k}}\\mathrm{D}_{\\mathbb{H}}^{2}\\left({\\mathbb P}_{\\widehat{\\theta}^{k}}^{\\nu_{h}(\\pi^{t},\\mathbf{u}_{h-1}^{\\mathrm{op}})}\\left(\\tau_{H}\\right),{\\mathbb P}_{\\theta^{*}}^{\\nu_{h}(\\pi^{t},\\mathbf{u}_{h-1}^{\\mathrm{op}})}\\left(\\tau_{H}\\right)\\right)}\\\\ &{\\displaystyle\\leq H+\\frac{28\\operatorname*{max}_{s\\in\\mathcal{A}}\\left|\\mathbb{X}_{s}\\right|^{2}Q_{\\mathcal{A}}^{2}\\beta}{\\gamma^{2}}}\\\\ &{\\displaystyle\\lesssim\\frac{\\operatorname*{max}_{s\\in\\mathcal{A}}\\left|\\mathbb{X}_{s}\\right|^{2}Q_{\\mathcal{A}}^{2}\\beta}{\\gamma^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where the second line is by the estimation guarantee of Lemma 4. ", "page_idx": 51}, {"type": "text", "text": "Thus, we have, ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\operatorname{cur}\\left(\\mathbb{P}_{\\rho}^{*}(\\tau),\\mathbb{P}_{\\rho}^{*}(\\tau_{H})\\right)\\leq\\sum_{s=1}^{M}\\sum_{i=1}^{n}\\left|\\widehat{m}^{k}(s_{i})\\right|^{\\top}\\left(\\widehat{M}_{h}^{k}(x_{h})-M_{h}^{k}(x_{h})\\right)\\psi^{*}(\\tau_{H-1})\\left|\\pi^{k}(\\tau_{H})\\right.}}\\\\ &{\\leq\\left.\\sum_{k=1}^{M}\\mathbb{E}_{s_{k}\\sim\\tau_{H}^{k}}\\left[\\left|\\widehat{\\psi}^{*}(\\tau_{h-1})\\right|_{A_{\\lambda}^{k}}\\right]\\sum_{i=1}^{n}\\eta_{k}\\mathrm{glap}(\\alpha_{k}^{\\top}\\pi_{j})w\\right|_{A_{\\lambda}}\\right]}\\\\ &{\\leq\\mathbb{E}_{s_{k}\\sim\\tau_{H}^{k}}\\left[\\left|\\widehat{\\psi}^{*}(\\tau_{h-1})\\right|_{A_{\\lambda}^{k}}\\right]\\left|\\sum_{\\sigma}\\tau_{H}\\mathrm{glap}(\\alpha_{k}^{\\top}\\pi_{j})w\\right|_{A_{\\lambda}}\\right]}\\\\ &{\\overset{(a)}{\\leq}\\mathbb{E}_{s_{k}\\sim\\tau_{H}^{k}}\\left[\\sqrt{\\displaystyle\\sum_{s=1}^{M}\\left|\\widehat{\\psi}^{*}(\\tau_{h-1})\\right|_{A_{\\lambda}^{k}}^{2}\\left|\\sum_{s=1}^{M}\\left|\\sum_{s=1}^{n}\\eta_{k}\\mathrm{glap}(\\alpha_{k}^{\\top}\\pi_{j})w\\right|_{s=1}^{2}}\\right]}\\\\ &{\\overset{(b)}{\\leq}\\hat{\\alpha}\\cdot\\mathbb{E}_{s_{k}\\sim\\tau_{H}^{k}}\\left[\\sqrt{\\displaystyle\\sum_{s=1}^{M}\\left|\\widehat{\\psi}^{*}(\\tau_{h-1})\\right|_{A_{\\lambda}^{k}}^{2}}\\right]}\\\\ &{\\leq\\hat{\\alpha}\\cdot\\sqrt{\\displaystyle\\sum_{s=1}^{M}\\mathbb{E}_{s_{k}\\sim\\tau_{H}^{k}}\\left[\\left|\\widehat{\\psi}^{*}(\\tau_{h-1})\\right|_{A_{\\lambda}^{k}}^{2}\\right]},}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where step (a) is by the Cauchy-Schwarz inequality and step (b) is by the bound established above. Since the total variation distance is bounded above by 2, we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathsf{D}_{\\mathsf{T V}}\\left(\\mathbb{P}_{\\theta^{\\star}}^{\\pi^{k}}(\\tau_{H}),\\mathbb{P}_{\\hat{\\theta}^{k}}^{\\pi^{k}}(\\tau_{H})\\right)\\leq\\operatorname*{min}\\left\\{\\tilde{\\alpha}\\cdot\\sqrt{\\sum_{h=1}^{H}\\mathbb{E}_{\\tau_{h-1}\\sim\\mathbb{P}_{\\theta^{\\star}}^{\\pi^{k}}}\\left[\\left\\lVert\\overline{{\\psi}}^{*}(\\tau_{h-1})\\right\\rVert_{\\Lambda_{h}^{\\dagger}}^{2}\\right]},2\\right\\}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Finally, the proof is completed by summing over $k$ using the elliptical potential lemma as follows, ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{k=1}^{K}\\mathbb{D}_{\\mathbb{P}}\\left(\\mathbb{P}_{*}^{k}(\\tau_{h}),\\mathbb{P}_{*}^{k+1}(\\tau_{H})\\right)\\leq\\displaystyle\\sum_{k=1}^{K}\\operatorname*{min}\\left\\{\\bar{\\alpha}\\cdot\\sqrt{\\displaystyle\\sum_{k=1}^{H}\\mathbb{E}_{\\tau_{h-1}\\sim\\tau_{h-1}^{k+1}}\\left[\\left\\|\\overline{{\\hat{\\psi}}}^{k}(\\tau_{h-1})\\right\\|_{\\boldsymbol{\\Lambda}_{\\mathbb{A}}}^{2}\\right]},2\\right\\}}\\\\ {\\displaystyle}&{\\overset{(a)}{\\leq}\\displaystyle\\sqrt{K}\\sqrt{\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{K}\\operatorname*{min}\\left\\{\\bar{\\alpha}^{\\iota}\\cdot\\mathbb{E}_{\\tau_{h-1}\\sim\\tau_{h-1}^{k+1}}\\left[\\left\\|\\overline{{\\hat{\\psi}}}^{k}(\\tau_{h-1})\\right\\|_{\\boldsymbol{\\Lambda}_{\\mathbb{A}}}^{2}\\right],4\\right\\}}}\\\\ {\\displaystyle}&{\\leq\\sqrt{K}\\bar{\\alpha}\\sqrt{\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\operatorname*{min}\\left\\{\\mathbb{E}_{\\tau_{h-1}\\sim\\tau_{h-1}^{k+1}}\\left[\\left\\|\\overline{{\\hat{\\psi}}}^{*}(\\tau_{h-1})\\right\\|_{\\boldsymbol{\\Lambda}_{\\mathbb{A}}}^{2}\\right],4/\\bar{\\alpha}^{2}\\right\\}}}\\\\ {\\displaystyle}&{\\overset{(b)}{\\leq}\\sqrt{K R}\\bar{\\alpha}\\sqrt{(1+(\\lambda^{2})r\\log(1+K/\\lambda_{0})}}\\\\ {\\displaystyle}&{\\overset{(c)}{\\leq}\\displaystyle\\operatorname*{max}_{\\bar{\\alpha}\\in\\mathcal{I}}\\lvert\\mathbb{S}_{\\tau}|){(d_{*}\\sqrt{K R}{\\beta}\\log(1+K/\\lambda_{0}))}}\\\\ {\\displaystyle}&{\\overset{(d)}{\\leq}\\displaystyle\\operatorname*{max}_{\\bar{\\alpha}\\in\\mathcal{I}}\\sum_{k=1}^{K}\\|\\mathcal{Q}_{\\epsilon}\\sqrt{r K\\mathcal{H}\\beta\\log(1+K/\\lambda_{0})}}\\\\ {\\displaystyle}&{\\overset{(d)}{\\leq}\\displaystyle\\operatorname*{max}_{\\bar{\\alpha}\\in\\mathcal{I}}\\sum_{k=1}^{K}\\|\\mathcal{Q}_{\\epsilon\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Here, step (a) is uses the relationship between the $\\ell_{1}$ and $\\ell_{2}$ norms $\\left\\|\\cdot\\right\\|_{1}\\leq{\\sqrt{d}}\\left\\|\\cdot\\right\\|_{2}$ . Step (b) is by the elliptical potential lemma ([35, Lemma 14]; see also [96-98]). Step (c) uses the bound on $\\tilde{\\alpha}$ established above and the fact that $\\sqrt{1+4/\\tilde{\\alpha}^{2}}$ is bounded by an absolute constant. Step (d) uses the definition of $\\lambda_{0}$ and the fact that $\\sqrt{28(1+4/\\tilde{\\alpha}^{2})}$ is bounded by an absolute constant. \u53e3 ", "page_idx": 52}, {"type": "text", "text": "Using the two lemmas above, we are now ready to show that $\\begin{array}{r}{\\sum_{k=1}^{K}V_{\\widehat{\\theta}^{k}}^{\\widehat{b}^{k}}(\\pi^{k})\\,=\\,O(\\sqrt{K})}\\end{array}$ The argument is identical to [35, Lemma 6] and does not require modification for generalized PSRs. We recount the argument for completeness. ", "page_idx": 52}, {"type": "text", "text": "Lemma 9. Under the event $\\mathcal{E}$ , with probability at least $1-\\delta$ we have: ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}V_{\\widehat{\\theta}^{k}}^{\\widehat{b}^{k}}(\\pi^{k})\\lesssim\\left(\\sqrt{r}+\\frac{Q_{A}\\sqrt{H}}{\\gamma}\\right)\\frac{\\operatorname*{max}_{s\\in\\cal A}^{2}Q_{A}^{2}H\\sqrt{d r H\\beta K\\beta_{0}}}{\\gamma^{2}}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where \u03b2o = maxlog(1 + K/\u5165),log(1 + dQAK/), and \u5165 = maxaeAlxlQA\u03b2max(V,QAVH/} ", "page_idx": 52}, {"type": "text", "text": "Proof. First, we note that, ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\hat{\\gamma}_{\\hat{\\theta}^{k}}^{\\hat{\\kappa}}(\\pi^{k})=\\sum_{\\tau}\\mathbb{P}_{\\hat{\\theta}^{k}}^{\\pi^{k}}(\\tau)\\widehat{b}^{k}(\\tau)=\\sum_{\\tau}\\mathbb{P}_{\\theta^{k}}^{\\pi^{k}}(\\tau)\\widehat{b}^{k}(\\tau)+\\sum_{\\tau}(\\mathbb{P}_{\\hat{\\theta}^{k}}^{\\pi^{k}}(\\tau)-\\mathbb{P}_{\\theta^{*}}^{\\pi^{k}}(\\tau))\\widehat{b}^{k}(\\tau)\\le V_{\\theta^{*}}^{\\hat{b}^{k}}(\\pi^{k})+\\operatorname*{Dr}\\Big(\\mathbb{P}_{\\hat{\\theta}^{k}}^{\\pi^{k}}(\\tau)+\\operatorname*{Dr}\\Big)\\mathbb{P}_{\\theta^{k}}^{\\pi^{k}}(\\tau)\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "wherewerecall that $\\widehat{b}^{k}(\\cdot)\\in[0,1]$ . Hence, we may focus on bounding the value of $\\widehat{b}^{k}$ under thetrue model $\\theta^{*}$ and use the bound on the cumulative total variation estimation error established in Lemma 8. Recall the definition of the bonus term, ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\widehat{b}^{k}(\\tau_{H}):=\\operatorname*{min}\\left\\{\\alpha\\sqrt{\\sum_{h}\\left\\|\\widehat{\\overline{{\\psi}}}^{k}(\\tau_{h})\\right\\|_{(\\widehat{U}_{h}^{k})^{-1}}},1\\right\\},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "which is defined in terms of the estimated prediction features $\\widehat{\\overline{{\\psi}}}$ . Recall also that in Lemma 7 we established a bound on the expectation of the prediction features under the true model, which ", "page_idx": 52}, {"type": "text", "text": "corresponds to $V_{\\theta^{*}}^{\\widehat{b}^{k}}$ . Hence, we proceed to bound $\\sum_{k}V_{\\theta^{*}}^{\\widehat{b}^{k}}(\\pi^{k})$ as follows, ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k}{V_{\\mu}^{\\mathbb{A}_{k}}(\\pi^{k})}}\\\\ &{=\\displaystyle\\sum_{k}{\\mathbb{E}_{\\tau_{\\mu}\\sim\\mathbb{R}_{+}^{n}}\\left[\\hat{b}^{k}(\\tau_{H})\\right]}}\\\\ &{=\\displaystyle\\sum_{k}{\\mathbb{E}_{\\tau_{\\mu}\\sim\\mathbb{R}_{+}^{n}}\\left[\\operatorname*{min}\\left\\{\\alpha\\sqrt{\\sum_{h}\\left\\|\\hat{\\mathcal{V}}^{k}(\\tau_{h})\\right\\|^{2}},1\\right\\}\\right]}}\\\\ &{\\displaystyle\\overset{(a)}{\\leq}\\displaystyle\\sum_{k=1}^{K}\\operatorname*{min}\\left\\{\\alpha\\left(1+\\frac{2\\operatorname*{max}_{\\leq4}\\lvert\\mathbb{X}_{\\leq}|Q_{4}\\sqrt{\\tau_{P}\\beta}\\rangle}{\\sqrt{\\lambda}}\\right)\\sum_{k=1}^{H-1}\\mathbb{E}_{\\tau_{\\mu}\\sim\\mathbb{R}_{+}^{n}}\\left[\\left\\|\\vec{\\psi}^{*}(\\tau_{h})\\right\\|_{(U_{k}^{k})^{-1}}\\right]+\\sum_{k=1}^{K}\\frac{\\alpha H Q_{4}}{\\sqrt{\\lambda}}\\mathbb{D}_{\\mathfrak{V}}}\\\\ &{\\displaystyle\\overset{(b)}{\\leq}\\displaystyle\\sum_{k=1}^{K}\\operatorname*{min}\\left\\{\\alpha\\left(1+\\frac{2\\operatorname*{max}_{\\leq4}\\lvert\\mathbb{X}_{\\leq}|Q_{4}\\sqrt{\\tau_{P}\\beta}\\rangle}{\\sqrt{\\lambda}}\\right)\\sum_{h=1}^{K-1}\\mathbb{E}_{\\tau_{\\mu}\\sim\\mathbb{R}_{+}^{n}}\\left[\\left\\|\\vec{\\psi}^{*}(\\tau_{h})\\right\\|_{(U_{k}^{k})^{-1}}\\right],1\\right\\}+\\sum_{k=1}^{K}\\frac{\\alpha H Q_{4}}{\\sqrt{\\lambda}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where step (a) is by Lemma 7 and step (b) is since $\\operatorname*{min}(a+b,c)\\leq\\operatorname*{min}(a,c)+b$ when $a,b,c$ are non-negative. ", "page_idx": 53}, {"type": "text", "text": "Next, we bound the term $I_{1}$ . Recall the definition of $\\begin{array}{r}{U_{h}^{k}:=\\lambda I+\\sum_{\\tau_{h}\\in\\mathcal{D}_{h}^{k}}\\overline{{\\psi}}^{*}(\\tau_{h})\\overline{{\\psi}}^{*}(\\tau_{h})^{\\top}}\\end{array}$ . Also, note that the process ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\left(\\mathbb{E}_{\\tau_{h}\\sim\\mathbb{P}_{\\theta^{*}}^{\\pi^{k}}}\\left[\\left\\lVert\\overline{{\\psi}}^{*}(\\tau_{h})\\right\\rVert_{(U_{h}^{k})^{-1}}\\right]-\\left\\lVert\\overline{{\\psi}}^{*}(\\tau_{h}^{k+1,h+1})\\right\\rVert_{(\\widehat{U}_{h}^{k})^{-1}}\\right)_{k=1}^{K}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "is a martingale. Hence, by the Azuma-Hoeffding inequality, we have that with probability at least $1\\!-\\!\\delta$ ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\Lambda}_{1}\\leq\\sqrt{2K\\log(2/\\delta)}+\\displaystyle\\sum_{k=1}^{K}\\operatorname*{min}\\left\\lbrace\\alpha\\left(1+\\frac{2\\operatorname*{max}_{s\\in A}\\lvert\\mathbb{X}_{s}\\rvert Q_{A}\\sqrt{7r\\beta}}{\\sqrt{\\lambda}}\\right)\\displaystyle\\sum_{h=0}^{H-1}\\left\\Vert\\overline{{\\psi}}^{*}(\\tau_{h}^{k+1,h+1})\\right\\Vert_{(U_{h}^{k})^{-1}},1\\right\\rbrace}\\\\ &{\\quad\\lesssim\\sqrt{2K\\log(2/\\delta)}+\\alpha\\left(1+\\frac{2\\operatorname*{max}_{s\\in A}\\lvert\\mathbb{X}_{s}\\rvert Q_{A}\\sqrt{7r\\beta}}{\\sqrt{\\lambda}}\\right)H\\sqrt{r K\\log(1+K/\\lambda)}}\\\\ &{\\quad\\lesssim\\alpha\\left(1+\\frac{\\operatorname*{max}_{s\\in A}\\lvert\\mathbb{X}_{s}\\rvert Q_{A}\\sqrt{7r\\beta}}{\\sqrt{\\lambda}}\\right)H\\sqrt{r K\\log(1+K/\\lambda)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where the second line is by the Elliptical potential lemma ([35, Lemma 14]; see also [96-98]). ", "page_idx": 53}, {"type": "text", "text": "We now return to bounding k=IV(\u03c0k). For convenience we define \u03b2o := max{log(1 + $K/\\lambda),\\log(1+d Q_{A}K/\\gamma)\\}$ and we choose $\\lambda$ as follows, ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\lambda=\\frac{\\gamma\\operatorname*{max}_{s\\in A}{|\\mathbb{X}_{s}|}^{2}\\,Q_{A}\\beta\\operatorname*{max}\\{\\sqrt{r},Q_{A}\\sqrt{H}/\\gamma\\}}{\\sqrt{d H}}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "We have, ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left(\\frac{\\mathbf{S}}{\\mathbf{S}}\\right)=\\frac{\\sum_{i=1}^{K}\\rho_{i}(x^{*})}{\\rho_{i}(x^{*})}+\\frac{\\mathbf{S}}{\\sum_{j=1}^{K}\\rho_{j}}(\\frac{\\rho_{j}(x^{*})}{\\rho_{j}(x^{*})})}\\\\ &{\\le t_{1}+(1+\\frac{\\rho_{0}(\\mathbf{S}_{\\theta}^{(2)})}{\\rho_{i}(x^{*})})\\sum_{i=1}^{K}\\rho_{i}(x^{*})}\\\\ &{\\overset{(a)}{\\underset{\\mathrm{\\tiny{d,max}}}{=}}\\left(1+\\frac{\\mathbf{S}\\mathbf{S}_{\\theta}^{(2)}}{\\rho_{i}(x^{*})}\\right)\\frac{t_{1}^{2}\\rho_{i}(x^{*})}{\\rho_{i}(x^{*})}\\,\\bigg)\\times\\mathbb{E}\\left(\\frac{\\mathbf{S}_{\\theta}^{(2)}}{\\rho_{i}(x^{*})}\\mathbf{S}_{\\theta}^{(2)}\\mathbf{S}_{\\theta}^{(2)}\\times\\sqrt{\\pi K_{S}}}\\\\ &{=\\left(1+\\frac{\\mathbf{S}_{\\theta}^{(2)}(\\mathbf{S}_{\\theta}^{2})(\\mathbf{S}_{\\theta}^{2})}{\\rho_{i}(x^{*})}+\\frac{\\mathbf{S}_{\\theta}^{(2)}(\\mathbf{S}_{\\theta}^{2})(\\mathbf{S}_{\\theta}^{2})}{\\rho_{i}(x^{*})}\\right)\\,\\bigg)\\times\\mathbb{V}(\\widehat{\\mathbf{S}}_{\\theta})=\\mathbb{E}\\left(\\frac{\\mathbf{S}_{\\theta}^{(2)}}{\\rho_{i}(x^{*})}\\right)}\\\\ &{\\overset{(b)}{\\underset{\\mathrm{\\tiny{d,max}}}{=}}\\left(\\frac{\\rho_{0}(\\mathbf{S}_{\\theta}^{(2)})}{\\rho_{i}(x^{*})}+\\frac{(\\mathbf{S}_{\\theta}^{(2)}\\mathbf{S}_{\\theta}^{(2)})(\\mathbf{S}_{\\theta}^{2})}{\\rho_{i}(x^{*})}\\right)\\bigg(1+\\frac{\\mathbf{S}_{\\theta}^{(2)}(\\mathbf{S}_{\\theta}^{2})(\\mathbf{S}_{\\theta \n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where step (a) is by Lemma 8 and the bound on $I_{1}$ established above, step (b) uses the definition of $\\alpha$ and the fact that \u03b1 < QAvHax $\\begin{array}{r}{\\alpha\\lesssim\\frac{Q_{A}\\sqrt{H d\\lambda}}{\\gamma^{2}}+\\frac{\\operatorname*{max}_{s\\in\\mathcal{A}}|\\mathbb{X}_{s}|Q_{A}\\sqrt{\\beta}}{\\gamma}}\\end{array}$ + maxeA|xlQAVB, and step (c)isby plugging in the choice of \u5165. ", "page_idx": 54}, {"type": "text", "text": "J.5 Proof of Theorem 4 ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Theorem (Restatement of Theorem 4). Suppose Assumption $I$ holds. Let $\\begin{array}{r}{p_{\\mathrm{min}}=O\\left(\\frac{\\delta}{K H\\prod_{h=1}^{H}\\left|\\mathbb{X}_{h}\\right|}\\right)}\\end{array}$ $\\begin{array}{r}{\\lambda=\\frac{\\gamma\\operatorname*{max}_{s\\in A}|\\mathbb{X}_{s}|^{2}Q_{A}\\beta\\operatorname*{max}\\left\\{\\sqrt{r},Q_{A}\\sqrt{H}/\\gamma\\right\\}}{\\sqrt{d H}},\\,\\alpha=O\\left(\\frac{Q_{A}\\sqrt{H d}}{\\gamma^{2}}\\sqrt{\\lambda}+\\frac{\\operatorname*{max}_{s\\in A}|\\mathbb{X}_{s}|Q_{A}\\sqrt{\\beta}}{\\gamma}\\right)\\!,}\\end{array}$ and let $\\beta=$ $O(\\log\\left|\\overline{{\\Theta}}_{\\varepsilon}\\right|)$ where $\\begin{array}{r}{\\varepsilon=O(\\frac{p_{\\mathrm{min}}}{K H})}\\end{array}$ .Then, with probaility a least $1-\\delta$ Algorithm $I$ returns a model $\\theta^{\\epsilon}$ andapolicy that satisfy ", "page_idx": 54}, {"type": "equation", "text": "$$\nV_{\\theta^{\\epsilon}}^{R}(\\pi^{*})-V_{\\theta^{\\epsilon}}^{R}(\\pi)\\leq\\varepsilon,\\,a n d\\,\\forall\\pi,\\,\\mathsf{D}_{\\mathsf{T V}}\\left(\\mathbb{P}_{\\theta^{\\epsilon}}^{\\pi}(\\tau_{H}),\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau_{H})\\right)\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "In addition, the algorithm terminates with $a$ sample complexity of, ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\tilde{O}\\left(\\left(r+\\frac{Q_{A}^{2}H}{\\gamma^{2}}\\right)\\frac{r d H^{3}\\operatorname*{max}_{s\\in\\mathcal{A}}\\left|\\mathbb{X}_{s}\\right|^{2}Q_{A}^{4}\\beta}{\\gamma^{4}\\epsilon^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Proof. By Propositions 6 to 8, the event $\\mathcal{E}$ Occurs with high probability, $\\mathbb{P}\\left[\\mathcal{E}\\right]\\geq1-3\\delta$ .Suppose $\\mathcal{E}$ holds. Then, by the upper confidence bound established in Corollary 4, if Algorithm 1 terminates, then the following must hold, ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\forall\\pi,\\;\\mathsf{D}_{\\mathsf{T V}}\\left(\\mathbb{P}_{\\theta^{\\epsilon}}^{\\pi}(\\tau_{H}),\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau_{H})\\right)=2\\operatorname*{max}_{R}\\left|V_{\\theta^{\\epsilon}}^{R}(\\pi)-V_{\\theta^{*}}^{R}(\\pi)\\right|\\leq V_{\\theta^{\\epsilon}}^{\\widehat b^{\\epsilon}}(\\pi)\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where the maximization is over reward functions $R:\\mathbb{H}_{H}\\to[0,1]$ . The last inequality is simply the termination condition of Algorithm 1. ", "page_idx": 54}, {"type": "text", "text": "Now, the difference between the optimal value and the value of $\\pi$ (the policy returned by the algorithm) can be bounded as follows, ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{\\theta^{*}}^{R}(\\pi^{*})-V_{\\theta^{*}}^{R}(\\pi)=V_{\\theta^{*}}^{R}(\\pi^{*})-V_{\\theta^{*}}^{R}(\\pi^{*})+V_{\\theta^{*}}^{R}(\\pi^{*})-V_{\\theta^{*}}^{R}(\\pi)+V_{\\theta^{*}}^{R}(\\pi)-V_{\\theta^{*}}^{R}(\\pi)}\\\\ &{\\phantom{V_{\\theta^{*}}^{R}(\\pi^{*})-V_{\\theta^{*}}^{R}(\\pi)}\\leq2\\operatorname*{max}V_{\\theta^{*}}^{\\hat{b}^{\\epsilon}}(\\pi)\\leq\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where the inequality follows from the fact that $\\pi=\\arg\\operatorname*{max}_{\\pi}V_{\\theta^{\\epsilon}}^{R}(\\pi)$ and by Corollary 4. Recall that by Lemma 9, we have, ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}V_{\\widehat{\\theta}^{k},\\bar{b}^{k}}^{\\pi^{k}}\\lesssim\\left(\\sqrt{r}+\\frac{Q_{A}\\sqrt{H}}{\\gamma}\\right)\\frac{\\operatorname*{max}_{s\\in A}|\\mathbb{X}_{s}|Q_{A}^{2}H\\sqrt{r d H K\\beta\\beta_{0}}}{\\gamma^{2}}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "By the pigeon-hole principle and the termination condition of Algorithm 1, the algorithm must terminatewithin ", "page_idx": 55}, {"type": "equation", "text": "$$\nK=\\tilde{O}\\left(\\left(r+\\frac{Q_{A}^{2}H}{\\gamma^{2}}\\right)\\frac{r d H^{2}Q_{A}^{4}\\operatorname*{max}_{s\\in A}{|\\mathbb{X}_{s}|}^{2}\\,\\beta}{\\gamma^{4}\\epsilon^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "episodes. Since each episode contains $H$ iterations, this implies a sample complexity of ", "page_idx": 55}, {"type": "equation", "text": "$$\nK=\\tilde{O}\\left(\\left(r+\\frac{Q_{A}^{2}H}{\\gamma^{2}}\\right)\\frac{r d H^{3}Q_{A}^{4}\\operatorname*{max}_{s\\in\\mathcal{A}}|\\mathbb{X}_{s}|^{2}\\,\\beta}{\\gamma^{4}\\epsilon^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "This concludes the proof of Theorem 4. ", "page_idx": 55}, {"type": "text", "text": "K Proof of Theorem 5: UCB Algorithm for Generalized PSRs (Game Setting) ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Theorem (Restatement of Theorem 5). Suppose Assumption 1 holds. Let $\\begin{array}{r}{p_{\\mathrm{min}}=O\\left(\\frac{\\delta}{K H\\prod_{h=1}^{H}\\left|\\mathbb{X}_{h}\\right|}\\right)}\\end{array}$ $\\begin{array}{r}{\\lambda=\\frac{\\gamma\\operatorname*{max}_{s\\in A}|\\mathbb{X}_{s}|^{2}Q_{A}\\beta\\operatorname*{max}\\left\\{\\sqrt{r},Q_{A}\\sqrt{H}/\\gamma\\right\\}}{\\sqrt{d H}},\\,\\alpha=O\\left(\\frac{Q_{A}\\sqrt{H d}}{\\gamma^{2}}\\sqrt{\\lambda}+\\frac{\\operatorname*{max}_{s\\in A}|\\mathbb{X}_{s}|Q_{A}\\sqrt{\\beta}}{\\gamma}\\right)\\!,}\\end{array}$ and let $\\beta=$ $O(\\log\\left|\\overline{{\\Theta}}_{\\varepsilon}\\right|)$ where $\\begin{array}{r}{\\varepsilon=O(\\frac{p_{\\mathrm{min}}}{K H})}\\end{array}$ .Then, with probability at least $1-\\delta$ Algorithm 2 returns amodel $\\theta^{\\epsilon}$ and a policy $\\pi$ which is an $\\varepsilon$ -approximate equilibrium(eitherNE or $C C E$ ). That is, ", "page_idx": 55}, {"type": "equation", "text": "$$\nV_{\\theta^{*}}^{i}(\\pi)\\geq V_{\\theta^{*}}^{i,\\dag}(\\pi^{-i})-\\varepsilon,\\,\\forall i\\in[N].\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "In addition, the algorithm terminates with a sample complexity of, ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\tilde{O}\\left(\\left(r+\\frac{Q_{A}^{2}H}{\\gamma^{2}}\\right)\\frac{r d H^{3}\\operatorname*{max}_{s\\in\\mathcal{A}}\\left|\\mathbb{X}_{s}\\right|^{2}Q_{A}^{4}\\beta}{\\gamma^{4}\\epsilon^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Proof. Recall that the model-estimation portion of Algorithm 2 is identical to Algorithm 1. Hence, by Theorem 4, the returned estimated model $\\theta^{\\varepsilon}$ satisfies, ", "page_idx": 55}, {"type": "equation", "text": "$$\n{\\sf D}_{\\mathrm{TV}}\\left(\\mathbb{P}_{\\theta^{\\varepsilon}}^{\\pi}(\\tau_{H}),\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau_{H})\\right)\\leq\\varepsilon/2,\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "for any collection of policies $\\pmb{\\pi}=(\\pi^{i}:i\\in[N])$ .This implies that $V_{\\theta^{*}}^{i}(\\pi)\\geq V_{\\theta^{\\varepsilon}}^{i}(\\pi)-\\varepsilon/2$ for all $i\\in[N]$ ", "page_idx": 55}, {"type": "text", "text": "Let $\\Gamma^{i}=\\Gamma_{\\mathrm{ind}}^{i}$ in the case of running the algorithm to find a Nash equilibrium and $\\Gamma^{i}=\\Gamma_{\\mathrm{cor}}^{i}$ the case of a coarse correlated equilibrium. Recallthat the collection of policies $\\pi=(\\pi^{1},\\cdot\\cdot..,\\pi^{N})$ returned by the algorithm are an equilibrium under $\\theta^{\\varepsilon}$ . That is, for all $i\\in[N]$ ", "page_idx": 55}, {"type": "equation", "text": "$$\nV_{\\theta^{\\varepsilon}}^{i}(\\pi)=\\operatorname*{max}_{\\tilde{\\pi}^{i}\\in\\Gamma_{\\mathrm{ind}}^{i}}V_{\\theta^{\\varepsilon}}^{i}(\\tilde{\\pi}^{i},\\pi^{-i})=:V_{\\theta^{\\varepsilon}}^{i,\\dagger}(\\pi^{-i}).\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Moreover, note that, ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|V_{\\theta^{\\varepsilon}}^{i,\\dagger}(\\pi^{-i})-V_{\\theta^{*}}^{i,\\dagger}(\\pi^{-i})\\right|=\\left|\\displaystyle\\operatorname*{max}_{\\tilde{\\pi}^{i}}V_{\\theta^{\\varepsilon}}^{i}(\\tilde{\\pi}^{i},\\pi^{-i})-\\displaystyle\\operatorname*{max}_{\\tilde{\\pi}^{i}}V_{\\theta^{*}}^{i}(\\tilde{\\pi}^{i},\\pi^{-i})\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\operatorname*{max}_{\\tilde{\\pi}^{i}}\\left|V_{\\theta^{\\varepsilon}}^{i}(\\tilde{\\pi}^{i},\\pi^{-i})-V_{\\theta^{*}}^{i}(\\tilde{\\pi}^{i},\\pi^{-i})\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\varepsilon/2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where the final inequality is since ${\\sf D}_{\\mathrm{TV}}\\left(\\mathbb{P}_{\\theta^{\\varepsilon}}^{\\pi}(\\tau_{H}),\\mathbb{P}_{\\theta^{*}}^{\\pi}(\\tau_{H})\\right)\\leq\\varepsilon/2$ for any $\\pi$ .Thus, $V_{\\theta^{\\varepsilon}}^{i,\\dagger}(\\pi^{-i})\\geq$ $V_{\\theta^{\\ast}}^{i,\\dagger}(\\pi^{-i})-\\varepsilon/2$ ", "page_idx": 56}, {"type": "text", "text": "Putting this together, we have, ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{V_{\\theta^{*}}^{i}(\\pi)\\geq V_{\\theta^{\\varepsilon}}^{i}(\\pi)-\\varepsilon/2}}\\\\ {{\\ \\ \\ \\ \\ \\ \\ \\ \\ =V_{\\theta^{\\varepsilon}}^{i,\\dagger}(\\pi^{-i})-\\varepsilon/2}}\\\\ {{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\geq V_{\\theta^{*}}^{i,\\dagger}(\\pi^{-i})-\\varepsilon.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Hence, $\\pi$ is an $\\varepsilon$ -approximate equilibrium (either NE or CCE) ", "page_idx": 56}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 57}, {"type": "text", "text": "Justification: The abstract and introduction summarize the main message and technical results of the paper. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 57}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Justification: This is discussed in the conclusion section. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \\*Limitations\u201d section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 57}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 57}, {"type": "text", "text": "Justification: The paper provides a complete, self-contained description of all technical assumptions, and a full proof (in the appendix) for all theoretical results. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 58}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification: N/A ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 58}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 59}, {"type": "text", "text": "Justification: There is no data or code. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips .cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 59}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 59}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 59}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 59}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 59}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 59}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 59}, {"type": "text", "text": "", "page_idx": 60}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 60}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 60}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 60}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: This work conforms with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 60}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 60}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 60}, {"type": "text", "text": "Justification: This work is theoretical in nature and does not have immediately-foreseeable negative or positive societal impact. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 60}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 61}, {"type": "text", "text": "\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 61}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 61}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 61}, {"type": "text", "text": "Justification: This paper does not release data or models. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 61}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 61}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 61}, {"type": "text", "text": "Justification: We do not use any existing assets. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 61}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 61}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 62}, {"type": "text", "text": "Justification: We do not introduce new assets. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 62}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 62}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 62}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 62}]