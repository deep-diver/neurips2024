[{"figure_path": "xzCuBjHQbS/figures/figures_2_1.jpg", "caption": "Figure 1: The stochastic Taylor approximation naturally contains a trust bound in contrast to the classical one. Here J is a Gaussian random function (with covariance as in Equation (11), with length scale s = 2 and variance \u03c3\u00b2 = 1). The ribbon represents two conditional standard deviations around the conditional expectation.", "description": "This figure compares three different approximations of a Gaussian random function J(w) around a point w=0. The blue line represents the actual function, the dashed orange line represents the expectation of the function given its value and gradient at w=0, and the solid green line represents a classical Taylor expansion approximation.  The shaded area shows the range of possible values of J(w) given the value and gradient at w=0, showcasing that this stochastic Taylor approximation naturally includes a form of trust bound.", "section": "The random function descent algorithm"}, {"figure_path": "xzCuBjHQbS/figures/figures_5_1.jpg", "caption": "Figure 2: RFD step sizes as a function of \u0398 = ||\u25bdJ(w)||/(\u03bc \u2212 J(w)) assuming scale s = 1 (cf. Table 1). A-RFD (Definition 5.1) is plotted as dashed lines. A-RFD of the rational quadratic coincides with A-RFD of the squared exponential covariance.", "description": "This figure shows the RFD step sizes for four different covariance models as a function of the gradient cost quotient, \u0398.  It demonstrates how the step size changes during the optimization process, starting with large steps at the beginning and converging to smaller, more constant steps as optimization proceeds. The dashed lines represent the asymptotic RFD (A-RFD), showing the behavior when the optimization is nearly complete. Note that for the rational quadratic covariance model, A-RFD coincides with the A-RFD of the squared exponential covariance model.", "section": "The RFD step size schedule"}, {"figure_path": "xzCuBjHQbS/figures/figures_9_1.jpg", "caption": "Figure 3: Training on the MNIST dataset (batch size 1024). Ribbons describe the range between the 10% and 90% quantile of 20 repeated experiments while lines represent their mean. SE stands for the squared exponential (11) and RQ for the rational quadratic (13) covariance. The validation loss uses the test data set, which provides a small advantage to Adam and SGD, as we also use it for tuning.", "description": "This figure compares the performance of RFD with different covariance functions (squared exponential and rational quadratic) against Adam and SGD optimizers on the MNIST dataset.  It shows the validation loss, learning rate, and step size over epochs and steps. The ribbons indicate the variability across 20 repeated experiments, highlighting the robustness and stability of RFD. The results show that RFD, particularly with the squared exponential covariance, exhibits performance comparable to or better than the tuned Adam and SGD optimizers.", "section": "7 MNIST case study"}, {"figure_path": "xzCuBjHQbS/figures/figures_14_1.jpg", "caption": "Figure 3: Training on the MNIST dataset (batch size 1024). Ribbons describe the range between the 10% and 90% quantile of 20 repeated experiments while lines represent their mean. SE stands for the squared exponential (11) and RQ for the rational quadratic (13) covariance. The validation loss uses the test data set, which provides a small advantage to Adam and SGD, as we also use it for tuning.", "description": "This figure shows the training results on the MNIST dataset using different optimization algorithms, namely RFD (with squared exponential and rational quadratic covariance), S-RFD, A-RFD, SGD, and Adam.  The results are presented with error bars showing the 10th and 90th percentiles across 20 repeated experiments. The figure illustrates the validation loss and the learning rate/step size for each algorithm over epochs and steps.  The key takeaway is that RFD with proper covariance models and step size scheduling outperforms other methods, even showing a form of gradual learning rate warmup.", "section": "7 MNIST case study"}, {"figure_path": "xzCuBjHQbS/figures/figures_15_1.jpg", "caption": "Figure 3: Training on the MNIST dataset (batch size 1024). Ribbons describe the range between the 10% and 90% quantile of 20 repeated experiments while lines represent their mean. SE stands for the squared exponential (11) and RQ for the rational quadratic (13) covariance. The validation loss uses the test data set, which provides a small advantage to Adam and SGD, as we also use it for tuning.", "description": "This figure shows the training results of different optimization algorithms on the MNIST dataset.  The algorithms compared are RFD (with squared exponential and rational quadratic covariance), S-RFD (stochastic RFD), Adam, and SGD.  The results are visualized using ribbons to show the 10th and 90th percentiles of the results from 20 repeated experiments, giving a sense of the variability.  The mean performance is plotted as a line within the ribbon.  The figure includes plots of the training loss, validation loss, learning rates, and step sizes over time (epochs and steps). The authors note that the validation loss uses the test data set which gives Adam and SGD a slight advantage since the test set is also used for hyperparameter tuning.", "section": "7 MNIST case study"}, {"figure_path": "xzCuBjHQbS/figures/figures_16_1.jpg", "caption": "Figure 3: Training on the MNIST dataset (batch size 1024). Ribbons describe the range between the 10% and 90% quantile of 20 repeated experiments while lines represent their mean. SE stands for the squared exponential (11) and RQ for the rational quadratic (13) covariance. The validation loss uses the test data set, which provides a small advantage to Adam and SGD, as we also use it for tuning.", "description": "This figure compares the performance of RFD (with squared exponential and rational quadratic covariance) against Adam and SGD optimizers on the MNIST dataset.  It shows validation loss, final validation loss, learning rate, and step size across epochs and steps.  Ribbons represent the variability (10th-90th percentile) across 20 repeated experiments.  The results indicate RFD's competitive performance, especially considering its automatic step size selection compared to the tuned hyperparameters of Adam and SGD.", "section": "7 MNIST case study"}, {"figure_path": "xzCuBjHQbS/figures/figures_17_1.jpg", "caption": "Figure 3: Training on the MNIST dataset (batch size 1024). Ribbons describe the range between the 10% and 90% quantile of 20 repeated experiments while lines represent their mean. SE stands for the squared exponential (11) and RQ for the rational quadratic (13) covariance. The validation loss uses the test data set, which provides a small advantage to Adam and SGD, as we also use it for tuning.", "description": "This figure displays the results of training a neural network on the MNIST dataset using different optimization methods: RFD (with squared exponential and rational quadratic covariance), S-RFD, A-RFD, SGD, and Adam.  The performance is evaluated using validation loss and learning rate/step size across epochs and steps. The shaded areas (ribbons) represent the variability in the results across multiple runs.  The results show that RFD exhibits competitive performance with proper step size management and is superior to A-RFD, highlighting the benefits of the learning rate warmup heuristic.", "section": "7 MNIST case study"}]