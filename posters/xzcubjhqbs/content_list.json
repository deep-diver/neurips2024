[{"type": "text", "text": "Random Function Descent ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Felix Benning Leif D\u00f6ring University of Mannheim University of Mannheim felix.benning@uni-mannheim.de leif.doering@uni-mannheim.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Classical worst-case optimization theory neither explains the success of optimization in machine learning, nor does it help with step size selection. In this paper we demonstrate the viability and advantages of replacing the classical \u2018convex function\u2019 framework with a \u2018random function\u2019 framework. With complexity $O(n^{3}d^{3})$ , where $n$ is the number of steps and $d$ the number of dimensions, Bayesian optimization with gradients has not been viable in large dimension so far. By bridging the gap between Bayesian optimization (i.e. random function optimization theory) and classical optimization we establish viability. Specifically, we use a \u2018stochastic Taylor approximation\u2019 to rediscover gradient descent, which is scalable in high dimension due to $O(n d)$ complexity. This rediscovery yields a specific step size schedule we call Random Function Descent (RFD). The advantage of this random function framework is that RFD is scale invariant and that it provides a theoretical foundation for common step size heuristics such as gradient clipping and gradual learning rate warmup. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Cost function minimization is one of the most fundamental mathematical problems in machine learning. Gradient-based methods, popular for this task, require a step size, typically chosen using established heuristics. This article aims to deepen the theoretical understanding of these heuristics and proposes a new algorithm based on this insight. ", "page_idx": 0}, {"type": "text", "text": "Classical optimization theory uses $L$ -smoothness, which limits the rate of change of the gradient by $L$ , to provide some convergence guarantees for learning rates smaller than $1/L$ [e.g. 38]. As this theory is based on an upper bound (the worst case), the learning rate $1/L$ is naturally much more conservative than necessary on average. Even if $L$ was known, this learning rate would therefore be impractical. Since line search algorithms typically require access to full cost function evaluations, the field of machine learning (ML) therefore relies heavily on step size heuristics [e.g. 48, 49, 42, 20]. To investigate these heuristics, we introduce new ideas based on a \u2018random function\u2019 perspective. ", "page_idx": 0}, {"type": "text", "text": "While automatic step size selection in the convex function framework is possible [11], convexity is generally only satisfied asymptotically and locally. So the understanding of the initial stages of optimization, which includes the warmup heuristic [20], greatly benefits from a framework which also admits non-convex functions. This objective is achieved by the \u2018random function\u2019 framework we investigate. ", "page_idx": 0}, {"type": "text", "text": "Many successful algorithms in computer science are significantly slower in the worst case than in the average case based on a probabilistic framework (e.g. Quicksort [23] or the simplex algorithm [e.g. 6]). On random quadratic functions the average case behavior of first order optimizers is already being investigated by the ML community [e.g. 58, 43, 33, 12, 9, 40, 41]. Interested in the landscape of high dimensional random functions as a model for \u2018spin glasses\u2018, the physics community independently started studying the average case of optimization as well [e.g. 4, 15, 37, 51, 24], albeit not geared for ML algorithms. ", "page_idx": 0}, {"type": "text", "text": "Average case analysis fundamentally requires a prior distribution over possible cost functions. The evaluations seen so far then result in a posterior over the cost of other parameter inputs. Using this posterior for optimization is called \u201cBayesian optimization\u201d (BO) [e.g. 32, 47, 16, 2], which is best known in the context of low dimensional optimization (e.g. hyperparameter tuning) in the ML community. BO is treated like a zero order method for low dimensional problems due to the ${\\mathcal{O}}(n^{3})$ complexity for the covariance matrix inversion of the $n$ evaluations seen so far, which increases to $\\mathcal{O}(n^{3}d^{3})$ when gradient information is included [e.g. 35, 55], where $d$ is the input dimension of our cost function. This limits classic BO to relatively small dimensions even under sparsity considerations [e.g. 45, 39]. ", "page_idx": 1}, {"type": "text", "text": "While the BO algorithms developed in the \u2018random function framework\u2019 might not have been viable in high dimension so far, due to their computational complexity, this framework is already used to explain the high relative frequency of saddle points in high dimension [10] and to explain the highly predictable progress optimizers make on high dimensional cost functions [5]. ", "page_idx": 1}, {"type": "text", "text": "In this work we bridge the gap between BO and (computationally viable) gradient based methods, derived from the first Taylor approximation, with the introduction of a stochastic Taylor approximation based on a forgetful BO posterior. The optimization method \u201cRandom Function Descent\u201d (RFD), resulting from the minimization of this stochastic Taylor approximation, coincides with a specific form of gradient descent which establishes its viability in high dimension. The advantages of its BO heritage are scale invariance and an explicit step size schedule, which illuminates the inner workings of step size heuristics such as gradient clipping [42] and gradual learning rate warmup [20]. ", "page_idx": 1}, {"type": "text", "text": "Our contributions and outline The main goal of this paper is to demonstrate the viability and advantages of replacing the classical \u201cconvex function\u201d framework with a \u201crandom function\u201d framework. Theorem 4.2 is the main theoretical result establishing viability (computatability and scalable complexity) for a given covariance model. Section 6 is concerned with practical estimation of the covariance model and viability is demonstrated with a practical example in the MNIST case study (Section 7). The advantages of this approach are scale invariance (Advantage 2.3) and an explicit step size schedule, which does not require expensive tuning and explains existing ML heuristics such as warmup (cf. Section 5.2). This explanation of the initial stage of optimization could never be delivered by the convex framework, because the convexity assumption is not fulfilled initially so it can at best explain asymptotic behavior. ", "page_idx": 1}, {"type": "text", "text": "Sec. 2 We motivate a stochastic Taylor approximation and RFD and prove its scale-invariance. ", "page_idx": 1}, {"type": "text", "text": "Sec. 3 We briefly motivate and discuss the common distributional assumptions in BO. ", "page_idx": 1}, {"type": "text", "text": "Sec. 4 We establish the connection between RFD and gradient descent. ", "page_idx": 1}, {"type": "text", "text": "Sec. 5 We investigate the step size schedule suggested by RFD. In particular we ", "page_idx": 1}, {"type": "text", "text": "0. calculate explicit formulas for the step size schedules resulting from common covariance models (Table 1, Sec. C),   \n1. analyze the general asymptotic behavior (Sec. 5.1),   \n2. discuss how RFD explains gradient clipping and learning rate warmup (Sec. 5.2), ", "page_idx": 1}, {"type": "text", "text": "Sec. 6 We develop a non-parametric variance estimation method, which is robust with respect to the choice of covariance kernel. Finally, we present an extension of RFD to mini-batch losses. ", "page_idx": 1}, {"type": "text", "text": "Sec. 7 We conduct a case study on the MNIST dataset.   \nSec. 8 We discuss extensions (see also Sec. E) and limitations. ", "page_idx": 1}, {"type": "text", "text": "2 The random function descent algorithm ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The classic derivation of gradient descent [e.g. 38, p. 29], adds an $L$ -smoothness based trust bound to the first Taylor approximation, $T[J(\\theta)\\mid J(w),\\nabla J(w)]$ , of the cost function $J$ around $w$ resulting in the gradient step ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{w-\\frac{1}{L}\\nabla J(w)=\\underset{\\theta}{\\operatorname{argmin}}\\,T[J(\\theta)\\mid J(w),\\nabla J(w)]+\\frac{L}{2}\\|\\theta-w\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Our unusual notation for the Taylor approximation $T[J(\\theta)\\mid J(w),\\nabla J(w)]$ is meant to highlight the connection to the stochastic Taylor approximation we define below. ", "page_idx": 1}, {"type": "text", "text": "Definition 2.1 (Stochastic Taylor approximation). We define the first order stochastic Taylor approximation of a random (cost) function $\\boldsymbol{\\dot{\\mathrm{~I~}}}_{\\mathbf{J}}$ around $w$ by the conditional expectation ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{J}(\\theta)\\mid\\mathbf{J}(w),\\nabla\\mathbf{J}(w)].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This is the best $L^{2}$ approximation [30, Cor. 8.17] of $\\mathbf{J}(\\theta)$ provided first order knowledge of $\\mathbf{J}$ at $w$ . ", "page_idx": 2}, {"type": "text", "text": "We call this the \u2018stochastic Taylor approximation\u2019 because this approximation only makes use of derivatives in a single point. While the standard Taylor approximation is a polynomial approximation, the \u2018stochastic Taylor approximation\u2019 is the best approximation in an $L^{2}$ sense and already mean-reverting by itself, i.e. it naturally incorporates covariance-based trust (cf. Figure 1). While $L$ -smoothness-based trust guarantees that the gradient still points in the direction we are going (for learning rates smaller $1/L)$ , covariance based trust tells us whether the derivative is still negative on average. Minimizing the stochastic Taylor approximation is therefore optimized for the average case. Since convergence proofs for gradient descent typically rely on an improvement guarantee, proving convergence is significantly harder in the average case and we answer this question only partially in Corollary 5.3. ", "page_idx": 2}, {"type": "image", "img_path": "xzCuBjHQbS/tmp/c0ffcf38fcbe47f09e42f34a697620dfe9092e1592c3fe592f2dbce7ecfce571.jpg", "img_caption": ["Figure 1: The stochastic Taylor approximation naturally contains a trust bound in contrast to the classical one. Here $\\mathbf{J}$ is a Gaussian random function (with covariance as in Equation (11), with length scale $s=2$ and variance $\\sigma^{2}=1$ ). The ribbon represents two conditional standard deviations around the conditional expectation. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Definition 2.2 (Random Function Descent \u2013 RFD). Select $w_{n+1}$ as the minimizer2 of the first order stochastic Taylor approximation ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{w_{n+1}:=\\underset{w}{\\mathrm{argmin}}\\,\\mathbb{E}[\\mathbf{J}(w)\\mid\\mathbf{J}(w_{n}),\\nabla\\mathbf{J}(w_{n})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Properties of RFD Before we make RFD more explicit in Section 4, we discuss some properties which are easier to see in the abstract form. ", "page_idx": 2}, {"type": "text", "text": "First, observe that RFD is greedy and forgetful in the same way gradient descent is greedy and forgetful when derived as the minimizer of the regularized first Taylor approximation, or the Newton method as the minimizer of the second Taylor approximation. This is because the Taylor approximation only uses derivatives from the last point $w_{n}$ (forgetful), and we minimize this approximation (greedy). Since momentum methods retain some information about past gradients, they are not as forgetful. We therefore expect a similar improvement could be made for RFD in the future. ", "page_idx": 2}, {"type": "text", "text": "Second, it is well known that classical gradient descent with exogenous step sizes (and most other first order methods) lack the scale invariance property of the Newton method [e.g. 21, 13]. Scale invariance means that scaling the input parameters $w$ or the cost itself (e.g. by switching from the mean squared error to the sum squared error) does not change the points selected by the optimization method. ", "page_idx": 2}, {"type": "text", "text": "Advantage 2.3 (Scale invariance). RFD is invariant to additive shifts and positive scaling of the cost J. RFD is also invariant with respect to transformations of the parameter input of J by differentiable bijections whose Jacobian is invertible everywhere (e.g. invertible linear maps). ", "page_idx": 2}, {"type": "text", "text": "While invariance to bijections of inputs is much stronger than the affine invariance offered by the Newton method, non-linear bijections will typically break the \u2018isotropy\u2019 assumption of the following section which makes RFD explicit. This invariance should therefore be viewed as an opportunity to look for the bijection of inputs which ensures isotropy (e.g. a whitening transformation). The discussion of geometric anisotropy in Section E.1 is conducive to build an understanding of this. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 A distribution over cost functions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "It is impossible to make average case analysis explicit without a distribution over functions, so we use the canonical distributional assumption of Bayesian optimization [e.g. 16, 55, 44], \u2018isotropic Gaussian random functions\u2019. This assumption was also used in the high dimensional setting by Dauphin et al. [10] to argue that saddle points are much more common than minima in high dimension, which is often cited to explain why second order methods are uncommon in machine learning. ", "page_idx": 3}, {"type": "text", "text": "To motivate isotropy, we note that in average case analysis the uniform distribution is popular, since it weighs all problem instances equally (e.g. all possible permutations in sorting). Isotropy is such a uniformity assumption, which essentially requires $\\mathbf{\\nabla}\\Phi(\\mathbf{J}=J)=\\mathbb{P}(\\mathbf{J}=J\\circ\\phi)^{*}$ \u201c, for all isometries $\\phi$ . In other words, the probability that our cost function is equal to $J$ is equal to the probability that it is equal to a shifted and turned version of $J$ , given by $J\\circ\\phi$ . ", "page_idx": 3}, {"type": "text", "text": "Since the probability of any single realization of a cost function $J$ is zero, the equation we put in quotes is mathematically unsound. The formal definition follows below. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (Isotropy). A random function $\\mathbf{J}$ is called isotropic if its distribution stays the same under isometric transformations of its input, i.e. for any isometry $\\phi$ we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathbf{J}}=\\mathbb{P}_{\\mathbf{J}\\circ\\phi}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "If $\\mathbf{J}$ is Gaussian, isotropy is well known [e.g. 44, 1] to be equivalent to the condition that there exists $\\mu\\in\\mathbb{R}$ and a function $C:\\mathbb{R}\\rightarrow\\mathbb{R}$ such that for all $\\bar{w},\\tilde{w}\\in\\dot{\\mathbb{R}}^{d}$ the expectation and covariance are ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\mathbf{J}(w)]=\\mu,\\qquad\\mathrm{Cov}(\\mathbf{J}(w),\\mathbf{J}(\\tilde{w}))=C\\big(\\frac{\\|w-\\tilde{w}\\|^{2}}{2}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For these isotropic Gaussian random functions we use the notation $\\mathbf{J}\\sim{\\mathcal{N}}({\\boldsymbol{\\mu}},{\\boldsymbol{C}})$ . ", "page_idx": 3}, {"type": "text", "text": "We discuss generalizations to isotropy in Section F and E.1, but for ease of exposition we retain the (stationary) isotropy assumption throughout the main body. Note that the Gaussian assumption can be statistically tested in practice (cf. Figure 4), but it is also straightforward to reproduce our results with the \u201cbest linear unbiased estimator\u201d (BLUE) (Section E.3) in place of the conditional expectation to remove the Gaussian assumption. We finally want to highlight that, in contrast to the uniformity assumption on finite sets, \u2018isotropic Gaussian random functions\u2019 leave us with a family of plausible distributions. It is therefore necessary to estimate $\\mu$ and $C$ , which is the topic of Section 6. ", "page_idx": 3}, {"type": "text", "text": "4 Relation to gradient descent ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "While we were able to define RFD abstractly without any assumptions on the distribution $\\mathbb{P}_{\\mathbf{J}}$ of the random cost $\\mathbf{J}$ , an explicit calculation requires distributional assumptions and we have motivated isotropic Gaussian random functions in Section 3 for this purpose. The assumption of isotropy allows for an explicit version of the stochastic Taylor approximation which then immediately leads to an explicit version of RFD. ", "page_idx": 3}, {"type": "text", "text": "Lemma 4.1 (Explicit first order stochastic Taylor approximation). For $\\mathbf{J}\\sim{\\mathcal{N}}({\\boldsymbol{\\mu}},{\\boldsymbol{C}})$ , the first order stochastic Taylor approximation is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{J}(w-\\mathbf{d})\\mid\\mathbf{J}(w),\\nabla\\mathbf{J}(w)]=\\mu+{\\frac{C\\big({\\frac{\\|\\mathbf{d}\\|^{2}}{2}}\\big)}{C(0)}}(\\mathbf{J}(w)-\\mu)-{\\frac{C^{\\prime}\\big({\\frac{\\|\\mathbf{d}\\|^{2}}{2}}\\big)}{C^{\\prime}(0)}}\\langle\\mathbf{d},\\nabla\\mathbf{J}(w)\\rangle.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The explicit version of RFD follows by fixing the step size $\\eta=\\|\\mathbf{d}\\|$ and optimizing over the direction first. ", "page_idx": 3}, {"type": "text", "text": "Theorem 4.2 (Explicit RFD). Let $\\mathbf{J}\\sim{\\mathcal{N}}({\\boldsymbol{\\mu}},{\\boldsymbol{C}})$ , then RFD coincides with gradient descent ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{w_{n+1}=w_{n}-\\eta_{n}^{*}\\frac{\\nabla{\\bf J}(w_{n})}{\\|\\nabla{\\bf J}(w_{n})\\|},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "table", "img_path": "xzCuBjHQbS/tmp/8c79b7bf283513d90c1f3f5d9d7d4918d8156178896e57c2142067c0dd80f435.jpg", "table_caption": ["Table 1: RFD step size (cf. Figure 2 and Eq. (11), (13), (14) for the formal definitions of the models). In particular, $s$ is the length scale in all covariance models. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "where the RFD step sizes are given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\eta_{n}^{*}:=\\operatorname*{argmin}_{\\eta\\in\\mathbb{R}}\\frac{C\\left(\\frac{\\eta^{2}}{2}\\right)}{C(0)}(\\mathbf{J}(w_{n})-\\mu)-\\eta\\frac{C^{\\prime}\\!\\left(\\frac{\\eta^{2}}{2}\\right)}{C^{\\prime}(0)}\\|\\nabla\\mathbf{J}(w_{n})\\|.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "While the descent direction is a universal property for all isotropic Gaussian random functions, it follows from (1) that the step sizes depend much more on the specific covariance structure. In particular it depends on the decay rate of the covariance acting as the trust bound. ", "page_idx": 4}, {"type": "text", "text": "Remark 4.3 (Scalable complexity). While Bayesian optimization typically has computational complexity $O(n^{3}d^{3})$ in number of steps $n$ and dimensions $d$ [55, 45], RFD under the isotropy assumption has the same computational complexity as gradient descent (i.e. $\\mathcal{O}(n d))$ . Remark 4.4 (Step until the given information is no longer informative). While $L$ -smoothness-based trust prescribes step sizes that guarantee the slope to point downwards over the entire step, RFD prescribes steps which are exactly large enough that the gradient is no longer correlated to the previously observed evaluation. This is because the first order condition demands ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{0\\overset{!}{=}\\nabla\\mathbb{E}[\\mathbf{J}(w)\\mid\\mathbf{J}(w_{n}),\\nabla\\mathbf{J}(w_{n})]=\\mathbb{E}[\\nabla\\mathbf{J}(w)\\mid\\mathbf{J}(w_{n}),\\nabla\\mathbf{J}(w_{n})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "And for measurable functions $\\phi:\\mathbb{R}^{d+1}\\,\\rightarrow\\,\\mathbb{R}$ such that $\\Phi\\,=\\,\\phi({\\bf J}(w_{n}),\\nabla{\\bf J}(w_{n}))$ is sufficiently integrable, $\\Phi$ is then uncorrelated from $\\partial_{i}\\mathbf{J}(w)$ by the first order condition ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Cov}(\\partial_{i}\\mathbf{J}(w),\\Phi)=\\mathbb{E}\\Big[\\underbrace{\\mathbb{E}[\\partial_{i}\\mathbf{J}(w)\\mid\\mathbf{J}(w_{n}),\\nabla\\mathbf{J}(w_{n})]}_{=0}(\\Phi-\\mathbb{E}[\\Phi])\\Big]=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "5 The RFD step size schedule ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "While classical theory leads to \u2018learning rates\u2019, RFD suggests \u2018step sizes\u2019 applied to normalized gradients representing the actual length of the step size. In the following we thus make the distinction ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{w_{n+1}=w_{n}-\\underbrace{h_{n}}_{\\mathrm{\\cdotlearning\\;rate}}\\nabla\\mathbf{J}(w_{n})=w_{n}-\\underbrace{\\eta_{n}}_{\\mathrm{\\cdotstep\\;size}}\\frac{\\nabla\\mathbf{J}(w_{n})}{\\|\\nabla\\mathbf{J}(w_{n})\\|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To get a better feel for the step sizes suggested by RFD, it is enlightening to divide (1) by $\\boldsymbol{\\mu}-\\mathbf{J}(w_{n})$ which results in a minimization problem ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\eta^{*}:=\\eta^{*}(\\Theta):=\\operatorname*{argmin}_{\\eta}q_{\\Theta}(\\eta)\\qquad\\mathrm{for}\\qquad q_{\\Theta}(\\eta):=-\\frac{C\\bigl(\\frac{\\eta^{2}}{2}\\bigr)}{C(0)}-\\eta\\frac{C^{\\prime}\\bigl(\\frac{\\eta^{2}}{2}\\bigr)}{C^{\\prime}(0)}\\Theta,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which is only parametrized by the \u201cgradient cost quotient\u201d ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Theta_{n}=\\frac{\\|\\nabla\\mathbf{J}(w_{n})\\|}{\\mu-\\mathbf{J}(w_{n})},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "i.e. $\\eta_{n}^{*}=\\eta^{*}(\\Theta_{n})$ . This minimization problem can be solved explicitly for the most common [44, ch. 4] differentiable isotropic covariance models, see Table 1, Figure 2 and Appendix $\\mathbf{C}$ for details. ", "page_idx": 4}, {"type": "text", "text": "Figure 2 can be interpreted as follows: At the start of optimization, the cost should be roughly equal to the average cost $\\mu\\approx\\mathbf{J}(w)$ , so the gradient cost quotient $\\Theta$ is infinite and the step sizes are therefore given by $\\eta^{*}(\\infty)$ (also listed in its own column in Table 1). As we start minimizing, the difference $\\mu-\\mathbf{J}(w)$ becomes positive. Towards the end of minimization this difference no longer changes as the cost no longer decreases. I.e. towards the end the gradient cost quotient $\\Theta$ is roughly linear in the gradient $\\|\\nabla\\mathbf{J}(w)\\|$ . The derivative $\\frac{d}{d\\Theta}\\eta^{*}(0)$ of $\\eta^{*}(\\Theta)$ at zero then effectively results in a constant asymptotic learning rate. ", "page_idx": 5}, {"type": "text", "text": "5.1 Asymptotic learning rate ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To explain the claim above, note that the gradient cost quotient $\\Theta$ converges to zero towards the end of optimization, because the gradient norm converges to zero. A first order Taylor expansion of $\\eta^{*}$ would therefore imply ", "page_idx": 5}, {"type": "image", "img_path": "xzCuBjHQbS/tmp/0e2d513fba1ce669847ffbada703b2d9d4b78c9638d0952545207624e76d2163.jpg", "img_caption": ["Figure 2: RFD step sizes as a function of $\\Theta=$ $\\frac{||\\breve{\\nabla}\\mathbf{J}(w)||}{\\mu\\!-\\!\\mathbf{J}(w)}$ assuming scale $s\\,=\\,1$ (cf. Table 1). ARFD (Definition 5.1) is plotted as dashed lines. A-RFD of the rational quadratic coincides with A-RFD of the squared exponential covariance. "], "img_footnote": [], "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\eta^{*}(\\Theta)\\approx\\eta^{*}(0)+\\frac{d}{d\\Theta}\\eta^{*}(0)\\Theta=}&{{}\\quad\\frac{\\frac{d}{d\\Theta}\\eta^{*}(0)}{\\mu-\\mathbf{J}(w)}\\qquad\\lVert\\nabla\\mathbf{J}(w)\\rVert}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "assuming $\\eta^{*}(0)=0$ and differentiability of $\\eta^{*}$ , which is a reasonable educated guess based on the the examples in Figure 2. But since the RFD step sizes $\\eta^{*}$ are abstractly defined as an argmin, it is necessary to formalize this intuition for general covariance models. First, we define asymptotic step sizes as an object towards which we can prove convergence. Then we prove convergence, proving they are well defined. In addition, we obtain a more explicit formula for the asymptotic learning rate. Definition 5.1 (A-RFD). We define the step sizes of \u201casymptotic RFD\u201d (A-RFD) to be the minimizer of the second order Taylor approximation $T_{2}q_{\\Theta}$ of $q_{\\Theta}$ around zero ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\eta}(\\Theta):=\\underset{\\eta}{\\mathrm{argmin}}\\,T_{2}q_{\\Theta}(\\eta)=\\frac{C(0)}{-C^{\\prime}(0)}\\Theta=\\underbrace{\\frac{C(0)}{C^{\\prime}(0)(\\mathbf{J}(w)-\\mu)}}_{\\mathrm{asvmntotic~learnino~rate}}\\|\\nabla\\mathbf{J}(w)\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the following we prove that these are truly asymptotically equal to the step sizes $\\eta^{*}$ of RFD. ", "page_idx": 5}, {"type": "text", "text": "Proposition 5.2 (A-RFD is well defined). Let $\\mathbf{J}\\sim{\\mathcal{N}}({\\boldsymbol{\\mu}},{\\boldsymbol{C}})$ and assume there exists $\\eta_{0}>0$ such that the correlation for larger distances \u03b7 \u2265\u03b70 are bounded smaller than 1, i.e. C(C\u03b7(20/)2) $\\begin{array}{r}{\\frac{C(\\eta^{2}/2)}{C(0)}<\\rho\\in(0,1)}\\end{array}$ . Then the step sizes of RFD are asymptotically equal to the step sizes of A-RFD, i.e. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\eta}(\\Theta)\\sim\\eta^{\\ast}(\\Theta)\\quad a s\\quad\\Theta\\to0.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that the assumption is essentially always satisfied, since the Cauchy-Schwarz inequality implies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C\\big(\\frac{\\|w-\\tilde{w}\\|^{2}}{2}\\big)=\\mathrm{Cov}(\\mathbf{J}(w),\\mathbf{J}(\\tilde{w}))\\leq\\sqrt{\\mathrm{Var}(\\mathbf{J}(w))\\,\\mathrm{Var}(\\mathbf{J}(\\tilde{w}))}=C(0),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where equality requires the random variables to be almost surely equal [30]. If the random function is not periodic or constant, this will generally be strict. In the proof, this requirement is only needed to ensure that $\\eta^{*}$ is not very large. The smallest local minimum of $q_{\\Theta}$ is always close to $\\hat{\\eta}$ even without this assumption (which ensures it is a global minimum). ", "page_idx": 5}, {"type": "text", "text": "Figure 2 illustrates that $\\eta^{*}\\to0$ should imply $\\Theta\\to0$ , resulting in a weak convergence guarantee. Corollary 5.3. Assume $\\eta^{*}\\to0$ implies $\\Theta\\to0$ , the cost $\\mathbf{J}$ is bounded, has continuous gradients and RFD converges to some point $w_{\\infty}$ . Then $w_{\\infty}$ is a critical point and the RFD step sizes $\\eta^{*}$ are asymptotically equal to $\\hat{\\eta}$ . ", "page_idx": 5}, {"type": "text", "text": "For the squared exponential covariance model we formally prove that $\\eta^{*}$ is strictly monotonously increasing in $\\Theta$ and thus $\\eta^{*}~\\to~0$ implies $\\Theta\\,\\rightarrow\\,0$ (Prop. C.3). The \u2018bounded\u2019 and \u2018continuous gradients\u2019 assumptions are almost surely satisfied for all sufficiently smooth covariance functions [cf. 1], where three times differentiable is more than enough smoothness. ", "page_idx": 5}, {"type": "text", "text": "5.2 RFD step sizes explain common step size heuristics ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Asymptotically, RFD suggests constant learning rates, similar to the classical $L$ -smooth setting. We thus define these asymptotic learning rates (as the limit of the learning rates $h_{n}$ of iteration $n$ ) to be ", "page_idx": 6}, {"type": "equation", "text": "$$\nh_{\\infty}:=\\frac{C(0)}{C^{\\prime}(0)(\\mathbf{J}(w_{\\infty})-\\mu)},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathbf{J}(w_{\\infty})$ is the cost we reach in the limit. If we used these asymptotic learning rates from the start, step sizes would become too large for large gradients, as RFD step sizes exhibit a plateau (cf. Figure 2). To emulate the behavior of RFD with a piecewise linear function, we could introduce a cutoff whenever our step size exceeds the initial step size $\\eta^{*}(\\infty)$ , i.e. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\boldsymbol{w}_{n+1}=\\boldsymbol{w}_{n}-\\operatorname*{min}\\Bigl\\{h_{\\infty},\\frac{\\eta^{*}(\\infty)}{\\|\\nabla\\mathbf{J}(\\boldsymbol{w}_{n})\\|}\\Bigr\\}\\nabla\\mathbf{J}(\\boldsymbol{w}_{n}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "At this point we have rediscovered \u2018gradient clipping\u2019 [42]. Since the rational quadratic covariance has the same asymptotic learning rate $h_{\\infty}$ for every $\\beta$ , its parameter $\\beta$ controls the step size bound $\\eta^{*}(\\infty)$ of gradient clipping (cf. Table 1, Figure 2). ", "page_idx": 6}, {"type": "text", "text": "Pascanu et al. [42] motivated gradient clipping with the geometric interpretation of movement towards a \u2018wall\u2019 placed behind the minimum. This suggests that clipping should happen towards the end of training. This stands in contrast to a more recent step size heuristic, \u201c(linear) warmup\u201d [20], which suggests smaller learning rates at the start (i.e. $\\begin{array}{r}{h_{0}=\\frac{\\eta^{*}(\\infty)}{\\|\\nabla\\mathbf{J}(w_{0})\\|})}\\end{array}$ \u2225\u2207\u03b7J((w\u221e0))\u2225) and gradual ramp-up to the asymptotic learning rate $h_{\\infty}$ . In other words, gradients are not clipped due to some wall next to the minimum, but because the step sizes would be too large at the start otherwise. Goyal et al. [20] further observe that \u2018constant warmup\u2019 (i.e. a step change of learning rates akin to gradient clipping) performs worse than gradual warmup. Since RFD step sizes suggest this gradual increase, we argue that they may have discovered RFD step sizes empirically (also cf. Figure 3). ", "page_idx": 6}, {"type": "text", "text": "6 Mini-batch loss and covariance estimation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Since we do not have access to evaluations of the cost $\\mathbf{J}$ in practice, we need to prove some results about stochastic losses $\\ell_{i}$ before we can apply RFD in practice. For this, assume that we have independent identically distributed (iid) data $X_{i}$ independent of the true relationship f drawn from $\\mathbb{P_{f}}$ resulting in labels $Y_{i}=\\mathbf{f}(X_{i})+\\varsigma_{i}$ , where we have added independent iid noise $\\varsigma_{i}$ , resulting in loss and cost ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\ell_{i}(w):=\\ell{\\big(}w,(X_{i},Y_{i}){\\big)}\\quad{\\mathrm{and}}\\quad\\mathbf{J}(w):=\\mathbb{E}[\\ell_{i}(w)\\mid\\mathbf{f}].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In this setting we confirm (cf. Lemma D.9), that the stochastic approximation errors ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\epsilon_{i}(w):=\\ell_{i}(w)-\\mathbf{J}(w)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "are independent conditional on the true relationship f. In particular they (and all their derivatives) are uncorrelated and also uncorrelated from $\\mathbf{J}$ . It follows that mini-batch losses ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{b}(w):=\\frac{1}{b}\\sum_{i=1}^{b}\\ell_{i}(w)=\\mathbf{J}(w)+\\frac{1}{b}\\sum_{i=1}^{b}\\epsilon_{i}(w)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "have variance ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Var}(\\mathscr{L}_{b}(w))=\\mathrm{Var}(\\mathbf{J}(w))+\\frac{1}{b}\\,\\mathrm{Var}(\\epsilon_{1}(w))\\overset{\\mathrm{isotropy}}{=}C(0)+\\frac{1}{b}C_{\\epsilon}(0),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where we assume $\\mathbf{J}\\sim{\\mathcal{N}}({\\boldsymbol{\\mu}},{\\boldsymbol{C}})$ and $\\epsilon_{i}\\sim\\mathcal{N}(0,C_{\\epsilon})$ in the last equation for simplicity. But this step did not yet require the distributional Gaussian assumption beyond the mean and variance. ", "page_idx": 6}, {"type": "text", "text": "6.1 Variance estimation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Recall that the asymptotic learning rate $h_{\\infty}$ in Equation (3) only depends on $C(0)$ and $C^{\\prime}(0)$ . So if we estimate these values, we are certain to get the right RFD step sizes asymptotically without knowing the entire covariance kernel $C$ . ", "page_idx": 6}, {"type": "text", "text": "Equation (5) reveals that for $Z_{b}:=(\\mathcal{L}_{b}(w)-\\mu)^{2}$ we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[Z_{b}]=\\beta_{0}+\\frac{1}{b}\\beta_{1}\\qquad\\mathrm{i.e.}\\qquad Z_{b}=\\beta_{0}+\\frac{1}{b}\\beta_{1}+\\mathrm{noise}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "with bias $\\beta_{0}=C(0)$ and slope $\\beta_{1}=C_{\\epsilon}(0)$ . So a linear regression on samples $\\begin{array}{r}{\\big(\\frac{1}{b_{k}},Z_{b_{k}}\\big)_{k\\leq n}}\\end{array}$ allows for the estimation of $\\beta_{0}$ and $\\beta_{1}$ . Using the Gaussian assumption from (5), the variance of $Z_{b}$ is the (centered) fourth moment of $\\mathcal{L}_{b}$ , which is given by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sigma_{b}^{2}:=\\operatorname{Var}(Z_{b})=\\mathbb{E}[Z_{b}^{4}]-\\mathbb{E}[Z_{b}^{2}]^{2}=2\\operatorname{Var}(\\mathcal{L}_{b}(w))^{2}=2(\\beta_{0}+\\frac{1}{b}\\beta_{1})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In particular the variance of $Z_{b}$ depends on the batch size $b$ . The linear regression is therefore heteroskedastic. Weighted least squares (WLS) [e.g. 28, Theorem 4.2] is designed to handle this case, but for its application the variance of $Z_{b}$ is needed. Since $\\beta_{0},\\beta_{1}$ are the parameters we wish to estimate, we find ourselves in the paradoxical situation that we need $\\beta$ to obtain $\\beta$ . Our solution to this problem is to start with a guess of $\\beta_{0},\\beta_{1}$ , apply WLS to obtain a better estimate and repeat this bootstrapping procedure until convergence. Since all $Z_{b}$ have the same underlying cost $\\mathbf{J}$ , we sample the parameters $w$ randomly to reduce their covariance (details in Sec. B). ", "page_idx": 7}, {"type": "text", "text": "The same procedure can be applied to obtain $C^{\\prime}(0)$ , where the counterpart of Equation (5) is given by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Var}(\\partial_{i}\\mathcal{L}_{b}(w))=\\mathrm{Var}(\\partial_{i}\\mathbf{J}(w))+\\frac{1}{b}\\,\\mathrm{Var}(\\partial_{i}\\epsilon_{1}(w))\\overset{\\mathrm{isotropy}}{=}-(C^{\\prime}(0)+\\frac{1}{b}C_{\\epsilon}^{\\prime}(0)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark 6.1. Under the isotropy assumption the partial derivatives are iid, so the expectation of $\\begin{array}{r}{\\|\\nabla\\mathcal{L}_{b}(w)\\|^{2}=\\sum_{i=1}^{d}(\\partial_{i}\\mathcal{L}_{b}(w))^{2}}\\end{array}$ is this variance scaled by $d$ . In particular the variance needs to scale with $\\textstyle{\\frac{1}{d}}$ to keep the gradient norms (and thus the Lipschitz constant of $\\mathbf{J}$ ) stable. This observation is closely related to \u201cisoperimetry\u201d [e.g. 7], for details see [5]. Removing the isotropy assumption and estimating the variance component-wise is most likely how \u201cadaptive\u201d step sizes [e.g. 14, 29], like the ones used by Adam, work (cf. Sec. E.1). ", "page_idx": 7}, {"type": "text", "text": "Batch size distribution Before we can apply linear regression to the samples $\\begin{array}{r}{\\big(\\frac{1}{b_{k}},Z_{b_{k}}\\big)_{k\\leq n}}\\end{array}$ , it is necessary to choose the batch sizes $b_{k}$ . As this choice is left to us, we calculate the variance of our estimator $\\hat{\\beta}_{0}$ of $\\beta_{0}$ explicitly (Lemma B.2), in order to minimize this variance subject to a sample budget $\\alpha$ over the selection of batch sizes ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{n,b_{1},\\ldots,b_{n}}\\mathrm{Var}(\\hat{\\beta}_{0})\\quad\\mathrm{s.t.}\\qquad\\sum_{k=1}^{n}b_{k}}\\ \\leq\\alpha.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Since this optimization problem is very difficult to solve, we rephrase it in terms of the empirical distribution of batch sizes $\\begin{array}{r}{\\nu_{n}=\\frac{1}{n}\\sum_{i=1}^{n^{\\star}}\\delta_{b_{i}}}\\end{array}$ in=1 \u03b4bi. Optimizing over distributions is still difficult, but we explain in Section B.1 how to heuristically arrive at the parametrization ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nu(b)\\propto\\exp\\bigl(\\lambda_{1}\\frac{1}{\\sigma_{b}^{2}}-\\lambda_{2}b\\bigr),\\qquad b\\in\\mathbb{N}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the parameters $\\lambda_{1},\\lambda_{2}\\geq0$ can then be used to optimize (6). Due to our usage of $\\sigma_{b}^{2}$ this has to be bootstrapped. ", "page_idx": 7}, {"type": "text", "text": "Covariance estimation While the variance estimates above ensure correct asymptotic learning rates, we motivated in Section 5.2 that asymptotic learning rates alone would result in too large step sizes at the beginning. We therefore use the estimates of $C(0)$ and $C^{\\prime}(0)$ to fit a covariance model, effectively acting as a gradient clipper while retaining the asymptotic guarantees. Note that covariance models with less than two parameters are generally fully determined by these values. ", "page_idx": 7}, {"type": "text", "text": "6.2 Stochastic RFD (S-RFD) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "It is reasonable to ask whether there is a \u2018stochastic gradient descent\u2019-like counterpart to the \u2018gradient descent\u2019-like RFD. The answer is yes, and we already have all the required machinery. ", "page_idx": 7}, {"type": "text", "text": "Extension 6.2 (S-RFD). For loss $\\mathbf{J}\\sim{\\mathcal{N}}({\\boldsymbol{\\mu}},{\\boldsymbol{C}})$ and stochastic errors $\\epsilon_{i}\\overset{\\mathrm{iid}}{\\sim}\\mathcal{N}(0,C_{\\epsilon})$ we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\b{\\lambda}}{\\mathrm{argmin}}\\,\\mathbb{E}[\\mathbf{J}(w-\\mathbf{d})\\mid\\mathcal{L}_{b}(w),\\nabla\\mathcal{L}_{b}(w)]=\\eta^{*}(\\Theta)\\frac{\\nabla\\mathcal{L}_{b}(w)}{\\|\\nabla\\mathcal{L}_{b}(w)\\|}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "with the same step size function $\\eta^{*}$ as for RFD, but modified $\\Theta$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\Theta=\\frac{C^{\\prime}(0)}{C^{\\prime}(0)+\\frac{1}{b}C_{\\epsilon}^{\\prime}(0)}\\frac{C(0)+\\frac{1}{b}C_{\\epsilon}(0)}{C(0)}\\frac{\\|\\nabla\\mathcal{L}_{b}(w)\\|}{\\mu-\\mathcal{L}_{b}(w)}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Note, that our non-parametric covariance estimation already provides us with estimates of $C_{\\epsilon}(0)$ and $C_{\\epsilon}^{\\prime}(0)$ , so no further adaptions are needed. The resulting asymptotic learning rate is given by ", "page_idx": 8}, {"type": "equation", "text": "$$\nh_{\\infty}={\\frac{C(0)+{\\frac{1}{b}}C_{\\epsilon}(0)}{(C^{\\prime}(0)+{\\frac{1}{b}}C_{\\epsilon}^{\\prime}(0))({\\mathcal{L}}_{b}(w_{\\infty})-\\mu)}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "7 MNIST case study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "For our case study we use the negative log likelihood loss to train a neural network [3, M7] on the MNIST dataset [34]. We choose this model as one of the simplest state-of-the-art models at the time of selection, consisting only of convolutional layers with ReLU activation interspersed by batch normalization layers and a single dense layers at the end with softmax activation. Assuming isotropy, we estimate $\\mu$ , $\\dot{C}(0)$ and $C^{\\prime}(0)$ as described in Section 6.1 and deduce the parameters $\\sigma^{2}$ and $s$ of the respective covariance model (more details in Section B). We then use the step sizes listed in Table 1 for the \u2018squared exponential\u2019 and \u2018rational quadratic\u2019 covariance in our RFD algorithm. ", "page_idx": 8}, {"type": "text", "text": "In Figure 3, RFD is benchmarked against step size tuned Adam [29] and stochastic gradient descent (SGD). Even with early stopping, their tuning would typically require more than 1 epoch worth of samples, in contrast to RFD (Section A.1.1). We highlight that A-RFD performs significantly worse than either of the RFD versions which effectively implement some form of learning rate warmup. This is despite the RFD learning rates converging to the asymptotic one within one epoch (ca. 30 out of 60 steps per epoch). The step sizes on the other hand are (up to noise) monotonously decreasing. This stands in contrast to the \u201cwall next to the minimum\u201d motivation of gradient clipping. ", "page_idx": 8}, {"type": "text", "text": "Code availability: Our implementation of RFD can be found at https://github.com/ FelixBenning/pyrfd and the package can also be installed from PyPI via \u2018pip install pyrfd\u2019. ", "page_idx": 8}, {"type": "text", "text": "8 Limitations and extensions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To cover the vast amount of ground that lays between the \u2018formulation of a general average case optimization problem\u2019 and the \u2018prototype of a working optimizer with theoretical backing\u2019, ", "page_idx": 8}, {"type": "text", "text": "1. we used the common [16, 52, 55, 10] isotropic and Gaussian distributional assumption for $\\mathbf{J}$ ,   \n2. we used very simple covariance models for the actual implementation,   \n3. we used WLS in our variance estimation procedure despite the violation of independence. ", "page_idx": 8}, {"type": "text", "text": "Since RFD is defined as the minimizer of an average instead of an upper bound \u2013 making it more risk affine \u2013 it naturally loses the improvement guarantee driving classical convergence proofs. It is therefore impossible to extend classical optimization proofs and new mathematical theory must be developed. This risk-affinity can also be observed in its comparatively large step sizes (cf. Fig. 3 and Sec. A). On CIFAR-100 [31], the step sizes were too large and it is an open question whether assumptions were violated or whether RFD is simply too risk-affine. But since the variance of random functions vanishes asymptotic with high dimension [5] we highly suspect the former (cf. Remark E.5). ", "page_idx": 8}, {"type": "text", "text": "Future work will therefore have to target these assumptions. Some of the assumptions were already simplifications for the sake of exposition, and we deferred their relaxation to the appendix. The Gaussian assumption can be relaxed with a generalization to the \u2018BLUE\u2019 (Sec. E.3), isotropy can be generalized to \u2018geometric anisotropies\u2019 (Sec. E.1) and the risk-affinity of RFD can be reduced with confidence intervals (Sec. E.2). Since simple random linear models already violate stationary isotropy (Sec. F.1), we believe that stationarity is the most important assumption to attack in future work. ", "page_idx": 8}, {"type": "text", "text": "9 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper we have demonstrated the viability (computability and scalable complexity) and advantages (scale invariance, explainable step size schedule which does not require expensive tuning) of replacing the classical \u201cconvex function\u201d framework with the \u201crandom function\u201d framework. Along the way we bridged the gap between Bayesian optimization (not scalable so far) and classical optimization methods (scalable). This theoretical framework not only sheds light on existing step size heuristics, but can also be used to develop future heuristics. ", "page_idx": 8}, {"type": "image", "img_path": "xzCuBjHQbS/tmp/bf6efa3b9e16326d0fb0a64b65712d8b5b736037a8c28662a83c61ec50cacbe5.jpg", "img_caption": ["Figure 3: Training on the MNIST dataset (batch size 1024). Ribbons describe the range between the $10\\%$ and $90\\%$ quantile of 20 repeated experiments while lines represent their mean. SE stands for the squared exponential (11) and RQ for the rational quadratic (13) covariance. The validation loss uses the test data set, which provides a small advantage to Adam and SGD, as we also use it for tuning. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "We envision the following improvements to RFD in the future: ", "page_idx": 9}, {"type": "text", "text": "1. The reliability of RFD can be improved by generalizing the distributional assumptions to cover more real world scenarios. In particular we are interested in the generalization to non-stationary isotropy because we suspect that regularization such as weight and batch normalization [46, 25] are used to patch violations of stationarity (cf. Section F). 2. The performance of RFD can also be improved. Since RFD is forgetful while momentum methods retains some information it is likely fruitful to relax the full forgetfulness. Furthermore, we suspect that adaptive learning rates [e.g. 14, 29], such as those used by Adam, can be incorporated with geometric anisotropies (cf. Sec. E.1). Performance could also be further improved by estimating the covariance (locally) online instead of globally at the start. Finally, the implementation itself can be made more performant. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We extend our sincere gratitude to our colleagues at the University of Mannheim, with special thanks to Rainer Gemulla and Julie Naegelen for insightful discussions and invaluable feedback. The Experiments in this work were partially carried out on the compute cluster of the state of Baden-W\u00fcrtemberg (bwHPC). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Robert J. Adler and Jonathan E. Taylor. Random Fields and Geometry. Springer Monographs in Mathematics. New York, NY: Springer New York, 2007. ISBN: 978-0-387-48112-8. DOI: 10.1007/978-0-387-48116-6.   \n[2] Apoorv Agnihotri and Nipun Batra. \u201cExploring Bayesian Optimization\u201d. In: Distill 5.5 (2020- 05-05), e26. ISSN: 2476-0757. DOI: 10.23915/distill.00026.   \n[3] Sanghyeon An et al. An Ensemble of Simple Convolutional Neural Network Models for MNIST Digit Recognition. 2020-10-04. DOI: 10.48550/arXiv.2008.10400. Pre-published.   \n[4] Antonio Auffinger and Qiang Zeng. \u201cComplexity of Gaussian Random Fields with Isotropic Increments\u201d. In: Communications in Mathematical Physics 402.1 (2023-08-01), pp. 951\u2013993. ISSN: 1432-0916. DOI: 10.1007/s00220-023-04739-0.   \n[5] Felix Benning and Leif D\u00f6ring. Gradient Span Algorithms Make Predictable Progress in High Dimension. 2024-10-13. DOI: 10.48550/arXiv.2410.09973. Pre-published.   \n[6] Karl Heinz Borgwardt. The Simplex Method: A Probabilistic Analysis. Softcover reprint of the original 1st ed. 1987 edition. Berlin Heidelberg: Springer, 1986-11-01. 282 pp. ISBN: 978-3-540-17096-9.   \n[7] Sebastien Bubeck and Mark Sellke. \u201cA Universal Law of Robustness via Isoperimetry\u201d. In: Advances in Neural Information Processing Systems. Vol. 34. Virtual Event: Curran Associates, Inc., 2021, pp. 28811\u201328822. arXiv: 2105 . 12806 [cs, stat]. URL: https : / / proceedings . neurips . cc / paper / 2021 / hash / f197002b9a0853eca5e046d9ca4663d5-Abstract.html (visited on 2023-09-22).   \n[8] Youngmin Cho and Lawrence Saul. \u201cKernel Methods for Deep Learning\u201d. In: Advances in Neural Information Processing Systems. Vol. 22. Curran Associates, Inc., 2009. URL: https : / / proceedings . neurips . cc / paper / 2009 / hash / 5751ec3e9a4feab575962e78e006250d-Abstract.html (visited on 2023-04-03).   \n[9] Leonardo Cunha et al. \u201cOnly Tails Matter: Average-Case Universality and Robustness in the Convex Regime\u201d. In: Proceedings of the 39th International Conference on Machine Learning. International Conference on Machine Learning. PMLR, 2022-06-28, pp. 4474\u20134491. URL: https://proceedings.mlr.press/v162/cunha22a.html (visited on 2023-11-09).   \n[10] Yann N Dauphin et al. \u201cIdentifying and Attacking the Saddle Point Problem in HighDimensional Non-Convex Optimization\u201d. In: Advances in Neural Information Processing Systems. Vol. 27. Montr\u00e9al, Canada: Curran Associates, Inc., 2014. URL: https://proceedings. neurips.cc/paper/2014/hash/17e23e50bedc63b4095e3d8204ce063b-Abstract. html (visited on 2022-06-10).   \n[11] Aaron Defazio and Konstantin Mishchenko. \u201cLearning-Rate-Free Learning by D-Adaptation\u201d. In: Proceedings of the 40th International Conference on Machine Learning. International Conference on Machine Learning. PMLR, 2023-07-03, pp. 7449\u20137479. arXiv: 2301.07733 [cs.LG]. URL: https://proceedings.mlr.press/v202/defazio23a.html (visited on 2024-03-31).   \n[12] Percy Deift and Thomas Trogdon. \u201cThe Conjugate Gradient Algorithm on Well-Conditioned Wishart Matrices Is Almost Deterministic\u201d. In: Quarterly of Applied Mathematics 79.1 (2021- 03), pp. 125\u2013161. ISSN: 0033-569X, 1552-4485. DOI: 10.1090/qam/1574. arXiv: 1901. 09007 [cs, math].   \n[13] P. Deuflhard and G. Heindl. \u201cAffine Invariant Convergence Theorems for Newton\u2019s Method and Extensions to Related Methods\u201d. In: SIAM Journal on Numerical Analysis 16.1 (1979-02), pp. 1\u201310. ISSN: 0036-1429. DOI: 10.1137/0716001.   \n[14] John Duchi, Elad Hazan, and Yoram Singer. \u201cAdaptive Subgradient Methods for Online Learning and Stochastic Optimization\u201d. In: The Journal of Machine Learning Research 12 (2011-07-01), pp. 2121\u20132159. ISSN: 1532-4435.   \n[15] Ahmed El Alaoui, Andrea Montanari, and Mark Sellke. \u201cOptimization of Mean-Field Spin Glasses\u201d. In: The Annals of Probability 49.6 (2021-11), pp. 2922\u20132960. DOI: 10.1214/21- AOP1519. Peter I. Frazier. \u201cBayesian Optimization\u201d. In: Recent Advances in Optimization and Modeling of Contemporary Problems. INFORMS TutORials in Operations Research. Phoenix, Arizona, USA: INFORMS, 2018-10, pp. 255\u2013278. ISBN: 978-0-9906153-2-3. DOI: 10.1287/educ. 2018.0188. arXiv: 1807.02811 [cs, math, stat].   \n[17] Fuchang Gao and Lixing Han. \u201cImplementing the Nelder-Mead Simplex Algorithm with Adaptive Parameters\u201d. In: Computational Optimization and Applications 51.1 (2012-01-01), pp. 259\u2013277. ISSN: 1573-2894. DOI: 10.1007/s10589-010-9329-3.   \n[18] Xavier Glorot and Yoshua Bengio. \u201cUnderstanding the Difficulty of Training Deep Feedforward Neural Networks\u201d. In: Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. Sardinia, Italy: JMLR Workshop and Conference Proceedings, 2010-03-31, pp. 249\u2013256. URL: https://proceedings.mlr.press/v9/ glorot10a.html (visited on 2023-04-11).   \n[19] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016-11-10. 801 pp. ISBN: 978-0-262-33737-3. Google Books: omivDQAAQBAJ.   \n[20] Priya Goyal et al. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. arXiv, 2018- 04-30. arXiv: 1706.02677 [cs]. URL: http://arxiv.org/abs/1706.02677 (visited on 2024-04-02).   \n[21] Anders Hansson. Optimization for Learning and Control. Hoboken, New Jersey: John Wiley & Sons, Inc., 2023. ISBN: 978-1-119-80914-2.   \n[22] Geoffrey Hinton. \u201cNeural Networks for Machine Learning\u201d. Massive Open Online Course (Coursera). 2012. URL: https://www.cs.toronto.edu/\\~hinton/coursera_lectures. html (visited on 2021-11-16).   \n[23] C. A. R. Hoare. \u201cQuicksort\u201d. In: The Computer Journal 5.1 (1962-01-01), pp. 10\u201316. ISSN: 0010-4620. DOI: 10.1093/comjnl/5.1.10.   \n[24] Brice Huang and Mark Sellke. \u201cTight Lipschitz Hardness for Optimizing Mean Field Spin Glasses\u201d. In: 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS). 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS). 2022-10, pp. 312\u2013322. DOI: 10.1109/FOCS54457.2022.00037.   \n[25] Sergey Ioffe and Christian Szegedy. \u201cBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\u201d. In: Proceedings of the 32nd International Conference on Machine Learning. International Conference on Machine Learning. PMLR, 2015-06-01, pp. 448\u2013456. arXiv: 1502.03167 [cs]. URL: https://proceedings.mlr. press/v37/ioffe15.html (visited on 2021-10-06).   \n[26] E. T. Jaynes. \u201cInformation Theory and Statistical Mechanics\u201d. In: Physical Review 106.4 (1957-05-15), pp. 620\u2013630. DOI: 10.1103/PhysRev.106.620.   \n[27] Richard Arnold Johnson and Dean W. Wichern. Applied Multivariate Statistical Analysis. 6th ed. Upper Saddle River, N.J: Pearson College Div, 2007. 767 pp. ISBN: 978-0-13-187715- 3.   \n[28] Steven M. Kay. Fundamentals of Statistical Signal Processing: Estimation Theory. USA: Prentice-Hall, Inc., 1993-02. 595 pp. ISBN: 978-0-13-345711-7.   \n[29] Diederik P. Kingma and Jimmy Ba. \u201cAdam: A Method for Stochastic Optimization\u201d. In: Proceedings of the 3rd International Conference on Learning Representations. ICLR. San Diego, 2015. arXiv: 1412.6980.   \n[30] Achim Klenke. Probability Theory: A Comprehensive Course. Universitext. London: Springer, 2014. ISBN: 978-1-4471-5360-3 978-1-4471-5361-0. DOI: 10.1007/978-1-4471-5361-0.   \n[31] Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. 2009. URL: https: //www.cs.toronto.edu/%20kriz/learning-features-2009-TR.pdf (visited on 2024-05-21).   \n[32] H. J. Kushner. \u201cA New Method of Locating the Maximum Point of an Arbitrary Multipeak Curve in the Presence of Noise\u201d. In: Journal of Basic Engineering 86.1 (1964-03-01), pp. 97\u2013 106. ISSN: 0021-9223. DOI: 10.1115/1.3653121.   \n[33] Jonathan Lacotte and Mert Pilanci. \u201cOptimal Randomized First-Order Methods for LeastSquares Problems\u201d. In: Proceedings of the 37th International Conference on Machine Learning. International Conference on Machine Learning. PMLR, 2020-11-21, pp. 5587\u20135597. URL: https://proceedings.mlr.press/v119/lacotte20a.html (visited on 2023-11-09).   \n[34] Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. THE MNIST DATABASE of Handwritten Digits. 2010. URL: http://yann.lecun.com/exdb/mnist/ (visited on 2024-01-24).   \n[35] Daniel James Lizotte. \u201cPractical Bayesian Optimization\u201d. PhD thesis. Alberta, Canada: University of Alberta, 2008. 168 pp.   \n[36] G. Matheron. \u201cThe Intrinsic Random Functions and Their Applications\u201d. In: Advances in Applied Probability 5.3 (1973-12), pp. 439\u2013468. ISSN: 0001-8678, 1475-6064. DOI: 10.2307/ 1425829.   \n[37] Andrea Montanari. \u201cOptimization of the Sherrington\u2013Kirkpatrick Hamiltonian\u201d. In: SIAM Journal on Computing (2021-01-07), FOCS19\u20131. ISSN: 0097-5397. DOI: 10.1137/20M132016X.   \n[38] Yurii Evgen\u2019evic\u02c7 Nesterov. Lectures on Convex Optimization. Second edition. Springer Optimization and Its Applications; Volume 137. Cham: Springer, 2018. ISBN: 978-3-319-91578-4. DOI: 10.1007/978-3-319-91578-4.   \n[39] Misha Padidar et al. \u201cScaling Gaussian Processes with Derivative Information Using Variational Inference\u201d. In: Advances in Neural Information Processing Systems. Vol. 34. Curran Associates, Inc., 2021, pp. 6442\u20136453. URL: https://proceedings.neurips.cc/paper/2021/ hash/32bbf7b2bc4ed14eb1e9c2580056a989-Abstract.html (visited on 2023-05-17).   \n[40] Courtney Paquette et al. \u201cHalting Time Is Predictable for Large Models: A Universality Property and Average-Case Analysis\u201d. In: Foundations of Computational Mathematics 23.2 (2022-02), pp. 597\u2013673. ISSN: 1615-3383. DOI: 10.1007/s10208-022-09554-y. arXiv: 2006.04299 [math, stat].   \n[41] Elliot Paquette and Thomas Trogdon. \u201cUniversality for the Conjugate Gradient and MINRES Algorithms on Sample Covariance Matrices\u201d. In: Communications on Pure and Applied Mathematics 76.5 (2022-09-01), pp. 1085\u20131136. ISSN: 1097-0312. DOI: 10.1002/cpa. 22081.   \n[42] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. \u201cOn the Difficulty of Training Recurrent Neural Networks\u201d. In: Proceedings of the 30th International Conference on Machine Learning. International Conference on Machine Learning. Atlanta: PMLR, 2013-05-26, pp. 1310\u20131318. URL: https://proceedings.mlr.press/v28/pascanu13.html (visited on 2024-04-02).   \n[43] Fabian Pedregosa and Damien Scieur. \u201cAcceleration through Spectral Density Estimation\u201d. In: Proceedings of the 37th International Conference on Machine Learning. International Conference on Machine Learning. Virtual Event (formerly Vienna): PMLR, 2020-11-21, pp. 7553\u20137562. URL: https://proceedings.mlr.press/v119/pedregosa20a.html (visited on 2023-11-09).   \n[44] Carl Edward Rasmussen and Christopher K.I. Williams. Gaussian Processes for Machine Learning. 2nd ed. Adaptive Computation and Machine Learning 3. Cambridge, Massachusetts: MIT Press, 2006. 248 pp. ISBN: 0-262-18253-X. URL: http://gaussianprocess.org/ gpml/chapters/RW.pdf (visited on 2023-04-06).   \n[45] Filip de Roos, Alexandra Gessner, and Philipp Hennig. \u201cHigh-Dimensional Gaussian Process Inference with Derivatives\u201d. In: Proceedings of the 38th International Conference on Machine Learning. International Conference on Machine Learning. PMLR, 2021-07-01, pp. 2535\u2013 2545. URL: https://proceedings.mlr.press/v139/de-roos21a.html (visited on 2023-05-15).   \n[46] Tim Salimans and Durk P Kingma. \u201cWeight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks\u201d. In: Advances in Neural Information Processing Systems. Vol. 29. Barcelona, Spain: Curran Associates, Inc., 2016. URL: https : / / proceedings . neurips . cc / paper / 2016 / hash / ed265bc903a5a097f61d3ec064d96d2e-Abstract.html (visited on 2023-10-16).   \n[47] Bobak Shahriari et al. \u201cTaking the Human Out of the Loop: A Review of Bayesian Optimization\u201d. In: Proceedings of the IEEE 104.1 (2016-01), pp. 148\u2013175. ISSN: 1558-2256. DOI: 10.1109/JPROC.2015.2494218.   \n[48] Leslie N. Smith. A Disciplined Approach to Neural Network Hyper-Parameters: Part 1 \u2013 Learning Rate, Batch Size, Momentum, and Weight Decay. 2018-04-24. DOI: 10.48550/ arXiv.1803.09820. arXiv: 1803.09820 [cs, stat]. Pre-published. Leslie N. Smith. \u201cCyclical Learning Rates for Training Neural Networks\u201d. In: 2017 IEEE Winter Conference on Applications of Computer Vision (WACV). 2017 IEEE Winter Conference on Applications of Computer Vision (WACV). 2017-03, pp. 464\u2013472. DOI: 10.1109/WACV. 2017.58.   \n[50] Michael L. Stein. Interpolation of Spatial Data. Springer Series in Statistics. New York, NY: Springer, 1999. ISBN: 978-1-4612-7166-6 978-1-4612-1494-6. DOI: 10.1007/978-1-4612- 1494-6.   \n[51] Eliran Subag. \u201cFollowing the Ground States of Full-RSB Spherical Spin Glasses\u201d. In: Communications on Pure and Applied Mathematics 74.5 (2021), pp. 1021\u20131044. ISSN: 1097-0312. DOI: 10.1002/cpa.21922.   \n[52] Ziyu Wang et al. \u201cBayesian Optimization in a Billion Dimensions via Random Embeddings\u201d. In: Journal of Artificial Intelligence Research 55 (2016-02-19), pp. 361\u2013387. ISSN: 1076-9757. DOI: 10.1613/jair.4806.   \n[53] C.K.I. Williams and D. Barber. \u201cBayesian Classification with Gaussian Processes\u201d. In: IEEE Transactions on Pattern Analysis and Machine Intelligence 20.12 (1998-12), pp. 1342\u20131351. ISSN: 1939-3539. DOI: 10.1109/34.735807.   \n[54] Christopher K. I. Williams. \u201cComputation with Infinite Neural Networks\u201d. In: Neural Computation 10.5 (1998-07-01), pp. 1203\u20131216. ISSN: 0899-7667. DOI: 10 . 1162 / 089976698300017412.   \n[55] Jian Wu et al. \u201cBayesian Optimization with Gradients\u201d. In: Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc., 2017. URL: https://proceedings. neurips.cc/paper/2017/hash/64a08e5f1e6c39faeb90108c430eb120-Abstract. html (visited on 2022-06-02).   \n[56] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: A Novel Image Dataset for Benchmarking Machine Learning Algorithms. 2017-09-15. DOI: 10.48550/arXiv.1708. 07747. arXiv: 1708.07747 [cs, stat]. Pre-published.   \n[57] Matthew D. Zeiler. \u201cADADELTA: An Adaptive Learning Rate Method\u201d. 2012-12-22. arXiv: 1212.5701 [cs].   \n[58] Guodong Zhang et al. \u201cWhich Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic Model\u201d. In: Advances in Neural Information Processing Systems. Vol. 32. Curran Associates, Inc., 2019. arXiv: 1907 . 04164 [cs, stat]. URL: https : / / proceedings . neurips . cc / paper / 2019 / hash / e0eacd983971634327ae1819ea8b6214-Abstract.html (visited on 2023-11-09). ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "image", "img_path": "xzCuBjHQbS/tmp/dc80f4f4364c11c337925bc612333d41d839d8254b0d4c4bcf471ffea5a85f1b.jpg", "img_caption": ["Figure 4: Visualization of the variance estimation (Section 6.1) with $95\\%$ -confidence intervals based on the assumed distribution. Quantile-quantile (QQ) plots of the losses (against a normal distribution), squared losses (against a $\\chi^{2}(1)$ distribution) and squared gradient norms (against a $\\chi^{2}(d)$ -distribution) are displayed on the right for a selection of batch sizes. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Appendix: Random Function Descent ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Covariance estimation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Figure 4 we visualize weighted least squares (WLS) regression of the covariance estimation from Section 6.1. Note, that we sampled much more samples per batch size for these plots than RFD would typically require by itself in order to be able to plot batch-wise means and batch-wise QQ-plots. The batch size distribution we described in Section B.1 would avoid sampling the same batch size multiple times to ensure better stability of the regression and generally requires much fewer samples than were used for this visualization (cf. A.1.1) ", "page_idx": 14}, {"type": "text", "text": "We can observe from the QQ-plots on the right, that the Gaussian assumption is essentially justified for the losses, resulting in a $\\bar{\\chi}^{\\dot{2}}(1)$ distribution for the squared losses and a $\\chi^{2}(d)$ distribution for the gradient norms squared. The confidence interval estimate for the squared norms appears to be much too small (it is plotted, but too small to be visible). Perhaps this signifies a violation of the isotropy ", "page_idx": 14}, {"type": "image", "img_path": "xzCuBjHQbS/tmp/38b000d3cf7265db34152dfba67c4dba7a943bbf3c6b1cbe88e6fb7c212215a0.jpg", "img_caption": ["Figure 5: 20 repeated covariance estimations of model M7 [3] applied to the MNIST dataset. On the left are the resulting asymptotic learning rates (assuming a final loss of zero) and on the right are the samples used until the stopping criterion interrupted sampling. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "assumption as the variance of ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\nabla\\mathcal{L}_{b}(w)\\|^{2}=\\sum_{i=1}^{d}(\\partial_{i}\\mathcal{L}_{b}(w))^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "does not appear to be the variance of independent $\\chi^{2}(d)$ Gaussian random variables, and the independence only follows from the isotropy assumption. ", "page_idx": 15}, {"type": "text", "text": "A.1.1 Sampling efficiency and stability ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To evaluate the sampling efficiency and stability of our variance estimation process, we repeated the covariance estimation of the model model M7 [3] applied to the MNIST dataset 20 times (Figure 5). We used a tolerance of $\\mathrm{{tol}=0.3}$ as a stopping criterion for the estimated relative standard deviation (10). ", "page_idx": 15}, {"type": "text", "text": "At this tolerance, the asymptotic learning rate already seems relatively stable (in the same order of magnitude) and the sample cost is quite cheap. The majority of runs $16/20$ runs or $80\\%$ ) required less than 60 000 samples (1 epoch). There was one large outlier which used 500 589 samples. A closer inspection revealed, that after the initial sample to estimate the optimal batch size distribution, it sampled almost exclusively at batch sizes 20 (which was the minimal cutoff to avoid instabilities caused by batch normalization) and batch sizes between 1700 and 1900. It therefore seems like the initial batch of samples caused a very unfavorable batch size distribution which then required a lot of samples to recover from. Our selection of an initial sample size of 6000 might therefore have been too small. ", "page_idx": 15}, {"type": "text", "text": "A more extensive empirical study is needed to tune this estimation process, but the process promises to be very sample efficient. Classical step size tuning would train models for a short duration in order to evaluate the performance of a particular learning rate [e.g. 48], but a single epoch worth of samples is very hard to beat. ", "page_idx": 15}, {"type": "text", "text": "Our implementation of this process on the other hand is very inefficient as of writing. Piping data of differing batch sizes into a model is not a standard use case. We implement this by repeatedly initializing data loaders, which is anything but performance friendly. ", "page_idx": 15}, {"type": "text", "text": "A.2 Other models and datasets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To estimate the effect of the batch size on RFD, we trained the same model (M7 [3]) on MNIST with batch size 128 (Figure 6). We can see that the asymptotic learning rate of S-RFD is reduced at a smaller batch size (cf. Equation 7) but the performance is barely different. Overall, RFD seems to be slightly too risk-affine, selecting larger step sizes than the tuned SGD models. ", "page_idx": 15}, {"type": "text", "text": "We also trained a different model (M5 [3]) on the Fashion MNIST dataset [56] with batch size 128 (Figure 7). Since the validation loss increases after epoch 5, early stopping would have been appropriate. We therefore include Adam with learning rate $10^{-3}$ , despite Adam with learning rate $10^{\\bar{-}4}$ technically performing better at the end of training. We can generally see, that RFD comes very close to tuned performance at the time early stopping would have been appropriate. Again, learning rates seem to be slightly too large (risk-affine) in comparison to tuned SGD. ", "page_idx": 15}, {"type": "image", "img_path": "xzCuBjHQbS/tmp/fb99939b9b0a1e69d39b3321d445b0c46451e0e0db2cb94049d1d255a9ed0242.jpg", "img_caption": ["Figure 6: Training model M7 [3] with batch size 128 on MNIST [34]. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "B Variance estimation in detail ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Recall that we are interested in the regression ", "page_idx": 16}, {"type": "equation", "text": "$$\nZ_{b}(w)=(\\mathcal{L}_{b}(w)-\\mu)^{2}\\sim\\beta_{0}+\\frac{1}{b}\\beta_{1}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the variance of $Z_{b}$ is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sigma_{b}^{2}=2(\\beta_{0}+\\frac{1}{b}\\beta_{1})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "under the Gaussian assumption on $\\mathcal{L}_{b}$ . ", "page_idx": 16}, {"type": "text", "text": "More specifically we for minibatch sizes $(b_{k})_{k\\le n}$ and parameter vectors $(w_{k})_{k\\le n}$ we want to sample mini batch losses ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}^{(k)}:=\\mathcal{L}_{b_{k}}(w_{k})=\\mathbf{J}(w_{k})+\\frac{1}{b_{k}}\\sum_{i=1}^{b_{k}}\\epsilon_{k,i}(w_{k})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As the $\\epsilon_{k,i}$ are all conditionally independent and therefore uncorrelated, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Cov}(\\mathcal{L}^{(k)},\\mathcal{L}^{(l)})=\\mathrm{Cov}(\\mathbf{J}(w_{k}),\\mathbf{J}(w_{l}))=C\\big(\\frac{\\|w_{k}-w_{l}\\|^{2}}{2}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since the covariance kernel $C$ is typically monotonously falling in the distance of parameters $\\lVert\\boldsymbol{w}_{k}-\\boldsymbol{w}_{l}\\rVert^{2}$ , we want to select them as spaced out as possible to minimize the covariance of $\\mathcal{L}^{(k)}$ (which is the next best thing to iid samples). Randomly selecting $w_{i}$ with Glorot initialization [18] will ensure a good spread. ", "page_idx": 16}, {"type": "text", "text": "Note that Glorot initialization places all parameters approximately on the same sphere. This is because Glorot initialization initializes all parameters independently, therefore their norm is the sum of independent squares, which converges by the law of large numbers due to the normalization Glorot uses. Since stationary isotropy and non-stationary isotropy coincides on the sphere, this is an important effect to consider (cf. Section F). ", "page_idx": 16}, {"type": "image", "img_path": "xzCuBjHQbS/tmp/064f60543fd28a7351719ec3224984ed19b750ce0af20ba178e2cf2bf44a2f5d.jpg", "img_caption": ["Figure 7: Model M5 [3] trained on Fashion MNIST [56] with batch size 128. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "What is left, is the selection of the batch sizes $b_{k}$ . ", "page_idx": 17}, {"type": "text", "text": "B.1 Batch size distribution ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Since we plan to use the data set $\\begin{array}{r}{\\big(\\frac{1}{b_{k}},\\mathcal{L}^{(k)}\\big)_{k\\leq n}}\\end{array}$ for weighted least squares (WLS) regression and do not have a selection process for the batch sizes $b_{k}$ yet, it might be appropriate to select the batch sizes $b_{k}$ in such a way, that the variance of our estimator $\\hat{\\beta}_{0}$ of $\\beta_{0}$ is minimized. Here we choose $\\operatorname{Var}(\\hat{\\beta}_{0})$ and not $\\operatorname{Var}({\\hat{\\beta}}_{1})$ as our optimization target, since $\\beta_{0}=C(0)$ is used to fit the covariance model, while $\\beta_{1}=C_{\\epsilon}(0)$ is only required for S-RFD. Without deeper analysis $\\beta_{0}$ therefore seems to be more important. ", "page_idx": 17}, {"type": "text", "text": "Optimization over $n$ parameters $b_{k}$ is quite difficult, but we can simplify this optimization problem by considering the empirical batch size distribution ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nu_{n}=\\frac{1}{n}\\sum_{k=1}^{n}\\delta_{b_{k}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using a random variable $B$ distributed according to $\\nu_{n}$ , the total number of sample losses can then be expressed as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{n}b_{k}=n\\mathbb{E}[B]={\\mathrm{samples~used}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Under an (unrealistic) independence assumption, the variance $\\operatorname{Var}({\\hat{\\beta}}_{0})$ also has a simple representation in terms of $\\nu_{n}$ (Lemma B.2). We now want to minimize this variance subject to compute ", "page_idx": 17}, {"type": "text", "text": "constraint $\\alpha$ limiting the number of sample losses we can use resulting in the optimization problem ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{Var}(\\hat{\\beta}_{0})=\\frac{1}{n}\\underbrace{\\frac{1}{\\mathbb{E}[\\frac{1}{\\sigma_{B}^{2}}]}}_{\\mathrm{:variance~of~}Z_{B}}\\underbrace{\\frac{\\mathbb{E}[\\frac{1}{\\sigma_{B}^{2}}\\big(\\frac{1}{B}-\\mathbb{E}[\\frac{1}{B\\sigma_{B}^{2}\\mathbb{E}[1/\\sigma_{B}^{2}]}]\\big)^{2}\\Big]}{\\mathrm{inverse~of~the~\\hat{\\spread}~\\hat{\\b{\\sigma}}_{B}~}}}_{\\mathrm{inverse~of~the~\\hat{\\spread}~\\hat{\\b{\\sigma}}_{B}~}}\\quad\\mathrm{s.t.}\\quad n\\mathbb{E}[B]\\leq\\alpha.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we recall that $\\sigma_{B}^{2}$ is the variance of $Z_{B}$ . So the inverse of the expectation of its inverse is roughly the average variance of $Z_{B}$ . The second half is the fraction of a weighted second moment divided by the weighted variance. Unless the mean is at zero, the former will be larger. In particular we want a spread of data otherwise the variance would be zero. This is in some conflict with the variance of $Z_{B}$ . ", "page_idx": 18}, {"type": "text", "text": "But first, let us get rid of $n$ . Note that we would always increase $n$ until our compute budget is used up, since this always reduces variance. So we approximately have $n\\mathbb{E}[B]=\\alpha$ . Thus ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{Var}(\\hat{\\beta}_{0})=\\frac{\\mathbb{E}[B]}{\\alpha}\\frac{1}{\\mathbb{E}[\\frac{1}{\\sigma_{B}^{2}}]}\\frac{\\mathbb{E}[\\frac{1}{\\sigma_{B}^{2}B^{2}}]}{\\mathbb{E}\\Big[\\frac{1}{\\sigma_{B}^{2}}\\big(\\frac{1}{B}-\\mathbb{E}[\\frac{1}{B\\sigma_{B}^{2}\\mathbb{E}[1/\\sigma_{B}^{2}]}]\\big)^{2}\\Big]}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $\\alpha$ is now just resulting in a constant factor, it can be assumed to be 1 without loss of generality. Over batch size distributions $\\nu$ we therefore want to solve the minimization problem ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\nu}\\underbrace{\\frac{\\mathbb{E}[B]}{\\mathbb{E}[\\frac{1}{\\sigma_{B}^{2}}]}}_{\\mathrm{moments}}\\underbrace{\\frac{\\mathbb{E}\\big[\\frac{1}{\\sigma_{B}^{2}}\\big(\\frac{1}{B}-\\mathbb{E}[\\frac{1}{B\\sigma_{B}^{2}\\mathbb{E}[1/\\sigma_{B}^{2}]}\\big]\\big)^{2}\\big]}{\\mathrm{spread}}}_{\\mathrm{spread}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Example B.1 (If we did not require spread). If we were not concerned with the variance of batch sizes, we could select a constant $B=b$ . Then it is straightforward to minimize the moments factor manually ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{b}\\frac{\\mathbb{E}[b]}{\\mathbb{E}[\\frac{1}{\\sigma_{b}^{2}}]}=b\\sigma_{b}^{2}=2b(\\beta_{0}+\\frac{1}{b}\\beta_{1})^{2},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "resulting in $\\begin{array}{r}{\\frac{1}{b}=\\frac{\\beta_{0}}{\\beta_{1}}}\\end{array}$ . In other words: If we did not have to be concerned with the spread of $B$ there is one optimal selection to minimize the first factor. But in reality we have to trade-off this target with the spread of $B$ . ", "page_idx": 18}, {"type": "text", "text": "To ensure a good spread of data, we use the maximum entropy distribution for $B$ , with the moment constraints ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\mathbb{E}[-B]\\geq-\\frac{\\alpha}{n}\\qquad\\qquad}&&{\\mathrm{average~sample~usage}}\\\\ &{\\mathbb{E}[\\frac{1}{\\sigma_{B}^{2}}]\\geq\\theta}&&{\\,\\,\\,Z_{B}\\mathrm{~variance}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which capture the first factor. Maximizing entropy under moment constraints is known [26] to result in the Boltzmann (a.k.a. Gibbs) distribution ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nu(b)=\\mathbb{P}(B=b)\\propto\\exp\\Bigl(\\lambda_{1}\\frac{1}{\\sigma_{b}^{2}}-\\lambda_{2}b\\Bigr),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\lambda_{1},\\lambda_{2}$ depend on the momentum constraints. We can now forget the origin of this distribution and use $\\lambda_{1},\\lambda_{2}$ as parameters for the distribution $\\nu$ in Equation (9) to get close to its minimum. In practice we use a zero order black box optimizer (Nelder-Mead [17]). One could calculate the expectations of (9) under this distribution explicitly and take manual derivatives with respect to $\\lambda_{i}$ to investigate this further, but we wanted to avoid getting too distracted by this tangent. ", "page_idx": 18}, {"type": "text", "text": "We also use the estimated relative standard deviation ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{rel\\_std}=\\frac{\\sqrt{\\mathrm{Var}}(\\hat{\\beta}_{0})}{\\hat{\\beta}_{0}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "as a stopping criterion for sampling. Without extensive testing we found a tolerance of rel_std $<$ $\\mathrm{{tol}=0.3}$ to be reasonable, cf. Section A.1.1. ", "page_idx": 18}, {"type": "text", "text": "Lemma B.2 (Variance of ${\\hat{\\beta}}_{0}$ in terms of the empirical batch size distribution). Assuming independence of the samples $\\begin{array}{r}{\\big(\\big(\\frac{1}{b_{k}}\\big),Z_{b_{k}}\\big)_{k\\leq n}}\\end{array}$ , the variance of ${\\hat{\\beta}}_{0}$ is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{Var}(\\hat{\\beta}_{0})=\\frac{1}{n}\\frac{1}{\\mathbb{E}[\\frac{1}{\\sigma_{B}^{2}}]}\\frac{\\mathbb{E}[\\frac{1}{\\sigma_{B}^{2}B^{2}}]}{\\mathbb{E}\\Big[\\frac{1}{\\sigma_{B}^{2}}\\big(\\frac{1}{B}-\\mathbb{E}[\\frac{1}{B\\sigma_{B}^{2}\\mathbb{E}[1/\\sigma_{B}^{2}]}]\\big)^{2}\\Big]}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $B$ is distributed according to the empirical batch size distribution $\\begin{array}{r}{\\nu_{n}=\\frac{1}{n}\\sum_{k=1}^{n}\\delta_{b_{k}}}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. With the notation $\\sigma_{k}^{2}=\\sigma_{b_{k}}^{2}$ to describe the variance of $Z_{b_{k}}$ it follows from [cf. 28, Thm. 4.2] that the variance of the estimator $\\hat{\\beta}$ of $\\beta$ using $n$ samples is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{Var}(\\hat{\\beta})=(H^{T}C^{-1}H)^{-1}}&{}\\\\ &{\\quad\\quad=\\frac{1}{\\left(\\sum_{k}\\frac{1}{\\sigma_{k}^{2}}\\right)\\left(\\sum_{k}\\frac{1}{(\\sigma_{k}b_{k})^{2}}\\right)-(\\sum_{k}\\frac{1}{\\sigma_{k}^{2}b_{k}})^{2}}\\left(\\sum_{k}\\frac{1}{\\sigma_{k}^{2}b_{k}}\\right.}&{\\left.-\\sum_{k}\\frac{1}{\\sigma_{k}^{2}b_{k}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C:=\\left(\\begin{array}{l l l}{\\sigma_{1}^{2}}&&\\\\ &{\\cdot\\,.}&\\\\ &&{\\sigma_{n}^{2}}\\end{array}\\right)\\qquad H:=\\left(\\begin{array}{l l}{1}&{\\frac{1}{b_{1}}}\\\\ {\\vdots}&\\\\ {1}&{\\frac{1}{b_{n}}}\\end{array}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In particular we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{Var}(\\hat{\\beta}_{0})=\\frac{\\sum_{k}\\frac{1}{\\sigma_{k}^{2}b_{k}^{2}}}{\\left(\\sum_{k}\\frac{1}{\\sigma_{k}^{2}}\\right)\\left(\\sum_{k}\\frac{1}{\\sigma_{k}^{2}b_{k}^{2}}\\right)-\\left(\\sum_{k}\\frac{1}{\\sigma_{k}^{2}b_{k}}\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "With the help of $\\begin{array}{r}{\\theta:=\\sum_{j}\\frac{1}{\\sigma_{j}^{2}}}\\end{array}$ and $\\begin{array}{r}{\\lambda_{k}:=\\frac{1}{\\sigma_{k}^{2}\\theta}}\\end{array}$ , we can reorder the divisor. For this note that since the $\\lambda_{k}$ sum to 1 we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{k}\\lambda_{k}\\Big(\\frac{1}{b_{k}}-\\sum_{j}\\lambda_{j}\\frac{1}{b_{j}}\\Big)^{2}=\\sum_{k}\\lambda_{k}\\Big(\\frac{1}{b_{k}^{2}}-2\\frac{1}{b_{k}}\\sum_{j}\\lambda_{j}\\frac{1}{b_{j}}+\\Big(\\sum_{j}\\lambda_{j}\\frac{1}{b_{j}}\\Big)^{2}\\Big)}}\\\\ &{}&{=\\sum_{k}\\lambda_{k}\\frac{1}{b_{k}^{2}}-2\\Big(\\underset{k}{\\sum_{k}}\\lambda_{k}\\frac{1}{b_{k}}\\Big)+\\Big(\\underset{k}{\\sum_{k}}\\lambda_{k}\\frac{1}{b_{j}}\\Big)^{2}}\\\\ &{}&{=\\sum_{k}\\lambda_{k}\\frac{1}{b_{k}^{2}}-\\Big(\\underset{k}{\\sum_{k}}\\lambda_{k}\\frac{1}{b_{k}}\\Big)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Where the above is essentially the well known statement $\\mathbb{E}[(Y-\\mathbb{E}[Y])^{2}]=\\mathbb{E}[Y^{2}]-\\mathbb{E}[Y]^{2}$ for an appropriate selection of $Y$ . This implies that our divisor is given by a weighted variance ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\theta^{2}\\sum_{k}\\lambda_{k}\\Big(\\frac{1}{b_{k}}-\\sum_{j}\\lambda_{j}\\frac{1}{b_{j}}\\Big)^{2}=\\theta\\sum_{k}\\frac{1}{\\sigma_{k}^{2}b_{k}^{2}}-\\Big(\\sum_{k}\\frac{1}{\\sigma_{k}^{2}b_{k}}\\Big)^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where it is only necessary to plug in the definition of $\\theta$ to see the right term is exactly our divisor. Expanding both the enumerator as well as the divisor by $\\scriptstyle{\\frac{1}{n}}$ , we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{Var}(\\hat{\\beta}_{0})=\\frac{1}{\\theta}\\frac{\\frac{1}{n}\\sum_{k}\\frac{1}{\\sigma_{k}^{2}b_{k}^{2}}}{\\frac{1}{n}\\sum_{k}\\frac{1}{\\sigma_{k}^{2}}\\left(\\frac{1}{b_{k}}-\\sum_{j}\\lambda_{j}\\frac{1}{b_{j}}\\right)^{2}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\theta=n\\mathbb{E}[1/\\sigma_{B}^{2}]$ for $\\begin{array}{r}{B\\sim{\\frac{1}{n}}\\sum_{k=1}^{n}\\delta_{b_{k}}}\\end{array}$ and $\\begin{array}{r}{\\lambda_{k}=\\frac{1}{n\\sigma_{k}^{2}\\mathbb{E}[1/\\sigma_{B}^{2}]}}\\end{array}$ , the above can thus be written as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{Var}(\\hat{\\beta}_{0})=\\frac{1}{n}\\frac{1}{\\mathbb{E}[1/\\sigma_{B}^{2}]}\\frac{\\mathbb{E}[\\frac{1}{\\sigma_{B}^{2}B^{2}}]}{\\mathbb{E}\\Big[\\frac{1}{\\sigma_{B}^{2}}\\big(\\frac{1}{B}-\\mathbb{E}[\\frac{1}{B\\sigma_{B}^{2}\\mathbb{E}[1/\\sigma_{B}^{2}]}]\\big)^{2}\\Big]},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which proves our claim. ", "page_idx": 19}, {"type": "text", "text": "C Covariance models ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section we calculate the step sizes of the covariance models listed in Table 1 and plotted in Figure 2. Additionally we calculate the asymptotic learning rate of A-RFD and prove an Assumption of Corollary 5.3 for the squared exponential covariance (Prop. C.3). ", "page_idx": 20}, {"type": "text", "text": "C.1 Squared exponential ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The squared exponential covariance function is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C\\big(\\frac{\\|x-y\\|^{2}}{2}\\big)=\\sigma^{2}\\exp\\bigl(-\\frac{\\|x-y\\|^{2}}{2s^{2}}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that $\\sigma^{2}$ will play no role in the step sizes of RFD due to its scale invariance (cf. Advantage 2.3). Theorem C.1. Let $\\mathbf{J}\\sim{\\mathcal{N}}({\\boldsymbol{\\mu}},{\\boldsymbol{C}})$ where $C$ is the squared exponential covariance function (11), then we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\eta^{*}\\frac{\\nabla{\\bf J}(w)}{\\|\\nabla{\\bf J}(w)\\|}=\\underset{{\\bf d}}{\\operatorname{argmin}}\\,\\mathbb{E}[{\\bf J}(w-{\\bf d})\\mid{\\bf J}(w),\\nabla{\\bf J}(w)]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with RFD step size ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\eta^{*}=\\frac{s^{2}\\|\\nabla\\mathbf{J}(w)\\|}{\\sqrt{\\left(\\frac{\\mu-\\mathbf{J}(w)}{2}\\right)^{2}+s^{2}\\|\\nabla\\mathbf{J}(w)\\|^{2}}+\\frac{\\mu-\\mathbf{J}(w)}{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. The covariance function $C$ is of the form ", "page_idx": 20}, {"type": "equation", "text": "$$\nC(h)=\\sigma^{2}e^{-\\frac{h}{s^{2}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By Equation (2) ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\eta^{*}=-\\operatorname*{argmin}_{\\eta}\\frac{C(\\frac{\\eta^{2}}{2})}{C(0)}-\\eta\\frac{C^{\\prime}(\\frac{\\eta^{2}}{2})}{C^{\\prime}(0)}\\Theta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\begin{array}{r}{\\Theta=\\frac{\\|\\nabla\\mathbf{J}(w)\\|}{\\mu-\\mathbf{J}(w)}}\\end{array}$ . We calculate ", "page_idx": 20}, {"type": "equation", "text": "$$\n-\\frac{C\\big(\\frac{\\eta^{2}}{2}\\big)}{C(0)}-\\eta\\frac{C^{\\prime}\\big(\\frac{\\eta^{2}}{2}\\big)}{C^{\\prime}(0)}\\Theta=-e^{-\\frac{\\eta^{2}}{2s^{2}}}(1+\\eta\\Theta).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This results in the first order condition ", "page_idx": 20}, {"type": "equation", "text": "$$\n0\\stackrel{!}{=}\\frac{\\eta}{s^{2}}e^{-\\frac{\\eta^{2}}{2s^{2}}}(1+\\eta\\Theta)-e^{-\\frac{\\eta^{2}}{2s^{2}}}\\Theta=\\frac{e^{-\\frac{\\eta^{2}}{2s^{2}}}}{s^{2}}(\\eta^{2}\\Theta+\\eta-s^{2}\\Theta).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since the exponential can never be zero, we have to solve a quadratic equation. Its solution results in ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\eta^{*}(\\Theta)=\\sqrt{\\left(\\frac{1}{2\\Theta}\\right)^{2}+s^{2}}-\\frac{1}{2\\Theta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "At this point we could stop, but the result is numerically unstable as it suffers from catastrophic cancellation. To solve this issue we set $\\textstyle x={\\frac{1}{2\\Theta}}$ and reorder ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\eta^{*}=\\sqrt{x^{2}+s^{2}}-x=(\\sqrt{x^{2}+s^{2}}-x)\\frac{\\sqrt{x^{2}+s^{2}}+x}{\\sqrt{x^{2}+s^{2}}+x}=\\frac{x^{2}+s^{2}-x^{2}}{\\sqrt{x^{2}+s^{2}}+x}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Re-substituting x =21\u0398 = $\\begin{array}{r}{x=\\frac{1}{2\\Theta}=\\frac{\\mu-\\mathbf{J}(w)}{2\\|\\nabla\\mathbf{J}(w)\\|}}\\end{array}$ 2\u00b5\u2212JJ((ww)) , we finally get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\eta^{*}=\\frac{s^{2}}{\\sqrt{\\big(\\frac{\\mu-\\mathbf{J}(w)}{2\\|\\nabla\\mathbf{J}(w)\\|}\\big)^{2}+s^{2}}+\\frac{\\mu-\\mathbf{J}(w)}{2\\|\\nabla\\mathbf{J}(w)\\|}}=\\frac{s^{2}\\|\\nabla\\mathbf{J}(w)\\|}{\\sqrt{\\big(\\frac{\\mu-\\mathbf{J}(w)}{2}\\big)^{2}+s^{2}\\|\\nabla\\mathbf{J}(w)\\|^{2}}+\\frac{\\mu-\\mathbf{J}(w)}{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proposition C.2 (A-RFD for the Squared Exponential Covariance). If $\\mathbf{J}$ is isotropic with squared exponential covariance (11), then the step size of A-RFD is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{\\eta}=\\frac{s^{2}}{\\mu-\\mathbf{J}(w)}\\|\\nabla\\mathbf{J}(w)\\|,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. By Definition 5.1 of A-RFD and \u0398 = \u2225\u00b5\u2207JJ((ww))\u2225 we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\boldsymbol{\\hat{\\eta}}(\\Theta)=\\frac{C(0)}{-C^{\\prime}(0)}\\Theta=\\frac{\\sigma^{2}\\exp(0)}{\\frac{\\sigma^{2}}{s^{2}}\\exp(0)}\\frac{\\|\\nabla\\mathbf{J}(w)\\|}{\\mu-\\mathbf{J}(w)}=s^{2}\\frac{\\|\\nabla\\mathbf{J}(w)\\|}{\\mu-\\mathbf{J}(w)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proposition C.3. If J is isotropic with squared exponential covariance (11), then the RFD step sizes are strictly monotonously increasing in $\\Theta$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Since we know that $\\Theta\\to0$ implies $\\eta^{*}\\sim\\hat{\\eta}\\rightarrow0$ strict monotonicity of $\\eta^{*}$ in $\\Theta$ is sufficient to show that $\\eta^{*}\\to0$ also implies $\\Theta\\to0$ . So we take the derivative of (12) resulting in ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{d}{d\\Theta}\\eta^{*}=\\frac{1-\\frac{1}{\\sqrt{1+s^{2}(2\\Theta)^{2}}}}{2\\Theta^{2}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which is greater zero for all $\\Theta>0$ . ", "page_idx": 21}, {"type": "text", "text": "C.2 Rational quadratic ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The rational quadratic covariance function is given by ", "page_idx": 21}, {"type": "equation", "text": "$$\nC\\big(\\frac{\\|x-y\\|}{2}\\big)=\\sigma^{2}\\left(1+\\frac{\\|x-y\\|^{2}}{\\beta s^{2}}\\right)^{-\\beta/2}\\,\\,\\,\\,\\,\\,\\beta>0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It can be viewed as a scale mixture of the squared exponential and converges to the squared exponential in the limit $\\beta\\rightarrow\\infty$ [44, p. 87]. ", "page_idx": 21}, {"type": "text", "text": "Theorem C.4 (Rational Quadratic). For $\\mathbf{J}\\sim{\\mathcal{N}}({\\boldsymbol{\\mu}},{\\boldsymbol{C}})$ where $C$ is the rational quadratic covariance we have for $\\begin{array}{r}{\\Theta=\\frac{\\|\\nabla\\mathbf{J}(w)\\|}{\\mu-\\mathbf{J}(w)}\\geq0}\\end{array}$ that the RFD step size is given by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\eta^{*}=s\\sqrt{\\beta}\\operatorname{Root}\\left(-1+\\frac{\\sqrt{\\beta}}{s\\Theta}\\eta+(1+\\beta)\\eta^{2}+\\frac{\\sqrt{\\beta}}{s\\Theta}\\eta^{3}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The unique root of the polynomial in \u03b7 can be found either directly with a formula for polynomials of third degree (e.g. using Cardano\u2019s method) or by bisection as it is contained in $[\\dot{0},1/\\dot{\\sqrt{1+\\beta}}]$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. By Theorem 4.2 we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\eta^{*}=\\underset{\\eta}{\\operatorname{argmin}}-\\frac{C\\big(\\frac{\\eta^{2}}{2}\\big)}{C(0)}-\\eta\\frac{C^{\\prime}\\big(\\frac{\\eta^{2}}{2}\\big)}{C^{\\prime}(0)}\\Theta\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for $\\begin{array}{r}{C(x)=\\sigma^{2}(1+\\frac{2x}{\\beta s^{2}})^{-\\beta/2}}\\end{array}$ . We therefore need to minimize ", "page_idx": 21}, {"type": "equation", "text": "$$\nf\\big(\\frac{\\eta}{\\sqrt{\\beta}s}\\big):=-\\left(1+\\frac{\\eta^{2}}{\\beta s^{2}}\\right)^{-\\beta/2}-\\eta\\left(1+\\frac{\\eta^{2}}{\\beta s^{2}}\\right)^{-\\beta/2-1}\\Theta.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Substitute in $\\begin{array}{r}{\\tilde{\\eta}:=\\frac{\\eta}{\\sqrt{\\beta}s}}\\end{array}$ , then the first order condition is ", "page_idx": 21}, {"type": "equation", "text": "$$\n0\\overset{!}{=}\\mathstrut^{\\prime}(\\tilde{\\eta})=-\\frac{d}{d\\tilde{\\eta}}\\left(1+\\tilde{\\eta}^{2}\\right)^{-\\beta/2}+\\sqrt{\\beta}s\\tilde{\\eta}\\left(1+\\tilde{\\eta}^{2}\\right)^{-\\beta/2-1}\\Theta\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Dividing both sides by $\\sqrt{\\beta}s\\Theta$ we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0=\\frac{f^{\\prime}(\\tilde{\\eta})}{\\sqrt{\\beta}s\\Theta}=\\frac{\\beta}{2}(1+\\tilde{\\eta}^{2})^{-\\frac{\\beta}{2}-1}2\\tilde{\\eta}\\frac{1}{\\sqrt{\\beta}s\\Theta}+(1+\\tilde{\\eta}^{2})^{-\\frac{\\beta}{2}-2}\\left[1+\\tilde{\\eta}^{2}-(\\frac{\\beta}{2}+1)2\\tilde{\\eta}^{2}\\right]}\\\\ &{\\qquad\\qquad=(1+\\tilde{\\eta}^{2})^{-\\frac{\\beta}{2}-2}\\underbrace{\\left[\\beta\\tilde{\\eta}\\frac{1}{\\sqrt{\\beta}s\\Theta}(1+\\tilde{\\eta}^{2})-\\left[1-\\tilde{\\eta}^{2}(1+\\beta)\\right]\\right]}_{=-1+\\frac{\\sqrt{\\beta}}{s\\Theta}\\tilde{\\eta}+(1+\\beta)\\tilde{\\eta}^{2}+\\frac{\\sqrt{\\beta}}{s\\Theta}\\tilde{\\eta}^{3}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $\\Theta\\ge0$ and $\\beta\\,>\\,0$ all coefficients of the polynomial are positive except for the shift. The polynomial thus starts out at $-1$ in zero and only increases from there. Therefore there exists a unique positive critical point which is a minimum. ", "page_idx": 21}, {"type": "text", "text": "At the point $\\tilde{\\eta}=\\sqrt{1+\\beta}$ the quadratic term is already larger than 1 so the polynomial is positive and we have passed the root. The minimum is therefore contained in the interval $[0,\\sqrt{1+\\beta}]$ . ", "page_idx": 21}, {"type": "text", "text": "After finding the minimum in $\\tilde{\\eta}$ we return to $\\eta$ by multiplication with $\\sqrt{\\beta}s$ ", "page_idx": 21}, {"type": "text", "text": "Proposition C.5 (A-RFD for the Rational Quadratic Covariance). If J is isotropic with rational quadratic covariance (13), then the step size of A-RFD is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{\\eta}=\\frac{s^{2}}{\\mu-\\mathbf{J}(w)}\\|\\nabla\\mathbf{J}(w)\\|.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. $\\begin{array}{r}{C(x)=\\sigma^{2}(1+\\frac{2x}{\\beta s^{2}})^{-\\beta/2}}\\end{array}$ implies by Definition 5.1 of A-RFD and $\\begin{array}{r}{\\Theta=\\frac{\\|\\nabla\\mathbf{J}(w)\\|}{\\mu-\\mathbf{J}(w)}}\\end{array}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{\\eta}(\\Theta)=\\frac{C\\mathbf{J}(0)}{-C_{\\mathbf{J}}^{\\prime}(0)}\\Theta=\\frac{\\sigma^{2}(1+0)^{-\\beta/2}}{\\frac{\\sigma^{2}}{s^{2}}(1+0)^{-\\beta/2-1}}\\frac{\\|\\nabla\\mathbf{J}(x)\\|}{\\mu-\\mathbf{J}(x)}=s^{2}\\frac{\\|\\nabla\\mathbf{J}(x)\\|}{\\mu-\\mathbf{J}(x)}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "C.3 Mat\u00e9rn ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Definition C.6. The Mat\u00e9rn model parametrized by $s>0,\\nu\\geq0,\\sigma^{2}\\geq0$ is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\nC\\big(\\frac{\\|x-y\\|^{2}}{2}\\big)=\\sigma^{2}\\frac{2^{1-\\nu}}{\\Gamma(\\nu)}\\left(\\frac{\\sqrt{2\\nu}\\|x-y\\|}{s}\\right)^{\\nu}K_{\\nu}\\left(\\frac{\\sqrt{2\\nu}\\|x-y\\|}{s}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $K_{\\nu}$ is the modified Bessel function. ", "page_idx": 22}, {"type": "text", "text": "For $\\begin{array}{r}{\\nu=p+\\frac12}\\end{array}$ with $p\\in\\ensuremath{\\mathbb{N}}_{0}$ , it can be simplified [cf. 44, sec. 4.2.1] to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C\\big(\\frac{\\|x-y\\|^{2}}{2}\\big)=\\sigma^{2}e^{-\\frac{\\sqrt{2\\nu}\\|x-y\\|}{s}}\\frac{p!}{(2p)!}\\sum_{k=0}^{p}\\frac{(2p-k)!}{(p-k)!k!}\\left(\\frac{2\\sqrt{2\\nu}}{s}\\|x-y\\|\\right)^{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The Mat\u00e9rn model encompasses Rasmussen and Williams [44] \u2022 the nugget effect for $\\nu=0$ (independent randomness) \u2022 the exponential model for $\\nu=\\textstyle{\\frac{1}{2}}$ (Ornstein-Uhlenbeck process) \u2022 the squared exponential model for $\\nu\\to\\infty$ with the same scale $s$ and variance $\\sigma^{2}$ . ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "The random functions induced by the Mat\u00e9rn model are a.s. $\\lfloor\\nu\\rfloor$ -times differentiable Rasmussen and Williams [44], i.e. the smoothness of the model increases with increasing $\\nu$ . While the exponential covariance model with $\\nu\\,=\\,\\textstyle{\\frac{1}{2}}$ results in a random function which is not yet differentiable, larger $\\nu$ result in increasing differentiability. As differentiability starts with $\\begin{array}{r}{\\nu=\\frac{3}{2}}\\end{array}$ and we have a more explicit formula for $\\begin{array}{r}{\\nu=p+\\frac{1}{2}}\\end{array}$ the cases $\\begin{array}{r}{\\nu=\\frac{3}{2}}\\end{array}$ and $\\begin{array}{r}{\\nu=\\frac{5}{2}}\\end{array}$ are of particular interest. ", "page_idx": 22}, {"type": "text", "text": "\u201c[F]or $\\nu\\geq7/2$ , in the absence of explicit prior knowledge about the existence of higher order derivatives, it is probably very hard from finite noisy training examples to distinguish between values of $\\nu\\geq7/2$ (or even to distinguish between finite values of $\\nu$ and $\\nu\\to\\infty$ , the smooth squared exponential, in this case)\u201d [44, p. 85]. ", "page_idx": 22}, {"type": "text", "text": "Theorem C.7. Assuming $\\mathbf{J}\\sim{\\mathcal{N}}({\\boldsymbol{\\mu}},{\\boldsymbol{C}})$ is a random function where $C$ is the Mat\u00e9rn covariance such that $\\begin{array}{r}{\\nu=p+\\frac{1}{2}\\,w i t h\\,p\\in\\{1,2\\}}\\end{array}$ . Then the RFD step is given for $\\begin{array}{r}{\\Theta:=\\frac{\\|\\nabla\\mathbf{J}(w)\\|}{\\mu-\\mathbf{J}(w)}\\geq0}\\end{array}$ by ", "page_idx": 22}, {"type": "text", "text": "$p=1$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\eta^{*}=\\frac{s}{\\sqrt{3}}\\frac{1}{\\left(1+\\frac{\\sqrt{3}}{s\\Theta}\\right)}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "\u2022 $p=2$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\eta^{\\ast}=\\frac{s}{\\sqrt{5}}\\frac{(1-\\zeta)+\\sqrt{4+(1+\\zeta)^{2}}}{2(1+\\zeta)}\\qquad\\zeta:=\\frac{\\sqrt{5}}{3s\\Theta}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We define $\\begin{array}{r}{\\mathcal{C}(\\eta):=C(\\frac{\\eta^{2}}{2})}\\end{array}$ , which implie ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{C}^{\\prime}(\\eta)=C^{\\prime}\\big(\\frac{\\eta^{2}}{2}\\big)\\eta}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "or conversely ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C^{\\prime}\\big(\\frac{\\eta^{2}}{2}\\big)=\\frac{1}{\\eta}\\mathcal{C}^{\\prime}(\\eta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By Theorem 4.2, we need to calculate ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\eta^{*}=\\underset{\\eta}{\\operatorname{argmin}}-\\frac{C(\\frac{\\eta^{2}}{2})}{C(0)}-\\eta\\frac{C^{\\prime}(\\frac{\\eta^{2}}{2})}{C^{\\prime}(0)}\\Theta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Discarding $\\sigma$ w.l.o.g. due to scale invariance (Advantage 2.3), we have in the case $p=1$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{C}(\\eta)=\\Big(1+\\frac{\\sqrt{3}}{s}\\eta\\Big)\\exp{\\!\\Big(\\!-\\!\\frac{\\sqrt{3}}{s}\\eta\\Big)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The derivative is then given by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{C}^{\\prime}(\\eta)=-\\big(\\frac{\\sqrt{3}}{s}\\big)^{2}\\eta\\exp\\bigl(-\\frac{\\sqrt{3}}{s}\\eta\\bigr)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which implies using (15) ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C^{\\prime}\\big(\\frac{\\eta^{2}}{2}\\big)=-\\big(\\frac{\\sqrt{3}}{s}\\big)^{2}\\exp\\!\\left(-\\frac{\\sqrt{3}}{s}\\eta\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We therefore need to minimize (16) which is given by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\eta}{\\operatorname{argmin}}-\\big(1+\\frac{\\sqrt{3}}{s}\\eta\\big)\\exp\\bigl(-\\frac{\\sqrt{3}}{s}\\eta\\bigr)-\\eta\\exp\\bigl(-\\frac{\\sqrt{3}}{s}\\eta\\bigr)\\Theta=\\underset{\\eta}{\\operatorname{argmin}}-\\bigl(1+(\\frac{\\sqrt{3}}{s}+\\Theta)\\eta\\bigr)\\exp\\bigl(-\\frac{\\sqrt{3}}{s}\\eta\\bigr)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The first order condition is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{0\\stackrel{!}{=}\\left(\\frac{\\sqrt{3}}{s}(\\chi+(\\frac{\\sqrt{3}}{s}+\\Theta)\\eta)-(\\frac{\\sqrt{3}}{s}+\\Theta)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which (divided by $\\Theta$ and noting that the exponential can never be zero) is equivalent to ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{0\\stackrel{!}{=}\\frac{\\sqrt{3}}{s}(\\frac{\\sqrt{3}}{s\\Theta}+1)\\eta-1}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "reordering for $\\eta$ implies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\eta\\stackrel{!}{=}\\frac{s}{\\sqrt{3}}\\frac{1}{\\left(1+\\frac{\\sqrt{3}}{s\\Theta}\\right)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "It is also not difficult to see that this is the point where the derivative switches from negative to positive (i.e. a minimum). ", "page_idx": 23}, {"type": "text", "text": "Let us now consider the case $p=2$ , i.e. ", "page_idx": 23}, {"type": "text", "text": "which results in ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{C}(\\eta)=\\left(1+\\frac{\\sqrt{5}}{s}\\eta+\\frac{5}{3s^{2}}\\eta^{2}\\right)\\exp\\!\\left(-\\frac{\\sqrt{5}}{s}\\eta\\right)\\!,}\\\\ {\\mathcal{C}^{\\prime}(\\eta)=-\\frac{5}{3s^{2}}\\big(\\eta+\\frac{\\sqrt{5}}{s}\\eta^{2}\\big)\\exp\\!\\left(-\\frac{\\sqrt{5}}{s}\\eta\\right)\\!,}\\\\ {C^{\\prime}\\big(\\frac{\\eta^{2}}{2}\\big)=-\\frac{5}{3s^{2}}\\big(1+\\frac{\\sqrt{5}}{s}\\eta\\big)\\exp\\!\\left(-\\frac{\\sqrt{5}}{s}\\eta\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We therefore need to minimize (16) which is given by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underbrace{\\left(-\\left(1+\\frac{\\sqrt{5}}{s}\\eta+\\frac{5}{3s^{2}}\\eta^{2}\\right)-\\eta\\big(1+\\frac{\\sqrt{5}}{s}\\eta\\big)\\Theta\\right)}_{=-\\left(1+\\left(\\frac{\\sqrt{5}}{s}+\\Theta\\right)\\eta+\\left(\\frac{5}{3s^{2}}+\\frac{\\sqrt{5}}{s}\\Theta\\right)\\eta^{2}\\right)}\\exp\\!\\left(-\\frac{\\sqrt{5}}{s}\\eta\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The first order condition results in ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\overset{!}{=}\\frac{\\sqrt{5}}{s}\\Big(\\dot{\\chi}+\\big(\\frac{\\sqrt{5}}{s}+\\Theta\\big)\\eta+\\big(\\frac{5}{3s^{2}}+\\frac{\\sqrt{5}}{s}\\Theta\\big)\\eta^{2}\\Big)-\\Big(\\big(\\frac{\\sqrt{8}}{s}+\\Theta\\big)+2\\big(\\frac{5}{3s^{2}}+\\frac{\\sqrt{5}}{s}\\Theta\\big)\\eta\\Big)}\\\\ &{\\quad=-\\Theta+\\big(\\frac{5}{3s^{2}}-\\frac{\\sqrt{5}}{s}\\Theta\\big)\\eta+\\frac{\\sqrt{5}}{s}\\big(\\frac{5}{3s^{2}}+\\frac{\\sqrt{5}}{s}\\Theta\\big)\\eta^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Dividing everything by $\\Theta$ and using $\\textstyle\\zeta:={\\frac{\\sqrt{5}}{3s\\Theta}}$ we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{0\\stackrel{!}{=}-1-\\big(\\zeta-1\\big)\\big(\\frac{\\sqrt{5}}{s}\\eta\\big)+\\big(\\zeta+1\\big)\\big(\\frac{\\sqrt{5}}{s}\\eta\\big)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Taking a closer look at the sign changes of the derivative it becomes obvious, that the positive root is the minimum, i.e. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\sqrt{5}}{s}\\eta\\stackrel{!}{=}\\frac{(1-\\zeta)+\\sqrt{(1-\\zeta)^{2}+4(1+\\zeta)}}{2(1+\\zeta)}=\\frac{(1-\\zeta)+\\sqrt{4+(1+\\zeta)^{2}}}{2(1+\\zeta)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proposition C.8 (A-RFD for the Mat\u00e9rn Covariance). If J is isotropic with Mat\u00e9rn covariance (14) such that $\\begin{array}{r}{\\nu=p+\\frac{1}{2}}\\end{array}$ , then the step size of A-RFD for $p\\in\\{1,2\\}$ is given by ", "page_idx": 24}, {"type": "equation", "text": "$$\np=1\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{\\eta}=\\frac{s^{2}}{3}\\frac{\\|\\nabla{\\mathbf J}(x)\\|}{\\mu-{\\mathbf J}(x)}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "\u2022 p = 2 ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{\\eta}=\\frac{3s^{2}}{5}\\frac{\\|\\nabla{\\mathbf J}(x)\\|}{\\mu-{\\mathbf J}(x)}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Noting \u0398 = \u2225\u2207J(x)\u2225 , we have by Definition 5.1 of A-RFD for $p=1$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{\\eta}=\\frac{C(0)}{-C^{\\prime}(0)}\\Theta\\stackrel{(17)}{=}\\frac{s^{2}}{3}\\Theta,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and in the case $p=2$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{\\eta}=\\frac{C(0)}{-C^{\\prime}(0)}\\Theta\\stackrel{(18)}{=}\\frac{3s^{2}}{5}\\Theta.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "D Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section we prove all the claims made in the main body. ", "page_idx": 24}, {"type": "text", "text": "D.1 Section 2: Random function descent ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "D.1.1 Formal RFD ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "As we mentioned in a footnote at the definition of RFD, the fact that the parameters become random variables as they are selected by random gradients poses some mathematical challenges which would have been distracting to address in the main body. In following paragraphs leading up to Definition D.1 we introduce and discuss the probability theory required to provide a mathematically sound definition. ", "page_idx": 24}, {"type": "text", "text": "For a fixed cost distribution $\\mathbb{P}_{\\mathbf{J}}$ and any weight vectors $w$ and $\\tilde{w}$ the conditional distribution ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{J}(\\tilde{w})\\mid\\mathbf{J}(w),\\nabla\\mathbf{J}(w)]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "is by its axiomatic definition a $(\\mathbf{J}(w),\\nabla\\mathbf{J}(w))$ -measurable random variable. By the factorization lemma [30, Cor. 1.9.7], there therefore exists a measurable function $(j,g)\\mapsto\\varphi_{w,\\tilde{w}}(j,g)$ such that the following equation holds almost surely ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\varphi_{w,\\theta}(\\mathbf{J}(w),\\nabla\\mathbf{J}(w))=\\mathbb{E}[\\mathbf{J}(\\tilde{w})\\mid\\mathbf{J}(w),\\nabla\\mathbf{J}(w)].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since it is possible to calculate $\\varphi_{w,\\tilde{w}}$ explicitly in the Gaussian case (cf. G.1), the function ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Phi_{\\mathbb{P}_{\\mathbf{J}}}:\\left\\{(\\mathbb{R}^{d}\\times\\mathbb{R}\\times\\mathbb{R}^{d})\\to\\mathbb{R}\\right.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which implements some tie-breaker rules for set valued argmin is measurable when $\\mathbf{J}$ is Gaussian and its covariance function is sufficiently smooth. To prove measurability in the general case is a difficult problem of its own, which we do not attempt to solve here, since we would not utilize the conditional expectation outside of the Gaussian case anyway (cf. Section E.3). For deterministic $w$ , we therefore have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Phi_{\\mathbb{P}_{\\mathbf{J}}}(w,\\mathbf{J}(w),\\nabla\\mathbf{J}(w))=\\underset{\\Tilde{w}}{\\mathrm{argmin}}\\,\\varphi_{w,\\Tilde{w}}(\\mathbf{J}(w),\\nabla\\mathbf{J}(w))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\stackrel{(19)}{=}\\underset{\\Tilde{w}}{\\mathrm{argmin}}\\,\\mathbb{E}[\\mathbf{J}(\\Tilde{w})\\mid\\mathbf{J}(w),\\nabla\\mathbf{J}(w)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "So if the parameter vectors $w_{n}$ were deterministic, our formal definition of RFD and our initial definition would coincide. But for random weights $W$ (19) stops to hold in general3, i.e. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\varphi_{W,\\tilde{w}}(\\mathbf{J}(W),\\nabla\\mathbf{J}(W))\\neq\\mathbb{E}[\\mathbf{J}(\\tilde{w})\\mid\\mathbf{J}(W),\\nabla\\mathbf{J}(W)].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "If this equation does not need to hold, we similarly have in general ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Phi_{\\mathbb{P}_{\\mathbf{J}}}(W,\\mathbf{J}(W),\\nabla\\mathbf{J}(W))\\neq\\operatorname*{argmin}_{\\tilde{w}}\\mathbb{E}[\\mathbf{J}(\\tilde{w})\\mid\\mathbf{J}(W),\\nabla\\mathbf{J}(W)].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "So the following definition is not just a restatement of the original definition of RFD. ", "page_idx": 25}, {"type": "text", "text": "Definition D.1 (Formal RFD). For a Gaussian random cost function $\\mathbf{J}$ , we define the RFD algorithm with starting point $W_{0}=w_{0}\\in\\mathbb{R}^{d}$ by ", "page_idx": 25}, {"type": "equation", "text": "$$\nW_{n+1}:=\\Phi_{\\mathbb{P}_{\\mathbf{J}}}(W_{n},\\mathbf{J}(W_{n}),\\nabla\\mathbf{J}(W_{n}))\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This is what we effectively do in Theorem 4.2 under the additional isotropy assumption, where we calculate the argmin under the assumption that $w$ is deterministic (i.e. we determine $\\Phi_{\\mathbb{P}_{\\mathbf{J}}}$ ), before we plug-in the random variables $W_{n}$ to obtain $W_{n+1}$ . Similarly this is how the step size prescriptions of RFD actually work. We first assume deterministic weights and later plug the random variables into our formulas. For this reason, we avoided large letters indicating random variables for parameters $w$ in the main body. ", "page_idx": 25}, {"type": "text", "text": "D.1.2 Scale invariance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Advantage 2.3 (Scale invariance). RFD is invariant to additive shifts and positive scaling of the cost J. RFD is also invariant with respect to transformations of the parameter input of $\\mathbf{J}$ by differentiable bijections whose Jacobian is invertible everywhere (e.g. invertible linear maps). ", "page_idx": 25}, {"type": "text", "text": "Before we get to the proof, let us quickly formulate the statement in mathematical terms. Let $w_{n}$ be the parameters selected optimizing $\\mathbf{J}$ starting in $w_{0}$ and $\\tilde{w}_{n}$ the parameters selected by the same optimizer optimizing $\\tilde{\\mathbf{J}}$ starting in $\\tilde{w}_{0}$ . ", "page_idx": 25}, {"type": "text", "text": "If we apply affine linear scaling to cost $\\mathbf{J}$ such that $\\tilde{\\mathbf{J}}(w)=a\\mathbf{J}(w)+b$ and start optimization in the same point, i.e. $w_{0}=\\tilde{w}_{0}$ , then we expect a scale invariant optimizer to select ", "page_idx": 25}, {"type": "equation", "text": "$$\nw_{n}=\\tilde{w}_{n}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "If we scale inputs on the other hand (or more generally map them with a bijection $\\phi$ ), then we expect for $\\tilde{\\mathbf{J}}:=\\mathbf{J}\\circ\\bar{\\phi}$ and starting point $\\tilde{w}_{0}=\\phi^{-1}(w_{0})$ , that this relationship is retained by a scale invariant optimizer, i.e. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tilde{w}_{n}=\\phi^{-1}(w_{n}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Why do we use a different starting point? As an illustrating example, assume that $\\phi$ maps miles into kilometers. Then $\\tilde{\\mathbf{J}}$ accepts miles, while $\\mathbf{J}$ accepts kilometers. Then we have to map the initial starting point $w_{0}$ of $\\mathbf{J}$ measured in kilometers into miles $\\tilde{w}_{0}$ . $\\phi^{-1}$ is precisely this transformation from kilometers into miles. A scale invariant optimizer should retain this relation, i.e. no matter if the input is measured in miles or kilometers the same points are selected. ", "page_idx": 25}, {"type": "text", "text": "Proof. The following proof will be split into three parts. The first two parts of the proof will address a more general audience and ignore the mathematical subtleties we discussed in Section D.1.1. In the third part we explain to the interested probabilists how to resolve these issues. ", "page_idx": 25}, {"type": "text", "text": "1. Invariance with regard to affine linear scaling ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Let $\\tilde{\\mathbf{J}}(w):=a\\mathbf{J}(w)+b$ where $a>0$ and $b\\in\\mathbb{R}$ and assume $\\tilde{w}_{0}\\,=\\,w_{0}$ . With the induction start given, we only require the induction step to prove $\\tilde{w}_{n}=w_{n}$ . ", "page_idx": 25}, {"type": "equation", "text": "$$\nW=\\left(\\mathrm{argmin}\\,\\mathbf{J}\\right)\\mathbf{1}_{\\mathbf{J}(\\tilde{w})>0}+\\left(\\mathrm{argmax}\\,\\mathbf{J}\\right)\\mathbf{1}_{\\mathbf{J}(\\tilde{w})<0}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In this case, $\\mathbf{J}(W)$ is much more informative of ${\\bf J}(\\tilde{w})$ than $\\mathbf{J}(w)$ at some deterministic $w$ . ", "page_idx": 25}, {"type": "text", "text": "For the induction step, we assume this equation holds up to $n$ . Since $\\phi(x)=a x+b$ is a measurable bijection, the sigma algebra4 generated by ", "page_idx": 26}, {"type": "equation", "text": "$$\n(\\tilde{\\mathbf{J}}(w_{n}),\\nabla\\tilde{\\mathbf{J}}(w_{n}))=(\\phi\\circ\\mathbf{J}(w_{n}),a\\nabla\\mathbf{J}(w_{n}))\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "is therefore equal to the sigma algebra generated by $(\\mathbf{J}(w_{n}),\\nabla\\mathbf{J}(w_{n}))$ . This implies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{w}_{n+1}=\\underset{w}{\\mathrm{argmin}}\\,\\mathbb{E}[\\tilde{\\mathbf{J}}(w)\\mid\\tilde{\\mathbf{J}}(\\bar{w}_{n}),\\nabla\\tilde{\\mathbf{J}}(\\bar{w}_{n})]}\\\\ &{\\quad\\overset{\\mathrm{indeal~sorpmin}}{=}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Where we have used the linearity of the conditional expectation and the strict monotonicity of $\\phi(x)=a x+b$ . ", "page_idx": 26}, {"type": "text", "text": "2. Invariance with regard to certain input bijections ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Let $\\phi$ be a differentiable bijection whose jacobian is invertible everywhere and assume $\\tilde{\\mathbf{J}}:=\\mathbf{J}\\circ\\phi$ .   \nSince $\\phi$ is a bijection, $\\phi(M)$ is the domain of $\\mathbf{J}$ whenever $M$ is the domain of $\\tilde{\\mathbf{J}}$ . ", "page_idx": 26}, {"type": "text", "text": "For a starting point $w_{0}\\in\\phi(M)$ we now assume $\\tilde{w}_{0}=\\phi^{-1}(w_{0})\\in M$ and are again going to prove the claim ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\tilde{w}_{n}=\\phi^{-1}(w_{n}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "by induction. Assume that we have this claim up to $n$ . Then we have by induction ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{J}}(\\tilde{w}_{n})=\\mathbf{J}\\circ\\phi(\\phi^{-1}(w_{n}))=\\mathbf{J}(w_{n})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\nabla\\tilde{\\mathbf{J}}(\\tilde{w}_{n})=\\nabla_{\\tilde{w}_{n}}(\\mathbf{J}\\circ\\phi(\\tilde{w}_{n}))=\\phi^{\\prime}(\\tilde{w}_{n})(\\nabla\\mathbf{J})(\\phi(\\tilde{w}_{n}))=\\phi^{\\prime}(\\tilde{w}_{n})\\nabla\\mathbf{J}(w_{n}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since $\\phi^{\\prime}(\\tilde{w}_{n})$ is invertible by assumption, the sigma algebras generated by $(\\tilde{\\mathbf{J}}(\\tilde{w}_{n}),\\nabla\\tilde{\\mathbf{J}}(\\tilde{w}_{n}))$ and ${\\bf J}(w_{n}),\\nabla{\\bf J}(w_{n})$ are identical. But this results in the induction step ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{w}_{n+1}=\\underset{w\\in\\rightharpoondown{H}}{\\mathrm{argmin}}\\mathbb{E}[\\hat{\\mathbf{J}}(w)\\mid\\hat{\\mathbf{J}}(\\tilde{w}_{n}),\\nabla\\hat{\\mathbf{J}}(\\tilde{w}_{n})]}\\\\ &{\\quad\\underset{=\\varphi\\in\\rightharpoonup{H}}{\\mathrm{sigma}}}\\\\ &{\\qquad\\stackrel{\\mathrm{sigma}\\,\\mathrm{argmin}}{=}\\mathbb{E}[\\hat{\\mathbf{J}}(w)\\mid\\mathbf{J}(w_{n}),\\nabla\\mathbf{J}(w_{n})]}\\\\ &{\\qquad\\stackrel{\\mathrm{def.}}{=}\\underset{w\\in\\rightharpoonup{M}}{\\mathrm{argmin}}\\mathbb{E}[\\mathbf{J}\\circ\\phi(w)\\mid\\mathbf{J}(w_{n}),\\nabla\\mathbf{J}(w_{n})]}\\\\ &{\\qquad=\\phi^{-1}\\underset{\\underbrace{\\theta\\in\\rightharpoonup(M)}}{\\mathrm{argmin}}\\mathbb{E}[\\mathbf{J}(\\theta)\\mid\\mathbf{J}(w_{n}),\\nabla\\mathbf{J}(w_{n})]\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we simply optimize over $\\theta=\\phi(w)$ instead of $w$ and correct the argmin at the end. ", "page_idx": 26}, {"type": "text", "text": "3. Addressing the subtleties ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In equation (20) we have really proven for deterministic $w$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Phi_{\\mathbb{P}_{\\mathbf{\\tilde{J}}}}(w,\\tilde{\\mathbf{J}}(w),\\nabla\\tilde{\\mathbf{J}}(w))=\\Phi_{\\mathbb{P}_{\\mathbf{J}}}(w,\\mathbf{J}(w),\\nabla\\mathbf{J}(w)).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "But this implies with the induction assumption $W_{n}=\\tilde{W}_{n}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\tilde{W}_{n+1}=\\Phi_{\\mathbb{P}_{\\mathbb{J}}}(\\tilde{W}_{n},\\tilde{\\mathbf{J}}(\\tilde{W}_{n}),\\nabla\\tilde{\\mathbf{J}}(\\tilde{W}_{n}))\\overset{\\mathrm{ind.}}{=}\\Phi_{\\mathbb{P}_{\\mathbb{J}}}(W_{n},\\mathbf{J}(W_{n}),\\nabla\\mathbf{J}(W_{n}))=W_{n+1}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "4if you are unfamiliar with sigma algebras read them as \u201cinformation\u201d. ", "page_idx": 26}, {"type": "text", "text": "Similarly we have proven in (22) that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Phi_{\\mathbb{P}_{\\mathbb{J}}}\\big(\\phi^{-1}(w),\\tilde{\\mathbf{J}}(\\phi^{-1}(w)),\\nabla\\tilde{\\mathbf{J}}(\\phi^{-1}(w))\\big)=\\phi^{-1}\\big(\\Phi_{\\mathbb{P}_{\\mathbb{J}}}(w,\\mathbf{J}(w),\\nabla\\mathbf{J}(w))\\big).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By the induction assumption $\\tilde{W}=\\phi^{-1}(W_{n})$ , this implies ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{W}_{n+1}=\\Phi_{\\mathbb{P}_{\\tilde{\\mathbf{J}}}}(\\tilde{W}_{n},\\tilde{\\mathbf{J}}(\\tilde{W}_{n}),\\nabla\\tilde{\\mathbf{J}}(\\tilde{W}_{n}))}\\\\ &{\\quad\\quad\\overset{\\mathrm{ind.}}{=}\\Phi_{\\mathbb{P}_{\\tilde{\\mathbf{J}}}}\\left(\\phi^{-1}(W_{n}),\\tilde{\\mathbf{J}}(\\phi^{-1}(W_{n})),\\nabla\\tilde{\\mathbf{J}}(\\phi^{-1}(W_{n}))\\right)}\\\\ &{\\quad\\quad=\\phi^{-1}\\bigl(\\Phi_{\\mathbb{P}_{\\mathbf{J}}}(W_{n},\\mathbf{J}(W_{n}),\\nabla\\mathbf{J}(W_{n}))\\bigr)}\\\\ &{\\quad\\quad=\\phi^{-1}(W_{n+1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "D.2 Section 4: Relation to gradient descent ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Lemma 4.1 (Explicit first order stochastic Taylor approximation). For $\\mathbf{J}\\sim{\\mathcal{N}}({\\boldsymbol{\\mu}},{\\boldsymbol{C}})$ , the first order stochastic Taylor approximation is given by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{J}(w-\\mathbf{d})\\mid\\mathbf{J}(w),\\nabla\\mathbf{J}(w)]=\\mu+{\\frac{C{\\bigl(}{\\frac{\\|\\mathbf{d}\\|^{2}}{2}}{\\bigr)}}{C(0)}}(\\mathbf{J}(w)-\\mu)-{\\frac{C^{\\prime}{\\bigl(}{\\frac{\\|\\mathbf{d}\\|^{2}}{2}}{\\bigr)}}{C^{\\prime}(0)}}\\langle\\mathbf{d},\\nabla\\mathbf{J}(w)\\rangle.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. $(\\mathbf{J}(w),\\nabla\\mathbf{J}(w),\\mathbf{J}(w-\\mathbf{d}))$ is a Gaussian vector for which the conditional distribution is well known. It is only necessary to calculate the covariance matrix. The key ingredient here is to observe that $\\mathbf{J}(w),\\partial_{1}\\mathbf{J}(w),\\dots,\\partial_{d}\\mathbf{J}(w)$ are all independent, trivializing matrix inversion. ", "page_idx": 27}, {"type": "text", "text": "More formally, by Lemma G.2 we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname{Cov}\\!\\Big(\\!\\left(\\!\\!\\begin{array}{l}{\\mathbf{J}(w)}\\\\ {\\nabla\\mathbf{J}(w)}\\end{array}\\!\\!\\right)\\!\\Big)=\\left(\\!\\!\\begin{array}{l l}{C(0)}&{}\\\\ {}&{-C^{\\prime}(0)\\mathbb{I}_{d\\times d}\\!\\!}\\end{array}\\!\\!\\right)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Cov}\\Big(\\mathbf{J}(w-\\mathbf{d}),\\left(\\frac{\\mathbf{J}(w)}{\\nabla\\mathbf{J}(w)}\\right)\\Big)=\\left(\\begin{array}{l}{C(\\frac{\\|\\mathbf{d}\\|^{2}}{2})}\\\\ {C^{\\prime}(\\frac{\\|\\mathbf{d}\\|^{2}}{2})\\mathbf{d}}\\end{array}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By Theorem G.1 we therefore know that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{J}(w-\\mathbf{d})\\mid\\mathbf{J}(w),\\nabla\\mathbf{J}(w)]=\\mu+\\left(C(\\frac{\\|\\mathbf{d}\\|^{2}}{2})\\right)^{T}\\left({C}(0)\\quad-C^{\\prime}(0)\\mathbb{I}_{d\\times d}\\right)^{-1}\\left(\\stackrel{\\mathbf{J}(w)-\\,\\mu}{\\nabla\\mathbf{J}(w)}\\right),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which immediately yields the claim. ", "page_idx": 27}, {"type": "text", "text": "Theorem 4.2 (Explicit RFD). Let $\\mathbf{J}\\sim{\\mathcal{N}}({\\boldsymbol{\\mu}},{\\boldsymbol{C}})$ , then RFD coincides with gradient descent ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{w_{n+1}=w_{n}-\\eta_{n}^{*}\\frac{\\nabla{\\bf J}(w_{n})}{\\|\\nabla{\\bf J}(w_{n})\\|},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the RFD step sizes are given by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\eta_{n}^{*}:=\\operatorname*{argmin}_{\\eta\\in\\mathbb{R}}\\frac{C\\left(\\frac{\\eta^{2}}{2}\\right)}{C(0)}(\\mathbf{J}(w_{n})-\\mu)-\\eta\\frac{C^{\\prime}\\!\\left(\\frac{\\eta^{2}}{2}\\right)}{C^{\\prime}(0)}\\|\\nabla\\mathbf{J}(w_{n})\\|.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. The explicit version of RFD follows essentially by fixing the step size $\\eta=\\|\\mathbf{d}\\|$ and optimizing over the direction first. With Lemma 4.1 we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\min_{{\\bf\\alpha}}\\mathbb{E}[{\\bf J}(w-{\\bf d})\\mid{\\bf J}(w),\\nabla{\\bf J}(w)]}\\\\ {\\displaystyle=\\operatorname*{min}_{\\eta\\geq0}\\,\\operatorname*{min}_{{\\bf d}:|{\\bf d}||=\\eta}\\mu+\\frac{C\\left(\\frac{\\eta^{2}}{2}\\right)}{C(0)}({\\bf J}(w)-\\mu)-\\frac{C^{\\prime}\\left(\\frac{\\eta^{2}}{2}\\right)}{C^{\\prime}(0)}\\langle{\\bf d},\\nabla{\\bf J}(w)\\rangle}\\\\ {\\displaystyle=\\operatorname*{min}_{\\eta\\geq0}\\mu+\\frac{C\\left(\\frac{\\eta^{2}}{2}\\right)}{C(0)}({\\bf J}(w)-\\mu)-\\frac{C^{\\prime}\\left(\\frac{\\eta^{2}}{2}\\right)}{C^{\\prime}(0)}\\left\\{\\operatorname*{max}_{{\\bf d}:|{\\bf d}||=\\eta}\\langle{\\bf d},\\nabla{\\bf J}(w)\\rangle}&{\\frac{C^{\\prime}\\left(\\frac{\\eta^{2}}{2}\\right)}{C^{\\prime}(0)}\\geq0}\\\\ {\\displaystyle\\qquad=\\operatorname*{min}_{\\eta\\geq0}\\mu+\\frac{C\\left(\\frac{\\eta^{2}}{2}\\right)}{C(0)}({\\bf J}(w)-\\mu)-\\frac{C^{\\prime}\\left(\\frac{\\eta^{2}}{2}\\right)}{C^{\\prime}(0)}\\left\\{\\operatorname*{min}_{{\\bf d}:|{\\bf d}||=\\eta}\\langle{\\bf d},\\nabla{\\bf J}(w)\\rangle}&{\\frac{C^{\\prime}(\\frac{\\eta^{2}}{2})}{C^{\\prime}(0)}<0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By Lemma G.3 and Corollary G.4 the maximizing or minimizing step direction is then given by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbf{d}(\\eta)=\\pm\\eta\\frac{\\nabla\\mathbf{J}(w)}{\\|\\nabla\\mathbf{J}(w)\\|}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Where it is typically to be expected, that we have a positive sign. Since that depends on the covariance though, we avoid this problem with the following argument: Since $\\eta$ only appears as $\\eta^{2}$ in the remaining equation, we can optimize over $\\eta\\in\\mathbb{R}$ in the outer minimization instead of over $\\eta\\geq0$ to move the sign into the step size $\\eta$ and set without loss of generality ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbf{d}(\\eta)=\\eta\\frac{\\nabla\\mathbf{J}(w)}{\\|\\nabla\\mathbf{J}(w)\\|}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since $\\langle\\mathbf{d}(\\eta),\\nabla\\mathbf{J}(w)\\rangle=\\eta\\|\\nabla\\mathbf{J}(w)\\|$ the remaining outer minimization problem over the step size is then given by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\eta\\in\\mathbb{R}}\\frac{C\\big(\\frac{\\eta^{2}}{2}\\big)}{C(0)}(\\mathbf{J}(w)-\\mu)-\\eta\\frac{C^{\\prime}\\big(\\frac{\\eta^{2}}{2}\\big)}{C^{\\prime}(0)}\\|\\nabla\\mathbf{J}(w)\\|,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Its minimizer is by definition the RFD step size as given in the Theorem. ", "page_idx": 28}, {"type": "text", "text": "D.3 Section 5: RFD-step sizes ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Proposition D.2 (Tayloring the step size optimization problem). The second order Taylor approximation of the step size optimization problem ", "page_idx": 28}, {"type": "equation", "text": "$$\nq\\Theta^{}(\\eta)=-\\frac{C\\big(\\frac{\\eta^{2}}{2}\\big)}{C(0)}-\\eta\\frac{C^{\\prime}\\big(\\frac{\\eta^{2}}{2}\\big)}{C^{\\prime}(0)}\\Theta\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "around zero is given by ", "page_idx": 28}, {"type": "equation", "text": "$$\nT_{2}q_{\\Theta}(\\eta)=-1-\\eta\\Theta+\\eta^{2}\\frac{-C^{\\prime}(0)}{2C(0)}\\quad{m i n i m i z e d\\:b y}\\quad\\hat{\\eta}:=\\mathrm{argmin}\\:T_{2}q_{\\Theta}(\\eta)=\\frac{C(0)}{-C^{\\prime}(0)}\\Theta.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Furthermore, the Taylor residual is bounded by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|q(\\eta)-T_{2}q(\\eta)\\right|\\leq\\eta^{3}c_{0}\\big(\\frac{\\eta}{4}+\\Theta\\big)}\\\\ &{c_{0}=\\frac{1}{2}\\operatorname*{max}\\{\\operatorname*{sup}_{\\theta\\in[0,1]}|C^{\\prime\\prime}(\\theta)|,|C^{\\prime}(0)|\\}(\\frac{1}{C(0)}+\\frac{1}{|C^{\\prime}(0)|})<\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Using the Taylor approximation with the mean value reminder for $C$ , we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{C\\big(\\frac{\\eta^{2}}{2}\\big)=C(0)+C^{\\prime}(0)\\frac{\\eta^{2}}{2}+C^{\\prime\\prime}(\\theta_{2})\\frac{\\big(\\frac{\\eta^{2}}{2}\\big)^{2}}{2!}}}\\\\ {{C^{\\prime}\\big(\\frac{\\eta^{2}}{2}\\big)=C^{\\prime}(0)+C^{\\prime\\prime}(\\theta_{1})\\frac{\\eta^{2}}{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for some $\\theta_{1},\\theta_{2}\\in[0,\\frac{\\eta^{2}}{2}]$ . This implies ", "page_idx": 28}, {"type": "equation", "text": "$$\nq(\\eta)-\\underbrace{\\left(-\\big(1+\\frac{C^{\\prime}(0)}{C(0)}\\frac{\\eta^{2}}{2}\\big)-\\eta\\Theta\\right)}_{=:T_{2q\\Theta}(\\eta)}=-\\frac{C^{\\prime\\prime}(\\theta_{2})}{C(0)}\\frac{\\eta^{4}}{2^{3}}-\\frac{C^{\\prime\\prime}(\\theta_{1})}{C^{\\prime}(0)}\\frac{\\eta^{3}}{2}\\Theta\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By the following error the optimistically defined $T_{2}q_{\\Theta}(\\eta)$ is really the second Taylor approximation (which can be confirmed manually, but we deduce it by arguing that its residual is in $\\bar{\\mathcal{O}}(\\eta^{3}))$ . More specifically, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|q(\\eta)-T_{2}q(\\eta)\\right|\\leq\\eta^{3}\\Big(\\frac{\\operatorname*{sup}_{\\theta\\in[0,\\frac{\\eta^{2}}{2}]}|C^{\\prime\\prime}(\\theta)|}{2C(0)}\\frac{\\eta}{4}+\\frac{\\operatorname*{sup}_{\\theta\\in[0,\\frac{\\eta^{2}}{2}]}|C^{\\prime\\prime}(\\theta)|}{2|C^{\\prime}(0)|}\\Theta\\Big)\\overset{\\mathrm{Lem.D.8}}{\\leq}\\eta^{3}c_{0}\\big(\\frac{\\eta}{4}+\\Theta\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "It is easy to see for ${\\bf J}(w)<\\mu$ that $T_{2}q(\\eta)$ is a convex parabola due to $C^{\\prime}(0)<0$ . We thus have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\eta}:=\\underset{\\eta}{\\operatorname{argmin}}\\,T_{2}q_{\\Theta}(\\eta)=\\frac{C(0)}{-C^{\\prime}(0)}\\Theta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Theorem D.3 (Details of Proposition 5.2). Let $\\mathbf{J}\\sim{\\mathcal{N}}({\\boldsymbol{\\mu}},{\\boldsymbol{C}})$ and assume there exists $\\eta_{0}>0$ such that the correlation for larger distances $\\eta\\geq\\eta_{0}$ are bounded smaller than 1, i.e. $\\begin{array}{r}{\\frac{C(\\eta^{2}/2)}{C(0)}<\\rho\\in(0,1)}\\end{array}$ . Then there exists $K,\\Theta_{0}>0$ such that for all $\\Theta<\\Theta_{0}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n1-K\\Theta\\le\\frac{\\eta^{*}(\\Theta)}{\\hat{\\eta}(\\Theta)}\\le1+K\\Theta.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In particular we have $\\eta^{*}(\\Theta)\\sim\\hat{\\eta}(\\Theta)$ as $\\Theta\\to0$ or equivalently as $\\hat{\\eta}\\to0$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. This follows immediately from Lemma D.4, Lemma D.5 and Lemma D.6. ", "page_idx": 29}, {"type": "text", "text": "Corollary 5.3. Assume $\\eta^{*}\\to0$ implies $\\Theta\\to0$ , the cost J is bounded, has continuous gradients and RFD converges to some point $w_{\\infty}$ . Then $w_{\\infty}$ is a critical point and the RFD step sizes $\\eta^{*}$ are asymptotically equal to $\\hat{\\eta}$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. Assuming RFD converges, its step sizes $\\eta^{*}$ converge to zero. But this implies $\\Theta\\to0$ by assumption, i.e. ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Theta=\\frac{\\|\\nabla\\mathbf{J}(w)\\|}{\\mu-\\mathbf{J}(w)}\\rightarrow0\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since $\\mathbf{J}(w)$ is bounded, this implies $\\|\\nabla\\mathbf{J}(w)\\|\\rightarrow0$ and by continuity the of the gradient, it is zero in its limit. Thus we converge to a stationary point. The asymptotic equality follows by Lemma D.4 and Lemma D.5, as we know $\\eta^{*}$ converges so we do not require the assumptions of Lemma D.6. ", "page_idx": 29}, {"type": "text", "text": "D.3.1 Locating the Minimizer ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In the following we want to rule out locations for the RFD step size $\\eta^{*}$ by proving $q_{\\Theta}(\\eta)>q_{\\Theta}(\\hat{\\eta})$ for a wide range of $\\eta$ . For this endeavour the relative position of the step size $\\eta$ relative to $\\hat{\\eta}$ is a useful re-parametrization ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\eta:=\\eta(\\lambda)=\\lambda\\hat{\\eta}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Due to $\\begin{array}{r}{\\hat{\\eta}=\\frac{C(0)}{-C^{\\prime}(0)}\\Theta}\\end{array}$ we obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{2}q_{\\Theta}(\\eta)=-1-\\eta\\Theta+\\frac{\\eta^{2}}{2}\\frac{-C^{\\prime}(0)}{C(0)}=-1+\\lambda(\\frac{\\lambda}{2}-1)\\hat{\\eta}\\Theta}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "On the other hand we have for the bound ", "page_idx": 29}, {"type": "equation", "text": "$$\n|q_{\\Theta}(\\eta)-T_{2}q_{\\Theta}(\\eta)|\\leq\\lambda^{3}\\hat{\\eta}^{3}c_{0}\\Bigl(\\lambda\\frac{C(0)}{4|C^{\\prime}(0)|}+1\\Bigr)\\Theta\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since $\\hat{\\eta}=\\eta(1)$ we thus obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{q_{\\Theta}(\\eta)-q_{\\Theta}(\\hat{\\eta})}{\\hat{\\eta}\\Theta}\\ge\\frac{T_{2}q_{\\Theta}(\\eta)-|q_{\\Theta}(\\eta)-T_{2}q_{\\Theta}(\\eta)|-T_{2}q_{\\Theta}(\\hat{\\eta})-|q_{\\Theta}(\\hat{\\eta})-T_{2}q_{\\Theta}(\\hat{\\eta})|}{\\hat{\\eta}\\Theta}}\\\\ &{\\qquad\\qquad\\qquad\\ge\\underbrace{\\left(\\lambda(\\frac{\\lambda}{2}-1)-(-\\frac{1}{2})\\right)}_{=\\frac{1}{2}-\\lambda+\\frac{\\lambda^{2}}{2}}-\\hat{\\eta}^{2}c_{0}\\Big[\\lambda^{3}\\Big(\\lambda\\frac{C(0)}{4|C^{\\prime}(0)|}+1\\Big)+\\Big(\\frac{C(0)}{4|C^{\\prime}(0)|}+1\\Big)\\Big]}\\\\ &{\\qquad\\qquad\\qquad=\\frac{1}{2}-\\lambda+\\frac{\\lambda^{2}}{2}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{1}{2}(1-\\lambda)^{2}-\\hat{\\eta}^{2}c_{0}\\Big[\\lambda^{3}\\Big(\\lambda\\frac{C(0)}{4|C^{\\prime}(0)|}+1\\Big)+\\Big(\\frac{C(0)}{4|C^{\\prime}(0)|}+1\\Big)\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This equation will be the basis of a number of lemmas ruling out various step sizes as minimizers. ", "page_idx": 29}, {"type": "text", "text": "Lemma D.4 (Ruling out small step sizes). If the step size is (much) smaller than the asymptotic step size $\\hat{\\eta}=\\hat{\\eta}(\\Theta)$ , then it can not be a minimizer. More specifically ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\eta}{\\hat{\\eta}}\\,\\in[0,1-c_{1}\\Theta)\\implies q_{\\Theta}(\\eta)>q_{\\Theta}(\\hat{\\eta})\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\begin{array}{r}{c_{1}:=2\\frac{C(0)}{|C^{\\prime}(0)|}\\sqrt{c_{0}\\big(\\frac{C(0)}{4|C^{\\prime}(0)|}+1\\big)}<\\infty.}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "Proof. Here we consider the case $\\eta\\leq\\hat{\\eta}$ , i.e. $\\lambda\\in[0,1]$ . By (23) we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{q_{\\Theta}(\\eta)-q_{\\Theta}(\\hat{\\eta})}{\\hat{\\eta}\\Theta}\\geq\\frac{1}{2}(1-\\lambda)^{2}-\\hat{\\eta}^{2}c_{0}\\Big[\\lambda^{3}\\Big(\\lambda\\frac{C(0)}{4|C^{\\prime}(0)|}+1\\Big)+\\Big(\\frac{C(0)}{4|C^{\\prime}(0)|}+1\\Big)\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\geq\\frac{1}{2}(1-\\lambda)^{2}-2\\hat{\\eta}^{2}c_{0}\\Big(\\frac{C(0)}{4|C^{\\prime}(0)|}+1\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\vdots~0}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for which ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1-\\lambda)^{2}>4\\hat{\\eta}^{2}c_{0}\\Bigl(\\frac{C(0)}{4|C^{\\prime}(0)|}+1\\Bigr)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "is sufficient or equivalently ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda<1-2\\hat{\\eta}\\sqrt{c_{0}\\Big(\\frac{C(0)}{4|C^{\\prime}(0)|}+1\\Big)}=1-\\Theta\\underbrace{2\\frac{C(0)}{|C^{\\prime}(0)|}\\sqrt{c_{0}\\Big(\\frac{C(0)}{4|C^{\\prime}(0)|}+1\\Big)}}_{=:c_{1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "So for $\\lambda\\in\\left[0,1-\\Theta c_{1}\\right)$ we have $q_{\\Theta}(\\eta)>q_{\\Theta}(\\hat{\\eta})$ . ", "page_idx": 30}, {"type": "text", "text": "Lemma D.5 (Ruling out medium sized step sizes as minimizer). For $c_{2}=2c_{1}$ and $\\begin{array}{r}{\\Theta\\le\\Theta_{0}:=\\frac{1}{5c_{1}}}\\end{array}$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\eta}{\\hat{\\eta}}\\in\\bigl(1+c_{2}\\Theta,\\frac{1}{c_{2}\\Theta}\\bigr)\\implies q_{\\Theta}(\\eta)>q_{\\Theta}(\\hat{\\eta})\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. Here we consider the case $\\lambda\\geq1$ , i.e. $\\eta>\\hat{\\eta}$ . Again starting with (23) we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{q(\\eta)-q(\\hat{\\eta})}{\\hat{\\eta}\\Theta}\\geq\\frac{1}{2}(1-\\lambda)^{2}-\\hat{\\eta}^{2}c_{0}\\Big[\\lambda^{3}\\Big(\\lambda\\frac{C(0)}{4|C^{\\prime}(0)|}+1\\Big)+\\Big(\\frac{C(0)}{4|C^{\\prime}(0)|}+1\\Big)\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\geq\\frac{1}{2}(\\lambda-1)^{2}-2\\lambda^{4}\\hat{\\eta}^{2}c_{0}\\Big(\\frac{C(0)}{4|C^{\\prime}(0)|}+1\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\vdots}\\\\ &{\\qquad\\qquad\\qquad\\geq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for which ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda-1>2\\lambda^{2}\\hat{\\eta}\\sqrt{c_{0}\\biggl(\\frac{C(0)}{4|C^{\\prime}(0)|}+1\\biggr)}=c_{1}\\Theta\\lambda^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "or equivalently ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\lambda-1-c_{1}\\Theta\\lambda^{2}>0\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "is sufficient. Note that this is a concave parabola in $\\lambda$ . So it is positive between its zeros which are characterized by ", "page_idx": 30}, {"type": "equation", "text": "$$\nc_{1}\\Theta\\lambda^{2}-\\lambda+1=0.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "They are thus given by ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\lambda_{1/2}=\\frac{1\\pm\\sqrt{1-4c_{1}\\Theta}}{2c_{1}\\Theta}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "So whenever $\\lambda\\in(\\lambda_{1},\\lambda_{2})$ we have that $q_{\\Theta}(\\eta)>q_{\\Theta}(\\hat{\\eta})$ . In particular for $4c_{1}\\Theta\\leq1$ or equivalently $\\begin{array}{r}{\\Theta\\leq\\frac{1}{4c_{1}}}\\end{array}$ we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\lambda_{2}\\ge\\frac{1}{2c_{1}\\Theta}=\\frac{1}{c_{2}\\Theta}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "To get a bound on $\\lambda_{1}$ note that the original equation was essentially ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\lambda\\geq1+c_{1}\\Theta\\lambda^{2}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "with equality for $\\lambda=\\lambda_{1}$ , if $\\Theta$ is reduced, the inequality remains, which implies that $\\lambda_{1}$ is decreasing with $\\Theta$ . So assuming the inequality is satisfied for a particular $\\lambda$ e.g. $\\lambda={\\sqrt{2}}$ which requires ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sqrt{2}\\geq1+2c_{1}\\Theta\\iff\\Theta\\leq\\frac{\\sqrt{2}-1}{2c_{1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "then we know that $\\lambda_{1}\\leq\\sqrt{2}$ for all smaller $\\Theta$ . This implies for $\\begin{array}{r}{\\Theta\\le\\Theta_{0}=\\frac{1}{5c_{1}}\\le\\frac{\\sqrt{2}-1}{2c_{1}}}\\end{array}$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\lambda_{1}=1+c_{1}\\Theta\\lambda_{1}^{2}\\leq1+\\underbrace{2c_{1}}_{c_{2}}\\Theta.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Lemma D.6 (Ruling out large step sizes as minimizer). If there exists step size $\\eta_{0}>0$ such that the correlation is bounded by some $\\rho<1$ , i.e. ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{C\\big(\\frac{\\eta^{2}}{2}\\big)}{C(0)}\\leq\\rho\\in(0,1),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for larger step sizes $\\eta\\geq\\eta_{0}$ , then there exist $\\Theta_{0}>0$ such that for all $\\Theta<\\Theta_{0}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{\\eta}{\\hat{\\eta}}\\,\\in\\,\\bigl(1+c_{2}\\Theta,\\infty\\bigr)\\,\\implies q(\\eta)>q(\\hat{\\eta}),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $c_{2}$ is the constant from Lemma $D.5.$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. The upper bound $\\frac{1}{c_{2}\\Theta}$ in Lemma D.5 is only due to the loss of precision of the Taylor approximation. To remove it, we take a closer look at the actual $q_{\\Theta}$ itself. We have the following bound for our asymptotic minimum ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{q_{\\Theta}(\\hat{\\eta})}{\\Theta}\\leq\\frac{T_{2}q_{\\Theta}(\\hat{\\eta})+|q_{\\Theta}(\\hat{\\eta})-T_{2}q_{\\Theta}(\\hat{\\eta})|}{\\Theta}=-\\frac{1}{\\Theta}-\\frac{1}{2}\\hat{\\eta}+\\hat{\\eta}^{3}\\underbrace{c_{0}\\Big(\\frac{C(0)}{4|C^{\\prime}(0)|}+1\\Big)}_{=:c_{3}}}\\\\ &{\\qquad\\leq-\\frac{1}{\\Theta}+\\hat{\\eta}^{3}c_{3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Which means we have for ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{q_{\\Theta}(\\eta)-q_{\\Theta}(\\hat{\\eta})}{\\Theta}\\geq\\Big(1-\\frac{C\\big(\\frac{\\eta^{2}}{2}\\big)}{C(0)}\\Big)\\frac{1}{\\Theta}-\\eta\\frac{C^{\\prime}\\big(\\frac{\\eta^{2}}{2}\\big)}{C^{\\prime}(0)}-\\hat{\\eta}^{3}c_{3}}\\\\ &{\\qquad\\qquad\\qquad\\overset{\\mathrm{LemmanD.}7}{\\geq}\\Big(1-\\frac{C\\big(\\frac{\\eta^{2}}{2}\\big)}{C(0)}\\Big)\\frac{1}{\\Theta}-\\frac{\\sqrt{C(0)}}{\\sqrt{-C^{\\prime}(0)}}-\\hat{\\eta}^{3}c_{3}}\\\\ &{\\qquad\\qquad\\geq\\big(1-M\\big)\\frac{1}{\\Theta}-\\frac{\\sqrt{C(0)}}{\\sqrt{-C^{\\prime}(0)}}-\\hat{\\eta}^{3}c_{3}}\\\\ &{\\qquad\\qquad\\qquad\\vdots\\ 0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where we use the assumption that there exists $\\rho\\in(0,1)$ such that $\\rho\\ge\\frac{C\\Big(\\frac{\\eta^{2}}{2}\\Big)}{C(0)}$ for all $\\eta\\geq\\eta_{0}$ and the fact that we only need to consider $\\begin{array}{r}{\\eta\\geq\\frac{1}{c_{2}\\Theta}}\\end{array}$ (due to Lemma D.5) which allows a translation of $\\eta_{0}$ into some maximal $\\Theta_{0}$ . Note that $\\hat{\\eta}\\sim\\Theta$ vanishes as $\\Theta\\to0$ , so eventually the term $(1-M){\\frac{1}{\\Theta}}$ dominates. Selecting $\\Theta_{0}$ small small enough is thus sufficient to cover everything that is not already covered by Lemma D.5. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "D.3.2 Technical bounds ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Lemma D.7 (Bound on the first derivative of the covariance). ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{sup}_{\\eta\\ge0}|C^{\\prime}\\big(\\frac{\\eta^{2}}{2}\\big)\\eta|\\le\\sqrt{-C^{\\prime}(0)C(0)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. Since we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Cov}(D_{v}\\mathbf{J}(x),\\mathbf{J}(y))=C^{\\prime}\\big(\\frac{\\|x-y\\|^{2}}{2}\\big)\\langle x-y,v\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "we have for a standardized vector $\\|v\\|=1$ and $x-y=\\eta v$ by Cauchy-Schwarz ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|C^{\\prime}(\\frac{\\eta^{2}}{2})\\eta|=|\\operatorname{Cov}(D_{v}\\mathbf{J}(x),\\mathbf{J}(y))|\\overset{\\mathrm{CS.}}{\\leq}\\sqrt{\\operatorname{Var}(D_{v}\\mathbf{J}(x))\\operatorname{Var}(\\mathbf{J}(y))}=\\sqrt{-C^{\\prime}(0)C(0)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "As the bound is independent of $\\eta$ this yields the claim. ", "page_idx": 31}, {"type": "text", "text": "Lemma D.8 (Bound on the second derivative of the covariance). ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\theta\\geq0}|C^{\\prime\\prime}(\\theta)|\\leq\\operatorname*{max}\\Bigl\\{\\operatorname*{sup}_{\\theta\\in[0,1]}|C^{\\prime\\prime}(\\theta)|,|C^{\\prime}(0)|\\Bigr\\}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. Note that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Cov}(D_{v}\\mathbf{J}(x),D_{w}\\mathbf{J}(y))=-C^{\\prime\\prime}\\big(\\frac{\\|x-y\\|^{2}}{2}\\big)\\langle x-y,v\\rangle\\langle x-y,w\\rangle-C^{\\prime}\\big(\\frac{\\|x-y\\|^{2}}{2}\\big)\\langle v,w\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Selecting $v,w$ as orthonormal vectors (e.g. $v=e_{1},w=e_{2})$ ) and $x-y:=\\eta(v+w)$ for some $\\eta>0$ results in $\\|{\\boldsymbol{x}}-{\\boldsymbol{y}}\\|^{2}=2\\eta^{2}$ and thus by the Cauchy-Schwarz inequality ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\bigl|-C^{\\prime\\prime}(\\eta^{2})\\eta^{2}\\bigr|=\\bigl|\\mathrm{Cov}(D_{v}\\mathbf{J}(x),D_{w}\\mathbf{J}(y))\\bigr|\\ \\stackrel{\\mathrm{CS.}}{\\leq}\\sqrt{\\mathrm{Var}(D_{v}\\mathbf{J}(x))\\,\\mathrm{Var}(D_{w}\\mathbf{J}(y))}=\\sqrt{(-C^{\\prime}(0))^{2}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This implies the claim. ", "page_idx": 32}, {"type": "text", "text": "D.4 Section 6: Stochastic loss ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Lemma D.9. The stochastic approximation errors ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\epsilon_{i}(w):=\\ell_{i}(w)-\\mathbf{J}(w)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "are identically distributed, centered random functions, which are independent conditional on f. In particular, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\epsilon_{i}(w)\\epsilon_{j}(\\tilde{w})]=\\mathbb{E}[\\epsilon_{i}(w)\\epsilon_{j}(\\tilde{w})\\mid\\mathbf{f}]=0\\quad\\forall j\\neq i.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. The $\\epsilon_{i}$ are independent random functions conditional on $\\mathbf{f}$ , since for any $n\\in\\mathbb N$ , any bounded measurable functions $h$ and $g$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Big[h\\big(\\epsilon_{i}(w_{1}),\\dots,\\epsilon_{i}(w_{n})\\big)g\\big(\\epsilon_{j}(w_{1}),\\dots,\\epsilon_{j}(w_{n})\\big)\\ |\\ \\mathbf{f}\\Big]}\\\\ &{=\\Big[h\\big(\\epsilon_{i}(w_{1}),\\dots,\\epsilon_{i}(w_{n})\\big)\\underbrace{\\mathbb{E}\\Big[g\\big(\\epsilon_{j}(w_{1}),\\dots,\\epsilon_{j}(w_{n})\\big)\\ |\\ \\mathbf{f},X_{i},\\varsigma_{i}\\Big]}_{\\stackrel{(*)}{=}\\mathbb{E}\\Big[g\\big(\\epsilon_{j}(w_{1}),\\dots,\\epsilon_{j}(w_{n})\\big)|\\mathbf{f}\\Big]}\\Big|\\ \\mathbf{f}\\Big]}\\\\ &{=\\mathbb{E}\\Big[h\\big(\\epsilon_{i}(w_{1}),\\dots,\\epsilon_{i}(w_{n})\\big)\\ |\\ \\mathbf{f}\\Big]\\mathbb{E}\\Big[g\\big(\\epsilon_{j}(w_{1}),\\dots,\\epsilon_{j}(w_{n})\\big)\\ |\\ \\mathbf{f}\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $(*)$ uses the fact that $\\epsilon_{j}$ does not depend on the independent $X_{i},\\varsigma_{i}$ . Since almost by definition ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\epsilon_{i}\\mid\\mathbf{f}]=\\mathbb{E}[\\ell(\\cdot,X_{i},Y_{i})\\mid\\mathbf{f}]-\\mathbf{J}(\\cdot)=0,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "the stochastic approximation errors are thus uncorrelated ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\epsilon_{i}\\epsilon_{j}]=\\mathbb{E}\\Big[\\mathbb{E}[\\epsilon_{i}\\epsilon_{j}\\mid\\mathbf{f}]\\Big]=\\mathbb{E}\\Big[\\mathbb{E}[\\epsilon_{i}\\mid\\mathbf{f}]\\mathbb{E}[\\epsilon_{j}\\mid\\mathbf{f}]\\Big]=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Extension 6.2 (S-RFD). For loss $\\mathbf{J}\\sim{\\mathcal{N}}({\\boldsymbol{\\mu}},{\\boldsymbol{C}})$ and stochastic errors $\\epsilon_{i}$ ii\u223cd $\\mathcal{N}(0,C_{\\epsilon})$ we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\mathbf{d}}{\\mathrm{argmin}}\\,\\mathbb{E}[\\mathbf{J}(w-\\mathbf{d})\\mid\\mathcal{L}_{b}(w),\\nabla\\mathcal{L}_{b}(w)]=\\eta^{*}(\\Theta)\\frac{\\nabla\\mathcal{L}_{b}(w)}{\\|\\nabla\\mathcal{L}_{b}(w)\\|}}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "with the same step size function $\\eta^{*}$ as for RFD, but modified $\\Theta$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\Theta=\\frac{C^{\\prime}(0)}{C^{\\prime}(0)+\\frac{1}{b}C_{\\epsilon}^{\\prime}(0)}\\frac{C(0)+\\frac{1}{b}C_{\\epsilon}(0)}{C(0)}\\frac{\\|\\nabla\\mathcal{L}_{b}(w)\\|}{\\mu-\\mathcal{L}_{b}(w)}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. Since $\\epsilon_{i}$ are conditionally independent between each other and to $\\mathbf{J}$ , as entire functions, the same holds true for $\\nabla\\epsilon_{i}$ . As all the mixed covariances disappear we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Cov}\\Big(\\Big(\\frac{\\mathcal{L}_{b}(w)}{\\nabla\\mathcal{L}_{b}(w)}\\Big)\\Big)=\\mathrm{Cov}\\Big(\\Big(\\frac{\\mathbf{J}(w)}{\\nabla\\mathbf{J}(w)}\\Big)\\Big)+\\frac{1}{b^{2}}\\sum_{i=1}^{b}\\mathrm{Cov}\\Big(\\Big(\\frac{\\epsilon_{i}(w)}{\\nabla\\epsilon_{i}(w)}\\Big)\\Big)}\\\\ &{\\qquad\\qquad\\qquad=\\Big(^{C(0)}\\begin{array}{l l}{-C^{\\prime}(0)\\mathbb{I}_{d\\times d}\\Big)+\\frac{1}{b^{2}}\\sum_{i=1}^{b}\\Big(C_{\\epsilon}(0)}&{-C_{\\epsilon}^{\\prime}(0)\\mathbb{I}_{d\\times d}\\Big)}\\end{array}}\\\\ &{\\qquad\\qquad\\qquad=\\Bigg(^{C(0)}+\\frac{1}{b}C_{\\epsilon}(0)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\quad-\\Big(C^{\\prime}(0)+\\frac{1}{b}C_{\\epsilon}^{\\prime}(0)\\Big)\\mathbb{I}_{d\\times d}.\\Bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "by Lemma G.2. If you want to break up the first step we recommend considering individual entries of the covariance matrix to convince yourself that all the mixed covariances disappear. Together with the fact ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Cov}\\Big(\\mathbf{J}(w-\\mathbf{d}),\\left(\\frac{\\mathcal{L}_{b}(w)}{\\nabla\\mathcal{L}_{b}(w)}\\right)\\Big)}\\\\ &{=\\mathrm{Cov}\\Big(\\mathbf{J}(w-\\mathbf{d}),\\left(\\frac{\\mathbf{J}(w)}{\\nabla\\mathbf{J}(w)}\\right)\\Big)+\\frac{1}{b^{2}}\\displaystyle\\sum_{i=1}^{b}\\underbrace{\\mathrm{Cov}\\Big(\\mathbf{J}(w-\\mathbf{d}),\\left(\\frac{\\epsilon_{i}(w)}{\\nabla\\epsilon_{i}(w)}\\right)\\Big)}_{=0}}\\\\ &{=\\left(C(\\frac{\\|\\mathbf{d}\\|^{2}}{2})\\mathbf{d}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The rest is analogous to Lemma 4.1 and Theorem 4.2, so we only sketch the remaining steps. ", "page_idx": 33}, {"type": "text", "text": "Applying Theorem G.1 as in Lemma 4.1 we obtain a stochastic version of the stochastic Taylor approximation (\u201cstochastic2 Taylor approximation\u201d perhaps?) ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\{[\\mathbf{J}(w-\\mathbf{d})\\mid\\mathcal{L}_{b}(w),\\nabla\\mathcal{L}_{b}(w)]=\\mu+\\frac{C\\big(\\frac{\\|\\mathbf{d}\\|^{2}}{2}\\big)}{C(0)+\\frac{1}{b}C_{\\epsilon}(0)}(\\mathcal{L}_{b}(w)-\\mu)-\\frac{C^{\\prime}\\big(\\frac{\\|\\mathbf{d}\\|^{2}}{2}\\big)}{C^{\\prime}(0)+\\frac{1}{b}C_{\\epsilon}^{\\prime}(0)}\\langle\\mathbf{d},\\mathcal{L}_{b}(w)\\rangle.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Minimizing this subject to a constant step size as in Theorem 4.2 results in ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta^{*}=\\underset{\\eta\\in\\mathbb{R}}{\\operatorname{argmin}}\\,\\frac{C\\big(\\frac{\\|{\\bf d}\\|^{2}}{2}\\big)}{C(0)+\\frac{1}{b}C_{\\epsilon}(0)}(\\mathcal{L}_{b}(w)-\\mu)-\\eta\\frac{C^{\\prime}\\big(\\frac{\\|{\\bf d}\\|^{2}}{2}\\big)}{C^{\\prime}(0)+\\frac{1}{b}C_{\\epsilon}^{\\prime}(0)}\\|\\mathcal{L}_{b}(w)\\|}\\\\ &{\\quad=\\underset{\\eta\\in\\mathbb{R}}{\\operatorname{argmin}}-\\frac{C\\big(\\frac{\\|{\\bf d}\\|^{2}}{2}\\big)}{C(0)}-\\eta\\frac{C^{\\prime}\\big(\\frac{\\|{\\bf d}\\|^{2}}{2}\\big)}{C^{\\prime}(0)+\\frac{1}{b}C_{\\epsilon}^{\\prime}(0)}\\frac{C(0)}{C(0)+\\frac{1}{b}C_{\\epsilon}(0)}\\frac{\\|\\mathcal{L}_{b}(w)\\|}{\\mu-\\mathcal{L}_{b}(w)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where we divided the term by $\\begin{array}{r}{\\frac{C(0)}{C(0)+\\frac{1}{b}C_{\\epsilon}(0)}\\frac{1}{\\mu-\\mathscr{L}_{b}(w)}\\,\\geq\\,0}\\end{array}$ to obtain the last equation. The claim follows by definition of and our redefinition of $\\Theta$ . ", "page_idx": 33}, {"type": "text", "text": "E Extensions ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this section we present a few possible extensions to Theorem 4.2, which are all composable, i.e. it is possible to combine these extensions without any major problems (including S-RFD, i.e. Extension 6.2). ", "page_idx": 33}, {"type": "text", "text": "E.1 Geometric anisotropy/Adaptive step sizes ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this section, we discuss the generalization of isotropy to \u201cgeometric anisotropies\u201d [50, p. 17], which provide good insights into the inner workings of adaptive learning rates (e.g. AdaGrad [14] and Adam [29]). ", "page_idx": 33}, {"type": "text", "text": "Definition E.1 (Geometric Anisotropy). We say a random function $\\mathbf{J}$ exhibits a \u201cgeometric anisotropy\u201d, if there exists an invertible matrix $A$ such that $\\mathbf{J}(x)\\,=\\,\\mathbf{g}(A x)$ for some isotropic random function g. ", "page_idx": 33}, {"type": "text", "text": "This implies that the expectation of $\\mathbf{J}$ is still constant $(\\mathbb{E}[\\mathbf{J}(x)]=\\mathbb{E}[\\mathbf{g}(A x)]=\\mu)$ and the covariance function of $\\mathbf{J}$ is given by ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname{Cov}(\\mathbf{J}(x),\\mathbf{J}(y))=\\operatorname{Cov}(\\mathbf{g}(A x),\\mathbf{g}(A y))=C{\\Big(}{\\frac{\\|A(x-y)\\|^{2}}{2}}{\\Big)}=C{\\Big(}{\\frac{\\|x-y\\|_{A^{T}A}^{2}}{2}}{\\Big)}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\|\\cdot\\|_{\\Sigma}$ is the norm induced by $\\langle x,y\\rangle_{\\Sigma}:=\\langle x,\\Sigma y\\rangle$ for some strictly positive definite matrix $\\Sigma=A^{T}A$ . Here (24) characterizes the set of random functions with a geometric anisotropy in the Gaussian case, because for an $\\mathbf{J}$ with such a covariance we can always obtain an isotropic $\\mathbf{g}$ by $\\mathbf{g}(x):=\\mathbf{J}(A^{-1}x)$ . This is the whitening transformation we suggest looking for in order to ensure isotropy in the context of scale invariance (Section 2). ", "page_idx": 33}, {"type": "text", "text": "An important observation is, that Theorem F.2 implies that J is still stationary, so the distribution of $\\mathbf{J}$ is still invariant to translations. If stationarity is a problem, this is therefore not the solution. But geometric anisotropies are a beautiful model to explain preconditioning and adaptive step sizes. For this, we first determine the RFD steps. ", "page_idx": 34}, {"type": "text", "text": "Extension E.2 (RFD steps under geometric anisotropy). Let J be a Gaussian random function which exhibits a \u201cgeometric anisotropy\u201d $A$ and is based on an isotropic random function $\\mathbf{g}\\sim{\\mathcal{N}}({\\boldsymbol{\\mu}},C)$ . Then the RFD steps are given by ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\eta^{*}\\frac{\\Sigma^{-1}\\nabla{\\mathbf J}(w)}{\\|\\Sigma^{-1}\\nabla{\\mathbf J}(w)\\|_{\\Sigma}}=\\mathrm{argmin}\\,\\mathbb{E}[{\\mathbf J}(w-{\\mathbf d})\\mid{\\mathbf J}(w),\\nabla{\\mathbf J}(w)]\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "with ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\boldsymbol{\\eta}^{*}=\\operatorname*{argmin}_{\\boldsymbol{\\eta}}q_{\\Theta}(\\boldsymbol{\\eta})\\quad\\boldsymbol{w h e r e}\\quad\\Theta=\\frac{\\|\\boldsymbol{\\Sigma}^{-1}\\nabla{\\mathbf{J}}(\\boldsymbol{w})\\|_{\\boldsymbol{\\Sigma}}}{\\mu-{\\mathbf{J}}(\\boldsymbol{w})}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proofsketch. There are two ways to see this. Either we apply scale invariance (Advantage 2.3) directly to translate the steps on $\\mathbf{g}$ into steps on $\\mathbf{J}$ . Alternatively one can manually retrace the steps of the proof. Details in Subsection E.1.1 \u53e3 ", "page_idx": 34}, {"type": "text", "text": "The step direction is therefore ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\Sigma^{-1}\\nabla\\mathbf{J}(x)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and $\\Sigma^{-1}$ acts as a preconditioner. So how would one obtain $\\Sigma?$ As it turns out the following holds true (by Lemma G.2) ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\nabla\\mathbf{J}(w)\\nabla\\mathbf{J}(w)^{T}]=A^{T}\\mathbb{E}[\\nabla\\mathbf{g}(w)\\nabla\\mathbf{g}(w)^{T}]A=A^{T}(-C^{\\prime}(0)\\mathbb{I})A=-C^{\\prime}(0)\\Sigma}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "In their proposal of the first \u201cadaptive\u201d method, AdaGrad, Duchi et al. [14] suggest to use the matrix ", "page_idx": 34}, {"type": "equation", "text": "$$\nG_{t}=\\sum_{k=1}^{t}\\nabla\\mathbf{J}(w_{k})\\nabla\\mathbf{J}(w_{k})^{T},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which is basically already looking like an estimation method of $\\Sigma$ . They then restrict themselves to $\\mathrm{diag}(G_{t})$ due to the computational costs of a full matrix inversion. This results in entry-wise (\u201cadaptive\u201d) learning rates. Later adaptive methods like RMSProp [22], AdaDelta [57] and in particular Adam [29] replace this sum with an exponential mean estimate, i.e. in the case of Adam the decay rate $\\beta_{2}$ is used to get an exponential moving average ", "page_idx": 34}, {"type": "equation", "text": "$$\nv_{t}=\\beta_{2}v_{t-1}+(1-\\beta_{2})\\operatorname{diag}(\\nabla\\mathbf{J}(w_{t})\\nabla\\mathbf{J}(w_{t})^{T})=\\beta_{2}v_{t-1}+(1-\\beta_{2})(\\nabla\\mathbf{J}(w_{t}))^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "They then take the expectation ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[v_{t}]=\\mathbb{E}\\Big[(1-\\beta_{2})\\displaystyle\\sum_{k=1}^{t}\\beta_{2}^{t-k}\\nabla\\mathbf{J}(w_{k})^{2}\\Big]=\\mathbb{E}\\Big[(1-\\beta_{2})\\displaystyle\\sum_{k=1}^{t}\\beta_{2}^{t-k}\\nabla\\mathbf{J}(w_{k})^{2}\\Big]=(1-\\beta_{2}^{t})\\underbrace{\\mathbb{E}[\\nabla\\mathbf{J}(x_{t})^{2}]}_{\\mathrm{ocdiag}(\\Sigma)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "So $\\hat{v}_{t}=v_{t}/(1-\\beta_{2}^{t})$ in the Adam optimizer is essentially an estimator for $\\mathrm{diag}(\\Sigma)$ . It is noteworthy, that Kingma and Ba [29] already used the expectation symbol. This is despite the fact, that they did not yet model the optimization objective $\\mathbf{J}$ as a random function. ", "page_idx": 34}, {"type": "text", "text": "We can not yet explain why they then use the square root of their estimate $\\mathrm{diag(\\Sigma)^{-1/2}}$ instead of $\\mathrm{diag}(\\Sigma)^{-1}$ itself. This might have something to do with the fact that the estimation of $G_{t}$ happens online and the $\\mathbf{J}(w_{k})$ are therefore highly correlated. Another reason might be that the inverse of an estimator has different properties than the estimator itself. Finally, the fact that only the diagonal is used might also be the reason, if the preconditioner $\\mathrm{diag}(\\Sigma)^{-1/2}$ is simply better when we restrict ourselves to diagonal matrices. ", "page_idx": 34}, {"type": "text", "text": "E.1.1 Proof of Extension E.2 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Since the application of scale invariance provides no intuition, we provide a proof which retraces some of the steps of the original proof. ", "page_idx": 35}, {"type": "text", "text": "Recall, that for an isotropic random function g we have the stochastic Taylor approximation ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{g}(w-\\mathbf{d})\\mid\\mathbf{g}(x),\\nabla\\mathbf{g}(x)]=\\mu+\\frac{C\\big(\\frac{\\|\\mathbf{d}\\|^{2}}{2}\\big)}{C(0)}(\\mathbf{g}(w)-\\mu)+\\frac{C^{\\prime}\\big(\\frac{\\|\\mathbf{d}\\|^{2}}{2}\\big)}{C^{\\prime}(0)}\\langle\\mathbf{d},\\nabla\\mathbf{g}(w)\\rangle\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This implies for a random function with geometric anisotropy $\\mathbf{J}(w)=\\mathbf{g}(A w)$ that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathbf{J}(w-\\mathbf{d})\\mid\\mathbf{J}(w),\\nabla\\mathbf{J}(w)]=\\mathbb{E}[\\mathbf{g}(A(w-\\mathbf{d}))\\mid\\mathbf{g}(A w),\\nabla\\mathbf{g}(A w)]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\mu+\\frac{C\\left(\\frac{\\|A\\mathbf{d}\\|^{2}}{2}\\right)}{C(0)}(\\mathbf{g}(A w)-\\mu)-\\frac{C^{\\prime}\\left(\\frac{\\|A\\mathbf{d}\\|^{2}}{2}\\right)}{C^{\\prime}(0)}\\langle A\\mathbf{d},\\nabla\\mathbf{g}(A w)\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\mu+\\frac{C\\left(\\frac{\\|\\mathbf{d}\\|_{\\Sigma}^{2}}{2}\\right)}{C(0)}(\\mathbf{J}(w)-\\mu)-\\frac{C^{\\prime}\\left(\\frac{\\|\\mathbf{d}\\|_{\\Sigma}^{2}}{2}\\right)}{C^{\\prime}(0)}\\langle\\mathbf{d},\\underbrace{A^{T}\\nabla\\mathbf{g}(A w)}_{=\\nabla\\mathbf{J}(w)}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "with $\\Sigma:=A^{T}A$ . As in the original proof, we now optimize over the direction first, while keeping the step size constant, although we now fix the step size with regard to the norm $\\|\\cdot\\|_{\\Sigma}$ (which basically means that we still do the optimization in the isotropic space). Note that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{d}}\\langle\\mathbf{d},\\nabla\\mathbf{J}(x)\\rangle\\quad\\mathrm{s.t.}\\quad\\|\\mathbf{d}\\|_{\\Sigma}=\\eta\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "is equivalent to ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{d}}\\langle\\mathbf{d},{\\boldsymbol{\\Sigma}}^{-1}\\nabla\\mathbf{J}({\\boldsymbol{x}})\\rangle_{\\Sigma}\\quad\\mathrm{s.t.}\\quad\\|\\mathbf{d}\\|_{\\Sigma}=\\eta\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which is solved by ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\pm\\eta{\\frac{\\Sigma^{-1}\\nabla\\mathbf{J}(x)}{\\|\\Sigma^{-1}\\nabla\\mathbf{J}(x)\\|_{\\Sigma}}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The remainder of the proof is exactly the same as in the original. ", "page_idx": 35}, {"type": "text", "text": "E.2 Conservative RFD ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In the first paragraph of Section 2 we motivated the relation between RFD and classical optimization with the observation, that gradient descent is the minimizer of a regularized first order Taylor approximation ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{L}\\nabla{\\mathbf J}(w)=\\underset{\\mathbf d}{\\mathrm{argmin}}\\,T[{\\mathbf J}(w-{\\mathbf d})\\mid{\\mathbf J}(w),\\nabla{\\mathbf J}(w)]+\\frac{L}{2}\\|w\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This regularized Taylor approximation is in fact an upper bound on our function under the $L$ - smoothness assumption [38], i.e. ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{J}(w-\\mathbf{d})\\leq T[\\mathbf{J}(w-\\mathbf{d})\\mid\\mathbf{J}(w),\\nabla\\mathbf{J}(w)]+\\frac{L}{2}\\|\\mathbf{d}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "An improvement on of this upper bound compared to $\\mathbf{J}(x)$ therefore guarantees an improvement of the loss. This guarantee was lost with the conditional expectation (on purpose, as we wanted to consider the average case). Losing this guarantee also makes convergence proofs more difficult as they typically make use of this improvement. In view of the confidence intervals of Figure 1, it is natural to ask for a similar upper bound in the random setting, where this can only be the top of an confidence interval. This is provided in the following theorem ", "page_idx": 35}, {"type": "text", "text": "Lemma E.3 (An $\\gamma.$ -upper bound). We have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\Big(\\mathbf{J}(w-\\mathbf{d})\\leq\\mathbb{E}[\\mathbf{J}(w-\\mathbf{d})\\mid\\mathbf{J}(w),\\nabla\\mathbf{J}(w)]+\\rho_{\\gamma}(\\|\\mathbf{d}\\|^{2})\\Big)\\geq\\gamma}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "for $\\rho_{\\gamma}(\\eta^{2}):=\\Phi^{-1}(\\gamma)\\sigma(\\eta^{2})$ with ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sigma^{2}(\\eta^{2}):=C(0)-\\frac{C{(\\frac{\\eta^{2}}{2})}^{2}}{C(0)}-\\frac{C^{\\prime}{\\left(\\frac{\\eta^{2}}{2}\\right)}^{2}}{-C^{\\prime}(0)}\\eta^{2}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $\\Phi$ is the cumulative distribution function (cdf) of the standard normal distribution. ", "page_idx": 35}, {"type": "text", "text": "Proof. Note that the conditional variance is with the usual argument about the covariance matrices (cf. the proof of Thoerem 4.2) using Lemma G.2 and an application of Theorem G.1 given by ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sigma^{2}(\\|w\\|^{2}):=\\mathrm{Var}[\\mathbf{J}(w-\\mathbf{d})\\mid\\mathbf{J}(w),\\nabla\\mathbf{J}(w)]=C(0)-\\frac{C\\big(\\frac{\\|\\mathbf{d}\\|^{2}}{2}\\big)^{2}}{C(0)}-\\frac{C^{\\prime}\\big(\\frac{\\|\\mathbf{d}\\|^{2}}{2}\\big)^{2}}{-C^{\\prime}(0)}\\|\\mathbf{d}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Since the conditional distribution is normal (by Theorem G.1), we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{\\mathbf{J}(w-\\mathbf{d})-\\mathbb{E}[\\mathbf{J}(w-\\mathbf{d})\\mid\\mathbf{J}(w),\\nabla\\mathbf{J}(w)]}{\\sigma(\\|w\\|^{2})}\\sim{\\mathcal{N}}(0,1).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "But this implies the claim ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\Big(\\mathbf{J}(w-\\mathbf{d})\\leq\\mathbb{E}[\\mathbf{J}(w-\\mathbf{d})\\mid\\mathbf{J}(w),\\nabla\\mathbf{J}(w)]+\\rho_{\\gamma}(\\|\\mathbf{d}\\|^{2})\\Big)}\\\\ &{\\;=\\mathbb{P}\\Big(\\frac{\\mathbf{J}\\left(w-\\mathbf{d}\\right)-\\mathbb{E}[\\mathbf{J}(w-\\mathbf{d})\\mid\\mathbf{J}(w),\\nabla\\mathbf{J}(w)]}{\\sigma(\\|w\\|^{2})}\\leq\\Phi^{-1}(\\gamma)\\Big)}\\\\ &{\\;=\\Phi(\\Phi^{-1}(\\gamma))=\\gamma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "To avoid the Gaussian assumption, one could apply the Markov inequality instead, or another applicable concentration inequality. \u53e3 ", "page_idx": 36}, {"type": "text", "text": "Using this upper bound, we obtain a natural conservative extension of RFD ", "page_idx": 36}, {"type": "text", "text": "Extension E.4 $\\gamma$ -conservative RFD). Let $\\mathbf{J}\\sim{\\mathcal{N}}({\\boldsymbol{\\mu}},{\\boldsymbol{C}})$ and $\\rho_{\\gamma}(\\eta^{2})=\\Phi^{-1}(\\gamma)\\sigma(\\eta^{2})$ , where $\\sigma$ is the conditional standard deviation as defined in Lemma $E.3$ . Then the conservative RFD step direction is given by ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\eta^{*}\\frac{\\nabla\\mathbf{J}(w)}{\\|\\nabla\\mathbf{J}(w)\\|}=\\underset{\\mathbf{d}}{\\mathrm{argmin}}\\,\\mathbb{E}[\\mathbf{J}(w-\\mathbf{d})\\mid\\mathbf{J}(w),\\nabla\\mathbf{J}(w)]+\\rho_{\\gamma}(\\|\\mathbf{d}\\|^{2})\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and the $\\gamma$ -conservative RFD step size is given by ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\eta^{*}=\\arg\\!\\operatorname*{min}_{\\eta}\\frac{C\\big(\\frac{\\eta^{2}}{2}\\big)}{C(0)}(\\mathbf{J}(w)-\\mu)-\\eta\\frac{C^{\\prime}\\big(\\frac{\\eta^{2}}{2}\\big)}{C^{\\prime}(0)}\\|\\nabla\\mathbf{J}(w)\\|+\\rho_{\\gamma}(\\eta^{2}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. The proof is the same as in Theorem 4.2 with Lemma 4.1 replaced by Lemma E.3. ", "page_idx": 36}, {"type": "text", "text": "Taking multiple steps should generally have an averaging effect, so we expect faster convergence for almost risk neutral minimization of the conditional expectation (i.e. $\\begin{array}{r}{\\gamma\\approx\\frac{1}{2},}\\end{array}$ ). Here $\\gamma$ is a natural parameter to vary conservatism. In a software implementation it might be a good idea to call this parameter \u2018conservatism\u2019 and rescale it to be in $[\\bar{0},1]$ instead of $[{\\textstyle{\\frac{1}{2}}},{\\bar{1}}]$ . But formulas look cleaner with \u03b3. ", "page_idx": 36}, {"type": "text", "text": "In Bayesian optimization it is much more common to reverse this approach and minimize a lower confidence bound (\u2018conservatism\u2019 $<0$ or $\\gamma<\\textstyle{\\frac{1}{2}}.$ ) in order to encourage exploration. But since RFD is forgetful, this is not a good idea for RFD. ", "page_idx": 36}, {"type": "text", "text": "Remark E.5 (Conservative RFD coincides asymptotically with RFD in high dimension). While conservative RFD might seem like a good approach to fix the instability of RFD under the isotropy assumption on some optimization problems, the variance generally vanishes in high dimension [see 5] and conservative RFD coincides asymptotically with RFD. We therefore believe that the underlying issue is not an overly risk-affine algorithm but rather that distributional assumptions, in particular the stationarity assumption, are violated when instabilities occur (cf. Section F). ", "page_idx": 36}, {"type": "text", "text": "Nevertheless, conservative RFD might be a good approach for lower dimensional, risk-sensitive applications. ", "page_idx": 36}, {"type": "text", "text": "E.3 Beyond the Gaussian assumption ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In this section we sketch how the extension beyond the Gaussian case using the \u201cbest linear unbiased estimator\u201d BLUE [e.g. 27, ch. 7] works. ", "page_idx": 36}, {"type": "text", "text": "For this we recapitulate what a BLUE is. A linear estimator $\\hat{Y}$ of $Y$ using $X_{1},\\ldots,X_{n}$ is of the form ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\hat{Y}\\in\\operatorname{span}\\{X_{1},\\ldots,X_{n}\\}+\\mathbb{R}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "The set of unbiased linear estimators is defined as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{LUE}=\\mathrm{LUE}\\left[Y\\mid X_{1},\\ldots,X_{n}\\right]=\\{\\hat{Y}\\in\\mathrm{span}\\{X_{1},\\ldots,X_{n}\\}+\\mathbb{R}:\\mathbb{E}[\\hat{Y}]=\\mathbb{E}[Y]\\}}&{{\\mathrm{(2)~\\mathbb~{d}~\\mathbb~{d}~\\mathbb~{d}~\\hat{Y}~}}}\\\\ {=\\{\\hat{Y}+\\mathbb{E}[Y]:\\hat{Y}\\in\\mathrm{span}\\{X_{1}-\\mathbb{E}[X_{1}],\\ldots,X_{n}-\\mathbb{E}[X_{n}]\\}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "And the BLUE is the best linear unbiased estimator, i.e. ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathrm{BLUE}[Y\\mid X_{1},\\ldots,X_{n}]:=\\operatorname{argmin}_{\\hat{Y}\\in\\mathrm{LUE}}[\\Vert\\hat{Y}-Y\\Vert^{2}].\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Other risk functions to minimize are possible, but this is the usual one. ", "page_idx": 37}, {"type": "text", "text": "Lemma E.6. If $X,Y_{1},\\ldots,Y_{n}$ are multivariate normal distributed, then we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{BLUE}[Y\\mid X_{1},\\dots,X_{n}]=\\mathbb{E}[Y\\mid X_{1},\\dots,X_{n}]}\\\\ &{\\qquad\\qquad\\qquad\\Bigg(=\\underset{\\hat{Y}\\in\\{f(X_{1},\\dots,X_{n}):f\\,m e a s.\\}}{\\mathrm{argmin}}\\mathbb{E}[\\|Y-\\hat{Y}\\|^{2}]\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. We observe that the conditional expectation of Gaussian random variables is linear (Theorem G.1). So as a linear function its $L^{2}$ risk must be larger or equal to that of the BLUE. And as an $L^{2}$ projection [30, Cor. 8.17] the conditional expectation was already optimal. \u53e3 ", "page_idx": 37}, {"type": "text", "text": "If we now replace the conditional expectation with the BLUE, then all our theory remains the same because the result in Theorem G.1 remains the BLUE for general distributions [27]. Instead of minimizing ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{d}}\\mathbb{E}[\\mathbf{J}(w-\\mathbf{d})\\mid\\mathbf{J}(w),\\nabla\\mathbf{J}(w)]\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "we can therefore always minimize ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{d}}\\mathrm{BLUE}[\\mathbf{J}(w-\\mathbf{d})\\mid\\mathbf{J}(w),\\nabla\\mathbf{J}(w)]\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "without the Gaussian assumption and all our results can be translated to this case. The reader only needs to replace all mentions of Theorem G.1 with the BLUE equivalent and replace all \u201cidependence\u201d claims with \u201cuncorrelated\u201d. ", "page_idx": 37}, {"type": "text", "text": "F Input invariance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In this section we generalize the notion of isotropy to non-stationary isotropy and discuss why we believe this generalization is necessary. Recall that we motivated isotropy as an invariant distribution with regard to isometric transformations of the input. In particular its distribution stays invariant with regard to translations (also known as stationarity), which we do not believe plausible for cost functions, because the cost at zero $\\mathbf{J}(0)$ behaves fundamentally different from the cost of any other parameter vector. ", "page_idx": 37}, {"type": "text", "text": "In the following we will therefore generalize this notion to general input invariant distributions. And we will discuss their applicability to machine learning after we characterize the named categories. ", "page_idx": 37}, {"type": "text", "text": "Definition F.1 (Input Invariance). A random function f is $\\Phi$ -input invariant, if 5 ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathbf{f}}=\\mathbb{P}_{\\mathbf{f}\\circ\\phi}\\qquad\\forall\\phi\\in\\Phi.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "For certain sets of $\\Phi$ we give these $\\Phi$ -input invariant distributions names ", "page_idx": 37}, {"type": "text", "text": "\u2022 If $\\Phi$ is the set of isometries, we call f (stationary) isotropic. ", "page_idx": 38}, {"type": "text", "text": "\u2022 If $\\Phi$ is the set of translations, we call f stationary.   \n\u2022 If $\\Phi$ is the set of linear isometries, we call f non-stationary isotropic. ", "page_idx": 38}, {"type": "text", "text": "We further say a random function f is $n$ -weakly $\\Phi$ -input invariant, if for all $\\phi\\in\\Phi$ , all $k\\leq n$ and all $x_{i}$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{f}(\\phi(x_{1}))\\cdot\\cdot\\cdot\\cdot\\cdot\\mathbf{f}(\\phi(x_{k}))]=\\mathbb{E}[\\mathbf{f}(x_{1})\\cdot\\cdot\\cdot\\cdot\\cdot\\mathbf{f}(x_{k})].\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Since second moments fully determine Gaussian distributions, 2-weakly input invariance is special, because it is equivalent to full input invariance in the Gaussian case. So an omitted $n$ equals 2. \u201cWeakly isometry invariant\u201d naturally becomes \u201cweakly isotropic\u201d, etc. ", "page_idx": 38}, {"type": "text", "text": "While stationary and stationary isotropic random functions are well known [e.g. 44, 1], we are not aware of research on non-stationary isotropy although we doubt the concept is new. It turns out that the different notions of input isometry have simple characterizations in terms of the covariance functions. We present these in Theorem F.2 of which the stationary isotropic and stationary case are already well known. ", "page_idx": 38}, {"type": "text", "text": "Theorem F.2 (Characterization of Weak Input Invariances). Let f : $\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be a random function, then f is ", "page_idx": 38}, {"type": "text", "text": "1. weakly stationary, if and only if there exists $\\mu\\in\\mathbb{R}$ and function $C:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ such that for all x, y ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mu_{\\mathbf{f}}(x)=\\mu,\\qquad\\mathcal{C}_{\\mathbf{f}}(x,y)=C(x-y).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "2. weakly non-stationary isotropic, if and only if there exist functions $\\mu:\\mathbb{R}_{\\geq0}\\rightarrow\\mathbb{R}$ and $\\kappa:D\\rightarrow\\mathbb{R}$ with $D=\\{\\lambda\\in\\mathbb{R}_{\\geq0}^{2}\\times\\mathbb{R}:|\\lambda_{3}|\\leq2{\\sqrt{\\lambda_{1}\\lambda_{2}}}\\}\\subseteq\\mathbb{R}^{3}$ . such that for all $x,y$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{\\mathbf{f}}(x)=\\mu\\big(\\frac{\\lVert x\\rVert^{2}}{2}\\big)\\qquad}\\\\ {\\mathcal{C}_{\\mathbf{f}}(x,y)=\\kappa\\big(\\frac{\\lVert x\\rVert^{2}}{2},\\frac{\\lVert y\\rVert^{2}}{2},\\langle x,y\\rangle\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "3. weakly stationary isotropic, if and only if there exists $\\mu\\in\\mathbb{R}$ and a function $C:\\mathbb{R}_{\\geq0}\\rightarrow\\mathbb{R}$ such that for all $x,y$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{\\mathbf{f}}(x)=\\mu,\\qquad\\mathcal{C}_{\\mathbf{f}}(x,y)=C\\bigl(\\frac{\\|x-y\\|^{2}}{2}\\bigr)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. The proof essentially follow as a corollary from a characterization of isometries (Proposition F.4). For details see Subsec F.2. \u53e3 ", "page_idx": 38}, {"type": "text", "text": "Non-stationary isotropy is therefore a generalization of stationary isotropy. It allows the zero parameter vector to have special meaning because the distribution is only invariant to linear isometries (i.e. rotations and reflections) which keep the zero in place. ", "page_idx": 38}, {"type": "text", "text": "It is important to highlight, that a geometric anisotropy (Section E.1) retains stationarity, while breaking non-stationary isotropy. A similar geometric generalization could also be applied to nonstationary isotropy. ", "page_idx": 38}, {"type": "text", "text": "Another important observation is the fact, that non-stationary isotropy coincides with stationary isotropy on the sphere. I.e. when $\\left\\|x\\right\\|$ and $\\|y\\|$ are constant, the function ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\kappa{\\Big(}{\\frac{\\|x\\|^{2}}{2}},{\\frac{\\|y\\|^{2}}{2}},{\\frac{\\|x-y\\|^{2}}{2}}{\\Big)}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "only depends on $\\|x-y\\|$ and the mean is also constant. In other words, we have stationary isotropy on the sphere. ", "page_idx": 38}, {"type": "text", "text": "Isotropy might therefore \u2018get by\u2019 as an assumption in machine learning, as parameters are typically initialized on the sphere. This is because Glorot intialization [18] samples parameter entries independently, so their squared norm ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\|w\\|^{2}=\\sum_{i=1}^{d}(w^{(i)})^{2}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "is a sum of independent random variables which are normalized such that a law of large numbers applies. Up to small variance their lengths are therefore all the same, and are placed on a sphere at this radius. ", "page_idx": 39}, {"type": "text", "text": "If we leave this sphere, this equivalence stops being true. Weight normalization [46], batch normalization [25], weight decay [e.g. 19] or equivalently $L^{2}$ regularization, etc. might all contribute to keep this assumption intact. ", "page_idx": 39}, {"type": "text", "text": "But in the following section we will see, that even simple linear regressions considered by researches investigating the average case behavior on quadratic functions [e.g. 58, 43, 33, 12, 9, 40, 41], require non-stationary isotropy. Moreover the covariance kernels suggested by investigations into random neuronal networks [e.g. 54, 8] are also non-stationary isotropic but not stationary isotropic. ", "page_idx": 39}, {"type": "text", "text": "F.1 Random linear regression ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In this section, we determine the distribution of the cost function induced by a simple linear regression. For this we define the mean squared sample loss ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\ell_{i}(w)=(Y-f_{w}(X))^{2},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the random data $X$ is mapped by the true relationship f to labels $Y=\\mathbf{f}(X)$ and ", "page_idx": 39}, {"type": "equation", "text": "$$\nf_{w}(x)=\\langle x,w\\rangle\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "is a linear model. If the true relationship $\\mathbf{f}$ is also a random linear function $\\mathbf{f}(x)=\\langle\\theta,x\\rangle$ with random signal $\\theta\\sim\\mathcal{N}(0,\\mathbb{I})$ independent of input $X\\sim{\\mathcal{N}}(0,\\mathbb{I})$ , then the cost function is given by ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{J}(w)=\\mathbb{E}[\\ell_{i}(w)\\mid\\mathbf{f}]=\\mathbb{E}[\\langle\\theta-w,X\\rangle^{2}\\mid\\theta]}\\\\ &{\\quad\\quad=(\\theta-w)^{T}\\mathbb{E}[X X^{T}](\\theta-w)}\\\\ &{\\quad\\quad=\\|\\theta-w\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Lemma F.3. The expectation and covariance of $\\mathbf{J}$ are given by ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\mathbb{E}[\\mathbf{J}(w)]=c o n s t.+\\|w\\|^{2}}\\\\ {\\operatorname{Cov}(\\mathbf{J}(w),\\mathbf{J}(\\tilde{w}))=c o n s t.+4\\langle w,\\tilde{w}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "In particular, the cost $\\mathbf{J}$ is non-stationary isotropic, but not stationary isotropic. ", "page_idx": 39}, {"type": "text", "text": "Proof. Its expectation is given by ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathbf{J}(w)]=\\mathbb{E}[\\|\\theta-w\\|^{2}]=\\mathbb{E}[\\|\\theta\\|^{2}]-2\\langle\\underbrace{\\mathbb{E}[\\theta]}_{=0},w\\rangle+\\|w\\|^{2}}\\\\ &{\\qquad\\qquad=\\mathrm{const.}+\\|w\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "In particular it is not constant, but dependent on $\\lVert w\\rVert^{2}$ , which means that we do not have stationary isotropy. But there is still hope for non-stationary isotropy, and this is essentially true as can be seen by calculating ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Cov}(\\mathbf{J}(w),\\mathbf{J}(\\widetilde{w}))=\\mathbb{E}\\Big[(\\mathbf{J}(w)-\\mathbb{E}[\\mathbf{J}(w)])(\\mathbf{J}(\\widetilde{w})-\\mathbb{E}[\\mathbf{J}(\\widetilde{w})])\\Big]}\\\\ &{=\\mathbb{E}\\Big[(\\|\\theta\\|^{2}-\\mathbb{E}\\|\\theta\\|^{2}-2\\langle\\theta,w\\rangle)(\\|\\theta\\|^{2}-\\mathbb{E}\\|\\theta\\|^{2}-2\\langle\\theta,\\widetilde{w}\\rangle)\\Big]}\\\\ &{=\\mathrm{~Var}(\\|\\theta\\|^{2}-\\mathbb{E}\\|\\theta\\|^{2})}\\\\ &{\\quad-\\;2\\mathbb{E}[(\\|\\theta\\|^{2}-\\mathbb{E}\\|\\theta\\|^{2})\\langle\\theta,w\\rangle]}\\\\ &{\\quad-\\;2\\mathbb{E}[(\\|\\theta\\|^{2}-\\mathbb{E}\\|\\theta\\|^{2})\\langle\\theta,\\widetilde{w}\\rangle]}\\\\ &{\\quad+\\;4w^{T}\\mathbb{E}[\\theta\\theta^{T}]\\widetilde{w}}\\\\ &{=\\mathrm{Var}(\\|\\theta\\|^{2}-\\mathbb{E}\\|\\theta\\|^{2})+4\\langle w,\\widetilde{w}\\rangle}\\\\ &{=\\mathrm{const.~}+4\\langle w,\\widetilde{w}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "because the terms in the middle are zero, e.g. ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}[(\\|\\theta\\|^{2}-\\mathbb{E}\\|\\theta\\|^{2})\\langle\\theta,w\\rangle]=\\langle\\underbrace{\\mathbb{E}[\\|\\theta\\|^{2}\\theta]}_{=0},w\\rangle-\\mathbb{E}\\|\\theta\\|^{2}\\langle\\underbrace{\\mathbb{E}[\\theta]}_{=0},w\\rangle\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the entries of $\\mathbb{E}[\\Vert\\theta\\Vert^{2}\\theta]$ are zero, because of independence and first moments being zero and third moments being zero. ", "page_idx": 39}, {"type": "text", "text": "F.2 Proof of Theorem F.2 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Proposition F.4 (Characterizing isometries). Let $\\mathcal{X}$ be a vectorspace and $x_{i},y_{i}\\in\\mathcal{X}$ for $i=1,\\hdots,n$ , then the following pairs of statements are equivalent ", "page_idx": 40}, {"type": "text", "text": "1. (a) $x_{i}-x_{j}=y_{i}-y_{j}$ for all $i,j$ $(b)$ there exists a translation $\\phi$ with $\\phi(x_{i})=y_{i}$ for all $i$ . ", "page_idx": 40}, {"type": "text", "text": "In the remainder we further assume $\\mathcal{X}$ to be a Hilbertspace, ", "page_idx": 40}, {"type": "text", "text": "2. (a) $\\left\\|{\\boldsymbol{x}}_{i}\\right\\|=\\left\\|{\\boldsymbol{y}}_{i}\\right\\|$ and $\\|x_{i}-x_{j}\\|=\\|y_{i}-y_{j}\\|$ for all $i,j$ $(b)$ there exists a linear isometry $\\phi$ with $\\phi(x_{i})=y_{i}$ for all $i$ .   \n3. (a) $\\|x_{i}-x_{j}\\|=\\|y_{i}-y_{j}\\|$ for all $i,j$ $(b)$ there exists an (affine) isometry $\\phi$ with $\\phi(x_{i})=y_{i}$ for all $i$ . ", "page_idx": 40}, {"type": "text", "text": "Proof. $(1\\mathrm{a})\\Rightarrow(1\\mathrm{b})$ : we define ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\phi(x):=x+(y_{0}-x_{0}),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "which implies ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\phi(x_{i})=x_{i}-x_{0}+y_{0}=(y_{i}-y_{0})+y_{0}=y_{i}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "$(16)\\Rightarrow(1\\mathrm{a})$ : Let $\\phi(x)=x+c$ for some $c$ . Then we immediately have ", "page_idx": 40}, {"type": "equation", "text": "$$\ny_{i}-y_{j}=\\phi(x_{i})-\\phi(x_{j})=x_{i}+c-(x_{j}+c)=x_{i}-x_{j}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "$(2\\mathrm{a})\\Rightarrow(2\\mathrm{b})$ : By the polarization formula, for all $i,j$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\langle x_{i},x_{j}\\rangle=\\frac{\\|x_{i}\\|^{2}+\\|y_{i}\\|^{2}-\\|x_{i}-x_{j}\\|^{2}}{2}=\\langle y_{i},y_{j}\\rangle.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We apply the Gram-Schmidt orthonormalization procedure to both $x_{i}$ and $y_{i}$ such that ", "page_idx": 40}, {"type": "equation", "text": "$$\nU_{k_{n}}=\\operatorname{span}(u_{1},\\dots,u_{k_{n}})=\\operatorname{span}(x_{1},\\dots,x_{n})\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "for orthonormal $u_{i}$ where we skip $x_{m}$ if it is already in $U_{k_{m-1}}$ (resulting in $k_{m}=k_{m-1})$ ), and similarly ", "page_idx": 40}, {"type": "equation", "text": "$$\nV_{k_{n}}=\\operatorname{span}(v_{1},\\ldots,v_{k_{n}})=\\operatorname{span}(y_{1},\\ldots,y_{n}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Since this procedure only uses scalar products, we inductively get ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\langle x_{k},u_{j}\\rangle=\\langle y_{k},v_{j}\\rangle\\quad\\forall k,j\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We now extend $u_{i}$ and $v_{i}$ to orthonormal basis of $\\mathcal{X}$ and define the linear mapping by its behavior on the basis elements $\\phi\\;:\\;u_{i}\\;\\mapsto\\;v_{i}$ . Mapping an orthonormal basis to an orthonormal basis is an isometry and we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\phi(\\boldsymbol{x}_{k})=\\phi\\Bigl(\\sum_{j=1}^{k}\\langle\\boldsymbol{x}_{k},\\boldsymbol{u}_{j}\\rangle u_{j}\\Bigr)}\\\\ {\\displaystyle\\qquad=\\sum_{j=1}^{k}\\langle\\boldsymbol{x}_{k},\\boldsymbol{u}_{j}\\rangle\\phi(u_{j})=\\sum_{j=1}^{k}\\langle y_{k},v_{j}\\rangle v_{j}}\\\\ {\\displaystyle\\qquad=y_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "$(2\\mathsf{b})\\Rightarrow(2\\mathsf{a})$ : Isometries preserve distances by definition. This implies $\\|x_{i}-x_{j}\\|=\\|y_{i}-y_{j}\\|$ . And linear functions map 0 to $0$ , so we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\|x_{i}\\|=\\|x_{i}-0\\|=\\|\\phi(x_{i})-\\phi(0)\\|=\\|y_{i}\\|.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "$(3\\mathrm{a})\\Rightarrow(3\\mathrm{b})$ : We define ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\tilde{x}_{i}=x_{i}-x_{0}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and similarly for $y$ . In particular, $\\tilde{x}_{0}=\\tilde{y}_{0}=0$ . Since $\\tilde{x}_{i}$ and $\\tilde{y}_{i}$ satisfy the requirements of 2, there exists a linear isometry $\\tilde{\\phi}$ with $\\tilde{\\phi}(\\tilde{x}_{i})=\\tilde{y}_{i}$ . Then the isometry ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\phi:x\\mapsto\\tilde{\\phi}(x-x_{0})+y_{0}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "does the job. ", "page_idx": 40}, {"type": "text", "text": "$(3\\mathbf{b})\\Rightarrow(3\\mathbf{a})$ : This is precisely the distance preserving property of Isometries. ", "page_idx": 41}, {"type": "text", "text": "Theorem F.2 (Characterization of Weak Input Invariances). Let f : $\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be a random function, then f is ", "page_idx": 41}, {"type": "text", "text": "1. weakly stationary, if and only if there exists $\\mu\\in\\mathbb{R}$ and function $C:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ such that for all x, y ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mu_{\\mathbf{f}}(x)=\\mu,\\qquad\\mathcal{C}_{\\mathbf{f}}(x,y)=C(x-y).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "2. weakly non-stationary isotropic, if and only if there exist functions $\\mu:\\mathbb{R}_{\\geq0}\\,\\rightarrow\\,\\mathbb{R}$ and $\\kappa:D\\rightarrow\\mathbb{R}$ with $D=\\{\\lambda\\in\\mathbb{R}_{\\geq0}^{2}\\times\\mathbb{R}:|\\lambda_{3}|\\leq2{\\sqrt{\\lambda_{1}\\lambda_{2}}}\\}\\subseteq\\mathbb{R}^{3}$ . such that for all $x,y$ ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{\\mathbf{f}}(x)=\\mu\\big(\\frac{\\lVert x\\rVert^{2}}{2}\\big)\\qquad}\\\\ {\\mathcal{C}_{\\mathbf{f}}(x,y)=\\kappa\\big(\\frac{\\lVert x\\rVert^{2}}{2},\\frac{\\lVert y\\rVert^{2}}{2},\\langle x,y\\rangle\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "3. weakly stationary isotropic, if and only if there exists $\\mu\\in\\mathbb{R}$ and a function $C:\\mathbb{R}_{\\geq0}\\rightarrow\\mathbb{R}$ such that for all $x,y$ ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{\\mathbf{f}}(x)=\\mu,\\qquad\\mathcal{C}_{\\mathbf{f}}(x,y)=C\\bigl(\\frac{\\|x-y\\|^{2}}{2}\\bigr)}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof. Starting from the mean and covariance function it is easy to check 2-weak non-stationary isotropy. So we only need to check the other direction. ", "page_idx": 41}, {"type": "text", "text": "The proof is essentially an application of Prop. F.4. For brevity (and since the other two results are well known), we will only prove the weakly non-stationary isotropic case (the other two cases can be proven with minor adjustments to the proof). ", "page_idx": 41}, {"type": "text", "text": "Without loss of generality, we will find the slightly different representation ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{f}_{d}(x)]=\\tilde{\\mu}(\\|x\\|)\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "$\\mathcal{C}_{\\mathbf{f}_{d}}(x,y)=\\tilde{\\kappa}(\\|x\\|,\\|y\\|,\\langle x,y\\rangle),$ ", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the domain of $\\tilde{\\kappa}$ is given by $\\tilde{D}=\\{\\lambda\\in\\mathbb{R}_{\\geq0}^{2}\\times\\mathbb{R}:|\\lambda_{3}|\\leq\\lambda_{1}\\lambda_{2}\\}$ . The representation of the theorem is then equivalent by a change to ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu(\\lambda):=\\tilde{\\mu}\\big(\\frac{\\lambda^{2}}2\\big)\\quad\\mathrm{and}\\quad\\kappa(\\lambda_{1},\\lambda_{2},\\lambda_{2}):=\\tilde{\\kappa}\\big(\\frac{\\lambda_{1}^{2}}2,\\frac{\\lambda_{2}^{2}}2,\\lambda_{3}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "First we want to find $\\mu$ . Let $v$ be some vector (w.l.o.g. $\\|v\\|=1)$ ). Then we define ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mu}(\\boldsymbol{r}):=\\mathbb{E}[\\mathbf{f}_{d}(\\boldsymbol{r v})]\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Now we need to show that this definition of $\\mu$ is an appropriate mean function. For this choose any $x\\in\\mathscr{X}$ . Then for $r=\\|x\\|$ there exists by Prop. F.4 (2.) a non-stationary isometry $\\phi$ such that $\\phi(x)=r v$ (we use $n=1$ ). With 1-weak non-stationary isotropy of $\\mathbf{f}_{d}$ this implies ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{f}_{d}(x)]=\\mathbb{E}[\\mathbf{f}_{d}(r v)]=\\mu(r)=\\mu(\\|x\\|).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Next we need to define $\\kappa(r_{x},r_{y},r_{x y})$ . For this, choose two orthonormal vectors $v,w$ . For every $r=(r_{x},r_{y},r_{x y})\\in\\tilde{D}$ we define ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{x^{*}(r)=r_{x}v}}\\\\ {{y^{*}(r)=\\displaystyle\\frac{r_{x y}}{r_{x}}v+\\sqrt{r_{y}^{2}-\\frac{r_{x y}^{2}}{r_{x}^{2}}}w.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Where $r\\in\\tilde{D}$ ensures $|r_{x y}|\\le r_{x}r_{y}$ and thus $\\begin{array}{r}{r_{y}^{2}-\\frac{r_{x y}^{2}}{r_{x}^{2}}\\ge0}\\end{array}$ . Then we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\|x^{*}(r)\\|=r_{x},\\quad\\|y^{*}(r)\\|=r_{y},\\quad\\mathrm{and}\\quad\\langle x^{*}(r),x^{*}(y)\\rangle=r_{x y},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and define ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\kappa(r_{x},r_{y},r_{x y}):=\\mathcal{C}_{\\mathbf{f}_{d}}(x^{*}(r),y^{*}(r)).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Again, we need to show that this kernel does the job. For this, choose any $x,y\\in\\mathcal{X}$ . For ", "page_idx": 41}, {"type": "equation", "text": "$$\nr:=(\\|x\\|,\\|y\\|,\\langle x,y\\rangle),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "which is in $\\tilde{D}$ by the Cauchy-Schwarz inequality, the induced $x^{\\ast}(r)$ and $y^{\\ast}(r)$ satisfy by (27) ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\|x^{*}(r)\\|=\\|x\\|,\\quad\\|y^{*}(r)\\|=\\|y\\|\\quad{\\mathrm{and}}\\quad\\|x^{*}(r)-y^{*}(r)\\|=\\|x-y\\|\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "By Prop. F.4 (2.) there therefore exists an isometry $\\phi$ such that $\\phi(x)=x^{*}(r)$ and $\\phi(y)=y^{*}(r)$ . By 2-weak input isotropy of $\\mathbf{f}_{d}$ we conclude ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{C}_{\\mathtt{f}_{d}}(x,y)\\overset{\\mathrm{isotrop.}}{=}\\mathcal{C}_{\\mathtt{f}_{d}}(x^{*}(r),y^{*}(r))\\overset{\\mathrm{def.}}{=}\\kappa\\big(\\|x\\|,\\|y\\|,\\langle x,y\\rangle\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "G Technical ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "G.1 Conditional Gaussian distribution ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "For the following well known result we found a tidy proof giving insight into the reason it is true, so we wrote it down for your convenience but do not even expect this particular proof to be new. ", "page_idx": 42}, {"type": "text", "text": "Theorem G.1 (Conditional Gaussian distribution). Let $\\boldsymbol{X}\\sim\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$ be a multivariate Gaussian vector where the covariance matrix is a block matrix of the form ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mu=\\left[\\!\\!{\\begin{array}{c}{{\\mu_{1}}}\\\\ {{\\mu_{2}}}\\end{array}}\\!\\!\\right]\\quad a n d\\quad\\Sigma=\\left[\\!\\!{\\Sigma_{11}}\\quad{\\Sigma_{12}}\\!\\!\\right],\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "then assuming $\\Sigma_{11}$ is invertible, the conditional distribution of $X_{2}$ given $X_{1}$ is ", "page_idx": 42}, {"type": "equation", "text": "$$\nX_{2}\\mid X_{1}\\sim{\\mathcal{N}}(\\mu_{2|1},\\Sigma_{2|1}),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "with conditional mean and variance ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{2|1}:=\\mu_{2}+\\Sigma_{21}\\Sigma_{11}^{-1}(X_{1}-\\mu_{1})}\\\\ &{\\Sigma_{2|1}:=\\Sigma_{22}-\\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. Let ${\\bar{X}}:=X-\\mu$ be the centered version of $X$ . There exists some lower triangular matrix $L$ (even if $\\Sigma$ is only positive semidefinite only not uniquely) such that $\\Sigma=L L^{T}$ (i.e. the Cholesky Decomposition). We can then write without loss of generality ", "page_idx": 42}, {"type": "equation", "text": "$$\nX-\\mu=:\\left[\\!\\!\\begin{array}{c}{{\\bar{X}_{1}}}\\\\ {{\\bar{X}_{2}}}\\end{array}\\!\\!\\right]=\\left[\\!\\!\\begin{array}{c c}{{L_{11}}}&{{0}}\\\\ {{L_{21}}}&{{L_{22}}}\\end{array}\\!\\!\\right]\\left[\\!\\!\\begin{array}{c}{{Y_{1}}}\\\\ {{Y_{2}}}\\end{array}\\!\\!\\right]=L Y\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "with independent standard normal $Y_{i}$ , i.e. $Y\\sim{\\mathcal{N}}(0,\\mathbb{I})$ . Since $\\Sigma_{11}$ is invertible, so is $L_{11}$ and therefore the map from $Y_{1}$ to $X_{1}$ . Conditioning on $X_{1}$ is therefore equivalent to conditioning on $Y_{1}$ . But we have ", "page_idx": 42}, {"type": "equation", "text": "$$\nX_{2}=\\mu_{2}+\\bar{X}_{2}=\\underbrace{\\mu_{2}+L_{21}Y_{1}}_{\\begin{array}{l}{{}}\\end{array}}~+~~~~~\\underbrace{L_{22}Y_{2}}_{\\begin{array}{l}{{}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "So it follows that ", "page_idx": 42}, {"type": "equation", "text": "$$\nX_{2}\\mid X_{1}\\sim{\\mathcal{N}}(\\mu_{2|1},\\Sigma_{2|1})\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "with ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{2|1}:=\\mu_{2}+L_{21}Y_{1}}\\\\ &{\\Sigma_{2|1}:=L_{22}L_{22}^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "What is left to do, is find a representation for the $L_{i j}$ using the block matrices of $\\Sigma$ . For this note ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\Sigma=L L^{T}=\\left[\\!\\!\\begin{array}{c c}{{L_{11}L_{11}^{T}}}&{{L_{11}L_{21}^{T}}}\\\\ {{L_{21}L_{11}^{T}}}&{{L_{22}L_{22}^{T}+L_{21}L_{21}^{T}}}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "This implies ", "page_idx": 42}, {"type": "equation", "text": "$$\nL_{21}Y_{1}=(L_{21}L_{11}^{T}L_{11}^{-T})(L_{11}^{-1}\\bar{X}_{1})=\\Sigma_{21}\\Sigma_{11}^{-1}(X_{1}-\\mu_{1})\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "so we have the desired conditional expectation, and finally ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{22}L_{22}^{T}=\\Sigma_{22}-L_{21}L_{21}^{T}}\\\\ &{\\qquad\\qquad=\\Sigma_{22}-\\underbrace{L_{21}\\bigl(L_{11}^{T}}_{=\\Sigma_{21}}\\underbrace{L_{11}^{-T}}_{=\\Sigma_{11}^{-1}}\\underbrace{(L_{11}^{-1}}_{=\\Sigma_{12}}\\underbrace{L_{11}}_{=\\Sigma_{12}}\\bigr)L_{21}^{T}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "G.2 Covariance of derivatives ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "By Swapping integration and differentiation we have for a centered random function f ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathrm{Cov}(\\partial_{x_{i}}\\mathbf{f}(x),\\mathbf{f}(y))=\\mathbb{E}[\\partial_{x_{i}}\\mathbf{f}(x)\\mathbf{f}(y)]=\\partial_{x_{i}}\\mathbb{E}[\\mathbf{f}(x)\\mathbf{f}(y)]}\\\\ {\\quad\\quad\\quad\\quad\\quad=\\partial_{x_{i}}{\\mathcal{C}}_{\\mathbf{f}}(x,y)}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "So the covariance of a derivative of f with f is equal to a partial derivative of the covariance function [more details in 1]. Similarly other covariances can be calculated, e.g. ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathrm{Cov}(\\partial_{x_{i}}{\\bf f}(x),\\partial_{y_{i}}{\\bf f}(y))=\\partial_{x_{i}}\\partial_{y_{i}}\\mathcal{C}_{\\bf f}(x,y).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "For this reason the derivatives of the covariance function are interesting as they represent the covariance of derivatives. ", "page_idx": 43}, {"type": "text", "text": "Applying this observation to isotropic covariance functions ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Cov}(\\mathbf{f}(x),\\mathbf{f}(y))=C\\big(\\frac{\\|x-y\\|^{2}}{2}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "we obtain. ", "page_idx": 43}, {"type": "text", "text": "Lemma G.2 (Covariance of derivatives). Let f $\\sim{\\mathcal{N}}(\\mu,C)$ and $\\mathbf{d}=x-y,$ , then ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\mathrm{Cov}}{\\mathbf{f}(x)}\\left|\\begin{array}{c c}{\\mathbf{f}(y)}&{\\partial_{j}\\mathbf{f}(y)}\\\\ {C(\\frac{\\|\\mathbf{d}\\|^{2}}{2})}&{-C^{\\prime}(\\frac{\\|\\mathbf{d}\\|^{2}}{2})\\langle\\mathbf{d},e_{j}\\rangle}\\\\ {C^{\\prime}(\\frac{\\|\\mathbf{d}\\|^{2}}{2})\\langle\\mathbf{d},e_{i}\\rangle}&{-\\Big[C^{\\prime\\prime}(\\frac{\\|\\mathbf{d}\\|^{2}}{2})\\langle\\mathbf{d},e_{j}\\rangle\\langle\\mathbf{d},e_{i}\\rangle+C^{\\prime}(\\frac{\\|\\mathbf{d}\\|^{2}}{2})\\langle e_{j},e_{i}\\rangle\\Big]}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "G.3 Constrained linear optimization ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Let $U$ be a vectorspace. We define the projection of a vector $w$ onto $U$ by ", "page_idx": 43}, {"type": "equation", "text": "$$\nP_{U}(w):=\\underset{v\\in U}{\\mathrm{argmin}}\\;\\|v-w\\|^{2}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Lemma G.3 (Constrained maximiziation of scalar products). For a linear subspace $U\\subseteq\\mathbb{R}^{d}$ , we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{v\\in U}{\\mathrm{max}}\\left<v,w\\right>=\\lambda\\|P_{U}(w)\\|}\\\\ &{\\|v\\|{=}\\lambda}\\\\ &{\\underset{v\\in U}{\\mathrm{argmax}}\\langle v,w\\rangle=\\lambda\\frac{P_{U}(w)}{\\|P_{U}(w)\\|}}\\\\ &{\\|v\\|{=}\\lambda}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Before we get to the proof let us note that this immediately results in the following corollary about minimization. ", "page_idx": 43}, {"type": "text", "text": "Corollary G.4 (Constrained minimization of scalar products). ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{v\\in U}{\\mathrm{min}}\\left<v,w\\right>=-\\lambda\\|P_{U}(w)\\|}\\\\ &{\\|v\\|{=}\\lambda}\\\\ &{\\underset{v\\in U}{\\mathrm{argmin}}\\langle v,w\\rangle=-\\lambda\\frac{P_{U}(w)}{\\|P_{U}(w)\\|}}\\\\ &{\\|v\\|{=}\\lambda}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Proof of Corollary G.4. The trick is to move one \u2018\u2212\u2019 outside from $w=-(-w)$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{v\\in U}{\\operatorname*{min}}\\left\\langle v,w\\right\\rangle=-\\underset{v\\in U}{\\operatorname*{max}}\\left\\langle v,-w\\right\\rangle=-\\lambda\\|P_{U}(w)\\|}\\\\ &{\\|v\\|{=}\\lambda\\quad\\quad\\quad\\quad\\quad\\|v\\|{=}\\lambda}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where we have used in the last equation that the projection is linear (we can move the minus sign out) and the norm removes the inner minus sign. The argmin argument is similar. \u53e3 ", "page_idx": 43}, {"type": "text", "text": "Proof of Lemma G.3. Step 1: We claim that ", "page_idx": 43}, {"type": "equation", "text": "$$\nv^{*}=\\lambda\\frac{P_{U}(w)}{||P_{U}(w)||}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "results in the value $\\langle v^{*},w\\rangle=\\lambda\\|P_{U}(w)\\|$ . ", "page_idx": 43}, {"type": "text", "text": "For this we consider ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{U}(w)=\\underset{v\\in U}{\\mathrm{argmin}}\\ \\ \\ \\underbrace{\\|v-w\\|^{2}}_{=\\|v\\|^{2}-2\\langle v,w\\rangle+\\|w\\|^{2}}}\\\\ &{\\qquad\\quad=\\underset{v\\in U}{\\mathrm{argmin}}\\underbrace{\\|v\\|^{2}-2\\langle v,w\\rangle}_{=:f(v)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "we know that $t\\,\\mapsto\\,f(t\\langle w\\rangle_{U})$ is minimized at $t\\,=\\,1$ by the definition of $\\langle w\\rangle_{U}$ . The first order condition implies ", "page_idx": 44}, {"type": "equation", "text": "$$\n0\\overset{!}{=}\\frac{d}{d t}=2t\\|P_{U}(w)\\|^{2}-2\\langle P_{U}(w),w\\rangle\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and thus ", "page_idx": 44}, {"type": "equation", "text": "$$\n1=t^{*}=\\frac{\\langle P_{U}(w),w\\rangle}{||P_{U}(w)||^{2}}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Multiplying both sides by $\\lambda\\|P_{U}(w)\\|$ finishes this step ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\lambda\\|P_{U}(w)\\|=\\Big\\langle\\underbrace{\\lambda\\frac{P_{U}(w)}{\\|P_{U}(w)\\|}}_{=v^{*}},w\\Big\\rangle.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Step 2: By (33), we know that we can achieve the value we claim to be the maximum (and know the location $v^{*}$ to do so). So if we prove that we can not exceed this value, then it is a maximum and $v^{*}$ is the argmax. This would finish the proof. What remains to be shown is therefore ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\langle v,w\\rangle\\leq\\lambda\\|P_{U}(w)\\|\\qquad\\forall v\\in U:\\|v\\|=\\lambda.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Let $v\\in U$ with $\\|v\\|=\\lambda$ . Then for any $\\mu\\in\\mathbb{R}$ we can plug $\\mu v$ into $f$ from (32) to get ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu^{2}\\lambda^{2}-2\\mu\\langle v,w\\rangle=f(\\mu v)}\\\\ &{\\qquad\\qquad\\qquad\\overset{(32)}{\\geq}f(P_{U}w)=\\|P_{U}(w)\\|^{2}-2\\langle P_{U}w,w\\rangle}\\\\ &{\\qquad\\qquad\\qquad=-\\langle P_{U}w,w\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the last equation follows from (33) with $\\lambda=\\|P_{U}w\\|$ . Reordering we get for all $\\mu$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\langle P_{U}w,w\\rangle+\\mu^{2}\\lambda^{2}\\geq2\\mu\\langle v,w\\rangle\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "We now select \u00b5 = \u2225PU\u03bbw\u2225 and divide both sides by $\\mu$ to get ", "page_idx": 44}, {"type": "equation", "text": "$$\n2\\langle v,w\\rangle\\leq\\Big\\langle\\underbrace{\\frac{P_{U}(w)}{\\mu},w}_{=v^{*}}\\Big\\rangle+\\lambda\\|P_{U}(w)\\|=2\\lambda\\|P_{U}w\\|\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Dividing both sides by 2 yields the claim. ", "page_idx": 44}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 45}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: Section 8 recapitulates all the assumptions made and highlights possible generalizations. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 45}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: Section D provides all proofs of statements made in the main body and follows an identical structure for easier cross reference. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 46}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: While we do not have the necessary space to discuss all implementation details, we believe that we have discussed all relevant insights necessary to reproduce our results. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 46}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: The code of our algorithm is fairly well commented and passes pylint and flake8 linting. The code to perform the benchmarks is less polished and we have not seeded the covariance estimation sampling process, but, since we obtained similar results over multiple runs (Section A.1.1), we are confident that our results are reproducible. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 47}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: Training on the MNIST data set is fairly standard, so we feel like our brief outline is sufficient. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 47}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: Quantiles are plotted in Figure 3 and the figures of Section A and we provided a histogram of asymptotic learning rates resulting from multiple covariance estimation runs (Section A.1.1). ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 47}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 48}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 48}, {"type": "text", "text": "Answer: [No] ", "page_idx": 48}, {"type": "text", "text": "Justification: We have not kept careful track of resources used, as MNIST is a fairly small dataset for machine learning standards. We believe the compute was comparatively negligible, although the use of multiple GPUs was helpful in repeating experiments in parallel. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 48}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: See broader impacts. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 48}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: Since we focus on optimization theory our work has no societal impact beyond the advancement of the the field of Machine Learning, which may have many societal consequences, but none we feel necessary to address. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 49}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 49}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: We cite datasets and models used. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 49}, {"type": "text", "text": "", "page_idx": 50}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 50}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 50}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 50}]