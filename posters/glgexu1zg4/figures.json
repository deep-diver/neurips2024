[{"figure_path": "glGeXu1zG4/figures/figures_1_1.jpg", "caption": "Figure 1: The movie review \u201cHer acting never fails to impress\u201d is passed into a BERT language model fine-tuned to do sentiment analysis [8]. Presented are 1st, 2nd and 3rd order M\u00f6bius coefficients, with positive interactions in green and negative in red computed via (1). The coefficients explain how groups of words influence BERT's perception of sentiment. For instance, while never and fails have strong negative sentiments individually, when combined, they impose a profound positive sentiment. In the second row, the word never is deleted, resulting in a large change in sentiment. In contrast, the Shapley values of each word SV(\u00b7), presented at the bottom of the figure, are less informative.", "description": "This figure shows how the M\u00f6bius transform can provide a more nuanced understanding of sentiment analysis compared to simpler methods like Shapley values.  It uses a BERT model analyzing the sentence \"Her acting never fails to impress.\"  The M\u00f6bius coefficients, visualized by color-coded values, show the interaction effects of different word combinations on the overall sentiment. Note that the removal of the word \"never\" significantly alters the sentiment, highlighting the importance of higher-order interactions that Shapley values alone cannot capture.", "section": "1 Introduction"}, {"figure_path": "glGeXu1zG4/figures/figures_3_1.jpg", "caption": "Figure 2: These plots are strong indicators that sparsity and low degree assumptions are worthy of consideration. We consider three different learning tasks. The left-most plot shows results from an XGBoost [34] model used for breast cancer diagnosis. The middle plot shows results from word-level sentiment analysis task using a BERT model [8] like in Fig. 1. The right-most plot shows results from a multiple choice question and answer task also using a BERT model [35]. Error bars represent standard deviation over 10 different instances. Details for each setting are in Appendix B. In all cases, the number of features n \u2248 20, for which it is possible to perform the full M\u00f6bius transform. On the top row, we plot achievable faithfulness R2 as a function of sparsity. We observe that in all cases, faithfulness approaching 1 requires only a few thousand M\u00f6bius coefficients, motivating our sparsity assumption. The bottom row of plots considers achievable faithfulness vs. degree, i.e., what R\u00b2 can be achieved using only M\u00f6bius coefficients F up to a given degree. Here we observe that in nearly all cases, low degree coefficients suffice to get quite small R2, motivating our low degree assumption.", "description": "This figure empirically validates the sparsity and low-degree assumptions of the proposed algorithm.  It shows the faithfulness (R-squared) of the M\u00f6bius transform approximations for three different machine learning tasks (breast cancer diagnosis, sentiment analysis, and multiple choice) as a function of sparsity and degree. The plots demonstrate that a small number of coefficients (low sparsity and low degree) are sufficient to achieve high faithfulness, supporting the algorithm's assumptions.", "section": "Understanding Assumptions: Sparsity and Low Degree"}, {"figure_path": "glGeXu1zG4/figures/figures_5_1.jpg", "caption": "Figure 3: This figure considers a \u201csparsified\u201d version of the M\u00f6bius coefficients depicted in Fig 1, keeping only the largest 4 depicted. Two different sampling choices are shown, as well as the resulting aliasing sets. In the first aliasing set, there is one zeroton, two singletons, and one multiton. In the second aliasing set, there are two zerotons, one singleton, and one multiton.", "description": "This figure illustrates the concept of aliasing in the context of the M\u00f6bius transform.  It shows how different subsampling strategies (Aliasing 1 and Aliasing 2) can lead to different groupings of the non-zero M\u00f6bius coefficients. These groupings are categorized as zerotons (all zero coefficients), singletons (only one non-zero coefficient), and multitons (more than one non-zero coefficient). The figure highlights that the effectiveness of the sparse M\u00f6bius transform algorithm depends on maximizing the number of singletons.", "section": "3 Algorithm Overview"}, {"figure_path": "glGeXu1zG4/figures/figures_5_2.jpg", "caption": "Figure 3: This figure considers a \u201csparsified\u201d version of the M\u00f6bius coefficients depicted in Fig 1, keeping only the largest 4 depicted. Two different sampling choices are shown, as well as the resulting aliasing sets. In the first aliasing set, there is one zeroton, two singletons, and one multiton. In the second aliasing set, there are two zerotons, one singleton, and one multiton.", "description": "This figure shows two different ways of subsampling the M\u00f6bius transform to reduce the computational complexity.  Each subsampling method results in a different aliasing structure, shown as a bipartite graph with variable nodes (non-zero M\u00f6bius coefficients) and check nodes (subsampled function values). The different aliasing patterns highlight the impact of subsampling choices on the number of singletons (easily recovered coefficients) and multitons (coefficients that require further processing).", "section": "Algorithm Overview"}, {"figure_path": "glGeXu1zG4/figures/figures_8_1.jpg", "caption": "Figure 6: (a) Perfect reconstruction against n and sample complexity under Assumption 2.1. Holding C = 3, we scale b to increase the sample complexity. We observe that the number of samples required to achieve perfect reconstruction is scaling linearly in n as predicted. Results are plotted across 5 runs for each choice of b and n. (b) Plot of the noise-robust version of our algorithm. For various values of t, we set n = 500 and K = 500, using a group testing matrix with P = 1000. We plot the performance of our algorithm against SNR, measured in terms of the R\u00b2. Error bands represent the standard deviation over 10 runs. (c) Runtime comparison of SMT, SHAP-IQ [29], and t = 5 order FSI via LASSO [4]. All are computing the M\u00f6bius transform in the setting where all non-zero interactions are order t, K = 10. SMT easily outperforms both, while the other methods quickly become intractable. Error bands represent standard deviation over 10 runs.", "description": "This figure demonstrates the performance of the Sparse M\u00f6bius Transform (SMT) algorithm under different conditions. Subfigure (a) shows the perfect reconstruction percentage against the sample complexity and n under Assumption 2.1. Subfigure (b) shows the performance of the noise-robust version of the SMT algorithm for various t, n, K, and P. Subfigure (c) compares the runtime complexity of SMT, SHAP-IQ, and FSI via LASSO.", "section": "Results"}, {"figure_path": "glGeXu1zG4/figures/figures_8_2.jpg", "caption": "Figure 7: Since our ultimate goal is compact, meaningful and computable representations, we compare representations generated from SMT (Algorithm 1) with other popular explanation models. We plot R\u00b2 (faithfulness) vs. the number of terms used in the representation (sparsity). For Shapley and Banzhaf values, to generate an r-sparse representation, we use the top r magnitude values. For SMT and Faith-Banzhaf, we do a slightly more sophisticated refinement procedure. Faith-Banzhaf is included because it is the first-order metric that maximizes R\u00b2. As observed in the breast cancer and sentiment analysis tasks, SMT can achieve better R\u00b2 than other approaches by utilizing higher-order interactions. In the sentence-level multiple choice dataset, we observe less of a difference, since in those cases the entire answer to a question is usually contained in a single sentence, thus higher-order interactions provide little advantage. Error bands represent the standard deviation over 10 instances.", "description": "This figure compares the faithfulness (R\u00b2) of different explanation models (SMT, Shapley values, Banzhaf values, Faith-Banzhaf indices) against the sparsity (number of terms used in the representation) for three real-world machine learning tasks.  SMT consistently achieves higher faithfulness with the same or fewer terms compared to other methods, highlighting its ability to leverage higher-order interactions for better model interpretability.", "section": "Results"}, {"figure_path": "glGeXu1zG4/figures/figures_15_1.jpg", "caption": "Figure 2: These plots are strong indicators that sparsity and low degree assumptions are worthy of consideration. We consider three different learning tasks. The left-most plot shows results from an XGBoost [34] model used for breast cancer diagnosis. The middle plot shows results from word-level sentiment analysis task using a BERT model [8] like in Fig. 1. The right-most plot shows results from a multiple choice question and answer task also using a BERT model [35]. Error bars represent standard deviation over 10 different instances. Details for each setting are in Appendix B. In all cases, the number of features n \u2248 20, for which it is possible to perform the full M\u00f6bius transform. On the top row, we plot achievable faithfulness R2 as a function of sparsity. We observe that in all cases, faithfulness approaching 1 requires only a few thousand M\u00f6bius coefficients, motivating our sparsity assumption. The bottom row of plots considers achievable faithfulness vs. degree, i.e., what R\u00b2 can be achieved using only M\u00f6bius coefficients F up to a given degree. Here we observe that in nearly all cases, low degree coefficients suffice to get quite small R2, motivating our low degree assumption.", "description": "This figure shows plots that support the assumptions of sparsity and low degree for real-world functions.  Three different machine learning tasks (breast cancer diagnosis, sentiment analysis, and multiple choice) are examined. The top row shows how much of the function's behavior can be explained using a certain number of non-zero M\u00f6bius coefficients (sparsity). The bottom row shows how much can be explained by considering only interactions of a certain degree. In all cases, a relatively small number of coefficients suffices for high R-squared (faithfulness), which suggests sparsity and low degree are reasonable.", "section": "Understanding Assumptions: Sparsity and Low Degree"}, {"figure_path": "glGeXu1zG4/figures/figures_16_1.jpg", "caption": "Figure 2: These plots are strong indicators that sparsity and low degree assumptions are worthy of consideration. We consider three different learning tasks. The left-most plot shows results from an XGBoost [34] model used for breast cancer diagnosis. The middle plot shows results from word-level sentiment analysis task using a BERT model [8] like in Fig. 1. The right-most plot shows results from a multiple choice question and answer task also using a BERT model [35]. Error bars represent standard deviation over 10 different instances. Details for each setting are in Appendix B. In all cases, the number of features n \u2248 20, for which it is possible to perform the full M\u00f6bius transform. On the top row, we plot achievable faithfulness R2 as a function of sparsity. We observe that in all cases, faithfulness approaching 1 requires only a few thousand M\u00f6bius coefficients, motivating our sparsity assumption. The bottom row of plots considers achievable faithfulness vs. degree, i.e., what R\u00b2 can be achieved using only M\u00f6bius coefficients F up to a given degree. Here we observe that in nearly all cases, low degree coefficients suffice to get quite small R2, motivating our low degree assumption.", "description": "This figure empirically validates the assumptions of sparsity and low degree of the M\u00f6bius transform for real-world machine learning models. It shows that for three different tasks (breast cancer diagnosis, sentiment analysis, and multiple choice question answering), a small number of M\u00f6bius coefficients are sufficient to achieve high faithfulness (R^2). The plots depict R^2 as a function of sparsity and degree of the transform.", "section": "Understanding Assumptions: Sparsity and Low Degree"}, {"figure_path": "glGeXu1zG4/figures/figures_26_1.jpg", "caption": "Figure 14: Symmetric cross-over probability induced by hypothesis testing problem for noisy singleton identification/detection.", "description": "This figure shows the symmetric cross-over probability in a hypothesis testing problem for noisy singleton identification and detection.  The x-axis represents the signal-to-noise ratio (\u03c1/\u03c3), and the y-axis represents the crossover probability. The curve shows how the probability of making a correct decision changes with the signal-to-noise ratio.", "section": "C.7.3 Singleton Identification in i.i.d. Spectral Noise"}, {"figure_path": "glGeXu1zG4/figures/figures_33_1.jpg", "caption": "Figure 6: (a) Perfect reconstruction against n and sample complexity under Assumption 2.1. Holding C = 3, we scale b to increase the sample complexity. We observe that the number of samples required to achieve perfect reconstruction is scaling linearly in n as predicted. Results are plotted across 5 runs for each choice of b and n. (b) Plot of the noise-robust version of our algorithm. For various values of t, we set n = 500 and K = 500, using a group testing matrix with P = 1000. We plot the performance of our algorithm against SNR, measured in terms of the R\u00b2. Error bands represent the standard deviation over 10 runs. (c) Runtime comparison of SMT, SHAP-IQ [29], and t = 5 order FSI via LASSO [4]. All are computing the M\u00f6bius transform in the setting where all non-zero interactions are order t, K = 10. SMT easily outperforms both, while the other methods quickly become intractable. Error bands represent standard deviation over 10 runs.", "description": "This figure demonstrates the performance of the Sparse M\u00f6bius Transform (SMT) algorithm under different conditions.  Subfigure (a) shows the perfect reconstruction percentage against the number of samples under Assumption 2.1, demonstrating a linear scaling. Subfigure (b) illustrates the algorithm's noise robustness under various interaction orders (t), showcasing the R\u00b2 (faithfulness) against the signal-to-noise ratio (SNR). Lastly, subfigure (c) compares the runtime complexity of SMT against other approaches like SHAP-IQ and LASSO for computing the M\u00f6bius transform, clearly showing SMT's superiority.", "section": "Results"}, {"figure_path": "glGeXu1zG4/figures/figures_33_2.jpg", "caption": "Figure 6: (a) Perfect reconstruction against n and sample complexity under Assumption 2.1. Holding C = 3, we scale b to increase the sample complexity. We observe that the number of samples required to achieve perfect reconstruction is scaling linearly in n as predicted. Results are plotted across 5 runs for each choice of b and n. (b) Plot of the noise-robust version of our algorithm. For various values of t, we set n = 500 and K = 500, using a group testing matrix with P = 1000. We plot the performance of our algorithm against SNR, measured in terms of the R2. Error bands represent the standard deviation over 10 runs. (c) Runtime comparison of SMT, SHAP-IQ [29], and t = 5 order FSI via LASSO [4]. All are computing the M\u00f6bius transform in the setting where all non-zero interactions are order t, K = 10. SMT easily outperforms both, while the other methods quickly become intractable. Error bands represent standard deviation over 10 runs.", "description": "This figure shows the results of experiments to evaluate the performance of the Sparse M\u00f6bius Transform (SMT) algorithm.  Panel (a) demonstrates the linear scaling of sample complexity with the number of features (n) under perfect reconstruction conditions. Panel (b) illustrates the algorithm's robustness to noise, showing faithfulness (R\u00b2) against signal-to-noise ratio (SNR) for different interaction orders. Panel (c) compares the runtime of SMT to other methods (SHAP-IQ and LASSO), highlighting SMT's superior efficiency.", "section": "Results"}, {"figure_path": "glGeXu1zG4/figures/figures_33_3.jpg", "caption": "Figure 6: (a) Perfect reconstruction against n and sample complexity under Assumption 2.1. Holding C = 3, we scale b to increase the sample complexity. We observe that the number of samples required to achieve perfect reconstruction is scaling linearly in n as predicted. Results are plotted across 5 runs for each choice of b and n. (b) Plot of the noise-robust version of our algorithm. For various values of t, we set n = 500 and K = 500, using a group testing matrix with P = 1000. We plot the performance of our algorithm against SNR, measured in terms of the R2. Error bands represent the standard deviation over 10 runs. (c) Runtime comparison of SMT, SHAP-IQ [29], and t = 5 order FSI via LASSO [4]. All are computing the M\u00f6bius transform in the setting where all non-zero interactions are order t, K = 10. SMT easily outperforms both, while the other methods quickly become intractable. Error bands represent standard deviation over 10 runs.", "description": "This figure demonstrates the performance of the Sparse M\u00f6bius Transform (SMT) algorithm under different conditions. (a) shows the perfect reconstruction percentage against the number of samples and n under Assumption 2.1. (b) shows the performance of the noise-robust version of the algorithm against signal-to-noise ratio (SNR) under Assumption 2.2 with different maximum interaction orders (t). (c) compares the runtime of SMT with other methods (SHAP-IQ and LASSO) for computing the M\u00f6bius transform with a fixed number of non-zero interactions (K).", "section": "Results"}]