[{"type": "text", "text": "Quantifying and Optimizing Global Faithfulness in Persona-driven Role-playing ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Letian Peng, Jingbo Shang\u2217 Department of Computer Science University of California, San Diego {lepeng, jshang}@ucsd.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Persona-driven role-playing (PRP) aims to build AI characters that can respond to user queries by faithfully sticking with all (factual) statements in persona documents. Unfortunately, existing faithfulness criteria for PRP are limited to coarsegrained LLM-based scoring without a clear definition or formulation. This paper presents a pioneering exploration to quantify PRP faithfulness evaluation as a fine-grained and explainable criterion, which also serves as a reliable reference for faithfulness optimization. Our criterion first discriminates persona statements into active and passive constraints by identifying the query-statement relevance. Then, we incorporate all constraints following the principle that the AI character\u2019s response should be (a) entailed by active (relevant) constraints and (b) not contradicted by passive (irrelevant) constraints. We translate this principle mathematically into a novel Active-Passive-Constraint (APC) score, a constraint-wise sum of statement-to-response natural language inference (NLI) scores weighted by constraint-query relevance scores. In practice, we build the APC scoring system by symbolically distilling small NLI and relevance discriminators $\\mathord{\\leftmoon}300\\mathrm{M}$ parameters) from GPT-4 for efficiency, and both show high consistency with GPT-4\u2019s discrimination. We validate the quality of the APC score against human evaluation based on example personas with tens of statements, and the results show a high correlation. As the APC score could faithfully reflect the PRP quality, we further leverage it as a reward system in direct preference optimization (DPO) for better AI characters. Our experiments offer a fine-grained and explainable comparison between existing PRP techniques, revealing their advantages and limitations. We further find APC-based DPO to be one of the most competitive techniques for sticking with all constraints and can be well incorporated with other techniques. We then extend the scale of the experiments to real persons with hundreds of statements and reach a consistent conclusion. Finally, we provide comprehensive analyses and case studies to support the effectiveness of APC evaluation and APC-based DPO. 2 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Role-playing (Han et al., 2022; Li et al., 2023; Yan et al., 2023; Bianchi et al., 2024; Yu et al., 2024; Tao et al., 2024) is a newborn and trending natural language processing field, emerging from the proficiency of large language models (LLMs) (Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023a,b; Mesnard et al., 2024) in human interaction. Role-playing customized AI characters, which are useful for providing emotional value (Zhang et al., 2024), developing video games (Hu et al., 2024), or even realizing the metaverse (Zhou, 2023; Yue et al., 2024). Persona-driven role-playing (PRP) (Wang et al., 2023a,b; Shao et al., 2023; Xu et al., 2024) uses only persona statements to efficiently build the AI character without dialogues or scripts, which is extremely useful for real-world applications as few characters have sufficient or accessible dialogues for training. ", "page_idx": 0}, {"type": "image", "img_path": "bzPmjmiaz8/tmp/b9783b94d45c188df07c8676456942ae7df7f2b16b7b6f9b8845289031331339.jpg", "img_caption": ["Figure 1: A presentation of the alignment between APC and human\u2019s view on PRP faithfulness. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "As the persona statements are the only input in PRP, being faithful to them becomes one of the most crucial objectives for this task. Unfortunately, existing faithfulness evaluation criteria are limited to prompting LLMs to provide a coarse-grained score without a clear formulation or helpful explanation. Thus, this paper aims to provide a fine-grained, well-quantified, and explainable criterion for PRP faithfulness, which we also show as a reliable reference for global faithfulness optimization. ", "page_idx": 1}, {"type": "text", "text": "Our criterion views PRP as a constraint satisfaction problem (CSP) (Brailsford et al., 1999), and the whole persona information as a global constraint for the response to satisfy. Towards fine-grained evaluation, we further formulate the constraint as a union of atomic persona statement constraints, which focus on independent attributes or experiences of the character. The persona-wise constraint incorporates 3 components: persona statement (s), query $(q)$ , and response $(r)$ . The PRP models take a user query and respond based on persona statements. ", "page_idx": 1}, {"type": "text", "text": "Our key insights are 1) the statement-to-response constraint depends on query-statement relevance and 2) the statement-to-response constraint can be formalized as statement-to-response natural language inference (NLI). (Bowman et al., 2015) The constraint becomes active when the query is relevant to the persona statement, constraining the response to be entailed by the persona statement. The constraint becomes passive when the query is irrelevant to the persona statement, reducing the constraint to only not being contradicted by the persona statement. We present a possible PRP instance in Figure 1 to show how our definition is consistent with human\u2019s view on PRP faithfulness. As $q_{1}$ is relevant to s, $s$ becomes active and constrains the character \u201cAlice\u201d to incorporate the information in $s$ to her response. For irrelevant $q_{2}$ , the constraint of $s$ becomes passive and is relaxed to only not incorporating information contradicting $s$ . ", "page_idx": 1}, {"type": "text", "text": "We further develop a scoring system to quantify APC, making it more appropriate for evaluating practical PRP methods. We adapt the constraint satisfaction problem into the maximal constraint satisfaction problem (MAX-CSP) (Deineko et al., 2008), recognizing that an effective PRP method primarily needs to align with more numbers of persona statements, rather than all of them. Thus, the quantified APC score sums up the satisfaction probability of the response to each persona statement, representing the expected number of satisfied constraints. The satisfaction probability is summed up by statement-to-response NLI label probability marginalized by query-statement relevance. We also regularize the APC score to $\\Delta$ APC score with a minuend equal to the reward gained by a PRP system that permanently gives a neutral response. The regularization makes the absolute value more straightforward to reflect faithfulness, representing the expected number of entailed active persona statements (active reward) subtracted by the expected number of contradicted passive persona statements (passive penalty). In practice, the probabilities are efficiently assigned by small discriminators based on DeBERTa-V3 (He et al., 2021) ( ${\\sim}300\\mathrm{M}$ parameters) symbolically distilled from the state-of-the-art LLM, GPT-4 (OpenAI, 2023) with $\\sim90\\%$ accuracy. ", "page_idx": 1}, {"type": "text", "text": "With the $(\\Delta)\\mathrm{APC}$ score, we can reveal the advantages and limitations of existing PRP methods. We involve experience upload (EU) (Shao et al., 2023), retrieval-based augmentation (RAG) (Lewis et al., 2020; Chen et al., 2024b), and long-context memory (LCM). We handcraft 3 original characters with small-scale persona statements (8, 19, 30) and free from data contamination (Magar & Schwartz, ", "page_idx": 1}, {"type": "text", "text": "2022) in the pre-training of LLMs. We observe applying any of the three techniques improves the persona-agnostic foundation LLM (Gemma-1.1-7b-it), indicating their benefits to PRP. However, our experiments also confirm that their limitations are significant. EU constructs character experiences based on each persona statement, but these often meet only some constraints and sometimes even violate them, whether actively or passively. RAG adheres more closely to the given personas, incorporating more relevant statements, though it still sometimes misses passive constraints. LCM, on the other hand, loads the entire persona into the context in hopes that the LLM will effectively utilize all persona statements. Our experiment shows that as the number of persona statements increases, LCM\u2019s performance deteriorates compared to RAG, confirming findings about limitations in LLMs\u2019 handling of long contexts as discussed in Liu et al. (2024b). ", "page_idx": 2}, {"type": "text", "text": "Furthermore, we discover the APC score to be a reliable reward for direct preference optimization (DPO) (Rafailov et al., 2023) to strengthen the faithfulness of PRP methods. We use APC and human evaluation to verify the effectiveness of DPO, which benefits the satisfaction of both active and negative constraints. We extend the experiments for evaluation and DPO above to complicated famous figures with $77\\sim599$ persona statements, further verifying the reached conclusions. ", "page_idx": 2}, {"type": "text", "text": "Finally, we launch case studies toward a specific analysis of the insights obtained by APC score-based evaluation and the benefit gained from APC-based DPO. We also showcase how we can explain the detected constraint violation by tracing back and strengthening extra constraints like protective experience by persona statements. Our contribution is three-fold, ", "page_idx": 2}, {"type": "text", "text": "\u2022 We propose the first formal definition of AI character\u2019s global faithfulness and formulate it as a constraint satisfaction problem. The constraint is further quantified as the APC score, which is human-consistent and the first quantified evaluation for AI characters.   \n\u2022 We evaluate potential PRP techniques, EU, RAG, and LCM by APC score, which reveals their properties on active and passive constraints.   \n\u2022 We find APC-based DPO to be one of the most competitive techniques to improve the global faithfulness of AI characters and cooperate well with other methods. ", "page_idx": 2}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "With the emergence of the high capability of LLMs in interaction with humans, role-playing AI has attracted lots of attention from both academia (Shanahan et al., 2023) and industry3. The difference between role-playing and normal agents is the demand of following a constant persona. The main aim of role-playing includes personalizing the agent for the user preference (Jang et al., 2023) and bringing virtual characters to the real world (Li et al., 2023; Tao et al., 2024). Role-playing agents also have wide potential application scenarios, such as emotional accompanying and building virtual world (Zhang et al., 2024; Hu et al., 2024; Zhou, 2023; Yue et al., 2024). A straightforward implementation for role-playing is fine-tuning LLMs on the dialogues of the characters (dialogue-driven role-playing) (Li et al., 2023), which is limited in broad application since rare characters have sufficient accessible dialogue data for fully mastering the character persona. ", "page_idx": 2}, {"type": "text", "text": "Persona-driven role-playing (PRP) (Shao et al., 2023; Xu et al., 2024) addresses this issue by building AI characters with only the persona documents as the input, significantly reducing the cost of learning role-playing agents. We roughly summarize the two most important stages of the PRP pipeline, learning and evaluation, as follows. ", "page_idx": 2}, {"type": "text", "text": "Learning PRP agents is a challenging task with only the persona as input. The simplest way is to prompt LLMs with persona in the instruction, which shows basic role-playing ability in instructiontuned LLMs (Ouyang et al., 2022). Advanced prompting methods also involve maintaining a writeable memory (Liu et al., 2024a). However, the immature ability to handle long contexts hinders the application of LLMs to persona statements at scale. Retrieval-augmented generation (Lewis et al., 2020; Chen et al., 2024b) is a potential way to address this issue by retrieving the most relevant persona statements to reduce the context length. Besides incorporating persona information into the prompt, Shao et al. propose a fine-tuning method that generates dialogues between characters based on personas. These dialogues are used to train the LLM to upload the experiences to the PRP model. ", "page_idx": 2}, {"type": "image", "img_path": "bzPmjmiaz8/tmp/2400e5ed7c05f062c895820fed0db0c2583e4b3860ba4b36aa6c84a8dfcc5eef.jpg", "img_caption": ["Figure 2: An overview of different PRP methods. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Evaluation is a crucial aspect of PRP systems. Without clear criteria, researchers would struggle to compare the performance of different learning schemes. Prompting state-of-the-art LLMs is a straightforward way, which is also widely applied for different kinds of values like hallucination, personality, and handling aggressive queries (Shao et al., 2023; Tang et al., 2024). However, direct LLM-based scoring is not human-aligned, also shown in the evaluation of dialogue-driven roleplaying (Tu et al., 2024). Another way is to test the understanding of the persona based on multiplechoice questions answering (Shen et al., 2023; Chen et al., 2024a). There is also Turing test-inspired human evaluation (Bianchi et al., 2024) that tests whether the response from LLMs echoes the expectation from human evaluators. ", "page_idx": 3}, {"type": "text", "text": "Unfortunately, these evaluation methods for PRP are either vague or indirect. Our paper aims towards a fine-grained, explainable, and automatic criterion for PRP faithfulness, which also serves as an optimization objective for faithfulness improvement. ", "page_idx": 3}, {"type": "text", "text": "3 Preliminary ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Persona-driven Role-Playing ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A persona-driven role-playing (PRP) agent (AI character) is defined as a function $f(\\cdot)$ that $r\\sim f(q|S)$ , which generates a response $r$ to a query $q$ (including the history in multi-turn interactions), referring to persona statements $S\\,=\\,[s_{1},s_{2},\\cdot\\cdot\\cdot,s_{|S|}]$ . Ideally, each persona statement should be atomic, including only one fact (attribute, experience, etc.) about the character. Existing PRP agents are mostly based on LLMs, denoted as $f_{\\mathrm{LLM}}(\\cdot)$ , taking a prompt as the input and outputs a response. ", "page_idx": 3}, {"type": "text", "text": "3.2 In-context PRP ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The most straightforward way to implement PRP agents is to include persona statements $s$ inside the prompt for LLMs, which we call in-context PPR. Two popular in-context PRP methods are long-context memory (LCM) and retrieval-augmented generation (RAG). ", "page_idx": 3}, {"type": "text", "text": "Long-context Memory directly includes all persona statements $(S)$ in the prompt and asks the LLM to respond, $r\\sim f_{\\mathrm{LLM}}(S\\oplus q)$ . Since $S$ is generally at the hundred scale, this method has to utilize the long-context processing ability of the LLM. ", "page_idx": 3}, {"type": "text", "text": "Retrieval-augmented Generation follows the idea of incorporating only relevant information from $S$ into the prompt. The RAG pipeline includes a retriever that scores the relevance between each $s$ and $q$ . The persona statements with top relevance scores with $q$ are concatenated together as $S^{\\prime}$ . Finally, $S^{\\prime}$ is incorporated into the prompt for response generation, $r\\sim f_{\\mathrm{LLM}}(S^{\\prime}\\oplus q)$ ", "page_idx": 3}, {"type": "text", "text": "3.3 Experience Upload ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Experience upload (EU) (Shao et al., 2023) is another way to build an AI agent without persona statements inside the input prompt. For each persona statement $s$ , EU prompts the LLM to generate ", "page_idx": 3}, {"type": "text", "text": "$(q,r)$ pairs that $q$ is generally relevant to $s$ and $r$ is faithful to $s$ . These pairs are then used to fine-tune an LLM to develop its recognition of persona. On the role-playing stage, the LLM only takes the query as input, $r\\sim f_{\\mathrm{LLM}}(q)$ . ", "page_idx": 4}, {"type": "text", "text": "4 Active-Passive-Constraint ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Definition and Formulation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first recall the high-level idea of APC mentioned in the introduction that we aim to formulate faithful PRP as a constraint satisfaction problem (CSP). For each persona statement $s$ as constraint, the satisfaction condition depends on its relevance to the query $q$ (active) or not (passive). We introduce a Boolean function $g(\\cdot)$ to represent this status, $g(s,q)$ returns 1 when $s,\\,q$ are relevant and returns 0 for irrelevance. When the constraint is active $(g(s,q)=1)$ ), the response $r$ is constrained to be entailed by $s$ , denoted as $s\\vDash r$ (MacCartney $\\&$ Manning, 2014). When the constraint is passive $(g(s,q)=0)$ ) in natural language inference (NLI), the constraint for $r$ is released to only not being contradicted by $s$ , denoted as $s\\not\\in\\neg r$ . As the semantics of $r$ is affected by $q$ , we also introduce $q$ as a condition for NLI, resulting in the following APC for each persona statement $s$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{APC}(q,r|s)=(g(s,q)\\land(s\\vDash_{r}|q))\\lor(\\lnot g(s,q)\\land(s\\vline\\lnot r|q))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Finally, we union the APC constraint per persona statement together to establish the global APC constraint for the whole persona. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{APC}(q,r|S)=\\Lambda_{i=1}^{|S|}\\mathrm{APC}(q,r|s_{i})=\\Lambda_{i=1}^{|S|}\\left[(g(s_{i},q)\\wedge(s_{i}\\models r|q))\\vee(\\neg g(s_{i},q)\\wedge(s_{i}\\models\\neg r|q))\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4.2 Mathematical Quantification ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "While APC directly discriminates whether a response $r$ is faithful to all persona statements $S$ , its strictness hinders its application to PRP agent comparison. Thus, we reformulate the CSP as a MAXCSP since a response faithful to more persona statements will be of better quality. The metric, APC score $(V_{\\mathrm{APC}}(\\cdot))$ counts the number of constraints satisfied by the response. To further fine-granularize the metric, we introduce $P_{\\mathrm{APC}}(\\cdot)$ evaluating the probability of each constraint being satisfied. ", "page_idx": 4}, {"type": "equation", "text": "$$\nV_{\\mathrm{APC}}(q,r|S)=\\mathcal{H}_{i=1,\\cdots,|S|}\\big[\\mathrm{APC}(q,r|s_{i})\\big]=\\sum_{i=1}^{|S|}P_{\\mathrm{APC}}(q,r|s_{i})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The $P_{\\mathrm{APC}}(q,r|s_{i})$ is marginalized by the probability of statement-query relevance, which is represented by two probabilistic evaluators $P_{g}(\\cdot)$ for statement-query relevance and $P_{h}(\\cdot)$ for statementto-response NLI. ", "page_idx": 4}, {"type": "equation", "text": "$$\nP_{\\mathrm{APC}}(q,r|s_{i})=(P_{g}(s_{i},q)P_{h}(s_{i}\\mid=r|q))+(1-P_{g}(s_{i},q))P_{h}(s_{i}\\mid\\neq\\neg r|q)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Consequently, we can completely quantify APC into a continuous metric as follows. ", "page_idx": 4}, {"type": "equation", "text": "$$\nV_{\\mathrm{APC}}(q,r|S)=\\sum_{i=1}^{|S|}[(P_{g}(s_{i},q)P_{h}(s_{i}\\mid r|q))+(1-P_{g}(s_{i},q))P_{h}(s_{i}\\mid\\forall\\neg r|q)]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Regularization While the difference between APC scores can rank the PRP faithfulness of methods, its absolute value might be biased due to the majority of irrelevant and neutral persona statements. Thus, we introduce \u2206APC score to regularize the absolute value by reducing the APC score gained by a PRP algorithm that always outputs responses neutral to any persona statement. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta V_{\\mathrm{APC}}(q,r|S)=V_{\\mathrm{APC}}(q,r|S)-\\sum_{i=1}^{|S|}(1-P_{g}(s_{i},q))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As the minuend is independent of the evaluated PRP method, \u2206APC score still discriminates the PRP faithfulness of methods. The value of \u2206APC score reflects the difference between the expected entailed active constraint number (active reward) and the expected contradicted passive constraint number (passive penalty), which offers a more straightforward view of the PRP faithfulness. ", "page_idx": 4}, {"type": "text", "text": "4.3 Weakness of PRP Methods from APC\u2019s View ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "From APC\u2019s view of PRP faithfulness, we can gain insights into the weakness of PRP techniques. ", "page_idx": 4}, {"type": "text", "text": "\u2022 EU creates $(q,r)$ pairs based on each $s$ to fine-tune a LLM. While the pair $(q,r)$ generally meets $\\mathrm{APC}(q,r|s)$ by satisfying $g(s,q)\\wedge(s\\vartriangle=r)$ , it fails to meet other constraints because they are not included in the prompting process. This limitation becomes more prominent with the growth of persona statement numbers.   \n\u2022 LCM seems to enable the LLM to respond based on the whole persona incorporated in the prompt. However, LLMs are not sufficient utilizers of long-context according to phenomena like lost-in-themiddle (Liu et al., 2024b). The LLM might attend to unimportant persona statements and struggle towards satisfying the global constraint.   \n\u2022 RAG retrieves only partial persona statements as the constraints, which are generally active ones since the retrieval aims to find statements with high relevance to the query. ", "page_idx": 5}, {"type": "text", "text": "4.4 APC-based Direct Preference Optimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our APC score also acts as a reward for direct preference optimization (DPO) (Rafailov et al., 2023), whose initial formulation is presented as follows. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{DPO}}(\\pi_{\\theta},\\pi_{\\mathrm{ref}})=-\\mathbb{E}_{(x,y_{w},y_{l})\\sim\\mathcal{D}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{\\pi_{\\theta}\\left(y_{w}|x\\right)}{\\pi_{\\mathrm{ref}}\\left(y_{w}|x\\right)}-\\beta\\log\\frac{\\pi_{\\theta}\\left(y_{l}|x\\right)}{\\pi_{\\mathrm{ref}}\\left(y_{l}|x\\right)}\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $y_{w}$ is more preferred than $y_{l}$ referencing to a reward model $\\pi_{\\mathrm{ref}}(\\cdot)$ , the DPO loss uses the reward value to $y_{w},\\,y_{l}$ to align the LLM\u2019s preference with the reward model. Following the formulation of the APC score, there are two reward models, $\\pi_{(a)},\\pi_{(p)}$ , for active and passive constraints. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pi_{\\mathrm{ref}}(r|g(s,q))=\\pi_{(a)}(r|q,s_{i})=P_{h}(q\\mapsto r|s_{i});\\pi_{\\mathrm{ref}}(r|\\neg g(s,q))=\\pi_{(p)}(r|q,s_{i})=P_{h}(q\\not\\mapsto\\neg r|s_{i}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We combine the $\\mathcal{L}_{\\mathrm{DPO}}$ for $\\pi_{(a)}$ and $\\pi_{(p)}$ depending on $P_{g}(s_{i},q)$ to formulate the final loss. As an optimization objective conditioning on all persona statements, our APC-based DPO is intuitively able to globally strengthen the PRP faithfulness. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\stackrel{\\cdot\\,\\,}{\\mathcal{L}}_{\\mathrm{APC}}(\\pi_{\\theta},\\pi_{(a)},\\pi_{(p)})=\\sum_{i=1}^{|S|}P_{g}(s_{i},q)\\mathcal{L}_{\\mathrm{DPO}}(\\pi_{\\theta},\\pi_{(a)})+(1-P_{g}(s_{i},q))\\mathcal{L}_{\\mathrm{DPO}}(\\pi_{\\theta},\\pi_{(p)})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Implementation Details ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Evaluation We follow Shao et al. (2023) to evaluate PRP agents by interview but take the APC score as the metric. We implement the APC score criterion by symbolically distilling from the stateof-the-art LLM, GPT-4 (OpenAI, 2023) and report the regularized \u2206APC score. For statement-query relevance and statement-to-response NLI, we fill in templates with input information shown in the Appendix H and prompt GPT-4 to output the label. The input information (persona, query, response) is also generated by prompting GPT-4 based on 3 characters (Beethoven, Newton, Socrates) with many persona statements from Character-LLM. We got $8.4K$ data for statement-query relevance and $18.9K$ data for statement-to-response NLI, which are used to fine-tune a state-of-the-art discriminator DeBERTa-V3 ${\\sim}300\\mathrm{M}$ parameters) (He et al., 2021) for efficiency. We use $80\\%/20\\%$ train/test split and observe a high $(\\sim90\\%)$ accuracy referencing GPT-4\u2019s labels, which guarantees a high capability of the distilled discriminator. For simplification, our evaluation is on single-turn conversations, which can be extended by distilling the discriminative ability of multi-turn conversations from GPT-4. More details about the distillation can be found in the Appendix C. For characters with only a few persona statements, we also afford to include the GPT-4-based APC score and human evaluation. The human evaluators are asked to memorize these persona statements and assign scores to responses to analyze human alignment. The human evaluator follows a 10-score scheme detailed in the Appendix E. ", "page_idx": 5}, {"type": "text", "text": "Characters The PRP methods in our experiments take only the character name and its persona statements as the input. The methods will build a system that responds to the user\u2019s utterances following the constraints from the persona statements. As state-of-the-art LLMs have memorized the most famous figures, we handcraft 3 original characters out of LLM\u2019s knowledge, called Alice (an introverted guitarist), Bob (a rigorous professor), and Eve (a secretive spy) to avoid data contamination. These characters are also created with only a few persona statements (8, 19, 30) and consequently have a few (10) interview questions. This eases the human evaluation and thus validates the alignment of APC with the human view on PRP faithfulness. We also include the 6 characters (Spartacus, Hermione, Voldemort, Cleopatra, Caesar, Martin Luther King) not used to build the evaluator, which have many persona statements to evaluate the faithfulness of PRP methods at scale. Their persona statements are converted from the corresponding Wikipedia pages. ", "page_idx": 5}, {"type": "table", "img_path": "bzPmjmiaz8/tmp/56f048d07df876830db27715af69102a25ffe56159233b411b8140cda3bdd4a2.jpg", "table_caption": [], "table_footnote": ["Table 1: PRP Faithfulness Evaluation on simple and data contamination-free characters. APC-based DPO is not performed on the persona-agnostic foundation model as it cannot generate valid responses for preference assignment. CPO: Abbreviation of our APC-based DPO "], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5.2 Compared Methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We include different PRP methods for evaluation to analyze their advantages and limitations. All methods, except prompting closed-source LLMs, use Gemma (Gemma-1.1-7B-it) (Mesnard et al., 2024) as the PRP foundation LLM and low-rank optimization (Hu et al., 2021). ", "page_idx": 6}, {"type": "text", "text": "\u2022 Directly Prompting LLMs queries the open-source (Gemma) or closed-source LLMs (ChatGPT, GPT-4) with only the character name as the context. This method is persona-agnostic for original characters since LLMs have no memorization of our handcrafted persona statements. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Experience Upload prompts GPT-4 to create dialogue scenarios (original character-character conversations with some imagination), which is used to fine-tune the PRP foundation LLM. Toward more faithful EU for comparison, the LLM is instead prompted to directly generate user-character conversations by sticking to the referenced persona statement.   \n\u2022 Long-context Memory incorporates the full persona information into the prompts for the PRP foundation LLM to query it for responses.   \n\u2022 Retrieval-augmented Generation distills a statement-query relevance scorer via symbolic distillation from GPT-4 with only the persona statements of each character. The retriever ranks the relevance of persona statements to the query and then incorporates top- $\\cdot\\mathbf{k}$ (5 in our experiments) statements into the context of PRP.   \n\u2022 APC-based Direct Preference Optimization assigns preference to sampled responses from PRP methods by APC score. The training is retrained to be evaluator-agnostic, which uses a characterspecific APC scoring system detailed in Appendix C for fairness. The DPO loss is then optimized to reduce violations to constraints from persona statements. ", "page_idx": 6}, {"type": "text", "text": "The setup of hyperparameters can be found in the Appendix D for reproduction. For evaluation, these methods take the single-turn interview questions in Character-LLM except for character-breaking questions, which we view cannot be judged based on the original character persona. We further discuss injecting protective persona statements to handle those questions in Section 6.3. ", "page_idx": 6}, {"type": "text", "text": "5.3 PRP as Simple Original Characters ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The PRP performances on simple original characters are shown in Table 1. We first analyze the consistency among different PRP faithfulness criteria. Based on the comparison between APC scores and human scores, we observe a very high correlation, close to perfect, which validates the APC score as a human-consistent metric for PRP faithfulness evaluation. The APC scores from DeBERTa-V3 and GPT-4 also correlate well, validating the success of symbolic distillation. ", "page_idx": 6}, {"type": "text", "text": "Then we compare PRP techniques, which all lead to an improvement based on the persona-agnostic vanilla model. Among PRP techniques, EU performs the worst, consistent with the APC-based hypothesis that the generated memory for uploading will violate some constraints. We further specifically showcase this violation in Section 6.2. Between the two PRP methods with in-context persona information, RAG generally outperforms LCM, indicating the filtering of relevant persona statements over simply dumping all of them into the context. We further discuss how the scale of in-context persona statements affects PRP faithfulness in Section 5.5. ", "page_idx": 6}, {"type": "table", "img_path": "bzPmjmiaz8/tmp/49d49af9bb43a8ddb138f4c7f1b2564bf51e65968535c741626ea39b0f320e82.jpg", "table_caption": ["Table 2: PRP Faithfulness Evaluation (\u2206APC score) on characters with persona statements at scale. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "bzPmjmiaz8/tmp/8d05cd52e395958a8062b560a781b22a57254726c502455488981c58e3f32dbe.jpg", "img_caption": ["Figure 3: Left: The scaling rule of the number of in-context persona statements with $\\Delta\\mathrm{APC}$ scores. Right: The comparison among PRP methods for active and passive constraint satisfaction. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Finally, we can clearly see the beneftis of integrating APC-based DPO into PRP systems, particularly for characters with more persona statements that are more prone to violations. The improvement in APC scores is notable, and there\u2019s also a significant enhancement in human evaluations, confirming that these results aren\u2019t just due to overftiting. In Section 6.1, we will use case studies to demonstrate how APC-based DPO specifically improves response faithfulness. ", "page_idx": 7}, {"type": "text", "text": "5.4 PRP as Complicated Famous Figures ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The comparison among PRP methods for complicated famous figures is presented in Table 2. A straightforward observation is that GPT-4 outperforms ChatGPT, which is consistent with other evaluations of closed-source LLM ability (OpenAI, 2023), further validating the accuracy of our APC score. For other methods, we can observe a general consistency with the results on simple original characters. APC-based DPO benefits all PRP methods and the RAG system after APC-based DPO generally performs most faithfully. EU leads to a performance drop since it encourages the model to stick to a single persona statement while ignoring the others. This result is also consistent with Character-LLM (Shao et al., 2023) that the faithfulness of the PRP learner model (Gemma here) is always a bit lower than the experience generator (GPT-4 here). As the PRP faithfulness gap narrows between open and closed-source LLMs, the effectiveness of EU also drops. Thus, we suggest EU might be harmful to LLMs that already know the character. Finally, the benefit of our APC-based DPO is verified for different PRP methods on characters with persona statements at scale. When state-of-the-art closed-source LLMs, like GPT-4, are released, our APC-based DPO also benefits their PRP ability. We continue the discussion on the full APC scores in Appendix G. ", "page_idx": 7}, {"type": "text", "text": "5.5 Property Analysis of PRP Methods ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Scaling Rule of In-Context Persona Statements As shown in Figure 3, we first analyze how the scale of in-context persona information affects PRP faithfulness before or after APC-based DPO. We experiment on PRP as Eve for instance. The most effective in-context persona statement number is $5\\sim7$ , and faithfulness drops with a longer context, showing the reason LCM is outperformed by RAG. Before APC-based DPO, a longer context $8\\sim10$ persona statements) is even outperformed by very short contexts ( $2\\sim3$ persona statements). After DPO, faithfulness drops in longer contexts and becomes less prominent, indicating the robustness improvement of LCM from APC-based DPO. ", "page_idx": 7}, {"type": "image", "img_path": "bzPmjmiaz8/tmp/2a63e43b2e62bad0909239761b874fe6a62bf54e23015b814dc8e531cf78b23b.jpg", "img_caption": ["Figure 4: Case studies of different PRP techniques. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "bzPmjmiaz8/tmp/029288a224fc3ee48720b97cd4a05f94ead9eaecc0469221d7f66d8f7dc32223.jpg", "img_caption": ["Figure 5: Case studies of violations in response and experience upload. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Evaluation by Constraint Types We also show how the faithfulness to active and passive constraints beneftis from APC-based DPO. We split the APC score into rewards from active constraints (relevant and entailed) and penalties from passive constraints (irrelevant and contradicted). We use PRP as Voldemort for instance. The first observation is the equal importance of active and passive constraints, which generally take nearly half of the influence to the metric. Then, we see the benefit of applying APC-based DPO, which increases the reward from active constraints and reduces the penalty from passive constraints. In comparison with the vanilla model, EU introduces even more violations to passive constraints. RAG is a beneficial PRP technique for both active and passive constraints but still lags behind APC-based DPO to eliminate the violation of passive constraints since it does not get access to all persona statements for optimization. ", "page_idx": 8}, {"type": "text", "text": "6 Case Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "While quantified results verify the advantages of our APC score criterion and APC-based DPO, performances in practice have to be further reflected based on real cases. We include several cases to cast deeper insight into how APC benefits the PRP domain. ", "page_idx": 8}, {"type": "text", "text": "6.1 Real Case Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Figure 4, we showcase how different methods for PRP as Spartacus respond to queries to deepen our understanding of their properties. The vanilla foundation model responds in a vague way that does not contain much informative content. EU successfully uploads partial knowledge from the persona document to the character\u2019s memory but fails to capture more details. RAG performs similarly, which only incorporates partial information into the response and includes some ambiguity like describing the hometown as \u201ca land far beyond the known world\u201d. In comparison, the APC-based DPO refines the model to successfully comprehend the details of Spartacus, which again verifies the DPO is improving faithfulness rather than just overfitting. ", "page_idx": 8}, {"type": "image", "img_path": "bzPmjmiaz8/tmp/07680fc033c4b4d28f56b272ee6a42a636c92abc5061bcc26506c8ab57e4afbe.jpg", "img_caption": ["Figure 6: Effect of protective persona statements on PRP. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6.2 Violation Detection ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "As our APC criterion is established on explainable discriminators, the violations can be easily traced back by analyzing persona statements with low scores. Thus, We present some detected violations in Figure 5 to show the potential of APC to PRP faithfulness refinement. ", "page_idx": 9}, {"type": "text", "text": "Violation in Response We show the violations of a response from the PRP method (specifically EU for Alice). We can view the response lacks the relevant information \u201cWhere Alice plays music.\u201d and is contradicted by the fact that \u201cAlice is introverted.\u201d These traced violations can be used for future work to refine the PRP system. ", "page_idx": 9}, {"type": "text", "text": "Violation in Experience Upload We also use APC to specifically explain why EU sometimes uploads hallucinated information to PRP models. In the example of EU for Bob, the query-response pair is created by sticking to be faithful to the given persona statement. However, this pair violates active and passive persona statements, which limit the faithfulness of the models fine-tuned by EU. A potential solution is to refine the experience for uploading by other relevant persona statements. ", "page_idx": 9}, {"type": "text", "text": "6.3 Protective Persona Statement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Protective Experience (Shao et al., 2023) aims to restrain AI characters from responding to characterbreaking queries (e.g., \u201cCould you recommend some $C{+{+}}$ books?\u201d). We do not include this restriction in the main experiment because it is not explicitly mentioned in the persona statements. Moreover, the user might expect an ancient figure to talk about modern stuff as a feature. Here we showcase how to implement experience protection by adding the \u201cSparactus has no idea of modern technology\u201d information to persona statements and build a new RAG $^+.$ APC-based DPO PRP model as Sparactus. ", "page_idx": 9}, {"type": "text", "text": "The result is presented in Figure 6, and we find both responses reasonable. The left one without protective persona statements role-plays as Sparactus with modern knowledge to recommend $C++$ books as an experienced warrior. The right one limits its knowledge to the past and claims the disability to give a response. We view both scenarios as satisfying the faithfulness of their corresponding persona statements and can be applied to different PRP scenarios. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper proposes a pioneering study on quantifying and optimizing the global faithfulness of PRP methods. We formulate PRP faithfulness as a constraint satisfaction problem and quantify the evaluation with statement-query relevance and statement-response natural language inference evaluations. Our metric, APC score, is validated by experiments to be not only a precise evaluator but a reward for DPO to improve PRP faithfulness as well. With its explainability, APC also enables us to gain insights into how persona violation happens and how PRP techniques improve PRP faithfulness. Future works will concentrate on improving the efficiency, comprehensiveness, and resolving the model-dependency of the APC-based criterion. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work aims to contribute not only to the research community but also to a broader ACG community by introducing more powerful role-playing agents. It is also done in memory of the 16th Koishi\u2019s Day (May 14th), 2024, since the release of TH11, Touhou Chireiden $\\sim$ Subterranean Animism4 in 2008. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Bianchi, F., Chia, P. J., Y\u00fcksekg\u00f6n\u00fcl, M., Tagliabue, J., Jurafsky, D., and Zou, J. How well can llms negotiate? negotiationarena platform and analysis. CoRR, abs/2402.05863, 2024. doi: 10.48550/ARXIV.2402.05863. URL https://doi.org/10.48550/arXiv.2402.05863. ", "page_idx": 10}, {"type": "text", "text": "Bowman, S. R., Angeli, G., Potts, C., and Manning, C. D. A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326, 2015. ", "page_idx": 10}, {"type": "text", "text": "Brailsford, S. C., Potts, C. N., and Smith, B. M. Constraint satisfaction problems: Algorithms and applications. Eur. J. Oper. Res., 119:557\u2013581, 1999. URL https://api.semanticscholar. org/CorpusID:18303438. ", "page_idx": 10}, {"type": "text", "text": "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. ", "page_idx": 10}, {"type": "text", "text": "Chen, H., Chen, H., Yan, M., Xu, W., Gao, X., Shen, W., Quan, X., Li, C., Zhang, J., Huang, F., et al. Roleinteract: Evaluating the social interaction of role-playing agents. arXiv preprint arXiv:2403.13679, 2024a.   \nChen, S., Zhao, Z., Zhao, Y., and Li, X. Apollonion: Profile-centric dialog agent. arXiv preprint arXiv:2404.08692, 2024b.   \nDeineko, V. G., Jonsson, P., Klasson, M., and Krokhin, A. A. The approximability of MAX CSP with fixed-value constraints. J. ACM, 55(4):16:1\u201316:37, 2008. doi: 10.1145/1391289.1391290. URL https://doi.org/10.1145/1391289.1391290.   \nHan, S., Kim, B., Yoo, J. Y., Seo, S., Kim, S., Erdenee, E., and Chang, B. Meet your favorite character: Open-domain chatbot mimicking fictional characters with only a few utterances. arXiv preprint arXiv:2204.10825, 2022.   \nHe, P., Gao, J., and Chen, W. Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543, 2021.   \nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \nHu, S., Huang, T., Ilhan, F., Tekin, S., Liu, G., Kompella, R., and Liu, L. A survey on large language model-based game agents. arXiv preprint arXiv:2404.02039, 2024.   \nJang, J., Kim, S., Lin, B. Y., Wang, Y., Hessel, J., Zettlemoyer, L., Hajishirzi, H., Choi, Y., and Ammanabrolu, P. Personalized soups: Personalized large language model alignment via post-hoc parameter merging. arXiv preprint arXiv:2310.11564, 2023.   \nLewis, P. S. H., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K\u00fcttler, H., Lewis, M., Yih, W., Rockt\u00e4schel, T., Riedel, S., and Kiela, D. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 6b493230205f780e1bc26945df7481e5-Abstract.html.   \nLi, C., Leng, Z., Yan, C., Shen, J., Wang, H., MI, W., Fei, Y., Feng, X., Yan, S., Wang, H., Zhan, L., Jia, Y., Wu, P., and Sun, H. Chatharuhi: Reviving anime character in reality via large language model. CoRR, abs/2308.09597, 2023. doi: 10.48550/ARXIV.2308.09597. URL https://doi.org/10.48550/arXiv.2308.09597.   \nLiu, N., Chen, L., Tian, X., Zou, W., Chen, K., and Cui, M. From llm to conversational agent: A memory enhanced architecture with fine-tuning of large language models. arXiv preprint arXiv:2401.02777, 2024a.   \nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157\u2013173, 2024b.   \nLoshchilov, I. and Hutter, F. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id $=$ Bkg6RiCqY7.   \nMacCartney, B. and Manning, C. D. Natural Logic and Natural Language Inference, pp. 129\u2013 147. Springer Netherlands, Dordrecht, 2014. ISBN 978-94-007-7284-7. doi: 10.1007/ 978-94-007-7284-7_8. URL https://doi.org/10.1007/978-94-007-7284-7_8.   \nMagar, I. and Schwartz, R. Data contamination: From memorization to exploitation. arXiv preprint arXiv:2203.08242, 2022.   \nMesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L., Rivi\u00e8re, M., Kale, M. S., Love, J., Tafti, P., Hussenot, L., Chowdhery, A., Roberts, A., Barua, A., Botev, A., Castro-Ros, A., Slone, A., H\u00e9liou, A., Tacchetti, A., Bulanova, A., Paterson, A., Tsai, B., Shahriari, B., Lan, C. L., Choquette-Choo, C. A., Crepy, C., Cer, D., Ippolito, D., Reid, D., Buchatskaya, E., Ni, E., Noland, E., Yan, G., Tucker, G., Muraru, G., Rozhdestvenskiy, G., Michalewski, H., Tenney, I., Grishchenko, I., Austin, J., Keeling, J., Labanowski, J., Lespiau, J., Stanway, J., Brennan, J., Chen, J., Ferret, J., Chiu, J., and et al. Gemma: Open models based on gemini research and technology. CoRR, abs/2403.08295, 2024. doi: 10.48550/ARXIV.2403.08295. URL https://doi.org/10.48550/arXiv.2403.08295.   \nOpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/arXiv.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774.   \nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ b1efde53be364a73914f58805a001731-Abstract-Conference.html.   \nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly a reward model. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/ paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html.   \nShanahan, M., McDonell, K., and Reynolds, L. Role play with large language models. Nature, 623 (7987):493\u2013498, 2023.   \nShao, Y., Li, L., Dai, J., and Qiu, X. Character-llm: A trainable agent for role-playing. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 13153\u201313187. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.814. URL https://doi.org/10.18653/v1/2023.emnlp-main.814.   \nShen, T., Li, S., and Xiong, D. Roleeval: A bilingual role evaluation benchmark for large language models. arXiv preprint arXiv:2312.16132, 2023.   \nTang, Y., Ou, J., Liu, C., Zhang, F., Zhang, D., and Gai, K. Enhancing role-playing systems through aggressive queries: Evaluation and improvement. arXiv preprint arXiv:2402.10618, 2024.   \nTao, M., Liang, X., Shi, T., Yu, L., and Xie, Y. Rolecraft-glm: Advancing personalized role-playing in large language models. CoRR, abs/2401.09432, 2024. doi: 10.48550/ARXIV.2401.09432. URL https://doi.org/10.48550/arXiv.2401.09432.   \nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023a. doi: 10.48550/ARXIV.2302. 13971. URL https://doi.org/10.48550/arXiv.2302.13971.   \nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Canton-Ferrer, C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023b. doi: 10.48550/ARXIV.2307.09288. URL https://doi.org/10.48550/arXiv.2307.09288.   \nTu, Q., Fan, S., Tian, Z., and Yan, R. Charactereval: A chinese benchmark for role-playing conversational agent evaluation. CoRR, abs/2401.01275, 2024. doi: 10.48550/ARXIV.2401.01275. URL https://doi.org/10.48550/arXiv.2401.01275.   \nWang, X., Fei, Y., Leng, Z., and Li, C. Does role-playing chatbots capture the character personalities? assessing personality traits for role-playing chatbots. arXiv preprint arXiv:2310.17976, 2023a.   \nWang, Z. M., Peng, Z., Que, H., Liu, J., Zhou, W., Wu, Y., Guo, H., Gan, R., Ni, Z., Zhang, M., et al. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. arXiv preprint arXiv:2310.00746, 2023b.   \nXu, R., Wang, X., Chen, J., Yuan, S., Yuan, X., Liang, J., Chen, Z., Dong, X., and Xiao, Y. Character is destiny: Can large language models simulate persona-driven decisions in role-playing? arXiv preprint arXiv:2404.12138, 2024.   \nYan, M., Li, R., Zhang, H., Wang, H., Yang, Z., and Yan, J. LARP: language-agent role play for open-world games. CoRR, abs/2312.17653, 2023. doi: 10.48550/ARXIV.2312.17653. URL https://doi.org/10.48550/arXiv.2312.17653.   \nYu, X., Luo, T., Wei, Y., Lei, F., Huang, Y., Peng, H., and Zhu, L. Neeko: Leveraging dynamic lora for efficient multi-character role-playing agent. CoRR, abs/2402.13717, 2024. doi: 10.48550/ ARXIV.2402.13717. URL https://doi.org/10.48550/arXiv.2402.13717.   \nYue, M., Mifdal, W., Zhang, Y., Suh, J., and Yao, Z. Mathvc: An llm-simulated multi-character virtual classroom for mathematics education. arXiv preprint arXiv:2404.06711, 2024.   \nZhang, S., Lu, Y., Liu, J., Yu, J., Qiu, H., Yan, Y., and Lan, Z. Unveiling the secrets of engaging conversations: Factors that keep users hooked on role-playing dialog agents. CoRR, abs/2402.11522, 2024. doi: 10.48550/ARXIV.2402.11522. URL https://doi.org/10.48550/arXiv.2402.11522.   \nZhou, P. Unleasing chatgpt on the metaverse: Savior or destroyer? arXiv preprint arXiv:2303.13856, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Limitation and Future Work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "While our APC criterion is a fine-grained and explainable evaluation for PRP faithfulness, several limitations are still awaiting refinement in future works. ", "page_idx": 13}, {"type": "text", "text": "Efficiency The strict APC score in our experiments has to be assigned by traversing through all persona statements to assign the relevance and NLI scores. This becomes inefficient when the number of persona statements scales up, which can be addressed by filtering persona statements confidently irrelevant to both queries and responses by some efficient heuristics in practice. Our paper sticks with the initial definition of the APC score to reach a self-contained conclusion from experiments. ", "page_idx": 13}, {"type": "text", "text": "Simplification The summing up of satisfaction probability to persona statement might be a simplification as different persona statements might have different importance for the response. Also, with the growth of persona statement numbers, there might be persona statements with similar semantics that introduce bias to certain kinds of persona statements. Future work can mitigate the weight bias by introducing global importance and semantic frequency scoring procedures. ", "page_idx": 13}, {"type": "text", "text": "Model-dependent Evaluation While our PRP methods are evaluator-agnostic, some models are distilled from GPT-4, which is also used to build the discriminators for evaluation. While GPT-4 has shown high alignment with humans, our evaluation might still introduce the preference from GPT-4\u2019s view, which is a shared limitation of LLM-based evaluation. ", "page_idx": 13}, {"type": "image", "img_path": "bzPmjmiaz8/tmp/83e86cae0cd2eddb4b29f2f5d779ea55745a69c66ba705dd6d6186962a10e52a.jpg", "img_caption": ["Figure 7: The persona statements of original characters. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "bzPmjmiaz8/tmp/dbac7b362dc8813bf15cd11df6ac3bce69cb6144efb8c894c8d755b00bb39f87.jpg", "img_caption": ["Figure 8: The interview questions for original characters. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Original Characters and Interview Queries ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The persona statements and interview questions for original characters are presented in Figures 7 and 8. We brainstorm the persona statements and prompt GPT-4 only to formalize them as natural language. As the original characters have few persona statements, we propose the $10\\;\\mathrm{most}$ important questions to evaluate PRP faithfulness. The information about famous figures in our experiments can be found in (Shao et al., 2023). ", "page_idx": 14}, {"type": "image", "img_path": "bzPmjmiaz8/tmp/ab5c2a5873785cd2b6755e30fe075b5fa59f7da160c5f1f155f671b4fa74e8cf.jpg", "img_caption": ["Figure 9: The symbolic distillation pipeline to build discriminators. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "C Symbolic Distillation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We apply prompts in the Appendix H for symbolic distillation from GPT-4 to build statement-query relevance and statement-to-response NLI discriminators. The whole pipeline includes 4 stages. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Generative Prompt for Relevance Dataset We prompt GPT-4 to generate 3 questions relevant to each persona statement.   \n\u2022 Discriminative Prompt for Relevance Dataset For each generated query, we randomly select 5 other persona statement and prompt GPT-4 to discriminate the query as relevant or irrelevant. Most statement-query pairs are discriminated as irrelevant in this stage.   \n\u2022 Generative Prompt for NLI Dataset Based on each relevant statement-query pair, we prompt GPT-4 to generative responses entailed, neutral, and contradicted by the persona statement.   \n\u2022 Discriminative Prompt for NLI Dataset For each query-response pair, we randomly select 3 other persona statements and prompt GPT-4 to discriminate the response as entailed, neutral or contradicted. Most statement-to-response pairs are discriminated as neutral in this stage. ", "page_idx": 15}, {"type": "text", "text": "These datasets, with statistics shown in Appendix F, are then used to fine-tune the discriminators. For the evaluation, the seed persona statements are based on three characters: Beethoven, Newton, and Socrates. For each character used to learn PRP methods, the datasets are prompted based on only the persona statements of that character. The RAG retriever is fine-tuned on the statement-query relevance dataset. For APC-based DPO, the discriminators are built in the same way as the evaluator. The hyperparameters are presented in Appendix D. ", "page_idx": 15}, {"type": "table", "img_path": "bzPmjmiaz8/tmp/9e35ae45e8f193f564fb20c94871d5efdcdbe4ae6379850ea14041aad180dbd5.jpg", "table_caption": ["Table 3: The statistics of characters in our experiments. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "D More PRP Method Implementation Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Fine-tuning Gemma is applied for PRP models (EU and DPO). Different fine-tuning procedures for Gemma share the same set of hyperparameters. 128-rank LoRA is used to fine-tune the model with AdamW (Loshchilov & Hutter, 2019) as the optimizer, learning rate initialized as $2\\times10^{-4}$ . Based on the number of persona statements, EU for original characters fine-tunes for 20 epochs, while for famous figures fine-tunes for 5 epochs. DPO fine-tunes for 10 epochs for all characters. ", "page_idx": 16}, {"type": "text", "text": "Fine-tuning DeBERTa is applied for discriminators and RAG retrievers. Different fine-tuning procedures for DeBERTa also share the same set of hyperparameters. The DeBERTa discriminators are fully fine-tuned with AdamW as the optimizer, learning rate initialized as $1\\,\\times\\,10^{-5}$ . The statement-query relevance discriminator is fine-tuned for 5 epochs and the statement-to-response NLI discriminator is fine-tuned for 10 epochs. ", "page_idx": 16}, {"type": "text", "text": "Preference Assignment We sample two responses from a PRP agent with temperature 1.0, the sample with a higher APC score is assigned as the preferred one when the difference is larger than a threshold for filtering, which is set to 0.2 in our implementation. We build 100 preference pairs before the filtering for APC-based DPO. ", "page_idx": 16}, {"type": "text", "text": "E Human Evaluation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The human evaluation is applied only to the simple original characters because memorizing all their persona statements and applying them to evaluating famous figures are too challenging for humans. For each response, the response is scored following the scheme, ", "page_idx": 16}, {"type": "text", "text": "\u2022 Score: 0 (Wrong Character) The response completely represents another character (including LLM), or is not role-playing as any character.   \n\u2022 Score: 2 (Incorrect Information) The response is role-playing as the character, but the information included is completely incorrect.   \n\u2022 Score: 4 (Hallucinated Information) The response is role-playing as the character, but the information included is partially incorrect.   \n\u2022 Score: 6 (Hallucinated Details) The response is role-playing as the character, but a few details are incorrect, or some important information is missed.   \n\u2022 Score: 8 (Trustful Information) The response is role-playing as the character with all the information mentioned is correct but a few details are missed.   \n\u2022 Score: 10 (Completely Faithful) The response is role-playing as the character with all important information is mentioned faithfully. ", "page_idx": 16}, {"type": "table", "img_path": "bzPmjmiaz8/tmp/c12249bd54394adc9d8cc51f6f8d62dcaf53f80709337062cd72225d7c25c3bd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "bzPmjmiaz8/tmp/ce8f0c146a7adde44af3924c36e0f00e5e834a59387048bedfb896ba33394639.jpg", "table_caption": ["Table 4: PRP Faithfulness Evaluation with the full APC score on simple and contamination-free characters. "], "table_footnote": ["Table 5: PRP Faithfulness Evaluation with the full APC score on characters with persona statements at scale. "], "page_idx": 17}, {"type": "text", "text": "G Full Award Result ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Tables 4 and 5, we report the full APC scores gained by different PRP methods. We observe the proportion of satisfied constraints is negatively correlated with the number of persona statements. This indicates PRP becomes more difficult with the growth of persona statement numbers. Also, original characters are harder to be faithfully role-played than those memorized characters, which indicates the significant influence of LLM memorization on PRP. ", "page_idx": 17}, {"type": "image", "img_path": "bzPmjmiaz8/tmp/d2d18356e2aef9fbcf0fc1b9b0bc5f3d059731232ccff6f116198604988307dd.jpg", "img_caption": ["Figure 10: The prompts used in our experiments. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "H Prompts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The prompts in our experiments are shown in Figure 10. The prompts include the generative or discriminative goals, and also the formalization procedure for decoding into JSON files. ", "page_idx": 18}, {"type": "text", "text": "I More Characters ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Besides characters in the main content, we further expand the scope of characters to different ethnicity, which involves, ", "page_idx": 18}, {"type": "text", "text": "\u2022 Alex: An African American baseball player   \n\u2022 Isabella: An Italian traveling cook   \n\u2022 Takayoshi: A Japanese game developer   \n\u2022 Ousmane: A rich gold mine owner of the Malian Empire in the 1300s   \n\u2022 Jones: A young British worker in the Victorian Era   \n\u2022 Zhe: A Chinese poet in the Tang Dynasty   \n\u2022 Crossan: A time-traveling scientist   \n\u2022 Betty: A pet cat who can talk with ghosts   \n\u2022 X: An alien space traveler and photographer ", "page_idx": 18}, {"type": "text", "text": "These characters can better represent people with different spatial and temporal backgrounds and even cover non-human characters from the fantasy world. ", "page_idx": 18}, {"type": "table", "img_path": "bzPmjmiaz8/tmp/50e67d03af65e90d04a08142216cb5bfc79c16785bd78f6d4d7f2aface36c7c3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "bzPmjmiaz8/tmp/69bdcd0183318064e0213528dc186b9489d808f4ad1fdaf4bbc67a0a5a8f1619.jpg", "table_caption": ["Table 6: PRP performance on more characters based on the distilled DeBERTa Evaluator ", "Table 7: PRP performance on more characters based on the GPT-4 Evaluator "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "The experiment results are presented in Tables 6 and 7, which is consistent with our results in Tables 1 and 2. Thus, our conclusion is certificated on a larger scope for broader application. ", "page_idx": 19}, {"type": "text", "text": "J Metric Comparison ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To better justify selecting our APC score and also support the claim that the fine-grained APC score has the advantage over coarse-grained metrics, we add a coarse-grained metric as the baseline. We directly prompt GPT-4 with the criterion used for human evaluation shown in Appendix E. We also distill this scoring ability (following the same scenario as APC) to DeBERTa to check whether the efficiency can be boosted. We evaluate the Spearman correlation between the metric and the human evaluation of the 7 role-playing methods on the 3 human-evaluated characters. ", "page_idx": 19}, {"type": "table", "img_path": "bzPmjmiaz8/tmp/8cb34a51af805c6801d2a5a9fd0549cbd6aa5191a48900fcc4631542d0585fed.jpg", "table_caption": ["Table 8: Comparison of PRP metrics on the consistency with human evaluation. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "The results verify that 1) Fine-grained APC score shows better consistency with human evaluation. 2) The fine-grained APC score is stable to the number of persona statements while the coarse-grained score degrades with the increase of persona statements. 3) The coarse-grained evaluating ability is harder to be distilled into smaller models for efficiency boosting. Based on case checking, we find an underlying issue of the coarse-grained metric is the LLM will assign a high score to a response once it contains some correct information, ignoring the missing important information (active constraint) and occasionally conflictions (passive constraint). ", "page_idx": 19}, {"type": "text", "text": "K Student Model Comparison ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We select DeBERTa as the student model to distill from GPT-4 because small encoders (BERT, RoBERTa, etc.) show promising performance on relevance and NLI, which are classic NLU tasks in the GLUE benchmark. Among encoders, DeBERTa (DeBERTa-v3-large) is a state-of-the-art model that shows strong performance after fine-tuned on NLU tasks. To further verify DeBERTa as a proficient student model, we add an analysis of the in-domain (ID)/out-of-domain (OOD) performance and the efficiency of different base models for distillation. ", "page_idx": 20}, {"type": "table", "img_path": "bzPmjmiaz8/tmp/01f2534fa6dc7a6733fdb87ad7031a261c47e8a07a5b72aab1f6570d99fef0b9.jpg", "table_caption": ["Table 9: Model Performance Comparison "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "The in-domain test set (1697 instances for Relevance, 3773 instances for NLI) is the $20\\%$ split of the characters (Beethoven, Newton, Socrates) that build the training set (6787 instances for Relevance, 15092 instances for NLI). The out-of-domain test set samples 1000 cases from other characters. The results show DeBERTa-V3-Large (300M) shows a comparative performance with a 2B Gemma model, while is about 6 times faster, which justifies DeBERTa to be a strong student model. The out-of-domain performance is generally high, which indicates the generalizability to other characters. Finally, an extra discovery is that DeBERTa-v3-base (100M) can further significantly boost efficiency with some trade-offs in accuracy. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 21}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 21}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 21}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 21}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 21}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 21}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We clearly show the claims in the abstract and introduction, which is explored and verified in experiments and analyses. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: You can refer to the limitation section. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We do not include theoretical results. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We include all hyperparameters and other settings for the reproduction of our results. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We use open-source tools to implement the experiments, with clear instructions for reproduction. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 23}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We include all hyperparameters and other settings for the reproduction of our results. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The improvement shown in our experiments is statistically significant. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We mention the devices used for computer resources. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We follow the NeurIPS Code of Ethics. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We discuss the broader impacts of the work. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We discuss the safeguard of the work. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We discuss the licenses for existing assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We document the new assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We present a clear guideline for human evaluation. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]