[{"figure_path": "HBj86RMdZ8/figures/figures_8_1.jpg", "caption": "Figure 1: Mean validation reverse KL to the reference policy when DPO and HyPO are trained for 5 epoch on the TL;DR dataset. We repeat the experiment for 3 random seeds and plot the median and the shaded areas denote the min and max over the 3 repetitions.", "description": "This figure shows the mean validation reverse KL divergence to the reference policy over training timesteps for both DPO and HyPO algorithms on the TL;DR summarization dataset.  The experiment was repeated three times with different random seeds, and the median KL divergence is plotted along with shaded regions representing the minimum and maximum KL values across these repetitions. The plot visually demonstrates the effect of including KL regularization in the HyPO algorithm.", "section": "5 Hybrid Preference Optimization: Regularizing Offline Learning with Unlabeled Online Samples"}, {"figure_path": "HBj86RMdZ8/figures/figures_19_1.jpg", "caption": "Figure 2: Extrapolation behavior of Online RL method and DPO under linear function approximation. We plot the mean log probability of the preferred responses and the log probability of the best response, which is unseen in the training data. We see that both algorithms correctly assigns increasing probability to the best response.", "description": "The figure shows the extrapolation behavior of online RLHF and DPO methods under the linear function approximation setting.  The left panel shows that the online RLHF method correctly increases the probability of the best response (unseen in training) while simultaneously decreasing the probability of the preferred responses (seen in training). The right panel shows that DPO also exhibits this behavior, although with higher variance in its log probability trends. This demonstrates the generalization ability of both methods.", "section": "E.1 Extrapolation with function approximation"}, {"figure_path": "HBj86RMdZ8/figures/figures_19_2.jpg", "caption": "Figure 3: Extrapolation behavior of DPO without function approximation. We plot the average probability of out-of-distribution responses along the training and DPO assigns increasing probability to out-of-distribution responses.", "description": "The figure shows the average probability of out-of-distribution responses during the training of DPO without function approximation.  It demonstrates that as DPO trains, the probability of generating responses not seen in the training data increases. This is an interesting phenomenon because it indicates that DPO is extrapolating beyond the observed data.", "section": "E Extrapolation without function approximation"}]