{"references": [{"fullname_first_author": "P. Amortila", "paper_title": "Harnessing density ratios for online reinforcement learning", "publication_date": "2024", "reason": "This paper provides a general theoretical framework for understanding learning from human preferences, which is the core methodology of the main paper."}, {"fullname_first_author": "M. G. Azar", "paper_title": "A general theoretical paradigm to understand learning from human preferences", "publication_date": "2024", "reason": "This paper offers a theoretical understanding of the similarities and differences between online and offline techniques for preference fine-tuning, expanding the theoretical foundation of the main paper."}, {"fullname_first_author": "R. Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023", "reason": "This paper introduces Direct Preference Optimization (DPO), a key offline contrastive method that is compared against in the main paper, establishing a baseline for comparison."}, {"fullname_first_author": "L. Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022", "reason": "This paper introduces Reinforcement Learning from Human Feedback (RLHF), a dominant paradigm for fine-tuning LLMs that is analyzed and extended in the main paper."}, {"fullname_first_author": "W. Zhan", "paper_title": "Provable offline preference-based reinforcement learning", "publication_date": "2023", "reason": "This paper provides theoretical guarantees for offline preference-based RL algorithms, which is the foundation for the theoretical analysis of offline methods in the main paper."}]}