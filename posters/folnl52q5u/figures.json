[{"figure_path": "fOLNl52Q5U/figures/figures_1_1.jpg", "caption": "Figure 1: An overview of visual grounding structures: (a) Two-Stage: Applying a detector for proposals, followed by image-text encoding and feature similarity calculation for region matching. (b) One-Stage: Grounding in the fused features through dense prediction. (c) Transformer-based: Employing an encoder-decoder structure in the head. (d) Ours", "description": "This figure provides a visual comparison of four different visual grounding architectures.  (a) shows a two-stage approach using object detection to propose regions, followed by separate image and text encoding and a similarity measure for matching. (b) illustrates a one-stage method performing dense prediction on fused image-text features. (c) presents a transformer-based architecture employing an encoder-decoder to handle the task. Finally, (d) shows the proposed SimVG architecture, which uses a decoupled multimodal encoder and a lightweight MLP for efficient grounding.", "section": "1 Introduction"}, {"figure_path": "fOLNl52Q5U/figures/figures_1_2.jpg", "caption": "Figure 2: The expression length and relative improvement between Dynamic MDETR [57] and SimVG.", "description": "This figure shows the average expression length and the relative improvement of SimVG over Dynamic MDETR on six different visual grounding datasets.  The datasets are ordered from longest average expression length (RefCOCOg) to shortest (Flickr30k).  The results indicate that SimVG shows a more significant improvement on datasets with longer expressions, suggesting that its decoupled multi-modal understanding approach is particularly beneficial for more complex sentences.", "section": "1 Introduction"}, {"figure_path": "fOLNl52Q5U/figures/figures_3_1.jpg", "caption": "Figure 3: Overview of the proposed SimVG. The token branch refers to the upper light yellow region, while the decoder branch refers to the lower light blue region. During model inference, we can independently apply the more lightweight token branch to improve inference speed and simplify the model architecture.", "description": "This figure shows the overall architecture of SimVG, a visual grounding framework. It highlights the multi-modality encoder which processes image, text, and object tokens.  The decoder is split into two branches: a heavier transformer-based decoder branch and a lightweight MLP-based token branch.  A dynamic weight-balance distillation method is employed to train these two branches synchronously, with the decoder branch acting as a teacher to guide the token branch.  The text-guided query generation (TQG) module is also shown, which incorporates text information into object queries.  The figure further illustrates the distillation head, which computes losses to guide the learning process, and how it uses weights to dynamically balance the contributions of the decoder and token branches during training.", "section": "3 The Proposed Method"}, {"figure_path": "fOLNl52Q5U/figures/figures_7_1.jpg", "caption": "Figure 4: The convergence speed of three different multimodal pretraining architecture models.", "description": "This figure shows the training curves for three different multimodal pre-training architectures: BEiT-3, CLIP, and ViLT.  The y-axis represents the precision@0.5 metric, a common evaluation metric for visual grounding tasks, indicating the accuracy of the model in locating the correct region of an image given a textual description. The x-axis represents the number of training epochs.  The plot demonstrates that BEiT-3 and ViLT converge significantly faster than CLIP, reaching higher precision scores with fewer training iterations.  This suggests that decoupling multimodal fusion from downstream tasks, as done in BEiT-3 and ViLT, offers benefits in terms of training efficiency.", "section": "4.4.1 Multi-Modality Encoder Architecture"}, {"figure_path": "fOLNl52Q5U/figures/figures_8_1.jpg", "caption": "Figure 4: The convergence speed of three different multimodal pretraining architecture models.", "description": "The figure shows the training curves for three different multimodal pre-training architectures (CLIP, ViLT, and BEiT-3) on the RefCOCO dataset, focusing on the convergence speed of Prec@0.5.  It demonstrates that models using decoupled multimodal fusion (ViLT and BEiT-3) converge significantly faster than the CLIP model, which performs only cross-modal alignment.  The faster convergence of BEiT-3 and ViLT suggests the benefit of separating multimodal fusion from downstream tasks in the overall model architecture.", "section": "4.4.1 Multi-Modality Encoder Architecture"}, {"figure_path": "fOLNl52Q5U/figures/figures_8_2.jpg", "caption": "Figure 6: Curves of Ldwbd and Wat, along with the accuracy of the decoder and token branches.", "description": "This figure shows the curves of the dynamic weight-balance distillation loss (Ldwbd) and the dynamic weight (Wdt) during the training process.  It also displays the accuracy of both the decoder branch (Accdb) and the token branch (Acctb).  The plot illustrates how the loss and weights change over iterations, and how these changes relate to the performance of the two branches. The dynamic weight (Wdt) initially favors the ground truth, shifting towards the decoder branch's prediction as training progresses. This shows the effectiveness of the dynamic weight-balance distillation in balancing the learning between the two branches.", "section": "3.2 Dynamic Weight-Balance Distillation"}, {"figure_path": "fOLNl52Q5U/figures/figures_9_1.jpg", "caption": "Figure 3: Overview of the proposed SimVG. The token branch refers to the upper light yellow region, while the decoder branch refers to the lower light blue region. During model inference, we can independently apply the more lightweight token branch to improve inference speed and simplify the model architecture.", "description": "This figure shows a detailed overview of the SimVG architecture, highlighting the multi-modality encoder, decoder branch (with transformer layers, multi-head self-attention, and feed-forward network), and token branch (with object token, text-guided query generation, and MLP).  It emphasizes the dynamic weight-balance distillation method used to train both branches synchronously, improving the token branch's performance while maintaining efficiency. The figure shows the loss calculation for both branches, indicating how the decoder branch acts as a teacher to guide the lighter token branch. The diagram illustrates the early and late iterations of training with differing weights to balance the influence of the teacher and ground truth.", "section": "3 The Proposed Method"}, {"figure_path": "fOLNl52Q5U/figures/figures_17_1.jpg", "caption": "Figure 3: Overview of the proposed SimVG. The token branch refers to the upper light yellow region, while the decoder branch refers to the lower light blue region. During model inference, we can independently apply the more lightweight token branch to improve inference speed and simplify the model architecture.", "description": "This figure provides a detailed overview of the SimVG architecture, highlighting its three main components: the multi-modality encoder, the decoder branch, and the token branch.  The multi-modality encoder processes image, text, and object tokens. The decoder branch, similar to DETR's decoder, produces predictions.  The novel token branch, a lightweight MLP, learns from the decoder branch using dynamic weight-balance distillation (DWBD).  The figure also illustrates the TQG (Text-guided Query Generation) module which incorporates textual prior information. Notably, during inference, the lightweight token branch can be used independently for faster processing and reduced model complexity.", "section": "3 The Proposed Method"}, {"figure_path": "fOLNl52Q5U/figures/figures_17_2.jpg", "caption": "Figure 4: The convergence speed of three different multimodal pretraining architecture models.", "description": "This figure shows the convergence speed of three different multimodal pretraining architectures (CLIP, ViLT, and BEiT-3) during the training process for visual grounding.  It highlights how decoupling multimodal fusion, as implemented in ViLT and BEIT-3, leads to significantly faster convergence compared to CLIP, which only performs cross-modal alignment.  The y-axis represents the Precision@0.5 metric, and the x-axis represents the number of training epochs. The graph visually demonstrates the improved efficiency of the decoupled methods.", "section": "4.4.1 Multi-Modality Encoder Architecture"}, {"figure_path": "fOLNl52Q5U/figures/figures_18_1.jpg", "caption": "Figure 12: The box-plot on validation set of RefCOCO dataset.", "description": "The figure shows the box plot of the validation results on the RefCOCO dataset for three different visual grounding models: VGTR, SeqTR, and the proposed SimVG model.  The box plots illustrate the distribution of the Precision@0.5 metric across multiple runs of each model, showing the median, quartiles, and range of the results. The plot visually demonstrates that the proposed SimVG model achieves higher median accuracy and shows better stability (tighter distribution) compared to the other two models.", "section": "D.4 Analysis of Error Bars"}, {"figure_path": "fOLNl52Q5U/figures/figures_19_1.jpg", "caption": "Figure 1: An overview of visual grounding structures: (a) Two-Stage: Applying a detector for proposals, followed by image-text encoding and feature similarity calculation for region matching. (b) One-Stage: Grounding in the fused features through dense prediction. (c) Transformer-based: Employing an encoder-decoder structure in the head. (d) Ours", "description": "This figure compares four different visual grounding architectures.  (a) Two-Stage: This method uses a detector to generate region proposals, followed by separate image and text encoders and a fusion module that calculates the similarity between features to determine the final grounding.  (b) One-Stage: This architecture performs dense prediction to directly determine the location of the grounding.  (c) Transformer-based: This uses an encoder-decoder structure with transformers to process image and text features before doing grounding. (d) Ours (SimVG):  SimVG uses a multi-modality encoder to process images, text, and object tokens and then employs a lightweight MLP for grounding, separating the fusion and prediction phases.", "section": "1 Introduction"}, {"figure_path": "fOLNl52Q5U/figures/figures_20_1.jpg", "caption": "Figure 3: Overview of the proposed SimVG. The token branch refers to the upper light yellow region, while the decoder branch refers to the lower light blue region. During model inference, we can independently apply the more lightweight token branch to improve inference speed and simplify the model architecture.", "description": "This figure shows the architecture of SimVG, a visual grounding model. It consists of three main parts: a multi-modality encoder, a decoder branch, and a token branch. The multi-modality encoder processes the image, text, and object tokens. The decoder branch is a standard transformer decoder that generates bounding boxes and class predictions. The token branch consists of a lightweight MLP that also produces bounding boxes and class predictions.  A dynamic weight-balance distillation method is used to improve the performance of the token branch by using the decoder branch as a teacher. Text-guided query generation is used to incorporate textual prior information into object queries. During inference, either branch can be used independently, with the token branch preferred for its efficiency.", "section": "3 The Proposed Method"}, {"figure_path": "fOLNl52Q5U/figures/figures_21_1.jpg", "caption": "Figure 1: An overview of visual grounding structures: (a) Two-Stage: Applying a detector for proposals, followed by image-text encoding and feature similarity calculation for region matching. (b) One-Stage: Grounding in the fused features through dense prediction. (c) Transformer-based: Employing an encoder-decoder structure in the head. (d) Ours", "description": "This figure shows four different visual grounding architectures.  (a) illustrates a two-stage approach using a detector for object proposals followed by image and text encoding to find matches. (b) presents a one-stage approach using dense prediction.  (c) shows a transformer-based model using an encoder-decoder structure. (d) is the proposed SimVG architecture, which uses a multi-modality encoder and a lightweight MLP for visual grounding.", "section": "1 Introduction"}]