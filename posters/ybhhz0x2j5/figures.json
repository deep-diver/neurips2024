[{"figure_path": "YbhHz0X2j5/figures/figures_1_1.jpg", "caption": "Figure 1: VidMan's two-stage training paradigm mirrors dual process theory: its first stage (like System 2) pre-trains on understanding environment dynamics through video diffusion, forming a foundation for accurate action prediction, while its second stage (like System 1) was adapted from the first stage to leverage the learned dynamics knowledge for rapid, low-level action inference.", "description": "This figure illustrates VidMan's two-stage training process, drawing an analogy to the dual-process theory in neuroscience.  The first stage, akin to System 2 (slow, deliberate processing), pre-trains a video diffusion model on a large dataset to learn environmental dynamics.  This provides a strong foundation for action prediction. The second stage, mirroring System 1 (fast, intuitive responses), adapts the pre-trained model using a lightweight adapter for rapid action prediction, leveraging the learned dynamics knowledge. The diagram visually depicts this process, showing the iterative denoising of the first stage and the streamlined single-pass process of the second stage, highlighting the parameter sharing between the stages.", "section": "4 Method"}, {"figure_path": "YbhHz0X2j5/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of VidMan. (a) We use Video Tokenizer to tokenize the uniform sampled robot visual trajectory Os to video tokens Vs. (b) In the 1st Stage, we concatenate the video tokens processed through the diffusion process with the historical tokens along the channel dimension to form Vk. Vk along with the language tokens and diffusion step k are fed into Open-Sora for video prediction training. In the 2nd Stage, we use a learnable action token through a layer-wise adapter applied to the output of the Open-Sora Block to obtain tokens Vaction that integrate future frame information. Vaction are then fed into the Diffusion Action Head \u03c0\u03c6\u03b1\u03b5\u03c2 for action prediction training.", "description": "This figure provides a visual overview of the VidMan architecture. It's divided into two stages, mirroring the dual-process theory.  The first stage pre-trains a video diffusion model (Open-Sora) using robot visual trajectories to understand environment dynamics. The second stage uses a layer-wise adapter to integrate this learned knowledge into an action prediction head, allowing for the prediction of dynamics-modulated actions.  The diagram showcases the data flow, from video tokenization and language encoding through the two stages, to final action prediction.", "section": "4.1 Dynamics-aware Visionary Stage"}, {"figure_path": "YbhHz0X2j5/figures/figures_8_1.jpg", "caption": "Figure 4: Efficiency comparison between two types of training.", "description": "This figure shows the training loss curves for two different training strategies.  The \"not freeze\" curve represents training both the layer-wise adapter and the Open-Sora blocks, while the \"freeze\" curve represents training only the adapter with the Open-Sora blocks frozen (weights fixed).  The x-axis represents training iterations, and the y-axis represents the training loss. The results indicate that training only the adapter results in faster convergence (loss decreasing more quickly), but training both components achieves slightly better overall performance (lower final loss and higher average task length, indicated in the legend).", "section": "5.3 Extra Ablation Studies"}, {"figure_path": "YbhHz0X2j5/figures/figures_14_1.jpg", "caption": "Figure 5: Our model utilizes a layer-wise adapter, which includes a self-attention layer and a feed-forward network (FFN). This block uses a gating mechanism to distill the information extracted by the Open-Sora block into the action query.", "description": "This figure shows the architecture of the layer-wise adapter used in VidMan's second stage (Dynamics-modulated Action Stage).  The adapter takes as input the outputs from each layer of the pretrained Open-Sora video diffusion model. It consists of a self-attention layer and a feed-forward network (FFN), both employing tanh gating to integrate information effectively.  The adapter's output is then used to generate action queries, which are crucial for the model's ability to predict robot actions guided by learned dynamics.", "section": "4.2 Dynamics-modulated Action Stage"}, {"figure_path": "YbhHz0X2j5/figures/figures_17_1.jpg", "caption": "Figure 6: Video prediction results on OXE. The images in yellow boxes are ground-truth images; the images in blue boxes are predicted images. The language instruction is placed below the image.", "description": "This figure shows six examples of video prediction results from the VidMan model trained on the Open X-Embodiment (OXE) dataset. Each example includes a language instruction and a sequence of images showing ground truth and predicted future frames.  The model successfully predicts future image frames, although with some inaccuracies and limitations such as missing occluded objects. The figure demonstrates the model's ability to understand and predict future frames based on the observed states and language instructions.", "section": "A.5 Visualization"}, {"figure_path": "YbhHz0X2j5/figures/figures_17_2.jpg", "caption": "Figure 6: Video prediction results on OXE. The images in yellow boxes are ground-truth images; the images in blue boxes are predicted images. The language instruction is placed below the image.", "description": "This figure shows several examples of video prediction results from VidMan on the OXE dataset. Each row represents a different instruction. The top half of each row shows the actual video frames (ground truth), and the bottom half shows the video frames predicted by the model.  The model is capable of producing video sequences that align with the given instructions, although it does have some limitations in detail and occlusions.", "section": "A.5 Visualization"}, {"figure_path": "YbhHz0X2j5/figures/figures_17_3.jpg", "caption": "Figure 6: Video prediction results on OXE. The images in yellow boxes are ground-truth images; the images in blue boxes are predicted images. The language instruction is placed below the image.", "description": "This figure shows six examples of video prediction results from the VidMan model trained on the OXE dataset.  Each example includes a short language instruction (e.g., \"put cup from anywhere into sink\") and a sequence of images. The top row in each example shows the ground truth image sequence, while the bottom row shows the sequence of images predicted by VidMan. The figure demonstrates the model's ability to predict future video frames based on language instructions and previous frames. Note that while the model generally captures the main action, fine details may not be perfectly accurate in the predicted images.", "section": "A.5 Visualization"}, {"figure_path": "YbhHz0X2j5/figures/figures_17_4.jpg", "caption": "Figure 6: Video prediction results on OXE. The images in yellow boxes are ground-truth images; the images in blue boxes are predicted images. The language instruction is placed below the image.", "description": "This figure visualizes the video prediction capabilities of the VidMan model.  It shows several examples of video prediction tasks, each with the input language instruction at the bottom. The top row of images in each example shows the ground truth frames, while the bottom row shows the frames predicted by VidMan.  The figure demonstrates the model's ability to generate realistic and temporally coherent video sequences from language instructions, though it also shows some limitations in predicting fine details, such as occluded objects.", "section": "A.5 Visualization"}, {"figure_path": "YbhHz0X2j5/figures/figures_17_5.jpg", "caption": "Figure 6: Video prediction results on OXE. The images in yellow boxes are ground-truth images; the images in blue boxes are predicted images. The language instruction is placed below the image.", "description": "This figure shows six examples of video prediction results obtained using VidMan on the OXE dataset. Each example includes a language instruction (e.g., \"put cup from anywhere into sink\") and a sequence of images. The images in the yellow boxes represent the ground truth, while the images in the blue boxes are the predictions made by the model. The figure demonstrates the model's ability to generate realistic and coherent video sequences based on the given instructions.  The predictions generally capture the essence of the actions described but may sometimes miss fine details.", "section": "A.5 Visualization"}, {"figure_path": "YbhHz0X2j5/figures/figures_17_6.jpg", "caption": "Figure 6: Video prediction results on OXE. The images in yellow boxes are ground-truth images; the images in blue boxes are predicted images. The language instruction is placed below the image.", "description": "This figure showcases the video prediction capabilities of the VidMan model.  It presents several examples of video sequences, where the top row in each example shows the ground truth frames from the OXE dataset, and the bottom row displays the frames predicted by VidMan. Each example is accompanied by a language instruction that guides the prediction. The figure visually demonstrates the model's ability to generate plausible future video frames based on past observations and natural language instructions.", "section": "A.5 Visualization"}, {"figure_path": "YbhHz0X2j5/figures/figures_18_1.jpg", "caption": "Figure 6: Video prediction results on OXE. The images in yellow boxes are ground-truth images; the images in blue boxes are predicted images. The language instruction is placed below the image.", "description": "This figure shows the video prediction results of Vidman on the OXE dataset.  Each row presents a specific language instruction given to the robot. The yellow boxes highlight the ground truth image frames from the video, while the blue boxes display the corresponding frames predicted by the model. This visually demonstrates VidMan's ability to predict future video frames based on the given language instruction and previous frames. The quality of the predictions varies; while some details are accurately captured, others, such as occluded objects, are sometimes missing. This showcases both the strengths and limitations of the model's video prediction capabilities, underscoring that although it effectively predicts future scenarios, more complex details might be missed.", "section": "A.5 Visualization"}, {"figure_path": "YbhHz0X2j5/figures/figures_18_2.jpg", "caption": "Figure 7: Offline action prediction results on OXE. The upper part of each group of images shows subsampled frames from an episode, while the lower part displays the true and predicted 7D pose results, including x, y, yaw, pitch, roll, and grasp over time.", "description": "This figure visualizes the results of offline action prediction experiments conducted using the VidMan model on the Open X-Embodiment (OXE) dataset.  It shows several example tasks, with the top of each section displaying a sequence of images from a single trial, and the bottom showing graphs of ground truth versus predicted robot movements (x, y, yaw, pitch, roll, and grasp) over time. The graphs clearly demonstrate the accuracy of VidMan's action prediction, illustrating its capacity to generate precise and realistic actions for the robot.", "section": "A.5 Visualization"}, {"figure_path": "YbhHz0X2j5/figures/figures_18_3.jpg", "caption": "Figure 7: Offline action prediction results on OXE. The upper part of each group of images shows subsampled frames from an episode, while the lower part displays the true and predicted 7D pose results, including x, y, yaw, pitch, roll, and grasp over time.", "description": "This figure shows the results of offline action prediction on the OXE dataset. Each row represents a different action, with the top showing a sequence of images (subsampled frames) from a single episode and the bottom showing graphs of the predicted and ground truth 7D poses (x, y, yaw, pitch, roll, and grasp) over time for that same action.  The figure visually demonstrates the model's ability to predict the robot's actions accurately in various scenarios.", "section": "A.5 Visualization"}, {"figure_path": "YbhHz0X2j5/figures/figures_18_4.jpg", "caption": "Figure 7: Offline action prediction results on OXE. The upper part of each group of images shows subsampled frames from an episode, while the lower part displays the true and predicted 7D pose results, including x, y, yaw, pitch, roll, and grasp over time.", "description": "This figure shows the results of offline action prediction on the OXE dataset.  The top of each section displays a series of images from a single episode, showing the robot's progress through a task. The bottom section shows a graph that compares the actual robot movements (ground truth) to the movements predicted by VidMan model. The graphs show the movement of the end-effector, including its x, y, z position, yaw, pitch, roll, and the state of the gripper (open or closed).  It demonstrates VidMan's accuracy in predicting robot movements during the execution of offline tasks.", "section": "A.5 Visualization"}, {"figure_path": "YbhHz0X2j5/figures/figures_18_5.jpg", "caption": "Figure 7: Offline action prediction results on OXE. The upper part of each group of images shows subsampled frames from an episode, while the lower part displays the true and predicted 7D pose results, including x, y, yaw, pitch, roll, and grasp over time.", "description": "This figure visualizes the results of offline action prediction on the OXE dataset.  For several example actions (e.g., \"Flip orange pot upright in sink\"), the top portion shows a sequence of images from the episode, and the bottom portion compares the ground truth 7D robot pose (x, y, yaw, pitch, roll, and grasp) to the model's predictions over time. The comparison allows for a visual assessment of the model's accuracy in predicting the robot's actions.", "section": "A.5 Visualization"}, {"figure_path": "YbhHz0X2j5/figures/figures_18_6.jpg", "caption": "Figure 7: Offline action prediction results on OXE. The upper part of each group of images shows subsampled frames from an episode, while the lower part displays the true and predicted 7D pose results, including x, y, yaw, pitch, roll, and grasp over time.", "description": "This figure visualizes the offline action prediction results of the VidMan model on the OXE dataset. It presents a series of image sequences showing robot arm movements in various tasks, each accompanied by a graph comparing the predicted and ground truth 7D poses (x, y, yaw, pitch, roll, and grasp) over time.  The images show the model successfully predicting the robot's actions, but with minor discrepancies in specific parameters. The graphs help quantify the accuracy of VidMan's predictions in a quantitative manner.", "section": "A.5 Visualization"}]