[{"figure_path": "YbhHz0X2j5/tables/tables_6_1.jpg", "caption": "Table 1: Zero-shot long-horizon evaluation on CALVIN. All denotes that the model is trained on the entire dataset, including visual data without language annotations, while Lang refers to training on only the language-labeled data.. Our method outperforms the hierarchical 2D policies (MCIL [31], HULC [32] and SuSIE [33]) and large-scale 2D transformer-based policies (RT-1 [47] RoboFlamingo [26] and GR-1 [9]), while also remaining competitive compared to 3D-based policies (3D Diffusion Policy [34] and 3D Diffuser Actor [35]).", "description": "This table presents a comparison of VidMan's performance against several baseline methods on the CALVIN benchmark.  The benchmark involves zero-shot long-horizon tasks where models are trained on subsets of the data and tested on unseen data.  The table shows the average number of tasks successfully completed in a row, indicating the model's ability to perform sequential tasks. The results demonstrate VidMan's superior performance compared to other methods, particularly in handling long sequences of tasks.", "section": "5.2 Comparison Results"}, {"figure_path": "YbhHz0X2j5/tables/tables_7_1.jpg", "caption": "Table 2: Ablation Studies on Key Factors of VidMan.. We conduct finetune experiments on CALVIN. Average length is used. Best practice settings are marked in gray", "description": "This table presents ablation study results on the VidMan model, focusing on three key aspects: the two-stage training strategy, the type of pretraining used, and the inclusion of a layer-wise adapter.  For each setting, the average task completion length on the CALVIN benchmark is reported.  The best-performing setting for each aspect is highlighted in gray, demonstrating the importance of each component for optimal performance.", "section": "5.3 Extra Ablation Studies"}, {"figure_path": "YbhHz0X2j5/tables/tables_7_2.jpg", "caption": "Table 2: Ablation Studies on Key Factors of VidMan.. We conduct finetune experiments on CALVIN. Average length is used. Best practice settings are marked in gray", "description": "This table presents ablation study results on the CALVIN benchmark, evaluating the impact of different design choices in the VidMan model.  It compares the average task completion length achieved with different model variations focusing on the two-stage training process, pretraining datasets, and the use of the layer-wise adapter. The results highlight the importance of each component in achieving optimal performance. The gray shaded cell indicates the best performing configuration.", "section": "5.3 Extra Ablation Studies"}, {"figure_path": "YbhHz0X2j5/tables/tables_7_3.jpg", "caption": "Table 2: Ablation Studies on Key Factors of VidMan.. We conduct finetune experiments on CALVIN. Average length is used. Best practice settings are marked in gray", "description": "This table presents ablation study results on the CALVIN benchmark to analyze the impact of key components in the VidMan model.  It shows the average task completion length (a metric of performance) under various settings. Specifically, it evaluates the effects of the two-stage training, the type of pretraining (using general or robot-specific data), the inclusion of the layer-wise adapter, and whether the Open-Sora model parameters are frozen during the second stage. The best performing setting is highlighted.", "section": "5.3 Extra Ablation Studies"}, {"figure_path": "YbhHz0X2j5/tables/tables_8_1.jpg", "caption": "Table 3: Effect of Frame Sampling Interval. Setting the frame sampling interval to 3 is effective.", "description": "This table presents the results of an ablation study investigating the impact of different frame sampling intervals (1, 2, 3, and 4) on the performance of the VidMan model.  The performance metrics evaluated include FID (Fr\u00e9chet Inception Distance), FVD (Fr\u00e9chet Video Distance), MSE (Mean Squared Error) for action prediction, xyz accuracy (accuracy of predicted 3D position), angle accuracy (accuracy of predicted orientation), and the average task length.  The results demonstrate that increasing the frame sampling interval to 3 improves performance, while further increases yield diminishing returns. The data is separated for both the Bridge Dataset and the CALVIN benchmark.", "section": "5.3 Extra Ablation Studies"}, {"figure_path": "YbhHz0X2j5/tables/tables_13_1.jpg", "caption": "Table 4: Training Hyperparameters", "description": "This table lists the hyperparameters used for training the VidMan model.  It breaks down the settings used for each of the three training stages: the initial dynamics-aware visionary stage, the second stage using the OXE dataset, and the second stage using the CALVIN dataset.  The hyperparameters include batch size, learning rate, dropout rate, optimizer, weight decay, learning rate schedule, and number of training steps.", "section": "5.1 Experiment Settings"}, {"figure_path": "YbhHz0X2j5/tables/tables_15_1.jpg", "caption": "Table 5: Vk type in stage 2. Concatenating historical frames with noise in the channel dimension is an effective way.", "description": "This table presents the ablation study results on the impact of different input types (Vk) in stage 2 of the VidMan model.  Specifically, it compares the model's performance when using (1) no noise added to historical frames, (2) only pure noise, and (3) pure zero embeddings concatenated with historical frames. The performance metrics used are Mean Squared Error (MSE), xyz accuracy, and angle accuracy. Lower MSE indicates better performance, while higher xyz and angle accuracy values are also desirable. The results show that concatenating historical frames with pure noise yields significantly better performance than other input variations.", "section": "5.1 Experiment Settings"}, {"figure_path": "YbhHz0X2j5/tables/tables_15_2.jpg", "caption": "Table 6: Effect of historical frames v.s. future frames. The length of historical frames is more important than the length of future frames.", "description": "This table presents the results of an ablation study comparing different lengths of historical and future frames in the VidMan model.  The experiment varied the number of historical frames (m) and future frames (n) used in the model's training and evaluated the impact on several metrics, including Fr\u00e9chet Inception Distance (FID), Fr\u00e9chet Video Distance (FVD), Mean Squared Error (MSE), xyz accuracy, angle accuracy, and average length.  The results show that increasing the number of historical frames has a more significant positive impact on the model's performance than increasing the number of future frames. This suggests that the model benefits more from a strong understanding of past events than from highly detailed predictions of distant future states.", "section": "5.2 Comparison Results"}, {"figure_path": "YbhHz0X2j5/tables/tables_15_3.jpg", "caption": "Table 7: Detailed numerical metrics on OXE data. All models were trained on OXE and validated on offline performance across four datasets. VidMan outperformed Octo-base [7] by 5.6% on Bridge, 2.6% on Taco Play, 9.9% on Cable Routing, and 9.0% on Autolab UR5. Additionally, Our method also shows improvements over the VidMan-GPT approach.", "description": "This table presents a detailed comparison of the offline performance of VidMan against several baseline models across four datasets from the Open X-Embodiment dataset (OXE).  The metrics used for comparison include Mean Squared Error (MSE), XYZ accuracy, angle accuracy, and the average of XYZ and angle accuracy (avg xyz ang). The results highlight VidMan's superior performance, particularly in datasets with limited data, showcasing its effective data utilization strategy.", "section": "5.2 Comparison Results"}, {"figure_path": "YbhHz0X2j5/tables/tables_16_1.jpg", "caption": "Table 1: Zero-shot long-horizon evaluation on CALVIN. All denotes that the model is trained on the entire dataset, including visual data without language annotations, while Lang refers to training on only the language-labeled data.. Our method outperforms the hierarchical 2D policies (MCIL [31], HULC [32] and SuSIE [33]) and large-scale 2D transformer-based policies (RT-1 [47] RoboFlamingo [26] and GR-1 [9]), while also remaining competitive compared to 3D-based policies (3D Diffusion Policy [34] and 3D Diffuser Actor [35]).", "description": "This table presents a comparison of VidMan's performance against various state-of-the-art (SOTA) methods on the CALVIN benchmark for zero-shot long-horizon robot manipulation tasks.  It shows the average number of tasks completed consecutively for each method, broken down by training data type (All or Lang) and the average length of the successfully completed sequences. VidMan significantly outperforms most prior methods, especially 2D hierarchical and transformer-based approaches, achieving results competitive with the more recent 3D methods.", "section": "5.2 Comparison Results"}]