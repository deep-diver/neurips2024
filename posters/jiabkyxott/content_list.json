[{"type": "text", "text": "Sparsity-Agnostic Linear Bandits with Adaptive Adversaries ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tianyuan Jin Department of Electrical and Computer Engineering National University of Singapore Singapore tianyuan@nus.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Kyoungseok Jang   \nDipartimento di Informatica   \nUniversita degli Studi di Milano   \nMilano, Italy   \nksajks@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Nicolo Cesa-Bianchi Universita degli Studi di Milano Politecnico di Milano Milano, Italy nicolo.cesa-bianchi@unimi.it ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study stochastic linear bandits where, in each round, the learner receives a set of actions (i.e., feature vectors),from which it chooses an element and obtains a stochastic reward. The expected reward is a fixed but unknown linear function of the chosen action. We study sparse regret bounds, that depend on the number $S$ of non-zero coefficients in the linear reward function. Previous works focused on thecasewhere $S$ is known, or the action sets satisfy additional assumptions. In this work, we obtain the first sparse regret bounds that hold when $S$ is unknown and the action sets are adversarially generated. Our techniques combine online to confidence set conversions with a novel randomized model selection approach over a hierarchy of nested confidence sets. When $S$ is known, our analysis recovers state-of-the-art bounds for adversarial action sets. We also show that a variant of our approach, using Exp3 to dynamically select the confidence sets, can be used to improve the empirical performance of stochastic linear bandits while enjoying a regret bound with optimal dependence on the time horizon. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "$K$ -armed bandits are a basic model of sequential decision-making in which a learner sequentially chooses which arm to pull in a set of $K$ arms. After each pull, the learner only observes the reward returned by the chosen arm. After $T$ pulls, the learner must obtain a total reward as close as possible to the reward obtained by always pulling the overall best arm. Linear bandits extend $K$ -armed bandits to a setting in which arms belong to a $d$ -dimensional feature space. In each round $t$ of a linear bandit problem, the learner receives an action set $\\mathcal{A}_{t}\\subset\\mathbb{R}^{d}$ from the environment, chooses an arm $A_{t}\\in\\mathcal A_{t}$ based on the past observations, and then receives a reward $X_{t}$ . In this work, we consider the stochastic setting in which rewards are defined by $X_{t}=\\langle\\theta_{*},A_{t}\\rangle+\\varepsilon_{t}$ ,where $\\boldsymbol{\\theta}_{\\ast}\\in\\mathbb{R}^{d}$ is a fixed latent parameter and $\\varepsilon_{t}$ is zero-mean independent noise. In linear bandits, the learner's goal is to minimize the difference between the total reward obtained by pulling in each round $t$ the arm $a\\in\\mathcal{A}_{t}$ maximizing $\\langle\\theta_{*},a\\rangle$ and the total reward obtained by the learner. ", "page_idx": 0}, {"type": "text", "text": "In stochastic linear bandits, the regret after $T$ rounds is known to be of order $d\\sqrt{T}$ up to logarithmic factors. The linear dependence on the number $d$ of features implies that the learner is better off by ignoring features corresponding to negligible components of the latent target vector $\\theta_{*}$ . Hence, one would like to design algorithms that depend on the number $S\\ll d$ of relevant features without requiring any preliminary knowledge on $\\theta_{*}$ . This is captured by the setting of sparse linear bandits, where $\\boldsymbol{\\theta}_{*}$ is assumed to have only $0<S\\leq d$ nonzero components. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In the sparse setting, Lattimore and Szepesvari [17, Section 24.3] show that a regret of $\\Omega\\big(\\sqrt{S d T}\\big)$ is unavoidable for any algorithm, even with knowledge of $S$ .When $S$ is known, this lower bound is matched (up to log factors) by an algorithm of Abbasi-Yadkori et al. [2] who, under the same asumptioand forth aealgr,alproaninstaedendenteof $\\widetilde{\\cal O}\\big(\\frac{S d}{\\Delta}\\big)$ Here is the minimum gap, over all rounds, between the expected reward of the optimal arm and that of any suboptimal arm. In this work we focus on the sparsity-agnostic setting, i.e., when $S$ is unknown. Fewer results are known for this case, and all of them rely on additional assumptions on the action set, or assumptions on the sparsity structure. For example, if the action set is stochastically generated, Oh et al. [22] prove a $\\tilde{O}(\\bar{S}\\sqrt{T})$ sparsity-agnostic regret bound. More recently, Dai et al. [9] showed a sparsity-agnostic bound $\\widetilde{\\cal O}\\big(\\dot{S}^{2}\\sqrt{T}+S\\sqrt{d T}\\big)$ when the action set is fixed and equal to the unit sphere. In a model selection setting, Cutkosky et al. [8] prove a $\\widetilde{\\cal O}(S^{2}\\sqrt{T})$ sparsity-agnostic regret bound for adversarial action sets, but under an additional nestedness assumption: $(\\theta_{*})_{i}\\neq0$ for $i=1,\\ldots,S$ . Surprisingly, no bounds improving on the $\\widetilde{O}(d\\sqrt{T})$ regret of the OFUL algorithm [1] in the sparsity-agnostic case are known that avoid additional assumptions on the sparsity structure or on the action set generation. ", "page_idx": 1}, {"type": "text", "text": "Main contributions. Here is the summary of our main contributions. All the proofs of our results can be found in the appendix. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We introduce a randomized sparsity-agnostic linear bandit algorithm, SparseLinUCB, achieving regret $\\widetilde{O}(S\\sqrt{d T})$ with no assumptions on the sparsity structure (e.g., nestedness) or on the action set (which may be controlled by an adaptive adversary). When $S=o({\\sqrt{d}})$ , our bound is strictly better than the OFUL bound $\\widetilde{O}(d\\sqrt{T})$ \u00b7 Our analysis of SparseLinUCB simultaneously guarantees an instance-dependent regret bound $\\widetilde{O}\\big(\\operatorname*{max}\\{d^{2},S^{2}d\\}/\\Delta\\big)$ , where $\\Delta$ is the smallest suboptimality gap over the $T$ rounds. \u00b7 If the sparsity level is known, our algorithm recovers the optimal bound $\\widetilde{\\Theta}(\\sqrt{S d T})$ \u00b7 We also introduce AdaLinUCB, a variant of SparseLinUCB that uses Exp3 to learn the probability distribution over a hierarchy of confidence sets in stochastic linear bandits. Unlike previous works, which only showed a $\\tilde{O}(T^{2/3})$ regret bound for similar approaches, AdaLinUCB has a $\\widetilde{O}(\\sqrt{T})$ regret bound. In experiments on synthetic data, AdaLinUCB performs better than OFUL. ", "page_idx": 1}, {"type": "text", "text": "Technical challenges. Recall that the arm chosen in each round by OFUL is ", "page_idx": 1}, {"type": "equation", "text": "$$\nA_{t}=\\underset{a\\in\\mathcal{A}_{t}}{\\mathrm{argmax}}\\langle a,\\widehat{\\theta}_{t}\\rangle+\\sqrt{\\gamma_{t}}\\|a\\|_{V_{t-1}^{-1}}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where ${\\widehat{\\theta}}_{t}$ is the regularized least-squares estimate of $\\theta_{*}$ \uff0c $\\begin{array}{r}{V_{t-1}=I+\\sum_{s<t}A_{s}A_{s}^{\\top}}\\end{array}$ is the regularized covariance matrix of past actions, and $\\sqrt{\\gamma_{t}}$ is the radius of the confidence set ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\left\\{\\theta\\in\\mathbb{R}^{d}:\\|\\theta-\\widehat{\\theta}_{t-1}\\|_{V_{t-1}}^{2}\\leq\\gamma_{t}\\right\\}~.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The squared radius $\\gamma_{t}=O(d\\ln t)$ is such that $\\theta_{*}$ belongs to (1.2) with high probability simultaneously for all $t\\geq1$ . Our approach, instead, builds on the online to confidence set conversion technique of Abbasi- Yadkori et al. [2], where they show how to design a different confidence set for $\\boldsymbol{\\theta}_{*}$ based on the predictions of an arbitrary algorithm for online linear regression, such that the squared radius of the confidence set is roughly equal to the regret bound of the algorithm. Using the algorithm SeqSEW for sparse online linear regression [13], whose regret bound is $O(S\\log T)$ , they obtain the optimal regret $\\widetilde{\\cal O}(\\sqrt{S d T})$ for sparse linear bandits. Unfortunately, this result requires knowing $S$ to properly set the radius of the confidence set. Our strategy SparseLinUCB (Algorithm 1) bypasses this problem by running the online to confidence set conversion technique over a hierarchy of nested confidence sets with radii $\\alpha_{i}=2^{i}\\log T$ for $i=1,\\ldots,n=\\Theta(\\log d)$ . The framework of Abbasi-Yadkori et al. [2] guarantees that, for any sparsity value $S\\in[d]$ , there is a critical radius $\\alpha_{o}=O(S\\log T)$ such that, with high probability, $\\theta_{*}$ lies in the set with radius $\\alpha_{i}$ for all $i\\geq o$ . SparseLinUCB randomizes the choice of the index $i$ of the confidence radius $\\alpha_{i}$ , used for selecting the action at time $t$ .If ", "page_idx": 1}, {"type": "text", "text": "Table 1: Comparison with other sparse linear bandit works. $S\\in[d]$ is the sparsity level and $\\Delta$ is the suboptimality gap (3.3). The nested assumption refers to $(\\theta_{*})_{i}\\neq0$ for $i=1,\\ldots,S$ . The minimum signal and the compatibility condition refer to assumptions on the distribution of the action set and on the smallest value of the non-zero elements in $\\theta_{*}$ . Smoothed adversary refers to adversarially selected action sets with added Gaussian noise. The regret bounds listed in [1, 2, 23, 8, 25, 18, 9] are high-probability bounds: with high probability, the regret is of the same order as the bound in the table. ", "page_idx": 2}, {"type": "table", "img_path": "jIabKyXOTt/tmp/ccfce1c63dbf34ccc21acb4ad131934996788f76c05c4e1f93c78f77f7f780de.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "the random index $I_{t}$ is such that $I_{t}~\\geq~o$ then we can bound the regret incurred at step $t$ using standard techniques [1, 2]. By choosing $\\mathbb{P}(I_{t}=i)$ proportional to $2^{\\bar{-}i}$ , we make sure that larger confidence sets (delivering suboptimal regret bounds) are chosen with exponentially small probability. If $I_{t}<o$ , then $\\theta_{*}$ is not guaranteed to lie in the confidence set of radius $\\alpha_{I_{t}}$ with high probability. Our main technical contribution is to show that the regret summed over these bad rounds is bounded by $\\widetilde{\\cal O}\\big(\\sqrt{S d T/Q}\\big)$ , where $Q=\\mathbb{P}(I_{t}\\geq o)$ . The proof of this bound requires showing that the regret in a bad round $t$ (when $I_{t}<o$ can be bounded by ${\\sqrt{\\alpha_{o}}}\\Vert A_{t}^{o}\\Vert_{V_{t-1}^{-1}}$ . Proving that $\\|A_{t}^{o}\\|_{V_{t-1}^{-1}}$ shrinks fast enough uses the fact that $\\operatorname*{det}V_{t}$ grows fast enough due to the exploration in the good rounds $t$ (when $I_{t}\\geq o$ 0. This is done by a carefully designed peeling technique that partitions $[T]$ in blocks based on the value of $\\operatorname*{det}V_{t}$ ", "page_idx": 2}, {"type": "text", "text": "To extend our analysis of SparseLinUCB and obtain instance-dependent regret bound, we apply the techniques of Abbasi- Yadkori et al. [1] to show that the regret over the good rounds is bounded by $\\widetilde O\\big(d^{2}/\\Delta\\big)$ Theregretovera bad round $t$ iscontroledbye $(\\alpha_{o}/\\Delta)\\|A_{t}^{o}\\|_{V_{t-1}^{-1}}^{2}$ and-usingtechnigues similar to the instance-independent analysis\u2014we bound the regret summed over all bad rounds with $\\widetilde{O}(S d/(Q\\Delta))$ ", "page_idx": 2}, {"type": "text", "text": "Given that SparseLinUCB uses a fixed probability of order $2^{-i}$ to choose its confidence radius $\\alpha_{i}$ , it is tempting to explore adaptive probability assignments, that increase the probability of a confidence set proportionally to the rewards obtained by the actions that were selected based on that set. Algorithm AdaLinUCB (see Algorithm 2) is a variant of SparseLinUCB using Exp3 [4] to assign probabilities to confidence sets. The analysis of AdaLinUCB combines--in a non-trivial way-\u2014-the analysis of Exp3 (including a forced exploration term $q$ ) with that of SparseLinUCB. Although the resulting regret bound does not improve on OFUL, our algorithm provides a new principled solution to the problem of tuning the radius in (1.1). Experiments show that SparseLinUcB can perform better thanOFUL. ", "page_idx": 2}, {"type": "text", "text": "1.1 Additional related work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Sparse linear bandits. With the goal of obtaining sparsity-agnostic regret bounds, different types of assumptions on the action set have been considered in the past. Starting from the $\\widetilde{O}(S\\sqrt{T})$ regret upper bound of [18], where the action set is assumed fixed and equal to the hypercube, some works considered stochastic action sets and proved regret bounds depending on spectral parameters of the action distribution, such as the minimum eigenvalue of the covariance matrix [9, 15, 16, 20]. Others assumed a stochastic action set with strong properties, such as compatibility conditions or margin conditions [3, 6, 7, 19, 22]. As far as we know, there has been no research on adaptive adversarial action sets after [2]. ", "page_idx": 3}, {"type": "text", "text": "Model selection. Sparse linear bandits can be naturally viewed as a bandit model selection problem. For example, Ghosh et al. [14] establish a regret bound of $\\widetilde{O}(\\sqrt{S T}+d^{2}/\\alpha^{4.65})$ for a fixed action set, where $\\alpha$ is the minimum absolute value of the nonzero components of $\\theta_{*}$ . Quite a bit of work has been devoted to sparse regret bounds in the nested setting. With i.i.d. and fixed-size actions sets, Foster et al. [12] achieve a regret bound of order $\\widetilde{\\cal O}(S^{1/3}T^{2\\bar{/}3}/\\gamma^{3})$ in the nested setting, where $\\gamma$ is the smallest eigenvalue of the covariance matrix of $\\dot{\\mathcal{A}}_{t}$ . Under the same assumption on the action set, Pacchiano et al. [24, 23] obtain a regret bound of $\\widetilde{\\cal O}(S^{2}\\sqrt{T})$ . For adversarial action sets, Cutkosky et al. [8] obtain $\\widetilde{\\cal O}(S^{2}\\sqrt{T})$ in the nested setting. When actions are sampled i.i.d., Cutkosky et al. [8] and Pacchiano et al. [25] obtain a regret bound of $\\widetilde{\\cal O}(S^{2}\\sqrt{T})$ for nested settings. They also obtain simultaneous instance-dependent bounds, in particular, Pacchiano et al. [25] achieve $\\tilde{\\cal O}\\big((S d)^{2}/\\Delta\\big)$ Compared to the instance-dependent regret bound, our results are more general, as we allow the action set to be adaptively chosen by an adversary and do not require the nested assumption. ", "page_idx": 3}, {"type": "text", "text": "Parameter tuning. Although the theoretical anlysis of OFUL only holds for $\\gamma_{t}=O(d\\log t)$ ,smaller choices of the radius in (1.2) are known to perform better in practice. Our design of SparseLinUCB and AdaLinUcB borrows ideas from the parameter tuning setting, which is typically addressed using a set of base algorithms and a randomized master algorithm that adaptively changes the probability of selecting each base algorithm [21, 23, 24]. In particular, AdaLinUCB builds on [11], where they show that running Exp3 as the master algorithm over instances of OFUL with different radii has a better empirical performance than Thompson Sampling and UCB. Yet, they only show a regret bound of $\\tilde{O}(\\bar{T}^{2/3})$ when the action set is drawn i.i.d. in each round (they also prove a bound of order $\\sqrt{T}$ but only under additional assumptions on the best model). This is consistent with the results of Pacchiano et al. [24], who also obtained a regret of the same order using Exp3 as master algorithm. ", "page_idx": 3}, {"type": "text", "text": "2 Problem definition ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In linear bandits, a learner and an adversary interact over $T$ rounds. In each round $t=1,\\dots,T$ ", "page_idx": 3}, {"type": "text", "text": "1. The adversary chooses an arm set $A_{t}\\subset\\mathbb{R}^{d}$   \n2. The learner choose an arm $A_{t}\\in\\mathcal A_{t}$   \n3. The learner obtains a reward $X_{t}$ ", "page_idx": 3}, {"type": "text", "text": "We assume the adversary is adaptive, i.e., $\\boldsymbol{A}_{t}$ can depend in an arbitrary way on the (possibly randomized) past choices of the learner. The reward in each round $t$ satisfies ", "page_idx": 3}, {"type": "equation", "text": "$$\nX_{t}=\\langle A_{t},\\theta_{*}\\rangle+\\varepsilon_{t}\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here $\\theta_{*}\\,\\in\\,\\mathbb{R}^{d}$ is a fixed and unknown target vector and $\\{\\varepsilon_{t}\\}_{t\\in[T]}$ are independent conditionally 1-subgaussian random variables. We also assume $\\lVert\\theta_{*}\\rVert_{2}\\leq1$ and $\\|a\\|_{2}\\leq1$ for all $a\\in\\mathcal{A}_{t}$ and all $t\\,\\in\\,[\\bar{T}]$ .1 The regret of a strategy over $T$ rounds is defined as the difference between the reward obtained by the optimal policy, always choosing the best arm in $\\boldsymbol{A}_{t}$ , and the reward obtained by the strategy choosing arm $A_{t}\\in\\mathcal A_{t}$ for $t\\in[T]$ \uff0c ", "page_idx": 3}, {"type": "equation", "text": "$$\nR_{T}=\\sum_{t=1}^{T}\\operatorname*{max}_{a\\in A_{t}}\\langle a,\\theta_{*}\\rangle-\\sum_{t=1}^{T}\\langle A_{t},\\theta_{*}\\rangle.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In the sparse setting, we would like to devise strategies whose regret depends on ", "page_idx": 4}, {"type": "equation", "text": "$$\nS=\\lVert\\theta_{*}\\rVert_{0}=\\sum_{i=1}^{d}\\mathbb{1}\\{\\theta_{i}\\neq0\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "corresponding to the number of nonzero components of $\\boldsymbol{\\theta}_{*}$ ", "page_idx": 4}, {"type": "text", "text": "2.1  Online to confidence set conversions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Establishing a confidence set including the target vector with high probability is at the core of linear bandit algorithms, and our approach for designing a sparsity-agnostic algorithm is based on a result by Abbasi-Yadkori et al. [2]. They show how to construct a confidence set for OFUL [1] based on the predictions of a generic algorithm for online linear regression, a sequential decision-making setting defined as follows. For $t=1,\\dots,T$ ", "page_idx": 4}, {"type": "text", "text": "1. The adversary privately chooses input $A_{t}\\in\\mathbb{R}^{d}$ and outcome $X_{t}\\in\\ensuremath{\\mathbb{R}}$   \n2. The learner observes $A_{t}$ and chooses prediction $\\widehat{X}_{t}\\in\\mathbb{R}$   \n3. The adversary reveals $X_{t}$ and the learner suffers loss $(\\widehat{X}_{t}-X_{t})^{2}$ ", "page_idx": 4}, {"type": "text", "text": "The learner's goal in online linear regression is to minimize the following notion of regret against any comparator $\\bar{\\theta^{\\mathrm{~}}}\\in\\mathbb{R}^{d}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\rho_{T}(\\theta)=\\sum_{t=1}^{T}(X_{t}-\\widehat{X}_{t})^{2}-\\sum_{t=1}^{T}(X_{t}-\\langle A_{t},\\theta\\rangle)^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The confidence set proposed by Abbasi- Yadkori et al. [2] is established by the following result. ", "page_idx": 4}, {"type": "text", "text": "Lemma 2.1 (Abbasi-Yadkori et al. [2, Corollary 2]). Let $\\delta\\,\\in\\,(0,1/4]$ and $\\lVert{\\boldsymbol{\\theta}}_{*}\\rVert_{2}\\leq1$ Assumea sequence $\\{(A_{t},X_{t})\\}_{t\\in[T]}.$ where $X_{t}$ satisfies (2.1) for all $t\\in[T]$ is fed to an online linear regression algorithm $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ generating predictions $\\{\\widehat{X}_{t}\\}_{t\\in[T]}$ .Then $\\mathbb{P}\\big(\\exists t\\in[T]\\,:\\,\\theta_{*}\\not\\in\\mathcal{C}_{t}\\big)\\le\\delta_{t}$ where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal C_{t}=\\left\\{\\theta\\in\\mathbb R^{d}:\\|\\theta\\|_{2}^{2}+\\displaystyle\\sum_{s=1}^{t-1}(\\widehat{X}_{s}-\\langle A_{s},\\theta\\rangle)^{2}\\leq\\gamma(\\delta,\\theta_{*})\\right\\}}\\\\ {\\gamma(\\delta,\\theta_{*})=2+2B_{T}(\\theta_{*})+32\\log\\left(\\frac{\\sqrt{8}+\\sqrt{1+B_{T}(\\theta_{*})}}{\\delta}\\right)\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and $B_{T}(\\theta_{*})$ is an upper bound on the regret $\\rho_{T}(\\theta_{*})$ of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ When understood from the context, we will abuse the notation and denote the best radius in hindsight by $\\gamma(\\delta):=\\gamma(\\delta,\\theta_{\\ast})$ and the regret bound by $B_{T}$ ", "page_idx": 4}, {"type": "text", "text": "Gerchinovitz [13] designed an algorithm, SeqSEw, for sparse linear regression that bounds $\\rho_{T}(\\theta)$ in terms of $\\|\\theta\\|_{0}$ simultaneously for all comparators $\\boldsymbol{\\theta}\\in\\mathbb{R}^{d}$ . Below here, we state his bound in the formulation of Lattimore and Szepesvari [17]. ", "page_idx": 4}, {"type": "text", "text": "Lemma 2.2 (Lattimore and Szepesvari [17, Theorem 23.6]). Assume $\\operatorname*{max}_{t\\in[T]}\\|A_{t}\\|_{2}\\,\\leq\\,1$ and $\\operatorname*{max}_{t\\in[T]}|X_{t}|\\leq1$ . There exists a universal constant c such that algorithm SeqSEw achieves, for any $\\boldsymbol{\\theta}\\in\\mathbb{R}^{d}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\rho_{T}(\\theta)\\leq B_{T}(\\theta):=c\\|\\theta\\|_{0}\\left\\{\\log(e+T^{1/2})+C_{T}\\log\\left(1+\\frac{\\|\\theta\\|_{1}}{\\|\\theta\\|_{0}}\\right)\\right\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $C_{T}=2+\\log_{2}\\log(e+T^{1/2})$ ", "page_idx": 4}, {"type": "text", "text": "Using the confidence set (2.2) with $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ set to SeqSEW, Abbasi-Yadkori et al. [2] achieved the minimax optimal regret bound of $\\widetilde{O}(\\sqrt{S d T})$ . However, to construct $\\ensuremath{\\mathcal{C}}_{t}$ , the learner must know $\\gamma(\\delta)$ , which depends on the unknown sparsity level $S=\\lVert\\boldsymbol{\\theta}_{*}\\rVert_{0}$ through $B_{T}$ ", "page_idx": 4}, {"type": "text", "text": "1: Input: $T\\in\\mathbb N$ \uff0c $n\\in\\mathbb N$ and $\\begin{array}{r}{\\mathbf{q}\\in\\Delta_{n}:=\\{(q_{1},\\ldots,q_{n})\\in[0,1]^{n}:\\sum_{i=1}^{n}q_{i}=1\\}}\\end{array}$   \n2: Initialization: Let $V_{0}=I$ $\\widehat{\\theta}_{0}=(0,\\ldots,0)$   \n3: for $t=1,2,\\ldots,T$ do   \n4:Receive action set $A_{t}$ and draw $I_{t}$ from distribution q   \n5:  Choose action $A_{t}=\\operatorname*{argmax}_{a\\in A_{t}}\\left(\\langle a,\\widehat{\\theta}_{t-1}\\rangle+\\|a\\|_{V_{t-1}^{-1}}\\sqrt{2^{I_{t}}\\log T}\\right)$   \n6: Receive reward $X_{t}$   \n7: $V_{t}=V_{t-1}+A_{t}A_{t}^{\\top}$   \n8: Feed $(A_{t},X_{t})$ to SeqSEW and obtain prediction ${\\widehat{X}}_{t}$   \n9: Compute regularized least squares estimate $\\widehat{\\theta}_{t}=\\operatorname*{argmin}_{\\theta\\in\\mathbb{R}^{d}}\\bigg(\\|\\theta\\|_{2}^{2}+\\sum_{s=1}^{t}\\big(\\widehat{X}_{s}-\\langle\\theta,A_{s}\\rangle\\big)^{2}\\bigg)$ ", "page_idx": 5}, {"type": "text", "text": "3  A multi-level sparse linear bandit algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we introduce our main algorithm, SparseLinUcB, whose pseudo-code is shown in Algorithm 1. The algorithm, which runs SeqSEW as base algorithm $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , uses a hierarchy of confidence sets of increasing radius. In each round $t=1,\\dots,T$ , after receiving the action set $\\boldsymbol{A}_{t}$ the algorithm draws the index $I_{t}$ of the confidence set for time $t$ by sampling from the distribution $\\begin{array}{r}{\\mathbf q\\in\\Delta_{n}:=\\{(q_{1},\\ldots,q_{n})\\in[0,1]^{n}:\\sum_{i=1}^{n}q_{i}=1\\}.}\\end{array}$ Then the algorithm plays the action $A_{t}$ using the confidence set $\\mathcal{C}_{t}^{I_{t}}:=\\{\\theta\\in\\mathbb{R}^{d}:\\|\\theta-\\widehat{\\theta}_{t-1}\\|_{V_{t-1}}^{2}\\leq2^{I_{t}}\\log T\\}$ (where a larger $I_{t}$ implies a larger radius, and thus more exploration). Following the online to confidence set approach, upon receiving the reward $X_{t}$ , the algorithm feeds the pair $\\left(A_{t},X_{t}\\right)$ to SeqSEW and uses the prediction ${\\widehat{X}}_{t}$ to update the regularized least squares estimate ${\\widehat{\\theta}}_{t}$ ", "page_idx": 5}, {"type": "text", "text": "Let $\\alpha_{i}=2^{i}\\log T$ for all $i\\in\\mathbb N$ and set $n\\in{\\mathbb N}$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\nn=\\left\\lceil\\log_{2}\\frac{\\operatorname*{max}_{\\theta\\in\\mathbb{R}^{d}:\\|\\theta\\|_{2}\\leq1}\\gamma(1/T,\\theta)}{\\log T}\\right\\rceil\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\gamma(\\delta,\\theta)$ is defined in Lemma 2.1 for $\\mathcal{B}=\\mathtt{S e q S E W}$ ", "page_idx": 5}, {"type": "text", "text": "One can check that $n=\\Theta(\\log d)$ (when $\\|\\theta\\|_{0}=d)$ , which gives $\\alpha_{n}=\\Theta(d\\log T)$ . Our bounds depend on the following quantity, which defines the index of the smallest \u201csafe\u201d confidence set (i.e., the smallest $i\\in[n]$ such that $\\bar{\\theta_{*}}\\in\\bar{\\mathcal{C}_{t}^{i}}$ for all $t\\in[T])$ ", "page_idx": 5}, {"type": "equation", "text": "$$\no:=\\underset{i\\in[n]}{\\operatorname{argmin}}\\left\\{\\gamma(1/T)\\leq\\alpha_{i}\\right\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The choice of our confidence set (Line 4 in Algorithm 1) is justified by the following result, which implies that $o$ is safe. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.1. For $\\ensuremath{\\mathcal{C}}_{t}$ defined in (2.2), we have that $\\mathcal{C}_{t}\\subseteq\\mathcal{C}_{t}^{o}$ for all $t\\in[T]$ ", "page_idx": 5}, {"type": "text", "text": "As we use SeqSEW as base algorithm $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ $\\alpha_{o}=O(S\\log T)$ . Our main result is an upper bound on the regret of SparseLinUCB. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2. The expected regret of SparseLinUCB run with the number of models n in (3.1) and adistribution $\\pmb q=\\{q_{s}\\}_{s\\in[n]}$ satisies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]=O\\left((\\log T)\\sum_{s\\geq o}{\\sqrt{d^{2}^{s}T q_{s}}}+(\\log T){\\sqrt{S T d/Q}}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where Q = \u2211s\u2265o ls. ", "page_idx": 5}, {"type": "text", "text": "If the sparsity level $S$ is indeed known, then $o$ in (3.2) can be computed and we get the following bound, which is tight up to log factors [17]. ", "page_idx": 5}, {"type": "text", "text": "Corollary 3.3. Assume that the sparsity level $S$ is known and choose the number of models $n>o$ and the distribution $\\{q_{s}\\}_{s\\in[n]}$ with $q_{o}=1$ where o is set as in (3.2). Then, the expected regret of SparseLinUCB is $\\mathbb{E}[R_{T}]=O\\big(\\sqrt{S d T}\\log T\\big)$ ", "page_idx": 6}, {"type": "text", "text": "An instance-dependent bound. SparseLinUCB also enjoys an instance-dependent regret bound comparable to that of OFUL. Let $\\Delta$ be the minimum gap between the optimal arm and any suboptimal arms over all rounds, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Delta=\\operatorname*{min}_{t\\in[T]}\\operatorname*{min}_{a\\in\\mathcal{A}_{t}\\backslash A_{t}^{*}}\\langle A_{t}^{*}-a,\\theta_{*}\\rangle,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $A_{t}^{*}=\\operatorname*{max}_{a\\in\\mathcal{A}_{t}}\\langle a,\\theta_{*}\\rangle$ is the optimal arm for round $t$ ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.4. The expected regret of SparseLinUCB run with the number of models n in (3.1), $a$ distribution ${\\pmb q}=\\{{\\boldsymbol q}_{s}\\}_{s\\in[n]}$ and using SeqsEw as base algorithm satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]=O\\left(\\frac{(d S/Q)+d^{2}}{\\Delta}(\\log T)^{2}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{Q=\\sum_{s\\geq o}q_{s}}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "Sparsity-agnostic tuning of randomization. Next, we look at a specific choice of $\\mathbf{q}$ Fix $C\\geq1$ and let ", "page_idx": 6}, {"type": "equation", "text": "$$\nq_{s}=\\left\\{\\begin{array}{c l}{{C^{2}2^{-s}}}&{{\\mathrm{if}\\;C^{2}2^{-s}<1}}\\\\ {{\\kappa}}&{{\\mathrm{otherwise},}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\kappa>0$ is chosen so to normalize the probabilities. It is easy to verify that for any $C\\geq1$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{s\\in[n]}\\mathbb{1}\\big\\{C^{2}2^{-s}<1\\big\\}q_{s}\\leq1\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "implying that $\\kappa$ can be chosen in $[0,1]$ . Combining Theorem 3.2 and 3.4, we obtain the following corollary providing a hybrid distribution-free and distribution-dependent bound. ", "page_idx": 6}, {"type": "text", "text": "Corollary 3.5. Pick any $C\\geq1$ . Let the number of models n as in (3.1) and $\\pmb q=\\{q_{s}\\}_{s\\in[n]}$ bechosen as in (3.4). Then the expected regret of SparseLinUCB is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]=\\widetilde{O}\\left(\\operatorname*{min}\\left\\{\\operatorname*{max}\\big\\{C,S/C\\big\\}\\sqrt{d T},\\,\\frac{\\operatorname*{max}\\big\\{d^{2},S^{2}d/C^{2}\\big\\}}{\\Delta}\\right\\}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For $C\\,=\\,1$ the above bound is $\\widetilde{O}(S\\sqrt{d T})$ , which is tight up to the factor $\\sqrt{S}$ due to the lower bound of $\\Omega({\\sqrt{S d T}})$ [17]. However, as mentioned in Lattimore and Szepesvari [17, Section 23.5], no algorithm can enjoy the regret of $\\widetilde{O}(\\sqrt{S d T})$ simultaneously for all possible sparsity levels $S$ While our worst-case regret bound improves with a smaller $S$ , the problem-dependent regret bound scales at least as $(d^{2}/\\Delta\\bar{)}\\log T$ , which is independent of $S$ . This raises an interesting question: could the problem-dependent bound also benefit from sparsity? Even with a very small probability $p$ of choosing radius $\\alpha_{n}$ , the expected number of steps using $\\alpha_{n}$ would be $p T$ . The results in [2] demonstrate that running the OFUL algorithm with $\\alpha_{n}$ over $p T$ steps results in a regret of $\\widetilde{O}(d^{2}/\\Delta)$ One simple way is to decrease the frequency of selecting radius $\\alpha_{n}$ . However, selecting $\\alpha_{n}$ less than $d^{2}/\\dot{\\Delta}^{2}$ times may prevent the algorithm from obtaining a good enough estimate of $\\theta_{*}$ in certain settings. ", "page_idx": 6}, {"type": "text", "text": "Remark 3.6. At first glance, it may seem straightforward to select $C$ in Corollary 3.5, as setting $C={\\sqrt{d}}$ yields a regret of $\\widetilde O(d^{2}/\\bar{\\Delta})$ without apparent trade-offs.However, the trade-off lies in balancing the instance-dependent and worst-case regret bounds. Opting for $C=d$ indeedyields an instance-dependentboundof $\\widetilde O(d^{2}/\\Delta)$ . However, this comes at the expense of the worst-case bound, which remains $\\widetilde{O}(d\\sqrt{T})$ , negating any advantages derived from the sparsity assumption $S\\ll d,$ ", "page_idx": 6}, {"type": "text", "text": "If the sparsity level $S$ is indeed known, then $o$ in (3.2) can be computed and we get the following bound. ", "page_idx": 6}, {"type": "text", "text": "Corollary 3.7. Assume that the sparsity level $S$ is known and choose $\\{q_{s}\\}_{s\\in[n]}$ with $q_{o}=1$ where $o$ is set as in (3.2). Then, the expected regret of SparseLinUCB is $\\begin{array}{r}{\\mathbb{E}[R_{T}]=\\widetilde{O}\\big(\\frac{S d}{\\Delta}\\big)}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "We note that by setting $q_{o}=1$ in Theorem 3.4, the regret bound becomes $\\widetilde{O}(d^{2}/\\Delta)$ . This result, as detailed in Theorem 3.4, arises from the parameter $q_{n}>0$ . However, in this case, $q_{n}=0$ which allows us to achieve a more favorable regret bound. ", "page_idx": 6}, {"type": "text", "text": "1: Input: $T\\in\\mathbb N$ \uff0c $\\eta>0$ \uff0c $q\\in(0,1]$   \n2: Initialization: Let $S_{i,0}=0$ for all $i\\in[n]$ $V_{0}=I,\\widehat{\\theta}_{0}=(0,\\dots,0)$   \n3: for $t=1,2,\\cdots,T$ do   \n4:  Receive action set $A_{t}$ and draw a Bernoullirandom variable $Z_{t}$ with $\\mathbb{P}(Z_{t}=1)=q$   \n5: if $Z_{t}=1$ then   \n6: Choose optimisticaction $A_{t}=\\operatorname*{argmax}_{a\\in{\\mathcal{A}}_{t}}\\left(\\langle a,\\widehat{\\theta}_{t-1}\\rangle+\\|a\\|_{V_{t-1}^{-1}}\\sqrt{2^{n}\\log T}\\right)$   \n7: Receive reward $X_{t}$   \n8: else   \n9: $I_{t}$ $P_{t,i}=\\frac{\\exp{(\\eta S_{t-1,i})}}{\\sum_{j=1}^{n}\\exp{\\left(\\eta S_{t-1,j}\\right)}}$ for $i\\in[n]$ .\uff0c   \n10: Choose action $A_{t}=\\operatorname*{argmax}_{a\\in A_{t}}\\left(\\langle a,\\widehat{\\theta}_{t-1}\\rangle+\\|a\\|_{V_{t-1}^{-1}}\\sqrt{2^{I_{t}}\\log T}\\right)$   \n11: Receive reward $X_{t}$ .\uff0c   \n12: Compute $S_{t,j}=S_{t-1,j}-\\frac{\\mathbb{1}\\{I_{t}=j\\}(2-X_{t})/4}{P_{t,j}}\\;\\mathrm{for}\\;j\\in[n]$   \n13: end if   \n14: $V_{t}=V_{t-1}+A_{t}A_{t}^{\\top}$   \n15: Feed $(A_{t},X_{t})$ to SeqSEW and obtain prediction ${\\widehat{X}}_{t}$   \n16: Compute regularized least squares estimate $\\widehat{\\theta}_{t}=\\operatorname*{argmin}_{\\theta\\in\\mathbb{R}^{d}}\\bigg(\\|\\theta\\|_{2}^{2}+\\sum_{s=1}^{t}\\big(\\widehat{X}_{s}-\\langle\\theta,A_{s}\\rangle\\big)^{2}\\bigg);$ ", "page_idx": 7}, {"type": "text", "text": "17: end for ", "page_idx": 7}, {"type": "text", "text": "4   Adaptive model selection for stochastic linear bandits ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "SparseLinUcB is also designed to handle adaptive adversarial action sets. A crucial parameter of SparseLinUCBis $\\{q_{s}\\}_{s\\in[n]}$ , the distribution from which the radius of the confidence set is drawn. It is a natural question whether there exists an algorithm that adaptively updates this distribution based on the observed rewards. In this section we introduce AdaLinUCB (Algorithm 2), which runs Exp3 to dynamically adjust the distribution used by SparseLinUCB. ", "page_idx": 7}, {"type": "text", "text": "AdaLinUCB takes as input a forced exploration term $q$ and the learning rate $\\eta$ for Exp3. Similarly to SparseLinUCB, AdaLinUCB designs confidence sets of various radii, but its selection method differs in two aspects. First, with probability $q$ , the algorithm performs exploration based on the confidence set with the largest radius. With probability $1-q$ , the algorithm instead draws the action based on Exp3. The distribution $P_{t}$ used by Exp3 at round $t$ is based on exponential weights applied to the total estimated loss, denoted by $S_{t}$ (for technical reasons, we translate losses into rewards). The algorithm then draws $I_{i}$ from $P_{t}$ and selects the action $A_{t}$ based on the confidence set with radius $2^{I_{t}}\\log T$ Finally, reward $X_{t}$ is observed and the pair $(A_{t},X_{t})$ is fed to SeqSEw. The prediction ${\\widehat{X}}_{t}$ returned by SeqSEw is used to update the regularized least squares estimate ${\\widehat{\\theta}}_{t}$ ", "page_idx": 7}, {"type": "text", "text": "The following theorem states the theoretical regret upper bound of AdaLinUCB. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.1. If the random independent noise $\\varepsilon_{t}$ in (2.1) satisfies $\\varepsilon_{t}\\in[-1,1]$ for all $t\\in[T]$ then the regret of AdaLinUCB run with $\\eta=\\sqrt{(\\log n)/(T n)}$ for $n$ in (3.1) and $q\\in(0,1]$ satisfies ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[R_{T}]\\leq\\left(\\sqrt{8\\alpha_{n}q}+4\\sqrt{(2\\alpha_{o})/q}\\right)\\sqrt{d T\\log\\left(1+\\frac{T L^{2}}{d}\\right)+1}+O\\big(\\sqrt{n T\\log n}\\big)}\\\\ &{\\qquad=\\widetilde{O}\\left(\\operatorname*{max}\\left\\{\\sqrt{d q},\\sqrt{S/q}\\right\\}\\sqrt{d T}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Although AdaLinUCB dynamically adjusts the distribution used by SparseLinUCB and may achieve better empirical performance, its regret bound is no better than that of SparseLinUcB. The issue is that the action chosen by AdaLinUcB in Line 10 does not ensure enough exploration to control the regret. Consequently, the algorithm needs to choose the optimistic action in Line 6 with constant probability $q$ . SparseLinUCB has a similar parameter, $Q$ , that bounds from the above the probability of choosing the optimistic action. The key difference is that $Q$ can be optimized for an unknown $S$ by carefully selecting the distribution $\\mathbf{q}=\\{q_{s}\\}_{s\\geq1}$ , whereas the parameter $q$ does not provide a similar flexibility. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5  Model selection experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section we describe some experiments we performed on synthetic data to verify whether AdaLinUCB could outperform OFUL in a model selection task. We also test the empirical performance of SparseLinUcB on the same data (additional details on all the algorithms and the experimental setting are in Appendix E). The data for our model selection experiments are generated using targets $\\boldsymbol{\\theta}_{*}$ with different sparsity levels, as we know that sparsity affects the radius of the optimal confidence set. On the other hand, since no efficient implementation of SeqSEw is known [17, Section 23.5], we cannot implement the online to confidence set approach as described in [2] to capture sparsity. Instead, we run SparseLinUCB and AdaLinUCB with ${\\widehat{X}}_{t}=X_{t}$ for all $t\\in[T]$ , which\u2014due to the form of our confidence sets--amounts to running the algorithms over multiple instances of OFUL with different choices of radius $\\alpha_{i}$ for $i\\in[n]$ ", "page_idx": 8}, {"type": "text", "text": "We run SparseLinUCB and AdaLinUCB with $\\alpha_{i}~=~\\alpha_{i,t}~=~2^{i}\\log t$ (a mildly time-dependent choice) for $i\\,=\\,0,1,\\cdot\\cdot\\cdot\\,,\\log_{2}d$ . We also include $\\alpha_{0}~=~0$ corresponding to the greedy strategy $A_{t}=\\operatorname{argmax}_{a\\in\\mathcal{A}_{t}}\\langle a,\\widehat{\\theta}_{t-1}\\rangle$ . The suffix _Unif indicates $\\{q_{s}\\}_{s\\in[n]}$ set to $\\textstyle\\left({\\frac{1}{n}},\\cdot\\cdot\\cdot\\;,{\\frac{1}{n}}\\right)$ . The suffx Theory indicates $q_{s}\\,=\\,\\Theta(2^{-s})$ for $s\\,=\\,0,\\ldots,n$ as prescribed by (3.4). Finally, we included SparseLinUCB_Known using $q_{i}=\\mathbb{1}\\{i=o\\}$ to test the performance when the optimal index $o$ (for the given $S$ ) is known in advance (see Corollary 3.3). We run our experiments with a fixed set of random actions, $\\mathcal{A}_{t}=\\mathcal{A}$ for all $t\\in[T]$ , where $\\lvert A\\rvert=30$ and $\\boldsymbol{\\mathcal{A}}$ is a set of vectors drawn i.i.d. from the unit sphere in $\\mathbb{R}^{16}$ . The target vector $\\theta_{*}$ is a $S$ -sparse $(S=1,2,4,8,16)$ vector whose non-zero coordinates are drawn from the unit sphere in $\\mathbb{R}^{S}$ . The noise $\\varepsilon_{t}$ is drawn i.i.d. from the uniform distribution over $[-1,1]$ . Each curve is an average over 20 repetitions with $T=10^{4}$ where, in each repetition, we draw fresh instances of $\\boldsymbol{\\mathcal{A}}$ and $\\boldsymbol{\\theta}_{*}$ ", "page_idx": 8}, {"type": "text", "text": "As our implementations are not sparsity-aware, we cannot expect the regret to strongly depend on the sparsity level. Indeed, only the regret of SparseLinUCB_Known (which is tuned to the sparsity $S$ )is significantly affected by sparsity. The theory-driven choice of $\\{q_{s}\\}_{s\\in[n]}$ (SparseLinUCB_Theory) performs better than the uniform assignment (SparseLinUCB_Unif), and is in the same ballpark as OFUL. On the other hand, AdaLinUCB_Unif and AdaLinUCB_Theory outperform all the competitors, including OFUL. This provides evidence that using Exp3 for adaptive model selection may significantly boost the empirical performance of stochastic linear bandits. ", "page_idx": 8}, {"type": "text", "text": "6  Limitations and open problems ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Unlike previous works, we prove sparsity-agnostic regret bounds with no assumptions on the action sets or on $\\boldsymbol{\\theta}_{*}$ (other than boundedness of $\\lVert\\theta_{*}\\rVert$ and $\\lVert a\\rVert$ for $a\\ \\in{\\mathcal{A}}_{t}$ , which are rather standard assumptions). For AdaLinUCB, however, we do require boundedness of the noise $\\varepsilon_{t}$ (instead of just subgaussianity). We conjecture this requirement could be dropped at the expense of a further $\\log T$ factor in the regret. Finally, for effciency reasons our implementations are not designed to capture sparsity. Hence our experiments are limited to testing the impact of model selection. ", "page_idx": 8}, {"type": "text", "text": "Our work leaves some open problems: ", "page_idx": 8}, {"type": "text", "text": "1. Proving a lower bound on the regret of sparse linear bandits when the sparsity level is unknown to the learner would be important. Citing again [17, Section 23.5], no algorithm can enjoy regret $\\widetilde{O}(\\sqrt{S d T})$ simultaneously for all sparsity levels $S$ . However, we do not know whether the known lower bound $\\Omega({\\sqrt{S d T}})$ can be strengthened to $\\Omega(S\\sqrt{d T})$ in the agnostic case.   \n2. Our instance-dependent regret bound is of order $\\widetilde{O}\\big(\\operatorname*{max}\\{S^{2},d\\}\\frac{d}{\\Delta}\\big)$ . It would be interesting to prove an upper bound that improves on the factor $d^{2}/\\Delta$ , or a lower bound showing that $d^{2}/\\Delta$ cannot be improved on.   \n3. Our bound on the regret of AdaLinUCB looks pessimistic due to the presence of the constant exploration probability $q$ . It would be interesting to prove a bound that more closely reflects the good empirical performance of this algorithm. ", "page_idx": 8}, {"type": "image", "img_path": "jIabKyXOTt/tmp/ddf3d70530491c03529e2983a9e8e4d8154b58c760ba17855bb045f916588573.jpg", "img_caption": ["Figure 1: Experimental results with different sparsity levels $S\\in\\{1,2,4,8,16\\}$ . In each plot, the $X$ -axis are time steps in $\\lbrack1,10^{4}]$ and the $Y$ -axis is cumulative regret. AL stands for AdaLinUCB and SL stands for SparseLinUCB. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank the anonymous reviewers for their helpful comments. This research is supported by the MUR PRIN grant 2022EKNE5K (Learning in Markets and Society), the FAIR (Future Artificial Intelligence Research) project, funded by the NextGenerationEU program within the PNRR-PE-AI scheme, the EU Horizon CL4-2022-HUMAN-02 research, innovation action under grant agreement 101120237, project ELIAS (European Lighthouse of AI for Sustainability), a Singapore Ministry of Education AcRF Tier 2 grant (A-8000423-00-00), and the National Research Foundation, Singapore under its AI Singapore Program (AISG Award No: AISG-PhD/2021-01-004[T]). ", "page_idx": 9}, {"type": "text", "text": "In particular, NCB and KJ acknowledge the financial support from the MUR PRIN grant, the FAIR project, and the ELIAS project. TJ is funded by a Singapore Ministry of Education AcRF Tier 2 grant (A-8000423-00-00) and the National Research Foundation, Singapore under its AI Singapore Program (AISG Award No: AISG-PhD/202101-004[T]). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1]  Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. Advances in neural information processing systems, 24, 2011.   \n[2]  Yasin Abbasi- Yadkori, David Pal, and Csaba Szepesvari. Online-to-confidence-set conversions and application to sparse stochastic bandits. In Artificial Intelligence and Statistics, pages 1-9. PMLR, 2012.   \n[3]  Kaito Ariu, Kenshi Abe, and Alexandre Proutiere. Thresholded lasso bandit. In International Conference on Machine Learning, pages 878-928. PMLR, 2022.   \n[4]  Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48-77, 2002.   \n[5]  S\u00e9bastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends? in Machine Learning, 5(1):1-122, 2012.   \n[6] Sunrit Chakraborty, Saptarshi Roy, and Ambuj Tewari. Thompson sampling for highdimensional sparse linear contextual bandits. In International Conference on Machine Learning, pages 3979-4008. PMLR, 2023.   \n[7] Yi Chen, Yining Wang, Ethan X Fang, Zhaoran Wang, and Runze Li. Nearly dimensionindependent sparse linear bandit over small action spaces via best subset selection. Journal of the American Statistical Association, 119(545):246-258, 2024.   \n[8]  Ashok Cutkosky, Christoph Dann, Abhimanyu Das, Claudio Gentile, Aldo Pacchiano, and Manish Purohit. Dynamic balancing for model selection in bandits and rl. In International Conference on Machine Learning, pages 2276-2285. PMLR, 2021.   \n[9] Yan Dai, Ruosong Wang, and Simon Shaolei Du. Variance-aware sparse linear bandits. In The Eleventh International Conference on Learning Representations, 2023.   \n[10] Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit feedback. 2008.   \n[11] Qin Ding, Yue Kang, Yi-Wei Liu, Thomas Chun Man Lee, Cho-Jui Hsieh, and James Sharpnack. Syndicated bandits: A framework for auto tuning hyper-parameters in contextual bandit algorithms. Advances in Neural Information Processing Systems, 35:1170-1181, 2022.   \n[12] Dylan J Foster, Akshay Krishnamurthy, and Haipeng Luo. Model selection for contextual bandits. Advances in Neural Information Processing Systems, 32, 2019.   \n[13] Sebastien Gerchinovitz. Sparsity regret bounds for individual sequences in online linear regression. In Proceedings of the 24th Annual Conference on Learning Theory, pages 377-396. JMLR Workshop and Conference Proceedings, 2011.   \n[14]  Avishek Ghosh, Abishek Sankararaman, and Ramchandran Kannan. Problem-complexity adaptive model selection for stochastic linear bandits. In International Conference on Artificial Intelligence and Statistics, pages 1396-1404. PMLR, 2021.   \n[15]  Botao Hao, Tor Lattimore, and Mengdi Wang. High-dimensional sparse linear bandits. Advances in Neural Information Processing Systems, 33:10753-10763, 2020.   \n[16]  Kyoungseok Jang, Chicheng Zhang, and Kwang-Sung Jun. Popart: Efficient sparse regression and experimental design for optimal sparse linear bandits. Advances in Neural Information Processing Systems, 35:2102-2114, 2022.   \n[17]  Tor Lattimore and Csaba Szepesvari. Bandit algorithms. Cambridge University Press, 2020.   \n[18]  Tor Lattimore, Koby Crammer, and Csaba Szepesvari. Linear multi-resource allocation with semi-bandit feedback. Advances in Neural Information Processing Systems, 28, 2015.   \n[19] Ke Li, Yun Yang, and Naveen N Narisetty. Regret lower bound and optimal algorithm for high-dimensional contextual linear bandit. Electronic Journal of Statistics, 15(2):5652-5695, 2021.   \n[20] Wenjie Li, Adarsh Barik, and Jean Honorio. A simple unifed framework for high dimensional bandit problems. In International Conference on Machine Learning, pages 12619-12655. PMLR, 2022.   \n[21] Teodor Vanislavov Marinov and Julian Zimmert. The pareto frontier of model selection for general contextual bandits. Advances in Neural Information Processing Systems, 34:17956- 17967, 2021.   \n[22]  Min-Hwan Oh, Garud Iyengar, and Assaf Zeevi. Sparsity-agnostic lasso bandit. In International Conference on Machine Learning, pages 8271-8280. PMLR, 2021.   \n[23] Aldo Pacchiano, Christoph Dan, Claudio Gentile, and Peter Bartlett Regret bound balancing and elimination for model selection in bandits and RL. arXiv preprint arXiv:2012.13045, 2020.   \n[24]  Aldo Pacchiano, My Phan, Yasin Abbasi Yadkori, Anup Rao, Julian Zimmert, Tor Lattimore, and Csaba Szepesvari. Model selection in contextual stochastic bandit problems. Advances in Neural Information Processing Systems, 33:10328-10337, 2020.   \n[25]  Aldo Pacchiano, Christoph Dann, and Claudio Gentile. Best of both worlds model selection. Advances in Neural Information Processing Systems, 35:1883-1895, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "[26]  Vidyashankar Sivakumar, Steven Wu, and Arindam Banerjee. Structured linear contextual bandits: A sharp and geometric smoothed analysis. In International Conference on Machine Learning, pages 9026-9035. PMLR, 2020. ", "page_idx": 11}, {"type": "table", "img_path": "jIabKyXOTt/tmp/2fa2b1cff16bae64e8aaf9ef0995cba939f24be78e2f693ced784b2f1a03161c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "A Notation ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In Table 2 we list the most used notations. Next, we recall some definitions that are used throughout this appendix. We have $\\begin{array}{r}{V_{t}=I+\\sum_{s=1}^{t}A_{s}A_{s}^{\\top}}\\end{array}$ and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\widehat{\\theta}_{t}=\\operatorname*{argmin}_{\\theta\\in\\mathbb{R}^{d}}\\left(\\|\\theta\\|_{2}^{2}+\\sum_{s=1}^{t}\\left(\\widehat{X}_{s}-\\langle\\theta,A_{s}\\rangle\\right)^{2}\\right)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $A_{s}\\in A_{s}$ is the action chosen by the learner at round $t$ . Recall that $\\alpha_{i}=2^{i}\\log T$ . For $i\\in[n]$ ", "page_idx": 12}, {"type": "equation", "text": "$$\nA_{t}^{i}=\\operatorname*{argmax}_{a\\in\\mathcal{A}_{t}}\\operatorname*{max}_{\\theta\\in\\mathcal{C}_{t}^{i}}\\langle\\theta,a\\rangle=\\operatorname*{argmax}_{a\\in\\mathcal{A}_{t}}\\Big(\\langle a,\\widehat\\theta_{t-1}\\rangle+\\|a\\|_{V_{t-1}^{-1}}\\sqrt{\\alpha_{i}}\\Big)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{C}_{t}^{i}=\\left\\{\\theta\\in\\mathbb{R}^{d}:\\|\\theta-\\widehat{\\theta}_{t-1}\\|_{V_{t-1}}^{2}\\leq\\alpha_{i}\\right\\}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Finally, recall definition (2.2) with $\\delta=1/T$ \uff0c ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{C}_{t}=\\left\\{\\theta\\in\\mathbb{R}^{d}:\\|\\theta\\|_{2}^{2}+\\sum_{s=1}^{t-1}(\\widehat{X}_{s}-\\langle A_{s},\\theta\\rangle)^{2}\\leq\\gamma(1/T)\\right\\}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and recall that $\\textstyle{\\mathcal{E}}=\\bigcap_{t\\in[T]}\\left\\{\\theta_{*}\\in{\\mathcal{C}}_{t}\\right\\}$ ", "page_idx": 12}, {"type": "text", "text": "B  Analysis of SparseLinUCB ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Theorem 3.2. The expected regret of SparseLinUCB run with the number of models n in (3.1) and adistribution $\\pmb q=\\{q_{s}\\}_{s\\in[n]}$ satisfies ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]=O\\left((\\log T)\\sum_{s\\geq o}{\\sqrt{d^{2}^{s}T q_{s}}}+(\\log T){\\sqrt{S T d/Q}}\\right)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\begin{array}{r}{Q=\\sum_{s\\geq o}q_{s}}\\end{array}$ ", "page_idx": 12}, {"type": "text", "text": "Proof. Lemma 3.1 implies $\\mathcal{C}_{t}\\subset\\mathcal{C}_{t}^{o}$ . Hence, if $\\mathcal{E}$ is true and $s<o$ , then ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\theta_{*},A_{t}^{s}\\rangle\\geq\\langle\\widehat{\\theta}_{t-1},A_{t}^{s}\\rangle-\\sqrt{\\alpha_{o}}\\|A_{t}^{s}\\|_{V_{t-1}^{-1}}}\\\\ &{\\qquad\\qquad=\\Big(\\langle\\widehat{\\theta}_{t-1},A_{t}^{s}\\rangle+\\sqrt{\\alpha_{s}}\\|A_{t}^{s}\\|_{V_{t-1}^{-1}}\\Big)-\\big(\\sqrt{\\alpha_{o}}+\\sqrt{\\alpha_{s}}\\big)\\|A_{t}^{s}\\|_{V_{t-1}^{-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n(\\theta_{*}\\in\\mathcal{C}_{t}\\subset\\mathcal{C}_{t}^{o})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\geq\\Big(\\langle\\widehat{\\theta}_{t-1},A_{t}^{o}\\rangle+\\sqrt{\\alpha_{s}}\\|A_{t}^{o}\\|_{V_{t-1}^{-1}}\\Big)-\\big(\\sqrt{\\alpha_{o}}+\\sqrt{\\alpha_{s}}\\big)\\|A_{t}^{s}\\|_{V_{t-1}^{-1}}\\!\\!\\!\\!\\!\\!\\!(\\operatorname*{maximality~of}A_{t}^{s}\\operatorname*{in}{c_{t}^{s}})}&\\\\ &{=\\langle\\widehat{\\theta}_{t-1},A_{t}^{o}\\rangle+\\sqrt{\\alpha_{o}}\\|A_{t}^{o}\\|_{V_{t-1}^{-1}}-\\big(\\sqrt{\\alpha_{o}}-\\sqrt{\\alpha_{s}}\\big)\\|A_{t}^{o}\\|_{V_{t-1}^{-1}}-\\big(\\sqrt{\\alpha_{o}}+\\sqrt{\\alpha_{s}}\\big)\\|A_{t}^{s}\\|_{V_{t-1}^{-1}}}&\\\\ &{\\geq\\langle\\theta_{*},A_{t}^{o}\\rangle-\\big(\\sqrt{\\alpha_{o}}-\\sqrt{\\alpha_{s}}\\big)\\|A_{t}^{o}\\|_{V_{t-1}^{-1}}-\\big(\\sqrt{\\alpha_{o}}+\\sqrt{\\alpha_{s}}\\big)\\|A_{t}^{s}\\|_{V_{t-1}^{-1}}\\quad\\quad\\quad(\\theta_{*}\\in\\mathcal{C}_{t}\\subset\\mathcal{C}_{t}^{o})}&\\\\ &{\\geq\\langle\\theta_{*},A_{t}^{o}\\rangle-\\big(\\sqrt{\\alpha_{o}}-\\sqrt{\\alpha_{s}}\\big)\\|A_{t}^{o}\\|_{V_{t-1}^{-1}}-\\big(\\sqrt{\\alpha_{o}}+\\sqrt{\\alpha_{s}}\\big)\\|A_{t}^{o}\\|_{V_{t-1}^{-1}}\\quad\\quad\\quad(\\mathrm{by~Lemma~D.3)}}&\\\\ &{=\\langle\\theta_{*},A_{t}^{o}\\rangle-2\\sqrt{\\alpha_{o}}\\|A_{t}^{o}\\|_{V_{t-1}^{-1}}}&\\\\ &{\\geq\\langle\\theta_{*},A_{t}^{*}\\rangle-3\\sqrt{\\alpha_{o}}\\|A_{t}^{o}\\|_{V_{t-1}^{-1}}\\qquad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(\\mathrm{B.1})}&\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where in the first and the third inequalities, we use the fact that for any $A\\in\\mathbb{R}^{d}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\theta_{*}-\\widehat{\\theta}_{t-1},A\\rangle\\leq\\|\\theta_{*}-\\widehat{\\theta}_{t-1}\\|_{V_{t-1}}\\|A\\|_{V_{t-1}^{-1}},}\\\\ &{\\qquad\\qquad\\qquad\\leq\\sqrt{\\alpha_{o}}\\|A\\|_{V_{t-1}^{-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and for the last inequality, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\langle\\theta_{*},A_{t}^{*}\\rangle\\leq\\langle\\widehat{\\theta}_{t-1},A_{t}^{o}\\rangle+\\sqrt{\\alpha_{0}}\\|A_{t}^{o}\\|_{V_{t-1}^{-1}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We can decompose the regret as follows, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\bar{V}}_{T}=\\displaystyle\\sum_{t=1}^{T}\\mathbb{\\bar{\\Xi}}\\{\\boldsymbol{\\mathcal{E}}\\}\\langle\\boldsymbol{\\mathcal{E}}_{s},\\boldsymbol{A}_{t}^{*}-\\boldsymbol{A}_{t}^{L}\\rangle+\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{\\boldsymbol{\\mathcal{E}}^{c}\\}\\langle\\boldsymbol{\\mathcal{E}}_{s},\\boldsymbol{A}_{t}^{*}-\\boldsymbol{A}_{t}^{L}\\rangle}\\\\ &{\\quad=\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{\\boldsymbol{\\mathcal{E}}^{c}\\}\\langle\\boldsymbol{\\mathcal{E}}_{s},\\boldsymbol{A}_{t}^{*}-\\boldsymbol{A}_{t}^{L}\\rangle+\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{\\boldsymbol{\\mathcal{I}}_{t}\\geq\\boldsymbol{\\alpha},\\boldsymbol{\\mathcal{E}}\\}\\langle\\theta_{s},\\boldsymbol{A}_{t}^{*}-\\boldsymbol{A}_{t}^{L}\\rangle+\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{\\boldsymbol{\\mathcal{I}}_{t}<\\boldsymbol{\\alpha},\\boldsymbol{\\mathcal{E}}\\}\\langle\\theta_{s},\\boldsymbol{A}_{t}^{*}-\\boldsymbol{A}_{t}^{L}\\rangle}\\\\ &{\\quad\\leq\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{\\boldsymbol{\\mathcal{E}}^{c}\\}\\langle\\boldsymbol{\\mathcal{E}}_{s},\\boldsymbol{A}_{t}^{*}-\\boldsymbol{A}_{t}^{L}\\rangle+\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{\\boldsymbol{\\mathcal{I}}_{t}\\geq\\boldsymbol{\\alpha},\\boldsymbol{\\mathcal{E}}\\}\\langle\\theta_{s},\\boldsymbol{A}_{t}^{*}-\\boldsymbol{A}_{t}^{L}\\rangle}\\\\ &{\\quad\\quad\\quad+\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{\\boldsymbol{\\mathcal{I}}_{t}<\\boldsymbol{\\alpha},\\boldsymbol{\\mathcal{E}}\\}\\operatorname*{min}\\left\\{2,3,\\sqrt{\\alpha}\\mathrm{o}\\mathrm{i}\\boldsymbol{\\mathcal{E}}\\right\\}\\|_{\\mathcal{V}_{t}^{-1}}\\right\\}}\\\\ &{\\quad\\leq\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{\\boldsymbol{\\mathcal{E}}^{c}\\}\\langle\\boldsymbol{\\mathcal{E}}_{s},\\boldsymbol{A}_{t}^{*}-\\boldsymbol{A}_{t}^{L}\\rangle+\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{\\boldsymbol{\\mathcal{I}}_{t}\\geq\\boldsymbol{\\\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since $\\begin{array}{r}{\\mathbb{P}(\\mathcal{E}^{c})\\le\\delta\\le\\frac{1}{T}}\\end{array}$ , the first sum in the above line is easily bounded, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{1}\\{\\mathcal{E}^{c}\\}\\langle\\theta_{*},A_{t}^{*}-A_{t}^{I_{t}}\\rangle\\leq2T\\mathbb{P}(\\mathcal{E}^{c})\\leq2\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Bounding term $\\clubsuit$ . For each $s\\geq0$ , let ", "page_idx": 13}, {"type": "equation", "text": "$$\nT^{s}=\\biggl\\{t\\in[T]:\\operatorname*{det}(V_{t-1})\\in\\Bigl[2^{s d},2^{(s+1)d}\\Bigl)\\biggr\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that $\\operatorname*{det}(V_{t})$ is monotone increasing w.r.t $t$ . Define $s^{\\prime}=\\lceil\\log_{2}\\operatorname*{det}(V_{T})/d\\rceil$ . Then, ", "page_idx": 13}, {"type": "equation", "text": "$$\n[T]=\\bigcup_{s=1}^{s^{\\prime}}T^{s}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\clubsuit\\leq\\sum_{s=1}^{s^{\\prime}}\\sum_{t\\in\\mathcal{T}^{s}}\\operatorname*{min}\\left\\lbrace2,3\\sqrt{\\alpha_{o}}\\|A_{t}^{o}\\|_{V_{t-1}^{-1}}\\right\\rbrace.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By applying Lemma D.5, we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\{\\left|\\mathbf{a}\\right|=\\mathbb{E}\\left[\\displaystyle\\sum_{i=1}^{r}\\sum_{l=1}^{m}\\operatorname*{min}\\left\\{2,3\\sqrt{\\alpha_{0}}|A_{i}^{\\alpha}|\\nu_{i-1}^{-1}\\right\\}\\right]}\\\\ &{\\quad=3\\sqrt{\\alpha_{0}}\\mathbb{E}\\left[\\displaystyle\\sum_{s=1}^{r}\\sum_{i=1}^{r}\\sum_{l=1}^{m}\\operatorname*{min}\\left\\{1,\\|A_{i}^{\\alpha}\\|_{V_{i-1}^{-1}}\\right\\}\\right]}\\\\ &{\\quad\\le3\\sqrt{\\alpha_{0}}\\sqrt{T}\\cdot\\mathbb{E}\\left[\\displaystyle\\sum_{s=1}^{r}\\left[\\sum_{i=1}^{m}\\operatorname*{min}\\left\\{1,|A_{i}^{\\alpha}|\\nu_{i-1}^{-1}\\right\\}\\right]\\right]}\\\\ &{\\quad\\le3\\sqrt{\\alpha_{0}T}\\sqrt{\\displaystyle\\sum_{s=1}^{r}(2\\alpha/2+1/\\sigma)}}\\\\ &{\\quad\\le3\\sqrt{\\alpha_{0}T}\\sqrt{(2d+1)^{\\alpha/2}\\theta}}\\\\ &{\\quad=3\\sqrt{\\alpha_{0}T}\\sqrt{(2d+1)/\\sigma}[\\log_{2}\\mathrm{det}(V_{T})/d]}\\\\ &{\\quad=O\\big(\\log_{2}T\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "(Cauchy-Schwarz inequality) ", "page_idx": 14}, {"type": "text", "text": "(due to Lemma D.5) ", "page_idx": 14}, {"type": "text", "text": "Bounding term $\\spadesuit$ . Let $\\begin{array}{r}{T_{s}=\\sum_{t=1}^{T}\\mathbb{1}\\{I_{t}=s\\}.}\\end{array}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}\\{\\mathbf{s}\\}^{\\prime}=\\mathbf{I\\}\\left[\\sum_{i=1}^{N}\\left(I_{i}\\geq\\alpha_{i}\\xi\\right)(\\mu_{i},A_{i}^{\\prime}-A_{i}^{\\prime})\\right]}\\qquad}&{}\\\\ &{\\leq\\mathbf{I}\\left[\\sum_{i=1}^{N}\\left(I_{i}\\geq\\alpha_{i}\\xi\\right)\\cdot\\operatorname{max}\\left\\{\\sum_{j=1}^{N}\\rho_{j}(\\zeta_{j},\\zeta_{j},L_{i}^{\\prime})\\right\\}\\right]}\\\\ &{\\leq2\\sum_{i=1}^{N}\\left[\\sqrt{\\alpha_{i}\\sqrt{\\sum_{i=1}^{N}\\left(I_{i}-\\alpha_{i}\\right)\\operatorname{max}\\left\\{\\sum_{k=1}^{N}L_{i}^{\\prime}\\right\\}}}\\right]}\\\\ &{\\leq2\\sum_{j=1}^{N}\\left[\\sqrt{\\alpha_{i}\\sqrt{\\sum_{i=1}^{N}\\left(I_{i}\\right)\\left(\\sum_{k=1}^{N}\\left\\{1,\\lambda_{j}\\zeta_{j}^{2}-1\\right\\}\\right)}}\\right]}\\\\ &{\\leq2\\sum_{i=1}^{N}\\left[\\sqrt{\\alpha_{i}\\sqrt{\\sum_{i=1}^{N}\\left(\\sum_{j=1}^{N}\\phi_{i}\\xi V_{i}\\right)}}\\right]}\\\\ &{\\leq2\\sum_{j=1}^{N}\\xi\\left[\\sqrt{\\alpha_{i}\\sqrt{\\sum_{i=1}^{N}\\left(\\sum_{j=1}^{N}\\phi_{i}\\phi_{j}\\right)}}\\right]}\\\\ &{\\leq2\\sum_{j=1}^{N}\\xi\\left[\\phi_{i},\\overline{{\\alpha_{i}\\sqrt{\\sum_{j=1}^{N}\\left(\\sum_{i=1}^{N}\\phi_{i}\\right)}}}\\right]}\\\\ &{\\leq2\\sum_{j=1}^{N}\\xi\\left[\\phi_{i},\\overline{{\\alpha_{j}\\sqrt{\\sum_{i=1}^{N}\\left(\\sum_{j=1}^{N}\\phi_{i}\\right)}}}\\right]}\\\\ &{=\\alpha\\left(\\sum_{j=1}^{N}\\phi_{i},\\overline{{\\alpha_{i}\\sqrt{\\sum_{j=1}^{N}\\left(\\sum_{i=1}^{N}\\phi_{i}\\right)}}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Substituting the bounds of $\\spadesuit$ and $\\clubsuit$ to (B.2), we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]\\leq O\\bigg(\\sum_{s\\geq o}\\sqrt{d\\alpha_{s}T\\log T q_{s}}+\\log T\\sqrt{S d T/Q}\\bigg)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "concluding the proof. ", "page_idx": 14}, {"type": "text", "text": "Theorem 3.4. The expected regret of SparseLinUCB run with the number of models n in (3.1), $a$ distribution ${\\pmb q}=\\{{\\boldsymbol q}_{s}\\}_{s\\in[n]}$ and using SeqSEW as base algorithm satisfies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]=O\\left(\\frac{(d S/Q)+d^{2}}{\\Delta}(\\log T)^{2}\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{Q=\\sum_{s\\geq o}q_{s}}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Proof. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{R_{T}=\\displaystyle\\sum_{i=1}^{T}\\displaystyle1\\{\\mathcal{E}^{*}\\}(\\theta_{i},A_{i}^{*}-A_{i}^{t})+\\displaystyle\\sum_{t=1}^{T}\\displaystyle1\\{\\mathcal{E}^{*}\\}(\\theta_{i},A_{t}^{*}-A_{i}^{t})}}\\\\ {{\\displaystyle\\leq\\displaystyle\\sum_{i=1}^{T}\\displaystyle1\\{\\mathcal{E}^{*}\\}(\\theta_{i},A_{i}^{*}-A_{i}^{t})+\\displaystyle\\sum_{t=1}^{T}\\displaystyle1\\{I_{i}\\geq\\sigma,\\xi\\}\\displaystyle\\frac{(\\theta_{i},A_{i}^{*}-A_{i}^{t})^{2}}{\\Delta}}}\\\\ {{\\displaystyle+\\sum_{i=1}^{T}\\displaystyle1\\{I_{i}<\\sigma,\\xi\\}\\displaystyle\\frac{(\\theta_{i},A_{i}^{*}-A_{i}^{t})^{2}}{\\Delta}}}\\\\ {{\\displaystyle\\leq\\displaystyle\\sum_{i=1}^{T}1\\{\\mathcal{E}^{*}\\}\\langle\\theta_{i},A_{i}^{*}-A_{i}^{t}\\rangle+\\displaystyle\\sum_{t=1}^{T}\\displaystyle1\\{I_{i}\\geq\\sigma,\\xi\\}\\displaystyle\\frac{(\\theta_{i},A_{i}^{*}-A_{i}^{t})^{2}}{\\Delta}}}\\\\ {{\\displaystyle+\\sum_{s\\geq1}^{T}\\displaystyle1\\{I_{i}<\\sigma,\\xi\\}\\displaystyle\\frac{\\operatorname*{min}\\left\\{1,\\alpha_{\\infty}\\left\\}[A_{i}\\right]_{i}^{*}\\right\\}_{i=1}^{2}.}}\\end{array}\\quad\\mathrm{(b y~a p p i v i n g~(B)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "1) to $\\clubsuit$ ", "page_idx": 15}, {"type": "text", "text": "Bounding term . Let $s^{\\prime}=\\lceil\\log_{2}\\operatorname*{det}(V_{T})/d\\rceil$ . By applying Lemma D.5, we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\pmb{\\mathscr{B}}]\\leq\\frac{9}{\\Delta}\\mathbb{E}\\left[\\displaystyle\\sum_{s\\geq0}\\sum_{t\\leq T^{\\prime}}\\operatorname*{min}\\left\\{1,\\alpha_{\\sigma}\\|A_{t}^{\\sigma}\\|_{V_{t-1}^{-1}}^{2}\\right\\}\\right]}\\\\ &{\\quad\\leq\\frac{9\\alpha_{\\sigma_{\\sigma}}s^{\\prime}}{\\Delta}\\mathbb{E}\\left[\\displaystyle\\sum_{t\\in T^{\\prime}}\\operatorname*{min}\\left\\{1,\\|A_{t}^{\\sigma}\\|_{V_{t-1}^{-1}}^{2}\\right\\}\\right]}\\\\ &{\\quad\\leq\\frac{9\\alpha_{\\sigma}s^{\\prime}}{\\Delta}(2d/Q+1/Q)}\\\\ &{\\quad\\leq\\frac{27\\alpha_{\\sigma}d\\delta^{\\prime}/Q}{\\Delta}}\\\\ &{\\quad=O\\bigg(\\frac{d S(\\log T)^{2}/Q}{\\Delta}\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Bounding term $\\spadesuit$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\bullet]=\\mathbb{E}\\left[\\underset{t=1}{\\overset{T}{\\sum}}\\mathbb{1}\\{I_{t}\\geq o,{\\mathcal{E}}\\}\\frac{\\langle\\theta_{*},A_{t}^{*}-A_{t}^{t}\\rangle^{2}}{\\Delta}\\right]}\\\\ &{\\quad\\quad\\leq\\frac{4}{\\Delta}\\mathbb{E}\\left[\\underset{t=1}{\\overset{T}{\\sum}}\\underset{0}{\\leq}\\{I_{t}\\geq o,{\\mathcal{E}}\\}\\cdot\\operatorname*{min}\\left\\lbrace1,a_{t}\\rfloor\\underset{t=1}{\\overset{A}{\\prod}}\\psi_{t-1}^{t}\\right\\rbrace\\right]}\\\\ &{\\quad\\quad\\leq4\\underset{s\\geq0}{\\sum}\\frac{\\alpha_{s}}{\\Delta}\\mathbb{E}\\left[\\underset{t\\in[T]}{\\sum}\\operatorname*{min}\\left\\lbrace1,\\|A_{t}\\|_{V_{t-1}^{-1}}^{2}\\right\\rbrace\\right]}\\\\ &{\\quad\\quad\\leq8\\underset{s\\geq0}{\\sum}\\frac{\\alpha_{s}}{\\Delta}\\cdot\\log\\operatorname*{det}V_{T}}\\\\ &{\\quad\\quad=O\\bigg(\\frac{d^{2}(\\log T)^{2}}{\\Delta}\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(due to Lemma D.1) ", "page_idx": 15}, {"type": "text", "text": "where the first inequality comes from ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\langle\\theta_{*},A_{t}^{*}-A_{t}^{I_{t}}\\rangle\\leq\\operatorname*{max}_{\\theta\\in{\\mathcal{C}}_{t-1}^{I_{t}}}\\langle\\theta-\\theta_{*},A_{t}^{I_{t}}\\rangle\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{\\theta\\in\\mathcal{C}_{t-1}^{I_{t}}}\\|\\theta-\\theta_{*}\\|_{V_{t-1}}\\|A_{t}^{I_{t}}\\|_{V_{t-1}^{-1}}}}\\\\ &{\\leq2\\sqrt{\\alpha_{I_{t}}}\\|A_{t}^{I_{t}}\\|_{V_{t-1}^{-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "(Cauchy-Schwarz Inequality) ", "page_idx": 16}, {"type": "text", "text": "where the last inequality holds because $\\mathcal{E}$ and $I_{t}\\geq o$ both hold, and so. $\\theta_{*}\\in\\mathcal{C}_{t-1}\\subset\\mathcal{C}_{t-1}^{I_{t}}$ due to Lemma 3.1. The factor 2 is due to an application of the triangular inequality. Sustituting the bounds of $\\spadesuit$ and $\\clubsuit$ in (B.2), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]=O\\bigg(\\operatorname*{max}\\{S/Q,d\\}\\cdot\\frac{d(\\log T)^{2}}{\\Delta}\\bigg).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Corollary B.1. Pick any $C\\geq1$ and let $\\{q_{s}\\}_{s\\in[n]}$ be chosen as in (3.4). Then the expected regret of SparseLinUCB is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]=\\widetilde{O}\\left(\\operatorname*{min}\\left\\{\\operatorname*{max}\\big\\{C,S/C\\big\\}\\sqrt{d T},\\,\\frac{\\operatorname*{max}\\big\\{d^{2},S^{2}d/C^{2}\\big\\}}{\\Delta}\\right\\}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. We consider the following cases based on the value of $C$ ", "page_idx": 16}, {"type": "text", "text": "Case 1: $C^{2}<2^{o}$ $q_{o}=C^{2}\\cdot2^{-o}$ . According to Theorem 3.2, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb E[R_{T}]=O\\left((\\log T)\\sum_{s\\geq o}\\sqrt{d2^{s}T q_{s}}+(\\log T)\\sqrt{S T d/Q}\\right)}}\\\\ &{}&{\\leq O\\left((\\log T)n\\sqrt{d2^{o}T q_{o}}+(\\log T)\\sqrt{S T d/q_{o}}\\right)}\\\\ &{}&{=\\widetilde O\\left(\\operatorname*{max}\\{C,S/C\\}\\sqrt{d T}\\right).\\qquad\\mathrm{~(due~to~}n=O(\\log d)\\mathrm{~and~}2^{o}=\\Theta(S\\log T))}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Besides, according to Theorem 3.4, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[R_{T}]=O\\bigg(\\operatorname*{max}\\{S/Q,d\\}\\cdot\\frac{d\\log^{2}T}{\\Delta}\\bigg)}\\\\ &{\\qquad=O\\bigg(\\operatorname*{max}\\{S/q_{o},d\\}\\cdot\\frac{d\\log^{2}T}{\\Delta}\\bigg)}\\\\ &{\\qquad=\\widetilde O\\bigg(\\frac{\\operatorname*{max}\\{d^{2},S^{2}d/C^{2}\\}}{\\Delta}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]=\\widetilde{O}\\left(\\operatorname*{min}\\left\\{\\operatorname*{max}\\left\\{C,S/C\\right\\}\\sqrt{d T},\\,\\frac{\\operatorname*{max}\\left\\{d^{2},S^{2}d/C^{2}\\right\\}}{\\Delta}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Case 2: $C^{2}\\in[2^{o},2^{n})$ ", "page_idx": 16}, {"type": "text", "text": "For $s$ with $C^{2}<2^{s}$ \uff0c $q_{s}=C^{2}2^{-s}$ . Let $o^{\\prime}=\\arg\\operatorname*{min}_{s>o}\\{C^{2}<2^{s}\\}$ Then, $q_{o^{\\prime}}\\ge1/4$ . According to Theorem 3.2, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[R_{T}]=O\\left((\\log T)\\displaystyle\\sum_{s\\geq o}\\sqrt{d2^{s}T q_{s}}+(\\log T)\\sqrt{S T d/Q}\\right)}\\\\ &{\\quad\\quad\\leq O\\left((\\log T)\\displaystyle\\sum_{s\\in[o,o^{\\prime})}\\sqrt{d2^{s}T}+(\\log T)\\displaystyle\\sum_{s\\in[o^{\\prime},n]}\\sqrt{d2^{s}T q_{s}}+(\\log T)\\sqrt{S T d/q_{o^{\\prime}}}\\right)}\\\\ &{\\quad\\quad\\leq\\widetilde O\\left(\\sqrt{d2^{o^{\\prime}}T}+n C\\sqrt{d T}+\\sqrt{S T d}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n=\\widetilde{O}\\bigg(\\operatorname*{max}\\{C,S/C\\}\\sqrt{d T}\\bigg).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Besides, according to Theorem 3.4, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[R_{T}]=O\\bigg(\\operatorname*{max}\\{S/Q,d\\}\\cdot\\frac{d(\\log T)^{2}}{\\Delta}\\bigg)}\\\\ &{\\qquad\\quad=O\\bigg(\\operatorname*{max}\\{S/q_{\\sigma^{\\prime}},d\\}\\cdot\\frac{d(\\log T)^{2}}{\\Delta}\\bigg)}\\\\ &{\\qquad\\quad=\\widetilde O\\bigg(\\frac{\\operatorname*{max}\\{d^{2},S^{2}d/C^{2}\\}}{\\Delta}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]=\\widetilde{O}\\left(\\operatorname*{min}\\left\\{\\operatorname*{max}\\left\\{C,S/C\\right\\}\\sqrt{d T},\\,\\frac{\\operatorname*{max}\\left\\{d^{2},S^{2}d/C^{2}\\right\\}}{\\Delta}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Case $C^{2}~\\geq~2^{n}$ $q_{s}\\,=\\,1/n$ for all $s~\\in~[n]$ . It is easy to verify that $\\mathbb{E}[R_{T}]\\;=\\;\\tilde{O}(d\\sqrt{T})$ and $\\mathbb{E}[R_{T}]=\\widetilde{O}(d^{2}/\\Delta)$ . Thus, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]=\\widetilde{O}\\left(\\operatorname*{min}\\left\\{\\operatorname*{max}\\left\\{C,S/C\\right\\}\\sqrt{d T},\\,\\frac{\\operatorname*{max}\\left\\{d^{2},S^{2}d/C^{2}\\right\\}}{\\Delta}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Corollary B.2. Assume that the sparsiy level $S$ is known and choose a distribution ${\\pmb q}=\\{{q}_{s}\\}_{s\\in[n]}$ With $q_{o}=1$ where $o$ is set as in (3.2). Then, the expected regret of SparseLinUCB is $\\mathbb{E}[R_{T}]=$ $\\widetilde{O}\\big(\\frac{S d}{\\Delta}\\big)$ ", "page_idx": 17}, {"type": "text", "text": "Proof. We follow the same steps as in the proof of Theorem 3.4. The only difference lies in the bounding term $\\spadesuit$ in (B.3). We have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\bullet]=\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{I_{t}\\geq\\sigma,\\mathcal{E}\\}\\frac{\\langle\\theta_{\\bullet},A_{t}^{*}-A_{t}^{I_{t}}\\rangle^{2}}{\\Delta}\\right]}\\\\ &{\\quad=\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{\\mathcal{E}\\}\\frac{\\langle\\theta_{\\bullet},A_{t}^{*}-A_{t}\\rangle^{2}}{\\Delta}\\right]}\\\\ &{\\quad\\leq\\displaystyle\\frac{4\\alpha_{o}}{\\Delta}\\mathbb{E}\\left[\\displaystyle\\sum_{t\\in\\{T\\}}\\operatorname*{min}\\left\\{1,\\|A_{t}\\|_{V_{t-1}^{-1}}^{2}\\right\\}\\right]}\\\\ &{\\quad=O\\bigg(\\frac{S d\\langle\\log T\\rangle^{2}}{\\Delta}\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second equality is because $q_{o}=1$ and so $A_{t}^{I_{t}}=A_{t}^{o}=A_{t}$ , and the first inequality comes from ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\theta_{*},A_{t}^{*}-A_{t}\\rangle\\leq\\underset{\\theta\\in\\mathcal{C}_{t-1}^{o}}{\\operatorname*{max}}\\langle\\theta-\\theta_{*},A_{t}\\rangle}\\\\ &{\\qquad\\qquad\\leq\\underset{\\theta\\in\\mathcal{C}_{t-1}^{o}}{\\operatorname*{max}}\\ \\Vert\\theta-\\theta_{*}\\Vert_{V_{t-1}}\\Vert A_{t}^{o}\\Vert_{V_{t-1}^{-1}}}\\\\ &{\\qquad\\qquad\\leq2\\sqrt{\\alpha_{o}}\\Vert A_{t}\\Vert_{V_{t-1}^{-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "(Cauchy-Schwarz Inequality) ", "page_idx": 17}, {"type": "text", "text": "where the last inequality holds because $\\mathcal{E}$ and $I_{t}=o$ both hold, and so $\\theta_{*}\\in\\mathcal{C}_{t-1}\\subset\\mathcal{C}_{t-1}^{o}$ due to Lemma 3.1. Substituting the bounds of $\\spadesuit$ and $\\clubsuit$ in (B.2), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]=O\\bigg(\\frac{S d(\\log T)^{2}}{\\Delta}\\bigg).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C Analysis of AdaLinUCB ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Theorem 4.1. If the random independent noise $\\varepsilon_{t}$ in (2.1) satisfies $\\varepsilon_{t}\\in[-1,1]$ for all $t\\in[T]$ then the regret of AdaLinUCB run with $\\eta=\\sqrt{(\\log n)/(T n)}$ for $n$ in (3.1) and $q\\in(0,1]$ satisfies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[R_{T}]\\leq\\left(\\sqrt{8\\alpha_{n}q}+4\\sqrt{(2\\alpha_{o})/q}\\right)\\sqrt{d T\\log\\left(1+\\frac{T L^{2}}{d}\\right)+1}+O\\big(\\sqrt{n T\\log n}\\big)}\\\\ &{\\qquad=\\widetilde{O}\\left(\\operatorname*{max}\\left\\{\\sqrt{d q},\\sqrt{S/q}\\right\\}\\sqrt{d T}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Let $\\mathcal{T}_{1}$ be the set of time steps where $Z_{t}=1$ in Line 4 of AdaLinUCB and let $\\mathcal{T}_{2}=[T]\\setminus\\mathcal{T}_{1}$ For all $t\\in\\mathcal T_{2}$ the action $A_{t}\\in\\mathcal A_{t}$ is chosen by Exp3 through an adversarial mapping $\\mu_{t}:[n]\\to A_{t}$ defined in Lines 9-10. The resulting reward $\\bar{X_{t}^{\\bar{\\ }}}\\in\\bar{[-2,\\bar{2}]}$ is fed to Exp3 as a $[0,1]$ valued loss $\\ell_{t}(I_{t})=(2-X_{t})/4$ , where $\\mu_{t}(I_{t})=\\bar{A}_{t}$ . Since $\\mathcal{T}_{2}$ is selected through independent coin tosses with fixed bias $q$ , we can apply the Exp3 regret analysis (for losses chosen by a non-oblivious adversary) to bound the regret in $\\mathcal{T}_{2}$ and show that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t\\in{\\mathcal{T}}_{2}}\\left\\langle A_{t}^{o}-A_{t},\\theta_{*}\\right\\rangle=4\\sum_{t\\in{\\mathcal{T}}_{2}}\\left(\\ell_{t}(I_{t})-\\ell_{t}(o)\\right)=O{\\big(}{\\sqrt{n\\log n}}{\\big)}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we used $\\mu_{t}(o)=A_{t}^{o}$ for all $t\\in[T]$ ", "page_idx": 18}, {"type": "text", "text": "We have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{7_{2}}=\\displaystyle\\sum_{t\\in\\mathcal{T}_{2}}\\underset{a\\in\\mathcal{A}_{t}}{\\mathrm{argmax}}\\left\\langle a-A_{t},\\theta_{*}\\right\\rangle}\\\\ &{\\quad=\\displaystyle\\sum_{t\\in\\mathcal{T}_{2}}\\underset{a\\in\\mathcal{A}_{t}}{\\mathrm{argmax}}\\left\\langle a-A_{t}^{o}+A_{t}^{o}-A_{t},\\theta_{*}\\right\\rangle}\\\\ &{\\quad=\\displaystyle\\sum_{t\\in\\mathcal{T}_{2}}\\underset{a\\in\\mathcal{A}_{t}}{\\mathrm{argmax}}\\left\\langle a-A_{t}^{o},\\theta_{*}\\right\\rangle+\\underbrace{\\sum_{t\\in\\mathcal{T}_{2}}\\left\\langle A_{t}^{o}-A_{t},\\theta_{*}\\right\\rangle}_{\\mathrm{Reg_{Exp3}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$\\mathrm{Reg_{Exp3}}$ is bounded in (C.1), hence we only need to bound ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{Reg}_{\\mathrm{Img}}=\\sum_{t\\in\\mathcal{T}_{2}}\\underset{a\\in\\mathcal{A}_{t}}{\\mathrm{argmax}}\\left\\langle a-A_{t}^{o},\\theta_{*}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widetilde{\\theta}_{t}^{i}=\\underset{\\theta\\in\\mathcal{C}_{t}^{i}}{\\mathrm{argmax}}\\,\\underset{a\\in\\mathcal{A}_{t}}{\\mathrm{max}}\\langle a,\\theta\\rangle\\quad\\mathrm{and}\\quad A_{t}^{*}=\\underset{a\\in\\mathcal{A}_{t}}{\\mathrm{argmax}}\\langle a,\\theta_{*}\\rangle.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Recall $\\mathcal{E}$ is the event that $\\theta_{*}\\in\\mathcal{C}_{t}$ for all $t\\in[T]$ . Assume $\\mathcal{E}$ holds. We obtain that for $t\\in[T]$ ", "page_idx": 18}, {"type": "text", "text": "(Cauchy-Schwarz inequality) ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\theta_{*},A_{t}^{*}-A_{t}^{o}\\rangle\\leq\\langle\\widetilde{\\theta}_{t}^{o}-\\theta_{*},A_{t}^{o}\\rangle}\\\\ &{\\qquad\\qquad\\leq\\lVert\\widetilde{\\theta}_{t}^{o}-\\theta_{*}\\rVert_{V_{t-1}}\\lVert A_{t}^{o}\\rVert_{V_{t-1}^{-1}}}\\\\ &{\\qquad\\qquad\\leq\\sqrt{\\alpha_{o}}\\lVert A_{t}^{o}\\rVert_{V_{t-1}^{-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last inequality is because $\\widetilde{\\theta}_{t}^{o}\\in\\mathcal{C}_{t}^{o}$ . Then, assuming $\\mathcal{E}$ holds, we can bound $\\mathrm{Reg_{Img}}$ as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Reg}_{\\mathrm{Img}}=\\displaystyle\\sum_{t\\in\\mathcal{T}_{2}}\\langle\\theta_{*},A_{t}^{*}-A_{t}^{o}\\rangle}\\\\ &{\\qquad\\quad\\le\\displaystyle\\sum_{t\\in\\mathcal{T}_{2}}\\operatorname*{min}\\Big\\{2,\\sqrt{\\alpha_{o}}\\big\\|A_{t}^{o}\\big\\|_{V_{t-1}^{-1}}\\Big\\}}\\\\ &{\\qquad\\le\\displaystyle\\sum_{t\\in\\mathcal{T}_{2}}\\operatorname*{min}\\Big\\{2,\\sqrt{2\\gamma(1/T)}\\big\\|A_{t}^{o}\\big\\|_{V_{t-1}^{-1}}\\Big\\}}\\\\ &{\\qquad\\le2\\sqrt{\\gamma(1/T)}\\displaystyle\\sum_{t\\in\\mathcal{T}_{2}}\\operatorname*{min}\\Big\\{1,\\|A_{t}^{o}\\|_{V_{t-1}^{-1}}\\Big\\}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where in the frst inequality, we use the facts $\\langle\\theta_{*},A_{t}^{*}-A_{t}^{o}\\rangle\\leq\\sqrt{\\alpha_{o}}\\|A_{t}^{o}\\|_{V_{t-1}^{-1}}$ and $\\left\\langle\\theta_{*},A_{t}^{*}-A_{t}^{o}\\right\\rangle\\leq$ $\\langle\\theta_{*},A_{t}^{*}\\rangle\\leq2$ ", "page_idx": 19}, {"type": "text", "text": "From Lemma D.3, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{t\\in\\mathcal{T}_{2}}\\operatorname*{min}\\left\\{1,\\|A_{t}^{o}\\|_{V_{t-1}^{-1}}^{2}\\right\\}\\leq\\sum_{t\\in\\mathcal{T}_{2}}\\operatorname*{min}\\left\\{1,\\|A_{t}^{n}\\|_{V_{t-1}^{-1}}^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For each $s\\geq0$ ,let ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\mathcal{T}}_{2}^{s}=\\left\\{t\\in{\\mathcal{T}}_{2}:\\operatorname*{det}(V_{t-1})\\in\\left[2^{d s},2^{d(s+1)}\\right)\\right\\}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "so that ", "page_idx": 19}, {"type": "equation", "text": "$$\nT_{2}=\\bigcup_{s\\geq0}T_{2}^{s}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $s^{\\prime}=\\lceil\\log_{2}\\operatorname*{det}(V_{T})/d\\rceil$ . We have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\{\\left|\\mathcal{R}_{{\\alpha}_{1}{\\alpha}_{4}}:\\mathbb{I}\\right|\\leq\\mathbb{E}\\left[\\mathcal{Z}\\sqrt{\\pi(1/\\sum_{i=1}^{m}\\zeta_{i})}\\Bigg\\|\\sum_{i=1}^{m}\\mathrm{tr}\\Bigg\\{1,1.1_{A}^{\\mathbb{H}}\\Vert\\Gamma_{{\\alpha}_{1};\\zeta_{1}}^{-1}\\Bigg\\}\\Bigg]}\\\\ {\\leq2\\sqrt{2\\alpha_{2}}\\mathrm{tr}\\Bigg[\\sum_{i=1}^{m}\\mathrm{tr}\\Bigg\\{1,1.4^{\\alpha}\\Gamma_{{\\alpha}_{1};\\zeta_{1}}^{-1}\\Bigg\\}\\Bigg]}\\\\ {\\leq2\\sqrt{2\\alpha_{2}}\\mathrm{tr}\\Bigg[\\frac{\\sqrt{\\pi}}{\\sqrt{\\alpha_{2}}}\\Bigg\\{1,1.2^{\\alpha}\\Gamma_{{\\alpha}_{2};\\alpha}^{-1}\\Bigg\\}\\Bigg]}\\\\ {\\leq2\\sqrt{2\\alpha_{2}}\\mathrm{tr}\\Bigg[\\frac{\\sqrt{\\alpha_{2}}}{\\sqrt{\\alpha_{2}}}\\Bigg\\{\\sum_{i=1}^{m}\\sum_{i=1}^{m}\\mathrm{tr}\\Bigg\\{1,1.4^{\\alpha}\\}\\Bigg\\}\\Bigg]}\\\\ {\\leq2\\sqrt{2\\alpha_{2}}\\mathrm{tr}\\Bigg[\\frac{\\sqrt{\\alpha_{2}}}{\\sqrt{\\alpha_{2}}}\\Bigg\\{\\sum_{i=1}^{m}\\sum_{i=1}^{m}\\mathrm{tr}\\Bigg\\{\\sum_{i=1}^{m}\\mathrm{tr}\\Bigg\\}\\Bigg]}\\\\ {\\leq2\\sqrt{2\\alpha_{2}}\\mathrm{tr}\\Bigg[\\frac{\\sqrt{\\alpha_{2}}}{\\sqrt{\\alpha_{2}}}\\Bigg\\{\\sum_{i=1}^{m}\\mathrm{tr}\\Bigg\\{\\alpha_{1}^{\\alpha}\\Bigg\\}\\Bigg\\}\\Bigg]}\\\\ {\\leq2\\sqrt{2\\alpha_{2}}\\mathrm{tr}\\Bigg[\\frac{\\sqrt{\\alpha_{2}}}{\\sqrt{\\alpha_{2}}}\\Bigg\\{\\sum_{i=1}^{m}\\mathrm{tr}\\Bigg\\}\\Bigg]}\\\\ {\\leq2\\sqrt{2\\alpha_{2}}\\mathrm{tr}\\Bigg[1+m\\mathrm{\\tilde{e}}\\Bigg\\{\\sqrt{\\alpha_{2}}/\\sqrt{\\alpha_{2}}\\mathrm{t}\\Bigg\\}\\Bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To obtain the final results, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{det}(V_{T})=\\prod_{i=1}^{d}\\lambda_{i}\\leq\\bigg(\\frac{1}{d}\\mathrm{trace}(V_{T})\\bigg)^{d}\\leq\\bigg(1+\\frac{T L^{2}}{d}\\bigg)^{d},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\lambda_{1},\\cdot\\cdot\\cdot\\,,\\lambda_{d}$ are the eigenvalues of $V_{T}$ . Therefore, we have $s^{\\prime}\\leq\\lceil\\log_{2}(1+T L^{2}/d)\\rceil$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\big[\\mathrm{Reg}_{\\mathrm{Img}}\\cdot\\mathbb{1}[\\mathcal{E}]\\big]\\leq2\\sqrt{2(2d+1)\\alpha_{o}T/q}\\cdot\\sqrt{s^{\\prime}}}\\\\ {\\leq2\\sqrt{(6d\\alpha_{o}T/q)\\lceil\\log_{2}(1+T L^{2}/d)\\rceil}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By substituting the bounds on $\\mathrm{Reg_{Img}}$ and $\\mathrm{Reg_{Exp3}}$ into $R_{\\ensuremath{\\ T}_{2}}$ , we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[R_{\\mathcal{T}_{2}}]=\\mathbb{E}[\\mathrm{Reg}_{\\mathrm{Exp3}}\\mathbb{1}\\{\\mathcal{E}\\}]+\\mathbb{E}[\\mathrm{Reg}_{\\mathrm{Img}}]+T\\cdot\\mathbb{P}(\\mathcal{E}^{c})}\\\\ &{\\qquad\\qquad\\leq2\\sqrt{6d\\alpha_{o}T/q}\\sqrt{\\lceil\\log(1+T L^{2}/d)\\rceil}+O(\\sqrt{n T\\log n}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last inequality is because $\\mathbb{P}(\\mathcal{E}^{c})\\leq1/T$ from Lemma 2.1 with our choice of $\\delta=1/\\delta$ ", "page_idx": 19}, {"type": "text", "text": "Finally, we bound $R_{\\ensuremath{\\mathcal{T}}_{1}}$ . Conditioned on event $\\mathcal{E}$ and using (D.3), $\\forall t\\in[T],\\theta_{*}\\in\\mathcal{C}_{t}\\subseteq\\mathcal{C}_{t}^{0}\\subseteq\\mathcal{C}_{t}^{n}$ Hence, we can obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\theta_{*},A_{t}^{*}-A_{t}^{n}\\rangle\\leq\\langle\\widetilde{\\theta}_{t}^{n}-\\theta_{*},A_{t}^{n}\\rangle}\\\\ &{\\qquad\\qquad\\leq\\lVert\\widetilde{\\theta}_{t}^{n}-\\theta_{*}\\rVert_{V_{t-1}}\\lVert A_{t}^{n}\\rVert_{V_{t-1}^{-1}}}\\\\ &{\\qquad\\qquad\\leq\\sqrt{\\alpha_{n}}\\lVert A_{t}^{n}\\rVert_{V_{t-1}^{-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, conditioned on event $\\mathcal{E}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{T_{1}}\\leq\\displaystyle\\sum_{t\\in T_{1}}\\operatorname*{min}\\left\\{2,\\left(\\theta_{s},A_{t}^{*}-A_{t}^{u}\\right)\\right\\}}\\\\ &{\\quad\\leq\\displaystyle\\sum_{t\\in T_{1}}\\operatorname*{min}\\left\\{2,\\sqrt{\\alpha_{n}}\\left\\|A_{t}^{*}\\right\\|_{V_{t-1}^{-\\frac{1}{\\lambda}}}\\right\\}}\\\\ &{\\quad\\leq2\\sqrt{\\alpha_{n}}\\|T_{1}\\sqrt{\\displaystyle\\sum_{t\\in T_{1}}\\operatorname*{min}\\left\\{1,\\|A_{t}^{*}\\|_{V_{t-1}^{-\\frac{1}{\\lambda}}}\\right\\}}\\qquad\\qquad(\\mathrm{Cauchy-Schwa~}}\\\\ &{\\quad\\leq2\\sqrt{\\alpha_{n}}\\|T_{1}\\sqrt{\\displaystyle\\sum_{t\\in T_{1}}\\operatorname*{min}\\left\\{1,\\|A_{t}^{*}|_{V_{t-1}^{-\\frac{1}{\\lambda}}}\\right\\}}+\\displaystyle\\sum_{t\\in T_{2}}\\operatorname*{min}\\left\\{1,\\|A_{t}|_{V_{t-1}^{-\\frac{1}{\\lambda}}}\\right\\}}\\\\ &{\\quad=2\\sqrt{\\alpha_{n}|T_{1}|}\\sqrt{\\displaystyle\\sum_{t\\in T_{1}}\\operatorname*{min}\\left\\{1,\\|A_{t}|_{V_{t-1}^{-\\frac{1}{\\lambda}}}\\right\\}}}\\\\ &{\\quad\\leq2\\sqrt{\\alpha_{n}|T_{1}|}\\cdot\\sqrt{2\\log(\\mathrm{det}(V_{T}))},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last inequality is due to Lemma D.1. Therefore, the expected regret for $t\\in\\mathcal T_{1}$ is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[R_{T_{1}}]\\leq\\mathbb{E}[R_{T_{1}}\\{\\xi\\}]+T\\cdot\\mathbb{P}(\\xi^{c})}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}\\left[2\\sqrt{\\alpha_{n}|T_{1}|}\\cdot\\sqrt{2\\log(\\operatorname*{det}(V_{T}))}\\right]+1}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}\\left[2\\sqrt{\\alpha_{n}|T_{1}|}\\cdot\\sqrt{2d\\log\\left(1+\\frac{T L^{2}}{d}\\right)}\\right]+1}\\\\ &{\\qquad=2\\sqrt{\\alpha_{n}}\\mathbb{E}\\left[\\sqrt{|T_{1}|}\\right]\\cdot\\sqrt{2d\\log\\left(1+\\frac{T L^{2}}{d}\\right)}+1}\\\\ &{\\qquad\\qquad\\leq2\\sqrt{\\alpha_{n}T q}\\cdot\\sqrt{2d\\log\\left(1+\\frac{T L^{2}}{d}\\right)}+1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "(Jensen's inequality) ", "page_idx": 20}, {"type": "text", "text": "where the first inequality is due to (C.7) and $\\mathbb{P}(\\mathcal{E}^{c})\\leq1/T$ from Lemma 2.1 and the last inequality is due to the fact that for any $t\\in[T]$ , with probability $q$ $t\\in\\mathcal T_{1}$ . Finally, we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{^{-}[R_{T}]=\\mathbb{E}[R_{T_{1}}]+\\mathbb{E}[R_{T_{2}}]}\\\\ &{\\qquad\\le2\\sqrt{\\alpha_{n}T q}\\cdot\\sqrt{2d\\log\\left(1+\\frac{T L^{2}}{d}\\right)}+2\\sqrt{6d\\alpha_{o}T/q}\\sqrt{\\lceil\\log(1+T L^{2}/d)\\rceil}+O(\\sqrt{n T\\log n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that $n=\\Theta(\\log d)$ and $\\alpha_{o}=\\Theta(S\\log T)$ . We obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]=O\\left(\\operatorname*{max}\\left\\{{\\sqrt{q d}},{\\sqrt{{\\frac{S}{q}}}}\\right\\}\\cdot{\\sqrt{d T}}\\cdot\\log T\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which completes the proof. ", "page_idx": 20}, {"type": "text", "text": "D Supporting lemmas ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma D.1 (Dani et al. [10]). Let $A_{1},A_{2},\\dots,A_{T}\\in\\ensuremath{\\mathbb{R}}^{d}$ and $\\begin{array}{r}{V_{t}=I+\\sum_{s=1}^{t}A_{s}A_{s}^{\\top}}\\end{array}$ for all $t\\in[T]$ Then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\operatorname*{min}\\left\\{1,\\|A_{t}\\|_{V_{t-1}^{-1}}^{2}\\right\\}\\leq2\\log\\operatorname*{det}(V_{T}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Moreover, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left\\{1,\\|A_{t}\\|_{V_{t-1}^{-1}}^{2}\\right\\}\\leq2\\log\\left(1+\\|A_{t}\\|_{V_{t-1}^{-1}}^{2}\\right)=2\\log\\left(\\frac{\\operatorname*{det}(V_{t})}{\\operatorname*{det}(V_{t-1})}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma D.2. For $\\ensuremath{\\mathcal{C}}_{t}$ defined in (2.2), we have that $\\mathcal{C}_{t}\\subseteq\\mathcal{C}_{t}^{o}$ for all $t\\in[T]$ ", "page_idx": 21}, {"type": "text", "text": "Proof. Recall ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal C_{t+1}=\\left\\{\\theta\\,:\\,\\|\\theta\\|_{2}^{2}+\\sum_{s=1}^{t}\\left(\\widehat{X}_{s}-\\langle\\theta,A_{s}\\rangle\\right)^{2}\\leq\\gamma(1/T)\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\theta\\|_{2}^{2}+\\displaystyle\\sum_{s=1}^{t}\\left(\\widehat{X}_{s}-\\langle\\theta,A_{s}\\rangle\\right)^{2}-\\|\\widehat\\theta_{t}\\|_{2}^{2}-\\displaystyle\\sum_{s=1}^{t}\\left(\\widehat{X}_{s}-\\langle\\widehat\\theta_{t},A_{s}\\rangle\\right)^{2}}\\\\ &{=\\|\\theta\\|_{2}^{2}-2\\theta^{\\top}\\left(\\displaystyle\\sum_{s=1}^{t}\\widehat{X}_{s}A_{s}\\right)+\\theta^{\\top}\\left(\\displaystyle\\sum_{s=1}^{t}A_{s}A_{s}^{\\top}\\right)\\theta}\\\\ &{\\quad-\\|\\widehat\\theta_{t}\\|_{2}^{2}+2\\widehat\\theta_{t}^{\\top}\\left(\\displaystyle\\sum_{s=1}^{t}\\widehat\\widehat{X}_{s}A_{s}\\right)-\\widehat\\theta_{t}^{\\top}\\left(\\displaystyle\\sum_{s=1}^{t}A_{s}A_{s}^{\\top}\\right)\\widehat\\theta_{t}}\\\\ &{=\\|\\theta\\|_{V_{t}}^{2}+2(\\widehat\\theta_{t}-\\theta)^{\\top}V_{t}\\widehat\\theta_{t}-\\|\\widehat\\theta_{t}\\|_{V_{t}}^{2}\\qquad(V_{t}=I+\\sum_{s=1}^{t}A_{s}A_{s}^{\\top},\\widehat\\theta_{t}=V_{t}^{-1}\\sum_{s=1}^{t}A_{s}\\widehat\\chi_{s})}\\\\ &{=\\|\\theta\\|_{V_{t}}^{2}-2\\theta^{\\top}V_{t}\\widehat\\theta_{t}+\\|\\widehat\\theta_{t}\\|_{V_{t}}^{2}=\\|\\theta-\\widehat\\theta_{t}\\|_{V_{t}}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence, we can express the ellipsoid as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{C}_{t+1}=\\left\\{\\Vert\\theta-\\widehat{\\theta}_{t}\\Vert_{V_{t}}^{2}+\\Vert\\widehat{\\theta}_{t}\\Vert_{2}^{2}+\\sum_{s=1}^{t}\\left(\\widehat{X}_{s}-\\langle\\widehat{\\theta}_{t},A_{s}\\rangle\\right)^{2}\\leq\\gamma(1/T)\\right\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, for all $t\\geq0$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{C}_{t+1}=\\left\\{\\theta\\ :\\ \\|\\theta-\\widehat{\\theta}_{t}\\|_{V_{t}}^{2}+\\|\\widehat{\\theta}_{t}\\|_{2}^{2}+\\displaystyle\\sum_{s=1}^{t}\\left(\\widehat{X}_{s}-\\langle\\widehat{\\theta}_{t},A_{s}\\rangle\\right)^{2}\\leq\\gamma(1/T)\\right\\}}\\\\ &{\\quad\\quad\\leq\\left\\{\\theta\\ :\\ \\|\\theta-\\widehat{\\theta}_{t}\\|_{V_{t}}^{2}\\leq\\gamma(1/T)\\right\\}}\\\\ &{\\quad\\quad\\leq\\left\\{\\theta\\ :\\ \\|\\theta-\\widehat{\\theta}_{t}\\|_{V_{t}}^{2}\\leq\\alpha_{o}\\right\\}}\\\\ &{\\quad=\\mathcal{C}_{t+1}^{o}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "concluding the proof. ", "page_idx": 21}, {"type": "text", "text": "Lemma D.3. For any $1\\leq p\\leq q\\leq n$ and $t\\in[T]$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|A_{t}^{p}\\|_{V_{t-1}^{-1}}\\leq\\|A_{t}^{q}\\|_{V_{t-1}^{-1}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Note that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{t}^{p}=\\underset{a\\in\\mathcal{A}_{t}}{\\mathrm{argmax}}\\,\\underset{\\theta\\in\\mathcal{C}_{t}^{p}}{\\mathrm{max}}\\langle\\theta,a\\rangle=\\underset{a\\in\\mathcal{A}_{t}}{\\mathrm{argmax}}\\langle a,\\widehat{\\theta}_{t-1}\\rangle+\\sqrt{\\alpha_{p}}\\|a\\|_{V_{t-1}^{-1}},}\\\\ &{A_{t}^{q}=\\underset{a\\in\\mathcal{A}_{t}}{\\mathrm{argmax}}\\,\\underset{\\theta\\in\\mathcal{C}_{t}^{q}}{\\mathrm{max}}\\langle\\theta,a\\rangle=\\underset{a\\in\\mathcal{A}_{t}}{\\mathrm{argmax}}\\langle a,\\widehat{\\theta}_{t-1}\\rangle+\\sqrt{\\alpha_{q}}\\|a\\|_{V_{t-1}^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For contradiction, we assume $\\|A_{t}^{q}\\|_{V_{t-1}^{-1}}<\\|A_{t}^{p}\\|_{V_{t-1}^{-1}}$ .Since $\\|A_{t}^{q}\\|_{V_{t-1}^{-1}}<\\|A_{t}^{p}\\|_{V_{t-1}^{-1}}$ \uff0c $\\|A_{t}^{q}\\|_{V_{t-1}^{-1}}\\neq$ $\\|A_{t}^{p}\\|_{V_{t-1}^{-1}}$ . Besides, according to the definition of $A_{t}^{p}$ and $A_{t}^{q}$ , wehave ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle A_{t}^{q},\\widehat{\\theta}_{t-1}\\rangle+\\sqrt{\\alpha_{p}}\\|A_{t}^{q}\\|_{V_{t-1}^{-1}}\\leq\\langle A_{t}^{p},\\widehat{\\theta}_{t-1}\\rangle+\\sqrt{\\alpha_{p}}\\|A_{t}^{p}\\|_{V_{t-1}^{-1}}}\\\\ {<\\langle A_{t}^{p},\\widehat{\\theta}_{t-1}\\rangle+\\sqrt{\\alpha_{q}}\\|A_{t}^{p}\\|_{V_{t-1}^{-1}}}\\\\ {\\leq\\langle A_{t}^{q},\\widehat{\\theta}_{t-1}\\rangle+\\sqrt{\\alpha_{q}}\\|A_{t}^{q}\\|_{V_{t-1}^{-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first inequality is due to the definition of $A_{t}^{p}$ , the second inequality is due to $\\sqrt{\\alpha_{p}}<\\sqrt{\\alpha_{q}}$ and the last inequality is due to the definition of $A_{t}^{q}$ . From the above results, we further have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\langle A_{t}^{q}-A_{t}^{p},\\widehat{\\theta}_{t-1}\\rangle\\leq\\sqrt{\\alpha_{p}}\\big(\\lVert A_{t}^{p}\\rVert_{V_{t-1}^{-1}}-\\lVert A_{t}^{q}\\rVert_{V_{t-1}^{-1}}\\big)}}\\\\ &{}&{<\\sqrt{\\alpha_{q}}\\big(\\lVert A_{t}^{p}\\rVert_{V_{t-1}^{-1}}-\\lVert A_{t}^{q}\\rVert_{V_{t-1}^{-1}}\\big)}\\\\ &{}&{\\quad\\mathrm{(Due~tossumption~\\lVertA_{t}^{q}\\rVert}_{V_{t-1}^{-1}}<\\lVert A_{t}^{p}\\rVert_{V_{t-1}^{-1}}\\leq\\lVert A_{t}^{q}\\rVert_{V_{t-1}^{-1}}\\leq\\lVert A_{t}^{q}\\rVert_{V_{t-1}^{-1}}\\mathrm{)}}\\\\ &{}&{\\quad\\mathrm{(I)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We obtain a contradiction. Therefore, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|A_{t}^{q}\\|_{V_{t-1}^{-1}}\\geq\\|A_{t}^{p}\\|_{V_{t-1}^{-1}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma D.4. ", "text_level": 1, "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t\\in\\mathcal{T}_{2}^{s}}\\operatorname*{min}\\left\\{1,\\|A_{t}^{n}\\|_{V_{t-1}^{-1}}^{2}\\right\\}\\right]\\leq2d/q+1/q.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Let $\\operatorname*{det}(V_{t-1})=(q_{t-1})^{d}$ and $\\operatorname*{det}(V_{t-1}+A_{t}^{n}(A_{t}^{n})^{\\top})=(q_{t-1}+x_{t})^{d}$ . For each $s\\geq0$ , let ", "page_idx": 22}, {"type": "equation", "text": "$$\nT^{s}=\\biggl\\{t\\in[T]:\\operatorname*{det}(V_{t-1})\\in\\Bigl[2^{s d},2^{d(s+1)}\\Bigr)\\biggr\\}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $\\mathcal{T}_{2}^{s}\\subseteq\\mathcal{T}^{s}$ , to show ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t\\in\\mathcal{T}_{2}^{s}}\\operatorname*{min}\\left\\lbrace1,\\|A_{t}^{n}\\|_{V_{t-1}^{-1}}^{2}\\right\\rbrace\\right]\\leq2d/q+1/q,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "we only need to prove ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t\\in\\mathcal{T}^{s}}\\operatorname*{min}\\left\\{1,\\|A_{t}^{n}\\|_{V_{t-1}^{-1}}^{2}\\right\\}\\right]\\leq2d/q+1/q.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We let $I_{t}=1$ if the coin tosses in the $t$ 's round is Head and O otherwise. We divide $\\mathcal{T}^{s}$ into two disjoint parts $\\mathcal{T}^{s}$ and $\\overline{{\\mathcal{T}^{s}}}$ . Specifically, ", "page_idx": 22}, {"type": "text", "text": "\u00b7 for $\\mathcal{T}^{s}$ , it holds that for $t\\in\\mathcal{T}^{s}$ $\\underline{{\\boldsymbol{r}}}_{\\bullet}^{s},\\operatorname*{det}(V_{t-1}+A_{t}^{n}(A_{t}^{n})^{\\top})\\leq2^{d(s+1)}.$ ", "page_idx": 22}, {"type": "text", "text": "\u00b7 for $\\overline{{\\mathcal{T}^{s}}}$ , it holds that for $t\\in\\overline{{\\mathcal{T}^{s}}}$ \uff0c $\\operatorname*{det}(V_{t-1}+A_{t}^{n}(A_{t}^{n})^{\\top})>2^{d(s+1)}.$ ", "page_idx": 22}, {"type": "text", "text": "From definition of $\\mathcal{T}^{s}$ and the fact that if $I_{t}=1$ $1,V_{t}=V_{t-1}+A_{t}^{n}(A_{t}^{n})^{\\top}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{t\\in\\mathcal{T}^{s}}x_{t}\\cdot I_{t}\\leq2^{s}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "From Algorithm 2, with probability $q,I_{t}=1$ . Therefore, if we let $\\{{\\mathcal{F}}_{t}\\}_{t\\in[T]}$ be the natural filtration of $\\{A_{t},I_{t},X_{t}\\}_{t\\in[T]}$ we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n2^{s}\\geq\\mathbb{E}\\left[\\sum_{t\\in\\mathcal{T}^{s}}x_{t}\\cdot I_{t}\\right]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbb{E}\\left[\\underset{t\\in[T]}{\\sum}x_{t},I_{t}+\\mathbb{E}\\left[t\\in\\mathbb{Z}^{*}\\right]\\right]}\\\\ &{=\\underset{t\\in[T]}{\\sum}\\mathbb{E}\\left[x_{t}\\cdot I_{t}:\\mathbb{I}\\{t\\in\\mathbb{Z}^{*}\\}\\right]}\\\\ &{=\\underset{t\\in[T]}{\\sum}\\mathbb{E}\\left[\\mathbb{E}\\left[x_{t}\\cdot I_{t}\\cdot\\mathbb{I}\\{t\\in\\mathbb{Z}^{*}\\}\\right]\\mathbb{F}_{t-1}\\right]}\\\\ &{=\\underset{t\\in[T]}{\\sum}\\mathbb{E}\\left[\\mathbb{E}\\left[x_{t}\\cdot\\mathbb{I}_{t}\\cdot\\mathbb{I}\\{t\\in\\mathbb{Z}^{*}\\}\\cdot\\mathbb{E}\\left[I_{t}|\\mathcal{F}_{t-1}\\right]\\right]}\\\\ &{=\\underset{t\\in[T]}{\\sum}\\mathbb{E}\\left[x_{t}\\cdot\\mathbb{I}\\{t\\in\\mathbb{Z}^{*}\\}\\cdot\\mathbb{E}\\left[I_{t}|\\mathcal{F}_{t-1}\\right]}\\\\ &{=\\underset{t\\in[T]}{\\sum}\\mathbb{E}\\left[x_{t}\\cdot\\mathbb{I}\\{t\\in\\mathbb{Z}^{*}\\}\\cdot\\mathbb{q}\\right]}\\\\ &{=\\mathbb{E}\\left[\\underset{t\\in\\mathbb{Z}^{*}}{\\sum}x_{t}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "(Law of total expectation) ", "page_idx": 23}, {"type": "text", "text": "$I_{t}$ is an independent coin-tossing) ", "page_idx": 23}, {"type": "text", "text": "and therefore $\\begin{array}{r}{\\mathbb{E}\\left[\\sum_{t\\in\\underline{{T}}^{s}}x_{t}\\right]\\leq2^{s}/q}\\end{array}$ For $t\\in\\mathcal{T}^{s}$ , we further have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\operatorname*{min}\\left\\{1,\\|A_{t}^{n}\\|_{V_{t-1}^{-1}}^{2}\\right\\}\\leq2\\log\\left(\\frac{\\operatorname*{det}(V_{t-1}+A_{t}^{n}(A_{t}^{n})^{\\top})}{\\operatorname*{det}(V_{t-1})}\\right)}&{}\\\\ {=2d\\log\\left(1+\\frac{x_{t}}{q_{t-1}}\\right)}&{}\\\\ {\\leq2d\\log\\left(1+\\frac{x_{t}}{2^{s}}\\right)}&{}\\\\ {\\leq\\frac{2d\\cdot x_{t}}{2^{s}}}&{\\mathrm{(du)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Bigg[\\sum_{t\\in\\mathcal{T}^{s}}\\operatorname*{min}\\Big\\{1,\\|A_{t}^{n}\\|_{V_{t-1}^{-1}}^{2}\\Big\\}\\Bigg]\\leq\\mathbb{E}\\Bigg[\\sum_{t\\in\\mathcal{T}^{s}}\\frac{2d\\cdot x_{t}}{2^{s}}\\Bigg]=\\frac{2d}{2^{s}}\\mathbb{E}\\Bigg[\\sum_{t\\in\\mathcal{T}^{s}}x_{t}\\Bigg]\\leq2d/q.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "From definition of $\\overline{{\\mathcal{T}^{s}}}$ if $I_{t}\\,=\\,1$ and $t\\,\\in\\,\\overline{{\\mathcal{T}^{s}}}$ \uff0c $\\operatorname*{det}(V_{t})\\,>\\,2^{d(s+1)}$ . Then, for all $\\tau>t,\\tau\\notin\\tau^{s}$ Therefore, there is at most one $t\\in\\overline{{\\mathcal{T}^{s}}}$ with $I_{t}=1$ . We obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Bigg[\\sum_{t\\in\\overline{{T^{s}}}}\\operatorname*{min}\\Big\\{1,\\|A_{t}^{n}\\|_{V_{t-1}^{-1}}^{2}\\Big\\}\\Bigg]\\leq\\mathbb{E}\\Bigg[\\vert\\overline{{T^{s}}}\\vert\\Bigg]\\leq1/q,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last inequality is because with probability $q,I_{t}=1$ By combining the bounds for $t\\in\\overline{{\\mathcal{T}^{s}}}$ and $t\\in\\mathcal{T}^{s}$ together, lemma follows. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Lemma D.5. Let $\\begin{array}{r}{Q=\\sum_{s\\geq o}q_{s}}\\end{array}$ . Then, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t\\in\\mathcal{T}^{s}}\\operatorname*{min}\\left\\{1,\\|A_{t}^{o}\\|_{V_{t-1}^{-1}}^{2}\\right\\}\\right]\\leq2d/Q+1/Q.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Let $\\operatorname*{det}(V_{t-1})=(q_{t-1})^{d}$ . Let $x_{t}$ satisfies $\\operatorname*{det}(V_{t-1}+A_{t}^{o}(A_{t}^{o})^{\\top})=(q_{t-1}+x_{t})^{d}$ . We let $I_{t}=\\mathbb{1}\\{I_{t}\\geq o\\}$ . We divide $\\mathcal{T}^{s}$ into two disjoint parts $\\mathcal{T}^{s}$ and $\\overline{{\\mathcal{T}^{s}}}$ . Specifically, ", "page_idx": 23}, {"type": "text", "text": "\u00b7 for $\\mathcal{T}^{s}$ , it holds that for $t\\in\\mathcal{T}^{s}$ $\\operatorname*{det}(V_{t-1}+A_{t}^{o}(A_{t}^{o})^{\\top})\\le2^{d(s+1)}.$ ", "page_idx": 23}, {"type": "text", "text": "\u00b7for $\\overline{{\\mathcal{T}^{s}}}$ , it holds that for $t\\in\\overline{{\\mathcal{T}^{s}}}$ \uff0c $\\operatorname*{det}(V_{t-1}+A_{t}^{o}(A_{t}^{o})^{\\top})>2^{d(s+1)}.$ ", "page_idx": 23}, {"type": "text", "text": "Note that for $I_{t}\\geq o$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\log(\\operatorname*{det}(V_{t}))-\\log(\\operatorname*{det}(V_{t-1}+A_{t}^{o}(A_{t}^{o})^{\\top}))}\\\\ &{=\\log(\\operatorname*{det}(V_{t}))-\\log(V_{t-1})-\\bigg(\\log(\\operatorname*{det}(V_{t-1}+A_{t}^{o}(A_{t}^{o})^{\\top}))-\\log(V_{t-1})\\bigg)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(\\operatorname*{due}\\tan{\\mathrm{Let}})}\\\\ &{=\\log(1+\\|A_{t}^{I_{t}}\\|_{V_{t-1}^{-1}})-\\log(1+\\|A_{t}^{o}\\|_{V_{t-1}^{-1}})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(\\operatorname*{due}\\tan{\\mathrm{Let}})}\\\\ &{\\ge0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "emma D.3) ", "page_idx": 24}, {"type": "text", "text": "Therefore, if we let $\\operatorname*{det}(V_{t-1}+A_{t}^{I_{t}}(A_{t}^{I_{t}})^{\\top})=(q_{t-1}+x_{t}^{\\prime})^{d}$ and $I_{t}\\geq o$ then $x_{t}^{\\prime}\\geq x_{t}$ . Hence, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t\\in\\mathcal{T}^{s}}x_{t}\\cdot I_{t}\\leq2^{s}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\mathbb{P}(I_{t}\\geq o)=\\sum_{s\\geq o}q_{s}=Q}\\end{array}$ Let $\\{{\\mathcal{F}}_{t}\\}_{t\\in[T]}$ be the natural fitration of $\\{A_{t},I_{t},X_{t}\\}_{t\\in[T]}$ We have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{Z}^{n}\\geq\\mathbb{E}\\left[\\sum_{t=1}^{n}\\mathcal{L}_{t}\\right]}\\\\ &{\\phantom{=}=\\mathbb{E}\\left[\\sum_{t=1}^{n}\\mathcal{L}_{t}\\cdot\\left\\{\\boldsymbol{t}(\\boldsymbol{\\epsilon}\\mathbb{Z}^{\\nu})\\right\\}\\right]}\\\\ &{\\phantom{=}=\\mathbb{E}\\left[\\sum_{t=1}^{n}\\mathcal{L}_{t}\\cdot\\left\\boldsymbol{t}_{1}\\cdot\\left\\{\\boldsymbol{t}(\\boldsymbol{\\epsilon}\\mathbb{Z}^{\\nu})\\right\\}\\right]}\\\\ &{\\phantom{=}=\\sum_{t=1}^{n}\\mathbb{E}\\left[\\left\\{\\boldsymbol{t}_{1}\\cdot\\boldsymbol{t}_{1}\\cdot\\boldsymbol{1}(\\boldsymbol{t}\\in\\mathbb{Z}^{\\nu})\\right\\}\\right]}\\\\ &{\\phantom{=}\\sum_{t=1}^{n}\\mathbb{E}\\left[\\left\\{\\boldsymbol{t}_{1}\\cdot\\boldsymbol{t}_{1}\\cdot\\boldsymbol{1}(\\boldsymbol{t}\\in\\mathbb{Z}^{\\nu})\\right\\}\\right]}\\\\ &{\\phantom{=}-\\sum_{t=1}^{n}\\mathbb{E}\\left[\\left\\{\\boldsymbol{t}_{1}\\cdot\\boldsymbol{t}_{1}\\cdot\\boldsymbol{1}(\\boldsymbol{t}\\in\\mathbb{Z}^{\\nu})\\right\\}\\right]}\\\\ &{\\phantom{=}-\\sum_{t=1}^{n}\\mathbb{E}\\left[\\left\\{\\boldsymbol{t}_{1}\\cdot\\boldsymbol{1}(\\boldsymbol{\\epsilon}\\mathbb{Z}^{\\nu})\\cdot\\mathbb{E}\\left[\\boldsymbol{t}_{1}\\cdot\\boldsymbol{\\tilde{\\epsilon}}_{1}\\right]\\right\\}\\right.}\\\\ &{\\phantom{=}\\left.\\sum_{t=1}^{n}\\mathbb{E}\\left[\\left\\{\\boldsymbol{t}_{1}\\cdot\\boldsymbol{1}(\\boldsymbol{\\epsilon}\\mathbb{Z}^{\\nu})\\cdot\\boldsymbol{1}(\\boldsymbol{t}\\right)\\right\\}\\right.}\\\\ &{\\phantom{=}\\left.\\sum_{t=1}^{n}\\mathbb{E}\\left[\\left\\{\\sum_{t=1}^{n}\\boldsymbol{\\epsilon}\\left(\\boldsymbol{t}\\in\\mathbb{Z}^{\\nu}\\right)\\cdot\\boldsymbol{q}\\right\\}\\right.}\\\\ &{\\phantom{=}\\left.\\sum_{t=1}^{n}\\mathbb{E}\\left[\\left\\{\\sum_{t=1}^{n}\\boldsymbol{\\epsilon}\\left(\\boldsymbol{t}\\right)\\cdot\\sum_{t=1}^{n}\\boldsymbol{\\epsilon}\\left(\\boldsymbol{t}\\right)\\right\\}\\right]}\\\\ &{\\phantom{=}-\\mathbb{E}\\left[\\left\\{\\sum_{t=1}^{n}\\boldsymbol{\\epsilon}\\left(\\boldsymbol{\\epsilon}\\right)\\cdot\\sum_{t= \n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "$\\textstyle x_{t}$ and $\\mathbb{1}\\{t\\in\\mathcal{T}^{s}\\}$ are $\\mathcal{F}_{t-1}$ -measurable) ", "page_idx": 24}, {"type": "text", "text": "$I_{t}$ is an independent coin-tossing) ", "page_idx": 24}, {"type": "text", "text": "and therefore ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t\\in\\mathcal{T}^{s}}x_{t}\\right]\\leq2^{s}/Q.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For $t\\in\\mathcal{T}^{s}$ , we further have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\operatorname*{min}\\left\\{1,\\|A_{t}^{\\sigma}\\|_{V_{t-1}^{-1}}^{2}\\right\\}\\leq\\operatorname*{min}\\left\\{1,\\|A_{t}^{t}\\|_{V_{t-1}^{-1}}^{2}\\right\\}}\\\\ &{\\leq2\\log\\left(\\frac{\\operatorname*{det}(V_{t-1}+A_{t}^{I_{t}}(A_{t}^{I_{t}})^{\\top})}{\\operatorname*{det}(V_{t-1})}\\right)}\\\\ &{=2d\\log\\left(1+\\frac{x_{t}}{q_{t-1}}\\right)}\\\\ &{\\leq2d\\log\\left(1+\\frac{x_{t}}{2^{s}}\\right)}\\\\ &{\\leq\\frac{2d\\cdot x_{t}}{2^{s}}}&{\\mathrm{(due)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "(due to Lemma D.1) ", "page_idx": 24}, {"type": "text", "text": "Therefore, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Bigg[\\sum_{t\\in\\mathcal{T}^{s}}\\operatorname*{min}\\Big\\{1,\\|A_{t}^{\\sigma}\\|_{V_{t-1}^{-1}}^{2}\\Big\\}\\Bigg]\\leq\\mathbb{E}\\Bigg[\\sum_{t\\in\\mathcal{T}^{s}}\\frac{2d\\cdot x_{t}}{2^{s}}\\Bigg]=\\frac{2d}{2^{s}}\\mathbb{E}\\Bigg[\\sum_{t\\in\\mathcal{T}^{s}}x_{t}\\Bigg]\\leq2d/Q.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "From definition of $\\overline{{\\mathcal{T}^{s}}}$ ,if $I_{t}=1$ and $t\\in\\overline{{\\mathcal{T}^{s}}}$ , then $I_{t}\\geq o$ and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{det}(V_{t})=\\operatorname*{det}(V_{n-1}+A_{t}^{I_{t}}(A_{t}^{I_{t}})^{\\top})>\\operatorname*{det}(V_{n-1}+A_{t}^{o}(A_{t}^{o})^{\\top})>(s+1)^{d}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then, for all $\\tau>t,\\tau\\notin\\tau^{s}$ . Therefore, there is at most one $t\\in\\overline{{\\mathcal{T}^{s}}}$ with $I_{t}=1$ . We obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Bigg[\\sum_{t\\in\\overline{{T^{s}}}}\\operatorname*{min}\\left\\lbrace1,\\|A_{t}^{o}\\|_{V_{t-1}^{-1}}^{2}\\right\\rbrace\\Bigg]\\leq\\mathbb{E}\\Bigg[\\left|\\overline{{T^{s}}}\\right|\\Bigg]\\leq1/Q,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last inequality isbecause with probability $\\begin{array}{r}{\\sum_{s\\geq o}q_{s}=Q,I_{t}=1}\\end{array}$ By combining the bounds for $t\\in\\overline{{\\mathcal{T}^{s}}}$ and $t\\in\\mathcal{T}^{s}$ together, lemma follows. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "E Experimental details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The code used in the experiments can be found in the following repository: https : //github . com/ jajajang/sparsity_agnostic_model_selection. ", "page_idx": 25}, {"type": "text", "text": "E.1   Settings common to all algorithms ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u00b7 Arm set: $\\mathbf{\\mathcal{A}}_{t}\\mathbf{\\Sigma}=\\mathbf{\\mathcal{A}}$ for all $t\\ \\in\\ [T]$ $A\\subset\\mathbb{S}^{d-1}$ (the unit sphere in $\\mathbb{R}^{d}$ ) is a set of $d\\!.$ dimensional vectors drawn independently and uniformly at random from $\\mathbb{S}^{d-1}$ , with $d=16$ and $\\lvert A\\rvert=30$ \uff1a   \n\u00b7 $\\boldsymbol{\\theta}_{*}$ is an $S$ -sparse $(S=1,2,4,8,16)$ vector generated as follows: before the game starts, draw $(\\theta_{*})_{1},\\dot{\\theta}\\dots,(\\theta_{*})_{S}\\sim\\mathbb{S}^{S-1}$ , and $(\\theta_{*})_{k}=\\bar{0}$ for all $k>S$   \n\u00b7 The noise on rewards: $\\{\\varepsilon_{t}\\}_{t\\in[T]}$ are i.i.d. with $\\xi_{t}\\sim\\operatorname{Unif}([-1,1])$   \n\u00b7 Number of iterations: $T=10^{4}$   \n\u00b7 Number of models: $n=6$   \n\u00b7 Radius of confidence sets: $\\alpha_{0}=0$ , and $\\alpha_{i}=2^{i}\\log t$ for $i=1,\\cdots,5$   \n\u00b7 Prior distribution {qs}se[6] - For _Unif. $\\{q_{s}\\}_{s\\in[6]}=\\left(\\textstyle{\\frac{1}{6}},\\textstyle{\\frac{1}{6}},\\textstyle{\\frac{1}{6}},\\textstyle{\\frac{1}{6}},\\textstyle{\\frac{1}{6}},\\textstyle{\\frac{1}{6}}\\right)$ - For _Theory, $\\{q_{s}\\}_{s\\in[6]}\\;=\\;\\left({\\frac{C}{2}},{\\frac{C}{4}},{\\frac{C}{8}},{\\frac{C}{16}},{\\frac{C}{32}},{\\frac{C}{64}}\\right)$ where $C\\;=\\;\\frac{63}{64}$ is a normalizing constant.   \n\u00b7 Each plot is the result of 20 repetitions for each method. The shade represents the 1-standard deviation bound.   \n\u00b7 Hardware: Lenovo Thinkpad P16s Gen 2 Laptop - Type 21HL - CPU: 13th Gen Intel(R) Core(TM) i7-1360P 2.20 GHz - RAM: 32GB   \n\u00b7 Computation time: total 1338.38 seconds. ", "page_idx": 25}, {"type": "text", "text": "E.2  AdaLinUCB details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u00b7 Since we empirically observed that $\\mathtt{E x p3}$ provided enough exploration, we aggressively set the forced exploration parameter $q$ to zero.   \n\u00b7 The learning rate of Exp3 was set to $\\begin{array}{r}{\\eta_{t}=2\\sqrt{\\frac{\\log n}{n t}}}\\end{array}$ \"nt', see [5].   \n\u00b7 Given the prior distribution $\\{q_{s}\\}_{s\\in[6]}$ ,we set $P_{t}$ as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\nP_{t,s}=\\frac{q_{s}\\exp{(\\eta_{t}S_{t,s})}}{\\sum_{j=1}^{n}q_{j}\\exp{(\\eta_{t}S_{t,j})}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "E.3 OFUL details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u00b7 We used the log-determinant form of the confidence set based on Abbasi-Yadkori et al. [1, Theorem 2], which gives the choice ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sqrt{\\gamma_{t}}=\\sqrt{2\\log T+\\log\\operatorname*{det}(V_{t})}+1\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for the parameter $\\gamma_{t}$ in (1.1) when $\\lambda=1$ and $\\delta=1/T$ ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The main claims presented in the abstract and introduction accurately represent the contributions and scope of the paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We have added a paragraph in the conclusions section discussing the limitations ofourwork. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide the full set of assumptions and complete proofs Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct thedataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We include our code in our supplementary material and will make the full code public if this work gets accepted. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide all the details of our experimental setting. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We reported 1-sigma error bars in our plots. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We state all information on the computer resources in Appendix E. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: Our research conform with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our paper poses no such risks. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our paper does not use existing assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 31}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our paper does not release new assets. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]