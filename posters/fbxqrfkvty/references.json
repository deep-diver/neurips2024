{"references": [{"fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This paper introduced the concept of in-context learning in LLMs, a key paradigm explored in the current research."}, {"fullname_first_author": "J. Wei", "paper_title": "Emergent abilities of large language models", "publication_date": "2022-00-00", "reason": "This paper investigated emergent capabilities of LLMs which provides context for the current research on in-context learning."}, {"fullname_first_author": "S. Min", "paper_title": "MetaICL: Learning to learn in context", "publication_date": "2022-00-00", "reason": "This paper introduced a meta-learning approach to improve in-context learning, offering a comparative method to the current research."}, {"fullname_first_author": "J. Von Oswald", "paper_title": "Transformers learn in-context by gradient descent", "publication_date": "2023-00-00", "reason": "This paper provided a theoretical understanding of in-context learning, which helps to interpret the results of the current work."}, {"fullname_first_author": "D. Dai", "paper_title": "Why can GPT learn in-context?", "publication_date": "2023-00-00", "reason": "This paper offered another theoretical perspective on in-context learning, providing a different interpretation of the mechanisms involved."}]}