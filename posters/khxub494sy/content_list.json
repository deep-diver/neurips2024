[{"type": "text", "text": "Nesterov acceleration despite very noisy gradients ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kanan Gupta Jonathan W. Siegel Stephan Wojtowytsch Department of Mathematics Department of Mathematics Department of Mathematics University of Pittsburgh Texas A&M University University of Pittsburgh kanan.g@pitt.edu jwsiegel@tamu.edu s.woj@pitt.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We present a generalization of Nesterov\u2019s accelerated gradient descent algorithm. Our algorithm (AGNES) provably achieves acceleration for smooth convex and strongly convex minimization tasks with noisy gradient estimates if the noise intensity is proportional to the magnitude of the gradient at every point. Nesterov\u2019s method converges at an accelerated rate if the constant of proportionality is below 1, while AGNES accommodates any signal-to-noise ratio. The noise model is motivated by applications in overparametrized machine learning. AGNES requires only two parameters in convex and three in strongly convex minimization tasks, improving on existing methods. We further provide clear geometric interpretations and heuristics for the choice of parameters. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The recent success of deep learning [LeCun et al., 2015] is built on stochastic first order optimization methods such as stochastic gradient descent [LeCun et al., 1998] and ADAM [Kingma and Ba, 2014], which have enabled the large-scale training of neural networks. While such tasks are generally non-convex, accelerated first order methods for convex optimization have proved practically useful. Specifically, Nesterov [1983]\u2019s accelerated gradient descent has become a standard training method [Sutskever et al., 2013]. ", "page_idx": 0}, {"type": "text", "text": "Modern neural networks tend to operate in the overparametrized regime, i.e. the number of model parameters exceeds the number of data points to be fit [Belkin, 2021]. In this setting, minibatch gradient estimates are exact (namely, exactly 0) on the set of global minimizers since data can be interpolated exactly. Motivated by such applications, Vaswani et al. [2019] proved that Nesterov [2012]\u2019s accelerated coordinate descent method (ACDM) achieves acceleration in (strongly) convex optimization with multiplicative noise, i.e. when assuming stochastic gradient estimates for which the noise intensity scales linearly with the magnitude of the gradient. Conversely, Liu and Belkin [2018] show that the original version of Nesterov [1983]\u2019s method generally does not achieve acceleration in this setting. ", "page_idx": 0}, {"type": "text", "text": "Another algorithm with a similar goal is the continuized Nesterov method (CNM), which has been studied by Even et al. [2021], Berthier et al. [2021] in convex optimization (deterministic or with additive noise) and with multiplicative noise for overparametrized linear least squares regression. For a more extensive discussion of the context of our work in the literature, please see Section 2. ", "page_idx": 0}, {"type": "text", "text": "Vaswani et al. [2019]\u2019s algorithm is a four parameter scheme in the strongly convex case, which reduces to a three parameter scheme in the convex case. Liu and Belkin [2018] introduce a simpler three parameter scheme, but only prove that it achieves acceleration for overparametrized linear problems. In this work, we demonstrate that it is possible to achieve the same theoretical guarantees as Vaswani et al. [2019] with a simpler scheme, which can be considered as a reparametrized version of Liu and Belkin [2018]\u2019s Momentum-Added Stochastic Solver (MaSS) method. More precisely, we prove the following: ", "page_idx": 0}, {"type": "text", "text": "1. We show that Nesterov\u2019s accelerated gradient descent achieves an accelerated convergence rate, but only with noise which is strictly smaller than the gradient in the $L^{2}$ -sense. We also show numerically that when the noise is larger than the gradient, the algorithm diverges for a choice of step size for which gradient descent remains convergent.   \n2. Motivated by this, we introduce a generalization of Nesterov\u2019s method, which we call Accelerated Gradient descent with Noisy EStimators (AGNES), which provably achieves acceleration no matter how large the noise is relative to the gradient, both in the convex and strongly convex cases.   \n3. When moving from NAG to AGNES, the learning rate \u2018bifurcates\u2019 to two parameters in order to accommodate stochastic gradient estimates. The extension requires three hyperparameters in the strongly convex case and two in the convex case.   \n4. We provide a transparent geometric interpretation of the AGNES parameters in terms of their scaling with problem parameters (Appendix F.3) and the continuum limit models for various scaling regimes (Appendix C).   \n5. We build strong intuition for the choice of hyperparameters for machine learning applications and empirically demonstrate that AGNES improves the training of CNNs relative to SGD with momentum and Nesterov\u2019s accelerated gradient descent. ", "page_idx": 1}, {"type": "text", "text": "2 Literature Review ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Accelerated first order methods. Accelerated first order methods have been extensively studied in convex optimization. Beginning with the conjugate gradient (CG) algorithm introduced by Hestenes and Stiefel [1952], the Heavy ball method of Polyak [1964], and Nesterov [1983]\u2019s seminal work on accelerated gradient descent, many authors have developed and analyzed accelerated first order methods for convex problems, including Beck and Teboulle [2009], Nesterov [2012, 2013], Chambolle and Dossal [2015], Kim and Fessler [2018] to name just a few. ", "page_idx": 1}, {"type": "text", "text": "An important line of research is to gain an understanding of how accelerated methods work. After Polyak [1964] derived the original Heavy ball method as a discretization of an ordinary differential equation, Alvarez et al. [2002], Su et al. [2014], Wibisono et al. [2016], Zhang et al. [2018], Siegel [2019], Shi et al. [2019], Muehlebach and Jordan [2019], Wilson et al. [2021], Shi et al. [2021], Suh et al. [2022], Attouch et al. [2022], Aujol et al. [2022b,a], Dambrine et al. [2022] studied accelerated first order methods from the point of view of ODEs. This perspective has facilitated the use of Lyapunov functional analysis to quantify the convergence properties. We remark that in addition to the intuition provided by differential equations, Joulani et al. [2020] and Gasnikov and Nesterov [2018] have also proposed interesting ideas for explaining and deriving accelerated first-order methods. In addition, there has been a large interest in deriving adaptive accelerated first order methods, see for instance Levy et al. [2018], Cutkosky [2019], Kavis et al. [2019]. ", "page_idx": 1}, {"type": "text", "text": "Stochastic optimization. Robbins and Monro [1951] first introduced optimization algorithms where gradients are only estimated by a stochastic oracle. For convex optimization, Nemirovski et al. [2009], Ghadimi and Lan [2012] obtained minimax-optimal convergence rates with additive stochastic noise. ", "page_idx": 1}, {"type": "text", "text": "In deep learning, stochastic algorithms are ubiquitous in the training of deep neural networks, see [LeCun et al., 1998, 2015, Goodfellow et al., 2016, Bottou et al., 2018]. Here, the additive noise assumption not usually appropriate. As Wojtowytsch [2023], Wu et al. [2022a] show, the noise is of low rank and degenerates on the set of global minimizers. Stich [2019], Stich and Karimireddy [2022], Bassily et al. [2018], Gower et al. [2019], Damian et al. [2021], Wojtowytsch [2023], Zhou et al. [2020] consider various non-standard noise models and [Wojtowytsch, 2021, Zhou et al., 2020, Li et al., 2022] study the continuous time limit of stochastic gradient descent. These include noise assumptions for degenerate noise due to Bassily et al. [2018], Damian et al. [2021], Wojtowytsch [2023, 2021], low rank noise studied by Damian et al. [2021], Li et al. [2022] and noise with heavy tails explored by Zhou et al. [2020]. ", "page_idx": 1}, {"type": "text", "text": "Acceleration with stochastic gradients. Kidambi et al. [2018] prove that there are situations in which it is impossible for any first order oracle method to improve upon SGD due to informationtheoretic lower bounds. More generally, lower bounds in the stochastic first order oracle (SFO) model were presented by Nemirovski et al. [2009] (see also [Ghadimi and Lan, 2012]). ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "A partial improvement on the state of the art is given by Jain et al. [2018], who present an accelerated stochastic gradient method motivated by a particular low-dimensional and strongly convex problem. Laborde and Oberman [2020] obtain faster convergence of an accelerated method under an additive noise assumption by a Lyapunov function analysis. Bollapragada et al. [2022] study an accelerated gradient method for the optimization of a strongly convex quadratic objective function with minibatch noise. ", "page_idx": 2}, {"type": "text", "text": "Closest to our work are Liu and Belkin [2018], Vaswani et al. [2019], Even et al. [2021], Berthier et al. [2021] who study generalizations of Nesterov\u2019s method in stochastic optimization. Liu and Belkin [2018], Even et al. [2021] obtain guarantees with noise of approximately multiplicative noise in overparametrized linear least squares problems and for general convex objective functions with additive noise and in deterministic optimization. Vaswani et al. [2019] obtain comparable guarantees for the more complicated method of Nesterov [2013]. ", "page_idx": 2}, {"type": "text", "text": "3 Algorithm and Convergence Guarantees ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Assumptions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the remainder of this article, we consider the task of minimizing an objective function $f:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}$ using stochastic gradient estimates $g$ . We assume that $f,g$ and the initial condition $x_{0}$ satisfy: ", "page_idx": 2}, {"type": "text", "text": "1. The initial condition $x_{0}$ is a (potentially random) point such that $\\mathbb{E}[f(x_{0})+\\|x_{0}\\|^{2}]<\\infty$ .   \n2. $f$ is $L-$ smooth, i.e. $\\nabla f$ is $L$ \u2212Lipschitz continuous with respect to the Euclidean norm.   \n3. There exists a probability space $(\\Omega,A,\\mathbb{P})$ and a gradient estimator, i.e. a measurable function $g:\\mathbb{R}^{m}\\times\\Omega\\rightarrow\\mathbb{R}^{m}$ such that for all $x\\in\\mathbb{R}^{m}$ the properties \u2022 $\\mathbb{E}_{\\omega}[g(x,\\omega)]=\\nabla f(x)$ (unbiased gradient oracle) and \u2022 $\\begin{array}{r}{\\mathbb{E}_{\\omega}\\big[\\|g(x,\\omega)-\\nabla f(x)\\|^{2}\\big]\\leq\\sigma^{2}\\,\\|\\nabla f(x)\\|^{2}}\\end{array}$ (multiplicative noise scaling) hold. ", "page_idx": 2}, {"type": "text", "text": "A justification of the multiplicative noise scaling is given in Section 4. In the setting of machine learning, the space $\\Omega$ is given by the random subsampling of the dataset. A rigorous discussion of the probabilistic foundations is given in Appendix D. ", "page_idx": 2}, {"type": "text", "text": "3.2 Nesterov\u2019s Method with Multiplicative Noise ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "First we analyze Nesterov [1983]\u2019s accelerated gradient descent algorithm (NAG) in the setting of multiplicative noise. NAG is given by the initialization $x_{0}=x_{0}^{\\prime}$ and the two-step iteration ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{n+1}=x_{n}^{\\prime}-\\eta g_{n}^{\\prime},\\qquad x_{n+1}^{\\prime}=x_{n+1}+\\rho_{n}\\big(x_{n+1}-x_{n}\\big)=x_{n+1}+\\rho_{n}(x_{n}^{\\prime}-\\eta g_{n}^{\\prime}-x_{n})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $g_{n}^{\\prime}=g(x_{n}^{\\prime},\\omega_{n})$ and the variables $\\omega_{n}$ are iid samples from the probability space $\\Omega$ , i.e. $g_{n}^{\\prime}$ is an unbiased estimate of $\\nabla f(x_{n}^{\\prime})$ . We write $\\rho$ instead of $\\rho_{n}$ in cases where a dependence on $n$ is not required. We show that this scheme achieves an $O(1/n^{2})$ convergence rate for convex functions but only in the case that $\\sigma<1$ . To the best of our knowledge, this analysis is optimal. ", "page_idx": 2}, {"type": "text", "text": "Theorem 1 (NAG, convex case). Suppose that $x_{n}$ and $x_{n}^{\\prime}$ are generated by the time-stepping scheme $(I),\\;f$ and $g$ satisfy the conditions laid out in Section 3.1, $f$ is convex, and $x^{*}$ is a point such that $f(x^{*})=\\operatorname*{inf}_{x\\in\\mathbb{R}^{m}}f(x)$ . If $\\sigma<1$ and the parameters are chosen such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n0<\\eta\\leq\\frac{1-\\sigma^{2}}{L(1+\\sigma^{2})},\\ \\ \\ a n d\\ \\ \\ \\rho_{n}=\\frac{n}{n+3},\\ \\ \\ \\ \\ \\ t h e n\\ \\ \\ \\ \\ \\mathbb{E}[f(x_{n})-f(x^{*})]\\leq\\frac{2\\mathbb{E}[\\|x_{0}-x^{*}\\|^{2}]}{\\eta n^{2}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The expectation on the right hand side is over the random initialization $x_{0}$ . ", "page_idx": 2}, {"type": "text", "text": "The proof of Theorem 1 is given in Appendix E. Note that the constant $1/\\eta$ blows up as $\\sigma\\nearrow1$ and the analysis yields no guarantees for $\\sigma>1$ . This mirrors numerical experiments in Section 5. ", "page_idx": 2}, {"type": "text", "text": "Theorem 2 (NAG, strongly convex case). In addition to the assumptions in Theorem $^{\\,l}$ , suppose that $f$ is $\\mu$ -strongly convex and the parameters are chosen such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n0<\\eta\\leq\\frac{1-\\sigma^{2}}{L(1+\\sigma^{2})}\\;a n d\\;\\rho=\\frac{1-\\sqrt{\\mu\\eta}}{1+\\sqrt{\\mu\\eta}},\\;t h e n\\;\\mathbb{E}[f(x_{n})-f(x^{*})]\\leq2(1-\\sqrt{\\mu\\eta})^{n}\\,\\mathbb{E}\\left[f(x_{0})-f(x^{*})\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Just like in the convex case, the step size $\\eta$ decreases to zero as $\\sigma\\nearrow1$ , and we fail to obtain convergence guarantees for $\\sigma\\geq1$ . We argue in the proof of Theorem 2, given in Appendix F, that it is not possible to modify the Lyapunov sequence analysis to obtain a better rate of convergence. This motivates our introduction of the more general AGNES method below. ", "page_idx": 3}, {"type": "text", "text": "Notably, there cannot be a diverging lower bound for NAG since gradient descent arises in the special case $\\rho=0$ , and gradient descent converges for small stepsize with multiplicative noise [Wojtowytsch, 2023]. On the other hand, Liu and Belkin [2018] show that NAG does not achieve accelerated convergence with multiplicative type noise even for quadratic strongly convex functions. ", "page_idx": 3}, {"type": "text", "text": "3.3 AGNES Descent algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The proofs of Theorems 1 and 2 suggest that the momentum step in (1) is quite sensitive to the step size used for the gradient step, which severely restricts the step size $\\eta$ . We propose the Accelerated Gradient descent with Noisy EStimators (AGNES) scheme, which addresses this problem by introducing an additional parameter $\\alpha$ in the momentum step: ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{0}=x_{0}^{\\prime},\\qquad x_{n+1}=x_{n}^{\\prime}-\\eta g_{n}^{\\prime},\\qquad x_{n+1}^{\\prime}=x_{n+1}+\\rho_{n}\\big(x_{n}^{\\prime}-\\alpha g_{n}^{\\prime}-x_{n}\\big),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $g_{n}^{\\prime}=g(x_{n}^{\\prime},\\omega_{n})$ as before. Equivalently, AGNES can be formulated as a three-step scheme with an auxiliary velocity variable $v_{n}$ , initialized as $v_{0}=0$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{n}^{\\prime}=x_{n}+\\alpha v_{n},\\qquad x_{n+1}=x_{n}^{\\prime}-\\eta g_{n}^{\\prime},\\qquad v_{n+1}=\\rho_{n}\\bigl(v_{n}-g_{n}^{\\prime}\\bigr).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We show that the two formulations of AGNES are equivalent in Appendix B.1. However, we find (3) more intuitive (see Appendix $\\mathbf{C}$ for a continuous time interpretation) and easier to implement as an algorithm without storing past values of $x_{n}$ . The pseudocode and a set of suggested default parameters are given in Algorithm 1. ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1: Accelerated Gradient descent with Noisy EStimators (AGNES) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Input: $f$ (objective/loss function), $x_{0}$ (initial point), $\\alpha=10^{-3}$ (learning rate), $\\eta=10^{-2}$ (correction step size), $\\rho=0.99$ (momentum), $N$ (number of iterations) ", "page_idx": 3}, {"type": "text", "text": "$n\\gets0$ $v_{0}\\leftarrow0$ ", "page_idx": 3}, {"type": "text", "text": "// gradient estimate ", "page_idx": 3}, {"type": "equation", "text": "$x_{n+1}\\leftarrow x_{n}+\\alpha v_{n+1}-\\eta g_{n}$ ", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "end ", "page_idx": 3}, {"type": "text", "text": "gN \u2190\u2207xf(xN) xN \u2190xN \u2212\u03b7gn Return: xN ", "page_idx": 3}, {"type": "text", "text": "From (1) and (2), we note that NAG is AGNES with the special choice $\\alpha=\\eta$ . Allowing $\\alpha$ and $\\eta$ to be different helps AGNES achieve an accelerated rate of convergence for both convex and strongly convex functions, no matter how large $\\sigma$ is. While for gradient descent, only the product $L(1+\\bar{\\sigma}^{2})$ has to be considered, this is not the case for momentum-based schemes. We consider first the convergence rate in the convex case. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3 (AGNES, convex case). Suppose that $x_{n}$ and $x_{n}^{\\prime}$ are generated by the time-stepping scheme (3), $f$ and $g_{n}^{\\prime}=g(x_{n}^{\\prime},\\omega_{n})$ satisfy the conditions laid out in Section 3.1, $f$ is convex, and $x^{*}$ is a point such that $f(x^{*})=\\operatorname*{inf}_{x\\in\\mathbb{R}^{m}}f(x)$ . If the parameters are chosen such that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c c c}{0<\\eta<\\displaystyle\\frac{1}{L(1+\\sigma^{2})},}&{\\alpha=\\displaystyle\\frac{\\eta}{1+\\sigma^{2}},}&{\\rho_{n}=\\displaystyle\\frac{n}{n+1+a_{0}},\\quad f o r\\quad a_{0}\\ge\\displaystyle\\frac{2(1-\\eta L)}{1-\\eta L(1+\\sigma^{2})},}\\\\ {\\mathbb{E}\\big[f(x_{n})-f(x^{*})\\big]\\le\\displaystyle\\frac{a_{0}^{2}\\,\\mathbb{E}\\big[\\|x_{0}-x^{*}\\|^{2}\\big]}{2\\,\\alpha\\,n^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In particular, if $\\begin{array}{r}{\\eta\\le\\frac{1}{L(1+2\\sigma^{2})}}\\end{array}$ , we may make the universal choice $a_{0}=4$ , i.e. $\\textstyle\\rho_{n}={\\frac{n}{n+5}}$ . Only the parameters $\\eta,\\alpha$ depend on the specific problem. The proof of Theorem 3 is given in Appendix E. ", "page_idx": 3}, {"type": "text", "text": "There, we also present an alternative version of Theorem 3 for a different choice of parameters ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\eta\\leq\\frac{1}{L(1+\\sigma^{2})},\\quad\\alpha<\\frac{\\eta}{1+\\sigma^{2}},\\quad\\rho_{n}=\\frac{n+n_{0}}{n+n_{0}+3}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for a potentially large \u03b7\u2212\u03b12(\u03b71\u03c3+2\u03c32) \u22652\u03c32. The convergence guarantees are similar in both cases. ", "page_idx": 4}, {"type": "text", "text": "The benefit of the accelerated scheme is an improvement from a decay rate of $O(1/n)$ to the rate $O(1/n^{2})$ , which is optimal under the given assumptions even in the deterministic case. While the noise can be orders of magnitude larger than the quantity we want to estimate, it only affects the constants in the convergence, not the rate. We get an analogous result for strongly convex functions. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4 (AGNES, strongly convex case). In addition to the assumptions in Theorem 3, suppose that $f$ is $\\mu$ -strongly convex and the parameters are chosen such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n0<\\eta\\leq\\frac{1}{L(1+\\sigma^{2})},\\qquad\\rho=\\frac{1-\\sqrt{\\frac{\\mu\\eta}{1+\\sigma^{2}}}}{1+\\sqrt{\\frac{\\mu\\eta}{1+\\sigma^{2}}}},\\qquad a n d\\qquad\\alpha=\\frac{1-\\sqrt{\\frac{\\mu}{L}}}{1-\\sqrt{\\frac{\\mu}{L}}+\\sigma^{2}}\\,\\eta\\qquad t h e n\\,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(x_{n})-f(x^{*})]\\leq2\\left(1-\\sqrt{\\frac{\\mu\\eta}{1+\\sigma^{2}}}\\right)^{n}\\mathbb{E}[f(x_{0})-f(x^{*})].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Choosing $\\eta$ too small can be interpreted as overestimating $L$ or $\\sigma$ . Choosing $\\alpha$ too small (with respect to $\\eta$ ) can be interpreted as overestimating $\\sigma$ . Since every $L$ -Lipschitz function is $L^{\\prime}$ -Lipschitz for $L^{\\prime}>L$ , and since the multiplicative noise bound with constant $\\sigma$ implies the same bound with $\\sigma^{\\prime}>\\sigma$ , exponential convergence still holds at a generally slower rate. ", "page_idx": 4}, {"type": "text", "text": "We note that since $|\\nabla f(x)|^{2}\\leq2L(f(x)-\\operatorname*{inf}f)$ (Lemma 12 in Appendix D), Theorems 3 and 4 lead to analogous convergence results for $\\mathbb{E}[\\nabla f(x_{n})]$ as well. Due to the summability of the sequences $n^{-2}$ and $r^{n}$ for $r<1$ , we get not only convergence in expectation but also almost sure convergence. The proof is given in Appendix E. ", "page_idx": 4}, {"type": "text", "text": "Corollary 5. In the setting of Theorems 3 and 4, $f(x_{n})\\to\\operatorname{inf}\\,f$ with probability $^{\\,I}$ . ", "page_idx": 4}, {"type": "text", "text": "In the deterministic case $\\sigma\\,=\\,0$ , we have $\\alpha\\,=\\,\\eta$ in both Theorems 3 and 4. In Theorem 4, the parameters coincide with the usual choice for NAG, while we opted for a simple statement in Theorem 3 which does not exactly recover the standard choice $\\eta=1/L$ and $\\rho_{n}=\\bar{n}/(n+3)$ . The proofs below easily cover these special cases as well. If $0<\\sigma<1$ , both AGNES and NAG converge with the same rate $n^{-2}$ in the convex case, but the constant of NAG is always larger. In the strongly convex case, even the decay rate of NAG is slower than AGNES for $\\sigma\\in(0,1)$ since $1-\\sigma^{2}<(\\bar{1}+\\sigma^{2})^{-1}$ . We see the real power of AGNES in the stochastic setting where it converges for very high values of $\\sigma$ when Nesterov\u2019s method may diverge. For the optimal choice of parameters, we summarize the results in terms of the time-complexity of SGD and AGNES in Figure 1. For the related guarantee for SGD, see Theorems 17 and 22 in Appendices $\\boldsymbol{\\mathrm E}$ and $\\boldsymbol{\\mathrm{F}}$ respectively. ", "page_idx": 4}, {"type": "text", "text": "Remark 6 (Batching). Let us compare AGNES with two families of gradient estimators: ", "page_idx": 4}, {"type": "text", "text": "1. $g_{n}^{\\prime}=g(x_{n}^{\\prime},\\omega_{n})$ as studied in Theorems 3 and 4.   \n2. A gradient estimator $\\begin{array}{r}{g_{n}^{\\prime}\\;:=\\;\\frac{1}{n_{b}}\\sum_{j=1}^{n_{b}}g(x_{n}^{\\prime},\\omega_{n,j})}\\end{array}$ which averages multiple independent estimates to reduce the variance. ", "page_idx": 4}, {"type": "text", "text": "The second gradient estimator falls into the same framework with $\\tilde{\\Omega}\\,=\\,\\Omega^{n_{b}}$ and $\\tilde{\\sigma}^{2}\\,=\\,\\sigma^{2}/n_{b}$ . Assuming vector additions cost negligible time, optimizer steps are only as expensive as gradient evaluations. In this setting \u2013 which is often realistic in deep learning \u2013 it is appropriate to compare $\\mathbb{E}[f(x_{n_{b}n})]$ $(n_{b}\\cdot n$ iterations using $g_{n}^{\\prime},$ ) and $\\mathbb{E}[f(X_{n})]$ ( $\\ln$ iterations with $g_{n}^{\\prime}$ ). For the strongly convex case, we note that $\\begin{array}{r}{\\left(1-\\sqrt{\\frac{\\mu}{L}}\\,\\frac{1}{1+\\sigma^{2}}\\right)^{n_{b}}\\leq1-\\sqrt{\\frac{\\mu}{L}}\\,\\frac{1}{1+\\sigma^{2}/n_{b}}}\\end{array}$ if and only if ", "page_idx": 4}, {"type": "equation", "text": "$$\nn_{b}\\geq\\frac{\\log\\left(1-\\sqrt\\frac\\mu L\\,\\frac1{1+\\sigma^{2}/n_{b}}\\right)}{\\log\\left(1-\\sqrt\\frac\\mu L\\,\\frac1{1+\\sigma^{2}}\\right)}\\approx\\frac{\\sqrt\\frac\\mu L\\,\\frac{1}{1+\\sigma^{2}/n_{b}}}{\\sqrt\\frac\\mu L\\,\\frac{1}{1+\\sigma^{2}}}=\\frac{1+\\sigma^{2}}{1+\\sigma^{2}/n_{b}}=\\frac{1+\\sigma^{2}}{n_{b}+\\sigma^{2}}\\,n_{b}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "table", "img_path": "kHXUb494SY/tmp/0381da14cd56a58492873d276b33027df53a5b4a8502f477a2b949aae13048be.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 1: The minimal $n$ for AGNES and SGD such that $\\mathbb{E}[f(x_{n})-\\operatorname*{inf}f]<\\varepsilon$ when minimizing an $L$ -smooth function with multiplicative noise intensity $\\sigma$ in the gradient estimates and under a convexity assumption. The SGD rate of the $\\mu$ -strongly convex case is achieved more generally under a PL condition with PL-constant $\\mu$ . While SGD requires the optimal choice of one variable to achieve the optimal rate, AGNES requires three (two in the determinstic case). ", "page_idx": 5}, {"type": "text", "text": "The approximation is well-justified in the important case that $\\mu\\ll L$ . In particular, the upper bound for non-batching AGNES is always favorable compared to the batching version as $n_{b}\\in\\mathbb{N}_{\\geq1}$ , and the two only match for the optimal batch size $n_{b}=1$ . The optimal batch size for minimizing $f$ is the largest one that can be processed in parallel without increasing the computing time for a single step. A similar argument holds for the convex case. ", "page_idx": 5}, {"type": "text", "text": "With a slight modification, the proof of Theorem 3 extends to the situation of convex objective functions which do not have minimizers. Such objectives arise for example in linear classification with the popular cross-entropy loss function and linearly separable data. ", "page_idx": 5}, {"type": "text", "text": "Theorem 7 (Convexity without minimizers). Let $f$ be a convex objective function satisfying the assumptions in Section 3.1 and $x_{n}$ be generated by the time-stepping scheme (3). Assume that $\\eta,\\alpha$ and $\\rho_{n}$ are as in Theorem 3. Then li $\\operatorname{n\\,inf}_{n\\to\\infty}\\mathbb{E}[f(x_{n})]=\\operatorname*{inf}_{x\\in\\mathbb{R}^{m}}f(x)$ . ", "page_idx": 5}, {"type": "text", "text": "The proof and more details are given in Appendix E. For completeness, we consider the case of non-convex optimization in Appendix G. As a limitation, we note that multiplicative noise is well-motivated in machine learning for global minimizers, but not at generic critical points. ", "page_idx": 5}, {"type": "text", "text": "3.4 Geometric Interpretation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Let us briefly discuss the parameter choices in Theorem 4. As we consider larger $\\sigma$ for fixed $\\mu$ and $L$ , the decay factor $\\rho$ moves closer to 1. This slows the \u2018forgetting\u2019 of past gradients in $v_{n}$ , allowing us to better average out stochastic noise. The price we pay is computing with more outdated gradients, slowing convergence. Our choice balances these effects. ", "page_idx": 5}, {"type": "text", "text": "In AGNES, $\\rho$ inadvertently also governs magnitude of the momentum variable $v_{n}$ , which scales as $(1-\\rho)^{-1}$ for objective functions with constant gradient and $n\\gg1$ . To compensate, we choose $\\alpha$ smaller compared to $\\eta$ when $\\sigma$ (and thus $(1\\bar{-}\\rho)^{-1})$ is large. Nevertheless, the effect of the momentum step does not decrease. For further details, see Appendix F.3. ", "page_idx": 5}, {"type": "text", "text": "For further interpretability, we obtain several ODE and SDE continuous time descriptions of AGNES in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "4 Motivation for Multiplicative Noise ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In supervised learning applications, the learning task often corresponds to minimizing a risk or loss function $\\begin{array}{r}{\\mathcal{R}(w)\\,=\\,\\frac{\\bar{1}}{N}\\!\\sum_{i=1}^{N}\\ell\\big(h(w,x_{i}),y_{i}\\big)\\stackrel{\\cdot}{=}:\\frac{1}{N}\\sum_{i=1}^{N}\\ell_{i}(w)}\\end{array}$ , where $h:\\mathbb{R}^{m}\\,\\times\\,\\mathbb{R}^{d}\\,\\to\\,\\mathbb{R}^{k}$ , $(w,x)\\mapsto h(w,x)$ and $\\ell:\\mathbb{R}^{k}\\times\\mathbb{R}^{k}\\rightarrow[0,\\infty)$ are a parametrized function of weights $w$ and data $x$ and a loss function measuring compliance between $h(w,x_{i})$ and $y_{i}$ respectively.1 Safran and Shamir [2018], Chizat and Bach [2018], Du et al. [2018] show that working in the overparametrized regime $m\\gg N$ simplifies the optimization process and Belkin et al. [2019, 2020] illustrate that it facilitates generalization to previously unseen data. Cooper [2019] shows that fitting $N$ constraints with $m$ parameters typically leads to an $m-N$ -dimensional submanifold $\\mathcal{M}$ of the parameter space $\\mathbb{R}^{m}$ such that all given labels $y_{i}$ are fti exactly by $h(w,\\cdot)$ at the data points $x_{i}$ for $w\\in\\mathcal{M}$ , i.e. $\\mathcal{R}\\equiv0$ on the smooth set of minimizers ${\\mathcal{M}}={\\mathcal{R}}^{-\\mathrm{{i}}}(\\{0\\})$ . ", "page_idx": 5}, {"type": "image", "img_path": "kHXUb494SY/tmp/6d3c18805ed52d6626d809dfc51c369d1b2fe8d1188ff691d84068d4863d8019.jpg", "img_caption": ["Figure 2: To be able to quantify the gradient noise exactly, we choose relatively small models and data sets. Left: A ReLU network with four hidden layers of width 250 is trained by SGD to fit random labels $y_{i}$ (drawn from a 2-dimensional standard Gaussian) at $1,000$ random data points $x_{i}$ (drawn from a 500-dimensional standard Gaussian). The variance $\\sigma^{2}$ of the gradient estimators is $\\sim10^{5}$ times larger than the loss function and $\\sim10^{6}$ times larger than the parameter gradient. This relationship is stable over approximately ten orders of magnitude. Right: A ReLU network with two hidden layers of width 50 is trained by SGD to fti the Runge function $1/(1+x^{2})$ on equispaced data samples in the interval $[-8,8]$ . Also here, the variance in the gradient estimates is proportional to both the loss function and the magnitude of the gradient. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "If $N$ is large, it is computationally expensive to evaluate the gradient $\\begin{array}{r}{\\nabla\\mathcal{R}(w)=\\frac{1}{N}\\sum_{i=1}^{N}\\nabla\\ell_{i}}\\end{array}$ of the risk function $\\mathcal{R}$ exactly and we commonly resort to stochastic estimates ", "page_idx": 6}, {"type": "equation", "text": "$$\ng=\\frac{1}{n_{b}}\\sum_{i\\in I_{b}}\\nabla\\ell_{i}(w)=\\frac{1}{n_{b}}\\sum_{i\\in I_{b}}\\sum_{j=1}^{k}(\\partial_{h_{j}}\\ell)\\big(h(w,x_{i}),y_{i}\\big)\\,\\nabla_{w}h_{j}(w,x_{i}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $I_{b}\\subseteq\\{1,\\ldots,N\\}$ is a subsampled collection of $n_{b}$ data points (a batch or minibatch). Minibatch gradient estimates are very different from the stochasticity we encounter e.g. in statistical mechanics: ", "page_idx": 6}, {"type": "text", "text": "1. The covariance matrix $\\begin{array}{r}{\\Sigma=\\frac{1}{N}\\sum_{i=1}^{N}\\left(\\nabla\\ell_{i}-\\nabla\\mathcal{R}\\right)\\otimes\\left(\\nabla\\ell_{i}-\\nabla\\mathcal{R}\\right)}\\end{array}$ of the gradient estimators $\\nabla\\ell_{i}$ has low rank $N\\ll m$ . 2. Assume specifically that $\\ell$ is a loss function which satisfies $\\ell(y,y)=0$ for all $y\\in\\mathbb{R}^{k}$ , such as the popular $\\ell^{2}$ -loss function $\\ell(h,y)=\\|h\\!-\\!y\\|^{2}$ . Then ${\\nabla}\\ell_{i}(w)=0$ for all $i\\in\\{1,\\ldots,N\\}$ and all $\\bar{w}\\in\\mathcal{M}=\\mathcal{R}^{-1}(0)$ . In particular, minibatch gradient estimates are exact on $\\mathcal{M}$ . ", "page_idx": 6}, {"type": "text", "text": "The following Lemma makes the second observation precise in the overparameterized regime and bounds the stochasticity of mini-batch estimates more generally. ", "page_idx": 6}, {"type": "text", "text": "Lemma 8 (Noise intensity). Assume that $\\ell(h,y)=\\|h-y\\|^{2}$ and $h:\\mathbb{R}^{m}\\times\\mathbb{R}^{d}\\to\\mathbb{R}^{k}$ satisfies $\\|\\nabla_{w}h(w,x_{i})\\|^{2}\\leq C\\big(1+\\|w\\|\\big)^{p}$ for some $C,p>0$ and all $w\\in\\mathbb{R}^{m}$ and $i=1,\\ldots,N$ . Then for all $w\\in\\mathbb{R}^{m}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\sum_{i=1}^{N}\\left\\|\\nabla\\ell_{i}-\\nabla\\mathcal{R}\\right\\|^{2}\\,\\leq\\,4C^{2}\\,(1+\\|w\\|)^{2p}\\,\\mathcal{R}(w).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Lemma 8 is proved in Appendix H. It is a modification of [Wojtowytsch, 2023, Lemma 2.14] for function models which are locally, but not globally Lipschitz-continuous in the weights $w$ , such as deep neural networks with smooth activation function. The exponent $p$ may scale with network depth. ", "page_idx": 6}, {"type": "text", "text": "Lemma 8 describes the variance of a gradient estimator which uses a random index $i\\in\\{1,\\ldots,N\\}$ and the associated gradient $\\nabla\\ell_{i}$ is used to approximate $\\nabla\\mathcal{R}$ . If a batch $I_{b}$ of $n_{b}$ indices is selected randomly with replacement, then the variance of the estimates scales in the usual way: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}_{I_{b}}\\left[\\left\\lVert\\frac{1}{n_{b}}\\sum_{i\\in I_{b}}\\nabla\\ell_{i}-\\nabla\\mathcal{R}\\right\\rVert^{2}\\right]\\leq\\frac{4C^{2}(1+\\|w\\|)^{2p}}{n_{b}}\\,\\mathcal{R}(w).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "image", "img_path": "kHXUb494SY/tmp/b8022e8ee9bc9f47b07fb01721db6dc2866301c1bf4be3fc8bd399af4e7be7e8.jpg", "img_caption": ["Figure 3: We plot $\\mathbb{E}\\big[f_{d}(x_{n})\\big]$ on a loglog scale for SGD (blue), AGNES (red), NAG (green), ACDM (orange) and CNM (maroon) with $d=4$ (left) and $d=16$ (right) for noise levels $\\sigma\\,=\\,0$ (solid line), $\\sigma\\,=\\,10$ (dashed) and $\\sigma\\,=\\,50$ (dotted). The initial condition is $x_{0}\\,=\\,1$ in all simulations. Means are computed over 200 runs. After an initial plateau, AGNES, CNM and ACDM significantly outperform SGD in all settings, while NAG (green) diverges if $\\sigma$ is large. The length of the initial plateau increases with $\\sigma$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "As noted by Wu et al. [2019, 2022b], $\\mathcal{R}$ and $\\lVert\\nabla\\mathcal{R}\\rVert^{2}$ often behave similarly in overparametrized deep learning. We illustrate this in Figure 2 together with Lemma 8. Heuristically, we therefore replaced (4) by a more manageable assumption akin to $\\begin{array}{r l}{\\lefteqn{\\mathbb{E}[\\frac{1}{N}\\sum_{i=1}^{N}\\|\\nabla\\ell_{i}-\\nabla\\mathcal{R}\\|^{2}]\\leq\\sigma^{2}\\|\\nabla\\mathcal{R}\\|^{2}}}\\end{array}$ in Section 3.1. The setting where the signal-to-noise ratio (the quotient of estimate variance and true magnitude) is $\\Omega(1)$ is often referred to as \u2018multiplicative noise\u2019, as it resembles the noise generated by estimates of the form $g=(1+\\sigma Z)\\nabla\\mathcal{R}$ , where $Z\\sim\\mathcal{N}(0,1)$ . When the objective function is $L$ -smooth and satisfies a PL condition (see e.g. [Karimi et al., 2016]), both scaling assumptions are equivalent. ", "page_idx": 7}, {"type": "text", "text": "5 Numerical Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "5.1 Convex optimization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare the optimization algorithms for the family of objective functions ", "page_idx": 7}, {"type": "equation", "text": "$$\nf_{d}:\\mathbb{R}\\rightarrow\\mathbb{R},\\qquad f_{d}(x)={\\left\\{\\left|x\\right|^{d}}\\qquad\\qquad{\\mathrm{if~}}|x|<1\\right.}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for $d\\geq2$ with gradient estimators $g=(1+\\sigma N)f^{\\prime}(x)$ , where $N$ is a unit normal random variable. The functions are convex and their derivatives are Lipschitz-continuous with $L=d(d-1)$ . Various trajectories are compared for different values of $d$ and $\\sigma$ in Figure 3. We run AGNES with the parameters $\\begin{array}{r}{\\alpha=\\frac{\\eta^{\\mathrm{~\\!~-~}}}{1+\\sigma^{2}}}\\end{array}$ , $\\begin{array}{r}{\\eta=\\frac{1}{L(1+2\\sigma^{2})}}\\end{array}$ , $\\textstyle\\rho_{n}={\\frac{n}{n+5}}$ derived above and SGD with the optimal step size $\\begin{array}{r}{\\eta=\\frac{1}{L(1+\\sigma^{2})}}\\end{array}$ (see Lemmas 16 and 17). For NAG, we select $\\begin{array}{r}{\\eta=\\frac{1}{L(1+\\sigma^{2})}}\\end{array}$ and $\\textstyle\\rho_{n}={\\frac{n}{n+3}}$ . We present a similar experiment in the strongly convex case in Appendix A. ", "page_idx": 7}, {"type": "text", "text": "We additionally compare to two other methods of accelerated gradient descent which were recently proposed for multiplicative noise models: The ACDM method of Nesterov [2012], Vaswani et al. [2019], and the continuized Nesterov method (CNM) of Even et al. [2021], Berthier et al. [2021] with the proposed parameters. In this simple setting where all constants are known, AGNES, ACDM and CNM perform comparably in the long run and on average. ", "page_idx": 7}, {"type": "text", "text": "5.2 Neural network regression ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We generated $n=100,000\\;12{\\cdot}$ -dimensional random vectors. Using a fixed, randomly initialized neural network $f^{*}$ (with 10 hidden layers, each with width 10, and output dimension 1), we produced labels $y_{i}=f^{*}(x_{i})$ . The resulting dataset was split into $90\\%$ training and $10\\%$ testing data. We then trained identically initialized copies of a larger neural network (15 hidden layers, each with width 15) using Adam, NAG, SGD with momentum, and AGNES to minimize the mean-squared error (MSE) loss. ", "page_idx": 7}, {"type": "image", "img_path": "kHXUb494SY/tmp/8058f1628624d077f098685d87e577f2a38d8cd20da9c583c8aa10e221d83a43.jpg", "img_caption": ["Figure 4: We report the training loss as a running average with decay rate 0.99 (top row) and test loss (bottom row) for batch sizes 100 (left column), 50 (middle column), and 10 (right column) in the setting of Section 5.2. The horizontal axis represents the number of optimizer steps. The performance gap between AGNES and other algorithms widens for smaller batch sizes, where the gradient estimates are more stochastic and the two different parameters $\\alpha,\\eta$ add the most benefit. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We selected the learning rate $10^{-3}$ for Adam as it performed poorly at higher or lower rates $10^{-2}$ and $10^{-4}$ . For AGNES, NAG, and SGD, based on initial exploratory experiments, we used a learning rate of $10^{-4}$ , a momentum value of 0.99, and for AGNES, a correction step size $\\eta=10^{-3}$ . The experiment was repeated 10 times each for batch sizes 100, 50, and 10, and run for 45,000 optimizer steps each time. The average loss and standard deviation for each algorithm are reported in Figure 4. The results show that AGNES performs better than SGD and NAG for all batch sizes. With large batch size, Adam performs well with default hyperparameters. The performance of AGNES relative to other algorithms especially improves as the batch size decreases. ", "page_idx": 8}, {"type": "text", "text": "5.3 Image classification ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We trained ResNet-34 [He et al., 2016] with batch sizes 50 and 10, and ResNet-50 with batch size 50 on the CIFAR-10 image dataset [Krizhevsky et al., 2009] with standard data augmentation (normalization, random crop, and random flip) using Adam, SGD with momentum, NAG, and AGNES. The model implementations were based on [Liu, 2017]. Each algorithm was provided an identically initialized model and the experiment was repeated 5 times for 50 epochs each. The averages and standard deviations of training loss and test accuracy are reported in Figure 5. We used the same initial learning rate $10^{-3}$ for all the algorithms, which was dropped to $10^{-4}$ after 25 epochs. A momentum value of 0.99 was used for SGD, NAG, and AGNES and a constant correction step size $\\eta=10^{-2}$ was used for AGNES. ", "page_idx": 8}, {"type": "text", "text": "AGNES reliably outperforms SGD and NAG both in terms of training loss and test accuracy. The gap in performance appears to increase as model size increases or batch size decreases, suggesting that AGNES primarily excels in situations where gradients are harder to estimate accurately. For the sake of completeness, we include Adam with default hyperparameters as a comparison. ", "page_idx": 8}, {"type": "text", "text": "In congruence with convergence guarantees from convex optimization, grid search suggests that $\\alpha$ is the primary learning rate and $\\eta$ should be chosen larger than $\\alpha$ . We tried NAG and Adam with higher learning rates $10^{-\\bar{2}}$ and $10^{-1}$ as well to ensure a fair comparison with AGNES, but found that they become unstable or perform worse for larger learning rates in our experiments. The AGNES default parameters $\\alpha=10^{\\frac{\\circ}{-3}},\\eta=10^{-2}$ , $\\rho=0.99$ in Algorithm 1 give consistently strong performance on different models but can be further tuned to improve performance. While the numerical experiments we performed support our theoretical predictions, we acknowledge that our focus lies on theoretical guarantees and we did not test these predictions over a broad set of benchmark problems. ", "page_idx": 8}, {"type": "image", "img_path": "kHXUb494SY/tmp/5392fc401f577180df386f49ddd51540009b0d65d2a8e5fdd424bec233d7f926.jpg", "img_caption": ["Figure 5: We report the training loss as a running average with decay rate 0.99 (top row) and test accuracy (bottom row) for ResNet-34 trained on CIFAR-10 with batch sizes 50 (left column) and 10 (middle column), and ResNet-50 trained with batch size 50 (right column). The performance of AGNES with the proposed hyperparameters is stable over the changes in model and batch size. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "kHXUb494SY/tmp/14a48817645f51d24d40fe27417513228a9b177f3b43296e178fefc9244b5b52.jpg", "img_caption": ["Figure 6: We report the average training loss after each epoch for six epochs for training LeNet-5 on MNIST with AGNES for various combinations of the hyperparameters $\\alpha$ and $\\eta$ to illustrate that $\\alpha$ is the algorithm\u2019s primary learning rate. Left: For a given $\\alpha$ (color coded), the difference in the trajectory for the three values of $\\eta$ (line style) is marginal. On the other hand, choosing $\\alpha$ well significantly affects performance. Middle: For any given $\\alpha$ , the largest value of $\\eta$ performs much better than the other three values which have near-identical performance. Nevertheless, the worst performing value of $\\eta$ with well chosen $\\alpha=5\\cdot10^{-3}$ performs better than the best performing value of $\\eta$ with $\\alpha=5\\cdot10^{-4}$ . Right: When $\\alpha$ is too large, the loss increases irrespective of the value of $\\eta$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "We present a more thorough comparison of NAG and AGNES with various parameter selections in Figure 8 in Appendix A. With default parameters or minimal parameter tuning, AGNES reliably achieves superior performance compared to NAG (training loss) and smoother curves, suggesting more stable behavior (test accuracy). ", "page_idx": 9}, {"type": "text", "text": "5.4 Hyperparameter comparison ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We tried various combinations of AGNES hyperparameters $\\alpha$ and $\\eta$ to train LeNet-5 on the MNIST dataset to determine which hyperparameter has a greater impact on training. With a fixed batch size of 60 and a momentum value $\\rho=0.99$ , we trained independent copies of the model for 6 epochs for each combination of the hyperparameters. The average training loss over the epoch was recorded after each epoch. The results are reported in Figure 6. We see that $\\alpha$ has the largest impact on the rate of decay of the loss, which establishes it as the \u2018primary learning rage\u2019. If $\\alpha$ is too small, the algorithm converges slowly and if $\\alpha$ is too large, it diverges. If $\\alpha$ is chosen correctly, a good choice of the correction step size $\\eta$ (which can be orders of magnitude larger than $\\alpha$ ) further accelerates convergence, but $\\eta$ cannot compensate for a poor choice of $\\alpha$ . ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Portions of this research were conducted with the advanced computing resources provided by Texas A&M High Performance Research Computing. This research was also supported in part by the University of Pittsburgh Center for Research Computing, RRID:SCR_022735, through the resources provided. Specifically, this work used the H2P cluster, which is supported by NSF award number OAC-2117681. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Felipe Alvarez, Hedy Attouch, J\u00e9r\u00f4me Bolte, and Patrick Redont. A second-order gradient-like dissipative dynamical system with Hessian-driven damping: Application to optimization and mechanics. Journal de math\u00e9matiques pures et appliqu\u00e9es, 81(8):747\u2013779, 2002.   \nHedy Attouch, Zaki Chbani, Jalal Fadili, and Hassan Riahi. First-order optimization algorithms via inertial systems with hessian driven damping. Mathematical Programming, pages 1\u201343, 2022.   \nJ. F. Aujol, Ch. Dossal, and A. Rondepierre. Convergence rates of the heavy-ball method under the \u0142ojasiewicz property. Mathematical Programming, 2022a. doi: 10.1007/s10107-022-01770-2. URL https://doi.org/10.1007/s10107-022-01770-2.   \nJ.-F. Aujol, Ch. Dossal, and A. Rondepierre. Convergence rates of the heavy ball method for quasistrongly convex optimization. SIAM Journal on Optimization, 32(3):1817\u20131842, 2022b. doi: 10.1137/21M1403990. URL https://doi.org/10.1137/21M1403990.   \nRaef Bassily, Mikhail Belkin, and Siyuan Ma. On exponential convergence of SGD in non-convex over-parametrized learning. CoRR, abs/1811.02564, 2018. URL http://arxiv.org/abs/1811. 02564.   \nAmir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences, 2(1):183\u2013202, 2009.   \nMikhail Belkin. Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation. Acta Numerica, 30:203\u2013248, 2021.   \nMikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice and the classical bias\u2013variance trade-off. Proceedings of the National Academy of Sciences, 116(32):15849\u201315854, 2019.   \nMikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. SIAM Journal on Mathematics of Data Science, 2(4):1167\u20131180, 2020.   \nRapha\u00ebl Berthier, Francis Bach, Nicolas Flammarion, Pierre Gaillard, and Adrien Taylor. A continuized view on nesterov acceleration. arXiv preprint arXiv:2102.06035, 2021.   \nRaghu Bollapragada, Tyler Chen, and Rachel Ward. On the fast convergence of minibatch heavy ball momentum. arXiv preprint arXiv:2206.07553, 2022.   \nL\u00e9on Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. Siam Review, 60(2):223\u2013311, 2018.   \nAntonin Chambolle and Charles H Dossal. On the convergence of the iterates of FISTA. Journal of Optimization Theory and Applications, 166(3):25, 2015.   \nL\u00e9na\u00efc Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/ a1afc58c6ca9540d057299ec3016d726-Paper.pdf.   \nY. Cooper. The loss landscape of overparameterized neural networks, 2019. URL https:// openreview.net/forum?id=SyezvsC5tX.   \nAshok Cutkosky. Anytime online-to-batch, optimism and acceleration. In International conference on machine learning, pages 1446\u20131454. PMLR, 2019.   \nMarc Dambrine, Ch Dossal, B\u00e9n\u00e9dicte Puig, and Aude Rondepierre. Stochastic differential equations for modeling first order optimization methods. HAL preprint hal-03630785, 2022.   \nAlex Damian, Tengyu Ma, and Jason D. Lee. Label noise SGD provably prefers flat global minimizers. In Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/ forum?id $\\equiv$ x2TMPhseWAW.   \nJelena Diakonikolas and Michael I Jordan. Generalized momentum-based methods: A hamiltonian perspective. SIAM Journal on Optimization, 31(1):915\u2013944, 2021.   \nSimon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. arXiv:1810.02054 [cs.LG], 2018.   \nMathieu Even, Rapha\u00ebl Berthier, Francis Bach, Nicolas Flammarion, Pierre Gaillard, Hadrien Hendrikx, Laurent Massouli\u00e9, and Adrien Taylor. A continuized view on Nesterov acceleration for stochastic gradient descent and randomized gossip. arXiv preprint arXiv:2106.07644, 2021.   \nAlexander Vladimirovich Gasnikov and Yu E Nesterov. Universal method for stochastic composite optimization problems. Computational Mathematics and Mathematical Physics, 58:48\u201364, 2018.   \nSaeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization, 22(4):1469\u20131492, 2012.   \nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.   \nRobert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter Richt\u00e1rik. Sgd: General analysis and improved rates. In International conference on machine learning, pages 5200\u20135209. PMLR, 2019.   \nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \nMagnus R Hestenes and Eduard Stiefel. Methods of conjugate gradients for solving linear systems. Journal of research of the National Bureau of Standards, 49(6):409\u2013436, 1952.   \nPrateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerating stochastic gradient descent for least squares regression. In Conference On Learning Theory, pages 545\u2013604. PMLR, 2018.   \nRichard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the fokker\u2013 planck equation. SIAM Journal on Mathematical Analysis, 29(1):1\u201317, 1998. doi: 10.1137/ S0036141096303359. URL https://doi.org/10.1137/S0036141096303359.   \nPooria Joulani, Anant Raj, Andras Gyorgy, and Csaba Szepesv\u00e1ri. A simpler approach to accelerated optimization: iterative averaging meets optimism. In International conference on machine learning, pages 4984\u20134993. PMLR, 2020.   \nHamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximalgradient methods under the Polyak-Lojasiewicz condition. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 795\u2013811. Springer, 2016.   \nAli Kavis, Kfir Y Levy, Francis Bach, and Volkan Cevher. Unixgrad: A universal, adaptive algorithm with optimal guarantees for constrained optimization. Advances in neural information processing systems, 32, 2019.   \nRahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham Kakade. On the insufficiency of existing momentum schemes for stochastic optimization. In 2018 Information Theory and Applications Workshop (ITA), pages 1\u20139. IEEE, 2018.   \nDonghwan Kim and Jeffrey A Fessler. Another look at the fast iterative shrinkage/thresholding algorithm (fista). SIAM Journal on Optimization, 28(1):223\u2013250, 2018.   \nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \nA. Klenke. Probability Theory: A Comprehensive Course. Universitext. Springer London, 2013. ISBN 978-1-4471-5361-0.   \nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \nMaxime Laborde and Adam Oberman. A lyapunov analysis for accelerated gradient methods: from deterministic to stochastic case. In International Conference on Artificial Intelligence and Statistics, pages 602\u2013612. PMLR, 2020.   \nY. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998. doi: 10.1109/5.726791.   \nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436\u2013444, 2015.   \nKfir Y Levy, Alp Yurtsever, and Volkan Cevher. Online adaptive methods, universality and acceleration. Advances in neural information processing systems, 31, 2018.   \nZhiyuan Li, Tianhao Wang, and Sanjeev Arora. What happens after SGD reaches zero loss? \u2013a mathematical framework. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id $\\equiv$ siCt4xZn5Ve.   \nChaoyue Liu and Mikhail Belkin. Accelerating sgd with momentum for over-parameterized learning. arXiv preprint arXiv:1810.13395, 2018.   \nKuang Liu. Train cifar10 with pytorch. https://github.com/kuangliu/pytorch-cifar, 2017. [Online; accessed 16-May-2024].   \nIlya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam, 2018. URL https: //openreview.net/forum?id $=$ rk6qdGgCZ.   \nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019.   \nMichael Muehlebach and Michael Jordan. A dynamical systems perspective on nesterov acceleration. In International Conference on Machine Learning, pages 4656\u20134662. PMLR, 2019.   \nArkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on optimization, 19(4):1574\u2013 1609, 2009.   \nYurii Nesterov. A method for solving the convex programming problem with convergence rate $o(1/k^{2})$ . Dokl. Akad. Nauk SSSR, 269:543\u2013547, 1983.   \nYurii Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341\u2013362, 2012.   \nYurii Nesterov. Gradient methods for minimizing composite functions. Mathematical programming, 140(1):125\u2013161, 2013.   \nMark A Peletier. Variational modelling: Energies, gradient flows, and large deviations. arXiv preprint arXiv:1402.1990, 2014.   \nBoris T Polyak. Some methods of speeding up the convergence of iteration methods. Ussr computational mathematics and mathematical physics, 4(5):1\u201317, 1964.   \nHerbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400\u2013407, 1951. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 4430\u20134438. PMLR, 2018. URL http://proceedings.mlr.press/ v80/safran18a.html.   \nBin Shi, Simon S Du, Weijie Su, and Michael I Jordan. Acceleration via symplectic discretization of high-resolution differential equations. Advances in Neural Information Processing Systems, 32, 2019.   \nBin Shi, Simon S Du, Michael I Jordan, and Weijie J Su. Understanding the acceleration phenomenon via high-resolution differential equations. Mathematical Programming, pages 1\u201370, 2021.   \nJonathan W Siegel. Accelerated first-order methods: Differential equations and Lyapunov functions. arXiv preprint arXiv:1903.05671, 2019.   \nJonathan W Siegel and Stephan Wojtowytsch. A qualitative difference between gradient flows of convex functions in finite-and infinite-dimensional Hilbert spaces. arXiv preprint arXiv:2310.17610, 2023.   \nSebastian U. Stich. Unified Optimal Analysis of the (Stochastic) Gradient Method. arXiv e-prints, art. arXiv:1907.04232, July 2019. doi: 10.48550/arXiv.1907.04232.   \nSebastian U. Stich and Sai Praneeth Karimireddy. The error-feedback framework: Better rates for sgd with delayed gradients and compressed updates. J. Mach. Learn. Res., 21(1), jun 2022. ISSN 1532-4435.   \nWeijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling nesterov\u2019s accelerated gradient method: theory and insights. Advances in neural information processing systems, 27, 2014.   \nJaewook J Suh, Gyumin Roh, and Ernest K Ryu. Continuous-time analysis of accelerated gradient methods via conservation laws in dilated coordinate systems. In International Conference on Machine Learning, pages 20640\u201320667. PMLR, 2022.   \nIlya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pages 1139\u2013 1147. PMLR, 2013.   \nSharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of sgd for overparameterized models and an accelerated perceptron. In The 22nd international conference on artificial intelligence and statistics, pages 1195\u20131204. PMLR, 2019.   \nAndre Wibisono, Ashia C Wilson, and Michael I Jordan. A variational perspective on accelerated methods in optimization. proceedings of the National Academy of Sciences, 113(47):E7351\u2013E7358, 2016.   \nAshia C Wilson, Ben Recht, and Michael I Jordan. A lyapunov analysis of accelerated methods in optimization. The Journal of Machine Learning Research, 22(1):5040\u20135073, 2021.   \nStephan Wojtowytsch. Stochastic gradient descent with noise of machine learning type. Part II: Continuous time analysis. arXiv:2106.02588 [cs.LG], 2021.   \nStephan Wojtowytsch. Stochastic gradient descent with noise of machine learning type. Part I: Discrete time analysis. Journal of Nonlinear Science, 33, 2023.   \nLei Wu, Mingze Wang, and Weijie J Su. The alignment property of sgd noise and how it helps select flat minima: A stability analysis. In Advances in Neural Information Processing Systems, 2022a.   \nXiaoxia Wu, Simon S Du, and Rachel Ward. Global convergence of adaptive gradient methods for an over-parameterized neural network. arXiv preprint arXiv:1902.07111, 2019.   \nXiaoxia Wu, Yuege Xie, Simon Shaolei Du, and Rachel Ward. Adaloss: A computationally-efficient and provably convergent adaptive gradient method. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8691\u20138699, 2022b. ", "page_idx": 13}, {"type": "text", "text": "Jingzhao Zhang, Aryan Mokhtari, Suvrit Sra, and Ali Jadbabaie. Direct runge-kutta discretization achieves acceleration. Advances in neural information processing systems, 31, 2018. ", "page_idx": 14}, {"type": "text", "text": "Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven Hoi, and E. Weinan. Towards theoretically understanding why sgd generalizes better than adam in deep learning. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS\u201920, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546. ", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Additional experiments 17 ", "page_idx": 15}, {"type": "text", "text": "A.1 Numerical experiments for AGNES in smooth strongly convex optimization . . 17   \nA.2 Extensively comparing against NAG . . . 17 ", "page_idx": 15}, {"type": "text", "text": "B Multiple versions of the AGNES scheme 18 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Equivalence of the two formulations of AGNES 18   \nB.2 Equivalence of AGNES and MaSS 18 ", "page_idx": 15}, {"type": "text", "text": "C Continuous time interpretation of AGNES 20 ", "page_idx": 15}, {"type": "text", "text": "D Background material and auxiliary results 22 ", "page_idx": 15}, {"type": "text", "text": "D.1 A brief review of L-smoothness and (strong) convexity 22   \nD.2 Stochastic processes, conditional expectations, and a decrease property for SGD . . 23 ", "page_idx": 15}, {"type": "text", "text": "E Convergence proofs: convex case 25 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "E.1 Gradient Descent (GD) 25   \nE.2 AGNES and NAG 26 ", "page_idx": 15}, {"type": "text", "text": "F Convergence proofs: strongly convex case 34 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "F.1 Gradient Descent 34   \nF.2 AGNES and NAG . 35   \nF.3 On the role of momentum parameters 40 ", "page_idx": 15}, {"type": "text", "text": "G AGNES in non-convex optimization 41 ", "page_idx": 15}, {"type": "text", "text": "H Proof of Lemma 8: Scaling intensity of minibatch noise 42 ", "page_idx": 15}, {"type": "text", "text": "I Implementation aspects 43 ", "page_idx": 15}, {"type": "text", "text": "I.1 The last iterate . 43   \nI.2 Weight decay 44 ", "page_idx": 15}, {"type": "text", "text": "A Additional experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.1 Numerical experiments for AGNES in smooth strongly convex optimization ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We compare SGD and AGNES for the family of objective functions ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{\\mu,L}:\\mathbb{R}^{2}\\to\\mathbb{R},\\qquad f_{\\mu,L}(x)=\\frac{\\mu}{2}\\,x_{1}^{2}+\\frac{L}{2}\\,x_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We considered several stochastic estimators with multiplicative gradient scaling such as ", "page_idx": 16}, {"type": "text", "text": "\u2022 collinear noise $g=(1+\\sigma N)\\nabla f(x)$ , where $N$ is one-dimensional standard normal.   \n\u2022 isotropic noise $\\begin{array}{r}{g=\\nabla f(x)+\\frac{\\sigma\\,\\|\\nabla f(x)\\|}{\\sqrt{d}}\\,N}\\end{array}$ , where $N$ is a $d$ -dimensional unit Gaussian.   \n\u2022 Gaussian noise with standard variation $\\sigma\\|\\nabla f(x)\\|$ only in the direction orthogonal to $\\nabla f(x)$ .   \n\u2022 Gaussian no\u221aise with standard variation $\\sigma\\|\\nabla f(x)\\|$ only in the direction of the fixed vector $v=(1,1)/\\sqrt{2}$ .   \n\u2022 Noise of the form $\\nabla f(x)+{\\sqrt{1+\\sigma^{2}\\,\\|\\nabla f(x)\\|^{2}}}\\,N\\,v$ where $v=(1,1)/\\sqrt{2}$ and a variable $N$ which takes values 1 or $-1$ with probability $\\begin{array}{r}{\\frac{1}{2}\\;\\frac{\\sigma^{2}\\;\\|\\nabla f(x)\\|^{2}}{1+\\sigma^{2}\\;\\|\\nabla f(x)\\|^{2}}}\\end{array}$ each; $N=0$ otherwise. In this setting, the noise remains macroscopically large at the global minimum, but the probability of encountering noise becomes small. ", "page_idx": 16}, {"type": "text", "text": "Numerical results were found to be comparable for all settings on a long time-scale, but the geometry of trajectories may change in the early stages of optimization depending on the noise structure. ", "page_idx": 16}, {"type": "text", "text": "For collinear and isotropic noise, the results obtained for $f_{\\mu,L}$ on $\\mathbb{R}^{2}$ were furthermore found comparable (albeit not identical) to simulations with a quadratic form on $\\mathbb{R}^{d}$ with $d=10$ and ", "page_idx": 16}, {"type": "text", "text": "\u2022 $(d-1)$ eigenvalues $=\\mu$ and one eigenvalue $=L$ \u2022 $(d-1)$ eigenvalues $=L$ and one eigenvalue $=\\mu$ \u2022 eigenvalues equi-spaced between $\\mu$ and $L$ . ", "page_idx": 16}, {"type": "text", "text": "The evolution of $\\mathbb{E}[f(x_{n})]$ for different values of $\\sigma$ and $L\\ge\\mu\\equiv1$ is considered for both SGD and AGNES in Figure 7. ", "page_idx": 16}, {"type": "text", "text": "dTehrei voebd jaebctoivvee  ffour nActGioNnEs Sa raen $\\mu=1$ -pctiomnavle sxt eapn sdi $L$ - fuosre  SthGeD o. pTtihem aml epaanr aofm tehtee rosb $\\alpha,\\eta,\\rho$ d the o ze \u03b7 =L(11+\u03c32) value at each iteration is computed over 1,000 samples for each optimizer. ", "page_idx": 16}, {"type": "text", "text": "A.2 Extensively comparing against NAG ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We ran additional experiments testing a much wider range of hyperparameters for NAG for the task of classifying images from CIFAR-10. The results, presented in Figure 8 indicate that AGNES outperforms NAG with little fine-tuning of the hyperparameters. ", "page_idx": 16}, {"type": "text", "text": "We trained ResNet-34 using batch size of 50 for 40 epochs using NAG with learning rate in $\\{8\\cdot10^{-5},10^{-4},2\\cdot10^{-4},5\\cdot^{-}10^{-4},8\\cdot10^{-4},10^{-3},2\\cdot10^{-5},5\\cdot10^{-5},8\\cdot10^{-3},$ $10^{-2},2\\cdot\\bar{1}0^{-2},5\\ \\cdot$ $10^{-2},8\\cdot10^{-2},10^{-1},2\\cdot10^{-1},5\\cdot10^{-1}\\}$ and momentum value in $\\{0.2,0.5,0.8,0.9,0.99\\}$ . These 80 combinations of hyperparameters for NAG were compared against AGNES with the default hyperparameters suggested $\\alpha=10^{-3}$ (learning rate), $\\eta\\doteq10^{-2}$ (correction step), and $\\rho=0.99$ (momentum) as well as AGNES with a slightly smaller learning rate $5\\cdot10^{-4}$ (with the other two hyperparameters being the same). ", "page_idx": 16}, {"type": "text", "text": "AGNES consistently achieved a lower training loss as well as a better test accuracy faster than any combination of NAG hyperparameters tested. The same random seed was used each time to ensure a fair comparison between the optimizers. Overall, AGNES remained more stable and while other versions of NAG occasionally achieved a higher classification accuracy in certain epochs. ", "page_idx": 16}, {"type": "image", "img_path": "kHXUb494SY/tmp/c6dd70ec42ab9dd5a123c1d1182832841511cb36c3b76589ca0f393ca6747ded.jpg", "img_caption": ["Figure 7: We compare AGNES (red) and SGD (blue) for the optimization of $f_{\\mu,L}$ with $\\mu=1$ and $L=500$ (left) / $L=10^{4}$ (right) for different noise levels $\\sigma=0$ (solid line), $\\sigma=10$ (dashed) and $\\sigma=50$ (dotted). In all cases, AGNES improves significantly upon SGD. The noise model used is isotropic Gaussian, but comparable results are obtained for different versions of multiplicatively scaling noise. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "B Multiple versions of the AGNES scheme ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Equivalence of the two formulations of AGNES ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma 9. The two formulations of the AGNES time-stepping scheme (2) and (3) produce the same sequence of points. ", "page_idx": 17}, {"type": "text", "text": "Proof. We consider the three-step formulation (3), ", "page_idx": 17}, {"type": "equation", "text": "$$\nv_{0}=0,\\quad x_{n}^{\\prime}=x_{n}+\\alpha v_{n},\\qquad x_{n+1}=x_{n}^{\\prime}-\\eta g_{n}^{\\prime},\\qquad v_{n+1}=\\rho_{n}\\bigl(v_{n}-g_{n}^{\\prime}\\bigr),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and use it to derive (2) by eliminating the velocity variable $v_{n}$ . If $v_{0}=0$ , then $x_{0}^{\\prime}=x_{0}$ . From the definition $x_{n}^{\\prime}$ , we get $\\alpha v_{n}=x_{n}^{\\prime}-x_{n}$ . Substituting this into the definition of $v_{n+1}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\nv_{n+1}=\\rho_{n}\\left(\\frac{x_{n}^{\\prime}-x_{n}}{\\alpha}-g_{n}^{\\prime}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then using this expression for $v_{n+1}$ to compute $x_{n+1}^{\\prime}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{x_{n+1}^{\\prime}=x_{n+1}+\\alpha v_{n+1}}}\\\\ {{=x_{n+1}+\\alpha\\rho_{n}\\left(\\displaystyle\\frac{x_{n}^{\\prime}-x_{n}}{\\alpha}-g_{n}^{\\prime}\\right)}}\\\\ {{=x_{n+1}+\\rho_{n}(x_{n}^{\\prime}-\\alpha g_{n}^{\\prime}-x_{n}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Together with the definition of $x_{n+1}$ and the initialization $x_{0}^{\\prime}\\,=\\,x_{0}$ , this is exactly the two-step formulation (2) of AGNES. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "B.2 Equivalence of AGNES and MaSS ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "After the completion of this work, we learned of Liu and Belkin [2018]\u2019s Momentum-Added Stochastic Solver (MaSS) method, which generates sequences according to the iteration ", "page_idx": 17}, {"type": "equation", "text": "$$\nx_{n+1}=x_{n}^{\\prime}-\\eta_{1}g_{n}^{\\prime},\\qquad x_{n+1}^{\\prime}=(1+\\gamma)x_{n+1}-\\gamma x_{n}+\\eta_{2}g_{n}^{\\prime}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $g_{n}^{\\prime}$ is an estimate for $\\nabla f(x_{n}^{\\prime})$ . This is a version of AGNES with the choice $\\eta=\\eta_{1}$ and the momentum step ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{n+1}^{\\prime}=x_{n+1}+\\rho(x_{n}^{\\prime}-\\alpha g_{n}^{\\prime}-x_{n})}\\\\ &{\\qquad=x_{n+1}+\\rho(x_{n+1}+\\eta g_{n}^{\\prime}-\\alpha g_{n}^{\\prime}-x_{n})}\\\\ &{\\qquad=(1+\\rho)x_{n+1}-\\rho x_{n}+(\\eta-\\alpha)\\rho g_{n}^{\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "i.e. MaSS coincides with AGNES for $\\gamma=\\rho$ and $\\eta_{2}=(\\eta-\\alpha)\\rho$ . ", "page_idx": 17}, {"type": "image", "img_path": "kHXUb494SY/tmp/d7c0afbe526a4e74ada5f25fde4efa7a37e05b9efe5fcdaa0f023698559ccca1.jpg", "img_caption": ["Figure 8: We trained ResNet34 on CIFAR-10 with batch size 50 for 40 epochs using NAG. Training losses are reported as a running average with decay rate 0.999 in the left column and test accuracy after every epoch is reported in the right column. Each row represents a specific value of momentum used for NAG (from top to bottom: 0.99, 0.9, 0.8, 0.5, and 0.2) with learning rates ranging from $8\\cdot10^{-5}$ to 0.5. These hyperparameter choices for NAG were compared against AGNES with the default hyperparameters suggested $\\alpha=10^{-3}$ (learning rate), $\\eta=10^{-2}$ (correction step), and $\\rho=0.99$ (momentum) as well as AGNES with a slightly smaller learning rate $5\\cdot10^{-4}$ (with $\\rho=0.99$ , $\\eta=10^{-2}$ as well). The same two training trajectories with AGNES are shown in all the plots in shades of blue. The horizontal axes represent the number of optimizer steps. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "C Continuous time interpretation of AGNES ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For better interpretability, we consider the continuous time limit of the AGNES algorithm. Similar ODE analyses of accelerated first order methods have been considered by many authors, including Su et al. [2014], Siegel [2019], Wilson et al. [2021], Attouch et al. [2022], Aujol et al. [2022a], Zhang et al. [2018], Dambrine et al. [2022]. ", "page_idx": 19}, {"type": "text", "text": "Consider the time-stepping scheme ", "page_idx": 19}, {"type": "equation", "text": "$$\nv_{0}=0,\\quad x_{n}^{\\prime}=x_{n}+\\gamma_{1}v_{n},\\qquad x_{n+1}=x_{n}^{\\prime}-\\eta g_{n}^{\\prime},\\qquad v_{n+1}=\\rho_{n}(v_{n}-\\gamma_{2}g_{n}^{\\prime}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which reduces to AGNES as in (3) with the choice of parameters $\\gamma_{1}=\\alpha,\\gamma_{2}=1$ . For the derivat\u221aion of continuous time dynamics, we show that the same scheme arises with the choice $\\gamma_{1}=\\gamma_{2}=\\sqrt{\\alpha}$ . ", "page_idx": 19}, {"type": "text", "text": "Lemma 10. Let $\\rho\\in(0,1)$ and $\\eta>0$ parameters. Assume that $\\gamma_{1},\\gamma_{2}$ and $\\widetilde{\\gamma}_{1},\\widetilde{\\gamma}_{2}$ are parameters such that $\\tilde{\\gamma}_{1}\\tilde{\\gamma}_{2}=\\gamma_{1}\\gamma_{2}$ . Consider the sequences $(\\tilde{x}_{n},\\tilde{x}_{n}^{\\prime},\\tilde{v}_{n})$ and $(x_{n},x_{n}^{\\prime},v_{n})$ generated by the time stepping scheme (5) with parameters $(\\rho,\\eta,\\tilde{\\gamma}_{1},\\tilde{\\gamma}_{2})$ and $(\\rho,\\eta,\\gamma_{1},\\gamma_{2})$ respectively. If $x_{0}=\\tilde{x}_{0}$ and $\\gamma_{1}v_{0}=\\tilde{\\gamma}_{1}\\,\\tilde{v}_{0}$ , then $x_{n}=\\tilde{x}_{n}$ , $x_{n}^{\\prime}=\\tilde{x}_{n}^{\\prime}$ and $\\tilde{\\gamma}_{1}\\,\\tilde{v}_{n}=\\gamma_{1}\\,v_{n}$ for all $n\\in\\mathbb N$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. We proceed by mathematical induction on $n$ . For $n=0$ , the claim holds by the hypotheses of the lemma. For the inductive hypothesis, we suppose that $x_{n}=\\tilde{x}_{n}$ and $\\gamma_{1}v_{n}=\\tilde{\\gamma}_{1}\\tilde{v}_{n}$ and prove the claim for $n+1$ . Note that since $x_{n}^{\\prime}=x_{n}+\\gamma_{1}v_{n}$ , it automatically follows that $x_{n}^{\\prime}=\\tilde{x}_{n}^{\\prime}$ . This implies that ", "page_idx": 19}, {"type": "equation", "text": "$$\nx_{n+1}=x_{n}^{\\prime}-\\eta g_{n}^{\\prime}=\\tilde{x}_{n}^{\\prime}-\\eta g_{n}^{\\prime}=\\tilde{x}_{n+1}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Considering the velocity term, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma_{1}v_{n+1}=\\rho_{n}\\big(\\gamma_{1}v_{n}-\\gamma_{1}\\gamma_{2}g_{n}^{\\prime}\\big)=\\rho_{n}\\big(\\tilde{\\gamma}_{1}\\tilde{v}_{n}-\\tilde{\\gamma}_{1}\\tilde{\\gamma}_{2}g_{n}^{\\prime}\\big)=\\tilde{\\gamma}_{1}\\rho_{n}\\big(\\tilde{v}_{n}-\\tilde{\\gamma}_{2}g_{n}^{\\prime}\\big)=\\tilde{\\gamma}_{1}\\tilde{v}_{n+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus $x_{n+1}=\\tilde{x}_{n+1}$ and $\\gamma_{1}v_{n+1}=\\tilde{\\gamma}_{1}\\tilde{v}_{n+1}$ . The induction can therefore be continued. ", "page_idx": 19}, {"type": "text", "text": "Consider the choice of parameters in Theorem 4 by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\eta=\\frac{1}{L(1+\\sigma^{2})},\\quad\\alpha=\\frac{1-\\sqrt{\\mu/L}}{1-\\sqrt{\\mu/L}+\\sigma^{2}}\\eta\\approx\\frac{\\eta}{1+\\sigma^{2}},\\quad\\rho=\\frac{\\sqrt{L}(1+\\sigma^{2})-\\sqrt{\\mu}}{\\sqrt{L}(1+\\sigma^{2})+\\sqrt{\\mu}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "if $\\mu\\ll L$ . We denote $\\begin{array}{r}{h:=\\frac{1}{\\sqrt{L}(1+\\sigma^{2})}}\\end{array}$ and note that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\gamma_{1}=\\gamma_{2}={\\sqrt{\\alpha}}\\approx h,\\qquad\\eta=(1+\\sigma^{2})h^{2}={\\frac{h}{\\sqrt{L}}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\rho=1-2\\frac{\\sqrt{\\mu}}{\\sqrt{L}(1+\\sigma^{2})+\\sqrt{\\mu}}=1-2\\sqrt{\\mu}\\,\\frac{\\sqrt{L}(1+\\sigma^{2})}{\\sqrt{L}(1+\\sigma^{2})+\\sqrt{\\mu}}h\\approx1-2\\sqrt{\\mu}h.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Depending on which interpretation of $\\eta$ we select, we obtain a different continuous time limit. First, consider the deterministic case $\\sigma=0$ . Then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\binom{x_{n+1}}{v_{n+1}}=\\binom{x_{n}}{v_{n}}+h\\left(-2\\sqrt{\\mu}\\,v_{n}-(1-\\sqrt{\\mu}h)\\nabla f(x_{n}+h v_{n})\\right)}\\\\ &{\\qquad\\qquad=\\binom{x_{n}}{v_{n}}+h\\left(-2\\sqrt{\\mu}\\,v_{n}-(1-\\sqrt{\\mu}h)\\nabla f(x_{n}+h v_{n})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Keeping $f$ fixed and taking $h\\to0$ , this is a time-stepping scheme for the coupled ODE system ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\binom{\\dot{x}}{\\dot{v}}=\\binom{v}{-2\\sqrt{\\mu}\\,v-\\nabla f(x)}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Differentiating the first equation and using the system ODEs to subsequently eliminate $v$ from the expression, we observe that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\ddot{x}=\\dot{v}=-2\\sqrt{\\mu}\\,v-\\nabla f(x)=-2\\sqrt{\\mu}\\,\\dot{x}-\\nabla f(x),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "i.e. we recover the heavy ball ODE. The alternative interpretation $\\eta\\,=\\,h/\\sqrt{L}$ can be analized equivalently and leads to a system ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\binom{\\dot{x}}{\\dot{v}}=\\left(\\begin{array}{l}{{v-\\frac{1}{\\sqrt{L}}\\,\\nabla f(x)}}\\\\ {{-2\\sqrt{\\mu}\\,v-\\nabla f(x)}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which corresponds to a second order ODE ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\ddot{x}=-\\dot{v}-\\frac{1}{\\sqrt{L}}D^{2}f(x)\\,\\dot{x}}\\\\ {\\displaystyle\\quad=-2\\sqrt{\\mu}v-\\nabla f(x)-\\frac{1}{\\sqrt{L}}D^{2}f(x)\\,\\dot{x}}\\\\ {\\displaystyle\\quad=-2\\sqrt{\\mu}\\,\\left(\\dot{x}+\\frac{1}{\\sqrt{L}}\\nabla f(x)\\right)-\\nabla f(x)-\\frac{1}{\\sqrt{L}}D^{2}f(x)\\,\\dot{x}}\\\\ {\\displaystyle\\quad=-\\left(2\\sqrt{\\mu}\\,I_{m\\times m}+\\frac{1}{\\sqrt{L}}\\,D^{2}f(x)\\right)\\dot{x}-\\left(1+2\\,\\sqrt{\\frac{\\mu}{L}}\\right)\\nabla f(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This continuum limit is not a simple heavy-ball ODE, but rather a system with adaptive friction modelled by Hessian damping. A related Newton/heavy ball hybrid dynamical system was studied in greater detail by Alvarez et al. [2002]. For $L$ -smooth functions, the $\\ell^{2}$ -operator \u221anorm of $D^{2}f(x)$ satisfies $\\|D^{2}f(x)\\|\\;\\leq\\;L$ , i.e. the additional friction term can be as large as $\\sqrt{L}$ in directions corresponding to high eigenvalues of the Hessian. This provides significant regularization in directions which would otherwise be notably underdamped. ", "page_idx": 20}, {"type": "text", "text": "Following Appendix F.3, we maintain that the scaling ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\eta(1-\\rho)}{\\alpha}=2\\,\\sqrt{\\frac{\\mu}{L}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "is \u2018natural\u2019 as we vary $\\eta,\\alpha,\\rho$ . The same fixed ratio is maintained for the scaling choice $\\eta=h/\\sqrt{L}$ as ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\frac{\\eta(1-\\rho)}{\\alpha}}={\\frac{h/{\\sqrt{L}}\\cdot2{\\sqrt{\\mu}}\\,h}{h^{2}}}=2{\\sqrt{\\frac{\\mu}{L}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Indeed, numerical experiments in Section A suggest that such regularization may be observed in practice as high eigenvalues in the quadratic map do not exhibit \u2018underdamped\u2019 behavior. We therefore believe that Hessian dampening is the potentially more instructive continuum description of AGNES. A similar analysis can be conducted in the stochastic case with the scaling ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\gamma_{1}=\\gamma_{2}=h,\\quad\\eta\\in\\left\\{(1+\\sigma^{2})h^{2},\\,\\frac{h}{\\sqrt{L}}\\right\\},\\quad\\rho=1-2\\sqrt{\\mu}h\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for large $\\sigma$ . We incorporate noise as ", "page_idx": 20}, {"type": "equation", "text": "$$\ng_{n}^{\\prime}=(1+\\sigma N_{n})\\nabla f(x_{n}^{\\prime})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and write ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\binom{x_{n+1}}{v_{n+1}}=\\binom{x_{n}}{v_{n}}+h\\left(-2\\sqrt{\\mu}\\,v_{n}-(1-\\sqrt{\\mu}h)\\big(1+\\sigma N_{n}\\big)\\nabla f(x_{n}+h v_{n})\\right)}\\\\ &{\\qquad\\qquad=\\binom{x_{n}}{v_{n}}+h\\left(-2\\sqrt{\\mu}\\,v_{n}-\\nabla f(x_{n})-\\sqrt{h}\\,\\frac{\\sigma\\sqrt{h}}{2}\\,N_{n}\\nabla f(x)\\right)+O(h^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which can be viewed as an approximation of the coupled ODE/SDE system ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\binom{\\mathrm{d}x}{\\mathrm{d}v}=\\binom{v\\,\\mathrm{d}t}{\\big(-2\\sqrt{\\mu}\\,v-\\nabla f(x)\\big)\\,\\mathrm{d}t+\\sigma\\sqrt{h}\\,\\mathrm{d}B\\cdot\\nabla f(x)}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "under moment bounds on the noise $N_{n}$ . The precise noise type depends on the assumptions on the covariance structure of $N_{n}$ \u2013 noise can point only in gradient direction or be isotropic on the entire space. For small $h$ , the dynamics become deterministic. Again, an alternative continuous time limit is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\binom{\\mathrm{d}x}{\\mathrm{d}v}=\\binom{\\left(v-\\nabla f(x)/\\sqrt{L}\\right)\\mathrm{d}t+\\frac{\\sigma\\sqrt{h}}{\\sqrt{L}}\\,\\mathrm{d}B\\cdot\\nabla f(x)}{\\left(\\big-2\\sqrt{\\mu}\\,v-\\nabla f(x)\\right)\\mathrm{d}t+\\sigma\\sqrt{h}\\,\\mathrm{d}B\\cdot\\nabla f(x)}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "if $\\eta$ is scaled towards zero as $h/\\sqrt{L}$ . The first limiting structure is recoverd\u221a in the limit $L\\rightarrow\\infty$ . Notably, the noise in the first equation is expected to be non-negligible if $\\sigma\\gg\\sqrt{L}$ . A similar analysis can be conducted in the convex case, noting that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{n+n_{0}}{n+n_{0}+3}=1-\\frac{3}{n+n_{0}+3}=1-\\frac{3}{(n+n_{0}+3)h}\\,h\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $(n+n_{0}+3)h$ roughly corresponds to the time $t$ in the continuous time setting. ", "page_idx": 21}, {"type": "text", "text": "D Background material and auxiliary results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this appendix, we gather a few auxiliary results that will be used in the proofs below. We believe that these will be familiar to the experts and can be skipped by experienced readers. ", "page_idx": 21}, {"type": "text", "text": "D.1 A brief review of L-smoothness and (strong) convexity ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Recall that if a function $f$ is $L$ -smooth, then ", "page_idx": 21}, {"type": "equation", "text": "$$\nf(y)\\leq f(x)+\\nabla f(x)\\cdot(y-x)+{\\frac{L}{2}}\\|x-y\\|^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For convex functions, this is in fact equivalent to $\\nabla f$ being $L$ -Lipschitz. ", "page_idx": 21}, {"type": "text", "text": "Lemma 11. If $f$ is convex and differentiable and satisfies (6), then $\\|\\nabla f(x)-\\nabla f(y)\\|\\leq L\\|x-y\\|$ for all $x$ and $y$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Setting $\\begin{array}{r}{y=x-\\frac{1}{L}\\nabla f(x),}\\end{array}$ in (6) implies that $\\begin{array}{r}{f(x)-\\operatorname*{inf}_{z}f(z)\\geq\\frac{1}{2L}\\|\\nabla f(x)\\|_{2}^{2}}\\end{array}$ . Applying this to the modified function $f_{y}(x)=f(x)-\\nabla f(y)\\cdot(x-y)$ , which is still convex and satifies (6), we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{y}(x)-\\operatorname*{inf}_{\\boldsymbol{z}}f_{y}(\\boldsymbol{z})=f_{y}(x)-f_{y}(y)=f(x)-\\nabla f(y)\\cdot(x-y)-f(y)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\geq\\displaystyle\\frac{1}{2L}\\|\\nabla f_{y}(x)\\|^{2}=\\frac{1}{2L}\\|\\nabla f(x)-\\nabla f(y)\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that here we have used the convexity to conclude that $\\operatorname*{inf}_{z}f_{y}(z)\\,=\\,f_{y}(y)$ , i.e. that $f_{y}$ is minimized at $y$ , since by construction $\\nabla f_{y}(y)=0$ (this is the only place where we use convexity!). Swapping the role of $x$ and $y$ , adding these inequalities, and applying Cauchy-Schwartz we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{L}\\|\\nabla f(x)-\\nabla f(y)\\|^{2}\\leq(\\nabla f(x)-\\nabla f(y))\\cdot(x-y)\\leq\\|\\nabla f(x)-\\nabla f(y)\\|\\|x-y\\|,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which implies the result. ", "page_idx": 21}, {"type": "text", "text": "From the first order strong convexity condition, ", "page_idx": 21}, {"type": "equation", "text": "$$\nf(y)\\geq f(x)+\\nabla f(x)\\cdot(y-x)+{\\frac{\\mu}{2}}\\|x-y\\|^{2},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "we deduce the more useful formulation $\\begin{array}{r}{\\nabla f(x)\\cdot(x-y)\\geq f(x)-f(y)+\\frac{\\mu}{2}\\,\\|x-y\\|^{2}.}\\end{array}$ . The convex case arises as the special case $\\mu=0$ . We note a special case of these conditions when one of the points is a minimizer of $f$ . ", "page_idx": 21}, {"type": "text", "text": "Lemma 12. If $f$ is an $L$ -smooth function and $x^{*}$ is a point such that $f(x^{*})=\\operatorname*{inf}_{x\\in\\mathbb{R}^{m}}f(x)$ then for any $x\\in\\mathbb{R}^{m}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\nf(x)-f(x^{*})\\leq\\frac{L}{2}\\left\\|x-x^{*}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Similarly, if $f$ is differentiable and $\\mu$ -strongly convex then for any $x\\in\\mathbb{R}^{m}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\frac{\\mu}{2}}\\left\\|x-x^{*}\\right\\|^{2}\\leq f(x)-f(x^{*}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. This follows from the two first order conditions stated above by noting that $\\nabla f(x^{*})=0$ if $x^{*}$ is a minimizer of $f$ . \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Additionally, $L$ -smooth functions which are bounded from below satisfy the inequality ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\nabla f\\|^{2}\\leq2L\\,(f-\\operatorname*{inf}f).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Intuitively, if the gradient is large at a point, then we reduce $f$ quickly by walking in the gradient direction. The $L$ -smoothness condition prevents the gradient from decreasing quickly along our path. Thus if the gradient is larger than a threshold at a point where $f$ is close to inf $f$ , then the inequality $f\\geq\\operatorname{inf}f$ would be violated. ", "page_idx": 22}, {"type": "text", "text": "Let us record a modified gradient descent estimate, which is used only in the non-convex case. The difference to the usual estimate is that the gradient is evaluated at the terminal point of the interval rather than the initial point. ", "page_idx": 22}, {"type": "text", "text": "Lemma 13. For any $x,v$ and $\\alpha$ : If $f$ is $L$ -smooth, then ", "page_idx": 22}, {"type": "equation", "text": "$$\nf(x+\\alpha v)\\leq f(x)+\\alpha\\nabla f(x+\\alpha v)\\cdot v+\\frac{L\\alpha^{2}}{2}\\|v\\|^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that if $f$ is convex, this follows immediately from (6) and the convexity condition $(\\nabla f(y)-$ $\\nabla f(x))\\cdot(y-x)\\geq0$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. The proof is essentially identical to the standard decay estimate. We compute ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle f({\\boldsymbol x})=f({\\boldsymbol x}+\\alpha{\\boldsymbol v})-\\int_{0}^{\\alpha}\\frac{d}{d t}f({\\boldsymbol x}+t{\\boldsymbol v})\\mathrm{d}t}\\\\ {\\displaystyle\\qquad=f({\\boldsymbol x}+\\alpha{\\boldsymbol v})-\\int_{0}^{\\alpha}\\left[\\nabla f({\\boldsymbol x}+\\alpha{\\boldsymbol v})+\\left\\{\\nabla f({\\boldsymbol x}+t{\\boldsymbol v})-\\nabla f({\\boldsymbol x}+\\alpha{\\boldsymbol v})\\right\\}\\right]\\cdot{\\boldsymbol v}\\,\\mathrm{d}t}\\\\ {\\displaystyle\\qquad\\geq f({\\boldsymbol x}+\\alpha{\\boldsymbol v})-\\nabla f({\\boldsymbol x}+\\alpha{\\boldsymbol v})\\cdot{\\boldsymbol v}-\\int_{0}^{\\alpha}L\\left(\\alpha-t\\right)\\|{\\boldsymbol v}\\|^{2}\\,\\mathrm{d}t}\\\\ {\\displaystyle\\qquad=f({\\boldsymbol x}+\\alpha{\\boldsymbol v})-\\alpha\\,\\nabla f({\\boldsymbol x}+\\alpha{\\boldsymbol v})\\cdot{\\boldsymbol v}-\\frac{L\\alpha^{2}}{2}\\|{\\boldsymbol v}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "D.2 Stochastic processes, conditional expectations, and a decrease property for SGD ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Now, we turn towards a very brief review of the stochastic process theory used in the analysis of gradient descent type algorithms. Recall that $(\\Omega,A,\\mathbb{P})$ is a probablity space from which we draw elements $\\omega_{n}$ for gradient estimates $g(x_{n}^{\\prime},\\omega_{n})$ (AGNES) or $g(x_{n},\\omega_{n})$ (SGD). We consider $x_{0}$ as a random variable on $\\mathbb{R}^{m}$ with law $\\mathbb{Q}$ . Let us introduce the probability space $(\\widehat{\\Omega},\\widehat{A},\\widehat{\\mathbb{P}})$ where ", "page_idx": 22}, {"type": "text", "text": "1. $\\begin{array}{r}{\\widehat\\Omega=\\mathbb R^{d}\\times\\prod_{n\\in\\mathbb N}\\Omega,}\\end{array}$   \n2. $\\widehat{A}$ is the cylindrical/product $\\sigma$ -algebra on $\\widehat{\\Omega}$ , and   \n3. $\\widehat{\\mathbb{P}}=\\mathbb{Q}\\times\\otimes\\mathbb{P}$ . ", "page_idx": 22}, {"type": "text", "text": "The product $\\sigma$ -algebra and product measure are objects suited to events which are defined using only finitely many variables in the product space. A more detailed introduction can be found in [Klenke, 2013, Example 1.63]. We furthermore define the filtration $\\{\\mathcal{F}_{n}\\}_{n\\in\\mathbb{N}}$ where ${\\mathcal{F}}_{n}$ is the $\\sigma$ -algebra generated by sets of the form ", "page_idx": 22}, {"type": "equation", "text": "$$\nB\\times\\prod_{i=1}^{n}A_{i}\\times\\prod_{i\\in\\mathbb{N}}\\Omega,\\qquad B\\subseteq\\mathbb{R}^{m}\\;{\\mathrm{Borel}},\\quad A_{i}\\in{\\mathcal{A}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In particular, $\\bigcup_{n\\in\\mathbb{N}}\\mathcal{F}_{n}\\ \\subseteq\\ \\sigma\\left(\\bigcup_{n\\in\\mathbb{N}}\\mathcal{F}_{n}\\right)\\ =\\ \\widehat{\\mathcal{A}}$ and, examining the time-stepping scheme, it is immediately apparent that $x_{n},x_{n}^{\\prime},v_{n}$ are ${\\mathcal{F}}_{n}$ -measurable random variables on $\\widehat{\\Omega}$ . In particular, they are $\\boldsymbol{\\mathcal{A}}$ -measurable. Alternatively, we can consider ${\\mathcal{F}}_{n}$ as the $\\sigma$ -algebra generated by the random variables $x_{1},x_{1}^{\\prime},\\ldots,x_{n},x_{n}^{\\prime}$ , i.e. all the information that is known after intialization and taking $n$ gradient steps. All probabilities in the main article are with respect toP. ", "page_idx": 22}, {"type": "text", "text": "Recall that conditional expectations are a technical tool to capture the stochasticity in a random variable $X$ which can be predicted from another random quantity $Y$ . This allows us to quantify the randomness in the gradient estimators $g_{n}^{\\prime}$ which comes from the fact that $x_{n}$ is a random variable (not known ahead of time) and which randomness comes from the fact that on top of the inherent randomness due to e.g. initialization, we do not compute exact gradients. In particular, even at run time when $x_{n}$ is known, there is additional noise in the estimators $g_{n}^{\\prime}$ in our setting due to the selection of $\\omega_{n}$ . ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "In the next Lemma, we recall two important properties of conditional expectations. ", "page_idx": 23}, {"type": "text", "text": "Lemma 14. [Klenke, 2013, Theorem 8.14] Let g and h be $\\boldsymbol{\\mathcal{A}}$ -measurable random variables on a probability space $(\\Omega,A,\\mathbb{P})$ and ${\\mathcal{F}}\\subseteq A$ be a $\\sigma-,$ algebra. Then the conditional expectations $\\mathbb{E}[g\\mid{\\mathcal{F}}]$ and $\\mathbb{E}[h\\mid{\\mathcal{F}}]$ satisfy the following properties: ", "page_idx": 23}, {"type": "text", "text": "1. (linearity) $\\mathbb{E}[\\alpha g+\\beta h\\mid\\mathcal{F}]=\\alpha\\,\\mathbb{E}[g]+\\beta\\,\\mathbb{E}[h\\mid\\mathcal{F}]\\,f o r\\,a l l\\,\\alpha,\\beta\\in\\mathbb{R}$   \n2. (tower identity) $\\mathbb{E}[\\mathbb{E}[g\\mid\\mathcal{F}]]=\\mathbb{E}[g]$   \n3. If g is $\\mathcal{F}$ \u2212measurable then $\\mathbb{E}[g h\\mid{\\mathcal{F}}]=g\\mathbb{E}[h\\mid{\\mathcal{F}}]$ . In particular, $\\mathbb{E}[g\\mid{\\mathcal{F}}]=g$ ", "page_idx": 23}, {"type": "text", "text": "For a more thorough introduction to filtrations and conditional expectations, see e.g. [Klenke, 2013, Chapter 8]. $\\mathbb{E}[g_{n}^{\\prime}|\\mathcal{F}_{n}]$ is the mean of $g_{n}^{\\prime}$ if all previous steps are already known. ", "page_idx": 23}, {"type": "text", "text": "Lemma 15. Suppose $g_{n}^{\\prime},x_{n}$ , and $x_{n}^{\\prime}$ satisfy the assumptions laid out in Section 3.1, then the following statements hold ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I.\\ \\mathbb{E}\\big[g_{n}^{\\prime}|\\mathcal{F}_{n}\\big]=\\nabla f(x_{n}^{\\prime})}\\\\ &{2.\\ \\mathbb{E}\\big[\\|g_{n}^{\\prime}-\\nabla f(x_{n}^{\\prime})\\|^{2}\\big]\\leq\\sigma^{2}\\,\\mathbb{E}\\big[\\|\\nabla f(x_{n}^{\\prime})\\|^{2}\\big].}\\\\ &{3.\\ \\mathbb{E}[\\|g_{n}^{\\prime}\\|^{2}]=(1+\\sigma^{2})\\mathbb{E}[\\|\\nabla f(x_{n}^{\\prime})\\|^{2}]}\\\\ &{4.\\ \\mathbb{E}[\\nabla f(x_{n}^{\\prime})\\cdot g_{n}^{\\prime}]=\\mathbb{E}[\\|\\nabla f(x_{n}^{\\prime})\\|^{2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. First and second claim. This follows from Fubini\u2019s theorem. ", "page_idx": 23}, {"type": "text", "text": "Third claim. The third result then follows by an application of the tower identity with ${\\mathcal{F}}_{n}$ , expanding the square of the norm as a dot product, and then using the linearity of conditional expectation: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\Vert g_{n}^{\\prime}\\Vert^{2}\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[\\Vert g_{n}^{\\prime}\\Vert^{2}\\mid\\mathcal{F}_{n}\\right]\\right]}\\\\ &{\\phantom{=}=\\mathbb{E}\\left[\\mathbb{E}\\left[\\Vert g_{n}^{\\prime}-\\nabla f(x_{n}^{\\prime})\\Vert^{2}+2g\\cdot\\nabla f(x_{n}^{\\prime})-\\Vert\\nabla f(x_{n}^{\\prime})\\Vert^{2}\\mid\\mathcal{F}_{n}\\right]\\right]}\\\\ &{\\phantom{=}=\\mathbb{E}\\left[\\mathbb{E}\\left[\\Vert g_{n}^{\\prime}-\\nabla f(x_{n}^{\\prime})\\Vert^{2}\\mid\\mathcal{F}_{n}\\right]+2\\mathbb{E}\\left[g\\cdot\\nabla f(x_{n}^{\\prime})\\mid\\mathcal{F}_{n}\\right]-\\mathbb{E}\\left[\\Vert\\nabla f(x_{n}^{\\prime})\\Vert^{2}\\mid\\mathcal{F}_{n}\\right]\\right]}\\\\ &{\\phantom{=}\\leq\\mathbb{E}\\left[\\sigma^{2}\\nabla f(x_{n}^{\\prime})+2\\left\\Vert\\nabla f(x_{n}^{\\prime})\\right\\Vert^{2}-\\left\\Vert\\nabla f(x_{n}^{\\prime})\\right\\Vert^{2}\\right]}\\\\ &{\\phantom{=}=(1+\\sigma^{2})\\mathbb{E}\\left[\\Vert\\nabla f(x_{n}^{\\prime})\\Vert^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Fourth claim. For the fourth result, we observe that since $f$ is a deterministic function and $x_{n}^{\\prime}$ is ${\\mathcal{F}}_{n}$ -measurable, $\\nabla f(x_{n}^{\\prime})$ is also measurable with respect to the $\\sigma$ -algebra. Then using the tower identity followed by the third property in Lemma 14, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\nabla f(x_{n}^{\\prime})\\cdot g_{n}^{\\prime}\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[\\nabla f(x_{n}^{\\prime})\\cdot g_{n}^{\\prime}\\mid\\mathcal{F}_{n}\\right]\\right]}\\\\ &{\\phantom{\\quad\\quad}=\\mathbb{E}\\left[\\nabla f(x_{n}^{\\prime})\\cdot\\mathbb{E}\\left[g_{n}^{\\prime}\\mid\\mathcal{F}_{n}\\right]\\right]}\\\\ &{\\phantom{\\quad\\quad}=\\mathbb{E}\\left[\\nabla f(x_{n}^{\\prime})\\cdot\\nabla f(x_{n}^{\\prime})\\right]}\\\\ &{\\phantom{\\quad\\quad}=\\mathbb{E}\\left[\\left\\Vert\\nabla f(x_{n}^{\\prime})\\right\\Vert^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "As a consequence, we note the following decrease estimate. ", "page_idx": 23}, {"type": "text", "text": "Lemma 16. Suppose that $f,x_{n}^{\\prime}$ , and $g_{n}^{\\prime}=g(x_{n}^{\\prime},\\omega_{n})$ satisfy the conditions laid out in Section 3.1, then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[f(x_{n}^{\\prime}-\\eta g_{n}^{\\prime})\\big]\\leq\\mathbb{E}\\big[f(x_{n}^{\\prime})\\big]-\\eta\\left(1-\\frac{L(1+\\sigma^{2})\\eta}{2}\\right)\\,\\mathbb{E}\\left[\\|\\nabla f(x_{n}^{\\prime})\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Using $L$ -smoothness of $f$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\nf(\\boldsymbol{x}_{n}^{\\prime}-\\boldsymbol{\\eta}g_{n}^{\\prime})\\leq f(\\boldsymbol{x}_{n}^{\\prime})-\\eta g_{n}^{\\prime}\\cdot\\nabla f(\\boldsymbol{x}_{n}^{\\prime})+\\frac{L\\eta^{2}}{2}\\left\\|g_{n}^{\\prime}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then taking the expectation and using the results of the previous lemma, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f(x_{n}^{\\prime}-\\eta g_{n}^{\\prime})\\right]\\leq\\mathbb{E}[f(x_{n}^{\\prime})]-\\eta\\mathbb{E}\\left[\\left\\Vert\\nabla f(x_{n}^{\\prime})\\right\\Vert^{2}\\right]+\\frac{L\\eta^{2}}{2}(1+\\sigma^{2})\\mathbb{E}\\left[\\left\\Vert\\nabla f(x_{n}^{\\prime})\\right\\Vert^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathbb{E}\\bigl[f(x_{n}^{\\prime})\\bigr]-\\eta\\left(1-\\frac{L(1+\\sigma^{2})\\eta}{2}\\right)\\,\\mathbb{E}\\left[\\left\\Vert\\nabla f(x_{n}^{\\prime})\\right\\Vert^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In particular, if $\\begin{array}{r}{\\eta\\le\\frac{1}{L(1+\\sigma^{2})}}\\end{array}$ , then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[f(x_{n}^{\\prime}-\\eta g_{n}^{\\prime})\\big]\\leq\\mathbb{E}\\big[f(x_{n}^{\\prime})\\big]-\\frac{\\eta}{2}\\,\\mathbb{E}\\left[\\|\\nabla f(x_{n}^{\\prime})\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "E Convergence proofs: convex case ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "E.1 Gradient Descent (GD) ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We first present a convergence result for stochastic gradient descent for convex functions with multiplicative noise scaling. To the best of our knowledge, convergence proofs for this type of noise which degenerates at the global minimum have been given by Bassily et al. [2018], Wojtowytsch [2023] under a Polyak-Lojasiewicz (or PL) condition (which holds automatically in the strongly convex case), but not for functions which are merely convex. We note that, much like AGNES, SGD achieves the same rate of convergence in stochastic convex optimization with multiplicative noise as in the deterministic case (albeit with a generally much larger constant). In particular, SGD with multiplicative noise is more similar to deterministic gradient descent than to SGD with additive noise in this way. ", "page_idx": 24}, {"type": "text", "text": "Analyses of SGD with non-standard noise under various conditions are given by Stich and Karimireddy [2022], Stich [2019]. ", "page_idx": 24}, {"type": "text", "text": "Theorem 17 (GD, convex case). Assume that $f$ is a convex function and that the assumptions laid out in Section 3.1 are satisfied. If the sequence $x_{n}$ is generated by the gradient descent scheme ", "page_idx": 24}, {"type": "equation", "text": "$$\ng_{n}=g(x_{n},\\omega_{n}),\\qquad x_{n+1}=x_{n}-\\eta g_{n},\\qquad\\eta\\leq\\frac{1}{L(1+\\sigma^{2})},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "then for any $x^{*}\\in\\mathbb{R}^{m}$ and any $n_{0}\\geq1+\\sigma^{2}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(x_{n})-f(x^{*})]\\leq\\frac{\\eta n_{0}\\,\\mathbb{E}[f(x_{0})-f(x^{*})]+\\frac{1}{2}\\,\\mathbb{E}\\big[\\|x_{0}-x^{*}\\|^{2}\\big]}{\\eta(n+n_{0})}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In particular, if $\\begin{array}{r}{\\eta=\\frac{1}{L(1+\\sigma^{2})}}\\end{array}$ , $n_{0}=1+\\sigma^{2}$ , and $x^{*}$ is a point such that $f(x^{*})=\\operatorname*{inf}_{x\\in\\mathbb{R}^{m}}f(x).$ , then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(x_{n})-f(x^{*})]\\leq\\frac{L(1+\\sigma^{2})\\,\\mathbb{E}\\big[\\|x_{0}-x^{*}\\|^{2}\\big]}{2(n+1+\\sigma^{2})}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Let $n_{0}\\geq0$ and consider the Lyapunov sequence ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{L}_{n}=\\mathbb{E}\\left[\\eta(n+n_{0})\\big(f(x_{n})-\\operatorname*{inf}f\\big)+\\frac{1}{2}\\left\\|x_{n}-x^{*}\\right\\|^{2}\\right]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We find that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{L}_{n+1}=\\mathbb{E}\\bigg[\\eta(n+n_{0}+1)\\big\\{f(x_{n}-\\eta g_{n})-\\operatorname*{inf}f\\big\\}+\\frac{1}{2}\\|x_{n}-\\eta g_{n}-x^{*}\\|^{2}\\bigg]}&{}\\\\ {\\leq\\mathbb{E}\\bigg[\\eta(n+n_{0}+1)\\left\\{f(x_{n})-\\frac{\\eta}{2}\\|\\nabla f(x_{n})\\|^{2}-\\operatorname*{inf}f\\right\\}}\\\\ {\\phantom{\\frac{(1)}{\\eta}}+\\frac{1}{2}\\|x_{n}-x^{*}\\|^{2}-\\eta\\left(x_{n}-x^{*}\\right)\\cdot g_{n}+\\frac{\\eta^{2}}{2}\\left\\|g_{n}\\right\\|^{2}\\bigg]}&{}\\\\ {=\\mathbb{E}\\bigg[\\eta(n+n_{0})\\big\\{f(x_{n})-\\operatorname*{inf}f\\big\\}+\\frac{1}{2}\\|x_{n}-x^{*}\\|^{2}+f(x_{n})-\\operatorname*{inf}f+\\eta\\nabla f(x_{n})\\cdot\\left(x^{*}-x_{n}\\right)}\\\\ {\\phantom{\\frac{(1)}{\\eta}}-\\frac{\\eta^{2}(n+n_{0})}{2}\\|\\nabla f(x_{n})\\|^{2}+\\frac{\\eta^{2}}{2}\\left\\|g_{n}\\right\\|^{2}\\bigg]}&{}\\\\ {\\leq\\mathcal{L}_{n}+0-\\frac{\\eta^{2}}{2}\\big(n+n_{0}-(1+\\sigma^{2})\\big)\\mathbb{E}\\big[\\|\\nabla f(x_{n})\\|^{2}\\big]}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "by the convexity of $f$ . The result therefore holds if $n_{0}$ is chosen large since ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(x_{n})-f(x^{*})]\\leq\\frac{\\mathcal{L}_{n}}{\\eta(n+n_{0})}\\leq\\frac{\\mathcal{L}_{0}}{\\eta(n+n_{0})}=\\frac{\\eta n_{0}\\,\\mathbb{E}[f(x_{0})-f(x^{*})]+\\frac{1}{2}\\,\\mathbb{E}\\big[\\|x_{0}-x^{*}\\|^{2}\\big]}{\\eta(n+n_{0})}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "If $x^{*}$ is a minimizer of $f$ then the last claim in the theorem follows by using the upper bound $\\begin{array}{r}{f(x_{0})-f(x^{*})\\le\\frac{L}{2}\\|x_{0}-x^{*}\\|^{2}}\\end{array}$ from Lemma 12 and substituting $\\begin{array}{r}{\\eta=\\frac{\\ '_{1}}{L(1+\\sigma)^{2}},\\stackrel{\\smile}{n}_{0}={1+\\sigma^{2}}}\\end{array}$ . ", "page_idx": 25}, {"type": "text", "text": "E.2 AGNES and NAG ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The proofs of Theorems 1 and 3 in this section are constructed in analogy to the simplest setting of deterministic continuous-time optimization. As noted by $\\mathrm{Su}$ et al. [2014], Nesterov\u2019s time-stepping scheme can be seen as a non-standard time discretization of the heavy ball ODE ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\ddot{x}}&{=-\\frac{3}{t}\\,\\dot{x}-\\nabla f(x)}&{t>0}\\\\ {\\dot{x}}&{=0}&{t=0}\\\\ {x}&{=x_{0}}&{t=0}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with a decaying friction coefficient. The same is true for AGNES, which reduces to Nesterov\u2019s method in the determinstic case. Taking the derivative and exploiting the first-order convexity condition, we see that the Lyapunov function ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{L}(t):=t^{2}\\big(f(x(t))-f(x^{*})\\big)+\\frac{1}{2}\\left\\|t\\dot{x}+2\\big(x(t)-x^{*}\\big)\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "is decreasing in time along the heavy ball ODE, see e.g. [Su et al., 2014, Theorem 3]. Here $x^{*}$ is a minimizer of the convex function $f$ . In particular ", "page_idx": 25}, {"type": "equation", "text": "$$\nf(x(t))-f(x^{*})\\leq{\\frac{\\mathcal{L}(t)}{t^{2}}}\\leq{\\frac{{\\mathcal{L}}(0)}{t^{2}}}={\\frac{2\\,\\|x_{0}-x^{*}\\|^{2}}{t^{2}}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "To prove Theorems 1 and 3, we construct an analogue to $\\mathcal{L}$ in (7). Note that $\\alpha v_{n}=x_{n}^{\\prime}-x_{n}$ is a discrete analogue of the velocity $\\dot{x}$ in the continuous setting. Both the proofs follow the same outline. Since Nesterov\u2019s algorithm is a special case of AGNES, we first prove Theorem 3. We present the Lyapunov sequence in a fairly general form, which allows us to reuse calculations for both proofs and suggests the optimality of our approach for Nesterov\u2019s original algorithm. ", "page_idx": 25}, {"type": "text", "text": "For details on the probalistic set-up and useful properties of gradient estimators, see Appendix D.2. Let us recall the two-step formulation of AGNES, which we use for the proof, ", "page_idx": 25}, {"type": "equation", "text": "$$\nx_{0}=x_{0}^{\\prime},\\qquad x_{n+1}=x_{n}^{\\prime}-\\eta g_{n}^{\\prime},\\qquad x_{n+1}^{\\prime}=x_{n+1}+\\rho_{n}\\bigl(x_{n}^{\\prime}-\\alpha g_{n}^{\\prime}-x_{n}\\bigr).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We first prove the alternative version mentioned after Theorem 3 in the main text. Both proofs proceed initially identically and only diverge in Step 3. The reader interested mainly in Theorem 3 is invited to read the first two steps of the proof of Theorem 18 and then skip ahead to the proof of Theorem 3 below. ", "page_idx": 25}, {"type": "text", "text": "Theorem 18 (AGNES, convex case, $n_{0}$ version). Suppose that $x_{n}$ and $x_{n}^{\\prime}$ are generated by the time-stepping scheme $(3,$ , $f$ and $g_{n}^{\\prime}=g(x_{n}^{\\prime},\\omega_{n})$ satisfy the conditions laid out in Section 3.1, $f$ is convex, and $x^{*}$ is a point such that $f(x^{*})=\\operatorname*{inf}_{x\\in\\mathbb{R}^{m}}f(x)$ . If the parameters are chosen such that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\eta\\le\\frac{1}{L(1+\\sigma^{2})},\\qquad\\alpha<\\frac{\\eta}{1+\\sigma^{2}},\\qquad n_{0}\\ge\\frac{2\\sigma^{2}\\eta}{\\eta-\\alpha(1+\\sigma^{2})},\\qquad\\rho_{n}=\\frac{n+n_{0}}{n+n_{0}+3},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "then ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[f(x_{n})-f(x^{*})\\big]\\leq\\frac{(\\alpha n_{0}+2\\eta)n_{0}\\,\\mathbb{E}\\big[f(x_{0})-\\operatorname*{inf}f\\big]+2\\,\\mathbb{E}\\big[\\,\\|x_{0}-x^{*}\\|^{2}\\big]}{\\alpha\\,(n+n_{0})^{2}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In particular, if $\\begin{array}{r}{\\alpha\\le\\frac{\\eta}{1+2\\sigma^{2}}}\\end{array}$ then it suffices to choose $n_{0}\\ge2\\eta/\\alpha\\ge2(1+2\\sigma^{2})$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. Set-up. Mimicking the continuous time model in (7), we consider the Lyapunov sequence given by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{L}_{n}=P(n)\\,\\mathbb{E}\\left[f(x_{n})-f(x^{*})\\right]+\\frac{1}{2}\\mathbb{E}\\left[\\left\\|b(n)(x_{n}^{\\prime}-x_{n})+a(n)(x_{n}^{\\prime}-x^{*})\\right\\|^{2}\\right]\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $P(n)$ some function of $n$ , $a(n)\\,=\\,a_{0}\\,+\\,a_{1}n$ , and $b(n)\\,=\\,b_{0}\\,+\\,b_{1}n$ for some coefficients $a_{0},a_{1},b_{0},b_{1}$ . Our goal is to choose these in such a way that ${\\mathcal{L}}_{n}$ is a decreasing sequence. ", "page_idx": 26}, {"type": "text", "text": "Step 1. If we denote the first half of the Lyapunov sequence as $\\mathcal{L}_{n}^{1}=P(n)\\mathbb{E}\\left[f(x_{n})-f(x^{*})\\right]$ , then ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{n+1}^{1}-\\mathcal{L}_{n}^{1}=P(n+1)\\mathbb{E}[f(x_{n+1})-f(x^{*})]-P(n)\\mathbb{E}[f(x_{n})-f(x^{*})]}\\\\ &{\\qquad\\qquad\\leq(P(n+1)+k)\\mathbb{E}[f(x_{n+1})-f(x^{*})]-P(n)\\,\\mathbb{E}[f(x_{n})-f(x^{*})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $k$ is a positive constant that can be chosen later to balance out other terms. Using Lemma 15, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{n+1}^{1}-\\mathcal{L}_{n}^{1}\\leq(P(n+1)+k)\\mathbb{E}\\left[f(x_{n}^{\\prime})-c_{\\eta,\\sigma,L}\\left\\|\\nabla f(x_{n}^{\\prime})\\right\\|^{2}-f(x^{*})\\right]-P(n)\\mathbb{E}[f(x_{n})-f(x^{*})]}\\\\ &{\\qquad\\qquad=P(n)\\mathbb{E}\\left[f(x_{n}^{\\prime})-f(x_{n})\\right]+(P(n+1)+k-P(n))\\,\\mathbb{E}[f(x_{n}^{\\prime})-f(x^{*})]}\\\\ &{\\qquad\\qquad\\qquad-\\left(P(n+1)+k\\right)c_{\\eta,\\sigma,L}\\mathbb{E}[\\left\\|\\nabla f(x_{n}^{\\prime})\\right\\|^{2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\begin{array}{r}{c_{\\eta,\\sigma,L}=\\eta\\left(1-\\frac{L(1+\\sigma^{2})\\eta}{2}\\right)}\\end{array}$ . Using convexity, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathcal{L}_{n+1}^{1}-\\mathcal{L}_{n}^{1}\\leq P(n)\\mathbb{E}\\left[\\nabla f(x_{n}^{\\prime})\\cdot(x_{n}^{\\prime}-x_{n})\\right]+\\left(P(n+1)+k-P(n)\\right)\\mathbb{E}\\big[\\nabla f(x_{n}^{\\prime})\\cdot(x_{n}^{\\prime}-x^{*})\\big]}&\\\\ &{\\qquad\\qquad\\qquad-\\left(P(n+1)+k\\right)\\!c_{\\eta,\\sigma,L}\\mathbb{E}\\big[\\|\\nabla f(x_{n}^{\\prime})\\|^{2}\\big].}&{\\quad}&{(84.52\\%),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Step 2. We denote ", "text_level": 1, "page_idx": 26}, {"type": "equation", "text": "$$\nw_{n}=b(n)(x_{n}^{\\prime}-x_{n})+a(n)(x_{n}^{\\prime}-x^{*})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and use the definition of $x_{n+1}^{\\prime}$ from (2), ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{n+1}=b(n+1)(x_{n+1}^{\\prime}-x_{n+1})+a(n+1)(x_{n+1}^{\\prime}-x^{*})}\\\\ &{\\qquad=b(n+1)\\rho_{n}\\,(x_{n}^{\\prime}-\\alpha g_{n}^{\\prime}-x_{n})+a(n+1)\\,(x_{n+1}+\\rho_{n}(x_{n}^{\\prime}-\\alpha g_{n}^{\\prime}-x_{n})-x^{*})}\\\\ &{\\qquad=(b(n+1)+a(n+1))\\rho_{n}\\,(x_{n}^{\\prime}-\\alpha g_{n}^{\\prime}-x_{n})+a(n+1)(x_{n+1}-x^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We will choose ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\rho_{n}=\\frac{b(n)}{b(n+1)+a(n+1)},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "such that the expression becomes ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{n+1}=b(n)\\left(x_{n}^{\\prime}-\\alpha g_{n}^{\\prime}-x_{n}\\right)+a(n+1)(x_{n+1}-x^{*})}\\\\ &{\\qquad=b(n)\\left(x_{n}^{\\prime}-\\alpha g_{n}^{\\prime}-x_{n}\\right)+(a_{0}+a_{1}n+a_{1})(x_{n}^{\\prime}-\\eta g_{n}^{\\prime}-x^{*})}\\\\ &{\\qquad=w_{n}+a_{1}(x_{n}^{\\prime}-x^{*})-(\\alpha b(n)+\\eta a(n+1))g_{n}^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\left\\|w_{n+1}\\right\\|^{2}-\\displaystyle\\frac{1}{2}\\left\\|w_{n}\\right\\|^{2}=w_{n}\\cdot\\big(w_{n+1}-w_{n}\\big)+\\displaystyle\\frac{1}{2}\\left\\|w_{n+1}-w_{n}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=w_{n}\\cdot\\big(a_{1}(x_{n}^{\\prime}-x^{*})-(\\alpha b(n)+\\eta a(n+1))g_{n}^{\\prime}\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\frac{1}{2}\\left\\|a_{1}(x_{n}^{\\prime}-x^{*})-(\\alpha b(n)+\\eta a(n+1))g_{n}^{\\prime}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We want the terms in this expression to balance the terms in L n1+1 \u2212L n1, so we choose a1 = 0, i.e. $a(n)=a_{0}$ is a constant. This implies, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{E}\\!\\left[\\frac{1}{2}\\left\\Vert w_{n+1}\\right\\Vert^{2}-\\frac{1}{2}\\left\\Vert w_{n}\\right\\Vert^{2}\\right]=\\mathbb{E}\\left[-(\\alpha b(n)+\\eta a_{0})w_{n}\\cdot g_{n}^{\\prime}+\\frac{1}{2}(\\alpha b(n)+\\eta a_{0})^{2}\\left\\Vert g_{n}^{\\prime}\\right\\Vert^{2}\\right]}&{}&\\\\ {\\leq-(\\alpha b(n)+\\eta a_{0})\\mathbb{E}[w_{n}\\cdot\\nabla f(x_{n}^{\\prime})]+\\frac{1}{2}(\\alpha b(n)+\\eta a_{0})^{2}(1+\\sigma^{2})\\mathbb{E}[\\left\\Vert\\nabla f(x_{n}^{\\prime})\\right\\Vert^{2}]}&{}&\\\\ {=-(\\alpha b(n)+\\eta a_{0})b(n)\\mathbb{E}[(x_{n}^{\\prime}-x_{n})\\cdot\\nabla f(x_{n}^{\\prime})]}&{}&\\\\ {-\\left(\\alpha b(n)+\\eta a_{0})a_{0}\\mathbb{E}[(x_{n}^{\\prime}-x^{*})\\cdot\\nabla f(x_{n}^{\\prime})]}&{}&\\\\ {+\\frac{1}{2}(\\alpha b(n)+\\eta a_{0})^{2}(1+\\sigma^{2})\\mathbb{E}[\\left\\Vert\\nabla f(x_{n}^{\\prime})\\right\\Vert^{2}].}&{}&{\\quad\\mathrm{()}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Step 3. Combining the estimates (8) and (9) from the last two steps, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{n+1}-\\mathcal{L}_{n}\\leq(P(n)-(\\alpha b(n)+\\eta a_{0})b(n))\\,\\mathbb{E}\\left[\\nabla f(x_{n}^{\\prime})\\cdot(x_{n}^{\\prime}-x_{n})\\right]}\\\\ &{\\qquad\\qquad\\qquad+\\,(P(n+1)+k-P(n)-(\\alpha b(n)+\\eta a_{0})a_{0})\\,\\mathbb{E}[\\nabla f(x_{n}^{\\prime})\\cdot(x_{n}^{\\prime}-x^{*})]}\\\\ &{\\qquad\\qquad\\qquad+\\,\\left(\\frac{1}{2}(\\alpha b(n)+\\eta a_{0})^{2}(1+\\sigma^{2})-(P(n+1)+k)c_{\\eta,\\sigma,L}\\right)\\mathbb{E}[\\|\\nabla f(x_{n}^{\\prime})\\|^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since $\\left\\|\\nabla f(x_{n}^{\\prime})\\right\\|^{2}\\geq0$ and $\\nabla f(x_{n}^{\\prime})\\cdot(x_{n}^{\\prime}-x^{*})\\geq f(x_{n}^{\\prime})-f(x^{*})\\geq0.$ , we require the coefficients of these two terms to be non-positive and the coefficient of $\\nabla f(x_{n}^{\\prime})\\cdot(x_{n}^{\\prime}-x_{n})$ to be zero. That gives us the following system of inequalities, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{c}{P(n)=(\\alpha b(n)+\\eta a_{0})b(n)}\\\\ {P(n+1)+k-P(n)\\leq(\\alpha b(n)+\\eta a_{0})a_{0}}\\\\ {\\frac{1}{2}(\\alpha b(n)+\\eta a_{0})^{2}(1+\\sigma^{2})\\leq(P(n+1)+k)\\eta\\left(1-\\frac{L(1+\\sigma^{2})\\eta}2\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Step 4. Now we can choose values that will satisfy the above system of inequalities. We substitute $a_{0}=2,b_{1}=1,b_{0}=n_{0}$ , and $k=2\\eta-\\alpha$ . From (10), we get $P(n)=\\left(\\alpha(n+n_{0})+2\\eta\\right)(n+n_{0})$ . Next, we observe that ", "page_idx": 27}, {"type": "equation", "text": "$$\nP(n+1)=P(n)+\\alpha+2\\alpha(n+n_{0})+2\\eta.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then (11) holds because ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(n+1)+k-P(n)=\\alpha+2\\alpha(n+n_{0})+2\\eta+2\\eta-\\alpha}\\\\ &{\\qquad\\qquad\\qquad\\qquad=2(\\alpha(n+n_{0})+2\\eta)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=(\\alpha b(n)+\\eta a_{0})a_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We now choose $\\eta$ to satisfy $\\begin{array}{r}{\\eta\\le\\frac{1}{L(1+\\sigma^{2})}}\\end{array}$ , which ensures that $\\begin{array}{r}{\\frac{\\eta}{2}\\le\\eta\\left(1-\\frac{L(1+\\sigma^{2})\\eta}{2}\\right)}\\end{array}$ . Consequently, for (12), it suffices to ensure that ", "page_idx": 27}, {"type": "equation", "text": "$$\n(\\alpha b(n)+\\eta a_{0})^{2}(1+\\sigma^{2})\\leq(P(n+1)+k)\\eta,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which is equivalent to showing that the polynomial, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{q(z)=\\left(\\alpha z^{2}+2\\eta z+\\alpha+2\\alpha z+2\\eta+2\\eta-\\alpha\\right)\\eta}}\\\\ {{-\\left(\\alpha^{2}z^{2}+4\\eta^{2}+4\\alpha\\eta z\\right)(1+\\sigma^{2}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "is non-negative for all $z\\geq n_{0}.\\;q(z)$ simplifies to ", "page_idx": 27}, {"type": "equation", "text": "$$\nq(z)=\\alpha(\\eta-\\alpha(1+\\sigma^{2}))z^{2}+2\\eta(\\eta+\\alpha-2\\alpha(1+\\sigma^{2}))z-4\\eta^{2}\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "To guarantee that $q$ is non-negative for $z=n+n_{0}\\ge n_{0}$ , we require that ", "page_idx": 27}, {"type": "text", "text": "1. the leading order coefficient is strictly positive2 and ", "page_idx": 27}, {"type": "text", "text": "2. $n_{0}\\geq0,q(n_{0})\\geq0.$ . ", "page_idx": 28}, {"type": "text", "text": "Since $q(0)<0$ and $q$ is quadratic, this suffices to guarantee that $q$ is increasing on $[n_{0},\\infty)$ . The first condition reduces to the fact that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\eta-\\alpha(1+\\sigma^{2})>0.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We can find the minimal admissible value of $n_{0}$ by the quadratic formula. We first consider the term outside the square root: ", "page_idx": 28}, {"type": "equation", "text": "$$\n-{\\frac{2\\eta(\\eta+\\alpha-2\\alpha(1+\\sigma^{2}))}{2\\alpha(\\eta-\\alpha(1+\\sigma^{2}))}}=-{\\frac{\\eta}{\\alpha}}\\left(1-{\\frac{\\alpha\\,\\sigma^{2}}{\\eta-\\alpha(1+\\sigma^{2})}}\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and thus ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\tan\\geq-{\\frac{\\eta}{\\alpha}}\\left(1-{\\frac{\\alpha\\sigma^{2}}{\\eta-\\alpha(1+\\sigma^{2})}}\\right)+{\\sqrt{{\\frac{\\eta^{2}}{\\alpha^{2}}}\\left(1-{\\frac{\\alpha\\sigma^{2}}{\\eta-\\alpha(1+\\sigma^{2})}}\\right)^{2}+{\\frac{4\\eta^{2}\\sigma^{2}}{\\alpha(\\eta-\\alpha(1+\\sigma^{2}))}}}}}\\\\ &{={\\frac{\\eta}{\\alpha}}\\left\\{{\\sqrt{1-2{\\frac{\\alpha\\sigma^{2}}{\\eta-\\alpha(1+\\sigma^{2})}}+\\left({\\frac{\\alpha\\sigma^{2}}{\\eta-\\alpha(1+\\sigma^{2})}}\\right)^{2}+4{\\frac{\\alpha\\sigma^{2}}{\\eta-\\alpha(1+\\sigma^{2})}}}}-\\left(1-{\\frac{\\alpha\\sigma^{2}}{\\eta-\\alpha(1+\\sigma^{2})}}\\right)\\right.}\\\\ &{={\\frac{\\eta}{\\alpha}}\\left\\{{\\sqrt{\\left(1+{\\frac{\\alpha\\sigma^{2}}{\\eta-\\alpha(1+\\sigma^{2})}}\\right)^{2}}}-\\left(1-{\\frac{\\alpha\\sigma^{2}}{\\eta-\\alpha(1+\\sigma^{2})}}\\right)\\right\\}}\\\\ &{={\\frac{\\eta}{\\alpha}}{\\frac{2\\alpha\\sigma^{2}}{\\eta-\\alpha(1+\\sigma^{2})}}}\\\\ &{={\\frac{2\\eta\\sigma^{2}}{\\eta-\\alpha(1+\\sigma^{2})}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In particular, in the deterministic case $\\sigma=0$ , the choice $n_{0}=0$ is admissible. Notably, we require $\\begin{array}{r}{n_{0}\\stackrel{\\cdot}{\\geq}2\\sigma^{2}\\,\\frac{\\eta}{\\eta}=2\\sigma^{2}}\\end{array}$ . Furthermore, if $\\begin{array}{r}{\\alpha\\le\\frac{\\eta}{1+2\\sigma^{2}}}\\end{array}$ , then ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{2\\sigma^{2}\\eta}{\\eta-\\alpha(1+\\sigma^{2})}\\le\\frac{2\\sigma^{2}\\eta}{\\alpha\\sigma^{2}}=\\frac{2\\eta}{\\alpha},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "so it suffices to choose $n_{0}\\ge2\\eta/\\alpha$ in this case. ", "page_idx": 28}, {"type": "text", "text": "Step 5. We have shown that the Lyapunov sequence, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{L}_{n}=((n+n_{0})\\alpha+2\\eta)(n+n_{0})\\mathbb{E}\\left[f(x_{n})-f(x^{*})\\right]+\\frac{1}{2}\\mathbb{E}\\left[\\|(n+n_{0})(x_{n}^{\\prime}-x_{n})+2(x_{n}^{\\prime}-x^{*})\\|^{2}\\right],\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "is monotone decreasing. It follows that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[f(x_{n})-f(x^{*})\\big]\\leq\\frac{\\mathcal{L}_{n}}{P(n)}\\leq\\frac{\\mathcal{L}_{0}}{P(n)}\\leq\\frac{\\mathbb{E}\\big[(n_{0}\\alpha+2\\eta)n_{0}\\left(f(x_{0})-f(x^{*})\\right)+2\\left\\|x_{0}-x^{*}\\right\\|^{2}\\big]}{\\alpha\\left(n+n_{0}\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "If $2\\eta\\leq\\alpha n_{0}$ , we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[f(x_{n})-f(x^{*})\\big]\\leq\\frac{2\\alpha n_{0}^{2}\\,\\mathbb{E}\\big[f(x_{0})-\\operatorname*{inf}f\\big]+2\\,\\mathbb{E}\\big[\\,\\|x_{0}-x^{*}\\|^{2}\\big]}{\\alpha\\,(n+n_{0})^{2}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Finally, if $\\begin{array}{r}{\\eta=\\frac{1}{L(1+\\sigma^{2})},~\\alpha=\\frac{1}{L(1+\\sigma^{2})(1+2\\sigma^{2})}}\\end{array}$ , and $n_{0}=2(1+2\\sigma^{2})$ , then using Lemma 12, the ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[f(x_{n})-f(x^{*})\\big]\\leq\\frac{2L(1+2\\sigma^{2})(3+5\\sigma^{2})\\mathbb{E}\\left[\\left\\Vert x_{0}-x^{*}\\right\\Vert^{2}\\right]}{n^{2}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Remark 19. Note that SGD arises as a special case of this analysis if we consider $\\alpha=0,n_{0}\\geq2\\sigma^{2}$ since $P(n)$ is a linear polynomial in this case. ", "page_idx": 28}, {"type": "text", "text": "Remark 20. Note that the proof of Theorem 18 implies more generally that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{n+1}\\leq\\mathcal{L}_{n}-q(n+n_{0})\\,E\\big[\\|\\nabla f(x_{n}^{\\prime})\\|^{2}\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "even if $n_{0}$ is not chosen such that $q(n\\!+\\!n_{0})\\geq0$ for all $n$ . However, $q(n\\!+\\!n_{0})\\geq0$ for all sufficiently large $n\\in\\mathbb{N}$ , i.e. ${\\mathcal{L}}_{n}$ decreases eventually (assuming that $\\mathcal{L}_{n}<\\infty$ for all finite $n$ ) . More precisely, for given $\\eta,\\alpha$ if ", "page_idx": 29}, {"type": "equation", "text": "$$\nn+n_{0}\\geq n^{*}:=\\left\\lceil\\frac{\\eta\\sigma^{2}}{\\eta-\\alpha(1+\\sigma^{2})}\\right\\rceil,\\qquad\\mathrm{then}\\quad\\mathcal{L}_{n}\\leq\\frac{\\mathcal{L}_{n^{*}}}{\\alpha\\,(n+n_{0})^{2}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus a poor choice of $n_{0}$ will not prevent convergence, but it may delay it. ", "page_idx": 29}, {"type": "text", "text": "We now prove the version of this result stated in the main text, for which $\\begin{array}{r}{\\rho_{n}=\\frac{n}{n+a_{0}+1}}\\end{array}$ with $a_{0}>2$ i.e. with slightly more friction. ", "page_idx": 29}, {"type": "text", "text": "Theorem 3 (AGNES, convex case). Suppose that $x_{n}$ and $x_{n}^{\\prime}$ are generated by the time-stepping scheme (3), $f$ and $g_{n}^{\\prime}=g(x_{n}^{\\prime},\\omega_{n})$ satisfy the conditions laid out in Section 3.1, $f$ is convex, and $x^{*}$ is a point such that $f(x^{*})=\\operatorname*{inf}_{x\\in\\mathbb{R}^{m}}f(x)$ . If the parameters are chosen such that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{c c c c}{0<\\eta<\\displaystyle\\frac{1}{L(1+\\sigma^{2})},}&{\\alpha=\\displaystyle\\frac{\\eta}{1+\\sigma^{2}},}&{\\rho_{n}=\\displaystyle\\frac{n}{n+1+a_{0}},}&{f o r}&{a_{0}\\ge\\displaystyle\\frac{2(1-\\eta L)}{1-\\eta L(1+\\sigma^{2})},}\\\\ &{\\mathbb{E}\\big[f(x_{n})-f(x^{*})\\big]\\le\\displaystyle\\frac{a_{0}^{2}\\,\\mathbb{E}\\big[\\|x_{0}-x^{*}\\|^{2}\\big]}{2\\,\\alpha\\,n^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. The proof for this version of Theorem 3 is identical to the proof of Theorem 18 until Step 3, after which we take an alternate approach. Let us recall the expression we got in the beginning of Step 3. ", "page_idx": 29}, {"type": "text", "text": "Step 3. We want to show that the bound on the right hand side of the inequality ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{n+1}-\\mathcal{L}_{n}\\leq(P(n)-(\\alpha b(n)+\\eta a_{0})b(n))\\,\\mathbb{E}\\left[\\nabla f(x_{n}^{\\prime})\\cdot(x_{n}^{\\prime}-x_{n})\\right]}\\\\ &{\\qquad\\qquad\\qquad+\\,(P(n+1)+k-P(n)-(\\alpha b(n)+\\eta a_{0})a_{0})\\,\\mathbb{E}[\\nabla f(x_{n}^{\\prime})\\cdot(x_{n}^{\\prime}-x^{*})]}\\\\ &{\\qquad\\qquad\\qquad+\\,\\left(\\frac{1}{2}(\\alpha b(n)+\\eta a_{0})^{2}(1+\\sigma^{2})-(P(n+1)+k)c_{\\eta,\\sigma,L}\\right)\\mathbb{E}[\\|\\nabla f(x_{n}^{\\prime})\\|^{2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "is non-positive. Using convexity and $L$ -smoothness in the form of [Wojtowytsch, 2023, Lemma B.1], we get the inequality ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\nabla f(x_{n}^{\\prime})\\cdot(x_{n}^{\\prime}-x^{*})\\geq f(x_{n}^{\\prime})-f(x^{*})\\geq\\frac{1}{2L}\\left\\|\\nabla f(x_{n}^{\\prime})\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which allows us to combine the second and third line in (13), assuming that the coefficient in the second line is non-positive. If this is the case, then the entire right hand side of (13) is bounded from above by ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big(P(n)-(\\alpha b(n)+\\eta a_{0})b(n)\\big)\\,\\mathbb{E}\\left[\\nabla f(x_{n}^{\\prime})\\cdot(x_{n}^{\\prime}-x_{n})\\right]}\\\\ &{\\quad+\\bigg\\{\\,\\mathbb{E}\\big(P(n+1)+k-P(n)-(\\alpha b(n)+\\eta a_{0})a_{0}\\big)\\,\\frac{1}{2L}}\\\\ &{\\quad\\quad+\\bigg(\\frac{1}{2}\\big(\\alpha b(n)+\\eta a_{0}\\big)^{2}(1+\\sigma^{2})-(P(n+1)+k)c_{\\eta,\\sigma,L}\\bigg)\\,\\bigg\\}\\mathbb{E}[\\|\\nabla f(x_{n}^{\\prime})\\|^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "As $\\mathbb{E}\\left[\\nabla f(x_{n}^{\\prime})\\cdot(x_{n}^{\\prime}-x_{n})\\right]$ does not have a sign, we choose to set its coefficient to zero, and we require both the coefficient in the second line of (13) and the coefficient of $\\mathbb{E}[\\|\\nabla f(x_{n}^{\\prime})\\|^{2}]$ in the combined version to be non-positive. Noting that $c_{\\eta,\\sigma,L}\\geq\\eta/2$ if $\\eta\\le1/L(1+\\bar{\\sigma}^{2})$ , this leads to the system of inequalities ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{P(n)=(\\alpha b(n)+\\eta a_{0})b(n)}\\\\ &{}&{\\quad\\quad\\quad P(n+1)+k-P(n)\\leq(\\alpha b(n)+\\eta a_{0})a_{0}}\\\\ &{}&{\\quad\\quad\\quad P(n+1)+k-P(n)-(\\alpha b(n)+\\eta a_{0})a_{0}\\leq L\\bigl((P(n+1)+k)\\eta-(\\alpha b(n)+\\eta a_{0})^{2}(1+\\sigma^{2})\\bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In principle, this approach is more general than that of Theorem 3 as we do not require two terms to be individually non-positive, but only one of them and their weighted sum. In the proof of Theorem 3, a similar role was played by the parameter $k$ , which allowed to shift a small positive term between expressions. ", "page_idx": 30}, {"type": "text", "text": "Step 4. Now we can choose the parameters and variables so as to satisfy the inequalities above. We begin by setting $b_{1}=1,b_{0}=0,k=0.$ , and choosing $\\alpha,\\eta,a_{0}$ as in the theorem statement. Using (14) as the definition of $P(n)$ , we note that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P(n+1)-P(n)=2\\alpha n+\\alpha+\\eta a_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus, (15) simplifies to ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{2\\alpha n+\\alpha+\\eta a_{0}\\leq a_{0}(\\alpha n+\\eta a_{0}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which holds since $a_{0}\\geq2$ and $\\eta\\geq\\alpha$ . The right hand side of (16) simplifies to ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\therefore\\Big(\\eta(n+1)(\\alpha(n+1)+\\eta a_{0})-L\\big(\\alpha n+\\eta a_{0})^{2}(1+\\sigma^{2})\\Big)}\\\\ &{\\qquad\\qquad=L\\left(\\left\\{\\eta\\alpha-(1+\\sigma^{2})\\alpha^{2}\\right\\}n^{2}+\\left\\{\\eta(2\\alpha+\\eta a_{0})-2\\eta a_{0}\\alpha(1+\\sigma^{2})\\right\\}n-\\eta^{2}a_{0}^{2}(1+\\sigma^{2})+\\eta^{2}a_{0}^{2}(1+\\sigma^{2})\\right\\}}\\\\ &{\\qquad\\qquad=L\\left(\\left\\{\\eta(2\\alpha+\\eta a_{0})-2\\eta a_{0}\\alpha(1+\\sigma^{2})\\right\\}n-\\eta^{2}a_{0}^{2}(1+\\sigma^{2})+\\eta(\\alpha+\\eta a_{0})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the last equality holds since $\\alpha=\\eta/(1+\\sigma^{2})$ . Thus for (16) to hold, it suffices that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{;\\alpha n+\\alpha+\\eta a_{0}-a_{0}(\\alpha n+\\eta a_{0})\\leq L\\left(\\left\\{\\eta(2\\alpha+\\eta a_{0})-2\\eta a_{0}\\alpha(1+\\sigma^{2})\\right\\}n-\\eta^{2}a_{0}^{2}(1+\\sigma^{2})+\\eta(\\alpha+\\eta a_{0})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which is equivalent to ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\dot{\\alpha}(2-a_{0})-L\\eta(2\\alpha+\\eta a_{0})+2L\\eta a_{0}\\alpha(1+\\sigma^{2})\\right\\}n+\\left\\{\\alpha+\\eta a_{0}-a_{0}^{2}\\eta+L\\eta^{2}a_{0}^{2}(1+\\sigma^{2})-L\\eta(\\alpha+\\eta a_{0})+L\\eta^{2}a_{0}^{2}(1+\\sigma^{2})\\right\\}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "A linear polynomial is non-negative for all $n\\,\\geq\\,0$ if and only if both of its coefficients are. The leading order coefficient in (17) is ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\alpha(2-a_{0})-L\\eta(2\\alpha+\\eta a_{0})+2L\\eta a_{0}\\alpha(1+\\sigma^{2})=\\alpha(2-a_{0})-L\\eta(2\\alpha+\\eta a_{0})+2L\\eta^{2}a_{0}}&{}\\\\ {=2\\alpha-2L\\eta\\alpha+a_{0}(-\\alpha+L\\eta^{2})}&{}\\\\ {=\\cfrac{\\eta}{1+\\sigma^{2}}\\big(2(1-L\\eta)+a_{0}(L\\eta(1+\\sigma^{2})-1)\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which is non-positive if and only if a0 \u22651\u22122L(1\u03b7(\u22121L+\u03b7\u03c3)2). We remark that it is this part of the computation that forces us to choose \u03b7 strictly smaller thanL(11+\u03c32). In the deterministic case $\\sigma=0$ , we would encounter no such limitation as the term would be automatically zero for $\\eta=1/L$ . Finally, we consider the constant term in (17) and use the fact that $1<a_{0}$ and $\\alpha\\leq\\eta$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{x+\\eta a_{0}-a_{0}^{2}\\eta+L\\eta^{2}a_{0}^{2}(1+\\sigma^{2})-L\\eta(\\alpha+\\eta a_{0})=(\\alpha+\\eta a_{0})(1-L\\eta)+a_{0}^{2}\\eta(L\\eta(1+\\sigma^{2})-1)}&{}\\\\ {\\leq2\\eta a_{0}(1-L\\eta)+a_{0}^{2}\\eta(L\\eta(1+\\sigma^{2})-1)}&{}\\\\ {\\leq\\eta a_{0}\\left(2(1-L\\eta)+a_{0}(L\\eta(1+\\sigma^{2})-1)\\right)}&{}\\\\ {\\leq0,}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "using again that $\\begin{array}{r}{a_{0}\\geq2\\frac{1-L\\eta}{1-L\\eta\\left(1+\\sigma^{2}\\right)}}\\end{array}$ . This shows that ${\\mathcal{L}}_{n+1}\\leq{\\mathcal{L}}_{n}$ . ", "page_idx": 30}, {"type": "text", "text": "Step 5. The conclusion again follows as in the proof of Theorem 18. ", "page_idx": 30}, {"type": "text", "text": "In addition to convergence in expectation, we get almost sure convergence as well. ", "page_idx": 30}, {"type": "text", "text": "Corollary 5. In the setting of Theorems $^3$ and 4, $f(x_{n})\\to\\operatorname{inf}\\,f$ with probability $^{\\,I}$ . ", "page_idx": 30}, {"type": "text", "text": "The same is of course true for Theorem 18. ", "page_idx": 30}, {"type": "text", "text": "Proof. The conclusion follows by standard arguments from the fact that the sequence of expectations $\\mathbb{E}[f(x_{n})-\\operatorname*{inf}{f}]$ is summable: By the previous argument, the estimate ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\big|f(x_{n})-f(x^{*})\\big|\\big]=\\mathbb{E}\\big[f(x_{n})-f(x^{*})\\big]\\le\\frac{C}{n^{2}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "holds for some $C>0$ . Since ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\displaystyle\\left(\\operatorname*{lim}_{n\\to\\infty}f(x_{n})\\neq\\operatorname*{inf}f\\right)=\\mathbb{P}\\left(\\operatorname*{lim}_{n\\to\\infty}|f(x_{n})-\\operatorname*{inf}f|>0\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{P}\\left(\\bigcup_{k=1}^{\\infty}\\left\\{\\underset{n\\to\\infty}{\\operatorname*{lim}\\operatorname*{sup}}\\,|f(x_{n})-\\operatorname*{inf}f|>\\frac1k\\right\\}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{k=1}^{\\infty}\\mathbb{P}\\left(\\underset{n\\to\\infty}{\\operatorname*{lim}\\operatorname*{sup}}\\,|f(x_{n})-\\operatorname*{inf}f|>\\frac1k\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "it suffices to show that $\\mathbb{P}\\left(\\operatorname*{lim}\\operatorname*{sup}_{n\\to\\infty}|f(x_{n})-\\operatorname*{inf}f|>\\varepsilon\\right)=0$ for any $\\varepsilon>0$ . We further note that for any $N\\in\\mathbb{N}$ we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\left(\\underset{n\\to\\infty}{\\operatorname*{limsup}}\\,|f(x_{n})-\\operatorname*{inf}f|>\\varepsilon\\right)\\leq\\mathbb{P}\\left(\\exists\\,n\\geq N\\,\\mathbf{s}.\\mathrm{L},\\,|f(x_{n})-\\operatorname*{inf}f|>\\varepsilon\\right)}\\\\ &{\\ =\\mathbb{P}\\left(\\bigcup_{n=N}^{\\infty}\\,\\{|f(x_{n})-\\operatorname*{inf}f|>\\varepsilon\\}\\right)}\\\\ &{\\ \\leq\\displaystyle\\sum_{n=N}^{\\infty}\\mathbb{P}\\left(|f(x_{n})-\\operatorname*{inf}f|>\\varepsilon\\right)}\\\\ &{\\ \\leq\\displaystyle\\sum_{n=N}^{\\infty}\\frac{\\mathbb{E}\\big[|f(x_{n})-\\operatorname*{inf}f|\\big]}{\\varepsilon}}\\\\ &{\\ \\leq\\frac{C}{\\varepsilon}\\displaystyle\\sum_{n=N}^{\\infty}\\frac{1}{n^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "by Markov\u2019s inequality. As the series over $n^{-2}$ converges, the expression on the right can be made arbitrarily small by choosing $N$ sufficiently large. Thus the quantity on the left must be zero, which concludes the proof. In the strongly convex case, the series $\\begin{array}{r}{\\sum_{n=1}^{\\infty}\\left(1-\\sqrt{\\frac{\\mu}{L}}\\frac{1}{1+\\sigma^{2}}\\right)^{n}}\\end{array}$ converges and thus the same argument applies there as well. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "Next we turn to NAG. Let us recall the statement of Theorem 1. ", "page_idx": 31}, {"type": "text", "text": "Theorem 1 (NAG, convex case). Suppose that $x_{n}$ and $x_{n}^{\\prime}$ are generated by the time-stepping scheme (1), $f$ and $g$ satisfy the conditions laid out in Section 3.1, $f$ is convex, and $x^{*}$ is a point such that $f(x^{*})=\\operatorname*{inf}_{x\\in\\mathbb{R}^{m}}f(x)$ . If $\\sigma<1$ and the parameters are chosen such that ", "page_idx": 31}, {"type": "equation", "text": "$$\n0<\\eta\\leq\\frac{1-\\sigma^{2}}{L(1+\\sigma^{2})},\\ \\ \\ a n d\\ \\ \\ \\rho_{n}=\\frac{n}{n+3},\\ \\ \\ \\ \\ \\ t h e n\\ \\ \\ \\ \\ \\mathbb{E}[f(x_{n})-f(x^{*})]\\leq\\frac{2\\mathbb{E}[\\|x_{0}-x^{*}\\|^{2}]}{\\eta n^{2}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The expectation on the right hand side is over the random initialization $x_{0}$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. We consider a Lyapunov sequence of the same form as before, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{L}_{n}=P(n)\\mathbb{E}\\left[f(x_{n})-f(x^{*})\\right]+\\frac{1}{2}\\mathbb{E}\\left[\\left\\|b(n)(x_{n}^{\\prime}-x_{n})+a(n)(x_{n}^{\\prime}-x^{*})\\right\\|^{2}\\right]\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $P(n)$ is some function of $n$ , $a(n)=a_{0}+a_{1}n,$ , and $b(n)=b_{0}+b_{1}n$ . ", "page_idx": 31}, {"type": "text", "text": "Since Nesterov\u2019s algorithm is a special case of AGNES, after substituting $\\alpha=\\eta$ , the analysis in steps 1, 2, and 3 of the proof of Theorem 3 remains valid. With that substitution, we get the following system of inequalities corresponding to step 3, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{c}{P(n)=\\eta(b(n)+a_{0})b(n)}\\\\ {P(n+1)+k-P(n)\\leq\\eta(b(n)+a_{0})a_{0}}\\\\ {\\frac{\\eta^{2}}{2}(b(n)+a_{0})^{2}(1+\\sigma^{2})\\leq(P(n+1)+k)\\eta\\left(1-\\frac{L(1+\\sigma^{2})\\eta}2\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Using the definition of $P(n)$ from (18), (20) is equivalent to ", "page_idx": 32}, {"type": "equation", "text": "$$\n(1+\\sigma^{2})\\le\\frac{2(b_{1}n+b_{1}+b_{0}+a_{0}+k)(b_{1}n+b_{0})\\left(1-\\frac{L(1+\\sigma^{2})\\eta}2\\right)}{(b_{1}n+b_{0}+a_{0})^{2}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "which should still hold in limit as $n\\to\\infty$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(1+\\sigma^{2})\\le\\displaystyle\\operatorname*{lim}_{n\\to\\infty}\\frac{2(b_{1}n+b_{1}+b_{0}+a_{0}+k)(b_{1}n+b_{0})\\left(1-\\frac{L(1+\\sigma^{2})\\eta}2\\right)}{(b_{1}n+b_{0}+a_{0})^{2}}}\\\\ &{\\qquad\\qquad=2\\left(1-\\frac{L(1+\\sigma^{2})\\eta}2\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This implies ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\eta\\le\\frac{1-\\sigma^{2}}{L(1+\\sigma^{2})}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We can choose $a_{0}=2,b(n)=n$ , and $k=\\eta$ . Then (18) implies that $P(n)=\\eta n(n+2)$ . (19) holds because ", "page_idx": 32}, {"type": "equation", "text": "$$\nP(n+1)+k-P(n)=\\eta(2n+4)=\\eta(b(n)+a_{0})a_{0}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and (20) holds because ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\eta}{2}(b(n)+a_{0})^{2}(1+\\sigma^{2})=\\frac{\\eta(n+2)^{2}(1+\\sigma^{2})}{2}}\\\\ {\\displaystyle\\leq\\frac{\\eta((n+1)(n+3)+1)(1+\\sigma^{2})}{2}}\\\\ {\\displaystyle=(P(n+1)+k)\\left(1-\\frac{L(1+\\sigma^{2})\\eta}2\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We have shown that the Lyapunov sequence ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathcal{L}_{n}=\\eta n(n+2)\\mathbb{E}[f(x_{n})-f(x^{*})]+\\frac{1}{2}\\mathbb{E}[\\|n(x_{n}^{\\prime}-x_{n})+2(x_{n}^{\\prime}-x^{*})\\|^{2}],\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\begin{array}{r}{\\eta\\le\\frac{1-\\sigma^{2}}{L(1+\\sigma^{2})}}\\end{array}$ , is monotonically decreasing. It follows that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\eta n(n+2)\\mathbb{E}[f(x_{n})-f(x^{*})]\\leq\\mathcal{L}_{n}\\leq\\mathcal{L}_{0}=2\\mathbb{E}[\\|x_{0}-x^{*}\\|^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We emphasize again that this analysis works only if \u03c3 < 1. The condition that \u03b7 \u2264 L(11\u2212+\u03c3\u03c322) is imposed by (20) and does not depend on any specific choice of $a_{0},b_{0}$ , or $b_{1}$ . On the other hand, (18) forces the rate of convergence to be inversely proportional to $\\eta$ . This means that as $\\sigma$ approaches 1, the step size $\\eta$ decreases to zero, and the rate of convergence blows up to infinity. On the other hand, as the proof of Theorem 3 shows, AGNES does not suffer from this problem. Having an additional parameter enables AGNES to converge even if the noise $\\sigma$ is arbitrarily large. ", "page_idx": 32}, {"type": "text", "text": "Let us point out how the same techniques used in Theorem 3 can be adapted to prove convergence $f(x_{n}){\\stackrel{*}{\\to}}\\operatorname*{inf}f$ , even if a global minimizer does not exist. We recall the main statement. ", "page_idx": 32}, {"type": "text", "text": "Theorem 7 (Convexity without minimizers). Let $f$ be a convex objective function satisfying the assumptions in Section 3.1 and $x_{n}$ be generated by the time-stepping scheme (3). Assume that $\\eta,\\alpha$ and $\\rho_{n}$ are as in Theorem 3. Then $\\begin{array}{r}{\\operatorname*{lim}\\operatorname*{inf}_{n\\to\\infty}\\mathbb{E}[f(x_{n})]=\\operatorname*{inf}_{x\\in\\mathbb{R}^{m}}f(x)}\\end{array}$ . ", "page_idx": 32}, {"type": "text", "text": "Proof. The first step follows along the same lines as the proof of Theorem 18 with minor modifications. Note that we did not use the minimizing property of $x^{*}$ except for Step 5.2. Assume for the moment that inf $f>-\\infty$ . ", "page_idx": 32}, {"type": "text", "text": "Assume first that $\\varepsilon:=\\operatorname*{lim}\\operatorname*{inf}_{n\\to\\infty}\\mathbb{E}[f(x_{n})]-\\operatorname*{inf}f>0$ . Select $x^{*}$ such that $f(x^{*})<\\operatorname*{inf}f+\\varepsilon/4$ and define the Lyapunov sequence ${\\mathcal{L}}_{n}$ just as in the proof of Theorem 3 with the selected point $x^{*}$ . We distinguish between two situations. First, assume that $n$ satisfies $\\mathbb{E}\\big[f(x_{n}^{\\prime})\\big]\\geq f(x^{*})$ . In this case we find that also $\\mathbb{E}\\big[f(x_{n+1})\\le\\mathbb{E}\\big[f(x_{n}^{\\prime})\\big]\\le f(x^{*})$ . ", "page_idx": 32}, {"type": "text", "text": "On the other hand, assume that $\\mathbb{E}\\big[f(x_{n}^{\\prime})\\big]\\geq f(x^{*})$ for $n=0,\\ldots,N$ . In that case, the proof of Theorem 3 still applies, meaning that $\\mathbb{E}[f(x_{N})]$ cannot remain larger than $f(x^{*})+\\varepsilon/2$ indefinitely. In either case, we find that there exists $N\\in\\mathbb{N}$ such that $\\begin{array}{r}{\\mathbb{E}\\big[f(x_{N})\\big]\\leq f(x^{*})+\\varepsilon/2<\\operatorname*{lim}\\operatorname*{inf}_{n\\to\\infty}\\mathbb{E}[f(x_{n})]}\\end{array}$ . ", "page_idx": 33}, {"type": "text", "text": "Note that the proof of Theorem 3 applies with $n^{\\prime}\\geq n_{0}$ as a starting point and a non-zero initial velocity $v_{n}$ . The argument therefore shows that, for every $n^{\\prime}\\in\\mathbb{N}$ there exists $N\\in\\mathbb{N}$ such that $\\mathbb{E}[f(x_{N})]\\leq\\operatorname*{lim}\\operatorname*{inf}_{n\\rightarrow\\infty}\\mathbb{E}[f(x_{n})]$ . Inserting the definition of the lower limit, we have reached a contradiction. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "We conjecture that the statement holds with the limit in place of the lower limit, but that it is impossible to guarantee a rate of convergence $O(n^{-\\beta})$ for any $\\beta>0$ in this setting. When following this strategy, the key question is how far away the point $x^{*}$ must be chosen. For very flat functions such as ", "page_idx": 33}, {"type": "equation", "text": "$$\nf_{\\alpha}:\\mathbb{R}\\rightarrow\\mathbb{R},\\qquad f_{\\alpha}(x)={\\binom{x^{-\\alpha}}{1+\\alpha(1-x)}}\\quad x>1,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "$x^{*}$ may be very far away from the initial point $x_{0}$ , and the rate of decay can be excrutiatingly slow if minimizers do not exist. For an easy example, we turn to the continuous time model. The solution to the heavy ball ODE ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{r l l}{x^{\\prime\\prime}}&{=-\\frac{3}{t}\\,x^{\\prime}-f_{\\alpha}^{\\prime}(x)}&{t>1}\\\\ {x}&{=1}&{t=1}\\\\ {x^{\\prime}}&{=-\\beta}&{t=1}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "is given by ", "page_idx": 33}, {"type": "equation", "text": "$$\nx(t)=\\left(\\frac{4\\left(3+\\alpha\\right)}{\\alpha(2+\\alpha)^{2}}\\right)^{\\frac{2}{2+\\alpha}}\\,t^{\\frac{2}{2+\\alpha}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for $\\begin{array}{r}{\\beta=\\frac{2}{2+\\alpha}\\left(\\frac{4\\,(3+\\alpha)}{\\alpha(2+\\alpha)^{2}}\\right)^{\\frac{2}{2+\\alpha}}>0.}\\end{array}$ . Ignoring the complicated constant factor, we see that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{\\alpha}(x(t))=x(t)^{-\\alpha}\\sim t^{-\\frac{2\\alpha}{2+\\alpha}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "the decay rate can be as close to zero as desired for $\\alpha$ close to zero, and indeed Siegel and Wojtowytsch [2023] show that no rate of decay can be guaranteed even beyond the situation of algebraic rates. For comparison, the solution of the gradient flow equation ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left[\\begin{array}{l l l l l l}{z^{\\prime}}&{=-f_{\\alpha}^{\\prime}(z)}&{t>0}&{}&{\\mathrm{is~given~by~}z(t)=\\left(1+\\alpha(2+\\alpha)t\\right)^{\\frac{1}{2+\\alpha}}}&{\\Rightarrow}&{f_{\\alpha}(z(t))\\sim t^{-\\frac{\\alpha}{2+\\alpha}}.}\\\\ {z}&{=1}&{t=0}&{}&{\\mathrm{is~given~by~}z(t)=\\left(1+\\alpha(2+\\alpha)t\\right)^{\\frac{1}{2+\\alpha}}}&{\\Rightarrow}&{f_{\\alpha}(z(t))\\sim t^{-\\frac{\\alpha}{2+\\alpha}}.}\\end{array}\\right]\\,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, while both the heavy ball ODE and the gradient flow can be made arbitrarily slow in this setting, the heavy ball remains much faster in comparison. ", "page_idx": 33}, {"type": "text", "text": "F Convergence proofs: strongly convex case ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "F.1 Gradient Descent ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Bassily et al. [2018], Wojtowytsch [2023] analyze stochastic gradient descent under the $\\mathrm{PL}$ condition ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mu{\\bigl(}f(x)-\\operatorname*{inf}f{\\bigr)}\\leq{\\frac{1}{2}}\\,\\|\\nabla f(x)\\|^{2}\\qquad\\forall\\,x\\in\\mathbb{R}^{m}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and the noise scaling assumption ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\omega}\\big[\\|g(x,\\omega)-\\nabla f(x)\\|^{2}\\big]\\leq\\sigma\\big(f(x)-\\operatorname*{inf}{f}\\big)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "motivated by Lemma 8. The assumption is equivalent to multiplicative noise scaling within a constant since every $L$ -smooth function which satisfies a PL condition satisfies ", "page_idx": 33}, {"type": "equation", "text": "$$\n2\\mu\\left(f(x)-\\operatorname*{inf}f\\right)\\leq\\|\\nabla f(x)\\|^{2}\\leq2L\\left(f(x)-\\operatorname*{inf}f\\right)\\!.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For completeness, we provide a statement and proof directly in the multiplicative noise scaling regime which attains the optimal constant. ", "page_idx": 33}, {"type": "text", "text": "Additionally, we note that strong convexity implies the PL condition. The PL condition holds in many cases where convexity is false, e.g. ", "page_idx": 34}, {"type": "equation", "text": "$$\nf(x,y)=(y-\\sin x)^{2},\\qquad\\|\\nabla f\\|^{2}\\geq|\\partial_{y}f|^{2}=4\\,f.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The set of minimizers $\\{(x,y):y=\\sin x\\}$ is non-convex, so $f$ cannot be convex. While this result is well-known to the experts, we have been unable to locate a reference and hence provide a proof. ", "page_idx": 34}, {"type": "text", "text": "Lemma 21. Assume that $f:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}$ is $\\mu$ -strongly convex and $C^{2}$ -smooth. Then $f$ satisfies the $P L$ -condition with constant $\\mu>0$ . ", "page_idx": 34}, {"type": "text", "text": "Proof. Let $x,y\\in\\mathbb{R}^{d}$ . Strong convexity combined with the Cauchy-Schwartz inequality means that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(x)-f(y)\\leq-\\langle\\nabla f(x),y-x\\rangle-\\displaystyle\\frac{\\mu}{2}\\|x-y\\|^{2}\\leq\\|\\nabla f(x)\\|\\|y-x\\|-\\displaystyle\\frac{\\mu}{2}\\|x-y\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\operatorname*{max}_{z\\in\\mathbb{R}}\\|\\nabla f(x)\\|z-\\displaystyle\\frac{\\mu}{2}z^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{2\\mu}\\|\\nabla f(x)\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Since this is true for $y=x^{*}$ , the result follows. ", "page_idx": 34}, {"type": "text", "text": "Several results in this vein are also collected in [Karimi et al., 2016, Theorem 2] together with additional generalizations of convexity, but with a suboptimal implication $\\stackrel{.}{\\mu}$ -strongly convex $\\&$ $L{\\mathrm{-smooth}})\\,{\\stackrel{\\cdot}{\\to}}\\,\\mu/L{\\mathrm{-PL}}$ . The additional implication (convexity & PL) $\\Rightarrow{}$ strong convexity can also be found there. ", "page_idx": 34}, {"type": "text", "text": "Theorem 22 (GD, PL condition). Assume that $f$ satisfies the $P L$ -condition (21) and that the assumptions laid out in Section 3.1 are satisfied. Let $x_{n}$ be the sequence generated by the gradient descent scheme ", "page_idx": 34}, {"type": "equation", "text": "$$\ng_{n}=g(x_{n},\\omega_{n}),\\qquad x_{n+1}=x_{n}-\\eta g_{n}^{\\prime},\\qquad\\eta\\leq\\frac{1}{L(1+\\sigma^{2})},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\omega_{1},\\omega_{2},\\ldots.$ are elements of $\\Omega$ which are drawn independently of each other and the initial condition $x_{0}$ . Then the estimate ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(x_{n})-\\operatorname*{inf}_{x\\in\\mathbb{R}^{m}}f(x)\\right]\\leq\\left(1-\\mu\\eta\\right)^{n}\\mathbb{E}\\left[f(x_{0})-\\operatorname*{inf}f\\right]\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "holds for any $n\\in\\mathbb{N}.$ . Additionally, the sequence $x_{n}$ converges to a limiting random variable $x_{\\infty}$ almost surely and in $L^{2}$ such that $f(x_{\\infty})\\equiv\\operatorname*{inf}f$ almost surely. ", "page_idx": 34}, {"type": "text", "text": "Proof. We denote ", "page_idx": 34}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{n}:=\\mathbb{E}\\left[f(x_{n})-\\operatorname*{inf}\\,f\\right]\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and compute by Lemma 16 that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{n+1}\\leq\\mathbb{E}\\left[f(x_{n})-\\frac{\\eta}{2}\\left\\Vert\\nabla f(x_{n})\\right\\Vert^{2}-\\operatorname*{inf}f\\right]}\\\\ &{\\qquad\\leq\\mathbb{E}\\left[f(x_{n})-\\mu\\eta\\left(f(x_{n})-f(x^{*})\\right)-\\operatorname*{inf}f\\right]}\\\\ &{\\qquad=\\left(1-\\mu\\eta\\right)\\!\\mathcal{L}_{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The proof of almost sure convergence is identical to the corresponding argument in [Wojtowytsch, 2023, Theorem 2.2] and similar in spirit to that of Corollary 5. ", "page_idx": 34}, {"type": "text", "text": "As usual, the optimal step-size is $\\begin{array}{r}{\\eta=\\frac{1}{L(1+\\sigma^{2})}}\\end{array}$ as used in Figure 1. ", "page_idx": 34}, {"type": "text", "text": "F.2 AGNES and NAG ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Just like the convex case, we first prove Theorem 4 and set up the Lyapunov sequence with variable coefficients that can be chosen as per the time-stepping scheme. The continuous time analogue in this case is the heavy-ball ODE ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l l}{\\ddot{x}}&{=-2\\sqrt{\\mu}\\,\\dot{x}-\\nabla f(x)}&{t>0}\\\\ {\\dot{x}}&{=0}&{t=0}\\\\ {x}&{=x_{0}}&{t=0}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For $\\mu$ -strongly convex $f$ , a simple calculation shows that the Lyapunov function ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{L}(t)=f(x(t))-f(x^{*})+\\frac{1}{2}\\left\\|\\dot{x}+\\sqrt{\\mu}\\big(x(t)-x^{*}\\big)\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "satisfies ${\\mathcal{L}}^{\\prime}(t)\\leq-{\\sqrt{\\mu}}\\,{\\mathcal{L}}(t)$ and thus ", "page_idx": 35}, {"type": "equation", "text": "$$\nf(x(t))-f(x^{*})\\leq\\mathcal{L}(t)\\leq e^{-\\sqrt{\\mu}\\,t}\\mathcal{L}(0)=e^{-\\sqrt{\\mu}\\,t}\\left(f(x_{0})-f(x^{*})+{\\frac{\\mu}{2}}\\,\\|x_{0}-x^{*}\\|^{2}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "See for instance [Siegel, 2019, Theorem 1] for details. ", "page_idx": 35}, {"type": "text", "text": "Here, we state and prove a slightly generalized version of Theorem 4 in the main text. While we assumed an optimal choice of parameters in the main text, we allow for a suboptimal selection here. Theorem 4 (AGNES, strongly convex case \u2013 general version). In addition to the assumptions in Theorem 3, suppose that $f$ is $\\mu$ -strongly convex and that ", "page_idx": 35}, {"type": "equation", "text": "$$\n0<\\eta\\leq\\frac{1}{L(1+\\sigma^{2})},\\qquad0<\\psi\\leq\\sqrt{\\frac{\\eta}{1+\\sigma^{2}}},\\qquad\\rho=\\frac{1-\\sqrt{\\mu}\\psi}{1+\\sqrt{\\mu\\psi}},\\qquad\\alpha=\\frac{\\psi-\\eta\\sqrt{\\mu}}{1-\\sqrt{\\mu}\\psi}\\,\\psi,\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "then ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[f(x_{n})-f(x^{*})\\big]\\leq\\left(1-\\sqrt{\\mu}\\,\\psi\\right)^{n}\\mathbb{E}\\left[f(x_{0})-f(x^{*})+\\frac{\\mu}{2}\\left\\|x_{0}-x^{*}\\right\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\mathbb{E}\\left[f(x_{0})-f(x^{*})+\\frac{\\mu}{2}\\left\\Vert x_{0}-x^{*}\\right\\Vert^{2}\\right]\\leq2\\mathbb{E}\\left[f(x_{0})-f(x^{*})\\right]}\\end{array}$ due to Lemma 12. A discussion about the set of admissible parameters is provided after the proof. We note several special cases here. ", "page_idx": 35}, {"type": "text", "text": "1. If $\\psi$ is selected optimally as $\\sqrt{\\eta/(1+\\sigma^{2})}$ for $\\eta$ , the order of decay is $\\begin{array}{r}{1-\\sqrt{\\frac{\\mu\\eta}{1+\\sigma^{2}}}}\\end{array}$ 1+\u00b5\u03b7\u03c32 , strongly resembling Theorem 3.   \n2. If additionally $\\eta\\,=\\,1/(L(1+\\sigma^{2}))$ is chosen optimally, then we recover the decay rate $1-\\sqrt{\\mu/L}\\,/(1+\\sigma^{2})$ claimed in the main text.   \n3. We recover the gradient descent algorithm with the choice $\\alpha\\,=\\,0$ which is achieved for $\\psi=\\eta\\sqrt{\\mu}$ . This selection is admissible in our analysis since ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sqrt{\\mu\\eta}\\leq\\sqrt{\\frac{\\mu}{L}}\\,\\frac{1}{1+\\sigma^{2}}\\leq\\sqrt{\\frac{1}{1+\\sigma^{2}}}\\quad\\Rightarrow\\quad\\eta\\sqrt{\\mu}\\leq\\sqrt{\\frac{\\eta}{1+\\sigma^{2}}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "As expected, the constant of decay is $1-\\sqrt{\\mu}\\,\\psi=1-\\mu\\eta$ , as achieved in Theorem 22. In this sense, our analysis of AGNES interpolates fully between the optimal AGNES scheme (a NAG-type scheme in the deterministic case) and (stochastic) gradient descent. However, this proof only applies in the strongly convex setting, but not under a mere PL assumption. 4. If $\\mu<L-\\mathrm{i.e.}$ . if $f(x)\\not\\equiv A+\\mu\\|x-x^{*}\\|^{2}$ for some $A\\in\\mathbb{R}$ and $x_{0}\\in\\mathbb{R}^{m}$ \u2013 then we can choose $0<\\psi<\\sqrt{\\mu}\\,\\eta$ , corresponding to $\\alpha<0$ . In this case, the gradient step is sufficiently strong to compensate for momentum taking us in the wrong direction. Needless to say, this is a terrible idea and the rate of convergence is worse than that of gradient descent. ", "page_idx": 35}, {"type": "text", "text": "Proof. Set-up. Consider the Lyapunov sequence ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{L}_{n}=\\mathbb{E}\\big[f(x_{n})-f(x^{*})\\big]+\\frac{1}{2}\\,\\mathbb{E}\\left[\\|b(x_{n}^{\\prime}-x_{n})+a(x_{n}^{\\prime}-x^{*})\\|^{2}\\right]\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "for constants $b,a$ to be chosen later. We want to show that there exists some decay factor $0<\\delta<1$ such that ${\\mathcal{L}}_{n+1}\\leq\\delta{\\mathcal{L}}_{n}$ . ", "page_idx": 35}, {"type": "text", "text": "Step 1. Let us consider the first term. Note that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\big[f(x_{n+1})\\big]=\\mathbb{E}\\big[f(x_{n}^{\\prime}-\\eta g_{n}^{\\prime})\\big]}&{}\\\\ {\\le\\mathbb{E}\\big[f(x_{n}^{\\prime})\\big]-c_{\\eta,\\sigma,L}\\mathbb{E}\\big[\\|\\nabla f(x_{n}^{\\prime})\\|^{2}\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $\\begin{array}{r}{c_{\\eta,\\sigma,L}=\\eta\\left(1-\\frac{L\\eta(1+\\sigma^{2})}{2}\\right)\\geq\\eta/2}\\end{array}$ if $\\begin{array}{r}{\\eta\\le\\frac{1}{L(1+\\sigma^{2})}}\\end{array}$ . ", "page_idx": 35}, {"type": "text", "text": "Step 2. We now turn to the second term and use the definition of $x_{n+1}^{\\prime}$ from (2), ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{b(x_{n+1}^{\\prime}-x_{n+1})+a(x_{n+1}^{\\prime}-x^{*})=b\\rho(x_{n}^{\\prime}-\\alpha g_{n}^{\\prime}-x_{n})+a(x_{n+1}+\\rho(x_{n}^{\\prime}-\\alpha g_{n}^{\\prime}-x_{n})-x^{*})}\\\\ &{\\phantom{b(x_{n+1}^{\\prime}-x_{n+1})+}=(b+a)\\rho(x_{n}^{\\prime}-\\alpha g_{n}^{\\prime}-x_{n})+a(x_{n}^{\\prime}-\\eta g_{n}^{\\prime}-x^{*})}\\\\ &{\\phantom{b(x_{n+1}^{\\prime}-x_{n+1})+a(x_{n}^{\\prime}-x_{n})+a(x_{n}^{\\prime}-x^{*})-((b+a)\\rho\\alpha+\\eta a)g_{n}^{\\prime}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "To simplify notation, we introduce two new dependent variables: ", "page_idx": 36}, {"type": "equation", "text": "$$\nc:=(b+a)\\rho,\\qquad\\psi:=(b+a)\\rho\\alpha+\\eta a=\\alpha c+\\eta a.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "With these variables, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\nb\\big({x}_{n+1}^{\\prime}-{x}_{n+1}\\big)+a\\big({x}_{n+1}^{\\prime}-{x}^{*}\\big)=c\\,({x}_{n}^{\\prime}-{x}_{n})+a({x}_{n}^{\\prime}-{x}^{*})-\\psi g_{n}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Taking expectation of the square, we find that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Xi\\left[\\left\\|b(x_{n+1}^{\\prime}-x_{n+1})+a\\big(x_{n+1}^{\\prime}-x^{*}\\big)\\right\\|^{2}\\right]}\\\\ &{\\quad=c^{2}\\,\\mathbb{E}\\big[\\|x_{n}^{\\prime}-x_{n}\\|^{2}\\big]+2a c\\,\\mathbb{E}\\big[(x_{n}^{\\prime}-x_{n})\\cdot(x_{n}^{\\prime}-x^{*})\\big]+a^{2}\\mathbb{E}\\big[\\|(x_{n}^{\\prime}-x^{*})\\|^{2}\\big]}\\\\ &{\\quad\\quad-2c\\,\\psi\\,\\mathbb{E}\\big[g_{n}^{\\prime}\\cdot(x_{n}^{\\prime}-x_{n})\\big]-2a\\psi\\,\\mathbb{E}\\big[g_{n}^{\\prime}\\cdot(x_{n}^{\\prime}-x^{*})\\big]+\\psi^{2}\\,\\mathbb{E}\\big[\\|g_{n}^{\\prime}\\|^{2}\\big]}\\\\ &{\\quad\\le c^{2}\\,\\mathbb{E}\\big[\\|x_{n}^{\\prime}-x_{n}\\|^{2}\\big]+2a c\\,\\mathbb{E}\\big[(x_{n}^{\\prime}-x_{n})\\cdot(x_{n}^{\\prime}-x^{*})\\big]+a^{2}\\mathbb{E}\\big[\\|(x_{n}^{\\prime}-x^{*})\\|^{2}\\big]}\\\\ &{\\quad\\quad-2c\\,\\psi\\,\\mathbb{E}\\big[\\nabla f(x_{n}^{\\prime})\\cdot(x_{n}^{\\prime}-x_{n})\\big]-2a\\psi\\,\\mathbb{E}\\big[\\nabla f(x_{n}^{\\prime})\\cdot(x_{n}^{\\prime}-x^{*})\\big]+\\psi^{2}(1+\\sigma^{2})\\,\\mathbb{E}\\big[\\|\\nabla f(x_{n}^{\\prime})\\|^{2}\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Step 3. We now use strong convexity to deduce that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert b\\left(x_{n+1}^{\\prime}-x_{n+1}\\right)+a\\left(x_{n+1}^{\\prime}-x^{*}\\right)\\right\\Vert^{2}\\right]}\\\\ &{\\quad\\leq c^{2}\\mathbb{E}\\left[\\Vert x_{n}^{\\prime}-x_{n}\\Vert^{2}\\right]+2a c\\mathbb{E}\\left[\\left(x_{n}^{\\prime}-x_{n}\\right)\\cdot\\left(x_{n}^{\\prime}-x^{*}\\right)\\right]+a^{2}\\mathbb{E}\\left[\\Vert\\left(x_{n}^{\\prime}-x^{*}\\right)\\Vert^{2}\\right]}\\\\ &{\\quad\\quad-2c\\psi\\mathbb{E}\\left[f(x_{n}^{\\prime})-f(x_{n})+\\frac{\\mu}{2}\\left\\Vert x_{n}^{\\prime}-x_{n}\\right\\Vert^{2}\\right]-2a\\psi\\,\\mathbb{E}\\left[f(x_{n}^{\\prime})-f(x^{*})+\\frac{\\mu}{2}\\left\\Vert x_{n}^{\\prime}-x^{*}\\right\\Vert^{2}\\right]}\\\\ &{\\quad\\quad+\\psi^{2}(1+\\sigma^{2})\\mathbb{E}\\left[\\Vert\\nabla f(x_{n}^{\\prime})\\Vert^{2}\\right]}\\\\ &{=(c^{2}-c\\psi\\mu)\\mathbb{E}\\left[\\Vert x_{n}^{\\prime}-x_{n}\\Vert^{2}\\right]+2a c\\mathbb{E}\\left[(x_{n}^{\\prime}-x_{n})\\cdot(x_{n}^{\\prime}-x^{*})\\right]+\\left(a^{2}-a\\psi\\mu\\right)\\,\\mathbb{E}\\big[\\Vert x_{n}^{\\prime}-x^{*}\\Vert^{2}\\big]}\\\\ &{\\quad\\quad-2c\\psi\\mathbb{E}\\left[f(x_{n}^{\\prime})-f(x_{n})\\right]-2a\\psi\\,\\mathbb{E}\\left[f(x_{n}^{\\prime})-f(x^{*})\\right]+\\psi^{2}(1+\\sigma^{2})\\,\\mathbb{E}\\big[\\Vert\\nabla f(x_{n}^{\\prime})\\Vert^{2}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Step 4. We now add the estimates of Steps 1 and 3: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{n+1}=\\mathbb{E}\\left[f(x_{n+1})-f(x^{*})+\\displaystyle\\frac{1}{2}\\left\\|b(x_{n+1}^{\\prime}-x_{n+1})+a(x_{n+1}^{\\prime}-x^{*})\\right\\|^{2}\\right]}\\\\ &{\\qquad\\leq(1-c\\psi-a\\psi)\\ \\mathbb{E}\\big[f(x_{n}^{\\prime})\\big]+c\\psi\\,\\mathbb{E}\\big[f(x_{n})\\big]-(1-a\\psi)\\,\\mathbb{E}\\big[f(x^{*})\\big]}\\\\ &{\\qquad\\quad+\\displaystyle\\frac{1}{2}(c^{2}-c\\psi\\mu)\\,\\mathbb{E}\\big[\\|x_{n}^{\\prime}-x_{n}\\|^{2}\\big]+a c\\,\\mathbb{E}\\big[(x_{n}^{\\prime}-x_{n})\\cdot(x_{n}^{\\prime}-x^{*})\\big]}\\\\ &{\\qquad+\\displaystyle\\frac{1}{2}\\left(a^{2}-a\\psi\\mu\\right)\\,\\mathbb{E}\\big[\\|x_{n}^{\\prime}-x^{*}\\|^{2}\\big]+\\left(\\frac{\\psi^{2}(1+\\sigma^{2})}{2}-c_{\\eta,\\sigma,L}\\right)\\,\\mathbb{E}\\big[\\|\\nabla f(x_{n}^{\\prime})\\|^{2}\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We require the coefficient of $\\mathbb{E}[f(x_{n}^{\\prime})]$ to be zero, i.e. $1-a\\psi=c\\psi$ , so the inequality simplifies to ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{L}_{n+1}\\leq c\\psi\\,\\mathbb{E}\\big[f(x_{n})-f(x^{*})\\big]+\\frac{1}{2}(c^{2}-c\\psi\\mu)\\,\\mathbb{E}\\big[\\|x_{n}^{\\prime}-x_{n}\\|^{2}\\big]}\\\\ {\\displaystyle\\qquad+\\,a c\\,\\mathbb{E}\\big[(x_{n}^{\\prime}-x_{n})\\cdot(x_{n}^{\\prime}-x^{*})\\big]+\\frac{1}{2}\\left(a^{2}-a\\psi\\mu\\right)\\,\\mathbb{E}\\big[\\|x_{n}^{\\prime}-x^{*}\\|^{2}\\big]}\\\\ {\\displaystyle\\qquad+\\,\\left(\\frac{\\psi^{2}(1+\\sigma^{2})}{2}-c_{\\eta,\\sigma,L}\\right)\\,\\mathbb{E}\\big[\\|\\nabla f(x_{n}^{\\prime})\\|^{2}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The smallest decay factor we can get at this point is the coefficient of $\\mathbb{E}[f(x_{n})-f(x^{*})]$ . So we hope to show that ${\\mathcal{L}}_{n+1}\\leq c\\psi{\\mathcal{L}}_{n}$ , which leads to the following system of inequalities on comparing it ", "page_idx": 36}, {"type": "text", "text": "with the coefficients in the upper bound that we obtained in the previous step, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{c=(b+a)\\rho}}\\\\ {{\\psi=\\alpha c+\\eta a}}\\\\ {{(c+a)\\psi=1}}\\\\ {{c^{2}-c\\psi\\mu\\leq c\\psi b^{2}}}\\\\ {{a c=c\\psi a b}}\\\\ {{a^{2}-a\\psi\\mu\\leq c\\psi a^{2}}}\\\\ {{\\displaystyle\\frac{(1+\\sigma^{2})\\psi^{2}}{2}\\leq\\eta\\left(1-\\frac{L\\left(1+\\sigma^{2}\\right)\\eta}{2}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Step 5. Now we try to choose constants such that the system of inequalities holds. We assume that $\\begin{array}{r}{\\eta\\le\\frac{1}{L(1+\\sigma^{2})}}\\end{array}$ . Then since $\\begin{array}{r}{\\frac{\\eta}{2}\\le\\eta\\left(1-\\frac{L(1+\\sigma^{2})\\eta}{2}\\right)}\\end{array}$ , for (29) it suffices that $(1+\\sigma^{2})\\psi^{2}\\leq\\eta$ , i.e. ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\psi\\leq\\sqrt{\\frac{\\eta}{1+\\sigma^{2}}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Note that (27) implies $\\psi=1/b$ and substituting that into (25), we get $c=b-a$ . Using this, (28) is equivalent to ", "page_idx": 37}, {"type": "equation", "text": "$$\na^{2}-a\\psi\\mu\\leq c\\psi a^{2}=({\\frac{1}{\\psi}}-a)\\psi a^{2}=a^{2}-a^{2}\\psi,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "which holds with equality if $a=\\sqrt{\\mu}$ . (26) holds because ", "page_idx": 37}, {"type": "equation", "text": "$$\nc-\\psi\\mu=b-a-\\psi\\mu\\leq b=\\psi b^{2},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "if $\\mu,\\psi>0$ . Finally (23) implies ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\rho=\\frac{b-a}{b+a}=\\frac{1-\\sqrt{\\mu}\\psi}{1+\\sqrt{\\mu}\\psi},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and (24) implies ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\alpha=\\frac{\\frac{1}{b}-\\eta a}{b-a}=\\frac{\\psi^{2}-\\eta\\sqrt{\\mu}\\psi}{1-\\sqrt{\\mu}\\psi}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "With these choices of parameters, $\\mathcal{L}_{n+1}\\leq c\\psi\\mathcal{L}_{n}=(1-\\sqrt{\\mu}\\psi)\\mathcal{L}_{n}$ , and thus ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}[f(x_{n})-f(x^{*})]\\leq(1-\\sqrt{\\mu}\\psi)^{n}\\mathcal{L}_{0}}\\\\ {\\displaystyle=(1-\\sqrt{\\mu}\\psi)^{n}\\mathbb{E}\\left[f(x_{0})-f(x^{*})+\\frac{\\mu}{2}\\left\\|x_{0}-x^{*}\\right\\|^{2}\\right]}\\\\ {\\displaystyle\\leq2(1-\\sqrt{\\mu}\\psi)^{n}\\mathbb{E}\\left[f(x_{0})-f(x^{*})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where we have used Lemma 12 for strong convexity in the last step. When the parameters are chosen optimally, i.e. $\\begin{array}{r}{\\eta=\\frac{1}{L(1+\\sigma^{2})}}\\end{array}$ and 1+\u03b7\u03c32 =\u221aL(11+\u03c32), we get \u03c1, \u03b1 and the convergence rate as stated in the theorem. \u53e3 ", "page_idx": 37}, {"type": "text", "text": "We focus on the meaningful case in which $\\sqrt{\\mu}\\psi>0$ . As discussed in Section 3, for given $f,g$ we can replace $L,\\sigma$ by larger values $L^{\\prime},\\sigma^{\\prime}$ and $\\mu$ by a smaller value $\\mu^{\\prime}$ . Let us briefly explore the effect of these substitutions. The parameter range described in this version of Theorem 4 can be understood as a three parameter family of AGNES parameters $\\eta,\\alpha,\\rho$ parametrized by $\\eta,\\psi,\\mu^{\\prime}$ and constraints given by $L,\\mu,\\sigma$ as ", "page_idx": 37}, {"type": "equation", "text": "$$\nD:=\\left\\lbrace(\\eta,\\psi,\\mu^{\\prime})\\;\\middle|\\;0<\\eta\\leq\\frac{1}{L(1+\\sigma^{2})},\\;\\;0<\\psi\\leq\\sqrt{\\frac{\\eta}{1+\\sigma^{2}}},\\;\\;0<\\mu^{\\prime}\\leq\\mu\\right\\rbrace.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "The parameter map is given by ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left(\\eta,\\psi,\\mu^{\\prime}\\right)\\mapsto\\left(\\eta,\\rho,\\alpha\\right)=\\left(\\eta,\\,\\frac{1-\\sqrt{\\mu^{\\prime}}\\psi}{1+\\sqrt{\\mu^{\\prime}}\\psi},\\;\\frac{\\psi-\\eta\\sqrt{\\mu^{\\prime}}}{1-\\sqrt{\\mu^{\\prime}}\\,\\psi}\\,\\psi\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We can co\u221anveresely obtain $\\sqrt{\\mu^{\\prime}}\\,\\psi$ from $\\rho$ since the function $z\\mapsto(1-z)/(1+z)$ is its own inverse and thus  \u00b5\u2032 \u03c8 = 11\u2212+\u03c1\u03c1 . In particular, in terms of the algorithms parameters, the decay rate is ", "page_idx": 38}, {"type": "equation", "text": "$$\n1-\\sqrt{\\mu^{\\prime}}\\psi=1-\\frac{1-\\rho}{1+\\rho}=\\frac{2\\rho}{1+\\rho}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Furthermore, we see that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\alpha=\\frac{\\psi^{2}-\\eta\\,\\sqrt{\\mu^{\\prime}}\\psi}{1-\\sqrt{\\mu^{\\prime}}\\psi}=\\frac{\\psi^{2}-\\eta\\,\\frac{1-\\rho}{1+\\rho}}{1-\\frac{1-\\rho}{1+\\rho}}=\\frac{1+\\rho}{2\\rho}\\left(\\psi^{2}-\\eta\\,\\frac{1-\\rho}{1+\\rho}\\right)\\quad\\Leftrightarrow\\quad\\psi=\\sqrt{\\frac{2\\rho}{1+\\rho}\\alpha+\\eta\\,\\frac{1-\\rho}{1+\\rho}}\\;.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "since $\\psi\\,>\\,0$ . Thus, at the cost of a more complicated representation, we could work directly in the parameter variables rather than using the auxiliary quantities $\\psi,\\mu^{\\prime}$ . In particular, both the parameter map and its inverse are continuous on $D$ and its image respectively. Hence,despite the rigid appearance of the parameter selection in Theorem 4, there exists an open set of admissible parameters $\\eta,\\alpha,\\rho$ for which we obtain exponentially fast convergence. ", "page_idx": 38}, {"type": "text", "text": "We provide a more general version of Theorem 2 as well. Just as in the convex case, as $\\sigma\\nearrow1$ , the step size $\\eta$ decreases to zero and the theorem fails to guarantee convergence for $\\sigma>1$ . ", "page_idx": 38}, {"type": "text", "text": "Theorem 2 (NAG, strongly convex case). In addition to the assumptions in Theorem $^{\\,l}$ , suppose that $f$ is $\\mu$ -strongly convex and the parameters are chosen such that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathrm{1<\\eta\\leq}\\frac{1-\\sigma^{2}}{L(1+\\sigma^{2})}\\ a n d\\rho=\\frac{1-\\sqrt{\\mu\\eta}}{1+\\sqrt{\\mu\\eta}},\\ t h e n\\,\\mathbb{E}[f(x_{n})-f(x^{*})]\\leq2(1-\\sqrt{\\mu\\eta})^{n}\\,\\mathbb{E}\\left[f(x_{0})-f(x^{*})\\right].\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. Consider the Lyapunov sequence ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathcal{L}_{n}=\\mathbb{E}\\left[f(x_{n})-f(x^{*})\\right]+\\frac{1}{2}\\mathbb{E}\\left[\\|b(x_{n}^{\\prime}-x_{n})+a(x_{n}^{\\prime}-x^{*})\\|^{2}\\right],\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $a$ and $b$ are to be determined later. Since NAG is a special case of AGNES with $\\alpha=\\eta$ , the first four steps are identical to the proof of Theorem 4. We get the following system of inequalities, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{c=(a+b)\\rho}}\\\\ {{\\psi=\\eta(a+c)}}\\\\ {{(c+a)\\psi=1}}\\\\ {{c^{2}-c\\psi\\mu\\leq b^{2}\\psi}}\\\\ {{a^{2}-a\\psi\\mu\\leq a^{2}\\psi}}\\\\ {{a c=a b c\\psi}}\\\\ {{\\psi^{2}(1+\\sigma^{2})}}\\\\ {{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Substituting (31) into (32), we get $\\begin{array}{r}{(a+c)^{2}=\\frac{1}{\\eta}}\\end{array}$ and $\\psi=\\eta/\\sqrt{\\eta}=\\sqrt{\\eta}$ . Thus (35) simplifies to ", "page_idx": 38}, {"type": "equation", "text": "$$\n{\\frac{1-\\sigma^{2}}{2}}\\leq1-{\\frac{L\\eta(1+\\sigma^{2})}{2}},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which is equivalent to ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\eta\\le\\frac{1-\\sigma^{2}}{L(1+\\sigma^{2})}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "From (35), $b=1/\\psi=1/\\sqrt{\\eta}$ . The rest of the inequalities can be verified to work with $a={\\sqrt{\\mu}},c=$ $\\begin{array}{r}{b-a,\\rho=\\frac{b-a}{b+a}}\\end{array}$ . This shows that $\\mathcal{L}_{n+1}\\leq c\\psi\\mathcal{L}_{n}=(1-\\sqrt{\\mu\\eta})\\mathcal{L}_{n}$ . Finally, we get ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[f(x_{n})-f(x^{*})]\\leq(1-\\sqrt{\\mu\\eta})^{n}\\,\\mathbb{E}\\left[f(x_{0})-f(x^{*})+\\displaystyle\\frac{\\mu}{2}\\,\\|x_{0}-x^{*}\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq2(1-\\sqrt{\\mu\\eta})^{n}\\,\\mathbb{E}\\left[f(x_{0})-f(x^{*})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "F.3 On the role of momentum parameters ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Two different AGNES parameters are associated with momentum: $\\alpha$ and $\\rho$ . In this section, we disentangle their respective contributions to keeping AGNES stable for highly stochastic noise. ", "page_idx": 39}, {"type": "text", "text": "For simplicity, first consider the case $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ , $f(x)=x$ and $g(x)=(1+\\sigma N)\\,f^{\\prime}(x)$ where $N$ is a standard normal random variable. Then ", "page_idx": 39}, {"type": "equation", "text": "$$\nv_{n+1}=\\rho(v_{n}-g_{n}^{\\prime})=\\cdots=-\\rho\\sum_{i=0}^{n}\\rho^{n-i}g_{i}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "since $v_{0}=0$ . In particular, we note that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}[v_{n+1}]=-\\rho\\sum_{i=1}^{n}\\rho^{n-i}\\mathbb{E}[g_{i}^{\\prime}]=-\\rho\\sum_{i=1}^{n}\\rho^{n-i}=-\\rho\\frac{1-\\rho^{n+1}}{1-\\rho}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left|v_{n+1}-\\left(-\\rho\\frac{1-\\rho^{n+1}}{1-\\rho}\\right)\\right|^{2}\\right]=\\rho^{2}\\mathbb{E}\\left[\\left|\\displaystyle\\sum_{i=0}^{n}\\rho^{n-i}(g_{i}^{\\prime}-1)\\right|^{2}\\right]=\\sigma^{2}\\rho^{2}\\sum_{i=0}^{n}\\rho^{2(n-i)}\\mathbb{E}\\left[|g_{i}^{\\prime}-1|^{2}\\right]}\\\\ {=\\sigma^{2}\\rho^{2}\\sum_{i=0}^{n}\\rho^{2(n-i)}=\\sigma^{2}\\rho^{2}\\frac{1-\\rho^{2(n+1)}}{1-\\rho^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "due to the independence of different gradient estimators between time steps. In particular, we see that ", "page_idx": 39}, {"type": "text", "text": "1. as $\\rho$ becomes closer to 1, the eventual magnitude of the velocity variable increases as limn\u2192\u221eE\u2225vn\u2225=1\u2212\u03c1\u03c1.   \n2. as $\\rho$ becomes closer to 1, the eventual variance of the velocity variable increases as limn\u2192\u221eE \u2225vn \u2212E[vn]\u22252 =1\u2212\u03c12\u03c12 .   \n3. the noise in the normalized velocity estimate asymptotically satisfies $\\operatorname*{lim}_{n\\to\\infty}\\mathbb{E}\\left[\\left\\Vert\\frac{v_{n}-\\mathbb{E}[v_{n}]}{\\mathbb{E}[\\Vert v_{n}\\Vert]}\\right\\Vert^{2}\\right]=\\sigma^{2}\\frac{(1-\\rho)^{2}}{1-\\rho^{2}}=\\sigma^{2}\\frac{(1-\\rho)^{2}}{(1-\\rho)(1+\\rho)}=\\sigma^{2}\\frac{1-\\rho}{1+\\rho}$ ", "page_idx": 39}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Thus, if $\\rho$ is closer to 1, both the magnitude and the variance of the velocity variable increase, but the the relative importance of noise approaches zero as $\\rho\\rightarrow1$ . This is not surprising \u2013 if $\\rho$ is close to 1, the sequence $\\rho^{n}$ decays much slower than if $\\rho$ is small. Gradient estimates from different times enter at a similar scale and cancellations can occur easily. As the influence of past gradients remains large, we say that the momentum variable has a \u2018long memory\u2019. ", "page_idx": 39}, {"type": "text", "text": "Of course, when minimizing a non-linear function $f$ , the gradient is not constant, and we face a trade-off: ", "page_idx": 39}, {"type": "text", "text": "1. A long memory allows us to cancel random oscillations in the gradient estimates more easily.   \n2. A long memory also means we compute with more out-of-date gradient estimates from points much further in the past along the trajectory. ", "page_idx": 39}, {"type": "text", "text": "Naturally, the relative importance of the first point increases with the stochasticity $\\sigma$ of the gradient estimates. Even if the gradient evaluations are deterministic, we benefit from integrating historic information gained throughout the optimization process, but the rate at which we \u2018forget\u2019 outdated information is much higher. ", "page_idx": 39}, {"type": "text", "text": "Thus the parameter $\\rho$ corresponds to the rate at which we forget old information. It also impacts the magnitude of the velocity variable. The parameter $\\alpha$ compensates for the scaling of $v_{n}$ with $1/(1-\\rho)$ . We can think of $\\rho$ as governing the rate at which we forget past gradients, and $\\alpha$ as a measure of the confidence with which we integrate past gradient information into time-steps for $x$ . ", "page_idx": 39}, {"type": "text", "text": "Let us explore this relationship in strongly convex optimization. In Theorem 4, the optimal choice of hyper-parameters is given by $\\begin{array}{r}{\\dot{\\eta}=\\frac{1^{-}}{L(1+\\sigma^{2})}}\\end{array}$ and ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\alpha=\\frac{1-\\sqrt{\\mu/L}}{1-\\sqrt{\\mu/L}+\\sigma^{2}}\\eta,\\;\\;\\;\\;\\;\\;\\;\\;\\rho=\\frac{\\sqrt{L}\\left(1+\\sigma^{2}\\right)-\\sqrt{\\mu}}{\\sqrt{L}(1+\\sigma^{2})+\\sqrt{\\mu}}=1-\\frac{2\\sqrt{\\mu}}{\\sqrt{L}(1+\\sigma^{2})+\\sqrt{\\mu}}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Let us consider the simplified regime $\\mu\\ll L$ in which ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\alpha\\approx\\frac{\\eta}{1+\\sigma^{2}},\\qquad\\rho\\approx1-2\\,\\sqrt{\\frac{\\mu}{L}}\\,\\frac{1}{1+\\sigma^{2}}\\quad\\Rightarrow\\quad\\frac{\\alpha}{1-\\rho}=\\frac{\\eta}{2\\,\\sqrt{\\mu/L}}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "In particular, we note: The larger $\\sigma$ , the closer $\\rho$ is to 1, i.e. the longer the memory we keep. The relative importance of the momentum step compared to the gradient step, on the other hand, remains constant, depending only on the \u2018condition number\u2019 $L/\\mu$ . ", "page_idx": 40}, {"type": "text", "text": "We note that also in the convex case, high stochasticity forces $n_{0}$ to be large, meaning that $\\rho_{n}$ is always close to 1. Notably for generic non-convex objective functions, it is unclear that past gradients along the trajectory would carry useful information, as there is no discernible geometric relationship between gradients at different points. This mirrors an observation of Appendix G, just after Theorem 23. ", "page_idx": 40}, {"type": "text", "text": "G AGNES in non-convex optimization ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "We consider the case of non-convex optimization. In the deterministic setting, momentum methods for non-convex optimization have recently been studied by Diakonikolas and Jordan [2021]. We note that the algorithm may perform worse than stochastic gradient descent, but that for suitable parameters, the performance is comparable to that of SGD within a constant factor. ", "page_idx": 40}, {"type": "text", "text": "Theorem 23 (Non-convex case). Assume that $f$ satisfies the assumptions laid out in Section 3.1. Let $\\eta,\\alpha,\\rho$ be such that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\eta\\leq\\frac{1}{L(1+\\sigma^{2})},\\qquad\\alpha<\\frac{\\eta}{1+\\sigma^{2}},\\qquad(L\\alpha+1)\\rho^{2}\\leq1.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Then ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq i\\leq n}\\mathbb{E}\\big[\\|\\nabla f(x_{i})\\|^{2}\\big]\\leq\\frac{2\\,\\mathbb{E}\\left[f(x_{0})-\\operatorname*{inf}f+\\frac{1}{\\alpha\\rho^{2}}\\,\\|v_{0}\\|^{2}\\right]}{(n+1)\\,\\left(\\eta-\\alpha(1+\\sigma^{2})\\right)}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "If $v_{0}~=~0$ , the bound is minimal for gradient descent (i.e. $\\alpha\\,=\\,0$ ) since the decay factor $\\varepsilon=$ $\\eta-\\alpha(1+\\sigma^{2})$ is maximal. ", "page_idx": 40}, {"type": "text", "text": "Proof. Consider ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathcal{L}_{n}=\\mathbb{E}\\left[f(x_{n})+\\frac{\\lambda}{2}\\left\\Vert x_{n}^{\\prime}-x_{n}\\right\\Vert^{2}\\right].\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "for a parameter $\\lambda>0$ to be fixed later. We have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathbb{E}\\big[f(x_{n+1})\\big]\\leq\\mathbb{E}\\big[f(x_{n}^{\\prime})\\big]-\\frac{\\eta}{2}\\,\\mathbb{E}\\big[\\|\\nabla f(x_{n}^{\\prime})\\|^{2}\\big]}\\\\ {\\displaystyle\\leq\\mathbb{E}\\left[f(x_{n})+\\nabla f(x_{n}^{\\prime})\\cdot(x_{n}^{\\prime}-x_{n})+\\frac{L\\,\\alpha^{2}}{2}\\|v_{n}\\|^{2}-\\frac{\\eta}{2}\\,\\|\\nabla f(x_{n}^{\\prime})\\|^{2}\\right]}\\\\ {\\displaystyle\\mathbb{E}\\big[\\|x_{n+1}^{\\prime}-x_{n+1}\\|^{2}\\big]=\\rho^{2}\\mathbb{E}\\big[\\|(x_{n}^{\\prime}-x_{n})\\|^{2}-2\\alpha\\,(x_{n}^{\\prime}-x_{n})\\cdot g_{n}^{\\prime}+\\alpha^{2}\\,\\|g_{n}^{\\prime}\\|^{2}\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "by Lemmas 13 and 16. We deduce that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathcal{L}_{n+1}\\leq\\mathbb{E}\\big[f(x_{n})\\big]+\\left(1-\\lambda\\alpha\\rho^{2}\\right)\\mathbb{E}\\big[\\nabla f(x_{n}^{\\prime})\\cdot(x_{n}^{\\prime}-x_{n})\\big]+\\displaystyle\\frac{L+\\lambda\\rho^{2}}{2}\\,\\mathbb{E}\\big[\\|x_{n}^{\\prime}-x_{n}\\|^{2}\\big]}\\\\ {\\qquad\\qquad+\\,\\displaystyle\\frac{\\lambda\\rho^{2}\\alpha\\cdot\\alpha(1+\\sigma^{2})-\\eta}{2}\\,\\mathbb{E}\\big[\\|\\nabla f(x_{n}^{\\prime})\\|^{2}\\big]}\\\\ {\\leq\\mathcal{L}_{n}+\\displaystyle\\frac{\\lambda\\rho^{2}\\alpha\\cdot\\alpha(1+\\sigma^{2})-\\eta}{2}\\,\\mathbb{E}\\big[\\|\\nabla f(x_{n}^{\\prime})\\|^{2}\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "under the conditions ", "page_idx": 41}, {"type": "equation", "text": "$$\n1-\\lambda\\alpha\\rho^{2}=0,\\qquad L+\\lambda\\rho^{2}\\le\\lambda.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The first condition implies that $\\lambda=(\\alpha\\rho^{2})^{-1}$ , so the second one reduces to ", "page_idx": 41}, {"type": "equation", "text": "$$\n(1-\\rho^{2})\\lambda=\\frac{1-\\rho^{2}}{\\rho^{2}\\alpha}\\geq L\\quad\\Leftrightarrow\\quad1-\\rho^{2}\\geq L\\rho^{2}\\alpha\\quad\\Leftrightarrow\\quad1\\geq(1+L\\alpha)\\rho^{2}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Finally, we consider the last equation. If ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\varepsilon:=\\eta-\\lambda\\rho^{2}\\alpha\\cdot\\alpha(1+\\sigma^{2})=\\eta-\\alpha(1+\\sigma^{2})>0,\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "then we find that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(x_{0})+\\frac{1}{\\alpha\\rho^{2}}\\left\\|v_{0}\\right\\|^{2}-\\operatorname*{inf}f\\right]\\geq\\mathcal{L}_{1}-\\mathcal{L}_{n+1}=\\sum_{i=0}^{n}(\\mathcal{L}_{i}-\\mathcal{L}_{i+1})\\geq\\frac{\\varepsilon}{2}\\sum_{i=1}^{n}\\mathbb{E}\\big[\\|\\nabla f(x_{i})\\|^{2}\\big]\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and hence ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq i\\leq n}\\mathbb{E}\\left[\\Vert\\nabla f(x_{i})\\Vert^{2}\\right]\\leq\\frac{1}{n+1}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\Vert\\nabla f(x_{i})\\Vert^{2}\\right]\\leq\\frac{2\\mathbb{E}\\left[f(x_{0})-\\operatorname*{inf}f+\\frac{1}{\\alpha\\rho^{2}}\\left\\Vert v_{0}\\right\\Vert^{2}\\right]}{\\varepsilon(n+1)}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "H Proof of Lemma 8: Scaling intensity of minibatch noise ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "In this appendix, we provide theoretical justification for the multiplicative noise scaling regime considered in this article. Recall our main statement: ", "page_idx": 41}, {"type": "text", "text": "Lemma 8 (Noise intensity). Assume that $\\ell(h,y)=\\|h-y\\|^{2}$ and $h:\\mathbb{R}^{m}\\times\\mathbb{R}^{d}\\to\\mathbb{R}^{k}$ satisfies $\\|\\nabla_{w}h(w,x_{i})\\|^{2}\\leq C\\big(1+\\|w\\|\\big)^{p}$ for some $C,p>0$ and all $w\\in\\mathbb{R}^{m}$ and $i=1,\\ldots,N$ . Then for all $w\\in\\mathbb{R}^{m}$ : ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\sum_{i=1}^{N}\\left\\|\\nabla\\ell_{i}-\\nabla\\mathcal{R}\\right\\|^{2}\\,\\leq\\,4C^{2}\\,(1+\\|w\\|)^{2p}\\,\\mathcal{R}(w).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof. Since $\\begin{array}{r}{\\nabla{\\mathcal R}=\\frac{1}{n}\\sum_{i=1}^{n}\\nabla\\ell_{i}}\\end{array}$ , we observe that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|\\nabla\\ell_{i}-\\nabla\\mathcal{R}\\right\\|^{2}\\leq\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|\\nabla\\ell_{i}\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "as the average of a quantity is the unique value which minimizes the mean square discrepancy: $\\mathbb{E}X=\\operatorname{argmin}_{a\\in\\mathbb{R}}\\mathbb{E}\\left[|X-\\mathbf{\\mu}^{\\bullet}\\right]$ . We further find by H\u00f6lder\u2019s inequality that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\|\\nabla\\ell_{i}\\|^{2}=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|\\displaystyle\\sum_{j=1}^{k}2(h_{j}(w,x_{i})-y_{i,j})\\nabla_{w}h_{j}(w,x_{i})\\right\\|^{2}}\\\\ &{\\leq\\displaystyle\\frac{4}{n}\\sum_{i=1}^{n}\\left(\\displaystyle\\sum_{j=1}^{k}\\left(h_{j}(w,x_{i})-y_{i,j}\\right)^{2}\\right)\\left(\\displaystyle\\sum_{j=1}^{k}\\|\\nabla_{w}h_{j}(w,x_{i})\\|_{2}^{2}\\right)}\\\\ &{=\\displaystyle\\frac{4}{n}\\sum_{i=1}^{n}\\|h(w,x_{i})-y_{i}\\|_{2}^{2}\\|\\nabla_{w}h(w,x_{i})\\|^{2}}\\\\ &{\\leq4C^{2}\\big(1+\\|w\\|^{2}\\big)^{2p}\\,\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\|h(w,x_{i})-y_{i}\\|_{2}^{2}}\\\\ &{=4C^{2}\\big(1+\\|w\\|^{2}\\big)^{2p}\\,\\mathcal{R}(w).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "I Implementation aspects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "We discuss some implementation in this section. All the code used for the experiments in the paper has been provided in the supplementary materials. The experiments in section Section 5 and Appendix A were run on Google Colab for compute time less than an hour. The experiments in Section 5.2 were run on a laptop CPU with compute time less than an hour. The experiments in Sections 5.3 and 5.4 were run on a single current generation GPU in a local cluster for up to 50 hours. An additional compute of no more than 200 hours on a single GPU was used for experiments which were ultimately not used in the submitted version. ", "page_idx": 42}, {"type": "text", "text": "I.1 The last iterate ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "All neural-network based experiments were performed using the PyTorch library. Gradient-based optimizers in PyTorch and TensorFlow are implemented in such a way that gradients are computed outside of the optimizer and the point returned by an optimizer step is the point for the next gradient evaluation. This strategy facilitates the manual manipulation of gradients by scaling, clipping or masking to train only a subset of the network parameters. ", "page_idx": 42}, {"type": "text", "text": "The approach is theoretically justified for SGD. Guarantees for NAG and AGNES, on the other hand, are given for $f(x_{n})$ rather than $f(x_{n}^{\\prime})$ , i.e. not at the point where the gradient is evaluated. A discrepancy arises between theory and practice.3 In Algorithm 1, this discrepancy is resolved by taking a final gradient descent step in the last time step and returning the sequence $x_{n}^{\\prime}$ at intermediate steps. In our numerical experiments, we did not include the final gradient descent step. Skipping the gradient step in particular allows for an easier continuation of simulations beyond the initially specified stopping time, if so desired. We do not anticipate major differences under realistic circumstances. This can be justified analytically in convex and strongly convex optimization, at least for a low learning rate. ", "page_idx": 42}, {"type": "text", "text": "Lemma 24. If $\\begin{array}{r}{\\eta<\\frac{1}{3L}}\\end{array}$ , then ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[f(x_{n}^{\\prime})-f(x^{*})\\big]\\leq\\frac{\\mathbb{E}[f(x_{n+1})-f(x^{*})]}{1-3L\\eta}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. By essentially the same proof as Lemma 16, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(x_{n}^{\\prime})-\\frac{3\\eta}{2}\\lVert\\nabla f(x_{n}^{\\prime})\\rVert^{2}\\right]\\le\\mathbb{E}\\left[f(x_{n+1})\\right]\\le\\mathbb{E}\\left[f(x_{n}^{\\prime})-\\frac{\\eta}{2}\\lVert\\nabla f(x_{n}^{\\prime})\\rVert^{2}\\right],\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "since the correction term to linear approximation is bounded by the $L$ -Lipschitz continuity of $\\nabla f$ both from above and below. Recall furthermore that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\|\\nabla f(x)\\|^{2}\\leq2L\\left(f(x)-f(x^{*})\\right)\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "for all $L$ -smooth functions. Thus ", "page_idx": 42}, {"type": "equation", "text": "$$\n(1-3L\\eta)\\mathbb{E}\\big[f(x_{n}^{\\prime})-f(x^{*})\\big]\\leq\\mathbb{E}\\left[f(x_{n}^{\\prime})-\\frac{3\\eta}{2}\\|\\nabla f(x_{n}^{\\prime})\\|^{2}\\right]\\leq\\mathbb{E}\\big[f(x_{n+1})\\big].\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "In particular, if $1-3L\\eta>0$ , then ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[f(x_{n}^{\\prime})-f(x^{*})\\big]\\leq\\frac{1}{1-3L\\eta}\\,\\mathbb{E}\\big[f(x_{n+1})-f(x^{*})\\big].\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The condition $\\eta<1/(3L)$ is guaranteed if the stochastic noise scaling satisfies $\\sigma>\\sqrt{2}$ since then $\\begin{array}{r}{1-3L\\eta\\ge1-\\frac{3}{1+\\sigma^{2}}}\\end{array}$ . For $\\bar{\\eta}\\doteq1/((1+\\sigma^{2})L$ , we than find that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[f(x_{n}^{\\prime})-f(x^{*})\\big]\\leq\\frac{\\mathbb{E}[f(x_{n+1})-f(x^{*})]}{1-\\frac{3}{1+\\sigma^{2}}}=\\frac{\\sigma^{2}+1}{\\sigma^{2}-2}\\,\\mathbb{E}[f(x_{n+1})-f(x^{*})].\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "3 For instance, the implementations of NAG in PyTorch and Tensorflow return $x_{n}^{\\prime}$ rather than $x_{n}$ ", "page_idx": 42}, {"type": "text", "text": "I.2 Weight decay ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Weight decay is a machine learning tool which controls the magnitude of the coefficients of a neural network. In the simplest SGD setting, weight decay takes the form of a modified update step ", "page_idx": 43}, {"type": "equation", "text": "$$\nx_{n+1}=(1-\\lambda\\eta)x_{n}-\\eta g_{n}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "for $\\lambda>0$ . A gradient flow is governed by (1) an energy to be minimized and (2) an energy dissipation mechanism [Peletier, 2014]. It is known that different energy/dissipation pairings may induce the same dynamics \u2013 for instance, Jordan et al. [1998] show that the heat equation is both the $L^{2}$ -gradient flow of the Dirichlet energy and the Wasserstein gradient flow of the entropy function. ", "page_idx": 43}, {"type": "text", "text": "In this language, weight decay can be interpreted in two different ways: ", "page_idx": 43}, {"type": "text", "text": "1. We minimize a modified objective function $x\\mapsto f(x)+{\\textstyle{\\frac{\\lambda}{2}}}\\,\\|x\\|^{2}$ which includes a Tikhonov regularizer. The gradient estimates are stochastic for $f$ and deterministic for the regularizer. This perspective corresponds to including weight decay as part of the energy. 2. We dynamically include a confinement into the optimizer which pushes back against large values of $x_{n}$ . This perspective corresponds to including weight decay as part of the dissipation. ", "page_idx": 43}, {"type": "text", "text": "In GD, both perspectives lead to the same optimization algorithm. In advanced minimizers, the two perspectives no longer coincide. For Adam, Loshchilov and Hutter [2018, 2019] initiated a debate on the superior strategy of including weight decay. We note that the two strategies do not coincide for AGNES, but do not comment on the superiority of one over the other: ", "page_idx": 43}, {"type": "text", "text": "1. Treating weight decay as a dynamic property of the optimizer leads to an update rule like ", "page_idx": 43}, {"type": "equation", "text": "$$\nx_{n}^{\\prime}=x_{n}+\\alpha v_{n},\\qquad v_{n+1}=\\rho\\big(v_{n}-g_{n}^{\\prime}\\big),\\qquad x_{n+1}=(1-\\lambda\\eta)x_{n}^{\\prime}-\\eta g_{n}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "2. Treating weight decay as a component of the objective function to be minimized leads to the update rule ", "page_idx": 43}, {"type": "equation", "text": "$$\nx_{n}^{\\prime}=x_{n}+\\alpha v_{n},\\qquad v_{n+1}=\\rho\\left(v_{n}-g_{n}^{\\prime}-\\lambda~x_{n}^{\\prime}\\right),\\qquad x_{n+1}=(1-\\lambda\\eta)x_{n}^{\\prime}-\\eta g_{n}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "In our numerical experiments, we choose the second approach, viewing weight decay as a property of the objective function rather than the dissipation. This coincides with the approach taken by the SGD (and SGD with momentum) optimizer as well as Adam (but not AdamW). ", "page_idx": 43}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We wrote the abstract and introduction with the goal to summarize our main contributions accurately and precisely. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 44}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: We compare the algorithm proposed to commonly used methods both in convex optimization and deep learning. We dedicate Section 3 to the derivation of the noise modelling assumption and illustrate the heuristics which are being made. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 44}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: All assumptions are summarized in Section 3.1. Wherever additional assumptions are made, they are stated clearly in the proof. Complete and correct proofs for all the lemmas and theorems are provided in the appendices. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 45}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: All code from experiments is provided in the supplementary materials. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 45}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: All code as well as the synthetically generated data used for the regression experiments are provided in the supplementary materials. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 46}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: All experimental settings are described in the article and its supplementary materials. They can also be inferred in the code provided. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 46}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: All experiments were repeated multiple times. We provide means and standard deviations over all runs. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 46}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 47}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: The information on compute resources is provided in the appendix. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 47}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: The work presented here is primarily theoretical. No human subjects were involved. The datasets used are standard benchmark datasets (MNIST, CIFAR-10) or purely synthetic. No direct social consequences are anticipated. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 47}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: The main contribution of the work is an algorithm for smooth convex optimization. Foundational as the topic at large may be in various fields, it is impossible to link directly to societal impact. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 48}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: The main contribution of the work is theoretical and no data or models with a high risk for misuse are produced. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 48}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We use the MNIST and CIFAR-10 datasets, which are cited accurately. We also use an implementation of ResNets, for which we cite the GitHub repository and reproduce the license terms in the code provided. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 48}, {"type": "text", "text": "", "page_idx": 49}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: No new assets are released. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 49}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 49}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 49}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 50}]