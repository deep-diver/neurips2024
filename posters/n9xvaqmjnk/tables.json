[{"figure_path": "n9xVaQMJNK/tables/tables_7_1.jpg", "caption": "Table 1: Adversarial base-to-new Generalization performance. We report the average result of the Base Natural Accuracy (%), Base Adversarial Accuracy (%), New Natural Accuracy (%), and New Adversarial Accuracy (%) on 11 datasets. Detailed results for each dataset are provided in Appendix D.10.", "description": "This table presents the results of an adversarial base-to-new generalization experiment.  The experiment evaluates the model's ability to generalize from a limited set of base classes to new, unseen classes, both under normal and adversarial conditions. The table shows the average natural accuracy and adversarial accuracy (using PGD-100 attacks) across 11 different datasets, with separate results for base and new classes.  These results help illustrate how well the model generalizes to new data and its robustness to adversarial attacks.  Appendix D.10 provides more detailed, dataset-specific results.", "section": "4.2 Main Results"}, {"figure_path": "n9xVaQMJNK/tables/tables_8_1.jpg", "caption": "Table 2: Comparison with benchmark result [11] which adapts models on the entire ImageNet-1K. We report the average natural and robust accuracy across downstream datasets. Running time is computed on a single NVIDIA RTX A40 GPU.", "description": "This table compares the performance of the proposed FAP method with the AdvVP method from a previous study.  It shows that the FAP method, even with a small fraction (1.25%) of the ImageNet-1K dataset, achieves comparable zero-shot performance to the AdvVP method trained on the entire dataset (100%). The table also reports the training time and model parameters required for each method.", "section": "4.2 Main Results"}, {"figure_path": "n9xVaQMJNK/tables/tables_8_2.jpg", "caption": "Table 3: Adversarial base-to-new generalization performance (%) w.r.t. different \u03bb values.", "description": "This table presents the results of an experiment evaluating the trade-off between natural and adversarial robustness in a few-shot adversarial prompt learning setting.  The experiment varies a weight parameter (\u03bb) that controls the balance between the natural and adversarial components of the loss function.  The table shows the Base Natural Accuracy, Base Adversarial Accuracy, New Natural Accuracy, and New Adversarial Accuracy for different values of \u03bb.  The results demonstrate how adjusting \u03bb affects both natural generalization (accuracy on clean examples) and adversarial robustness (accuracy on adversarial examples).", "section": "4.3 More Analysis"}, {"figure_path": "n9xVaQMJNK/tables/tables_8_3.jpg", "caption": "Table 4: Natural and robust performance (%) w.r.t. different prompt depth and length settings. Results are obtained in under 16-shot adversarial prompt learning on StanfordCars.", "description": "This table presents the results of an ablation study on the impact of prompt depth and length on the performance of the proposed Few-shot Adversarial Prompt learning (FAP) framework.  The experiment was conducted on the StanfordCars dataset using a 16-shot setting for adversarial prompt learning. The table shows the natural accuracy and PGD-100 accuracy (robustness) for different numbers of prompt tokens (2, 4, 6, 8, 10, and 12). The results are separated into two groups: prompt depth and prompt length.  The depth refers to how many transformer layers the prompts are applied to, while the length refers to the number of tokens in the prompt.", "section": "4.3 More Analysis"}, {"figure_path": "n9xVaQMJNK/tables/tables_18_1.jpg", "caption": "Table 6: Overall methodological explanations of baselines and our methods.", "description": "This table summarizes the design choices and loss functions used by the baselines (AdvVP, AdvTP, AdvVLP, and AdvMaPLe) and the proposed method (FAP).  It clarifies the differences in prompt designs (visual and text prompt tokens, projection types, presence of deep prompts), loss functions used during training and for attack-time evaluation.  The table highlights the key differences in the approach taken by each method to address adversarial robustness in the context of prompt-based learning.", "section": "C Additional Implementation Details"}, {"figure_path": "n9xVaQMJNK/tables/tables_18_2.jpg", "caption": "Table 8: Cross-dataset generalization from ImageNet-1K to various downstream recognition datasets. We report the mean and standard deviation of natural and robust (PGD-100) accuracy. Bolded numbers denote the state-of-the-art results.", "description": "This table presents the results of a cross-dataset generalization experiment.  Models were initially trained on ImageNet-1K and then evaluated on 11 diverse downstream datasets without further fine-tuning. The table shows the mean and standard deviation of both natural accuracy and robust accuracy (measured using PGD-100 attacks).  The best-performing results for each dataset are highlighted in bold.", "section": "4.2 Main Results"}, {"figure_path": "n9xVaQMJNK/tables/tables_19_1.jpg", "caption": "Table 8: Cross-dataset generalization from ImageNet-1K to various downstream recognition datasets. We report the mean and standard deviation of natural and robust (PGD-100) accuracy. Bolded numbers denote the state-of-the-art results.", "description": "This table presents the results of a cross-dataset generalization experiment.  Models were initially trained on a subset of ImageNet-1K (16 shots per class). The table shows the natural accuracy and robust accuracy (using a PGD-100 attack) across 11 downstream datasets. The results are shown as mean \u00b1 standard deviation and bolded numbers highlight the best-performing method for each metric.", "section": "4.2 Main Results"}, {"figure_path": "n9xVaQMJNK/tables/tables_20_1.jpg", "caption": "Table 9: Incremental changes with respect to AdvMaPLe. Our method combines Imp.1 and Imp.2 based on AdvMaPLe, achieving a significant performance improvement (results in the last row).", "description": "This table presents an ablation study showing the incremental improvements achieved by the proposed method (FAP) over AdvMaPLe. It breaks down the performance gains into two key aspects: 1) Optimizing the projection direction of the prompt, and 2) Utilizing a novel training objective.  The table demonstrates that each improvement contributes positively to the overall performance, with the combination of both leading to a significant enhancement.", "section": "D.3 Incremental Changes from AdvMaPLe"}, {"figure_path": "n9xVaQMJNK/tables/tables_21_1.jpg", "caption": "Table 10: Few-shot base-to-new transfer results (%) on AdvVLP with different learning objectives. We also report performance gains achieved by adapting with our Lfinal", "description": "This table presents a comparison of the performance of AdvVLP with and without the proposed learning objective (Lfinal) in a few-shot base-to-new generalization setting.  It shows the average natural and adversarial accuracy on base and new classes for four different metrics: Base Natural Accuracy, Base Adversarial Accuracy, New Natural Accuracy, and New Adversarial Accuracy. The \"+ \number\" column indicates the improvement achieved by using Lfinal compared to the baseline AdvVLP.", "section": "4.2 Main Results"}, {"figure_path": "n9xVaQMJNK/tables/tables_21_2.jpg", "caption": "Table 8: Cross-dataset generalization from ImageNet-1K to various downstream recognition datasets. We report the mean and standard deviation of natural and robust (PGD-100) accuracy. Bolded numbers denote the state-of-the-art results.", "description": "This table presents the results of a cross-dataset generalization experiment.  The model, initially trained on a subset of ImageNet-1K, is evaluated on 10 other image recognition datasets without further fine-tuning.  The table shows the mean and standard deviation of the natural accuracy (clean images) and robust accuracy (images subjected to PGD-100 attacks) for each dataset.  Bolded values indicate state-of-the-art performance.", "section": "4.2 Main Results"}, {"figure_path": "n9xVaQMJNK/tables/tables_22_1.jpg", "caption": "Table 1: Adversarial base-to-new Generalization performance. We report the average result of the Base Natural Accuracy (%), Base Adversarial Accuracy (%), New Natural Accuracy (%), and New Adversarial Accuracy (%) on 11 datasets. Detailed results for each dataset are provided in Appendix D.10.", "description": "This table presents the results of an adversarial base-to-new generalization experiment.  The model was trained on a subset of data (base classes) and then tested on both that subset (base classes) and a new set of unseen data (new classes). The results show the natural accuracy (without adversarial attacks) and adversarial accuracy (after applying adversarial attacks) for both the base and new classes.  The purpose of this experiment was to evaluate the model's ability to generalize to new, unseen data, as well as its robustness to adversarial attacks in these new settings.  The 11 different datasets represent a variety of image recognition tasks.", "section": "4.2 Main Results"}, {"figure_path": "n9xVaQMJNK/tables/tables_23_1.jpg", "caption": "Table 13: Comparison in train-time attack generation methods.", "description": "This table compares the performance of two methods for generating adversarial examples during training. The first method uses the KL divergence loss, while the second method adds a cosine similarity constraint to the KL divergence loss. The results show that the performance of both methods is very similar, suggesting that the cosine similarity constraint is not necessary for effective adversarial training.", "section": "D.8 Discussions on Training-time Attack Generation"}, {"figure_path": "n9xVaQMJNK/tables/tables_24_1.jpg", "caption": "Table 16: Detailed results for base-to-new generalization on 11 datasets. We report the Natural and PGD-100 Accuracy (%) on the base and new classes that adapted with 16-shot adversarial prompt learning.", "description": "This table presents a detailed breakdown of the performance of the proposed Few-shot Adversarial Prompt learning (FAP) method and several baselines on eleven different image recognition datasets.  The results are separated into base classes (used for training) and new classes (used for testing).  For each dataset and method, the natural accuracy (without adversarial attacks) and adversarial accuracy (PGD-100 accuracy with adversarial attacks) are reported for both base and new classes, providing a comprehensive assessment of the generalization ability and adversarial robustness of each approach in the base-to-new generalization setting.", "section": "4.2 Main Results"}, {"figure_path": "n9xVaQMJNK/tables/tables_25_1.jpg", "caption": "Table 16: Detailed results for base-to-new generalization on 11 datasets. We report the Natural and PGD-100 Accuracy (%) on the base and new classes that adapted with 16-shot adversarial prompt learning.", "description": "This table presents a detailed breakdown of the performance of the proposed Few-shot Adversarial Prompt Learning (FAP) framework, as well as several baseline methods, on 11 different image recognition datasets.  The evaluation is done in a base-to-new generalization setting, where models are trained on a subset of classes (base classes) and then tested on both the base and unseen classes (new classes). For each dataset and method, the table shows the natural accuracy (without adversarial attacks) and adversarial accuracy (with PGD-100 attacks) for both base and new classes.  This allows for a comprehensive assessment of both the natural generalization ability and adversarial robustness of the various methods.", "section": "4.2 Main Results"}, {"figure_path": "n9xVaQMJNK/tables/tables_26_1.jpg", "caption": "Table 16: Detailed results for base-to-new generalization on 11 datasets. We report the Natural and PGD-100 Accuracy (%) on the base and new classes that adapted with 16-shot adversarial prompt learning.", "description": "This table presents a detailed breakdown of the performance of the proposed Few-shot Adversarial Prompt Learning (FAP) method, along with several baseline methods, on 11 different image recognition datasets.  The performance is evaluated in a base-to-new generalization setting, meaning that the models are trained on a subset of the classes (base classes) and then tested on both the training classes and a set of new classes.  The table reports both the natural accuracy (standard accuracy) and the adversarial accuracy (accuracy against PGD-100 attacks) for each dataset and class type (base and new). This allows for a comprehensive assessment of the model's ability to generalize to unseen data while maintaining robustness to adversarial attacks.", "section": "4.2 Main Results"}]