[{"figure_path": "7Ntft3U7jj/figures/figures_2_1.jpg", "caption": "Figure 1: Neuron dropout. To initialize a smaller variant of the self-supervised pre-trained GNNs, we select parameters from self-supervised GNNs in different ways. From left to right: randomly reduce the number of neurons in each layer proportionally, the original GNNs, retain only the first two layers while randomly reducing the number of neurons in the second layer proportionally.", "description": "This figure illustrates three different methods for creating smaller variants of self-supervised pre-trained Graph Neural Networks (GNNs). The first method involves proportionally reducing the number of neurons in each layer. The second shows the original, full-sized GNN.  The third method retains only the first two layers, while proportionally reducing the number of neurons in the second layer. These variations are used to investigate the impact of reduced model size on performance.", "section": "2 The Model Redundancy in Graph Self-supervised Learning"}, {"figure_path": "7Ntft3U7jj/figures/figures_3_1.jpg", "caption": "Figure 2: CKA scores between the representations of the slim GNNs and the same layer in the original GNNs with GraphMAE and GRACE on several datasets. \"all\" means we remove the neurons from all layers in the same proportion. \"11\" means that we calculate CKA scores of the representations from the first layer, and \"12\" means CKA scores from the second layer, and so on.", "description": "This figure displays the centered kernel alignment (CKA) scores, a measure of similarity between representations, for slim GNNs (Graph Neural Networks with reduced neurons) and their corresponding layers in the original GNNs.  Different datasets (Cora, Citeseer, Pubmed, Photo, Computers, and Ogbn-arxiv) are shown, each with CKA scores calculated for the first, second, and potentially third layers of the model. The neuron removal rate (x-axis) indicates the percentage of neurons randomly removed from each layer. The results demonstrate the impact of neuron reduction on the similarity of representations between the slim GNNs and their full-sized counterparts, showing a remarkable degree of redundancy within the models.", "section": "Redundancy analysis on neuron level"}, {"figure_path": "7Ntft3U7jj/figures/figures_4_1.jpg", "caption": "Figure 3: CKA scores between the representations of each layer and its adjacent layer of the original GNNs for GraphMAE and GRACE on several datasets.", "description": "This figure visualizes the redundancy analysis on the layer level using Centered Kernel Alignment (CKA) scores.  It shows the CKA scores between the representations of each layer and its adjacent layer in the original Graph Neural Networks (GNNs) for two different graph self-supervised learning models, GraphMAE and GRACE. High CKA scores (close to 1) indicate high similarity between representations of adjacent layers, suggesting redundancy in the model's design. The low CKA score between the original features and the representation of the first layer is also shown.", "section": "Redundancy analysis on layer level"}, {"figure_path": "7Ntft3U7jj/figures/figures_4_2.jpg", "caption": "Figure 4: The overall framework of SLIDE.", "description": "The figure illustrates the overall framework of the SLIDE (SLIm DE-correlation Fine-tuning) method. It is divided into two main parts: model slimming and model de-correlation.  The model slimming part takes a pre-trained GNN and reduces it to a slimmer version by removing redundant neurons. The model de-correlation part takes the output embeddings from the slim GNN, applies Random Fourier Features (RFF) to extract features, uses RFF maps to minimize the correlation between the features, and finally uses reweighted loss to update the weights and achieve better classification performance. The prediction and loss computation are shown in the bottom part of the figure.", "section": "3 Our Proposed Tuning Approach: Slimming De-correlation Fine-tuning"}, {"figure_path": "7Ntft3U7jj/figures/figures_7_1.jpg", "caption": "Figure 5: Ablation studies of model de-correlation on six benchmark datasets and three pre-training frameworks. \"w/o de\" means that we fine-tune the slim GNNs without model de-correlation methods. \"Mi\" means Micro-F1 scores and \"Ma\" means Macro-F1 scores. The results of Ogbn-arxiv with GRACE are unseen because of \"out of memory\".", "description": "This figure presents the ablation study results on the impact of model de-correlation in SLIDE. It shows the performance of SLIDE with and without de-correlation on six benchmark datasets (Cora, Citeseer, Pubmed, Photo, Computers, Ogbn-arxiv) across three pre-training frameworks (GraphMAE, GRACE, MaskGAE).  The results are presented separately for Micro-F1 and Macro-F1 scores.  The results demonstrate that incorporating the de-correlation module significantly improves the performance of SLIDE. The Ogbn-arxiv results with GRACE are missing due to memory limitations.", "section": "4.2 Model Analysis"}, {"figure_path": "7Ntft3U7jj/figures/figures_7_2.jpg", "caption": "Figure 6: The number of parameters on several datasets with GraphMAE and GRACE.", "description": "This figure compares the number of parameters in the original model (full fine-tuning) versus the slimmed model (SLIDE) for GraphMAE and GRACE on several datasets.  It visually demonstrates the significant reduction in parameters achieved by SLIDE without substantial performance loss, as reported in the paper.", "section": "4.2 Model Analysis"}]