[{"figure_path": "yWSxjlFsmX/tables/tables_5_1.jpg", "caption": "Table 1: The average result of DT, RNN-like DeMa and Transformer-like DeMa in Atari [58] and MuJoCo [59]. The results are reported with the normalization following [60, 11]. Detailed results can be seen in Appendix E.", "description": "This table presents the average performance of three different models (Decision Transformer, RNN-like Decision Mamba, and Transformer-like Decision Mamba) on Atari and MuJoCo benchmark tasks.  The results are normalized using methods described in references [60, 11].  More detailed results are available in Appendix E of the paper.", "section": "4 The Analysis of DeMa"}, {"figure_path": "yWSxjlFsmX/tables/tables_6_1.jpg", "caption": "Table 2: Input concatenation types comparison: \"BL3D\" refers to the concatenation of input tokens across the embedding dimension, while \"B3LD\" indicates concatenation across the temporal dimension, as depicted in Figure 1. Outcomes are averaged across three random seeds.", "description": "This table compares the performance of two different input concatenation methods (BL3D and B3LD) for the Transformer-like DeMa model in Atari games. BL3D concatenates the state, action, and reward along the embedding dimension, while B3LD concatenates them along the temporal dimension. The results show that B3LD generally outperforms BL3D, suggesting that temporal concatenation is more effective for trajectory optimization.", "section": "4.1 Input Data Structures"}, {"figure_path": "yWSxjlFsmX/tables/tables_7_1.jpg", "caption": "Table 3: Performance comparison between DT, hidden attention with post up-projection residual block in the transformer (DeMa with post.) and hidden attention with pre up-projection residual block (DeMa) in Atari.", "description": "This table compares the performance of three different models in eight Atari games: Decision Transformer (DT), DeMa with a post up-projection residual block, and DeMa with a pre up-projection residual block.  The results show the average normalized scores for each game, with standard deviations included.  It highlights the impact of the residual block placement on DeMa's performance compared to DT.", "section": "5 Evaluations on Offline RL Benchmarks"}, {"figure_path": "yWSxjlFsmX/tables/tables_7_2.jpg", "caption": "Table 4: Performance comparison between DT, hidden attention with post up-projection residual block in the transformer (DeMa with post.) and hidden attention with pre up-projection residual block (DeMa) in MuJoCo.", "description": "This table compares the performance of Decision Transformer (DT), DeMa with post-projection residual block, and DeMa with pre-projection residual block on nine MuJoCo tasks.  The results show DeMa's performance improvement in most of the environments compared to DT and the DeMa with post-projection residual block, highlighting the effectiveness of the hidden attention mechanism and pre-projection residual block structure in DeMa.", "section": "5 Evaluations on Offline RL Benchmarks"}, {"figure_path": "yWSxjlFsmX/tables/tables_8_1.jpg", "caption": "Table 1: The average result of DT, RNN-like DeMa and Transformer-like DeMa in Atari [58] and MuJoCo [59]. The results are reported with the normalization following [60, 11]. Detailed results can be seen in Appendix E.", "description": "This table shows the average performance results of three different models (Decision Transformer (DT), RNN-like Decision Mamba (DeMa), and Transformer-like Decision Mamba (DeMa)) on Atari and MuJoCo benchmark tasks.  The results are normalized following methods described in references [60, 11]. More detailed results are available in Appendix E of the paper.", "section": "4 The Analysis of DeMa"}, {"figure_path": "yWSxjlFsmX/tables/tables_8_2.jpg", "caption": "Table 5: Results for 1% DQN-replay datasets. We evaluate the performance of DeMa on eight Atari games.", "description": "This table presents the average results of different offline reinforcement learning algorithms on eight Atari games using a 1% DQN-replay dataset.  The algorithms compared include CQL, BC, Decision Transformer (DT), Decision Convformer (DC), Decision Transformer Hybrid (DChybrid), and Decision Mamba (DeMa).  The table shows the average normalized scores for each algorithm on each game, providing a comparison of their performance.  DeMa shows significantly improved performance over the other algorithms.", "section": "5 Evaluations on Offline RL Benchmarks"}, {"figure_path": "yWSxjlFsmX/tables/tables_8_3.jpg", "caption": "Table 7: The resource usage for training DT, DC and DeMa on Atari and MuJoCo.", "description": "This table compares the computational resource usage of three different models: Decision Transformer (DT), Decision Convformer (DC), and Decision Mamba (DeMa) on Atari and MuJoCo tasks.  It shows the training time per step, GPU memory usage, number of Multiply-Accumulate operations (MACs), and the total number of parameters for each model on each platform. This helps in understanding the efficiency and scalability of each model.", "section": "5 Evaluations on Offline RL Benchmarks"}, {"figure_path": "yWSxjlFsmX/tables/tables_14_1.jpg", "caption": "Table 8: Hyper-parameters of DeMa for MuJoCo.", "description": "This table shows the hyperparameter settings used for the Decision Mamba (DeMa) model when applied to the MuJoCo environment.  It lists values for parameters such as the number of layers, embedding dimension, nonlinearity function, batch size, context length (K), dropout rate, learning rate, gradient norm clipping, weight decay, learning rate decay schedule, and the dimensions of the model (d_model, d_state).  These settings were chosen based on prior work and experimental tuning to optimize DeMa's performance on MuJoCo tasks.", "section": "D.1 Hyper-parameters in MuJoCo"}, {"figure_path": "yWSxjlFsmX/tables/tables_15_1.jpg", "caption": "Table 9: Hyper-parameters of Transformer-like DeMa for Atari.", "description": "This table lists the hyperparameters used for the Transformer-like DeMa model when applied to Atari games.  It includes settings for network architecture (layers, embedding dimension), activation function, batch size, sequence length (context length K), return-to-go conditioning (specific values for different games), dropout, learning rate, gradient clipping, weight decay, learning rate decay schedule, maximum number of epochs, Adam optimizer hyperparameters (betas), warmup tokens, final tokens, and model dimensions (d_model, d_conv, d_state, expand).", "section": "D.2 Hyper-parameters in Atari"}, {"figure_path": "yWSxjlFsmX/tables/tables_15_2.jpg", "caption": "Table 10: Hyper-parameters of RNN-like DeMa for Atari. The other hyper-parameters are kept consistent with those in Table 9.", "description": "This table shows the hyperparameters used for the RNN-like version of Decision Mamba (DeMa) when applied to Atari games.  It lists values for the context length, batch size, learning rate, and the number of inner iterations.  Note that other hyperparameters remain consistent with those defined in Table 9, which is not shown here.", "section": "D.2 Hyper-parameters in Atari"}, {"figure_path": "yWSxjlFsmX/tables/tables_16_1.jpg", "caption": "Table 11: The Comparison of DT, RNN-like DeMa, and Transformer-like DeMa in Atari Games.", "description": "This table compares the performance of Decision Transformer (DT), RNN-like Decision Mamba (DeMa), and Transformer-like DeMa across eight Atari games.  The results showcase the average normalized scores for each algorithm in each game, highlighting the significant performance improvement of the Transformer-like DeMa compared to the other methods. The table provides a quantitative assessment of the effectiveness of the different model architectures in the Atari environment.", "section": "E Detailed results"}, {"figure_path": "yWSxjlFsmX/tables/tables_16_2.jpg", "caption": "Table 12: The comparison between DT, RNN-like DeMa, and Transformer-like DeMa in MuJoCo.", "description": "This table presents a comparison of the average performance results of three different methods in MuJoCo environment: Decision Transformer (DT), RNN-like Decision Mamba (DeMa), and Transformer-like Decision Mamba (DeMa).  The results are broken down by dataset (M, M-R representing medium and medium-replay), environment (HalfCheetah, Hopper, Walker), and method. The table shows the average performance across all three environments, highlighting the superior performance of the Transformer-like DeMa compared to the other two methods.", "section": "E Detailed results"}, {"figure_path": "yWSxjlFsmX/tables/tables_17_1.jpg", "caption": "Table 13: Results for D4RL datasets with delayed (sparse) reward. The \"Origin Average\" in the table represents the normalized scores of evaluations across six datasets under the original dense reward setting.", "description": "This table presents the results of evaluating the performance of DeMa (Decision Mamba) and several baseline algorithms on D4RL datasets with delayed rewards.  It compares the normalized scores of DeMa against CQL, DS4, DT, and GDT across six different environments (three with standard reward and three with delayed reward). The \"Origin Average\" column shows the average performance on the same datasets with the original dense reward scheme for comparison.  The results highlight DeMa's robustness to delayed rewards compared to some baseline algorithms.", "section": "F Tasks Requires Long Horizon Planning Skills"}, {"figure_path": "yWSxjlFsmX/tables/tables_17_2.jpg", "caption": "Table 1: The average result of DT, RNN-like DeMa and Transformer-like DeMa in Atari [58] and MuJoCo [59]. The results are reported with the normalization following [60, 11]. Detailed results can be seen in Appendix E.", "description": "This table presents the average performance results of three different models (Decision Transformer, RNN-like Decision Mamba, and Transformer-like Decision Mamba) on Atari and MuJoCo benchmark tasks.  The results are normalized according to the methods described in references [60] and [11].  Detailed results are available in Appendix E of the paper.  The table highlights the relative performance of each model in terms of average scores achieved. ", "section": "4 The Analysis of DeMa"}, {"figure_path": "yWSxjlFsmX/tables/tables_18_1.jpg", "caption": "Table 16: Results for maze2d and antmaze.", "description": "This table presents the results of DeMa, DT, GDT, and DC on maze2d and antmaze environments.  The results show the normalized scores achieved by each method across various datasets (umaze medium, umaze large, umaze-diverse, antmaze umaze).  It demonstrates the performance comparison of DeMa against several baselines on these long-horizon planning tasks.", "section": "5 Evaluations on Offline RL Benchmarks"}, {"figure_path": "yWSxjlFsmX/tables/tables_18_2.jpg", "caption": "Table 17: The affection of position embedding.", "description": "This table presents the ablation study results on the impact of position embedding on DeMa's performance. It compares the performance of DeMa with and without position embedding across different MuJoCo datasets (M and M-R) and environments (HalfCheetah, Hopper, Walker). The results demonstrate that removing position embedding leads to a slight improvement in average performance while substantially reducing the number of parameters.", "section": "G Further Ablation Study"}, {"figure_path": "yWSxjlFsmX/tables/tables_19_1.jpg", "caption": "Table 4: Performance comparison between DT, hidden attention with post up-projection residual block in the transformer (DeMa with post.) and hidden attention with pre up-projection residual block (DeMa) in MuJoCo.", "description": "This table compares the performance of three different models (DT, DeMa with post-projection residual block, and DeMa with pre-projection residual block) on nine MuJoCo tasks.  It shows the average normalized scores for each model across three different datasets (M, M-R, and M-E) representing different data distributions. The table also lists the total number of parameters for each model, highlighting the parameter efficiency of DeMa.", "section": "5 Evaluations on Offline RL Benchmarks"}, {"figure_path": "yWSxjlFsmX/tables/tables_19_2.jpg", "caption": "Table 19: MuJoCo and Atari baseline scores used for normalization", "description": "This table presents the baseline scores used for normalization in the MuJoCo and Atari environments.  For each environment (Hopper, Halfcheetah, Walker2d in MuJoCo; Breakout, Qbert, Pong, Seaquest, Asterix, Frostbite, Assault, Gopher in Atari), it shows the random score and the expert/gamer score, representing the performance of a random agent and an expert/gamer respectively.  These baseline scores provide a context for understanding the performance of the proposed DeMa algorithm relative to chance and human-level performance.", "section": "5 Evaluations on Offline RL Benchmarks"}]