{"references": [{"fullname_first_author": "Sergey Levine", "paper_title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems", "publication_date": "2020-05-01", "reason": "This paper provides a comprehensive overview of offline reinforcement learning, which is the core topic of the current research."}, {"fullname_first_author": "Lili Chen", "paper_title": "Decision transformer: Reinforcement learning via sequence modeling", "publication_date": "2021-12-01", "reason": "This paper introduces Decision Transformer, a key method in trajectory optimization that is directly compared against in this research."}, {"fullname_first_author": "Michael Janner", "paper_title": "Offline reinforcement learning as one big sequence modeling problem", "publication_date": "2021-12-01", "reason": "This paper presents an approach to offline RL that directly informs the current work's methodology and is therefore highly relevant."}, {"fullname_first_author": "Albert Gu", "paper_title": "Efficiently modeling long sequences with structured state spaces", "publication_date": "2021-11-01", "reason": "This paper introduces structured state space models, the basis for the Mamba model used in the current research."}, {"fullname_first_author": "Albert Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2023-12-01", "reason": "This paper introduces the Mamba model, a core component of the proposed DeMa model and the main focus of the experimental investigation."}]}