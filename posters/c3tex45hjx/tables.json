[{"figure_path": "C3tEX45hJX/tables/tables_8_1.jpg", "caption": "Table 1: Performances of Diff-SR and baseline RL algorithms after 200K environment steps. Results are averaged across 4 random seeds and a window size of 10K steps. Results marked with * are taken from MBBL [Wang et al., 2019] and \u2020 are taken from LV-Rep [Ren et al., 2022a].", "description": "This table presents the performance comparison between Diff-SR and various baseline reinforcement learning algorithms (model-based, model-free, and representation learning methods) across multiple continuous control tasks from the Gym-MuJoCo locomotion benchmark.  The results are averaged across four random seeds and a 10K step window after running for 200K environment steps.  The table highlights Diff-SR's performance relative to existing methods.", "section": "5.1 Results of Gym-MuJoCo Tasks"}, {"figure_path": "C3tEX45hJX/tables/tables_17_1.jpg", "caption": "Table 1: Performances of Diff-SR and baseline RL algorithms after 200K environment steps. Results are averaged across 4 random seeds and a window size of 10K steps. Results marked with * are taken from MBBL [Wang et al., 2019] and \u2020 are taken from LV-Rep [Ren et al., 2022a].", "description": "This table presents the performance comparison of Diff-SR against various baseline reinforcement learning algorithms across multiple continuous control tasks from the MuJoCo locomotion benchmark after 200,000 environment steps.  The results are averaged across four random seeds, utilizing a window size of 10,000 steps for smoothing.  Results from MBBL (Wang et al., 2019) and LV-Rep (Ren et al., 2022a) are also included for comparison.", "section": "5.1 Results of Gym-MuJoCo Tasks"}, {"figure_path": "C3tEX45hJX/tables/tables_18_1.jpg", "caption": "Table 1: Performances of Diff-SR and baseline RL algorithms after 200K environment steps. Results are averaged across 4 random seeds and a window size of 10K steps. Results marked with * are taken from MBBL [Wang et al., 2019] and \u2020 are taken from LV-Rep [Ren et al., 2022a].", "description": "This table compares the performance of the proposed Diff-SR algorithm against various baseline reinforcement learning algorithms across multiple continuous control tasks from the Gym-MuJoCo locomotion benchmark.  The performance metrics are averaged over four random seeds and a 10k step window.  The algorithms are categorized into model-based RL, model-free RL, and representation RL methods for better comparison.  Asterisks and daggers indicate that the results for certain methods were taken from other papers, referenced in the caption.", "section": "5.1 Results of Gym-MuJoCo Tasks"}, {"figure_path": "C3tEX45hJX/tables/tables_20_1.jpg", "caption": "Table 4: Hyperparameters used for Diff-SR in state-based POMDP experiments.", "description": "This table lists the hyperparameters used in the experiments for Diff-SR on state-based Partially Observable Markov Decision Processes (POMDPs).  It includes settings for the actor and critic networks, the Diff-SR representation, and the training process.  Note that the Diff-SR representation dimension is specific to the cheetah environment, while other hyperparameters are generally applicable.", "section": "E Details and Analysis for State-based Partially Observable MDP Experiments"}]