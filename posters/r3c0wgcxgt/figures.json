[{"figure_path": "r3c0WGCXgt/figures/figures_2_1.jpg", "caption": "Figure 2: Differences between text control information and general ControlNet control information, including anime line drawings, M-LSD lines, and Canny edges. General controls focus on the overall structure and tolerate localized errors, while text control requires precise detail.", "description": "This figure showcases the differences between text control information used in visual text generation and other general control information used in ControlNet.  It highlights how general controls, such as Canny edges or line drawings, focus on the overall structure of an image, allowing for minor errors in detail. In contrast, text control requires much more precise detail, as even small inaccuracies can result in unreadable or incorrect text.", "section": "3.1 Motivations"}, {"figure_path": "r3c0WGCXgt/figures/figures_3_1.jpg", "caption": "Figure 3: Visualization of the impact of control at different denoising stages. Control information is removed in the gray segments of the color bar during denoising. (a) Since visual text generation requires much detail texture, control information in later stages still plays an important role. (b) Even with only glyph and position images as control information, early-stage control influences non-text regions, ensuring the text region is coherent and matches the background.", "description": "This figure visualizes how control information affects image generation at different stages of the denoising process.  Panel (a) shows that detailed textural information requires control information at later stages for high-quality results. Panel (b) demonstrates that early-stage control, even with limited input, impacts the overall coherence of the image, ensuring a consistent background and matching text regions.", "section": "3 Method"}, {"figure_path": "r3c0WGCXgt/figures/figures_4_1.jpg", "caption": "Figure 4: The pipeline of our TextGen. It comprises two stages: the global control stage and the detail control stage. Control information utilizes a Fourier Enhancement Convolution (FEC) block and a Spatial Convolution (SC) block to extract features. During inference, we introduce a novel denoising paradigm to unify generation and editing based on our framework design. Best shown in color.", "description": "This figure illustrates the architecture of TextGen, a two-stage framework for visual text generation and editing.  The global control stage focuses on overall structure and style, while the detail stage refines details and allows for editing.  Both stages utilize Fourier Enhancement Convolution (FEC) and Spatial Convolution (SC) blocks to process control information.  A novel inference paradigm unifies generation and editing tasks.", "section": "3 Method"}, {"figure_path": "r3c0WGCXgt/figures/figures_4_2.jpg", "caption": "Figure 5: The relative log amplitude of three parts of features in U-Net decoder.", "description": "This figure visualizes the frequency components of different feature types within the U-Net decoder of a diffusion model used for text image generation.  It shows the relative log amplitude across various frequencies (from 0 to \u03c0) for four feature types: skip features, control features, base features, and the fusion of these features.  The plot helps illustrate how these features differ in their frequency distribution, which is relevant to understanding their respective roles in the image generation process, particularly concerning the balance between high and low-frequency information in controlling texture details.", "section": "3.1.3 Control Information Output"}, {"figure_path": "r3c0WGCXgt/figures/figures_5_1.jpg", "caption": "Figure 6: (a) The Spatial Convolution Block. (b) The Frequency Enhancement Convolution Block.", "description": "This figure shows the architecture of two blocks used in the TextGen model: the Spatial Convolution Block (SC) and the Frequency Enhancement Convolution Block (FEC).  The SC block uses standard convolutions to process spatial information. The FEC block uses two branches, one for spatial information processing via convolutions and another for frequency information processing using a Fast Fourier Transform (FFT), convolutions in the frequency domain, and an Inverse Fast Fourier Transform (IFFT).  Both blocks have a global perception layer using a large kernel convolution to capture global image context.", "section": "3.3 Model Control"}, {"figure_path": "r3c0WGCXgt/figures/figures_8_1.jpg", "caption": "Figure 7: Qualitative comparison of generation performance in English texts. Our TextGen can generate more artistic and realistic texts.", "description": "This figure showcases a qualitative comparison of English text generation performance between different methods: ControlNet, GlyphControl, TextDiffuser, TextDiffuser2, AnyText, a baseline model, ground truth, and the proposed TextGen model.  Each method is shown generating images for several different prompts demonstrating differences in text quality, style, and artistic rendering. TextGen is highlighted as producing significantly more realistic and artistically pleasing results compared to other approaches.", "section": "4.4.2 Qualitative Results"}, {"figure_path": "r3c0WGCXgt/figures/figures_8_2.jpg", "caption": "Figure 8: Comparison of generation in Chinese.", "description": "This figure compares the visual text generation results in Chinese from ControlNet, AnyText, ground truth and the proposed TextGen method.  The examples show that TextGen produces more realistic and accurate results than the other methods, especially in terms of text clarity and overall image quality.", "section": "4.4.2 Qualitative Results"}, {"figure_path": "r3c0WGCXgt/figures/figures_8_3.jpg", "caption": "Figure 9: Visualization of editing performance.", "description": "This figure shows examples of the text editing capabilities of the TextGen model.  It showcases how the model can successfully replace text in images while maintaining background consistency and overall image quality.  Different variations of text are edited in several examples, highlighting the model's flexibility and precision in altering text content within various visual contexts.", "section": "4.4 Qualitative Results"}, {"figure_path": "r3c0WGCXgt/figures/figures_12_1.jpg", "caption": "Figure 10: Comparison of captions by BLIP-2 and Qwen-VL.", "description": "This figure shows examples of captions generated by BLIP-2 and Qwen-VL for the same images.  BLIP-2 captions are often repetitive and nonsensical, while Qwen-VL captions are more accurate and descriptive, highlighting the improvement in caption quality achieved by using Qwen-VL to refine BLIP-2's initial output. This showcases the need for caption refinement in datasets used for visual text generation.", "section": "A Details of Dataset"}, {"figure_path": "r3c0WGCXgt/figures/figures_13_1.jpg", "caption": "Figure 11: Some cases in our proposed TG-2M dataset.", "description": "This figure shows some example images from the TG-2M dataset, highlighting the diversity of image styles and text content present in the dataset.  The images illustrate different fonts, languages (English and Chinese), background styles and text arrangements, demonstrating the complexity handled by the model.", "section": "A Details of Dataset"}, {"figure_path": "r3c0WGCXgt/figures/figures_14_1.jpg", "caption": "Figure 1: The overall pipeline of recent text generation works. It utilizes a ControlNet for guiding the text generation process, employing a glyph image with a standard font as the control information. Control information at different stages is generated in the same manner and directly added to the skip features in the U-Net decoder.", "description": "This figure illustrates the typical process of generating text images using a ControlNet. A glyph image (text image with standard font) is used as control information, which is added to the skip features of the U-Net decoder.  The process is the same for control information at all stages, which is a limitation addressed by the paper's proposed method.", "section": "1 Introduction"}]