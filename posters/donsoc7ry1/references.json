{"references": [{"fullname_first_author": "M. Zaheer", "paper_title": "Deep sets", "publication_date": "2017-12-04", "reason": "This paper introduces the Deep Sets architecture, a foundational model for many other neural set function methods, including those compared in this paper."}, {"fullname_first_author": "J. Lee", "paper_title": "Set transformer: A framework for attention-based permutation-invariant neural networks", "publication_date": "2019-06-09", "reason": "This paper proposes the Set Transformer architecture, which improves upon Deep Sets by incorporating self-attention mechanisms to better model complex interactions within sets."}, {"fullname_first_author": "Z. Ou", "paper_title": "Learning neural set functions under the optimal subset oracle", "publication_date": "2022-00-00", "reason": "This paper introduces the EquiVSet method, one of the state-of-the-art methods for neural subset selection that is directly compared against the proposed HORSE method."}, {"fullname_first_author": "B. Xie", "paper_title": "Enhancing neural subset selection: Integrating background information into set representations", "publication_date": "2024-00-00", "reason": "This paper introduces the INSET method, another state-of-the-art method that directly improves upon EquiVSet and is compared against the proposed HORSE method."}, {"fullname_first_author": "J. Willette", "paper_title": "Scalable set encoding with universal mini-batch consistency and unbiased full set gradient approximation", "publication_date": "2023-00-00", "reason": "This paper introduces the concept of slots and a novel attention mechanism for encoding sets, which directly inspires the attention mechanism used in the proposed HORSE method."}]}