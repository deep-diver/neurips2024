[{"type": "text", "text": "Semantics and Spatiality of Emergent Communication ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Rotem Ben Zion1 Boaz Carmeli1 Orr Paradise2 Yonatan Belinkov1 1 Technion \u2013 Israel Institute of Technology 2 UC Berkeley ", "page_idx": 0}, {"type": "text", "text": "rotm@campus.technion.ac.il boaz.carmeli@campus.technion.ac.il orrp@eecs.berkeley.edu belinkov@technion.ac.il ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "When artificial agents are jointly trained to perform collaborative tasks using a communication channel, they develop opaque goal-oriented communication protocols. Good task performance is often considered sufficient evidence that meaningful communication is taking place, but existing empirical results show that communication strategies induced by common objectives can be counterintuitive whilst solving the task nearly perfectly. In this work, we identify a goal-agnostic prerequisite to meaningful communication, which we term semantic consistency, based on the idea that messages should have similar meanings across instances. We provide a formal definition for this idea, and use it to compare the two most common objectives in the field of emergent communication: discrimination and reconstruction. We prove, under mild assumptions, that semantically inconsistent communication protocols can be optimal solutions to the discrimination task, but not to reconstruction. We further show that the reconstruction objective encourages a stricter property, spatial meaningfulness, which also accounts for the distance between messages. Experiments with emergent communication games validate our theoretical results. These findings demonstrate an inherent advantage of distancebased communication goals, and contextualize previous empirical discoveries. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Humans use language in multi-agent social interactions. Pressures of synchronization and collaboration play a central role in shaping the way we communicate. Motivated by this observation and the goal of creating artificial agents capable of meaningful communication, the field of emergent communication (EC) employs a multi-agent environment jointly trained to accomplish a task that requires active transmission of information. The agents utilize a messaging channel that usually takes the form of a discrete sequence of abstract symbols, resembling the structure of human language. Successful optimization results in synchronized agents operating a newly developed communication protocol tailored to the objective. We study a type of EC setup inspired by Lewis games [32] where a sender agent describes a given input and a receiver agent makes a prediction based on that description. The game objective is designed to make the receiver demonstrate knowledge of the original input, which in turn compels the sender to encode relevant information in the message. There are two common objectives used in this type of EC setup (illustrated in Figure 1): ", "page_idx": 0}, {"type": "text", "text": "Reconstruction In the reconstruction task [34, 44, 45], the original input is re-generated based solely on the message. We are specifically interested in a reconstruction objective that quantifies distance between prediction and target, forming a discrete version of autoencoding [18]. ", "page_idx": 0}, {"type": "text", "text": "Discrimination In the discrimination task [17, 28, 33], the original input is retrieved from a set of candidates, incentivized by negative log-likelihood. ", "page_idx": 0}, {"type": "image", "img_path": "me1MpmENpw/tmp/67f0d0d4caa5990160e8fbe80168ae9d38c4908ae9aa8ab8854a2006a68a2b51.jpg", "img_caption": ["Figure 1: Illustration of the reconstruction and discrimination tasks. The discrimination receiver is given the candidates in addition to the message. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "A central goal in EC, which motivates our work, is understanding how different factors in the environment affect the emergent protocol, and specifically developing agents and objectives that create protocols with similar characteristics to natural language [2, 25, 40, 41, 45]. Tools and experiments have been developed to evaluate the proximity to natural language by testing for properties such as compositionality [6] or efficiency [7]. Unfortunately, many of these empirical methods show great dissimilarity to human communication. One particularly surprising experiment [3] revealed that protocols created via the discrimination task can generalize extremely well to random noise data, suggesting that they do not signal human-recognizable (meaningful) properties of the input. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we identify a fundamental property of any meaningful communication protocol, and thus a prerequisite for similarity to natural language. We observe that the discrete bottleneck forces EC protocols to be many-to-one mappings, i.e., that messages likely represent more than one input. With this in mind, we claim that inputs mapped to the same message should be semantically similar, as is the case with human language. We formalize this idea with a mathematical definition that we term semantic consistency. We further develop a stricter version of this definition, spatial meaningfulness, which also accounts for distances between messages, and is therefore better suited to indicate similarity to natural language. ", "page_idx": 1}, {"type": "text", "text": "Armed with these definitions, we analyze theoretical solutions to the two common EC environments. In the reconstruction setting, under mild assumptions, we prove that every optimal solution is semantically consistent. With a different set of assumptions, we also show that reconstructioninduced communication protocols are spatially meaningful. In sharp contrast, we surprisingly find that the discrimination objective does not guarantee semantic consistency, i.e., a communication protocol can be optimal in a discrimination environment but still not semantically consistent nor spatially meaningful. In fact, we show that uniformly random messages can lead to a globally optimal discrimination objective value, meaning that the relationship between inputs mapped to the same message is potentially arbitrary despite optimally solving the task. Our results provide theoretical support to previous empirical findings, such as the discrimination generalization to noise. We further analyze several common variations of the discrimination game from the EC literature. ", "page_idx": 1}, {"type": "text", "text": "To support our findings, we run experiments on Shapes [27] and MNIST [30]. The empirical results agree with most of our theoretical findings. However, we observe a gap between theory and practice regarding the level of channel utilization, which we leave for future work to further investigate. ", "page_idx": 1}, {"type": "text", "text": "2 Background and related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Emergent communication with Lewis games ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A large portion of EC research, based on Lewis signaling games [28, 32], defines two-agent cooperative tasks where one agent, $S_{\\theta}$ (\u201csender\u201d), receives an input $x\\in\\mathscr{X}$ (\u201ctarget\u201d) and generates a message $m\\in M$ , and the second agent, $R_{\\varphi}$ (\u201creceiver\u201d), takes that message and outputs a prediction. The two most common such tasks are reconstruction and discrimination. ", "page_idx": 1}, {"type": "text", "text": "In the reconstruction task, the receiver tries to predict the target directly from the message. This is a generative method, so it can naturally be modeled with a receiver that outputs a distribution over the input space [7, 41, 42, 44], i.e., $R_{\\varphi}:M\\to\\Delta(\\mathcal{X})$ . In this case, the objective is usually negative log-likelihood or accuracy. However, this approach has two major flaws: First, explicitly outputting a distribution is not always feasible, e.g., over images. Thus, this game is mostly implemented on simple synthetic datasets. Second, the incentive of likelihood or accuracy does not measure how close the receiver\u2019s output is to $x$ .1In this work, we assume a different approach to reconstruction, inspired by autoencoders [18, 34], where the receiver outputs an object in the input space rather than a distribution, i.e., $R_{\\varphi}:M\\to\\mathcal X$ . Thus, the loss function in our analysis is straightforwardly the distance between prediction and target. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In the discrimination task, the receiver tries to identify the target within a set of candidates [17, 28, 29, 33, 44]. It can be modeled as $R_{\\varphi}:M\\times\\mathcal{X}^{d}\\dot{\\rightarrow}\\Delta\\{1,\\dot{.}\\dot{.}\\cdot,d\\}$ . We explore some alternative formulations in Appendix F. This setting introduces two major choices: the number of distractors (candidates other than $x$ ) and how to select them [12, 29]. These parameters have been shown to have a major effect on the resulting protocols [15, 29]. The results in the main body of this paper concern the vanilla version of the discrimination task, where candidates are chosen independently. We analyze other discrimination versions in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "2.2 Towards interpretable human-like communication ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A central line of study aims to develop EC systems that create protocols with high proximity to natural language. Some have tried aligning the two explicitly, by using pretrained language models [31, 43] or training translation systems [45, 46]. In this work, we focus on properties of the EC protocol itself, as induced by the objective rather than exposure to text. The most studied such characteristic is compositionality, which refers to the ability of agents to assemble complex units (e.g., sentences) from simpler units (e.g., words). Compositionality is hard to measure [1, 5, 6, 24, 37], and while some claim that it is correlated with generalization [2, 40, 42], the opposite has also been argued [8, 11, 13]. In any case, emergent languages are often not compositional [25], inefficient [7, 41], and generalize to noise [3], suggesting a mismatch between common EC objectives and the evolution of human language. In this work, we advance the goal of creating human-like communication by interpreting properties of natural language into formal constraints, and analyzing whether the EC objectives create protocols that follow them. A commonly used compositionality measure, topographic similarity (topsim) [5], is related to the definitions presented in this work. It evaluates the correlation between distances in the input space and the corresponding distances in the messages space. Notably, topsim considers the relationship between every pair of inputs, whereas our definitions only consider pairs that correspond to similar messages. The latter follows an intuitive asymmetry: inputs with similar messages are expected to be similar, but inputs with dissimilar messages do not have to be different (for example, the same image can often be described by several distinct human captions). ", "page_idx": 2}, {"type": "text", "text": "2.3 Input information encoded in the message ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The game objective dictates the information that needs to be stored in messages. In this work, we focus on spatial information by considering the geometric relationship between inputs mapped to the same message. To the best of our knowledge, no previous work has theoretically analyzed this idea in EC literature. A related concept is the mutual information (MI) between messages and inputs, which has been studied both empirically [1, 15, 23, 35, 45] and theoretically [19, 21, 42]. MI is strongly related to the infoNCE objective [38] often used in implementations of the discrimination game. MI can be used to measure the complexity of the communication channel [44, 45], and is correlated with task performance [23, 45] and even compositionality [1]. Finally, The mutual information gap [9] can be used to measure disentanglement [8, 13], which is also related to compositionality. However, Guo et al. [15] show that MI is not sufficient to indicate transfer-learning generalization, and connect that to pairwise distances of inputs mapped to the same message, effectively testing for semantic consistency. The first part of this work, combined with their findings, would suggest that languages developed by agents in the reconstruction setting should be more expressive and generalize better to different tasks. ", "page_idx": 2}, {"type": "text", "text": "On the theoretical side, a relevant work is Rita et al. [42], where the global discrimination objective is decomposed into two interpretable components, a co-adaptation loss and MI. The latter is equivalent to our Lemma A.1. In this work, we use such equivalent objectives for both the reconstruction and discrimination tasks to investigate the behaviour of optimal solutions, and specifically whether the tasks encourage semantic consistency. ", "page_idx": 2}, {"type": "image", "img_path": "me1MpmENpw/tmp/5bbfb2e8913ed90ac81c9ca0019359845da460740f9c6c7a9db1e0b3a197add3.jpg", "img_caption": ["Figure 2: Notation for the emergent communication (EC) setup. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3 Notation and task definitions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 General notation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We begin by introducing general notation for the EC setup; see Figure 2 for illustration. The input space is modeled by a real-valued random variable $\\bar{X}\\,=\\,(\\mathcal{X},\\bar{f}_{X})$ , where $\\mathcal{X}\\subseteq\\mathbb{R}^{d_{\\boldsymbol{x}}}$ is the bounded set of possible inputs (e.g., images), and $f_{X}\\colon{\\mathcal{X}}\\to\\mathbb{R}_{+}$ is a prior distribution. Denote by $M=\\left\\{m_{1},m_{2},\\dot{.}\\mathrm{\\dots}\\right\\}\\subset\\mathbb{R}^{\\hat{d}_{M}}$ the finite or countably infinite set of possible messages.2 We assume that the minimal distance between a pair of messages is attained, i.e., that $\\operatorname*{min}_{m_{1}\\neq m_{2}\\in M}\\|\\bar{m}_{1}-m_{2}\\|\\in(0,\\infty)$ . ", "page_idx": 3}, {"type": "text", "text": "The sender agent $S_{\\theta}\\colon\\mathcal{X}\\rightarrow M$ , parameterized by $\\theta\\in\\Theta$ , maps each input to a message. Sender $S_{\\theta}$ defines the communication protocol; we compare EC setups based on which optimal sender agents they induce. The receiver agent $R_{\\varphi}$ , maps the message and optionally additional input to a prediction. It is parameterized by $\\varphi\\in\\varPhi$ . During EC training, the goal is to learn hypotheses $(\\theta,\\varphi)$ that minimize a loss described by a function $\\ell\\colon\\Theta\\times\\varPhi\\times\\mathcal{X}\\to\\mathbb{R}$ , which maps a pair of agents and an input to a real value. To group all of the above notation into a single definition, denote an $E C$ setup as a sextuple $(\\mathcal{X},f_{X},M,\\Theta,\\varPhi,\\ell)$ . An EC setup induces a set of optimal agents by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\theta^{*},\\varphi^{*}\\in\\underset{\\theta\\in\\Theta,\\varphi\\in\\varPhi}{\\arg\\operatorname*{min}}\\underset{x\\sim X}{\\mathbb{E}}\\ell(\\theta,\\varphi,x).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that every part of the setup can affect the set of optimal agents. We also define a notion of conditional optimality, where one of the agents is fixed at a potentially sub-optimal hypothesis; sender $S_{\\theta}$ is synchronized with receiver $R_{\\varphi}$ if the pair is optimal for the EC setup $(\\mathcal{X},f_{X},M,\\Theta,\\{\\varphi\\},\\ell)$ . The same definition applies to the receiver agent respectively. Note that a pair of agents can be synchronized in both directions without being optimal. ", "page_idx": 3}, {"type": "text", "text": "3.2 Task definitions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the reconstruction setting, the receiver agent predicts the original input (the \u201ctarget\u201d) given only the message. In the discrimination setting, the receiver is given the message and a set of candidate inputs containing the target in a random position, and attempts to predict that position based on the message. A visual illustration can be seen in Figure 1. We now present formal definitions of these setups. ", "page_idx": 3}, {"type": "text", "text": "Reconstruction. The receiver maps a message $S_{\\theta}(x)$ to a prediction in the input space. The loss is the Euclidean distance between that prediction and the target $x$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\cdot\\ \\ell_{\\mathrm{reconstruction}}(\\theta,\\varphi,x):=\\|R_{\\varphi}(S_{\\theta}(x))-x\\|^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Definition 1. An EC setup (X, fX, M, \u0398, \u03a6, \u2113) is a reconstruction game if \u03a6 \u2286\u03a6rMec,oXnst ruction and \u2113= \u2113reconstruction. It has an unrestricted receiver hypothesis class if \u03a6 = \u03a6rMec,oXnst ruction ", "page_idx": 3}, {"type": "text", "text": "Discrimination. In addition to a message $S_{\\theta}(x)$ , the receiver sees a set of candidates $\\{x_{1},\\ldots,x_{d}\\}$ , which contains the target at a random position $t$ , i.e. $x_{t}=x$ , and the rest are $d-1$ independently sampled distractors. The receiver outputs a probability distribution over the candidates. The loss is the negative log-likelihood of the correct position $t$ according to this distribution, averaged over the target position and distractors. ", "page_idx": 3}, {"type": "text", "text": "$\\varPhi_{\\mathrm{discrimination}}^{M,\\mathcal{X},d}$ is defined as the set of all functions of the form $R\\colon M\\times\\mathcal{X}^{d}\\rightarrow\\Delta\\{1,\\dots,d\\}$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bullet\\begin{array}{r}{\\ell_{\\mathrm{discrimination}}^{X,d}(\\theta,\\varphi,x):=\\underset{x_{1},\\ldots,x_{(t-1)},x_{(t+1)},\\ldots,x_{d}\\sim X^{d-1}}{\\mathbb{E}}-\\log P(R_{\\varphi}(S_{\\theta}(x),x_{1},\\ldots,x_{d})=t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "2a.n dA $(\\mathcal{X},f_{X},M,\\Theta,\\varPhi,\\ell)$ i ciste ad $d$ -cceaivnedri dhaytpeost hdeissicsr icmlaisnsa itfi $\\varPhi\\subseteq$ $\\varPhi_{\\mathrm{discrimination}}^{M,\\mathcal{X},d}$ $\\ell=\\ell_{\\mathrm{discrimination}}^{X,d}$ $\\varPhi=\\varPhi_{\\mathrm{discrimin}}^{M,\\mathcal{X},d}$ ", "page_idx": 4}, {"type": "text", "text": "In this vanilla version of the discrimination game, the candidates are sampled independently. Several other choice mechanisms have been explored in EC literature. In Appendix A, we define and analyze the most common such variations: ", "page_idx": 4}, {"type": "text", "text": "Global discrimination In Appendix A.1, we analyze a version of the discrimination game where the receiver outputs a distribution over the entire data rather than a small set of candidates. We find that optimal communication protocols in this setting maximize mutual information between the inputs and messages, as shown in Rita et al. [42]. ", "page_idx": 4}, {"type": "text", "text": "Supervised discrimination Appendix A.2 explores a variant of the discrimination game that incorporates labels by selecting distractors with labels different from the target. We find that optimal communication strategies in this setting are both diverse (the sender is encouraged to output messages uniformly) and pure (labels distribute with low entropy given a message). ", "page_idx": 4}, {"type": "text", "text": "Classification discrimination In Appendix A.3, we consider a format of the discrimination game where the target is excluded from the candidate set, requiring the receiver to identify a candidate that matches the target\u2019s label. We find that optimal solutions in this setup maximize mutual information between messages and labels. ", "page_idx": 4}, {"type": "text", "text": "4 Semantic consistency: a prerequisite to meaningful communication ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In many EC environments, the message is a latent representation of the input, and is often directly analogous to the corresponding latent vector from continuous training setups (e.g. autoencoding, contrastive learning). That said, the discrete bottleneck in EC offers an additional perspective to the concept of a message; when the message space is smaller than the input space, the communication protocol is forced to be a many-to-one mapping, meaning that each message represents a set of inputs. ", "page_idx": 4}, {"type": "text", "text": "We consider inputs mapped to the same message as equivalent with respect to the communication protocol, thus defining a set of equivalence classes that partitions the input space. With this perspective in mind, we wish to understand whether this partition signals meaningful properties of the inputs. In other words, we ask the question: Is there a meaningful relationship between inputs mapped to the same message? This motivation is illustrated in Figure 3. ", "page_idx": 4}, {"type": "image", "img_path": "me1MpmENpw/tmp/48f339edd68ed605f9c2775ee79e271b781d80870430634d3100ff517d316ee9.jpg", "img_caption": ["Figure 3: A message describes a set of inputs. Note: the shapes and colors are not part of the input. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "The semantics of a message can be interpreted as the set of properties shared across all inputs in its equivalence class. We would like to design a theoretical test that evaluates whether meaningful properties are encoded in this way in an emergent communication protocol. We term this notion semantic consistency, based on the idea that inputs with shared properties are more semantically similar, as illustrated in Figure 3. We use the same idea to develop a formal definition: We define a communication protocol to be semantically consistent if a random pair of inputs mapped to the same message is, in expectation, more similar than two completely random inputs. Formally, for a communication protocol $S_{\\theta}$ , consider the following inequality: 3 ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{x_{1},x_{2}\\sim X}{\\mathbb{E}}\\Big[\\|x_{1}-x_{2}\\|^{2}\\Bigm|S_{\\theta}(x_{1})=S_{\\theta}(x_{2})\\Big]<\\underset{x_{1},x_{2}\\sim X}{\\mathbb{E}}\\Big[\\|x_{1}-x_{2}\\|^{2}\\Bigm].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Equation (1) is simplified in Appendix C.1 into the following formal definition: ", "page_idx": 5}, {"type": "text", "text": "Definition 3. A communication protocol $S_{\\theta}$ is semantically consistent if ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{\\underset{m\\sim S_{\\theta}(X)}{\\mathbb{E}}}\\left[\\operatorname{Var}\\left[X\\mid S_{\\theta}(X)=m\\right]\\right]<\\operatorname{Var}\\left[X\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The difference between the two expressions in Definition 3 is the explained variance, given by $\\mathrm{Var}_{m\\sim S_{\\theta}(X)}$ [E $:[X\\mid S_{\\theta}(X)=m]]$ . Thus, a communication protocol is semantically consistent if it explains some of the variance in $X$ . ", "page_idx": 5}, {"type": "text", "text": "Remark. If the message space is larger than the input space, a sender could map each input to a unique message and lose no information. Any such lossless protocol is trivially semantically consistent. ", "page_idx": 5}, {"type": "text", "text": "5 Are EC systems semantically consistent? ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Having defined the two common EC environments in Section 3.2, we now apply Definition 3 to assess whether semantically consistent protocols are induced by reconstruction and discrimination tasks. Formally, we answer the following question for each of the tasks: ", "page_idx": 5}, {"type": "text", "text": "Is every optimal emergent protocol semantically consistent? ", "page_idx": 5}, {"type": "text", "text": "Under the assumption of an unrestricted receiver hypothesis class, we find that: ", "page_idx": 5}, {"type": "text", "text": "\u2022 In the reconstruction game, the answer is yes. We show that equivalent inputs in reconstructioninduced communication protocols are clustered together in input space, matching Figure 3b. \u2022 In the discrimination game, the answer is no. The relationship between equivalent inputs in a discrimination-induced protocol can be arbitrary, matching Figure 3c. ", "page_idx": 5}, {"type": "text", "text": "All proofs are given in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "5.1 Reconstruction results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Ideally, we would like to examine a closed-form set of optimal communication protocols. Unfortunately, there is no closed-form solution to the set of optimal agent pairs in a general reconstruction setting. That said, we do have such a solution to the synchronized receiver, i.e., the optimal receiver for a fixed sender $S$ (the following arg min set contains a single item): ", "page_idx": 5}, {"type": "equation", "text": "$$\nR^{*}(m)=\\underset{r\\in\\mathcal{X}}{\\arg\\operatorname*{min}}\\ \\underset{x\\sim X}{\\mathbb{E}}\\left[\\|r-x\\|^{2}\\ \\middle|\\ S(x)=m\\right]=\\mathbb{E}\\left[X\\ |\\ S(X)=m\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "By assuming that $\\varPhi$ is unrestricted, we can plug the closed-form synchronized receiver back into the loss function. This yields a transformed objective given in the following lemma. ", "page_idx": 5}, {"type": "text", "text": "Lemma 5.1. [proof in page 16] Let $(\\mathcal{X},f_{X},M,\\Theta,\\varPhi,\\ell)$ be a reconstruction game. Assuming $\\varPhi$ is unrestricted, a sender $S_{\\theta}$ is optimal if and only if it minimizes the following objective: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sum_{m\\in M}P(S_{\\theta}(X)=m)\\cdot V a r\\left[X\\mid S_{\\theta}(X)=m\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This equivalent objective clearly shows the connection between inputs mapped to the same message: In every optimal solution, these inputs will have high proximity. (In fact, this is the objective function of $\\mathbf{k}$ -means clustering [36], as elaborated in Appendix B.) Moreover, this formula is the unexplained variance, so minimizing it will maximize the explained variance, leading to the following theorem. ", "page_idx": 5}, {"type": "text", "text": "Theorem 5.2. [proof in page 17] Let $(\\mathcal{X},f_{X},M,\\Theta,\\varPhi,\\ell)$ be a reconstruction game. Assuming $\\varPhi$ is unrestricted and $\\Theta$ contains at least one semantically consistent protocol, every optimal communication protocol is semantically consistent. ", "page_idx": 5}, {"type": "text", "text": "5.2 Discrimination results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In a similar fashion to the reconstruction setting, we have a closed form solution to the synchronized discrimination receiver:4 ", "page_idx": 6}, {"type": "equation", "text": "$$\nR^{*}(m,x_{1},\\ldots,x_{d})=\\mathbb{P}\\left(t\\mid x_{1},\\ldots,x_{d},m\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Explicitly (see Lemma 5.3\u2019s proof): ", "page_idx": 6}, {"type": "equation", "text": "$$\nP\\left(R^{*}(m,x_{1},\\ldots,x_{d})=j\\right)=\\frac{1}{|\\{x_{1},\\ldots,x_{d}\\}\\cap S^{-1}(m)|}\\cdot\\mathbf{1}\\{S(x_{j})=m\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Applying this receiver, we get the following discrimination loss:5 ", "page_idx": 6}, {"type": "text", "text": "Lemma 5.3. [proof in page 17] Let $(\\mathcal{X},f_{X},M,\\Theta,\\varPhi,\\ell)$ be a $d$ -candidates discrimination game. Assuming $\\varPhi$ is unrestricted, a sender $S_{\\theta}$ is optimal if and only if it minimizes the following objective: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{m\\in M}P(S_{\\theta}(X)=m)\\cdot\\mathbb{E}\\,\\log\\left(1+\\mathrm{Binomial}\\big(d-1,P(S_{\\theta}(X)=m)\\big)\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "And when $d=2$ (a single-distractor game), this simplifies into: $\\begin{array}{r}{\\sum_{m\\in M}P(S_{\\theta}(X)=m)^{2}}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "This equivalent objective reveals the interesting disparity between reconstruction and discrimination. Note that this objective is solely a function of the \u2018sizes\u2019 of equivalence classes (the probability for each message). Thus, while the above formula incentivizes the sender to distribute the messages uniformly, it does not impose any constraint on their content; the connection between inputs mapped to the same message could be arbitrary. In formal terms, we prove the following corollary: ", "page_idx": 6}, {"type": "text", "text": "Corollary 5.4. [proof in page 19] If the set of possible messages is finite, any sender agent that assigns them such that their distribution is uniform can be paired with some receiver to obtain $a$ globally optimal loss for the discrimination game. ", "page_idx": 6}, {"type": "text", "text": "Using this corollary, we prove that optimal solutions can be inconsistent in the discrimination setting: ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.5. [proof in page 20] There exists a discrimination game $(\\mathcal{X},f_{X},M,\\Theta,\\varPhi,\\ell)$ where $\\varPhi$ is unrestricted and $\\Theta$ contains at least one semantically consistent protocol, in which not all of the optimal communication protocols are semantically consistent. ", "page_idx": 6}, {"type": "text", "text": "6 A missing dimension: spatiality in the message space ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "While the notion of semantic consistency successfully differentiates between the two EC frameworks, it has a major shortcoming: is does not take into account distances between messages. Spatiality in the message space is integral to concepts like compositionality [5] and ease of teaching [33]. A hypothetical communication protocol that maps similar messages to very different meanings may satisfy Definition 3, but is fundamentally different from natural language. We now present a stricter version of semantic consistency, spatial meaningfulness, which does consider distances in the message space, and therefore better indicates an inherent similarity to natural language. In addition, we eliminate the assumption that the receiver\u2019s hypothesis class is unrestricted, in favor of simplicity and non-degeneracy conditions, which are more realistic in the context of natural language. ", "page_idx": 6}, {"type": "text", "text": "To introduce distance between messages, we replace the message equality with similarity in Equation (1). Let $\\varepsilon_{0}$ denote a threshold under which messages are considered similar. To avoid reverting back to Definition 3, we require that $\\varepsilon_{0}$ be greater than or equal to $\\varepsilon_{M}$ , defined as the minimal distance between messages: $\\varepsilon_{M}:=\\operatorname*{min}_{m_{1}\\neq m_{2}\\in M}\\left\\|m_{1}-m_{2}\\right\\|$ ", "page_idx": 6}, {"type": "text", "text": "Definition 4. For $\\varepsilon_{0}\\geq\\varepsilon_{M}$ , a communication protocol $S_{\\theta}$ is $\\varepsilon_{0}$ -spatially meaningful if $\\forall0<\\varepsilon\\leq\\varepsilon_{0}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\underset{x_{1},x_{2}\\sim X}{\\mathbb{E}}\\left[\\|x_{1}-x_{2}\\|^{2}\\;\\big|\\;\\|S_{\\theta}(x_{1})-S_{\\theta}(x_{2})\\|\\leq\\varepsilon\\right]<\\underset{x_{1},x_{2}\\sim X}{\\mathbb{E}}\\left[\\|x_{1}-x_{2}\\|^{2}\\right]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "A communication protocol $S_{\\theta}$ is spatially meaningful if this definition holds for $\\varepsilon_{0}=\\varepsilon_{M}$ ", "page_idx": 6}, {"type": "text", "text": "Note that this definition is stricter than semantic consistency. Informally, Definition 4 requires the protocol to be semantically consistent when combining pairs of close messages. ", "page_idx": 7}, {"type": "text", "text": "With this new definition, we are able to reach similar results to the original semantic consistency: ", "page_idx": 7}, {"type": "text", "text": "\u2022 In the reconstruction game, for any receiver that satisfies two conditions, we show that every synchronized sender is spatially meaningful. \u2022 In the discrimination game, given the same conditions, we show that a synchronized sender is not necessarily semantically consistent (and hence nor spatially meaningful). ", "page_idx": 7}, {"type": "text", "text": "We now define two conditions on the receiver. ", "page_idx": 7}, {"type": "text", "text": "Simplicity constraint. The first condition limits the receiver\u2019s complexity, and introduces the effect of distance between messages. We do that by bounding the rate of change in receiver\u2019s output, similarly to a Lipschitz constant. The bounding term depends on the variance of the input distribution and the chosen distance threshold. Formally: ", "page_idx": 7}, {"type": "text", "text": "Definition 5. Receiver $R_{\\varphi}$ is $(X,M,\\varepsilon_{0})$ -simple if $\\forall x_{1},x_{2}\\in\\mathrm{domain}(R_{\\varphi});$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|R_{\\varphi}(x_{1})-R_{\\varphi}(x_{2})\\|\\leq k\\cdot\\|x_{1}-x_{2}\\|,\\qquad{\\mathrm{where}}\\quad k={\\frac{{\\sqrt{2}}-1}{2\\varepsilon_{0}}}{\\sqrt{\\mathrm{var}[X]}}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Non-degeneracy constraint. The second condition prevents the receiver from being too weak. After applying the first condition, we no longer have a closed-form solution for the synchronized receiver. Instead, we take a non-optimal receiver, but add a condition to ensure it is strictly better than any constant function. Formally, let $\\varphi^{C}$ be a receiver that always outputs $C$ , and let $C^{*}=$ arg min $\\underset{x\\sim X}{\\mathbb{E}}\\ell(\\theta,\\varphi^{C},x)$ be the optimal such constant. (Note the sender does not matter here.) ", "page_idx": 7}, {"type": "text", "text": "Definition 6. Receiver $R_{\\varphi}$ is non-degenerate if ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in\\mathcal{X}}\\ell(\\theta^{*},\\varphi,x)\\leq\\frac{1}{4}\\cdot\\underset{x\\sim X}{\\mathbb{E}}\\ell(\\theta,\\varphi^{C^{*}},x)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for every sender $\\theta^{*}$ that is synchronized with $R_{\\varphi}$ ", "page_idx": 7}, {"type": "text", "text": "6.1 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "With these definitions, we have the following theorems: ", "page_idx": 7}, {"type": "text", "text": "Theorem 6.1. [proof in page 21] Let $(\\mathcal{X},f_{X},M,\\Theta,\\varPhi,\\ell)$ be a reconstruction game, let $\\varepsilon_{0}\\geq\\varepsilon_{M}$ and let $\\varphi\\in\\varPhi$ such that $R_{\\varphi}$ is $(X,M,\\varepsilon_{0})$ -simple and non-degenerate. Every sender that is synchronized with $R_{\\varphi}$ is $\\varepsilon_{0}$ -spatially meaningful. ", "page_idx": 7}, {"type": "text", "text": "Theorem 6.2. [proof in page 22] There exists a discrimination game $(\\mathcal{X},f_{X},M,\\Theta,\\varPhi,\\ell),$ , $\\varepsilon_{0}\\geq\\varepsilon_{M}$ and a receiver $\\varphi\\in\\varPhi$ which is $(X,M,\\varepsilon_{0})$ -simple and non-degenerate, where a synchronized sender matching $R_{\\varphi}$ is not $\\varepsilon$ -spatially meaningful for any $\\varepsilon$ . ", "page_idx": 7}, {"type": "text", "text": "7 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To support our theoretical findings, we run experiments on two datasets: (i) the MNIST dataset [30] contains images of a single hand-written digit; (ii) the Shapes dataset [27] contains images of an object with random shape, color and position. We train reconstruction and discrimination (40 distractors) agents using a communication channel of vocabulary size 10 and message length 4 (no EOS token), optimized with Gumbel-Softmax [20, 22]. In the MNIST experiments, we use the digit labels to train another set of agents with the supervised discrimination task (40 distractors). Further data and training details are found in Appendix D.1. ", "page_idx": 7}, {"type": "text", "text": "7.1 Semantic consistency, compositionality and task performance ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To evaluate semantic consistency empirically, we develop the empirical message variance measure (in Appendix D.2) which calculates the empirical variance of each equivalence class and takes a weighted average. We measure task performance using discrimination accuracy over 41 candidates (40 distractors), where the referential prediction of reconstruction-trained agents is defined as the closest candidate to the reconstructed target. We also report Topographic Similarity [5], the most common compositionality measure from literature. See Appendix E.1 for results with other compositionality measures (PosDis [8], BosDis [8] and Speaker-PosDis [13]). Additional information on each metric is given in Appendix D.2. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Table 1 and Table 2 show results over the Shapes and MNIST validation sets, respectively (showing standard deviation). While all setups achieve good task performance over MNIST, the discrimination game yields higher discrimination accuracy over the more difficult Shapes dataset, As expected. Interestingly, the discrimination-trained agents utilize less of the communication channel\u2019s capacity, as shown by the number of unique messages in both tables. This presents a gap from the theoretical analysis, as the discrimination setting should benefti from maximal message diversity (this can be seen in Lemma 5.3). To enable a fair comparison, we establish individual random baselines that maintain the number of inputs mapped to each message but randomize their assignment. Thus, the baseline\u2019s message variance shows the level of message diversity induced by the task, whereas the improvement over the baseline indicates semantic consistency. Over both datasets, the reconstruction task induces significantly more semantically consistent protocols both in absolute value and improvement over the baseline, as predicted by our theoretical findings. Furthermore, TopSim strongly favors the reconstruction setting, suggesting a connection between semantic consistency and compositionality. We note that the difference measured by TopSim is more significant over the Shapes dataset, which is itself more compositional, as objects have several attributes. These results, and the correlations reported in Appendix E.1, indicate little to no correlation between the evaluated compositionality and the discrimination objective performance. In fact, within the set of reconstruction runs on Shapes, their correlation is negative $(-0.24)$ . This illustrates the counter-intuitive nature of this objective, which we discuss in Section 5.2. ", "page_idx": 8}, {"type": "table", "img_path": "me1MpmENpw/tmp/fb1f5fab5bc699c562b11e8fd04a4b307a55b07228c3727c643b41322d98ce3d.jpg", "table_caption": ["Table 1: Empirical results on Shapes, averaged over five randomly initialized training runs. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "me1MpmENpw/tmp/35617e6114500e7308b494f15652f16ec5fc146784e8a8ab3d92e47b78e89b0b.jpg", "table_caption": ["Table 2: Empirical results on MNIST, averaged over three randomly initialized training runs. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "7.2 Message purity ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Since the MNIST dataset has only the digit label, we expect meaningful communication protocols to signal digits in their messages. To evaluate this, we calculate the percentage of images that have the majority digit label of their equivalence class. This can be interpreted as the highest attainable accuracy of a classifier mapping messages to digits. Figure 4 shows the most significant improvement over the baseline in the supervised discrimination game, as expected by Lemma A.2. The reconstruction task induces a nearly perfect digit description protocol as well, despite being unsupervised, providing evidence of its ability to unveil meaningful properties, in agreement with our findings. On the other hand, message purity is much lower in the unsupervised discrimination setting, again showcasing that task success does not guarantee an intuitive communication strategy. ", "page_idx": 8}, {"type": "text", "text": "In Appendix E.2, we perform a similar analysis over the Shapes dataset, by evaluating message purity with respect to each attribute individually or to a maximum aggregation. We find similar but weaker results. ", "page_idx": 8}, {"type": "image", "img_path": "me1MpmENpw/tmp/0c0eee1165e25ea844916f48d3d0da92565c5c339067d2caa6300aed128c5218.jpg", "img_caption": ["Figure 4: Average message purity, comparing trained models to random baselines. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7.3 Spatial meaningfulness analysis ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We develop an evaluation method for spatial meaningfulness, which we term cluster variance, based on the message variance measure. While we do not find a decisive result using this method, we suggest several ideas for further investigation; see Appendix E.3 for the full analysis. ", "page_idx": 9}, {"type": "text", "text": "8 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our analysis compares the two frameworks via their optimal solutions, and often assumes unrestricted hypothesis classes. In reality, the complexity of the agents is limited by the chosen architectures. Optimization-error affects the emergent protocol as well, especially when it is discrete, as regular differentiable training is not possible. Future work may further analyze these assumptions from the theoretical side, or evaluate their effect empirically. This limitation is evident in our empirical results (Section 7), where channel utilization in the discrimination game behaves differently than expected. ", "page_idx": 9}, {"type": "text", "text": "Our main results regarding the discrimination game deal with a simplistic version, where candidates are chosen independently. While we analyze some common variations in the appendix, many other ideas have been proposed and shown to have significant benefits, including multiple-target games [6, 37], different-view candidate representations [10, 16], and multi-modality [14]. ", "page_idx": 9}, {"type": "text", "text": "Our results assume, as illustrated in Figure 3, that proximity in the input space entails semantic information, and specifically that Euclidean distance broadly indicates the level of semantic similarity. Future work may investigate other distance metrics or evaluate the validity of this assumption. ", "page_idx": 9}, {"type": "text", "text": "9 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Based on properties of natural language, we have defined notions of semantic consistency and spatial meaningfulness, meant to evaluate meaningfulness in emergent communication protocols. Using these definitions, we found that communication protocols generated to solve the reconstruction objective have messages with inherent meaning, while the discrimination objective can be solved by unintuitive systems. Our findings provide insight into known empirical results in EC, such as the ability of agents in the discrimination game to perform well on unseen random noise. The theoretical tools that we have proposed can be used for future investigation and design of EC environments. As a main takeaway, we conclude that distance-based EC environments have promising potential in the prospect of inducing characteristics of natural language, whereas probability-based objectives must be more carefully designed. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research was supported by grant no. 2022330 from the United States - Israel Binational Science Foundation (BSF), Jerusalem, Israel, the Israel Science Foundation (grant no. 448/20), an Azrieli Foundation Early Career Faculty Fellowship, and an AI Alignment grant from Open Philanthropy. OP was funded by Project CETI via grants from Dalio Philanthropies and Ocean X; Sea Grape Foundation; Virgin Unite and Rosamund Zander/Hansjorg Wyss through The Audacious Project: a collaborative funding initiative housed at TED. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jacob Andreas. Measuring compositionality in representation learning. In International Conference on Learning Representations, 2018.   \n[2] Michal Auersperger and Pavel Pecina. Defending compositionality in emergent languages. NAACL 2022, page 285, 2022.   \n[3] Diane Bouchacourt and Marco Baroni. How agents see things: On visual representations in an emergent language game. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 981\u2013985, 2018.   \n[4] Nicolo\u2019 Brandizzi. Toward more human-like ai communication: A review of emergent communication research. IEEE Access, 11:142317\u2013142340, 2023.   \n[5] Henry Brighton and Simon Kirby. Understanding linguistic evolution by visualizing the emergence of topographic mappings. Artificial life, 12(2):229\u2013242, 2006.   \n[6] Boaz Carmeli, Yonatan Belinkov, and Ron Meir. Concept-best-matching: Evaluating compositionality in emergent communication. arXiv preprint arXiv:2403.14705, 2024.   \n[7] Rahma Chaabouni, Eugene Kharitonov, Emmanuel Dupoux, and Marco Baroni. Anti-efficient encoding in emergent communication. Advances in Neural Information Processing Systems, 32, 2019.   \n[8] Rahma Chaabouni, Eugene Kharitonov, Diane Bouchacourt, Emmanuel Dupoux, and Marco Baroni. Compositionality and generalization in emergent languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4427\u20134442, 2020.   \n[9] Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disentanglement in variational autoencoders. Advances in neural information processing systems, 31, 2018.   \n[10] Edward Choi, Angeliki Lazaridou, and Nando De Freitas. Compositional obverter communication learning from raw visual input. In 6th International Conference on Learning Representations, ICLR 2018. International Conference on Learning Representations, ICLR, 2018.   \n[11] Kevin Denamgana\u00ef and James Alfred Walker. On (emergent) systematic generalisation and compositionality in visual referential games with straight-through gumbel-softmax estimator. arXiv preprint arXiv:2012.10776, 2020.   \n[12] Kevin Denamgana\u00ef and James Alfred Walker. Referentialgym: A nomenclature and framework for language emergence & grounding in (visual) referential games. arXiv preprint arXiv:2012.09486, 2020.   \n[13] Kevin Denamgana\u00ef, Sondess Missaoui, and James Alfred Walker. Visual referential games further the emergence of disentangled representations. arXiv preprint arXiv:2304.14511, 2023.   \n[14] Katrina Evtimova, Andrew Drozdov, Douwe Kiela, and Kyunghyun Cho. Emergent communication in a multi-modal, multi-step referential game. In International Conference on Learning Representations, 2018.   \n[15] Shangmin Guo, Yi Ren, Kory Wallace Mathewson, Simon Kirby, Stefano V Albrecht, and Kenny Smith. Expressivity of emergent languages is a trade-off between contextual complexity and unpredictability. In International Conference on Learning Representations, 2021.   \n[16] Kamal Gupta, Gowthami Somepalli, Anubhav Anubhav, Vinoj Yasanga Jayasundara Magalle Hewa, Matthias Zwicker, and Abhinav Shrivastava. Patchgame: Learning to signal mid-level patches in referential games. Advances in Neural Information Processing Systems, 34:26015\u201326027, 2021.   \n[17] Serhii Havrylov and Ivan Titov. Emergence of language with multi-agent games: Learning to communicate with sequences of symbols. Advances in neural information processing systems, 30, 2017.   \n[18] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. science, 313(5786):504\u2013507, 2006.   \n[19] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In International Conference on Learning Representations, 2018.   \n[20] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparametrization with gumble-softmax. In International Conference on Learning Representations (ICLR 2017). OpenReview. net, 2017.   \n[21] Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, DJ Strouse, Joel Z Leibo, and Nando De Freitas. Social influence as intrinsic motivation for multi-agent deep reinforcement learning. In International conference on machine learning, pages 3040\u20133049. PMLR, 2019.   \n[22] Eugene Kharitonov, Rahma Chaabouni, Diane Bouchacourt, and Marco Baroni. Egg: a toolkit for research on emergence of language in games. EMNLP-IJCNLP 2019, page 55, 2019.   \n[23] Eugene Kharitonov, Rahma Chaabouni, Diane Bouchacourt, and Marco Baroni. Entropy minimization in emergent languages. In International Conference on Machine Learning, pages 5220\u20135230. PMLR, 2020.   \n[24] Tomasz Korbak, Julian Zubek, and Joanna R a\u02dbczaszek-Leonardi. Measuring non-trivial compositionality in emergent communication. arXiv preprint arXiv:2010.15058, 2020.   \n[25] Satwik Kottur, Jos\u00e9 Moura, Stefan Lee, and Dhruv Batra. Natural language does not emerge \u2018naturally\u2019in multi-agent dialog. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2962\u20132967, 2017.   \n[26] Harold W Kuhn and Albert W Tucker. Nonlinear programming. In Traces and emergence of nonlinear programming, pages 247\u2013258. Springer, 2013.   \n[27] Alexander Kuhnle and Ann Copestake. Shapeworld-a new test methodology for multimodal language understanding. arXiv preprint arXiv:1704.04517, 2017.   \n[28] Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. Multi-agent cooperation and the emergence of (natural) language. In International Conference on Learning Representations, 2016.   \n[29] Angeliki Lazaridou, Karl Moritz Hermann, Karl Tuyls, and Stephen Clark. Emergence of linguistic communication from referential games with symbolic and pixel input. In International Conference on Learning Representations, 2018.   \n[30] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.   \n[31] Jason Lee, Kyunghyun Cho, and Douwe Kiela. Countering language drift via visual grounding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4385\u20134395, 2019.   \n[32] David Lewis. Convention: A philosophical study. John Wiley & Sons, 2008.   \n[33] Fushan Li and Michael Bowling. Ease-of-teaching and language structure from emergent communication. Advances in neural information processing systems, 32, 2019.   \n[34] Toru Lin, Jacob Huh, Christopher Stauffer, Ser Nam Lim, and Phillip Isola. Learning to ground multi-agent communication with autoencoders. Advances in Neural Information Processing Systems, 34:15230\u201315242, 2021.   \n[35] Ryan Lowe, Jakob Foerster, Y-Lan Boureau, Joelle Pineau, and Yann Dauphin. On the pitfalls of measuring emergent communication. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, pages 693\u2013701, 2019.   \n[36] JB MACQUEEN. Some methods for classification and analysis of multivariate observations. In Proceedings of the 5th Berkeley Symposium on Mathematical Statistics & Probability, volume 1, pages 281\u2013297. University of California Press, 1967.   \n[37] Jesse Mu and Noah Goodman. Emergent communication of generalizations. Advances in Neural Information Processing Systems, 34:17994\u201318007, 2021.   \n[38] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.   \n[39] Gustav Grund Pihlgren, Fredrik Sandin, and Marcus Liwicki. Improving image autoencoder embeddings with perceptual loss. In 2020 International Joint Conference on Neural Networks (IJCNN), pages 1\u20137. IEEE, 2020.   \n[40] Yi Ren, Shangmin Guo, Matthieu Labeau, Shay B Cohen, and Simon Kirby. Compositional languages emerge in a neural iterated learning model. In 8th International Conference on Learning Representations, 2020.   \n[41] Mathieu Rita, Rahma Chaabouni, and Emmanuel Dupoux. \u201clazimpa\u201d: Lazy and impatient neural agents learn to communicate efficiently. In CONLL 2020-The SIGNLL Conference on Computational Natural Language Learning, 2020.   \n[42] Mathieu Rita, Corentin Tallec, Paul Michel, Jean-Bastien Grill, Olivier Pietquin, Emmanuel Dupoux, and Florian Strub. Emergent communication: Generalization and overfitting in lewis games. Advances in Neural Information Processing Systems, 35:1389\u20131404, 2022.   \n[43] Shane Steinert-Threlkeld, Xuhui Zhou, Zeyu Liu, and CM Downey. Emergent communication fine-tuning (ec-ft) for pretrained language models. In Emergent Communication Workshop at ICLR 2022, 2022.   \n[44] Mycal Tucker, Roger Levy, Julie A Shah, and Noga Zaslavsky. Trading off utility, informativeness, and complexity in emergent communication. Advances in neural information processing systems, 35: 22214\u201322228, 2022.   \n[45] Mycal Tucker, Roger P Levy, Julie Shah, and Noga Zaslavsky. Generalization and translatability in emergent communication via informational constraints. In NeurIPS 2022 Workshop on InformationTheoretic Principles in Cognitive Systems, 2022.   \n[46] Shunyu Yao, Mo Yu, Yang Zhang, Karthik Narasimhan, Joshua Tenenbaum, and Chuang Gan. Linking emergent and natural languages via corpus transfer. In International Conference on Learning Representations, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Analysis of discrimination variations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we extend our analysis of the discrimination game to three common variations from EC literature. ", "page_idx": 13}, {"type": "text", "text": "A.1 Global discrimination ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this variation, rather than sampling a small set of candidates, the receiver must discriminate between the entire set of data points, which does not have to be finite nor countable. In related work this game is often referred to as the reconstruction game [7, 41, 42, 44], as it can be interpreted as a generative task. However, since it uses the negative log-likelihood objective and not a distance-based one, we call it global discrimination. Note that when the set of data inputs $\\mathcal{X}$ is finite, this is similar to a special case of the regular discrimination game, where the number of candidates $d$ is set to $|{\\mathcal{X}}|$ (and candidates are sampled without replacement). This equivalence is also mentioned by [12]. Denote: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\ell_{\\mathrm{global}}(\\theta,\\varphi,x):=-\\log P(R_{\\varphi}(S_{\\theta}(x))=x)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Definition 7. An EC setup $(\\mathcal{X},f_{X},M,\\Theta,\\varPhi,\\ell)$ is a global discrimination game if $\\boldsymbol{\\varPhi}\\subseteq\\boldsymbol{\\varPhi}_{\\mathrm{global}}^{M,\\mathcal{X}}$ \u03a6global and \u2113= \u2113global. ", "page_idx": 13}, {"type": "text", "text": "Lemma A.1. [proof in page 24] Let $(\\mathcal{X},f_{X},M,\\Theta,\\varPhi,\\ell)$ be a global discrimination game. Assuming $\\varPhi$ is unrestricted, a sender $S_{\\theta}$ is optimal if and only if it minimizes the following objective: ", "page_idx": 13}, {"type": "equation", "text": "$$\n-I(X;S_{\\theta}(X))\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We see that optimal protocols maximize mutual information between inputs and messages. This corresponds to a popular idea in representation learning literature [19, 38]. However, similarly to the objective shown in Lemma 5.3 for the regular discrimination game, this communication goal does not consider distances between inputs. In fact, mutual information can be calculated for non-numerical input spaces. ", "page_idx": 13}, {"type": "text", "text": "A.2 Supervised discrimination ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this variation, labels are incorporated into the game via the candidate choice mechanism. The distractors (candidates that are not the target) are guaranteed to not have the same label as the target. This means that the receiver will never have to tell apart two inputs from the same class. ", "page_idx": 13}, {"type": "text", "text": "Let $\\boldsymbol{\\wp}$ be a finite set of labels, and let label : $:\\mathcal{X}\\rightarrow\\mathcal{Y}$ be a (deterministic) function mapping input to label. Denote the distribution over labels $Y=\\operatorname{label}(X)$ . We assume that this random variable has a uniform distribution, i.e., that the classes are balanced. The supervised variation uses the same receiver functions as in Definition 2, and a slightly different loss function: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\frac{X,Y,d}{\\mathrm{superissed}}(\\theta,\\varphi,x):=\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\frac{\\mathbb{E}}{\\exp\\left\\{1,\\ldots,d\\right\\},\\ x_{t}=x}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!-1\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\log P(R_{\\varphi}(S_{\\theta}(x),x_{1},\\ldots,x_{d})=t)}\\\\ &{}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Definition 8. For $d\\leq|\\mathcal{Y}|$ , an EC setup $(\\mathcal{X},f_{X},M,\\Theta,\\varPhi,\\ell)$ is a $d$ -candidates supervised discrimination game if $\\varPhi\\subseteq\\varPhi_{\\mathrm{discrimination}}^{M,\\mathscr{X},d}$ \u03a6discrimination and \u2113= \u2113s $\\ell=\\ell_{\\mathrm{supervis}}^{X,Y,d}$ ed. ", "page_idx": 13}, {"type": "text", "text": "Lemma A.2. [proof in page 24] Let $(\\mathcal{X},f_{X},M,\\Theta,\\varPhi,\\ell)$ be a 2-candidates supervised discrimination game. Assuming $\\varPhi$ is unrestricted, a sender $S_{\\theta}$ is optimal $i f$ and only if it minimizes the following objective: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{m\\in M}P(S_{\\theta}(X)=m)^{2}-\\sum_{m\\in M}\\sum_{y\\in\\mathcal{Y}}P(S_{\\theta}(X)=m,l a b e l(X)=y)^{2}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This result is interpretable too. The first expression is the unsupervised objective that encourages uniformity of message probabilities, which we encountered in Lemma 5.3. The second expression encourages non-diversity of labels within each equivalence class (optimal if all inputs mapped to a message share the same label). This means that optimal messages would have high mutual information with the labels, which in turn could introduce semantic consistency. However, within each class, the communication protocol would not necessarily be able to distinguish between inputs; perfect accuracy can be achieved with just $|\\mathcal{V}|$ unique messages. ", "page_idx": 13}, {"type": "text", "text": "This result presents another key implication, regarding complexity in the discrimination game [15]. We can consider a candidate choice mechanism that makes sure all distractors have the same label as the target (the opposite of the classic supervised game). This task will require the receiver to tell apart similar inputs, resulting in high contextual complexity. Interestingly, this task incentivizes the sender to map same-label inputs to different messages, in sharp contrast from Lemma A.2. Therefore, messages learned to solve this high-complexity task will have low mutual information with the labels, i.e., this game is likely to result in counter-intuitive communication protocols. ", "page_idx": 14}, {"type": "text", "text": "A.3 Classification discrimination ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The classification variation builds on top of the supervised version, by swapping out the target from the set of candidates, and replacing it with a random input that has the same label. In other words, the receiver attempts to guess the candidate with the same label as the target. This game is structured such that every class has exactly one representation in the set of candidates. This is somewhat related to the object-focused referential game [10, 12]. ", "page_idx": 14}, {"type": "text", "text": "Let $\\mathcal{V}=\\{1,\\ldots,n\\}$ be a finite set of labels, and let label : $\\mathcal X\\rightarrow\\mathcal Y$ be a (deterministic) function mapping input to label. Denote the distribution over labels $Y=\\operatorname{label}(X)$ . The classification variation uses the same receiver functions as in Definition 2, and the following loss function: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{classification}}^{X,Y}(\\theta,\\varphi,x):=\\underset{\\{x_{i}\\sim P(X|Y=i)\\}_{i=1}^{n}}{\\mathbb{E}}-\\log P(R_{\\varphi}(S_{\\theta}(x),x_{1},\\cdot\\cdot\\cdot,x_{n})=\\mathrm{label}(x))\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Definition 9. An EC setup $(\\mathcal{X},f_{X},M,\\Theta,\\varPhi,\\ell)$ is a classification discrimination game if $\\varPhi\\subseteq$ \u03a6M,X,n discrimination and \u2113= \u2113 $\\ell=\\ell_{\\mathrm{classi}}^{X,Y}$ classification. ", "page_idx": 14}, {"type": "text", "text": "Note that if $\\mathcal{X}$ is finite, we could choose $n=|{\\boldsymbol{\\mathcal{X}}}|$ and $\\operatorname{abel}(x_{i})=i$ , i.e. define labels as the identity mapping, and end up with the global discrimination game. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.3. [proof in page 25] Let $(\\mathcal{X},f_{X},M,\\Theta,\\varPhi,\\ell)$ be a classification discrimination game. Assuming $\\varPhi$ is unrestricted, a sender $S_{\\theta}$ is optimal if and only if it minimizes the following objective: ", "page_idx": 14}, {"type": "equation", "text": "$$\n-I(Y;S_{\\theta}(X))\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Optimal communication protocols in the classification game maximize mutual information with the labels. If the labels signal meaningful properties of the inputs, protocol induced by this game will reflect that, and be semantically consistent. However, similarly to the supervised discrimination game, optimal solutions can use just $|\\mathcal{V}|$ different messages, as they do not have to describe any properties of the input other than its label. ", "page_idx": 14}, {"type": "text", "text": "B Reconstruction and $\\mathbf{k}$ -means clustering ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma 5.1 shows that optimal communication protocols in the reconstruction setting minimize the objective function of $\\boldsymbol{\\mathrm{k}}$ -means clustering [36], where each equivalence class corresponds to a cluster. In this section we show a stronger connection between the two environments. Namely, we show that if $X$ is a uniform distribution with finite support, both $\\Theta$ and $\\varPhi$ are unrestricted, and the message space $M$ is finite, the reconstruction game can simulate $\\mathbf{k}$ -means clustering with $k=|M|$ (the equivalence classes correspond to clusters). This happens if we alternate between each agent\u2019s optimization: ", "page_idx": 14}, {"type": "text", "text": "Assignment step When the receiver is frozen and sender is optimized, the best solution (synchronized sender) assigns each input to its projection on Receiver\u2019s Image: ", "page_idx": 14}, {"type": "equation", "text": "$$\nR(S^{*}(x))=R(\\underset{m\\in{\\cal M}}{\\arg\\operatorname*{min}}\\,\\|x-R(m)\\|)=\\underset{x^{\\prime}\\in\\mathrm{Img}(R)}{\\arg\\operatorname*{min}}\\,\\|x-x^{\\prime}\\|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In other words, the sender chooses the message (cluster) with the closest receiver output (centroid) to its input. ", "page_idx": 14}, {"type": "text", "text": "Update step When the sender is frozen and receiver is optimized, the best solution (synchronized receiver) sets each output to be the mean over the relevant inputs: ", "page_idx": 14}, {"type": "equation", "text": "$$\nR^{*}(m)=\\underset{r\\in\\mathcal{X}}{\\arg\\operatorname*{min}}\\ \\underset{x\\sim I}{\\mathbb{E}}\\left[\\|r-x\\|^{2}\\;\\middle|\\;S(x)=m\\right]=\\mathbb{E}\\left[X\\mid S(X)=m\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In other words, the receiver updates its output (centroid) to be the mean over inputs mapped to the given message (cluster). ", "page_idx": 14}, {"type": "text", "text": "C Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For ease of notation, we often write $S,S^{*},R,R^{*}$ instead of $S_{\\theta},S_{\\theta^{*}},R_{\\varphi},R_{\\varphi^{*}}$ when the hypotheses are clear from context. Additionally, we use square brackets as a simple notation of a message\u2019s equivalence class, i.e., ", "page_idx": 15}, {"type": "equation", "text": "$$\n[m]\\equiv[m]_{\\theta}:=S_{\\theta}^{-1}(m)=\\{x\\in\\mathcal{X}:\\;S_{\\theta}(x)=m\\}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "C.1 Simplification of the semantic consistency definition ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "simplification of Equation (1). Recall Equation (1): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{x_{1},x_{2}\\sim X}{\\mathbb{E}}\\Big[\\|x_{1}-x_{2}\\|^{2}\\Bigm|S_{\\theta}(x_{1})=S_{\\theta}(x_{2})\\Big]<\\underset{x_{1},x_{2}\\sim X}{\\mathbb{E}}\\Big[\\|x_{1}-x_{2}\\|^{2}\\Bigm]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that since $\\mathcal{X}$ is bounded, these values as well as the expectation and variance of $X$ are finite. Using the identity $\\mathbb{E}_{\\substack{z_{1},x_{2}\\sim X}}[\\|x_{1}-x_{2}\\|^{2}]=2\\mathrm{Var}[X]$ , the left term is equal to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{x_{1},x_{2}\\sim X}{\\mathbb{E}}\\Big[\\|x_{1}-x_{2}\\|^{2}\\Bigm|S_{\\theta}(x_{1})=S_{\\theta}(x_{2})\\Big]}\\\\ &{=\\underset{m\\sim S_{\\theta}(X),x_{1},x_{2}\\sim X}{\\mathbb{E}}\\Big[\\|x_{1}-x_{2}\\|^{2}\\Bigm|S_{\\theta}(x_{1})=S_{\\theta}(x_{2})\\mathbf{\\Theta},\\ S_{\\theta}(x_{1})=m\\Big]}\\\\ &{=\\underset{m\\sim S_{\\theta}(X),x_{1},x_{2}\\sim X}{\\mathbb{E}}\\Big[\\|x_{1}-x_{2}\\|^{2}\\Bigm|S_{\\theta}(x_{1})=m,\\ ,S_{\\theta}(x_{2})=m\\Big]}\\\\ &{=2\\cdot\\underset{m\\sim S_{\\theta}(X)}{\\mathbb{E}}[\\mathrm{Var}\\left[X\\mid S_{\\theta}(X)=m\\right]]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and the right term is simply $2\\mathrm{Var}[X]$ . Applying this to Equation (1) yields Definition 3. ", "page_idx": 15}, {"type": "text", "text": "C.2 Semantic consistency proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Reconstruction ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma 5.1. Let $(\\mathcal{X},f_{X},M,\\Theta,\\varPhi,\\ell)$ be a reconstruction game. Assuming $\\varPhi$ is unrestricted, a sender $S_{\\theta}$ is optimal if and only if it minimizes the following objective: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{m\\in M}P(S_{\\theta}(X)=m)\\cdot\\operatorname{Var}\\left[X\\mid S_{\\theta}(X)=m\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 5.1. Let $X_{[m]}$ denote the distribution $P\\left(X\\mid X\\in[m]\\right)$ The synchronized receiver for sender $S$ is defined by: ", "page_idx": 15}, {"type": "equation", "text": "$$\nR^{*}=\\arg\\operatorname*{min}_{R\\in\\phi}\\mathbb{E}_{x\\sim X}\\|R(S(x))-x\\|^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $\\varPhi$ is assumed to be unrestricted, we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R^{*}(m)=\\underset{r\\in\\mathcal{X}}{\\arg\\operatorname*{min}}\\,\\underset{x\\sim X}{\\mathbb{E}}\\left[\\|r-x\\|^{2}\\;\\middle|\\;S(x)=m\\right]}\\\\ &{\\qquad\\quad=\\underset{r\\in\\mathcal{X}}{\\arg\\operatorname*{min}}\\,\\underset{x\\sim X_{[m]}}{\\mathbb{E}}\\|r-x\\|^{2}}\\\\ &{\\qquad\\quad=\\mathbb{E}\\left[X_{[m]}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can apply this synchronized receiver back in the reconstruction loss function without changing the set of optimal sender hypotheses, since a sender is optimal if and only if when paired with a ", "page_idx": 15}, {"type": "text", "text": "synchronized receiver, the pair is optimal. Note that $[m]=S^{-1}(m)$ depends on sender. We get: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S^{*}\\in\\underset{s\\in\\Theta}{\\operatorname{arg\\min}}\\ \\underset{x\\sim\\times x}{\\mathbb{E}}\\left\\|R^{*}(S(x))-x\\right\\|^{2}}\\\\ &{\\quad=\\underset{\\tilde{S}\\in\\Theta}{\\operatorname{arg\\min}}\\ \\underset{x\\sim\\times x}{\\mathbb{E}}\\left\\|\\mathbb{E}\\left[X_{[S(x)]}\\right]-x\\right\\|^{2}}\\\\ &{\\quad=\\underset{\\tilde{S}\\in\\Theta}{\\operatorname{arg\\min}}\\ \\underset{m\\in\\cal M}{\\sum}\\left[P(X\\in[m])\\cdot\\underset{x\\sim X_{[m]}}{\\mathbb{E}}\\left\\|\\mathbb{E}\\left[X_{[S(i)]}\\right]-x\\right\\|^{2}\\right]}\\\\ &{\\quad=\\underset{s\\in\\Theta}{\\operatorname{arg\\min}}\\ \\underset{m\\in\\cal M}{\\sum}\\left[P(X\\in[m])\\cdot\\underset{x\\sim X_{[m]}}{\\mathbb{E}}\\left\\|\\mathbb{E}\\left[X_{[m]}\\right]-x\\right\\|^{2}\\right]}\\\\ &{\\quad=\\underset{s\\in\\Theta}{\\operatorname{arg\\min}}\\ \\underset{m\\in\\cal M}{\\sum}P(X\\in[m])\\cdot\\underset{\\nabla^{\\!\\mathrm{arg}}}{\\operatorname{sur}}\\left[X_{[m]}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The third line is the law of total expectation, where the events are the assignment to message $m$ . ", "page_idx": 16}, {"type": "text", "text": "Theorem 5.2. Let $(\\mathcal{X},f_{X},M,\\Theta,\\varPhi,\\ell)$ be a reconstruction game. Assuming $\\varPhi$ is unrestricted and $\\Theta$ contains at least one semantically consistent protocol, every optimal communication protocol is semantically consistent. ", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 5.2. Following Lemma 5.1, let $\\theta^{*}$ be an optimal reconstruction sender, i.e., ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta^{*}\\in\\underset{\\theta}{\\arg\\operatorname*{min}}\\displaystyle\\sum_{m\\in M}P(X\\in[m]_{\\theta})\\cdot\\mathrm{\\operatorname{Var}}\\left[X\\mid X\\in[m]_{\\theta}\\right]}\\\\ &{\\quad=\\underset{\\theta}{\\arg\\operatorname*{min}}\\ \\mathbb{E}\\left[\\mathrm{Var}\\left[X\\mid S_{\\theta}(X)\\right]\\right]}\\\\ &{\\quad=\\underset{\\theta}{\\arg\\operatorname*{max}}\\ -\\mathbb{E}\\left[\\mathrm{Var}\\left[X\\mid S_{\\theta}(X)\\right]\\right]}\\\\ &{\\quad=\\underset{\\theta}{\\arg\\operatorname*{max}}\\ \\mathrm{\\operatorname{Var}}\\left[X\\right]-\\mathbb{E}\\left[\\mathrm{Var}\\left[X\\mid S_{\\theta}(X)\\right]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The final formula is non-negative by variance decomposition. Since $\\Theta$ contains a semantically consistent protocol, this expressions can be greater than zero, so since $\\theta^{*}$ maximizes it, we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname{Var}\\left[X\\right]-\\mathbb{E}\\left[\\operatorname{Var}\\left[X\\mid S_{\\theta}(X)\\right]\\right]>0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Which proves that Definition 3 is satisfied. ", "page_idx": 16}, {"type": "text", "text": "Discrimination ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 5.3. Let $(\\mathcal{X},f_{X},M,\\Theta,\\varPhi,\\ell)$ be a $d$ -candidates discrimination game. Assuming $\\varPhi$ is unrestricted, a sender $S_{\\theta}$ is optimal if and only if it minimizes the following objective: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{m\\in M}P(S_{\\theta}(X)=m)\\cdot\\mathbb{E}\\,\\log\\left(1+\\mathrm{Binomial}\\big(d-1,P(S_{\\theta}(X)=m)\\big)\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "And when $d=2$ (a single-distractor game), this simplifies into ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{m\\in M}P(S_{\\theta}(X)=m)^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 5.3. Given message $m$ and candidates $(x_{1},\\ldots,x_{d})$ , such that $t$ is the index of the target $(S(x_{t})=m)$ ), denote $C=\\{x_{1},\\ldots,x_{d}\\}\\cap[m]$ . Since $t$ is sampled uniformly and only candidates in $C$ have a non-zero likelihood of being the target, we have for all $j\\in\\{1,\\dotsc,d\\}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{^{>}(t=j\\mid x_{1},\\dots,x_{d},m)={\\frac{f_{X}\\left(x_{1},\\dots,x_{d}\\right)\\cdot P\\left(t=j\\mid x_{1},\\dots,x_{d}\\right)\\cdot P\\left(S(x_{t})=m\\mid x_{1},\\dots,x_{d},t\\right)}{f_{X}\\left(x_{1},\\dots,x_{d}\\right)\\cdot P\\left(S(x_{t})=m\\mid x_{1},\\dots,x_{d}\\right)}}}&{={\\frac{1}{d}}\\cdot\\mathbf{1}\\left\\{S(x_{j})=m\\right\\}}\\\\ {={\\frac{{\\frac{1}{d}}\\cdot\\mathbf{1}\\left\\{S(x_{j})=m\\right\\}}{\\frac{|C|}{d}}}}&{}\\\\ {={\\frac{1}{|C|}}\\cdot\\mathbf{1}\\left\\{x_{j}\\in C\\right\\}}&{}\\end{array}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Denote this probability $p_{j}$ . A synchronized receiver for sender $S$ is defined as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R^{*}\\in\\underset{R\\in\\phi}{\\operatorname{arg\\,min}}\\underset{x\\sim X}{\\mathbb{E}}\\ell(\\theta,R,x)}\\\\ &{\\quad=\\underset{R\\in\\phi}{\\operatorname{arg\\,min}}\\underset{x\\sim X}{\\mathbb{E}}\\qquad\\underset{t\\sim U\\{1,\\dots,d\\},\\;x_{t}=x}{\\mathbb{E}}-\\log P(R(S(x),x_{1},\\dots,x_{d})=t)}\\\\ &{\\quad=\\underset{R\\in\\phi}{\\operatorname{arg\\,min}}\\underset{x_{1},\\dots,x_{d}\\sim X}{\\mathbb{E}}-\\log P(R(S(x_{t}),x_{1},\\dots,x_{d})=t)}\\\\ &{\\quad=\\underset{R\\in\\phi}{\\operatorname{arg\\,min}}\\underset{x_{1},\\dots,x_{d}\\sim X}{\\mathbb{E}}-\\log P(R(S(x_{t}),x_{1},\\dots,x_{d})=t)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\varPhi$ is assumed to be unrestricted, we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R^{*}(m,x_{1},\\ldots,x_{d})\\in\\underset{(q_{1},\\ldots,q_{d})\\in\\Delta\\{1,\\ldots,d\\}}{\\arg\\operatorname*{min}}\\,t\\sim\\underset{\\{1,\\ldots,d\\}}{\\mathbb{E}}\\left[-\\log q_{t}\\;|\\;x_{1},\\ldots,x_{d},m,S(x_{t})=m\\right]}\\\\ &{=\\underset{(q_{1},\\ldots,q_{d})\\in\\Delta\\{1,\\ldots,d\\}}{\\arg\\operatorname*{min}}-\\underset{j=1}{\\overset{d}{\\sum}}p_{j}\\log q_{j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A lower bound on the objective (Gibbs\u2019 inequality): ", "page_idx": 17}, {"type": "equation", "text": "$$\n-\\sum_{j=1}^{d}p_{j}\\log q_{j}\\geq-\\sum_{j=1}^{d}p_{j}\\log p_{j}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This lower bound is achieved by selecting $q_{j}\\mathrm{~=~}p_{j}$ (negative log-likelihood is a proper scoring rule). Note that $p_{j}\\geq0\\ \\forall j$ and $\\textstyle\\sum_{j=1}^{d}p_{j}=1$ , making it a legal output for $R$ . We now apply this receiver back in the loss function, similarly to the proof of Lemma 5.1. Recall that $x_{1},\\ldots,x_{d}$ are the candidates, and given the target\u2019s index $t$ , the other candidates $(x_{1},\\ldots,x_{(t-1)},x_{(t+1)},\\ldots,x_{d})$ are called distractors. Also note that $[m]=S^{-1}(m)$ depends on sender. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{\\mathbf V}^{*}\\in\\mathrm{ang~scall~}\\mathbb{P}_{-1}^{N},\\;[{\\mathbf S}^{*},T^{*},\\tau^{*}]}\\\\ {=}&{\\!\\!\\operatorname*{ang}_{s\\in\\mathrm{~thim~}}\\underbrace{\\mathbf R}_{\\neq,T^{*}}-\\log P(F^{*}(S(x_{t}),x_{1},\\ldots,x_{d})=t)}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "if $d=2$ (a single-distractor game), the expression simplifies to: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{S\\in\\Theta}{\\operatorname{arg\\,min}}\\,\\sum_{m\\in M}P(X\\in[m])\\cdot\\mathbb{E}\\,\\log\\Big(1+\\mathtt{B i n o m i a l}\\big(1,P(I\\in[m])\\big)\\Big)}\\\\ &{=\\underset{S\\in\\Theta}{\\operatorname{arg\\,min}}\\,\\sum_{m\\in M}P(X\\in[m])\\cdot\\Big(P(X\\in[m])\\cdot\\log2+(1-P(X\\in[m]))\\cdot\\log1\\Big)}\\\\ &{=\\underset{S\\in\\Theta}{\\operatorname{arg\\,min}}\\,\\sum_{m\\in M}P(X\\in[m])^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Corollary 5.4. If the set of possible messages is finite, any sender agent that assigns them such that their distribution is uniform can be paired with some receiver to generate globally optimal loss for the discrimination game. ", "page_idx": 18}, {"type": "text", "text": "Proof of Corollary 5.4. Let $n=|M|$ be the finite number of available messages. Denote the vector $\\mathbf{p}=(p_{1},\\ldots,p_{n})$ such that $p_{i}={\\dot{P}}({\\dot{X}}\\in[m_{i}])$ . For $x\\in[0,1]$ , denote the function ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(x)=x\\cdot\\mathbb{E}\\ \\log(1+\\mathrm{Binomial}(d-1,x))\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Following Lemma 5.3, we can formulate the task as a constrained optimization problem on the simplex: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{min}_{\\mathbf{p}\\in\\mathbb{R}^{n}}}&{F(\\mathbf{p}):=\\displaystyle\\sum_{i=1}^{n}f(p_{i})}\\\\ {\\mathrm{s.t.}}&{\\mathbf{p}^{T}\\mathbf{1}=1}\\\\ &{\\mathbf{p}\\geq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that the distribution of $X$ may create additional constraints. We will prove that $f$ is convex, meaning that this is a convex problem, so any KKT solution is global minimum [26]. The uniform solution $\\mathbf{p}^{*}=({\\textstyle{\\frac{1}{n}}},\\dots,{\\textstyle{\\frac{1}{n}}})$ is indeed KKT: assigning $\\lambda={\\vec{0}}$ and $\\textstyle\\mu=-\\nabla f({\\frac{1}{n}})$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla F(\\mathbf{p}^{*})+\\lambda^{T}\\nabla(-\\mathbf{p})(\\mathbf{p}^{*})+{\\boldsymbol{\\mu}}\\cdot\\nabla(\\mathbf{p}^{T}\\mathbf{1}-\\mathbf{1})(\\mathbf{p}^{*})=\\nabla f\\left({\\frac{1}{n}}\\right)+\\lambda^{T}(-I_{n})+{\\boldsymbol{\\mu}}\\cdot\\mathbf{1}={\\vec{0}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It is left to prove that $f$ is convex over [0, 1]. For simplicity, we replace $d-1$ with $d\\in\\mathbb{N}^{+}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(x)=x\\cdot\\mathbb{E}[\\log(1+\\mathrm{Binom}(d,x))]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This convexity proof is adjusted from mathoverflow (link). Denote ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\tilde{g}(k)=\\log(1+k)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\nB_{g}^{d}(x)=\\mathbb{E}[g({\\mathrm{Binom}}(d,x))]=\\sum_{k=0}^{d}g(k){\\binom{d}{k}}x^{k}(1-x)^{d-k}\\quad\\Rightarrow\\quad f(x)=x\\cdot B_{\\bar{g}}^{d}(x)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{B_{g}^{d\\prime}(x)=\\displaystyle\\sum_{k=0}^{d}{g(k)\\binom{d}{k}k x^{k-1}(1-x)^{d-k}-\\displaystyle\\sum_{k=0}^{d}{g(k)\\binom{d}{k}(d-k)x^{k}(1-x)^{d-k-1}}}}\\\\ {=\\displaystyle\\sum_{k=1}^{d}{g(k)\\binom{d}{k}k x^{k-1}(1-x)^{d-k}-\\displaystyle\\sum_{k=0}^{d-1}{g(k)\\binom{d}{k}(d-k)x^{k}(1-x)^{d-k-1}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using the identities $\\binom{d}{k}k=d\\binom{d-1}{k-1}$ and $\\binom{d}{k}(d-k)=d\\binom{d-1}{k}$ , we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{g}^{d\\prime}(x)=\\displaystyle\\sum_{k=1\\atop k=1}^{d}g(k)d\\binom{d-1}{k-1}x^{k-1}(1-x)^{d-k}-\\displaystyle\\sum_{k=0}^{d-1}g(k)d\\binom{d-1}{k}x^{k}(1-x)^{d-k-1}}\\\\ &{\\qquad=d\\displaystyle\\sum_{k=0\\atop k=1}^{d-1}g(k+1)\\binom{d-1}{k}x^{k}(1-x)^{d-1-k}-d\\displaystyle\\sum_{k=0}^{d-1}g(k)\\binom{d-1}{k}x^{k}(1-x)^{d-1-k}}\\\\ &{\\qquad=d\\displaystyle\\sum_{k=0}^{d-1}(g(k+1)-g(k))\\binom{d-1}{k}x^{k}(1-x)^{d-1-k}}\\\\ &{\\qquad=d\\cdot B_{\\Delta_{g}^{d-1}}^{d-1}(x)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\Delta g(k)=g(k+1)-g(k)$ . ", "page_idx": 18}, {"type": "text", "text": "We can apply this again to get the second derivative of $B_{g}^{d}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\nB_{g}^{d\\prime\\prime}(x)=d\\cdot B_{\\Delta g}^{d-1\\prime}(x)=d(d-1)B_{\\Delta^{2}g}^{d-2}(x)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We can now write the second derivative of $f$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\quad^{\\prime\\prime}(x)=2B_{\\bar{g}}^{d}(x)+x\\cdot B_{\\bar{g}}^{d\\prime}(x)}\\\\ &{\\qquad=2d\\cdot B_{\\Delta\\bar{g}}^{d-1}(x)+x\\cdot d(d-1)B_{\\Delta^{2}\\bar{g}}^{d-2}(x)}\\\\ &{\\qquad=2d{\\underset{k=0}{\\overset{d-1}{\\sum}}}\\Delta\\tilde{g}(k){\\binom{d-1}{k}}x^{k}(1-x)^{d-1-k}+d(d-1)\\sum_{k=0}^{d-2}\\Delta^{2}\\tilde{g}(k){\\binom{d-2}{k}}x^{k+1}(1-x)^{d-2-k}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\begin{array}{r}{\\binom{d-2}{k-1}=\\frac{k}{d-1}\\binom{d-1}{k}}\\end{array}$ , the second term can be written as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle l(d-1)\\sum_{k=0}^{d-2}\\Delta^{2}\\tilde{g}(k)\\binom{d-2}{k}x^{k+1}(1-x)^{d-2-k}=d(d-1)\\sum_{k=1}^{d-1}\\Delta^{2}\\tilde{g}(k-1)\\binom{d-2}{k-1}x^{k}(1-x)^{d-1}}}\\\\ {{=d\\displaystyle\\sum_{k=1}^{d-1}\\Delta^{2}\\tilde{g}(k-1)k\\binom{d-1}{k}x^{k}(1-x)^{d-1-k}}}\\\\ {{=d\\displaystyle\\sum_{k=0}^{d-1}\\Delta^{2}\\tilde{g}(k-1)k\\binom{d-1}{k}x^{k}(1-x)^{d-1-k}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{f^{\\prime\\prime}(x)=2d\\displaystyle\\sum_{k=0}^{d-1}\\Delta\\tilde{g}(k){\\binom{d-1}{k}}x^{k}(1-x)^{d-1-k}+d\\displaystyle\\sum_{k=0}^{d-1}\\Delta^{2}\\tilde{g}(k-1)k{\\binom{d-1}{k}}x^{k}(1-x)^{d-1-k}}}\\\\ {{=d\\displaystyle\\sum_{k=0}^{d-1}\\left(2\\Delta\\tilde{g}(k)+\\Delta^{2}\\tilde{g}(k-1)k\\right){\\binom{d-1}{k}}x^{k}(1-x)^{d-1-k}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To finish the proof, we show that the term $h(k)\\equiv2\\Delta\\tilde{g}(k)+\\Delta^{2}\\tilde{g}(k-1)k$ is positive, proving that $f$ is convex. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\Delta^{2}g(k)=\\Delta g(k+1)-\\Delta g(k)=g(k+2)-g(k+1)-(g(k+1)-g(k))}}\\\\ {{=g(k+2)+g(k)-2g(k+1)}}\\\\ {{2\\Delta g(k)+\\Delta^{2}g(k-1)k=2g(k+1)-2g(k)+k(g(k+1)+g(k-1)-2g(k))}}\\\\ {{=(k+2)\\cdot g(k+1)+k\\cdot g(k-1)-2(k+1)\\cdot g(k)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We get ", "page_idx": 19}, {"type": "equation", "text": "$$\nh(k)\\equiv2\\Delta\\tilde{g}(k)+\\Delta^{2}\\tilde{g}(k-1)k=(k+2)\\cdot\\log(k+2)+k\\cdot\\log(k)-2(k+1)\\cdot\\log(k+1)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For $k\\,>\\,0$ , the function $t\\log t$ is convex, thus $h(k)\\,>\\,0$ by Jensen\u2019s inequality. Also, $h(0)\\,=$ $2\\log2>0$ . ", "page_idx": 19}, {"type": "text", "text": "Theorem 5.5. There exists a discrimination game $(\\mathcal{X},f_{X},M,\\Theta,\\varPhi,\\ell)$ where $\\varPhi$ is unrestricted and $\\Theta$ contains at least one semantically consistent protocol, in which not all of the optimal communication protocols are semantically consistent. ", "page_idx": 19}, {"type": "text", "text": "Proof of Theorem 5.5. If $X$ is a uniform distribution over a finite support, then as shown by Corollary 5.4, a Sender is optimal if it maps the same number of inputs to every message. Thus, for example, if $|\\mathcal{X}|=2\\cdot|M|$ (both finite), any Sender that maps exactly two inputs to each message is optimal, and we can define such an agent by mapping the least similar pairs of inputs together, creating an optimal Sender that does not induce a meaning-consistent latent space. For a concrete example, see the proof for Theorem 6.2. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "C.3 Spatial meaningfulness proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Theorem 6.1. Let $(\\mathcal{X},f_{X},M,\\Theta,\\varPhi,\\ell)$ be a reconstruction game, let $\\varepsilon_{0}\\geq\\varepsilon_{M}$ and let $\\varphi\\in\\varPhi$ such that $R_{\\varphi}$ is $(X,M,\\varepsilon_{0})$ -simple and non-degenerate. Every sender that is synchronized with $R_{\\varphi}$ is $\\varepsilon_{0}$ -spatially meaningful. ", "page_idx": 20}, {"type": "text", "text": "Proof of Theorem 6.1. In the reconstruction task, the optimal constant receiver is ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{C^{*}=\\arg\\operatorname*{min}_{\\scriptstyle\\atop{\\scriptstyle x\\sim X}}{\\mathbb{E}}\\ell(\\theta,\\varphi^{C},x)}\\\\ &{\\quad={\\arg\\operatorname*{min}_{\\scriptstyle\\ x\\sim X}{\\mathbb{I}}\\left\\|C-x\\right\\|^{2}}}\\\\ &{\\quad={\\underset{\\scriptstyle x\\sim X}{\\mathbb{E}}}[x]\\equiv{\\bar{X}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $\\theta^{*}$ be a sender synchronized with $R_{\\varphi}$ . Denote $L=\\operatorname*{sup}_{x\\in\\mathcal{X}}\\ell(\\theta^{*},\\varphi,x)$ and $\\textstyle{\\bar{L}}={\\frac{1}{2}}\\cdot\\underbrace{\\mathbb{E}}_{x\\sim X}\\|x-{\\bar{X}}\\|^{2}$ . Since $R_{\\varphi}$ is strictly better than constant, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\forall x\\in\\mathcal{X}\\ :\\ \\ \\ \\ell(\\theta^{*},\\varphi,x)\\leq L\\leq\\frac{\\bar{L}}{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Denote the receiver\u2019s prediction $R_{\\varphi}(S_{\\theta^{*}}(x)):=\\hat{x}_{\\varphi}$ . Note that when the sender\u2019s hypothesis class $\\Theta$ is unrestricted, this becomes the projection on $R_{\\varphi}$ \u2019s image: ", "page_idx": 20}, {"type": "equation", "text": "$$\nR_{\\varphi}(S_{\\theta^{*}}(x))=R_{\\varphi}(\\underset{m\\in M}{\\mathrm{arg}\\,\\mathrm{min}}\\,\\|x-R_{\\varphi}(m)\\|)=\\underset{x^{\\prime}\\in\\mathrm{Img}(R_{\\varphi})}{\\mathrm{arg}\\,\\mathrm{min}}\\,\\|x-x^{\\prime}\\|=\\underset{\\mathrm{Img}(R_{\\varphi})}{\\mathrm{Proj}}\\,(x).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By the definition of $\\ell_{\\mathrm{reconstruction}}$ , we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\forall\\,x\\in\\mathcal{X}}&{\\|x-\\hat{x}_{\\varphi}\\|^{2}\\leq L}\\\\ {\\Rightarrow\\ \\ \\forall\\,x\\in\\mathcal{X}}&{\\|x-\\hat{x}_{\\varphi}\\|\\leq\\sqrt{L}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $\\begin{array}{r}{l_{R}=\\frac{\\sqrt{2}-1}{2\\varepsilon_{0}}\\sqrt{\\operatorname{Var}[X]}}\\end{array}$ be the constant from Definition 5. For any $\\varepsilon>0$ and $x_{1},x_{2}\\in\\mathcal{X}$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\lVert S_{\\boldsymbol{\\theta}^{*}}(x_{1})-S_{\\boldsymbol{\\theta}^{*}}(x_{2})\\rVert\\leq\\varepsilon\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "we have, since $R_{\\varphi}$ is $(X,M,\\varepsilon_{0})$ -simple, that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\hat{x}_{1\\varphi}-\\hat{x}_{2\\varphi}\\right\\|=\\left\\|R_{\\varphi}(S_{\\theta^{*}}(x_{1}))-R_{\\varphi}(S_{\\theta^{*}}(x_{2}))\\right\\|\\le l_{R}\\cdot\\varepsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|x_{1}-x_{2}\\|\\leq\\|x_{1}-\\hat{x}_{1\\varphi}\\|+\\|\\hat{x}_{1\\varphi}-\\hat{x}_{2\\varphi}\\|+\\|x_{2}-\\hat{x}_{2\\varphi}\\|\\leq2\\sqrt{L}+l_{R}\\cdot\\varepsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We now show that every $\\varepsilon\\leq\\varepsilon_{0}$ holds $2\\sqrt{L}+l_{R}\\cdot\\varepsilon<2\\sqrt{\\bar{L}}$ . First, note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\varepsilon_{0}=\\frac{\\frac{\\sqrt{2}-1}{2}\\sqrt{V a r[X]}}{l_{R}}=\\frac{(1-\\frac{1}{\\sqrt{2}})\\sqrt{\\frac{1}{2}V a r[X]}}{l_{R}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In addition, since $L\\leq\\frac{\\bar{L}}{2}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sqrt{\\bar{L}}-\\sqrt{L}\\geq\\sqrt{\\bar{L}}-\\sqrt{\\frac{\\bar{L}}{2}}=(1-{\\frac{1}{\\sqrt{2}}})\\sqrt{\\bar{L}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "so for any $\\varepsilon\\leq\\varepsilon_{0}$ , we get: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\varepsilon\\leq\\varepsilon_{0}=\\frac{(1-\\frac{1}{\\sqrt{2}})\\sqrt{\\bar{L}}}{l_{R}}<\\frac{2(1-\\frac{1}{\\sqrt{2}})\\sqrt{\\bar{L}}}{l_{R}}\\leq\\frac{2(\\sqrt{\\bar{L}}-\\sqrt{L})}{l_{R}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Which finally leads to ", "page_idx": 20}, {"type": "equation", "text": "$$\n2\\sqrt{L}+l_{R}\\cdot\\varepsilon<2\\sqrt{\\bar{L}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, for any $x_{1},x_{2}\\in\\mathcal{X}$ that hold $\\|S_{\\theta^{*}}(x_{1})-S_{\\theta^{*}}(x_{2})\\|\\le\\varepsilon\\le\\varepsilon_{0}$ , we get: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|x_{1}-x_{2}\\|^{2}\\leq(2\\sqrt{L}+l_{R}\\cdot\\varepsilon)^{2}}\\\\ &{\\qquad\\qquad<(2\\sqrt{\\bar{L}})^{2}=4\\bar{L}}\\\\ &{\\qquad\\qquad=2\\cdot\\underset{x\\sim X}{\\mathbb{E}}\\|x-\\bar{X}\\|^{2}}\\\\ &{\\qquad=\\underset{x\\sim X}{\\mathbb{E}}\\|x-\\bar{X}\\|^{2}+\\underset{x\\sim X}{\\mathbb{E}}\\|x-\\bar{X}\\|^{2}-2\\cdot\\underset{x\\sim X}{\\mathbb{E}}[x-\\bar{X}]^{T}\\underset{x\\sim X}{\\mathbb{E}}[x-\\bar{X}]}\\\\ &{\\qquad=\\underset{x_{1},x_{2}\\sim X}{\\mathbb{E}}\\|(x_{1}-\\bar{X})-(x_{2}-\\bar{X})\\|^{2}}\\\\ &{\\qquad=\\underset{x_{1},x_{2}\\sim X}{\\mathbb{E}}\\|x_{1}-x_{2}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Taking expectation yields the desired result for Definition 4: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\underset{x_{1},x_{2}\\sim X}{\\mathbb{E}}\\Big[\\|x_{1}-x_{2}\\|^{2}\\;\\Big|\\;\\|S_{\\theta}(x_{1})-S_{\\theta}(x_{2})\\|\\leq\\varepsilon\\Big]<\\underset{x_{1},x_{2}\\sim X}{\\mathbb{E}}\\Big[\\|x_{1}-x_{2}\\|^{2}\\Big]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Theorem 6.2. There exists a discrimination game $(\\mathcal{X},f_{X},M,\\Theta,\\varPhi,\\ell)$ , $\\varepsilon_{0}~\\geq~\\varepsilon_{M}$ and a receiver $\\varphi\\in\\varPhi$ which is $(X,M,\\varepsilon_{0})$ -simple and non-degenerate, where a synchronized sender matching $R_{\\varphi}$ is not $\\varepsilon$ -spatially meaningful for any $\\varepsilon$ . ", "page_idx": 21}, {"type": "text", "text": "Proof of Theorem 6.2. We propose the following counter example. Let $M=\\left\\{1,2,3,4,5,6\\right\\}$ and let $X$ be the uniform distribution over the finite support set $\\{1,\\dotsc.\\,,6\\}\\cup\\{-1,\\dotsc,\\dot{-6}\\}$ . Let $\\varepsilon_{0}=\\varepsilon_{M}=1$ . We let both hypothesis classes be unrestricted, set the number of candidates $d$ to 2 (a single distractor), and denote the following sets: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{A_{1}=\\{1,-1\\}}\\\\ {A_{2}=\\{2,-2\\}}\\\\ {A_{3}=\\{3,-3\\}}\\\\ {A_{4}=\\{4,-4\\}}\\\\ {A_{5}=\\{5,-5\\}}\\\\ {A_{6}=\\{6,-6\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We construct a receiver function that optimally tells apart members from different sets, but is unable to differentiate within each one. Formally: ", "page_idx": 21}, {"type": "text", "text": "\u2022 $R_{\\varphi}$ cannot tell apart inputs within each set: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\forall k\\in\\{1,\\ldots,6\\},\\,\\forall x_{1},x_{2}\\in A_{k},\\,\\forall m\\in M:\\quad R(m,x_{1},x_{2})=(0.5,0.5)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "\u2022 $R_{\\varphi}$ is optimal for inputs from different sets; Given the message $m\\ =\\ k$ where $k\\ \\in$ $\\{1,\\ldots,6\\}$ , $R_{\\varphi}$ outputs the true probabilities of each input being the target conditioned on the event $\\dot{X}\\in A_{k}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\nR_{\\varphi}(k,x_{1},x_{2})={\\left\\{\\begin{array}{l l}{(0.5,0.5)}&{x_{1}\\in A_{k}{\\mathrm{~and~}}x_{2}\\in A_{k}}\\\\ {(1,0)}&{x_{1}\\in A_{k}{\\mathrm{~and~}}x_{2}\\not\\in A_{k}}\\\\ {(0,1)}&{x_{1}\\not\\in A_{k}{\\mathrm{~and~}}x_{2}\\in A_{k}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The largest possible distance between $R_{\\varphi}$ \u2019s outputs is $\\textstyle{\\frac{1}{\\sqrt{2}}}$ , and the minimal distance between messages is $\\varepsilon_{M}=1$ . Thus, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|R_{\\varphi}(x_{1})-R_{\\varphi}(x_{2})\\|\\leq{\\frac{1}{\\sqrt{2}}}\\cdot1\\leq{\\frac{1}{\\sqrt{2}}}\\cdot\\|x_{1}-x_{2}\\|.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\frac{{\\sqrt{2}}-1}{2\\varepsilon_{0}}}\\cdot{\\sqrt{V a r[X]}}={\\frac{{\\sqrt{2}}-1}{2}}\\cdot{\\sqrt{\\frac{91}{6}}}>{\\frac{1}{\\sqrt{2}}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Which means $R_{\\varphi}$ is $(X,M,\\varepsilon_{0})$ -simple. We now show that the sender that maps each input to the index of its set, i.e., ", "page_idx": 22}, {"type": "equation", "text": "$$\nS_{\\tilde{\\theta}}(i)=\\left\\{\\begin{array}{l l}{1}&{\\mathrm{if~}x\\in A_{1}}\\\\ {2}&{\\mathrm{if~}x\\in A_{2}}\\\\ {3}&{\\mathrm{if~}x\\in A_{3}}\\\\ {4}&{\\mathrm{if~}x\\in A_{4}}\\\\ {5}&{\\mathrm{if~}x\\in A_{5}}\\\\ {6}&{\\mathrm{if~}x\\in A_{6}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "is synchronized for $R_{\\varphi}$ . Let $x\\in\\mathscr{X}$ belong to set $A_{k}$ . The loss function is bounded by: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell(\\theta,\\varphi,x)=\\underset{t\\in\\mathcal{X}_{\\neq\\times\\mathcal{X}}}{\\mathbb{E}}\\left[1_{\\varphi}\\left(\\mathrm{S}_{\\ell}(S_{\\ell}(i),x,z_{2})\\right)\\right.\\left.\\mathrm{\\#}t\\left.t=1\\right]}\\\\ &{\\quad=P(X\\in A_{\\star}):=\\left\\{\\underset{t\\in\\mathcal{X}}{\\mathbb{R}}\\left(1_{\\varphi}\\left(\\mathrm{S}_{\\ell}(i),x,z_{2}\\right)\\right)\\right.\\left.\\mathrm{\\#}t:=2\\right\\}}\\\\ &{\\qquad=P(X\\in A_{\\star}):=\\underset{t\\in\\mathcal{X}_{\\neq}\\times\\mathcal{X}_{\\neq}_{\\neq\\times\\mathcal{X}}}{\\mathbb{E}}\\left[-\\log\\left\\{R_{\\varphi}(S_{\\theta}(x),x,z_{2})\\right\\}\\right.\\mathrm{\\#}t:=1\\right]}\\\\ &{\\qquad\\qquad\\left.\\qquad\\frac{\\mathbb{E}}{\\mathrm{\\#}\\tau}{\\mathscr{X}}\\mathrm{X}_{\\neq\\times\\mathcal{X}_{\\neq}}\\right]}\\\\ &{\\qquad\\qquad+P(X\\notin A_{k}):\\underset{t\\in\\mathcal{X}_{\\neq}\\times\\mathcal{X}_{\\neq}^{\\tau}(1,2)}{\\mathbb{E}}\\left[-\\log\\left\\{R_{\\varphi}(S_{\\theta}(x),x,z_{2})\\right\\}\\right.\\mathrm{\\#}t:=1\\right]}\\\\ &{\\qquad\\qquad\\qquad\\left.\\qquad\\frac{\\mathbb{E}}{\\mathrm{\\#}\\tau}{\\mathscr{X}}\\mathrm{C}_{\\neq\\times\\mathcal{X}}^{\\tau}[1_{\\Psi}:\\mathscr{X}_{\\neq}(\\tau),x,z_{2}]\\right.\\mathrm{\\#}t=2\\right]}\\\\ &{\\qquad=P(X\\in A_{k}):\\left(-\\log\\frac{1}{2}\\right)+P(X\\notin A_{k}):\\underset{t\\in\\mathcal{X}_{\\neq}\\times\\mathcal{X}_{\\neq}^{\\tau}[1,2]}{\\mathbb{E}}\\left[-\\log\\left\\{R_{\\varphi}(S_{\\theta}(i),x,z_{2})\\right\\}\\mathrm{\\#}t\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.\\geq P(X\\in A_{k}):(\\log2)+P(X\\notin A_{k}):(-\\log1)\\right\\}}\\\\ &{\\qquad=\\varepsilon(\\tilde{\\theta},\\varphi,x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This means that $\\tilde{\\theta}$ is synchronized for $R_{\\varphi}$ . In fact, $S_{\\tilde{\\theta}}$ is optimal by Corollary 5.4. We can continue simplifying the expression to show that $R_{\\varphi}$ is strictly better than constant: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell(\\tilde{\\theta},\\varphi,x)=P(X\\in A_{k})\\cdot(\\log2)+P(X\\notin A_{k})\\cdot(-\\log1)}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac{1}{6}\\cdot\\log2}\\\\ &{\\qquad\\qquad<\\displaystyle\\frac{1}{4}\\cdot\\log2}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "And indeed, the average loss of the optimal constant is: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell(\\theta,\\varphi^{C},i)=\\underset{t=\\pi/\\lfloor1.2\\rfloor}{\\mathbb{E}}\\left[-\\log\\left\\{R_{\\varphi^{C}}(S_{\\theta}(i),x,x_{2})\\right\\}\\;\\;\\mathrm{if}\\;i=1\\right]}\\\\ &{\\qquad\\qquad\\quad\\times\\pi/\\lfloor1.2\\rfloor}\\\\ &{\\qquad\\qquad\\quad\\times\\frac{\\prod_{j=1}^{\\infty}\\big(X_{j}(i)-X_{j}\\big)}{\\pi/\\sqrt{\\pi}\\times X_{j}}\\;\\bigg[-\\log\\left\\{\\begin{array}{l l}{X}&{\\mathrm{if}\\;i=1}\\\\ {1-C}&{\\mathrm{if}\\;t=2}\\end{array}\\right\\}}\\\\ &{\\qquad\\qquad\\quad=\\frac{1}{2}\\cdot(-\\log C)+\\frac{1}{2}\\cdot(-\\log(1-C))}\\\\ &{\\qquad\\qquad\\quad=-\\frac{1}{2}\\cdot(\\log c+\\log(1-c))}\\\\ &{\\qquad\\qquad\\quad\\nabla\\ell(\\theta,\\varphi^{C},x)=-\\frac{1}{2}\\cdot\\bigg(\\frac{1}{C}-\\frac{1}{1-c}\\bigg)\\Rightarrow\\;C^{*}=\\frac{1}{2}}\\\\ &{\\qquad\\qquad\\quad\\mathbb{E}\\;\\ell(\\theta,\\varphi^{C},x)=-\\frac{1}{2}\\cdot2\\cdot\\log\\frac{1}{2}=\\log2}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Finally, we can show that $S_{\\tilde{\\theta}}$ does not induce a spatially meaningful latent space. We make the observation that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[X\\mid S_{\\tilde{\\theta}}(X)=m\\right]=0\\quad\\forall\\;m\\in I m g(S_{\\tilde{\\theta}})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This is a constant function in $m$ which means $\\operatorname{Var}(\\mathbb{E}\\left[X\\mid S_{\\tilde{\\theta}}(X)=m\\right])=0$ . By variance decomposition: ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\operatorname{Var}\\left[X\\right]=\\operatorname{\\mathbb{E}}\\!\\left(\\operatorname{Var}\\left[X\\mid S_{\\tilde{\\theta}}(X)=m\\right]\\right)+\\operatorname{Var}\\!\\left(\\operatorname{\\mathbb{E}}\\left[X\\mid S_{\\tilde{\\theta}}(X)=m\\right]\\right)}\\\\ &{\\operatorname{Var}\\left[X\\right]=\\operatorname{\\mathbb{E}}\\!\\left(\\operatorname{Var}\\left[X\\mid S_{\\tilde{\\theta}}(X)=m\\right]\\right)}\\\\ &{\\quad\\underbrace{\\mathbb{E}}_{x_{1},x_{2}\\sim X}\\!\\left[\\left\\Vert x_{1}-x_{2}\\right\\Vert^{2}\\right]=\\underbrace{\\mathbb{E}}_{m}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\subset\\!\\!\\!\\!\\!\\!\\mathbb{E}_{x_{1},x_{2}\\sim X_{[m]_{\\tilde{\\theta}}}}\\!\\!\\left[\\left\\Vert x_{1}-x_{2}\\right\\Vert^{2}\\right]}\\\\ &{\\quad\\underbrace{\\mathbb{E}}_{x_{1},x_{2}\\sim X}\\!\\!\\left[\\left\\Vert x_{1}-x_{2}\\right\\Vert^{2}\\right]=\\underbrace{\\mathbb{E}}_{x_{1},x_{2}\\sim X}\\!\\!\\left[\\left\\Vert x_{1}-x_{2}\\right\\Vert^{2}\\;\\right]S_{\\theta}(x_{1})=S_{\\theta}(x_{2})\\right]}\\end{array}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This equality contradicts Definition 4 that requires strict inequality (for any $\\varepsilon_{0}>0$ , the requirement will not hold with $\\varepsilon=m i n(\\varepsilon_{0},0.9).$ ), showing that $S_{\\tilde{\\theta}}$ is neither meaning consistent nor spatially meaningful. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "C.4 Proofs for variations of discrimination setups ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Lemma $A.I$ . Let $(\\mathcal{X},f_{X},M,\\Theta,\\varPhi,\\ell)$ be a global discrimination game. Assuming $\\varPhi$ is unrestricted, a sender $S_{\\theta}$ is optimal if and only if it minimizes the following objective: ", "page_idx": 23}, {"type": "equation", "text": "$$\n-I(X;S_{\\theta}(X))\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma A.1. Since negative log-likelihood is a proper scoring rule and $\\varPhi$ is unrestricted, the synchronized receiver for sender $S$ is ", "page_idx": 23}, {"type": "equation", "text": "$$\nR^{*}(m)=f_{X}(\\ \\cdot\\ |\\ S(X)=m)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Which means that $S^{*}$ is optimal if and only if: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S^{*}\\in\\underset{S\\in\\Theta}{\\mathrm{arg}\\,\\mathrm{min}}\\underset{x\\sim X}{\\mathbb{E}}\\ell(S,R^{*},x)}\\\\ &{\\quad=\\underset{S\\in\\Theta}{\\mathrm{arg}\\,\\mathrm{min}}\\,\\underset{x\\sim X}{\\mathbb{E}}-\\log f_{X}\\big(x\\mid S(X)=S(x)\\big)}\\\\ &{\\quad=\\underset{S\\in\\Theta}{\\mathrm{arg}\\,\\mathrm{min}}\\,H(X\\mid S(X))}\\\\ &{\\quad=\\underset{S\\in\\Theta}{\\mathrm{arg}\\,\\mathrm{min}}-I(X;S(X))}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma A.2. Let $(\\mathcal{X},f_{X},M,\\Theta,\\varPhi,\\ell)$ be a 2-candidates supervised discrimination game. Assuming $\\varPhi$ is unrestricted, a sender $S_{\\theta}$ is optimal if and only if it minimizes the following objective: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{m\\in M}P(S_{\\theta}(X)=m)^{2}-\\sum_{m\\in M}\\sum_{y\\in\\mathcal{Y}}P(S_{\\theta}(X)=m,\\operatorname{label}(X)=y)^{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma A.2. Since negative log-likelihood is a proper scoring rule (shown in the proof of Lemma 5.3) and $\\varPhi$ is unrestricted, the synchronized receiver for sender $S$ is ", "page_idx": 23}, {"type": "equation", "text": "$$\nR^{*}(m,x_{1},x_{2})=\\mathbb{P}(t\\mid x_{1},x_{2},m)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By the game construction, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(t=j,x_{1},x_{2},m)=f_{X}(x_{j})\\cdot f_{X}(x_{3-j}\\mid Y\\neq\\mathrm{label}(x_{j}))\\cdot\\frac{1}{2}\\cdot\\mathbf{1}\\{S(x_{j})=m\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{f_{X}\\left(x_{1}\\right)f_{X}\\left(x_{2}\\right)}{P\\left(Y\\neq\\mathrm{label}(x_{j})\\right)}\\cdot\\frac{1}{2}\\cdot\\mathbf{1}\\{S(x_{j})=m\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $Y$ is assumed to have uniform distribution, $\\begin{array}{r}{P(Y\\neq\\operatorname{label}(x_{j}))=\\frac{|\\mathcal{Y}|-1}{|\\mathcal{Y}|}}\\end{array}$ |Y||Y\u2212|1 , which means ", "page_idx": 23}, {"type": "equation", "text": "$$\nP(t=j,x_{1},x_{2},m)=f_{X}(x_{1})f_{X}(x_{2})\\cdot{\\frac{|y|}{|y|-1}}\\cdot{\\frac{1}{2}}\\cdot\\mathbf{1}\\{S(x_{j})=m\\}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, the synchronized receiver is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(R^{*}(m,x_{1},x_{2})=j)=P(t=j\\mid x_{1},x_{2},m)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{P(t=j,x_{1},x_{2},m)}{P(t=j,x_{1},x_{2},m)+P(t=3-j,x_{1},x_{2},m)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{\\mathbf{1}\\{S(x_{j})=m\\}}{\\mathbf{1}\\{S(x_{j})=m\\}+\\mathbf{1}\\{S(x_{3-j})=m\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Applying this synchronized receiver to the loss function, we get that $S^{*}$ is optimal if and only if: S ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{4}}&{\\mathrm{Gara~}\\operatorname*{lim}_{{\\mathbb{R}}}\\sum_{i,j}(X,\\ell(X,\\ell))}\\\\ &{=\\operatorname*{lim}_{{\\mathbb{R}}}\\frac{{\\mathbb{E}}_{X}}{C_{i}}\\langle\\exp_{i}X,\\ell(X)\\rangle}\\\\ &{=\\operatorname*{lim}_{{\\mathbb{R}}}\\frac{{\\mathbb{E}}_{Y}}{C_{i}}\\langle\\exp_{i}X,\\ell(X)\\rangle}\\\\ &{\\quad+2\\operatorname*{lim}_{{\\mathbb{R}}}\\frac{{\\mathbb{E}}_{Y}}{C_{i}}\\langle\\exp_{i}X,\\ell(X)\\rangle-\\,X\\Big\\{{\\mathrm{Fim}}\\Big({\\mathrm{Fim}}\\langle{\\mathrm{R}}\\rangle,Y,\\ell(X)\\Big)-2\\ell_{i}\\leq1\\Big\\}}\\\\ &{\\quad+2\\operatorname*{lim}_{{\\mathbb{R}}}X\\sum_{i,j}\\sum_{k\\in\\mathcal{N}_{i}\\setminus\\{k,1\\}}{\\mathrm{Fim}}\\langle\\exp_{i}X,\\ell(X,\\ell)\\rangle}\\\\ &{=\\alpha_{{\\mathbb{R}}}\\operatorname*{lim}_{{\\mathbb{R}}}X_{i,k\\in[1]}\\sum_{\\ell(X,\\ell)\\in[1]}-\\frac{{\\mathbb{V}}_{X}}{C_{i}}\\langle\\exp_{i}X,\\ell(X,\\ell)\\rangle}\\\\ &{\\quad+2\\alpha_{{\\mathbb{R}}}\\operatorname*{lim}_{{\\mathbb{R}}}X_{i,k\\in[1]}\\sqrt{\\mathrm{Fim}}\\langle\\exp_{i}X,\\ell(X,\\ell)\\rangle}\\\\ &{=\\alpha_{{\\mathbb{R}}}\\operatorname*{lim}_{{\\mathbb{R}}}\\frac{{\\mathbb{E}}_{Y}}{C_{i}}P\\langle X,\\ell(X)\\rangle\\Big|\\sum_{\\ell({\\mathrm{H}})}\\left\\langle\\exp_{i}X,\\ell(X,\\ell)\\right\\rangle\\langle\\exp_{i}\\left\\vert\\sum_{\\ell({\\mathrm{H}})}\\sum_{k\\in[1]}\\sum_{\\ell({\\mathrm{H}})}\\right\\rangle}\\\\ &{\\quad-\\alpha_{{\\mathbb{R}}}\\operatorname*{lim}_{{\\mathbb{R}}}X_{i,\\ell}\\langle X,\\ell(X)\\rangle\\Big|\\gamma+\\mathrm{fim}\\langle\\ell_{i}\\rangle\\Big|}\\\\ &{=\\alpha_{{\\mathbb{R}}}\\operatorname*{lim}_{{\\mathbb{R}}}\\frac{{\\mathbb{E}}_{Y}}{C_{i}}\\langle X,\\ell(X)\\rangle\\Big\\langle\\exp_{i}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma A.3. Let $(\\mathcal{X},f_{X},M,\\Theta,\\varPhi,\\ell)$ be a classification discrimination game. Assuming $\\varPhi$ is unrestricted, a sender $S_{\\theta}$ is optimal if and only if it minimizes the following objective: ", "page_idx": 24}, {"type": "equation", "text": "$$\n-I(Y;S_{\\theta}(X))\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof of Lemma A.3. Since negative log-likelihood is a proper scoring rule (shown in the proof of Lemma 5.3) and $\\varPhi$ is unrestricted, the synchronized receiver for sender $S$ is ", "page_idx": 24}, {"type": "equation", "text": "$$\nR^{*}(m,x_{1},x_{2})=\\mathbb{P}(t\\mid x_{1},\\ldots,x_{n},m)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By the game construction, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\nP(t=j,x_{1},\\ldots,x_{n},m)=P(X\\in[m],Y=j)\\cdot\\prod_{i=1}^{n}f_{X}(x_{i}\\mid Y=i)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, the synchronized receiver is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{P(R^{*}(m,x_{1},\\ldots,x_{n})=j)=P(t=j\\mid x_{1},\\ldots,x_{n},m)}}\\\\ &{}&{=\\frac{P(X\\in[m],Y=j)}{P(X\\in[m])}}\\\\ &{}&{=P(Y=j\\mid X\\in[m])}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Applying this synchronized receiver to the loss function, we get that $S^{*}$ is optimal if and only if: ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{S^{*}\\in\\underset{S\\in\\Theta}{\\operatorname{arg\\,min}}\\ \\underset{x\\sim X}{\\mathbb{E}}\\ell(S,R^{*},x)}\\\\ &{\\qquad=\\underset{S\\in\\Theta}{\\operatorname{arg\\,min}}\\ \\underset{x\\sim X}{\\mathbb{E}}\\,\\underset{\\{x_{i}\\sim P(X\\mid Y=i)\\}_{i=1}^{n}}{\\mathbb{E}}-\\log P(R^{*}(S(x),x_{1},\\ldots,x_{n})=\\mathrm{label}(x))}\\\\ &{\\qquad=\\underset{S\\in\\Theta}{\\operatorname{arg\\,min}}\\ \\underset{x\\sim X}{\\mathbb{E}}-\\log P(Y=\\mathrm{label}(x)\\mid X\\in[S(x)])}\\\\ &{\\qquad=\\underset{S\\in\\Theta}{\\operatorname{arg\\,min}}\\,H(Y|S(X))}\\\\ &{\\qquad\\overset{S\\in\\Theta}{\\operatorname{ste}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "D Details on empirical work ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "D.1 Training details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Data. We use a version of the Shapes dataset [22], where each input is an image of dimensions (3, 64, 64) containing a single object. The object has a random shape out of (square, rectangle, triangle, pentagon, cross, circle, semicircle, ellipse), and a random color out of (red, green, blue, yellow, magenta, cyan, gray). In addition, the object has a random position, rotation, size and distortion. See the Shapes github repo and our code for exact details. We generate $\\mathord{\\sim}10\\mathrm{K}$ data samples, split into 9088 (142 batches of 64) training samples and 896 (14 batches of 64) validation samples. The MNIST dataset [30] contains $(1,28,28)$ images of hand-written digits, split into $\\sim54K$ training samples and 5440 validation samples. ", "page_idx": 25}, {"type": "text", "text": "Agent architectures and loss functions. We use the format {Shapes value}/{MNIST value} to indicate values that differ between the datasets. The sender consists of two parts: an image encoder and a message generator. The image encoder receives an input image of dimensions $(3,64,64)/(1,28,28)$ and outputs a latent vector of dimension 100. The exact encoder architecture is given in Figure 5a / Figure 6a. A trainable linear layer then maps this output to a vector of dimension 150 which serves as input to the message generator, which is implemented by a single-layer GRU with hidden dimension 150. Its input is used as an initial hidden state, together with a learnable BOS vector. After the GRU cell, the resulting hidden state is mapped by a linear layer to the vocabulary size (10). We use the Gumbel-Softmax relaxation to approximate a symbol sampling, then an embedding matrix is used on the relaxed on-hot vector and the result is fed to the next generation step (along with the current hidden state). ", "page_idx": 25}, {"type": "text", "text": "The receiver has different architectures for reconstruction and discrimination. Both receivers start with a message embedder, implemented by a single-layer GRU with hidden size 150. In the reconstruction setup, the final hidden state from that network is given to an image decoder implemented by transposeconvolutions, see Figure 5b / Figure 6b for full details. The loss in the reconstruction setting is simply the Mean Squared Error (MSE) between the receiver\u2019s output and the target image. In the discrimination setting, the receiver uses another image encoder (with the architecture from Figure 5a / Figure 6a) and applies it to every image in the set of candidates. The resulting vector is further processed by a Multilayer perceptron (MLP) with two layers, hidden dimension 150 and relu activation. The output of this MLP is a representation for each candidate. We take a dot product of this representation with the final hidden state of the message embedder (both normalized) to get the score for each candidate. The discrimination loss is then the Cross-Entropy classification loss with the target position as label (also called the infoNCE objective). Visual illustrations of the two tasks are given in Figure 7/Figure 8. ", "page_idx": 25}, {"type": "text", "text": "Training procedure. While the architectures described above can be jointly trained from scratch, the discrete optimization with Gumbel-Softmax is much slower and less stable than continuous training. We account for that by employing a two-stage training procedure. In the first stage, we train a continuous autoencoder, which is a concatenation of an image encoder and an image decoder. The trained image encoder is used by the sender agent in both setups, and also by the receiver in the discrimination game. The trained decoder is used by the receiver in the reconstruction game. Both of these pretrained modules are kept frozen during the second stage of training, in which the discrete game is played out. This means that during EC training, the sender only optimizes the message generator (and a linear layer), and the receiver only optimizes the message embedder (and the MLP in the discrimination setting). ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Training parameters. We train (during the EC phase) for 200/100 epochs with batch size 64. We use vocabulary size 10 and message length 4. Every message is full length as we do not use a special end-of-sentence token. We use a fixed temperature value of 1.0 for the Gumbel-Softmax sampling method. ", "page_idx": 26}, {"type": "text", "text": "Computation resources. The main training stage (EC) takes $\\sim4$ hours on Shapes, $\\sim1.5$ hours on MNIST using NVIDIA GeForce RTX 2080 Ti. The continuous training takes $<10$ minutes. ", "page_idx": 26}, {"type": "image", "img_path": "me1MpmENpw/tmp/1bf9d7167a85a2743ae3bb53fd8bf4489492bdd063a6727268559e926eeea8a9.jpg", "img_caption": ["Figure 5: Encoder and Decoder architectures used on Shapes. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "me1MpmENpw/tmp/0251f56e4aec907000481ee4f2b8125b7c46a9e40b45f05d3a57c956b0224c90.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "D.2 Metrics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Discrimination accuracy. We sample 40 distractors independently, so the receiver predicts the target out of 41 given candidates. Every image in the validation set is used as target exactly once, and the reported accuracy is the precentage of images that were correctly predicted as targets. In the reconstruction setting, the prediction is defined by first passing the given message to the receiver, which outputs a reconstruction of the target, and then picking the candidate which minimizes the loss function (Euclidean distance) with respect to that output. ", "page_idx": 27}, {"type": "image", "img_path": "me1MpmENpw/tmp/036caea53ad4318988241220bf8a1eb28da56170d11914392616c54619899974.jpg", "img_caption": ["Figure 7: Reconstruction (top) and discrimination (bottom) examples on Shapes "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "me1MpmENpw/tmp/fdc5a8d10b7d99e6b50e2ffcbed07ec6b8dc021e2c02cdbd9d22f860b4cd6bcd.jpg", "img_caption": ["Figure 8: Reconstruction (top) and discrimination (bottom) examples on MNIST "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Message variance. The unexplained variance is $\\begin{array}{r}{\\mathbb{E}\\left[\\operatorname{Var}[X\\mid S(X)]\\right]=\\sum_{m\\in M}P(m)\\cdot\\operatorname{Var}\\left[[m]\\right]}\\end{array}$ where $[m]$ is the set of images mapped to message $m$ . Using the empirical measures ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathrm{Var}\\left[\\left[m\\right]\\right]=\\frac{1}{2\\cdot|[m]|^{2}}\\sum_{x_{1},x_{2}\\in[m]}\\left\\|x_{1}-x_{2}\\right\\|^{2}}\\\\ {P(m)=\\frac{|[m]|}{N},\\qquad N=896}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{m\\in M}P(m)\\cdot\\mathrm{\\mathbf{V}}\\mathrm{ar}\\left[\\left[m\\right]\\right]=\\sum_{m\\in M}\\frac{\\left\\vert\\left[m\\right]\\right\\vert}{2\\cdot\\left\\vert\\left[m\\right]\\right\\vert^{2}\\cdot N}\\sum_{x_{1},x_{2}\\in\\left[m\\right]}\\left\\Vert x_{1}-x_{2}\\right\\Vert^{2}}}\\\\ &{}&{=\\frac{1}{2N}\\sum_{m\\in M}\\frac{1}{\\left\\vert\\left[m\\right]\\right\\vert}\\sum_{x_{1},x_{2}\\in\\left[m\\right]}\\left\\Vert x_{1}-x_{2}\\right\\Vert^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We calculate message variance using this formula. A lower value indicates more semantic consistency (less unexplained variance). See Algorithm 1 for the full calculation procedure. Instead of calculating ", "page_idx": 28}, {"type": "text", "text": "Euclidean distance between raw pixels [4, 10], which might be uninformative, we use embeddings created by the continuous encoder (perceptual loss [39]). ", "page_idx": 29}, {"type": "table", "img_path": "me1MpmENpw/tmp/eb721632d0a7dd31115fa3cf095c3ff64de206402d6b6d301f2f17a414001133.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "Random baseline. The distribution of messages induced by the trained sender significantly affects some of the metrics. To facilitate an insightful comparison between the two EC setups, we establish a random baseline per-protocol which randomizes the assignment of images while preserving the number of inputs mapped to each message. Comparing the outcomes across different baselines allows us to examine the extent to which channel utilization impacts the results. ", "page_idx": 29}, {"type": "text", "text": "Topographic similarity. TopSim [5] is defined as the (Spearman) correlation between pairwise distances in the input space and corresponding distances in the message space. We use edit distance in the message space, and Euclidean distance between embeddings in the input space. The embeddings are generated by the frozen encoder trained in the continuous stage (see Appendix D.1). ", "page_idx": 29}, {"type": "text", "text": "Bag-of-symbols disentanglement. BosDis [8] calculates per-symbol the gap between the highest and second highest mutual information to an attribute, normalizes by the symbol\u2019s entropy and sums over the vocabulary. We consider only two attributes (shape and color), so the mutual information gap is simply the difference between the two. ", "page_idx": 29}, {"type": "text", "text": "Positional disentanglement. PosDis [8] operates similarly to BosDis, but considers positions within the message rather than symbols. ", "page_idx": 29}, {"type": "text", "text": "Speaker-centered topographic similarity. S-TopSim [13] operates similarly to PosDis, but rather than summing over positions, the mutual information gap is calculated per attribute. This is a more straightforward application of MIG [9]. ", "page_idx": 29}, {"type": "text", "text": "Message purity. The purity of a message with respect to an attribute is the percentage of images in its equivalence class that have the majority value. The minimal purity value is achieved by a uniform distribution over all of the attribute\u2019s possible values; the maximal value, $100\\%$ , is reached only if all images mapped to the message share the same value (e.g., if the attribute is shape and all images mapped to that message contain squares). We take a weighted average of message purity over messages. The final measure can be interpreted as the highest attainable accuracy by a deterministic classifier of messages to the attribute. An easier version of this measure, which we term max-purity, defines the purity of each message as the maximum purity over all attributes. ", "page_idx": 29}, {"type": "text", "text": "E Additional experiments ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "E.1 Full results: Compositionality, semantic consistency and game performance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We consider three additional compositionality measures from literature (BosDis, PosDis and SPosDis), see Appendix D.2 for their definitions. Some of these measures require multiple attributes (based on the mutual information gap), so we report results on Shapes. Table 3 shows these results, and Table 4 shows the correlation between each pair of measures, over three subsets of the collected measurements: the joint set (10 runs), reconstruction only (5 runs) and discrimination only (5 runs). We note that the correlations calculated over the joint set are heavily affected by the two distributions and often show opposite values to the individual tables. Furthermore, the results regarding S-PosDis are somewhat noisy due to its low values reported in Table 3. A key observation is that the correlation between compositionality and game performance (disc. accuracy) is negative or nonexistent across all compositionality measures in both the reconstruction subset and the discrimination subset, other than the S-PosDis value in the discrimination table (which we believe is noise, see previous note). This further corroborates our findings in Section 7. ", "page_idx": 30}, {"type": "table", "img_path": "me1MpmENpw/tmp/eaf3f4a266d74b0b27e6deca14573ee40e5d6ededc521efec52cc7d898339ab9.jpg", "table_caption": ["Table 3: Empirical results with compositionality measures from literature. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "E.2 Message purity over the Shapes dataset ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Figure 9 presents the color purity, shape purity and max purity in each game type. See Appendix D.2 for their definitions. Table 5 shows the baseline values as well. The reconstruction-trained agents generate messages that convey more information about each attribute. However, the generated distribution of messages (which defines the random baseline) seems to have significant effect on the purity results. We hypothesize that higher multiplicity of attributes may better differentiate the two tasks. ", "page_idx": 30}, {"type": "image", "img_path": "me1MpmENpw/tmp/afaa000e88dde53b02ef386de0a92c54668422f4b1c30608a8e18b03cb3b0564.jpg", "img_caption": ["Figure 9: Message purity per attribute and game type. "], "img_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "me1MpmENpw/tmp/e5bbfb7ada152303e4bc42d4337e0f90aaa9aed33d6fc7a5a1b07ca86f8f7a75.jpg", "table_caption": ["Table 4: Correlation Matrices for All Data, Reconstruction, and Discrimination Groups ", "(a) Correlation Matrix (All Data) "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "me1MpmENpw/tmp/29010c1c58f0191f1605e3f9e467fc332d0ec7cbef17c1ad48ca099ca7472d1a.jpg", "table_caption": ["(b) Correlation Matrix (Reconstruction) "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "me1MpmENpw/tmp/be50fb61b248b8cc305c64a93ce03cc5ad11eccff800a693c077fee999759359.jpg", "table_caption": ["(c) Correlation Matrix (Discrimination) "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "me1MpmENpw/tmp/d40d3c818427fc2f589ba1959f0d6cd2f39b0e2ef1189bf85bd26c0e1377d838.jpg", "table_caption": ["Table 5: Purity measures compared with random baselines. "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "E.3 Spatial meaningfulness empirical analysis ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Recall that spatial meaningfulness requires close messages to have similar meanings. We propose an evaluation method that alters the message space such that it is well clustered, and evaluates the semantic consistency of message clusters rather than individual messages. To do so, we partition the 10 vocabulary symbols to five group ${\\mathrm{s}}\\colon\\{0,1\\},\\,\\{2,3\\},\\,\\{4,5\\},\\,\\{6,7\\}$ , and $\\{8,9\\}$ . During training, agents are restricted to using messages composed of symbols from a single group. This is implemented by letting the sender generate the first symbol freely, and then masking out all symbols except those belonging to the same set as the initial symbol for the remainder of the message generation. Consequently, messages such as $(3,2,3,2)$ and $(7,7,7,7)$ are possible, whereas a message like $(3,4,5,9)$ can never be generated. There are 16 possible messages within each cluster, and only messages from the same cluster can have a Hamming distance smaller than 4 (the maximum). We then calculate spatial meaningfulness as the empirical unexplained variance (Algorithm 1) of these clusters; a lower value indicates more spatially meaningful messages. Table 6 and Table 7 present this measurement over the validation sets of Shapes and MNIST respectively. While the reconstruction setting leads to a better improvement over the random baseline on Shapes, as expected by our findings, the results do not demonstrate high levels of spatial meaningfulness in any of the trained protocols. We hypothesize that the size of each cluster (16 messages) is too large, and perhaps a different partitioning method will provide more insight. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "table", "img_path": "me1MpmENpw/tmp/228e8bff3a36aaee83f2717bfd8c45003e0243112238e2ee8742eb123d19b2cf.jpg", "table_caption": ["Table 6: Cluster Variance on Shapes. "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "me1MpmENpw/tmp/585871de252244f903551710880047cdf1d95e356b0816dc45c61403f1e5a568.jpg", "table_caption": ["Table 7: Cluster Variance on MNIST. "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "F Alternative formulations of the discrimination game ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Recall that we formally model the discrimination receiver as a function $R\\colon M\\times\\mathcal{X}^{d}\\rightarrow\\Delta\\{1,\\dots,d\\}$ (Definition 2). In this setting, assuming an unrestricted hypothesis class, we show that the conditionally optimal receiver outputs a uniform distribution over the subset of candidates that belongs to the given message\u2019s equivalence class. This functionality is demonstrated in Figure 10. ", "page_idx": 32}, {"type": "text", "text": "In this section, we tackle two alternative formulations which provide stronger resemblance to common implementations of the discrimination game, at the cost of added complexity. The first alleviates the receiver\u2019s ability to consider more than one candidate at a time; the second explicitly models the prediction using dot product in a latent space. With the assumption of an unrestricted receiver hypothesis class, our results are persistent. ", "page_idx": 32}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{S({\\mathrm{candidate}}1)=m}\\\\ &{S({\\mathrm{candidate}}2)\\neq m}\\\\ &{S({\\mathrm{candidate}}3)\\neq m\\Longleftrightarrow\\gamma\\ R^{*}(m,{\\mathrm{candidates}})={\\left\\{\\begin{array}{l l}{\\mathtt{c a n d i d a t e}1}&{{\\mathrm{~w.p.~}}\\quad0.5}\\\\ {{\\mathtt{c a n d i d a t e}}2}&{{\\mathrm{~w.p.~}}\\quad0}\\\\ {{\\mathtt{c a n d i d a t e s}}}&{{\\mathrm{~w.p.~}}\\quad0}\\\\ {{\\mathtt{c a n d i d a t e}}_{4}}&{{\\mathrm{~w.p.~}}\\quad0.5}\\end{array}\\right.}}\\\\ &{S({\\mathsf{c a n d i d a t e}}5)\\neq m}\\end{array}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "image", "img_path": "", "img_caption": ["Figure 10: The conditionally optimal discrimination receiver, assuming it is available in $\\varPhi$ . "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Candidate-unaware discrimination ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this formulation, rather than directly outputting a distribution over the candidates, the receiver predicts a score for each of the candidates independently, and those scores are normalized to create the final probabilities. Referred to as the descriptive-only variation by [12]. Formally, the receiver takes the form ", "page_idx": 32}, {"type": "equation", "text": "$$\nR\\colon M\\times\\mathcal{X}\\rightarrow[0,1]\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and the game objective is ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\ell(S,R,x):=\\underset{x_{1},\\ldots,x_{(t-1)},x_{(t+1)},\\ldots,x_{d}\\sim X^{d-1}}{\\mathbb{E}}-\\log\\frac{R(S(x_{t}),x_{t})}{\\sum_{i=1}^{d}R(S(x_{t}),x_{i})}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The key difference is that each score only depends on the corresponding candidate and the message. However, the conditionally optimal receiver from Figure 10 can still be achieved, for example by the following receiver: ", "page_idx": 33}, {"type": "equation", "text": "$$\nR^{*}(m,x)={\\left\\{\\begin{array}{l l}{1}&{{\\mathrm{if~}}S(x)=m}\\\\ {0}&{{\\mathrm{otherwise}}}\\end{array}\\right.}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "By assuming an unrestricted $\\varPhi$ , this receiver is available and therefore optimal. Hence, the set of optimal communication protocols is the same in this setting as in the original one. ", "page_idx": 33}, {"type": "text", "text": "Latent similarity discrimination ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "This formulation further restricts the receiver by generating the scores via cosine similarity in a latent space, and using the softmax operator to create the final probabilities. This formulation closely resembles common implementations of the discrimination game. Formally, let $z\\in\\mathbb{N}$ be the latent dimension, the receiver now consists of two parts: an input encoder $R_{X}\\colon X\\to\\mathbb{R}^{z}$ and a message encoder $R_{M}\\colon M\\to\\mathbb{R}^{z}$ . The game objective becomes ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\ell(S,R,x):=\\underset{\\substack{x_{1},\\ldots,x_{(t-1)},x_{(t+1)},\\ldots,x_{d}\\sim X^{d-1}}}{\\mathbb{E}}-\\log\\frac{\\exp(\\cos(R_{M}(S(x_{t})),R_{X}(x_{t})))}{\\sum_{i=1}^{d}\\exp(\\cos(R_{M}(S(x_{t})),R_{X}(x_{i})))}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Note that the softmax output is strictly positive in every entry, so the conditionally optimal receiver from Figure 10 cannot be achieved in this alternative formulation. However, we make the observation that the optimal input encoder outputs the same latent vector as the corresponding message encoder: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\log\\frac{\\exp(\\cos(R_{M}(S(x_{t})),R_{X}(x_{t})))}{\\sum_{i=1}^{d}\\exp(\\cos(R_{M}(S(x_{t})),R_{X}(x_{i})))}}\\\\ &{=-\\log\\left(1-\\frac{\\sum_{i\\neq t}\\exp(\\cos(R_{M}(S(x_{t})),R_{X}(x_{i})))}{\\exp(\\cos(R_{M}(S(x_{t})),R_{X}(x_{t})))+\\sum_{i\\neq t}\\exp(\\cos(R_{M}(S(x_{t})),R_{X}(x_{i})))}\\right)}\\\\ &{\\geq-\\log\\left(1-\\frac{\\sum_{i\\neq t}\\exp(\\cos(R_{M}(S(x_{t})),R_{X}(x_{i})))}{e+\\sum_{i\\neq t}\\exp(\\cos(R_{M}(S(x_{t})),R_{X}(x_{i})))}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and this lower bound can be achieved by selecting ", "page_idx": 33}, {"type": "equation", "text": "$$\nR_{X}^{*}(x)=R_{M}(S(x)).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "In other words, the conditionally optimal receiver needs to map equivalent inputs to the same latent vector (up to a multiplicative constant). In the case of unrestricted receiver hypothesis class, this still allows the relationship between equivalent inputs to be arbitrary. However, the representations of different messages are incentivized to be well separated, meaning that if similar inputs are mapped to different messages, their encodings by $R_{X}^{*}$ will be far apart. Restrictions on $R_{X}^{*}$ may render this infeasible, opening the way for semantic consistency in the discrimination setting. We leave further investigation of this idea to future work. ", "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The main results are outlined in the abstract and introduction. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: Discussion of this work\u2019s limitation is given in Section 8. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Assumptions are properly defined and stated for each theorem. Complete proofs are given in Appendix C. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We give a rigorous explanation of the training setup and procedure in Appendix D.1, and release code with reproducibility instructions. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Exact commands for reproducibility are given with the code. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Training details, including data splits and hyperparameters, are given in Appendix D.1. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We report the mean and standard deviation in our results, see Section 7. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Compute information is given in Appendix D.1. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The research presented in the paper adheres to the NeurIPS Code of Ethics in all aspects. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This theoretical work may improve interpretability of learned representations in the long run, but has no direct societal impacts that we foresee. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 37}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: This is a theoretical work and does not include released data or models, so there is no risk of this paper\u2019s contributions being misused. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We cite the most relevant code package (EGG) in Section 7. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Training details are given in Appendix D.1. The code contains documentation and usage instructions for reproducibility. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: No experiments in this paper involve human participants. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: No experiments in this paper involve human participants. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}]