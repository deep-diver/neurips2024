[{"figure_path": "nqWaya7hiX/tables/tables_7_1.jpg", "caption": "Table 1: Performance comparisons of WINGS and the baseline MLLMs under the same training data. We consider 8 baseline MLLMs, including LLMs as Vicunav1.5 & Qwen1.5, visual encoders as CLIP [98] & SigLIP [137], and training strategies as full-parameter & LoRA fine-tuning. The first entry represents the initial LLM, upon which each MLLM is trained. Our evaluation spans 6 domains with 20 datasets. WINGS is based on the Qwen1.5 and SigLIP, and the column \u201cOur Improvement", "description": "This table presents a comparison of WINGS' performance against eight baseline multimodal large language models (MLLMs).  The baselines vary in their underlying LLM (Vicuna or Qwen), visual encoder (CLIP or SigLIP), and fine-tuning method (full parameter or LoRA). The table evaluates performance across 20 datasets spanning six domains (Exam, Understanding, Reasoning, Math, Code, and Multimodal).  The \"Our Improvement\" column shows how much WINGS outperforms its corresponding baseline MLLM using the same underlying LLM and visual encoder. ", "section": "4 Experiments"}, {"figure_path": "nqWaya7hiX/tables/tables_8_1.jpg", "caption": "Table 2: Performance comparisons of the equal-scale MLLMs and the efficient multimodal LLMs on text-only and multimodal datasets. We evaluate the open-source, efficient, and private API MLLMs. We select 18 representative evaluation datasets. C* represents the CMMLU dataset.", "description": "This table compares the performance of various multimodal large language models (MLLMs) across 18 different datasets, categorized into text-only and multimodal question-answering tasks.  It includes a comparison of equal-scale open-source models, efficient multimodal LLMs, and advanced private models, offering a comprehensive evaluation of model capabilities.", "section": "4 Experiments"}]