{"importance": "This paper is crucial for researchers working on multimodal large language models (MLLMs). It addresses the significant issue of **text-only forgetting**, where MLLMs lose their ability to handle text-only instructions after visual data integration. The proposed solution, **WINGS**, offers a novel architectural approach that can be widely applied to improve MLLMs' robustness and versatility, impacting research on efficient training and multimodal learning. The **Interleaved Image-Text (IIT)** benchmark introduced also contributes to the field by offering a more realistic evaluation setting.", "summary": "WINGS: A novel multimodal LLM combats 'text-only forgetting' by using complementary visual and textual learners, achieving superior performance on text-only and visual tasks.", "takeaways": ["Multimodal LLMs often suffer from 'text-only forgetting' after training on visual data.", "WINGS, using parallel visual and textual learners, effectively mitigates text-only forgetting.", "WINGS outperforms existing MLLMs on text-only and multimodal benchmarks, and a new IIT benchmark is introduced for more realistic evaluation."], "tldr": "Multimodal Large Language Models (MLLMs) are increasingly used but suffer from a critical problem: 'text-only forgetting'.  Existing MLLMs, trained initially on text-only instructions, lose this capability after incorporating visual information. This significantly limits their application in real-world scenarios requiring mixed-modality interactions, like conversational AI that can handle image and text.  This is a significant challenge, demanding more research efforts to maintain the performance of LLMs in various modalities.\nThe paper introduces WINGS, a novel MLLM architecture to solve the 'text-only forgetting' issue. WINGS uses an additional low-rank residual attention (LoRRA) block that acts as a 'modality learner', effectively expanding the learning space and compensating for attention shifts that cause forgetting.  Experimental results demonstrate that WINGS outperforms other MLLMs in text-only and visual question answering tasks. A new benchmark, Interleaved Image-Text (IIT), was also created for better evaluation of MLLMs in multi-turn, mixed-modality scenarios.", "affiliation": "Alibaba International Digital Commerce", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "nqWaya7hiX/podcast.wav"}