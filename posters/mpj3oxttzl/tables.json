[{"figure_path": "MPJ3oXtTZl/tables/tables_2_1.jpg", "caption": "Table 1: Observation and mitigation of hallucination in graph LLMs.", "description": "This table demonstrates the issue of hallucination in graph LLMs (Large Language Models) using a baseline method adapted from MiniGPT-4.  The baseline uses a frozen LLM with a trainable GNN (Graph Neural Network). The table shows an example question about a graph, and compares the incorrect response with hallucinated nodes and edges given by the baseline LLM, with the correct response generated by the proposed G-Retriever method, along with references to support the answer. The comparison highlights how G-Retriever effectively mitigates hallucination by accurately recalling and utilizing information directly from the graph, unlike the baseline LLM.", "section": "Tackling Hallucination in Graph LLMs"}, {"figure_path": "MPJ3oXtTZl/tables/tables_7_1.jpg", "caption": "Table 3: Performance comparison across ExplaGraphs, SceneGraphs, and WebQSP datasets for different configurations, including Inference-only, Frozen LLM with prompt tuning (PT), and Tuned LLM settings. Mean scores and standard deviations (mean \u00b1 std) are presented. The first best result for each task is highlighted in bold and the second best result is highlighted with an underline.", "description": "This table presents a comparison of the performance of different model configurations on three datasets: ExplaGraphs, SceneGraphs, and WebQSP.  The configurations tested include using a frozen LLM for inference only, a frozen LLM with prompt tuning (PT), and a tuned LLM.  The performance is measured by mean scores and standard deviations, with the best and second-best results highlighted for each dataset and task.", "section": "6.2 Main Results"}, {"figure_path": "MPJ3oXtTZl/tables/tables_8_1.jpg", "caption": "Table 4: Retrieval on graphs significantly improves efficiency.", "description": "This table shows that using retrieval on graphs significantly improves the efficiency of the model.  The number of tokens required to represent graphs, the number of nodes in the graph, and the training time (minutes per epoch) are all substantially reduced after implementing retrieval. For the SceneGraphs dataset, tokens decreased by 83%, nodes by 74%, and training time by 29%. For the WebQSP dataset, tokens decreased by 99%, nodes by 99%, and training time by 67%. This highlights the method's efficiency for handling large-scale graph data.", "section": "6.3 Efficiency Evaluation"}, {"figure_path": "MPJ3oXtTZl/tables/tables_8_2.jpg", "caption": "Table 1: Observation and mitigation of hallucination in graph LLMs.", "description": "This table shows a comparison of hallucination mitigation between a baseline method (LLM w/ Graph Prompt Tuning) and the proposed G-Retriever method.  The baseline shows significant hallucination in answering questions about a graph, as evidenced by a low percentage of correctly identified nodes and edges. In contrast, G-Retriever significantly reduces hallucination by retrieving information directly from the graph.", "section": "Tackling Hallucination in Graph LLMs"}, {"figure_path": "MPJ3oXtTZl/tables/tables_8_3.jpg", "caption": "Table 6: Ablation study on the WebQSP dataset showing performance drops (Hit@1) when each component is removed.", "description": "This table presents the results of an ablation study conducted on the WebQSP dataset to evaluate the impact of each component of the G-Retriever model.  It shows the Hit@1 (a metric for evaluating the accuracy of top-ranked answers) achieved by removing each of four key components: Graph Encoder, Projection Layer, Textualized Graph, and Retrieval.  The results quantify the decrease in performance when each component is removed, demonstrating their relative importance to the overall effectiveness of the G-Retriever model.", "section": "6.5 Ablation Study"}, {"figure_path": "MPJ3oXtTZl/tables/tables_16_1.jpg", "caption": "Table 7: Performance of different graph encoders on the WebQSP and ExplaGraphs datasets.", "description": "This table compares the performance of three different graph encoders (GCN, GAT, and Graph Transformer) on two different datasets (WebQSP and ExplaGraphs).  The results show that the Graph Transformer generally performs best, but the performance differences are small on the WebQSP dataset.", "section": "B.4 The Choice of Graph Encoder"}, {"figure_path": "MPJ3oXtTZl/tables/tables_16_2.jpg", "caption": "Table 8: Performance of different LLMs on the WebQSP dataset.", "description": "This table presents a comparison of the performance achieved by two different Large Language Models (LLMs), Llama2-7b and Llama2-13b, on the WebQSP dataset.  The performance metric used is Hit@1, representing the accuracy of retrieving the correct answer in the top result.", "section": "B.5 The Choice of LLM"}, {"figure_path": "MPJ3oXtTZl/tables/tables_17_1.jpg", "caption": "Table 3: Performance comparison across ExplaGraphs, SceneGraphs, and WebQSP datasets for different configurations, including Inference-only, Frozen LLM with prompt tuning (PT), and Tuned LLM settings. Mean scores and standard deviations (mean \u00b1 std) are presented. The first best result for each task is highlighted in bold and the second best result is highlighted with an underline.", "description": "This table presents the performance comparison of different model configurations on three datasets: ExplaGraphs, SceneGraphs, and WebQSP.  The configurations tested are Inference-only (using a frozen LLM), Frozen LLM with Prompt Tuning (PT), and Tuned LLM. The table shows the mean and standard deviation of the scores for each dataset and configuration. The best performing configuration for each task is highlighted.", "section": "6.2 Main Results"}, {"figure_path": "MPJ3oXtTZl/tables/tables_18_1.jpg", "caption": "Table 10: Comparison of retrieval methods on the WebQSP dataset.", "description": "This table compares the performance of four different graph retrieval methods on the WebQSP dataset, using Hit@1 as the evaluation metric. The methods compared are PCST retrieval (the proposed method), top-k triples retrieval (KAPING), top-k nodes plus its neighbors, and shortest path retrieval. The results show that the PCST retrieval method outperforms all the baseline methods.", "section": "D Graph Retrieval-Augmented Generation (GraphRAG)"}, {"figure_path": "MPJ3oXtTZl/tables/tables_19_1.jpg", "caption": "Table 11: The impact of k on the webqsp dataset.", "description": "This table presents the results of an experiment evaluating the effect of different values of the hyperparameter k on the performance of the G-Retriever model on the WebQSP dataset.  The hyperparameter k controls the number of top-k nodes/edges retrieved during the retrieval phase of the method.  The table shows that performance (measured by Hit@1, a metric indicating the accuracy of retrieving the correct answer in the top-ranked result), initially increases with k, peaks at k=10 and then starts decreasing for higher values of k. This suggests an optimal value for k exists that balances recall (finding relevant information) and precision (avoiding irrelevant information).", "section": "D.3 The Impact of K for Retrieval"}, {"figure_path": "MPJ3oXtTZl/tables/tables_20_1.jpg", "caption": "Table 3: Performance comparison across ExplaGraphs, SceneGraphs, and WebQSP datasets for different configurations, including Inference-only, Frozen LLM with prompt tuning (PT), and Tuned LLM settings. Mean scores and standard deviations (mean \u00b1 std) are presented. The first best result for each task is highlighted in bold and the second best result is highlighted with an underline.", "description": "This table compares the performance of different model configurations on three graph question-answering datasets: ExplaGraphs, SceneGraphs, and WebQSP. The configurations include an Inference-only model, a Frozen LLM with prompt tuning, and a Tuned LLM.  The table shows mean scores and standard deviations for each dataset and configuration, highlighting the best and second-best performing models for each task.", "section": "6.2 Main Results"}, {"figure_path": "MPJ3oXtTZl/tables/tables_22_1.jpg", "caption": "Table 3: Performance comparison across ExplaGraphs, SceneGraphs, and WebQSP datasets for different configurations, including Inference-only, Frozen LLM with prompt tuning (PT), and Tuned LLM settings. Mean scores and standard deviations (mean \u00b1 std) are presented. The first best result for each task is highlighted in bold and the second best result is highlighted with an underline.", "description": "This table compares the performance of three different model configurations (Inference-only, Frozen LLM with prompt tuning, and Tuned LLM) across three different datasets (ExplaGraphs, SceneGraphs, and WebQSP) for graph question answering.  The results are presented as mean scores and standard deviations.  The best and second best performing methods for each dataset are highlighted.", "section": "6.2 Main Results"}, {"figure_path": "MPJ3oXtTZl/tables/tables_22_2.jpg", "caption": "Table 1: Observation and mitigation of hallucination in graph LLMs.", "description": "This table showcases the results of an experiment designed to observe and mitigate hallucination in graph LLMs (Large Language Models). The experiment compared a baseline method (LLM w/ Graph Prompt Tuning) with the proposed G-Retriever method. The comparison focuses on answering questions and identifying the nodes or edges that support the answers. The table presents a sample question and highlights the difference in response quality, demonstrating that G-Retriever successfully mitigates hallucination by providing accurate responses and references to nodes and edges, in contrast to the baseline method which frequently produces incorrect answers and hallucinated references.", "section": "Tackling Hallucination in Graph LLMs"}, {"figure_path": "MPJ3oXtTZl/tables/tables_23_1.jpg", "caption": "Table 1: Observation and mitigation of hallucination in graph LLMs.", "description": "This table showcases the presence of hallucination in graph Large Language Models (LLMs) and how the proposed G-Retriever method mitigates this issue. It compares the performance of a baseline method (LLM with Graph Prompt Tuning) with G-Retriever in answering questions about a graph.  The comparison highlights the accuracy of the responses and the correctness of the nodes and edges referenced in the answers. G-Retriever's retrieval mechanism helps reduce hallucination by ensuring accurate information retrieval from the graph.", "section": "Tackling Hallucination in Graph LLMs"}, {"figure_path": "MPJ3oXtTZl/tables/tables_24_1.jpg", "caption": "Table 1: Observation and mitigation of hallucination in graph LLMs.", "description": "This table showcases an example of a question-answering task applied to a graph. The first row demonstrates the incorrect response with hallucinated nodes and edges from a baseline method (LLM w/ Graph Prompt Tuning). The second row shows that the proposed method (G-Retriever) correctly answers the question with accurate nodes and edge references, mitigating the hallucination problem.", "section": "Tackling Hallucination in Graph LLMs"}, {"figure_path": "MPJ3oXtTZl/tables/tables_24_2.jpg", "caption": "Table 3: Performance comparison across ExplaGraphs, SceneGraphs, and WebQSP datasets for different configurations, including Inference-only, Frozen LLM with prompt tuning (PT), and Tuned LLM settings. Mean scores and standard deviations (mean \u00b1 std) are presented. The first best result for each task is highlighted in bold and the second best result is highlighted with an underline.", "description": "This table presents a comparison of the performance of three different model configurations (Inference-only, Frozen LLM with prompt tuning, and Tuned LLM) across three different datasets (ExplaGraphs, SceneGraphs, and WebQSP).  The results show the mean scores and standard deviations for each model configuration and dataset.  The best-performing model for each dataset is highlighted.", "section": "6.2 Main Results"}]