[{"figure_path": "cA9gLXFaRo/figures/figures_1_1.jpg", "caption": "Figure 1: The most advanced LMMs (e.g., GPT4-V) still fail on complex instruction following tasks. With IVM assistance to simplify visual inputs, existing LMMs can gain significant improvement.", "description": "This figure demonstrates the limitations of Large Multimodal Models (LMMs) in handling complex instructions and how Instruction-Guided Visual Masking (IVM) can improve their performance.  It shows examples of questions posed to a vanilla GPT4-V model and an IVM-enhanced GPT4-V model, along with their respective answers and the corresponding masked images. The IVM-enhanced model outperforms the vanilla model by focusing only on the relevant parts of the image, as determined by the IVM mask, leading to more accurate and nuanced answers.", "section": "1 Introduction"}, {"figure_path": "cA9gLXFaRo/figures/figures_1_2.jpg", "caption": "Figure 2: Comparison between IVM and Reasoning Segmentation (RS) [31]. Traditional methods such as semantic segmentation [68] and referring expression comprehension [64] are limited to fixed categories or fixed instruction formats, thus inapplicable to complex instruction following tasks. RS has reasoning ability, but only allows single object localization. IVM, instead, is universally applicable to any instruction.", "description": "This figure compares Instruction-Guided Visual Masking (IVM) with other visual grounding methods like Reasoning Segmentation (RS), semantic segmentation, and referring expression comprehension.  It highlights that while traditional methods are limited to specific object categories or fixed instruction types, IVM is more versatile and can handle any instruction by masking irrelevant image regions. The examples shown demonstrate that IVM successfully localizes multiple objects and handles complex instructions where others fail.  It showcases IVM's ability to focus on instruction-relevant areas, even in scenarios requiring high-resolution or first-person perspective understanding.", "section": "1 Introduction"}, {"figure_path": "cA9gLXFaRo/figures/figures_3_1.jpg", "caption": "Figure 4: LLM-empowered Mixture-of-Expert pipeline for auto-annotation. (1) For labeled VG data, we utilize an LLM to generate complex instruction annotations. (2) For unlabeled VIF or robot data, we first use an LLM to simplify the instruction and then leverage a mixture of VG models to generate candidate labels.", "description": "This figure illustrates the LLM-empowered Mixture-of-Expert pipeline used to automatically generate annotations for the IVM dataset.  It shows two main processes. First, for labeled Visual Grounding (VG) data, a Large Language Model (LLM) generates complex instructions based on existing simple instructions.  Second, for unlabeled Visual Instruction Following (VIF) and robot data, the LLM first simplifies complex instructions before using a mixture of state-of-the-art visual grounding models to generate candidate labels. This two-stage approach is designed to create a large-scale, high-quality dataset for training the Instruction-Guided Visual Masking (IVM) model.", "section": "3.2 Data Preparation"}, {"figure_path": "cA9gLXFaRo/figures/figures_3_2.jpg", "caption": "Figure 3: Instruction-guided Visual Masking.", "description": "The figure illustrates the concept of Instruction-guided Visual Masking (IVM).  It shows an example image, a textual instruction (\"Describe the scene on the left bank\"), and a heatmap generated by the IVM model. The heatmap highlights the regions of the image that are most relevant to the given instruction, allowing a multimodal model to focus on these relevant areas and ignore irrelevant ones. This improves the accuracy and nuance of multimodal instruction following.", "section": "3 Instruction-Guided Visual Masking"}, {"figure_path": "cA9gLXFaRo/figures/figures_4_1.jpg", "caption": "Figure 5: Data analysis on the IVM-Mix-1M dataset: data quantity v.s percentage of instruction-related areas.", "description": "This figure shows a data analysis of the IVM-Mix-1M dataset.  It displays a stacked bar chart illustrating the relationship between the quantity of data (on the y-axis, represented as the cubic root of the data quantity for better visualization) and the percentage of annotated areas in images that are relevant to the given instructions (on the x-axis).  Each bar is segmented into colored sections, each representing a different source of data used to create the IVM-Mix-1M dataset: COCO, VG, GQA, Flickr30K, OpenImages, TextVQA, OpenX and Human. This breakdown shows the contribution of each data source to the overall dataset and highlights the relative proportions of data with varying degrees of instruction relevance. The figure's purpose is to demonstrate the relative scarcity of high-quality, instruction-focused data in existing datasets and the need for the more complex approach undertaken by the authors to generate data.", "section": "3.2 Data Preparation"}, {"figure_path": "cA9gLXFaRo/figures/figures_5_1.jpg", "caption": "Figure 6: IVM model architecture and training pipeline. Stage I: A LoRA-tuned LMMs is trained to discriminate human- and machine-annotated data. Stage II: A frozen SAM vision backbone and a LoRA-tuned LMMs are utilized to extract dense image features and multimodal representations, respectively. These features are then fed into a generator for dense prediction and is trained via DWSL. Same color represents the same model. See Appendix C.1 for more details.", "description": "This figure illustrates the architecture and training process of the Instruction-Guided Visual Masking (IVM) model.  The training is a two-stage process. Stage 1 trains a discriminator using a LoRA-tuned Large Multimodal Model (LMM) to distinguish between high-quality human-annotated data and machine-generated data.  Stage 2 uses a frozen Segment Anything Model (SAM) for image feature extraction, a LoRA-tuned LMM for multimodal representation, and a generator trained with Discriminator Weighted Supervised Learning (DWSL) to produce the final heatmap. The DWSL algorithm prioritizes learning from reliable samples by weighting the training based on the discriminator's assessment of data quality.", "section": "3.3 Discriminator-Weighted Supervised Learning Framework"}, {"figure_path": "cA9gLXFaRo/figures/figures_5_2.jpg", "caption": "Figure 6: IVM model architecture and training pipeline. Stage I: A LoRA-tuned LMMs is trained to discriminate human- and machine-annotated data. Stage II: A frozen SAM vision backbone and a LoRA-tuned LMMs are utilized to extract dense image features and multimodal representations, respectively. These features are then fed into a generator for dense prediction and is trained via DWSL. Same color represents the same model. See Appendix C.1 for more details.", "description": "This figure illustrates the architecture and training process of the Instruction-Guided Visual Masking (IVM) model. The training is divided into two stages. Stage 1 trains a discriminator using a LoRA-tuned large multimodal model (LMM) to distinguish between human- and machine-generated annotations.  Stage 2 freezes the vision backbone (SAM) and uses a LoRA-tuned LMM to extract features, which are then fed to a generator producing a heatmap (dense prediction) and trained using Discriminator Weighted Supervised Learning (DWSL). The discriminator's output weights the training loss, prioritizing high-quality annotations.  Both the generator and discriminator share the same LMM but use separate LoRA parameters to avoid interference.", "section": "3.4 Model Architecture"}, {"figure_path": "cA9gLXFaRo/figures/figures_6_1.jpg", "caption": "Figure 7: IVM inference pipeline. IVM generates heatmap given a pair of image and instruction. Then, instruction-irrelevant visual areas are masked out via post process methods. LMMs can correctly follow the instruction given the masked images.", "description": "This figure illustrates the process of using the Instruction-Guided Visual Masking (IVM) model for image processing before feeding it to a Large Multimodal Model (LMM).  First, an image and an instruction are inputted into the IVM model, which generates a heatmap highlighting the relevant image regions specified by the instruction. Then, a post-processing step masks out irrelevant regions based on the heatmap, effectively focusing the LMM's attention on the task-relevant parts of the image. This process of masking improves the accuracy of the LMM in following complex instructions.", "section": "4 Experiments"}, {"figure_path": "cA9gLXFaRo/figures/figures_6_2.jpg", "caption": "Figure 1: The most advanced LMMs (e.g., GPT4-V) still fail on complex instruction following tasks. With IVM assistance to simplify visual inputs, existing LMMs can gain significant improvement.", "description": "This figure demonstrates the limitations of Large Multimodal Models (LMMs) in accurately following complex instructions.  The top row shows an original image. The middle row highlights the image areas relevant to four different questions.  The bottom row shows the results of a vanilla GPT4-V model versus a GPT4-V model enhanced with Instruction-Guided Visual Masking (IVM).  The IVM-enhanced model significantly improves the accuracy of the answers by focusing on the relevant image regions and masking out irrelevant parts.", "section": "1 Introduction"}, {"figure_path": "cA9gLXFaRo/figures/figures_8_1.jpg", "caption": "Figure 9: Real robot results with or without IVM assistance. IVM greatly helps LCBC agent to overcome major distractions, enjoying better robustness and generalization. See Appendix C.4 for experiment setups.", "description": "This figure demonstrates the effectiveness of IVM in improving the robustness and generalization of a language-conditioned behavior cloning (LCBC) robot agent.  The top part shows a bar graph comparing the success rates of the LCBC agent performing pick-and-place tasks with and without IVM assistance under various levels of distraction. The bottom part visually depicts the agent's performance on specific tasks in scenarios with and without distractions. The results show that the agent with IVM assistance significantly outperforms the agent without IVM assistance when facing distractions, indicating that IVM effectively helps the agent focus on task-relevant information and ignore irrelevant details.", "section": "4.1 Main Results"}, {"figure_path": "cA9gLXFaRo/figures/figures_9_1.jpg", "caption": "Figure 10: Ablations on training data and the proposed DWSL framework.", "description": "This figure presents ablation studies on the training data and the Discriminator-Weighted Supervised Learning (DWSL) framework.  The left panel (a) shows the performance gains of IVM-enhanced GPT4-V model trained with different data and methods. It shows that the incorporation of both human and machine-annotated data, along with the DWSL framework, achieves the highest performance.  The right panel (b) shows a data analysis, depicting the data quantities from various data sources with respect to the discriminator's output values. This shows the distribution of data quality within the dataset used for IVM model training.", "section": "3.3 Discriminator-Weighted Supervised Learning Framework"}, {"figure_path": "cA9gLXFaRo/figures/figures_9_2.jpg", "caption": "Figure 11: Different mask deployment methods.", "description": "This figure demonstrates four different post-processing methods used to apply the IVM-generated heatmaps to images. These methods include overlaying the heatmap, blurring the irrelevant regions, converting the image to grayscale while preserving the relevant area highlighted by the heatmap, and finally overlaying the heatmap and then cropping the image to retain only the relevant area specified by the heatmap. The overlay+crop method is highlighted in the figure.", "section": "3.3 Discriminator-Weighted Supervised Learning Framework"}, {"figure_path": "cA9gLXFaRo/figures/figures_16_1.jpg", "caption": "Figure 6: IVM model architecture and training pipeline. Stage I: A LoRA-tuned LMMs is trained to discriminate human- and machine-annotated data. Stage II: A frozen SAM vision backbone and a LoRA-tuned LMMs are utilized to extract dense image features and multimodal representations, respectively. These features are then fed into a generator for dense prediction and is trained via DWSL. Same color represents the same model. See Appendix C.1 for more details.", "description": "This figure illustrates the architecture and training process of the Instruction-Guided Visual Masking (IVM) model. The training is divided into two stages. Stage I involves training a LoRA-tuned Large Multimodal Model (LMM) to discriminate between human-annotated and machine-generated labels. This discriminator helps to identify high-quality data samples. Stage II involves using a frozen Segment Anything Model (SAM) vision backbone and another LoRA-tuned LMM to extract dense image features and multimodal representations. These features are then fed into a generator, which produces a dense prediction of a heatmap used for visual masking. The entire training process is guided by Discriminator-Weighted Supervised Learning (DWSL), which prioritizes the use of high-quality data for model training.", "section": "3.4 Model Architecture"}, {"figure_path": "cA9gLXFaRo/figures/figures_17_1.jpg", "caption": "Figure 13: Visual input view for LCBC policy.", "description": "This figure shows two different viewpoints used as visual input for the Language-Conditioned Behavior Cloning (LCBC) policy in the real robot experiments. (a) shows a side camera view, providing a broader perspective of the scene, while (b) shows a wrist camera view, offering a closer, more focused view of the robot's interaction with the objects.", "section": "C.4 Real Robot Evaluations"}, {"figure_path": "cA9gLXFaRo/figures/figures_20_1.jpg", "caption": "Figure 14: Visualization results of IVM generated masks.", "description": "This figure shows several examples of how the Instruction-Guided Visual Masking (IVM) model generates masks for different images and questions.  The top row displays the original images.  The bottom row shows the IVM-generated masks overlaid on the original images, highlighting the regions relevant to answering the accompanying questions. The questions themselves focus on aspects requiring varying degrees of visual understanding and localization, from simple object identification to more complex scene analysis.", "section": "E.2 Visualization Result"}, {"figure_path": "cA9gLXFaRo/figures/figures_20_2.jpg", "caption": "Figure 15: Some failure cases.", "description": "This figure showcases instances where the Instruction-Guided Visual Masking (IVM) model encounters challenges.  The three examples highlight common failure modes: (a) Missing Target, where the model fails to identify small or obscured target objects; (b) Misguided Target, where the model focuses on the wrong object due to ambiguity or visual distractions; and (c) Insufficient Reasoning, where the model struggles to process complex instructions requiring higher-level understanding of the scene.", "section": "E.2 Visualization Result"}, {"figure_path": "cA9gLXFaRo/figures/figures_21_1.jpg", "caption": "Figure 1: The most advanced LMMs (e.g., GPT4-V) still fail on complex instruction following tasks. With IVM assistance to simplify visual inputs, existing LMMs can gain significant improvement.", "description": "This figure compares the performance of vanilla GPT4-V and IVM-enhanced GPT4-V on a complex instruction-following task.  It shows that the vanilla GPT4-V model struggles to correctly identify and process relevant information from an image based on a given instruction, leading to inaccurate responses. In contrast, the IVM-enhanced GPT4-V, which uses visual masking to simplify the image by focusing on relevant areas, shows significant improvement in accuracy. This highlights the effectiveness of IVM in enhancing the performance of existing LMMs by enabling better alignment between textual instructions and visual content.", "section": "1 Introduction"}]