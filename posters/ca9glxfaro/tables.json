[{"figure_path": "cA9gLXFaRo/tables/tables_6_1.jpg", "caption": "Table 1: V* bench results.", "description": "This table presents the results of the V*bench benchmark, comparing the performance of various Large Multimodal Models (LMMs) on complex instruction-following tasks.  The models are categorized into Open-Sourced LMMS, Commercial Chatbots, and Specific Visual Search Models.  The results show the percentage of correct answers for Attribute, Spatial, and Overall aspects of the tasks, with IVM-enhanced GPT4-V achieving the highest overall performance.", "section": "4 Experiments"}, {"figure_path": "cA9gLXFaRo/tables/tables_7_1.jpg", "caption": "Table 2: Results on other multimodal benchmarks. MME* denotes the aggregate of scores from -p and -c.", "description": "This table presents the results of several large multimodal models (LMMs) on various benchmark datasets.  The benchmarks evaluate different aspects of multimodal understanding, such as visual question answering, visual reasoning, and other multimodal capabilities.  The table shows the number of parameters for each LMM, and the performance on each benchmark, including the improvement achieved by integrating the Instruction-guided Visual Masking (IVM) method. The MME* column represents the aggregated score from two sub-scores (-p and -c).  The numbers in parentheses represent the performance change relative to a baseline model (LLaVA-7B).", "section": "4.1 Main Results"}, {"figure_path": "cA9gLXFaRo/tables/tables_9_1.jpg", "caption": "Table 1: V* bench results.", "description": "This table presents the results of the V* benchmark, a challenging VQA-type benchmark characterized by images with abundant redundancies. It compares the performance of several language models, including open-sourced models and commercial chatbots, on the V* benchmark with and without the IVM model.  The results demonstrate IVM's ability to significantly improve the performance of existing language models on this challenging benchmark.", "section": "4 Experiments"}, {"figure_path": "cA9gLXFaRo/tables/tables_17_1.jpg", "caption": "Table 4: Hyper-parameters for pretraining.", "description": "This table shows the hyperparameters used for pretraining the IVM model.  It includes the number of training iterations, the optimizer used (AdamW), the learning rate, batch size, weight decay, optimizer momentum, and the data augmentation techniques applied.", "section": "C.2 Training Details"}, {"figure_path": "cA9gLXFaRo/tables/tables_17_2.jpg", "caption": "Table 1: V* bench results.", "description": "This table presents the results of the V* benchmark, a challenging VQA-type benchmark characterized by images with abundant redundancies.  It compares the performance of several language models (LLMs) and visual search models on various tasks within the V* benchmark. The table shows the performance of each model in terms of attribute accuracy, spatial accuracy, and overall accuracy, highlighting the significant performance improvement achieved by the IVM-enhanced GPT4-V model compared to other models and establishing a new state-of-the-art on this benchmark.", "section": "4 Experiments"}, {"figure_path": "cA9gLXFaRo/tables/tables_18_1.jpg", "caption": "Table 6: Real robot LCBC training details", "description": "This table presents the hyperparameters used during the training of the Language-Conditioned Behavior Cloning (LCBC) policies for the real robot experiments.  It specifies the backbone networks used (Resnet50 for image encoding and a frozen T5 for text encoding), the DDPM hyperparameters (noise schedule and denoising steps), and other training hyperparameters such as the optimizer, learning rate schedule, batch size, gradient steps, and augmentation methods employed.", "section": "C.4 Real Robot Evaluations"}, {"figure_path": "cA9gLXFaRo/tables/tables_19_1.jpg", "caption": "Table 7: result in REC", "description": "This table presents the results of the IVM model on three visual grounding benchmarks: RefCoCo, RefCoCo+, and RefCoCog.  It compares the performance of IVM (a generalist model) against G-DINO-L (a specialist model) and LLaVA-7B (another generalist model). The results show that IVM achieves comparable performance to the specialist model and outperforms the other generalist model, demonstrating its effectiveness in visual grounding tasks.", "section": "E More result"}]