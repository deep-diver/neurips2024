[{"figure_path": "FfFcDNDNol/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of RLbreaker.", "description": "This figure shows the overall architecture of RLbreaker, a deep reinforcement learning-based system for jailbreaking LLMs. It consists of three main components: a target LLM (the model being attacked), a DRL agent (which learns to select effective prompt modifications), and a helper LLM (used to generate prompt variations).  The DRL agent takes the current prompt as input, selects an action (a mutator from a set of predefined actions), and receives a reward based on the target LLM's response. The process iteratively refines the jailbreaking prompt until it successfully elicits a harmful response or reaches a time limit. The helper LLM facilitates diverse prompt variations by applying the selected mutators. An unaligned LLM is used to generate reference answers used in calculating rewards during training.", "section": "3.2 Overview"}, {"figure_path": "FfFcDNDNol/figures/figures_8_1.jpg", "caption": "Figure 1: Overview of RLbreaker.", "description": "This figure shows the overall architecture of RLbreaker, a deep reinforcement learning (DRL)-driven system for black-box jailbreaking attacks against large language models (LLMs). It illustrates the interaction between a DRL agent, a helper LLM, a target LLM, and the overall attack process.  The DRL agent selects mutators to modify the jailbreaking prompt, which is then input into the target LLM. The reward is calculated based on the target LLM's response. The process continues until the attack succeeds or reaches a maximum number of steps.", "section": "3.2 Overview"}, {"figure_path": "FfFcDNDNol/figures/figures_16_1.jpg", "caption": "Figure 3: Guided vs. stochastic search in a grid search problem. Here we assume the initial point is the block in the bottom left corner and the goal is to reach the red block on the top right corner following a certain strategy. The guided search moves towards the target following a fixed direction (for example given by the gradients), while the stochastic search jumps across different sub-regions.", "description": "This figure illustrates the difference between guided and stochastic search strategies using a simple grid search analogy.  The guided search method systematically moves toward the target, represented by the red block, using a directed approach. In contrast, the stochastic search explores the grid randomly, without a specific direction, jumping between different areas. This highlights how guided search, by focusing the search on promising areas, is significantly more efficient than random stochastic methods.", "section": "3.1 Problem Definition"}, {"figure_path": "FfFcDNDNol/figures/figures_17_1.jpg", "caption": "Figure 4: Illustration of prompt structure & Toxicity score of testing questions.", "description": "This figure shows two subfigures. (a) illustrates the structure of a jailbreaking prompt, which combines a prompt structure and a harmful question. The prompt structure creates a scenario that tricks the target LLM into answering the harmful question. (b) presents the toxicity scores of the questions in the testing dataset, showing the distribution of toxicity levels among the selected questions.", "section": "3. Methodology"}, {"figure_path": "FfFcDNDNol/figures/figures_21_1.jpg", "caption": "Figure 1: Overview of RLbreaker.", "description": "This figure presents a schematic overview of the RLbreaker system, illustrating its components and workflow. The DRL agent plays a central role, interacting with a helper LLM and the target LLM to iteratively refine jailbreaking prompts. It shows the input of a harmful question, the selection of a mutator, the update of the jailbreaking prompt, the response of the target LLM, and the calculation of the reward.  The diagram highlights the interaction between the DRL agent, the helper LLM, and the target LLM, showcasing the dynamic nature of the jailbreaking process.", "section": "3.2 Overview"}, {"figure_path": "FfFcDNDNol/figures/figures_22_1.jpg", "caption": "Figure 1: Overview of RLbreaker.", "description": "This figure shows the architecture of RLbreaker, a system that uses deep reinforcement learning to guide the search for effective jailbreaking prompts.  It depicts the interactions between the DRL agent, a helper LLM used for prompt mutation, the target LLM (the model being attacked), and the environment. The agent selects mutators (actions), and the helper LLM modifies the prompt accordingly. The target LLM's response determines the reward, which guides the agent's learning process. The diagram illustrates a sequence of states (s(0), s(1)), actions (a(0)), prompts (p(0), p(1)) and responses (u(0)).", "section": "3.2 Overview"}, {"figure_path": "FfFcDNDNol/figures/figures_23_1.jpg", "caption": "Figure 8: Mean rewards during agent training, when we use and without using value network to estimate advantage values.", "description": "This figure shows the mean reward curves during the training process of the DRL agent in RLbreaker, comparing two approaches: one using a value network to estimate the advantage function, and another without using a value network. The x-axis represents the number of training updates, while the y-axis shows the mean reward.  The graph visually demonstrates the performance difference between the two training methods, allowing for a comparison of their effectiveness in maximizing the agent's reward during the training process. This comparison is important for understanding the impact of the value network on the agent's performance and the overall effectiveness of the RLbreaker system.", "section": "4.4 Ablation Study and Sensitivity Test"}, {"figure_path": "FfFcDNDNol/figures/figures_24_1.jpg", "caption": "Figure 1: Overview of RLbreaker.", "description": "This figure shows a flowchart illustrating the architecture and workflow of the RLbreaker system.  It demonstrates how the DRL agent interacts with the target and helper LLMs to generate and refine jailbreaking prompts.  The agent receives the current prompt as its state, selects a mutator action, observes the LLM response, and receives a reward based on the LLM's response. The process iterates until a successful jailbreaking prompt is generated or a time limit is reached.", "section": "3.2 Overview"}]