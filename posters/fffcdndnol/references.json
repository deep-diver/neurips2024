{"references": [{"fullname_first_author": "Patrick Chao", "paper_title": "Jailbreaking black box large language models in twenty queries", "publication_date": "2023-10-26", "reason": "This paper is among the most important because it establishes a benchmark for the efficiency of jailbreaking attacks, achieving success within a limited number of queries."}, {"fullname_first_author": "Xiaogeng Liu", "paper_title": "AutoDAN: Generating stealthy jailbreak prompts on aligned large language models", "publication_date": "2023-10-26", "reason": "This paper is highly relevant due to its introduction of a novel genetic algorithm-based approach for automated jailbreaking, advancing the sophistication of these attacks."}, {"fullname_first_author": "Yuzhou Nie", "paper_title": "TrojFM: Resource-efficient backdoor attacks against very large foundation models", "publication_date": "2024-05-16", "reason": "This paper is important for exploring resource-efficient backdoor attacks against LLMs, a significant concern in the field."}, {"fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-07-06", "reason": "This work is crucial as it details the PPO algorithm, which is foundational to the deep reinforcement learning approach employed in this paper's jailbreaking method."}, {"fullname_first_author": "Volodymyr Mnih", "paper_title": "Human-level control through deep reinforcement learning", "publication_date": "2015-02-26", "reason": "This paper is highly influential in the field of deep reinforcement learning, providing a foundational work that underpins the DRL method used in the current paper's proposed approach."}]}