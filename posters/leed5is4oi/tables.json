[{"figure_path": "LEed5Is4oi/tables/tables_6_1.jpg", "caption": "Table 1: Experiment results of success rate on the Meta-world benchmark.", "description": "This table presents the success rates achieved by different methods on various tasks within the Meta-world benchmark.  The methods include TaskReward (using the ground truth reward), Behavior Cloning (BC), Generative Adversarial Imitation Learning (GAIfO), Optimal Transport reward with different discount factors (OT0.99 and OT0.9), Adaptive Discount Scheduling (ADS), and the proposed TemporalOT method.  The results highlight the performance improvements of the TemporalOT method compared to existing approaches, particularly in achieving higher success rates without relying on ground truth rewards.  Success rates are expressed as mean (standard deviation).", "section": "4.3 Results on the Meta-world Benchmark Tasks"}, {"figure_path": "LEed5Is4oi/tables/tables_15_1.jpg", "caption": "Table 2: Pretraining with action-inclusive demonstrations is helpful.", "description": "This table presents the ablation study results on the effectiveness of using imitation learning (behavior cloning, BC) to pre-train the robot policy before fine-tuning with TemporalOT.  It compares the success rate (%) of three methods: BC (pure offline behavior cloning), TemporalOT (online Temporal Optimal Transport RL), and TemporalOT-P (online Temporal Optimal Transport RL with pre-training).  The success rate is measured at different training steps (0, 2e4, 4e4, 6e4, 8e4, 1e5, 5e5, 1e6), showing the improvement in sample efficiency provided by the pretraining step.", "section": "A.3 Pretraining with Expert Data"}, {"figure_path": "LEed5Is4oi/tables/tables_15_2.jpg", "caption": "Table 3: Experiment results of success rate for demonstrations with different speed.", "description": "This table shows the success rates for different tasks (Basketball, Button-press, Door-open) when training with expert demonstrations at different speeds (1x, 2x, 3x, 4x).  The numbers in parentheses represent the standard deviation.  The results demonstrate how the performance is affected when the expert demonstrations are sped up or slowed down, indicating the influence of temporal consistency on model performance.", "section": "4.3 Results on the Meta-world Benchmark Tasks"}, {"figure_path": "LEed5Is4oi/tables/tables_18_1.jpg", "caption": "Table 1: Experiment results of success rate on the Meta-world benchmark.", "description": "This table presents the success rates achieved by different methods on various tasks within the Meta-world benchmark.  The methods compared include TaskReward (using the ground truth reward), BC (Behavior Cloning), GAIFO (Generative Adversarial Imitation from Observation), OT (Optimal Transport reward with discount factor 0.99), OT (Optimal Transport reward with discount factor 0.9), ADS (Adaptive Discount Scheduling), and TemporalOT (the proposed method).  Success rate is the percentage of successful trials out of 100 attempts. The results show TemporalOT outperforms other baselines in most tasks without using the ground truth reward.", "section": "4 Experiments"}]