[{"figure_path": "LEed5Is4oi/figures/figures_2_1.jpg", "caption": "Figure 1: An illustration of the pipeline of applying OT-based reward in RL. In this toy example, we rollout two agents for five steps of transitions. Both agents start from the initial state and take same actions a0 and a1 at the first two states. Then the two agents take different actions a2 and a3 to generate different trajectories \u03c4a = (o0, a0, o1, a1, o2, a2, o3, a3, o4, a4, o5) and \u03c4b = (o0, a0, o1, a1, o2, a2, o3, a3, o4, a4, o5). The OT rewards for (o0, a0) and (o1, a1) in \u03c4a and \u03c4b are different even though the state-action pairs are exactly the same.", "description": "This figure illustrates how the Optimal Transport (OT)-based reward is calculated in Reinforcement Learning (RL). Two agent trajectories are compared to an expert trajectory.  Even with identical initial state-action pairs, differences in subsequent actions lead to different OT rewards, highlighting the method's sensitivity to the entire trajectory and not just individual state-action pairs. This difference is crucial to the TemporalOT method's goal of incorporating temporal context.", "section": "Method"}, {"figure_path": "LEed5Is4oi/figures/figures_3_1.jpg", "caption": "Figure 2: Why OT reward could be useful? When the OT reward is generally correct, it helps to rank the goodness of different states and induce the policy to take better actions. (left) In the toy example, two agents takes different action a2 and a3 at o2 and thereafter. The goodness of a2 and a3 is measured by the OT reward computed w.r.t. to the observation of the next state o4 and o5. (right) A comparison of the true OT reward curves for trajectory \u03c4a and \u03c4b, where o0/o1/o2/o3/o4/o5 correspond to observations at the 0/20/40/60/80/100-th step. We can observe that the OT reward for trajectory b is generally larger, which shows that the OT reward is generally correct.", "description": "This figure demonstrates how the Optimal Transport (OT) reward helps in reinforcement learning.  The left panel shows a simplified example with two agents taking different actions, resulting in different OT rewards. The right panel displays a graph illustrating the OT reward over time for two different trajectories. The higher OT reward for one trajectory indicates its superiority, showcasing how OT reward effectively ranks state-action pairs, guiding the learning process to select better actions.", "section": "3.1 A Recap of OT Reward in RL"}, {"figure_path": "LEed5Is4oi/figures/figures_5_1.jpg", "caption": "Figure 3: An illustration of the proposed TemporalOT method. (left) Instead of using a pair-wise cosine similarity as the transport cost, we use a group-wise cosine similarity to learn a more accurate cost matrix. (right) We use a temporal mask to enforce the OT reward to focus on a narrow scope to avoid potential distractions from observations outside of the mask window.", "description": "This figure illustrates the TemporalOT method's pipeline. The left side shows how a group-wise cosine similarity, calculated by averaging the cosine similarity between observations within a context window, is used to create a more accurate cost matrix for optimal transport.  The right side demonstrates the use of a temporal mask to focus the OT reward calculation on a narrower time window, reducing the influence of temporally distant observations and improving the quality of the reward signal. This is a key improvement over traditional OT-based reward methods which ignore temporal context.", "section": "3.2 Temporal Optimal Transport Reward (TemporalOT)"}, {"figure_path": "LEed5Is4oi/figures/figures_7_1.jpg", "caption": "Figure 5: Influences of key parameters. A medium number of context length  \ud835\udc58\ud835\udc50  or mask length  \ud835\udc58\ud835\udc5a  performs the best. The agent performs better with more expert demonstrations.", "description": "This figure presents the ablation study on the influence of key parameters in the TemporalOT model. The study varied the context length (\ud835\udc58\ud835\udc50) and mask length (\ud835\udc58\ud835\udc5a) parameters to observe their impact on the model's performance.  The results show that a medium value for both  \ud835\udc58\ud835\udc50 and \ud835\udc58\ud835\udc5a  leads to the best performance. The number of expert demonstrations (\ud835\udc41\ud835\udc38) also had a positive effect, with more demonstrations leading to better results.", "section": "4 Experiments"}, {"figure_path": "LEed5Is4oi/figures/figures_7_2.jpg", "caption": "Figure 4: Ablation for model components. Both proposed components are useful.", "description": "This figure displays ablation studies on the proposed TemporalOT method. It shows the success rate curves for three different configurations of the model: using both the context embedding-based cost matrix and the temporal mask (TemporalOT), using only the context embedding-based cost matrix (no-mask), and using only the temporal mask (no-context). The results demonstrate that both proposed components are beneficial to the overall performance and contribute to improved performance compared to using either component alone.", "section": "4 Experiments"}, {"figure_path": "LEed5Is4oi/figures/figures_8_1.jpg", "caption": "Figure 7: Bad case analysis. Compared with the expert trajectory (top), the agent focused on imitating the arm behavior (bottom) and missed the details, i.e., grasping the block. make it difficult for the pretrained visual encoder to capture the subtle information. More bad case analyses are available in the Appendix A.2.", "description": "This figure shows a comparison between an expert trajectory and the trajectory of an agent trained using the proposed TemporalOT method. The expert successfully grasps a brown block and places it into a designated hole. However, the agent imitates only the arm movement from the expert trajectory and fails to grasp the block. This illustrates a failure case for the method, highlighting potential issues where subtle details are missed during training, particularly when a pretrained visual encoder struggles to capture subtle information.  The caption notes additional examples are available in Appendix A.2.", "section": "4.7 Visualization for Bad Cases"}, {"figure_path": "LEed5Is4oi/figures/figures_13_1.jpg", "caption": "Figure 8: Evaluation curves on the Meta-world benchmark tasks.", "description": "This figure presents the success rate curves for nine Meta-world benchmark tasks across different algorithms: TaskReward, GAIFO, OT0.99, OT0.9, ADS, and TemporalOT.  Each curve represents the average success rate across multiple random seeds, and shaded areas show the standard deviations.  The graph visually compares the performance of the proposed TemporalOT method against existing baselines over time (training steps). It allows for a direct comparison of how quickly and effectively each algorithm learns to successfully complete each task.", "section": "A Additional Experiment Results"}, {"figure_path": "LEed5Is4oi/figures/figures_14_1.jpg", "caption": "Figure 3: An illustration of the proposed TemporalOT method. (left) Instead of using a pair-wise cosine similarity as the transport cost, we use a group-wise cosine similarity to learn a more accurate cost matrix. (right) We use a temporal mask to enforce the OT reward to focus on a narrow scope to avoid potential distractions from observations outside of the mask window.", "description": "This figure illustrates the two main improvements in the Temporal Optimal Transport (TemporalOT) method. The left side shows how a group-wise cosine similarity is used to improve the accuracy of the cost matrix, compared to the pairwise cosine similarity used in previous methods. The right side illustrates the use of a temporal mask to improve the focus of the OT reward, preventing distractions from observations that are too far in the past or future.", "section": "3.2 Temporal Optimal Transport Reward (TemporalOT)"}, {"figure_path": "LEed5Is4oi/figures/figures_16_1.jpg", "caption": "Figure 10: More ablation studies. (left) A comparison of different visual encoders. (right) Using a dynamic temporal mask slightly improves the performance.", "description": "This figure presents the ablation study results for different visual encoders (ResNet18, ResNet50, ResNet152) and different mask designs (diagonal and dynamic). The left panel shows that ResNet50 and ResNet152 achieve similar performance, while ResNet18 underperforms, indicating the importance of a sufficiently powerful visual encoder for extracting relevant features.  The right panel compares the performance of using a diagonal temporal mask (as in the main method) versus a dynamic temporal mask. The dynamic mask, which adapts to the specific trajectory, provides a slight performance improvement.", "section": "4.4 Ablation Studies on Different Model Components"}, {"figure_path": "LEed5Is4oi/figures/figures_17_1.jpg", "caption": "Figure 11: Visualization of the nine evaluation tasks in the experiments.", "description": "This figure shows screenshots of the nine Meta-world tasks used in the paper's experiments.  Each image depicts a robotic arm in a simulated environment, ready to perform a specific manipulation task. The tasks are: Basketball, Button-press, Door-lock, Door-open, Hand-insert, Lever-pull, Push, Stick-push, and Window-open. The screenshots illustrate the diversity of manipulation skills tested in the experiments.", "section": "B Experimental Setup"}]