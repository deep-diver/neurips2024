[{"type": "text", "text": "Text-Guided Attention is All You Need for Zero-Shot Robustness in Vision-Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Lu Yu 1 Haiyang Zhang 1 Changsheng Xu 2 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1School of Computer Science and Engineering, Tianjin University of Technology 2State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, University of Chinese Academy of Sciences {luyu@email, zshy@stud}.tjut.edu.cn, csxu@nlpr.ia.ac.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Due to the impressive zero-shot capabilities, pre-trained vision-language models (e.g. CLIP), have attracted widespread attention and adoption across various domains. Nonetheless, CLIP has been observed to be susceptible to adversarial examples. Through experimental analysis, we have observed a phenomenon wherein adversarial perturbations induce shifts in text-guided attention. Building upon this observation, we propose a simple yet effective strategy: Text-Guided Attention for Zero-Shot Robustness (TGA-ZSR). This framework incorporates two components: the Attention Refinement module and the Attention-based Model Constraint module. Our goal is to maintain the generalization of the CLIP model and enhance its adversarial robustness: The Attention Refinement module aligns the text-guided attention obtained from the target model via adversarial examples with the text-guided attention acquired from the original model via clean examples. This alignment enhances the model\u2019s robustness. Additionally, the Attention-based Model Constraint module acquires text-guided attention from both the target and original models using clean examples. Its objective is to maintain model performance on clean samples while enhancing overall robustness. The experiments validate that our method yields a $9.58\\%$ enhancement in zero-shot robust accuracy over the current state-of-the-art techniques across 16 datasets. Our code is available at https://github.com/zhyblue424/TGA-ZSR. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large-scale pre-trained vision-language models (VLMs) have showcased remarkable success in artificial intelligence by seamlessly integrating visual and textual data to understand complex multimodal information, such as CLIP [48]. Leveraging vast datasets and powerful architectures such as BERT [10] and its variants [8, 33], these models adeptly capture semantic relationships between images and texts, offering significant advantages across numerous applications. From image classification [14, 67, 55] and semantic segmentation [50] to image captioning [39] and vision question answering [44], pre-trained VLMs revolutionize how machines perceive and interact with multimodal information. Their importance lies in their ability to learn rich representations from varied data streams, enabling zero-shot learning and transfer learning across domains and tasks. Thus ensuring the reliability of large-scale models is crucial. However, these models are vulnerable to adversarial attacks as many other networks as demonstrated by recent studies [38, 59], even slight perturbations to input data can result in misclassification or altered outputs. Such attacks pose a significant challenge, particularly in critical applications like autonomous vehicles [60], medical diagnosis [32], and maritime navigation [29], where the consequences of erroneous decisions can be severe. As these large-scale models become increasingly prevalent in real-world applications, understanding and mitigating the risks posed by adversarial attacks is essential to maintain trust and reliability in AI systems. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Adversarial training [53, 61, 69] has emerged as a crucial technique in enhancing the robustness of deep learning models against adversarial attacks. By augmenting training data with adversarial examples generated through perturbations of input data, models are forced to learn more robust decision boundaries, thereby improving their resilience to adversarial manipulation. Given the rising significance of large-scale VLMs in various applications, understanding their vulnerability to adversarial attacks is essential. While adversarial training presents practical challenges when applied to downstream tasks, especially with large-scale models. Firstly, adversarial training typically involves generating adversarial examples during each training iteration, which increases the computational overhead and may lead to overfitting on the training data. This phenomenon is exacerbated in large-scale models with vast parameter spaces, where fine-tuning becomes more susceptible to overfitting. Moreover, adversarial training may not adequately prepare models for all possible adversarial scenarios, potentially leaving them vulnerable to unknown data distributions encountered in real-world settings. Exploring zero-shot adversarial robustness in these models is particularly pertinent as it sheds light on their ability to generalize and perform reliably in unseen scenarios. Additionally, considering the multimodal nature of VLMs, the exploration of zero-shot adversarial robustness offers insights into the complex interactions between visual and textual modalities, paving the way for more robust and trustworthy multimodal AI systems. ", "page_idx": 1}, {"type": "text", "text": "Text-guided Contrastive Adversarial Training (TeCoA) method [38] represents the pioneering effort in investigating the zero-shot adversarial robustness of large-scale VLMs. They aim to bolster CLIP\u2019s zero-shot generalization capacity against adversarial inputs. While their primary focus lies on enhancing accuracy in the face of adversarial samples, this improvement comes at the expense of decreased performance on clean data. Subsequent work by PMG-AFT [59] builds upon this by introducing a pre-trained model guided adversarial fine-tuning technique, further enhancing both generalizability and adversarial robustness. However, despite the advancements made by both studies in enhancing CLIP\u2019s zero-shot robustness, significant questions regarding the interpretability of adversarial attacks and the efficacy of adversarial training remain unanswered. Specifically, the mechanisms through which adversarial attacks influence network outputs and the reasons behind the effectiveness of adversarial training strategies remain elusive. In our paper, we delve into the text-guided attention shift phenomenon to shed light on how adversarial attacks alter model outputs. Leveraging these insights, we propose a simple yet effective strategy, TGA-ZSR, aimed at enhancing the robustness of the CLIP model and preserving its performance on clean examples. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions are summarized follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 To our knowledge, we are the first to introduce text-guided attention to enhance zero-shot robustness on vision-language models while maintaining performance on clean sample.   \n\u2022 We improve the interpretability of adversarial attacks for zero-shot robustness on vision-language models through a text-guided attention mechanism.   \n\u2022 The experimental results show that TGA-ZSR surpasses previous state-of-the-art methods, establishing a new benchmark in model zero-shot robust accuracy. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Pre-trained Vision-language Models. In recent years, advancements in computer vision[12, 17, 34] have primarily relied on training models with image-label pairs to recognize predefined object categories. However, these approaches often overlook the inherent semantic connections between textual descriptions and visual content. Motivated by the remarkable progress witnessed in natural language processing (NLP), exemplified by breakthroughs like Transformer [56], BERT [10], and GPT-3 [3], researchers are increasingly drawn to the prospect of using textual data to enhance the capabilities of DNNs. These methodologies are referred to as VLMs [21, 48, 49, 64] and one prominent approach is to directly learn the semantic similarity between images and corresponding textual descriptions through image-text pairs. By aligning the embeddings of these two modalities, models like CLIP [48], ALIGN [21], BLIP [25], Visual-BERT [47], and ALBEF [26] aim to achieve superior performance across various tasks. CLIP [48] leverages a vast dataset of 400 million imagetext pairs sourced from the internet and employs contrastive loss to effectively align the embeddings of both modalities, thereby enhancing the model\u2019s capabilities. Experimental results underscore the significant performance gains achieved by incorporating textual information into the model, with zero-shot performance surpassing that of earlier deep neural network architectures. However, despite its impressive zero-shot accuracy, experiments [38, 59] reveal vulnerabilities to adversarial examples, resulting in a notable decline in robustness. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Adversarial Robustness. Deep neural networks have been found to be vulnerable to adversarial examples [54, 36, 40, 66], which can fool DNNs to produce false outputs, rendering trained models unreliable. To bolster robustness against such adversarial attacks, various advanced methods have been proposed, including data augmentation [28, 58, 27, 65], adversarial training [69, 53, 61, 68], progressive self-distillation [1], randomization strategy [11, 35], and adversarial purification [41, 24, 62]. While these strategies aim to improve DNNs\u2019 adversarial robustness, they often come with increased complexity or limited generalizability. Adversarial training [69, 53, 61, 68] stands out as one of the most widely used and effective approaches, fine-tuning DNNs by generating adversarial examples during training. After the emergence of CLIP [48], many subsequent works [45, 16, 63] have utilized CLIP as a backbone, yet little attention has been given to studying its adversarial robustness. CLIP is shown to be susceptible to adversarial examples [38] as well, posing a significant threat to downstream tasks utilizing CLIP as a backbone. Hence, investigating the adversarial robustness of CLIP is crucial. ", "page_idx": 2}, {"type": "text", "text": "Zero-shot Adversarial Robustness for VLMs. The visual-language model, trained on both image and text data, serves as a foundational model for various tasks. However, it has shown vulnerability to adversarial examples [38, 59], and training from scratch is time-intensive. TeCoA [38] was the first to explore zero-shot adversarial robustness for VLMs, aiming to enhance CLIP\u2019s adversarial robustness by minimizing the cross-entropy loss between image logits and targets. While TeCoA solely utilizes cross-entropy loss, yielding only marginal performance improvements, PMG-AFT [59] extends this approach by minimizing the distance between features of adversarial examples and those of the pre-trained model. FARE [51] primarily focuses on maintaining high clean accuracy while improving model robustness, achieving this by constraining the distance between the original and target model embeddings. Our experiments reveal significant differences in attention maps between original examples and adversarial examples. Leveraging this insight, we enhance model robustness by constraining it with text-guided attention. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries and Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Following the previous works [38, 59], we choose CLIP model as the pre-trained VLMs for image classification task. Given an image-text pair $(x,t)$ , where $x$ represents an image and $t$ represents a textual prompt, CLIP learns to encode both the image and the text into fixed-dimensional embeddings. Let $f(x)$ denote the embedding of the image $x$ and $g(t)$ denote the embedding of the text prompt $t,\\,y$ is the one-hot vector label. For training or fine-tuning on the downstream tasks, we use the cross-entropy loss, denoted as $L(x,t,y)$ . ", "page_idx": 2}, {"type": "equation", "text": "$$\nL(x,t,y)=-\\mathbb{E}_{i,j}\\bigg[y_{i j}l o g\\frac{e x p(c o s(f(x)_{i},g(t)_{j}))/\\tau)}{\\sum_{k}e x p(c o s(f(x)_{i},g(t)_{k}))/\\tau)}\\bigg]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where we set $y_{i j}=1$ if the image-text pair is positive, otherwise, $y_{i j}\\,=\\,0$ . $\\tau$ is the temperature parameter and cos indicates calculating the cosine similarity of the two embeddings. ", "page_idx": 2}, {"type": "text", "text": "Adversarial Attacks. Adversarial attacks are a concerning phenomenon where small, often imperceptible perturbations are intentionally applied to input data with the aim of deceiving a model into producing incorrect outputs. These perturbations are crafted with the goal of causing the model to misclassify or generate erroneous predictions while appearing indistinguishable to human observers. The Projected Gradient Descent (PGD) [36] method is an iterative approach for crafting adversarial examples. It starts with the original input data and then iteratively adjusts the data in the direction that maximizes the model\u2019s loss function while ensuring the perturbed data remains within a specified perturbation budget. Mathematically, the PGD attack can be expressed as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{a+1}=\\Pi_{x+S}(x_{a}+\\varepsilon\\cdot s i g n(\\nabla_{x_{a}}L(x_{a},t,y)))\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $L$ represents the loss function, $x$ denotes the original input data, $\\varepsilon$ controls the magnitude of perturbation, and $\\bigtriangledown_{x}L$ represents the gradient of the loss function with respect to the input data. ", "page_idx": 2}, {"type": "image", "img_path": "cOw65A9FGf/tmp/0c92eac6fa89c89ab8975b9db60cca14c76dcf766a54175f1df788d993827674.jpg", "img_caption": ["Figure 1: The four rows depict the original image, its associated attention map, the generated adversarial example, and the attention map of the adversarial example. Labels in black indicate the ground truth, while those in red represent mis-classified labels for the adversarial examples. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "By adding or subtracting $\\varepsilon$ times the sign of this gradient to the original input data, the PGD attack generates adversarial examples that lead to misclassification or incorrect predictions by the model. $\\Pi_{x+S}$ makes the perturbed data remains within an $\\varepsilon$ -neighborhood of the original input, preventing the generated adversarial examples from straying too far. $S$ is a set of allowed perturbations that formalizes the manipulative power of the adversary. ", "page_idx": 3}, {"type": "text", "text": "Adversarial Examples Generation and Adversarial Training. The optimization objective for crafting adversarial examples aims to maximize the loss of model $f_{\\theta}$ with respect to a perturbed input $x_{a}$ which can be formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{a}=\\underset{x_{a}}{a r g m a x}\\,L(f_{\\theta}(x_{a},t,y))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Adversarial training is a technique to generate adversarial examples from the original training data and then use these examples to train the model, forcing it to learn to resist adversarial perturbations. To adapt the model to the downstream tasks, we apply adversarial fine-tuning on one target model towards robustness with the following loss: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\theta=a r g m i n\\,\\mathcal{T}(f_{\\theta}(x_{a},t,y))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Where $\\mathcal{I}$ represents the total loss function used for training the model. ", "page_idx": 3}, {"type": "text", "text": "Zero-Shot Adversarial Robustness. In this paper, we investigate the zero-shot adversarial robustness of CLIP model, which refers to the ability of these models to maintain performance and reliability even when encountering unseen adversarial samples during inference, with only adversarial fine-tuning the original CLIP model on one target dataset, such as Tiny-ImageNet. ", "page_idx": 3}, {"type": "text", "text": "3.2 Text-Guided Attention based Interpretation of Adversarial Attacks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Text-Guided Attention. Attention mechanisms [30, 16, 31] play a crucial role in enhancing vision model performance across various tasks. At its core, attention enables models to focus on relevant parts of the input data while suppressing irrelevant information. Similarly, in VLMs, by incorporating textual guidance, the models can effectively focus on relevant visual features while processing language, thus facilitating more accurate and coherent multimodal understanding. Additionally, text-guided attention enhances interpretability by providing insights into the model\u2019s decision-making process, fostering trust and understanding in complex multimodal systems. Thus, we investigate the impact of text-guided attention on enhancing and interpreting zero-shot adversarial robustness in VLMs in this paper. We define the text-guided attention as following: ", "page_idx": 3}, {"type": "image", "img_path": "cOw65A9FGf/tmp/9d39a4b31031febb70fb402a8d4c373443e0a489d50d86c247ce8f1857f00e30.jpg", "img_caption": ["Figure 2: An overview of our TGA-ZSR framework: We generate adversarial examples and feed them into the target image encoder. To enhance the adversarial robustness of the CLIP model and maintain its generalization, we introduce text-guided attention. This involves refining the framework for adversarial examples through the Attention Refinement module and constraining the model to prevent significant drift via the Attention-based Model Constraint module. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nA(\\boldsymbol{x})=f_{g}(\\boldsymbol{x})\\cdot g(t)^{\\mathsf{T}},\\quad A\\in\\mathbb{R}^{P\\times1}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Where $f_{g}(x)$ represents the global image feature before the pooling operation of $f(x)$ , and $P$ denotes the dimension of the attention embeddings. We reshape $A$ to $\\mathbb{R}^{\\sqrt{P}\\times\\sqrt{P}}$ to obtain the attention map, which is then resized to $A\\in\\mathbb{R}^{H\\times W}$ . Finally, we apply a normalization operation (norm) on $A$ to obtain the final text-guided attention map. ", "page_idx": 4}, {"type": "text", "text": "Interpretation of Adversarial Attacks. The previous research has predominantly focused on bolstering the zero-shot robustness of Vision-Language Models (VLMs), yet the reasons leading to mis-classifications induced by adversarial attacks remain unclear. This paper aims to shed light on interpreting the impact of adversarial attacks on VLMs. By employing Eq. 5, we compute the text-guided attention for both the original image (Ori. image) and its corresponding adversarial counterpart (Adv. image), as depicted in Fig. 1. Remarkably, despite the subtle discrepancies imperceptible to the human eye between the adversarial example and the original image, the former is mis-classified (labels in red). However, a significant difference emerges in the respective text-guided attention maps. Specifically, we observe a notable shift in the text-guided attention of the adversarial example, characterized by instances of displacement towards other objects, backgrounds, or even disappearance. For instance, while the original images in the first, second, and fourth columns pay attention to their subjects\u2019 heads, in their adversarial counterparts, attention diverges elsewhere. In the third column, the attention shift leads from the correct object to an incorrect one, resulting in mis-classification. In the fifth and seventh columns, the attention in their adversarial counterparts is redirected towards the background. ", "page_idx": 4}, {"type": "text", "text": "3.3 Text-Guided Attention for Zero-Shot Robustness (TGA-ZSR) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The semantic information embedded within text representations are preserved through a frozen text encoder, offering invaluable guidance when adversarial perturbations disrupt relevant visual features, which has not been explored for zero-shot robustness of vision-language models. We introduce the Attention Refinement Module, designed to effectively fliter out irrelevant information, thereby mitigating the impact of adversarial attacks seeking to exploit vulnerabilities in the model\u2019s decision-making process. Moreover, to maintain model\u2019s ability to generalize effectively on clean images, we introduce the Attention-based Model Constraint Module. This module ensures consistent performance on clean data while enhancing the model against adversarial disruptions. Additionally, employing text-guided attention enhances interpretability, offering crucial insights into how the model integrates and processes information across modalities. This interpretability not only instills trust in the model\u2019s predictions but also facilitates the detection and mitigation of adversarial attacks. Our approach (i.e. TGA-ZSR) presents a comprehensive framework (as shown in Fig. 2) for enhancing model robustness to adversarial perturbations while concurrently improving interpretability. We will introduce the details as follows. ", "page_idx": 5}, {"type": "text", "text": "Attention Refinement Module. Based on the insights gained in Section 3.2, we propose an attention refinement module aimed at enhancing the robustness of the model. This module is designed to rectify the text-guided attention of adversarial samples, which often leads to altered predictions. Our approach aligns the adversarial attention map with that of the clean samples, known for their high-accuracy attention distribution. This simple yet effective strategy serves to mitigate the impact of adversarial perturbations on the model\u2019s predictions. ", "page_idx": 5}, {"type": "text", "text": "We take the generated adversarial sample $x_{a}$ to the target model $f_{g}^{t a r}(\\cdot)$ and the clean sample $x$ to the original model $f_{g}^{o r i}(\\cdot)$ and obtain the adversarial attention map $A(x_{a}^{i})_{t a r}$ and the clean attention map $A(x^{i})_{o r i}$ respectively. The attention refinement loss $L_{A R}$ is thus defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nL_{A R}=\\frac{1}{N}\\cdot\\sum_{i=0}^{N}\\|A(x_{a}^{i})_{t a r}-A(x^{i})_{o r i}\\|_{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $A(x_{a})_{t a r}=f_{g}^{t a r}(x_{a})\\cdot g(t)^{\\sf T}$ and $A(x)_{o r i}=f_{g}^{o r i}(x)\\cdot g(t)^{\\mathsf{T}}\\mathbf{\\Sigma}^{1},\\,\\|\\|_{2}$ denotes the $L_{2}$ distance computation between two attention maps. ", "page_idx": 5}, {"type": "text", "text": "Attention-based Model Constraint Module. The Attention Refinement module serves to enhance the robustness of the models, consequently improving the accuracy of adversarial samples. However, this enhancement comes with a trade-off: it may marginally sacrifice the accuracy on clean samples due to shifts in model parameters. To preserve the generalization capability of pre-trained VLMs, we introduce an Attention-based Model Constraint module. This module aims to mitigate performance drops on clean images, thereby ensuring the overall effectiveness and reliability of the model. ", "page_idx": 5}, {"type": "text", "text": "Specifically, we input the clean sample $x$ into the target model $f_{g}^{t a r}(\\cdot)$ , adversarially fine-tuned on the Tiny-ImageNet dataset, to acquire the text-guided attention map $A(x)_{t a r}$ . Concurrently, the original text-guided attention map outputted from the original CLIP model $f_{g}^{o r i}(\\cdot)$ is denoted as $A\\bar{(x)}_{o r i}$ . To ensure the preservation of importance parameters for clean images, we enforce an $L_{2}$ distance constraint between these two attention maps. The attention-based model constraint loss $L_{A M C}$ is formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nL_{A M C}=\\frac{1}{N}\\cdot\\sum_{i=0}^{N}\\left\\|A(x^{i})_{t a r}-A(x^{i})_{o r i}\\right\\|_{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Thus the final loss function can be represented as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nL_{t o t a l}=L_{C E}+\\alpha\\cdot L_{A R}+\\beta\\cdot L_{A M C}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. Our experiments begin with training the pre-trained CLIP model on the Tiny-ImageNet [9]. Then we evaluate the model\u2019s zero-shot adversarial robustness across 15 subsequent datasets, fol", "page_idx": 5}, {"type": "text", "text": "Table 1: Zero-shot robust accuracy on images attacked with 100 steps of PGD [36]. We performed several different methods on Tiny-ImageNet and evaluated across 16 datasets. The optimal accuracy is highlighted in bold, while the second-best accuracy is underlined. The values in parentheses represent the standard deviation. ", "page_idx": 6}, {"type": "table", "img_path": "cOw65A9FGf/tmp/b10312726f1d4423bad075ab6c308a9a838e7e1cec40e5efbd98d58618ddd5fa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "cOw65A9FGf/tmp/68d759b9459d8c1d4aff3dfba386fef050b29be5107c14cd1e6121c646d0c3b8.jpg", "table_caption": ["Table 2: Zero-shot clean accuracy. We performed several different methods on Tiny-ImageNet and evaluated across 16 datasets. The values in parentheses represent the standard deviation. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "lowed by previous studies, such as TeCoA [38] and PMG-AFT [59]. These datasets include several commonly used classfication datasets, including CIFAR-10 [23], CIFAR-100 [23], STL-10 [6], ImageNet [9], Caltech-101 [13], and Caltech-256 [15]. Additionally, fine-grained image classification datasets such as StanfordCars [22], Flowers102 [42], Food101 [2], FGVCAircraft [37], and OxfordPets [46] are included. Furthermore, the scene recognition dataset SUN397 [43], the medical image dataset PCAM [57], and the satellite image classifacation dataset EuroSAT [18] and the texture recognition dataset DTD [5] are incorporated for comprehensive evaluation. We also conduct experiments on four additional datastes (i.e. ImageNet_subset, ImageNet-A, ImageNet-O and ImageNet-R) as shown in Supp. Mat. A.1. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details. Following the protocol of previous works [59], we fine-tuned the CLIP model on the adversarial samples of Tiny-ImageNet [9] as \u2018adversarial fine-tuning\u2019 and subsequently evaluated its performance across 15 datasets and Tiny-ImageNet itself. We employ ViT-B/32 as the backbone in CLIP and utilize the SGD optimizer to minimize loss. During adversarial fine-tuning, we update all parameters of the image encoder with a learning rate of 1e-4, weight decay of 0, momentum of 0.9, and a batch size of 128. We utilize $l_{\\infty}$ norm PGD-2 [36] with 2 iterations to generate adversarial examples, with an attack strength $\\varepsilon$ of 1/255 and the attack step size is 1/255. To evaluate zero-shot adversarial inference, we employ $l_{\\infty}$ norm PGD-100 [36] with 100 iterations, attack step of 1/255 and a batch size of 256 to generate adversarial examples for verifying CLIP\u2019s adversarial robustness. Additionally, to assess the model\u2019s robustness under different attack strengths, we perform inference using adversarial strengths $\\varepsilon$ of 1/255, 2/255, and 4/255. The hyper-parameters $\\alpha$ and $\\beta$ are set to 0.08 and 0.05 respectively in Eq. 8 in the main experiments. Maintain the same parameters for the CW attack. For the AutoAttack [7] experiments, $\\alpha$ and $\\beta$ are set to 0.08 and 0.009. We conducted the experiment utilizing the RTX 3090, which required a training period ranging from 3 to 4 hours. ", "page_idx": 6}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To validate the effectiveness of our approach, we conduct comparisons with several state-of-theart methods such as TeCoA [38], PMG-AFT [59], and FARE [51]. Additionally, we extend the comparison to include CLIP (the original pre-trained CLIP model), FT-Adv. (adversarial fine-tuning using the contrastive loss of the original CLIP) and FT-Clean (fine-tuning on clean examples with the contrastive loss of the original CLIP) for a comprehensive evaluation. ", "page_idx": 6}, {"type": "table", "img_path": "cOw65A9FGf/tmp/90a12b92ee0e7b5189c4866c0f7f51f095dc009d3f79cde8a836ac70a98d9fba.jpg", "table_caption": ["Table 3: Zero-shot robust accuracy on images attacked with $\\varepsilon$ of 1/255 of AutoAttack [7]. We performed several different methods on Tiny-ImageNet and evaluated on 16 datasets. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 4: Zero-shot robust accuracy across 16 datasets with CW attack [4]. The optimal accuracy is highlighted in bold. ", "page_idx": 7}, {"type": "table", "img_path": "cOw65A9FGf/tmp/22c5df2dbc510c5a3ea843fe429aa73611e347c640ba04900cf3a79f847571de.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Adversarial Zero-shot Robust Accuracy. Table 1 shows that the average accuracy of our TGA-ZSR outperforms the original CLIP model by $37.19\\%$ . Compared to current stat-of-the-art method, PMGAFT, the proposed method achieve an average improvement of $9.58\\%$ . In general, our method is superior than all the other methods on most datasets except a comparable result on PCAM dataset. In addition, we obtain the best result on Tiny-ImageNet, which is not a strict zero-shot test. It indicates that our method is robust on the adversarial attack on both seen and unseen datasets. ", "page_idx": 7}, {"type": "text", "text": "Zero-shot Clean Accuracy. Table 2 illustrates the model\u2019s accuracy for clean examples using different methods. Our method outperforms PMG-AFT by $9.84\\%$ and FT-clean by $2.07\\%$ in terms of average accuracy. Similar to Table 1, zero-shot clean accuracy exhibits improvement not only on an individual dataset but across all datasets. However, we observed that our zero-shot clean accuracy is $3.41\\%$ lower than that achieved by FARE. It is important to note that FARE prioritizes preserving zero-shot clean accuracy. However, we have significantly enhanced the zero-shot robust accuracy in adversarial scenarios with $23.84\\%$ gain compared to FARE in Table 1. ", "page_idx": 7}, {"type": "text", "text": "4.3 Experiments on More Attack Types ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Results against AutoAttack. AutoAttack [7] stands out as a strong attack method for assessing model robustness. We follow TeCoA and PMG-AFT to verify the perturbation bound $\\varepsilon$ of $1/255$ in the standard version of AutoAttack. The results are summarized in Table 3. We can see that the original CLIP model experienced a significant performance decline, decreasing to $0.09\\%$ on the adversarial example. Our TGA-ZSR also demonstrates a decline but still achieves superior results compared to other methods, validating its effectiveness against stronger attacks. ", "page_idx": 7}, {"type": "text", "text": "Results against CW Attack. CW attack [4] is an optimization-based approach designed to generate small perturbations to input data, causing the model to make incorrect predictions while keeping the perturbed input visually similar to the original. We further evaluate the robustness of our approach against this challenging attack, using a perturbation bound of $\\varepsilon\\,=\\,1/255$ . The results, shown in Table 4, demonstrate that our method significantly outperforms the state-of-the-art method PMG-AFT on both adversarial and clean samples. This substantial margin indicates the adversarial robustness of our proposed method. ", "page_idx": 7}, {"type": "text", "text": "4.4 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Different Types of Attentions. To validate the important role of text-guided attention in our method, we conducted experiments by replacing it with vision-based attention. We employ Grad-CAM [52], a widely adopted method, for generating attention maps based on vision. Table 5 demonstrates that replacing the text-guided attention with vision-based attention yields results that are still comparable to ", "page_idx": 7}, {"type": "text", "text": "Table 5: Comparison of vision-based attention and our text-guided attention. We evaluate the stateof-the-art method PMG-AFT alongside our pipeline, incorporating two different types of attention mechanisms on Tiny-ImageNet and evaluating performance across 16 datasets. ", "page_idx": 8}, {"type": "table", "img_path": "cOw65A9FGf/tmp/f09b3786b6acbe803df0eac5be506a99c4401bb7718d45a9891a1bd87d2d96fa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 6: Zero-shot robust accuracy on images attacked with $\\varepsilon$ of 1/255, 2/255 and 4/255 of PGD [36]. We performed several different methods on Tiny-ImageNet and evaluated across 16 datasets. We represent the average accuracy across various attack strength. ", "page_idx": 8}, {"type": "table", "img_path": "cOw65A9FGf/tmp/b90899f2348e644f1e5f7c758e9d4d34e7bbc190d5e2c5fb6ff2d6d79d7ec93c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "the state-of-the-art method PMG-AFT in terms of both zero-shot robust accuracy and clean accuracy. This finding validates the effectiveness of our method\u2019s pipeline. Furthermore, our text-guided attention significantly improves the average accuracy, demonstrating the advantage of incorporating textual guidance. ", "page_idx": 8}, {"type": "text", "text": "Effect of Attack Strength. We further assess the robustness of the pre-trained model by different levels of PGD-2 attacks. Specifically, we set $\\varepsilon$ to values of 1/255, 2/255 and 4/255, progressively amplifying the magnitude of the adversarial perturbation. This allows us to investigate whether a model trained on weak adversarial examples exhibits robustness against stronger adversarial perturbations. In Table 6, we present the average results for three distinct levels of attack strength. Despite a general decline in the robustness of all methods, our approach still achieves superior results, outperforming PMG-AFT by $1.18\\%$ , and FARE by $11.91\\%$ . ", "page_idx": 8}, {"type": "text", "text": "Effect of Each Component. We conducted several experiments to thoroughly evaluate the effectiveness of each component of our method, as summarized in Table 7. Using $L_{C E}$ alone significantly enhances the model\u2019s robustness through standard adversarial training, but it also results in a notable decrease in clean accuracy compared to the original CLIP model. Applying our Attention Refinement module further improves the average zero-shot accuracy on both adversarial and clean samples. Finally, the Attention-based Model Constraint module dramatically boosts performance, increasing robustness by $10.25\\%$ and clean accuracy by $6.52\\%$ . ", "page_idx": 8}, {"type": "text", "text": "Trade-off between Robust and Clean Accuracy. Achieving the balance between robustness and clean accuracy is crucial in adversarial training. Overftiting in models tends to yield high robustness but low clean accuracy, whereas underfitting typically results in the opposite scenario. As shown in Fig. 3, methods positioned close to the dotted line excel in either adversarial accuracy or clean accuracy, yet they often struggle to strike a balance between robustness and clean accuracy. In contrast, our method demonstrates not only an enhancement in the model\u2019s robustness but also the maintenance of clean accuracy, resulting in an overall superior performance. ", "page_idx": 8}, {"type": "text", "text": "More comprehensive and detailed ablation studies, including hyperparameter selection, the effect of distance metrics on the loss function, and the effect of learning rate, can be found in Supp. Mat. A.2. ", "page_idx": 8}, {"type": "text", "text": "4.5 Computational Overhead and Time Efficiency ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We have evaluated our method against others in terms of memory usage, training time, and test time, and the results are summarized in Table 8. Our method increases memory consumption by approxi", "page_idx": 8}, {"type": "text", "text": "Table 7: Ablation study on each component. After adversarial fine-tuning the model using adversarial examples generated by PGD-2, we verify the robustness of the model using adversarial examples generated by PGD-100. ", "page_idx": 9}, {"type": "table", "img_path": "cOw65A9FGf/tmp/efe0108a7c5e7488addc39e512c474119f7e7e432919aa13f6b1115ced0c59a4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "cOw65A9FGf/tmp/72921bb004fcb920243daa8f317453ae1e5f6d4383a61b9e9288362db889744b.jpg", "img_caption": ["Figure 3: The trade-off between robustness and clean accuracy. Each point on the graph represents a method, with the size of the point indicating the extent to which it achieves a favorable trade-off between robustness and clean accuracy. "], "img_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "cOw65A9FGf/tmp/adb3716e498aadede3e0a3e9c6068075fbb53476a82a3ac68b53211ea2d93035.jpg", "table_caption": ["Table 8: Comparison of memory usage, training time, and test time. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "mately $15\\%$ compared to state-of-the-art method PMG-AFT. This is due to the additional computation required for the text-guided attention map. The training time for our method is comparable to that of PMG-AFT. The test time remains consistent across all methods. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we discovered that adversarial attacks lead shift of text-guided attention. Building on this observation, we introduce a text-guided approach, TGA-ZSR, which incorporates two key components to preform adversarial fine-tuning and constrain the model. This strategy prevents model drift while enhancing model robustness. Extensive experiments validate the performance of TGA-ZSR, which not only improves CLIP\u2019s zero-shot adversarial robustness but also maintains zero-shot clean accuracy on clean examples, gaining a favorable balance. ", "page_idx": 9}, {"type": "text", "text": "Limitations. We use a simple text-guided attention mechanism by multiplying the text embedding and vision embedding which is effective against most attack types. However, for more challenging attacks such as AutoAttack, the improvement remains limited. This indicates that while our approach shows promise, it may require further refinement to enhance robustness under stronger adversarial scenarios. ", "page_idx": 9}, {"type": "text", "text": "Border Impact. Large-scale pre-trained vision-language models (VLMs) like CLIP [48] integrate visual and textual data, revolutionizing applications such as image classification, semantic segmentation, and vision question answering. While these models excel in zero-shot learning and transfer learning, they are vulnerable to adversarial attacks, posing risks in critical applications like autonomous vehicles and medical diagnosis. Adversarial training improves robustness but has practical challenges, including increased computational overhead and potential overftiting. Exploring zero-shot adversarial robustness is essential to ensure reliability. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement. This work was supported by National Science and Technology Major Project under Grant 2021ZD0112200, in part by the National Natural Science Foundation of China under Grants 62202331, U23A20387, 62036012, 62276118. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Alex Andonian, Shixing Chen, and Raffay Hamid. Robust cross-modal representation learning with progressive self-distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16430\u201316441, 2022.   \n[2] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13, pages 446\u2013461. Springer, 2014.   \n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Advances in neural information processing systems, volume 33, pages 1877\u20131901, 2020.   \n[4] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 ieee symposium on security and privacy (sp), pages 39\u201357. Ieee, 2017.   \n[5] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3606\u20133613, 2014.   \n[6] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 215\u2013223, 2011.   \n[7] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on machine learning, pages 2206\u20132216. PMLR, 2020.   \n[8] Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu. Revisiting pre-trained models for Chinese natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 657\u2013668, 2020.   \n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics, 2019.   \n[11] Minjing Dong and Chang Xu. Adversarial robustness via random projection fliters. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4077\u20134086, 2023.   \n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.   \n[13] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In 2004 conference on computer vision and pattern recognition workshop, pages 178\u2013178. IEEE, 2004.   \n[14] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. International Journal of Computer Vision, 132(2):581\u2013595, 2024.   \n[15] Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. 2007.   \n[16] Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzheng Ma, Xupeng Miao, Xuming He, and Bin Cui. Calip: Zero-shot enhancement of clip with parameter-free attention. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 746\u2013754, 2023.   \n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[18] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7):2217\u20132226, 2019.   \n[19] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF international conference on computer vision, pages 8340\u20138349, 2021.   \n[20] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15262\u201315271, 2021.   \n[21] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 4904\u20134916. PMLR, 2021.   \n[22] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages 554\u2013561, 2013.   \n[23] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[24] Minjong Lee and Dongwoo Kim. Robust evaluation of diffusion-based adversarial purification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 134\u2013144, 2023.   \n[25] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 12888\u201312900. PMLR, 2022.   \n[26] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. In Advances in neural information processing systems, volume 34, pages 9694\u20139705, 2021.   \n[27] Lin Li, Jianing Qiu, and Michael Spratling. Aroid: Improving adversarial robustness through online instance-wise data augmentation. arXiv preprint arXiv:2306.07197, 2023.   \n[28] Lin Li and Michael W Spratling. Data augmentation alone can improve adversarial training. In The Eleventh International Conference on Learning Representations, 2022.   \n[29] Shuxin Li, Xu Cheng, Fan Shi, Hanwei Zhang, Hongning Dai, Houxiang Zhang, and Shengyong Chen. A novel robustness-enhancing adversarial defense approach to ai-powered sea state estimation for autonomous marine vessels. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 2024.   \n[30] Tang Li, Fengchun Qiao, Mengmeng Ma, and Xi Peng. Are data-driven explanations robust against out-of-distribution data? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3821\u20133831, 2023.   \n[31] Yi Li, Hualiang Wang, Yiqun Duan, Hang Xu, and Xiaomeng Li. Exploring visual interpretability for contrastive language-image pre-training. arXiv preprint arXiv:2209.07046, 2022.   \n[32] Jie Liu, Yixiao Zhang, Jie-Neng Chen, Junfei Xiao, Yongyi Lu, Bennett A Landman, Yixuan Yuan, Alan Yuille, Yucheng Tang, and Zongwei Zhou. Clip-driven universal model for organ segmentation and tumor detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21152\u201321164, 2023.   \n[33] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.   \n[34] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.   \n[35] Yanxiang Ma, Minjing Dong, and Chang Xu. Adversarial robustness through random weight sampling. In Advances in Neural Information Processing Systems, volume 36, pages 37657\u201337669, 2023.   \n[36] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.   \n[37] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.   \n[38] Chengzhi Mao, Scott Geng, Junfeng Yang, Xin Wang, and Carl Vondrick. Understanding zero-shot adversarial robustness for large-scale models. In The Eleventh International Conference on Learning Representations, 2023.   \n[39] Ron Mokady, Amir Hertz, and Amit H Bermano. Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734, 2021.   \n[40] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2574\u20132582, 2016.   \n[41] Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, and Animashree Anandkumar. Diffusion models for adversarial purification. In International Conference on Machine Learning, pages 16805\u201316827. PMLR, 2022.   \n[42] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian conference on computer vision, graphics & image processing, pages 722\u2013729. IEEE, 2008.   \n[43] A Omkar M Parkhi. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pages 3485\u20133492. IEEE, 2010.   \n[44] Maria Parelli, Alexandros Delitzas, Nikolas Hars, Georgios Vlassis, Sotirios Anagnostidis, Gregor Bachmann, and Thomas Hofmann. Clip-guided vision-language pre-training for question answering in 3d scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5606\u20135611, 2023.   \n[45] Maria Parelli, Alexandros Delitzas, Nikolas Hars, Georgios Vlassis, Sotirios Anagnostidis, Gregor Bachmann, and Thomas Hofmann. Clip-guided vision-language pre-training for question answering in 3d scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5606\u20135611, 2023.   \n[46] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE conference on computer vision and pattern recognition, pages 3498\u20133505. IEEE, 2012.   \n[47] Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, and Arun Sacheti. Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data. arXiv preprint arXiv:2001.07966, 2020.   \n[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[50] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu. Denseclip: Language-guided dense prediction with context-aware prompting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 18082\u201318091, 2022.   \n[51] Christian Schlarmann, Naman Deep Singh, Francesco Croce, and Matthias Hein. Robust clip: Unsupervised adversarial fine-tuning of vision embeddings for robust large vision-language models. In International Conference on Machine Learning, 2024.   \n[52] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pages 618\u2013626, 2017.   \n[53] Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! In Advances in neural information processing systems, volume 32, 2019.   \n[54] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations, 2014.   \n[55] Zhe Tao, Lu Yu, Hantao Yao, Shucheng Huang, and Changsheng Xu. Class incremental learning for light-weighted networks. IEEE Transactions on Circuits and Systems for Video Technology, 2024.   \n[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, volume 30, 2017.   \n[57] Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant cnns for digital pathology. In Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part II 11, pages 210\u2013218. Springer, 2018.   \n[58] Haotao Wang, Chaowei Xiao, Jean Kossaif,i Zhiding Yu, Anima Anandkumar, and Zhangyang Wang. Augmax: Adversarial composition of random augmentations for robust training. In Advances in neural information processing systems, volume 34, pages 237\u2013250, 2021.   \n[59] Sibo Wang, Jie Zhang, Zheng Yuan, and Shiguang Shan. Pre-trained model guided fine-tuning for zero-shot adversarial robustness. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2024.   \n[60] Dafeng Wei, Tian Gao, Zhengyu Jia, Changwei Cai, Chengkai Hou, Peng Jia, Fu Liu, Kun Zhan, Jingchen Fan, Yixing Zhao, et al. Bev-clip: Multi-modal bev retrieval methodology for complex scene in autonomous driving. arXiv preprint arXiv:2401.01065, 2024.   \n[61] Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. In Advances in neural information processing systems, volume 33, pages 2958\u20132969, 2020.   \n[62] Zhaoyuan Yang, Zhiwei Xu, Jing Zhang, Richard I. Hartley, and Peter Tu. Adversarial purification with the manifold hypothesis. In AAAI Conference on Artificial Intelligence, 2022.   \n[63] Hantao Yao, Rui Zhang, and Changsheng Xu. Visual-language prompt tuning with knowledge-guided context optimization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6757\u20136767, 2023.   \n[64] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. In International Conference on Learning Representations, 2021.   \n[65] Lu Yu, Malvina Nikandrou, Jiali Jin, and Verena Rieser. Quality-agnostic image captioning to safely assist people with vision impairment. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, pages 6281\u20136289, 2023.   \n[66] Lu Yu and Verena Rieser. Adversarial textual robustness of visual dialog. In Findings of 61st Annual Meeting of the Association for Computational Linguistics 2023, pages 3422\u20133438. Association for Computational Linguistics, 2023.   \n[67] Lu Yu, Zhe Tao, Hantao Yao, Joost Van de Weijer, and Changsheng Xu. Exploiting the semantic knowledge of pre-trained text-encoders for continual learning. arXiv preprint arXiv:2408.01076, 2024.   \n[68] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In International conference on machine learning, pages 7472\u20137482. PMLR, 2019.   \n[69] Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. Improving the robustness of deep neural networks via stability training. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4480\u20134488, 2016. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Experiments on More Datasets. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Experiments on ImageNet_subset. We follow the state-of-the-art method PMG-AFT, which was fine-tuned on Tiny-ImageNet. In addition to this, we further evaluate our method on the ImageNet_subset (a random selection of 100 classes from the full ImageNet dataset). The results are shown in Table 9 and Table 10. For adversarial robustness, our approach achieves optimal results on several datasets and sub-optimal results on the remaining datasets, with an overall performance that is approximately $1\\%$ higher than the previous state-of-the-art. In terms of clean accuracy, our method performs worse than the generalization-focused FARE but outperforms other methods. ", "page_idx": 14}, {"type": "text", "text": "Table 9: Zero-shot robust accuracy on images attacked with 100 steps of PGD [36]. We performed several different methods on ImageNet_subset and evaluated across 16 datasets. The optimal accuracy is highlighted in bold, while the second-best accuracy is underlined. ", "page_idx": 14}, {"type": "table", "img_path": "cOw65A9FGf/tmp/f6dcfd90123f5b18f838a6a9371ee0eed0329c682c7d0da21d33f97bfd2d6bb7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Table 10: Zero-shot clean accuracy. We performed several different methods on ImageNet_subset and evaluated across 16 datasets. ", "page_idx": 14}, {"type": "table", "img_path": "cOw65A9FGf/tmp/d02fc0faf143bbe52681290edaf55494d4f6c9aaf5d980b0799559de72f53ba0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Results on Diverse Datasets. In addition to validate the method on 16 datasets following the evaluation protocol of previous works, we also conduct evaluations on three additional datasets: ImageNet-A (natural adversarial examples) [20], ImageNet-O (out-of-distribution data) [20], and ImageNet-R (multiform art form) [19]. In Table 11 and Table 12, the clean accuracy on ImageNet-A is notably lower compared to ImageNet-O and ImageNet-R . In general, other methods enhance robust accuracy by around $2\\%$ , they also incur a steep $20\\%$ drop in clean accuracy. In contrast, our approach achieves a notable $5\\%$ improvement in zero-shot robust accuracy, although with a $16\\%$ decrease in clean accuracy, outperforming other methods. Although our method doesn\u2019t excel in zero-shot clean accuracy for ImageNet-O and ImageNet-R, it excels in optimizing zero-shot robust accuracy. Overall, our method yields the best results across these three datasets, as demonstrated by the Average. ", "page_idx": 14}, {"type": "text", "text": "Table 11: Zero-shot robust accuracy on images attacked with 100 steps of PGD [36]. We performed different several methods on Tiny-ImageNet and evaluated in the following three additional datasets. The optimal accuracy is highlighted in bold, while the second-best accuracy is underlined. ", "page_idx": 14}, {"type": "table", "img_path": "cOw65A9FGf/tmp/c4960d9182f95041ad97c7b675ee1aab90e4fe73fbfb5664f848b1a551aaa568.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Table 12: Zero-shot clean accuracy. We performed different several methods on Tiny-ImageNet and evaluated in the following three additional datasets. The optimal accuracy is highlighted in bold, while the second-best accuracy is underlined. ", "page_idx": 15}, {"type": "table", "img_path": "cOw65A9FGf/tmp/1f6e317e7dd4116a8621fd230db36a6724aa12d2e8cf27be6fd39f644dd37f74.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.2 More Ablation Studies ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Trade-off between $\\alpha$ and $\\beta$ . Following the protocol of previous works (TeCoA [38], PMG-AFT [59], FARE [51]), we fine-tuned the CLIP model on adversarial samples from a single dataset (TinyImageNet in our case) for \u2018adversarial fine-tuning\u2019 and subsequently evaluated its performance across 15 datasets, including Tiny-ImageNet itself. Thus we only need to tune hyperparameters on just the training dataset. We randomly selected $80\\%$ of the training set for training and the remaining $20\\%$ for validation to choose the hyperparameters. The validation set results are shown in Table 13. The final results on the test set were obtained by training on the entire training set using the optimal hyperparameters $\\scriptstyle\\alpha=0.08$ , $\\beta{=}0.05$ ) identified from the validation set. ", "page_idx": 15}, {"type": "table", "img_path": "cOw65A9FGf/tmp/98010a2660056198c229810c0598a3e4d1c58b6c35c8f61760e7099555861c20.jpg", "table_caption": ["Table 13: Results on validation set of Tiny-ImageNet dataset. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "After choosing the optimal hyper-parameter on the validation set of the Tiny-ImageNet dataset, we proceeded to analyze the sensitivity of these hyper-parameters on the overall performance across 16 datasets, including the Tiny-ImageNet and 15 additional datasets. This step was crucial for evaluating the robustness and generalizability of the model under different hyper-parameter settings. Table 14 and Table 15 demonstrate the results of different hyper-parameter of $\\alpha$ and $\\beta$ in Eq. 8. ", "page_idx": 15}, {"type": "table", "img_path": "cOw65A9FGf/tmp/10026575635f3cd0ba671443e65ab363beee3d1f7dfd8cee93e158979da6c3dd.jpg", "table_caption": ["Table 14: Zero-shot robust accuracy on images attacked with 100 steps of PGD [36]. We compared the results of different hyper-parameters on Tiny-ImageNet and evaluated across 16 datasets. The optimal accuracy is highlighted in bold, while the second-best accuracy is underlined. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 15: Zero-shot clean accuracy. We compared the results of different hyper-parameters on Tiny-ImageNet and evaluated across 16 datasets. The optimal accuracy is highlighted in bold, while the second-best accuracy is underlined. ", "page_idx": 16}, {"type": "table", "img_path": "cOw65A9FGf/tmp/ae3d4cece36e8e2f1d9af81cfcb7c380c6dcf7ea860b71e600fb5ca7bd4a733c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Effect of Each Component. Due to space constraints, a detailed experiment on the effect of each component was not provided in Table 7. Therefore, we present here a comprehensive experiment detailing the effect of each component. To explore the impact of each component on the final model performance, we conducted a series of ablation experiments to evaluate the effectiveness of each component. From the experimental results Table 16 and Table 17, it\u2019s clear that the inclusion of our text-guided attention components enhances CLIP\u2019s zero-shot robust accuracy and maintains its zero-shot clean accuracy. ", "page_idx": 16}, {"type": "text", "text": "Table 16: Zero-shot robust accuracy on images attacked with 100 steps of PGD. After adversarial fine-tuning the model using adversarial examples generated by PGD-2, we performed each component and evaluated across 16 datasets. The optimal accuracy is highlighted in bold. ", "page_idx": 16}, {"type": "table", "img_path": "cOw65A9FGf/tmp/2d8b8981b20102bf34d8efdc78fdd6586f9e22b2246d252091a50b88644c439f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 17: Zero-shot clean accuracy. After adversarial fine-tuning the model using adversarial examples generated by PGD-2, we verify the robustness of the model using adversarial examples generated by PGD-100. The optimal accuracy is highlighted in bold. ", "page_idx": 16}, {"type": "table", "img_path": "cOw65A9FGf/tmp/2a78ec667261740be38e6057b15458e3969eac77baeb2c0c3f3f99a06f14ef26.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Effect of Distance Metrics on Loss Function. Except $l_{2}$ , cosine and $l_{1}$ are also frequently utilized distance metrics. We compared the performance of our method using these distance metrics. The results from Table 18 and Table 19 demonstrate that cosine and $l_{1}$ exhibit similar performance but are inferior to $l_{2}$ , except for the zero-shot clear accuracy on PCAM. $l_{2}$ outperforms the other two distance measures by enhancing zero-shot adversarial robustness and zero-shot clean accuracy by approximately $12\\%$ and $11\\%$ , respectively. Thus we choose $l_{2}$ distance to measure in our loss function. ", "page_idx": 16}, {"type": "text", "text": "Effect of Learning Rate. Learning rate stands as a significant hyper-parameter in model training. Here we validate the effect of the learning rate for the experiment in Table 20 and Table 21. When the learning rate is set to 0.001, we observe the lowest values for both zero-shot robust accuracy and zeroshot clean accuracy. And, when we reduce the learning rate to 0.00001, we note that CLIP\u2019s zero-shot clean accuracy remains relatively stable, demonstrating the highest performance in this experiment. However, it affects CLIP\u2019s zero-shot robust accuracy, resulting in a less favorable balance. Selecting a learning rate of 0.0001 achieved a well-balanced improvement, with the average of zero-shot robust accuracy and zero-shot clean accuracy increasing by $18.48\\%$ and $6.41\\%$ , respectively, compared to the learning rates of 0.001 and 0.00001. ", "page_idx": 16}, {"type": "text", "text": "Table 18: Zero-shot robust accuracy on images attacked with 100 steps of PGD [36]. We compared the results of different distance metrics on Tiny-ImageNet and evaluated across 16 datasets. The optimal accuracy is highlighted in bold. ", "page_idx": 17}, {"type": "table", "img_path": "cOw65A9FGf/tmp/b9a76c9eb83452c512dccf9d2023682ed841307645ffb5af02510c3fe0ae711d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 19: Zero-shot clean accuracy. We compared the results of different distance metrics on TinyImageNet and evaluated across 16 datasets. The optimal accuracy is highlighted in bold. ", "page_idx": 17}, {"type": "table", "img_path": "cOw65A9FGf/tmp/1b2cc96c382ba4b411fceaff416258f9696c7f9746b186ff51335274770a23aa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 20: Zero-shot robust accuracy on images attacked with 100 steps of PGD [36]. We compared the results of different learning rates on Tiny-ImageNet and evaluated across 16 datasets. The optimal accuracy is highlighted in bold. ", "page_idx": 17}, {"type": "table", "img_path": "cOw65A9FGf/tmp/82017cfd6b0798e988d1534263a8b8572dd858e05d4c76f0d88b4a44d9a95975.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 21: Zero-shot clean accuracy. We compared the results of different learning rates on TinyImageNet and evaluated across 16 datasets. The optimal accuracy is highlighted in bold. ", "page_idx": 17}, {"type": "table", "img_path": "cOw65A9FGf/tmp/8531864464dc3c0b1df929d8d81e711c877441e1a48183ca4d5fc15a6e53ef05.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: In the abstract and introduction, we provide a clear explanation of the motivation behind our approach. At the conclusion of the introduction, we enumerate the contributions of this paper, etc. We provide details in Abstract and Introduction. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Our approach entails certain limitations, which we acknowledge in Section 5 and commit to addressing through subsequent research. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our approach emphasizes applied aspects and does not need theory assumptions and proofs. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: In Section 3, we introduce our methodology, while in Section 4.1, we outline the hyper-parameters employed for the experiments. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We will provide open access to the code. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: In Section 4.1, we introduce the experimental setup, including the hyperparameters utilized in this paper. Furthermore, in section A, we present highly detailed ablation experiments to enhance comprehension of the results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We conduct multiple experiments to report the error bars. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: In Section 4.5, we provide comprehensive details on the computational resources used and the duration of the training process. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The research conducted in the paper adheres to the NeurIPS Code of Ethics in all aspects. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We discuss both potential positive societal impacts and negative societal impacts of the work performed in Section 5. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The model and dataset employed in our study are widely recognized and do not exhibit any inherent issues. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The models, methodologies, datasets, and other elements used are appropriately aligned and referenced throughout the paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}]