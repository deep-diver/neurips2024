[{"type": "text", "text": "On the Efficiency of ERM in Feature Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ayoub El Hanchi Chris J. Maddison Murat A. Erdogdu   \nUniversity of Toronto & University of Toronto & University of Toronto & Vector Institute Vector Institute Vector Institute   \naelhan@cs.toronto.edu cmaddis@cs.toronto.edu erdogdu@cs.toronto.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Given a collection of feature maps indexed by a set $\\tau$ , we study the performance of empirical risk minimization (ERM) on regression problems with square loss over the union of the linear classes induced by these feature maps. This setup aims at capturing the simplest instance of feature learning, where the model is expected to jointly learn from the data an appropriate feature map and a linear predictor. We start by studying the asymptotic quantiles of the excess risk of sequences of empirical risk minimizers. Remarkably, we show that when the set $\\tau$ is not too large and when there is a unique optimal feature map, these quantiles coincide, up to a factor of two, with those of the excess risk of the oracle procedure, which knows a priori this optimal feature map and deterministically outputs an empirical risk minimizer from the associated optimal linear class. We complement this asymptotic result with a non-asymptotic analysis that quantifies the decaying effect of the global complexity of the set $\\tau$ on the excess risk of ERM, and relates it to the size of the sublevel sets of the suboptimality of the feature maps. As an application of our results, we characterize the performance of the best subset selection procedure in sparse linear regression under general assumptions. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A central idea in modern machine learning is that of data-driven feature learning. Specifically, instead of performing linear prediction on top of hand-crafted features, the current dominant paradigm suggests to use models that select useful features for linear prediction in a data-dependent way [e.g. KSH12; LBH15; $\\mathrm{He}{+16}$ ; Vas+17]. Of course, by putting the burden of picking a feature map on the model and data, we should expect that the resulting learning problem will require more samples to be solved. But just how many more samples do we need to learn such feature-learning-based models? ", "page_idx": 0}, {"type": "text", "text": "In this paper, we investigate this question in a general setting. We study the performance of empirical risk minimization (ERM) on regression tasks with square loss and over model classes induced by arbitrary collections of features maps. More precisely, let $X$ be the random input taking value in a set $\\mathcal{X}$ , and let $(\\phi_{t})_{t\\in{\\cal T}}$ , $\\phi_{t}:\\mathcal{X}\\to\\mathbb{R}^{d}$ , be a collection of feature maps indexed by a set $\\tau$ . For a given regression task and i.i.d. samples, our aim is to understand the performance of ERM over the class of predictors $\\cup_{t\\in\\mathcal{T}}\\{x\\mapsto\\langle w,\\phi_{t}^{\\cdot}(x)\\rangle\\mid w\\in\\mathbb{R}^{d}\\}$ as a function of the sample size, the distribution of the data, and relevant properties of the collection of feature maps $(\\phi_{t})_{t\\in{\\cal T}}$ . ", "page_idx": 0}, {"type": "text", "text": "Classical uniform-convergence-based analyses would suggest that the performance of ERM in this setting is determined by the size of the model class, appropriately measured. The main message of this paper is that in this case, this is wrong in a strong sense. Specifically, we prove an upper bound on the excess risk of ERM on this problem whose dependence on the size of the model class decays monotonically with the sample size, and eventually depends only on the size of the model class induced by the collection of optimal feature maps, which is typically much smaller. ", "page_idx": 0}, {"type": "text", "text": "Formal setup. We briefly formalize our problem here. Let $X$ be the random input taking value in a set $\\mathcal{X}$ , and let $(\\phi_{t})_{t\\in\\mathcal{T}},\\,\\phi_{t}:\\mathcal{X}\\to\\mathbb{R}^{\\dot{d}}$ , be a collection of feature maps indexed by a set $\\tau$ .1 Let $Y\\,\\in\\,\\mathbb{R}$ be the output random variable, jointly distributed with the input $X$ . Our goal is to learn to predict the output $Y$ given the input $X$ as well as possible within the class of predictors $\\left\\{x\\mapsto\\langle{\\dot{w}},\\phi_{t}(x)\\rangle\\mid(t,{\\dot{w}})\\in{\\bar{T^{\\prime}}}\\times\\mathbb{R}^{d}\\right\\}$ . We evaluate the quality of a single prediction $\\hat{y}$ given the ground truth $y$ through the loss function $\\ell(\\hat{y},y):=(\\hat{y}-y)^{2}/2$ , and the overall quality of a predictor $(t,w)\\in\\mathcal{T}\\times\\mathbb{R}^{d}$ through its risk ", "page_idx": 1}, {"type": "equation", "text": "$$\nR(t,w):=\\operatorname{E}[\\ell(\\langle w,\\phi_{t}(X)\\rangle,Y)],\\qquad R_{*}:=\\operatorname*{inf}_{(t,w)\\in{\\mathcal{T}}\\times\\mathbb{R}^{d}}R(t,w).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "We assume that we have access to $n$ i.i.d. samples $(X_{i},Y_{i})_{i=1}^{n}$ with the same distribution as $(X,Y)$ , and perform empirical risk minimization ", "page_idx": 1}, {"type": "equation", "text": "$$\n(\\widehat{t}_{n},\\widehat{w}_{n})\\in\\operatorname*{argmin}_{(t,w)\\in T\\times\\mathbb{R}^{d}}R_{n}(t,w)\\quad\\mathrm{~where~}\\quad R_{n}(t,w):=\\frac{1}{n}\\sum_{i=1}^{n}\\ell(\\langle w,\\phi_{t}(X_{i})\\rangle,Y_{i}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Our goal is to characterize the excess risk $\\begin{array}{r}{\\mathcal{E}(\\hat{t}_{n},\\hat{w}_{n}):=R(\\hat{t}_{n},\\hat{w}_{n})-R_{*}.}\\end{array}$ ", "page_idx": 1}, {"type": "text", "text": "Related work. The study of upper bounds on the excess risk of ERM in a general setting is a classical topic. It was initiated by Vapnik and Chervonenkis [VC74] who established a link between the excess risk of ERM and the uniform convergence of the underlying empirical process. More recently, and fuelled by the development of Talagrand\u2019s concentration inequality [Tal96] and its refinements [e.g. BLM00; Bou02], a literature emerged that provided more fine-grained control of the excess risk of ERM [e.g. BBM05; Kol06; BM06]. A key idea emerging from this line of work is localization. This concept, and in particular the iterative localization method of Koltchinskii [Kol06], plays an important role in our development. We refer the reader to the books [Kol11; Wai19], as well as the recent articles [LRS15b; KRV22] for more on this idea. ", "page_idx": 1}, {"type": "text", "text": "Focusing on the task of regression with square loss, upper bounds on the excess risk of ERM are available for many classes of predictors, including finite [e.g. Aud07; JRT08; LM09], linear [e.g. LM16b; Oli16; Mou22], and convex classes [e.g. LM16a; Men14; LRS15a]. A key development in this area over the last decade has been the realization that such bounds can be obtained under much weaker assumptions than previously thought [Men14; Oli16], owing to the fact that only one-sided control of a certain empirical process is needed [Men14; KM15; Oli16], and which can be obtained under very weak assumptions. The line of work most closely related to ours is the one on random-design linear regression [AC11; HKZ12; Oli16; LM16c; Sau18; Mou22; EE23], and we view our work as an extension of this literature. We review these results in more detail in Section 2. ", "page_idx": 1}, {"type": "text", "text": "Finally, and on a more conceptual level, our work is related to the recent effort to understand the effect of feature learning on the performance of neural networks [e.g. COB19; Gho $+19$ ; $\\mathrm{Ba}{+}22\\$ ]. Beyond this conceptual connection however, our work is quite distinct from this literature. Among other things, our setting is more general since we consider arbitrary features maps. In the same vein, it is worth mentioning the line of work on multiple kernel learning [e.g. Lan $\\pm04$ ; GA11; SD16], although we are not aware of results from this literature that are directly relevant to our setup. ", "page_idx": 1}, {"type": "text", "text": "Challenges. Our class of predictors is somewhat unstructured (e.g. it is in general non-convex), so that off-the-shelf results from the above literature are not directly applicable. Nevertheless, the analysis of the performance of ERM on linear classes provides a good starting point as we review in Section 2. Compared to that setting however, we are faced with two additional challenges. First, we need to control an additional source of error arising from the fact that ERM might select a suboptimal feature map. Second, we are lead to study the suprema of certain $\\tau$ -indexed empirical processes, which in the linear setting reduce to single random variables that are easily dealt with. ", "page_idx": 1}, {"type": "text", "text": "Organization. The rest of the paper is organized as follows. In Section 2, we review known result on the excess risk of linear regression under square loss. In Section 3, we state our main results that hold for the excess risk of ERM for general index sets $\\tau$ . In Section 4, we specialize our analysis to the case where the index set $\\tau$ is finite, obtain more explicit guarantees, and discuss their implications on the sparse linear regression problem. We conclude in Section 5 with a brief discussion. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The goal of this section is to provide more context for our results. We review known results on the excess risk of ERM over linear classes, which corresponds in our setting to the special case where the set $\\tau$ indexing the feature maps is a singleton. As such, to avoid introducing further notation, we use the one from the previous section, while dropping the dependence on $t$ whenever it occurs. ", "page_idx": 2}, {"type": "text", "text": "In the setting of linear regression with square loss, and when the sample covariance matrix of the feature map is invertible, there is a unique empirical risk minimizer and its excess risk admits an explicit expression. Specifically, define ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Sigma:=\\operatorname{E}\\!\\big[\\phi(X)\\phi(X)^{T}\\big],\\qquad\\Sigma_{n}:=\\frac{1}{n}\\sum_{i=1}^{n}\\phi(X_{i})\\phi(X_{i})^{T},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and let $w_{*}$ denote the unique minimizer of the risk $R(w)$ .2Then, an elementary calculation shows that when $\\Sigma_{n}$ is invertible, there is a unique empirical risk minimizer and it satisfies ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{w}_{n}=w_{*}-\\Sigma_{n}^{-1}\\nabla R_{n}(w_{*}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Furthermore, since the risk is a quadratic function of $w$ whose gradient at $w_{*}$ vanishes, replacing $R(\\hat{w}_{n})$ by the equivalent exact second order Taylor expansion around $w_{*}$ yields ", "page_idx": 2}, {"type": "equation", "text": "$$\nR(\\hat{w}_{n})-R(w_{*})=\\frac{1}{2}\\|\\hat{w}_{n}-w_{*}\\|_{\\Sigma}^{2}=\\frac{1}{2}\\big\\|\\Sigma_{n}^{-1}\\nabla R_{n}(w_{*})\\big\\|_{\\Sigma}^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "While exact, this expression is not readily interpretable. For example, how fast does this excess risk go to 0 as a function of the sample size? The following classical result from asymptotic statistics [e.g. Whi82; LC06; Van00] makes this rate more explicit. To state it, we define ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{g(X,Y):=\\nabla_{w}\\ell(\\langle w_{*},\\phi(X)\\rangle,Y),\\qquad G:=\\operatorname{E}\\!\\big[g(X,Y)g(X,Y)^{T}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Theorem 1. Assume that for all $j\\in[d],\\,\\mathrm{E}\\big[\\phi_{j}^{2}(X)\\big]<\\infty,\\,\\mathrm{E}\\big[Y^{2}\\big]<\\infty,$ , and $\\operatorname{E}[\\|g(X,Y)\\|_{\\Sigma^{-1}}^{2}]<\\infty$ Then, as $n\\to\\infty$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\nn\\cdot\\mathcal{E}(\\hat{w}_{n})\\stackrel{d}{\\rightarrow}\\frac{1}{2}\\cdot\\Vert Z\\Vert_{2}^{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $Z\\sim\\mathcal{N}(0,\\Sigma^{-1/2}G\\Sigma^{-1/2})$ . In particular, for any $\\delta\\in(0,0.1)$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}n\\cdot Q_{\\mathcal{E}(\\hat{w}_{n})}(1-\\delta)\\asymp\\mathrm{E}[\\|g(X,Y)\\|_{\\Sigma^{-1}}^{2}]+2\\lambda_{\\operatorname*{max}}(\\Sigma^{-1/2}G\\Sigma^{-1/2})\\log(1/\\delta),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $Q_{X}(p):=\\operatorname*{inf}\\{x\\in\\mathbb{R}\\mid\\operatorname{P}(X\\leq x)\\geq p\\}$ is the quantile function of a random variable $X$ , and where we write $a\\asymp b$ to mean that there exists absolute constants $C,c$ such that $c\\cdot b\\leq a\\leq C\\cdot b$ . In the above statement, they can be taken as $C=1$ and $c=1/32$ . ", "page_idx": 2}, {"type": "text", "text": "We provide a proof in Appendix A for completeness. For our purposes, this theorem is most easily interpreted as follows: for large enough $n$ and small enough $\\delta$ , if the excess risk of ERM is bounded by some quantity with probability at least $1-\\delta$ , then this quantity is, up to a constant, at least as large as the right-hand side of the second displayed equation divided by $n$ . While our primary interest is in non-asymptotic bounds, this asymptotic result, by virtue of its exactness, provides us with a benchmark against which such bounds can be compared. In particular, it identifies the quantity $\\operatorname{E}[\\|g(X,Y)\\|_{\\Sigma^{-1}}^{2}]$ as an intrinsic parameter determining the excess risk of ERM on this problem. ", "page_idx": 2}, {"type": "text", "text": "For large enough $n$ , Theorem 1 gives an interpretable expression for the excess risk. However, it says nothing about how large $n$ needs to be for this expression to be accurate. This motivates a non-asymptotic analysis of the excess risk of ERM, which has been carried out numerous times in recent years [e.g. Oli16; LM16c; EE23]. A goal of this literature has been to obtain upper bounds on the excess risk of ERM that hold in probability under weak moment assumptions, building on the observation that this is indeed possible [Men14]. The following theorem is comparable to the best known result in this area. We leave the proof to Appendix B. To state it, we define ", "page_idx": 2}, {"type": "equation", "text": "$$\nV:=\\operatorname{E}\\biggl[\\Bigl(\\Sigma^{-1/2}\\phi(X)\\phi(X)^{T}\\Sigma^{-1/2}-I\\Bigr)^{2}\\biggr],\\quad L:=\\operatorname*{sup}_{v\\in S^{d-1}}\\operatorname{E}\\biggl[\\Bigl(\\langle v,\\Sigma^{-1/2}\\phi(X)\\rangle^{2}-1\\Bigr)^{2}\\biggr].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Theorem 2. Assume that for all $j\\in[d],\\,\\mathrm{E}\\bigl[\\phi_{j}^{4}(X)\\bigr]<\\infty,\\,\\mathrm{E}\\bigl[Y^{2}\\bigr]<\\infty$ , and $\\operatorname{E}[\\|g(X,Y)\\|_{\\Sigma^{-1}}^{2}]<\\infty$ . Let $\\delta\\in(0,1)$ . If ", "page_idx": 3}, {"type": "equation", "text": "$$\nn\\ge(512\\lambda_{\\mathrm{max}}(V)+6)\\log(e d)+(128L+11)\\log(2/\\delta),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "then with probability at least $1-\\delta$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\hat{w}_{n})\\leq4\\cdot(n\\delta)^{-1}\\cdot\\mathrm{E}[\\|g(X,Y)\\|_{\\Sigma^{-1}}^{2}].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "At a high-level, this result says that above a certain explicit minimal sample size, the asymptotic expression of the excess risk of Theorem 1 is correct, up to a significantly worse dependence on $\\delta$ . The restriction on the sample size is almost the best one can hope for. To see why, note that to get guarantees on the excess risk of any empirical risk minimizer, we need at least that $\\Sigma_{n}$ is invertible, otherwise there exists an empirical risk minimizer arbitrarily far away from $w_{*}$ . To get quantitative guarantees, we need slightly more control in the form of a lower bound on $\\lambda_{\\operatorname*{min}}(\\bar{\\Sigma^{-1/2}\\Sigma}_{n}\\Sigma^{-1/2})$ . We refer the reader to a more detailed discussion in [EME24, Section 5]. ", "page_idx": 3}, {"type": "text", "text": "This result has two key qualities, which we aim to reproduce in our results. First, it is assumptionlean, requiring nothing more than a fourth moment assumption on the coordinates of the feature map compared to Theorem 1. Second, it recovers the right dependence on the intrinsic parameter $\\mathrm{E}[\\|\\dot{g}(X,\\dot{Y})\\|_{\\Sigma^{-1}}^{2}]$ identified in Theorem 1. A downside of this generality is the bad dependence on $\\delta$ . Without further assumptions, this cannot be improved; we refer the reader to the recent literature on robust linear regression for more on this issue [e.g. LM19; LL20; EME24]. ", "page_idx": 3}, {"type": "text", "text": "3 Main Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section we state our main results. They are most easily seen as extensions of Theorems 1 and 2 for general index sets $\\tau$ . In Section 3.1, we study the asymptotics of the excess risk of ERM in our setting, and in Section 3.2, we present a non-asymptotic upper bound on the excess risk. ", "page_idx": 3}, {"type": "text", "text": "To state our results, we require additional definitions and notation. We start with the population and the sample covariance matrices ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Sigma(t):=\\operatorname{E}\\bigl[\\phi_{t}(X)\\phi_{t}(X)^{T}\\bigr],\\qquad\\Sigma_{n}(t):=n^{-1}\\sum_{i=1}^{n}\\phi_{t}(X_{i})\\phi_{t}(X_{i})^{T}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We define the following collection of minimizers, ", "page_idx": 3}, {"type": "equation", "text": "$$\nw_{*}(t):=\\operatorname*{argmin}_{w\\in\\mathbb{R}^{d}}R(t,w),\\qquad\\mathcal{T}_{*}:=\\operatorname*{argmin}_{t\\in\\mathcal{T}}R(t,w_{*}(t)),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "the first is uniquely defined, while the second is set-valued in general. We define the gradient of the loss at these minimizers and their corresponding covariance matrices ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{g(t,(X,Y)):=\\nabla_{w}\\ell(\\langle w_{*}(t),\\phi_{t}(X)\\rangle,Y),\\qquad G(t):=\\mathtt{E}\\big[g(t,(X,Y))g(t,(X,Y))^{T}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Finally, we introduce the following processes which play a key role in our development ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Lambda_{n}(t):=\\sqrt{n}\\cdot\\lambda_{\\operatorname*{max}}(I-\\Sigma^{-1/2}(t)\\Sigma_{n}(t)\\Sigma^{-1/2}(t)),\\quad G_{n}(t):=\\sqrt{n}\\cdot\\Vert\\nabla_{w}R_{n}(t,w_{*}(t))\\Vert_{\\Sigma^{-1}(t)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "as well as, for $t_{*}\\in\\tau_{*}$ and $t\\in\\tau\\backslash\\mathcal{T}_{*}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Delta_{n}(t,t_{*}):=\\sqrt{n}\\cdot\\bigg(1-\\frac{R_{n}(t,w_{*}(t))-R_{n}(t_{*},w_{*}(t_{*}))}{R(t,w_{*}(t))-R_{*}}\\bigg).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We note that the process $(\\Delta_{n}(t,t_{*}))_{t\\in\\mathcal{T}\\backslash\\mathcal{T}_{*}}$ is an empirical process (see [VW96] for an introduction), while $(\\Lambda_{n}(t))_{t\\in\\mathcal{T}}$ and $(G_{n}(t))_{t\\in\\tau}$ are partial suprema of empirical processes. In the sequel, we will slightly abuse this terminology, and call all of these empirical processes, with the understanding that they can be viewed as one with more indexing. We will further assume that these processes are separable; see [BLM13, p.305-306] for a definition. This covers a wide range of applications, while avoiding delicate measurability issues. The suprema of such separable processes, which is the only way they enter our results, can be studied by taking the supremum over a countable dense subset of the index set. Therefore, without loss of generality, we assume that $\\tau$ is countable. ", "page_idx": 3}, {"type": "text", "text": "Finally, in line with the literature on the theory of empirical processes [VW96], we say that a sequence of empirical processes is Glivenko\u2013Cantelli if, when rescaled by $n^{-1/2}$ , the supremum of their absolute value taken over their index set converges to zero in probability as $n\\to\\infty$ . In other words, the weak law of large numbers holds uniformly over the index set. Similarly, we say that a sequence of empirical processes is Donsker if it converges in distribution to its limiting Gaussian process.3 In other words, the central limit theorem holds uniformly over the index set. ", "page_idx": 4}, {"type": "text", "text": "3.1 Asymptotic result ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our first main result is an asymptotic characterization of the quantiles of the excess risk of any sequence of empirical risk minimizers in our setting, which vastly generalizes that of Theorem 1. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3. Assume that $\\tau\\ne\\infty$ and for some $t_{*}\\,\\in\\,\\tau_{*}$ , assume that the empirical processes $(\\Lambda_{n}(t))_{t\\in\\mathcal{T}},$ , $(\\Delta(t,t_{*}))_{t\\in T\\backslash T_{*}}$ and $(G_{n}(t))_{t\\in\\tau}$ are Glivenko-Cantelli. Then, for all $\\varepsilon>0$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\operatorname{P}\\bigl(R(\\hat{t}_{n},w_{*}(\\hat{t}_{n}))-R_{*}>\\varepsilon\\bigr)=0.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Furthermore, if the sequence of processes $(G_{n}(t))_{t\\in\\tau}$ is Donsker, then for any $\\delta\\in(0,0.1)$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\cdot Q_{Z^{-}}(1-\\delta)\\leq\\operatorname*{lim}_{n\\to\\infty}\\operatorname*{inf}_{n}\\cdot Q_{\\xi(\\hat{t}_{n},\\hat{w}_{n})}(1-\\delta)\\leq\\operatorname*{lim}_{n\\to\\infty}\\operatorname*{sup}_{n}n\\cdot Q_{\\xi(\\hat{t}_{n},\\hat{w}_{n})}(1-\\delta)\\leq Q_{Z^{+}}(1-\\delta),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $Z^{-}:=\\operatorname*{inf}_{s\\in{\\mathcal{T}}_{*}}Z^{2}(s),$ , $\\begin{array}{r}{Z^{+}:=\\operatorname*{sup}_{s\\in\\mathcal{T}_{*}}Z^{2}(s),}\\end{array}$ , and $(Z(t))_{t\\in{\\mathcal{T}}}$ is the limiting Gaussian process of the empirical process $(G_{n}(t))_{t\\in\\tau}$ .4 ", "page_idx": 4}, {"type": "text", "text": "Remark 1. For $\\delta\\in(0,0.1)$ , the upper bound admits the more interpretable expression ", "page_idx": 4}, {"type": "equation", "text": "$$\nQ_{Z_{+}}(1-\\delta)\\asymp\\operatorname{E}\\biggl[\\operatorname*{sup}_{s\\in\\mathcal{T}_{*}}Z^{2}(s)\\biggr]+2\\log(1/\\delta)\\operatorname*{sup}_{s\\in\\mathcal{T}_{*}}\\lambda_{\\operatorname*{max}}(\\Sigma^{-1/2}(s)G(s)\\Sigma^{-1/2}(s)).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Furthermore, if $\\mathcal{T}_{*}$ is finite, the first term can be upper bounded as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{E}\\biggl[\\operatorname*{max}_{s\\in\\mathcal{T}_{*}}Z^{2}(s)\\biggr]\\leq80\\cdot(1+\\log|\\mathcal{T}_{*}|)\\cdot\\operatorname*{max}_{s\\in\\mathcal{T}_{*}}\\mathrm{E}[\\|g(s,(X,Y)\\|_{\\Sigma^{-1}(s)}^{2}].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We note that, up to a factor of two in the upper bound on the asymptotic quantiles, Theorem 3 reduces to Theorem 1 when $\\tau$ is a singleton, with the exact same assumptions. We are not aware of comparable results in the literature. The proof of Theorem 3 can be found in Appendix D. ", "page_idx": 4}, {"type": "text", "text": "To see why this result is surprising, let us first focus on the case where $\\mathcal{T}_{*}$ has a unique element $t_{*}$ , so that $Z_{+}\\,=\\,Z_{-}\\,\\stackrel{d}{=}\\,\\|Z\\|_{2}^{2}$ where $Z\\sim\\mathcal{N}(0,\\Sigma^{-1/2}(t_{*})G(t_{*})\\Sigma^{-1/2}(t_{*}))$ . Now consider the oracle procedure, which knows beforehand what the optimal feature map $t_{*}$ is, and outputs $t_{*}$ and a minimizer of $R_{n}(t_{*},w)$ . Theorem 3 says that, up to a factor of $t w o$ , the asymptotic quantiles of the excess risk of ERM, which needs to learn over the large class $\\cup_{t\\in\\mathcal{T}}\\{x\\mapsto\\langle w,\\phi_{t}(x)\\rangle\\mid w\\in\\mathbb{R}^{d}\\}$ , coincide with those of the oracle procedure (by Theorem 1), which only needs to learn over the linear class $\\left\\{x\\mapsto\\langle w,\\phi_{t_{*}}(x)\\rangle\\mid w\\in\\dot{\\mathbb{R}^{d}}\\right\\}!;$ ", "page_idx": 4}, {"type": "text", "text": "More generally, Theorem 3 establishes that asymptotically, any ERM picks a near-optimal feature map with probability one. It furthers shows that the asymptotic quantiles of the excess risk of any sequence of ERMs is controlled from above and below by those of the extrema of the limiting Gaussian process of $(G_{n}(t))_{t\\in\\tau}$ on the set of optimal feature maps $\\mathcal{T}_{*}$ . This is surprising, as it implies that asymptotically, and outside of its role in determining whether the assumptions of Theorem 3 hold, the global complexity of the set $\\tau$ is irrelevant to the excess risk of ERM. ", "page_idx": 4}, {"type": "text", "text": "Finally, we note that the Glivenko-Cantelli and Donsker assumptions in Theorem 3 can equivalently be viewed as restrictions on the size of $\\tau$ , for distribution and process dependent notions of size. We refer the reader to the books [VW96; GN21] for more on this connection. With this observation, we state the main takeaway from Theorem 3 as follows. ", "page_idx": 4}, {"type": "text", "text": "Asymptotically, if $\\tau$ is not too large, the excess risk of ERM depends, at worst, only on the complexity of the set of optimal feature maps $\\mathcal{T}_{*}$ , and is independent of the global complexity of $\\tau$ . ", "page_idx": 4}, {"type": "text", "text": "3.2 Non-asymptotic result ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The result in Theorem 3 hints at a dramatic localization phenomenon, whereby the influence of the size and complexity of the collection of feature maps $(\\phi_{t})_{t\\in{\\cal T}}$ on the excess risk of ERM vanishes as $n\\to\\infty$ under appropriate assumptions. The root of this localization phenomenon is the first statement of Theorem 3: eventually, ERM picks near-optimal feature maps with probability approaching one. For small enough sample sizes however, it is clear that ERM is likely to select suboptimal feature maps, so that this localization phenomenon cannot hold uniformly over $n$ . This raises a host of questions: (i) How fast, as measured by the sample size, does ERM learn the optimal feature map? (ii) What is the effect of this localization on the rate of decay of the excess risk of ERM non-asymptotically? (iii) What properties of the feature maps $(\\phi_{t})_{t\\in{\\cal T}}$ influence these rates? ", "page_idx": 5}, {"type": "text", "text": "Our answers to these questions in this very general setting are formally expressed in Theorem 4 below. To state it, we define the following parameter ", "page_idx": 5}, {"type": "equation", "text": "$$\nL:=\\operatorname*{sup}\\mathrm{E}\\,\\Big[\\Big({\\sum_{t\\in\\mathcal{T}}\\langle v_{t},\\Sigma^{-1/2}(t)\\phi_{t}(X)\\rangle^{2}-1}\\Big)^{2}\\Big],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the supremum is taken over vectors $(v_{t})_{t\\in\\tau}$ such that $\\begin{array}{r}{\\sum_{t\\in\\mathcal{T}}\\lVert v_{t}\\rVert_{2}^{2}\\,=\\,1}\\end{array}$ . For $n\\,\\in\\,\\mathbb{N}$ and $\\delta\\in(0,1)$ , we define the set function $F_{n,\\delta}$ , for any subset $s\\subset\\tau$ , by ", "page_idx": 5}, {"type": "equation", "text": "$$\nF_{n,\\delta}(S):=\\left\\{t\\in\\mathcal{T}\\,\\Big|\\,\\,R(t,w_{*}(t))-R_{*}\\leq2\\cdot(n\\delta)^{-1}\\cdot\\mathrm{E}[\\operatorname*{sup}_{s\\in\\mathcal{S}}G_{n}^{2}(s)]\\right\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This map acts as a contraction as shown in the next lemma, whose proof is deferred to Appendix E.   \nFor a function $f$ , we use $f^{k}$ to denote $f^{k}(x):=f(f^{k-1}(x))$ with ${\\dot{f}}^{0}(x):=x$ . ", "page_idx": 5}, {"type": "text", "text": "Lemma 1. Let $n\\in\\mathbb N$ , $\\delta\\in(0,1)$ , and assume that $\\mathcal{T}_{*}\\neq\\mathcal{O}$ . Then for all $k\\in\\mathbb{N}\\cup\\{0\\}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ F_{n,\\delta}^{k+1}(\\mathcal{T})\\subseteq F_{n,\\delta}^{k}(\\mathcal{T}).}\\\\ &{\\bullet\\ I\\!f\\exists\\,n_{0},B\\ s u c h\\ t h a t\\ E[\\operatorname*{sup}_{t\\in\\mathcal{T}}G_{n}^{2}(t)]\\leq B\\,f o r\\,a l l\\ n\\geq n_{0},\\,t h e n\\bigcap_{n\\geq1}F_{n,\\delta}^{k}(\\mathcal{T})=\\mathcal{T}_{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "With these definitions, we now state the second main result of the paper. A proof is in Appendix F. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4. Assume that $\\tau_{*}~\\neq~\\infty$ $\\mathrm{~\\boldmath~\\xi~}\\otimes,\\;\\mathrm{E}[Y^{2}]\\;<\\;\\infty,\\;\\forall(t,j)\\;\\in\\;{\\mathcal T}\\times[d],\\;\\mathrm{E}[\\phi_{t,j}^{2}(X)]\\;<\\;\\infty$ , and $\\begin{array}{r}{\\mathrm{E}[\\|g(t,(X,Y))\\|_{\\Sigma^{-1}(t)}^{2}]<\\infty}\\end{array}$ . Let $\\delta\\in(0,1)$ and $k\\in\\mathbb{N}$ . If, for some $t_{*}\\in\\tau_{*}$ , $n$ satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\nn\\ge64\\,\\mathrm{E}_{t\\in\\mathcal{T}}^{\\left[\\operatorname*{sup}_{h}\\Lambda_{n}(t)\\right]}+(128L+11)\\log(4/\\delta)+2\\cdot\\delta^{-2}\\cdot\\mathrm{E}[\\operatorname*{sup}_{t\\in\\mathcal{T}\\backslash\\mathcal{T}_{*}}\\Delta_{n}(t,t_{*})],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "then, with probability at least $1-\\delta$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{t}_{n}\\in F_{n,\\delta/2k}^{k}(\\mathcal{T})=:S_{n,\\delta,k},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\hat{t}_{n},\\hat{w}_{n})\\leq8k\\cdot(n\\delta)^{-1}\\cdot\\mathrm{E}[\\operatorname*{sup}_{s\\in S_{n,\\delta,k}}G_{n}^{2}(s)],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the processes $\\Lambda_{n},\\,\\Delta_{n}$ , and $G_{n}$ are as in (5) and (6). ", "page_idx": 5}, {"type": "text", "text": "We make a few remarks before interpreting the content of the theorem. First, we note that when the index set $\\tau$ is a singleton, the last term in the sample size restriction vanishes, while the first matches the sample size restriction from Theorem 2 after an application of Lemma 3 below; further taking $k=1$ in Theorem 4 recovers the upper bound on the excess risk of Theorem 2 up to a factor of two. Theorem 4 may therefore be viewed as a broad generalization of Theorem 2. Second, under Assumption 1 below, and by the second item of Lemma 1, the upper bound on the excess risk in Theorem 4 eventually matches the main term in the asymptotic rate of Theorem 3 as can be seen from (7), in the same way that Theorem 2 achieves this when compared with Theorem 1. Finally, the statement of Theorem 4 is very general, and in fact, too general for us to be able to interpret it precisely. As such, we will discuss it in the context of the following assumption. ", "page_idx": 5}, {"type": "text", "text": "Assumption 1. There exists constants $C_{\\Lambda}$ , $C_{\\Delta}$ , and $C_{G}$ independent of the sample size, but possibly dependent on the remaining parameters of the problem, such that for all $n\\in\\mathbb N$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{E}[\\operatorname*{sup}_{t\\in{\\mathcal{T}}}\\Lambda_{n}(t)]\\leq C_{\\Lambda},\\quad\\mathrm{E}\\Bigg[\\operatorname*{sup}_{t\\in{\\mathcal{T}}\\backslash{\\mathcal{T}}_{*}}\\Delta_{n}(t,t_{*})\\Bigg]\\leq C_{\\Delta},\\quad\\mathrm{E}[\\operatorname*{sup}_{t\\in{\\mathcal{T}}}G_{n}^{2}(t)]\\leq C_{G},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\Lambda_{n},\\Delta_{n}$ , and $G_{n}$ are as in (5) and (6). ", "page_idx": 5}, {"type": "text", "text": "These assumptions can be equivalently viewed as a restriction on the appropriately measured size of the index set $\\tau$ [VW96; GN21], and are slightly stronger than the assumptions of Theorem 3. Nevertheless, they always hold for finite index sets, and we will derive in Section 4 explicit estimates of the constants in Assumption 1 in terms of moments of the feature maps and target. ", "page_idx": 6}, {"type": "text", "text": "Let us now interpret the content of Theorem 4, which comes with a free parameter $k$ , in the context of Assumption 1. We fix $k$ here, and discuss its choice below. First, recalling the definition of $F_{n,\\delta}$ , this result says that above a certain sample size, both the suboptimality of the feature map picked by ERM and its excess risk decay at the fast rate $n^{-1}$ , answering the first question we raised at the beginning of the section. Second, this result provides an upper bound on the excess risk of ERM that depends on the index set $\\tau$ only through the size of shrinking subsets $S_{n,\\delta,k}$ , which might be large for small $n$ , but which by Lemma 1 converge to the set of optimal feature maps $\\tau_{*}$ as $n\\to\\infty$ . This transparently shows the effect of the localization phenomenon on the rate of decay of the excess risk of ERM, answering the second question we raised. Finally, looking at the definition of $S_{n,\\delta,k}$ , this result identifies the size of the sublevel sets of the suboptimality function $R(t,w_{*}(t))-R_{*}$ defined over feature maps as a key proprety of the collection of feature maps $(\\phi_{t})_{t\\in{\\mathcal{T}}}$ that influences the rate of convergence of the excess risk of ERM in this setting, answering the final question we raised. ", "page_idx": 6}, {"type": "text", "text": "Finally, let us turn to the choice of $k$ . Practically, we select the one that minimizes the bound on the excess risk. Looking at the first item of Lemma 1, this optimal $k$ balances the following trade-off: on the one hand, for small $k$ , applications of $F_{n,\\delta/2k}$ constrain the input set more severely, but only a few iterations are performed; on the other hand, larger values of $k$ allow more iterations, but at the cost of more weakly constraining the input set per application. ", "page_idx": 6}, {"type": "text", "text": "Stepping back, there are two main takeaways from Theorem 4. Firstly, and on a conceptual level, it shows that feature learning is easy when the suboptimality function $\\dot{R}(t,w_{*}(t))-R_{*}$ , defined over the set of features maps, has small sublevel sets. Secondly, and on a technical level, it provides a template which can be used to derive more explicit excess risk bounds on ERM given estimates on the expected suprema of the relevant empirical processes. Deriving such accurate estimates for infinite $\\tau$ is a highly non-trivial task, and cannot be done at the level of generality we have been operating at. The case of finite $\\tau$ however is tractable in a general setting as we discuss in the next section. ", "page_idx": 6}, {"type": "text", "text": "4 Case study: Finite index sets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we focus on the case where the index set $\\tau$ is finite, and aim, among other things, at establishing explicit estimates on the various expected suprema appearing in Theorem 4 in terms of moments of the feature maps and of the target. This problem becomes tractable in the case of finite $\\tau$ because, roughly speaking, a worst-case analysis still yields non-trivial upper bounds. This is decidedly not the case when $\\tau$ is infinite, in which case these expected suprema can be infinite. ", "page_idx": 6}, {"type": "text", "text": "We start with a slight strengthening of Theorem 3, whose assumptions reduce to simple moments conditions when $\\tau$ is finite. A proof is in Appendix H. ", "page_idx": 6}, {"type": "text", "text": "Corollary 1. Assume that $\\tau$ is finite, for all $\\begin{array}{r}{(t,j)\\in\\mathcal{T}\\times[d],\\,\\mathrm{E}\\big[\\phi_{t,j}^{2}(X)\\big]<\\infty,\\,\\mathrm{E}\\big[Y^{2}\\big]<\\infty,}\\end{array}$ , and for all $t\\in\\mathcal{T},\\,\\mathrm{E}[\\vert\\vert g(t,(X,Y))\\vert\\vert_{\\Sigma^{-1}(t)}^{2}]<\\infty$ . Then ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\mathrm{P}\\big(\\hat{t}_{n}\\notin T_{*}\\big)=0.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Furthermore, for any $\\delta\\in(0,1)$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\cdot Q_{Z^{-}}(1-\\delta)\\leq\\operatorname*{lim}_{n\\to\\infty}n\\cdot Q_{\\xi(\\hat{t}_{n},\\hat{w}_{n})}(1-\\delta)\\leq\\operatorname*{lim}_{n\\to\\infty}\\operatorname*{sup}n\\cdot Q_{\\xi(\\hat{t}_{n},\\hat{w}_{n})}(1-\\delta)\\leq\\frac{1}{2}\\cdot Q_{Z^{+}}(1-\\delta),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "wher $\\begin{array}{r}{\\varrho\\;Z^{-}:=\\operatorname*{min}_{s\\in\\mathcal{T}_{*}}\\|Z_{s}\\|_{2}^{2},\\;Z^{+}:=\\operatorname*{max}_{s\\in\\mathcal{T}_{*}}\\|Z_{s}\\|_{2}^{2}}\\end{array}$ , and the random vectors $(Z_{t})_{t\\in{\\mathcal{T}}}$ are jointly Gaussian with mean zero and $\\begin{array}{r}{\\mathrm{E}\\big[Z_{t}Z_{s}^{T}\\big]=\\mathrm{E}\\big[\\Sigma^{-1/2}(t)g(t,(X,Y))g(s,(X,Y))^{T}\\Sigma^{-1/2}(s)\\big]}\\end{array}$ for all $t,s\\in\\mathcal{T}$ . In particular, $i f\\mathcal{T}_{*}=\\dot{\\{}t_{*}}\\}$ , then ", "page_idx": 6}, {"type": "equation", "text": "$$\nn\\cdot\\mathcal{E}(\\hat{t}_{n},\\hat{w}_{n})\\stackrel{d}{\\rightarrow}\\frac{1}{2}\\cdot\\Vert Z\\Vert_{2}^{2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $Z\\sim\\mathcal{N}(0,\\Sigma^{-1/2}(t_{*})G(t_{*})\\Sigma^{-1/2}(t_{*}))$ . ", "page_idx": 6}, {"type": "text", "text": "The conclusions of Corollary 1 differ from those of Theorem 3 in two aspects. First, the feature map picked by ERM is guaranteed to be optimal rather than near-optimal with probability converging to one. Second, the upper bound on the asymptotic quantiles is improved by a factor of two, yielding the exact distribution of the rescaled excess risk when $\\mathcal{T}_{*}$ is a singleton. ", "page_idx": 7}, {"type": "text", "text": "Making Theorem 4 more explicit is a more laborious task. We recall here two known results that allow us to accomplish this. We start with the following bounds on the expectation of the supremum of a finitely-indexed empirical process, which we will later use to bound the suprema of the processes $(G_{n}(s))_{s\\in S}$ and $(\\Delta_{n}(t\\bar{,}t_{*}))_{t\\in\\bar{T}\\backslash T_{*}}$ appearing in Theorem 4. A proof can be found in Appendix G. ", "page_idx": 7}, {"type": "text", "text": "Lemma 2. Let $n,d\\in\\mathbb{N}$ , and let $Z$ be a random element taking value in a set $\\mathcal{Z}$ , and let $(Z_{i})_{i=1}^{n}$ be i.i.d. samples with the same distribution as $Z$ . Let $\\mathcal{F}$ be a finite collection of $\\mathbb{R}^{d}$ -valued measurable functions. Define ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sigma^{2}(\\mathcal{F}):=\\operatorname*{max}_{f\\in\\mathcal{F}}\\mathrm{E}\\big[\\|f(Z)-\\mathrm{E}[f(Z)]\\|_{2}^{2}\\big],\\qquad r(\\mathcal{F}):=\\mathrm{E}\\bigg[\\operatorname*{max}_{(i,f)\\in[n]\\times\\mathcal{F}}\\|f(Z_{i})-\\mathrm{E}[f(Z)]\\|_{2}^{2}\\bigg]^{1/2},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and let $\\begin{array}{r}{E_{n}(f):=\\sqrt{n}\\cdot(n^{-1}\\sum_{i=1}^{n}f(Z_{i})-\\mathrm{E}[f(Z)])}\\end{array}$ . Then, we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\cdot\\sigma(\\mathcal{F})+\\frac{1}{4}\\cdot\\frac{r(\\mathcal{F})}{\\sqrt{n}}\\leq\\mathrm{E}\\biggl[\\operatorname*{max}_{f\\in\\mathcal{F}}\\bigl\\|E_{n}(f)\\bigr\\|_{2}^{2}\\biggr]^{1/2}\\leq c(|\\mathcal{F}|)\\cdot\\sigma(\\mathcal{F})+c^{2}(|\\mathcal{F}|)\\cdot\\frac{r(\\mathcal{F})}{\\sqrt{n}},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $c(m):=5{\\sqrt{1+\\log m}}$ . ", "page_idx": 7}, {"type": "text", "text": "Lemma 2 allows us to compute the expected supremum of a finitely-indexed empirical process, up to log factors in the size of the index set. It is known that these factors cannot be removed from the upper bound nor added to the lower bound without more assumptions, we refer the reader to a related discussion in [Tro16]. Finally, while the term $r(\\mathcal{F})$ might grow with $n$ , by bounding the maximum with the sum, it grows at most as $\\sqrt{n}$ . In many applications however, the random vectors $f(Z)$ are bounded almost surely, so that $r(\\mathcal{F})$ is of order one, which justifies our presentation choice. ", "page_idx": 7}, {"type": "text", "text": "The second result we recall is the expectation version of a one sided Matrix Bernstein inequality due to Tropp [Tro15]. We use it below to bound the supremum of the process $(\\Lambda_{n}(t))_{t\\in{\\mathcal{T}}}$ appearing in Theorem 4. We do not known of a matching non-asymptotic lower bound, but an asymptotic one is known [EME24, Proposition 17]. Upper and lower bounds similar to those of Lemma 2 hold if one considers the expected operator norm instead of only the maximum eigenvalue [Tro16, Section 7]. ", "page_idx": 7}, {"type": "text", "text": "Lemma 3 ([Tro15], Theorem 6.6.1.). Let $n,d\\in\\mathbb{N}$ and for each $i\\,\\in\\,[n]$ , let $Z_{i}\\,\\in\\,\\mathbb{R}^{d\\times d}$ be i.i.d. positive semi-definite matrices with the same distribution as $Z$ . Define ", "page_idx": 7}, {"type": "equation", "text": "$$\nV:=\\operatorname{E}{\\Big[}{\\big(}\\operatorname{E}[Z]-Z{\\big)}^{2}{\\Big]},\\quad W_{n}:={\\sqrt{n}}\\cdot{\\Big(}\\operatorname{E}[Z]-{\\frac{1}{n}}\\sum_{i=1}^{n}Z_{i}{\\Big)}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Then, we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname{E}[\\lambda_{\\operatorname*{max}}(W_{n})]\\leq{\\sqrt{2\\lambda_{\\operatorname*{max}}(V)\\log(e d)}}+{\\frac{\\lambda_{\\operatorname*{max}}(\\operatorname{E}[Z])\\log(e d)}{3{\\sqrt{n}}}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Equipped with these estimates, we may now control the expected suprema of the empirical processes appearing in Theorem 4. To apply Lemma 2, define the following classes, for $s\\subset\\tau$ and $t_{*}\\in\\tau_{*}$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{G}(S):=\\Big\\{(x,y)\\mapsto\\Sigma^{-1/2}(s)g(s,(x,y))\\,\\Big|\\,s\\in S\\Big\\},}\\\\ &{\\mathcal{D}(t_{*}):=\\Bigg\\{(x,y)\\mapsto\\frac{\\ell(\\langle w_{*}(t),\\phi_{t}(x)\\rangle,\\,y)-\\ell(\\langle w_{*}(t_{*}),\\phi_{t_{*}}(x)\\rangle,\\,y)}{R(t,w_{*}(t))-R_{*}}\\,\\Big|\\,t\\in\\mathcal{T}\\setminus\\mathcal{T}_{*}\\Bigg\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Applying Lemma 2 on $\\mathcal{G}(S)$ bounds the expected supremum of the process $\\textstyle(G_{n}(s))_{s\\in S}$ while applying it on $\\mathcal{D}(t_{*})$ bounds that of $(\\Delta_{n}(t,t_{*}))_{t\\in\\mathcal{T}\\backslash\\mathcal{T}_{*}}$ . To control the supremum of $(\\Lambda_{n}(t))_{t\\in{\\mathcal{T}}}$ , the key idea is to notic\u221ae that it can be expressed as the maximum eigenvalue of a block diagonal matrix whose blocks are $\\sqrt{n}(I-\\Sigma^{-1/2}(t)\\bar{\\Sigma}_{n}(t)\\Sigma^{-1/2}(t))$ . Looking at Lemma 3, the relevant parameter is therefore a block diagonal matrix $V$ with the following blocks ", "page_idx": 7}, {"type": "equation", "text": "$$\nV(t):=\\operatorname{E}\\biggl[\\Bigl(\\Sigma^{-1/2}(t)\\phi_{t}(X)\\phi_{t}(X)^{T}\\Sigma^{-1/2}(t)-I\\Bigr)^{2}\\biggr].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "As the bound in Lemma 3 depends only on the maximum eigenvalue of $V$ , the ordering of the blocks does not matter. Putting together these estimates, we arrive at a fully explicit version of Theorem 4. ", "page_idx": 7}, {"type": "text", "text": "Corollary 2. Assume that $\\tau$ is finite and that for all $\\begin{array}{r}{(t,j)\\in\\mathcal{T}\\times[d],\\,\\mathrm{E}\\big[\\phi_{t,j}^{4}(X)\\big]<\\infty,\\,\\mathrm{E}\\big[Y^{4}\\big]<\\infty}\\end{array}$ . Let $\\delta\\in(0,1)$ , $k\\in[1+|T\\setminus T_{*}|],$ , and $c(\\cdot),\\sigma^{2}(\\cdot),r(\\cdot)$ as in Lemma 2. If, for some $t_{*}\\in\\tau_{*}$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{n\\geq(512\\lambda_{\\operatorname*{max}}(V)+6)\\log(e d|\\mathcal{T}|)+(128L+11)\\log(4/\\delta)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+16\\cdot\\delta^{-1}\\cdot c(|\\mathcal{T}|)\\sigma^{2}(\\mathcal{D}(t_{*}))+8\\cdot\\delta^{-1/2}\\cdot c^{2}(|\\mathcal{T}|)r(\\mathcal{D}(t_{*})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "then, with probability at least $1-\\delta$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\hat{t}_{n}\\in\\widetilde{F}_{n,\\delta/2k}^{k}(\\mathcal{T})=:\\widetilde{S}_{n,\\delta,k},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\widehat{t}_{n},\\widehat{w}_{n})\\leq8k\\cdot(n\\delta)^{-1}\\cdot A(\\widetilde{S}_{n,\\delta,k}),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where, for $s\\subset\\tau$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A(S):=c^{2}(S)\\cdot\\Big(\\sigma(\\mathcal{G}(S))+c(S)\\cdot\\frac{r(\\mathcal{G}(S))}{\\sqrt{n}}\\Big)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and $\\widetilde{F}_{n,\\delta}(S)$ is the same as $F_{n,\\delta}(S)$ defined in (10) but with $A(S)$ replacing $\\operatorname{E}[\\operatorname{sup}_{s\\in S}G_{n}^{2}(s)]$ . ", "page_idx": 8}, {"type": "text", "text": "We conclude this section with a few remarks about Corollary 2; a proof sketch is in Appendix I. The set function $A(S)$ controlling the contraction rate of the map $\\widetilde{F}_{n,{\\delta}}$ as well as the excess risk, has a pleasantly simple form. To first order, and ignoring constants,  it is given by ", "page_idx": 8}, {"type": "equation", "text": "$$\n(1+\\log|S|)\\cdot\\operatorname*{max}_{s\\in S}\\mathrm{E}\\Big[\\|g(s,(X,Y))\\|_{\\Sigma^{-1}(s)}^{2}\\Big].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "As such, as $n\\to\\infty$ and by Lemma 1, the upper bound on the excess risk in Corollary 2 matches the main term in the asymptotic rate derived in Theorem 3, as can be seen from (8). As the sets $\\widetilde{S}_{n,\\delta,k}$ are shrinking with $n$ , the above expression clearly shows the decaying effect of the global com plexity of $\\tau$ on the excess risk. Finally, we note that the restriction on $k$ in Corollary 2 is there only because after at most that many iterations, a fixed point is reached, and further iterations worsen the bound. ", "page_idx": 8}, {"type": "text", "text": "Example 1 (Sparse linear regression). Consider the classical sparse linear regression problem, and in particular the best subset selection procedure [Mil02]. This procedure corresponds to ERM over the restricted linear class $\\{x\\mapsto\\langle w,\\phi({\\bar{x}})\\rangle\\mid\\|w\\|_{0}\\leq s\\}$ in the linear regression setup of Section 2, where $\\|w\\|_{0}$ is the number of non-zero entries of $w$ and $\\bar{s}\\in[d]$ is a user-chosen sparsity level. ", "page_idx": 8}, {"type": "text", "text": "The literature on sparse linear regression is too vast for us to survey it in detail here. We briefly mention that there has been a strong renewed interest in the best subset selection procedure starting with the work of Bertsimas, King, and Mazumder [BKM16] who showed that it can be computed in reasonable time on moderately-sized instances despite being NP-hard [Nat95]. Since then, a large literature has emerged that devises increasingly efficient methods for its computation [e.g. $\\mathrm{Hua}{+}18$ ; BV20; HM20]. Our interest here however is in the statistical performance of the best subset selection procedure; we discuss the closest results in this direction below. ", "page_idx": 8}, {"type": "text", "text": "To see how the sparse linear regression problem fits in our feature learning setting, notice that $\\{x\\mapsto\\langle w,\\phi(x)\\rangle\\mid{\\overline{{\\|}}}w\\|_{0}\\leq s\\}={\\overline{{\\{x\\mapsto\\langle v,\\phi_{t}(x)\\rangle\\mid(t,v)\\in{\\mathcal{T}}\\times\\mathbb{R}^{s}\\}}}}$ where $\\tau$ is the set of all subsets of $[d]$ of size $s$ , and $\\phi_{t}(x):=(\\phi_{j_{1}}(x),\\phi_{j_{2}}(x),\\dots,\\phi_{j_{s}}(x))\\in\\mathbb{R}^{s}$ where $(j_{1},j_{2},\\dots,j_{s})$ are the elements of $t$ in increasing order. Each $(t,v)\\in\\mathcal{T}\\times\\mathbb{R}^{s}$ is associated to a unique $w\\in\\mathbb{R}^{d}$ satisfying $\\|w\\|_{0}\\leq\\,s$ given by $w_{j_{i}}~=~v_{i}$ for $(j_{1},\\dots,j_{s})$ the increasing elements of $t$ and $i\\,\\in\\,[s]$ , and zero otherwise. As such, Corollaries 1 and 2 are immediately applicable and provide general statements on the performance of an arbitrary empirical risk minimizer. ", "page_idx": 8}, {"type": "text", "text": "In particular, when there is a unique risk minimizer $w_{*}$ satisfying $\\|w_{*}\\|_{0}\\,=\\,s$ , the first item of Corollary 1 guarantees that we asymptotically exactly recover the support of $w_{*}$ . Furthermore, if the model is well-specified, i.e. $\\bar{\\varepsilon}:=\\bar{\\langle w_{*},\\phi(X)\\rangle}\\,\\bar{-}Y$ is independent of $X$ and satisfies $\\operatorname{E}\\left[\\varepsilon^{2}\\right]=\\sigma^{2}<\\infty$ , then the second item of Corollary 1 yields, for any $\\delta\\in(0,0.1)$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}n\\cdot Q_{\\mathcal{E}(\\hat{w}_{n})}(1-\\delta)\\asymp\\sigma^{2}(s+\\log(1/\\delta)).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Non-asymptotically, the first item of Corollary 2 shows that if the sample size $n$ further satisfies ", "page_idx": 8}, {"type": "equation", "text": "$$\nn>2\\cdot(\\gamma\\delta)^{-1}\\cdot A(\\widetilde{F}_{n,\\delta/2k}^{k-1}(\\mathcal{T})),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\begin{array}{r}{\\gamma:=\\operatorname*{min}_{t\\in\\mathcal{T}\\backslash\\mathcal{T}_{*}}\\{R(t,w_{*}(t))-R_{*}\\}>0}\\end{array}$ , then with probability at least $1-\\delta$ , ERM coincides with the oracle procedure which knows beforehand the support of $w_{*}$ , and performs ERM on the optimal $s$ -dimensional linear class. ", "page_idx": 9}, {"type": "text", "text": "The closest existing result in the literature is due to $[\\mathrm{She}{+13}]$ , who arrived at comparable conclusions but in a substantially different setting. In particular, this result was derived in the setting $n,d\\to\\infty$ , with an implicit assumption on the distribution of $\\phi(X)$ [She $+13$ , Equation (2)], and dealt with the in-sample prediction risk instead of the excess risk. Another closely related result is due to [RWY11] who showed that the rescaled expected minimax risk in a well-specified fixed-design setting is, up to constants, $\\sigma^{2}s\\log(d/s)$ , see also [Bac24, Chapter 8]. Our results show that for large enough $n$ , in the random-design setting, and when focusing on a single instance, the $\\log(d/s)$ factor is unnecessary. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Broadly speaking, there are two main conclusions one can draw from this work. Firstly, in the large sample regime and under mild assumptions, asking a model to additionally pick a feature map on top of learning a linear predictor has a negligible effect on the excess risk of ERM on regression problems with square loss. Secondly, for moderate sample sizes, the magnitude of this effect depends on the size of the sublevel sets of the suboptimality function $t\\mapsto R(t,\\bar{w}_{*}(t))-R_{*}$ . Plainly, learning feature maps is easy when only a small subset of them is good, as the bad ones can be quickly discarded. ", "page_idx": 9}, {"type": "text", "text": "The most tantalizing aspect of our results is their potential in explaining the experiments in $[Z\\mathrm{ha}+21]$ . It was shown there that complex neural networks trained by ERM were able to achieve good performance despite being expressive enough to fti random labels. This is paradoxical if one assumes that the performance of ERM is driven by the complexity of the model class. Our results refute this assumption for feature-learning-based models, of which neural networks are an example. While there are many works offering explanations for this (see e.g. [BMR21] for a survey), we are not aware of one that shows the vanishing influence of the size of the model class on the excess risk as Theorems 3 and 4 show. Formally connecting our statements to these experiments is beyond what we achieved here, yet, we believe that the new perspective we took might generate useful insights in this area. ", "page_idx": 9}, {"type": "text", "text": "We conclude by outlining a few limitations of our work. First, we do not deal with the question of how to solve the ERM problem. Our setting is so general that such a question cannot be meaningfully tackled. Second, we mention that it is a priori unclear whether ERM is an optimal procedure, in a minimax sense, for the model classes we consider; we suspect that recently developed tools might be relevant to address this question [Mou22]. Finally, while we focused on the case of regression with square loss, this was mostly done to simplify the presentation. Indeed, the only property of the loss used in the proofs is the exactness of its second order Taylor expansion. This is however not necessary if one can control the error term from above and below. It is known how to do this for many loss functions [e.g. OB21; EE23], and most importantly for logistic regression [Bac10; Bac14]. We have purposefully selected generic notation to make translating such arguments easier. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Resources used in preparing this research were provided in part by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute. CM acknowledges the support of the Natural Sciences and Engineering Research Council of Canada (NSERC), RGPIN-2021-03445. MAE was partially supported by NSERC Grant [2019-06167], CIFAR AI Chairs program, and CIFAR AI Catalyst grant. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[AC11] J.-Y. Audibert and O. Catoni. \u201cRobust linear least squares regression\u201d. In: (2011).   \n[Aud07] J.-y. Audibert. \u201cProgressive Mixture Rules Are Deviation Suboptimal\u201d. In: Advances in Neural Information Processing Systems. 2007. URL: https : / / papers . nips . cc/paper_files/paper/2007/hash/ef575e8837d065a1683c022d2077d342- Abstract.html.   \n$[\\mathrm{Ba}+22]$ J. Ba, M. A. Erdogdu, T. Suzuki, Z. Wang, D. Wu, and G. Yang. \u201cHigh-dimensional asymptotics of feature learning: How one gradient step improves the representation\u201d. In: Advances in Neural Information Processing Systems 35 (2022), pp. 37932\u201337946.   \n[Bac10] F. Bach. \u201cSelf-concordant analysis for logistic regression\u201d. In: (2010).   \n[Bac14] F. Bach. \u201cAdaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression\u201d. In: The Journal of Machine Learning Research 15.1 (2014), pp. 595\u2013627.   \n[Bac24] F. Bach. Learning theory from first principles. MIT press, 2024.   \n[BBM05] P. L. Bartlett, O. Bousquet, and S. Mendelson. \u201cLocal rademacher complexities\u201d. In: (2005).   \n[BKM16] D. Bertsimas, A. King, and R. Mazumder. \u201cBest subset selection via a modern optimization lens\u201d. In: (2016).   \n[BLM00] S. Boucheron, G. Lugosi, and P. Massart. \u201cA sharp concentration inequality with applications\u201d. In: Random Structures & Algorithms 16.3 (2000), pp. 277\u2013292.   \n[BLM13] S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: A Nonasymptotic Theory of Independence. OUP Oxford, 2013. ISBN: 9780199535255. URL: https: //books.google.ca/books?id $=$ 5oo4YIz6tR0C.   \n[BM06] P. L. Bartlett and S. Mendelson. \u201cEmpirical minimization\u201d. In: Probability theory and related fields 135.3 (2006), pp. 311\u2013334.   \n[BMR21] P. L. Bartlett, A. Montanari, and A. Rakhlin. \u201cDeep learning: a statistical viewpoint\u201d. In: Acta numerica 30 (2021), pp. 87\u2013201.   \n[Bou02] O. Bousquet. \u201cA Bennett concentration inequality and its application to suprema of empirical processes\u201d. In: Comptes Rendus Mathematique 334.6 (2002), pp. 495\u2013500.   \n[BV20] D. Bertsimas and B. Van Parys. \u201cSparse High-dimensional Regression: Exact Scalable Algorithms and Phase Transitions\u201d. In: The Annals of Statistics 48.1 (2020), pp. 300\u2013323.   \n[COB19] L. Chizat, E. Oyallon, and F. Bach. \u201cOn lazy training in differentiable programming\u201d. In: Advances in neural information processing systems 32 (2019).   \n[DG12] V. De la Pena and E. Gin\u00e9. Decoupling: from dependence to independence. Springer Science & Business Media, 2012.   \n[EE23] A. El Hanchi and M. A. Erdogdu. \u201cOptimal Excess Risk Bounds for Empirical Risk Minimization on $p$ -Norm Linear Regression\u201d. In: Advances in Neural Information Processing Systems 36 (2023).   \n[EME24] A. El Hanchi, C. Maddison, and M. Erdogdu. \u201cMinimax Linear Regression under the Quantile Risk\u201d. In: The Thirty Seventh Annual Conference on Learning Theory. PMLR. 2024, pp. 1516\u20131572.   \n[GA11] M. G\u00f6nen and E. Alpayd\u0131n. \u201cMultiple kernel learning algorithms\u201d. In: The Journal of Machine Learning Research 12 (2011), pp. 2211\u20132268.   \n[Gho+19] B. Ghorbani, S. Mei, T. Misiakiewicz, and A. Montanari. \u201cLimitations of lazy training of two-layers neural network\u201d. In: Advances in Neural Information Processing Systems 32 (2019).   \n[GN21] E. Gin\u00e9 and R. Nickl. Mathematical foundations of infinite-dimensional statistical models. Cambridge university press, 2021.   \n$[\\mathrm{He}+16]$ K. He, X. Zhang, S. Ren, and J. Sun. \u201cDeep residual learning for image recognition\u201d. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2016, pp. 770\u2013778.   \n[HKZ12] D. Hsu, S. M. Kakade, and T. Zhang. \u201cRandom design analysis of ridge regression\u201d. In: Conference on learning theory. JMLR Workshop and Conference Proceedings. 2012, pp. 9\u20131.   \n[HM20] H. Hazimeh and R. Mazumder. \u201cFast best subset selection: Coordinate descent and local combinatorial optimization algorithms\u201d. In: Operations Research 68.5 (2020), pp. 1517\u20131537.   \n$[\\mathrm{Hua}+18]$ J. Huang, Y. Jiao, Y. Liu, and X. Lu. \u201cA constructive approach to $L_{0}$ penalized regression\u201d. In: Journal of Machine Learning Research 19.10 (2018), pp. 1\u201337.   \n[JRT08] A. Juditsky, P. Rigollet, and A. B. Tsybakov. \u201cLearning by Mirror Averaging\u201d. In: The Annals of Statistics (Oct. 2008). DOI: 10.1214/07-AOS546. V. Koltchinskii and S. Mendelson. \u201cBounding the smallest singular value of a random matrix without concentration\u201d. In: International Mathematics Research Notices 2015.23 (2015), pp. 12991\u201313008. V. Koltchinskii. \u201cLocal Rademacher complexities and oracle inequalities in risk minimization\u201d. In: (2006).   \n[Kol11] V. Koltchinskii. Oracle inequalities in empirical risk minimization and sparse recovery problems: \u00c9cole D\u2019\u00c9t\u00e9 de Probabilit\u00e9s de Saint-Flour XXXVIII-2008. Vol. 2033. Springer Science & Business Media, 2011.   \n[KRV22] V. Kanade, P. Rebeschini, and T. Vaskevicius. \u201cExponential tail local rademacher complexity risk bounds without the bernstein condition\u201d. In: arXiv preprint arXiv:2202.11461 (2022).   \n[KSH12] A. Krizhevsky, I. Sutskever, and G. E. Hinton. \u201cImagenet classification with deep convolutional neural networks\u201d. In: Advances in neural information processing systems 25 (2012).   \n[Lan+04] G. R. Lanckriet, N. Cristianini, P. Bartlett, L. E. Ghaoui, and M. I. Jordan. \u201cLearning the kernel matrix with semidefinite programming\u201d. In: Journal of Machine learning research 5.Jan (2004), pp. 27\u201372.   \n[LBH15] Y. LeCun, Y. Bengio, and G. Hinton. \u201cDeep learning\u201d. In: nature 521.7553 (2015), pp. 436\u2013444.   \n[LC06] E. L. Lehmann and G. Casella. Theory of point estimation. Springer Science & Business Media, 2006.   \n[LL20] G. Lecu\u00e9 and M. Lerasle. \u201cRobust machine learning by median-of-means: theory and practice\u201d. In: (2020).   \n[LM09] G. Lecu\u00e9 and S. Mendelson. \u201cAggregation via Empirical Risk Minimization\u201d. en. In: Probability Theory and Related Fields (Nov. 1, 2009). DOI: 10.1007/s00440-008- 0180-8.   \n[LM16a] G. Lecu\u00e9 and S. Mendelson. Learning Subgaussian Classes : Upper and Minimax Bounds. Sept. 17, 2016. DOI: 10.48550/arXiv.1305.4825.   \n[LM16b] G. Lecu\u00e9 and S. Mendelson. \u201cPerformance of Empirical Risk Minimization in Linear Aggregation\u201d. In: Bernoulli (Aug. 2016). DOI: 10.3150/15-BEJ701.   \n[LM16c] G. Lecu\u00e9 and S. Mendelson. \u201cPerformance of empirical risk minimization in linear aggregation\u201d. In: (2016).   \n[LM19] G. Lugosi and S. Mendelson. \u201cRisk minimization by median-of-means tournaments\u201d. In: Journal of the European Mathematical Society 22.3 (2019), pp. 925\u2013965.   \n[LRS15a] T. Liang, A. Rakhlin, and K. Sridharan. \u201cLearning with Square Loss: Localization through Offset Rademacher Complexity\u201d. en. In: Proceedings of The 28th Conference on Learning Theory. June 26, 2015. URL: https://proceedings.mlr.press/v40/ Liang15.html.   \n[LRS15b] T. Liang, A. Rakhlin, and K. Sridharan. \u201cLearning with square loss: Localization through offset rademacher complexity\u201d. In: Conference on Learning Theory. PMLR. 2015, pp. 1260\u20131285.   \n[Men14] S. Mendelson. \u201cLearning without Concentration\u201d. en. In: Proceedings of The 27th Conference on Learning Theory. May 29, 2014. URL: https://proceedings.mlr. press/v35/mendelson14.html.   \n[Mil02] A. Miller. Subset selection in regression. chapman and hall/CRC, 2002.   \n[Mou22] J. Mourtada. \u201cExact Minimax Risk for Linear Least Squares, and the Lower Tail of Sample Covariance Matrices\u201d. In: The Annals of Statistics (Aug. 2022). DOI: 10.1214/ 22-AOS2181.   \n[Nat95] B. K. Natarajan. \u201cSparse approximate solutions to linear systems\u201d. In: SIAM journal on computing 24.2 (1995), pp. 227\u2013234.   \n[OB21] D. M. Ostrovskii and F. Bach. \u201cFinite-sample analysis of m-estimators using selfconcordance\u201d. In: (2021).   \n[Oli16] R. I. Oliveira. \u201cThe lower tail of random quadratic forms with applications to ordinary least squares\u201d. In: Probability Theory and Related Fields 166 (2016), pp. 1175\u20131194.   \n[RWY11] G. Raskutti, M. J. Wainwright, and B. Yu. \u201cMinimax rates of estimation for highdimensional linear regression over $e l l_{q}$ -balls\u201d. In: IEEE transactions on information theory 57.10 (2011), pp. 6976\u20136994.   \n[Sau18] A. Saumard. \u201cOn optimality of empirical risk minimization in linear aggregation\u201d. In: (2018).   \n[SD16] A. Sinha and J. C. Duchi. \u201cLearning kernels with random features\u201d. In: Advances in neural information processing systems 29 (2016).   \n[She+13] X. Shen, W. Pan, Y. Zhu, and H. Zhou. \u201cOn constrained and regularized high-dimensional regression\u201d. In: Annals of the Institute of Statistical Mathematics 65.5 (2013), pp. 807\u2013 832.   \n[Tal96] M. Talagrand. \u201cNew concentration inequalities in product spaces\u201d. In: Inventiones mathematicae 126.3 (1996), pp. 505\u2013563.   \n[Tro15] J. A. Tropp. \u201cAn introduction to matrix concentration inequalities\u201d. In: Foundations and Trends in Machine Learning 8.1-2 (2015), pp. 1\u2013230.   \n[Tro16] J. A. Tropp. \u201cThe expected norm of a sum of independent random matrices: An elementary approach\u201d. In: High Dimensional Probability VII: The Cargese Volume. Springer. 2016, pp. 173\u2013202.   \n[Van00] A. W. Van der Vaart. Asymptotic statistics. Vol. 3. Cambridge university press, 2000.   \n$[\\mathrm{Vas}{+17}]$ A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. \u201cAttention is all you need\u201d. In: Advances in neural information processing systems 30 (2017).   \n[VC74] V. Vapnik and A. Chervonenkis. \u201cTheory of pattern recognition\u201d. In: (1974).   \n[VW96] A. W. Van Der Vaart and J. A. Wellner. Weak convergence and empirical processes. Springer, 1996.   \n[Wai19] M. J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint. Vol. 48. Cambridge university press, 2019.   \n[Whi82] H. White. \u201cMaximum likelihood estimation of misspecified models\u201d. In: Econometrica: Journal of the econometric society (1982), pp. 1\u201325. C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. \u201cUnderstanding deep learning (still) requires rethinking generalization\u201d. In: Communications of the ACM 64.3 (2021), pp. 107\u2013115. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Proof of Theorem 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Let $A_{n}$ denote the event that $\\Sigma_{n}$ is invertible. By the weak law of large numbers, $\\Sigma_{n}$ converges to $\\Sigma$ in probability so that $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty}\\mathrm{P}(A_{n}^{c})=0.}\\end{array}$ Now on the event $A_{n}$ , we have by (2) ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sqrt{n}\\cdot(\\hat{w}_{n}-w_{*})=\\Sigma_{n}^{-1}\\cdot(\\sqrt{n}\\cdot\\nabla R_{n}(w_{*})).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By the continuous mapping theorem, $\\Sigma_{n}^{-1}$ converges to $\\Sigma^{-1}$ in probability and by the central limit theorem ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sqrt{n}\\cdot\\nabla R_{n}(w_{*})\\stackrel{d}{\\rightarrow}\\mathcal{N}(0,G).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore, by Slutsky\u2019s theorem ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sqrt{n}\\cdot(\\hat{w}_{n}-w_{*})\\xrightarrow{d}\\mathcal{N}(0,\\Sigma^{-1}G\\Sigma^{-1}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now since the risk is quadratic and the gradient vanishes at $w_{*}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\nn\\cdot[R(\\hat{w})-R(w_{*})]=\\frac{1}{2}\\cdot\\|\\sqrt{n}\\cdot(\\hat{w}-w_{*})\\|_{\\Sigma}^{2}\\overset{d}{\\to}\\frac{1}{2}\\|Z\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $Z$ is as in the theorem, and where the last statement follows by the continuous mapping theorem. This proves the first statement. The bounds on the quantiles are a consequence of concentration bounds for the norm of Gaussian vectors [e.g. EME24, Corollary 33]. ", "page_idx": 13}, {"type": "text", "text": "B Proof of Theorem 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Denote by $A_{n}$ the event that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\lambda_{\\mathrm{min}}\\biggl(\\Sigma^{-1/2}\\Sigma_{n}\\Sigma^{-1/2}\\biggr)\\geq\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We show that under the sample size restriction, $\\mathrm{P}(A_{n})\\geq1-\\delta/2$ . Indeed we have the variational representation ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{max}}(I-\\Sigma^{-1/2}\\Sigma_{n}\\Sigma^{-1/2})=\\operatorname*{sup}_{v\\in S^{d-1}}\\frac1n\\sum_{i=1}^{n}1-\\langle v,\\Sigma^{-1/2}\\phi(X_{i})\\rangle^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Each element in the sum is upper bounded by 1, and the variance parameter in Bousquet\u2019s concentration inequality [Bou02] is given by the parameter $L$ in the statement of the theorem. Applying this inequality yields that with probability at least $1-\\delta/2$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{max}}\\big(I-\\Sigma^{-1/2}\\Sigma_{n}\\Sigma^{-1/2}\\big)\\leq2\\,\\ensuremath{\\operatorname{E}}\\Big[\\lambda_{\\operatorname*{max}}\\big(I-\\Sigma^{-1/2}\\Sigma_{n}\\Sigma^{-1/2}\\big)\\Big]+\\sqrt{\\frac{2L\\log(2/\\delta)}{n}}+\\frac{4\\log(2/\\delta)}{3n}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Using Lemma 3 to upper bound the above expectation, and replacing the sample size $n$ in the resulting inequality with the minimal allowed by the theorem proves that $\\mathrm{\\bar{P}}(A_{n})\\stackrel{*}{\\geq}1-\\delta/2$ for all sample sizes allowable by the theorem. Now on this event we have, using (3), ", "page_idx": 13}, {"type": "equation", "text": "$$\nR(\\hat{w}_{n})-R(w_{*})=\\frac{1}{2}\\cdot\\|\\Sigma_{n}^{-1}\\nabla R_{n}(w_{*})\\|_{\\Sigma}^{2}\\leq2\\cdot\\|\\nabla R_{n}(w_{*})\\|_{\\Sigma^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "An elementary calculation shows ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname{E}\\big[\\|\\nabla R_{n}(w_{*})\\|_{\\Sigma^{-1}}^{2}\\big]=n^{-1}\\operatorname{E}\\big[\\|g(X,Y)\\|_{\\Sigma^{-1}}^{2}\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "so that an application of Markov\u2019s inequality yields that there is an event $B_{n}$ that holds with probability at least $1-\\delta/2$ and on which ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla R_{n}(w_{*})\\|_{\\Sigma^{-1}}^{2}\\leq2\\cdot(n\\delta)^{-1}\\cdot\\mathrm{E}\\big[\\|g(X,Y)\\|_{\\Sigma^{-1}}^{2}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The union bound $\\mathrm{P}(A_{n}\\cap B_{n})=1-\\mathrm{P}(A_{n}\\cup B_{n})\\geq1-\\delta$ finishes the proof. ", "page_idx": 13}, {"type": "text", "text": "C Main Lemma ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We state here a core lemma, which we use in many of our proofs. To state it, we define, for a function $F:S\\rightarrow\\mathbb{R}$ on a subset $S\\subseteq{\\mathcal{T}}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|F\\|_{\\infty}:=\\operatorname*{sup}_{s\\in\\mathcal{S}}|F(s)|,\\quad\\|F\\|_{\\infty,-}:=\\operatorname*{sup}_{s\\in\\mathcal{S}}\\{-F(s)\\},\\quad\\|F\\|_{\\infty,+}:=\\operatorname*{sup}_{s\\in\\mathcal{S}}F(s),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the first quantity is the $\\ell^{\\infty}$ norm of the function $F$ , and the remaining are one-sided variants of it. The processes appearing in the next statement are defined in (5) and (6). ", "page_idx": 14}, {"type": "text", "text": "Lemma 4. Assume that $\\mathcal{T}_{*}\\neq\\mathcal{O}$ and let $t_{*}\\in\\tau_{*}$ . On the event that $\\|n^{-1/2}\\Delta_{n}(\\cdot,t_{*})\\|_{\\infty,+}<1$ and $\\|n^{-1/2}\\Lambda_{n}\\|_{\\infty,+}<1$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nR(\\widehat{t}_{n},w_{*}(\\widehat{t}_{n}))-R_{*}\\leq\\frac{1}{2}\\cdot\\frac{1}{1-\\|n^{-1/2}\\Delta_{n}(\\cdot,t_{*})\\|_{\\infty,+}}\\cdot\\frac{1}{1-\\|n^{-1/2}\\Lambda_{n}\\|_{\\infty,+}}\\cdot(n^{-1}G_{n}^{2}(\\widehat{t}_{n})),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\frac{n^{-1}G_{n}^{2}(\\widehat{t}_{n})}{(1+\\|n^{-1/2}\\Lambda_{n}\\|_{\\infty,-})^{2}}\\leq R(\\widehat{t}_{n},\\widehat{w}_{n})-R(\\widehat{t}_{n},w_{*}(\\widehat{t}_{n}))\\leq\\frac{1}{2}\\cdot\\frac{n^{-1}G_{n}^{2}(\\widehat{t}_{n})}{(1-\\|n^{-1/2}\\Lambda_{n}\\|_{\\infty,+})^{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. To lighten the notation, we drop the dependence on $n$ , and write $\\hat{t}$ instead of $\\hat{t}_{n}$ . We start with the first statement. First, we note that if $\\hat{t}\\in\\mathcal{T}_{*}$ , then the statement holds trivially as the left-hand side is zero, so we only consider the other case in what follows. For any $t\\in\\mathcal T$ , define ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{w}(t)\\in\\operatorname*{argmin}_{w\\in\\mathbb R^{d}}R_{n}(t,w),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the choice of minimizer is arbitrary. With this definition, we have $\\hat{w}_{n}\\,=\\,\\hat{w}(\\hat{t})$ . Now, by definition of ERM, ", "page_idx": 14}, {"type": "equation", "text": "$$\nR_{n}(\\hat{t},\\hat{w}(\\hat{t}))-R_{n}(t_{*},w_{*}(t_{*}))\\leq0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "On the other hand, for any $t\\in\\tau\\backslash\\tau_{*}$ , we have the decomposition ", "page_idx": 14}, {"type": "equation", "text": "$$\nR_{n}(t,\\hat{w}(t))-R_{n}(t_{*},w_{*})=[R_{n}(t,\\hat{w}(t))-R_{n}(t,w_{*}(t))]+[R_{n}(t,w_{*}(t))-R_{n}(t_{*},w_{*}(t_{*}))].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We study each of the terms of (14) separately, and we start with the first. Note that since we are in the event ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{t\\in\\mathcal{T}}\\lambda_{\\operatorname*{min}}(\\Sigma^{-1/2}(t)\\Sigma_{n}(t)\\Sigma^{-1/2}(t))=1-\\|n^{-1/2}\\Lambda_{n}\\|_{\\infty,+}>0,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "the sample covariance matrices $\\Sigma_{n}(t)$ are invertible for all $t\\in\\mathcal T$ , so that $\\hat{w}(t)$ is uniquely defined and satisfies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{w}(t)=w_{*}(t)-\\Sigma_{n}^{-1}(t)\\nabla_{w}R_{n}(t,w_{*}(t)).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Furthermore, since the function $w\\,\\mapsto\\,R_{n}(t,w)$ is quadratic in $w$ and its gradient vanishes at its minimizer $\\hat{w}(t)$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nR_{n}(t,\\hat{w}(t))-R_{n}(t,w_{*}(t))=-\\frac{1}{2}\\|\\hat{w}(t)-w_{*}(t)\\|_{\\Sigma_{n}(t)}^{2}=-\\frac{1}{2}\\|\\nabla_{w}R_{n}(t,w_{*}(t))\\|_{\\Sigma_{n}^{-1}(t)}^{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last equality follows from (15). To bound this last term, define ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widetilde\\Sigma_{n}(t):=\\Sigma^{-1/2}(t)\\Sigma_{n}(t)\\Sigma^{-1/2}(t).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then we have, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|\\nabla_{w}R_{n}(t,w_{*}(t))\\|_{\\Sigma_{n}^{-1}(t)}^{2}=\\Big\\{\\Sigma^{-1/2}(t)\\nabla_{w}R_{n}(t,w_{*}(t))\\Big\\}^{T}\\widetilde{\\Sigma}_{n}^{-1}(t)\\Big\\{\\Sigma^{-1/2}(t)\\nabla_{w}R_{n}(t,w_{*}(t))\\Big\\}}}\\\\ &{}&{\\leq\\lambda_{\\operatorname*{max}}(\\widetilde{\\Sigma}_{n}^{-1}(t))\\cdot\\|\\nabla_{w}R_{n}(t,w_{*}(t))\\|_{\\Sigma^{-1}(t)}^{2}}\\\\ &{}&{=\\frac{1}{1-\\lambda_{\\operatorname*{max}}(I-\\widetilde{\\Sigma}_{n}(t))}\\cdot(n^{-1}G_{n}^{2}(t))}\\\\ &{}&{\\leq\\frac{1}{1-\\|n^{-1/2}\\Lambda_{n}\\|_{\\infty,+}}\\cdot(n^{-1}G_{n}^{2}(t)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Finally, the second term of (14) is lower bounded by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{n}(t,w_{*}(t))-R_{n}(t_{*},w_{*}(t_{*})))=(1-n^{-1/2}\\Delta_{n}(t,t_{*}))[R(t,w_{*}(t))-R_{*}]}\\\\ &{\\phantom{\\sum_{0}}\\ge(1-\\|n^{-1/2}\\Delta_{n}(\\cdot,t_{*})\\|_{\\infty,+})[R(t,w_{*}(t))-R_{*}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining (17) and (16) lower bounds the first term of (14), while (18) lower bounds the second.   \nCombining the resulting lower bound on (14) with (13) and rearranging yields the first statement. ", "page_idx": 15}, {"type": "text", "text": "For the upper bound in the second statement, note that for all $t\\in\\mathcal T$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{R(t,\\hat{w}(t))-R(t,w_{*}(t))=\\frac{1}{2}\\cdot\\|\\hat{w}(t)-w_{*}(t)\\|_{\\Sigma(t)}^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{1}{2}\\cdot\\|\\Sigma_{n}^{-1}(t)\\nabla_{w}R_{n}(t,w_{*}(t))\\|_{\\Sigma(t)}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\le\\frac{1}{2}\\cdot\\lambda_{\\operatorname*{max}}(\\widetilde{\\Sigma}_{n}^{-2}(t))\\cdot\\|\\nabla_{w}R_{n}(t,w_{*}(t))\\|_{\\Sigma^{-1}(t)}^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{1}{2}\\cdot\\frac{1}{(1-\\|n^{-1/2}\\Lambda_{n}\\|_{\\infty,+})^{2}}\\cdot(n^{-1}G_{n}^{2}(t)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the second line follows from (15). In particular the inequality holds for $\\hat{t}$ . The lower bound holds by a similar argument. ", "page_idx": 15}, {"type": "text", "text": "D Proof of Theorem 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Consistency of $\\hat{t}_{n}$ . We want to show that, as $n\\to\\infty$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nR(\\hat{t}_{n},w_{*}(\\hat{t}_{n}))-R_{*}\\stackrel{p}{\\rightarrow}0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using the notation introduced in Appendix C, the Glivenko-Cantelli assumptions in Theorem 3 amount to the statements that, for some $t_{*}\\in\\tau_{*}$ , and as $n\\to\\infty$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|n^{-1/2}\\Lambda_{n}\\|_{\\infty}\\xrightarrow{p}0,\\quad\\|n^{-1/2}\\Delta_{n}(\\cdot,t_{*})\\|_{\\infty}\\xrightarrow{p}0,\\quad\\|n^{-1/2}G_{n}\\|_{\\infty}\\xrightarrow{p}0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $A_{n}$ denote the event that both $\\|n^{-1/2}\\Lambda_{n}\\|_{\\infty}<1$ and $\\|n^{-1/2}\\Delta_{n}(\\cdot,t_{*})\\|_{\\infty}<1$ . The union bound and (20) show that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\mathrm{P}(A_{n}^{c})=0\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Furthermore, on the event $A_{n}$ , the first bound of Lemma 4 holds, and bounding $n^{-1}G_{n}^{2}(\\hat{t}_{n})$ by $\\|n^{-1/2}G_{n}\\|_{\\infty}^{2}$ yields that on $A_{n}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\nR(\\hat{t}_{n},w_{*}(\\hat{t}_{n}))-R_{*}\\leq\\frac{1}{2}\\cdot\\frac{1}{1-\\|n^{-1/2}\\Delta_{n}(\\cdot,t_{*})\\|_{\\infty}}\\cdot\\frac{1}{1-\\|n^{-1/2}\\Lambda_{n}\\|_{\\infty}}\\cdot\\|n^{-1/2}G_{n}\\|_{\\infty}^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now let $\\varepsilon\\,>\\,0$ , and denote by $B_{n}(\\varepsilon)$ the event that the right hand side of (22) is strictly larger than $\\varepsilon$ . Then the statements (20) together with the continuous mapping theorem show that $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty}\\mathrm{P}(B_{n}(\\varepsilon))=0}\\end{array}$ . Therefore, again by (22), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname{P}\\big(R(\\widehat{t}_{n},w_{*}(\\widehat{t}_{n}))-R_{*}>\\varepsilon\\big)\\leq\\operatorname{P}(B_{n}(\\varepsilon))+\\operatorname{P}(A_{n}^{c}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and taking $n\\to\\infty$ proves (19). ", "page_idx": 15}, {"type": "text", "text": "Asymptotic quantiles. We start with the upper bound. We have the simple decomposition ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{n\\cdot\\bigl[R(\\hat{t}_{n},\\hat{w}_{n})-R_{*}\\bigr]=n\\bigl[R(\\hat{t}_{n},\\hat{w}_{n})-R(\\hat{t}_{n},w_{*}(\\hat{t}_{n}))\\bigr]+n\\bigl[R(\\hat{t}_{n},w_{*}(\\hat{t}_{n}))-R_{*}\\bigr].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now on the event $A_{n}$ defined above, we have, by an application of Lemma 4, combining the two bounds in the lemma along with (23), that the rescaled excess risk is upper bounded by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\cdot\\frac{1}{1-\\Vert n^{-1/2}\\Lambda_{n}\\Vert_{\\infty}}\\cdot\\left(\\frac{1}{1-\\Vert n^{-1/2}\\Delta_{n}(\\cdot,t_{*})\\Vert_{\\infty}}+\\frac{1}{1-\\Vert n^{-1/2}\\Lambda_{n}\\Vert_{\\infty}}\\right)\\cdot G_{n}^{2}(\\hat{t}_{n}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "From the Glivenko-Cantelli assumptions (20), the first three factors converge in probability to 1. Our aim will be to bound the upper tail of the last factor, which will imply a bound on the upper tail of the rescaled excess risk. ", "page_idx": 16}, {"type": "text", "text": "We briefly make explicit the Donsker assumption before deriving this bound. Both define and note ", "page_idx": 16}, {"type": "equation", "text": "$$\nG_{n}(t,v):=\\sqrt{n}\\cdot\\langle v,\\Sigma^{-1/2}(t)\\nabla_{w}R_{n}(t,w_{*}(t))\\rangle,\\qquad G_{n}(t)=\\operatorname*{sup}_{v\\in S^{d-1}}G_{n}(t,v),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $S^{d-1}$ is the Euclidean unit sphere in $\\mathbb{R}^{d}$ . As pointed out in Section 3, the processes $G_{n}(t)$ are partial suprema of the empirical processes $G_{n}(t,v)$ . The Donsker assumption of the theorem states that the empirical processes $G_{n}(t,v)$ take value in the space of bounded functions on $\\tau\\times S^{d-1}$ , equipped with the $\\ell^{\\infty}(\\mathcal{T}\\times S^{d-1})$ norm and the metric it induces, and converge weakly to their unique Gaussian limit $(Z(t,v))_{(t,v)\\in\\mathcal{T}\\times S^{d-1}}$ as $n\\to\\infty$ . We define the partial supremum with respect to $v\\in S^{d-1}$ of the latter by $Z(t):=\\operatorname*{sup}_{v\\in S^{d-1}}Z(t,v)$ in analogy with the definition of $G_{n}(t)$ . ", "page_idx": 16}, {"type": "text", "text": "We now upper bound the upper tail of $G_{n}^{2}(\\hat{t}_{n})$ in (24). Let $(\\varepsilon_{k})_{k=1}^{\\infty}$ be a decreasing sequence of positive numbers such that $\\varepsilon_{k}\\to0$ as $k\\rightarrow\\infty$ , and define the sets ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{T}_{*}(\\varepsilon):=\\{t\\in\\mathcal{T}\\mid R(t,w_{*}(t))-R_{*}\\leq\\varepsilon\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "as well as the function $F_{k}:\\ell^{\\infty}(\\mathcal{T}\\times S^{d-1})\\to\\mathbb{R}$ by ", "page_idx": 16}, {"type": "equation", "text": "$$\nF_{k}(z):=\\operatorname*{sup}_{s\\in{\\mathcal{T}}_{*}(\\varepsilon_{k})}\\operatorname*{sup}_{v\\in S^{d-1}}z(s,v).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note on the one hand that $\\cap_{k\\geq1}{\\mathcal{T}}_{*}(\\varepsilon_{k})={\\mathcal{T}}_{*}$ , and on the other that $F_{k}$ is continuous for all $k\\in\\mathbb{N}$ , and in fact Lipschitz. Indeed, let $z,z^{\\prime}\\in\\ell^{\\infty}(\\mathcal{T}\\times S^{d-1})$ . Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n|F_{k}(z)-F_{k}(z^{\\prime})|=\\left|\\operatorname*{sup}_{s\\in\\mathcal{T}_{*}(\\varepsilon_{k})}\\operatorname*{sup}_{v\\in\\mathbb{S}^{d-1}}z(s,v)-\\operatorname*{sup}_{s\\in\\mathcal{T}_{*}(\\varepsilon_{k})}\\operatorname*{sup}_{v\\in\\mathbb{S}^{d-1}}z^{\\prime}(s,v)\\right|\\leq\\|z-z^{\\prime}\\|_{\\infty}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now let $k\\in\\mathbb{N}$ and $x\\in[0,\\infty)$ . Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{P}\\big(G_{n}^{2}(\\hat{t}_{n})>x\\big)=\\mathrm{P}\\big(\\{G_{n}^{2}(\\hat{t}_{n})>x\\}\\cap\\{\\hat{t}_{n}\\in\\mathcal{T}_{*}(\\varepsilon_{k})\\}\\big)+\\mathrm{P}\\big(\\{G_{n}^{2}(\\hat{t}_{n})>x\\}\\cap\\{\\hat{t}_{n}\\notin\\mathcal{T}_{*}(\\varepsilon_{k})\\}\\big\\}\\big)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathrm{P}\\big(\\{G_{n}^{2}(\\hat{t}_{n})>x\\}\\cap\\{\\hat{t}_{n}\\in\\mathcal{T}_{*}(\\varepsilon_{k})\\}\\big)+\\mathrm{P}\\big(\\{\\hat{t}_{n}\\notin\\mathcal{T}_{*}(\\varepsilon_{k})\\}\\big)}\\\\ &{\\qquad\\qquad\\leq\\mathrm{P}\\bigg(\\underset{s\\in\\mathcal{T}_{*}(\\varepsilon_{k})}{\\operatorname*{sup}}G_{n}^{2}(s)>x\\bigg)+\\mathrm{P}\\big(R(\\hat{t}_{n},w_{*}(\\hat{t}_{n}))-R_{*}>\\varepsilon_{k}\\big)}\\\\ &{\\qquad\\qquad=\\mathrm{P}\\big(F_{k}^{2}(G_{n})>x\\big)+\\mathrm{P}\\big(R(\\hat{t}_{n},w_{*}(\\hat{t}_{n}))-R_{*}>\\varepsilon_{k}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "taking the limit as $n\\to\\infty$ , the first term converges, by the continuous mapping theorem, to the probability of the event $\\{F_{k}^{2}(Z)>x\\}$ , where $Z$ is the limiting Gaussian process discussed above, while the second term vanishes by the first part of Theorem 3. Therefore, for all $k\\in\\mathbb N$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\operatorname{P}\\left(G_{n}^{2}(\\hat{t}_{n})>x\\right)\\leq\\operatorname{P}\\left(\\operatorname*{sup}_{s\\in\\mathcal{T}_{*}(\\varepsilon_{k})}Z^{2}(s)>x\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Taking the limit as $k\\rightarrow\\infty$ , noticing that the events ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\{\\operatorname*{sup}_{s\\in T_{*}(\\varepsilon_{k})}Z^{2}(s)>x\\right\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "are nested, using the continuity of probability from above, and recalling that $\\cap_{k\\geq1}{\\mathcal{T}}_{*}(\\varepsilon_{k})={\\mathcal{T}}_{*}$ gives ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\operatorname{P}\\left(G_{n}^{2}(\\hat{t}_{n})>x\\right)\\leq\\operatorname{P}\\biggl(\\operatorname*{sup}_{s\\in\\mathcal{T}_{*}}Z^{2}(s)>x\\biggr).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using properties of the quantile function (e.g. [EME24, Lemma 20]) finishes the proof of the upper bound. For the lower bound, we make a similar argument. We have, by an application of Lemma 4, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{n\\cdot[R(\\hat{t}_{n},\\hat{w}_{n})-R_{*}]\\geq n\\cdot[R(\\hat{t}_{n},\\hat{w}_{n})-R(\\hat{t}_{n},w_{*}(\\hat{t}_{n}))]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\frac{1}{2}\\cdot\\frac{1}{(1+\\Vert n^{-1/2}\\Lambda_{n}\\Vert_{\\infty,-})^{2}}\\cdot G_{n}^{2}(\\hat{t}_{n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By the Glivenko-Cantelli assumption on $\\Lambda_{n}$ , the first two factors converge to $1/2$ . For the third, we will lower bound its upper tails, which will imply a lower bound on the upper tails of the rescaled excess risk. We let $(\\varepsilon_{k})_{k=1}^{\\infty}$ be a decreasing sequence of positive numbers such that $\\varepsilon_{k}\\,\\rightarrow\\,0$ as $k\\rightarrow\\infty$ , and define $H_{k}:\\^{\\tilde{\\ell}^{\\infty}}(\\mathcal{T}\\times S^{d-1})\\rightarrow\\mathbb{R}$ by ", "page_idx": 17}, {"type": "equation", "text": "$$\nH_{k}(z):=\\operatorname*{inf}_{t\\in T_{*}(\\varepsilon_{k})}\\operatorname*{sup}_{v\\in S^{d-1}}z(t,v),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the subsets $\\mathcal{T}_{*}(\\varepsilon_{k})$ are as defined in (25). Clearly, for $z,z^{\\prime}\\in\\ell^{\\infty}(\\mathcal{T}\\times S^{d-1})$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n|H_{k}(z)-H_{k}(z^{\\prime})|\\leq\\|z-z^{\\prime}\\|_{\\infty}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "so $H_{k}$ is Lipschitz and therefore continuous. Now ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{P}\\big(G_{n}^{2}(\\hat{t}_{n})>x\\big)\\geq\\mathrm{P}\\big(\\{G_{n}^{2}(\\hat{t}_{n})>x\\}\\cap\\{\\hat{t}_{n}\\in\\mathcal{T}_{*}(\\varepsilon_{k})\\}\\big)}\\\\ &{\\qquad\\qquad\\qquad\\geq\\mathrm{P}\\bigg(\\underset{s\\in\\mathcal{T}_{*}(\\varepsilon_{k})}{\\operatorname*{inf}}G_{n}^{2}(s)>x\\bigg)-\\mathrm{P}\\big(\\{\\hat{t}_{n}\\notin\\mathcal{T}_{*}(\\varepsilon_{k})\\}\\big)}\\\\ &{\\qquad\\qquad\\qquad=\\mathrm{P}\\big(H_{k}^{2}(G_{n})>x\\big)-\\mathrm{P}\\big(R(\\hat{t}_{n},w_{*}(\\hat{t}_{n}))-R_{*}>\\varepsilon_{k}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By the same argument as above, we obtain, as $n\\to\\infty$ , and for all $k\\in\\mathbb{N}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\operatorname{inf}_{\\mathbf{\\mu}}\\operatorname{P}\\left(G_{n}^{2}({\\hat{t}}_{n})>x\\right)\\geq\\operatorname{P}{\\binom{\\operatorname*{inf}_{\\substack{s\\in{\\mathcal{T}}_{*}(\\varepsilon_{k})}}}{s\\in\\mathcal{T}_{*}(\\varepsilon_{k})}}Z^{2}(s)>x\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Taking the limit as $k\\rightarrow\\infty$ , and noticing that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\bigcup_{k\\geq1}\\left\\{\\operatorname*{inf}_{s\\in{\\mathcal{T}}_{*}(\\varepsilon_{k})}Z^{2}(s)>x\\right\\}=\\left\\{\\operatorname*{inf}_{s\\in{\\mathcal{T}}_{*}}Z^{2}(s)>x\\right\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "proves that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\operatorname{inf}_{\\mathbf{\\pi}}\\mathrm{P}\\big(G_{n}^{2}(\\widehat{t}_{n})>x\\big)\\geq\\operatorname{P}\\biggr(\\operatorname*{inf}_{s\\in\\mathcal{T}_{*}}Z^{2}(s)>x\\biggr).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using properties of the quantile function (e.g. [EME24, Lemma 20]) finishes the proof of the lower bound. The estimates on the quantiles of $Z^{+}$ in Remark 1 are a consequence of standard Gaussian concentration, see [e.g. EME24, Appendix A.3]. Finally, the second statement in Remark 1 can be derived as follows. By definition, $Z(s)\\triangleq\\|Z_{s}\\|_{2}$ where $Z_{s}\\sim\\mathcal{N}(0,\\Sigma^{-1/2}(s)G(s)\\Sigma^{-1/2}(s))$ , so ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\underset{s\\in T_{*}}{\\operatorname*{max}}Z^{2}(s)\\right]\\leq\\mathbb{E}\\left[\\left(\\underset{s\\in T_{*}}{\\sum}Z^{2p}(s)\\right)^{1/p}\\right]}\\\\ &{\\qquad\\qquad\\leq\\left(\\underset{s\\in T_{*}}{\\sum}\\mathbb{E}\\big[Z^{2p}(s)\\big]\\right)^{1/p}}\\\\ &{\\qquad\\qquad\\leq32\\cdot p\\cdot\\left(\\underset{s\\in T_{*}}{\\sum}\\mathbb{E}\\big[Z^{2}(s)\\big]^{p}\\right)^{1/p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last estimate follows from Gaussian concentration. Taking $p=1+\\log|\\mathcal{T}_{*}|$ , and recalling that for $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , $\\|x\\|_{p}\\leq d^{1/p}\\|x\\|_{\\infty}$ yields the result. ", "page_idx": 17}, {"type": "text", "text": "E Proof of Lemma 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We prove the first statement by induction. For $k\\,=\\,0$ , this follows directly from the fact that by definition $F_{n,\\delta}^{0}(\\mathcal{T})=\\mathcal{T}$ and $F_{n,\\delta}({\\mathcal{T}})\\subseteq{\\mathcal{T}}$ . Now let $k\\in\\mathbb{N}$ and assume that the statement holds for $k-1$ . Let $s\\in F_{n,\\delta}^{k+1}(\\mathcal{T})$ . Then by definition ", "page_idx": 17}, {"type": "equation", "text": "$$\nR(s,w_{*}(s))-R_{*}\\leq2\\cdot(n\\delta)^{-1}\\cdot\\mathrm{E}[\\operatorname*{sup}_{s\\in F_{n,\\delta}^{k}(T)}G_{n}^{2}(s)]\\leq2\\cdot(n\\delta)^{-1}\\cdot\\mathrm{E}[\\operatorname*{sup}_{s\\in F_{n,\\delta}^{k-1}(T)}G_{n}^{2}(s)].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second inequality follows from the fact that by the induction hypothesis, $F_{n,\\delta}^{k}(\\mathcal{T})\\subseteq$ $F_{n,\\delta}^{k-1}(\\mathcal{T})$ , and that the supremum is increasing. Therefore $s\\,\\in\\,F_{n,{\\delta}}^{k}(\\mathcal{T})$ since the last inequality is the defining inequality for $F_{n,\\delta}^{k}(\\mathcal{T})$ . We now turn to the second statement. Fix $k$ and $\\delta$ . On the one hand, $\\begin{array}{r}{T_{*}\\subseteq\\bigcap_{n\\geq1}F_{n,\\delta}^{k}({\\cal T})}\\end{array}$ . On the other, for any $t\\in\\bigcap_{n\\geq1}F_{n,\\delta}^{k}(\\mathcal{T})$ , we have for all $n\\geq n_{0}$ , $R(t,w_{*}(t))-R_{*}\\leq2B\\cdot(n\\delta)^{-1}$ . Therefore $R(t,w_{*}(t))-R_{*}=0$ , and hence $t\\in\\tau_{*}$ . ", "page_idx": 17}, {"type": "text", "text": "F Proof of Theorem 4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Recall the notation introduced in Appendix C. Let $A_{n}(t_{*})$ be the event that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|n^{-1/2}\\Lambda_{n}\\|_{\\infty,+}\\leq1/2,\\quad\\mathrm{~and~}\\quad\\|n^{-1/2}\\Delta_{n}(\\cdot,t_{*})\\|\\leq\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We start by showing that under the sample size inequality stated in the theorem, there exists a $t_{*}\\in\\tau_{*}$ such that $\\mathrm{P}(A_{n}(t_{*}))\\geq1-\\delta/2$ . Indeed, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|n^{-1/2}\\Lambda_{n}\\|_{\\infty,+}=\\operatorname*{sup}_{(t,v)\\in(T,S^{d-1})}\\frac{1}{n}\\sum_{i=1}^{n}1-\\langle v,\\Sigma^{-1/2}(t)\\phi_{t}(X_{i})\\rangle^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The elements of this sum are bounded by 1, and the variance parameter of Bousquet\u2019s inequality [Bou02] is given by $L$ as defined in Section 3.2. Applying this inequality yields that with probability at least $1-\\delta/4$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|n^{-1/2}\\Lambda_{n}\\|_{\\infty,+}\\leq\\frac{2}{n^{1/2}}\\cdot\\mathrm{E}\\biggl[\\underset{t\\in\\mathcal{T}}{\\operatorname*{sup}}\\,\\Lambda_{n}(t)\\biggr]+\\sqrt{\\frac{2L\\log(4/\\delta)}{n}}+\\frac{4\\log(4/\\delta)}{3n}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Furthermore, by Markov\u2019s inequality, with probability at least $1-\\delta/4$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|n^{-1/2}\\Delta(\\cdot,t_{*})\\|_{\\infty,+}\\leq\\frac{4\\cdot\\mathrm{E}[\\operatorname*{sup}_{t\\in T\\backslash T_{*}}\\Delta(t,t_{*})]}{n^{1/2}\\cdot\\delta}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, when the inequality on the sample size stated in the theorem holds for some $t_{*}$ , the event $A_{n}(t_{*})$ holds with probability at least $1-\\delta/2$ . Now on this event, the first bound of Lemma 4 applies, and we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nR({\\hat{t}}_{n},{\\hat{w}}_{*}({\\hat{t}}_{n}))-R_{*}\\leq2\\cdot n^{-1}\\cdot G_{n}^{2}({\\hat{t}}_{n}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now we use the iterative localization method of Koltchinskii [Kol06]. Initially, we have no information about where $\\hat{t}_{n}$ is located aside from belonging to $\\tau$ , so we start with the bound ", "page_idx": 18}, {"type": "equation", "text": "$$\nR(\\hat{t}_{n},\\hat{w}_{*}(\\hat{t}_{n}))-R_{*}\\leq2\\cdot n^{-1}\\cdot\\operatorname*{sup}_{t\\in\\mathcal{T}}G_{n}^{2}(t).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using Markov\u2019s inequality, we have on an event $B_{n,1}$ which holds with probability at least $1-\\delta/2k$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\in\\mathcal{T}}G_{n}^{2}(t)\\leq2k\\cdot\\delta^{-1}\\cdot\\mathrm{E}[\\operatorname*{sup}_{t\\in\\mathcal{T}}G_{n}^{2}(t)].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Replacing in (28) yields that on the event $A_{n}(t_{*})\\cap B_{n,1}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\nR(\\hat{t}_{n},w_{*}(\\hat{t}_{n}))-R_{*}\\le4k\\cdot(n\\delta)^{-1}\\cdot\\operatorname{E}[\\operatorname*{sup}_{t\\in\\mathcal{T}}G_{n}^{2}(t)],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which shows that on this event, $\\hat{t}_{n}\\,\\in\\,F_{n,\\delta/2k}(\\mathcal{T})$ , by definition of the map $F_{n,\\delta/2k}$ . With this knowledge, we now reuse the bound (27) to obtain that on $A_{n}(t_{*})\\cap B_{n,1}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\nR({\\hat{t}}_{n},w_{*}({\\hat{t}}_{n}))-R_{*}\\leq2\\cdot n^{-1}\\cdot\\operatorname*{sup}_{t\\in F_{n,\\delta/2k}(\\mathcal{T})}G_{n}^{2}(t).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Iterating the procedure we just described $k$ times, we obtain that on an event $E\\,:=\\,A_{n}(t_{*})\\uparrow$ $(\\cap_{j=1}^{k}B_{n,j})$ , where $\\mathrm{P}(B_{n,j})\\ge1-\\delta/2k$ for all $j\\in[k]$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{t}_{n}\\in F_{n,\\delta/2k}^{k}(\\mathcal{T})=S_{n,\\delta,k},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "as well as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\in S_{n,\\delta,k}}G_{n}^{2}(t)\\leq2k\\cdot\\delta^{-1}\\cdot\\mathrm{E}[\\operatorname*{sup}_{t\\in S_{n,\\delta,k}}G_{n}^{2}(t)]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{P}(A_{n}(t_{*})\\cap(\\cap_{j=1}^{k}B_{n,k}))\\ge1-\\delta/2-\\sum_{j=1}^{k}\\delta/2k=1-\\delta,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "equation (29) proves the first statement of the theorem. For the second statement, we have on the same event $E$ , and combining the two upper bounds from Lemma 4, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\hat{t}_{n},\\hat{w}_{n})\\leq4\\cdot n^{-1}\\cdot G_{n}^{2}(\\hat{t}_{n})\\leq4\\cdot n^{-1}\\cdot\\operatorname*{sup}_{t\\in\\mathcal{S}_{n,\\delta,k}}G_{n}^{2}(t)\\leq8k\\cdot(n\\delta)^{-1}\\cdot\\operatorname{E}[\\operatorname*{sup}_{t\\in\\mathcal{S}_{n,\\delta,k}}G_{n}^{2}(t)],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we used (29) and (30) in the above inequalities. ", "page_idx": 18}, {"type": "text", "text": "G Proof of Lemma 2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We prove a slightly more general result, from which Lemma 2 can be immediately deduced. ", "page_idx": 19}, {"type": "text", "text": "Lemma 5. Let $n$ , $d\\in\\mathbb{N}$ and let $\\tau$ be a finite set. For each $(i,t)\\in[n]\\times T$ , let $Z_{i,t}\\in\\mathbb{R}^{d}$ be random vectors such that for each $t\\in\\mathcal T$ , $(Z_{i,t})_{i=1}^{n}$ are i.i.d. with the same distribution as $Z_{t}$ . For all $t\\in\\mathcal T$ , assume that $\\operatorname{E}[Z_{t}]=0$ , and define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sigma^{2}(\\mathcal{T}):=\\operatorname*{sup}_{t\\in\\mathcal{T}}\\mathbb{E}\\big[\\|Z_{t}\\|_{2}^{2}\\big],\\qquad r(\\mathcal{T}):=\\mathbb{E}\\bigg[\\operatorname*{sup}_{(i,t)\\in[n]\\times\\mathcal{T}}\\|Z_{i,t}\\|_{2}^{2}\\bigg]^{1/2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\cdot\\frac{\\sigma(T)}{n^{1/2}}+\\frac{1}{4}\\cdot\\frac{r(T)}{n}\\leq\\operatorname{E}\\left[\\operatorname*{sup}_{t\\in T}\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}Z_{i,t}\\right\\|_{2}^{2}\\right]^{1/2}\\leq C(T)\\cdot\\frac{\\sigma(T)}{n^{1/2}}+C^{2}(T)\\cdot\\frac{r(T)}{n},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $C(\\mathcal{T}):=5\\sqrt{1+\\log|\\mathcal{T}|}$ . ", "page_idx": 19}, {"type": "text", "text": "To prove Lemma 5, we need to recall a few preliminary results. The first is a classical symmetrization inequality, see e.g. [BLM13, Lemma 11.4] or [Wai19, Proposition 4.11] for a proof. ", "page_idx": 19}, {"type": "text", "text": "Lemma 6. For each $(i,t)\\in[n]\\times T$ , let $W_{i,t}\\in\\mathbb{R}^{d}$ be random vectors such that for each $t\\in\\mathcal T$ , $(W_{i,t})_{i=1}^{n}$ are i.i.d. with the same distribution as $W_{t}$ . Let $(\\varepsilon_{i})_{i=1}^{n}$ be independent Rademacher random variables, independent of the collection of random vectors $W_{i,t}$ . Define $\\overline{{W}}_{i,t}:=W_{i,t}-\\operatorname{E}[W_{t}]$ . Then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\operatorname{E}\\left[\\operatorname*{sup}_{t\\in\\mathcal{T}}\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\varepsilon_{i}\\overline{{W}}_{i,t}\\right\\|_{2}^{2}\\right]^{1/2}\\leq\\operatorname{E}\\left[\\operatorname*{sup}_{t\\in\\mathcal{T}}\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\overline{{W}}_{i,t}\\right\\|_{2}^{2}\\right]^{1/2}\\leq2\\operatorname{E}\\left[\\operatorname*{sup}_{t\\in\\mathcal{T}}\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\varepsilon_{i}W_{i,t}\\right\\|_{2}^{2}\\right]^{1/2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The second result we recall is the Khinchin-Kahane inequality, the specific form we require is obtained from De la Pena and Gin\u00e9 [DG12, Theorem 1.3.1] by setting $q=p$ and $p=2$ in that theorem, see also Boucheron, Lugosi, and Massart [BLM13, page 141]. ", "page_idx": 19}, {"type": "text", "text": "Lemma 7. For $i\\in[n]$ , let $z_{i}\\in\\mathbb{R}^{d}$ be fixed vectors. Let $(\\varepsilon_{i})_{i=1}^{n}$ be independent Rademacher random variables. Then for all $p\\geq2$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname{E}\\left[\\left\\|\\sum_{i=1}^{n}\\varepsilon_{i}z_{i}\\right\\|_{2}^{p}\\right]^{1/p}\\leq{\\sqrt{p-1}}\\cdot\\left(\\sum_{i=1}^{n}\\|z_{i}\\|_{2}^{2}\\right)^{1/2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "A straightforward consequence of Lemma 7 is the following result, which follows from the elementary observation that for a vector $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , $\\|x\\|_{\\infty}\\leq\\|x\\|_{p}\\leq d^{\\bar{1}/p}\\|x\\|_{\\infty}$ . ", "page_idx": 19}, {"type": "text", "text": "Lemma 8. For $(i,t)\\in[n]\\!\\times\\!T$ , let $z_{i,t}\\in\\mathbb{R}^{d}$ be fixed vectors. Let $(\\varepsilon_{i})_{i=1}^{n}$ be independent Rademacher random variables. Then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{E}\\left[\\underset{t\\in\\mathcal{T}}{\\operatorname*{sup}}\\left\\Vert\\sum_{i=1}^{n}\\varepsilon_{i}z_{i,t}\\right\\Vert_{2}^{2}\\right]^{1/2}\\leq\\frac{5}{2}\\sqrt{1+\\log\\vert\\mathcal{T}\\vert}\\cdot\\left(\\underset{t\\in\\mathcal{T}}{\\operatorname*{sup}}\\sum_{i=1}^{n}\\Vert z_{i,t}\\Vert_{2}^{2}\\right)^{1/2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Let $p\\geq1$ . Then, by Jensen\u2019s inequality and Lemma 7 ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{E}\\left[\\underset{t\\in\\mathcal{T}}{\\operatorname*{sup}}\\left\\|\\sum_{i=1}^{n}\\varepsilon_{i}z_{i,t}\\right\\|_{2}^{2}\\right]\\leq\\mathrm{E}\\left[\\left(\\underset{t\\in\\mathcal{T}}{\\sum}\\left\\|\\sum_{i=1}^{n}\\varepsilon_{i}z_{i,t}\\right\\|_{2}^{2p}\\right)^{1/p}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(\\underset{t\\in\\mathcal{T}}{\\sum}\\mathrm{E}\\left[\\left\\|\\sum_{i=1}^{n}\\varepsilon_{i}z_{i,t}\\right\\|_{2}^{2p}\\right]\\right)^{1/p}}\\\\ &{\\qquad\\qquad\\leq(2p-1)\\cdot\\left(\\underset{t\\in\\mathcal{T}}{\\sum}\\left\\{\\underset{i=1}{\\sum}\\left\\{\\sum_{i=1}^{n}\\left\\|z_{i,t}\\right\\|_{2}^{2}\\right\\}^{p}\\right\\}^{1/p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Recalling that $\\|x\\|_{p}\\leq d^{1/p}\\|x\\|_{\\infty}$ for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ and taking $p:=1+\\log|\\mathcal{T}|$ yields the result. ", "page_idx": 19}, {"type": "text", "text": "Lemma 9. For each $(i,t)\\in[n]\\times T$ , let $W_{i,t}\\in\\mathbb{R}$ be random variables such that for each $t\\in\\mathcal T$ , $(W_{i,t})_{i=1}^{n}$ are i.i.d. with the same distribution as $W_{t}$ , with $W_{t}\\geq0$ almost surely. Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{E}\\left[\\underset{t\\in\\mathcal{T}}{\\operatorname*{sup}}\\sum_{i=1}^{n}W_{i,t}\\right]^{1/2}\\leq\\left(\\underset{t\\in\\mathcal{T}}{\\operatorname*{sup}}\\sum_{i=1}^{n}\\mathrm{E}[W_{i,t}]\\right)^{1/2}+5\\sqrt{1+\\log|\\mathcal{T}|}\\cdot\\mathrm{E}\\left[\\underset{(i,t)\\in[n]\\times\\mathcal{T}}{\\operatorname*{sup}}W_{i,t}\\right]^{1/2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. We have by Jensen\u2019s inequality and Lemma 6, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{E}\\left[\\underset{t\\in T}{\\operatorname*{sup}}\\displaystyle\\sum_{i=1}^{n}W_{i,t}\\right]\\leq\\mathrm{E}\\left[\\underset{t\\in T}{\\operatorname*{sup}}\\middle|\\sum_{i=1}^{n}W_{i,t}-\\mathrm{E}[W_{i,t}]\\right|\\right]+\\underset{t\\in T}{\\operatorname*{sup}}\\displaystyle\\sum_{i=1}^{n}\\mathrm{E}[W_{i,t}],}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\,\\mathrm{E}\\left[\\underset{t\\in T}{\\operatorname*{sup}}\\middle|\\sum_{i=1}^{n}\\varepsilon_{i}W_{i,t}\\right|^{2}\\right]^{1/2}+\\underset{t\\in T}{\\operatorname*{sup}}\\displaystyle\\sum_{i=1}^{n}\\mathrm{E}[W_{i,t}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Conditioning on the random vectors $W_{i,t}$ , we have by Lemma 8 and the assumption $W_{i,t}\\geq0$ a.s. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{2\\,\\mathrm{E}\\left[\\underset{t\\in T}{\\operatorname*{sup}}\\left|\\sum_{i=1}^{n}\\varepsilon_{i}W_{i,t}\\right|^{2}\\right]^{1/2}}&{\\le5\\sqrt{(1+\\log|T|)}\\cdot\\left(\\underset{t\\in T}{\\operatorname*{sup}}\\sum_{i=1}^{n}W_{i,t}^{2}\\right)^{1/2},}\\\\ &{\\le5\\sqrt{1+\\log|T|}\\cdot\\left(\\underset{(i,t)\\in[n]\\times T}{\\operatorname*{sup}}W_{i,t}\\right)^{1/2}\\cdot\\bigg(\\underset{t\\in T}{\\operatorname*{sup}}\\sum_{i=1}^{n}W_{i,t}\\bigg)^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Taking expectation with respect to $W_{i,t}$ , and using the Cauchy-Schwartz inequality yields ", "page_idx": 20}, {"type": "equation", "text": "$$\n2\\operatorname{E}\\left[\\operatorname*{sup}_{t\\in\\mathcal{T}}\\left|\\sum_{i=1}^{n}\\varepsilon_{i}W_{i,t}\\right|^{2}\\right]^{1/2}\\leq\\sqrt{6(1+\\log|\\mathcal{T}|)}\\cdot\\operatorname{E}\\left[\\operatorname*{sup}_{(i,t)\\in[n]\\times\\mathcal{T}}Z_{i,t}\\right]^{1/2}\\cdot\\operatorname{E}\\left[\\operatorname*{sup}_{t\\in\\mathcal{T}}\\sum_{i=1}^{n}W_{i,t}\\right]^{1/2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Replacing in (31) and solving the resulting quadratic inequality yields the result. ", "page_idx": 20}, {"type": "text", "text": "Equipped with these results, we now prove Lemma 1. The proof idea is taken from [Tro16]. ", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma $^{l}$ . We start with the lower bound. We have on the one hand ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname{E}\\left[\\operatorname*{sup}_{t\\in T}\\left\\|{\\frac{1}{n}}\\sum_{i=1}^{n}Z_{i,t}\\right\\|_{2}^{2}\\right]\\geq\\operatorname*{sup}_{t\\in T}\\operatorname{E}\\left[\\left\\|{\\frac{1}{n}}\\sum_{i=1}^{n}Z_{i,t}\\right\\|_{2}^{2}\\right]=\\sigma^{2}({\\mathcal{T}}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "On the on other hand, by Lemma 6, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname{E}{\\left[\\operatorname*{sup}_{t\\in\\mathcal{T}}\\left\\Vert\\frac{1}{n}\\sum_{i=1}^{n}Z_{i,t}\\right\\Vert_{2}^{2}\\right]}^{1/2}\\geq\\frac{1}{2}\\operatorname{E}{\\left[\\operatorname*{sup}_{t\\in\\mathcal{T}}\\left\\Vert\\frac{1}{n}\\sum_{i=1}^{n}\\varepsilon_{i}Z_{i,t}\\right\\Vert_{2}^{2}\\right]}^{1/2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Define the random index ", "page_idx": 20}, {"type": "equation", "text": "$$\nI\\in\\underset{i\\in[n]}{\\mathrm{argmax}}\\operatorname*{max}_{t\\in\\mathcal{T}}\\lVert Z_{i,t}\\rVert_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Conditioning on $Z_{i,t}$ , we have by Jensen\u2019s inequality ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{E}\\left[\\underset{t\\in\\mathcal{T}}{\\operatorname*{sup}}\\bigg\\|\\frac{1}{n}\\sum_{i=1}^{n}\\varepsilon_{i}Z_{i,t}\\bigg\\|_{2}^{2}\\right]\\ge\\underset{t\\in\\mathcal{T}}{\\operatorname*{sup}}\\mathrm{E}\\left[\\left\\|\\mathrm{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\varepsilon_{i}Z_{i,t}\\right]\\right\\|_{2}^{2}\\right]=\\underset{t\\in\\mathcal{T}}{\\operatorname*{sup}}\\frac{\\|Z_{I,t}\\|_{2}^{2}}{n^{2}}=\\underset{(i,t)\\in[n]\\times\\mathcal{T}}{\\operatorname*{sup}}\\frac{\\|Z_{i,t}\\|_{2}^{2}}{n^{2}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where in the inequality, the outer expectation is with respect to $\\varepsilon_{I}$ , and the inner one is with respect to $(\\varepsilon_{i})_{i\\neq I}$ . Taking expectation with respect to $Z_{i,t}$ gives ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname{E}\\left[\\operatorname*{sup}_{t\\in T}\\left\\|{\\frac{1}{n}}\\sum_{i=1}^{n}Z_{i,t}\\right\\|_{2}^{2}\\right]^{1/2}\\geq{\\frac{1}{2}}\\cdot{\\frac{r(T)}{n}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Averaging the lower bounds (32) and (33) yields the desired lower bound. We now turn to the upper bound. We have by Lemmas 6 and 8. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\mathrm{E}\\left[\\underset{t\\in\\mathcal{T}}{\\operatorname*{sup}}\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}Z_{i,t}\\right\\|_{2}^{2}\\right]^{1/2}\\leq2\\,\\boldsymbol{\\mathrm{E}}\\left[\\underset{t\\in\\mathcal{T}}{\\operatorname*{sup}}\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\varepsilon_{i}Z_{i,t}\\right\\|_{2}^{2}\\right]^{1/2}}\\\\ &{\\displaystyle\\leq5\\sqrt{1+\\log|\\mathcal{T}|}\\cdot\\mathbb{E}\\left[\\underset{t\\in\\mathcal{T}}{\\operatorname*{sup}}\\sum_{i=1}^{n}\\biggl\\|\\frac{1}{n}Z_{i,t}\\biggr\\|_{2}^{2}\\right]^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Applying Lemma 9 on the last term yields the desired upper bound. ", "page_idx": 21}, {"type": "text", "text": "H Proof of Corollary 1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The Glivenko-Cantelli and Donsker assumptions of Theorem 3 follow directly from the moment assumptions of the corollary, the weak law of large numbers, and the central limit theorem, and therefore the conclusions of Theorem 3 hold. For the first statement of the corollary, we may assume without loss of generality that $\\mathcal{T}_{*}\\neq\\mathcal{T}$ , otherwise the statement holds trivially. Define ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\varepsilon:=\\operatorname*{min}_{t\\in T\\backslash T_{*}}R(t,w_{*}(t))-R_{*}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then $\\varepsilon>0$ , and by the first item of Theorem 3, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\mathrm{P}\\big(\\widehat{t}_{n}\\notin\\mathcal{T}_{*}\\big)\\leq\\operatorname*{lim}_{n\\to\\infty}\\mathrm{P}\\big(R(\\widehat{t}_{n},w_{*}(\\widehat{t}_{n}))-R_{*}>\\varepsilon/2\\big)=0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It remains to prove the improved upper bound on the asymptotic quantiles. For this, referring to the proof of Theorem 3, and in particular to (23), it is enough to show that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\rightarrow\\infty}\\mathrm{P}\\big(n\\cdot\\big[R(\\hat{t}_{n},w_{*}(\\hat{t}_{n}))-R_{*})\\big]>0\\big)=0,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "but this follows directly from (34). ", "page_idx": 21}, {"type": "text", "text": "I Proof of Corollary 2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The statement follows from the same argument as Theorem 4 with only a few simple modifications. As explained in the main text, we use Lemma 3 to bound the quantity $\\mathrm{E}\\bar{[\\mathrm{max}_{t\\in\\mathcal{T}}\\,\\Lambda_{n}(t)]}$ by constructing a block diagonal matrix. We use Lemma 2 to control, for any subset $\\boldsymbol{S}$ , $\\operatorname{E}\\left[\\operatorname*{max}_{s\\in S}G_{n}^{2}(s)\\right]$ . The only minor deviation from Theorem 4 is that we bound the second moment ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname{E}\\left[\\operatorname*{sup}_{t\\in T\\backslash T_{*}}\\Delta_{n}^{2}(t,t_{*})\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "instead of the first. This explains the slightly better dependence on $\\delta$ in the sample size restriction of Theorem 2 compared to Theorem 4. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction are supported by our main results of Section 3 and 4 directly. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The last paragraph of the conclusion explicitly discusses the limitations of this work. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The proofs of all the statements we claimed are new can be found in the appendix. For known statements, we provided direct references to them. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not include experiments. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have reviewed the guidelines and confirm that our work adheres to them. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our paper is heavily theoretical, and is quite far removed from direct applications, which is why do not foresee any direct societal impact. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have cited all the relevant work we are aware of, but we do not use any existing code, data, or models in this work. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 26}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA]   \nJustification: We do not release new assets. Guidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Not applicable. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Not applicable. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]