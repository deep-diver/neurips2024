{"references": [{"fullname_first_author": "Kaplan, J.", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-08", "reason": "This paper introduces the concept of scaling laws for neural language models, which is foundational to the work in this paper on understanding and predicting the performance of large language models in continual pre-training."}, {"fullname_first_author": "Hoffmann, J.", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-03-15", "reason": "This paper presents the Chinchilla scaling laws, which are directly used and extended in this paper's proposed D-CPT law for optimal continual pre-training."}, {"fullname_first_author": "Brown, T.B.", "paper_title": "Language models are few-shot learners", "publication_date": "2020-05-28", "reason": "This work is highly influential in demonstrating the few-shot learning capabilities of large language models, and this work's findings are directly relevant to understanding the effectiveness of continual pre-training in the target paper."}, {"fullname_first_author": "Devlin, J.", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2018-10-04", "reason": "BERT is a foundational large language model architecture that has significantly influenced the field, providing context to the continual pre-training improvements studied in this paper."}, {"fullname_first_author": "Liu, Y.", "paper_title": "Roberta: A robustly optimized bert pretraining approach", "publication_date": "2019-07-11", "reason": "This paper introduces the improved RoBERTa model, which provides a benchmark for continual pre-training techniques discussed and improved in this paper."}]}