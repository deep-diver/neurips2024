{"importance": "This paper is crucial for researchers working with large language models (LLMs) and continual pre-training (CPT).  It **introduces a novel Scaling Law, the D-CPT Law**, which enables efficient prediction of optimal training parameters, significantly reducing computational costs.  Further, the **Cross-Domain D-CPT Law extends this efficiency to new domains**, opening avenues for faster, more cost-effective LLM adaptation and domain-specific model development.", "summary": "New D-CPT Law optimizes continual pre-training for LLMs by predicting optimal data mixture ratios, drastically cutting training costs.", "takeaways": ["The D-CPT Law accurately predicts LLM performance across various model sizes, data sizes, and data mixture ratios, using significantly less training.", "The Cross-Domain D-CPT Law extends the D-CPT Law's predictive power to new domains with minimal additional training.", "The study demonstrates three practical applications of the D-CPT Law: optimizing the trade-off between general and domain-specific abilities, handling limited domain-specific data, and allocating resources effectively."], "tldr": "Existing methods for domain-specific continual pre-training (D-CPT) in large language models (LLMs) rely on laborious, computationally expensive grid-searching for optimal data mixture ratios. This paper addresses this limitation.  The research highlights the challenges of selecting optimal mixture ratios between general and domain-specific corpora in D-CPT for LLMs.  These ratios significantly impact the model's performance and existing methods are inefficient and costly.\n\nTo overcome these challenges, the authors propose the D-CPT Law, a novel scaling law inspired by previous Scaling Laws.  The D-CPT Law accurately predicts performance using small-scale training, significantly reducing computational costs. It also introduces the Cross-Domain D-CPT Law, allowing efficient prediction for new domains with minimal training.  The effectiveness and generalizability of both laws are demonstrated across various downstream tasks, showcasing their utility in optimizing LLM training and resource allocation.", "affiliation": "Taobao & Tmall Group of Alibaba", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "JzKFN5fWOk/podcast.wav"}