[{"type": "text", "text": "D-CPT Law: Domain-specific Continual Pre-Training Scaling Law for Large Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Haoran $\\mathbf{Q}\\mathbf{ue}^{*1}$ , Jiaheng $\\mathbf{Liu^{*\\dagger}}^{1},1$ , Ge Zhang\\*3,7, Chenchen Zhang2, Xingwei $\\mathbf{Q}\\mathbf{u}^{4,7}$ , Yinghao $\\mathbf{M}\\mathbf{a}^{5,7}$ , Feiyu Duan1, Zhiqi $\\bf{B a i^{1}}$ , Jiakai $\\mathbf{Wang^{2}}$ , Yuanxing Zhang2, $\\mathbf{X}\\mathbf{u}\\ \\mathbf{T}\\mathbf{a}\\mathbf{n}^{7}$ , Jie $\\mathbf{Fu}^{6}$ , Jiamang $\\mathbf{Wang^{2}}$ , Lin $\\mathbf{Qu^{2}}$ , Wenbo $\\mathbf{S}\\mathbf{u}^{1}$ , Bo Zheng1 ", "page_idx": 0}, {"type": "text", "text": "1Taobao & Tmall Group of Alibaba, 2Alibaba Group, 3University of Waterloo 4University of Manchester, $^{5}\\mathrm{QMUL}$ , ${}^{6}\\mathrm{HKUST}$ , $^7\\mathbf{M}$ -A-P {quehaoran.qhr, ljh411989}@taobao.com, gezhang $@$ umich.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Continual Pre-Training (CPT) on Large Language Models (LLMs) has been widely used to expand the model\u2019s fundamental understanding of specific downstream domains (e.g., math and code). For the CPT on domain-specific LLMs, one important question is how to choose the optimal mixture ratio between the generalcorpus (e.g., Dolma, Slim-pajama) and the downstream domain-corpus. Existing methods usually adopt laborious human efforts by grid-searching on a set of mixture ratios, which require high GPU training consumption costs. Besides, we cannot guarantee the selected ratio is optimal for the specific domain. To address the limitations of existing methods, inspired by the Scaling Law for performance prediction, we propose to investigate the Scaling Law of the Domain-specific Continual Pre-Training (D-CPT Law) to decide the optimal mixture ratio with acceptable training costs for LLMs of different sizes. Specifically, by fitting the D-CPT Law, we can easily predict the general and downstream performance of arbitrary mixture ratios, model sizes, and dataset sizes using small-scale training costs on limited experiments. Moreover, we also extend our standard D-CPT Law on cross-domain settings and propose the Cross-Domain D-CPT Law to predict the D-CPT law of target domains, where very small training costs (about $1\\%$ of the normal training costs) are needed for the target domains. Comprehensive experimental results on six downstream domains demonstrate the effectiveness and generalizability of our proposed D-CPT Law and Cross-Domain D-CPT Law. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Continual Pre-Training (CPT) is an essential part of training better Large Language models (LLMs). In this work, we mainly focus on Domain-specific CPT (D-CPT), which aims to enhance the fundamental understanding abilities of the specific downstream domains and has been widely used in existing works [49, 41, 30]. In practice, for D-CPT, we usually need to collect high-quality domain-corpus to enhance the downstream performance and general-corpus to mitigate catastrophic forgetting on the general abilities [13, 40, 54, 47, 32, 20, 53]. Therefore, how to determine the data composition or mixture ratio of the domain-corpus and general-corpus plays an important role in producing well-performed domain-specific LLMs. Besides, grid-searching on the mixture ratios requires heavy GPU consumption costs, and we cannot always obtain the optimal ratio under limited GPU usage. Recently, Scaling Law has been widely used for performance prediction [31, 27, 43, 26], which can be used to find the optimal dataset size and model size under the given GPU consumption costs. Therefore, for $D$ -CPT, can we find the optimal mixture ratio in the training corpus using the Scaling Law to enhance the performance of domain-specific tasks? ", "page_idx": 0}, {"type": "image", "img_path": "JzKFN5fWOk/tmp/538b40cdec4391398cf008c22134399e70d41e30f52f9558a1117a5a2a8ef74b.jpg", "img_caption": ["Figure 1: Illustration of the performance of D-CPT Law. (Left): The curves show the relationship between $L_{g}$ and $r_{g}$ under different dataset sizes $D$ for Qwen1.5-1.8B model. CPT data are a mixture of code-corpus and general-corpus. Here, $L_{g}$ represents the loss on the general-corpus validation set, while $r_{g}$ indicates the percentage of the general corpus in the training data. The dashed curves denote the curves predicted by D-CPT Law, circular markers and star markers are fitting data points and unseen validation points, respectively. (Right): These curves are the corresponding results between the code-corpus validation loss $L_{d}$ and the percentage of the code-corpus data $r_{d}$ . "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address the above question, in this work, we investigate the Scaling Law of D-CPT and propose the D-CPT Law to find the optimal mixture ratio with limited training costs for LLMs with different sizes. Specifically, inspired by the robust predictive ability of Scaling Law across various scales, we first perform experiments under diverse mixture ratios and several relatively small model and data scales. Following the Chinchilla Scaling Law, we then introduce the mixture ratio $r$ into the D-CPT Law, where the parameterization is defined as follows: ", "page_idx": 1}, {"type": "equation", "text": "$$\nL(N,D,r)=E+\\frac{A}{N^{\\alpha}}+\\frac{B\\cdot r^{\\eta}}{D^{\\beta}}+\\frac{C}{r^{\\prime}{}^{\\gamma}},\\;\\mathrm{where\\;}r^{\\prime}=r+\\epsilon,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\epsilon$ is used to guarantee the stability of $L$ when $r$ near zero. Based on Equation 1, for a model with model size $N$ , dataset volume $D$ and mixture ratio $r$ , we can accurately predict the validation loss $L$ . Note that when $r$ denotes the domain-corpus mixture ratio $r_{d}$ , $L$ means domain-corpus validation loss $L_{d}$ . Similarly, general-corpus validation loss $L_{g}$ also follows the law relationship with the general-corpus mixture ratio $r_{g}$ . To illustrate our D-CPT Law clearly, as shown in Figure 1, we take the code domain as an example and provide the ftiting results on the general and domain-specific settings, where we validate the ftiting accuracy on different mixture ratios under a model with different dataset sizes $D$ . Our main contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "(1). To show the effectiveness and generalizability of D-CPT Law, we perform extensive experiments using model sizes from 0.5B to 4B parameters, dataset sizes from 0.1B to 26B tokens, and mixture ratios from 0 to 1. The experiments show that the D-CPT law exhibits a high fitting accuracy with Huber loss [28] lower than 0.02 and $R^{2}$ [9] greater than 0.97. Besides, experiments on generalizability show that D-CPT Law not only inherits model size and dataset size generalizability following previous Scaling Law, but also precisely predicts performance for different mixture ratios. ", "page_idx": 1}, {"type": "text", "text": "(2). Despite the effectiveness in an in-domain setting, where we fit the D-CPT Law based on data points from one downstream domain, we also apply our D-CPT Law in the cross-domain setting, which denotes that we use the data points from multiple domains to predict the performance of unseen domains. Specifically, we first introduce the Domain-specific Learnable Coefficient (DLC) to denote the domain-specific parameter of each domain and integrate the DLC into the D-CPT Law. We name this new law as Cross-Domain D-CPT Law. In this way, if we can obtain the DLC of a new domain, we can easily derive the D-CPT Law for this new domain. In our experiments, we fit the Cross-Domain D-CPT Law using data points from 4 domains and apply the Cross-Domain ", "page_idx": 1}, {"type": "image", "img_path": "JzKFN5fWOk/tmp/0cd8b7191ee14fb11db723128444f1916e3dd41900086cf4d9ee439c3021f9e5.jpg", "img_caption": ["Figure 2: Illustration of D-CPT Law and Cross-Domain CPT-Law pipeline. (Upper): In D-CPT Law, we first collect domain-corpus and general-corpus, and conduct experiments under a small-scale experimental setup to gather empirical data points to fit the D-CPT Law. After that, we can predict the model\u2019s performance in large-scale experimental settings. (Lower): In Cross-Domain CPT-Law, for an unseen downstream domain, like Physics, we can calculate its Domain-specific Learnable Coefficient value and incorporate it into the fitted Cross-Domain D-CPT Law to derive the D-CPT Law for this new domain. Based on the D-CPT Law, we introduce three application scenarios: optimal mixture on the trade-off between general and domain-specific abilities, optimal mixture for limited domain-specific data, and resource allocation in Section 4.3. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "D-CPT Law to predict the remaining 2 domains. The results show that DLC can represent the specific information for each downstream domain well, enabling efficient and effective fitting performance for the cross-domain setting and significantly reducing training costs for new domains. ", "page_idx": 2}, {"type": "text", "text": "(3). To show the real-world usages of the D-CPT Law, we apply our D-CPT Law on three important scenarios: optimal mixture on the trade-off between general and domain-specific abilities, optimal mixture for limited domain-specific data, and resource allocation setting in Figure 2 (Details are provided in Section 4.3). ", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Following previous work [43], we categorize the objectives of Scaling Law as Allocation and Return. Specifically, (1). Allocation: What is the optimal allocation of model size $N$ and dataset size $D$ given a fixed compute budget? (2). Return: What is the expected return on incremental resources? ", "page_idx": 2}, {"type": "text", "text": "The first objective on Allocation is as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\underset{N,D}{\\operatorname{argmin}}}\\,L(N,D)\\quad{\\mathrm{s.t.}}\\quad{\\mathrm{FLOPs}}(N,D)=C.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In Equation 2, given a fixed compute budget $C$ , the objective is to find the optimal model size $N$ and dataset size $D$ that minimize the loss. The second objective on Return fundamentally depends on the generalizability of Scaling Law to accurately predict beyond the fitting data points. ", "page_idx": 2}, {"type": "text", "text": "Chinchilla Scaling Law Hoffmann et al.[27] propose a parameterization as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nL=E+\\frac{A}{N^{\\alpha}}+\\frac{B}{D^{\\beta}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\{E,A,B,\\alpha,\\beta\\}$ are fitting parameters (See Appendix D for more details). ", "page_idx": 2}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In Figure 2, D-CPT Law aims to investigate the behaviors of the Domain-specific Continual PreTraining scenario with respect to different mixture ratios, and the objective of the D-CPT Law is to analyze an appropriate parameterization of law that represents the relationship of validation loss $L$ with respect to model size $N$ , dataset size $D$ , and mixture ratio $r$ . In this section, we first discuss the D-CPT Law in the in-domain setting (Section 3.1), where the ftiting and testing data points are from the same domains. Then, we propose to adapt D-CPT Law to the cross-domain setting (Section 3.2), where the ftiting and testing data points are from multiple domains, and introduce the Cross-Domain D-CPT Law, where a new term Domain-specific Learnable Coefficient (DLC) is used. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.1 D-CPT Law ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As the training data includes a mixture of general-corpus and domain-corpus, we introduce two mixture ratios (i.e., general-corpus mixture ratio $r_{g}$ and domain-corpus mixture ratio $r_{d}$ ). Correspondingly, we define two validation losses (i.e., general-corpus validation loss $L_{g}$ and domain-corpus validation loss $L_{d}$ ). Therefore, we can derive two D-CPT Laws (i.e., $L_{g}(N,D,\\bar{r_{g}})$ and $L_{d}(N,D,r_{d}))$ . For convenience, we directly use $r$ and $L(N,D,r)$ as default notations for D-CPT Law. Besides, in Appendix D, Chinchilla Scaling Law provides greater interpretability and clarity when compared to OpenAI Scaling Law. Thus, we choose Chinchilla Scaling Law as the foundational parameterization for D-CPT Law. In addition, since Scaling Law aims to fti data points, their parametric forms should be intrinsically related to the observed trends in the data points. Based on previous works and data trends with varying $N,D$ and $r$ , we have summarized 4 essential requirements for D-CPT Law: ", "page_idx": 3}, {"type": "text", "text": "\u2022 Adaptability: D-CPT Law is valid for values of $r$ between 0 and 1. ", "page_idx": 3}, {"type": "text", "text": "\u2022 Explicit trends: Based on the results across varying values of $N,D$ , and $r$ , we observed the following explicit trends of data points: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\partial L}{\\partial N}<0,\\quad\\frac{\\partial L}{\\partial D}<0,\\quad\\frac{\\partial L}{\\partial r}<0.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The first two trends are consistent with the previous Chinchilla Scaling Law, and the third trend also has an intuitive explanation. A larger $r$ indicates a higher proportion of valid corpus in the training corpus, leading to a lower $L$ . Details are provided in Appendix E.1. ", "page_idx": 3}, {"type": "text", "text": "\u2022 Implicit trends: We further discover inherent connections between $r$ , $D$ and $L$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}L}{\\partial D\\partial r}<0.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For detailed explanations, please refer to the Appendix E.2. ", "page_idx": 3}, {"type": "text", "text": "\u2022 Consistency: When $r$ is fixed, the D-CPT Law is supposed to transform into the Chinchilla Scaling Law. In this way, D-CPT Law can inherit the excellent features of Chinchilla Scaling Law and address the issues on resource allocation discussed in Section 2. ", "page_idx": 3}, {"type": "text", "text": "To satisfy these requirements, we have compared multiple parameterizations in Section 4.2 and eventually, we propose the parameterization as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nL(N,D,r)=E+{\\frac{A}{N^{\\alpha}}}+{\\frac{B\\cdot r^{\\eta}}{D^{\\beta}}}+{\\frac{C}{r^{\\prime\\gamma}}},\\quad{\\mathrm{where}}\\quad r^{\\prime}=r+\\epsilon.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In Equation 6, $\\{E,A,B,C,\\alpha,\\beta,\\gamma,\\eta,\\epsilon\\}$ are the ftiting parameters and we use L-BFGS[34] to fti the D-CPT Law due to its suitability for large-scale optimizations. As shown in Section 4.2, the Equation 6 can accurately fti the trends of data points at any scale and demonstrates strong performance in both effectiveness and generalizability. Besides, it effectively meets the aforementioned 4 requirements. (Please see Appendix E.3 for more details on the mathematical derivation.) ", "page_idx": 3}, {"type": "text", "text": "3.2 Cross-Domain D-CPT Law ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Apart from the in-domain setting for D-CPT Law, we also investigate the cross-domain setting and extend the D-CPT Law to the Cross-Domain D-CPT Law, which aims to reduce the training costs of the D-CPT Law significantly. Specifically, although the D-CPT Law collects data points using small LLMs, the GPU resource and time costs are still relatively substantial, which limits the applications of the Scaling Law. Therefore, in our Cross-Domain D-CPT Law, we first define the Domain-specific Learnable Coefficient (DLC) $K$ for each domain, which measures the learnability for a specific domain1 (See Section 4.4 for more details.). Then, we incorporate the $K$ into the D-CPT Law and obtain the Cross-Domain D-CPT Law, which is defined as follows: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nL(N,D,r,K)=E+{\\frac{A}{N^{\\alpha}}}+{\\frac{B\\cdot r^{\\eta}}{D^{\\beta}}}+{\\frac{C}{r^{\\prime\\gamma}}}+{\\frac{F}{K^{\\mu}}},\\quad{\\mathrm{where}}\\quad r^{\\prime}=r+\\epsilon.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In Equation 7, $\\{E,A,B,C,F,\\alpha,\\beta,\\eta,\\gamma,\\epsilon,\\mu\\}$ are fitting parameters. Thus, for an unseen domain, we only need to calculate the DLC with modest costs, which substantially increases the domain generalizability of D-CPT Law. Besides, Cross-Domain D-CPT Law has the following features: ", "page_idx": 4}, {"type": "text", "text": "\u2022 Uniformity: Once we calculate the $K$ value of an unseen domain, we can convert Crossdomain D-CPT Law into normal D-CPT Law as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nL(K=K_{0})=E_{0}+{\\frac{A}{N^{\\alpha}}}+{\\frac{B\\cdot r^{\\eta}}{D^{\\beta}}}+{\\frac{C}{r^{\\prime\\gamma}}},\\quad{\\mathrm{where}}\\quad E_{0}=E+{\\frac{F}{K_{0}^{\\mu}}},\\quad r^{\\prime}=r+\\epsilon.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Therefore, the Cross-Domain D-CPT Law inherits all features of the D-CPT Law, ", "page_idx": 4}, {"type": "text", "text": "\u2022 Monotonicity: $K$ denotes the learnability of a specific domain, which aligns with the intuition that a more learnable domain yields lower validation loss. Meanwhile, the Crossdomain D-CPT Law confirms a monotonic decrease with respect to $K$ , i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\partial{\\cal L}}{\\partial K}=-\\frac{\\mu F}{K^{\\mu+1}}<0.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "After confirming the parameterization of Cross-domain D-CPT Law, it is essential to identify a representation for $K$ that accurately quantifies a domain\u2019s learnability. The representation of $K$ is supposed to be accessible, distinct and robust. Specifically, first, \u201cAccessible\u201d denotes that it is easy to obtain for an unseen domain with low costs. Second, \u201cDistinct\u201d indicates that $K$ values must exhibit significant variance across domains to ensure ftiting accuracy and maintain clear distinctions between domains. Third, \u201cRobust\u201d means that the representation of $K$ enhances the effectiveness and generalization ability of Cross-domain D-CPT Law. In Section 4.4, we compare several variants of the representations of $K$ , and the final representation is determined as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nK=\\frac{w_{1}}{k_{1}}+w_{2}\\times k_{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $w_{1}$ and $w_{2}$ are ftiting parameters, $k_{1}$ represents the initial validation loss for an unseen domain, and $k_{2}$ denotes the rate of decline in validation loss. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Data Setup To verify the effectiveness and generalizability of D-CPT Law and Cross-Domain DCPT Law, we have prepared the 6 different downstream domains, which include Code [39], Math [5], Law [24], Chemistry [10], Music [44] and Medical [33]. For general corpus, we use Dolma [48]. All the tokens of these training datasets are sufficient, so the experiments are not performed under a data-constrained setting. Besides, we build a high-quality and held-out validation set for each domain. (See Appendix F.1 for more details.) ", "page_idx": 4}, {"type": "text", "text": "Model Setup We use the Qwen-1.5 series due to its robust performance in both English and Chinese [6]. Furthermore, Qwen-1.5 has multiple open-sourced and well-performed pre-training base models. Specifically, we select Qwen-1.5-0.5B, Qwen-1.5-1.8B, and Qwen-1.5-4B as our base models to perform the continual pre-training for multiple downstream domains. ", "page_idx": 4}, {"type": "table", "img_path": "JzKFN5fWOk/tmp/2be222ad7fa63d9a81f17955639040248b75435f00e103e0f0bb772c5892134e.jpg", "table_caption": ["Table 1: Mean performance across five parameterizations over six domains. \u201cG\u201d and \u201cD\u201d denote general and downstream domains. Detailed results on all domains are shown in Appendix J. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Training Setup We follow Chinchilla [27] to fix model sizes and vary the number of training tokens for data point collection. Specifically, we test the validation loss every 1,000 steps 2 and the total training steps are $200\\mathbf{k}$ . Then, we establish 9 mixture ratios between general-corpus and domaincorpus as follows: {0:10, 1:9, 2:8, 3.3:6.7, 5:5, 6.7:3.3, 8:2, 9:1, 10:0}. Note that all experiments are conducted with the same learning rate schedule (Hyperparameters can be found in Appendix F.2). ", "page_idx": 5}, {"type": "text", "text": "Metrics Following [44, 43, 27], we use validation loss as the performance indicator. To compare various parameterizations, we follow [28, 44] to utilize the $R^{2}$ and Huber loss as evaluation metrics. Specifically, first, the coefficient of determination (i.e., $R^{2}$ ) indicates the ftiting quality and typically ranges from 0 to 1, where a higher value means better explanatory power of the regression model. Second, Huber loss combines the properties of mean squared error and mean absolute error, which is particularly useful for regression with outliers. Similarly, Huber loss also assesses the fti qualities of different parameterizations, where lower Huber loss shows better fitting performances. ", "page_idx": 5}, {"type": "text", "text": "4.2 D-CPT Law ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In Section 3.1, an ideal parameterization should meet four requirements (i.e., adaptability, explicit trends, implicit trends, and consistency), and we define the following five parameterizations: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{L_{1}=E+\\displaystyle\\frac{A}{N^{\\alpha}}+\\displaystyle\\frac{B}{D^{\\beta}}+\\displaystyle\\frac{C}{r^{\\prime\\gamma}},L_{2}=E+\\displaystyle\\frac{A}{N^{\\alpha}}+\\left(\\displaystyle\\frac{B}{D^{\\beta}}+\\displaystyle\\frac{C}{r^{\\prime\\gamma}}\\right)^{\\eta},L_{3}=E+\\displaystyle\\frac{A}{N^{\\alpha}}+\\displaystyle\\frac{B r^{\\eta}}{D^{\\beta}}+\\displaystyle\\frac{C}{r^{\\prime\\gamma}},}}\\\\ {{L_{4}=E+\\displaystyle\\frac{A}{N^{\\alpha}}+\\displaystyle\\frac{B\\cdot b^{r}}{D^{\\beta}}+\\displaystyle\\frac{C}{c^{r}},\\quad L_{5}=E+\\displaystyle\\frac{A}{N^{\\alpha}}+\\displaystyle\\frac{B}{\\left(r D+\\left(1-r\\right)\\sigma\\right)^{\\beta}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\{N,D,r\\}$ are variables and others are learned parameters fitted by L-BFGS algorithm[34] which is the same as Chinchilla Scaling Law. ", "page_idx": 5}, {"type": "text", "text": "Effectiveness As shown in Table 1, we present the performance of five different parameterizations. In effectiveness settings, we use entire data points for ftiting with the aim of validating the effectiveness of various parameterizations. In Table 1, we observe that although $L_{5}$ has the fewest ftiting parameters, its performance is significantly less impressive compared to the others. $L_{1}$ and $L_{4}$ , having relatively fewer fitting parameters, still fall short in performance compared to $L_{2}$ and $L_{3}$ . Moreover, $L_{1}$ fails to meet the requirements for implicit trends, while $L_{4}$ does not satisfy the explicit trends requirement. Finally, the results of $L_{2}$ and $L_{3}$ are comparable, but $L_{2}$ does not meet the requirements for consistency. Therefore, we choose $L_{3}$ for D-CPT Law. Moreover, Figure 3 shows the robust effectiveness of $L_{3}$ across varying dataset sizes, mixture ratios, model sizes, and domains. ", "page_idx": 5}, {"type": "text", "text": "Model Size Generalizability: Our main experiments focus on 3 model sizes: 0.5B, 1.8B, and 4B. We use 3-fold cross-validation to evaluate the model size generalizability of D-CPT Law, and the average results across domains are shown in Table 2. For example, we fti D-CPT Law with data points from 0.5B, and 1.8B and evaluate the Huber loss and $R^{2}$ for 4B. In Table 2, we observe that D-CPT Law can generalize well across model sizes and $L_{3}$ shows the best performance. Besides, we conduct experiments on the unseen 7B size (i.e., Qwen-1.5 7B), and observe that D-CPT Law can accurately predict the general-corpus validation loss with a general-corpus mixture ratio of 0.2 in Figure 4. ", "page_idx": 5}, {"type": "text", "text": "Dataset Size Generalizability: Our main experiments cover dataset sizes from 0.1B to 26B tokens, and we also utilize a 3-fold cross-validation approach. The data points are uniformly divided into three segments, with 2/3 used for fitting the model and the remaining 1/3 for testing. In Table 3, we report the average results across domains, and observe that $L_{3}$ shows notably enhanced dataset size generalizability. ", "page_idx": 6}, {"type": "image", "img_path": "JzKFN5fWOk/tmp/85f56799c67ba2faee32b757d77fe0e206d8a05796ec2ad093be1e1cb7079be7.jpg", "img_caption": ["Figure 4: $L_{g}$ with respect to $D$ , domain-corpus is code, $r_{g}=0.2$ , $N=7B$ . "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "JzKFN5fWOk/tmp/335d7f17a3686744eec281e62cb08fa62ab64a5af51704bf39fcd6c53d3be8f7.jpg", "table_caption": ["Table 2: Model Size Generalizability. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "JzKFN5fWOk/tmp/f9377d88ae17099ba8fa2e9184caaa11cb22560e159ef0558c57d66a9f62081a.jpg", "table_caption": ["Table 3: Dataset Size Generalizability. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "JzKFN5fWOk/tmp/d84d59669954331199ac10da7e46c5201897e95bce20e5d328d93b841d392013.jpg", "table_caption": ["Table 4: Mixture ratio Generalizability. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Mixture ratio Generalizability: We apply the $\\mathbf{k}$ -fold cross-validation method across various parameterizations. Specifically, we select 7 out of 9 mixture ratios for fitting and the remaining for testing, resulting in 36 experiments per domain. For simplicity, we show average results across domains in Table 4, and observe that that $L_{3}$ still shows significantly better performance on mixture ratio generalizability. Besides, in Figure 1, we observe that our D-CPT Law has well-performed generalizability on unseen mixture ratios. ", "page_idx": 6}, {"type": "text", "text": "4.3 Usages of D-CPT Law ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Usage 1: Trade-off between general and domain-specific abilities For D-CPT, training data is a mixture of general and domain-specific data, where $r_{g}$ and $r_{d}$ denote the corresponding proportions, respectively. In D-CPT Law, when $r_{g}$ increases, the $L_{g}$ will decrease and $L_{d}$ will increase, indicating a trade-off between the general and domain-specific abilities of LLM. Fortunately, D-CPT Law can identify the optimal mixture ratio under any trade-off scenario. Specifically, we assume that an LLM with parameter size $N_{0}$ , it exhibits general-corpus validation loss of $L_{g}^{0}$ and domain-corpus validation loss of ${\\cal L}_{d}^{0}$ before continual pretraining. After mixing training data size of $D_{0}$ with a ratio $r_{d}$ of domain-specific data and $1-r_{d}$ of general data, we obtain general-corpus validation loss $L_{g}$ and domain-corpus validation loss $L_{d}$ after D-CPT. Then, we can identify the optimal mixture ratio while limiting the decline in the model\u2019s general abilities within a threshold $T$ as follows: ", "page_idx": 6}, {"type": "image", "img_path": "JzKFN5fWOk/tmp/74afba0187ea9d1d4e4ac0bf5cf83b7af29e6e6b7d7b2883eeed94454c7035be.jpg", "img_caption": ["Figure 3: Effectiveness of D-CPT Law $(L_{3})$ . (left two): General-corpus validation loss $L_{g}$ with respect to dataset size $D$ across different model sizes $N$ , domain-corpus is code and general-corpus mixture ratio $r_{g}=0.5$ . (right two): Domain-corpus validation loss $L_{d}$ with respect to dataset size $D$ across different model sizes $N$ , domain-corpus is code and domain-corpus mixture ratio $r_{d}=0.5$ . "], "img_footnote": [], "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{argmin}_{r_{d}}L_{d}(N=N_{0},D=D_{0},r_{d})\\quad\\mathrm{s.t.}\\quad\\frac{L_{g}-L_{g}^{0}}{L_{g}^{0}}<T,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $T$ is the threshold based on practical need. In Appendix G.1, given a fixed $T$ , a unique optimal solution $r_{d}$ is obtained. To validate it in real scenarios, by applying D-CPT Law, we calculate the optimal domain-corpus mixture ratio $r_{d}=0.924$ given a dataset size $D_{0}=10B$ , model size $N_{0}\\,=\\,1.8B$ , $T\\,=\\,3\\%$ , domain-corpus is chemistry, and initial general validation loss value of $L_{g}^{0}=2.8602$ . Table 5 presents the results of real general-corpus validation loss and domain-corpus validation loss with respect to different domain-corpus mixture ratios, we find that the real value exactly matches the predicted values $[L_{g\\_p r e d}\\,=\\,2.9458$ and $L_{d\\_p r e d}\\,=\\,1.7284)$ , domain-corpus mixture ratio exceeding 0.924 leads to a general validation loss that surpasses the $3\\%$ threshold of $L_{g}^{0}$ . ", "page_idx": 7}, {"type": "text", "text": "Usage 2: Optimal mixture on limited domain-specific data Given that domain-corpus is typically limited relative to the abundance of the general corpus, we study how to determine the optimal mixture ratio when domain-corpus is limited and general-corpus is sufficient. Specifically, for an LLM with parameter $N_{0}$ and limited domain-corpus $D_{d}^{0}$ , we aim to minimize the domain-corpus validation loss $L_{d}$ by selecting the optimal domain-corpus mixture ratio $r_{d}$ as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\underset{r_{d}}{\\operatorname{argmin}}\\,L_{d}(N=N_{0},D,r_{d})\\quad\\mathrm{s.t.}\\quad D_{d}=D_{d}^{0},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In Equation 11, we can reach the minimum value within the $0<r_{d}<1$ discussed in Appendix G.2. To validate it in real scenarios, we conducted experiments within the music domain by setting the model parameters $N_{0}=1.8B$ and the domain-specific dataset size $D_{d}\\,=\\,5B$ . As we have data points at a large scale, we fti D-CPT Law using only data where $D_{d}<2B$ to align with the use case scenarios. After using the D-CPT Law, we find that the optimal domain-corpus mixture ratio is 0.732. Table 6 shows the results of real domain-corpus validation loss of the music domain. We observe that $r_{d}=0.732$ yields the lowest domain-corpus validation loss. Moreover, our predicted domain-corpus validation loss is 0.7328 when $r_{d}=0.732$ , which is close to the real value (0.7309). ", "page_idx": 7}, {"type": "text", "text": "Usage 3: Resource allocation D-CPT Law is consistent with Chinchilla Scaling Law under the fixed mixture ratio to address resource allocation. Specifically, how to find the optimal values of $N$ and $D$ given a fixed compute budget. Detailed results are shown in Appendix G.3. ", "page_idx": 7}, {"type": "text", "text": "4.4 Cross-Domain D-CPT Law ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Section 3.2, we have mentioned that the learnability of a specific domain is measured by DLC (i.e., $K_{\\star}$ ). For Cross-Domain D-CPT Law, $K$ must satisfy 3 core requirements: accessible, distinct, and robust. Based on these requirements, 4 different representations of $K$ are defined as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\nK_{1}=\\frac{w_{1}}{k_{1}},\\quad K_{2}=w_{2}\\times k_{2},\\quad K_{3}=\\frac{w_{1}}{k_{1}}+w_{2}\\times k_{2},\\quad K_{4}=\\frac{w_{1}}{k_{1}}+w_{2}\\times k_{2}+\\frac{w_{3}}{k_{3}},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\{w_{1},w_{2},w_{3}\\}$ are fitting parameters. In the approximate Taylor expansion for the validation loss function near the initial points, $\\{k_{1},k_{2},k_{3}\\}$ represent the first three coefficients. Due to the discrete nature of data points in practical scenarios, $\\{k_{1},k_{2},k_{3}\\}$ are approximated using the variants of validation loss. Specifically, $k_{1}$ denotes the precise value of the validation loss at the initial point, $k_{2}$ represents the difference in validation loss close to the initial points, and $k_{3}$ is an approximation of the second derivative of the validation loss near the initial points, details are provided in Appendix H. To compare these four representations of $K$ , we have conducted experiments in both effectiveness and generalizability aspects. ", "page_idx": 7}, {"type": "table", "img_path": "JzKFN5fWOk/tmp/5d6ef095d3aab337c0b3e857225decf566a08ae6d0d3ae7bac5b6127efcce7fb.jpg", "table_caption": ["Table 5: The real $\\underline{{L}}_{g}$ and $L_{d}$ with respect to $r_{d}$ in usage 1 setting "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 6: The real domain-corpus validation loss with respect to $r_{d}$ when $D_{d}$ is fixed with 5B. ", "page_idx": 8}, {"type": "table", "img_path": "JzKFN5fWOk/tmp/25e7017033dd1de8de9996fc33cd59fd0742fcc9548aac73fd5fbe8b1f8ff845.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Effectiveness We utilize data points from all 6 domains for fitting and then evaluate their performance using $R^{2}$ and Huber loss. In Table 7, we find that 4 representations of $K$ yield comparable results in the general domain. However, $K_{1}$ and $K_{2}$ demonstrate a noticeable decline in domainspecific aspects. Although $K_{4}$ slightly outperforms $K_{3}$ in domain-specific aspects, it requires a larger number of fitting parameters. Therefore, considering the balance between fitting efficiency, fitting performance, and accessibility, We consider $K_{3}$ to be the optimal representation. To further visualize it, Figure 5 illustrates the predicted curves in comparison to the real curves under various settings. ", "page_idx": 8}, {"type": "image", "img_path": "JzKFN5fWOk/tmp/a6af10029acdae306549ded3cd8d244536ae7f4bd36c5a64864fee6c723745b2.jpg", "img_caption": ["Figure 5: Effectiveness of Cross-Domain D-CPT Law $(K_{3})$ . (left two): $L_{g}$ with respect to dataset size $D$ across different model size $N$ , domain-corpus is music and $r_{g}$ is 0.2. (right two): $L_{d}$ with respect to dataset size $D$ across different model size $N$ , domain-corpus is music and $r_{d}$ is 0.8. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Generalizability When $K$ is identified, Cross-Domain D-CPT Law can be transformed into D-CPT Law, we believe that the former inherits the latter\u2019s generalizability in terms of model size, dataset size, and mixture ratio. In this part, we will specifically focus on discussing the domain generalizability of Cross-Domain D-CPT Law. To evaluate domain generalizability, we use data points from 4 out of 6 domains for fitting and assign the remaining 2 domains for testing. For simplicity, we only show the averaged results across 15 combinations in Table 8. Among these 4 representations of $K$ , $K_{3}$ exhibits the superior performance, further proving its strength. ", "page_idx": 8}, {"type": "text", "text": "5 Related Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Scaling Law Many studies [26, 31, 27, 7, 16, 58] show a power-law relationship between performance and the increases in both the number of parameters and the size of the training data [31, 27, 18], which are crucial for large language models (LLMs [45, 51, 29, 3, 55, 35, 59, 36, 60]) and provide a predictive structure for determining the most efficient setups for ex", "page_idx": 8}, {"type": "table", "img_path": "JzKFN5fWOk/tmp/5e9511d0baef129bbc8b079a050e91cb1f427784b19f1db9b960cdda476addef.jpg", "table_caption": ["Table 8: Domain generalizability. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "panded models using the insights gained from smaller models [17]. Moreover, the extension of scaling laws to autoregressive generative models widens their relevance to encompass tasks beyond text [18, 25, 17]. Recently, Muennighoff et al. [43] studied the Scaling Law of data-constrained settings by using the full pre-training dataset across multiple epochs. Ye et al. [57] investigate the data-mixing scaling law for the general LLMs to improve the pretraining efficiency. ", "page_idx": 8}, {"type": "table", "img_path": "JzKFN5fWOk/tmp/750d3f95b410bb7e8d23b7193583d3e3c909eff0cb66aa890cd45703e7ee1d30.jpg", "table_caption": ["Table 7: The performance of 4 representations under effectiveness setting. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "\\* For Numbers of fitting parameters, more $\"+\"$ indicates a larger number of fitting parameters, implying lower fitting efficiency. Accessibility denotes the accessibility of $K$ , fewer $\"+\"$ signifies higher accessibility. ", "page_idx": 8}, {"type": "text", "text": "Domain-specific Continual Pre-Training Domain-specific Continual Pre-Training aims to continually pre-train LLMs to adapt them to new domains [23, 11, 22, 30, 19]. For example, Gururangan et al. [23] introduces a growing mixture of expert architecture for domain-adaptive continual pretraining. Cossu et al. [13] show that continually pre-trained models (RoBERTa [38] and BERT [14]) are robust against catastrophic forgetting on downstream tasks. However, the above works only investigate small encoder-decoder models on limited tasks. Recently, Gupta et al. [21] study different warm-up strategies for continual pertraining for better results. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we have investigated the Scaling Law of Domain-specific Continual Pre-Training (D-CPT), which provides a significant step forward in the optimization of training LLMs for specific downstream domains. By developing and validating the D-CPT Law, we can easily predict the optimal mixture ratio of general and domain-specific corpora, greatly reducing the previously necessary but costly grid-searching efforts. Besides, we also adapt our D-CPT Law to the cross-domain setting and introduce the Cross-Domain D-CPT Law to further reduce the efforts of fitting the D-CPT Law of new domains. Moreover, we discuss the three practical usages of our D-CPT Law. Finally, we believe our D-CPT Law is an initial investigation into quantitative prediction methods for the domain-specific continual pre-training. With its increasing focus on data engineering, we hope our exploration facilitates further quantitative studies and theoretical analyses in this research area. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Aghajanyan, A., Yu, L., Conneau, A., Hsu, W.N., Hambardzumyan, K., Zhang, S., Roller, S., Goyal, N., Levy, O., Zettlemoyer, L., 2023. Scaling laws for generative mixed-modal language models, in: International Conference on Machine Learning, PMLR. pp. 265\u2013279.   \n[2] Agiza, A., Mostagir, M., Reda, S., 2024. Analyzing the impact of data selection and fine-tuning on economic and political biases in llms. arXiv preprint arXiv:2404.08699 .   \n[3] AI, ., :, Young, A., Chen, B., Li, C., Huang, C., Zhang, G., Zhang, G., Li, H., Zhu, J., Chen, J., Chang, J., Yu, K., Liu, P., Liu, Q., Yue, S., Yang, S., Yang, S., Yu, T., Xie, W., Huang, W., Hu, X., Ren, X., Niu, X., Nie, P., Xu, Y., Liu, Y., Wang, Y., Cai, Y., Gu, Z., Liu, Z., Dai, Z., 2024. Yi: Open foundation models by 01.ai. arXiv:2403.04652.   \n[4] Alabdulmohsin, I.M., Neyshabur, B., Zhai, X., 2022. Revisiting neural scaling laws in language and vision. Advances in Neural Information Processing Systems 35, 22300\u201322312.   \n[5] Azerbayev, Z., Schoelkopf, H., Paster, K., Santos, M.D., McAleer, S., Jiang, A.Q., Deng, J., Biderman, S., Welleck, S., 2023. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631 .   \n[6] Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et al., 2023. Qwen technical report. arXiv preprint arXiv:2309.16609 .   \n[7] Bi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C., Ding, H., Dong, K., Du, Q., Fu, Z., et al., 2024. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954 .   \n[8] Cai, Z., Cao, M., Chen, H., Chen, K., Chen, K., Chen, X., Chen, X., Chen, Z., Chen, Z., Chu, P., Dong, X., Duan, H., Fan, Q., Fei, Z., Gao, Y., Ge, J., Gu, C., Gu, Y., Gui, T., Guo, A., Guo, Q., He, C., Hu, Y., Huang, T., Jiang, T., Jiao, P., Jin, Z., Lei, Z., Li, J., Li, J., Li, L., Li, S., Li, W., Li, Y., Liu, H., Liu, J., Hong, J., Liu, K., Liu, K., Liu, X., Lv, C., Lv, H., Lv, K., Ma, L., Ma, R., Ma, Z., Ning, W., Ouyang, L., Qiu, J., Qu, Y., Shang, F., Shao, Y., Song, D., Song, Z., Sui, Z., Sun, P., Sun, Y., Tang, H., Wang, B., Wang, G., Wang, J., Wang, J., Wang, R., Wang, Y., Wang, Z., Wei, X., Weng, Q., Wu, F., Xiong, Y., Xu, C., Xu, R., Yan, H., Yan, Y., Yang, X., Ye, H., Ying, H., Yu, J., Yu, J., Zang, Y., Zhang, C., Zhang, L., Zhang, P., Zhang, P., Zhang, R., Zhang, S., Zhang, S., Zhang, W., Zhang, W., Zhang, X., Zhang, X., Zhao, H., Zhao, Q., Zhao, X., Zhou, F., Zhou, Z., Zhuo, J., Zou, Y., Qiu, X., Qiao, Y., Lin, D., 2024. Internlm2 technical report. arXiv:2403.17297.   \n[9] Carpenter, R., 1960. Principles and procedures of statistics, with special reference to the biological sciences. The Eugenics Review 52, 172.   \n[10] Chemrxiv, . https://chemrxiv.org/engage/chemrxiv/public-dashboard.   \n[11] Chen, W., Zhou, Y., Du, N., Huang, Y., Laudon, J., Chen, Z., Cui, C., 2023. Lifelong language pretraining with distribution-specialized experts, in: International Conference on Machine Learning, PMLR. pp. 5383\u20135395.   \n[12] Clark, A., de Las Casas, D., Guy, A., Mensch, A., Paganini, M., Hoffmann, J., Damoc, B., Hechtman, B., Cai, T., Borgeaud, S., et al., 2022. Unified scaling laws for routed language models, in: International conference on machine learning, PMLR. pp. 4057\u20134086.   \n[13] Cossu, A., Tuytelaars, T., Carta, A., Passaro, L., Lomonaco, V., Bacciu, D., 2022. Continual pre-training mitigates forgetting in language and vision. arXiv preprint arXiv:2205.09357 .   \n[14] Devlin, J., Chang, M.W., Lee, K., Toutanova, K., 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 .   \n[15] Eloundou, T., Manning, S., Mishkin, P., Rock, D., 2023. Gpts are gpts: An early look at the labor market impact potential of large language models. arXiv preprint arXiv:2303.10130 .   \n[16] Frantar, E., Riquelme, C., Houlsby, N., Alistarh, D., Evci, U., 2023. Scaling laws for sparselyconnected foundation models. arXiv preprint arXiv:2309.08520 .   \n[17] Gao, L., Schulman, J., Hilton, J., 2023. Scaling laws for reward model overoptimization, in: International Conference on Machine Learning, PMLR. pp. 10835\u201310866.   \n[18] Ghorbani, B., Firat, O., Freitag, M., Bapna, A., Krikun, M., Garcia, X., Chelba, C., Cherry, C., 2021. Scaling laws for neural machine translation. arXiv preprint arXiv:2109.07740 .   \n[19] Guo, D., Zhu, Q., Yang, D., Xie, Z., Dong, K., Zhang, W., Chen, G., Bi, X., Wu, Y., Li, Y., et al., 2024. Deepseek-coder: When the large language model meets programming\u2013the rise of code intelligence. arXiv preprint arXiv:2401.14196 .   \n[20] Guo, H., Yang, J., Liu, J., Yang, L., Chai, L., Bai, J., Peng, J., Hu, X., Chen, C., Zhang, D., et al., 2023. Owl: A large language model for it operations. arXiv preprint arXiv:2309.09298 .   \n[21] Gupta, K., Th\u00e9rien, B., Ibrahim, A., Richter, M.L., Anthony, Q., Belilovsky, E., Rish, I., Lesort, T., 2023. Continual pre-training of large language models: How to (re) warm your model? arXiv preprint arXiv:2308.04014 .   \n[22] Gururangan, S., Lewis, M., Holtzman, A., Smith, N.A., Zettlemoyer, L., 2021. Demix layers: Disentangling domains for modular language modeling. arXiv preprint arXiv:2108.05036 .   \n[23] Gururangan, S., Marasovic\u00b4, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey, D., Smith, N.A., 2020. Don\u2019t stop pretraining: Adapt language models to domains and tasks. arXiv preprint arXiv:2004.10964 .   \n[24] Henderson, P., Krass, M., Zheng, L., Guha, N., Manning, C.D., Jurafsky, D., Ho, D., 2022. Pile of law: Learning responsible data flitering from the law and a 256gb open-source legal dataset. Advances in Neural Information Processing Systems 35, 29217\u201329234.   \n[25] Hernandez, D., Kaplan, J., Henighan, T., McCandlish, S., 2021. Scaling laws for transfer. arXiv:2102.01293.   \n[26] Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, M.M.A., Yang, Y., Zhou, Y., 2017. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409 .   \n[27] Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D.d.L., Hendricks, L.A., Welbl, J., Clark, A., et al., 2022. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 .   \n[28] Huber, P.J., 1992. Robust estimation of a location parameter, in: Breakthroughs in statistics: Methodology and distribution. Springer, pp. 492\u2013518.   \n[29] Jiang, A.Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.S., Casas, D.d.l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al., 2023. Mistral 7b. arXiv preprint arXiv:2310.06825 .   \n[30] Jin, X., Zhang, D., Zhu, H., Xiao, W., Li, S.W., Wei, X., Arnold, A., Ren, X., 2021. Lifelong pretraining: Continually adapting language models to emerging corpora. arXiv preprint arXiv:2110.08534 .   \n[31] Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., Amodei, D., 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 .   \n[32] Ke, Z., Shao, Y., Lin, H., Konishi, T., Kim, G., Liu, B., 2023. Continual pre-training of language models. arXiv preprint arXiv:2302.03241 .   \n[33] Li, J., Wang, X., Wu, X., Zhang, Z., Xu, X., Fu, J., Tiwari, P., Wan, X., Wang, B., 2023. Huatuo-26m, a large-scale chinese medical qa dataset. arXiv:2305.01526.   \n[34] Liu, D.C., Nocedal, J., 1989. On the limited memory bfgs method for large scale optimization. Mathematical programming 45, 503\u2013528.   \n[35] Liu, J., Bai, Z., Zhang, Y., Zhang, C., Zhang, Y., Zhang, G., Wang, J., Que, H., Chen, Y., Su, W., et al., 2024a. E2-llm: Efficient and extreme length extension of large language models. arXiv preprint arXiv:2401.06951 .   \n[36] Liu, J., Zhang, C., Guo, J., Zhang, Y., Que, H., Deng, K., Bai, Z., Liu, J., Zhang, G., Wang, J., et al., 2024b. Ddk: Distilling domain knowledge for efficient large language models. arXiv preprint arXiv:2407.16154 .   \n[37] Liu, X., Yan, H., Zhang, S., An, C., Qiu, X., Lin, D., 2023. Scaling laws of rope-based extrapolation. arXiv preprint arXiv:2310.05209 .   \n[38] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V., 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 .   \n[39] Lozhkov, A., Li, R., Allal, L.B., Cassano, F., Lamy-Poirier, J., Tazi, N., Tang, A., Pykhtar, D., Liu, J., Wei, Y., et al., 2024. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173 .   \n[40] Mehta, S.V., Patil, D., Chandar, S., Strubell, E., 2023. An empirical investigation of the role of pre-training in lifelong learning. Journal of Machine Learning Research 24, 1\u201350.   \n[41] Mendieta, M., Han, B., Shi, X., Zhu, Y., Chen, C., 2023. Towards geospatial foundation models via continual pretraining, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 16806\u201316816.   \n[42] Motoki, F., Pinho Neto, V., Rodrigues, V., 2024. More human than human: Measuring chatgpt political bias. Public Choice 198, 3\u201323.   \n[43] Muennighoff, N., Rush, A., Barak, B., Le Scao, T., Tazi, N., Piktus, A., Pyysalo, S., Wolf, T., Raffel, C.A., 2024. Scaling data-constrained language models. Advances in Neural Information Processing Systems 36.   \n[44] Qu, X., Bai, Y., Ma, Y., Zhou, Z., Lo, K.M., Liu, J., Yuan, R., Min, L., Liu, X., Zhang, T., et al., 2024. Mupt: A generative symbolic music pretrained transformer. arXiv preprint arXiv:2404.06393 .   \n[45] Rae, J.W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et al., 2021. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446 .   \n[46] Rillig, M.C., \u00c5gerstrand, M., Bi, M., Gould, K.A., Sauerland, U., 2023. Risks and benefits of large language models for the environment. Environmental Science & Technology 57, 3464\u20133466.   \n[47] Rongali, S., Jagannatha, A., Rawat, B.P.S., Yu, H., 2020. Continual domain-tuning for pretrained language models. arXiv preprint arXiv:2004.02288 .   \n[48] Soldaini, L., Kinney, R., Bhagia, A., Schwenk, D., Atkinson, D., Authur, R., Bogin, B., Chandu, K., Dumas, J., Elazar, Y., et al., 2024. Dolma: An open corpus of three trillion tokens for language model pretraining research. arXiv preprint arXiv:2402.00159 .   \n[49] Sun, Y., Wang, S., Li, Y., Feng, S., Tian, H., Wu, H., Wang, H., 2020. Ernie 2.0: A continual pre-training framework for language understanding, in: Proceedings of the AAAI conference on artificial intelligence, pp. 8968\u20138975.   \n[50] Thakur, V., 2023. Unveiling gender bias in terms of profession across llms: Analyzing and addressing sociological implications. arXiv preprint arXiv:2307.09162 .   \n[51] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., Lample, G., 2023a. Llama: Open and efficient foundation language models. arXiv:2302.13971.   \n[52] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al., 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 .   \n[53] Wang, Z.M., Peng, Z., Que, H., Liu, J., Zhou, W., Wu, Y., Guo, H., Gan, R., Ni, Z., Zhang, M., Zhang, Z., Ouyang, W., Xu, K., Chen, W., Fu, J., Peng, J., 2023. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. arXiv preprint arXiv: 2310.00746 .   \n[54] Wu, T., Caccia, M., Li, Z., Li, Y.F., Qi, G., Haffari, G., 2021. Pretrained language model in continual learning: A comparative study, in: International conference on learning representations.   \n[55] Yang, A., Xiao, B., Wang, B., Zhang, B., Bian, C., Yin, C., Lv, C., Pan, D., Wang, D., Yan, D., Yang, F., Deng, F., Wang, F., Liu, F., Ai, G., Dong, G., Zhao, H., Xu, H., Sun, H., Zhang, H., Liu, H., Ji, J., Xie, J., Dai, J., Fang, K., Su, L., Song, L., Liu, L., Ru, L., Ma, L., Wang, M., Liu, M., Lin, M., Nie, N., Guo, P., Sun, R., Zhang, T., Li, T., Li, T., Cheng, W., Chen, W., Zeng, X., Wang, X., Chen, X., Men, X., Yu, X., Pan, X., Shen, Y., Wang, Y., Li, Y., Jiang, Y., Gao, Y., Zhang, Y., Zhou, Z., Wu, Z., 2023. Baichuan 2: Open large-scale language models. arXiv:2309.10305.   \n[56] Yao, Y., Duan, J., Xu, K., Cai, Y., Sun, Z., Zhang, Y., 2024. A survey on large language model (llm) security and privacy: The good, the bad, and the ugly. High-Confidence Computing , 100211.   \n[57] Ye, J., Liu, P., Sun, T., Zhou, Y., Zhan, J., Qiu, X., 2024. Data mixing laws: Optimizing data mixtures by predicting language modeling performance. arXiv preprint arXiv:2403.16952 .   \n[58] Zhang, B., Liu, Z., Cherry, C., Firat, O., 2024a. When scaling meets llm finetuning: The effect of data, model and finetuning method. arXiv preprint arXiv:2402.17193 .   \n[59] Zhang, G., Qu, S., Liu, J., Zhang, C., Lin, C., Yu, C.L., Pan, D., Cheng, E., Liu, J., Lin, Q., et al., 2024b. Map-neo: Highly capable and transparent bilingual large language model series. arXiv preprint arXiv:2405.19327 .   \n[60] Zhou, Z., Liu, J., Dong, Z., Liu, J., Yang, C., Ouyang, W., Qiao, Y., 2024. Emulated disalignment: Safety alignment for large language models may backfire!, in: Ku, L.W., Martins, A., Srikumar, V. (Eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Association for Computational Linguistics, Bangkok, Thailand. pp. 15810\u201315830. URL: https://aclanthology.org/2024.acl-long.842, doi:10.18653/v1/2024.acl-long.842. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Limitations and Future works ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Experiments on more downstream domains In our work, main experiments cover six downstream domains [39, 5, 24, 10, 44, 33]. In future works, it is important to experiments on more downstream domains. We will attempt to conduct experiments on CPT in more domains and fit the D-CPT Law as well as the Cross-Domain D-CPT Law. ", "page_idx": 13}, {"type": "text", "text": "Experiments on more LLMs We primarily conduct experiments based on Qwen-1.5 and lack exploration of other Pre-Trained Base LLMs. ", "page_idx": 13}, {"type": "text", "text": "Multilingualism We lack research on multilingualism settings. Although the medical data is in fact in Chinese, the other data are in English. Moreover, the experimental results show that the fitting results in the medical domain are poor compared to others. We lack a detailed experimental analysis of different language settings. In future research, we hope to realize cross-linguistic and multi-linguistic D-CPT Law and thereby further extend the generalizability of D-CPT Law. ", "page_idx": 13}, {"type": "text", "text": "Difficulty on fitting parameters We find that when using L-BFGS for ftiting, the initialization of the ftiting parameters is essential. Different parameter initializations can lead to significantly distinct results. Besides, we find that ftiting algorithms also matter, in subsequent works, we hope to compare different ftiting algorithms and design methods to reduce the dependency on the initialization of the fitting parameters. ", "page_idx": 13}, {"type": "text", "text": "Extensive training costs of Scaling Law Although we attempt to ameliorate the training costs and enhance the fitting efficiency of Scaling Law, which are detailed in Section 4.4 and Appendix I.1, Scaling Law [12, 4, 1, 37] still remains prohibitively expensive for the majority. We hope that future research endeavors will seek to reduce the training costs of Scaling Law, thereby facilitating a wider usage and understanding of these laws within the community. ", "page_idx": 13}, {"type": "text", "text": "B Broader Impacts ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "LLMs, particularly those involving pre-training on massive Internet data, have been identified to carry significant societal impacts and inherent biases [56, 50, 15, 2]. For instance, large language models (LLMs) may generate content that carries political bias [42]. With the rise of downstream applications of LLMs, there is a growing effort to limit their output of offensive content, rendering LLMs more controllable and mitigating their potential negative impacts. We hope that our research to make the downstream applications of LLMs more controllable. ", "page_idx": 13}, {"type": "text", "text": "Besides, LLMs have a significant environmental impact due to the substantial energy consumption required for their training and inference stages [46]. The extensive computational resources needed result in a high carbon footprint, thus raising concerns about the sustainability of such models in the context of global efforts to reduce greenhouse gas emissions. To this, our research can also partially reduce the consumption of GPU, thereby reducing the environmental impact of LLMs. ", "page_idx": 13}, {"type": "text", "text": "C Symbols ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To enhance the reader\u2019s experience, we have listed the symbols used in this paper in Table 9. ", "page_idx": 13}, {"type": "text", "text": "D D-CPT Law with a constant Mixture Ratio ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "OpenAI Scaling Law Kaplan et al.[31] propose a parameterization as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\nL=\\left[\\left(\\frac{N_{c}}{N}\\right)^{\\frac{\\alpha_{N}}{\\alpha_{D}}}+\\frac{D_{c}}{D}\\right]^{\\alpha_{D}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\{N_{c},D_{c},\\alpha_{N},\\alpha_{D}\\}$ are fitting parameters. ", "page_idx": 13}, {"type": "table", "img_path": "JzKFN5fWOk/tmp/35c9cffc80961bda2f9d5355b533bccc5b409e9fdf4c0de3ff9e6a498fc27c8b.jpg", "table_caption": ["Table 9: List of symbols presented in this paper. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Chinchilla Scaling Law Continuing along the trajectory established by OpenAI Scaling Law, Hoffmann et al.[27] propose a parameterization as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\nL=E+\\frac{A}{N^{\\alpha}}+\\frac{B}{D^{\\beta}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\{E,A,B,\\alpha,\\beta\\}$ are ftiting parameters. After ftiting, the Allocation problem can be resolved by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{\\displaystyle N_{o p t}=G\\left(\\frac{C}{6}\\right)^{a},}}&{{\\displaystyle D_{o p t}=G^{-1}\\left(\\frac{C}{6}\\right)^{b},}}\\\\ {{\\mathrm{where}}}&{{\\displaystyle G=\\left(\\frac{\\alpha A}{\\beta B}\\right)^{\\frac{1}{\\alpha+\\beta}},\\quad a=\\frac{\\beta}{\\alpha+\\beta},\\quad b=\\frac{\\alpha}{\\alpha+\\beta},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $N_{o p t}$ and $D_{o p t}$ represent the optimal value of model size and dataset size, respectively. ", "page_idx": 14}, {"type": "text", "text": "If we fix the mixture ratio in the training corpus, the D-CPT Law narrows down to the relationship involving only the model size $N$ and dataset size $D$ . Although previous works have proposed Scaling Law to describe the relationship between variables and performance, it has not been validated under our experimental setup. Here, we present the performance of OpenAI Scaling Law and Chinchilla Scaling Law in our experimental setup. For simplicity, we present results only in the code domain, with a 1:1 mixture ratio. The experimental results are shown in Figure 6 and Table 10. We find that the Chinchilla Scaling Law is obviously better in our experimental setup. ", "page_idx": 14}, {"type": "table", "img_path": "JzKFN5fWOk/tmp/50e28a36d14244b55e37f6f0c35005ec6aff3400ca0b9cb01d44f0a7e477a540.jpg", "table_caption": ["Table 10: The fitting performance of two laws on code-corpus with 1:1 mixture ratio. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "E Supplementary Materials of D-CPT Law ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "E.1 Explicit trends ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To provide a clear visualization of Equation 4, we have provided figures under 3 different settings, depicted in Figure 7, Figure 8, and Figure 9. All plots are trends of real data points. ", "page_idx": 14}, {"type": "image", "img_path": "JzKFN5fWOk/tmp/5b8d4f38969b1b55d2cf1c4e37ffd499a75e4a2322f632bc3a5911fd9aa81a7f.jpg", "img_caption": ["Figure 6: $L_{g}$ with respect to $D$ across multiple model sizes $N$ . Blue solid lines stand for real data points and orange dashed lines stand for the predicted curve. Fitting law is Chinchilla Scaling Law. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "JzKFN5fWOk/tmp/c7e202b5d7380e70d849d48752491a7f5918ef458b6e5b05429e55f0153412e2.jpg", "img_caption": ["Figure 7: Domain-corpus validation loss $L_{d}$ with respect to model size $N$ while $\\{D,r\\}$ are fixed, domain-corpus is law and domain-corpus mixture ratio $r_{d}=0.2$ . "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "JzKFN5fWOk/tmp/afdd054ea75f352f5e1f67dea64e855cab979d4a45a2b6d2c56f529def0b406d.jpg", "img_caption": ["Figure 8: Domain-corpus validation loss $L_{d}$ with respect to dataset size $D$ while $\\{N,r\\}$ are fixed, domain-corpus is law and domain-corpus mixture ratio $r_{d}=0.2$ . "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "JzKFN5fWOk/tmp/850e20c57b0a71a4aa9291b31ca90122be8ce3baece3a8a4f02569cef24fe65a.jpg", "img_caption": ["Domain-specific validation loss on different domain-specific mixture ratio ", "Figure 9: Domain-corpus validation loss $L_{d}$ with respect to domain-corpus mixture ratio $r_{d}$ while $\\{N,D\\}$ are fixed, domain-corpus is law and model size $N=1.8B$ . "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "E.2 Implicit trends ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we start from the perspective of experimental observations to illustrate why we can arrive at the conclusions presented in Equation 5. Subsequently, we will briefly analyze the underlying reasons for these implicit trends. For convenience, we replicate here for clarity: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}L}{\\partial D\\partial r}<0,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In mathematics, D-CPT Law has continuous second partial derivatives with respect to $D$ and $r$ . Based on Clairaut\u2019s Theorem, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\frac{\\partial^{2}L}{\\partial D\\partial r}}={\\frac{\\partial^{2}L}{\\partial r\\partial D}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which implies that the order of partial derivative does not affect the pattern presented in Equation 17. Based on the experiments, we have plotted the approximate values of dDg as a function of the general-corpus mixture ratio, as shown in Figure 10. Since data points are discrete, we take the difference of every $5\\mathrm{k}$ steps as approximate values for ddLDg . We present the curves of ddLDg with respect to $r_{g}$ across multiple dataset sizes $D$ . It is clear that $\\frac{d L_{g}}{d D}\\_$ monotonically decreases with $r_{g}$ . Thus, based on the real experimental observations, we can infer Equation 17. ", "page_idx": 16}, {"type": "text", "text": "In fact, there exists an explicit relationship between $r$ and $D$ , which can be represented as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{D_{g}=\\boldsymbol{r}_{g}\\cdot\\boldsymbol{D},\\;}\\\\ {D_{d}=\\boldsymbol{r}_{d}\\cdot\\boldsymbol{D},\\;}\\\\ {\\boldsymbol{r}_{g}+\\boldsymbol{r}_{d}=1,\\;}\\\\ {D_{g}+D_{d}=\\boldsymbol{D},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $D_{g}$ represents the general-corpus dataset size and $D_{d}$ represents the domain-corpus dataset size. If we focus on the domain-corpus validation loss, then $D_{g}$ is noisy data to domain-corpus, and $D_{d}$ is valid data to domain-corpus. If we consider $L_{d}$ , the domain-corpus validation loss, to be solely dependent on $D_{d}$ and $N$ , then $D$ and $r_{d}$ influence each other and cannot be considered independent. ", "page_idx": 16}, {"type": "image", "img_path": "JzKFN5fWOk/tmp/992fdfa6b1590f881ab0ae03b9b42a1f4ad7c176ca1d14207ed0596d872a84f8.jpg", "img_caption": ["Figure 10: Approximate values of $\\frac{\\partial L_{g}}{\\partial D}$ with respect to general-corpus mixture ratio $r_{g}$ while $\\{N,D\\}$ are fixed, domain-corpus is law and model size $N=1.8B$ . "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Previous works have treated $N$ and $D$ as independent variables, not influencing each other. However, in our works, $D$ and $r$ are not able to be independent of each other, both from the perspective of experimental phenomena and the explicit relationship. ", "page_idx": 17}, {"type": "text", "text": "Additionally, we can explain Equation 5 by the principle of data efficiency. The term $\\frac{d L}{d D}$ can be interpreted as the efficiency of each unit of data. With the increase of $r$ , the proportion of valid data in each unit of data rises while the proportion of noisy data diminishes, resulting in enhanced efficiency of each unit of data. Given that lower loss signifies improved model performance, ddDL consequently displays a decreasing trend as $r$ increases. ", "page_idx": 17}, {"type": "text", "text": "E.3 Details behind D-CPT Law ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we will first derive and demonstrate that D-CPT law satisfies the 4 requirements mentioned in Section 3.1. Subsequently, we will briefly describe the algorithm\u2019s setup and some minor improvements. ", "page_idx": 17}, {"type": "text", "text": "\u2022 Adaptability: The newly introduced variable, the mixture ratio $r$ , significantly differs from $N$ and $D$ , in that the range of values for $N$ and $D$ is greater than 0, whereas $r$ is limited to the range [0,1]. This means that $r$ should yield valid results at both 0 and 1, and it is crucial to ensure that values of $r$ near $0^{+}$ or $1^{-}$ do not cause $L$ to exhibit infinity. The trend of $L$ with respect to $r$ generally exhibits an initially rapid and subsequently slow pattern, a behavior that can be accurately modeled by a power function. However, positioning $r$ in the denominator leads to an asymptotic increase to infinity as $r$ approaches zero from the positive direction. To mitigate this issue, we have introduced a small positive bias $\\epsilon$ to $r$ , which is a fitting parameter. Typically, the value of $\\epsilon$ lies near 0.1. This adjustment effectively prevents explosive growth near $r=0^{+}$ . ", "page_idx": 17}, {"type": "text", "text": "\u2022 Explicit trends: ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{\\partial L}{\\partial N}=-\\displaystyle\\frac{\\alpha\\cdot A}{N^{\\alpha+1}}<0,}&{}\\\\ {\\displaystyle\\frac{\\partial L}{\\partial D}=-\\displaystyle\\frac{\\beta\\cdot B\\cdot r^{\\eta}}{D^{\\beta+1}}<0,}&{}\\\\ {\\displaystyle\\frac{\\partial L}{\\partial r}=\\displaystyle\\frac{B\\cdot\\eta}{D^{\\beta}}\\cdot r^{\\eta-1}-\\displaystyle\\frac{\\gamma\\cdot C}{r^{\\prime\\gamma+1}},\\quad\\mathrm{where}\\quad r^{\\prime}=r+\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It is important to note that for the third equation, having $\\begin{array}{r}{\\frac{\\partial L}{\\partial r}<0}\\end{array}$ requires certain constraints on the fitting parameters, specifically: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\eta>1}\\\\ {C>C_{0}}\\end{array}\\right.,\\quad\\mathrm{where}\\quad C_{0}=\\frac{B\\eta\\,(1+\\epsilon)^{\\gamma+1}}{\\gamma D_{m i n}^{\\beta}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If these two constraints are satisfied, we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial L}{\\partial r}=\\frac{B\\cdot\\eta}{D^{3}}\\cdot r^{\\eta-1}-\\frac{\\gamma\\cdot C}{r^{\\eta+1}}}\\\\ &{\\qquad=\\frac{B\\cdot\\eta}{D^{3}r^{\\gamma+1}}\\cdot\\left(r^{\\eta-1}r^{\\eta+1}-\\frac{\\gamma C D^{\\beta}}{B\\eta}\\right)}\\\\ &{\\qquad\\le\\frac{B\\cdot\\eta}{D^{3}r^{\\gamma+1}}\\cdot\\left((1+e)^{\\gamma+1}-\\frac{\\gamma C D^{\\beta}}{B\\eta}\\right)}\\\\ &{\\qquad\\le\\frac{\\gamma}{r^{\\gamma+1}}\\cdot\\left(\\frac{B\\eta\\left(1+e\\right)^{\\gamma+1}}{D^{\\beta}}-C\\right)}\\\\ &{\\qquad\\le\\frac{\\gamma}{r^{\\gamma+1}}\\cdot\\left(\\frac{B\\eta\\left(1+e\\right)^{\\gamma+1}}{D_{m i}^{\\beta}}-c\\right)}\\\\ &{\\qquad\\le\\frac{\\gamma}{r^{\\gamma+1}}\\cdot\\left(C_{m}-C\\right)<0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In our experimental setup, $D$ has a minimum value, with the minimum value $D_{m i n}$ being approximately 0.1311B. Therefore, as long as we set $C$ greater than $C_{0}$ and $\\eta$ greater than 1, the condition \u2202\u2202rL $\\begin{array}{r}{\\frac{\\partial L}{\\partial r}<0}\\end{array}$ can be satisfied. This effectively imposes constraints on the fitting parameters. In our actual fitting process, we have modified the algorithm to seamlessly incorporate these constraints. Specific details will be mentioned when introducing the algorithm. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Implicit trends: ", "text_level": 1, "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}L}{\\partial D\\partial r}=\\frac{\\partial^{2}L}{\\partial r\\partial D}=\\frac{\\partial\\left(-\\frac{\\beta B r^{\\eta}}{D^{\\beta+1}}\\right)}{\\partial r}=-\\frac{\\eta\\beta B r^{\\eta-1}}{D^{\\beta+1}}<0,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "\u2022 Consistency: ", "text_level": 1, "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle{\\cal L}(N,D,r=r_{0})={\\cal E}+\\frac{\\cal A}{N^{\\alpha}}+\\frac{B\\cdot r_{0}^{\\eta}}{D^{\\beta}}+\\frac{C}{(r_{0}+\\epsilon)^{\\gamma}}}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle~={\\cal E}_{0}+\\frac{\\cal A}{N^{\\alpha}}+\\frac{B_{0}}{D^{\\beta}},}}\\\\ {{\\mathrm{where}}}&{{\\displaystyle{\\cal E}_{0}={\\cal E}+\\frac{C}{(r_{0}+\\epsilon)^{\\gamma}}}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}&{{\\displaystyle B_{0}=B\\cdot r_{0}^{\\eta},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which means that if $r$ is a constant $r_{0}$ , then D-CPT Law can be transformed into a conventional Chinchilla Scaling Law. This suggests that under specific conditions where $r$ assumes a fixed value, D-CPT Law aligns with the more universally recognized Chinchilla Scaling Law. ", "page_idx": 18}, {"type": "text", "text": "Constrained L-BFGS We utilize L-BFGS to fit data points, with the objective being: ", "text_level": 1, "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{min}_{\\substack{a,b,c,e,\\alpha,\\beta,\\gamma,\\epsilon,\\eta}}\\mathrm{Huber}_{\\delta}\\big(L_{f i t}-\\log L_{r e a l}\\big),}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad a,b,c,e,\\alpha,\\beta,\\gamma,\\epsilon,\\eta}\\\\ &{\\therefore_{f i t}=\\mathrm{LSE}(e,a-\\alpha\\log N,b+(1+\\exp(\\eta_{1}))\\log r-\\beta\\log D,c_{1}-\\gamma\\log(r+\\epsilon),c_{0}-\\gamma\\log(r+\\epsilon))}\\\\ &{\\qquad\\qquad\\qquad\\mathrm{where}\\quad c_{0}=\\log C_{0},a=\\log A,b=\\log B,c_{1}=\\log C_{1},e=\\log E,}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad C=C_{0}+C_{1},\\eta=1+\\exp(\\eta_{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where LSE is the log-sum-exp operator. Our improvements to the algorithm primarily focus on the third and the last item. Previously, we mentioned that $C$ must be greater than $C_{0}$ to ensure the monotonic decrease of the D-CPT law with respect to $r$ . Without any restrictions and ftiting directly, it would sometimes lead to fitting results where $C$ does not satisfy $C\\geq C_{0}$ . Therefore, to ensure that the fitted $C$ must be greater than $C_{0}$ , we have indirectly imposed certain restrictions on the algorithm. We decomposed the original $C$ into two parts: $C_{0}$ and $C_{1}$ , and due to the characteristics of the exponential function, the fitted result of $C_{1}=\\exp c_{1}$ will be greater than 0. Consequently, $C$ will be greater than $C_{0}$ , i.e., ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{C=C_{0}+C_{1}=C_{0}+\\exp(c_{1})>C_{0},}&{}\\\\ {\\eta=1+\\exp(\\eta_{1})>1,}&{}\\\\ {\\mathrm{where}}&{C_{0}=\\frac{B(1+\\exp(\\eta_{1}))(1+\\epsilon)^{\\gamma+1}}{\\gamma D_{m i n}^{\\beta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Following Chinchilla Scaling Law, we find local minima of the objective function, initiating our search on a predefined grid of starting points as follows: $a\\in\\{-1.,-0.,...,5.\\},b\\in\\{-1.,0.,...,5.\\},c\\in$ {\u22121., 0., ..., 5.}, e \u2208 {\u22121., 0.5, ..., 1.}, \u03b1 \u2208 $\\{-0.5.,0.,0.5\\},\\beta\\quad\\in\\quad\\{-0.5.,0.,0.5\\},\\gamma\\quad\\in$ $\\left\\{-0.5,0.,0.5\\right\\}$ , $\\eta_{1}~\\in~\\{-0.5.,0.,0.5\\},\\epsilon~\\in~\\{0.,0.5\\}$ . Besides, we use $\\delta\\,=\\,10^{-3}$ for the Huber loss. ", "page_idx": 19}, {"type": "text", "text": "E.4 The choices of D-CPT Law Parameterizations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Section 4.2, we have proposed five possible parameterizations of the D-CPT Law. After analyzing the experimental results, we have selected $L_{3}$ as the final parameterization of the D-CPT Law. Could there be other parameterizations better than $L_{3}?$ We acknowledge that there may exist better parameterization than $L_{3}$ , but we believe that this is not crucial because our core objective is to find one that satisfies the 4 requirements outlined in Section 3.1, and we assume that when these 4 requirements are met, the parameterization is considered a good option of D-CPT Law. Specifically, regarding the choice of parameterization for the D-CPT Law, our research goal can be understood as finding a parameterization that matches the trend of real (N, D, r) data points and possesses certain mathematical properties. These trends and mathematical properties can be explicitly expressed as the 4 requirements in Section 3.1: Adaptability, Explicit trends, Implicit trends, and Consistency. Besides, in Section 4.2, we provide 5 parameterizations, only $L_{3}$ can meet all 4 requirements. ", "page_idx": 19}, {"type": "text", "text": "Moreover, as there are also other parameterizations that can meet these 4 requirements, we provide another 2 parameterizations that satisfy these 4 requirements as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{L_{6}=E+\\frac{A}{N^{\\alpha}}+\\frac{B\\cdot e^{r}}{D^{\\beta}}+\\frac{C}{r^{\\prime\\gamma}},}}\\\\ {{L_{7}=E+\\frac{A}{N^{\\alpha}}+\\frac{B\\cdot r^{\\prime\\eta}}{D^{\\beta}}+\\frac{C}{r^{\\prime\\gamma}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The ftiting results for $L_{6},L_{7}$ , and $L_{3}$ on general-corpus and domain-corpus are as listed in Table 11. We observe that when 4 requirements are met, the fitting results do not differ a lot. In conclusion, first, as there are many parameterizations that meet the 4 requirements, it is challenging to find the optimal parameterization. Second, the $L_{3}$ mentioned in the paper is relatively simple and meets the 4 requirements with good fitting results. ", "page_idx": 19}, {"type": "text", "text": "E.5 Compute resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Our main experiment requires approximately $150\\mathbf{k}$ hours of runtime on a single A100. ", "page_idx": 19}, {"type": "text", "text": "Table 11: Mean performance across $L_{3}$ , $L_{6}$ , and $L_{7}$ over six domains. $\\mathbf{\\hat{U}}^{\\bullet}$ and \u201cD\u201d denote general and downstream domains. ", "page_idx": 20}, {"type": "table", "img_path": "JzKFN5fWOk/tmp/23ab344ddc2e88d32bbe9e7c4ce3e3b1bfdc4bfbe0956163a1ac99e89312a16e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "F Supplementary Materials of Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "F.1 Validation datasets collection ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Specifically, for each domain, we first randomly select 5,000 samples from the original dataset, and then we use four open-sourced LLMs (i.e., Qwen-1.5 72B [6], Yi-34B [3], LLaMA2-13B [52], InternLM2-20B [8]) to compute the perplexity (PPL) and sort these samples based on the PPL values. Specifically, a lower PPL value denotes higher fluency of the data indicated by the model. If a sample ranks in the bottom $10\\%$ under all four open-source LLMs, we consider this sample to be noisy and exclude it. Subsequently, we randomly sample 1,000 samples from the flitered sample pool to serve as the validation set for each domain. In this way, we can obtain a high-quality validation set for all domains. ", "page_idx": 20}, {"type": "text", "text": "F.2 Hyperparameters ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The hyperparameters for the experiments are listed in Table 12. ", "page_idx": 20}, {"type": "table", "img_path": "JzKFN5fWOk/tmp/4db5f5729558d9a5b926fa57e11f9129a8d4aabd11c0e50e6ac5b96377f0731c.jpg", "table_caption": ["Table 12: The list of hyperparameters. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "G Mathematical Derivation behind use case ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "G.1 Usage 1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "First, we will standardize the notation: $r_{g}$ denotes the proportion of the general corpus, $r_{d}$ represents the proportion of the domain-corpus, $L_{g}$ signifies the general-corpus validation loss, $L_{d}$ indicates the domain-corpus validation loss, $D$ represents the dataset size, and $N$ denotes the model size. Therefore, we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{L_{g}=E+\\displaystyle\\frac{A}{N^{\\alpha}}+\\frac{B\\cdot(1-r_{d})^{\\eta}}{D^{\\beta}}+\\frac{C}{(1-r_{d}+\\epsilon)^{\\gamma}},}}\\\\ {{L_{d}=E+\\displaystyle\\frac{A}{N^{\\alpha}}+\\frac{B\\cdot r_{d}^{\\eta}}{D^{\\beta}}+\\frac{C}{(r_{d}+\\epsilon)^{\\gamma}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that we have the loss $L$ monotonically decreasing with respect to $r$ , therefore we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\partial L_{g}}{\\partial r_{g}}<0\\implies\\frac{\\partial L_{g}}{\\partial(1-r_{d})}<0\\implies\\frac{\\partial L_{g}}{\\partial r_{d}}>0,}\\\\ {\\frac{\\partial L_{d}}{\\partial r_{d}}<0\\implies\\frac{\\partial L_{d}}{\\partial(1-r_{g})}<0\\implies\\frac{\\partial L_{d}}{\\partial r_{g}}>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Within the context of D-CPT, we focus on $L_{d}$ , the domain-corpus validation loss. As the proportion of domain-corpus $r_{d}$ increases, $L_{d}$ is expected to decrease, indicating an improvement in domainspecific performance. Conversely, $L_{g}$ , the general-corpus validation loss, is expected to increase with the growing $r_{d}$ , suggesting a decline in general abilities. Therefore, we need to strike a balance between general and domain-specific abilities. To be specific, we will revisit the objective function of Usage 1: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{argmin}_{r_{d}}L_{d}(N=N_{0},D=D_{0},r_{d})\\quad\\mathrm{s.t.}\\quad\\frac{L_{g}-L_{g}^{0}}{L_{g}^{0}}<T,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\boldsymbol{L}_{g}^{0}$ represents the initial general validation loss. Since $L_{g}$ monotonically increases with $r_{d}$ , a maximal $r_{d}$ will certainly be attained under the constraint. Concurrently, as $L_{d}$ monotonically decreases with $r_{d}$ , there must exist a unique $r_{d}$ that minimizes $L_{d}$ . ", "page_idx": 21}, {"type": "text", "text": "G.2 Usage 2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For simplicity, we restate the objective function for usage 2: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\underset{r_{d}}{\\operatorname{argmin}}\\,L_{d}(N=N_{0},D=\\frac{D_{d}}{r_{d}},r_{d})\\quad\\mathrm{s.t.}\\quad D_{d}=D_{d}^{0},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $D_{d}$ denotes the domain-corpus dataset size, for $L_{d}$ in format of D-CPT Law, we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{d}(N=N_{0},D=\\frac{D_{d}}{r_{d}},r_{d})=E+\\frac{A}{N_{0}^{\\alpha}}+\\frac{B r_{d}^{\\eta}}{(\\frac{D_{d}^{0}}{r_{d}})^{\\beta}}+\\frac{C}{(r_{d}^{\\prime})^{\\gamma}},\\quad\\mathrm{where}\\quad r_{d}^{\\prime}=r_{d}+\\epsilon,}\\\\ &{\\frac{d L_{d}}{d r_{d}}=\\frac{B(\\eta+\\beta)}{(D_{d}^{0})^{\\beta}}r_{d}^{\\eta+\\beta-1}-\\frac{\\gamma C}{(r_{d}^{\\prime})^{\\gamma+1}}\\implies}\\\\ &{\\frac{d^{2}L_{d}}{d r_{d}^{2}}=\\frac{B(\\eta+\\beta)(\\beta+\\eta-1)}{(D_{d}^{0})^{\\beta}}r_{d}^{\\eta+\\beta-2}+\\frac{\\gamma(\\gamma+1)C}{(r_{d}^{\\prime})^{\\gamma+2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Based on Appendix E.3, we have $\\eta>1$ , therefore we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\eta>1\\implies\\frac{d^{2}L_{d}}{d r_{d}^{2}}>0,}\\\\ {\\displaystyle\\frac{d L_{d}}{d r_{d}}(r_{d}=0)=-\\frac{\\gamma C}{\\epsilon^{\\gamma+1}}<0,}\\\\ {\\displaystyle\\frac{d L_{d}}{d r_{d}}(r_{d}=1)=\\frac{B(\\eta+\\beta)}{(D_{d}^{0})^{\\beta}}-\\frac{\\gamma C}{(1+\\epsilon)^{\\gamma+1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The derivativ  ddrLdd e is continuously differentiable and monotonically increasing. Given that $\\frac{d L_{d}}{d r_{d}}$ is negative at $r_{d}=0$ and if $\\frac{d L_{d}}{d r_{d}}$ is greater than 0 at $r_{d}=1$ , then it follows that Equation 49 attains its minimum within the interval $[0<r_{d}<1]^{3}$ . Therefore, to ensure the existence of a valid minimum for the objective function 48, the following conditions must be satisfied: ", "page_idx": 21}, {"type": "equation", "text": "$$\nD_{d}^{0}<\\left(\\frac{B(\\eta+\\beta)(1+\\epsilon)^{\\gamma}+1}{\\gamma C}\\right)^{\\frac{1}{\\beta}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "G.3 Usage 3 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For convenience, we repeat the objective function of resource allocation as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\underset{N,D}{\\operatorname{argmin}}}\\,L(N,D)\\quad{\\mathrm{s.t.}}\\quad{\\mathrm{FLOPs}}(N,D)=C.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Following [31], we calculate compute budget $\\mathbf{C}$ by: ", "page_idx": 22}, {"type": "equation", "text": "$$\nC\\approx6N D.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To validate its effectiveness in real-world scenarios, we take the law domain as an example and by fixing the mixture ratio at 1:1, fit D-CPT Law. We fix compute budget $C=5e^{19}$ . Subsequently, based on the Efficient Frontier of Chinchilla[27], we obtain: ", "page_idx": 22}, {"type": "equation", "text": "$$\na=0.6252,b=0.3748,G=4.1282,N_{o p t}=15.54\\mathrm{B},D_{o p t}=0.536\\mathrm{B}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "As the closest available model size to the optimal model size indicated by Qwen1.5 is 14B, we conducted our experiments using this 14B model. The experimental results are as shown in Table 13. The experimental results reveal that the model sizes of 0.5B, 1.8B, and 4B suffer from data insufficiency. The optimal model size (14B) indeed exhibits the best performance. ", "page_idx": 22}, {"type": "text", "text": "Table 13: Domain-corpus validation loss with respect to various model sizes and dataset sizes while keeping the same compute budget. ", "page_idx": 22}, {"type": "table", "img_path": "JzKFN5fWOk/tmp/98c232539ce1b22d5b261402a2c523c808ae552e22cc079865b42c0b1448cdc4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "H Details behind Domain-specific Learnable Coefficient ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In practice, the data points we obtain are discrete, thus we can only utilize approximate values to express $k_{2}$ and $k_{3}$ . Specifically, we use the difference between the initial validation loss and the validation loss after $5\\mathrm{k}$ -steps4 continual pre-training,i.e., ", "page_idx": 22}, {"type": "equation", "text": "$$\nk_{2}=L_{\\mathrm{0steps}}-L_{\\mathrm{5000steps}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Besides, we define $k_{3}$ as the difference in the decline values between the intervals of 0 to $5\\mathrm{k}$ steps and $5\\mathrm{k}$ to $10\\mathbf{k}$ steps,i.e., ", "page_idx": 22}, {"type": "equation", "text": "$$\nk_{3}=(L_{\\mathrm{0steps}}-L_{\\mathrm{5000steps}})-(L_{\\mathrm{5000steps}}-L_{\\mathrm{10000steps}})=L_{\\mathrm{0steps}}-L_{\\mathrm{10000steps}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lastly, we denote $k_{1}$ as the validation loss obtained after training for 1k steps. ", "page_idx": 22}, {"type": "text", "text": "I Further Analysis ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "I.1 Fitting Efficiency ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "As each data point requires computational resources, we also investigate to improve the fitting efficiency with relatively low computational resources. In Table 14, we have compared different sampling methods for data points and introduced a decay sampling method based on the exponential decay function to enhance fitting efficiency. Specifically, we focus on the fitting efficiency across dataset size while maintaining a constant model size. ", "page_idx": 23}, {"type": "table", "img_path": "JzKFN5fWOk/tmp/4e637f97b93bc1b7d33dc645cb11a6a1283496f16b0f7ad2cf4ffcc9057a2f77.jpg", "table_caption": ["Table 14: The fitting performance of different sampling methods. "], "table_footnote": ["\\* For Resource consumption, we focus on evaluation costs and storage costs. "], "page_idx": 23}, {"type": "text", "text": "We have experimented with 4 different sampling methods, as follows: ", "page_idx": 23}, {"type": "text", "text": "\u2022 $M_{1}$ : Dense sampling, evaluating validation loss every 1,000 steps.   \n\u2022 $M_{2}$ : Sparse sampling, evaluating validation loss every 5,000 steps.   \n\u2022 $M_{3}$ : Sectional sampling, evaluating every 4,000 steps in the initial $60\\%$ steps, every 8,000 steps in the remaining $40\\%$ steps.   \n\u2022 $M_{4}$ : Sampling-based on an exponential decay function, detailed in Appendix I.2. ", "page_idx": 23}, {"type": "text", "text": "Experimental results show that the performance of $M_{1}$ is relatively poor. In situations where resource consumption is comparatively high, no significant improvement in fitting performance is observed, thus indicating that the sampling density in our main experiments is excessively high. The overall performance of $M_{3}$ and $M_{4}$ surpasses that of $M_{2}$ because both $M_{3}$ and $M_{4}$ adopt a strategy of dense sampling in the initial phase and sparser sampling in the later phase. The trend of $L$ with respect to $D$ also shifts from rapid to slow changes, and sampling more points during phases of faster decline can considerably enhance ftiting efficiency. However, the sampling setup of $M_{3}$ is of fixed paradigm and the sampling function follows a step-wise pattern. Of course, the overall performance of $M_{4}$ is slightly better than $M_{3}$ , it also offers a richer paradigm. In summary, sampling more points in the early phase of $D$ can improve the overall fitting efficiency. In practical applications, it has the potential to save on evaluation costs and storage costs. ", "page_idx": 23}, {"type": "text", "text": "I.2 Decay function ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In our main experiments, each experiment trains for 200,000 steps, with evaluations every 1,000 steps, resulting in a total of 200 data points. The decay function is represented as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\nf(x)=e^{-\\lambda x}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For $M_{4}$ in Section I.1, we set the decay parameter $\\lambda$ to 0.02 which yields 45 data points sampled.   \nFigure 11 illustrates the decay function. ", "page_idx": 23}, {"type": "text", "text": "I.3 Analysis of near-zero ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Interestingly, we have found from the experiments that the trends between $L$ and $D$ are reversed when $r$ approaches 0, in this section, we will explore it in depth and find that D-CPT Law between $L$ and $D$ has an inflection point $r_{i}$ to change its trend. ", "page_idx": 23}, {"type": "text", "text": "We take the Law domain for example, the experimental results show that most of $L_{g}$ decreases strictly with $r_{g}$ when $N$ and $D$ are fixed, which is consistent with D-CPT Law. However, as $r_{g}$ approaches ", "page_idx": 23}, {"type": "image", "img_path": "JzKFN5fWOk/tmp/93290d922874c395b2dd414e4321d7b9466ca5d0fd649585e328a707a41c37c5.jpg", "img_caption": ["Figure 11: Illustration of decay function. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "JzKFN5fWOk/tmp/5401eca013c1c39bcaf38ad2d655a86a3df2d6945d7165e986fd73cb67d1e76b.jpg", "img_caption": ["Figure 12: General validation loss with respect to dataset size across various mixture ratios, the domain-specific corpus is the law and $N=1.8B$ . "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "0, the trend of $\\mathrm{L}$ changes. Through analysis of Figure 12, we observe that when $r_{g}$ is greater than 0.1, $L_{g}$ monotonically decreases with $D$ , which aligns with the findings of D-CPT Law and previous works. However, when $r_{g}$ is less than or equal to 0.05, $L_{g}$ monotonically increases with $D$ . This phenomenon is not limited to just one domain, we find that almost all domains exhibit this kind of behavior. We name the mixture ratio which changes the trends of $L$ as inflection point $r_{i}$ . Accurately pinpointing $r_{i}$ is challenging. From an experimental perspective, it requires repeated experiments to approach $r_{i}$ progressively, which requires high experimental costs. Additionally, the exact value of $r_{i}$ changes across different domains, in our experimental setup, we find that $r_{i}$ for 6 domains all fall between 0 and 0.1. ", "page_idx": 24}, {"type": "text", "text": "When the mixture ratio is less than the inflection point, $L$ monotonically increases with $D$ , which is inconsistent with the D-CPT Law. Therefore, the D-CPT Law predicts poorly when the mixture ratio is less than $r_{i}$ . Fortunately, predictions when the mixture ratio is less than $r_{i}$ are meaningless in the context of our works for two reasons: (1) In practical situations, we may not be particularly concerned with cases where the mixture ratio is very small, as the inflection point in most domains is less than 0.05. (2) When the mixture ratio is lower than $r_{i}$ , $L$ monotonically increases with $D$ , meaning that as the training cost increases, the performance of the model worsens. This is contrary to our initial objective, as we hope that after D-CPT, the domain-specific ability is enhanced. Thus, predictions when the mixture ratio is less than $r_{i}$ are considered meaningless. ", "page_idx": 24}, {"type": "text", "text": "Of course, if we collect data points of small mixture ratios which means that the curves for these data points all show $L$ increasing with $D$ , then we can fit these data points. In that case, the fitting parameter $B$ in D-CPT Law would be a negative value. If we know accurately the value of $r_{i}$ , we can express D-CPT Law in form of a piecewise function or represent it with a unified equation. However, the problem lies in precisely determining the value of $r_{i}$ . In future works, we hope to propose a low-cost method to accurately determine the value of $r_{i}$ . For example, we could conduct experiments with both small and large mixture ratios and fti them separately, then determine the value of $r_{i}$ based on the intersection of two resulting laws. ", "page_idx": 25}, {"type": "text", "text": "J Supplementary Tables ", "text_level": 1, "page_idx": 26}, {"type": "table", "img_path": "JzKFN5fWOk/tmp/da1ede91fa7f6364eddbf7bdc61dbd6d449757346e96c2e8f9825048709b5c80.jpg", "table_caption": ["Table 15: Supplementary Table of Table 1. Huber loss of 5 parameterizations across 6 domains. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "JzKFN5fWOk/tmp/b96434ae50589a85bceef9dc50ef142d9c2ad9dee8e5bef6e34c72b67da31e9f.jpg", "table_caption": ["Table 16: Supplementary Table of Table 1. $R^{2}$ of 5 parameterizations across 6 domains. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "JzKFN5fWOk/tmp/10742eb4a92e9ca098a270e0a787f872ecbe5612ff6b3a7d9058058d8a0744d5.jpg", "table_caption": ["Table 17: Supplementary Table of Table 2. Huber loss of 5 parameterizations across 6 domains, each unit displays the average value of 3-fold cross-validation. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "JzKFN5fWOk/tmp/e55a1f54af164fb196ee1ff064f05330fcabd119b93c5a3ee598e5102b3205f7.jpg", "table_caption": ["Table 18: Supplementary Table of Table 2. $R^{2}$ of 5 parameterizations across 6 domains, each unit displays the average value of 3-fold cross-validation. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "JzKFN5fWOk/tmp/acb1ac2b22e195c36de4e96cdf4ddfff9bb023fd80b4175729f85a7f192d6345.jpg", "table_caption": ["Table 19: Supplementary Table of Table 3. Huber loss of 5 parameterizations across 6 domains, each unit displays the average value of 3-fold cross-validation. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "JzKFN5fWOk/tmp/d83063488a8c9fae73cd92e86973412cf96afc16ca3bdfce864c1f71c1433c80.jpg", "table_caption": ["Table 20: Supplementary Table of Table 3. $R^{2}$ of 5 parameterizations across 6 domains, each unit displays the average value of 3-fold cross-validation. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "JzKFN5fWOk/tmp/4b893b74c7cc39a23c41dbe6716ed0afca4d551adfcc8817ffcf463a087f74c2.jpg", "table_caption": ["Table 21: Supplementary Table of Table 4. Huber loss of 5 parameterizations across 6 domains, each unit displays the average value of k-fold cross-validation. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "JzKFN5fWOk/tmp/89cca0055e77bac2c72a38bfe0769a699b890988d1e96a50a5cdd71db174be42.jpg", "table_caption": ["Table 22: Supplementary Table of Table 4. $R^{2}$ of 5 parameterizations across 6 domains, each unit displays the average value of $\\boldsymbol{\\mathrm{k}}$ -fold cross-validation. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "K Supplementary Figures ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "K.1 Effectiveness of D-CPT Law ", "page_idx": 28}, {"type": "image", "img_path": "JzKFN5fWOk/tmp/30e1dddf608b5d53d06d33ad4d64a39a65f565d637dfdf1726cf0001c1c28eb8.jpg", "img_caption": ["Figure 13: Effectiveness of D-CPT Law $\\left(L_{3}\\right)$ : General-corpus validation loss $L_{g}$ with respect to dataset size $D$ across different model size $N$ , domain-corpus is code and general-corpus mixture ratio $r_{g}$ is 0.33. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "JzKFN5fWOk/tmp/791e41298542055d410e8e5d3b27fcd2330e6fbb6744e7a246b01c34297320b1.jpg", "img_caption": ["Figure 14: Effectiveness of D-CPT Law $(L_{3})$ : Domain-corpus validation loss $L_{d}$ with respect to dataset size $D$ across different model size $N$ , domain-corpus is chemistry and domain-corpus mixture ratio $r_{d}$ is 0.5. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "JzKFN5fWOk/tmp/7953e6d8c3f0a263c8a8b5baf9953660dbed35f28341a27997ea9a1c593e4c1f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 15: Dataset Size Generalizability of the D-CPT Law: General-corpus validation loss $L_{g}$ with respect to dataset size $D$ across various model sizes $N$ , domain-corpus is math and general-corpus mixture ratio $r_{g}=0.8$ . The experiments use data from the first 2/3 of the steps for fitting, to verify whether the D-CPT Law exhibits generalizability across different dataset sizes. ", "page_idx": 29}, {"type": "image", "img_path": "JzKFN5fWOk/tmp/cbe6c39cc108ec672a6e6a8eddeb8e160d45a6d6b698c7a29cd3961fcb7c3059.jpg", "img_caption": ["K.3 Domain Generalizability of the Cross-Domain D-CPT Law "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 16: Domain Generalizability of the Cross-Domain D-CPT Law: General-corpus validation loss $L_{g}$ with respect to dataset size $D$ across various model sizes $N$ , domain-corpus is Music and general-corpus mixture ratio $r_{g}=0.8$ . The experiments use data points from {Code, Math, Law, Medical} domains for ftiting, to verify whether the Cross D-CPT Law exhibits generalizability across different domains. ", "page_idx": 29}, {"type": "image", "img_path": "JzKFN5fWOk/tmp/02ed907a6866391d92fdd9e686f4b6a657c3b782bb14a26ab74493c22d0bf07d.jpg", "img_caption": ["Figure 17: Domain Generalizability of the Cross-Domain D-CPT Law: General-corpus validation loss $L_{g}$ with respect to dataset size $D$ across various model sizes $N$ , domain-corpus is Chemistry and general-corpus mixture ratio $r_{g}=0.8$ . The experiments use data points from {Code, Math, Law, Medical} domains for ftiting, to verify whether the Cross D-CPT Law exhibits generalizability across different domains. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have clearly stated our main contributions in our Introduction and Abstract. The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We have discussed the limitations of our work in Appendix A ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Certainly, we have corresponding detailed derivations for all theoretical results, such as Appendix E.3 and Appendix G. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The main experimental results are reproducible. We have mentioned our experimental setup in Section 4 and Appendix F.2. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [No] ", "page_idx": 32}, {"type": "text", "text": "Justification: The code belongs to the company\u2019s intellectual property, but the data can be downloaded from open-source repositories. For example, the Dolma dataset can be downloaded from https://github.com/allenai/dolma. In the main text, we cited all the data sources. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We specify all the training and test details in Section 4 and Appendix F.2. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our experimental results have been validated through cross-validation and repeated experiments. The effectiveness and significance of the method have been confirmed by multiple domains and multiple $\\boldsymbol{\\mathrm{k}}$ -fold cross-validation experimental results. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have discussed our compute resources in Appendix E.5. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: Our research respects the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have discussed the broader impacts of our works in Appendix B. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 33}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our paper poses no such risks. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We have respected all the licenses and terms of use for CPT data and the Qwen model. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: We do not release new assets. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our works do not involve crowdsourcing and research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our works do not involve crowdsourcing and research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}]