[{"figure_path": "oPvBnPTbQv/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of DETR-like method and our proposed method for visual grounding. (a) The existing method typically adopts the random initialization queries directly into the decoder to predict the target object. (b) We introduce the query adaption module (QA) to learn target-related context progressively, providing valuable prior knowledge for the decoder. (c) The attention map of the last layer in every QA module and decoder (bottom), respectively.", "description": "This figure compares a typical DETR-like visual grounding method with the proposed RefFormer method.  Panel (a) shows the standard approach, where randomly initialized queries are fed directly to the decoder for object localization.  Panel (b) illustrates the RefFormer architecture, which incorporates a query adaptation module (QA). The QA module iteratively refines the decoder's input by learning context-related information from multiple levels within the CLIP backbone. This refined query provides a valuable prior, allowing the decoder to focus more effectively on the target object. Finally, panel (c) shows attention maps from the last layer of each QA module and the decoder, visually demonstrating the improved focus on the target object by the proposed method.", "section": "1 Introduction"}, {"figure_path": "oPvBnPTbQv/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of RefFormer. It adopts a DETR-like structure, consisting of a query adaptation (QA) module that seamlessly integrates into various layers of CLIP, along with a task-specific decoder. By incorporating the QA module, RefFormer can iteratively refine the target-related context and generate referential queries, which provide the decoder with prior context.", "description": "This figure illustrates the architecture of RefFormer, a novel approach for visual grounding.  RefFormer uses a DETR-like structure and integrates a Query Adaptation (QA) module into multiple layers of the CLIP model. The QA module refines the target-related context iteratively, generating referential queries that improve the decoder's performance and accuracy in locating target objects.  The figure shows the flow of image and text features through CLIP, the iterative refinement process within the QA modules, and the final prediction by the task-specific decoder.", "section": "4 Method"}, {"figure_path": "oPvBnPTbQv/figures/figures_4_1.jpg", "caption": "Figure 3: Illustration of our proposed Query Adaption Module, which mainly consists of CAMF and TR modules to generate the referential queries and promote the multi-modal features interaction. \"R\" represents the feature modulation.", "description": "This figure shows the architecture of the Query Adaption Module (QA) which is a core component of the RefFormer model.  The QA module takes image and text features as input and generates referential queries that are then used by the decoder. It consists of two main parts:\n\n1.  **CAMF (Condition Aggregation and Multi-modal Fusion):** This module fuses image and text features with learnable queries to generate a refined representation of the target object.\n2.  **TR (Target-related Context Refinement):** This module refines the output from the CAMF module using self-attention to further enhance target-related contextual semantics.\n\nThe module iteratively refines the target-related context at different layers of the CLIP backbone. The output is a refined referential query and updated image and text features.", "section": "4.1 Query Adaptation Module (QA)"}, {"figure_path": "oPvBnPTbQv/figures/figures_9_1.jpg", "caption": "Figure 4: Convergence curves. Our method achieves better results with fewer training epochs on RefCOCOg.", "description": "This figure shows the convergence curves of the proposed RefFormer method compared to two other visual grounding methods, TransVG and RefTR, on the RefCOCOg dataset.  The y-axis represents the Prec@0.5 metric (precision at an IoU threshold of 0.5), indicating the accuracy of object localization. The x-axis represents the number of training epochs.  The plot demonstrates that RefFormer achieves comparable or better performance than the other methods, but with significantly fewer training epochs, indicating improved training efficiency.", "section": "5 Experiment"}, {"figure_path": "oPvBnPTbQv/figures/figures_9_2.jpg", "caption": "Figure 5: Qualitative results on RefCOCOg. The bounding boxes in green and red correspond to predictions of our model and the ground truth. Columns 2\u20136 showcase the attention maps generated by each QA module, while the last column represents the attention map from the decoder.", "description": "This figure shows several examples of visual grounding results using the RefFormer model. Each row represents a different image and its corresponding natural language expression. The first column shows the input image with a green bounding box indicating the ground truth location of the object mentioned in the expression. The following six columns visualize the attention maps of the query adaptation (QA) module at different layers of the CLIP backbone network. These attention maps highlight the regions of the image that the QA module focuses on at each layer, showcasing the progressive refinement of attention towards the target object. The last column displays the attention map from the decoder, indicating the final prediction made by the model. The attention maps illustrate how the QA module iteratively refines the target-related context and guides the decoder towards a more accurate localization of the object.", "section": "5.5 Qualitative Results"}, {"figure_path": "oPvBnPTbQv/figures/figures_14_1.jpg", "caption": "Figure 6: The performance of different numbers of learnable queries on RefCOCOg.", "description": "This figure shows the performance (Prec@0.5) of the model on the RefCOCOg dataset as the number of learnable queries is varied from 1 to 6.  The left subplot displays the validation performance, while the right subplot shows the test performance.  The results indicate an optimal number of learnable queries exists where increasing the number beyond that point does not significantly improve performance.", "section": "A.2 Effect on the number of learnable queries"}, {"figure_path": "oPvBnPTbQv/figures/figures_15_1.jpg", "caption": "Figure 5: Qualitative results on RefCOCOg. The bounding boxes in green and red correspond to predictions of our model and the ground truth. Columns 2\u20136 showcase the attention maps generated by each QA module, while the last column represents the attention map from the decoder.", "description": "This figure shows the qualitative results of the proposed RefFormer model on the RefCOCOg dataset.  Each row represents a different image and its corresponding caption. The first column displays the original image with the ground truth bounding box (green) and the model's prediction (red). The following columns visualize the attention maps at different layers of the Query Adaption Modules (QA) and the final decoder. These attention maps illustrate how the model focuses its attention on the target object throughout the process. The progression from noisy attention maps to focused attention demonstrates the effectiveness of the model in refining its understanding of the target object based on textual information and multi-level visual features.", "section": "5.5 Qualitative Results"}, {"figure_path": "oPvBnPTbQv/figures/figures_15_2.jpg", "caption": "Figure 5: Qualitative results on RefCOCOg. The bounding boxes in green and red correspond to predictions of our model and the ground truth. Columns 2\u20136 showcase the attention maps generated by each QA module, while the last column represents the attention map from the decoder.", "description": "This figure shows qualitative results of the RefFormer model on the RefCOCOg dataset.  For each image, the green bounding box indicates the ground truth location of the target object, while the red box represents the model's prediction.  The columns between the image and the final decoder output show the attention maps from the intermediate query adaptation (QA) modules at different layers of the CLIP backbone. These attention maps visualize how the model focuses on the relevant areas of the image as it refines the target-related context in each layer. The final column displays the attention map of the decoder, demonstrating the model's final attention to the target.", "section": "5.5 Qualitative Results"}, {"figure_path": "oPvBnPTbQv/figures/figures_15_3.jpg", "caption": "Figure 5: Qualitative results on RefCOCOg. The bounding boxes in green and red correspond to predictions of our model and the ground truth. Columns 2\u20136 showcase the attention maps generated by each QA module, while the last column represents the attention map from the decoder.", "description": "This figure shows qualitative results of the RefFormer model on the RefCOCOg dataset. Each row represents a different image and its corresponding caption. The first column shows the original image with the ground truth bounding box (green) and the model's prediction (red). The next six columns visualize the attention maps from each of the six query adaptation (QA) modules within the RefFormer, illustrating how the model focuses on the target object across different layers. The final column displays the attention map from the decoder, highlighting where the model ultimately focuses its attention for object localization.", "section": "5.5 Qualitative Results"}, {"figure_path": "oPvBnPTbQv/figures/figures_15_4.jpg", "caption": "Figure 1: Comparison of DETR-like method and our proposed method for visual grounding. (a) The existing method typically adopts the random initialization queries directly into the decoder to predict the target object. (b) We introduce the query adaption module (QA) to learn target-related context progressively, providing valuable prior knowledge for the decoder. (c) The attention map of the last layer in every QA module and decoder (bottom), respectively.", "description": "This figure compares a standard DETR-like visual grounding method with the proposed RefFormer method.  Panel (a) shows the existing method which uses randomly initialized queries fed directly into the decoder to predict the target object's bounding box.  Panel (b) illustrates the RefFormer method, which incorporates a Query Adaption Module (QA). The QA module learns target-related context iteratively, refining it layer by layer and providing more informative prior knowledge to the decoder.  This leads to improved accuracy in locating the target object. Panel (c) displays the attention maps of the final layer within both the QA module and the decoder, visualizing where the models focus their attention during the prediction process.", "section": "1 Introduction"}, {"figure_path": "oPvBnPTbQv/figures/figures_15_5.jpg", "caption": "Figure 5: Qualitative results on RefCOCOg. The bounding boxes in green and red correspond to predictions of our model and the ground truth. Columns 2\u20136 showcase the attention maps generated by each QA module, while the last column represents the attention map from the decoder.", "description": "This figure displays several examples of visual grounding results. Each row shows an image, the ground truth bounding box (red), the model's prediction (green), and attention maps from each query adaptation module (QA) and the decoder. The attention maps visualize where the model focuses its attention during the process of localizing the target object, illustrating the iterative refinement process of the QA module and the decoder's final focus on the target.", "section": "5.5 Qualitative Results"}, {"figure_path": "oPvBnPTbQv/figures/figures_15_6.jpg", "caption": "Figure 5: Qualitative results on RefCOCOg. The bounding boxes in green and red correspond to predictions of our model and the ground truth. Columns 2\u20136 showcase the attention maps generated by each QA module, while the last column represents the attention map from the decoder.", "description": "This figure shows example results of the RefFormer model on the RefCOCOg dataset.  For each example, it displays the input image with a bounding box highlighting the target object specified by the language expression. Then, it shows the attention maps generated by each Query Adaptation (QA) module within the RefFormer model.  These attention maps visualize which parts of the image the model is focusing on at each stage of processing. Finally, it displays the attention map generated by the decoder of the model, which shows where the model predicts the object is located.  The sequence of attention maps demonstrates how the model progressively refines its focus on the target object throughout the QA modules and the decoder.", "section": "5.5 Qualitative Results"}, {"figure_path": "oPvBnPTbQv/figures/figures_15_7.jpg", "caption": "Figure 5: Qualitative results on RefCOCOg. The bounding boxes in green and red correspond to predictions of our model and the ground truth. Columns 2\u20136 showcase the attention maps generated by each QA module, while the last column represents the attention map from the decoder.", "description": "This figure shows the qualitative results of the RefFormer model on the RefCOCOg dataset.  For each example, it displays the input image, the ground truth bounding box (red), the model's predicted bounding box (green), and a series of attention maps. The attention maps visualize the model's focus at different stages of processing, showing how the query adaptation (QA) modules and the decoder attend to different parts of the image. The QA modules progressively refine the attention to the target object, ultimately leading to a more accurate prediction by the decoder. ", "section": "5.5 Qualitative Results"}]