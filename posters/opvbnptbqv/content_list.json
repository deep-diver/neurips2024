[{"type": "text", "text": "Referencing Where to Focus: Improving Visual Grounding with Referential Query ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yabing Wang 1, Zhuotao Tian 2, Qingpei Guo 3, Zheng Qin 1, Sanping Zhou 1, Ming Yang 3, Le Wang ", "page_idx": 0}, {"type": "text", "text": "1 National Key Laboratory of Human-Machine Hybrid Augmented Intelligence,   \nNational Engineering Research Center for Visual Information and Applications,   \nand Institute of Artificial Intelligence and Robotics, Xi\u2019an Jiaotong University 2 Harbin Institute of Technology, Shenzhen, China 3 AntGroup {wyb7wyb7,qinzheng}@stu.xjtu.edu.cn tianzhuotao@link.cuhk.edu.hk {spzhou, lewang}@xjtu.edu.cn {qingpei.gqp, m.yang}@antgroup.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Visual Grounding aims to localize the referring object in an image given a natural language expression. Recent advancements in DETR-based visual grounding methods have attracted considerable attention, as they directly predict the coordinates of the target object without relying on additional efforts, such as pre-generated proposal candidates or pre-defined anchor boxes. However, existing research primarily focuses on designing stronger multi-modal decoder, which typically generates learnable queries by random initialization or by using linguistic embeddings. This vanilla query generation approach inevitably increases the learning difficulty for the model, as it does not involve any target-related information at the beginning of decoding. Furthermore, they only use the deepest image feature during the query learning process, overlooking the importance of features from other levels. To address these issues, we propose a novel approach, called RefFormer. It consists of the query adaption module that can be seamlessly integrated into CLIP and generate the referential query to provide the prior context for decoder, along with a task-specific decoder. By incorporating the referential query into the decoder, we can effectively mitigate the learning difficulty of the decoder, and accurately concentrate on the target object. Additionally, our proposed query adaption module can also act as an adapter, preserving the rich knowledge within CLIP without the need to tune the parameters of the backbone network. Extensive experiments demonstrate the effectiveness and efficiency of our proposed method, outperforming state-of-the-art approaches on five visual grounding benchmarks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Visual grounding is a challenging multi-modal task that involves localizing a specific object based on a given natural language description. This task requires algorithms to comprehend fine-grained human language expressions and accurately establish correspondences with the target objects. In recent years, it has gained significant attention in research due to its potential for advancing vision-language understanding, such as cross-modal retrieval [42, 30, 44, 9, 41, 43] and image captioning [14, 29]. ", "page_idx": 0}, {"type": "image", "img_path": "oPvBnPTbQv/tmp/30a473bc96599042479b0f33ad9fe45dbbd31622aa8c938ff85697ff6718ee63.jpg", "img_caption": ["Figure 1: Comparison of DETR-like method and our proposed method for visual grounding. (a) The existing method typically adopts the random initialization queries directly into the decoder to predict the target object. (b) We introduce the query adaption module (QA) to learn target-related context progressively, providing valuable prior knowledge for the decoder. (c) The attention map of the last layer in every QA module and decoder (bottom), respectively. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Existing works in this field typically follow the object detection framework and incorporate multimodal fusion to tackle this task. Earlier studies [52, 12, 27, 3, 58] mainly focus on a two-stage pipeline, which first generates a set of region proposals using object detectors, and then finds the bestmatched region by interacting these regions with linguistic expressions. However, the performance of this method is limited by the quality of the generated region candidates. To address this issue, some studies [48, 47, 4] adopt the one-stage pipeline, which removes the proposal generation stage. Unfortunately, these methods make dense predictions with a sliding window over pre-defined anchor boxes, resulting in sub-optimal performance due to the failure to capture object relations effectively. Recently, some methods [17, 7, 10, 21, 8, 36] inspired by the DETR [1] structure, which adopt a standard multi-modal transformer framework to establish the multi-modal correspondence (as shown in Figure 1 (a)). These methods predict bounding boxes of target objects directly from learnable queries, eliminating the need for extra efforts to obtain candidates, such as region proposals or predefined anchor boxes. ", "page_idx": 1}, {"type": "text", "text": "While these methods have shown promising results, their primary focus remains on designing stronger multi-modal decoders. By contrast, much less work has been done to improve the learnable queries, which have been gained extensive attention in the object detection field. The queries that are inputted to the decoder in these methods are typically generated through random initialization or by utilizing linguistic embeddings. We argued that this vanilla approach has two critical issues: i) this target-agnostic query inevitably increases the learning difficulty of the decoder. ii) During the query learning process, these methods tend to focus solely on the deepest visual features of the backbone, overlooking the texture information that is crucial for the grounding task and present in low and mid-level features, as emphasized by [15, 38]. ", "page_idx": 1}, {"type": "text", "text": "Drawing from these discussions, this paper seeks to address two critical research questions: i) Can we produce the target-related referential queries for the decoder to alleviate the learning difficulty that the decoder faces? and ii) How can we effectively incorporate the multi-level visual context information into the query learning process? We believe that tackling these issues together would promote the learnable query to more comprehensively and accurately learn the corresponding target object information in the image for the visual grounding task. ", "page_idx": 1}, {"type": "text", "text": "Considering CLIP [34] carries rich visual-language alignment knowledge, thus we adopt it as the backbone of our approach. Existing methods typically apply CLIP on the visual grounding by finetuning its parameters, as CLIP\u2019s training objective is to match entire images with text descriptions, rather than capturing fine-grained alignment between regions and textual elements. This may risk losing the general knowledge of CLIP and require significant computational resources. To tackle the challenges mentioned above, we propose a novel approach called RefFormer. Our approach incorporates a query adaptation (QA) module to generate referential queries, which provide the decoder with target-related context (as illustrated in Figure 1 (b)). By strategically inserting QA module into different layers of CLIP, the query adaptively learns target-related information from multilevel image feature maps, and iteratively refines the acquired information layer by layer. Furthermore, our proposed Reformer can also act as an adapter, enabling CLIP to keep frozen and preserve the original rich knowledge. It adopts the bi-directional interaction scheme, performs the multi-modal fusion by incorporating a small number of trainable parameters, and residually injects new taskspecific knowledge into CLIP throughout the entire feature extraction process. Extensive experiments conducted on five popular visual grounding benchmarks (i.e., $\\mathrm{RefCOCO}/+/\\mathrm{g}$ [53, 31], Flickr30K [33], and ReferItGame [18]) demonstrate the superior performance of our proposed method. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Our contributions can be summarized as follows: (1) Unlike the previous methods that focus on designing sophisticated multi-modal decoders, we further improve the learning process of the learnable queries, a crucial aspect that has been overlooked in existing work. (2) We propose a query adaption module (QA), which can adaptively capture the target-related context, providing valuable referential knowledge for the decoder. (3) We conduct extensive experiments on five visual grounding benchmarks, demonstrating the effectiveness and potential of our method. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Visual Grounding ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Visual grounding aims to ground the target objects based on natural language descriptions by understanding the given images and expressions. Early work [52, 12, 27, 3, 58] primarily focuses on two-stage methods, which formulates the grounding task as a matching task. These methods employ object detectors to generate proposal candidates and then identify the best-matched candidate based on the matching score computed between each proposal and the referring expression. For example, MAttNet [52] proposes to decompose the language expression into three phrase embeddings, which are used to trigger three separate visual modules. While achieving successful performance, two-stage methods heavily rely on the quality of the generated proposals. Based on this, some studies [48, 47, 4] have been dedicated to one-stage methods to remove the proposal generation stage. These methods typically fuse visual features and language features first and then densely regress the bounding box on each position of the feature map grid. For instance, FAOA [48] incorporates linguistic embedding into the YOLOv3 detector to establish a one-stage pipeline, balancing between accuracy and speed. ", "page_idx": 2}, {"type": "text", "text": "Recently, transformer-based visual grounding methods [17, 7, 21, 23, 57, 40, 50, 8, 36, 10] have emerged, which leverages the self-attention mechanism to effectively capture intra- and inter-modality relationships and achieve improved performance. Among these methods, the mainstream approach [17, 7, 10, 21, 8, 36] adopts DETR-like structures to decode bounding boxes from learnable queries. For example, Transvg [7] and Transvg $^{++}$ [8] employ a standard multimodal transformer framework, along with the REG token, to establish multi-modal correspondence and predict the coordinates of the referring object. Notably, the performance improvement of these methods primarily arises from the design of stronger backbones or multi-modal decoders. In this work, we focus on the design of learnable queries, which have received considerable attention in object detection field. ", "page_idx": 2}, {"type": "text", "text": "2.2 Learnable Queries in DETR and Its Variants ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the object detection field, DETR presents an end-to-end object detection model that is built in an encoder-decoder transformer architecture. However, it suffers from slow training convergence. To address this issue, some follow-up works [55, 19, 49, 45, 20, 26, 24, 54] solve this issue by optimizing the learnable queries in DETR. For instance, Anchor DETR [45] directly treats 2D reference points as queries, while DAB-DETR [24] further investigates the role of queries in DETR and proposes the use of 4D anchor boxes as queries. In contrast to these model-level improvements, DN-DETR [20] introduces query denoising training to mitigate the instability of bipartite graph matching, which is further enhanced by DINO [54]. ", "page_idx": 2}, {"type": "text", "text": "Additionally, similar research have been explored in other tasks [22, 13, 37]. For example, EaTR [13] formulates a video as a set of event units and treats video-specific event units as dynamic moment queries in video grounding tasks. $\\mathrm{MTR++}$ [37] introduces distinct learnable intention queries generated by the $\\mathbf{k}$ -means clustering algorithm to handle trajectory prediction across different motion modes in motion prediction tasks. ", "page_idx": 2}, {"type": "image", "img_path": "oPvBnPTbQv/tmp/23d6ce873f57566503122be21682aa40da8d3643f963c67666ffe98d0a5ca55d.jpg", "img_caption": ["Figure 2: Overview of RefFormer. It adopts a DETR-like structure, consisting of a query adaptation (QA) module that seamlessly integrates into various layers of CLIP, along with a task-specific decoder. By incorporating the QA module, RefFormer can iteratively refine the target-related context and generate referential queries, which provide the decoder with prior context. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3 Preliminary ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Considering the impressive vision-language alignment capability of CLIP, we take it as the backbone of our method to extract image and text representations, and keep the parameters frozen during training. The feature extraction process can be represented as follows: ", "page_idx": 3}, {"type": "text", "text": "Isimzea ,o dwehr.e rFeo .a gTeh $V\\in\\mathbb{R}^{H\\times W\\times3}$ ,  tiht eins  dfilavtitdeende di nitnot $N$ nsoetn -oofv verelcatpopris,n gr eppartecsheenst eodf $P\\times P$ $\\begin{array}{r}{N_{v}=\\frac{^{\\mathbf{\\lambda}}^{\\!\\!\\lambda}H\\times W}{P^{2}}}\\end{array}$   \nas $\\{\\mathbf{x}_{v}^{i}\\,\\in\\,\\mathbb{R}^{3P^{2}}\\}_{i=1}^{N}$ . Next, these vectors are transformed into token embeddings using a linear projection layer $\\phi_{e}(\\cdot)$ . Furthermore, a classification token $\\mathbf{x}_{c l s}\\in\\mathbb{R}^{D}$ is added at the beginning of the token embeddings. Subsequently, the positional embeddings $\\mathbf{E}_{v}$ are incorporated, and a layer normalization (LN) is applied. This process can be expressed as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\bf Z}_{v}^{0}=L N([{\\bf x}_{c l s};\\phi_{e}({\\bf X}_{v})]+{\\bf E}^{v})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $[;]$ denotes the concatenate operation. The sequence of tokens ${\\bf Z}_{v}^{0}$ is then passed through $L$ transformer layers. Each transformer layer comprises two submodules: the multi-head self-attention (MHSA) and the multilayer perceptron (MLP), with each submodule preceded by layer normalization. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\mathbf Z}_{v}^{i}=M H S A(L N(\\mathbf Z_{v}^{i-1}))+\\mathbf Z_{v}^{i-1},\\ i=1,...,L}\\\\ &{\\mathbf Z_{v}^{i}=M L P(L N(\\bar{\\mathbf Z}_{v}^{i}))+\\bar{\\mathbf Z}_{v}^{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{Z}_{v}^{i}\\in\\mathbb{R}^{N\\times D}$ denote the output of $i$ -th transformer layer. ", "page_idx": 3}, {"type": "text", "text": "Text Encoder. Given an referring expression $T$ , it is first transformed into a sequence of word embeddings using lower-cased byte pair encoding representations $\\mathbf{X}_{t}$ . The word embeddings are bracketed with the [SOS] and [EOS] tokens, producing a sequence of length $N_{t}$ . Similar to the image encoder, these tokens are summed with positional embeddings $\\mathbf{E}_{t}$ and passed through the $L$ transformer layers to extract the text representations: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\mathbf{Z}}_{t}^{i}=M H S A(L N(\\mathbf{Z}_{t}^{i-1}))+\\mathbf{Z}_{t}^{i-1},\\;i=1,...,L}\\\\ &{\\mathbf{Z}_{t}^{i}=M L P(L N(\\bar{\\mathbf{Z}}_{t}^{i}))+\\bar{\\mathbf{Z}}_{t}^{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ${\\bf Z}_{t}^{0}=[{\\bf x}_{s o s};{\\bf X}_{t};{\\bf x}_{e o s}]+{\\bf E}_{t}$ , representing the word embedding layer in text encoder. ", "page_idx": 3}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The framework is shown in Figure 2. In the following, we first describe our query adaptation module in Section 4.1. We then introduce our decoder that decodes with referential query and training objectives in Section 4.2 and Section 4.3. Furthermore, we extend RefFormer to dense grounding task in Section 4.4. Finally, we provide a discussion in Section 4.5. ", "page_idx": 3}, {"type": "image", "img_path": "oPvBnPTbQv/tmp/472d64fa3a8a378be3a59d92a15c6da586bc7dc0e794c41ef3b736d8c0e83a64.jpg", "img_caption": ["Figure 3: Illustration of our proposed Query Adaption Module, which mainly consists of CAMF and TR modules to generate the referential queries and promote the multi-modal features interaction. \"R\" represents the feature modulation. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4.1 Query Adaptation Module (QA) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we propose a QA module (as shown in Figure 3) that can generate the referential query to provide the decoder with the target-related context, thereby enhancing the decoder\u2019s grounding capabilities. Importantly, our approach incorporates multi-level features into the query learning process, enabling the queries to capture more comprehensive target object information and can be refined layer by layer. Furthermore, $Q A$ can also act as an adapter, eliminating the need to fine-tune the entire parameters of the backbone. ", "page_idx": 4}, {"type": "text", "text": "Down-projection. Considering the image and language representations $\\mathbf{Z}_{v}^{i}$ and $\\mathbf{Z}_{t}^{i}$ obtained from the $i$ -th layer of the backbone, we initially use the MLP layers $\\phi_{v d}^{i}(\\cdot)$ and $\\bar{\\phi}_{t d}^{i}(\\cdot)$ to project them to lower-dimensional features to reduce the computation memory: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{F}_{v}^{i}=\\phi_{v d}^{i}(\\mathbf{Z}_{v}^{i}),\\;\\mathbf{F}_{t}^{i}=\\phi_{t d}^{i}(\\mathbf{Z}_{t}^{i})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Condition Aggregation and Multi-modal Fusion (CAMF). We randomly initialize $N_{q}$ learnable queries $\\mathbf{Q}\\in\\mathbb{R}^{N_{q}\\times D_{l}}$ , where $D_{l}$ denotes the dimension after projected. These queries are specifically designed to capture potential target object context. Next, we concatenate these queries with the image features and input them, along with the language features into the CAMF block. Specifically, the CAMF block mainly consists of a cross-attention layer, which takes the image and query features $[\\mathbf{Q};\\mathbf{F}_{v}]$ and language features $\\mathbf{F}_{t}$ as the query respectively. This approach enables us to not only incorporate the expression condition into the learnable queries $\\mathbf{Q}$ but also to extract relevant information from other modalities, thereby facilitating the fusion of target-related crossmodal features. Besides, we incorporate two learnable regulation tokens $\\mathbf{r}_{v},\\mathbf{r}_{t}\\in\\mathbb{R}^{D_{l}}$ to modulate the final output of each QA. This process can be formalized as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{r}}_{v},\\bar{\\mathbf{Q}}_{c}^{i},\\bar{\\mathbf{F}}_{v}^{i}=M H C A([\\mathbf{r}_{v};\\mathbf{Q}^{i-1};\\mathbf{F}_{v}^{i}],\\mathbf{F}_{t}^{i},\\mathbf{F}_{t}^{i})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{Q}}_{c}^{i}=L N(\\bar{\\mathbf{Q}}_{c}^{i})+\\mathbf{Q}^{i-1},\\,\\hat{\\mathbf{F}}_{v}^{i}=L N(\\bar{\\mathbf{F}}_{v}^{i})+\\mathbf{F}_{v}^{i}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{r}}_{t},\\bar{\\mathbf{F}}_{t}^{i}=M H C A([\\mathbf{r}_{t};\\mathbf{F}_{t}^{i}],\\mathbf{F}_{v}^{i},\\mathbf{F}_{v}^{i}),\\;\\hat{\\mathbf{F}}_{t}^{i}=L N(\\bar{\\mathbf{F}}_{t}^{i})+\\mathbf{F}_{t}^{i}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{Q}^{i-1}$ represents learnable queries that output from the previous QA, while ${\\bf{Q}}^{0}$ are randomly initialized. The symbol [; ] indicates the concatenate operation, and $M H C A(,,)$ and $L N(\\cdot)$ denote the multi-head cross-attention layers and layer normalization, respectively. ", "page_idx": 4}, {"type": "text", "text": "Target-related Context Refinement (TR). Following this, we feed the queries $\\hat{\\mathbf{Q}}_{c}$ and multi-modal ehanced feature maps ${\\hat{\\mathbf{F}}}_{v}^{i}$ and $\\hat{\\mathbf{F}}_{t}^{i}$ into the TR block. First, we use the queries $\\hat{\\mathbf{Q}}_{c}$ that have aggregated conditions to interact with the multi-modal enhanced image feature maps ${\\hat{\\mathbf{F}}}_{v}^{i}$ , refining the targetrelated visual context within them. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{Q}_{v}^{i}=M H C A(\\hat{\\mathbf{Q}}_{c}^{i},\\hat{\\mathbf{F}}_{v}^{i},\\hat{\\mathbf{F}}_{v}^{i}),\\ \\mathbf{Q}^{i}=L N(M L P(\\mathbf{Q}_{v}^{i}))+\\hat{\\mathbf{Q}}_{c}^{i}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Moreover, for feature maps ${\\hat{\\mathbf{F}}}_{v}^{i}$ and $\\hat{\\mathbf{F}}_{t}^{i}$ that have aggreaged other modality information, we use the self-attention to further enhance their target-related contextual semantics: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\mathbf{r}}_{v},\\widetilde{\\mathbf{F}}_{v}^{i}=M H S A([\\bar{\\mathbf{r}}_{v};\\hat{\\mathbf{F}}_{v}^{i}],\\hat{\\mathbf{F}}_{v}^{i},\\hat{\\mathbf{F}}_{v}^{i}),\\;\\mathbf{G}_{v}^{i}=L N(M L P(\\widetilde{\\mathbf{F}}_{v}^{i}))+\\hat{\\mathbf{F}}_{v}^{i}}\\\\ {\\widetilde{\\mathbf{r}}_{t},\\widetilde{\\mathbf{F}}_{t}^{i}=M H S A([\\bar{\\mathbf{r}}_{v};\\hat{\\mathbf{F}}_{t}^{i}],\\hat{\\mathbf{F}}_{t}^{i},\\hat{\\mathbf{F}}_{t}^{i}),\\;\\mathbf{G}_{t}^{i}=L N(M L P(\\widetilde{\\mathbf{F}}_{t}^{i}))+\\hat{\\mathbf{F}}_{t}^{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Up-projection. Finally, we utilize MLP to restore the channel dimension of the image and language features back to their original sizes. These features are then passed as inputs to the next layer of the ", "page_idx": 4}, {"type": "text", "text": "backbone in a residual manner. Prior to this, we utilize the regulation token to modulate the features ${\\bf G}_{v}$ and $\\mathbf{G}_{t}$ , which helps prevent the multi-modal signal from overpowering the original signal. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbf{Z}}_{v}^{i}=\\phi_{v u}^{i}(\\mathbf{G}_{v}^{i}\\times\\sigma(\\widetilde{\\mathbf{r}}_{v}))+\\mathbf{Z}_{v}^{i},\\,\\hat{\\mathbf{Z}}_{t}^{i}=\\phi_{t u}^{i}(\\mathbf{G}_{t}^{i}\\times\\sigma(\\widetilde{\\mathbf{r}}_{t}))+\\mathbf{Z}_{t}^{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\phi_{v u}(\\cdot)$ and $\\phi_{t u}(\\cdot)$ denote the MLP layer, and $\\sigma(\\cdot)$ denotes the sigmoid function. ", "page_idx": 5}, {"type": "text", "text": "Finally, by iteratively performing the above process, the queries $Q$ can progressively focus on the target-related context, and generate the referential queries to provide the prior context for the decoder. ", "page_idx": 5}, {"type": "text", "text": "4.2 Decoding with Referential Query. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Language-guided Multi-level Fusion. By inserting the QA at different layers of CLIP, the referential queries can be adaptively updated using the multi-level image feature maps. Additionally, to enhance the image features in decoder, we aggregate the multi-level visual features under the language guidance to yield language-aware multi-level image features. Specifically, given a multi-level image feature set $\\{\\hat{\\mathbf{Z}}_{v}^{k}\\}$ (including low, mid, and high levels), where $k\\in\\mathcal{K}$ represents selected layer index, we inject the language features ${\\bf Z}_{t}^{l a s t}$ (the final output of the text encoder) into each level of image features using MHCA: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{H}_{t_{s o s}}=\\phi_{m t}(\\mathbf{Z}_{t}^{l a s t}),\\;\\mathbf{H}_{v}^{k}=\\phi_{m v}^{k}(\\hat{\\mathbf{Z}}_{v}^{k})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{H}}_{v}^{k}=M H C A(\\mathbf{H}_{v}^{k},\\mathbf{H}_{t_{s o s}},\\mathbf{H}_{t_{s o s}})+\\mathbf{H}_{v}^{k},\\ k\\in\\mathcal{K}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\phi_{m t}(\\cdot)$ and $\\phi_{m v}(\\cdot)$ denote the linear project function used to map features to the same dimension. Besides, $\\mathbf{H}_{t_{s o s}}$ represents the [SOS] token in $\\mathbf{H}_{t}$ , which extracts the global information of the text. Subsequently, the multi-level language-aware image features are produced by simple concatenation, followed by a linear projection function $\\phi_{v m l}(\\cdot)$ to map to the original dimension: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\mathbf{H}}_{v m l}=C o n c a t(\\{\\hat{\\mathbf{H}}_{v}^{k}\\}),k\\in\\mathcal{K}}\\\\ {\\mathbf{H}_{v m l}=\\phi_{v m l}(\\bar{\\mathbf{H}}_{v m l})\\,\\,\\,\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Decoding. Following, we first initialize the queries $\\mathbf{Q^{\\prime}}$ with the same size as the referential query $\\mathbf{Q}$ , and add them together to utilize the prior context in $\\mathbf{Q}$ . Note that, to avoid interference from $\\mathbf{Q^{\\prime}}$ during the initial stage, we initialize $\\mathbf{Q^{\\prime}}$ as an all-zero matrix. Then, we concatenate the queries with the image features to interact with the language features to aggregate the condition information and produce the multi-modal feature map $H_{m m}$ . This can be represented as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\mathbf{O}}_{c},\\bar{\\mathbf{H}}_{m m}=M H C A([\\boldsymbol{\\phi}_{q}(\\mathbf{Q})+\\mathbf{Q}^{\\prime};\\mathbf{H}_{v m l}],\\mathbf{H}_{t},\\mathbf{H}_{t})}\\\\ &{\\mathbf{O}_{c}=L N(\\bar{\\mathbf{O}}_{c})+\\bar{\\mathbf{O}}_{c},\\;\\mathbf{H}_{m m}=L N(\\bar{\\mathbf{H}}_{m m})+\\bar{\\mathbf{H}}_{m m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\phi_{q}(\\cdot)$ is the MLP layer, which regulates the significance of the query $\\mathbf{Q}$ . As the importance approaches zero, the query degenerate into a vanilla query. Then, we feed the queries $\\mathbf{O_{c}}$ and multi-modal feature map ${\\bf H}_{m m}$ into the MHCA layer to extract target embddings $\\dot{\\mathbf{O}}\\in\\mathbb{R}^{N_{q}\\times D}$ . It can be formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\bar{\\mathbf{O}}=M H C A(\\mathbf{O}_{c},\\mathbf{H}_{m m},\\mathbf{H}_{m m})}}\\\\ {{\\mathbf{O}=L N(\\phi_{r}(\\bar{\\mathbf{O}}))+\\bar{\\mathbf{O}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\phi_{r}(\\cdot)$ represents the linear projection function. ", "page_idx": 5}, {"type": "text", "text": "Grounding Head. We built the two MLPs $(\\phi_{b o x}(\\cdot)$ and $\\phi_{c l s}(\\cdot),$ over the target embeddings $\\mathbf{O}$ . The final outputs consist of the predicted center coordinates of the target object, denoted as $b=$ $(x,y,h,w)\\in\\overline{{\\mathbb{R}}}^{4}$ , and the predicted confidence score $y\\in\\mathbb{R}^{2}$ that encompass the target object: ", "page_idx": 5}, {"type": "equation", "text": "$$\nb=\\phi_{b o x}(\\mathbf{O}),\\ y=\\phi_{c l s}(\\mathbf{O})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4.3 Training Objectives ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Similar to DETR, we employ bipartite matching to find the best match between the predictions $\\{b,y\\}$ and the ground-truth targets $\\left\\{b_{t g t},y_{t g t}\\right\\}$ . In our case, the class prediction is confidence prediction aims to estimate the confidence of a query containing a target object. To supervise the training, we use the box prediction losses (L1 and GIoU), and a cross-entropy loss after matching. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{d e t}=\\lambda_{i o u}\\mathcal{L}_{i o u}(b_{g t},b)+\\lambda_{L1}\\vert\\vert b_{g t}-b\\vert\\vert+\\lambda_{c e}\\mathcal{L}_{c e}(y_{g t},y)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda$ denotes the corresponding loss weight. Additionally, to encourage the referential queries in every QA module to effectively focus on the target-related context, we also introduce the auxiliary loss $\\mathcal{L}_{a u x}$ that is similar to the above objective function to provide supervision for them. The final training objective can be defined as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{f i n a l}=\\mathcal{L}_{d e t}+\\lambda_{a u x}\\mathcal{L}_{a u x}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\lambda_{a u x}$ denotes the weight of the auxiliary loss. ", "page_idx": 6}, {"type": "text", "text": "4.4 Extend to Dense Grounding ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In addition to object-level grounding, our method can easily extend to the dense grounding task by incorporating a segmentation head. Specifically, similar to the MaskFormer [5], we utilize the MLP to transform the target embeddings $\\mathbf{O}$ into mask embeddings $\\mathbf{M}\\in\\mathbb{R}^{N_{q}\\times D}$ . The binary mask prediction $s_{i}=[0,1]\\in\\bar{\\mathbb{R}}^{H\\times W}$ is then computed by performing a dot product between the mask embeddings M and the multi-modal feature map ${\\bf H}_{m m}$ and followed by a sigmoid activation. During training, we use the mask prediction losses (Focal and Dice), which can be defined as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{s e g}=\\lambda_{f o c a l}\\mathcal{L}_{f o c a l}(s_{g t},s)+\\lambda_{d i c e}\\mathcal{L}_{d i c e}(s_{g t},s)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $s_{g t}$ denotes the ground-truth mask. ", "page_idx": 6}, {"type": "text", "text": "4.5 Discussion ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this work, we aim to explore how to further optimize the learning process of queries. To reduce the learning difficulties posed by vanilla query, we introduce a simple query adaption module to adaptively capture target-related context and iteratively refine it. As illustrated in Figure 5, the attention maps produced by each query adaption module consistently align with our objective: to progressively focus on the target-related context and provide prior context for the decoder. It is worth noting that while \"multi-level\", \"adapter\", and \"self-attention\" may be extensively applied in other research fields, our approach aims to integrate them to address the challenges in visual grounding tasks, instead of designing a specific module to achieve the mentioned functions individually. ", "page_idx": 6}, {"type": "text", "text": "5 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Datasets and Evaluation Metric ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "RefCOCO/RefCOCO $+,$ /RefCOCOg. RefCOCO [53] comprises 19,994 images featuring 50,000 referred objects, divided into train, val, testA, and testB sets. Similarly, RefCOCO $^+$ [53] contains 19,992 images with 49,856 referred objects and 141,564 referring expressions. It contains more attributes than absolute locations compared to RefCOCO, and has the same split. RefCOCOg [31] has 25,799 images with 49,856 referred objects and expressions. Following a common version of split [32], i.e., train, val, and test sets. ", "page_idx": 6}, {"type": "text", "text": "Flickr30K. Flickr30k Entities [33] contains 31,783 images and $158\\mathbf{k}$ caption sentences with $427\\mathbf{k}$ annotated phrases. We follow [7] to split the images into 29,783 for training, 1000 for validation, and 1000 for testing, and report the performance on the test set. ", "page_idx": 6}, {"type": "text", "text": "ReferItGame. ReferItGame [18] includes 20,000 images with 120,072 referring expressions for 19,987 referred objects. We follow [7] to split the dataset into train, validation and test sets, and report the performance on the test set. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metric. For referring expression comprehension (REC), we use Prec $@0.5$ evaluation protocol to evaluate the accuracy, which is consistent with prior works. In this evaluation, a predicted region is considered correct if its intersection-over-union (IoU) with the ground-truth bounding box is greater than 0.5. For referring expression segmentation (RES), we report the Mean IoU (MIoU) between the predicted segmentation mask and ground truth mask. ", "page_idx": 6}, {"type": "text", "text": "5.2 Implementation Details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Following [7, 36], the resolution of the input image is resized to $640\\times640$ . We employ the pre-trained CLIP as our backbone to extract both image and language features, and we freeze its parameters during training. The model is optimized end-to-end using AdamW for 40 epochs, with a batch size of 32. We set the learning rate to 1e-4 and the weight decay to 1e-2. The experiments are conducted on V100 GPUs. The loss weight $\\lambda_{i o u},\\lambda_{L1},\\lambda_{c e}$ , and $\\lambda_{a u x}$ , we set to 3.0, 1.0, 1.0, and 0.1. For dense grounding, we set the parameters $\\lambda_{f o c a l}$ , and $\\lambda_{d i c e}$ to 5.0, and 1.0. ", "page_idx": 6}, {"type": "table", "img_path": "oPvBnPTbQv/tmp/7d3e68a33b2582be990813ba4b7f2761181f58c6ca83a2e20cbc7f607a8396d9.jpg", "table_caption": ["Table 1: Comparisons with the state-of-the-art approaches on three benchmarks, i.e., RefCOCO, RefCOCO+, RefCOCOg. \\* indicates models that use additionally data beyond RefCOCO series. $\\dagger$ indicates that models simply combine RefCOCO, RefCOCO+, and RefCOCOg. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "oPvBnPTbQv/tmp/63bd8e60aa374b0d8e42ef78f64f7faaf50c1f9685d3b56132ae540c0996ea61.jpg", "table_caption": ["Table 2: Comparison with state-of-theart approaches on the Flickr30K Entities and ReferItGame. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "oPvBnPTbQv/tmp/2c646c9753e538c81a7ead7d3fb818538eabcc06db2e341f64e898bcb35fa4c6.jpg", "table_caption": ["Table 3: Comparisons with the state-of-the-art dense grounding approaches on three benchmarks for RES task, i.e., RefCOCO, $\\mathrm{RefCOCO+}$ , and RefCOCOg. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.3 Comparisons with State-of-the-art Methods ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "REC Task. For REC task, we compare the performance with the state-of-the-art REC methods, including the two-stage methods, one-stage methods, and transformer-based methods. As reported in Table 1 and Table 2, our proposed method achieves the best performance. In particular, when comparing to the transformer-based method Dynamic MDETR, which adopts the DETR-like structure and uses the same backbone as ours, we can see that our method performs better with $+0.53\\%$ , $+1.90\\%$ , $+1.62\\%$ on RefCOCO, $+1.11\\%$ , $+2.44\\%$ , $+6.21\\%$ on $\\mathbf{RefCOCO+}$ , and $+4.94\\%$ , $+4.18\\%$ on RefCOCOg. Additionally, under multiple/extra datasets setting, our method also surpasses recent state-of-the-art methods that incorporate large language models or utilize more training data. ", "page_idx": 7}, {"type": "text", "text": "RES Task. Following RefTR [21] and VG-LAW [40], we also conduct the dense grounding experiments and report the results in Table 3 in terms of mIoU. It can be seen that our model achieves superior performance without extra deliberate design to dense grounding, demonstrating the generalization of our method. ", "page_idx": 7}, {"type": "table", "img_path": "oPvBnPTbQv/tmp/e7794c61301462dfbf4fb3d1e2382c6e17e4947087ae7a8e3b02168f4ec88c52.jpg", "table_caption": ["Table 4: Ablation study of the generation method of learnable queries on $\\mathrm{RefCOCOg}$ . "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "oPvBnPTbQv/tmp/0a4e57242e99c18e281719c59e840a8ff45e9013d51819d4562ac7bb64e06eb3.jpg", "table_caption": ["Table 5: Ablation study of the QA position on RefCOCOg. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.4 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we conduct the ablation studies to verify the effectiveness the each part of our proposed method on RefCOCOg. Following previous work [7, 17, 8], the visual backbone we apply the ViT-Base/32. ", "page_idx": 8}, {"type": "text", "text": "Effect on the position of QA. As presented in Table 5, firstly, we can observe that removing the QA would lead to a Sharp decline in performance, highlighting the effectiveness of QA. We then explored the impact of QA\u2019s position in the CLIP to determine where QA should be added. We chose three groups for the ablation study: $\\{4,8,12\\}$ , $\\{4,6,8,10,12\\}$ , and $\\{2,4,6,8,10,12\\}$ . The results indicate that performance is best when we use the $\\{4,6,8,10;12\\}$ configuration. Therefore, we default to this position in our experiments. ", "page_idx": 8}, {"type": "text", "text": "Effect on the layers of multi-level fusion. In Table 4, we analyze the impact of the fusion layers in the decoder. We first conduct experiments using single-level image features, and then proceed with multilevel features. The results show that utilizing multi-level features significantly improves performance, demonstrating that low- and mid-level features complement high-level features. Additionally, using the $\\lbrace4,8,12\\rbrace$ achieves the best performance, which we adopt for our experiments. ", "page_idx": 8}, {"type": "text", "text": "Effect on the different backbone. In the first line of Table 6, we apply our method to singlemodal encoders, i.e., Swin-Transformer $^+$ Bert. The results demonstrate that our method is not only applicable to multi-modal encoders but is also compatible with single-modal encoders. ", "page_idx": 8}, {"type": "text", "text": "Effect on the auxiliary loss. In the second line of Table 6, We experiment with auxiliary loss, and the results demonstrate the effectiveness of auxiliary loss. By employing auxiliary loss, the reference query can capture the target-related visual contexts more effectively. ", "page_idx": 8}, {"type": "text", "text": "Effect on learnable queries. In the third line of Table 6, we validate the effectiveness of the learnable queries. Specifically, we replace the prior queries generated by the QA module with randomly initialized queries or linguistic embeddings input to decoder while keeping other modules unchanged. We can observe that introducing the prior queries can bring significant performance improvement. This result demonstrates that prior queries aid the decoder in more accurately locating the target object. Additionally, we investigate the accuracy of our referential queries, which are designed to provide prior information to the decoder. Since the channel dimension in the QA module is lower, the reference query may not accurately predict the coordinates of the targets. ", "page_idx": 8}, {"type": "text", "text": "Convergence curves. In Figure 4 we illustrate the convergence curve of our proposed method compared to other open-source DETR-like visual grounding methods. Notably, our method demonstrates accelerated training convergence, reducing the training time by half compared to the TransVG, while also outperforming other existing methods. ", "page_idx": 8}, {"type": "text", "text": "5.5 Qualitative Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As shown in Figure 5, the attention maps in the QA module illustrate the refined process of how the referential query captures the target-related context. Initially, the attention map appears noisy but gradually focuses on the target-related context, such as the couch in (a). By incorporating the referential query, the attention map in the decoder accurately concentrates on the target object. ", "page_idx": 8}, {"type": "table", "img_path": "oPvBnPTbQv/tmp/d3b0544ba7c71ef108fec4ee8f9bef9658040d81d66f4fa4d5bb46520cc0287c.jpg", "table_caption": ["Table 6: Ablation studies of backbone, auxiliary loss, and learnable queries on $\\mathbf{RefCOCOg}$ . "], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "oPvBnPTbQv/tmp/df3baee56709da344940573095ca09b751a785236951075150d63d9f76526153.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 4: Convergence curves. Our method achieves better results with fewer training epochs on $\\mathbf{RefCOCOg}$ . ", "page_idx": 9}, {"type": "image", "img_path": "oPvBnPTbQv/tmp/0f82cdb3ff630b034145d430477664cd7d85d1d770b8374e15b332631df53782.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "(c) Expression: a womanskiingwith herchild ", "page_idx": 9}, {"type": "text", "text": "Figure 5: Qualitative results on RefCOCOg. The bounding boxes in green and red correspond to predictions of our model and the ground truth. Columns 2-6 showcase the attention maps generated by each QA module, while the last column represents the attention map from the decoder. ", "page_idx": 9}, {"type": "text", "text": "Besides, it is important to note that the referential query may not precisely focus on the target object due to the lower feature dimension in the QA module, but it still captures target-related information. ", "page_idx": 9}, {"type": "text", "text": "6 Concluding and Remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a novel approach, called RefFormer that can be seamlessly integrated into CLIP. The RefFormer can not only generate the referential query to provide the target-related context for decoder, but also act as the adaptor to preserve the original knowledge of CLIP and reduce the training cost. Extensive experiments demonstrate the effectiveness of our method, and visualization results illustrate the refined process of our proposed RefFormer. ", "page_idx": 9}, {"type": "text", "text": "Limitations : Although our method is specifically designed for the REC task and surpasses existing SOTA methods in REC, there is still significant room for improvement in the RES task. This is because we have not yet optimized our approach specifically for the RES task. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by National Science and Technology Major Project under Grant 2023ZD0121300, National Natural Science Foundation of China under Grants 62088102, 12326608 and 62106192, Natural Science Foundation of Shaanxi Province under Grant 2022JC-41, and Fundamental Research Funds for the Central Universities under Grant XTR042021005. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213\u2013229. Springer, 2020.   \n[2] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm\u2019s referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023.   \n[3] Long Chen, Wenbo Ma, Jun Xiao, Hanwang Zhang, and Shih-Fu Chang. Ref-nms: Breaking proposal bottlenecks in two-stage referring expression grounding. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 1036\u20131044, 2021.   \n[4] Xinpeng Chen, Lin Ma, Jingyuan Chen, Zequn Jie, Wei Liu, and Jiebo Luo. Real-time referring expression comprehension by single-stage grounding network. arXiv preprint arXiv:1812.03426, 2018.   \n[5] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification is not all you need for semantic segmentation. Advances in neural information processing systems, 34:17864\u2013 17875, 2021.   \n[6] Zesen Cheng, Kehan Li, Peng Jin, Siheng Li, Xiangyang Ji, Li Yuan, Chang Liu, and Jie Chen. Parallel vertex diffusion for unified visual grounding. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1326\u20131334, 2024.   \n[7] Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang Zhou, and Houqiang Li. Transvg: End-to-end visual grounding with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1769\u20131779, 2021.   \n[8] Jiajun Deng, Zhengyuan Yang, Daqing Liu, Tianlang Chen, Wengang Zhou, Yanyong Zhang, Houqiang Li, and Wanli Ouyang. Transvg++: End-to-end visual grounding with language conditioned vision transformer. IEEE transactions on pattern analysis and machine intelligence, 2023.   \n[9] Jianfeng Dong, Yabing Wang, Xianke Chen, Xiaoye Qu, Xirong Li, Yuan He, and Xun Wang. Reading-strategy inspired visual representation learning for text-to-video retrieval. IEEE transactions on circuits and systems for video technology, 32(8):5680\u20135694, 2022.   \n[10] Ye Du, Zehua Fu, Qingjie Liu, and Yunhong Wang. Visual grounding with transformers. In 2022 IEEE International Conference on Multimedia and Expo (ICME), Jul 2022.   \n[11] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial training for vision-and-language representation learning. Advances in Neural Information Processing Systems, 33:6616\u20136628, 2020.   \n[12] Richang Hong, Daqing Liu, Xiaoyu Mo, Xiangnan He, and Hanwang Zhang. Learning to compose and reason with language tree structures for visual grounding. IEEE transactions on pattern analysis and machine intelligence, 44(2):684\u2013696, 2019.   \n[13] Jinhyun Jang, Jungin Park, Jin Kim, Hyeongjun Kwon, and Kwanghoon Sohn. Knowing where to focus: Event-aware transformer for video grounding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13846\u201313856, 2023.   \n[14] Jiayi Ji, Yiwei Ma, Xiaoshuai Sun, Yiyi Zhou, Yongjian Wu, and Rongrong Ji. Knowing what to learn: a metric-oriented focal mechanism for image captioning. IEEE Transactions on Image Processing, 31:4321\u20134335, 2022.   \n[15] Dongsheng Jiang, Yuchen Liu, Songlin Liu, Xiaopeng Zhang, Jin Li, Hongkai Xiong, and Qi Tian. From clip to dino: Visual encoders shout in multi-modal large language models. 2023.   \n[16] Ya Jing, Tao Kong, Wei Wang, Liang Wang, Lei Li, and Tieniu Tan. Locate then segment: A strong pipeline for referring image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9858\u20139867, 2021.   \n[17] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1780\u20131790, 2021.   \n[18] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787\u2013798, 2014.   \n[19] Feng Li, Ailing Zeng, Shilong Liu, Hao Zhang, Hongyang Li, Lei Zhang, and Lionel M Ni. Lite detr: An interleaved multi-scale encoder for efficient detr. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18558\u201318567, 2023.   \n[20] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni, and Lei Zhang. Dn-detr: Accelerate detr training by introducing query denoising. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13619\u201313627, 2022.   \n[21] Muchen Li and Leonid Sigal. Referring transformer: A one-step approach to multi-task visual grounding. Advances in neural information processing systems, 34:19652\u201319664, 2021.   \n[22] Pandeng Li, Chen-Wei Xie, Hongtao Xie, Liming Zhao, Lei Zhang, Yun Zheng, Deli Zhao, and Yongdong Zhang. Momentdiff: Generative video moment retrieval from random to real. Advances in neural information processing systems, 36, 2024.   \n[23] Yue Liao, Aixi Zhang, Zhiyuan Chen, Tianrui Hui, and Si Liu. Progressive language-customized visual feature learning for one-stage visual grounding. IEEE Transactions on Image Processing, 31:4266\u20134277, 2022.   \n[24] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang. Dab-detr: Dynamic anchor boxes are better queries for detr. arXiv preprint arXiv:2201.12329, 2022.   \n[25] Sun-Ao Liu, Yiheng Zhang, Zhaofan Qiu, Hongtao Xie, Yongdong Zhang, and Ting Yao. Caris: Context-aware referring image segmentation. In Proceedings of the 31st ACM International Conference on Multimedia, pages 779\u2013788, 2023.   \n[26] Wenze Liu, Hao Lu, Yuliang Liu, and Zhiguo Cao. Box-detr: Understanding and boxing conditional spatial queries. arXiv preprint arXiv:2307.08353, 2023.   \n[27] Xihui Liu, Zihao Wang, Jing Shao, Xiaogang Wang, and Hongsheng Li. Improving referring expression grounding with cross-modal attention-guided erasing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1950\u20131959, 2019.   \n[28] Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, and Rongrong Ji. Multi-task collaborative network for joint referring expression comprehension and segmentation. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 10034\u201310043, 2020.   \n[29] Yiwei Ma, Jiayi Ji, Xiaoshuai Sun, Yiyi Zhou, and Rongrong Ji. Towards local visual modeling for image captioning. Pattern Recognition, 138:109420, 2023.   \n[30] Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Zhang, and Rongrong Ji. X-clip: End-toend multi-grained contrastive learning for video-text retrieval. In Proceedings of the 30th ACM International Conference on Multimedia, pages 638\u2013647, 2022.   \n[31] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11\u201320, 2016.   \n[32] Varun K Nagaraja, Vlad I Morariu, and Larry S Davis. Modeling context between objects for referring expression understanding. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11\u201314, 2016, Proceedings, Part IV 14, pages 792\u2013807. Springer, 2016.   \n[33] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 2641\u20132649, 2015.   \n[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[35] Yunhang Shen, Chaoyou Fu, Peixian Chen, Mengdan Zhang, Ke Li, Xing Sun, Yunsheng Wu, Shaohui Lin, and Rongrong Ji. Aligning and prompting everything all at once for universal visual perception. 2024.   \n[36] Fengyuan Shi, Ruopeng Gao, Weilin Huang, and Limin Wang. Dynamic mdetr: A dynamic multimodal transformer decoder for visual grounding. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n[37] Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele. $\\mathrm{Mtr++}$ : Multi-agent motion prediction with symmetric scene modeling and guided intention querying. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.   \n[38] Mainak Singha, Ankit Jha, Bhupendra Solanki, Shirsha Bose, and Biplab Banerjee. Applenet: Visual attention parameterized prompt learning for few-shot remote sensing image generalization using clip. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.   \n[39] Wei Su, Peihan Miao, Huanzhang Dou, Yongjian Fu, and Xi Li. Referring expression comprehension using language adaptive inference. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 2357\u20132365, 2023.   \n[40] Wei Su, Peihan Miao, Huanzhang Dou, Gaoang Wang, Liang Qiao, Zheyang Li, and Xi Li. Language adaptive weight generation for multi-task visual grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10857\u201310866, 2023.   \n[41] Yabing Wang, Jianfeng Dong, Tianxiang Liang, Minsong Zhang, Rui Cai, and Xun Wang. Cross-lingual cross-modal retrieval with noise-robust learning. In Proceedings of the 30th ACM International Conference on Multimedia, pages 422\u2013433, 2022.   \n[42] Yabing Wang, Fan Wang, Jianfeng Dong, and Hao Luo. Cl2cm: Improving cross-lingual crossmodal retrieval via cross-lingual knowledge transfer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 5651\u20135659, 2024.   \n[43] Yabing Wang, Le Wang, Qiang Zhou, Zhibin Wang, Hao Li, Gang Hua, and Wei Tang. Multimodal llm enhanced cross-lingual cross-modal retrieval. arXiv preprint arXiv:2409.19961, 2024.   \n[44] Yabing Wang, Shuhui Wang, Hao Luo, Jianfeng Dong, Fan Wang, Meng Han, Xun Wang, and Meng Wang. Dual-view curricular optimal transport for cross-lingual cross-modal retrieval. IEEE Transactions on Image Processing, 33:1522\u20131533, 2024.   \n[45] Yingming Wang, Xiangyu Zhang, Tong Yang, and Jian Sun. Anchor detr: Query design for transformer-based detector. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages 2567\u20132575, 2022.   \n[46] Shiyu Xuan, Qingpei Guo, Ming Yang, and Shiliang Zhang. Pink: Unveiling the power of referential comprehension for multi-modal llms. 2024.   \n[47] Zhengyuan Yang, Tianlang Chen, Liwei Wang, and Jiebo Luo. Improving one-stage visual grounding by recursive sub-query construction. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XIV 16, pages 387\u2013404. Springer, 2020.   \n[48] Zhengyuan Yang, Boqing Gong, Liwei Wang, Wenbing Huang, Dong Yu, and Jiebo Luo. A fast and accurate one-stage approach to visual grounding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4683\u20134693, 2019.   \n[49] Zhuyu Yao, Jiangbo Ai, Boxun Li, and Chi Zhang. Efficient detr: improving end-to-end object detector with dense prior. arXiv preprint arXiv:2104.01318, 2021.   \n[50] Jiabo Ye, Junfeng Tian, Ming Yan, Xiaoshan Yang, Xuwu Wang, Ji Zhang, Liang He, and Xin Lin. Shifting more attention to visual backbone: Query-modulated refinement networks for end-to-end visual grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15502\u201315512, 2022.   \n[51] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704, 2023.   \n[52] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg. Mattnet: Modular attention network for referring expression comprehension. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1307\u20131315, 2018.   \n[53] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expressions. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 69\u201385. Springer, 2016.   \n[54] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and HeungYeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022.   \n[55] Chuyang Zhao, Yifan Sun, Wenhao Wang, Qiang Chen, Errui Ding, Yi Yang, and Jingdong Wang. Ms-detr: Efficient detr training with mixed supervision. arXiv preprint arXiv:2401.03989, 2024.   \n[56] Peizhi Zhao, Shiyi Zheng, Wenye Zhao, Dongsheng Xu, Pijian Li, Yi Cai, and Qingbao Huang. Rethinking two-stage referring expression comprehension: A novel grounding and segmentation method modulated by point. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 7487\u20137495, 2024.   \n[57] Chaoyang Zhu, Yiyi Zhou, Yunhang Shen, Gen Luo, Xingjia Pan, Mingbao Lin, Chao Chen, Liujuan Cao, Xiaoshuai Sun, and Rongrong Ji. Seqtr: A simple yet universal network for visual grounding. In European Conference on Computer Vision, pages 598\u2013615. Springer, 2022.   \n[58] Bohan Zhuang, Qi Wu, Chunhua Shen, Ian Reid, and Anton Van Den Hengel. Parallel attention: A unified framework for visual object discovery through dialogs and queries. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4252\u20134261, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table 7: Ablation study of the direction of the features flow from the QA module on RefCOCOg. ", "page_idx": 14}, {"type": "table", "img_path": "oPvBnPTbQv/tmp/471465d215db1a4a62a5a9b2986697e5a356cf245f1a30a2f6bfa9905dc58390.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "oPvBnPTbQv/tmp/48179bd00f2cc497944fabbeb5912bde4c8aa17c2b43ceb4578f71ac2eb1aeda.jpg", "img_caption": ["Figure 6: The performance of different numbers of learnable queries on RefCOCOg. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.1 Effect on the RefFormer\u2019s direction. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In RefFormer, the QA module can serve as an adapter, injecting specific knowledge into the frozen CLIP model. In Table 7, we investigate the direction of feature flow from QA module. We find that using a dual-direction approach achieves the best performance. Through QA module, language features have aggregated relevant visual context information. As pointed to [25], incorporating rich visual context into linguistic features aids in achieving strong vision-language alignment and better indicating target objects. ", "page_idx": 14}, {"type": "text", "text": "A.2 Effect on the number of learnable queries. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We depict the performance in terms of $\\mathrm{Prec}@0.5$ according to the number of learnable queries $N_{q}$ in Figure 6. When we adopt the $N_{q}=3$ , the performance is best. However, further increases yield only slight improvements in metrics, as a large number of $N_{q}$ increases the difficulty of the model. Therefore, we default set the $N_{q}=3$ in our experiments. ", "page_idx": 14}, {"type": "text", "text": "A.3 Visualization ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Due to space limitations, we present additional visualization results here. As shown in Figure 7, the referential queries gradually focus on the target object and effectively provide target-related context for the decoder. These results demonstrate the effectiveness of our proposed methods. ", "page_idx": 14}, {"type": "image", "img_path": "oPvBnPTbQv/tmp/f8f5d4403056686da138577c7ebd67bab6fd9be874ce0a05b58e99b570d1b384.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "(a) Expression: a man with an orange t-shirt with pizza ", "page_idx": 15}, {"type": "image", "img_path": "oPvBnPTbQv/tmp/ca0f98498bf557509e7edaa5106452cf7c00c738bdd664cacb1879643eefa9b7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "(b) Expression: standing giraffe in the background ", "page_idx": 15}, {"type": "image", "img_path": "oPvBnPTbQv/tmp/c3fd90657da5900c18660a4a6ff4a572bad3db93a758602f40c9ed36d67b4f45.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "(c) Expression: a chair which a boy is sitting in ", "page_idx": 15}, {"type": "image", "img_path": "oPvBnPTbQv/tmp/f35b7e90f78528bf403b70ca1bca7204e85430d1ba6bd976181612fb27ef036e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "(d) Expression: a sandwich with colby jack cheese , tomato , and lettuce , on fresh cut bread ", "page_idx": 15}, {"type": "image", "img_path": "oPvBnPTbQv/tmp/6c55f4ff6aaf20a19d53da0e8c9bd2f23fd41df7b9a487fcc082bb12c7a85c3a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "(e) Expression: a man wearing nike shoes in a bright neon green top is playing tennis ", "page_idx": 15}, {"type": "image", "img_path": "oPvBnPTbQv/tmp/8277c83cd039d896e474428b20bc0bd758196d57cc10bde440fd1f406c26f794.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "(f) Expression: a red taxi standing next to a yellow sign ", "page_idx": 15}, {"type": "image", "img_path": "oPvBnPTbQv/tmp/ef39e4aa78c120d4447305d9ee26df2a32ac589f97a0d755d6fc0c36991cb6fc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "(g) Expression: young kid closest to projector ", "page_idx": 15}, {"type": "text", "text": "Figure 7: Qualitative results on RefCOCOg. The bounding boxes in green and red correspond to outputs from our model and the ground truth. Columns 2-6 showcase the attention maps generated by each QA module, while the last column represents the attention map from the decoder. ", "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Claims ", "page_idx": 15}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction (Section 1) accurately reflect our paper\u2019s contributions and scope. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. \u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We discuss the limitations of the work in the paper. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: Our paper does not include theoretical results. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: We fully describe the proposed model and implementation details in Section 4 and Section 5.2, and we submit our main code in the form of a zipped file in additional supplementary materials. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: We submit our main code in the form of a zipped flie in additional supplementary materials, and we will release the complete code after review. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. ", "page_idx": 17}, {"type": "text", "text": "\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We describe the datasets, metrics and implementation details in Sec. ?? and supplemental material. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: Error bars are not reported because it would be too computationally expensive. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). ", "page_idx": 18}, {"type": "text", "text": "\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: We only provide the GPU type in the supplemental material. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The research conducted in the paper conforms with the NeurIPS Code of Ethics in every respect. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 19}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper does not use existing assets. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]