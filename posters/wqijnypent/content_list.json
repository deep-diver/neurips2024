[{"type": "text", "text": "Batched Energy-Entropy acquisition for Bayesian Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Felix Teufel12 Carsten Stahlhut1 Jesper Ferkinghoff-Borg ", "page_idx": 0}, {"type": "text", "text": "1Machine Intelligence, Novo Nordisk A/S 2Department of Biology, University of Copenhagen {fegt,ctqs,jfgb}@novonordisk.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bayesian optimization (BO) is an attractive machine learning framework for performing sample-efficient global optimization of black-box functions. The optimization process is guided by an acquisition function that selects points to acquire in each round of BO. In batched BO, when multiple points are acquired in parallel, commonly used acquisition functions are often high-dimensional and intractable, leading to the use of sampling-based alternatives. We propose a statistical physics inspired acquisition function for BO with Gaussian processes that can natively handle batches. Batched Energy-Entropy acquisition for BO (BEEBO) enables tight control of the explore-exploit trade-off of the optimization process and generalizes to heteroskedastic black-box problems. We demonstrate the applicability of BEEBO on a range of problems, showing competitive performance to existing methods. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bayesian Optimization (BO) has since its inception [1, 2] made a profound contribution to the realm of global optimization of black-box functions through the usage of Bayesian statistics. For global optimization problems pursuing $x_{*}\\,=\\,\\operatorname{argmax}_{x\\in\\mathcal{X}}f_{\\mathrm{true}}(x)$ , BO has surfaced as a premier strategy for efficiently handling especially complex and costly unknown functions, $f_{\\mathrm{true}}(x)$ . While BO is traditionally formulated in a single-point scenario, where individual points are queried and results are observed sequentially, there are situations where batched acquisition is needed. Such situations arise when $f_{\\mathrm{true}}(x)$ is expensive to evaluate in either time or cost, but can be effectively evaluated in parallel by dispatching multiple experiments, reducing the overall optimization time. This is often the case in e.g. drug discovery, materials design or hyperparameter tuning for deep models [3, 4, 5, 6, 7]. ", "page_idx": 0}, {"type": "text", "text": "The realization that BO could be employed for the training of deep neural networks, as suggested by [3], sparked renewed research interest, with advancements encompassing a variety of areas, including the generalization to accommo", "page_idx": 0}, {"type": "image", "img_path": "wQiJNyPENt/tmp/02f514fa77fd8e368db2320ceea9abc9b42e127dcbdd9ddc9b06abfb640e6282.jpg", "img_caption": ["Figure 1: $q$ -UCB does not allow for controlling its explore-exploit trade-off with large batches. A GP surrogate (background) was initialized with 100 random points of the Ackley function. $q_{\\mathrm{{'}}}$ -UCB was run with $\\kappa=0.1$ and $\\kappa=100$ , BEEBO with $T^{\\prime}\\!=$ 0.05 and $T^{\\prime}{=}50$ . Batch size $Q{=}100$ . "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "date noisy inputs [8, 9], heteroskedastic noise [10, 11], multi-task problems [12], multi-fidelity [13], high-dimensional input spaces [14], and parallel methods with batch queries [15, 16]. Generally, these desired properties are addressed by customizing one of the two key components in BO, either the surrogate model or the acquisition function. The surrogate model $f$ approximates the black-box function $f_{\\mathrm{true}}$ using the available data. In BO, the surrogate is formulated from a Bayesian perspective, allowing us to quantify the model\u2019s uncertainty when evaluating new points. Typically, the model of choice is a Gaussian Process (GP) [17]. The acquisition function is responsible for guiding the selection of new input point(s) to evaluate at each optimization step, utilizing the surrogate model to identify promising regions in the input domain and exploring the unknown function further. ", "page_idx": 1}, {"type": "text", "text": "Any acquisition process needs to trade off exploration (reducing uncertainty to learn a better surrogate model) against exploitation (selecting points with a high expected $f_{\\mathrm{true}}(x)$ based on the current surrogate). In this work, we are particularly interested in acquisition processes that make this trade-off controllable using a hyperparameter. Controllability can be a desirable property if e.g. domain knowledge relating to the difficulty of the optimization process and the quality of the surrogate model is available, or if the strategy needs to be adjusted depending on future experimental budgets. Similarly, it can be desirable to acquire multiple $x$ with high $f_{\\mathrm{true}}(x)$ in a batch (as opposed to just finding the optimum $x_{*}$ , with the remaining $x$ being considered explorative). This is useful when optima identified in BO can be subject to constraints that are unknown at optimization time, but may render $x_{*}$ intractable [18]. Such constraints arise when the $f_{\\mathrm{true}}$ explored in BO is a necessary simplification of the actual objective. Practical examples include e.g. the synthesizability of a material at larger scale, when BO experiments are performed at lab scale; or the in vivo activity of a molecule with BO experiments performed in vitro. ", "page_idx": 1}, {"type": "text", "text": "A wide range of batch mode acquisition functions has been proposed, with approaches often leveraging random sampling strategies or Monte Carlo (MC) integration, which can adversely affect controllability for large batches (Figure 1). In contrast, we here introduce BEEBO (Batched EnergyEntropy acquisition for BO), a statistical physics inspired acquisition function for BO with GP surrogate models that natively generalizes to batched acquisition. BEEBO enables ", "page_idx": 1}, {"type": "text", "text": "\u2022 Parallel gradient-based optimization of the inputs, without requiring sampling or Monte Carlo integrals.   \n\u2022 Tight control of the explore-exploit trade-off in batch mode using a single temperature hyperparameter.   \n\u2022 Risk-averse BO under heteroskedastic noise. ", "page_idx": 1}, {"type": "text", "text": "We demonstrate the application of BEEBO on a wide range of test problems, and investigate its behaviour under heteroskedastic noise. ", "page_idx": 1}, {"type": "text", "text": "2 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Batch variants of traditional strategies Parallel acquisition in BO has seen a variety of approaches, often starting from established single-point acquisition functions like probability of improvement (PI), expected improvement (EI), knowledge gradient (KG) or upper confidence bound (UCB) [2, 19, 20, 21, 22]. Reformulating these to batch mode with $Q$ query points, we obtain $q_{\\l}$ -PI, $q\\cdot$ - EI, and $q_{\\l}$ -UCB [23, 24]. While the single-point specifications provide an analytical form and enable gradient-based optimization, batch expressions are more challenging and require different optimization strategies, typically involving greedy algorithms [25] or deriving an integral expression over multiple points. ", "page_idx": 1}, {"type": "text", "text": "For instance, in the popular EI acquisition function, a single point is selected by maximizing the expression $\\begin{array}{r}{a_{\\mathrm{EI}}\\left(\\Bar{x_{\\right)}}\\;\\stackrel{}{=}\\;\\mathbb{E}[\\operatorname*{max}\\left(\\Bar{0_{}}f\\left(x\\right)-f_{t}^{\\ast}\\right)]\\;=\\;\\int\\operatorname*{max}\\left(0,\\overbar{f}\\left(x\\right)-f_{t}^{\\ast}\\right)P\\left(f|x\\right)d f.}\\end{array}$ . Here $f_{t}^{*}$ represents the best observed evaluation of $f_{\\mathrm{true}}$ so far. With a surrogate model in the form of a GP, the acquisition function depends only on the predictive mean and variance functions, $\\mu\\left(x\\right)$ and $C\\left(x\\right)$ . Effectively, we need to evaluate the cumulative normal distribution, which quickly becomes intractable for large batch sizes and approximating the gradient of the $q$ -EI acquisition function typically requires MC estimation [26, 27]. However, proper MC integration can be laborious and is sensitive to both the dimension of the problem and the choice of batch size $Q$ . Specifically, MC methods face the curse of dimensionality problem when applied to high-dimensional integrals, as they require an exponentially increasing number of sample points to maintain accuracy, making them computationally impractical for such tasks [28, 29]. Of particular interest is Wilson et al. [24], in which they adopt the reparameterization trick [30, 31] on acquisition functions integrals, enabling gradient based approaches to the optimization of PI, EI, and UCB. This demonstrates particular usefulness in modest to higher dimensions. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "While EI trades off exploration and exploitation, users do not have a direct control over the balance. To alleviate this, Sobester et al. [32] proposed a weighted EI formulation. An alternative strategy with an explicit explore-exploit trade-off is offered by the UCB acquisition function, $a_{\\mathrm{UCB}}\\left(x\\right)=$ $\\mu(x)+{\\sqrt{\\kappa}}\\cdot{\\sqrt{C(x)}}$ , which directly expresses exploration and exploitation as two terms, traded off by the parameter $\\kappa$ . As we are particularly interested in enabling this direct user control, we focus our primary comparison on $q$ -UCB in the main text, while a more extensive comparison with alternative methods can be found in Appendix B, both theoretically and experimentally. ", "page_idx": 2}, {"type": "text", "text": "Greedy strategies As mentioned, a popular approach for leveraging single-point acquisition functions is devising batch fliling strategies that score candidate points sequentially. Kriging Believer (KB) [33] uses EI to select points and iteratively updates the GP by fantasizing an observation with the posterior mean. Likewise, GP-BUCB [34] uses fantasized observations to update $\\sqrt{C(x)}$ at each step. Local penalization (LP) [35] introduces a penalization function that repulses selection away from already selected points. Contal et al. [36] propose selecting a single point using UCB and dedicating the remainder of the batch budget for exploration in a restricted region around the believed optimum. GLASSES [37] treats batch selection as a multi-step lookahead problem to overcome the myopia of only considering the immediate effect of selecting a point. ", "page_idx": 2}, {"type": "text", "text": "Entropy based strategies From an information theory perspective, BO can be interpreted as seeking to reduce uncertainty over the location of optima of the unknown function. This has given rise to entropy-based acquisition functions such as entropy search (ES) [38], predictive entropy search (PES) [39] and max-value entropy search (MES) [13, 40, 41]. MES is distinct in that it seeks to quantify the mutual information between the unknown $f_{\\mathrm{true}}(x_{*})$ and the observations $y|D$ , rather than the location of $x_{*}$ . General-purpose Information-Based Bayesian OptimizatioN (GIBBON) [42] provides an extension of MES that enables application to batched acquisition as well as other challenges such as multi-fidelity BO. GIBBON proposes a lower bound formulation for the intractable batch MES criterion, which is then optimized using greedy selection. Despite being formulated to handle a large degree of parallelism, Moss et al. [42] reported that GIBBON fails in practice for large batches with $Q>50$ . Potentially, this behaviour is a consequence of the accuracy of the lower bound approximation. A heuristic scaling of the batch diversity was proposed to improve performance with large batches. GIBBON may also be interpreted as a determinantal point process (DPP) [4, 43]. In Appendix B we provide a detailed discussion of the relationship of the BEEBO acquisition function to GIBBON and DPPs. Note that while we will also make use of the term entropy in BEEBO, the quantity is distinct from the ones leveraged by the aforementioned approaches in the sense that it does not relate to an unknown optimum. ", "page_idx": 2}, {"type": "text", "text": "Thompson sampling Given the challenges of generalizing acquisition functions to batch mode, Thompson sampling (TS), which was originally adopted from bandit problems [44, 45, 46, 47, 48], is a popular alternative strategy for guiding batched BO. While being an attractive approach in general, it has been demonstrated that default TS can become too exploitative, motivating the use of alternatives such as Bayesian Quadrature [49], or advanced strategies on top of TS that ensure diversity [18]. Eriksson et al. [50] demonstrate that overexploration also can be problematic in higher dimensions, and alleviate this using local trust regions in TuRBO. Maintaining such regions with high precision discretization can be memory-expensive, as indicated by [51], who suggest using MCMC-BO with adaptive local optimization to address this by transitioning a set of candidate points towards more promising positions. ", "page_idx": 2}, {"type": "text", "text": "3 The BEEBO acquisition function ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Assume $f_{\\mathrm{true}}\\,:\\,X\\,\\rightarrow\\,\\mathbb{R}$ is some real output associated with the input and a set of data be given $D=\\{(x_{i},y_{i})\\}_{i=1}^{N}$ where $y_{i}\\in\\mathbb{R}$ represent some noisy observations of $f_{\\mathrm{true}}(x_{i})$ , say ", "page_idx": 2}, {"type": "equation", "text": "$$\ny_{i}=f_{\\mathrm{true}}(x_{i})+\\epsilon_{i}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with $\\epsilon_{i}$ denoting the measurement noise. Let $\\mathbf{x}=(x_{1},\\cdot\\cdot\\cdot,x_{Q})\\in X^{Q}$ represent a collection of test points we wish to assign an acquisition value to. In keeping with the BO framework, we assume a given posterior probability distribution over the surrogate function f evaluated at $\\mathbf{x}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{f}\\left(\\mathbf{x}\\right)\\sim P(\\mathbf{f}\\mid D,\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The lack of knowledge we have of the surrogate function at $\\mathbf{x}$ is quantified by the differential entropy $H$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nH\\big(\\mathbf{f}\\mid D,\\mathbf{x}\\big)=-\\int P\\big(\\mathbf{f}\\mid D,\\mathbf{x}\\big)\\ln\\Bigl(P\\big(\\mathbf{f}\\mid D,\\mathbf{x}\\big)\\Bigr)d\\mathbf{f}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This entropy can be contrasted with the expected entropy of the surrogate function if $Q$ observations $\\mathbf{y}=(y_{1},\\cdot\\cdot\\cdot,y_{Q})$ were acquired at $\\mathbf{x}$ , i.e. if the training data $D$ would be augmented with $D^{\\prime}(\\mathbf{y})=$ $\\{(x_{q},y_{q})\\}_{q=1}^{Q}$ to form the joint data set $D_{\\mathrm{aug}}({\\mathbf{y}})=\\bigl(D,D^{\\prime}({\\mathbf{y}})\\bigr)$ . We refer to this entropy as $H_{\\mathrm{aug}}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nH_{\\mathrm{aug}}\\big(\\mathbf{f}\\mid D,\\mathbf{x}\\big)=\\int P(\\mathbf{y}\\mid D,\\mathbf{x})H\\big(\\mathbf{f}\\mid D_{\\mathrm{aug}}(\\mathbf{y})\\big)d\\mathbf{y},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $P(\\mathbf{y}\\mid D,\\mathbf{x})$ represent the posterior predictive distribution at $\\mathbf{x}$ . The expected information gain, $I({\\bf x})$ , from acquiring observations at $\\mathbf{x}$ is given by the expected reduction of entropy from this process: ", "page_idx": 3}, {"type": "equation", "text": "$$\nI(\\mathbf{x})=H\\big(\\mathbf{f}\\mid D,\\mathbf{x}\\big)-H_{\\mathrm{aug}}\\big(\\mathbf{f}\\mid D,\\mathbf{x}\\big)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We propose to represent the explore component of the acquisition function, $a_{\\mathrm{BEEBO}}$ , by $I({\\bf x})$ . The information gain $I({\\bf x})$ is distinct from the quantities exploited by entropy search approaches, as it quantifies global uncertainty reduction, rather than estimating the information over an unknown $x_{*}$ The information gain is directly applicable to multivariate functions and to heteroskedastic settings where $\\sigma^{2}=\\sigma^{2}(\\bar{\\bf x})$ . Since large measurement uncertainties imply smaller information gain, aBEEBO exhibits risk-averse behaviour [11] by automatically prioritizing regions of small uncertainties from where more precise information of $f_{\\mathrm{true}}$ can be obtained, everything else being equal. ", "page_idx": 3}, {"type": "text", "text": "The exploit component of BEEBO relies on taking expectation values of a scalar function of the random variable $\\mathbf{f}\\left(\\mathbf{x}\\right)$ , $\\tilde{E}:\\mathbb{R}^{Q}\\rightarrow\\mathbb{R}$ , that summarizes the optimality properties of a given batch $\\mathbf{x}$ Natural choices would be the mean or the maximum of $\\mathbf{f}\\left(\\mathbf{x}\\right)$ . Of particular interest is expressing the optimality as a softmax-weighted sum over $\\mathbf{f}(\\mathbf{x})$ , as this allows us to smoothly interpolate between the two regimes: ", "page_idx": 3}, {"type": "equation", "text": "$$\nE(\\mathbf{x})=-\\mathbb{E}[\\tilde{E}(\\mathbf{x})]\\cdot Q=-\\mathbb{E}\\left[\\sum_{q=1}^{Q}\\mathrm{softmax}(\\beta\\mathbf{f})_{q}f_{q}\\right]\\cdot Q,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\beta$ is the softmax inverse temperature. At $\\beta=0$ , we recover the mean. We scale the expectation with $Q$ so that both $I$ and $E$ scale linearly with increasing batch size. While the mean provides a closed form expression for its expectation, this is not the case for the general softmax-weighted sum of a multivariate normal. Using Taylor expansion, we introduce an approximation of the expectation of the softmax-weighted sum that is fully differentiable and can be computed in closed form. A detailed derivation is provided in Appendix A. At $\\beta=0$ , all $Q$ points contribute equally to $E(\\mathbf{x})$ , whereas at $\\beta>0$ , points that do not compete for optimality are dynamically released. This effect can be quantified as the effective number of points via the entropy of the softmax weights. In the following, we will refer to the (exact) $\\beta=0$ limit as meanBEEBO, and the (approximated) general case as maxBEEBO. ", "page_idx": 3}, {"type": "text", "text": "The BEEBO acquisition function then takes the form ", "page_idx": 3}, {"type": "equation", "text": "$$\na_{\\mathrm{BEEBO}}(\\mathbf{x})=-E(\\mathbf{x})+T\\cdot I(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $T$ sets the balance between exploitation (small $T$ ) and exploration (large $T$ ). As both $E$ and $I$ scale with the batch size $Q$ , a given choice of $T$ would set the explore-exploit balance in an approximately $Q$ -independent manner. This acquisition function bears a strong similarity to the definition of (negative) free energies in statistical physics, where $E$ and $I$ correspond to respectively the thermodynamic energy and entropy of the system and $T$ corresponds to the temperature. ", "page_idx": 3}, {"type": "text", "text": "3.1 BEEBO with Gaussian processes ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Gaussian processes offer a particular convenient framework for BO, due to the availability of closedform expressions for the inference step [17]. Specifically ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{P(\\mathbf{f}\\mid D,\\mathbf{x})}&{=}&{\\mathcal{N}(\\mathbf{f}\\mid\\mu(\\mathbf{x}),C(\\mathbf{x}))}\\\\ {\\mu(\\mathbf{x})}&{=}&{K(\\mathbf{x},\\mathbf{x}_{D})\\cdot M_{D}^{-1}\\cdot\\mathbf{y}_{D}}\\\\ {C(\\mathbf{x})}&{=}&{K(\\mathbf{x},\\mathbf{x})-K(\\mathbf{x},\\mathbf{x}_{D})\\cdot M_{D}^{-1}\\cdot K(\\mathbf{x}_{D},\\mathbf{x})}\\\\ {M_{D}}&{=}&{K(\\mathbf{x}_{D},\\mathbf{x}_{D})+\\sigma^{2}(\\mathbf{x}_{D})}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{N}(\\cdot\\,|\\,\\mu,C)$ is the multivariate Gaussian distribution with mean $\\mu$ , and covariance $C,\\mathbf{x}_{D}$ and $\\mathbf{y}_{D}$ are the $x$ and $y$ values of the acquired data, $\\boldsymbol{\\sigma}(\\mathbf{x}_{D})=\\mathrm{diag}\\left(\\sigma_{1}^{2},\\cdots,\\sigma_{N}^{2}\\right)$ is a diagonal matrix with the measurement uncertainties in the diagonal and $K(\\cdot,\\cdot)$ are matrices derived from the GP-kernel, $k(\\cdot,\\cdot)$ , i.e. $K(\\mathbf{x},\\mathbf{x}^{\\prime})_{i j}=k(x_{i},x_{j}^{\\prime})$ . It is worth noting that $C(\\mathbf{x})$ only depends on the input location of the test points $\\mathbf{x}$ and the data points $\\mathbf{x}_{D}$ with their corresponding measurement uncertainties, $\\sigma^{2}(\\mathbf{x}_{D})$ , but not on the actual observations, $\\mathbf{y}_{D}$ . Consequently, the entropy of the posterior distribution ", "page_idx": 4}, {"type": "equation", "text": "$$\nH\\big(\\mathbf{f}\\mid D,\\mathbf{x}\\big)=\\frac{Q}{2}\\ln(2\\pi e)+\\frac{1}{2}\\ln\\operatorname*{det}(C(\\mathbf{x}))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "is independent of $\\mathbf{y}_{D}$ as well, with ln det denoting the log determinant. Similarly, the expected entropy of $\\mathbf{f}$ if observations at $\\mathbf{x}$ were acquired, simply reads ", "page_idx": 4}, {"type": "equation", "text": "$$\nH_{\\mathrm{aug}}\\big(\\mathbf{f}\\,|\\,D,\\mathbf{x}\\big)=\\frac{Q}{2}\\ln(2\\pi e)+\\frac{1}{2}\\ln\\operatorname*{det}(C_{\\mathrm{aug}}(\\mathbf{x})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{C_{\\mathrm{aug}}(\\mathbf{x})}&{=}&{K(\\mathbf{x},\\mathbf{x})-K(\\mathbf{x},\\mathbf{x}_{\\mathrm{aug}})\\cdot M_{\\mathrm{aug}}^{-1}\\cdot K(\\mathbf{x}_{\\mathrm{aug}},\\mathbf{x})}\\\\ {M_{\\mathrm{aug}}}&{=}&{K(\\mathbf{x}_{\\mathrm{aug}},\\mathbf{x}_{\\mathrm{aug}})+\\sigma^{2}(\\mathbf{x}_{\\mathrm{aug}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and $\\mathbf{x}_{\\mathrm{aug}}=\\mathbf{x}_{D_{\\mathrm{aug}}}$ . The BEEBO acquisition function is then given by ", "page_idx": 4}, {"type": "equation", "text": "$$\na_{\\mathrm{BEEBO}}(\\mathbf{x})=\\mathbb{E}[\\tilde{E}(\\mathbf{x})]\\cdot Q+T\\cdot I(\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the expectation is either the mean, qQ=1 \u00b5q, or the closed form approximation of the softmax-weighted sum described in Appendix A, and ", "page_idx": 4}, {"type": "equation", "text": "$$\nI(\\mathbf{x})=\\frac{1}{2}\\ln\\operatorname*{det}(C(\\mathbf{x}))-\\frac{1}{2}\\ln\\operatorname*{det}(C_{\\mathrm{aug}}(\\mathbf{x})).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "All operations needed to compute the acquisition value $a_{\\mathrm{BEEBO}}(\\mathbf{x})$ are analytical. Using automatic differentiation, the batch of points $\\mathbf{x}$ can therefore be optimized with gradient-based methods, as laid out for meanBEEBO in Algorithm 1, with learning rate $\\gamma$ . In the pseudocode, ${\\mathcal{G P}}$ denotes a trained GP model object that holds the training data and the kernel function. Using the kernel\u2019s learned amplitude $A$ , we can relate BEEBO\u2019s $T$ parameter to the $\\kappa$ of UCB. This allows us to configure BEEBO using a scaled temperature $T^{\\bar{\\prime}}$ that ensures both methods have equal gradients at isosurfaces, enabling the user to follow existing guidance and intuition from UCB to control the trade-off. A derivation is provided in Appendix B.1. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Algorithm 1: meanBEEBO optimization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Input: model ${\\mathcal{G P}}$ , initial batch points $\\mathbf{x}$ ,   \ntemperature $T$   \nrepeat Calculate $\\mu(\\mathbf{x}),C(\\mathbf{x})$ from Equation 8 using ${\\mathcal{G P}}$ E \u2190\u2212 qQ=1 \u00b5q $\\mathcal{G P}_{\\mathrm{aug}}\\leftarrow\\mathrm{fantasize}(\\mathcal{G P},\\mathbf{x})$ Calculate $C_{\\mathrm{aug}}(\\mathbf{x})$ from Equation 11 using $\\mathcal{G P}_{\\mathrm{aug}}$ $\\begin{array}{r l}&{I\\leftarrow\\frac{1}{2}\\ln\\operatorname*{det}\\dot{(}C(\\mathbf{x}))-\\frac{1}{2}\\ln\\operatorname*{det}\\left(C_{\\mathrm{aug}}(\\mathbf{x})\\right)}\\\\ &{a\\leftarrow-E+T*I}\\\\ &{\\mathbf{x}\\leftarrow\\mathbf{x}+\\gamma\\nabla a}\\end{array}$   \nuntil converged   \nOutput: optimized batch points $\\mathbf{x}$ ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Test problems We benchmark acquisition function performance on a range of maximization test problems with varying dimensions (Table 1) available in BoTorch [52]. Test problems that are evaluated on multiple dimensions support specifying the respective arbitrary $d$ . As a high-dimensional problem with low inherent dimensionality, we embed the six-dimensional Hartmann function in $d=100$ [50, 53, 54]. We additionally test on two robot control problems (robot arm pushing and rover trajectory planning) in Appendix D.3 [55, 56]. ", "page_idx": 5}, {"type": "table", "img_path": "wQiJNyPENt/tmp/20c640be4aa3598939b1fedf9a3a511b9e4405cebcf71e35a38b29acc37e6d34.jpg", "table_caption": ["Table 1: Overview of the test problems used in the experiments. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "On each test problem, we perform 10 rounds of BO using $q$ -UCB or BEEBO with a given explore-exploit parameter for direct comparison. We use the scaled temperature $T^{\\prime}$ (B.1) to ensure that both methods operate at the same trade-off. In round 0, we seed the surrogate GP with $Q$ random points that were drawn so that each point has a minimum distance of 0.5 to the test problem\u2019s true optimum. We perform ten replicate runs for each problem and method, with replicate seeds controlled so that all methods start from the same $Q$ random points in a replicate. As we evaluate performance in a fixed-round, fixed- $Q$ optimization scenario, we set the explore hyperparameter to 0 in the last round (for maxBEEBO, we also set the softmax $\\beta$ to 0). We use $Q=100$ for all experiments, which is commonly understood to be a large batch size [50]. Additional results on small batch sizes (5, 10) are provided in Appendix D.2. All experiments use BoTorch\u2019s default utilities for acquisition function optimization and GPyTorch [57] GP training (C.1). ", "page_idx": 5}, {"type": "text", "text": "Heteroskedastic noise We investigate performance when optimizing under heteroskedastic noise on the 2D Branin function with three global optima. To construct a heteroskedastic problem, we specify noise so that the noise level is maximal at optima 2 and 3, decaying exponentially with distance to any of the two noised optima (C.4). No noise maximum is added at optimum 1. Therefore, while all three optima share the same $f_{\\mathrm{true}}(x)$ (Figure A1), only optimum 1 is favorable in terms of heteroskedastic risk. We perform BO for ten rounds with $\\beta=0.1$ and $Q=10$ using a heteroskedastic GP that learns surrogate models for both $f_{\\mathrm{true}}(x)$ and $\\sigma^{2}(x)$ . We report results over five replicate runs. ", "page_idx": 5}, {"type": "text", "text": "Metrics We report the mean best observed objective value after 10 rounds over the five replicates. As test problems have highly varying scales, we normalize the results on each test problem using min-max normalization. Typically, the minimum of a maximization problem is not known explicitly. We therefore set the minimum for normalization to the highest value observed among the random seed points. The maximum is given by the $f_{\\mathrm{true}}(x^{*})$ of the problem. The metric thus directly quantifies how much progress has been made to the true optimum from the random starting configuration on a 0-1 scale. ", "page_idx": 5}, {"type": "text", "text": "As we are not only interested in identifying a single $x$ with good $f_{\\mathrm{true}}(x)$ , we additionally quantify the overall quality of the final (exploitative) batch. We compute the batch instantaneous regret $\\begin{array}{r}{R=\\sum_{q<Q}\\dot{f}_{\\mathrm{true}}(\\dot{x}^{*})-f_{\\mathrm{true}}(x_{q})}\\end{array}$ of the last, exploitative batch. To bring results on all test problems to a similar scale, we divide it by the batch instantaneous regret of a batch of $Q$ random points on each problem. We refer to this metric as the relative batch instantaneous regret, $R_{\\mathrm{rel}}=R_{t=10}/R_{\\mathrm{random}}$ . ", "page_idx": 5}, {"type": "text", "text": "For BO under heteroskedastic noise, we wish to quantify the preference of a given method for different optima. As optima share the same $f_{\\mathrm{true}}(x^{*})$ , metrics operating on $f_{\\mathrm{true}}(x)$ are inherently unsuitable, and preference needs to be evaluated on $x$ directly. For each acquired point $x_{i}$ , we compute the distances $\\|\\boldsymbol{x}_{i}-\\boldsymbol{x}_{j}^{*}\\|_{2}$ to the $J$ individual optima. We report the mean distance to each optimum over all points in a batch. ", "page_idx": 5}, {"type": "table", "img_path": "wQiJNyPENt/tmp/bf9f93130733cd1b537ddc49d24c987f52694ceed10f7ffcb37d0cc51169c887.jpg", "table_caption": ["Table 2: Highest observed value after 10 rounds of BO w\u221aith $Q=100$ . The best value at each $\\kappa$ is indicated in blue. BEEBO is configured with $T^{\\prime}={1}/{2\\sqrt{\\kappa}}$ . Full BO curves are provided in D.6, confidence intervals and statistical tests in Tables A2, A3 and A4 "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 BO on test problems ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We benchmark BEEBO against $q$ -UCB at three rates of $\\kappa$ . Overall, we find that the simpler meanBEEBO variant outperforms maxBEEBO in terms of mean performance on all but the lowest rate of $\\kappa$ (Table 2). As we consider the configuration with the lowest rate to be exploit-dominated, this can be understood as a consequence of maxBEEBO effectively releasing non-contributing points for further exploration, with the low explore rate seeming sufficient to induce the necessary diversity. ", "page_idx": 6}, {"type": "text", "text": "While results on individual test problems vary, meanBEEBO shows improved performance over $q_{\\l}$ -UCB especially in the medium dimension range up to 50. For $d{=}100$ , we find mixed performance, with meanBEEBO gradually becoming more competitive with increasing $\\kappa$ . This is due to the fact that with increasing dimensionality, more exploration is beneficial for learning a good surrogate model before an actual BO process becomes effective. As we find that $q$ -UCB inherently performs more random-like sampling at large $Q$ , irrespective of $\\kappa$ , it benefits in such situations. ", "page_idx": 6}, {"type": "text", "text": "On average over all 33 performed experiments, meanBEEBO improves upon $q$ -UCB for large batches at any of the three rates (Table A3). We additionally benchmarked BEEBO against other popular BO strategies without explore-exploit hyperparameters. Interestingly, we found that the Kriging Believer (KB) iterative heuristic [33] can perform very competitively for large batches when using LogEI [58] as the acquisition function (Table A1), especially on the two robot control problems (Appendix D.3), but can be slower to optimize than BEEBO (Table A9). ", "page_idx": 6}, {"type": "text", "text": "When evaluating $R_{\\mathrm{rel}}$ in the ultimate round of BO, we find that BEEBO allows us to effectively acquire a batch with high $f_{\\mathrm{true}}(x)$ , highlighting the controllability of the acquisition function (Table 3). The $R_{\\mathrm{rel}}$ of $q$ -UCB is only slightly better than the $R$ of a random batch in many cases, even though the explore component was explicitly set to 0. We note that this is not due to the surrogate function being unsuitable - the results in Table 2 indicate that in most cases the location of $f_{\\mathrm{true}}(x^{*})$ is approximately known by round 10. Rather, we assume that this a consequence of the challenges of MC-based optimization of the acquisition function at large $Q$ . ", "page_idx": 6}, {"type": "table", "img_path": "wQiJNyPENt/tmp/32fee7c747ff585a7609636e64ae54aa7b6262ac41562240488fc0f0dabc950d.jpg", "table_caption": ["Table 3: Relative batch instantaneous regret $R_{r e l}$ in round 10 $\\left(\\kappa=0\\right)$ ) with $Q=100$ . The best value at each $\\kappa$ is indicated in blue. BEEBO is configured with $T^{\\prime}={}^{1}\\!/\\!2\\sqrt{\\kappa}$ . Lower means better. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.2 BO under heteroskedastic noise ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare performance of meanBEEBO and $q_{\\mathrm{{'}}}$ -UCB on the 3-optimum Branin function. Under heteroskedastic noise, we find that BEEBO preferentially optimizes towards the low-noise optimum 1 at the expense of the noisy optima 2 and 3 (Figure 2) and is therefore risk-averse. In the homoskedastic case, BEEBO does not exhibit this preference and optimizes for multiple optima. As expected, $q$ -UCB, which only uses the model posterior variance $\\sqrt{C(x)}$ instead of quantifying the actual information gain, does not display any preference for low-noise optima, showing similar behaviour under heteroskedastic and homoskedastic noise and remaining risk-neutral. ", "page_idx": 7}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We introduce BEEBO, an acquisition function for BO with GPs that can be optimized analytically and that scales natively to batched acquisition. By exploiting the independence of the information gain $I({\\bf x})$ on measurements y when using GP surrogates, BEEBO models the interdependence of unknown points $\\mathbf{x}$ in a batch and can optimize their positions jointly using gradient descent. ", "page_idx": 7}, {"type": "text", "text": "BEEBO enables full control of its explore-exploit trade-off using a hyperparameter $T$ that directly balances two terms, akin to UCB. Unlike in the reparametrization-based $q$ -methods, BEEBO\u2019s $T$ has predictable behaviour also at increasing batch sizes. ", "page_idx": 7}, {"type": "text", "text": "The numerical complexity of BEEBO is dominated by the need to compute the inverse of $M_{\\mathrm{aug}}$ in Equation 11, which in a plain implementation scales as $\\mathcal{O}((N+Q)^{3})$ . However, this can be reduced to $\\Dot{O}(N^{2}Q)$ ; specifically, the Cholesky decomposition of $M_{\\mathrm{aug}}$ can be expressed as $Q$ rank-1 updates of the pre-computed Cholesky decomposition of $M_{D}$ , where each update will have the complexity of $\\mathcal{O}(\\bar{N}^{2})$ . The calculation of the energy, $E$ , and the information gain, $I$ , scales as $\\mathcal{O}(N\\cdot Q)$ and $O(Q^{3})$ , respectively, and are thus sub-dominant to the update needed for $M_{\\mathrm{aug}}^{-1}$ . For large $N$ this approach may nevertheless become prohibitively slow. To overcome this limitation, methods for scalable GPs and fast predictive covariances such as LOVE [59] can be considered. The LOVE method allows a further reduction of the complexity of the Cholelsky update of $M_{\\mathrm{aug}}$ to $O(N\\cdot r\\cdot Q)$ [60], where $r$ is the rank of the LOVE approximation for $M_{D}$ , typically $r\\ll N$ . ", "page_idx": 7}, {"type": "image", "img_path": "wQiJNyPENt/tmp/f1908f0296bf09eb3826ad2bd4c6fa84817f059e16bc07a121f3777a47caef5e.jpg", "img_caption": ["Figure 2: Mean distances of acquired points to the different optima of the Branin function. Under heteroskedastic noise, BEEBO is risk-averse and preferentially optimizes towards the low-noise optimum 1. Under homoskedastic noise, there is no preference. $q$ -UCB does not adapt its behaviour to noise, remaining risk-neutral. The means and standard deviations over five replicates are shown. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "As opposed to $q$ -UCB, BEEBO can take heteroskedastic noise into account when computing the information gain, and preferentially acquires more informative low-noise points. We note that when the noise function $\\sigma^{2}\\dot{(x)}$ is unknown, and needs to be explored at the same time with $f_{\\mathrm{true}}(x)$ , it is critical that the initial random points sufficiently capture the noise landscape well enough for the information gain component to be useful, as the uncertainty of the surrogate on $\\sigma^{2}(x)$ is not used. This would require a fully Bayesian approach that integrates over the distribution of $\\sigma^{2}(x)$ . The problem does not arise if the heteroskedastic noise of an experiment is known beforehand by e.g. instrument calibration. While not the focus of this work, we note that using the information gain could also be beneficial in sequential single-sample BO on heteroskedastic problems. ", "page_idx": 8}, {"type": "text", "text": "7 Outlook ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In our experiments, we have focused on maintaining consistent explore-exploit ratios throughout the optimization rounds to ensure an equitable experimental comparison with $q$ -UCB and demonstrate the effect of the hyperparameter choice. However, a more dynamic approach involving variable ratios could be more effective in real-world applications with a predetermined number of rounds [61]. Adopting a fully Bayesian perspective, one could consider the temperature hyperparameter $T$ as a random variable. This opens up an intriguing avenue for BEEBO, where $T$ could be drawn from a prior distribution that e.g. varies across optimization rounds, depending on the specific application. By tailoring this distribution, one could encourage a high level of exploration in the initial rounds, gradually transitioning towards a more exploitation-focused approach towards the end. In the presented experiments, we have implemented this as a strict constraint, maintaining a fixed $T$ until the final round, at which point we shift to full exploitation, i.e., $T=0$ . ", "page_idx": 8}, {"type": "text", "text": "While not explored in this work, we note that the BEEBO expression could naturally be extended to multi-objective optimization problems by capitalizing on GPs that handle vector-valued functions, such as multi-task GPs [12, 62]. Through e.g. the usage of the intrinsic model of coregionalization, we obtain a covariance function $k$ , and thereby a covariance matrix $C(\\mathbf{x})$ , over all input-task pairs. As the multi-task covariance matrix is jointly Gaussian, the expression of the information gain remains unchanged and can be computed like in the single-task case. The energy $E(\\mathbf{x})$ becomes vector-valued, providing an energy term for each of the tasks. This would allow for the introduction of task-specific weights in the acquisition function. As the extension only affects the surrogate model, the scaling remains cubic in the number of input-task observations. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Beyond GPs, BEEBO could be generalized to work with any probabilistic model. However, GPs are unique in that $H_{\\mathrm{aug}}$ is available in closed form and can be used to compute $I({\\bf x})$ analytically, without solving the integral over $\\mathbf{y}$ in Equation 4. Other models may require approximations and sampling-based approaches for computing the information gain. ", "page_idx": 9}, {"type": "text", "text": "Availability ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "A BoTorch implementation of BEEBO is available at https://github.com/novonordisk-research/BEE-BO. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Christoffer Riis, Jan C. Refsgaard and Kilian W. Conde-Frieboes for helpful discussions. FT was funded in part by the Novo Nordisk Foundation through the Center for Basic Machine Learning Research in Life Science (NNF20OC0062606). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] H. J. Kushner. A New Method of Locating the Maximum Point of an Arbitrary Multipeak Curve in the Presence of Noise. Journal of Basic Engineering, 86(1):97\u2013106, 03 1964.   \n[2] Jonas Mo\u02c7ckus. On bayesian methods for seeking the extremum. In Optimization Techniques IFIP Technical Conference: Novosibirsk, July 1\u20137, 1974, pages 400\u2013404. Springer, 1975.   \n[3] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical Bayesian Optimization of Machine Learning Algorithms. Advances in neural information processing systems, 25, 2012.   \n[4] Jesse Dodge, Kevin Jamieson, and Noah A Smith. Open loop hyperparameter optimization and determinantal point processes. arXiv preprint arXiv:1706.01566, 2017.   \n[5] Ryan-Rhys Grifftihs and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Constrained bayesian optimization for automatic chemical design using variational autoencoders. Chemical science, 11(2):577\u2013586, 2020.   \n[6] Ji Won Park, Samuel Stanton, Saeed Saremi, Andrew Watkins, Henri Dwyer, Vladimir Gligorijevic, Richard Bonneau, Stephen Ra, and Kyunghyun Cho. PropertyDAG: Multi-objective Bayesian optimization of partially ordered, mixed-variable properties for biological sequence design. arXiv, 2022.   \n[7] Samuel Stanton, Wesley Maddox, Nate Gruver, Phillip Maffettone, Emily Delaney, Peyton Greenside, and Andrew Gordon Wilson. Accelerating Bayesian optimization for biological sequence design with denoising autoencoders. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 20459\u201320478. PMLR, 17\u201323 Jul 2022.   \n[8] Peter I Frazier. A Tutorial on Bayesian Optimization. arXiv, 2018.   \n[9] Samuel Daulton, Sait Cakmak, Maximilian Balandat, Michael A Osborne, Enlu Zhou, and Eytan Bakshy. Robust Multi-Objective Bayesian Optimization Under Input Noise. arXiv, 2022.   \n[10] Benjamin Letham, Brian Karrer, Guilherme Ottoni, and Eytan Bakshy. Constrained bayesian optimization with noisy experiments, 2018.   \n[11] Anastasia Makarova, Ilnura Usmanova, Ilija Bogunovic, and Andreas Krause. Risk-averse heteroscedastic bayesian optimization. Advances in Neural Information Processing Systems, 34:17235\u201317245, 2021.   \n[12] Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task bayesian optimization. Advances in neural information processing systems, 26, 2013.   \n[13] Shion Takeno, Hitoshi Fukuoka, Yuhki Tsukada, Toshiyuki Koyama, Motoki Shiga, Ichiro Takeuchi, and Masayuki Karasuyama. Multi-fidelity bayesian optimization with max-value entropy search and its parallelization. In International Conference on Machine Learning, pages 9334\u20139345. PMLR, 2020.   \n[14] Riccardo Moriconi, Marc P Deisenroth, and K S Sesh Kumar. High-dimensional Bayesian optimization using low-dimensional feature spaces. arXiv, 2019.   \n[15] Javad Azimi, Alan Fern, and Xiaoli Fern. Batch bayesian optimization via simulation matching. Advances in Neural Information Processing Systems, 23, 2010.   \n[16] Tarun Kathuria, Amit Deshpande, and Pushmeet Kohli. Batched gaussian process bandit optimization via determinantal point processes. Advances in neural information processing systems, 29, 2016.   \n[17] Christopher Williams and Carl Rasmussen. Gaussian processes for regression. Advances in neural information processing systems, 8, 1995.   \n[18] Natalie Maus, Kaiwen Wu, David Eriksson, and Jacob Gardner. Discovering many diverse solutions with bayesian optimization. In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent, editors, Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, volume 206 of Proceedings of Machine Learning Research, pages 1779\u20131798. PMLR, 25\u201327 Apr 2023.   \n[19] Donald R Jones, Matthias Schonlau, and William J Welch. Efficient global optimization of expensive black-box functions. Journal of Global optimization, 13:455\u2013492, 1998.   \n[20] Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3(Nov):397\u2013422, 2002.   \n[21] Peter Frazier, Warren Powell, and Savas Dayanik. The knowledge-gradient policy for correlated normal beliefs. INFORMS journal on Computing, 21(4):599\u2013613, 2009.   \n[22] Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. arXiv preprint arXiv:0912.3995, 2009.   \n[23] James T Wilson, Riccardo Moriconi, Frank Hutter, and Marc Peter Deisenroth. The reparameterization trick for acquisition functions. arXiv, 2017.   \n[24] James T Wilson, Frank Hutter, and Marc Peter Deisenroth. Maximizing acquisition functions for Bayesian optimization. arXiv, 2018.   \n[25] Amar Shah and Zoubin Ghahramani. Parallel Predictive Entropy Search for Batch Global Optimization of Expensive Objective Functions. Advances in neural information processing systems, 28, 2015.   \n[26] David Ginsbourger, Janis Janusevskis, and Rodolphe Le Riche. Dealing with asynchronicity in parallel Gaussian process based global optimization. PhD thesis, Mines Saint-Etienne, 2011.   \n[27] Enrico Crovini, Simon L Cotter, Konstantinos Zygalakis, and Andrew B Duncan. Batch bayesian optimization via particle gradient flows. arXiv preprint arXiv:2209.04722, 2022.   \n[28] J. Dick, F. Y. Kuo, and I. H. Sloan. High-dimensional integration: the quasi-monte carlo way. Acta Numerica, 22:133\u2013288, 2013.   \n[29] R. E. Caflisch. Monte carlo and quasi-monte carlo methods. Acta Numerica, 7:1\u201349, 1998.   \n[30] Durk P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameterization trick. Advances in neural information processing systems, 28, 2015.   \n[31] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International conference on machine learning, pages 1278\u20131286. PMLR, 2014.   \n[32] Andr\u00e1s S\u00f3bester, Stephen J Leary, and Andy J Keane. On the design of optimization strategies based on global response surface approximation models. Journal of Global Optimization, 33:31\u201359, 2005.   \n[33] David Ginsbourger, Rodolphe Le Riche, and Laurent Carraro. Kriging Is Well-Suited to Parallelize Optimization, pages 131\u2013162. Springer Berlin Heidelberg, Berlin, Heidelberg, 2010.   \n[34] Thomas Desautels, Andreas Krause, and Joel W. Burdick. Parallelizing exploration-exploitation tradeoffs in gaussian process bandit optimization. Journal of Machine Learning Research, 15(119):4053\u20134103, 2014.   \n[35] Javier Gonzalez, Zhenwen Dai, Philipp Hennig, and Neil Lawrence. Batch bayesian optimization via local penalization. In Arthur Gretton and Christian C. Robert, editors, Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, volume 51 of Proceedings of Machine Learning Research, pages 648\u2013657, Cadiz, Spain, 09\u201311 May 2016. PMLR.   \n[36] Emile Contal, David Buffoni, Alexandre Robicquet, and Nicolas Vayatis. Parallel gaussian process optimization with upper confidence bound and pure exploration. In Hendrik Blockeel, Kristian Kersting, Siegfried Nijssen, and Filip \u017delezn\u00fd, editors, Machine Learning and Knowledge Discovery in Databases, pages 225\u2013240, Berlin, Heidelberg, 2013. Springer Berlin Heidelberg.   \n[37] Javier Gonzalez, Michael Osborne, and Neil Lawrence. Glasses: Relieving the myopia of bayesian optimisation. In Arthur Gretton and Christian C. Robert, editors, Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, volume 51 of Proceedings of Machine Learning Research, pages 790\u2013799, Cadiz, Spain, 09\u201311 May 2016. PMLR.   \n[38] Philipp Hennig and Christian J. Schuler. Entropy search for information-efficient global optimization. Journal of Machine Learning Research, 13(57):1809\u20131837, 2012.   \n[39] Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Matthew W. Hoffman, and Zoubin Ghahramani. Predictive entropy search for efficient global optimization of black-box functions. ArXiv, abs/1406.2541, 2014.   \n[40] Zi Wang and Stefanie Jegelka. Max-value entropy search for efficient bayesian optimization. In International Conference on Machine Learning, pages 3627\u20133635. PMLR, 2017.   \n[41] Henry B Moss, David S Leslie, and Paul Rayson. Mumbo: Multi-task max-value bayesian optimization. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14\u201318, 2020, Proceedings, Part III, pages 447\u2013462. Springer, 2021.   \n[42] Henry B. Moss, David S. Leslie, Javier I. Gonz\u00e1lez, and Paul Rayson. Gibbon: General-purpose information-based bayesian optimisation. ArXiv, abs/2102.03324, 2021.   \n[43] Alex Kulesza and Ben Taskar. Determinantal point processes for machine learning. Found. Trends Mach. Learn., 5:123\u2013286, 2012.   \n[44] William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285\u2013294, 1933.   \n[45] Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 844\u2013853. PMLR, 06\u201311 Aug 2017.   \n[46] Kirthevasan Kandasamy, Akshay Krishnamurthy, Jeff Schneider, and Barnab\u00e1s P\u00f3czos. Parallelised bayesian optimisation via thompson sampling. In International Conference on Artificial Intelligence and Statistics, pages 133\u2013142. PMLR, 2018.   \n[47] Vahab Mirrokni, Mohammad Shadravan, et al. Parallelizing thompson sampling. In Advances in Neural Information Processing Systems, 2021.   \n[48] Amin Karbasi, Vahab Mirrokni, and Mohammad Shadravan. Parallelizing thompson sampling. Advances in Neural Information Processing Systems, 34:10535\u201310548, 2021.   \n[49] Masaki Adachi, Satoshi Hayakawa, Saad Hamid, Martin J\u00f8rgensen, Harald Oberhauser, and Micheal A Osborne. Sober: Scalable batch bayesian optimization and quadrature using recombination constraints. arXiv preprint arXiv:2301.11832, 2023.   \n[50] David Eriksson and Martin Jankowiak. High-dimensional Bayesian optimization with sparse axis-aligned subspaces. In Cassio de Campos and Marloes H. Maathuis, editors, Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence, volume 161 of Proceedings of Machine Learning Research, pages 493\u2013503. PMLR, 27\u201330 Jul 2021.   \n[51] Zeji Yi, Yunyue Wei, Chu Xin Cheng, Kaibo He, and Yanan Sui. Improving sample efficiency of high dimensional bayesian optimization with mcmc. arXiv preprint arXiv:2401.02650, 2024.   \n[52] Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, Andrew Gordon Wilson, and Eytan Bakshy. Botorch: a framework for efficient monte-carlo bayesian optimization. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS\u201920, Red Hook, NY, USA, 2020. Curran Associates Inc.   \n[53] Ziyu Wang, Frank Hutter, Masrour Zoghi, David Matheson, and Nando De Freitas. Bayesian optimization in a billion dimensions via random embeddings. J. Artif. Int. Res., 55(1):361\u2013387, jan 2016.   \n[54] Leonard Papenmeier, Luigi Nardi, and Matthias Poloczek. Increasing the scope as you learn: Adaptive bayesian optimization in nested subspaces. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[55] Zi Wang, Clement Gehring, Pushmeet Kohli, and Stefanie Jegelka. Batched large-scale bayesian optimization in high-dimensional spaces. In Amos Storkey and Fernando Perez-Cruz, editors, Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning Research, pages 745\u2013754. PMLR, 09\u201311 Apr 2018.   \n[56] Zi Wang, Chengtao Li, Stefanie Jegelka, and Pushmeet Kohli. Batched high-dimensional bayesian optimization via structural kernel learning. In International Conference on Machine Learning, 2017.   \n[57] Jacob R. Gardner, Geoff Pleiss, David Bindel, Kilian Q. Weinberger, and Andrew Gordon Wilson. Gpytorch: blackbox matrix-matrix gaussian process inference with gpu acceleration. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS\u201918, page 7587\u20137597, Red Hook, NY, USA, 2018. Curran Associates Inc.   \n[58] Sebastian Ament, Sam Daulton, David Eriksson, Maximilian Balandat, and Eytan Bakshy. Unexpected improvements to expected improvement for bayesian optimization. In Thirtyseventh Conference on Neural Information Processing Systems, 2023.   \n[59] Geoff Pleiss, Jacob Gardner, Kilian Weinberger, and Andrew Gordon Wilson. Constanttime predictive distributions for Gaussian processes. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 4114\u20134123. PMLR, 10\u201315 Jul 2018.   \n[60] Shali Jiang, Daniel R. Jiang, Maximilian Balandat, Brian Karrer, Jacob R. Gardner, and R. Garnett. Efficient nonmyopic bayesian optimization via one-shot multi-step trees. ArXiv, abs/2006.15779, 2020.   \n[61] George De Ath, Richard M. Everson, Alma A. M. Rahat, and Jonathan E. Fieldsend. Greed is good: Exploration and exploitation trade-offs in bayesian optimisation. ACM Trans. Evol. Learn. Optim., 1(1), apr 2021.   \n[62] Edwin V Bonilla, Kian Chai, and Christopher Williams. Multi-task gaussian process prediction. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems, volume 20. Curran Associates, Inc., 2007.   \n[63] Andreas Krause, Ajit Singh, and Carlos Guestrin. Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies. Journal of Machine Learning Research, 9(8):235\u2013284, 2008.   \n[64] Benjamin Charlier, Jean Feydy, Joan Alexis Glaun\u00e8s, Fran\u00e7ois-David Collin, and Ghislain Durif. Kernel operations on the gpu, with autodiff, without memory overflows. Journal of Machine Learning Research, 22(74):1\u20136, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Approximating the expectation of the softmax weighted sum 15 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Motivation . 15   \nA.2 Derivation . 16   \nA.3 Practical considerations 17   \nA.4 Number of effective points 18 ", "page_idx": 14}, {"type": "text", "text": "B Relationship to other acquisition strategies 18 ", "page_idx": 14}, {"type": "text", "text": "B.1 Relationship of BEEBO $T$ and UCB $\\kappa$ hyperparameters 18   \nB.2 GIBBON . . 19   \nB.3 Determinantal Point Processes . . 20   \nB.4 Local penalization . . . 21   \nB.5 RAHBO . 21   \nC.1 Acquisition function optimization . . 22   \nC.2 Benchmark BO methods . . . 22   \nC.3 Test problems . . 23   \nC.4 Heteroskedastic noise 23   \nExtended results 24   \nD.1 Results including additional baselines . . 24   \nD.2 Results for batch sizes 5 and 10 . . 26   \nD.3 Control problems . . . 28   \nD.4 Run time . . 28   \nD.5 Results with random initialization in round 0 . 29   \nD.6 BO curves for all experiments in Table 2 and Table A1 30 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Approximating the expectation of the softmax weighted sum ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Motivation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We are free to choose any energy function $\\tilde{E}$ in BEEBO, the only requirement being that we are able to compute an expectation of $\\bar{\\tilde{E}}$ in order to obtain the scalar summary $E=\\mathbb{E}[-\\tilde{E}(\\mathbf{\\bar{f}})]$ . Of particular interest is the softmax weighted sum, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tilde{E}(\\mathbf{f})=\\sum_{i=1}^{Q}\\mathrm{softmax}(\\beta\\mathbf{f})_{i}f_{i},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\beta$ is the softmax inverse temperature. The softmax weight vector $\\omega$ computed as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\omega_{i}=\\frac{\\exp(\\beta f_{i})}{\\sum_{j=1}^{Q}\\exp(\\beta f_{j})+\\exp(\\beta_{y}y_{m a x})}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\exp(\\beta_{y}y_{m a x})$ is an optional reference threshold value, as in expected improvement, which we set to 0 if not used ( $\\beta_{y}$ is either simply $\\beta$ or a dynamically scaled value that ensures $\\tilde{E}(\\mathbf{f})$ does ", "page_idx": 14}, {"type": "text", "text": "not become 0, see Equation A28). The parameter $\\beta$ allows us to interpolate between two extreme regimes, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle{\\tilde{E}}({\\bf f})=\\!\\frac{1}{Q}\\sum_{i=1}^{Q}f_{i}~~~~}}&{{~~~~~~~~~~~~~~~~~~~~~~~~~\\mathrm{for}\\beta\\rightarrow0}}\\\\ {{\\displaystyle{\\tilde{E}}({\\bf f})=\\!\\operatorname*{max}({\\bf f})~~~~~}}&{{~~~~~~~~~~~~~~~~~~~~~~~~\\mathrm{for}\\beta\\rightarrow\\infty}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In the first regime, all points of a batch equally contribute to the energy, whereas in the second regime only the single point \"responsible\" for the maximum is controlling the energy. Note that for numerical reasons, operating towards the $\\beta\\to\\infty$ limit is impractical, as it will lead to zero gradients for all but one point, preventing optimization. We can set $\\bar{\\beta}=A^{-1/2}$ , with $A$ being the prior uncertainty scale of the GP kernel. Since $A$ represents the expected energy fluctuations for points far from data, this weighting scheme will reflect a natural compromise between Equation A3 and Equation A4. ", "page_idx": 15}, {"type": "text", "text": "As opposed to the mean, in the general case, the expectation of the maximum of a $Q$ -dimensional multivariate normal is not available in closed form. To our best knowledge, this is also the case for the softmax weighted sum. In the following, we derive a closed-form approximation of the expectation of the softmax of $\\beta\\mathbf{f}$ that can be used for gradient-based optimization. ", "page_idx": 15}, {"type": "text", "text": "A.2 Derivation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Consider the softmax denominator ", "page_idx": 15}, {"type": "equation", "text": "$$\nd(\\mathbf{f})=\\sum_{j=1}^{Q}\\exp(\\beta f_{j})+\\exp(\\beta_{y}y_{\\operatorname*{max}}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We will Taylor expand $\\ln(d)$ to the second order, using ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial\\ln(d)}{\\partial f_{i}}=\\beta\\frac{1}{d}\\exp(\\beta f_{i})=\\beta\\omega_{i}}\\\\ &{\\displaystyle\\frac{\\partial^{2}\\ln(d)}{\\partial f_{i}\\partial f_{j}}=\\beta^{2}\\left(-\\frac{1}{d^{2}}\\exp(\\beta(f_{i}+f_{j})+\\frac{1}{d^{2}}\\delta_{i j}\\exp(\\beta f_{i}))\\right)}\\\\ &{\\quad\\quad\\quad=\\beta^{2}(\\omega_{i}\\delta_{i j}-\\omega_{i}\\omega_{j}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\delta_{i j}$ is the Kronecker delta. So ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\ln(d)\\approx\\ln(d_{\\mathbf{a}})+\\beta\\mathbf{w}^{T}\\cdot\\Delta\\mathbf{f}+\\frac{\\beta^{2}}{2}\\Delta\\mathbf{f}^{T}\\cdot W\\cdot\\Delta\\mathbf{f},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where a is the Taylor expansion point, $d_{\\mathbf{a}}$ is $d$ evaluated at a, $\\Delta\\mathbf{f}=\\mathbf{f}-\\mathbf{a}$ , w is the $Q$ -dimensional vector $\\omega$ evaluated at $\\mathbf{a}$ , and $W$ is the $Q\\times Q$ matrix $W=\\mathrm{diag}(\\mathbf{w})-\\mathbf{w}\\mathbf{w}^{T}$ . Inserted into Equation A2 we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\omega(\\mathbf{f})_{i}\\approx w_{i}*\\exp(\\beta\\Delta f_{i}-\\beta\\mathbf{w}^{T}\\cdot\\Delta\\mathbf{f}-\\frac{\\beta^{2}}{2}\\Delta\\mathbf{f}^{T}\\cdot W\\cdot\\Delta\\mathbf{f})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "With this approximation we can calculate the expectation value ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}[\\omega(\\mathbf{f})_{i}*f_{i}]=\\frac{w_{i}\\sqrt{\\operatorname*{det}(C(\\mathbf{x})^{-1})}}{(2\\pi)^{Q/2}}\\int\\exp(\\lambda_{i}(\\mathbf{f}))*f_{i}d f}}&{}&{\\mathrm{(A)}}\\\\ &{}&{\\lambda_{i}(\\mathbf{f})=\\beta\\Delta f_{i}-\\beta\\mathbf{w}^{T}\\cdot\\Delta\\mathbf{f}^{T}-\\frac{\\beta^{2}}{2}\\Delta\\mathbf{f}^{T}\\cdot W\\cdot\\Delta\\mathbf{f}-\\frac{1}{2}(\\mathbf{f}-\\pmb{\\mu})^{T}\\cdot C(\\mathbf{x})^{-1}\\cdot(\\mathbf{f}-\\pmb{\\mu})}\\\\ &{}&{\\mathrm{(A)}}\\\\ &{}&{=c^{(i)}-\\frac{1}{2}(\\mathbf{f}-\\pmb{\\nu}^{(i)})^{T}\\cdot C(\\mathbf{x})_{\\mathrm{sofmax}}^{-1}\\cdot(\\mathbf{f}-\\pmb{\\nu}^{(i)}),}&{\\mathrm{(A)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Where $\\boldsymbol{c}^{(i)},\\boldsymbol{\\nu}^{(i)}$ and $C(\\mathbf{x})_{\\mathrm{softmax}}$ are defined as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C(\\mathbf{x})_{\\mathrm{softmax}}=\\left(C(\\mathbf{x})^{-1}+\\beta^{2}W\\right)^{-1}}\\\\ &{\\qquad\\quad\\mathbf{b}^{(i)}=\\mathbf{e}^{(i)}-\\mathbf{w}}\\\\ &{\\qquad\\quad\\nu^{(i)}=C(\\mathbf{x})_{\\mathrm{softmax}}\\cdot\\left(\\beta\\mathbf{b}^{(i)}+C(\\mathbf{x})^{-1}\\cdot\\boldsymbol{\\mu}+\\beta^{2}W\\cdot\\mathbf{a}\\right)}\\\\ &{\\qquad\\quad c^{(i)}=\\frac{1}{2}(\\nu^{(i)})^{T}\\cdot C(\\mathbf{x})_{\\mathrm{softmax}}^{-1}\\cdot\\nu^{(i)}-\\beta(\\mathbf{b}^{(i)})^{T}\\cdot\\mathbf{a}}\\\\ &{\\qquad\\quad\\quad-\\frac{\\beta^{2}}{2}\\mathbf{a}^{T}\\cdot W\\cdot\\mathbf{a}-\\frac{1}{2}\\mu^{T}\\cdot C(\\mathbf{x})^{-1}\\cdot\\boldsymbol{\\mu}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and $\\mathbf{e}^{(i)}$ is the $i$ \u2019th basis vector with components $e_{j}^{(i)}=\\delta_{i j}$ . We can avoid the explict use of the precision matrix by rewriting the updated covariance matrix as ", "page_idx": 16}, {"type": "equation", "text": "$$\nC(\\mathbf{x})_{\\mathrm{softmax}}=\\bigl(C(\\mathbf{x})^{-1}\\cdot(I+\\beta^{2}C(\\mathbf{x})\\cdot W)\\bigr)^{-1}=U(\\mathbf{x})\\cdot C(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we have defined $U(\\mathbf{x})=\\left(I+\\beta^{2}C(\\mathbf{x})\\cdot W\\right)^{-1}$ . The updated mean vectors can conveniently be expressed as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{\\nu}^{(i)}=C(\\mathbf{x})_{\\mathrm{softmax}}\\cdot(\\beta\\mathbf{b}^{(i)}+C(\\mathbf{x})^{-1}\\cdot\\pmb{\\mu}+\\beta^{2}W\\cdot\\mathbf{a})}\\\\ &{\\pmb{\\nu}^{(i)}=\\beta C(\\mathbf{x})_{\\mathrm{softmax}}\\cdot\\mathbf{e}^{(i)}+C(\\mathbf{x})_{\\mathrm{softmax}}\\cdot(-\\beta\\mathbf{w}+C(\\mathbf{x})^{-1}\\cdot\\pmb{\\mu}+\\beta^{2}W\\cdot\\mathbf{a})}\\\\ &{\\pmb{\\nu}^{(i)}=\\beta C(\\mathbf{x})_{\\mathrm{softmax}}\\cdot\\mathbf{e}^{(i)}+\\pmb{\\nu}^{\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\nu^{\\prime}$ is a constant vector for all $(i)$ . Similarly ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{c^{(i)}=-\\beta a_{i}+\\displaystyle\\frac{1}{2}\\beta^{2}\\big(C(\\mathbf{x})_{\\mathrm{softmax}}\\big)_{i,i}+\\beta\\nu_{i}^{\\prime}+c^{\\prime}}\\\\ {c^{\\prime}=\\beta\\mathbf{w}^{T}\\cdot\\mathbf{a}+\\displaystyle\\frac{1}{2}\\nu^{\\prime T}\\cdot C(\\mathbf{x})_{\\mathrm{softmax}}^{-1}\\cdot\\boldsymbol{\\nu}^{\\prime}-\\displaystyle\\frac{\\beta^{2}}{2}\\mathbf{a}^{T}\\cdot W\\cdot\\mathbf{a}-\\displaystyle\\frac{1}{2}\\mu^{T}\\cdot C(\\mathbf{x})^{-1}\\cdot\\boldsymbol{\\mu}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $c^{\\prime}$ again being a constant for all $(i)$ . The expectation of the softmax weighted summary is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\tilde{E}]=K*\\sum_{i=1}^{Q}w_{i}*\\exp(c^{(i)})*\\nu_{i}^{(i)}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{K=\\frac{\\sqrt{\\operatorname*{det}(C(\\ensuremath{\\mathbf{x}})^{-1})}}{\\sqrt{\\operatorname*{det}(C(\\ensuremath{\\mathbf{x}})^{-1}+\\beta^{2}W)}}=\\sqrt{\\operatorname*{det}(U(\\ensuremath{\\mathbf{x}}))}.}\\end{array}$ . The most natural choice for the expansion point is $\\mathbf{a}=\\pmb{\\mu}$ in which case $\\nu^{(i)}$ and $\\boldsymbol{c}^{(i)}$ reduces to ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\pmb{\\nu}^{(i)}=\\beta C(\\mathbf{x})_{\\mathrm{softmax}}\\cdot\\left(\\mathbf{e}^{(i)}-\\mathbf{w}\\right)+\\pmb{\\mu}}\\\\ {c^{(i)}=\\frac{\\beta^{2}}{2}\\left(\\mathbf{e}^{(i)}-\\mathbf{w}\\right)^{T}\\cdot C(\\mathbf{x})_{\\mathrm{softmax}}\\cdot\\left(\\mathbf{e}^{(i)}-\\mathbf{w}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.3 Practical considerations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Linear algebra For numerical reasons, we avoid computing $C(\\mathbf{x})_{\\mathrm{softmax}}$ explicitly, and instead use the $U(\\mathbf{x})\\cdot C(\\mathbf{x})$ factorization to compute solutions with $U(\\bar{\\bf x})^{-1}=I+\\beta^{2}\\bar{C}({\\bf x})\\cdot\\bar{W}$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\nC(\\mathbf{x})_{\\mathrm{softmax}}\\cdot\\left(\\mathbf{e}^{(i)}-\\mathbf{w}\\right)=U(\\mathbf{x})\\cdot\\left(C(\\mathbf{x})\\cdot\\mathbf{e}^{(i)}-C(\\mathbf{x})\\cdot\\mathbf{w}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Following GPyTorch practices, we make use of the LinearOperator package to exploit the structure of $U(\\mathbf{x})^{-1}$ as an AddedDiagLinearOperator when solving. For determinants, we find that LinearOperator\u2019s logdet implementation gives nondeterministic results, and we therefore perform a dense cast before computing $K$ using default Pytorch. ", "page_idx": 16}, {"type": "text", "text": "While the factorization is numerically advantageous, it is still limited with regards to $\\beta$ . We find that at $\\beta>5$ , numerical errors prevent a reliable calculation of the expectation. In practice, $A^{-1/2}$ lies in a range that allows numerically accurate solutions. ", "page_idx": 16}, {"type": "text", "text": "Softmax When $y_{\\mathrm{{max}}}$ grows much larger than the softmax input vector f - a situation that can arise easily when initializing with random points for gradient-based optimization - the softmax weights $\\omega$ can become numerically zero for all \"real\" points, thus leading to $E(\\mathbf{x})=0$ , and vanishing gradients. As we always wish to preserve a minimal energy contribution from the real points, we parametrize the inverse temperature applied to $y_{\\mathrm{{max}}}$ , $\\beta_{y}$ , using a hyperparameter $\\alpha$ that denotes the minimal fraction of probability mass pertaining to real points. This parametrization resembles the LogEI version of the expected improvement acquisition function [58] to address the problem of vanishing EI-gradients. ", "page_idx": 17}, {"type": "text", "text": "Let $N$ denote the softmax denominator excluding $y_{\\mathrm{{max}}}$ , $\\begin{array}{r}{N=\\sum_{j=1}^{Q}\\exp(\\beta\\Delta f_{i})}\\end{array}$ . We define ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\exp(\\beta_{y}\\Delta y_{\\mathrm{max}})=\\operatorname*{min}\\left(\\frac{1-\\alpha}{\\alpha}N,\\exp(\\beta\\Delta y_{\\mathrm{max}})\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We used $\\alpha=0.05$ as a default in all our experiments. ", "page_idx": 17}, {"type": "text", "text": "A.4 Number of effective points ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We can interpret the softmax as the number of effective points contributing to the energy of the batch. The entropy $H$ of the softmax is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\nH(\\omega)=-\\sum_{i=1}^{Q}\\omega_{i}\\ln(\\omega_{i}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and the number of effective points, $D_{\\mathrm{eff}}$ , is $\\exp(H(\\omega))$ , so that ", "page_idx": 17}, {"type": "equation", "text": "$$\nD_{\\mathrm{eff}}=\\exp\\left(-\\sum_{i=1}^{Q}\\omega_{i}\\ln(\\omega_{i})\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "$D_{\\mathrm{eff}}$ is bounded by 1 (approaching the maximum) and $Q$ (approaching the mean). Note that if we include $y_{\\mathrm{{max}}}$ in the softmax denominator, we add $\\omega_{y_{\\mathrm{max}}}\\ln(\\omega_{y_{\\mathrm{max}}})$ to $H(\\omega)$ , and the resulting number becomes bounded by 1 and $Q+1$ . ", "page_idx": 17}, {"type": "text", "text": "B Relationship to other acquisition strategies ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the following section, we will discuss how BEEBO is related to UCB, GIBBON, Determinantal Point Processes (DPP), the Local Penalization heuristic and RAHBO. We will base our analysis on meanBEEBO, as the softmax-mediated interdependency of points in maxBEEBO prevents a simple interpretation of the objective in a single-point stepwise manner and does not allow for the same direct analogies to other strategies. ", "page_idx": 17}, {"type": "text", "text": "B.1 Relationship of BEEBO $T$ and UCB $\\kappa$ hyperparameters ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "BEEBO bears some resemblance to the UCB acquisition function, which in the single particle mode, $Q=1$ , reads ", "page_idx": 17}, {"type": "equation", "text": "$$\na_{\\mathrm{UCB}}(x)=\\mu(x)+\\sqrt{\\kappa}\\sqrt{C(x)},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the parameter $\\kappa$ controls the balance between exploitation and exploration and $\\mu(x)$ and $C(x)$ are respectively the mean and variance of the posterior distribution, $\\textstyle P(f\\,|\\,x,D)$ , as before. We note that $a_{\\mathrm{UCB}}$ does not account for the uncertainty of the measurement at $x$ , and therefore remains risk-neutral under heteroskedastic noise [11]. To understand the relationship between BEEBO and UCB, we will therefore limit ourselves to the homoskedastic case and furthermore assume that measurement variances $\\sigma^{2}$ are much smaller than the typical prior variance of the GP surrogate, $A$ , of $f$ , e.g. $A\\simeq N^{-1}T r(K)$ , so $\\sigma^{2}\\ll A$ and $M^{-1}=(\\dot{K}^{+}\\sigma^{2})_{~~~}^{-1}\\approx K^{-1}$ . In this limit, the variance of $f(x)$ after measurement (indexed at $i=n$ , say) reduces to $\\sigma^{2}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\nC(x)=\\left((K^{-1}+\\sigma^{-2}I)^{-1}\\right)_{n n}=\\left(K(K+\\sigma^{2}I)^{-1}\\sigma^{2}\\right)_{n n}\\approx\\sigma^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and the information gain becomes ", "page_idx": 18}, {"type": "equation", "text": "$$\nI(x)\\approx\\frac{1}{2}\\ln(C(x))-\\log(\\sigma).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Consequently, the gradient of the two acquisition functions reads ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\nabla a_{\\mathrm{UCB}}(\\boldsymbol{x})}&{=}&{\\displaystyle\\nabla\\mu(\\boldsymbol{x})+\\frac{\\sqrt{\\kappa}}{2\\cdot\\sqrt{C(\\boldsymbol{x})}}\\cdot\\nabla C(\\boldsymbol{x})}\\\\ &&{\\displaystyle\\nabla a_{\\mathrm{BEEBO}}(\\boldsymbol{x})}&{=}&{\\displaystyle\\nabla\\mu(\\boldsymbol{x})+\\frac{T}{2\\cdot C(\\boldsymbol{x})}\\cdot\\nabla C(\\boldsymbol{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The two gradients will be identical at points $x$ where the posterior uncertainties satisfy $\\textstyle{\\sqrt{C(x)}}={\\frac{T}{\\sqrt{\\kappa}}}$ . For comparison, we may desi\u221are equal gradients at iso-surfaces corresp\u221aonding to a given fraction, $\\nu$ , of the prior uncertainty scale $\\sqrt{A}$ , by setting $T$ accordingly as $T=\\nu\\cdot{\\sqrt{A}}\\cdot{\\sqrt{\\kappa}}$ . In our experiments, we use $\\begin{array}{r}{\\nu=\\frac12}\\end{array}$ and configure BEEBO using a dimensionless $T^{\\prime}$ explore-exploit parameter, defined as $\\begin{array}{r}{T^{\\prime}=\\frac{T}{\\sqrt{A}}}\\end{array}$ =\u221aTA, and set T \u2032 = 12 \u03ba for a given benchmark experiment. ", "page_idx": 18}, {"type": "text", "text": "B.2 GIBBON ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "GIBBON [42] approximates the (intractable) General-purpose max-value Entropy Search acquisition function, which quantifies the mutual information $M I(f_{\\mathrm{true}}^{*};\\mathbf{y}|D)$ of a batch of measurements $\\mathbf{y}$ and the unknown optimum $f_{\\mathrm{true}}^{\\ast}$ . It does so using a lower bound on the information gain and MC estimation of the expectation over $f_{\\mathrm{true}}^{\\ast}$ . It can be written as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\boldsymbol{\\alpha}_{\\mathrm{GIBBON}}(\\mathbf{x})=\\frac{1}{2}\\ln\\operatorname*{det}(R)-\\frac{1}{2\\left|\\mathcal{M}\\right|}\\sum_{m\\in\\mathcal{M}}\\sum_{i=1}^{Q}\\ln\\left(1-\\rho_{i}^{2}\\frac{\\phi\\left(\\gamma_{i}(m)\\right)}{\\phi\\left(\\gamma_{i}(m)\\right)}\\left[\\gamma_{i}(m)+\\frac{\\phi\\left(\\gamma_{i}(m)\\right)}{\\phi\\left(\\gamma_{i}(m)\\right)}\\right]\\right)}\\\\ &{\\boldsymbol{\\alpha}_{\\mathrm{GIBBON}}(\\mathbf{x})=\\frac{1}{2}\\ln\\operatorname*{det}(R)+\\displaystyle\\sum_{i=1}^{Q}\\hat{\\alpha}_{\\mathrm{GIBBON}}(\\boldsymbol{x}_{i}),}&{(\\mathrm{A}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where R is the correlation matrix with entries Rij = \u221a $\\begin{array}{r}{R_{i j}\\,=\\,\\frac{C(\\mathbf{x})_{i j}}{\\sqrt{C(\\mathbf{x})_{i i}C(\\mathbf{x})_{j j}}}}\\end{array}$ , $\\mathcal{M}$ is a set of samples for the max-value $f_{\\mathrm{true}}^{\\ast}$ , and $\\rho_{i}$ is the correlation of $y_{i}$ and $f_{\\mathrm{true}}(x_{i})$ . $\\phi$ and $\\phi$ are the standard normal cumulative distribution and probability density functions, and $\\textstyle\\gamma_{i}(m)={\\frac{m-\\mu}{\\sigma}}$ . ", "page_idx": 18}, {"type": "text", "text": "The definition of BEEBO introduced in Equation 7, with the scalar summarization function set to the expected mean, $\\begin{array}{r}{E(\\mathbf{x})=-\\frac{1}{Q}\\sum_{i=1}^{Q}\\mu(x_{i})}\\end{array}$ , gives ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\alpha_{\\mathrm{BEEBO}}(\\mathbf{x})=T*\\frac{1}{2}\\left(\\ln\\operatorname*{det}\\left(C(\\mathbf{x})\\right)-\\ln\\operatorname*{det}\\left(C_{\\mathrm{aug}}(\\mathbf{x})\\right)\\right)+\\sum_{i=1}^{Q}\\mu(x_{i}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "From the second formulation of GIBBON, it becomes obvious that although being distinct in their motivation and derivation, BEEBO and GIBBON implement acquisition functions with a similar structure. Taking an information theoretic and multi-fidelity BO standpoint, GIBBON refers to this trade-off as diversity against quality, whereas in BEEBO we follow the intuitions of UCB, and use exploration and exploitation. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Quality - Exploitation: GIBBON employs an MC estimate of the lower bound approximation of the information gain provided by each point, whereas BEEBO directly summarizes the optimality of all points in closed form, either as their mean or an approximated softmax weighted sum.   \n\u2022 Diversity - Exploration: In GIBBON, the diversity derived from the differential entropy $H(\\mathbf{f}\\vert D,\\mathbf{\\hat{x}})$ is the entropy of the posterior correlation ${\\textstyle\\frac{1}{2}}\\ln\\operatorname*{det}(R)$ . In BEEBO, we employ the reduction of entropy, the information gain $I(\\mathbf{\\tilde{x}})$ . Under homoskedastic noise, $I(\\mathbf{x})\\propto\\ln\\operatorname*{det}(C(\\mathbf{x}))$ . Since $R(\\mathbf{x})=\\mathrm{diag}(C(\\mathbf{x}))^{-1/2}\\cdot C(\\mathbf{x})\\cdot\\mathrm{diag}(C(\\mathbf{x}))^{-1/2}$ , we have that $\\begin{array}{r}{\\ln\\operatorname*{det}(R)=\\ln\\operatorname*{det}(C(\\mathbf{x}))-\\sum_{i}^{Q}\\ln(C(\\mathbf{x})_{i i})}\\end{array}$ . Therefore, maximizing the log determinant of $R$ penalizes points that have high variance. ", "page_idx": 18}, {"type": "text", "text": "Therefore, while GIBBON presents an attractive approximation of max-value Entropy Search for batched acquisition, BEEBO is an alternative that avoids approximating a quality criterion using MC. Moreover, GIBBON\u2019s diversity criterion implicitly penalizes points that have high variance, whereas BEEBO\u2019s criterion maximizes the reduction of variance. We find that BEEBO is orders of magnitudes faster to compute than GIBBON (Figure A3). ", "page_idx": 19}, {"type": "text", "text": "In the context of large batches $Q>>10)$ ), a modification of GIBBON exists that is further similar to BEEBO. Departing from the strict max-value entropy search derivation, a scaling factor $Q^{-2}$ is introduced to counteract a growing dominance of the diversity term: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\alpha_{\\mathrm{scaledGIBBON}}(\\mathbf{x})=\\frac{1}{2Q^{2}}*\\ln\\operatorname*{det}(R)+\\sum_{i=1}^{Q}\\hat{\\alpha}_{\\mathrm{MES}}(x_{i}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This scaling is motivated by the fact that $R$ contains $Q^{2}$ elements. However, we note that $R$ is summarized by its log determinant, which scales linearly in $Q$ : As the determinant is the product of the eigenvalues, the log determinant is the sum of the log-eigenvalues. The number of eigenvalues scales linearly with matrix size $Q$ , and so does the log determinant. ", "page_idx": 19}, {"type": "text", "text": "B.3 Determinantal Point Processes ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "A Determinantal Point Process [43] specifies a probability over a set of points, or a \"configuration of points\" drawn from a ground set. Specifically, the probability of a set of $Q$ points $\\mathbf{x}$ is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\nP(\\mathbf{x})\\propto\\operatorname*{det}\\left(L_{\\mathbf{x}}\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $L_{\\mathbf{x}}$ is a $Q\\times Q$ symmetric matrix. Kulesza et al. [43] provide a decomposition of the general DPP kernel $L$ that makes quality and diversity components explicit, so that ", "page_idx": 19}, {"type": "equation", "text": "$$\nL_{i j}=q(x_{i})q(x_{j})k(x_{i},x_{j}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with $k$ being a $\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{+}$ similarity kernel, and $q$ being a unary $\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ scalar quality function. This framework is naturally amenable to batch BO, as we seek to select a collection of points that trade off quality (optimality) and diversity. Note that both $k$ and $q$ are distinct functions that need to be specified by the user, leading to the practical complication that they must be chosen very carefully so that their scales do not dominate each other, which limits the utility of this decomposition in practice [42]. ", "page_idx": 19}, {"type": "text", "text": "In the following, we show how BEEBO is equivalent to a DPP, and derive the necessary $k$ and $q$ . Again, we consider BEEBO ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\alpha_{\\mathrm{BEEBO}}(\\mathbf{x})=-E(\\mathbf{x})+T*I(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with the scalar summarization function set to $\\begin{array}{r}{E(\\mathbf{x})\\,=\\,-\\sum_{i=1}^{Q}f(x_{i})}\\end{array}$ . We will first focus on the information gain term $I({\\bf x})$ , which we can rearrange as ", "page_idx": 19}, {"type": "equation", "text": "$$\nI(\\mathbf{x})=\\frac{1}{2}\\ln\\operatorname*{det}(C(\\mathbf{x}))-\\frac{1}{2}\\ln\\operatorname*{det}(C_{\\mathrm{aug}}(\\mathbf{x}))=\\frac{1}{2}\\ln\\operatorname*{det}\\left(C(\\mathbf{x})\\cdot C_{\\mathrm{aug}}^{-1}(\\mathbf{x})\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Our similarity kernel $k$ is therefore given by the entries of the matrix $S=C(\\mathbf{x})\\cdot C_{\\mathrm{aug}}(\\mathbf{x})^{-1}$ , so that $k(x_{i},x_{j})=S_{i j}$ . Note that due to the augmented covariance term, the implied $k$ also depends on all other currently selected points in $\\mathbf{x}$ , and $L_{\\mathbf{x}}$ is not a submatrix of an all-sample $L$ . Therefore, BEEBO does not implement a DPP under heteroskedastic noise. However, if we only consider homoskedastic noise, BEEBO\u2019s $I({\\bf x})$ simplifies to the posterior entropy [63], and therefore $S=C(\\mathbf{x})$ . As $C(\\mathbf{x})$ can be accessed as a submatrix of an all-sample $C$ , this permits a DPP. ", "page_idx": 19}, {"type": "text", "text": "Given the choice of $E(\\mathbf{f})$ , we can rewrite BEEBO as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{\\mathrm{OBEBO}}(\\mathbf{x})=\\ln\\operatorname*{det}\\left({S}\\right)+T*\\frac{1}{2}+\\sum_{i=1}^{Q}{\\mu_{i}}}\\\\ &{\\quad_{\\mathrm{IREBO}}(\\mathbf{x})=\\ln\\operatorname*{det}\\left({S}\\right)+\\sum_{i=1}^{Q}{\\frac{2}{T}}*{\\mu_{i}}}\\\\ &{\\quad_{\\mathrm{II}}^{\\mathrm{2}}*\\alpha_{\\mathrm{BEBO}}(\\mathbf{x})=\\ln\\operatorname*{det}\\left({S}\\right)+\\ln\\operatorname*{det}\\left({D}\\right))}\\\\ &{\\quad_{\\mathrm{II}}^{\\mathrm{2}}*\\alpha_{\\mathrm{BEBO}}(\\mathbf{x})=\\ln\\operatorname*{det}\\left({D}^{\\mathrm{1}}\\cdot{S}\\cdot{D}^{\\mathrm{1}}\\right)}\\\\ &{\\quad_{\\mathrm{IREBO}}(\\mathbf{x})=\\ln\\operatorname*{det}\\left({D}^{\\mathrm{1}}\\cdot{S}\\cdot{D}^{\\mathrm{1}}\\right)*{T}*\\frac{1}{2}}\\\\ &{\\quad_{\\mathrm{OBEBO}}(\\mathbf{x})=\\ln\\operatorname*{det}\\left({D}^{\\mathrm{1}}\\cdot{S}\\cdot{D}^{\\mathrm{1}}\\right)*{T}*\\frac{1}{2}}\\\\ &{\\quad_{\\mathrm{OBEBO}}(\\mathbf{x})=\\ln\\operatorname*{det}\\left({L}\\right)*{T}*\\frac{1}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\nD_{i i}^{\\frac{1}{2}}=\\sqrt{\\exp(\\frac{2}{T}\\mu_{i})}=\\exp(\\frac{1}{T}\\mu_{i})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $L$ is a matrix with entries $\\begin{array}{r}{L_{i j}=S_{i j}\\exp(\\frac{1}{T}*\\mu_{i})\\exp(\\frac{1}{T}*\\mu_{j})}\\end{array}$ . BEEBO therefore uses the DPP quality function $\\begin{array}{r}{q(x_{i})=\\exp(\\frac{1}{T}*\\mu_{i}).}\\end{array}$ , and, like proven previously for GIBBON, a batch $\\mathbf{x}$ with maximal $\\alpha$ BEEBO corresponds to the MAP of a DPP. ", "page_idx": 20}, {"type": "text", "text": "B.4 Local penalization ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Local penalization (LP) is a greedy batch selection strategy that given any arbitrary single-point acquisition function, ensures diversity by applying a penalization function $\\psi(x,x^{\\prime})$ that downweights the acquisition value of candidate locations $x$ based on their proximity to already selected points. The criterion for selecting $x_{i}$ is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\nx_{i}=\\arg\\operatorname*{max}\\alpha(x)\\prod_{j=1}^{i-1}\\psi(x,x_{j}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that in this formulation, the product includes all previously selected points, not just the current batch. The penalization function $\\psi$ may in principle be chosen freely. Gonzalez et al. [35] propose exploiting the fact that $f_{\\mathrm{true}}$ is Lipschitz continuous in order to bound the position of the unknown optimum and penalize accordingly. The Lipschitz constant $L$ is inferred from the GP surrogate and used to parametrize $\\psi$ . In LP, acquisition function optimization proceeds iteratively. After an $x_{i}$ is chosen, the corresponding penalizing multiplier is added to the objective before optimizing for the next xi+1. ", "page_idx": 20}, {"type": "text", "text": "While BEEBO enables optimization to proceed in parallel, it is of course possible to also optimize BEEBO greedily (under homoskedastic noise, $I$ is submodular). In this case, it implements an LP strategy where $\\dot{\\alpha(x)}=\\mu(x)$ . Rather than a product of individual $\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ function evaluations, the penalizer implied by BEEBO is the information gain $I(\\mathbf{x}):\\mathbb{R}^{i\\times d}\\rightarrow\\mathbb{R}$ that we evaluate by concatenating a candidate point to the already acquired $x$ at each iteration. Like in GIBBON, this constitutes an LP strategy that does not require estimation of any properties of $f_{\\mathrm{true}}$ beyond learning the GP surrogate. ", "page_idx": 20}, {"type": "text", "text": "B.5 RAHBO", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Risk-averse Heteroskedastic Bayesian Optimization (RAHBO) [11] is a UCB-derived single-point acquisition function that avoids heteroskedastic risk, preferentially selecting points with low noise. While it is not applicable to batched acquisition directly, we here compare it to single-sample BEEBO to highlight different ways of addressing noise. Given a heteroskedastic surrogate model that learns an additional GP for the noise, the variance proxy, RAHBO reads ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\alpha_{\\mathrm{RAHBO}}(x)=}&{{}\\quad}&{\\mathrm{UCB}_{f}(x)\\!-\\!\\alpha*\\mathrm{LCB}_{\\mathrm{var}}(x)}\\\\ {\\alpha_{\\mathrm{RAHBO}}(x)=}&{{}\\quad}&{\\mu_{f}(x)+\\beta_{f}*\\sigma_{f}(x)\\!-\\!\\alpha(\\mu_{\\mathrm{var}}(x)-\\beta_{\\mathrm{var}}*\\sigma_{\\mathrm{var}}(x))}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\mu_{f}$ and $\\sigma_{f}$ are the posterior mean and variance of the surrogate model and $\\beta_{f}$ is the standard UCB trade-off hyperparameter, yielding the standard upper confidence bound $\\mathrm{UCB}_{f}$ . $\\alpha$ is the chosen risk tolerance, and LCB is the lower confidence bound of the variance GP with posterior mean $\\mu_{\\mathrm{var}}$ and variance $\\sigma_{\\mathrm{var}}$ traded off using $\\beta_{\\mathrm{var}}$ . ", "page_idx": 21}, {"type": "text", "text": "At $Q=1$ , BEEBO can be expressed as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{\\mathrm{BEEBO}}=\\mu_{f}(x)+T*\\frac{1}{2}\\ln(\\sigma_{f}(x))-T*\\frac{1}{2}\\ln(\\sigma_{f}^{\\mathrm{aug}}(x))}\\\\ &{\\alpha_{\\mathrm{BEEBO}}=\\mu_{f}(x)+T*\\frac{1}{2}\\ln\\left(\\frac{\\sigma_{f}(x)}{\\sigma_{f}^{\\mathrm{aug}}(x)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the variance proxy at $x$ is considered via the augmented posterior variance $\\sigma_{f}^{\\mathrm{aug}}$ ", "page_idx": 21}, {"type": "text", "text": "While RAHBO penalizes risk on an absolute scale, subject to $\\alpha$ , BEEBO optimizes for high uncertainty reduction, quantified as the log ratio of the variance before and after making measurements. ", "page_idx": 21}, {"type": "text", "text": "Moreover, RAHBO differentiates between known and unknown variance proxies, and uses the $\\mathrm{LCB}_{\\mathrm{var}}$ term to discount the predicted variance according to its uncertainty. In its closed-form analytical expression, BEEBO does not permit for the uncertainty of the variance proxy to be taken into account, being more similar to the known variance RAHBO ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\alpha_{\\mathrm{RAHBO}}(x)=\\mu_{f}(x)+\\beta_{f}*\\sigma_{f}(x)-\\alpha\\mu_{\\mathrm{var}}(x)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\mu_{\\mathrm{var}}$ is a noise-free proxy. Either a sampling-based approach, or approximations to $I(x)$ would need to be introduced to handle variance proxy uncertainty in BEEBO. ", "page_idx": 21}, {"type": "text", "text": "C Implementation details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "C.1 Acquisition function optimization ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "BEEBO was implemented for full compatibility with the BoTorch framework (version 0.9.4) [52] as an AnalyticAcquistionFunction. Standard BoTorch utilities for initializing and training GPs, initializing $q$ -batches and performing gradient descent optimization of the acquisition function are used. We trained GPyTorch (version 1.11) [57] GP models with KeOps [64] Mat\u00e9rn 5/2 kernels (following BoTorch defaults with a separate length scale for each input dimension, and Gamma priors on the length and output scales). Log determinants for the information gain were computed using singular value decomposition for numerical stability. ", "page_idx": 21}, {"type": "text", "text": "GPyTorch provides a get_fantasy_model method that allows for the efficient augmentation of the training data of a GP with a set of points, as done in BEEBO. However, we observed that GPyTorch\u2019s implementation suffers from GPU memory leaks when used with automatic differentiation enabled. We therefore instantiate augmented models explicitly, not making use of the (more efficient) augmentation strategy. ", "page_idx": 21}, {"type": "text", "text": "All experiments were performed with double precision. SobolQMCNormalSampler was used for acquisition functions making use of the reparametrization trick. Experiments were run on individual Nvidia RTX 6000 and V100 GPUs. Five replicates for the benchmarking experiments required a total of approx. 5,000 RTX 6000 GPU hours, with the majority of the run time dedicated to the GIBBON baseline, rather than BEEBO itself (Figure A3, Table A9). ", "page_idx": 21}, {"type": "text", "text": "C.2 Benchmark BO methods ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "All methods were benchmarked in BoTorch. For $q$ -EI, we used LogEI [58]. For TS, 10.000 base Sobol samples were drawn and sampled with MaxPosteriorSampling using the Cholesky decomposition of the covariance matrix. GIBBON was optimized using sequential optimization following the BoTorch tutorial. We additionally implemented a custom version of GIBBON that applies the $\\overline{{Q}}^{-2}$ scaling factor to the diversity term, as proposed in GIBBON\u2019s supplementary material. We used 100,000 random discretized candidates for max-value sampling. In a few iterations, optimizing GIBBON seemed challenging, with BoTorch reporting that no nonzero initialization candidate could be identified. KB was optimized using a custom greedy optimization loop with fantasized observations, using (single-sample) LogEI as the underlying acquisition function. TuRBO-1 was optimized following its BoTorch tutorial. None of the methods use a hyperparameter for controlling their explore-exploit trade-off. The results are therefore based on 10 iterations at defaults. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "C.3 Test problems ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Test functions All test functions were used in their BoTorch implementations. As done in previous work, the embedded Hartmann function was created by appending all-0 dummy dimensions to the original six dimensions [53, 54, 50]. ", "page_idx": 22}, {"type": "text", "text": "Control problems We consider two control problems from previous work: A 14-dimensional parameter tuning task for controlling robot arms pushing two objects to a target location [55], and a 60-dimensional trajectory planning task for a rover navigating through a maze of obstacles [56]. Instead of converting the problem objectives into rewards as in the original work, we operate on the actual minimization objectives directly (distance to target, navigation loss), and follow BoTorch\u2019s approach of simply inverting the objective in order to yield maximization problems. Both problems were adapted from their available implementations in Wang et al. [56] to follow the BoTorch test problem API. ", "page_idx": 22}, {"type": "text", "text": "C.4 Heteroskedastic noise ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The (inverted) Branin function has three global optima $\\begin{array}{r c l}{f(x^{*})}&{=}&{-0.397887}\\end{array}$ at $\\boldsymbol{x}_{1}^{*}=$ $(9.42478,2.475),x_{2}^{*}\\,=\\,(-\\pi,12.275)$ and $x_{3}^{*}\\,=\\,(\\pi,2.275)$ . We define heteroskedastic noise so that the variance is maximal at $x_{2}^{*}$ and ${x}_{3}^{*}$ . The noise decays exponentially with the distance from any of the two noised optima at a rate $\\lambda$ . ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sigma^{2}(x)=\\sigma_{m a x}^{2}*\\exp(-\\lambda*\\operatorname*{min}(\\|x-x_{2}^{*}\\|_{2},\\|x-x_{3}^{*}\\|_{2})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For our experiments, we set $\\sigma_{m a x}^{2}\\,=\\,100$ and $\\lambda\\:=\\:0.05$ . As the surrogate function, we use a HeteroskedasticSingleTaskGP provided in BoTorch. This model learns two GPs simultaneously, one for the function $f(x)$ and one for the (also unknown) variance function $\\sigma^{2}(x)$ . When querying the oracle with a batch of points, noised observations of $f(x)$ are provided together with the true $\\sigma^{\\frac{\\smile}{2}}$ at each point. The homoskedastic control experiment uses a SingleTaskGP with inferred noise level. The homoskedastic noise is set to $\\sigma^{2}=77.\\bar{5}$ , which is the average noise level of the heteroskedastic function over the whole domain. ", "page_idx": 22}, {"type": "image", "img_path": "wQiJNyPENt/tmp/20f362dfb7e8be811249f1a4f4106bfc800e543470f5f5698f70100a952d31b4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure A1: The Branin function with added heteroskedastic noise following Equation A45. $\\sigma_{m a x}^{2}=$   \n100, $\\lambda=0.05$ . ", "page_idx": 23}, {"type": "text", "text": "D Extended results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "D.1 Results including additional baselines ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Table A1: BO on noise-free synthetic test problems. The normalized highest observed value after 10 rounds of BO with $q{=}100$ is shown. Colors are normalized row-wise. The BEE-BO and $q$ -UCB columns are equivalent to Table 2. Higher means better. Results are means over five replicate runs. ", "page_idx": 23}, {"type": "image", "img_path": "wQiJNyPENt/tmp/0083ae07b67e2eaee82b0ec654d0d7366dab30ddafcf30b13032d371cb7f09a4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table A2: BO on noise-free synthetic test problems. The relative batch instantaneous regret of the last, exploitative batch is shown. Colors are normalized row-wise. The BEEBO and $q$ -UCB columns are equivalent to Table 3. Lower means better. Results are means over five replicate runs. ", "page_idx": 24}, {"type": "image", "img_path": "wQiJNyPENt/tmp/4100a05399dd034bc87790e3187a305d04a2206abb283bfacd091106daff23a5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table A3: Paired t-test p-values for the results of meanBEEBO in Table 2. The combined p-value was computed using Fisher\u2019s method. P-values smaller than 0.05 are indicated in bold. ", "page_idx": 24}, {"type": "image", "img_path": "wQiJNyPENt/tmp/d875b45ad78cca9966dfbbdf4f828f9168162a07c2b90557baf274afa6a60865.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "wQiJNyPENt/tmp/88d1ebd7e958661a2c03100dcbc9c2979f7f9caa039bd87149f74a6c72e0c425.jpg", "table_caption": ["Table A4: Paired t-test p-values for the results of maxBEEBO in Table 2. The combined p-value was computed using Fisher\u2019s method. P-values smaller than 0.05 are indicated in bold. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "D.2 Results for batch sizes 5 and 10 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Table A5: BO on noise-free synthetic test problems. The normalized highest observed value after 10 rounds of BO with $q{=}5$ is shown. Colors are normalized row-wise. Higher means better. Results are means over ten replicate runs. ", "page_idx": 25}, {"type": "image", "img_path": "wQiJNyPENt/tmp/adeb7951eb55086f834dd00e49281bdf21cb3d7f74652b5533f11e7b6aeec0e8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table A6: BO on noise-free synthetic test problems. The normalized highest observed value after 10 rounds of BO with $q{=}10$ is shown. Colors are normalized row-wise. Higher means better. Results are means over ten replicate runs. ", "page_idx": 25}, {"type": "image", "img_path": "wQiJNyPENt/tmp/a893740d69d28fcfe68b87ca04229af82ece260b05c68b921535098a838e5d6b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table A7: BO on noise-free synthetic test problems. The relative batch instantaneous regret of the last, exploitative batch with $q{=}5$ is shown. Colors are normalized row-wise. Lower means better. Results are means over ten replicate runs. ", "page_idx": 26}, {"type": "image", "img_path": "wQiJNyPENt/tmp/ce0bc84a937880c9d26636278da6a4d33a741b25652ff29be7b7ea5b43d21282.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Table A8: BO on noise-free synthetic test problems. The relative batch instantaneous regret of the last, exploitative batch with $\\scriptstyle q=10$ is shown. Colors are normalized row-wise. Lower means better. Results are means over ten replicate runs. ", "page_idx": 26}, {"type": "image", "img_path": "wQiJNyPENt/tmp/edc6ec4a6368cd4a5c1b5ac32b659c5bbb9c254c35cb5298c28cf9a11c882791.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "D.3 Control problems ", "page_idx": 27}, {"type": "image", "img_path": "wQiJNyPENt/tmp/9ea55a9d3ae0bd034d39a937737c2ea11e072d9d1ed7b24dfaea5a7b995e5420.jpg", "img_caption": ["Figure A2: Experiments on the 14D robot arm pushing and 60D rover trajectory planning control problems. 10 replicates each. GIBBON (s) refers to the scaled larged-batch variant of GIBBON. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "D.4 Run time ", "page_idx": 27}, {"type": "image", "img_path": "wQiJNyPENt/tmp/6c30b08fcb74789201a36f0558a897be65d0f53116ee0ee98aeb7dc902ef2d95.jpg", "img_caption": ["Figure A3: Example run times for the 10-round BO experiment on the 6D Hartmann problem with $Q{=}100$ . Error bars are over 5 replicate runs. Run times vary depending on the test problem, with GIBBON appearing especially sensitive, becoming e.g. $10\\mathrm{x}$ slower on the 50D Ackley problem. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table A9: Total run times for five replicates of the experiments presented in Table A1. We sum over all test problems. ", "page_idx": 28}, {"type": "table", "img_path": "wQiJNyPENt/tmp/bc6b7733df0576907a9f020b2a0493f8a448826845977bb22328b875f263e1dd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "D.5 Results with random initialization in round 0 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Table A10: BO with random initialization on noise-free synthetic test problems. The normalized highest observed value after 10 rounds of BO with $q{=}100$ is shown. Colors are normalized row-wise. Higher means better. Results are means over five replicate runs. ", "page_idx": 28}, {"type": "image", "img_path": "wQiJNyPENt/tmp/d3a7219a1fa81adf16aca12c7c600e0ff5509ad1cd6d428f9f84254b12d21cb7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table A11: BO with random initialization on noise-free synthetic test problems. The relative batch instantaneous regret of the last, exploitative batch is shown. Colors are normalized row-wise. Lower means better. Results are means over five replicate runs. ", "page_idx": 29}, {"type": "image", "img_path": "wQiJNyPENt/tmp/19d32d4375c07a3a301f2546d4ddb2591dd3052017f70f962eef5a1e2641c932.jpg", "img_caption": ["D.6 BO curves for all experiments in Table 2 and Table A1 "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "wQiJNyPENt/tmp/fc74591c7ac9453658450460023d648a94445644116fed7c089fbaacc5761fd2.jpg", "img_caption": ["Figur\u221ae A4: Experiments on the Shekel, Hartmann, Cosine and embedded Hartmann test functions with $\\sqrt{\\kappa}=0.1$ for BEEBO and $q$ -UCB. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "wQiJNyPENt/tmp/2f57b5385699fb7bc47ab2fc51e21d31f1143967946b130b4bc4e3fbef623455.jpg", "img_caption": ["Figur\u221ae A5: Experiments on the Shekel, Hartmann, Cosine and embedded Hartmann test functions with $\\sqrt{\\kappa}=1.0$ for BEEBO and $q$ -UCB. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "wQiJNyPENt/tmp/7061f571b881422f1dca746074dc5c4f501bbb59cd43f9d250421f1f987996fc.jpg", "img_caption": ["Figur\u221ae A6: Experiments on the Shekel, Hartmann, Cosine and embedded Hartmann test functions with $\\sqrt{\\kappa}=10\\bar{.}0$ for BEEBO and $q$ -UCB. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "wQiJNyPENt/tmp/77a9710dec638b88f433376e1cfefde504b4524aac69112a5125582b70e38005.jpg", "img_caption": ["Figure A7: Experiments on the Ackley test function with $\\sqrt{\\kappa}=0.1$ for BEEBO and $\\scriptstyle q-\\mathrm{UCB}$ . "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "wQiJNyPENt/tmp/5024b482c9d4e35054f080a6c1799de6e90b354f2f7f249f27845661f189a5a8.jpg", "img_caption": ["Figure A8: Experiments on the Ackley test function with $\\sqrt{\\kappa}=1.0$ for BEEBO and $\\scriptstyle q-\\mathrm{UCB}$ . "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "wQiJNyPENt/tmp/e6dcc899be5c00628f52ee08c23336287aa91a1d40d964b10204022331232f95.jpg", "img_caption": ["Figure A9: Experiments on the Ackley test function with $\\sqrt{\\kappa}=10.0$ for BEEBO and $\\scriptstyle q-\\mathrm{UCB}$ . "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "wQiJNyPENt/tmp/045bf4ca2d8b139f77297c3c6bede19fa8634300918a9cf5342ea821ad3f9d30.jpg", "img_caption": ["Figure A10: Experiments on the Levy test function with $\\sqrt{\\kappa}=0.1$ for BEEBO and $\\scriptstyle q-\\mathrm{UCB}$ . "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "wQiJNyPENt/tmp/7174f511af6a8a6dab09f66f2cbde8416f71b466adac7ba67a33d9fc6995cb5f.jpg", "img_caption": ["Figure A11: Experiments on the Levy test function with $\\sqrt{\\kappa}=1.0$ for BEEBO and $\\scriptstyle q-\\mathrm{UCB}$ . "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "wQiJNyPENt/tmp/5ceb37d829c11636a82b22ab9d0396769b698c135ceb3d5a79bf5784dfb2615c.jpg", "img_caption": ["Figure A12: Experiments on the Levy test function with $\\sqrt{\\kappa}=10.0$ for BEEBO and $\\scriptstyle q-\\mathrm{UCB}$ . "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "wQiJNyPENt/tmp/780f8015d161e03f9c30115d098daed839be285e95062f886b3ec10d5e0b7096.jpg", "img_caption": ["Figure A13: Experiments on the Rastrigin test function with $\\sqrt{\\kappa}=0.1$ for BEEBO and $q$ -UCB. "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "wQiJNyPENt/tmp/1b046601c5fc6db946fa5fe72eadc7d2b79a4ce3353b7cb45765d34e752d6632.jpg", "img_caption": ["Figure A14: Experiments on the Rastrigin test function with $\\sqrt{\\kappa}=1.0$ for BEEBO and $q$ -UCB. "], "img_footnote": [], "page_idx": 38}, {"type": "image", "img_path": "wQiJNyPENt/tmp/cefac154657a99a248cc7d2b1ee17e252fbf8d021281752efefe7082538c6f04.jpg", "img_caption": ["Figure A15: Experiments on the Rastrigin test function with $\\sqrt{\\kappa}=10.0$ for BEEBO and $q$ -UCB. "], "img_footnote": [], "page_idx": 39}, {"type": "image", "img_path": "wQiJNyPENt/tmp/376007111c47fa072d6ef5b8ae291d9ac0ace4e1737e78932d8ae6c2a14ba797.jpg", "img_caption": ["Figure A16: Experiments on the Rosenbrock test function with $\\sqrt{\\kappa}=0.1$ for BEEBO and $q$ -UCB. "], "img_footnote": [], "page_idx": 40}, {"type": "image", "img_path": "wQiJNyPENt/tmp/b796cd0d53696985fe1b1767dffc92efaa360c1b9896cf724a751f14da3a99c6.jpg", "img_caption": ["Figure A17: Experiments on the Rosenbrock test function with $\\sqrt{\\kappa}=1.0$ for BEEBO and $q$ -UCB. "], "img_footnote": [], "page_idx": 41}, {"type": "image", "img_path": "wQiJNyPENt/tmp/528ff3b3470402e19f7f049eab4c929a907db2747822185d066c4037929373cf.jpg", "img_caption": ["Figure A18: Experiments on the Rosenbrock test function with $\\sqrt{\\kappa}=10.0$ for BEEBO and $q_{\\mathrm{~\\,~}}$ -UCB. "], "img_footnote": [], "page_idx": 42}, {"type": "image", "img_path": "wQiJNyPENt/tmp/2e2d4c70d026f1856f8acd9f1271ece44188a7e92ae532ff4ac434a86d27a730.jpg", "img_caption": ["Figure A19: Experiments on the Powell test function with $\\sqrt{\\kappa}=0.1$ for BEEBO and $q$ -UCB. "], "img_footnote": [], "page_idx": 43}, {"type": "image", "img_path": "wQiJNyPENt/tmp/b3b9c685e5e328b1300cc31625ba90d2be0b4f24014853c9fb49affe4132ed8b.jpg", "img_caption": ["Figure A20: Experiments on the Powell test function with $\\sqrt{\\kappa}=1.0$ for BEEBO and $q_{\\mathrm{{'}}}$ -UCB. "], "img_footnote": [], "page_idx": 43}, {"type": "image", "img_path": "wQiJNyPENt/tmp/eaf4a2fc694b1c0f121e66af1a87979de6b9d210eba49a26d33b71b6913bc963.jpg", "img_caption": ["Figure A21: Experiments on the Powell test function with $\\sqrt{\\kappa}=10.0$ for BEEBO and $q$ -UCB. "], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: As claimed, we experimentally demonstrate a) the controllability of the acquisition strategy, b) competitive performance on 33 test problems compared to $q$ -UCB, $\\scriptstyle q-\\operatorname{EI}$ , Thompson sampling, GIBBON, TuRBO and Kriging Believer, and c) behaviour under heteroskedastic noise. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 45}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Justification: We address limitations in our discussion section, highlighting computational complexity constraints in exact GP inference as well as challenges under heteroskedastic noise. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 45}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper does not make use of any theoretical results. All reported results are based on empirical experiments. All underlying assumptions are standard in research on BO with GPs. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 46}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: As described in the methods section, we use standard BoTorch and GPyTorch utilities for all our experiments, and provide extended details on the technical implementation in the supplementary section. Our repository includes the full benchmarking setup with appropriate run scripts and instructions. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 46}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 47}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: The repository includes the implementation of the proposed method as well as the benchmarking setup with alternative methods. No additional data is required for reproduction. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 47}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: We follow GPyTorch and BoTorch for all hyperparameters pertaining to GPs, and describe this accordingly. Our appendix includes additional details on method hyperparameters to ensure reproducibility. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 47}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We include full BO curves with standard deviations over five replicates for all quantitative experiments in the appendix. These detailed curves are referenced in the main text at the appropriate place. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 48}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We list the used hardware and total GPU hours in the supplement and provide example timings for experiment runtimes. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 48}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: The paper does not make use of human participants or datasets. To the best of our understanding, there are no potential harmful consequences and wider negative societal impact expected from the proposed method. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 48}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 49}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: The paper introduces a method for Bayesian optimization (BO). While BO has widespread applications in the sciences and engineering, there is no direct societal impact expected from this contribution. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 49}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: The paper does not introduce any trained models or novel data. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 49}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: We credit the GPyTorch and BoTorch packages that our codebase builds upon.   \nThe packages are used as dependencies, and as such are not included directly as assets. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 50}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Justification: The implementation of BEEBO constitutes the only asset, which follows BoTorch APIs and has a README file demonstrating its application when working in BoTorch. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 50}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: None of the above are included in this paper. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 50}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 50}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: The presented paper does not involve any human subjects. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 51}]