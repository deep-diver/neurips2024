{"references": [{"fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-03-15", "reason": "This paper introduces the Chinchilla scaling laws, which are central to the empirical neural scaling laws studied in the target paper."}, {"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-27", "reason": "This paper is one of the first to establish empirical scaling laws for neural networks, providing foundational work for the target paper."}, {"fullname_first_author": "Yasaman Bahri", "paper_title": "Explaining neural scaling laws", "publication_date": "2021-02-16", "reason": "This paper provides a theoretical analysis of neural scaling laws, which is crucial for understanding the target paper's theoretical contributions."}, {"fullname_first_author": "Blake Bordelon", "paper_title": "A dynamical model of neural scaling laws", "publication_date": "2024-02-08", "reason": "This paper presents a dynamical model of neural scaling laws, offering a different perspective on the target paper's theoretical framework."}, {"fullname_first_author": "Difan Zou", "paper_title": "The benefits of implicit regularization from SGD in least squares problems", "publication_date": "2021-12-01", "reason": "This paper studies the implicit regularization effect of SGD, which is essential to the target paper's analysis of the variance error in SGD."}]}