{"references": [{"fullname_first_author": "Chen, M.", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-XX-XX", "reason": "This paper is foundational for the study of LLMs trained on code, a crucial area for evaluating the capabilities and limitations of such models in complex reasoning tasks."}, {"fullname_first_author": "Cobbe, K.", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-XX-XX", "reason": "This paper introduces GSM8K, a benchmark dataset used extensively in the paper to evaluate the performance of contextual sparsity methods on challenging arithmetic reasoning tasks."}, {"fullname_first_author": "Dong, H.", "paper_title": "Prompt-prompted mixture of experts for efficient LLM generation", "publication_date": "2024-XX-XX", "reason": "This paper proposes a novel method for efficient LLM generation, which is highly relevant to the paper's focus on improving the efficiency of LLMs, especially in resource-constrained environments."}, {"fullname_first_author": "Hendrycks, D.", "paper_title": "Measuring massive multitask language understanding", "publication_date": "2021-XX-XX", "reason": "This paper introduces the MMLU benchmark, used in the paper to evaluate contextual sparsity methods on a diverse range of knowledge-based tasks."}, {"fullname_first_author": "Lee, J.-Y.", "paper_title": "CATS: Contextually-aware thresholding for sparsity in large language models", "publication_date": "2024-XX-XX", "reason": "This paper introduces a fine-grained contextual sparsity method that is directly compared and contrasted with the SIRIUS method proposed in the main paper."}]}