[{"figure_path": "5bR2l1b2eh/tables/tables_4_1.jpg", "caption": "Table 1: We show the difference between cases when Contextual Sparsity (CS) succeeds or fails. CS is generally good at prompt understanding tasks and tasks that measure the trustworthiness of the language models while not good at tasks that require reasoning and world knowledge understanding.", "description": "This table shows the performance of contextual sparsity (CS) methods on various tasks.  It highlights that CS methods are effective for tasks involving prompt understanding, such as summarization and question answering, but perform poorly on tasks requiring reasoning and knowledge, such as arithmetic reasoning and code generation. The table compares the performance of full models and CS-sparse models on several benchmark datasets, including CNN/DailyMail, COQA, TruthfulQA, GSM8K, HumanEval, and MMLU.", "section": "Where CS Succeeds and Where CS Fails"}, {"figure_path": "5bR2l1b2eh/tables/tables_5_1.jpg", "caption": "Table 2: We show SIRIUS effectiveness and efficiency in the following table. We select GSM8K for Arithmetic Reasoning, CSQA for Commonsense Reasoning, and HumanEval for code generation. Under the \"SIRIUS Perf.\" column, A(B) is shown. A denotes the accuracy after SIRIUS correction in the dataset evaluated, while (B) represents the optimal treewidth selected under the current model dataset settings. Under the column of \"AAL\", X/Y is shown, where X is the AAL, while Y is the period.", "description": "This table presents the results of the SIRIUS model's effectiveness and efficiency on three different tasks: arithmetic reasoning (GSM8K), commonsense reasoning (CSQA), and code generation (HumanEval).  For each task, it shows the full model performance, the sparse model performance before correction, the performance after correction by SIRIUS, the average advance length (AAL), the sparse model density, and the effective density after correction. The optimal treewidth used for SIRIUS is also indicated.", "section": "5 Experiments"}, {"figure_path": "5bR2l1b2eh/tables/tables_8_1.jpg", "caption": "Table 2: We show SIRIUS effectiveness and efficiency in the following table. We select GSM8K for Arithmetic Reasoning, CSQA for Commonsense Reasoning, and HumanEval for code generation. Under the \"SIRIUS Perf.\" column, A(B) is shown. A denotes the accuracy after SIRIUS correction in the dataset evaluated, while (B) represents the optimal treewidth selected under the current model dataset settings. Under the column of \"AAL\", X/Y is shown, where X is the AAL, while Y is the period.", "description": "This table presents a quantitative evaluation of SIRIUS's performance on three different tasks: arithmetic reasoning (GSM8K), commonsense reasoning (CSQA), and code generation (HumanEval).  It shows the full model performance, the performance of contextually sparse models (CSparse and FSparse), and the improved performance after applying SIRIUS.  Key metrics include accuracy, sparsity density, average advance length (AAL), and the optimal treewidth used in SIRIUS for each model and dataset combination.", "section": "5 Experiments"}, {"figure_path": "5bR2l1b2eh/tables/tables_9_1.jpg", "caption": "Table 3: Performance and Speedup Ratios on GSM8K-COT with Different Hardware Configurations.", "description": "This table presents the performance and speedup ratios achieved by CSparse, Sirius, and the full model on the GSM8K-COT dataset across various hardware configurations (A40, L40, A100, and H100).  The metrics presented include accuracy (ACC) and latency (in milliseconds) for each model and hardware setup. Speedup ratios relative to the full model's performance are also provided, showcasing the efficiency gains of Sirius.", "section": "5.2 Wallclock Speedup"}, {"figure_path": "5bR2l1b2eh/tables/tables_9_2.jpg", "caption": "Table 4: Llama-3-70B-Instruct with Offloading.", "description": "This table presents the performance, latency, and ratio to full performance for Llama-3-70B-Instruct model with offloading using Sparse, Sirius, and Full methods.  It demonstrates the efficiency gains achieved by using SIRIUS, showing that it maintains a significant portion of the accuracy while reducing latency, compared to running the full model.", "section": "5 Experiments"}, {"figure_path": "5bR2l1b2eh/tables/tables_14_1.jpg", "caption": "Table 2: We show SIRIUS effectiveness and efficiency in the following table. We select GSM8K for Arithmetic Reasoning, CSQA for Commonsense Reasoning, and HumanEval for code generation. Under the \"SIRIUS Perf. \" column, A(B) is shown. A denotes the accuracy after SIRIUS correction in the dataset evaluated, while (B) represents the optimal treewidth selected under the current model dataset settings. Under the column of \"AAL\", X/Y is shown, where X is the AAL, while Y is the period.", "description": "This table presents the effectiveness and efficiency of SIRIUS on three different tasks: arithmetic reasoning (GSM8K), commonsense reasoning (CSQA), and code generation (HumanEval).  For each task and model, it shows the full model performance, the sparse model performance before correction, the performance after applying SIRIUS correction, the average advance length (AAL) which represents the average number of tokens generated before a correction is needed, the sparsity density, and finally the effective density after applying SIRIUS.", "section": "5 Experiments"}, {"figure_path": "5bR2l1b2eh/tables/tables_16_1.jpg", "caption": "Table 3: Performance and Speedup Ratios on GSM8K-COT with Different Hardware Configurations.", "description": "This table presents the performance and speedup ratios achieved by CSparse, Sirius (with and without tree building), and the full model on the GSM8K-COT dataset using different hardware configurations (A40, L40, A100, and H100).  It shows the accuracy (ACC), latency in milliseconds, and speedup ratios relative to the full model for each setting and hardware.", "section": "5.2 Wallclock Speedup"}, {"figure_path": "5bR2l1b2eh/tables/tables_16_2.jpg", "caption": "Table 2: We show SIRIUS effectiveness and efficiency in the following table. We select GSM8K for Arithmetic Reasoning, CSQA for Commonsense Reasoning, and HumanEval for code generation. Under the \"SIRIUS Perf.\" column, A(B) is shown. A denotes the accuracy after SIRIUS correction in the dataset evaluated, while (B) represents the optimal treewidth selected under the current model dataset settings. Under the column of \"AAL\", X/Y is shown, where X is the AAL, while Y is the period.", "description": "This table presents a quantitative evaluation of the SIRIUS model's performance on three distinct tasks: arithmetic reasoning (GSM8K), commonsense reasoning (CSQA), and code generation (HumanEval).  For each task and several language models, the table shows the full model's performance, the performance of the contextually sparse (CS) model, and the performance of the CS model after correction by SIRIUS.  Key metrics include accuracy (after correction by SIRIUS), the optimal treewidth used for the correction, the average advance length (AAL), and the effective density.  The optimal treewidth is a hyperparameter that determines the frequency of full model calls during the correction process. The AAL indicates how far the model can proceed before requiring a correction, which is related to efficiency.  Effective density reflects the balance between sparsity and the cost of using the full model for corrections.", "section": "5 Experiments"}, {"figure_path": "5bR2l1b2eh/tables/tables_16_3.jpg", "caption": "Table 8: Ablation on the threshold for correction (FSparse Llama-3-8B-Instruct).", "description": "This table presents the ablation study on the threshold used in the SIRIUS method for correcting sparse model outputs.  It shows how varying the likelihood threshold affects both the accuracy of the corrected outputs and the efficiency (measured by Average Advance Length, AAL). A higher threshold leads to higher accuracy but lower efficiency (shorter AAL), while a lower threshold results in lower accuracy but higher efficiency (longer AAL). The trade-off between accuracy and efficiency is explored by testing various threshold values.", "section": "B.3 Ablation: Various Aspects of Sirius Are Tested and Challenged"}, {"figure_path": "5bR2l1b2eh/tables/tables_17_1.jpg", "caption": "Table 1: We show the difference between cases when Contextual Sparsity (CS) succeeds or fails. CS is generally good at prompt understanding tasks and tasks that measure the trustworthiness of the language models while not good at tasks that require reasoning and world knowledge understanding.", "description": "This table compares the performance of contextual sparsity (CS) models on various tasks, highlighting their strengths and weaknesses.  It shows that CS models perform well on prompt understanding tasks (like summarization and question answering) but poorly on reasoning and knowledge-based tasks.  The table presents accuracy results across different models and tasks, demonstrating the limitations of CS in complex reasoning scenarios.", "section": "3 Observations"}, {"figure_path": "5bR2l1b2eh/tables/tables_17_2.jpg", "caption": "Table 2: We show SIRIUS effectiveness and efficiency in the following table. We select GSM8K for Arithmetic Reasoning, CSQA for Commonsense Reasoning, and HumanEval for code generation. Under the \"SIRIUS Perf. \" column, A(B) is shown. A denotes the accuracy after SIRIUS correction in the dataset evaluated, while (B) represents the optimal treewidth selected under the current model dataset settings. Under the column of \"AAL\", X/Y is shown, where X is the AAL, while Y is the period.", "description": "This table presents the results of the SIRIUS model on three different tasks: arithmetic reasoning (GSM8K), commonsense reasoning (CSQA), and code generation (HumanEval).  For each task and model, it shows the full model performance, the sparse model performance, the performance after correction using SIRIUS, the average advance length (AAL) which shows the efficiency of correction, and the effective density, which represents the efficiency of parameter usage.  The optimal treewidth used for SIRIUS correction is also indicated.", "section": "5 Experiments"}, {"figure_path": "5bR2l1b2eh/tables/tables_18_1.jpg", "caption": "Table 1: We show the difference between cases when Contextual Sparsity (CS) succeeds or fails. CS is generally good at prompt understanding tasks and tasks that measure the trustworthiness of the language models while not good at tasks that require reasoning and world knowledge understanding.", "description": "This table shows the performance of contextual sparsity (CS) models on various tasks.  It highlights that CS methods perform well on prompt understanding tasks (e.g., summarization, question answering) but struggle with reasoning, deduction, and knowledge-based tasks. The table provides accuracy scores for different models and tasks, illustrating the effectiveness of CS where it succeeds and where it fails. ", "section": "3 Observations"}, {"figure_path": "5bR2l1b2eh/tables/tables_19_1.jpg", "caption": "Table 1: We show the difference between cases when Contextual Sparsity (CS) succeeds or fails. CS is generally good at prompt understanding tasks and tasks that measure the trustworthiness of the language models while not good at tasks that require reasoning and world knowledge understanding.", "description": "This table shows the performance comparison of different models on various tasks, highlighting the strengths and weaknesses of contextual sparsity.  It demonstrates how contextual sparsity methods perform well on prompt understanding and tasks assessing the trustworthiness of language models but struggle on reasoning and knowledge-based tasks.  The table contrasts the performance of full models with contextual sparsity variants (CSparse and FSparse) across a variety of tasks.", "section": "Observations"}, {"figure_path": "5bR2l1b2eh/tables/tables_20_1.jpg", "caption": "Table 2: We show SIRIUS effectiveness and efficiency in the following table. We select GSM8K for Arithmetic Reasoning, CSQA for Commonsense Reasoning, and HumanEval for code generation. Under the \"SIRIUS Perf.\" column, A(B) is shown. A denotes the accuracy after SIRIUS correction in the dataset evaluated, while (B) represents the optimal treewidth selected under the current model dataset settings. Under the column of \"AAL\", X/Y is shown, where X is the AAL, while Y is the period.", "description": "This table presents the performance and efficiency gains of the SIRIUS model on three different tasks: arithmetic reasoning (GSM8K), commonsense reasoning (CSQA), and code generation (HumanEval). For each task, and for both fine-grained and coarse-grained sparsity, it shows the original performance of the sparse models, the improved performance after applying SIRIUS, and the optimal treewidth (a hyperparameter of SIRIUS).  Additionally, it provides the effective density and average advance length (AAL), metrics used to evaluate the efficiency and effectiveness of SIRIUS.", "section": "5 Experiments"}, {"figure_path": "5bR2l1b2eh/tables/tables_21_1.jpg", "caption": "Table 2: We show SIRIUS effectiveness and efficiency in the following table. We select GSM8K for Arithmetic Reasoning, CSQA for Commonsense Reasoning, and HumanEval for code generation. Under the \"SIRIUS Perf. \" column, A(B) is shown. A denotes the accuracy after SIRIUS correction in the dataset evaluated, while (B) represents the optimal treewidth selected under the current model dataset settings. Under the column of \"AAL\", X/Y is shown, where X is the AAL, while Y is the period.", "description": "This table presents a quantitative evaluation of SIRIUS's performance on three different tasks: arithmetic reasoning (GSM8K), commonsense reasoning (CSQA), and code generation (HumanEval).  For each task and several different language models, the table shows the original performance, the performance with contextual sparsity (CSparse and FSparse), the performance after applying SIRIUS, the average advance length (AAL), and the effective density.  The optimal treewidth used in each setting is also indicated. The results demonstrate the effectiveness and efficiency gains achieved by SIRIUS in improving the performance of contextually sparse models on complex reasoning tasks.", "section": "5 Experiments"}, {"figure_path": "5bR2l1b2eh/tables/tables_22_1.jpg", "caption": "Table 2: We show SIRIUS effectiveness and efficiency in the following table. We select GSM8K for Arithmetic Reasoning, CSQA for Commonsense Reasoning, and HumanEval for code generation. Under the \"SIRIUS Perf.\" column, A(B) is shown. A denotes the accuracy after SIRIUS correction in the dataset evaluated, while (B) represents the optimal treewidth selected under the current model dataset settings. Under the column of \"AAL\", X/Y is shown, where X is the AAL, while Y is the period.", "description": "This table presents the results of the SIRIUS model on three different tasks: GSM8K (arithmetic reasoning), CSQA (commonsense reasoning), and HumanEval (code generation).  For each task and model, the table shows the full model performance, the sparse model performance, the SIRIUS-corrected performance, the average advance length (AAL), the sparse density, and the effective density. The AAL value indicates the efficiency of the correction mechanism, with higher values indicating greater efficiency. The optimal treewidth used for each model and task is shown in parentheses. The table highlights SIRIUS's ability to significantly improve the performance of sparse models while maintaining efficiency.", "section": "5 Experiments"}, {"figure_path": "5bR2l1b2eh/tables/tables_23_1.jpg", "caption": "Table 2: We show SIRIUS effectiveness and efficiency in the following table. We select GSM8K for Arithmetic Reasoning, CSQA for Commonsense Reasoning, and HumanEval for code generation. Under the \"SIRIUS Perf. \" column, A(B) is shown. A denotes the accuracy after SIRIUS correction in the dataset evaluated, while (B) represents the optimal treewidth selected under the current model dataset settings. Under the column of \"AAL\", X/Y is shown, where X is the AAL, while Y is the period.", "description": "This table presents the results of the SIRIUS model on three different tasks: arithmetic reasoning (GSM8K), commonsense reasoning (CSQA), and code generation (HumanEval).  For each task and model, it shows the full model performance, the performance of the contextual sparsity model alone, the performance after correction by the SIRIUS model, the average advance length (AAL) which represents the efficiency of the correction process, and the effective density which is the ratio of parameters used per token. The optimal treewidth used for each model and task is also reported.", "section": "5 Experiments"}, {"figure_path": "5bR2l1b2eh/tables/tables_24_1.jpg", "caption": "Table 2: We show SIRIUS effectiveness and efficiency in the following table. We select GSM8K for Arithmetic Reasoning, CSQA for Commonsense Reasoning, and HumanEval for code generation. Under the \"SIRIUS Perf.\" column, A(B) is shown. A denotes the accuracy after SIRIUS correction in the dataset evaluated, while (B) represents the optimal treewidth selected under the current model dataset settings. Under the column of \"AAL\", X/Y is shown, where X is the AAL, while Y is the period.", "description": "This table presents a quantitative evaluation of SIRIUS on three tasks: arithmetic reasoning (GSM8K), commonsense reasoning (CSQA), and code generation (HumanEval).  For each task, the table shows the full model performance, the performance of the sparse model without correction, and the performance of the sparse model after correction with SIRIUS. The optimal treewidth used for SIRIUS is specified for each model and dataset combination.  Key metrics include accuracy, sparsity density, average parameter used per token (APU), and average advancement length (AAL), providing a comprehensive view of SIRIUS's effectiveness and efficiency.", "section": "5 Experiments"}, {"figure_path": "5bR2l1b2eh/tables/tables_25_1.jpg", "caption": "Table 2: We show SIRIUS effectiveness and efficiency in the following table. We select GSM8K for Arithmetic Reasoning, CSQA for Commonsense Reasoning, and HumanEval for code generation. Under the \"SIRIUS Perf. \" column, A(B) is shown. A denotes the accuracy after SIRIUS correction in the dataset evaluated, while (B) represents the optimal treewidth selected under the current model dataset settings. Under the column of \"AAL\", X/Y is shown, where X is the AAL, while Y is the period.", "description": "This table presents the effectiveness and efficiency of SIRIUS on three different tasks: arithmetic reasoning (GSM8K), commonsense reasoning (CSQA), and code generation (HumanEval). For each task and model, the table shows the full model performance, the sparse model performance, the performance after SIRIUS correction, the average advance length (AAL) and its corresponding period, and the effective density.  The optimal treewidth used for SIRIUS is also indicated.", "section": "5 Experiments"}, {"figure_path": "5bR2l1b2eh/tables/tables_26_1.jpg", "caption": "Table 2: We show SIRIUS effectiveness and efficiency in the following table. We select GSM8K for Arithmetic Reasoning, CSQA for Commonsense Reasoning, and HumanEval for code generation. Under the \"SIRIUS Perf.\" column, A(B) is shown. A denotes the accuracy after SIRIUS correction in the dataset evaluated, while (B) represents the optimal treewidth selected under the current model dataset settings. Under the column of \"AAL\", X/Y is shown, where X is the AAL, while Y is the period.", "description": "This table presents the results of the SIRIUS model on three different tasks: GSM8K (arithmetic reasoning), CSQA (commonsense reasoning), and HumanEval (code generation).  For each task, the table shows the full model performance, the performance of the contextual sparsity model, the performance after applying SIRIUS, and the average advance length (AAL) with its corresponding period. The optimal treewidth used for each model and dataset is also specified.", "section": "5 Experiments"}, {"figure_path": "5bR2l1b2eh/tables/tables_27_1.jpg", "caption": "Table 2: We show SIRIUS effectiveness and efficiency in the following table. We select GSM8K for Arithmetic Reasoning, CSQA for Commonsense Reasoning, and HumanEval for code generation. Under the \"SIRIUS Perf.\" column, A(B) is shown. A denotes the accuracy after SIRIUS correction in the dataset evaluated, while (B) represents the optimal treewidth selected under the current model dataset settings. Under the column of \"AAL\", X/Y is shown, where X is the AAL, while Y is the period.", "description": "This table presents a quantitative evaluation of the SIRIUS model's performance on three different tasks: arithmetic reasoning (GSM8K), commonsense reasoning (CSQA), and code generation (HumanEval).  For each task and model, the table shows the full model performance, the sparse model's performance before correction, the performance after correction with SIRIUS, the average advance length (AAL), the sparsity level and effective density. The optimal treewidth used in SIRIUS for each model and task is also indicated.", "section": "5 Experiments"}]