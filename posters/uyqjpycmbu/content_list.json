[{"type": "text", "text": "Integrating Deep Metric Learning with Coreset for Active Learning in 3D Segmentation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Arvind Murari Vepa Zukang Yang Andrew Choi UCLA UC Berkeley UCLA   \namvepa@ucla.edu zukangy@berkeley.edu asjchoi@ucla.edu Jungseock Joo Fabien Scalzo Yizhou Sun UCLA UCLA UCLA   \njjoo@comm.ucla.edu fab@cs.ucla.edu yzsun@cs.ucla.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep learning has seen remarkable advancements in machine learning, yet it often demands extensive annotated data. Tasks like 3D semantic segmentation impose a substantial annotation burden, especially in domains like medicine, where expert annotations drive up the cost. Active learning (AL) holds great potential to alleviate this annotation burden in 3D medical segmentation. The majority of existing AL methods, however, are not tailored to the medical domain. While weakly-supervised methods have been explored to reduce annotation burden, the fusion of AL with weak supervision remains unexplored, despite its potential to significantly reduce annotation costs. Additionally, there is little focus on slice-based AL for 3D segmentation, which can also significantly reduce costs in comparison to conventional volume-based AL. This paper introduces a novel metric learning method for Coreset to perform slice-based active learning in 3D medical segmentation. By merging contrastive learning with inherent data groupings in medical imaging, we learn a metric that emphasizes the relevant differences in samples for training 3D medical segmentation models. We perform comprehensive evaluations using both weak and full annotations across four datasets (medical and non-medical). Our findings demonstrate that our approach surpasses existing active learning techniques on both weak and full annotations and obtains superior performance with low-annotation budgets which is crucial in medical imaging. Source code for this project is available in the supplementary materials and on GitHub: https://github.com/arvindmvepa/al-seg. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the field of 3D medical segmentation, manual annotation of entire volumes, despite being laborious and time-consuming, has been the gold standard. Annotating a single 2D image can take minutes to hours depending on the complexity of the image (75; 73; 15; 71; 6), and a 3D medical volume, containing up to 200 slices, can require a significant amount of expert labor. Annotating a full dataset not only imposes a significant time burden on medical experts but also incurs high costs. Therefore, active learning (AL) is urgently needed to optimize annotation efforts. ", "page_idx": 0}, {"type": "text", "text": "Surprisingly, the potential of AL in the context of 3D medical segmentation has not been extensively explored. Traditional AL techniques typically focus on either diversity or model uncertainty, often neglecting relevant groupings within the data. For example, slices from the same patient or volume tend to show consistent characteristics. We propose a deep metric learning strategy that identifies and utilizes these similarities to better highlight diversity in the active learning process. Diverse samples help the model learn a wide variety of patterns and features, which can be crucial for generalization. While medical imaging has natural groupings which we can leverage, our approach extends to a wide range of real-world datasets, including video segmentation. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Several other strategies may also help in reducing costs. Most current AL approaches use volumebased AL (39) rather than slice-based AL for 3D segmentation, which tends to be more costly and less efficient. Alternatively, weakly supervised methods, which require simpler annotations like scribbles (34; 29; 11), bounding boxes (13; 83), points (82), or semi-automated techniques (5; 44; 62; 56), have been shown to perform comparably to fully supervised methods (72; 40; 54; 13; 26; 59). However, combining AL with weak supervision, especially in medical settings, remains unexplored. In our research, we explore both slice-based AL and the integration of AL with weak supervision as potential cost-reducing measures. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we present our contributions to AL for 3D medical segmentation: ", "page_idx": 1}, {"type": "text", "text": "1. The first work to integrate deep metric learning with Coreset during active learning for 3D medical segmentation. Our approach shows superior performance across four datasets (medical and non-medical) with low annotation budgets. 2. The first work to comprehensively compare new and existing algorithms for slice-based active learning for 3D medical segmentation utilizing both weak and full annotations. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Active learning AL methods can be broadly classified into 1) uncertainty-based and 2) diversitybased methods (76). Uncertainty-based methods include deep bayesian methods (21; 45; 38; 65), deep ensembles (3; 14; 52; 32; 20), contrastive learning methods (85; 41; 87; 36), and geometry-based methods (19). Diversity-based methods include coreset-based methods (64; 7), clustering-based methods (23), variational adversarial learning methods (66; 37), and random sampling methods (48). A limitation of previous methods is that they are too general and fail to utilize common data groupings found in real-world datasets. Prior methods (67; 9; 69) have tried to incorporate groupings but do not utilize domain information to generate them, which can be suboptimal. Recent research has started integrating domain-specific data groupings into AL algorithms with some success(28; 79), but these methods are designed for specific uses and lack broad applicability. While other methods have adapted Coreset (31; 30), they are not specifically tailored to 3D medical segmentation as our method is. ", "page_idx": 1}, {"type": "text", "text": "Deep metric learning Metric learning is focused on developing methods to measure similarity between data points and used in many applications. Recently, metric learning has focused on deep learning-based feature representations for data points (46). Contrastive losses are popular for metric learning, including Triplet Loss (63) and NT-Xent loss (67). Several non-contrastive approaches have been proposed based on center loss (77; 17; 16), proxy-based methods (47; 70; 27), and LLM guidance (61). One interesting approach is ensemble deep metric learning which combines embeddings from an ensemble of encoders (1; 43; 53; 60; 80). Recent work has improved on ensemble-based methods by factorizing the network training based on differerent objectives (74). However, these approaches can be computationally expensive and narrowly tailored to specific applications. Additionally, noncontrastive approaches often require class supervision to perform well. While some non-contrastive approaches do not (88; 18), they are also narrowly tailored to specific applications. ", "page_idx": 1}, {"type": "text", "text": "In contrast, contrastive learning is often used in self-supervised settings in diverse applications (10; 2; 55). The NT-Xent loss specifically outperformed other contrastive losses in self-supervised zero-shot image classification and outperformed fully-supervised ResNet-50 (12). However, prior work (including in active learning (85; 23; 39)) does not leverage any inherent data groupings in the contrastive loss, which can be useful weak supervision. Additionally, recent work in active learning can be computationally expensive because they retrain the feature encoder after each active learning round (85). ", "page_idx": 1}, {"type": "text", "text": "Active learning in medical segmentation There has been some notable research on AL in medical segmentation. Earlier work utilized bootstrapping to estimate sample uncertainty (81). In another work, the researchers built a mutual information-based metric between the labeled and unlabeled pools to improve diversity (49). However, both of these approaches are computationally expensive and not scalable for large datasets. There were also inefficiencies in previous studies, such as choosing whole volumes instead of individual slices in 3D segmentation, which increased costs (39). Random sampling proved effective in some cases (6) and calculating uncertainty using stochastic batches was also effective (20). None of the prior approaches leverage the data groupings inherent in medical 3D data and also do not focus on active learning with weak annotations (e.g., scribbles). While another method also utilizes groupings in medical data (86), their groupings are assumed to be quite large and it\u2019s unclear how they would extend their group classification approach to when there are a large number of patients, volumes, and adjacent slice groups like with our datasets. ", "page_idx": 1}, {"type": "image", "img_path": "uyqjpycMbU/tmp/c320da2e04a121c618218f80ce0aa5942c030e39fc1286d190911d7588d0fbbf.jpg", "img_caption": ["Figure 1: Overview of our active learning pipeline "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we formally describe the problem of active learning for 3D segmentation. Let $\\mathcal{X}\\subset\\mathbb{R}^{h\\times w\\times d}$ be the set of 3D volumes and $\\mathcal{V}\\subset\\{0,1\\}^{h\\times w\\times d\\times k}$ be the set of 3D masks where $h,w,d$ correspond to the height, width, and depth (number of slices) of a 3D volume and $k$ refers to the number of classes. In 3D segmentation, we learn a mapping $F:\\mathcal X\\to\\mathcal Y$ . ", "page_idx": 2}, {"type": "text", "text": "Consider a loss function $\\mathcal{L}:\\mathcal{F}\\times\\mathcal{y}\\rightarrow\\mathbb{R}$ where $\\mathcal{F}$ is the range of model prediction probabilities $(\\mathcal{F}=[0,1]^{h\\times w\\times d\\times k})$ . We consider the dataset $D$ a large collection of data points which are sampled i.i.d. over the space $\\mathcal{Z}=\\mathcal{X}\\times\\mathcal{Y}$ as $\\{\\mathbf{x}_{i},\\mathbf{y}_{i}\\}_{i\\in[n]}\\sim p z$ . We additionally consider a partially labeled subset $s\\subset D$ . Thus, active learning for 3D segmentation can be formulated as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\underset{\\Delta s\\subset D,|\\Delta s|\\leq k}{\\mathrm{argmin}}\\,E_{\\mathbf{x},\\mathbf{y}\\sim p_{\\mathcal{Z}}}[\\mathcal{L}(\\hat{\\mathbf{y}},\\mathbf{y})]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\hat{\\mathbf{y}}$ are the model predictions $F(V)$ (where $V=[{\\mathrm{slice}}_{1},...,{\\mathrm{slice}}_{d}]$ is the volume), $\\Delta s$ is the optimal requested labeled set, $k$ is the active learning budget, and $F$ is the deep learning method learned from $s\\cup\\Delta s$ . The Coreset approach is one method of solving Equation 1 (64). However, there are two distinguishing factors in our current work: 1) slice-based active learning for 3D segmentation and 2) deep metric learning. In slice-based 3D segmentation, we learn a mapping $f$ from $\\mathcal{X}^{\\prime}\\rightarrow\\mathcal{Y}^{\\prime}$ where $\\mathcal{X}^{\\prime}\\stackrel{\\cdot}{\\subset}\\mathbb{R}^{h\\times w}$ , $\\mathcal{V}^{\\prime}\\subset\\bar{\\{0,}1\\}^{h\\times w\\times k}$ , and $F(V)=[f(\\mathrm{slice}_{1}),...,f(\\mathrm{slice}_{d})]$ . Additionally, $D$ is a collection of slices which are sampled $i.i.d$ . over the space $\\mathcal{Z}^{\\prime}=\\mathcal{X}^{\\prime}\\times\\mathcal{Y}^{\\prime}$ as $\\{\\mathbf{x}_{i},\\mathbf{y}_{i}\\}_{i\\in[n]}\\sim p_{\\mathcal{Z}^{\\prime}}$ . ", "page_idx": 2}, {"type": "text", "text": "In the original Coreset paper (64), $s\\cup\\Delta s$ is the set cover of $D$ with radius $\\delta$ . However, the Euclidean metric used to calculate the radius does not utilize task-relevant information and may be sub-optimal. This motivates us to consider deep metric learning to utilize more task-relevant information, where $d_{\\phi}(x_{1},x_{2})$ is some parameterized metric. In the the original paper (64), the authors provide a theorem which bounds the loss from Equation 1 based on the radius $\\delta$ . We show a very similar bound where $\\delta=\\operatorname*{max}_{x_{1}\\in D}\\ \\operatorname*{min}_{x_{2}\\in s\\cup\\Delta s}\\ d_{\\phi}(x_{1},x_{2})$ (we defer the precise bound and proof to the Appendix B). This leads to our Coreset optimization formulation: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{argmin}_{\\Delta s\\subset D,|\\Delta s|\\leq k}\\;\\operatorname*{max}_{x_{1}\\in D}\\;\\operatorname*{min}_{x_{2}\\in s\\cup\\Delta s}\\;d_{\\phi}(x_{1},x_{2})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The above formulation can be intuitively defined as follows; choose $k$ additional center points such that the largest distance between a data point and its nearest center is minimized (64). ", "page_idx": 3}, {"type": "text", "text": "3.2 Metric learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The parameterized metric $d_{\\phi}(x_{1},x_{2})$ is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nd_{\\phi}(x_{1},x_{2})=\\ell_{2}(g_{\\phi}(x_{1}),g_{\\phi}(x_{2}))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\ell_{2}$ is the Euclidean metric. Our goal is to learn $g_{\\phi}$ , or a feature representation, that emphasizes task-relevant similarities and differences in the data for selecting diverse samples for Coreset. We do this by training a contrastive learning-based encoder with a unique Group-based Contrastive Learning (GCL) which utilizes inherent data groupings specific to medical imaging to generate the feature representation. $d_{\\phi}$ is then used by the Coreset algorithm to select the optimal set of slices. A flow chart illustrating our pipeline can be seen in Figure 1. The annotations for the collected slices are then used to train a segmentation model. ", "page_idx": 3}, {"type": "text", "text": "3.2.1 Group-based Contrastive Learning for feature representation in metric learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "While there may be several task-relevant groupings in 3D medical imaging, it is not immediately apparent which of these would be useful for feature representation for metric learning for Coreset. For the ACDC dataset, we note the mean pairwise absolute deviation of the normalized training slice pixel values within different groups averaged over the dataset: 0.217 over the entire dataset, 0.159 within patient groups, 0.166 within volume groups, and 0.115 within adjacent slice groups. Note that the volume groups and the adjacent slice groups are the most and least diverse respectively. While intuitively the most diverse group would have the most important features for diversity, this does not necessarily indicate what combinations of groups would be helpful as well. ", "page_idx": 3}, {"type": "text", "text": "Instead, we propose a general group contrastive loss based on NT-Xent loss (67). The NT-Xent loss focuses on generating and comparing embeddings for image pairs and their augmentations. It promotes similarity in embeddings for the same image and its augmentation while encouraging dissimilarity for different images. In the same vein, in our group-based loss, we promote similar embeddings for slices from the same group and dissimilar embeddings for slices from different groups, enhancing group-level representation. We define \u201cgroup\u201d as a set of 2D slices associated with one patient. The formula is as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{group}}=-\\frac{1}{N G}\\sum_{i=1}^{N}\\sum_{j\\neq i,g_{j}=g_{i}}^{N}\\log\\frac{\\exp(s i m(z_{i},z_{j})/\\tau)}{\\sum_{k=1}^{2N}\\mathbb{1}_{[(k\\neq i)\\land((g_{k}=g_{i})\\lor(p_{k}\\neq p_{i}))]}\\exp(s i m(z_{i},z_{k})/\\tau)}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "in which $i\\in\\{1,2,...,N\\}$ are the indices for the standard batch slices, $j\\in\\{N+1,N+2,...,2N\\}$ are the indices for the augmented batch slices, $g_{i}$ refers to the group associated with slice $i$ in the batch, $p_{i}$ refers to the patient associated with slice $i$ in the batch, $G$ represents the average number of slices per group (calculated as the total number of unaugmented slices divided by the total number of groups), $z$ is the embedding for a slice in the batch, $\\tau$ is a temperature parameter, and sim is a similarity function which was cosine similarity in this study. The formula shares similarities with NT-Xent loss but introduces some modifications: ", "page_idx": 3}, {"type": "text", "text": "1. There are two summations which reflects all the group slices for a particular data point. 2. In the numerator of the logarithmic term, all the group slices for particular data point are considered similar, encouraging the development of similar embeddings for these slices. ", "page_idx": 3}, {"type": "text", "text": "3. The denominator excludes patient slices for a particular data point that are not part of the same group ", "page_idx": 4}, {"type": "text", "text": "Excluding non-group patient slices ensures that the model does not promote dissimilarity between non-group slices from the same patient. This ensures that we can sum multiple group losses together without counteracting their effects. In our study, we considered patient, volume, adjacent slice group contrastive losses in addition to NT-Xent loss. Our overall loss formulation is as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathrm{contrastive}}={\\mathcal{L}}_{\\mathrm{NT-Xent}}+\\lambda_{1}{\\mathcal{L}}_{\\mathrm{patient\\,group}}+\\lambda_{2}{\\mathcal{L}}_{\\mathrm{volume\\,group}}+\\lambda_{3}{\\mathcal{L}}_{\\mathrm{slice\\,group}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{L}_{\\mathrm{patient\\;group}},\\mathcal{L}_{\\mathrm{volume\\;group}},\\mathcal{L}_{\\mathrm{slice\\;group}}$ are the group contrastive losses associated with patient, volume, and adjacent slice groups respectively and $\\{\\lambda_{1},\\lambda_{2},\\lambda_{3}\\}$ are constants. The summation of different contrastive losses in our overall loss $\\mathcal{L}_{\\mathrm{contrastive}}$ is the focus of the ablation study in Section 4.6. ", "page_idx": 4}, {"type": "text", "text": "Batch sampler We employ $\\mathcal{L}_{\\mathrm{contrastive}}$ to train a SimCLR network (35) using a ResNet-18 encoder (24), which is our $g_{\\phi}$ . In practice, standard random data loading would yield minimal impact from $\\mathcal{L}_{\\mathrm{contrastive}}$ due to the low probability of randomly selecting two slices from the same group (e.g., same patient, same volume, or adjacent slices) within a batch. To address this, we introduce a batch sampler designed to increase their occurrence. The batch sampling process can be summarized as follows: 1. create a singular slice group for all the dataset slices, 2. for each group, randomly include a slice from the same patient for each of the groupings used (e.g., patient and volume), 3. combine groups from different patients to form a batch. An epoch consists of all the dataset groups: thus, more contrastive loss groups result in larger epochs. The batch sampler would be reset every epoch to ensure randomness during training. Please see Appendix C for more details. ", "page_idx": 4}, {"type": "text", "text": "Training the SimCLR network with this batch sampler eliminates the need for complex algorithmic adjustments to accommodate multiple contrastive loss groups, a challenge in other AL contrastive learning methodologies (85). We conduct the training over 100 epochs using an ADAM optimizer, with a learning rate of 3e-4, a weight decay of $1.0\\mathrm{e}{-6}$ , and a batch size of 8 for one or three groups and 9 for two groups. ", "page_idx": 4}, {"type": "text", "text": "3.2.2 Segmentation model training ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Once we have $g_{\\phi}$ , we can calculate $d_{\\phi}$ and collect annotations for unlabeled slices to train our segmentation model. Our AL evaluation consists of several rounds of annotation collection from an oracle. After each round of annotation collection, we train five segmentation models and record the model\u2019s test score with the highest validation score. We repeat the AL experiment five times for each algorithm, each time with a different random seed, and report the average model test score per round. For weakly-supervised segmentation, we train a Dual-Branch Network with Dynamically Mixed Pseudo Labels Supervision (DMPLS) (40), which reported strong metrics on the ACDC dataset using weak supervision. For full supervision, we train UNet (58) which is a frequently used fully-supervised segmentation baseline model. We calculate the 3D DICE score on each 3D volume and report the average on all the volumes in the test set. For full supervision, we also provide results using a pre-trained segmentation model with a ResNet-50 backbone (42) (pre-trained on medical images (42) for medical datasets and ImageNetV2 for non-medical datasets). We do not report weakly-supervised results on pre-trained architectures because the weakly-supervised architectures cannot easily utilize pre-trained backbones. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Datasets ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "ACDC (Automated Cardiac Diagnosis Challenge) The ACDC dataset (4) consists of 200 shortaxis cine-MRI scans from 100 patients in the training set and 100 scans from 50 patients in the test set. Acquired data was fully anonymized and handled within the regulations set by the local ethical committee of the Hospital of Dijon (France). Each patient has two annotated end-diastolic (ED) and end-systolic (ES) phase scans. Annotations are available for three structures: the right ventricle (RV), myocardium (Myo), and left ventricle (LV). Additionally, scribble annotations have been provided for each scan by a previous study (72). The training set size consists of 1448 slices. ", "page_idx": 4}, {"type": "text", "text": "CHAOS (Combined Healthy Abdominal Organ Segmentation) The CHAOS dataset (33) comprises of abdominal CT images from 20 subjects in the training set, primarily used for liver and vessel segmentation. The anonymized dataset was collected from the Department of Radiology, Dokuz Eylul University Hospital, Izmir, Turkey and the study was approved by the Institutional Review Board of Dokuz Eylul University. Each patient\u2019s CT scans contains approximately 144 slices. Binary segmentation masks for the liver are provided. We resampled, cropped, and normalized the images, following the process described in a previous study (72). We partitioned the training set into training $(75\\%)$ , validation $(10\\%)$ , and test $(15\\%)$ subsets. The training set size consists of 2351 slices. ", "page_idx": 5}, {"type": "text", "text": "MS-CMR (Multi-sequence Cardiac MR Segmentation Challenge) The MS-CMR dataset (22; 89; 78) contains late gadolinium enhancement (LEG) MRI images from 45 patients who underwent cardiomyopathy. The data has been collected from Shanghai Renji hospital with institutional ethics approval and has been anonymized. These images were multi-slice, acquired in the ventricular short-axis views. We obtained realistic and manual scribble annotations from a prior study (84). Similar to this study (84), we split the data such that 25 patients were assigned to train, 5 to validation, and 20 to test, resulting in 382 slices in the training set. For data processing, we resampled the images into a resolution of $1.37\\mathrm{x}1.37\\;\\mathrm{mm}$ , and then they were cropped or padded to a fixed size of $212\\times212$ pixels. During training, the image pixel values were normalized to zero mean and unit variance. ", "page_idx": 5}, {"type": "text", "text": "DAVIS (Densely Annotated Video Segmentation) The DAVIS dataset (50; 51) is a densely annotated video dataset associated with the 2016 DAVIS and 2017 DAVIS Challenge. The dataset collection was partially funded by SNF and human subject data was collected ethically, to the best of our knowledge. In our study, we utilized the train and val sets associated with the 2016 DAVIS Challenge which contained 30 and 20 videos respectively and all frames with 480p resolution. While we used the 2016 DAVIS train set, we created the val and test set by further splitting the 2016 DAVIS val set into 5 and 15 videos respectively. Our train set consisted of 2079 densely annotated frames. ", "page_idx": 5}, {"type": "text", "text": "For additional generalizability of our approach, we evaluated our methodology on both weak and full supervision, depending on data availability. Because the CHAOS, MS-CMR, and DAVIS dataset do not contain any hierarchical organization of multiple volumes/videos, we did not use the patient group loss. For video segmentation, in our contrastive loss we treated each video as a \"volume\" and each video frame as a \"slice\"; thus, we considered both the volume and adjacent slice group loss. ", "page_idx": 5}, {"type": "text", "text": "4.2 Experimental settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "When conducting experiments with pre-trained segmentation models, for ACDC, CHAOS, and MS-CMR we collect annotations in cumulative increments of $2\\%$ , $3\\%$ , $4\\%$ , $5\\%$ , $10\\%$ , $15\\%$ , $20\\%$ , and $40\\%$ in each round. Because segmentation networks require more training data for natural images, for the DAVIS dataset we collect annotations in cumulative increments of $10\\%$ , $20\\%$ , $30\\%$ , and $40\\%$ in each round. When conducting experiments with pre-trained segmentation models, because of the benefit of prior pre-training, we collect annotations in cumulative increments of $1\\%,2\\%,3\\%,4\\%$ , and $5\\%$ in each round. ", "page_idx": 5}, {"type": "text", "text": "Because solving the Coreset problem is NP-Hard, we utilized the K-Center Greedy algorithm for our Coreset implementation (48), which is a $2-O P T$ solution (64) and produces very competitive results in comparison to other more computationally-intensive solutions. We compared our approach to vanilla Coreset (K-Center Greedy) (64), Random Sampling, CoreGCN (7), TypiClust (23; 39), Stochastic Batches (using Deep Ensembles with Entropy) (20), VAAL (66), Deep Ensembles (utilizing Variance Ratio scoring) (3), and Bayesian Deep Learning (utilizing the BALD score) (21). To ensure a fair comparison, all approaches were evaluated using the same experimental settings. ", "page_idx": 5}, {"type": "text", "text": "All of our experiments were primarily conducted with a single Tesla V100 GPU on an internal cluster. Our contrastive learning-based encoder and segmentation models consumed approximately 3 GB of GPU memory while training. The contrastive encoder\u2019s training speed was approximately 40 slices/second which resulted in a training time of 100 minutes, 200 minutes, and 300 minutes on ACDC for one, two, and three group losses respectively. One single AL experiment for our method on ACDC (eight rounds with five models trained per round) took approximately 24 hours and used about $400\\;\\mathrm{MB}$ of storage. ", "page_idx": 5}, {"type": "table", "img_path": "uyqjpycMbU/tmp/71dd3f51248013d6b45b88b5a3973b1399e713ff12e9037e06a39f91584820ac.jpg", "table_caption": ["Table 1: DICE scores for ACDC, MS-CMR, and CHAOS "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "uyqjpycMbU/tmp/1cd0ac75d64b263d5c43dd804772cb427ad052057b69491f59e79e3ef16d13bd.jpg", "table_caption": ["Table 2: DICE scores for DAVIS "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.3 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Tables 1 and 2 summarize the results from our experiments on weak and full annotations for the ACDC, MS-CMR, CHAOS, and DAVIS datasets when trained from scratch. Our method excels in low annotation budget scenarios $(2\\%{-}5\\%)$ on the ACDC dataset, outperforming other methods by up to $10\\%$ in some cases. This advantage is vital in the medical field where annotation costs are often high. On the MS-CMR, CHAOS, and DAVIS datasets, our method remains highly competitive, consistently achieving the highest or close to the highest performance for a particular annotation level. ", "page_idx": 6}, {"type": "table", "img_path": "uyqjpycMbU/tmp/87f3f083bc0cb9126e1260b35fa3473f0459de26872405f7dc79480ab59c8ed9.jpg", "table_caption": ["Table 3: Mean DICE scores over all annotation datapoints with pre-trained weights "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "uyqjpycMbU/tmp/919942d5870d0a422325541e07029ff72711229940824c9f1349cc299df9a58b.jpg", "img_caption": ["Figure 2: Describes the relationship between model performance and annotation time for our method utilizing weakly and fully-supervised 2D slices and random sampling of fully-supervised 3D volumes on the ACDC dataset. Annotation $\\%$ is measured as the percentage of the fully-labeled ACDC training data. For weak supervision, we extrapolate the percentage of fully-labeled data based on equivalent annotation time (we follow prior work which assumes that annotators annotate scribbles 15x as fast as the full masks (72)). The dashed green line represents the performance of our method using weakly-supervised 2D slices with $40\\%$ of the ACDC training data. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "uyqjpycMbU/tmp/ba069f965c00be9116a30cb8323490a417f3a5be13e8c6781de618d232a4d733.jpg", "img_caption": ["Figure 3: Qualitative comparison of our method, CoreGCN, and Coreset. Blue indicates agreement between model predictions and groundtruth masks and red indicates disagreement. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Additionally, our algorithm consistently demonstrates superior performance in both weak and full annotation scenarios, unlike other top-performing methods which struggle in one of these settings. Compared to other algorithms, our method shows superior performance on different datasets as well. ", "page_idx": 7}, {"type": "text", "text": "We note that clinically acceptable DICE scores for similar medical segmentation tasks range from 0.5-0.9, depending on the task (68; 25; 75; 8). However, even lower DICE scores can be clinically useful, especially for particular volumes with higher scores or in conjunction with semi-automated segmentation methods (57). ", "page_idx": 7}, {"type": "table", "img_path": "uyqjpycMbU/tmp/0db1f26247abbb58bc756ddf6f67d9caec6a6f2d1bb7c6d2f52a2eeac7bff4f8.jpg", "table_caption": ["Table 4: Ablation study based on the mean DICE scores for the $2.5\\%$ weak annotation datapoint "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "uyqjpycMbU/tmp/78f6ab6396d52e5d4230b870676eb663a8289548c9db7cf0fa416080dddf56d0.jpg", "img_caption": ["Figure 4: t-SNE visualization of dataset clusters generated by different $g_{\\phi}$ "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Our fully-supervised experiments with pre-trained models are in Table 3. We saw improvements with our method in comparison to the best performing comparison methods. Please refer to Appendix D for comprehensive results across all datasets, including calculated bootstrapping standard errors. ", "page_idx": 8}, {"type": "text", "text": "4.4 Relationship between model performance and annotation time ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Figure 2 we provided a graph which describes the relationship between model performance and annotation time for our method utilizing weakly and fully-supervised 2D slices and random sampling of fully-supervised 3D volumes on the ACDC dataset. To ensure a fair comparison between the different methods, we do not incorporate any of the results from the pre-trained segmentation models. For the 3D results, similar to the 2D U-Net, we train a 3D U-Net from scratch. Given comparable annotation time, our methods trained on both weakly and fully-supervised 2D slices far exceed the performance of random sampling of 3D volumes and achieve 3D volume maximum performance (with the given budget) with much less annotation time. ", "page_idx": 8}, {"type": "text", "text": "4.5 Comparison with related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Of the diversity-based comparison methods (Coreset, VAAL, TypiClust, Random), the performance for these three are generally worse than our method. For example, our method achieves the best performance on 21 out of 27 comparison points (Tables 1, 2, 3). The next closest is Random sampling, which achieves the best performance on 5 out of 27 comparison points. Of the entropy-based methods (BALD, Variance Ratio, Stochastic Batches), Stochastic Batches achieves the best performance on 4 out of 27 comparison points, the best out of the group. ", "page_idx": 8}, {"type": "text", "text": "Coreset and CoreGCN share similarities with our method. However, on almost all the comparison points, they perform much worse. In Figure 3, there is a qualitative comparison of our method, CoreGCN, and Coreset on a difficult slice in the volume after several weak annotation rounds. With more requested annotations, our method is able to reduce errors even in difficult slices. With $5\\%$ weak annotation, while Coreset and CoreGCN still retain large error artifacts, our method has minimally visible errors. ", "page_idx": 9}, {"type": "text", "text": "4.6 Ablation study ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In Table 4, we see our ablation experiments. The first three sections represent our experiments with one, two, and three or more contrastive losses respectively. We note that all the contrastive loss experiments perform better than vanilla Coreset. Additionally, generally the larger combination of losses tend to outperform the smaller combination of losses. The best loss combination involves the volume group loss, patient group loss, and NT-Xent. ", "page_idx": 9}, {"type": "text", "text": "One interesting observation is that the loss associated with the volume group \u2014 the most diverse group \u2014 tends to produce the best additive performance and the loss associated with the the adjacent slice group \u2014 the least diverse group \u2014 tends to have the worst additive performance. We see this trend in the one loss experiments as well in the two loss and three or more loss experiments, where the combinations with the volume group loss tend to produce the best performance and the combinations with the adjacent slice group loss tends to produce the worst performance. ", "page_idx": 9}, {"type": "text", "text": "In order to visualize how effective the learned $g_{\\phi}$ trained with different losses are for Coreset, we applied $\\mathbf{k}$ -means clustering to the generated dataset features with a contrastive encoder trained with NT-Xent and trained with our optimal loss and visualized the quality of cluster labels using a t-SNE plot, which can be seen in Figure 4. We note that the clusters from our loss show good cohesion (tightly grouped), separation between clusters, and are easy to differentiate. However, the clusters from NT-Xent show much less cohesion (points are spread over more space) and the separation is less defined. Higher quality clustering emphasizes points are well separated which leads to better performance when trying to find representative points using Coreset. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In our research, we introduced a novel metric learning method for Coreset to perform slice-based active learning in 3D medical segmentation. By leveraging diverse data groups in our feature representation, we were able to learn a metric that promoted diversity and our Coreset implementation was able to outperform all existing methods on medical and non-medical datasets in weak and full annotation scenarios with a low annotation budget. Due to limited computational resources, we restricted the number of experiment runs and models we trained. We also acknowledge that we did not fully consider training set bias which can result in unfair outcomes for underrepresented groups. In future work, we hope to remedy some of these issues and focus more robustly on the applications of our approach in diverse domains. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partially supported by NSF 2211557, NSF 1937599, NSF 2119643, NSF 2303037, NSF 2312501, NASA, SRC JUMP 2.0 Center, Amazon Research Awards, and Snapchat Gifts. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Aziere, N., Todorovic, S.: Ensemble deep manifold similarity learning using hard proxies. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.   \n7299\u20137307 (2019) [2] Baevski, A., Zhou, Y., Mohamed, A., Auli, M.: wav2vec 2.0: A framework for self-supervised learning of speech representations. Advances in neural information processing systems 33,   \n12449\u201312460 (2020) [3] Beluch, W.H., Genewein, T., N\u00fcrnberger, A., K\u00f6hler, J.M.: The power of ensembles for active learning in image classification. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 9368\u20139377 (2018) [4] Bernard, O., Lalande, A., Zotti, C., Cervenansky, F., Yang, X., Heng, P.A., Cetin, I., Lekadir, K., Camara, O., Ballester, M.A.G., et al.: Deep learning techniques for automatic mri cardiac multi-structures segmentation and diagnosis: is the problem solved? IEEE transactions on medical imaging 37(11), 2514\u20132525 (2018) [5] Bianconi, F., Fravolini, M.L., Pizzoli, S., Palumbo, I., Minestrini, M., Rondini, M., Nuvoli, S., Spanu, A., Palumbo, B.: Comparative evaluation of conventional and deep learning methods for semi-automated segmentation of pulmonary nodules on ct. Quantitative imaging in medicine and surgery 11(7), 3286 (2021) [6] Burmeister, J.M., Rosas, M.F., Hagemann, J., Kordt, J., Blum, J., Shabo, S., Bergner, B., Lippert, C.: Less is more: A comparison of active learning strategies for 3d medical image segmentation. arXiv preprint arXiv:2207.00845 (2022) [7] Caramalau, R., Bhattarai, B., Kim, T.K.: Sequential graph convolutional network for active learning. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 9583\u20139592 (2021) [8] Cardenas, C.E., McCarroll, R.E., Court, L.E., Elgohari, B.A., Elhalawani, H., Fuller, C.D., Kamal, M.J., Meheissen, M.A., Mohamed, A.S., Rao, A., et al.: Deep learning algorithm for auto-delineation of high-risk oropharyngeal clinical target volumes with built-in dice similarity coefficient parameter optimization function. International Journal of Radiation Oncology\\* Biology\\* Physics 101(2), 468\u2013478 (2018) [9] Chaitanya, K., Erdil, E., Karani, N., Konukoglu, E.: Contrastive learning of global and local features for medical image segmentation with limited annotations. Advances in neural information processing systems 33, 12546\u201312558 (2020) [10] Chen, L., Bentley, P., Mori, K., Misawa, K., Fujiwara, M., Rueckert, D.: Self-supervised learning for medical image analysis using image context restoration. Medical image analysis 58,   \n101539 (2019) [11] Chen, Q., Hong, Y.: Scribble2d5: Weakly-supervised volumetric image segmentation via scribble annotations. In: International Conference on Medical Image Computing and ComputerAssisted Intervention. pp. 234\u2013243. Springer (2022) [12] Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive learning of visual representations. In: International conference on machine learning. pp. 1597\u20131607. PMLR (2020) [13] Chibane, J., Engelmann, F., Anh Tran, T., Pons-Moll, G.: Box2mask: Weakly supervised 3d semantic instance segmentation using bounding boxes. In: European Conference on Computer Vision. pp. 681\u2013699. Springer (2022) [14] Chitta, K., Alvarez, J.M., Lesnikowski, A.: Large-scale visual active learning with deep probabilistic ensembles. arXiv preprint arXiv:1811.03575 (2018) [15] \u00c7i\u00e7ek, \u00d6., Abdulkadir, A., Lienkamp, S.S., Brox, T., Ronneberger, O.: 3d u-net: learning dense volumetric segmentation from sparse annotation. In: Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 19. pp. 424\u2013432. Springer (2016)   \n[16] Deng, J., Guo, J., Liu, T., Gong, M., Zafeiriou, S.: Sub-center arcface: Boosting face recognition by large-scale noisy web faces. In: Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XI 16. pp. 741\u2013757. Springer (2020)   \n[17] Deng, J., Guo, J., Xue, N., Zafeiriou, S.: Arcface: Additive angular margin loss for deep face recognition. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 4690\u20134699 (2019)   \n[18] Dutta, U.K., Harandi, M., Shekhar, C.C.: Semi-supervised metric learning: A deep resurrection. In: Proceedings of the AAAI Conference on artificial intelligence. vol. 35, pp. 7279\u20137287 (2021)   \n[19] Franco, L., Mandica, P., Kallidromitis, K., Guillory, D., Li, Y.T., Galasso, F.: Hyperbolic active learning for semantic segmentation under domain shift. arXiv preprint arXiv:2306.11180 (2023)   \n[20] Gaillochet, M., Desrosiers, C., Lombaert, H.: Active learning for medical image segmentation with stochastic batches. arXiv preprint arXiv:2301.07670 (2023)   \n[21] Gal, Y., Islam, R., Ghahramani, Z.: Deep bayesian active learning with image data. In: International conference on machine learning. pp. 1183\u20131192. PMLR (2017)   \n[22] Gao, S., Zhou, H., Gao, Y., Zhuang, X.: Bayeseg: Bayesian modeling for medical image segmentation with interpretable generalizability (2023)   \n[23] Hacohen, G., Dekel, A., Weinshall, D.: Active learning on a budget: Opposite strategies suit high and low budgets. arXiv preprint arXiv:2202.02794 (2022)   \n[24] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770\u2013778 (2016)   \n[25] Hoebel, K.V.: Domain and User-Centered Machine Learning for Medical Image Analysis. Ph.D. thesis, Massachusetts Institute of Technology (2023)   \n[26] Huang, Z., Guo, Y., Zhang, N., Huang, X., Decazes, P., Becker, S., Ruan, S.: Multi-scale feature similarity-based weakly supervised lymphoma segmentation in pet/ct images. Computers in Biology and Medicine 151, 106230 (2022)   \n[27] Jawade, B., Mohan, D.D., Ali, N.M., Setlur, S., Govindaraju, V.: Napreg: nouns as proxies regularization for semantically aware cross-modal embeddings. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 1135\u20131144 (2023)   \n[28] Ji, W., Liang, R., Zheng, Z., Zhang, W., Zhang, S., Li, J., Li, M., Chua, T.s.: Are binary annotations sufficient? video moment retrieval via hierarchical uncertainty-based active learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 23013\u201323022 (2023)   \n[29] Ji, Z., Shen, Y., Ma, C., Gao, M.: Scribble-based hierarchical weakly supervised learning for brain tumor segmentation. In: Medical Image Computing and Computer Assisted Intervention\u2013 MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13\u201317, 2019, Proceedings, Part III 22. pp. 175\u2013183. Springer (2019)   \n[30] Jin, Q., Yuan, M., Qiao, Q., Song, Z.: One-shot active learning for image segmentation via contrastive learning and diversity-based sampling. Knowledge-Based Systems 241, 108278 (2022)   \n[31] Ju, J., Jung, H., Oh, Y., Kim, J.: Extending contrastive learning to unsupervised coreset selection. IEEE Access 10, 7704\u20137715 (2022)   \n[32] Jung, S., Kim, S., Lee, J.: A simple yet powerful deep active learning with snapshots ensembles. In: The Eleventh International Conference on Learning Representations (2022)   \n[33] Kavur, A.E., Selver, M.A., Dicle, O., Bar\u0131\u00b8s, M., Gezer, N.S.: CHAOS - Combined (CT-MR) Healthy Abdominal Organ Segmentation Challenge Data (Apr 2019). https://doi.org/10.5281/zenodo.3362844, https://doi.org/10.5281/zenodo.3362844   \n[34] Ke, T.W., Hwang, J.J., Yu, S.X.: Universal weakly supervised segmentation by pixel-to-segment contrastive learning. arXiv preprint arXiv:2105.00957 (2021)   \n[35] Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot, A., Liu, C., Krishnan, D.: Supervised contrastive learning. Advances in neural information processing systems 33, 18661\u201318673 (2020)   \n[36] Kim, J., Kim, J., Hwang, S.: Deep active learning with contrastive learning under realistic data pool assumptions. arXiv preprint arXiv:2303.14433 (2023)   \n[37] Kim, K., Park, D., Kim, K.I., Chun, S.Y.: Task-aware variational adversarial active learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8166\u20138175 (2021)   \n[38] Kirsch, A., Van Amersfoort, J., Gal, Y.: Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning. Advances in neural information processing systems 32 (2019)   \n[39] Liu, H., Li, H., Yao, X., Fan, Y., Hu, D., Dawant, B.M., Nath, V., Xu, Z., Oguz, I.: Colossal: A benchmark for cold-start active learning for 3d medical image segmentation. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 25\u201334. Springer (2023)   \n[40] Luo, X., Hu, M., Liao, W., Zhai, S., Song, T., Wang, G., Zhang, S.: Scribble-supervised medical image segmentation via dual-branch network and dynamically mixed pseudo labels supervision. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 528\u2013538. Springer (2022)   \n[41] Margatina, K., Vernikos, G., Barrault, L., Aletras, N.: Active learning by acquiring contrastive examples. arXiv preprint arXiv:2109.03764 (2021)   \n[42] MH Nguyen, D., Nguyen, H., Diep, N., Pham, T.N., Cao, T., Nguyen, B., Swoboda, P., Ho, N., Albarqouni, S., Xie, P., et al.: Lvm-med: Learning large-scale self-supervised vision models for medical imaging via second-order graph matching. Advances in Neural Information Processing Systems 36 (2024)   \n[43] Michael, O., Georg, W., Horst, P., Horst, B.: Deep metric learning with bier: Boosting independent embeddings robustly. IEEE TPAMI 42(2), 276\u2013290 (2018)   \n[44] Militello, C., Rundo, L., Dimarco, M., Orlando, A., Conti, V., Woitek, R., D\u2019Angelo, I., Bartolotta, T.V., Russo, G.: Semi-automated and interactive segmentation of contrast-enhancing masses on breast dce-mri using spatial fuzzy clustering. Biomedical Signal Processing and Control 71, 103113 (2022)   \n[45] Mohamadi, S., Amindavar, H.: Deep bayesian active learning, a brief survey on recent advances. arXiv preprint arXiv:2012.08044 (2020)   \n[46] Mohan, D.D., Jawade, B., Setlur, S., Govindaraju, V.: Deep metric learning for computer vision: A brief overview. Handbook of Statistics 48, 59\u201379 (2023)   \n[47] Movshovitz-Attias, Y., Toshev, A., Leung, T.K., Ioffe, S., Singh, S.: No fuss distance metric learning using proxies. In: Proceedings of the IEEE international conference on computer vision. pp. 360\u2013368 (2017)   \n[48] Munjal, P., Hayat, N., Hayat, M., Sourati, J., Khan, S.: Towards robust and reproducible active learning using neural networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 223\u2013232 (2022)   \n[49] Nath, V., Yang, D., Landman, B.A., Xu, D., Roth, H.R.: Diminishing uncertainty within the training pool: Active learning for medical image segmentation. IEEE Transactions on Medical Imaging 40(10), 2534\u20132547 (2020)   \n[50] Perazzi, F., Pont-Tuset, J., McWilliams, B., Van Gool, L., Gross, M., Sorkine-Hornung, A.: A benchmark dataset and evaluation methodology for video object segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 724\u2013732 (2016)   \n[51] Pont-Tuset, J., Perazzi, F., Caelles, S., Arbel\u00e1ez, P., Sorkine-Hornung, A., Van Gool, L.: The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675 (2017)   \n[52] Pop, R., Fulop, P.: Deep ensemble bayesian active learning: Addressing the mode collapse issue in monte carlo dropout via ensembles. arXiv preprint arXiv:1811.03897 (2018)   \n[53] Qian, Q., Shang, L., Sun, B., Hu, J., Li, H., Jin, R.: Softtriple loss: Deep metric learning without triplet sampling. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 6450\u20136458 (2019)   \n[54] Qian, Z., Li, K., Lai, M., Chang, E.I.C., Wei, B., Fan, Y., Xu, Y.: Transformer based multiple instance learning for weakly supervised histopathology image segmentation. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 160\u2013170. Springer (2022)   \n[55] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748\u20138763. PMLR (2021)   \n[56] Ren, H., Zhou, L., Liu, G., Peng, X., Shi, W., Xu, H., Shan, F., Liu, L.: An unsupervised semi-automated pulmonary nodule segmentation method based on enhanced region growing. Quantitative Imaging in Medicine and Surgery 10(1), 233 (2020)   \n[57] Rhee, D.J., Cardenas, C.E., Elhalawani, H., McCarroll, R., Zhang, L., Yang, J., Garden, A.S., Peterson, C.B., Beadle, B.M., Court, L.E.: Automatic detection of contouring errors using convolutional neural networks. Medical physics 46(11), 5086\u20135097 (2019)   \n[58] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. pp. 234\u2013241. Springer (2015)   \n[59] Rossetti, S., Zappia, D., Sanzari, M., Schaerf, M., Pirri, F.: Max pooling with vision transformers reconciles class and shape in weakly supervised semantic segmentation. In: European Conference on Computer Vision. pp. 446\u2013463. Springer (2022)   \n[60] Roth, K., Brattoli, B., Ommer, B.: Mic: Mining interclass characteristics for improved metric learning. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 8000\u20138009 (2019)   \n[61] Roth, K., Vinyals, O., Akata, Z.: Integrating language guidance into vision-based deep metric learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16177\u201316189 (2022)   \n[62] Sakinis, T., Milletari, F., Roth, H., Korfiatis, P., Kostandy, P., Philbrick, K., Akkus, Z., Xu, Z., Xu, D., Erickson, B.J.: Interactive segmentation of medical images through fully convolutional neural networks. arXiv preprint arXiv:1903.08205 (2019)   \n[63] Schroff, F., Kalenichenko, D., Philbin, J.: Facenet: A unified embedding for face recognition and clustering. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 815\u2013823 (2015)   \n[64] Sener, O., Savarese, S.: Active learning for convolutional neural networks: A core-set approach. arXiv preprint arXiv:1708.00489 (2017)   \n[65] Siddhant, A., Lipton, Z.C.: Deep bayesian active learning for natural language processing: Results of a large-scale empirical study. arXiv preprint arXiv:1808.05697 (2018)   \n[66] Sinha, S., Ebrahimi, S., Darrell, T.: Variational adversarial active learning. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 5972\u20135981 (2019)   \n[67] Sohn, K.: Improved deep metric learning with multi-class n-pair loss objective. Advances in neural information processing systems 29 (2016)   \n[68] Sun, Y., Wang, Y., Gan, K., Wang, Y., Chen, Y., Ge, Y., Yuan, J., Xu, H.: Reliable delineation of clinical target volumes for cervical cancer radiotherapy on ct/mr dual-modality images. Journal of Imaging Informatics in Medicine pp. 1\u201314 (2024)   \n[69] Taleb, A., Loetzsch, W., Danz, N., Severin, J., Gaertner, T., Bergner, B., Lippert, C.: 3d selfsupervised methods for medical imaging. Advances in neural information processing systems 33, 18158\u201318172 (2020)   \n[70] Teh, E.W., DeVries, T., Taylor, G.W.: Proxynca++: Revisiting and revitalizing proxy neighborhood component analysis. In: Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXIV 16. pp. 448\u2013464. Springer (2020)   \n[71] Tsai, D.M., Fan, S.K.S., Chou, Y.H.: Auto-annotated deep segmentation for surface defect detection. IEEE Transactions on Instrumentation and Measurement 70, 1\u201310 (2021)   \n[72] Valvano, G., Leo, A., Tsaftaris, S.A.: Learning to segment from scribbles using multi-scale adversarial attention gates. IEEE Transactions on Medical Imaging 40(8), 1990\u20132001 (2021)   \n[73] Vepa, A., Choi, A., Nakhaei, N., Lee, W., Stier, N., Vu, A., Jenkins, G., Yang, X., Shergill, M., Desphy, M., et al.: Weakly-supervised convolutional neural networks for vessel segmentation in cerebral angiography. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 585\u2013594 (2022)   \n[74] Wang, C., Zheng, W., Li, J., Zhou, J., Lu, J.: Deep factorized metric learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7672\u20137682 (2023)   \n[75] Wang, S., Li, C., Wang, R., Liu, Z., Wang, M., Tan, H., Wu, Y., Liu, X., Sun, H., Yang, R., et al.: Annotation-efficient deep learning for automatic medical image segmentation. Nature communications 12(1), 5915 (2021)   \n[76] Wang, T., Li, X., Yang, P., Hu, G., Zeng, X., Huang, S., Xu, C.Z., Xu, M.: Boosting active learning via improving test performance. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 36, pp. 8566\u20138574 (2022)   \n[77] Wen, Y., Zhang, K., Li, Z., Qiao, Y.: A discriminative feature learning approach for deep face recognition. In: Computer vision\u2013ECCV 2016: 14th European conference, amsterdam, the netherlands, October 11\u201314, 2016, proceedings, part VII 14. pp. 499\u2013515. Springer (2016)   \n[78] Wu, F., Zhuang, X.: Minimizing estimated risks on unlabeled data: A new formulation for semi-supervised medical image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence 45(5), 6021\u20136036 (2022)   \n[79] Xie, B., Yuan, L., Li, S., Liu, C.H., Cheng, X., Wang, G.: Active learning for domain adaptation: An energy-based approach. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 36, pp. 8708\u20138716 (2022)   \n[80] Xuan, H., Souvenir, R., Pless, R.: Deep randomized ensembles for metric learning. In: Proceedings of the European conference on computer vision (ECCV). pp. 723\u2013734 (2018)   \n[81] Yang, L., Zhang, Y., Chen, J., Zhang, S., Chen, D.Z.: Suggestive annotation: A deep active learning framework for biomedical image segmentation. In: Medical Image Computing and Computer Assisted Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part III 20. pp. 399\u2013407. Springer (2017)   \n[82] Zhang, B., Xiao, J., Jiao, J., Wei, Y., Zhao, Y.: Affinity attention graph neural network for weakly supervised semantic segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence 44(11), 8082\u20138096 (2021)   \n[83] Zhang, H., Burrows, L., Meng, Y., Sculthorpe, D., Mukherjee, A., Coupland, S.E., Chen, K., Zheng, Y.: Weakly supervised segmentation with point annotations for histopathology images via contrast-based variational model. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15630\u201315640 (2023) ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "[84] Zhang, K., Zhuang, X.: Cyclemix: A holistic strategy for medical image segmentation from scribble supervision. arXiv preprint arXiv:2203.01475 (2022) ", "page_idx": 15}, {"type": "text", "text": "[85] Zhang, Y., Zhang, X., Xie, L., Li, J., Qiu, R.C., Hu, H., Tian, Q.: One-bit active query with contrastive pairs. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9697\u20139705 (2022) ", "page_idx": 15}, {"type": "text", "text": "[86] Zheng, H., Han, J., Wang, H., Yang, L., Zhao, Z., Wang, C., Chen, D.Z.: Hierarchical selfsupervised learning for medical image segmentation based on multi-domain data aggregation. In: Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2021: 24th International Conference, Strasbourg, France, September 27\u2013October 1, 2021, Proceedings, Part I 24. pp. 622\u2013632. Springer (2021) ", "page_idx": 15}, {"type": "text", "text": "[87] Zhu, Y., Xu, W., Liu, Q., Wu, S.: When contrastive learning meets active learning: A novel graph active learning paradigm with self-supervision. arXiv preprint arXiv:2010.16091 (2020) ", "page_idx": 15}, {"type": "text", "text": "[88] Zhuang, F., Moulin, P.: Deep semi-supervised metric learning with mixed label propagation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3429\u20133438 (2023) ", "page_idx": 15}, {"type": "text", "text": "[89] Zhuang, X.: Multivariate mixture model for myocardial segmentation combining multi-source images. IEEE transactions on pattern analysis and machine intelligence 41(12), 2933\u20132946 (2018) ", "page_idx": 15}, {"type": "text", "text": "A Loss weights ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In our experiments and ablation study, a 1.0 weight was applied whenever the NT-Xent loss was used. Additionally, a 1.0 weight was applied if only one group contrastive loss was used. In the ablation study on the ACDC dataset, for the experiments with multiple group contrastive losses, we tried different combinations of weights for the group contrastive losses and reported the best results. We will utilize the formulation from Equation 5 for the loss weights and report them as a four-tuple $(a,b,c,d)$ which corresponds to $\\left(\\lambda_{0},\\lambda_{1},\\lambda_{2},\\lambda_{3}\\right)$ (in which $\\lambda_{0}$ is 1 if the NT-Xent loss was used and 0 if it is not). The combinations we tried are as follow (with the best bolded): ", "page_idx": 16}, {"type": "text", "text": "\u2022 patient and volume group loss without NT-Xent loss: (0, 0.125, 0.875, 0), (0, 0.50, 0.50, 0)   \n\u2022 volume and slice group loss without NT-Xent loss: (0, 0, 0.125, 0.875), (0, 0, 0.50, 0.50)   \n\u2022 patient and volume group loss with NT-Xent loss: (1,0. 10 , 0.35, 0), (1, 0.117, 0.233, 0), (1, 0.05, 0.35, 0)   \n\u2022 patient, volume, and slice group loss without NT-Xent loss: (0, 0.05, 0.25, 0.7), (0, 0.33, 0.33, 0.33)   \n\u2022 patient, volume, and slice group loss with NT-Xent loss: (1, 0.10, 0.20, 0.05), (1, 0.05, 0.35, 0.025) ", "page_idx": 16}, {"type": "text", "text": "We utilized the best loss/weight combination for our ACDC experiments. For the CHAOS, MSCMR, and DAVIS datasets, because there is only one volume per patient and thus no difference between the patient and volume loss, we tested two loss/weight combinations: ", "page_idx": 16}, {"type": "text", "text": "\u2022 volume loss with weight 0.35 with NT-Xent loss \u2022 volume loss with weight 0.10 and slice loss with weight 0.30 with NT-Xent loss ", "page_idx": 16}, {"type": "text", "text": "Both weight combinations performed better than other comparison methods though volume loss with weight 0.35 with NT-Xent loss performed slightly better than the other combination. ", "page_idx": 16}, {"type": "text", "text": "For the pre-trained weights, we found that best results were obtained on the ACDC dataset with just the patient group without the NT-Xent loss. We tested different combinations of groups but found the results were worse. We used the same weight setting for CHAOS and DAVIS ", "page_idx": 16}, {"type": "text", "text": "B Theoretical bounds for loss ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "First, we assume that the expectation over the data distribution of the volume-based loss and slicebased loss are equivalent. Formally, this is described as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\nE_{\\mathbf{x},\\mathbf{y}\\sim p_{\\mathcal{Z}}}[\\mathcal{L}_{v o l u m e}(\\mathbf{y},\\hat{y})]=E_{\\mathbf{x}^{\\prime},\\mathbf{y}^{\\prime}\\sim p_{\\mathcal{Z}^{\\prime}}}[\\mathcal{L}_{s l i c e}(\\mathbf{y}^{\\prime},\\hat{y}^{\\prime})]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathcal{L}_{v o l u m e}:\\mathcal{X}\\times\\mathcal{Y}\\to\\mathbb{R}$ and $\\mathcal{L}_{s l i c e}:\\mathcal{X}^{\\prime}\\times\\mathcal{Y}^{\\prime}\\to\\mathbb{R}$ are the volume-based and slice-based loss respectively. For the rest of the proof, $\\mathcal{L}_{s l i c e}(\\cdot)$ is referred to by $\\mathcal{L}(\\cdot)$ . Following the derivation provided in the original Coreset paper (64) we have: ", "page_idx": 16}, {"type": "image", "img_path": "uyqjpycMbU/tmp/f98d58629e2adf99b14b1fe38c2b73a14aaee3e94755ba23e03e0864ee9de0ee.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "We get the last line of the inequality because we assume the Training Error is zero and, similar to the original Coreset paper, we assume the Generalization Error is zero, (which is a reasonable assumption because most CNNs have very small generalization error) (64). ", "page_idx": 17}, {"type": "text", "text": "Thus, our active learning objective can be re-defined as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{argmin}_{s^{1}\\subset D,|s^{1}|\\le k}\\;\\frac{1}{n}\\sum_{i\\in[n]}{\\mathcal{L}}(\\mathbf{y}^{\\prime},{\\hat{y}}^{\\prime})-\\frac{1}{|\\mathbf{s}|}\\sum_{j\\in\\mathbf{s}}{\\mathcal{L}}(\\mathbf{y}^{\\prime},{\\hat{y}}^{\\prime})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We will now present the following theorem: ", "page_idx": 17}, {"type": "text", "text": "Theorem 1. Given n i.i.d. samples drawn from $p_{\\mathcal{Z}^{\\prime}}$ as $\\{\\mathbf{x}_{i},\\mathbf{y}_{i}\\}_{i\\in[n]}$ , and set of points s. If loss function $\\mathcal{L}(\\hat{\\mathbf{y}},\\mathbf{y})$ is $\\lambda^{l}$ -Lipschitz continuous for all $\\hat{\\mathbf{y}}$ , y and bounded by $L,$ , segmentation function $\\eta_{\\mathbf{c}}(\\mathbf{x})=p(\\mathbf{\\dot{y}}=\\mathbf{c}|\\mathbf{x})$ is $\\lambda^{\\dot{\\eta}}$ -Lipshitz continuous for all $\\mathbf{x}\\in\\Omega$ and $\\mathbf{c}\\in\\mathcal{V}$ , s is $\\delta$ cover of $\\{\\mathbf{x}_{i},\\mathbf{\\dot{y}}_{i}\\}_{i\\in[n]}$ , and $l\\big(\\hat{\\mathbf{y}}_{s(j)},\\mathbf{y}_{s(j)}\\big)=0\\quad\\forall j\\in[m]$ ; with probability at least $1-\\gamma,$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i\\in[n]}\\mathcal{L}(\\mathbf{y}^{\\prime},\\hat{y}^{\\prime})-\\frac{1}{|\\mathbf{s}|}\\sum_{j\\in\\mathbf{s}}\\mathcal{L}(\\mathbf{y}^{\\prime},\\hat{y}^{\\prime})\\leq\\delta(\\lambda^{l}+\\lambda^{\\eta}L2^{h w d k})+\\sqrt{\\frac{L^{2}\\log(1/\\gamma)}{2n}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similar to (64), we can start our proof with bounding $E_{\\mathbf{y}_{i}\\sim\\eta(\\mathbf{x}_{i})}\\mathcal{L}(\\hat{\\mathbf{y}}_{i},\\mathbf{y}_{i})$ . Following a similar approach to (64), we get: ", "page_idx": 17}, {"type": "equation", "text": "$$\nE_{\\mathbf{y}_{i}\\sim\\eta(\\mathbf{x}_{i})}\\mathcal{L}(\\hat{\\mathbf{y}}_{i},\\mathbf{y}_{i})\\leq\\delta(\\boldsymbol{\\lambda}^{l}+\\boldsymbol{\\lambda}^{\\eta}L2^{h w d k})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Again, following (64), we can use the Hoeffding\u2019s Bound to conclude the rest of the proof. ", "page_idx": 17}, {"type": "text", "text": "C Batch sampler ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The pseudocode for the batch sampler can be seen in Algorithm 1. This implementation assumes the use of all three group contrastive losses. If the adjacent slice group is removed, then remove line 6. If the volume group is removed, then remove line 7. If the patient group is removed, then remove line 8. An illustration on how the batch sampler works can be seen in Figure 5 ", "page_idx": 17}, {"type": "image", "img_path": "uyqjpycMbU/tmp/1f43aa9f7267cb3586519e78d0de9f05374b21a1cb0b72759a57c564d58fd903.jpg", "img_caption": ["Figure 5: Overview of the batch sampler for Group-based Contrastive Learning "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Input: List of patient groups $\\overline{{P_{1},P_{2},...,P_{L}}}$ . Each patient group $i$ contains a list of volume   \ngroups $V_{1}^{i},V_{2}^{i},...,V_{K}^{i}$ . Each volume group $j$ for patient $i$ contains a list of slices $s_{1}^{i j},s_{2}^{i j},...,s_{T}^{i j}$ .   \nBatch size is $M$ .   \n1: $P G\\gets l i s t()$   \n2: for all $P_{1},P_{2},...,P_{L}$ do   \n3: $g r o u p s\\gets()$   \n4: for all $V_{1}^{i},\\tilde{V}_{2}^{i},...,V_{K}^{i}$   \n56:: 7: fo  all si1j , si2j , .. r $\\begin{array}{r l}&{\\textbf a\\mathbf u\\,s_{1}^{\\prime},s_{2}^{\\prime},...,s_{T}^{\\prime}\\,\\,\\mathbf q\\mathbf o}\\\\ &{s\\leftarrow R a n d o m C h o i c e(s_{k-1}^{i j},s_{k+1}^{i j})}\\\\ &{v\\leftarrow R a n d o m C h o i c e(\\cup_{t\\neq k}s_{t}^{i j})}\\\\ &{p\\leftarrow R a n d o m C h o i c e(\\cup_{V\\in P_{i}:s\\in V}s/\\{s_{j}^{i j}\\})}\\\\ &{g r o u p\\leftarrow[s_{k}^{i j},s,v,p]}\\end{array}$ $s_{1}^{i j},s_{2}^{i j},...,s_{T}^{i j}\\;{\\bf d e}$   \n8:   \n9:   \n10: Add group to groups   \n11: end for   \n12: end for   \n13: Add groups to $P G$   \n14: end for   \n15: batc $\\imath e s\\gets()$   \n16: while $|P G|\\geq M$ do   \n17: $b a t c h\\gets l i s t($ )   \n18: $S\\gets()$   \n19: while $|b a t c h|<M$ do   \n20: groups \u2190RandomChoice(\u222ag\u2208P G\u2227g \u2208/S)   \n21: group \u2190RandomChoice(groups)   \n22: $g r o u p s\\gets g r o u p s\\backslash g r o u p$   \n23: if $|g r o u p s|=0$ then   \n24: $P G\\leftarrow P G$ \\groups   \n25: else   \n26: Add groups to $S$   \n27: end if   \n28: Extend batch with group   \n29: end while   \n30: Add batch to batches   \n31: end while   \n32: return batches ", "page_idx": 18}, {"type": "text", "text": "D Additional results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We have included all collected results on all the datasets. We also report twice the bootstrap standard error for the means over the different experiment runs. Our process is as follows. We generate 1000 bootstraps. For each bootstrap, we generate a resampling with replacement of the test set volumes and generate the mean of the test set metrics (average 3D Dice score over the test set) over the different experiment runs at the desired evaluation point. We then calculate the sample standard deviation of the bootstrapped means and multiply by two. ", "page_idx": 18}, {"type": "text", "text": "Because the CHAOS and DAVIS have fewer volumes (and the bootstrapping errors are artifically inflated), rather generating a resampling with replacement on the test set volumes, we generate a resampling over the slices. ", "page_idx": 18}, {"type": "table", "img_path": "uyqjpycMbU/tmp/6a6f9bafbfe1c790d833d60428c9e5e3f24055037bfa1226b56a8d78b8ca96e6.jpg", "table_caption": ["Table 5: DICE scores for ACDC "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "uyqjpycMbU/tmp/bffa6206bed20d6eee9badf4fc30086f3c530041e261ff198916c68cc9e41995.jpg", "table_caption": ["Table 6: DICE scores for MSCMR "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "uyqjpycMbU/tmp/20678cb79c877164092937f481d6cd1ff674daa310dbd384f12c794e1986b6ed.jpg", "table_caption": ["Table 7: DICE scores for CHAOS "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "uyqjpycMbU/tmp/f68126f2c1418d3c1b65bcbb2375a82f2db3c2ed1a9f3f0ea56bac3e102e7451.jpg", "table_caption": ["Table 8: DICE scores for DAVIS "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "uyqjpycMbU/tmp/6369b2057fc406d4c9294eb76384e2e618ded7ba1d91ceff6eb7fd2b0493f914.jpg", "table_caption": ["Table 9: DICE scores for ACDC (pretrained) "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "uyqjpycMbU/tmp/b06a74a81f969fccb59ad7798ec9deb2a6e943627c8a1fc69b06f7c21e2277cf.jpg", "table_caption": ["Table 10: DICE scores for CHAOS (pretrained) "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "uyqjpycMbU/tmp/42248a34003587af392e3e6b84a4b2669e6dae84d248a979ff8ac38b79cea77f.jpg", "table_caption": ["Table 11: DICE scores for DAVIS (pretrained) "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Yes they do. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Experimental limitations are discussed in Section 5. Computational requirements are discussed in Section 4.2. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide a full set of assumptions and complete proof in Appendix B. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All implementation and experiment details are discussed in Section 3.2.1, ??, and 4.2 and Appendix. We have also provided our code in the supplementary materials and the data is publicly available. We will also post our code to Github. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have provided code and links to data in the supplementary materials. The code will also be posted to Github. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Yes, this information is provided in Section 3.2.1 and 4.2, Appendix, and can also be viewed in the code in the supplementary materials. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Yes, we report bootstrapping standard errors for our results and describe our approach in Appendix D. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "We discuss this in Section 4.2. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Because we used existing datasets with human subjects, in Section 4.1 we discussed whether the dataset owners followed data collection regulations and whether human consent was obtained. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: While positive social impacts are discussed throughout the paper (especially in the introduction), we also briefly discussed that we did not consider issues of bias which can have unfair outcomes for underrepresented groups in Section 5. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our models do not have a high risk of misuse. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: For the datasets we used, no license information was available and we were unable to reach out to the asset creators. However, we have properly credited the original owners of all the datasets and followed all the owners\u2019 directions for research use of the datasets. The license and terms of use for the segmentation model repository are both followed and provided within the wsl4mis subdirectory in the code submitted in the supplementary materials. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The only assets we are providing is our code, which will be anonymized and provided in the supplementary materials. The license information as well as documentation on how to use the code is provided in the code directory. Other relevant details can also be referenced in the paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We used existing datasets that contain human subject data. As mentioned before, in Section 4.1 we discussed whether the dataset owners followed data collection regulations and whether human consent was obtained. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We used existing datasets that contain human subject data. While we were not able to obtain all of this information, as mentioned before in Section 4.1 we discussed whether the dataset owners followed data collection regulations and whether human consent was obtained. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]