[{"figure_path": "A0HSmrwtLH/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "This figure illustrates the problem setup and the contributions of the paper.  A fixed model (encoder and classifier) receives an image as input.  The embedding from the encoder is then projected onto a set of user-specified semantic concepts. The paper introduces novel statistical methods (testing by betting) to rank the importance of these concepts for the model's predictions, providing a measure of statistical significance.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_7_1.jpg", "caption": "Figure 2: Importance results with CLIP:ViT-L/14 on 2 classes in the AwA2 dataset. Concepts are annotated with (p) if they are present in the class, or with (a) otherwise.", "description": "This figure visualizes the results of applying the SKIT and C-SKIT methods to the AwA2 dataset using the CLIP:ViT-L/14 model.  It shows the ranked importance of different semantic concepts for predicting two animal classes.  Concepts marked with (p) are present in the class according to human annotations, while those marked with (a) are absent. The figure likely illustrates the effectiveness of the proposed methods in identifying semantically relevant concepts for model predictions, highlighting their ability to distinguish between globally and conditionally important concepts.", "section": "4.1 AwA2 Dataset"}, {"figure_path": "A0HSmrwtLH/figures/figures_7_2.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "This figure illustrates the problem setup and contribution of the paper. It shows a fixed model composed of an encoder and a classifier, which is probed via a set of semantic concepts. The key difference with post-hoc concept bottleneck models is that the authors do not train a sparse linear layer to approximate E[Y|Z]. Instead, they focus on characterizing the dependence structure between \u0176 and Z. The figure also illustrates the main contributions of the paper, which includes a rank of importance and a testing by betting procedure for assessing the statistical significance of the results.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_8_1.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "The figure illustrates the problem setup and the contributions of the paper.  It shows an image X as input to a fixed model, consisting of an encoder f and classifier g. The encoder produces an embedding h which is then used by the classifier g to produce a prediction \u0177.  A set of concepts c is used to probe the model. The main contributions of the paper are shown as the output: a rank of semantic importance and statistical testing results for each concept.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_9_1.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "The figure illustrates the problem setup and contribution of the paper. It shows a fixed model composed of an encoder and classifier, which is probed via a set of concepts. The key difference with post-hoc concept bottleneck models is that the authors do not train a sparse linear layer to approximate E[Y|Z]. Instead, they focus on characterizing the dependence structure between \u0176 and Z. The figure highlights the key components of the proposed methodology: a fixed model, concepts, and the testing by betting strategy for determining the statistical importance of semantic concepts.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_23_1.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "This figure shows a schematic of the proposed method. It starts with an image X, which is encoded into an embedding h by an encoder f. This embedding is then projected into the space of semantic concepts, Z, by means of a concept bottleneck model (CBM). The concepts Z and the embedding h are used to predict the response of the model, Y, by a classifier g. Our contribution is to introduce a rank of statistical importance of concepts for the predictions of the model.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_24_1.jpg", "caption": "Figure 2: Importance results with CLIP:ViT-L/14 on 2 classes in the AwA2 dataset. Concepts are annotated with (p) if they are present in the class, or with (a) otherwise.", "description": "This figure shows the results of applying the proposed methods (SKIT and C-SKIT) to the AwA2 dataset. Specifically, it displays the importance scores for concepts related to two animal classes. Each bar represents a concept, with height indicating the importance score.  Concepts are labeled with (p) for presence in the class and (a) for absence. The figure demonstrates the ranking of concept importance produced by the SKIT and C-SKIT methods. The (a) subplot shows the global importance scores calculated by SKIT, while the (b) subplot presents the global conditional importance scores obtained by C-SKIT. This visualization helps compare the importance of different concepts in determining model predictions for the specified animal classes.", "section": "4.1 AwA2 Dataset"}, {"figure_path": "A0HSmrwtLH/figures/figures_24_2.jpg", "caption": "Figure 2: Importance results with CLIP:ViT-L/14 on 2 classes in the AwA2 dataset. Concepts are annotated with (p) if they are present in the class, or with (a) otherwise.", "description": "This figure shows the results of applying the SKIT and C-SKIT methods to the AwA2 dataset, focusing on two classes.  The left panel (a) displays global importance scores from SKIT, showing the rejection rates and times for various concepts. The right panel (b) illustrates global conditional importance scores from C-SKIT, again displaying rejection rates and times. Each bar represents a concept, and the (p) or (a) annotation indicates if the concept is present or absent in the class, respectively. The figure helps visualize the relative importance of different semantic concepts in classifying animal images within the AwA2 dataset.", "section": "4.1 AwA2 Dataset"}, {"figure_path": "A0HSmrwtLH/figures/figures_25_1.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "This figure illustrates the problem setup and the contributions of the paper.  A fixed model (encoder f and classifier g) takes an image X as input and produces a prediction \u0177.  The model's embedding h is projected onto a set of concepts c, resulting in a vector z representing the concepts' presence in the input. The paper introduces novel methods to rank the importance of these concepts (z) for the predictions \u0177, providing a statistically rigorous approach to semantic importance.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_26_1.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "This figure illustrates the problem setup and contribution of the paper.  A fixed model (encoder and classifier) receives an image as input. The model's embedding is projected onto a set of user-specified semantic concepts.  The paper introduces novel methods to assess the statistical importance of these concepts for model predictions, quantifying semantic importance via a betting approach and ranking of importance.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_28_1.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "This figure illustrates the problem setup and the contribution of the paper.  A fixed model (encoder f and classifier g) takes an image X as input and produces a prediction \u0177.  The embedding h from the encoder is projected onto a set of concepts c to obtain z. The authors' contribution is a method for ranking the importance of these semantic concepts (z) for the prediction, using statistical testing.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_28_2.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "This figure illustrates the problem setup and the contributions of the paper. A fixed model, composed of an encoder f and a classifier g, is probed via a set of concepts c.  The key difference with post-hoc concept bottleneck models (PCBMs) is highlighted: this work does not train a sparse linear layer to approximate E[Y|Z]; rather it focuses on characterizing the dependence structure between \u0176 and Z.  The figure shows how the model's prediction (\u0177) depends on the input image (X), embedding (h), classifier (g), and the concepts (c) which are projected to induce (z). The contributions of the paper are also shown, which include introducing a rank of statistical importance via testing by betting.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_29_1.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "This figure illustrates the problem setup and the contributions of the paper.  A fixed model (encoder and classifier) processes an image and produces a prediction. The model's embedding is projected onto interpretable semantic concepts (e.g., \"whiskers\", \"piano\"). The paper introduces methods to test the statistical importance of these concepts for the predictions, and produces a ranked list of importance scores.  The key novelty is the use of sequential testing procedures, which provides statistical guarantees and induces an importance ranking.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_30_1.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "This figure illustrates the key problem addressed in the paper: testing the statistical importance of semantic concepts (e.g., \"whiskers\", \"cute\") for the predictions of a black-box model.  The figure shows an image as input to an encoder and classifier, which produces a prediction.  Importantly, there is a concept layer that projects the embedding of the image onto interpretable semantic concepts. The authors' contributions focus on developing new sequential statistical tests to rank these concepts by importance and provide precise statistical guarantees on these findings.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_31_1.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "This figure illustrates the problem setup and the contribution of the paper.  A fixed model (encoder and classifier) receives an image as input. The model's embedding is projected onto a set of user-specified semantic concepts. The authors' contributions involve methods for ranking the importance of these concepts based on statistical significance tests, using testing by betting principles.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_32_1.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "This figure illustrates the workflow of the proposed methodology. An image is encoded into an embedding, which is projected onto a user-specified set of semantic concepts. The proposed method leverages these projections to test for the statistical importance of each concept via betting, thereby inducing a rank of importance.  This differs from existing methods that rely on training surrogate models to obtain semantic importance.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_32_2.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "This figure illustrates the problem setup and the contributions of the paper.  A fixed model (encoder and classifier) receives an image as input. This model's output is then analyzed with respect to a set of user-specified semantic concepts (represented as vectors). The paper's contributions involve novel statistical tests that assess the importance of these concepts in influencing the model's predictions and produce a ranking of importance, improving upon the transparency and rigor of existing methods.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_33_1.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "The figure illustrates the overall workflow of the proposed method.  It starts with an image X, which is processed by a fixed model (encoder f and classifier g) to produce a prediction \u0177.  A set of concepts C is defined which projects the embedding h onto an interpretable semantic space. The authors' contributions focus on using these concepts (Z) to test for semantic importance of specific concepts by applying statistical tests (Testing by Betting). The result is a rank of importance of concepts.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_34_1.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "This figure illustrates the problem setup and the contribution of the paper. It shows a fixed model composed of an encoder and a classifier that processes an image to produce a prediction. The model is probed with a set of semantic concepts, and the paper proposes novel statistical tests to assess the importance of these concepts for the predictions.  The key difference with existing methods is highlighted: the proposed method focuses on characterizing the dependence structure between the prediction and the concepts without training a surrogate model, providing more direct and reliable insights into the original model's behavior.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_35_1.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "This figure shows a schematic of the overall approach. An image is inputted into a fixed model composed of an encoder and classifier. The encoder generates an embedding which is then used by the classifier to produce a prediction. A set of concepts are provided as input to a layer that projects the embedding onto the subspace of interpretable semantic concepts. The contribution of this paper is to test the statistical importance of these concepts for the predictions of the model, and to induce a ranking of concepts using testing by betting. This is a key difference with previous methods based on concept bottleneck models, in that we do not train a surrogate model. ", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_36_1.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "This figure illustrates the overall workflow of the proposed method.  It shows an image X as input to a fixed model (encoder f and classifier g). The embedding h is projected into a space of semantic concepts C, resulting in the projection z.  The authors' contributions involve using sequential testing to determine a rank of importance of these concepts for the model's prediction \u0177, which aims to provide statistical guarantees on semantic importance.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_37_1.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "The figure illustrates the problem setup and the contribution of the paper. It shows a fixed model composed of an encoder and classifier. The encoder takes an image as input and produces a dense embedding, which is then fed into the classifier to produce a prediction.  A set of concepts is provided as input to the model. The contribution of the paper is to provide a method for ranking the importance of these concepts for the predictions of the model, along with statistical guarantees on the reliability of the ranking.  The concepts themselves are interpretable and meaningful to the user. This is in contrast to other methods that focus on feature importance in the input space, without explicit consideration of the interpretable semantics.  The process of ranking concept importance is presented as a flow, culminating in a list ordered by importance.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_38_1.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "This figure illustrates the problem setup and the contributions of the paper.  A fixed model (encoder and classifier) takes an image as input. The embedding is projected onto a user-specified set of semantic concepts. The paper introduces novel procedures to test for statistical importance, allowing for the ranking of concepts by importance.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_39_1.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "This figure illustrates the problem setup and the contribution of the paper.  It shows a fixed model (encoder and classifier) receiving an image as input. The image embedding is projected onto a set of user-specified semantic concepts. The paper's contribution is a novel procedure for ranking concepts by their importance in the model predictions, using a testing by betting approach.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_39_2.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "This figure illustrates the overall workflow of the proposed method.  It shows an image X as input, which is processed by a fixed model (encoder f and classifier g) to produce a prediction \u0177. A set of concepts C is used to probe the model's decision-making process, resulting in a vector representation z. The authors' contribution lies in providing a statistical testing framework ('Testing by Betting') to determine the importance of these concepts (ranking and statistical significance) for the model's prediction, with the final result being a ranked list of concepts according to their importance.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_40_1.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "This figure shows the overall architecture of the proposed approach. An image is input to an encoder that generates an embedding vector. The embedding is projected to a set of semantic concepts using a concept bottleneck model. The model outputs a prediction, which is compared to the actual label. This allows the model to assess the importance of different semantic concepts using a testing by betting methodology.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_41_1.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "This figure illustrates the problem setup and contributions of the paper.  It shows a fixed model (an encoder and a classifier) receiving an image as input. The image embedding is projected onto a set of semantic concepts. The paper's contributions are to provide methods to assess the statistical importance of these concepts for the model's predictions and to rank those concepts according to their importance.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_42_1.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "This figure illustrates the overall workflow of the proposed method.  It starts with an image as input, which is processed by a fixed model (encoder and classifier) to produce a prediction. This prediction is then analyzed with respect to a set of user-defined semantic concepts. The core contribution of the paper is a novel sequential testing procedure to assess the statistical importance of these concepts, providing a ranked list of importance and statistical guarantees on the findings.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_43_1.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "This figure illustrates the problem setup and the contributions of the paper.  It shows a fixed model (encoder and classifier) taking an image as input.  The image's embedding is projected onto a set of user-specified semantic concepts. The paper's contribution is a novel framework for statistically testing the importance of these concepts in the model's prediction, using sequential testing principles to determine a ranking of importance.", "section": "3 Testing Semantic Importance via Betting"}, {"figure_path": "A0HSmrwtLH/figures/figures_43_2.jpg", "caption": "Figure 1: Overview of the problem setup and our contribution.", "description": "This figure illustrates the key components of the proposed methodology.  It shows the input image, the encoder (f) that generates an embedding, the classifier (g) that produces a prediction, and the set of concepts (c) which are used to assess semantic importance in the model. The figure highlights the main contribution of the paper: providing a ranked list of statistical importance for the selected concepts through a testing-by-betting approach. This is in contrast to existing methods which may not provide statistical guarantees or induce a ranking.", "section": "3 Testing Semantic Importance via Betting"}]