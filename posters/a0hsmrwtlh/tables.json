[{"figure_path": "A0HSmrwtLH/tables/tables_5_1.jpg", "caption": "Table 1: Summary of results for each dataset. Metrics are reported as average across all VL models used in the experiments. See main text for details about the models and the metrics used.", "description": "This table summarizes the performance of the proposed methods (SKIT, C-SKIT, and X-SKIT) and a baseline method (PCBM) on three image datasets (AwA2, CUB, and Imagenette).  For each dataset, the table shows the original model accuracy, rank agreement (how well the ranking of importance aligns across different models), and F1 score (a measure of classification accuracy for the importance rankings) for each method.  The table highlights the effectiveness of the proposed methods in achieving high accuracy and consistent importance rankings across different vision-language models.", "section": "4 Results"}, {"figure_path": "A0HSmrwtLH/tables/tables_7_1.jpg", "caption": "Table 1: Summary of results for each dataset. Metrics are reported as average across all VL models used in the experiments. See main text for details about the models and the metrics used.", "description": "This table summarizes the performance of three proposed methods (SKIT, C-SKIT, and X-SKIT) and a baseline method (PCBM) on three image datasets (Imagenette, AwA2, and CUB).  For each dataset and method, the table shows the model accuracy, rank agreement (a measure of how well the methods agree on the ranking of important concepts), and F1 score (a measure of the accuracy of identifying important concepts). The results are averaged across multiple vision-language models.  The checkmarks in the \"Original model\" column indicate whether the method can be applied directly to the original model or requires training a surrogate model.  The 'X' indicates that PCBM requires training a surrogate model.", "section": "4 Results"}, {"figure_path": "A0HSmrwtLH/tables/tables_17_1.jpg", "caption": "Table 1: Summary of results for each dataset. Metrics are reported as average across all VL models used in the experiments. See main text for details about the models and the metrics used.", "description": "This table summarizes the performance of three different semantic importance testing methods (SKIT, C-SKIT, and X-SKIT) and compares them to a baseline method (PCBM) across three different datasets (AwA2, CUB, and Imagenette).  The metrics reported are the average accuracy, rank agreement (how well the rank of importance aligns across different models), and the F1 score (measuring the accuracy of the top-10 concepts identified). The table shows that SKIT, C-SKIT and X-SKIT generally perform better than the baseline PCBM in terms of rank agreement and F1-score.", "section": "4 Results"}, {"figure_path": "A0HSmrwtLH/tables/tables_32_1.jpg", "caption": "Table 1: Summary of results for each dataset. Metrics are reported as average across all VL models used in the experiments. See main text for details about the models and the metrics used.", "description": "This table summarizes the performance of different methods (SKIT, C-SKIT, X-SKIT, and PCBM) on three image classification datasets (Imagenette, AwA2, and CUB).  For each dataset and method, it shows the accuracy, rank agreement, and F1 score.  Rank agreement measures how well the ranking of semantic concepts by each method matches across different vision-language models.  The F1 score reflects how well the identified important concepts align with ground truth annotations (where available). The table provides a high-level overview of the comparative performance of the proposed and existing methods.", "section": "4 Results"}, {"figure_path": "A0HSmrwtLH/tables/tables_33_1.jpg", "caption": "Table 1: Summary of results for each dataset. Metrics are reported as average across all VL models used in the experiments. See main text for details about the models and the metrics used.", "description": "This table summarizes the performance of three different methods (SKIT, C-SKIT, and X-SKIT) and a baseline method (PCBM) on three different image datasets (Imagenette, AwA2, and CUB).  For each dataset and method, the table shows the accuracy, rank agreement, and F1 score.  Rank agreement and F1 scores measure the similarity of the rankings generated by each method compared to ground truth or other methods.  Higher scores indicate better performance.", "section": "4 Results"}, {"figure_path": "A0HSmrwtLH/tables/tables_35_1.jpg", "caption": "Table 1: Summary of results for each dataset. Metrics are reported as average across all VL models used in the experiments. See main text for details about the models and the metrics used.", "description": "This table summarizes the performance of different methods (SKIT, C-SKIT, X-SKIT, and PCBM) on three image datasets (Imagenette, AwA2, and CUB).  For each dataset, it reports the average accuracy, rank agreement, and F1 score across multiple vision-language models. Rank agreement measures the consistency of the ranking of important concepts across different models, while the F1 score assesses the accuracy of identifying important concepts. The table shows the overall performance of each method on each dataset, allowing for comparisons between them.", "section": "4 Results"}, {"figure_path": "A0HSmrwtLH/tables/tables_35_2.jpg", "caption": "Table 1: Summary of results for each dataset. Metrics are reported as average across all VL models used in the experiments. See main text for details about the models and the metrics used.", "description": "This table summarizes the performance of the proposed methods (SKIT, C-SKIT, and X-SKIT) and a baseline method (PCBM) on three image datasets (AwA2, CUB, and Imagenette).  For each dataset and method, it shows the accuracy, rank agreement (how well the importance ranking of concepts aligns across multiple vision-language models), and an f1 score evaluating the importance rankings against ground truth (where available). The table highlights the superior performance of the proposed methods compared to the PCBM baseline.", "section": "4 Results"}, {"figure_path": "A0HSmrwtLH/tables/tables_38_1.jpg", "caption": "Table 1: Summary of results for each dataset. Metrics are reported as average across all VL models used in the experiments. See main text for details about the models and the metrics used.", "description": "This table summarizes the performance of different methods (SKIT, C-SKIT, X-SKIT, and PCBM) on three image datasets (AwA2, CUB, and Imagenette).  For each dataset, it shows the model's accuracy, rank agreement, and F1 score. Rank agreement measures how well the importance ranks produced by each method align across different vision-language models. The F1 score evaluates the accuracy of the importance rankings compared to ground truth annotations (where available).", "section": "4 Results"}]