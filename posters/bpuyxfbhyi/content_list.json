[{"type": "text", "text": "Hybrid Reinforcement Learning Breaks Sample Size Barriers in Linear MDPs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kevin Tan, Wei Fan, Yuting Wei Department of Statistics and Data Science The Wharton School, University of Pennsylvania ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hybrid Reinforcement Learning (RL), where an agent learns from both an offline dataset and online explorations in an unknown environment, has garnered significant recent interest. A crucial question posed by Xie et al. (2022b) is whether hybrid RL can improve upon the existing lower bounds established for purely offline or online RL without requiring that the behavior policy visit every state and action the optimal policy does. While Li et al. (2023b) provided an affirmative answer for tabular PAC RL, the question remains unsettled for both the regretminimizing and non-tabular cases. In this work, building upon recent advancements in offline RL and reward-agnostic exploration, we develop computationally efficient algorithms for both PAC and regret-minimizing RL with linear function approximation, without requiring concentrability on the entire state-action space. We demonstrate that these algorithms achieve sharper error or regret bounds that are no worse than, and can improve on, the optimal sample complexity in offline RL (the first algorithm, for PAC RL) and online RL (the second algorithm, for regret-minimizing RL) in linear Markov decision processes (MDPs), regardless of the quality of the behavior policy. To our knowledge, this work establishes the tightest theoretical guarantees currently available for hybrid RL in linear MDPs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning (RL) holds great promise in attaining reliable decision-making in adaptive environments for a broad range of modern applications. Typical RL algorithms often require an enormous number of training samples, motivating a line of recent efforts to study the sample efficiency of RL algorithms. There are two mainstream paradigms of RL, distinguished by how samples are collected: online RL and offline RL. In online RL, an agent learns in a real-time manner, exploring the environment to maximize her cumulative rewards by executing a sequence of adaptively chosen policies (e.g. Azar et al. (2017); Jin et al. (2018); Sutton and Barto (2018); Zhang et al. (2023)). Whereas, in offline RL, an agent has only access to a pre-collected dataset, and tries to figure out how to perform well in a different environment without ever experiencing it (e.g. Jin et al. (2021b); Lange et al. (2012); Levine et al. (2020); Li et al. (2024)). Online methods are often sample-hungry, but offline methods often impose stringent requirements on the quality of the pre-collected data. ", "page_idx": 0}, {"type": "text", "text": "To address the limitations of both, the setting of hybrid RL (Song et al., 2023; Xie et al., 2022b) has recently received considerable attention from both theoretical and practical perspectives (Amortila et al., 2024; Ball et al., 2023; Kausik et al., 2024; Li et al., 2023b; Nair et al., 2020; Nakamoto et al., 2023; Song et al., 2023; Tan and Xu, 2024; Vecerik et al., 2017; Wagenmaker and Pacchiano, 2023; Zhou et al., 2023). In hybrid RL, an agent learns from a combination of both offline and online data, extracting information from offline data to enhance online exploration. Theoretical guarantees for hybrid RL algorithms can be categorized on: (1) the type of function approximation considered, (2) the level of coverage required by the behavior policy, (3) whether it improves on the minimax lower bounds for online-only and offline-only learning, and (4) whether they minimize regret or obtain a PAC guarantee. We elaborate below, and summarize the prior art in Table 1. ", "page_idx": 0}, {"type": "table", "img_path": "bPuYxFBHyI/tmp/512ddea3dd7aae8c9764f956084fc25de2dbaa6e9d5cd8ced56ddadc53c4621f.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison of our contributions to previous work in hybrid RL. "], "page_idx": 1}, {"type": "text", "text": "While much of the prior literature (Amortila et al., 2024; Nakamoto et al., 2023; Song et al., 2023; Tan and Xu, 2024; Zhou et al., 2023) tackles general function approximation in hybrid RL, they either require stringent concentrability assumptions on behavior policy quality, or fail to obtain tight theoretical guarantees. Under such single-policy concentrability assumptions (explained below), Xie et al. (2022b) show the optimal RL algorithm is either a purely offline reduction or a purely online algorithm if the agent can choose the ratio of offline to online samples, rendering the benefits of hybrid RL questionable. Without this assumption, Li et al. (2023b) show guarantees for PAC RL that improve over lower bounds for offline-only and online-only RL, but only for tabular MDPs. ", "page_idx": 1}, {"type": "text", "text": "This paper focuses on obtaining sharper theoretical guarantees in the setting of linear function approximation in linear MDPs. First proposed in Jin et al. (2019); Yang and Wang (2019), linear MDPs parameterize the transition probability kernel and reward function by linear functions of known features (e.g. pre-trained neural embeddings). It has been extensively studied due to its benefits in dimension reduction and mathematical tractability in both the online and offline settings (Du et al., 2019; Duan and Wang, 2020; He et al., 2023; Hu et al., 2023; Jin et al., 2019; Li et al., 2021; Min et al., 2021; Qiao and Wang, 2022; Xiong et al., 2023; Yang and Wang, 2019; Yin et al., 2022; Zanette et al., 2021). Despite these efforts, hybrid RL algorithms for linear MDPs (Amortila et al., 2024; Nakamoto et al., 2023; Song et al., 2023; Tan and Xu, 2024; Wagenmaker and Pacchiano, 2023) have suboptimal worst-case guarantees (Table 2), which raises the question: ", "page_idx": 1}, {"type": "text", "text": "Is it possible to develop sample efficient RL algorithms in the setting of hybrid RL that are provably better than online-only and offline-only algorithms for linear MDPs? ", "page_idx": 1}, {"type": "text", "text": "1.1 Hybrid RL: two approaches ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To answer the question above, we introduce two types of approaches widely-adopted in hybrid RL. ", "page_idx": 1}, {"type": "text", "text": "The offline-to-online approach: Most of the current literature (e.g. Amortila et al. (2024); Nakamoto et al. (2023); Song et al. (2023); Tan and Xu (2024)) initializes the online dataset with offline samples to perform regret-minimizing online RL. We refer to this as the offline-to-online approach. This method is simple and natural, and as the algorithm optimizes the reward during each online episode, it is suitable when the agent has to perform well during online exploration. ", "page_idx": 1}, {"type": "text", "text": "The online-to-offline approach: However, if our goal is to output a near-optimal policy, especially in real-world situations in medicine and defense, randomizing between policies can be suboptimal or even unethical. Recently, Wagenmaker and Pacchiano (2023) and Li et al. (2023b) propose using reward-agnostic online exploration to explore parts of the state space unseen by the behavior policy, to construct a dataset that is especially amenable to leverage the sharp performance guarantees of offline RL. We refer to this as the online-to-offline approach. While this approach does not optimize the \u201ctrue reward\u201d during online exploration, it avoids the need to deploy mixed policies to achieve a PAC bound, allowing for the deployment of fixed, and thus more interpretable, policies. ", "page_idx": 1}, {"type": "text", "text": "1.2 Our contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 We propose an online-to-offline method called Reward-Agnostic Pessimistic PAC Exploration-initialized Learning (RAPPEL) in Algorithm 1. It employs reward-agnostic online exploration to enhance the offline dataset, then learns a policy through a pessimistic offline RL algorithm. Algorithm 1 significantly improves upon the sample complexity of the only dedicated hybrid RL algorithm for linear MDPs (Wagenmaker and Pacchiano, 2023) by a factor of at least $H^{3}$ . This performs no worse than the offline-only minimaxoptimal error bound from Xiong et al. (2023), with the potential of significant gains from online data. This is the first work to explore the online-to-offline approach in linear MDPs. \u2022 In addition, we propose an offline-to-online method called Hybrid Regression for UpperConfidence Reinforcement Learning (HYRULE) in Algorithm 2, where one warm-starts an online RL algorithm with parameters estimated from offline data. In addition to improving the ambient dimension dependence, this algorithm enjoys a regret (or sample complexity) bound that is no worse than the online-only minimax optimal bound, with the potential of significant gains if the offline dataset is of high quality (Agarwal et al., 2022; He et al., 2023; Hu et al., 2023; Zhou et al., 2021). Our result demonstrates the provable benefits of hybrid RL in scenarios where offline samples are much cheaper or much easier to acquire. ", "page_idx": 1}, {"type": "table", "img_path": "bPuYxFBHyI/tmp/40cb9739e65c5676e771ea9ef8baaf840596d8d4e04a14eda0eefd7dcf794b39.jpg", "table_caption": [], "table_footnote": ["Table 2: Comparisons of our results to the best upper and lower bounds for offline and online RL, and existing results for hybrid RL, in linear MDPs. Often, offline data is cheaper or easier to obtain. When this happens, $N_{\\mathrm{off}}\\gg N_{\\mathrm{on}}$ , and the online term in our results (depending on $N_{\\mathrm{on}}$ ) dominates. "], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "To the best of our knowledge, we are the first to show improvements over the aforementioned lower bounds of hybrid RL algorithms (in the same vein as Li et al. (2023b)) in the presence of function approximation, without any explicit requirements on the quality of the behavior policy, and with both the offline-to-online and online-to-offline approaches. Our results are also, at the point of writing, the best bounds available in the literature for hybrid RL in linear MDPs (see Table 2). ", "page_idx": 2}, {"type": "text", "text": "Technical contributions. In this work, we build on recent advancements in offline and online RL, demonstrating that intuitive modifications suffice to achieve state-of-the-art sample complexity for hybrid RL in linear MDPs. At a high level, our sample efficiency gains are achieved by decomposing the error of interest into offline and online partitions, and optimizing them respectively, following the same idea in Tan and $\\mathrm{Xu}$ (2024). Below, we summarize our specific technical contributions. ", "page_idx": 2}, {"type": "text", "text": "1. We sharpen the dimensional dependence from $d$ to $d_{\\mathrm{on}}$ and $c_{\\mathrm{off}}(\\mathcal X_{\\mathrm{off}})$ via projections onto those partitions. The former is accomplished in Algorithm 1 by Kiefer-Wolfowitz in Lemma 1, and in Algorithm 2 by proving a sharper variant of Lemma B.1 from Zhou and Gu (2022) in Lemma 18, using this in Lemma 14 to reduce the dimensional dependence in the summation of bonuses, which helps achieve the desired result.   \n2. We maintain a $H^{3}$ dependence for the error or regret for both algorithms, which is nontrivial, in Algorithm 1 and for the offline partition in Algorithm 2 by combining the total variance lemma with a novel truncation argument for \u201cbad\u201d trajectories in Lemma 17. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Basics of Markov decision processes ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "An episodic MDP is a tuple $\\mathcal{M}\\,=\\,\\left(\\mathcal{S},\\mathcal{A},H,(\\mathbb{P}_{h})_{h=1}^{H},(r_{h})_{h=1}^{H}\\right)$ , where $\\boldsymbol{S}$ is the state space, $\\boldsymbol{\\mathcal{A}}$ the action space, $H$ the horizon, $(\\mathbb{P}_{h})_{h=1}^{H}$ the collection of transition\u02d8 probability kernels $\\mathbb{P}_{h}:S\\times A\\rightarrow$ $\\Delta(S)$ , and $(r_{h})_{h=1}^{H}$ the collection of reward functions $r_{h}:S\\times\\mathcal{A}\\to[0,1]$ . $\\Delta(\\cdot)$ is the collection of distributions over a set. At each $h\\in[H]=\\{1,...,H\\}$ , an agent observes the current state $s_{h}\\in\\mathcal{S}$ , takes an action $a_{h}\\in\\mathcal{A}$ according to $\\pi_{h}:{\\mathcal{S}}\\to\\Delta(A)$ , and observes the reward $r_{h}$ and next state $s_{h+1}\\sim\\mathbb{P}_{h}(\\cdot\\mid s_{h},a_{h})$ . We write $\\Pi$ for the set of policies $\\pi=\\{\\pi_{h}\\}_{h=1}^{H}$ , with value and $\\mathrm{{Q}}.$ -functions ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{r}\\operatorname{every}\\,(s,h)\\in\\mathcal{S}\\times[H]:\\;\\;V_{h}^{\\pi}(s):=\\mathbb{E}_{\\pi}[\\sum_{h^{\\prime}=h}^{H}r_{h^{\\prime}}|s_{h}=s],}\\\\ &{(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[H]:\\;\\;Q_{h}^{\\pi}(s,a):=\\mathbb{E}_{\\pi}[\\sum_{h^{\\prime}=h}^{H}r_{h^{\\prime}}|s_{h}=s,a_{h}=a].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "$\\pi^{*}~=~\\{\\pi^{*}\\}_{h=1}^{H}$ is the optimal policy attaining the highest value and Q-functions, and we write $V^{\\ast}=\\{V_{h}^{\\ast}\\}_{h=1}^{H}$ and $Q^{*}=\\{Q_{h}^{*}\\}_{h=1}^{H}$ for the optimal value and Q-functions. We consider the setting of hybrid RL, where an agent has access to two sources of data: ", "page_idx": 2}, {"type": "text", "text": "\u2022 $N_{\\mathrm{off}}$ independent episodes of length $H$ collected by a behavior policy $\\pi_{b}$ where the $n$ -th sample trajectory is a sequence of data $(s_{1}^{(n)},a_{1}^{(n)},\\bar{r_{1}^{(n)}},...,s_{H}^{(n)},\\bar{a_{H}^{(n)}},\\bar{r_{H}^{(n)}},s_{H+1}^{(n)})$ ; \u2022 $N_{\\mathrm{on}}$ sequential episodes of online data, where at each episode $n=1,...,N_{\\mathrm{on}}$ , the algorithm has knowledge of the $N_{\\mathrm{off}}$ offline episodes and the previous online episodes $1,...,n-1$ . ", "page_idx": 3}, {"type": "text", "text": "The quality of the behavior policy $\\pi_{b}$ is measured by the all-policy and single-policy concentrability coefficients proposed by Xie et al. (2023); Zhan et al. (2022): ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Occupancy Measure). For a policy $\\pi~=~\\{\\pi_{h}\\}_{h=1}^{H}$ , its occupancy measure $d^{\\pi}\\ =$ $\\{d_{h}^{\\pi}\\}_{h=1}^{H}$ corresponds to the collection of distributions over states and actions induced by running $\\pi$ within $\\mathcal{M}$ , where for some initial distribution $\\rho$ and $s_{1}\\sim\\rho_{!}$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\nd_{h}^{\\pi}(s,a):=\\mathbb{P}(s_{h}=s,a_{h}=a\\ |\\ s_{1}\\sim\\rho,\\pi).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Definition 2 (Concentrability Coefficient). The all-policy and single-policy concentrability coefficients of $\\pi$ with regard to the occupancy measure $\\dot{\\mu^{\\biggr.}}=\\{\\dot{\\mu_{h}}\\}_{h=1}^{H}$ of a behavior policy $\\pi_{b}$ are ", "page_idx": 3}, {"type": "equation", "text": "$$\nC_{a l l}:=\\operatorname*{sup}_{\\pi}\\operatorname*{sup}_{h,s,a}\\frac{d_{h}^{\\pi}(s,a)}{\\mu_{h}(s,a)}\\ a n d\\ C^{*}:=\\operatorname*{sup}_{h,s,a}\\frac{d_{h}^{*}(s,a)}{\\mu_{h}(s,a)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Policy learning and regret minimization. Hybrid RL aims to either learn an $\\epsilon_{}$ -optimal policy $\\widehat{\\pi}$ such that $V^{\\ast}-V^{\\widehat{\\pi}}\\leqslant\\epsilon$ with high probability, or to minimize the regret. Here, the regret of an opnline algorithm $\\mathcal{L}:\\mathcal{H}\\to\\Pi$ is $\\begin{array}{r}{\\bar{\\mathrm{Reg}}_{\\mathcal{L}}\\bar{(T)}:=\\mathbb{E}\\bigl[\\sum_{t=1}^{T}\\bigl(V_{1}^{*}(s_{1}^{(t)})-\\sum_{h=1}^{H}r_{h}^{(t)}\\bigr)\\bigr]}\\end{array}$ . We write $T=N_{\\mathrm{on}}$ interchangeably for the number of episodes taken\u0159 by a regret-mini m\u0159izing online RL algorithm. ", "page_idx": 3}, {"type": "text", "text": "2.2 Linear MDPs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Throughout this paper, we study linear MDPs, where the transition probabilities and rewards are linearly parametrizable as functions of known features. This was first proposed by Jin et al. (2019); Yang and Wang (2019), and further studied in He et al. (2023); Hu et al. (2023); Wagenmaker and Jamieson (2023); Wagenmaker and Pacchiano (2023); Xiong et al. (2023); Zanette et al. (2021). ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 (Linear MDP, Jin et al. (2019)). There exists a known feature map $\\phi:S\\times A\\rightarrow\\mathbb{R}^{d}$ , $d$ unknown signed measures $\\mu_{h}\\ =\\ (\\mu_{h}^{(1)},\\cdot\\cdot\\cdot\\ ,\\mu_{h}^{(d)})$ over $\\boldsymbol{S}$ for each $h$ , and an unknown vector $\\theta_{h}\\in\\mathbb{R}^{d}$ , such that for any $s,a,h$ we have $\\mathbb{P}_{h}(\\cdot\\mid\\overrightharpoon{s},a)=\\left\\langle\\phi(s,a),\\mu_{h}(\\cdot)\\right\\rangle,r_{h}(s,a)=\\left\\langle\\phi(s,a),\\theta_{h}\\right\\rangle$ . Assume $\\|\\phi(s,a)\\|\\leqslant1$ for all $s,a$ , and max $\\{\\|\\mu_{h}(S)\\|\\,,\\|\\theta_{h}\\|\\}\\leqslant{\\sqrt{d}}$ for all $h$ . ", "page_idx": 3}, {"type": "text", "text": "This allows for sample-efficient RL for a few reasons. Firstly, linear MDPs are Bellman complete (Jin et al., 2021a), a common assumption for sample-efficient RL in the literature (Duan and Wang, 2020; Fan et al., 2020; Munos and Szepesva\u00b4ri, 2008). Secondly, the value and Q-functions are linearly parametrizable in the features, allowing one to learn them via ridge regression. This allows for sample-efficient online (He et al., 2023; Hu et al., 2023) and offline (Xiong et al., 2023; Yin et al., 2022) RL with function approximation. However, existing guarantees for hybrid RL in linear MDPs (Wagenmaker and Pacchiano, 2023) are loose (Li et al., 2023b), inspiring our work. ", "page_idx": 3}, {"type": "text", "text": "Further notation. Write $\\phi_{n,h}\\ =\\ \\phi(s_{h}^{(n)},a_{h}^{(n)})$ for the feature vector at episode $n$ and horizon $h$ . Let $\\begin{array}{r l r}{{\\bf{A}}_{h}}&{=}&{\\sum_{n=1}^{N}\\phi_{n,h}\\boldsymbol{\\phi}_{n,h}^{\\top}\\;+\\;\\lambda{\\bf{I}}}\\end{array}$ and $\\begin{array}{r l r}{\\mathbf{\\delta}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\mathbf{A}_{\\mathrm{off},h}}&{=}&{\\!\\!\\!\\!\\sum_{n=1}^{N_{\\mathrm{off}}}\\phi_{n,h}\\boldsymbol{\\phi}_{n,h}^{\\intercal}\\;+\\;\\lambda\\mathbf{I}}\\end{array}$ nN\u201coff1 \u03d5n,h\u03d5nJ,h \\` \u03bbI be the covariance matrices of  th\u0159e entire dataset and the offline da ta\u0159set respectively, and $\\Omega$ the set of all covariates. We consider two kinds of variance-weighted covariance matrices, namely $\\begin{array}{r}{\\pmb{\\Sigma}_{n,h}^{*}\\,=\\,\\sum_{n=1}^{N}\\phi_{n,h}\\phi_{n,h}^{\\top}/\\left[\\mathbb{V}_{h}V_{h+1}^{*}\\right](s_{h}^{\\tau},a_{h}^{\\tau})+\\lambda\\mathbf{I}}\\end{array}$ and $\\begin{array}{r}{\\Sigma_{n,h}\\,=\\,\\sum_{n=1}^{N}\\bar{\\sigma}_{n,h}^{-2}\\phi_{n,h}\\phi_{n,h}^{\\top}+\\lambda\\mathbf{I}.}\\end{array}$ , where $\\left[\\mathbb{V}_{h}V_{h+1}^{*}\\right](s_{h}^{\\tau},a_{h}^{\\tau})\\,=\\,\\operatorname*{max}\\left\\{1,\\left[\\mathrm{Var}_{h}\\,V_{h+1}^{*}\\right](s,a)\\right\\}$ is the trunca t\u0159ed variance of the optimal value f\u201cunction (\u2030where $s,a$ are rand\u2423om \u201cvariables) a\u2030nd $\\bar{\\sigma}_{n,h}^{-2}$ ( is the variance estimator from He et al. (2023). ", "page_idx": 3}, {"type": "text", "text": "2.3 Exploring the state-action space ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We aim to develop efficient hybrid RL algorithms for linear MDPs that do not rely on single-policy concentrability over the entire state-action space, which entails that the behavior policy covers every state-action pair that $\\pi^{*}$ visits. A natural idea from Li et al. (2023b) is to partition this space into a component that is well-covered by the behavior policy, which we call the offline partition $\\chi_{\\mathrm{off}}$ , and a component requiring further exploration, which we call the online partition $\\chi_{\\mathrm{on}}$ . Based on this partition, similarly to Tan and $\\mathrm{Xu}\\left(2024\\right)$ , the estimation error or regret of a hybrid RL algorithm can be analyzed on each component separately. We define $\\mathcal{X}_{\\mathrm{on}}\\cup\\mathcal{X}_{\\mathrm{off}}\\,\\,\\bar{=}\\,[H]\\times\\dot{S}\\times\\mathcal{A}$ , with their images under the feature map $\\Phi_{\\mathrm{off}}\\;=\\;\\mathrm{Span}(\\phi(\\mathcal{X}_{\\mathrm{off},h}))_{h\\in[H]}\\;\\subseteq\\;\\mathbb{R}^{d}$ and $\\Phi_{\\mathrm{on}}\\;=\\;\\mathrm{Span}(\\phi(\\chi_{\\mathrm{on},h}))_{h\\in[H]}\\;\\subseteq$ $\\mathbb{R}^{d}$ being subspaces of dimension $d_{\\mathrm{off}}$ and $d_{\\mathrm{on}}$ respectively. Write $\\mathcal{P}_{\\mathrm{off}},\\mathcal{P}_{\\mathrm{on}}$ for the orthogonal projection operators onto these subspaces respectively. Let $\\lambda_{k}(M)$ denote the $k$ -th largest eigenvalue ", "page_idx": 3}, {"type": "text", "text": "1: Input: Offline dataset $\\mathcal{D}_{\\mathrm{off}}$ , samples sizes $N_{\\mathrm{on}}$ , $N_{\\mathrm{off}}$ , feature maps $\\phi_{h}$ , tolerance parameter for reward-agnostic exploration $\\tau$ .   \n2: Initialize: $\\mathcal D_{h}^{(0)}\\gets\\emptyset\\,\\,\\,\\forall h\\in[H],\\,\\lambda=1/H^{2},\\,\\beta_{2}=\\tilde{O}(\\sqrt{d}).$   \n3: for horizon $h=1,...,H$ do   \n4: Run an exploration algorithm (OPTCOV, Wagenmaker and Jamieson (2023)) to collect covariates $\\Lambda_{h}$ such that $\\begin{array}{r l}&{\\quad\\quad\\quad\\quad\\operatorname*{max}_{\\phi_{h}\\in\\Phi}\\phi_{h}^{\\top}\\big(\\mathbf{{A}}_{h}+\\lambda\\mathbf{{I}}+\\tilde{\\mathbf{A}_{\\mathrm{off},h}}\\big)^{-1}\\phi_{h}\\leqslant\\tau}\\end{array}$ .   \n5: end for   \n6: Output: $\\widehat{\\pi}$ from running a pessimistic offline RL algorithm $(\\mathrm{LinPEVI-ADV+}$ , Xiong et al. (2023)) w ipth hyperparameters $\\lambda,\\beta_{2}$ on the combined dataset $\\mathcal{D}_{\\mathrm{off}}\\cup\\{\\mathcal{D}_{h}^{(N_{\\mathrm{on}})}\\}_{h\\in[H]}$ . ", "page_idx": 4}, {"type": "text", "text": "of a symmetric matrix $M$ . We borrow the definition of partial offline all-policy concentrability,1 ", "page_idx": 4}, {"type": "equation", "text": "$$\nc_{\\mathrm{off}}(\\mathcal X_{\\mathrm{off}}):=\\operatorname*{max}_{h}~1/\\lambda_{d_{\\mathrm{off}}}\\big(\\mathbb{E}_{\\mu_{h}}\\bar{[}(\\mathcal P_{\\mathrm{off}}\\phi_{h})(\\mathcal P_{\\mathrm{off}}\\phi_{h})^{\\top}\\big]\\big),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "from Tan and $\\mathrm{Xu}$ (2024). This corresponds to the inverse of the $d_{\\mathrm{off}}$ -th largest eigenvalue of the covariance matrix of the projected feature maps. Similarly, the partial all-policy analogue of the coverability coefficient from Xie et al. (2022a) is ", "page_idx": 4}, {"type": "equation", "text": "$$\nc_{\\mathrm{on}}(\\mathcal{X}_{\\mathrm{on}}):=\\operatorname*{inf}_{\\pi}\\operatorname*{max}_{h}~1/\\lambda_{d_{\\mathrm{on}}}(\\mathbb{E}_{d_{h}^{\\pi}}[(\\mathcal{P}_{\\mathrm{on}}\\phi_{h})(\\mathcal{P}_{\\mathrm{on}}\\phi_{h})^{\\top}]).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As we shall see, these quantities characterize the estimation error of our proposed algorithms. ", "page_idx": 4}, {"type": "text", "text": "3 Algorithms and main results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We provide two algorithms with improved statistical guarantees to tackle the unsolved (Table 2) problem of achieving sharp guarantees with hybrid RL in linear MDPs, with different approaches: ", "page_idx": 4}, {"type": "text", "text": "1. Performing reward-agnostic online exploration (Wagenmaker and Pacchiano, 2023) to augment the offline data, then invoking offline RL (Xiong et al., 2023) to learn an $\\epsilon$ -optimal policy on the combined dataset, in the same vein of Li et al. (2023b). This is Algorithm 1. 2. Warm-starting an online RL algorithm (He et al., 2023) with parameters estimated from an offline dataset to minimize regret, as in Song et al. (2023), with details in Algorithm 2. ", "page_idx": 4}, {"type": "text", "text": "3.1 Offline RL after online exploration ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Algorithm 1 collects online samples informed by the degree of coverage (or lack thereof) of the offline dataset $\\mathcal{D}_{\\mathrm{off}}$ with a reward-agnostic online exploration algorithm called OPTCOV from Wagenmaker and Jamieson (2023). OPTCOV explores so that the smallest eigenvalue of the covariance matrix, $\\lambda_{\\operatorname*{min}}({\\bf A}_{h})$ , is no smaller than a tolerance parameter $1/\\tau$ . We then learn a policy from the combined dataset using a minimax-optimal pessimistic offline RL algorithm from Xiong et al. (2023), LinPEVI-ADV+. To employ OPTCOV, one requires a modified analogue of the full-rank covariate assumption from Wagenmaker and Pacchiano (2023) that ensures that the MDP is \u201dexplorable\u201d enough. This assumption is only imposed for Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "Assumption 2 (Full Rank Projected Covariates). For any partition $\\mathcal{X}_{\\mathrm{on}}\\cup\\mathcal{X}_{\\mathrm{off}}=[H]\\times\\mathcal{S}\\times\\mathcal{A},$ $c_{\\mathrm{on}}(\\mathcal{X}_{\\mathrm{on}})<\\infty$ , or equivalently that $\\operatorname*{inf}_{\\pi}\\operatorname*{min}_{h}\\lambda_{d_{\\mathrm{on}}}\\big(\\mathbb{E}_{d_{h}^{\\pi}}\\big[\\big(\\mathcal{P}_{\\mathrm{on}}\\phi_{h}\\big)\\big(\\mathcal{P}_{\\mathrm{on}}\\phi_{h}\\big)^{\\top}\\big]\\big)=\\lambda_{d_{\\mathrm{on}}}^{*}>0.$ . ", "page_idx": 4}, {"type": "text", "text": "Informally, this states that for any partition, there exists some \u201coptimal exploration policy\u201d that ensures that the projected covariates onto the online partition have the same rank as its dimension at every timestep. In practice, this is achievable for any linear MDP via projecting the features onto the eigenspace corresponding to the nonzero singular values. We can then establish the following: ", "page_idx": 4}, {"type": "text", "text": "Lemma 1 (Partial Coverability Is Bounded In Linear MDPs). For any partition $\\mathcal{X}_{\\mathrm{off}},\\mathcal{X}_{\\mathrm{on.}}$ , it satisfies that $c_{\\mathrm{on}}(\\mathcal{X}_{\\mathrm{on}})\\leqslant d_{\\mathrm{on}}$ . Also, there exists at least one partition such that $c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})=O(d)$ . ", "page_idx": 4}, {"type": "text", "text": "The proof of this lemma is deferred to Appendix D. This result allows us to bound the error on the offline and online partitions by the dimensionality of the partitions, instead of the coverability coefficient. Define $\\bar{\\alpha_{\\mathrm{off}}}:={N_{\\mathrm{off}}}/{\\bar{N}}$ , $\\alpha_{\\mathrm{on}}:=N_{\\mathrm{on}}/N$ , and the minimal online samples for exploration ", "page_idx": 4}, {"type": "equation", "text": "$$\nN^{*}(\\tau):=\\operatorname*{min}_{N}N\\quad\\mathrm{s.t.}\\ \\operatorname*{inf}_{\\Lambda\\in\\Omega}\\operatorname*{max}_{\\phi\\in\\Phi}\\phi^{\\top}\\left(N(\\mathbf{\\Lambda}+\\bar{\\lambda}I)+\\mathbf{\\Lambda}\\mathbf{A}_{\\mathrm{off}}\\right)^{-1}\\phi\\leqslant\\tau.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We now have, with full proof in Appendix B and sketch at the end of the subsection, the following: ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Error Bound for RAPPEL, Algorithm 1). For every $\\delta~\\in~(0,1)$ and any partition $\\mathcal{X}_{\\mathrm{off}},\\mathcal{X}_{\\mathrm{on}}$ $1-\\delta$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\mathbb{F}_{1}^{*}-\\nu_{1}^{\\hat{\\pi}}\\lesssim\\sqrt{a}\\sum_{h=1}^{H}\\mathbb{E}_{\\sigma^{*}}\\,\\big|\\,\\phi(\\operatorname*{max}\\{d_{\\mathrm{on}},c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{on}}),c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})/{N_{\\mathrm{off}}}\\}),R A P P E L\\,a c h i e v e s\\,w.\\,\\mathrm{p.}\\,\\mathrm{1}}\\\\ {\\displaystyle V_{1}^{*}-V_{1}^{\\hat{\\pi}}\\lesssim\\sqrt{d}\\sum_{h=1}^{H}\\mathbb{E}_{\\sigma^{*}}\\,\\big|\\,\\phi(s_{h},a_{h})\\big|\\big|_{(\\mathbf{S}_{\\mathrm{off},h}^{*}+\\Sigma_{\\mathrm{on},h}^{*})^{-1}}\\leqslant\\sqrt{d}\\sum_{h=1}^{H}\\mathbb{E}_{\\sigma^{*}}\\,\\big|\\,\\phi(s_{h},a_{h})\\big|\\big|_{\\mathbf{S}_{\\mathrm{off},h}^{*-1}}\\,\\mathrm{,}}\\\\ {\\displaystyle V_{1}^{*}-V_{1}^{\\hat{\\pi}}\\lesssim\\operatorname*{min}\\left\\{\\sqrt{\\frac{c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})d H^{4}}{N_{\\mathrm{off}}}}+\\sqrt{\\frac{d_{\\mathrm{on}}d H^{4}}{N_{\\mathrm{on}}}},\\sqrt{\\frac{c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})^{2}d H^{3}}{N_{\\mathrm{off}}\\alpha_{\\mathrm{off}}}}+\\sqrt{\\frac{d_{\\mathrm{on}}^{2}d H^{3}}{N_{\\mathrm{on}}\\alpha_{\\mathrm{on}}}}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "given $N\\geqslant\\operatorname*{max}\\big\\{\\alpha_{\\mathrm{on}}^{4}d_{\\mathrm{on}}^{-4},\\alpha_{\\mathrm{off}}^{4}c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})^{-4}\\big\\}\\operatorname*{max}\\{N^{*}(\\tau),p o l y(d,H,c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}}),\\log1/\\delta)\\}.$ This result, when applied to tabular MDPs with finite states and actions, yields: ", "page_idx": 5}, {"type": "text", "text": "Corollary 1. In tabular $M D P s$ , for every $\\delta\\in(0,1)$ , it satisfies that with probability at least $1-\\delta$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\nV_{1}^{\\star}(s)-V_{1}^{\\hat{\\pi}}(s)\\lesssim\\sqrt{H^{3}|S|^{2}|A|}\\left(\\sqrt{c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})/N_{\\mathrm{off}}}+\\sqrt{d_{\\mathrm{on}}/N_{\\mathrm{on}}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In sum, Theorem 1 shows that with a poly $(d,H)$ burn-in cost that is no smaller than $N^{*}$ (the minimal online samples for any algorithm to achieve our choice of OPTCOV tolerance), we require only ", "page_idx": 5}, {"type": "equation", "text": "$$\nc_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})d H^{3}\\operatorname*{min}\\{c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}}),H\\}/\\epsilon^{2}+d_{\\mathrm{on}}d H^{3}\\operatorname*{min}\\{d_{\\mathrm{on}},H\\}/\\epsilon^{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "trajectories to learn an $O(\\epsilon)$ -optimal policy. $N^{*}$ , from Wagenmaker and Pacchiano (2023), is essentially unavoidable in reward-agnostic exploration for linear MDPs. To compare with prior literature, our result leads to a better worst-case guarantee than the error bound of $\\sqrt{d^{2}H^{7}/N}$ attained in $\\mathrm{Wa}.$ - genmaker and Pacchiano (2023) (by at least a factor of $H^{3/2}$ ), the only other work on hybrid RL in linear MDPs thus far. While we employ the same online exploration procedure, we combine our exploration phase with an offline learning algorithm LinPEVI-ADV $^+$ from Xiong et al. (2023) and conduct a careful analysis. When comparing with the offline-only and online-only settings, Theorem 1 improves upon the offline-only minimax-optimal error bound of $\\begin{array}{r}{\\sqrt{d}\\sum_{h=1}^{H}\\mathbb{E}_{\\pi^{*}}||\\phi(s_{h},a_{h})||_{\\Sigma_{\\mathrm{off},h}^{*-1}}}\\end{array}$ from Xiong et al. (2023) as a consequence of $\\Sigma_{\\mathrm{off},h}^{*}+\\Sigma_{\\mathrm{on},h}^{*}\\;\\geq\\;\\Sigma_{\\mathrm{off},h}^{*}$ ; the best offline-only error bound is $\\sqrt{d^{2}H^{4}/N_{\\mathrm{off}}}$ obtained under the \u201cwell-covered\u201d assumption (Corollary 4.6, Jin et al. (2021b)) that $\\dot{\\lambda}_{\\mathrm{min}}(\\mathbf{A}_{h,\\mathrm{off}})\\geqslant\\Omega(1/d)$ , Theorem 1 enjoys better dimension and horizon dependence as there is always a partition such that $d_{\\mathrm{on}},c_{\\mathrm{off}}^{}(\\mathcal{X}_{\\mathrm{off}})\\leqslant d$ and $d_{\\mathrm{on}}H^{3}\\operatorname*{min}\\{d_{\\mathrm{on}},H\\}\\leqslant d^{2}H^{4}$ . ", "page_idx": 5}, {"type": "text", "text": "The literature has experienced considerable difficulty in sharpening the horizon dependence to $H^{3}$ in offline RL for linear MDPs. While Yin et al. (2022) and Xiong et al. (2023) provide minimaxoptimal algorithms for offline RL in linear MDPs, both only manage to achieve a $H^{3}$ horizon dependence in the special case of tabular MDPs, even under the aforementioned \u201cwell-covered\u201d assumption. We provide the same result in Corollary 1 with proof deferred to Appendix C, but encouragingly, hybrid RL lets us bypass the \u201cwell-covered\u201d assumption. In Appendix B and G, we use a novel truncation argument and the total variance lemma (Lemma C.5 of Jin et al. (2018)) to improve the dependence on $H$ , but our result falls slightly short of $\\sqrt{c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})d H^{3}/N_{\\mathrm{off}}}+\\sqrt{d_{\\mathrm{on}}d H^{3}/N_{\\mathrm{on}}}$ . ", "page_idx": 5}, {"type": "text", "text": "Computational efficiency. In terms of computational efficiency, Algorithm 1 inherits the computational costs of the previous proposed algorithms OPTCOV and LinPEVI-ADV $^+$ (Wagenmaker and Jamieson (2023); Xiong et al. (2023). OPTCOV runs in polynomial time $\\mathrm{poly}(d,H,c_{\\mathrm{on}}(\\mathcal{X}_{\\mathrm{on}}),\\log{1/\\delta})$ , and $\\mathrm{LinPEVI-ADV+}$ runs in $\\tilde{O}(d^{3}H N|A|)$ time when the action space is discrete. Algorithm 1 therefore remains computationally efficient in this case. ", "page_idx": 5}, {"type": "text", "text": "Requirement of choosing $d_{\\mathrm{on}}$ . There is the caveat that we require the user to choose the tolerance for OPTCOV. In practice, one can achieve this by performing SVD on the offline dataset and looking at the plot of eigenvalues. One can also choose a tolerance of $O(d/\\operatorname*{min}\\{N_{\\mathrm{off}},N_{\\mathrm{on}}\\})$ , but this would not achieve the reduction in the dependence on dimension from $d^{2}$ to $c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})d,d_{\\mathrm{on}}d$ . ", "page_idx": 5}, {"type": "text", "text": "Practical benefits of the online-to-offline approach. Algorithm 1 outputs a fixed policy satisfying a PAC bound. This enables policies to be deployed in critical real-world applications, such as in medicine or defense, where randomized policies from regret minimization are unacceptable. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 works for reward-agnostic hybrid RL. The use of reward-agnostic online exploration in Algorithm 1 enables one to use the hybrid dataset $\\mathcal{D}$ to learn policies for different reward functions offline. As the online exploration is not influenced by any single reward function, the collected data satisfies good coverage for any possible reward function even if it is revealed only after exploration, enabling one to use a single dataset to achieve success on many different tasks. This therefore also serves as an algorithm for the related setting of reward-agnostic hybrid $R L$ , where the reward function is unknown during online exploration and only revealed to the agent after it. ", "page_idx": 5}, {"type": "text", "text": "1: Input: Offline dataset $\\mathcal{D}_{\\mathrm{off}}$ , samples sizes $N_{\\mathrm{on}}$ , $N_{\\mathrm{off}}$ , feature maps $\\phi_{h}$ . Regularization parame  \nter $\\lambda>0$ , confidence radii $\\beta,\\bar{\\beta},\\bar{\\beta},t_{\\mathrm{last}}=0$ .   \n2: Initialize: For $\\textit{h}\\in\\ [H]$ , estimate $\\widehat{\\mathbf{w}}_{1,h},\\check{\\mathbf{w}}_{1,h},Q_{1,h},\\check{Q}_{1,h},\\sigma_{1,h},\\bar{\\sigma}_{1,h}$ from $\\mathcal{D}_{\\mathrm{off}}$ , and assign   \n$\\begin{array}{r}{\\pmb{\\Sigma}_{0,h}=\\pmb{\\Sigma}_{1,h}=\\pmb{\\Sigma}_{\\mathrm{off}}+\\lambda\\mathbf{I}=\\sum_{n=1}^{N_{\\mathrm{off}}}\\bar{\\sigma}_{n,h}^{-2}\\phi_{n,h}\\phi_{n,h}^{\\top}+\\lambda\\mathbf{I}}\\end{array}$ .q   \n3: for episodes $t=1,...,T$ do   \n4: Update optimistic and pessimistic weights $\\widehat{\\mathbf{w}}_{t,h},\\check{\\mathbf{w}}_{t,h}$ for all $h$ .   \n5: if there exists a stage $h^{\\bar{\\prime}}\\in[H]$ such that det $\\left(\\Sigma_{t,h^{\\prime}}\\right)\\geqslant2\\operatorname*{det}\\left(\\Sigma_{{t}_{\\mathrm{last}}\\,,h^{\\prime}}\\right)$ then   \n6: Update optimistic and pessimistic Q-functions $Q_{t,h}(s,a),\\check{Q}_{t,h}(s,a)$ , set $t_{\\mathrm{last}}\\,=\\,t$ .   \n7: end if   \n89:: for hPloariyz aocnt $h=1,...,H$ $a_{h}^{(t)}\\gets\\arg\\operatorname*{max}_{a}Q_{t,h}(s_{h}^{(t)},a)$ , receive reward $\\boldsymbol{r}_{h}^{(t)}$ , next state $s_{h+1}^{(t)}$   \n10: Estimate \u03c3t,h, \u03c3\u00aft,h \u00d0 maxt\u03c3t,h,?H, 2d3H2||\u03d5pspht q, apht qq||1\u03a3{t\u00b42,1hu 2, update \u03a3t\\`1,h. ", "page_idx": 6}, {"type": "text", "text": "11: end for ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "12: end for ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "13: Output: Greedy policy $\\widehat{\\pi}=\\pi^{Q_{T,h}}$ , $\\operatorname{Unif}(\\pi^{Q_{1,h}},...,\\pi^{Q_{T,h}})$ for PAC guarantee. ", "page_idx": 6}, {"type": "text", "text": "Proof sketch. The relation  p(7) in Theorem 1 follows from invoking Theorem 2 from Xiong et al. (2023) with $N>\\Omega(d^{2}H^{6}),\\lambda=1/H^{2},\\beta_{1}=O(\\sqrt{d})$ . To establish (8), we first bound ", "page_idx": 6}, {"type": "text", "text": "As $\\begin{array}{r l r}{\\Sigma_{h}^{*-1}}&{{}\\preceq}&{H^{2}\\Lambda_{h}^{-1}}\\end{array}$ (see Xiong et al. (2023)), it therefore boils down to controlling $\\operatorname*{max}_{\\phi_{h}\\in\\Phi}\\phi_{h}^{\\top}{\\mathbf{\\Lambda}}{\\mathbf{\\Lambda}}_{h}^{-1}\\phi_{h}$ . Towards this, first, we make the observation that Lemma 1 suggests that $c_{\\mathrm{on}}(\\mathcal{X}_{\\mathrm{on}})~\\leqslant~d_{\\mathrm{on}}$ . If we run OPTCOV with tolerance $\\tilde{O}(\\operatorname*{max}\\{d_{\\mathrm{on}}/N_{\\mathrm{on}},c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})/N_{\\mathrm{off}}\\})$ on partitions where the above hold, in Lemma 5, we prove that $\\begin{array}{r l}{\\operatorname*{max}_{\\phi_{h}\\in\\Phi}\\phi_{h}^{\\top}\\mathbf{A}_{h}^{-1}\\phi_{h}}&{\\lesssim}\\end{array}$ max $\\{c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})/N_{\\mathrm{off}},d_{\\mathrm{on}}/N_{\\mathrm{on}}\\}$ . This yields the $c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})d H^{4},d_{\\mathrm{on}}d H^{4}$ result. ", "page_idx": 6}, {"type": "text", "text": "To tighten the horizon dependence to $H^{3}$ , we employ an useful truncation argument. More specifically, from the total variance lemma (Lemma C.5 of Jin et al. (2018)), the average variance $\\mathbb{V}_{h}V_{h+1}^{*}$ is asymptotically on the order of $H$ . We therefore define the sets of trajectories $\\mathcal{E}_{h}(\\delta_{h})\\;=\\;\\left\\{\\tau\\:\\in\\:\\mathcal{D}\\::\\:\\left[\\mathbb{V}_{h}V_{h+1}^{*}\\right](s_{h}^{\\tau},a_{h}^{\\tau})\\;\\geqslant\\;H^{1+\\delta_{h}}\\right\\}$ . The cardinality of each set can be bounded by $|{\\mathcal E}_{h}(\\delta_{h})|~\\lesssim~N H^{1-\\delta_{h}}$ , and \u2030so truncating at the level where $\\begin{array}{r}{N H^{1-\\delta_{h}}\\ \\approx\\ \\operatorname*{min}(\\frac{N_{\\mathrm{off}}}{c_{\\mathrm{off}}(\\chi_{\\mathrm{off}})},\\frac{N_{\\mathrm{on}}}{d_{\\mathrm{on}}})}\\end{array}$ leads to $\\begin{array}{r}{\\operatorname*{min}_{\\phi_{h}\\in\\Phi}\\phi_{h}^{\\top}\\Sigma_{h}^{\\star}\\phi_{h}\\;\\gtrsim\\;\\frac{1}{N H^{2}}\\operatorname*{min}(\\frac{N_{\\mathrm{off}}}{c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})},\\frac{N_{\\mathrm{on}}}{d_{\\mathrm{on}}})^{2}}\\end{array}$ . Putting things together yoiffeldosf fthe loanst $c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})^{2}d H^{3},d_{\\mathrm{on}}^{2}d H^{3}$ result needed, and the theorem then follows. ", "page_idx": 6}, {"type": "text", "text": "3.2 Online regret minimization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Thus far, we described an online-to-offline strategy which collects online samples to augment the offline dataset. However, in certain critical cases, such as with a doctor treating patients, performanceagnostic online exploration is untenable. One may wish to minimize the regret of the online actions taken while learning a policy. We therefore explore another approach inspired by the work of Song et al. (2023); Tan and $\\mathrm{Xu}$ (2024) \u2013 that of warm-starting an online RL algorithm with parameters estimated from an offline dataset. We describe this in Algorithm 2, and show that hybrid RL enables provable gains over minimax-optimal online-only regret bounds in the offline-to-online case as well. ", "page_idx": 6}, {"type": "text", "text": "In order to warm-start an online RL algorithm with an offline dataset, we modify LSVI-UCB $^{++}$ from He et al. (2023) by estimating its parameters from $\\mathcal{D}_{\\mathrm{off}}$ with the same formulas it would use as if it had experienced the $N_{\\mathrm{off}}$ offline episodes itself. As Tan and $\\mathrm{Xu}$ (2024) suggest, this can be understood as including the offline episodes in the \u201cexperience replay buffer\u201d that the algorithm uses to learn parameters. The full version can be found in Appendix $\\mathrm{E}$ as Algorithm 4. Doing so allows us prove a regret bound depending on the partial all-policy concentrability coefficient. Below we state our theoretical guarantees for this algorithm. The proof of this result is deferred to Appendix E, and a brief proof sketch is provided at the end of this subsection. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 (Regret Bound for HYRULE, Algorithm 2). Given any $\\delta\\in(0,1)$ , for every partition $\\mathcal{X}_{\\mathrm{off}},\\mathcal{X}_{\\mathrm{on}},$ , i $f N_{\\mathrm{on}},N_{\\mathrm{off}}=\\tilde{\\Omega}(d^{13}H^{14})$ , the regret of HYRULE is bounded w.p. at least $1-\\delta$ by ", "page_idx": 7}, {"type": "equation", "text": "$$\nR e g(N_{\\mathrm{on}})\\lesssim\\operatorname*{inf}_{\\chi_{\\mathrm{off}},\\chi_{\\mathrm{on}}}\\sqrt{c_{\\mathrm{off}}(\\chi_{\\mathrm{off}})^{2}d H^{3}N_{\\mathrm{on}}^{2}/N_{\\mathrm{off}}}+\\sqrt{d_{\\mathrm{on}}d H^{3}N_{\\mathrm{on}}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Corollary 2. By the regret-to-PAC conversion, Algorithm 2 achieves a sub-optimality gap w.p. $1\\!-\\!\\delta$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\nV_{1}^{*}(s)-V_{1}^{\\pi}(s)\\lesssim\\operatorname*{inf}_{\\mathcal{X}_{\\mathrm{off}},\\mathcal{X}_{\\mathrm{on}}}\\sqrt{c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})^{2}d H^{3}/N_{\\mathrm{off}}}+\\sqrt{d_{\\mathrm{on}}d H^{3}/N_{\\mathrm{on}}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "To understand this result, we first note that bounding the regret over all possible partitions yields an improvement over the $\\sqrt{d^{2}H^{3}N_{\\mathrm{on}}}$ regret bound originally obtained by He et al. (2023), as we can simply take $\\mathcal{X}_{\\mathrm{on}}=\\mathcal{X}_{\\mathrm{on}}^{\\prime}=\\mathcal{X}$ to recover this result. In the scenario where offline samples are abundant (where $N_{\\mathrm{off}}\\gg N_{\\mathrm{on}})$ , it is possible to achieve significant improvements over online-only learning. Furthermore, in view of Lemma 1, there always exists a partition such that $c_{\\mathrm{off}}\\,(\\mathcal{X}_{\\mathrm{off}}),\\,\\dot{d}_{\\mathrm{on}}\\leqslant d$ . This result therefore yields provable improvements over the minimax-optimal online regret bound in linear MDPs (Agarwal et al., 2022; He et al., 2023; Hu et al., 2023; Zhou et al., 2021). ", "page_idx": 7}, {"type": "text", "text": "Additionally, Theorem 2 shows that Algorithm 2 attains the best known regret bound in hybrid RL for linear MDPs, as we illustrate in Table 2. The current best? known result is that of Tan and $\\mathrm{Xu}$ (2024), with a dependence of $\\sqrt{c_{\\mathrm{off}}\\left(\\mathcal{X}_{\\mathrm{off}}\\right)d H^{5}N_{\\mathrm{on}}^{2}/N_{\\mathrm{off}}}\\,+\\sqrt{d_{\\mathrm{on}}d H^{5}N_{\\mathrm{on}}}$ . Notably, we achieve the same a reduction in the diamension dependence on the online partition from $d^{2}$ to $d_{\\mathrm{on}}d$ that Tan and $\\mathrm{Xu}$ (2024) do by proving a sharper variant of Lemma B.1 from Zhou and Gu (2022) in Lemma 18, using this in Lemma 14 to reduce the dimensional dependence in the summation of bonuses. Song et al. (2023) and Amortila et al. (2024), on the other hand, have bounds on the order of $C^{*}\\sqrt{d^{2}H^{6}N_{\\mathrm{on}}}$ and $\\sqrt{(C^{*}+c_{\\mathrm{on}}(\\mathcal{X}))d^{3}H^{6}N_{\\mathrm{on}}}$ respectively. We produce a better bound than Amortila et al. (2024); Song et al. (2023); Tan and $\\mathrm{Xu}$ (2024) by at least a factor of $H^{2}$ by combining the total variance lemma and a novel truncation argument that rules out \u201cbad\u201d trajectories in Lemma 17, which allows us to maintain a desirable $H^{3}$ dependence on both partitions. ", "page_idx": 7}, {"type": "text", "text": "Computational efficiency. When the action space is finite and of cardinality $|{\\mathcal{A}}|$ , the computational complexity of Algorithm 2 is of order $\\tilde{O}\\left(d^{4}H^{3}N|A|\\right)$ , as outlined in He et al. (2023). Algorithm 2 is therefore computationally efficientr a\\`nd runs in p\u02d8olynomial time in this case. When the action space is continuous, one may need to solve an optimization problem over the continuous action space, making the computational complexity highly problem-dependent. ", "page_idx": 7}, {"type": "text", "text": "Algorithm 2 is unaware of the partition. Unlike Algorithm 1, Algorithm 2 is fully unaware of the choice of partition, and there is therefore no need to estimate $d_{\\mathrm{on}}$ or any relevant analogue to the choice of tolerance for OPTCOV. The regret bound therefore automatically adapts to the best possible partition, even though Algorithm 2 is unaware of it. ", "page_idx": 7}, {"type": "text", "text": "Practical benefits of the offline-to-online approach. While Algorithm 2 only satisfies a PAC bound with a randomized policy, it minimizes the regret of the actions it takes. This enables the algorithm to be deployed in situations where its performance during online exploration is of critical importance, e.g. in applications like mobile health (Nahum-Shani et al., 2017). ", "page_idx": 7}, {"type": "text", "text": "Technical challenges. Although Algorithm 2 is a straightforward generalization of $\\mathrm{LSVI-UCB++}$ in He et al. (2023), with $\\Sigma_{0}$ initialized with the offline dataset, we had to decompose the regret into the regret on the offline and online partitions to achieve the regret guarantee in Theorem 2. In the process, we faced the following challenges: ", "page_idx": 7}, {"type": "text", "text": "\u2022 Bounding the regret on the offline partition was challenging, as the argument of He et al. (2023) was not applicable. Instead, we used a truncation argument in Lemma 17 to bound the maximum eigenvalue of \u03a3o\u00b4f1f,h, maintaining a H3 dependence on the offline partition. \u2022 Bounding the regret on the online partition allowed us to use an analysis that was close to that of He et al. (2023). However, directly following their argument would have left us with a $d^{2}H^{3}$ dependence. To reduce the dimensional dependence to $d_{\\mathrm{on}}d$ , we prove a sharper variant of Lemma B.1 from Zhou and Gu (2022) in Lemma 18. We use this in Lemma 14 to reduce the dimensional dependence in the sum of bonuses, achieving the desired result. \u2022 Without the above two techniques, one could have used a simpler analysis to achieve a far looser $\\sqrt{c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})^{2}d^{6}H^{8}N_{\\mathrm{on}}^{2}/N_{\\mathrm{off}}}+\\sqrt{d^{2}H^{3}}$ regret bound by using the maximum magnitude of athe variance weights for the offline partition and the analysis from He et al. (2023) verbatim for the online partition, but this would not have yielded the same improvement. ", "page_idx": 7}, {"type": "text", "text": "Proof sketch. We first adopt the regret decomposition as in He et al. (2023) and bound ", "page_idx": 8}, {"type": "text", "text": "P $\\begin{array}{r}{\\mathrm{Reg}(T)\\lesssim\\sqrt{H^{3}T}+\\sum_{h,t}\\dot{\\beta}\\|\\Sigma_{t,h}^{-1\\slash2}\\phi_{h}(s_{h}^{(t)},a_{h}^{\\zeta t})\\mathbb{1}_{\\mathcal{X}_{\\mathrm{off}}}\\|_{2}+\\sum_{h,t}\\beta\\|\\Sigma_{t,h}^{-1\\slash2}\\dot{\\phi}_{h}(s_{h}^{(t)},a_{h}^{(t)})\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\|_{2}.}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "It then boils down  to controlling the second and th ird terms separately. We prove in Lemma 12 that the sum of bonuses on the offline partition can be bounded by $\\begin{array}{r}{\\sum_{h}\\sqrt{d N_{\\mathrm{on}}\\frac{N_{\\mathrm{on}}}{N_{\\mathrm{off}}}\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{off}}}\\phi_{h}^{\\top}\\bar{\\Sigma}_{\\mathrm{off},h}^{-1}\\phi_{h}}.}\\end{array}$ . Further, $\\begin{array}{r}{\\sum_{h}\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{off}}}\\sqrt{\\phi_{h}^{\\top}\\bar{\\Sigma}_{\\mathrm{off},h}^{-1}\\phi_{h}}\\lesssim c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})^{2}H^{3}}\\end{array}$ by Lemma 13. Putting things together, the second  term can be controlled as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta\\!\\sum_{h,t}\\!\\|\\boldsymbol{\\Sigma}_{t,h}^{-1/2}\\tilde{\\phi_{h}}\\!\\left(\\boldsymbol{s}_{h}^{(t)},\\boldsymbol{a}_{h}^{(t)}\\right)\\mathbb{1}_{\\mathcal{X}_{\\mathrm{off}}}\\|_{2}\\lesssim\\sqrt{c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})^{2}d H^{3}N_{\\mathrm{on}}^{2}/N_{\\mathrm{off}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "With respect to the third term, Lemma 14 (a sharpened version of Lemma E.1 in He et al. (2023)), combined with the Cauchy-Schwartz inequality, yields ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta\\sum_{h,t}\\lVert\\Sigma_{t,h}^{-1/2}\\phi_{h}(s_{h}^{(\\bar{t})},a_{h}^{(t)})\\rVert_{\\mathcal{X}_{\\mathrm{on}}}\\rVert_{2}\\stackrel{*}{\\lesssim}d^{4}\\overbrace{H^{8}}^{\\prime}+\\beta d^{7}H^{5}+\\beta\\sqrt{d_{\\mathrm{on}}H T+d_{\\mathrm{on}}H\\sum_{h,t}\\sigma_{t,h}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "By the total variance lemma (Appendix B, He et al. (2023)), $\\begin{array}{r}{\\sum_{h,t}\\sigma_{t,h}^{2}\\,\\leqslant\\widetilde{O}\\,\\big(H^{2}T+d^{10.5}H^{16}\\big).}\\end{array}$ Taking everything collectively establishes the desired result. ", "page_idx": 8}, {"type": "text", "text": "4 Numerical experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To demonstrate the benefits of hybrid RL in the offline-to-online and online-to-offline settings, we implement Algorithms 1 and 2 on a scaled-down Tetris environment (as in Tan and $\\mathrm{Xu}$ (2024)). For the purposes of brevity, we defer the details of the environment to Appendix H.3 ", "page_idx": 8}, {"type": "text", "text": "Figure 1 depicts the coverage (defined by $1/\\lambda_{\\operatorname*{min}}(\\Lambda),1/\\lambda_{d_{\\mathrm{off}}}(\\Lambda_{\\mathrm{off}}),1/\\lambda_{d_{\\mathrm{on}}}(\\Lambda_{\\mathrm{on}}))$ achieved by the reward-agnostic exploration algorithm, OPTCOV, when initialized with 200 trajectories from (1) a uniform behavioral policy, (2) an adversarial behavior policy obtained by the negative of the weights of a fully-trained agent under Algorithm 1, and (3) no offline trajectories at all. Although hybrid RL with the uniform behavior policy achieves the best coverage throughout as expected, hybrid RL with even adversarially collected offline data achieves better coverage than online-only exploration. This demonstrates the potential of hybrid RL as a tool for taking advantage of poor quality offline data. ", "page_idx": 8}, {"type": "text", "text": "Figure 2 shows the benefits of hybrid RL in the online-to-offline setting when the behavior policy is of poor quality. When applying LinPEVI-ADV to the hybrid dataset of 200 trajectories and 100 online trajectories, 300 trajectories of adversarially collected offline data, and 300 trajectories of online data under reward-agnostic exploration, we see that the hybrid dataset is most conducive for learning. Additionally, without a warm-start from offline data, online-only reward-agnostic exploration performs worse than the adversarially collected offline data due to significant burn-in costs. Hybrid RL, in this instance, performs better than both offline-only and online-only learning alone. Figure 3 compares the performances of LSVI-UCB $^{++}$ and Algorithm 2. Initializing a regret-minimizing online algorithm (LSVI-UCB $^{++}$ , (He et al., 2023)) with an offline dataset as in Algorithm 2 yields lower regret than LSVI- $\\mathrm{UCB++}$ without an offline dataset. This shows that even a nearly minimaxoptimal online learning algorithm can stand to benefit from offline data. ", "page_idx": 8}, {"type": "image", "img_path": "bPuYxFBHyI/tmp/ac00c54c8679c0ded07995cc1f0333f6702bd1c179c9ca40a287353c292ade6f.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 1: Coverage achieved by OPTCOV with 200 trajectories of offline data collected under a uniform and an adversarial behavior policy, and with no offline data. Results averaged over 30 trials, with the shaded area depicting 1.96-standard errors. Lower is better. ", "page_idx": 8}, {"type": "image", "img_path": "bPuYxFBHyI/tmp/4bcad1ebb9702faee4d5d901c7d75ac5898eac28751c8987352a4e49ac07f167.jpg", "img_caption": ["Figure 2: Value of policies learned by applying LinPEVI-ADV to the hybrid, offline, and online datasets, with an adversarial behavior policy. The reward is negative as it is the negative of the excess height. Results over 30 trials. Higher is better. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "bPuYxFBHyI/tmp/6ca61a3b821b1d738305394bb45414c88806d1d5c9f1410cb330bc76e87918c7.jpg", "img_caption": ["Figure 3: Comparison of LSVI- $\\scriptstyle\\mathrm{UCB}++$ and Algorithm 2 over 10 trials, with 1 s.d. error bars. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Discussion, limitations and future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we develop two hybrid RL algorithms for linear MDPs with desirable statistical guarantees for the online-to-offline and offline-to-online settings. Both algorithms demonstrate provable gains over the minimax-optimal rates in offline or online-only reinforcement learning, and provide the sharpest worst-case bounds for the performance of hybrid RL in linear MDPs thus far. ", "page_idx": 9}, {"type": "text", "text": "Throughout this paper, we have used both optimism and pessimism in our algorithm design. Other work in hybrid RL (Amortila et al., 2024; Li et al., 2023b; Nakamoto et al., 2023; Song et al., 2023; Tan and Xu, 2024; Wagenmaker and Pacchiano, 2023) uses optimism, pessimism, or sometimes even neither. We conjecture that optimism is still helpful in aiding online exploration within hybrid RL and that pessimism helps in hybrid RL when learning from a combined dataset. However, determining if or when optimism or pessimism is beneficial in hybrid RL remains an open question. ", "page_idx": 9}, {"type": "text", "text": "Achieving a $H^{3}$ horizon dependence in offline RL for linear MDPs has proven challenging. Even under strong coverage assumptions, Yin et al. (2022) and Xiong et al. (2023) only manage to achieve a $H^{3}$ horizon dependence for tabular MDPs. Obtaining a $\\sqrt{d^{2}H^{3}/N}$ bound is an open problem. ", "page_idx": 9}, {"type": "text", "text": "A result depending on a partial single-policy concentrability coefficient would be desirable, but may provide only limited benefits as we take the infimum over partitions. A good offline partition for the partial all-policy concentrability contains the portion of the state-action space well-covered by the offline dataset, while the same for the partial single-policy concentrability would be well-covered by both the offline dataset and the optimal policy. The smaller size of the latter offline partition may be offset by the larger size of the latter\u2019s online partition, and as such any gains may be limited. ", "page_idx": 9}, {"type": "text", "text": "Furthermore, while Algorithm 1 improves upon the offline-only error lower bound in Xiong et al. (2023) and Algorithm 2 improves upon the online-only regret lower bound in Zhou et al. (2021), we still desire a single algorithm that improves upon both the best possible offline-only and online-only rates at once. Additionally, the burn-in costs for Algorithms 1 and 2 are nontrivial. The former is inherited from OPTCOV (Wagenmaker and Jamieson, 2023), while the latter is inherited from He et al. (2023) and the truncation argument. Improving the former by devising new reward-agnostic exploration algorithms for linear MDPs, perhaps in the vein of Li et al. (2023a), would be welcome. ", "page_idx": 9}, {"type": "text", "text": "While we tackle the setting of linear MDPs, it remains a first step towards showing that hybrid RL breaks minimax-optimal barriers in the presence of function approximation. Further work in this vein on other types of function approximation would be an interesting contribution to the literature. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Y. Wei is supported in part by the NSF grant CCF-2106778, CCF-2418156 and the CAREER award DMS-2143215. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Abbasi-yadkori, Y., Pa\u00b4l, D., and Szepesva\u00b4ri, C. (2011). Improved algorithms for linear stochastic bandits. In Shawe-Taylor, J., Zemel, R., Bartlett, P., Pereira, F., and Weinberger, K., editors, Advances in Neural Information Processing Systems, volume 24. Curran Associates, Inc.   \nAgarwal, A., Jin, Y., and Zhang, T. (2022). Voql: Towards optimal regret in model-free rl with nonlinear function approximation.   \nAmortila, P., Foster, D. J., Jiang, N., Sekhari, A., and Xie, T. (2024). Harnessing density ratios for online reinforcement learning.   \nAzar, M. G., Osband, I., and Munos, R. (2017). Minimax regret bounds for reinforcement learning. In International conference on machine learning, pages 263\u2013272. PMLR.   \nBall, P. J., Smith, L., Kostrikov, I., and Levine, S. (2023). Efficient online reinforcement learning with offline data. In International Conference on Machine Learning, pages 1577\u20131594. PMLR.   \nDu, S. S., Kakade, S. M., Wang, R., and Yang, L. F. (2019). Is a good representation sufficient for sample efficient reinforcement learning? arXiv preprint arXiv:1910.03016.   \nDuan, Y. and Wang, M. (2020). Minimax-optimal off-policy evaluation with linear function approximation.   \nFan, J., Wang, Z., Xie, Y., and Yang, Z. (2020). A theoretical analysis of deep q-learning.   \nHe, J., Zhao, H., Zhou, D., and Gu, Q. (2023). Nearly minimax optimal reinforcement learning for linear markov decision processes.   \nHu, P., Chen, Y., and Huang, L. (2023). Nearly minimax optimal reinforcement learning with linear function approximation.   \nJin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M. I. (2018). Is q-learning provably efficient? Advances in neural information processing systems, 31.   \nJin, C., Liu, Q., and Miryoosef,i S. (2021a). Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms.   \nJin, C., Yang, Z., Wang, Z., and Jordan, M. I. (2019). Provably efficient reinforcement learning with linear function approximation.   \nJin, Y., Yang, Z., and Wang, Z. (2021b). Is pessimism provably efficient for offline rl? In International Conference on Machine Learning, pages 5084\u20135096. PMLR.   \nKausik, C., Tan, K., and Tewari, A. (2024). Leveraging offline data in linear latent bandits. arXiv preprint arXiv:2405.17324.   \nLange, S., Gabel, T., and Riedmiller, M. (2012). Batch reinforcement learning. In Reinforcement learning: State-of-the-art, pages 45\u201373. Springer.   \nLattimore, T., Szepesvari, C., and Weisz, G. (2020). Learning with good feature representations in bandits and in rl with a generative model.   \nLevine, S., Kumar, A., Tucker, G., and Fu, J. (2020). Offline reinforcement learning: Tutorial, review, and perspectives on open problems.   \nLi, G., Chen, Y., Chi, Y., Gu, Y., and Wei, Y. (2021). Sample-efficient reinforcement learning is feasible for linearly realizable mdps with limited revisiting. Advances in Neural Information Processing Systems, 34:16671\u201316685.   \nLi, G., Shi, L., Chen, Y., Chi, Y., and Wei, Y. (2024). Settling the sample complexity of model-based offline reinforcement learning. The Annals of Statistics, 52(1):233\u2013260.   \nLi, G., Yan, Y., Chen, Y., and Fan, J. (2023a). Minimax-optimal reward-agnostic exploration in reinforcement learning.   \nLi, G., Zhan, W., Lee, J. D., Chi, Y., and Chen, Y. (2023b). Reward-agnostic fine-tuning: Provable statistical benefits of hybrid reinforcement learning. arXiv preprint arXiv:2305.10282.   \nMin, Y., Wang, T., Zhou, D., and Gu, Q. (2021). Variance-aware off-policy evaluation with linear function approximation. Advances in neural information processing systems, 34:7598\u20137610.   \nMunos, R. and Szepesva\u00b4ri, C. (2008). Finite-time bounds for fitted value iteration. Journal of Machine Learning Research, 9(27):815\u2013857.   \nNahum-Shani, I., Smith, S. N., Spring, B. J., Collins, L. M., Witkiewitz, K. A., Tewari, A., and Murphy, S. A. (2017). Just-in-time adaptive interventions (jitais) in mobile health: Key components and design principles for ongoing health behavior support. Annals of Behavioral Medicine: A Publication of the Society of Behavioral Medicine, 52:446 \u2013 462.   \nNair, A., Gupta, A., Dalal, M., and Levine, S. (2020). Awac: Accelerating online reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359.   \nNakamoto, M., Zhai, Y., Singh, A., Mark, M. S., Ma, Y., Finn, C., Kumar, A., and Levine, S. (2023). Cal-ql: Calibrated offline rl pre-training for efficient online fine-tuning.   \nQiao, D. and Wang, Y.-X. (2022). Near-optimal deployment efficiency in reward-free reinforcement learning with linear function approximation. arXiv preprint arXiv:2210.00701.   \nSong, Y., Zhou, Y., Sekhari, A., Bagnell, J. A., Krishnamurthy, A., and Sun, W. (2023). Hybrid rl: Using both offline and online data can make rl efficient.   \nSutton, R. S. and Barto, A. G. (2018). Reinforcement Learning: An Introduction. The MIT Press, second edition.   \nTan, K. and Xu, Z. (2024). A natural extension to online algorithms for hybrid RL with limited coverage. Reinforcement Learning Journal, 1.   \nVecerik, M., Hester, T., Scholz, J., Wang, F., Pietquin, O., Piot, B., Heess, N., Rotho\u00a8rl, T., Lampe, T., and Riedmiller, M. (2017). Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. arXiv preprint arXiv:1707.08817.   \nWagenmaker, A., Chen, Y., Simchowitz, M., Du, S. S., and Jamieson, K. (2022). First-order regret in reinforcement learning with linear function approximation: A robust estimation approach.   \nWagenmaker, A. and Jamieson, K. (2023). Instance-dependent near-optimal policy identification in linear mdps via online experiment design.   \nWagenmaker, A. and Pacchiano, A. (2023). Leveraging offline data in online reinforcement learning.   \nXie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal, A. (2023). Bellman-consistent pessimism for offline reinforcement learning.   \nXie, T., Foster, D. J., Bai, Y., Jiang, N., and Kakade, S. M. (2022a). The role of coverage in online reinforcement learning. arXiv preprint arXiv:2210.04157.   \nXie, T., Jiang, N., Wang, H., Xiong, C., and Bai, Y. (2022b). Policy finetuning: Bridging sampleefficient offline and online reinforcement learning.   \nXiong, W., Zhong, H., Shi, C., Shen, C., Wang, L., and Zhang, T. (2023). Nearly minimax optimal offline reinforcement learning with linear function approximation: Single-agent mdp and markov game.   \nYang, L. F. and Wang, M. (2019). Sample-optimal parametric q-learning using linearly additive features.   \nYin, M., Duan, Y., Wang, M., and Wang, Y.-X. (2022). Near-optimal offline reinforcement learning with linear representation: Leveraging variance information with pessimism.   \nZanette, A., Wainwright, M. J., and Brunskill, E. (2021). Provable benefits of actor-critic methods for offline reinforcement learning.   \nZhan, W., Huang, B., Huang, A., Jiang, N., and Lee, J. D. (2022). Offline reinforcement learning with realizability and single-policy concentrability.   \nZhang, Z., Chen, Y., Lee, J. D., and Du, S. S. (2023). Settling the sample complexity of online reinforcement learning. arXiv preprint arXiv:2307.13586.   \nZhou, D. and Gu, Q. (2022). Computationally efficient horizon-free reinforcement learning for linear mixture mdps.   \nZhou, D., Gu, Q., and Szepesvari, C. (2021). Nearly minimax optimal reinforcement learning for linear mixture markov decision processes.   \nZhou, Y., Sekhari, A., Song, Y., and Sun, W. (2023). Offline data enhanced on-policy policy gradient with provable guarantees.   \n1: Input: Offline dataset $\\mathcal{D}_{\\mathrm{off}}$ , samples sizes $N_{\\mathrm{on}}$ , $N_{\\mathrm{off}}$ , feature maps $\\phi_{h}$ , , tolerance parameter for reward-agnostic exploration $\\tau$ .   \n2: Initialize: $\\mathcal{D}_{h}^{(0)}\\;\\leftarrow\\;\\emptyset\\quad\\forall h\\;\\in\\;[H],\\;\\lambda\\;=\\;1/H^{2},$ , $\\beta_{2}~=~\\tilde{O}(\\sqrt{d})$ . Set functions to optimize $\\begin{array}{r}{f_{i}(\\mathbf{A})=\\eta_{i}^{-1}\\log\\left(\\sum_{\\phi\\in\\Phi}\\exp(\\eta_{i}\\|\\phi\\|_{\\mathbf{A}_{i}(\\Lambda)^{-1}}^{2})\\right),\\mathbf{A}_{i}(\\Lambda)=\\Lambda+(T_{i}K_{i})^{-1}(\\Lambda_{0,i}+\\Lambda_{\\mathrm{off}})}\\end{array}$ for some $\\mathbf{\\Lambda}_{\\Lambda_{0,i}}$ satisfying $\\mathbf{A}_{0,i}\\geq\\mathbf{A}_{0}$ for all $i$ , and $\\eta_{i}=2^{2i/5}$ . Exploration Phase: Run an exploration algorithm (OPTCOV, Wagenmaker and Jamieson (2023)) to collect covariates $\\Lambda_{h}$ such that $\\begin{array}{r l}&{\\operatorname*{max}_{\\phi_{h}\\in\\Phi}\\phi_{h}^{\\top}\\big(\\mathbf{\\mathring{A}}_{h}+\\lambda\\mathbf{I}+\\mathbf{\\mathring{A}}_{\\mathrm{off},h}\\big)^{-1}\\phi_{h}\\leqslant\\tau}\\end{array}$ .   \n3: for $i=1,2,3,\\dots$ do   \n4: Set the number of iterates $T_{i}\\gets2^{i}$ , episodes per iterate $K_{i}\\gets2^{i}$ .   \n5: Play any policy for $K_{i}$ episodes to collect covariates ${\\Gamma}_{0}$ and data $\\mathfrak{D}_{0}$ .   \n6: Initialize covariance matrix $\\mathbf{\\Lambda}_{\\mathbf{1}}\\gets\\mathbf{\\Gamma}_{\\mathbf{1}}/K$ .   \n7: for $t=1,...,T_{i}$ do   \n8: if $\\begin{array}{r}{\\sum_{j=1}^{i}T_{j}K_{j}\\geqslant N_{\\mathrm{on}}}\\end{array}$ then   \n9: break   \n10: end if   \n11: Run FORCE (Wagenmaker et al., 2022) or another regret-minimizing algorithm on the exploration-focused synthetic reward $g_{h}^{(t)}(s,a)\\alpha\\mathrm{tr}(-\\nabla_{\\Lambda}f_{i}(\\Lambda)|_{\\Lambda=\\Lambda_{t}\\phi(s,a)\\phi(s,a)^{\\top}})$ .   \n12: Collect covariates $\\mathbf{\\Gamma}_{t}$ , data $\\mathfrak{D}_{t}$ .   \n13: Perform Frank-Wolfe update: $\\begin{array}{r}{\\mathbf{\\Gamma}_{t+1}\\leftarrow(1-\\frac{1}{t+1})\\mathbf{A}_{t}+\\frac{1}{t+1}\\mathbf{\\Gamma}_{t}/K_{i}}\\end{array}$ .   \n14: end for   \n15: Assign $\\widehat{\\mathbf{A}_{i,h}}\\gets\\mathbf{A}_{T_{i}+1},\\mathfrak{D}_{i}\\gets\\cup_{t=0}^{T_{i}}\\mathfrak{D}_{t}$ .   \n16: Set $\\mathbf{\\Lambda}_{\\mathbf{\\Lambda}_{h}}=\\widehat{\\Lambda_{i,h}},\\mathcal{D}_{\\mathrm{on}}=\\mathfrak{D}_{i}$ .   \n17: if $f_{i}(\\widehat{\\pmb{\\Lambda}_{i}})\\leqslant K_{i}T_{i}\\tau$ then   \n18: break   \n19: end if   \n20: end for Planning Phase: Estimate $\\widehat{\\pi}$ using a pessimistic offline RL algorithm (LinPEVI-ADV $^+$ , Xiong et al. (2023)) with hyperpa rpameters $\\lambda,\\beta_{2}$ on the combined dataset ${\\mathcal{D}}_{\\mathrm{off}}\\cup\\big\\{{\\mathcal{D}}_{h}^{(N_{\\mathrm{on}})}\\big\\}_{h\\in[H]}$   \n21: Split the dataset ${\\mathcal{D}}_{\\mathrm{off}}\\cup\\{{\\mathcal{D}}_{h}^{(N_{\\mathrm{on}})}\\}_{h\\in[H]};$ into $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ . Estimate, on $\\mathcal{D}^{\\prime}$ , ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\beta}_{h,2}=\\underset{\\beta\\in\\mathbb{R}^{d}}{\\mathrm{argmin}}\\,\\sum_{\\tau\\in\\mathcal{D}^{\\prime}}\\Big[\\big\\langle\\phi\\left(s_{h}^{\\tau},a_{h}^{\\tau}\\right),\\beta\\big\\rangle-\\left(\\widehat{V}_{h+1}^{\\prime}\\right)^{2}\\left(s_{h+1}^{\\tau}\\right)\\Big]^{2}+\\lambda\\|\\beta\\|_{2}^{2},}\\\\ &{\\widetilde{\\beta}_{h,1}=\\underset{\\beta\\in\\mathbb{R}^{d}}{\\mathrm{argmin}}\\,\\sum_{\\tau\\in\\mathcal{D}^{\\prime}}\\Big[\\big\\langle\\phi\\left(s_{h}^{\\tau},a_{h}^{\\tau}\\right),\\beta\\big\\rangle-\\widehat{V}_{h+1}^{\\prime}\\left(s_{h+1}^{\\tau}\\right)\\Big]^{2}+\\lambda\\|\\beta\\|_{2}^{2}.}\\\\ &{\\widehat{\\sigma}_{h}^{2}(s,a):=\\operatorname*{max}\\Big\\{1,\\big[\\phi(s,a)^{\\top}\\widetilde{\\beta}_{h,2}\\big]_{[0,H^{2}]}-\\big[\\phi(s,a)^{\\top}\\widetilde{\\beta}_{h,1}\\big]_{[0,H]}^{2}-\\widetilde{O}\\Big(\\frac{d H^{3}}{\\sqrt{N\\kappa}}\\Big)\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "22: for $h=1,...,H$ do   \n23: Compute covariance matrix $\\begin{array}{r}{\\Sigma_{h}=\\sum_{\\tau\\in\\mathcal{D}}\\phi\\left(s_{h}^{\\tau},a_{h}^{\\tau}\\right)\\phi\\left(s_{h}^{\\tau},a_{h}^{\\tau}\\right)^{\\top}\\left/\\widehat\\sigma_{h}^{2}\\left(s_{h}^{\\tau},a_{h}^{\\tau}\\right)+\\lambda\\mathbf{I_{d}}.}\\end{array}$   \n24: Compute weightswh \u201c \u03a3h\u00b41\u00b4  \u03c4 P\u0159D \u03d5 ps\u03c4h, a\u03c4hqr\u03c4h\\`\u03c3 2Vphps\\`\u03c41,pas\u03c4\u03c4hq\\`1q\u00af.   \n25: Compute pessimis tpic penalty \u0393hp\u00a8, \u00a8q \u00d0 \u03b22}\u03d5p\u00a8, \u00a8q}p\u03a3\u00b41 .   \n26: Compute pessimistic Q-functionQhp\u00a8, \u00a8q \u00d0 \u03d5p\u00a8, \u00a8qqJwh \u00b4 \u0393hp\u00a8, \u00a8q r0,H\u00b4h\\`1s.   \n27: Set $\\begin{array}{r}{\\widehat{\\pi}_{h}(\\cdot\\mid\\cdot)\\gets\\arg\\operatorname*{max}_{\\pi_{h}}\\big\\langle\\widehat{Q}_{h}(\\cdot,\\cdot),\\pi_{h}(\\cdot\\mid\\cdot)\\big\\rangle_{\\mathcal{A}},\\widehat{V}_{h}(\\cdot)\\gets\\big\\langle\\widehat{Q}_{h}(\\cdot,\\cdot),\\widehat{\\pi}_{h}(\\cdot\\mid\\cdot)\\big\\rangle_{\\mathcal{A}}.}\\end{array}$ .   \n28: end for   \n29: Output: $\\widehat{\\pi}$ . ", "page_idx": 13}, {"type": "text", "text": "1: Input: Offline dataset $\\mathcal{D}_{\\mathrm{off}}$ , samples sizes $N_{\\mathrm{on}}$ , $N_{\\mathrm{off}}$ , feature maps $\\phi_{h}$ . Regularization parameter $\\lambda>0$ , confidence radii $\\beta,\\bar{\\beta},\\bar{\\beta},t_{\\mathrm{last}}=0$ . ", "page_idx": 14}, {"type": "text", "text": "2: Initialize: For $h\\,\\in\\,[H]$ , estimate $\\widehat{\\mathbf{w}}_{1,h},\\check{\\mathbf{w}}_{1,h},Q_{1,h},\\check{Q}_{1,h},\\sigma_{1,h},\\bar{\\sigma}_{1,h}$ from $\\mathcal{D}_{\\mathrm{off}}$ with the same formulas outlined below, and assig n $\\begin{array}{r}{\\pmb{\\Sigma}_{0,h}=\\pmb{\\Sigma}_{1,h}=\\pmb{\\Sigma}_{\\mathrm{off}}+\\lambda\\mathbf{I}=\\sum_{n=1}^{N_{\\mathrm{off}}}\\bar{\\sigma}_{n,h}^{-2}\\phi_{n,h}\\phi_{n,h}^{\\top}+\\lambda\\mathbf{I}}\\end{array}$ . 3: for episodes $t=1,...,T$ do ", "page_idx": 14}, {"type": "text", "text": "Receive the initial state $s_{1}^{(t)}$ ", "page_idx": 14}, {"type": "equation", "text": "$\\begin{array}{r}{\\widehat{\\mathbf{w}}_{k,h}={\\uppercase{\\mathbf{D}}}_{t,h}^{-1}\\sum_{i=1}^{t-1}\\bar{\\sigma}_{i,h}^{-2}\\phi(s_{h}^{(i)},a_{h}^{(i)})V_{t,h+1}(s_{h+1}^{(i)}).}\\end{array}$ ", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$\\begin{array}{r}{\\check{\\mathbf{w}}_{t,h}=\\Sigma_{t,h}^{-1}\\sum_{i=1}^{t-1}\\bar{\\sigma}_{i,h}^{-2}\\phi(s_{h}^{(i)},a_{h}^{(i)})\\check{V}_{t,h+1}(s_{h+1}^{(i)})}\\end{array}$ ", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "9: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{t,h}(s,a)=\\operatorname*{min}\\Big\\{r_{h}(s,a)+\\widehat{\\mathbf{w}}_{t,h}^{\\top}\\phi(s,a)+\\beta\\sqrt{\\phi(s,a)^{\\top}\\Sigma_{t,h}^{-1}\\!\\phi(s,a)},Q_{t-1,h}(s,a),H\\Big\\}.}\\\\ &{\\widecheck{Q}_{t,h}(s,a)=\\operatorname*{max}\\Big\\{r_{h}(s,a)+\\check{\\mathbf{w}}_{t,h}^{\\top}\\phi(s,a)-\\bar{\\beta}\\sqrt{\\phi(s,a)^{\\top}\\Sigma_{t,h}^{-1}\\!\\phi(s,a)},\\check{Q}_{t-1,h}(s,a),0\\Big\\}.}\\\\ &{\\widecheck{\\ldots}\\widecheck{\\ldots}\\widecheck{\\ldots}\\widecheck{\\ldots}\\widecheck{\\ldots}\\widecheck{\\ldots}\\widecheck{\\ldots}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "else ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\check{Q}_{t,h}(s,a)=Q_{t-1,h}(s,a),\\check{Q}_{t,h}(s,a)=\\check{Q}_{t-1,h}(s,a).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "10:  \n11:  \n12:  \n13:  \n14:  \n15:  \n16:  \n17:  \n18:", "page_idx": 14}, {"type": "text", "text": "end if ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overleftrightarrow{V_{t,h}(s)}=\\operatorname*{max}_{a}Q_{t,h}(s,a),\\,\\breve{V}_{t,h}(s)=\\operatorname*{max}_{a}\\breve{Q}_{t,h}(s,a).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "end for ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "for horizon $h=1,...,H$ do ", "page_idx": 14}, {"type": "text", "text": "19: ", "page_idx": 14}, {"type": "text", "text": "20: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{t,h}=\\operatorname*{min}\\left\\{\\left\\langle\\tilde{\\beta}\\left\\|\\Sigma_{t,h}^{-1/2}\\phi(s_{h}^{(t)},a_{h}^{(t)})\\right\\|_{2},H^{2}\\right\\rangle+\\operatorname*{min}\\left\\{2H\\tilde{\\beta}\\left\\|\\Sigma_{t,h}^{-1/2}\\phi(s_{h}^{(t)},a_{h}^{(t)})\\right\\|_{2},H^{2}\\right\\},}\\\\ &{D_{t,h}=\\operatorname*{min}\\left\\{4d^{3}H^{2}\\Bigg(\\hat{\\mathbf{w}}_{t,h}^{\\top}\\phi(s_{h}^{(t)},a_{h}^{(t)})-\\check{\\mathbf{w}}_{t,h}^{\\top}\\phi(s_{h}^{(t)},a_{h}^{(t)})}\\\\ &{\\qquad\\qquad+\\;2\\tilde{\\beta}\\sqrt{\\phi(s_{h}^{(t)},a_{h}^{(t)})^{\\top}\\Sigma_{t,h}^{-1}\\phi(s_{h}^{(t)},a_{h}^{(t)})}\\Bigg),d^{3}H^{3}\\right\\}.}\\\\ &{\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;}\\\\ &{\\delta_{t,h}\\leftarrow\\operatorname*{max}\\left\\{\\sigma_{t,h},\\sqrt{H},2d^{3}H^{2}\\left\\|\\phi\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)\\right\\|_{\\Sigma_{t,h}^{-1}}^{1/2}\\right\\}.}\\\\ &{\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\Sigma_{t+1,h}=\\Sigma_{t,h}+\\bar{\\sigma}_{t,h}^{-2}\\phi\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)\\phi\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "21: ", "page_idx": 14}, {"type": "text", "text": "22:   \n23: ", "page_idx": 14}, {"type": "text", "text": "Receive reward $\\boldsymbol{r}_{h}^{(t)}$ , next state $s_{h+1}^{(t)}$ ", "page_idx": 14}, {"type": "text", "text": "end for ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "24: end for ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "25: Output: Greedy policy $\\widehat{\\pi}=\\pi^{Q_{T,h}}$ , $\\operatorname{Unif}(\\pi^{Q_{1,h}},...,\\pi^{Q_{T,h}})$ for PAC guarantee. ", "page_idx": 14}, {"type": "text", "text": "B Proofs for Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The proof of Theorem 1 follows from a series of distinct results, presented as three lemmas below. The first lemma demonstrates that RAPPEL achieves no higher error than LinPEVI-ADV $^+$ itself, the second produces a $d_{\\mathrm{on}}d H^{4}$ error bound, while the third produces a $d_{\\mathrm{on}}^{2}d H^{3}$ error bound via a slightly different truncation argument. We will prove Equation 7 in Lemma 2, which act as a general statistical guarantee for RAPPEL. We show the validity of the instance-dependent bound developed from Equation 7 in Lemmas 3 and 4. We observe that Theorem 1 follows immediately after. ", "page_idx": 15}, {"type": "text", "text": "Lemma 2 (General Statistical Guarantee for RAPPEL, Algorithm 1). For every $\\delta\\in(0,1)$ and any partition $\\mathcal{X}_{\\mathrm{off}},\\mathcal{X}_{\\mathrm{on}}$ , with probability at least $1-\\delta$ , RAPPEL achieves ", "page_idx": 15}, {"type": "equation", "text": "$$\nV_{1}^{*}(s)-V_{1}^{\\hat{\\pi}}(s)\\lesssim\\sqrt{d}\\sum_{h=1}^{H}\\mathbb{E}_{\\pi^{*}}\\big\\lVert\\phi(s_{h},a_{h})\\big\\rVert_{(\\Sigma_{\\mathrm{off},h}^{*}+\\Sigma_{\\mathrm{ou},h}^{*})^{-1}}\\leqslant\\sqrt{d}\\sum_{h=1}^{H}\\mathbb{E}_{\\pi^{*}}\\big\\lVert\\phi(s_{h},a_{h})\\big\\rVert_{\\Sigma_{\\mathrm{off},h}^{*-1}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Before we proof the desired result, we first recall that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\Lambda}_{h}=\\displaystyle\\sum_{\\tau\\in\\mathcal{D}}\\phi\\left(s_{h}^{\\tau},a_{h}^{\\tau}\\right)\\phi\\left(s_{h}^{\\tau},a_{h}^{\\tau}\\right)^{\\top}+\\boldsymbol{I}_{d},}\\\\ &{\\boldsymbol{\\Sigma}_{h}^{*}=\\displaystyle\\sum_{\\tau\\in\\mathcal{D}}\\phi\\left(s_{h}^{\\tau},a_{h}^{\\tau}\\right)\\phi\\left(s_{h}^{\\tau},a_{h}^{\\tau}\\right)^{\\top}/\\left[\\mathbb{V}_{h}V_{h+1}^{*}\\right]\\left(s_{h}^{\\tau},a_{h}^{\\tau}\\right)+\\boldsymbol{\\lambda}I_{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The?n, by invoking Theorem 2 from Xiong et al. (2023) with $N\\;>\\;\\Omega(d^{2}H^{6}),\\lambda\\;=\\;1/H^{2},\\beta_{1}\\;=\\;$ $O({\\sqrt{d}})$ , we see that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{1}^{*}(s)-V_{1}^{\\widehat{\\pi}}(s)\\lesssim\\sqrt{d}\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}_{\\pi^{*}}\\left[\\|\\phi(s_{h},a_{h})\\|_{\\Sigma_{h}^{*-1}}\\mid s_{1}=s\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\sqrt{d}\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}_{\\pi^{*}}\\left[\\|\\phi(s_{h},a_{h})\\|_{(\\Sigma_{\\mathrm{off},h}^{*}+\\Sigma_{\\mathrm{on},h}^{*})^{-1}}\\mid s_{1}=s\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "as $\\Sigma_{\\mathrm{off},h}^{*}+\\Sigma_{\\mathrm{on},h}^{*}$ $\\Sigma_{h}\\ =\\ \\Sigma_{\\mathrm{off},h}^{*}\\ +\\ \\Sigma_{\\mathrm{on},h}^{*}$ . Therefore, \u00b7 Noting that \u03a3o\u02dan,h is positive semi-definite, it then follows \u03a3o\u02daf $\\boldsymbol{\\Sigma}_{\\mathrm{off},h}^{*}\\;\\preceq\\;$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sqrt{d}\\sum_{h=1}^{H}\\mathbb{E}_{\\pi^{*}}||\\phi(s_{h},a_{h})||_{(\\Sigma_{\\mathrm{off},h}^{*}+\\Sigma_{\\mathrm{on},h}^{*})^{-1}}\\leqslant\\sqrt{d}\\sum_{h=1}^{H}\\mathbb{E}_{\\pi^{*}}||\\phi(s_{h},a_{h})||_{\\Sigma_{\\mathrm{off},h}^{*-1}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and the inequality holds. ", "page_idx": 15}, {"type": "text", "text": "Lemma 3 (First Error Bound for RAPPEL, Algorithm 1). For every $\\delta\\,\\in\\,(0,1)$ and any partition $\\mathcal{X}_{\\mathrm{off}},\\mathcal{X}_{\\mathrm{on}}$ , with probability at least $1-\\delta$ , RAPPEL achieves ", "page_idx": 15}, {"type": "equation", "text": "$$\nV_{1}^{*}(s)-V_{1}^{\\hat{\\pi}}(s)\\lesssim\\sqrt{\\frac{c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})d H^{4}}{N_{\\mathrm{off}}}}+\\sqrt{\\frac{d_{\\mathrm{on}}d H^{4}}{N_{\\mathrm{on}}}},\\,\\,w h e r e\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$N\\,\\geqslant\\,\\operatorname*{max}\\left\\{\\alpha_{\\mathrm{on}}^{4}d_{\\mathrm{on}}^{-4},\\alpha_{\\mathrm{off}}^{4}c_{\\mathrm{off}}(\\chi_{\\mathrm{off}})^{-4}\\right\\}\\operatorname*{max}\\{N^{*},p o l y(d,H,c_{\\mathrm{off}}(\\chi_{\\mathrm{off}}),\\log1/\\delta)\\}$ , where we define the quantities $\\begin{array}{r}{\\alpha_{\\mathrm{off}}=\\frac{N_{\\mathrm{off}}}{N}}\\end{array}$ , $\\begin{array}{r}{\\alpha_{\\mathrm{on}}=\\frac{N_{\\mathrm{on}}}{N}}\\end{array}$ ,( and the minimal samples for coverage is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{V}^{*}=\\operatorname*{min}_{N}C\\cdot N\\;s.t.\\;\\operatorname*{inf}_{\\Lambda\\in\\Omega}\\mathrm{max}\\,\\phi^{\\top}\\left(N(\\Lambda+\\bar{\\lambda}I)+\\Lambda_{\\mathrm{off}}\\right)^{-1}\\phi\\leqslant\\tilde{O}(\\operatorname*{max}\\{d_{\\mathrm{on}}/N_{\\mathrm{on}},c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})/N_{\\mathrm{off}}\\}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Let $\\mathcal{X}_{\\mathrm{off}},\\mathcal{X}_{\\mathrm{on}}$ be an arbitrary partition of $S\\times A\\times[H]$ . Let us leave the choice of OPTCOV tolerance unspecified for the moment, and simply assume for now that we have data $\\mathcal{D}$ collected under the success event of Lemma 16. ", "page_idx": 15}, {"type": "text", "text": "We now invoke Theorem 2 from Xiong et al. (2023) on this dataset. As we choose $N>\\Omega(d^{2}H^{6})$ , $\\lambda=1/H^{2}$ and $\\beta_{1}=O({\\sqrt{d}})$ , we obtain the suboptimality gap decomposition below: ", "page_idx": 15}, {"type": "equation", "text": "$$\nV_{1}^{*}(s)-V_{1}^{\\hat{\\pi}}(s)\\lesssim\\sqrt{d}\\sum_{h=1}^{H}\\mathbb{E}_{\\pi^{*}}\\left[\\|\\phi(s_{h},a_{h})\\|_{\\Sigma_{h}^{*-1}}\\;|\\;s_{1}=s\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This decomposition can be further decomposed into the sum of bonuses on the offline and online partitions $\\chi_{\\mathrm{off}}$ and $\\chi_{\\mathrm{on}}$ , respectively: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sqrt{d}\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}_{\\pi^{*}}\\left[\\left\\|\\phi(s_{h},a_{h})\\right\\|_{\\Sigma_{h}^{*-1}}\\mid s_{1}=s\\right]}\\\\ {\\displaystyle==\\sqrt{d}\\displaystyle\\sum_{h=1}^{H}\\left(\\mathbb{E}_{\\pi^{*}}\\left[\\left\\|\\phi(s_{h},a_{h})\\right\\|_{\\Sigma_{h}^{*-1}}\\mathbb{E}_{\\sigma_{m}}\\mid s_{1}=s\\right]+\\mathbb{E}_{\\pi^{*}}\\left[\\left\\|\\phi(s_{h},a_{h})\\right\\|_{\\Sigma_{h}^{*-1}}\\mathbb{E}_{\\sigma\\mid\\mathbf{\\bar{\\phi}}}\\mid s_{1}=s\\right]\\right)}\\\\ {\\displaystyle=\\sqrt{d}\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}_{\\pi^{*}}\\left[\\sqrt{\\phi(s_{h},a_{h})^{\\top}\\Sigma_{h}^{*-1}\\phi(s_{h},a_{h})}\\mathbb{1}_{\\mathcal{X}_{\\sigma_{m}}}\\mid s_{1}=s\\right]}\\\\ {\\displaystyle\\ \\ \\ +\\sqrt{d}\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}_{\\pi^{*}}\\left[\\sqrt{\\phi(s_{h},a_{h})^{\\top}\\Sigma_{h}^{*-1}\\phi(s_{h},a_{h})}\\mathbb{1}_{\\mathcal{X}_{\\sigma\\mid\\mathbf{\\bar{\\phi}}}}\\mid s_{1}=s\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We can further upper bound the above expectations under the optimal policy $\\pi^{*}$ by taking the maximum of the quadratic form over each partition, yielding ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sqrt{d}\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}_{\\boldsymbol\\pi^{*}}\\left[\\left\\|\\phi\\left(s_{h},a_{h}\\right)\\right\\|_{\\Sigma_{h}^{*-1}}\\mid s_{1}=s\\right]}\\\\ {\\displaystyle=\\sqrt{d}\\displaystyle\\sum_{h=1}^{H}\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{on}}}\\sqrt{\\phi_{h}^{\\top}\\Sigma_{h}^{*-1}\\phi_{h}}\\mathbb{I}_{\\mathcal{X}_{\\mathrm{on}}}+\\sqrt{d}\\displaystyle\\sum_{h=1}^{H}\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{off}}}\\sqrt{\\phi_{h}^{\\top}\\Sigma_{h}^{*-1}\\phi_{h}}\\mathbb{I}_{\\mathcal{X}_{\\mathrm{off}}}}\\\\ {\\displaystyle\\leqslant\\sqrt{d}\\displaystyle\\sum_{h=1}^{H}\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{on}}}\\sqrt{\\phi_{h}^{\\top}\\Sigma_{h}^{*-1}\\phi_{h}}+\\sqrt{d}\\displaystyle\\sum_{h=1}^{H}\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{off}}}\\sqrt{\\phi_{h}^{\\top}\\Sigma_{h}^{*-1}\\phi_{h}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From Xiong et al. (2023), as $\\left[\\mathbb{V}_{h}V_{h+1}^{*}\\right](\\cdot,\\cdot)\\in\\big[1,H^{2}\\big],$ , the weighted covariance matrix is uniformly upper bounded by the unwei\u201cghted cov\u2030ariance m\u201catrix i\u2030n the following manner: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma_{h}^{*-1}\\le H^{2}\\Lambda_{h}^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which leads to our conclusion that ", "page_idx": 16}, {"type": "equation", "text": "$$\nV_{1}^{*}(s)-V_{1}^{\\hat{\\pi}}(s)\\lesssim\\sqrt{d}\\sum_{h=1}^{H}\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{on}}}\\sqrt{H^{2}\\phi_{h}^{\\top}\\Lambda_{h}^{-1}\\phi_{h}}+\\sqrt{d}\\sum_{h=1}^{H}\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{off}}}\\sqrt{H^{2}\\phi_{h}^{\\top}\\Lambda_{h}^{-1}\\phi_{h}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We now further bound the above two quadratic forms over the online and offline partitions respectively. By Lemma 1, the partial online coverage coefficient is bounded by the dimensionality of the online partition: ", "page_idx": 16}, {"type": "equation", "text": "$$\nc_{\\mathrm{on}}(\\mathcal{X}_{\\mathrm{on}})=\\operatorname*{inf}_{\\pi}\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{on}}}\\phi_{h}^{\\top}\\mathbb{E}_{\\bar{\\phi}_{h}\\sim d_{h}^{\\pi}}[\\bar{\\phi}_{h}\\bar{\\phi}_{h}^{\\top}]^{-1}\\phi_{h}\\leqslant d_{\\mathrm{on}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As we have $N_{\\mathrm{on}}$ online episodes, the optimal covariates for online exploration would then yield ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\mathbf{A}}\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{on}}}\\phi_{h}^{\\top}\\mathbf{A}^{-1}\\phi_{h}\\lesssim c_{\\mathrm{on}}(\\mathcal{X}_{\\mathrm{on}})/N_{\\mathrm{on}}\\leqslant d_{\\mathrm{on}}/N_{\\mathrm{on}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Conversely, we also have access to $N_{\\mathrm{off}}$ episodes of offline data with the following guarantee that follows from an application of Matrix Chernoff: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{off}}}\\phi_{h}^{\\top}\\Lambda_{\\mathrm{off}}^{-1}\\phi_{h}\\lesssim c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})/N_{\\mathrm{off}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, by Lemma 5, we can conclude that on its success event, running OPTCOV with tolerance $\\tilde{O}(\\operatorname*{max}\\{d_{\\mathrm{on}}\\}/N_{\\mathrm{on}},c_{\\mathrm{off}}(\\chi_{\\mathrm{off}})/N_{\\mathrm{off}}\\})$ , provides us covariates such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\phi_{h}\\in\\Phi}\\phi_{h}^{\\top}\\mathbf{A}_{h}^{-1}\\phi_{h}\\lesssim\\operatorname*{max}\\left\\{c_{\\mathrm{off}}(\\boldsymbol{\\chi}_{\\mathrm{off}})/N_{\\mathrm{off}},d_{\\mathrm{on}}/N_{\\mathrm{on}}\\right\\},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "yielding the desired result. ", "page_idx": 16}, {"type": "text", "text": "It now remains to work out the burn-in cost from running OPTCOV. The following quantity of the minimal online samples any algorithm requires to establish coverage was first proposed in Wagenmaker and Pacchiano (2023): ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{V}^{\\ast}=\\operatorname*{min}_{N}C\\cdot N\\;\\mathrm{s.t.}\\;\\operatorname*{inf}_{\\Lambda\\in\\Omega}\\operatorname*{max}_{\\phi\\in\\Phi}\\phi^{\\top}\\left(N(\\Lambda+\\bar{\\lambda}I)+\\Lambda_{\\mathrm{off}}\\right)^{-1}\\phi\\leqslant\\frac{\\tilde{O}(\\operatorname*{max}\\{d_{\\mathrm{on}}/N_{\\mathrm{on}},c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})/N_{\\mathrm{off}}\\})}{6}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We can use this as follows. Invoking Lemma 16, we see that OPTCOV incurs ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{\\left(\\frac{N_{\\mathrm{off}}}{c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})}\\right)^{4/5},\\ \\left(\\frac{N_{\\mathrm{on}}}{d_{\\mathrm{on}}}\\right)^{4/5}\\right\\}\\operatorname*{max}\\{N^{*},\\mathrm{poly}(d,H,c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}}),\\log1/\\delta)\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "episodes of online exploration, for an overall burn-in cost of ", "page_idx": 17}, {"type": "equation", "text": "$$\nN_{\\mathrm{off}}+N_{\\mathrm{on}}\\geqslant\\operatorname*{max}\\left\\{\\frac{\\alpha_{\\mathrm{on}}^{4}}{d_{\\mathrm{on}}^{4}},\\ \\ \\frac{\\alpha_{\\mathrm{off}}^{4}}{c_{\\mathrm{off}}(\\chi_{\\mathrm{off}})^{4}}\\right\\}\\operatorname*{max}\\{N^{*},\\mathrm{poly}(d,H,c_{\\mathrm{off}}(\\chi_{\\mathrm{off}}),\\log1/\\delta)\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "episodes, where $\\begin{array}{r}{\\alpha_{\\mathrm{off}}\\,=\\,\\frac{N_{\\mathrm{off}}}{N_{\\mathrm{off}}+N_{\\mathrm{on}}}}\\end{array}$ and $\\begin{array}{r}{\\alpha_{\\mathrm{on}}=\\frac{N_{\\mathrm{on}}}{N_{\\mathrm{off}}+N_{\\mathrm{on}}}}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Note that the more even the proportion of offline to online samples, the smaller $\\alpha_{\\mathrm{off}},\\alpha_{\\mathrm{on}}$ are. In fact, as \u03b1o4ff, \u03b1o4 $\\alpha_{\\mathrm{off}}^{4},\\alpha_{\\mathrm{on}}^{4}\\in[0.0625,1]$ , this term contributes no more than a constant factor that is no greater than 1 to the final sample complexity. ", "page_idx": 17}, {"type": "text", "text": "We then have that ", "page_idx": 17}, {"type": "equation", "text": "$$\nV_{1}^{*}(s)-V_{1}^{\\hat{\\pi}}(s)\\lesssim\\operatorname*{inf}_{\\chi_{\\mathrm{off}},\\chi_{\\mathrm{on}}}\\left(\\sqrt{\\frac{c_{\\mathrm{off}}(\\chi_{\\mathrm{off}})d H^{4}}{N_{\\mathrm{off}}}}+\\sqrt{\\frac{d_{\\mathrm{on}}d H^{4}}{N_{\\mathrm{on}}}}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with probability at least 1\u00b4\u03b4, when N \u011b max!d\u03b14oonn , coffp\u03b1Xofofffq4) $\\begin{array}{r}{\\colon N\\geqslant\\operatorname*{max}\\Big\\{\\frac{\\alpha_{\\mathrm{on}}^{4}}{d_{\\mathrm{on}}^{4}},\\ \\ \\frac{\\alpha_{\\mathrm{off}}^{4}}{c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})^{4}}\\Big\\}\\operatorname*{max}\\{N^{*},\\mathrm{poly}(d,H,c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}}),\\underline{{\\log{1/\\delta}}})\\}.}\\end{array}$ \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Lemma 4 (Second Error Bound for RAPPEL, Algorithm 1). For every $\\delta\\in(0,1)$ and any partition $\\mathcal{X}_{\\mathrm{off}},\\mathcal{X}_{\\mathrm{on}}$ , with probability at least $1-\\delta$ , RAPPEL achieves ", "page_idx": 17}, {"type": "equation", "text": "$$\nV_{1}^{*}(s)-V_{1}^{\\hat{\\pi}}(s)\\lesssim\\sqrt{\\frac{c_{\\mathrm{off}}(\\chi_{\\mathrm{off}})^{2}d H^{3}}{N_{\\mathrm{off}}\\alpha_{\\mathrm{off}}}}+\\sqrt{\\frac{d_{\\mathrm{on}}^{2}d H^{3}}{N_{\\mathrm{on}}\\alpha_{\\mathrm{on}}}},\\,\\,w h e r e\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": ", \u03b1o4ffcoffpXoffq\u00b44 maxtN \u02da, polypd, H, coffpXoffq, log 1{\u03b4qu, we define the quantities $\\begin{array}{r}{\\alpha_{\\mathrm{off}}=\\frac{N_{\\mathrm{off}}}{N}}\\end{array}$ NNoff , \u03b1on \u201c $\\begin{array}{r}{\\alpha_{\\mathrm{on}}=\\frac{N_{\\mathrm{on}}}{N}}\\end{array}$ , and (the minimal samples for coverage is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{V}^{*}=\\operatorname*{min}_{N}C\\cdot N\\;s.t.\\;\\operatorname*{inf}_{\\Lambda\\in\\Omega}\\mathrm{max}\\,\\phi^{\\top}\\left(N(\\Lambda+\\bar{\\lambda}I)+\\Lambda_{\\mathrm{off}}\\right)^{-1}\\phi\\leqslant\\tilde{O}(\\operatorname*{max}\\{d_{\\mathrm{on}}/N_{\\mathrm{on}},c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})/N_{\\mathrm{off}}\\}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. First, we set up some preliminaries. Following the same argument as the proof of Lemma 3, we can establish that, for arbitrary partition $\\mathcal{X}=\\mathcal{X}_{\\mathrm{on}}\\cup\\mathcal{X}_{\\mathrm{off}}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nc_{\\mathrm{on}}(\\mathcal{X}_{\\mathrm{on}})\\leqslant d_{\\mathrm{on}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and running OPTCOV with tolerance $\\tilde{O}(\\operatorname*{max}\\{d_{\\mathrm{on}}/N_{\\mathrm{on}},c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})/N_{\\mathrm{off}}\\})$ , yields: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\phi_{h}\\in\\Phi}\\phi_{h}^{\\top}\\Lambda_{h}^{-1}\\phi_{h}\\lesssim\\operatorname*{max}\\left\\{c_{\\mathrm{off}}(\\chi_{\\mathrm{off}})/N_{\\mathrm{off}},d_{\\mathrm{on}}/N_{\\mathrm{on}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This incurs ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{\\left(\\frac{N_{\\mathrm{off}}}{c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})}\\right)^{4/5},\\ \\left(\\frac{N_{\\mathrm{on}}}{d_{\\mathrm{on}}}\\right)^{4/5}\\right\\}\\operatorname*{max}\\{N^{*},\\mathrm{poly}(d,H,c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}}),\\log1/\\delta)\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "episodes of online exploration, for an overall burn-in cost of ", "page_idx": 17}, {"type": "equation", "text": "$$\nN_{\\mathrm{off}}+N_{\\mathrm{on}}\\geqslant\\operatorname*{max}\\left\\{\\frac{\\alpha_{\\mathrm{on}}^{4}}{d_{\\mathrm{on}}^{4}},\\ \\ \\frac{\\alpha_{\\mathrm{off}}^{4}}{c_{\\mathrm{off}}(\\chi_{\\mathrm{off}})^{4}}\\right\\}\\operatorname*{max}\\{N^{*},\\mathrm{poly}(d,H,c_{\\mathrm{off}}(\\chi_{\\mathrm{off}}),\\log1/\\delta)\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "episodes. ", "page_idx": 18}, {"type": "text", "text": "To tighten the horizon dependence even further from the result of Lemma 3, we turn to the total variance lemma. i.e. Lemma C.5 in Jin et al. (2018), indicating that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{N H}\\sum_{\\tau\\in\\mathcal{D}}\\sum_{h=1}^{H}\\left[\\mathbb{V}_{h}V_{h+1}^{*}\\right](s_{h}^{\\tau},a_{h}^{\\tau})\\lesssim\\tilde{O}\\left(H+\\frac{H^{2}}{N}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, we directly apply Lemma 17 with $\\gamma=\\operatorname*{max}\\left\\{d_{\\mathrm{on}}/N_{\\mathrm{on}},c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})/N_{\\mathrm{off}}\\right\\}$ and $\\bar{\\sigma}=H+H^{2}/N$ , we will then obtain that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{h=1}^{H}\\operatorname*{max}\\sqrt{\\phi_{h}^{\\top}\\Sigma_{h}^{*-1}\\phi_{h}}\\leqslant\\bigg(\\frac{d_{\\mathrm{on}}}{N_{\\mathrm{on}}}+\\frac{c_{\\mathrm{off}}(\\bar{X}_{\\mathrm{off}})}{N_{\\mathrm{off}}}\\bigg)H\\sqrt{N\\bigg(H+\\frac{H^{2}}{N}\\bigg)}}}\\\\ &{\\leqslant\\bigg(\\frac{d_{\\mathrm{on}}}{N_{\\mathrm{on}}}+\\frac{c_{\\mathrm{off}}(\\bar{X}_{\\mathrm{off}})}{N_{\\mathrm{off}}}\\bigg)\\sqrt{N H^{3}+H^{4}}}\\\\ &{\\leqslant\\sqrt{\\frac{c_{\\mathrm{off}}(\\bar{X}_{\\mathrm{off}})^{2}H^{3}}{N_{\\mathrm{off}}\\alpha_{\\mathrm{off}}}+\\frac{c_{\\mathrm{off}}(\\bar{X}_{\\mathrm{off}})^{2}H^{4}}{N_{\\mathrm{off}}^{2}}}+\\sqrt{\\frac{d_{\\mathrm{on}}^{2}H^{3}}{N_{\\mathrm{on}}\\alpha_{\\mathrm{on}}}+\\frac{d_{\\mathrm{on}}^{2}H^{4}}{N_{\\mathrm{on}}^{2}}}}\\\\ &{\\lesssim\\sqrt{\\frac{c_{\\mathrm{off}}(\\bar{X}_{\\mathrm{off}})^{2}H^{3}}{N_{\\mathrm{off}}\\alpha_{\\mathrm{off}}}}+\\sqrt{\\frac{d_{\\mathrm{on}}^{2}H^{3}}{N_{\\mathrm{on}}\\alpha_{\\mathrm{on}}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which leads to our final result: ", "page_idx": 18}, {"type": "equation", "text": "$$\nV_{1}^{*}(s)-V_{1}^{\\hat{\\pi}}(s)\\lesssim\\operatorname*{inf}_{\\chi_{\\mathrm{off}},\\chi_{\\mathrm{on}}}\\left(\\sqrt{\\frac{c_{\\mathrm{off}}(\\chi_{\\mathrm{off}})^{2}d H^{3}}{N_{\\mathrm{off}}\\alpha_{\\mathrm{off}}}}+\\sqrt{\\frac{d_{\\mathrm{on}}^{2}d H^{3}}{N_{\\mathrm{on}}\\alpha_{\\mathrm{on}}}}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\alpha_{\\mathrm{off}}=N_{\\mathrm{off}}/N$ and $\\alpha_{\\mathrm{on}}=N_{\\mathrm{on}}/N$ . ", "page_idx": 18}, {"type": "text", "text": "C Proof of Corollary 1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. In tabular case, we set $\\phi(s,a)=\\mathbf{1}_{s,a}$ and $d=|S|\\cdot|A|$ . Let $N_{h}(s,a)$ be the number of visits to a specific state-action pair $(s,a,h)$ . As the exploration algorithm OPTCOV ensures that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{s,a,h}\\frac{1}{N_{h}(s,a)}\\leqslant\\operatorname*{max}\\left(\\frac{d_{\\mathrm{on}}}{N_{\\mathrm{on}}},\\frac{c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})}{N_{\\mathrm{off}}}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "we bound the error in the following way follows from Lemma 2, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{V_{1}^{*}(s)-V_{1}^{\\hat{\\pi}}(s)\\lesssim\\sqrt{d}\\sum_{h=1}^{H}\\mathbb{E}_{\\pi^{*}}||\\phi(s_{h},a_{h})||_{(\\Sigma_{\\mathrm{off},h}^{*}+\\Sigma_{\\mathrm{on},h}^{*})^{-1}}}}\\\\ &{}&{\\leqslant\\sqrt{|S||A|}\\sum_{h=1}^{H}\\sum_{s,a}d_{h}^{*}(s,a)\\sqrt{\\frac{\\left[\\mathbb{V}_{h}V_{h+1}^{*}\\right](s,a)}{N_{h}(s,a)}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last inequality follows from the fact that \u03a3\u2039h \u201c diag Nhps, aq{ VhV h\u02da\\`1 ps, aq sPS,aPA. We will then decompose the state-action space into $\\chi_{\\mathrm{off}}$ and $\\chi_{\\mathrm{on}}$ , and boun\u201cd the two\u2030 parts s\u02d8eperately based on the tolerance level of OPTCOV, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{1}^{*}(s)-V_{1}^{\\hat{\\pi}}(s)\\lesssim\\sqrt{|{\\cal S}||{\\cal A}|}\\displaystyle\\sum_{h=1}^{H}\\sum_{s,a}d_{h}^{*}(s,a)\\sqrt{\\frac{[\\mathbb{V}_{h}V_{h+1}^{*}]\\left(s,a\\right)}{N_{h}(s,a)}}\\mathbb{1}_{{\\boldsymbol X}_{\\mathrm{off}}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\sqrt{|{\\cal S}||{\\cal A}|}\\displaystyle\\sum_{h=1}^{H}\\sum_{s,a}d_{h}^{*}(s,a)\\sqrt{\\frac{[\\mathbb{V}_{h}V_{h+1}^{*}]\\left(s,a\\right)}{N_{h}(s,a)}}\\mathbb{1}_{{\\boldsymbol X}_{\\mathrm{on}}}}\\\\ &{\\leqslant\\sqrt{\\frac{|{\\cal S}||{\\cal A}||{\\cal c}_{\\mathrm{off}}\\left({\\boldsymbol X}_{\\mathrm{off}}\\right)}{N_{\\mathrm{off}}}}\\displaystyle\\sum_{h=1}^{H}\\sum_{s,a}d_{h}^{*}(s,a)\\sqrt{\\left[\\mathbb{V}_{h}V_{h+1}^{*}\\right]\\left(s,a\\right)}\\mathbb{1}_{{\\boldsymbol X}_{\\mathrm{off}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\sqrt{\\frac{|S||A|d_{\\mathrm{on}}}{N_{\\mathrm{on}}}\\displaystyle\\sum_{h=1}^{H}\\sum_{s,a}d_{h}^{\\star}(s,a)\\sqrt{\\left[\\mathbb{V}_{h}V_{h+1}^{\\ast}\\right](s,a)}}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}}\\\\ &{\\leqslant\\sqrt{|S||A|}\\left(\\sqrt{\\frac{c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})}{N_{\\mathrm{off}}}}+\\sqrt{\\frac{d_{\\mathrm{on}}}{N_{\\mathrm{on}}}}\\right)\\displaystyle\\sum_{h=1}^{H}\\sum_{s,a}\\sqrt{d_{h}^{\\star}(s,a)\\left[\\mathbb{V}_{h}V_{h+1}^{\\ast}\\right](s,a)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "As the optimal policy $\\pi^{\\star}$ executes a deterministic action $\\pi^{\\star}(s)$ for any state $s$ , the inequality can be further bounded as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\gamma_{1}^{*}(s)-V_{1}^{\\widehat{\\pi}}(s)\\lesssim\\sqrt{|{\\mathcal{S}}||\\mathcal{A}|}\\left(\\sqrt{\\frac{c_{\\mathrm{off}}(X_{\\mathrm{off}})}{N_{\\mathrm{off}}}}+\\sqrt{\\frac{d_{\\mathrm{on}}}{N_{\\mathrm{on}}}}\\right)\\sum_{h=1}^{H}\\sum_{h}^{\\widehat{\\pi}}\\sqrt{d_{h}^{*}(s,\\pi^{*}(s))\\left[\\nabla_{h}V_{h+1}^{*}\\right]\\left(s,\\pi^{*}(s)\\right)}}}\\\\ &{}&{\\leqslant\\sqrt{H|{\\mathcal{S}}|^{2}|\\mathcal{A}|}\\left(\\sqrt{\\frac{c_{\\mathrm{off}}(X_{\\mathrm{off}})}{N_{\\mathrm{off}}}}+\\sqrt{\\frac{d_{\\mathrm{on}}}{N_{\\mathrm{on}}}}\\right)\\sqrt{\\displaystyle\\sum_{h=1}^{H}\\sum_{s}d_{h}^{*}(s,\\pi^{*}(s))\\left[\\nabla_{h}V_{h+1}^{*}\\right]\\left(s,\\pi^{*}(s)\\right)}}\\\\ &{}&{\\leqslant\\sqrt{H|{\\mathcal{S}}|^{2}|\\mathcal{A}|}\\left(\\sqrt{\\frac{c_{\\mathrm{off}}(X_{\\mathrm{off}})}{N_{\\mathrm{off}}}}+\\sqrt{\\frac{d_{\\mathrm{on}}}{N_{\\mathrm{on}}}}\\right)\\sqrt{\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}_{(s,\\pi)\\sim d_{\\pi^{*}}}\\left[\\nabla_{h}V_{h+1}^{*}\\right]\\left(s,a\\right)}}\\\\ &{}&{\\leqslant\\sqrt{H^{3}|{\\mathcal{S}}|^{2}|\\mathcal{A}|}\\left(\\sqrt{\\frac{c_{\\mathrm{off}}(X_{\\mathrm{off}})}{N_{\\mathrm{off}}}}+\\sqrt{\\frac{d_{\\mathrm{on}}}{N_{\\mathrm{on}}}}\\right)\\sqrt{\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}_{(s,\\pi)\\sim d_{\\pi^{*}}}\\left[\\nabla_{h}V_{h+1}^{*}\\right]\\left(s,a\\right)}}\\\\ &{}&{\\leqslant\\sqrt{H^{3}|{\\mathcal{S}}|^{2}|\\mathcal{A}|}\\left(\\sqrt{\\frac{c_{\\mathrm{off}}(X_{\\mathrm{off}})}{N_{\\mathrm{o f f\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last inequality follows from the proof of Lemma C.5. in Jin et al. (2018). ", "page_idx": 19}, {"type": "text", "text": "D On concentrability and coverability ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma 1. For any partition $\\mathcal{X}_{\\mathrm{off}},\\mathcal{X}_{\\mathrm{on}}$ , we have that $c_{\\mathrm{on}}(\\mathcal{X}_{\\mathrm{on}})\\,\\leqslant\\,d_{\\mathrm{on}}$ . Similarly, there exists a partition such that $c_{\\mathrm{off}}\\left(\\mathcal{X}_{\\mathrm{off}}\\right)=O(d)$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. This proof follows a similar strategy to that of Lemma B.10 in Wagenmaker and Jamieson (2023), except that we exploit the projections onto $d_{\\mathrm{on}}$ to get a bound that depends on $d_{\\mathrm{on}}\\,\\leqslant\\,d$ , instead of $d$ . We wish to bound ", "page_idx": 19}, {"type": "equation", "text": "$$\nc_{\\mathrm{on}}(\\mathcal{X}_{\\mathrm{on}})=\\operatorname*{inf}_{\\pi}\\operatorname*{max}_{h}\\frac{1}{\\lambda_{d_{\\mathrm{on}}}(\\mathbb{E}_{d_{h}^{\\pi}}[(\\mathcal{P}_{\\mathrm{on}}\\phi_{h})(\\mathcal{P}_{\\mathrm{on}}\\phi_{h})^{\\top}])}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "$\\mathcal{P}_{\\mathrm{on}}\\in\\mathbb{R}^{d\\times d}$ has rank $d_{\\mathrm{on}}\\leqslant d$ , so we can decompose this with the thin SVD into $\\mathcal{P}_{\\mathrm{on}}\\,=\\,U_{\\mathrm{on}}U_{\\mathrm{on}}^{\\top}$ , where $U_{\\mathrm{on}}\\in\\mathbb{R}^{d\\times d_{\\mathrm{on}}}$ . It then holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lambda_{d_{\\mathrm{on}}}(\\mathbb{E}_{d_{h}^{\\pi}}[(\\mathcal{P}_{\\mathrm{on}}\\phi_{h})(\\mathcal{P}_{\\mathrm{on}}\\phi_{h})^{\\top}])=\\lambda_{\\operatorname*{min}}(\\mathbb{E}_{d_{h}^{\\pi}}[(U_{\\mathrm{on}}^{\\top}\\phi_{h})(U_{\\mathrm{on}}^{\\top}\\phi_{h})^{\\top}]),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and from Lemma 20 that ", "page_idx": 19}, {"type": "equation", "text": "$$\nc_{\\mathrm{on}}(\\mathcal{X}_{\\mathrm{on}})=\\operatorname*{inf}_{\\pi}\\operatorname*{sup}_{v_{h}\\in\\Phi_{\\mathrm{on}}}v_{h}^{\\top}U_{\\mathrm{on}}E_{d_{h}^{\\pi}}[(U_{\\mathrm{on}}^{\\top}\\phi_{h})(U_{\\mathrm{on}}^{\\top}\\phi_{h})^{\\top}]^{-1}U_{\\mathrm{on}}^{\\top}v_{h}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Apply Jensen\u2019s inequality to find that for any $v_{h}\\in\\Phi_{\\mathrm{on}}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v_{h}^{\\top}U_{\\mathrm{on}}E_{d_{h}^{\\top}}[(U_{\\mathrm{on}}^{\\top}\\phi_{h})(U_{\\mathrm{on}}^{\\top}\\phi_{h})^{\\top}]U_{\\mathrm{on}}^{\\top}v_{h}\\geqslant v_{h}^{\\top}U_{\\mathrm{on}}\\mathbb{E}_{\\phi_{h}\\sim d_{h}^{\\top}}[U_{\\mathrm{on}}^{\\top}\\phi_{h}]\\mathbb{E}_{\\phi_{h}\\sim d_{h}^{\\top}}[U_{\\mathrm{on}}^{\\top}\\phi_{h}]^{\\top}U_{\\mathrm{on}}^{\\top}v_{h}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, we can bound ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c_{\\mathrm{on}}(\\mathcal{X}_{\\mathrm{on}})=\\operatorname*{inf}_{\\begin{array}{l}{\\pi_{\\mathrm{\\Huge~v}}\\in\\Phi_{\\mathrm{on}}}\\\\ {\\leqslant\\operatorname*{inf}_{\\begin{array}{l}{\\forall_{h}\\in\\Phi_{\\mathrm{on}}}\\\\ {\\leqslant\\rho_{h}\\in\\Phi_{\\mathrm{on}}}\\end{array}}}v_{h}^{\\top}U_{\\mathrm{on}}E_{d_{h}^{\\top}}\\big[(U_{\\mathrm{on}}^{\\top}\\phi_{h})(U_{\\mathrm{on}}^{\\top}\\phi_{h})^{\\top}\\big]^{-1}U_{\\mathrm{on}}^{\\top}v_{h}}\\end{array}}\\\\ &{\\qquad\\qquad\\leqslant\\operatorname*{inf}_{\\begin{array}{l}{\\forall_{h}\\in\\Phi_{\\mathrm{on}}}\\\\ {\\leqslant\\rho_{h}\\in\\Phi_{\\mathrm{on}}}\\end{array}}v_{h}^{\\top}U_{\\mathrm{on}}\\left(\\mathbb{E}_{\\pi\\sim\\rho}\\left[\\mathbb{E}_{\\phi_{h}\\sim d_{h}^{\\top}}[U_{\\mathrm{on}}^{\\top}\\phi_{h}]E_{\\phi_{h}\\sim d_{h}^{\\top}}[U_{\\mathrm{on}}^{\\top}\\phi_{h}^{\\top}]\\right]\\right)^{-1}U_{\\mathrm{on}}^{\\top}v_{h}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By Kiefer-Wolfowitz (Lattimore et al., 2020), this is bounded by $d_{\\mathrm{on}}$ . ", "page_idx": 19}, {"type": "text", "text": "Similarly, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathcal{X}_{\\mathrm{off}},\\mathcal{X}_{\\mathrm{on}}}{\\operatorname*{inf}}c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})=\\underset{\\mathcal{X}_{\\mathrm{off}},\\mathcal{X}_{\\mathrm{on}}}{\\operatorname*{inf}}\\underset{h}{\\operatorname*{max}}\\,\\frac{1}{\\lambda_{d_{\\mathrm{off}}}\\left(\\mathbb{E}_{\\mu_{h}}\\left[(\\mathcal{P}_{\\mathrm{off}}\\phi_{h})(\\mathcal{P}_{\\mathrm{off}}\\phi_{h})^{\\top}\\right]\\right)}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\underset{\\mathcal{X}_{\\mathrm{off}},\\mathcal{X}_{\\mathrm{on}}}{\\operatorname*{inf}}\\underset{h}{\\operatorname*{max}}\\,\\frac{1}{\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}_{\\mu_{h}}\\left[(U_{\\mathrm{off}}^{\\top}\\phi_{h})(U_{\\mathrm{off}}^{\\top}\\phi_{h})^{\\top}\\right]\\right)}}\\\\ &{\\quad\\quad\\quad\\quad\\leqslant O(d).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the upper bound is achieved when, for instance, we choose $\\chi_{\\mathrm{off}}$ such that $\\begin{array}{r l}{\\Phi_{\\mathrm{off}}}&{{}=}\\end{array}$ Span $\\bigl((v_{h,1},...,v_{h,k_{h}})_{h\\in[H]}\\bigr)$ , where $v_{h,i}$ is the $i$ -th largest eigenvector of $\\begin{array}{r l}{\\mathbb{E}_{\\mu}[\\phi_{h}\\phi_{h}^{\\top}]}&{{}\\approx}\\end{array}$ $\\begin{array}{r}{\\frac{1}{N_{\\mathrm{off}}}\\sum_{\\tau\\in\\mathcal{D}_{\\mathrm{off}}}\\phi_{h}\\big(s_{h}^{\\tau},a_{h}^{\\tau}\\big)\\phi_{h}\\big(s_{h}^{\\tau},a_{h}^{\\tau}\\big)^{\\top}}\\end{array}$ , and $v_{h,k_{h}}$ is the eigenvector corresponding to the largest eigen\u0159value $\\lambda_{h,k_{h}}\\,\\geqslant\\,\\Omega(1/k_{h})$ . The largest eigenvalue $\\lambda_{h,1}$ is always $\\Omega(1/d)$ for non-null features, so there always exists such a partition where $d_{\\mathrm{off}}$ is at least 1. ", "page_idx": 20}, {"type": "text", "text": "Informally, one can choose the offline partition to be the span of the large eigenvectors of the covariance matrix, so the smallest eigenvalue of the projected covariance matrix, i.e. the partial all policy concentrability coefficient, is no larger than the dimension of the partition. ", "page_idx": 20}, {"type": "text", "text": "Lemma 5 (Maximum Eigenvalue Bound with OPTCOV). On any partition $\\mathcal{X}_{\\mathrm{off}},\\mathcal{X}_{\\mathrm{on}}$ , if we run OPTCOV with tolerance $\\tilde{O}(\\operatorname*{max}\\{d_{\\mathrm{on}}/N_{\\mathrm{on}},c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})/N_{\\mathrm{off}}\\})$ , on this partition we also have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\phi_{h}\\in\\Phi}\\phi_{h}^{\\top}\\Lambda_{h}^{-1}\\phi_{h}\\lesssim\\operatorname*{max}\\left\\{c_{\\mathrm{off}}(\\chi_{\\mathrm{off}})/N_{\\mathrm{off}},d_{\\mathrm{on}}/N_{\\mathrm{on}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. By Lemma 1, for any partition, we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\nc_{\\mathrm{on}}(\\mathcal{X}_{\\mathrm{on}})=\\operatorname*{inf}_{\\pi}\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{on}}}\\phi_{h}^{\\top}\\mathbb{E}_{\\bar{\\phi}_{h}\\sim d_{h}^{\\pi}}[\\bar{\\phi}_{h}\\bar{\\phi}_{h}^{\\top}]^{-1}\\phi_{h}\\leqslant d_{\\mathrm{on}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Applying Matrix Chernoff, we have that with probability at least $1-\\delta$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\substack{\\gamma_{h}\\in\\Phi_{\\mathrm{off}}}}\\phi_{h}^{\\top}\\Lambda_{h,\\mathrm{off}}^{-1}\\phi_{h}\\leqslant\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{off}}}\\phi_{h}^{\\top}\\mathbb{E}_{\\bar{\\phi}_{h}\\sim\\mu_{h}}[\\bar{\\phi}_{h}\\bar{\\phi}_{h}^{\\top}+N_{\\mathrm{off}}^{-1}\\mathbf{I})^{-1}\\phi_{h}N_{\\mathrm{off}}^{-1}\\left(1-\\sqrt{\\frac{2}{N_{\\mathrm{off}}}\\log\\left(\\frac{4d}{\\delta}\\right)}\\right)^{-1},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and similarly for $c_{\\mathrm{on}}(\\mathcal X_{\\mathrm{on}})$ we also have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\pi}\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{on}}}\\phi_{h}^{\\top}\\Lambda_{h,\\pi}^{-1}\\phi_{h}\\leqslant\\operatorname*{inf}_{\\pi}\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{on}}}\\phi_{h}^{\\top}\\mathbb{E}_{\\bar{\\phi}_{h}\\sim\\mu_{h}}[\\bar{\\phi}_{h}\\bar{\\phi}_{h}^{\\top}]^{-1}\\phi_{h}N_{\\mathrm{on}}^{-1}\\left(1-\\sqrt{\\frac{2}{N_{\\mathrm{on}}}\\log\\left(\\frac{4d}{\\delta}\\right)}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "As $\\Lambda_{h,\\mathrm{off}}+\\Lambda_{h,\\mathrm{on}}=\\Lambda_{h}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\underset{\\phi_{h}\\in\\Phi}{\\operatorname*{max}}\\,\\phi_{h}^{\\top}\\Lambda_{h}^{-1}\\phi_{h}=\\operatorname*{max}\\left\\{\\underset{\\phi_{h}\\in\\Phi_{\\mathrm{off}}}{\\operatorname*{max}}\\,\\phi_{h}^{\\top}\\Lambda_{h}^{-1}\\phi_{h},\\,\\underset{\\phi_{h}\\in\\Phi_{\\mathrm{on}}}{\\operatorname*{max}}\\,\\phi_{h}^{\\top}\\Lambda_{h}^{-1}\\phi_{h}\\right\\}}\\\\ &{}&{\\lesssim\\operatorname*{max}\\left\\{c_{\\mathrm{off}}(\\chi_{\\mathrm{off}})/N_{\\mathrm{off}},\\,\\underset{\\phi_{h}\\in\\Phi_{\\mathrm{on}}}{\\operatorname*{max}}\\,\\phi_{h}^{\\top}\\Lambda_{h}^{-1}\\phi_{h}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last step follows from the choice of partition. So it suffices to run OPTCOV with tolerance $\\tilde{O}(\\operatorname*{max}\\{d_{\\mathrm{on}}/N_{\\mathrm{on}},c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})/N_{\\mathrm{off}}\\})$ , ", "page_idx": 20}, {"type": "text", "text": "to find that there exists at least one partition such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\phi_{h}\\in\\Phi}\\phi_{h}^{\\top}\\Lambda_{h}^{-1}\\phi_{h}\\lesssim\\operatorname*{max}\\left\\{c_{\\mathrm{off}}(\\chi_{\\mathrm{off}})/N_{\\mathrm{off}},d_{\\mathrm{on}}/N_{\\mathrm{on}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma 6 (Coverability Coefficient Is Bounded In Tabular MDPs). If the underlying MDP is tabular, for any partition $\\mathcal{X}_{\\mathrm{off}},\\mathcal{X}_{\\mathrm{on.}}$ , we have that $c_{\\mathrm{on}}(\\mathcal{X}_{\\mathrm{on}})\\leqslant d_{\\mathrm{on}}$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. First, we write the concentrability coefficient in terms of densities. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c_{\\mathrm{on}}(\\mathcal{X}_{\\mathrm{on}})=\\underset{\\pi}{\\mathrm{min}}\\,\\underset{h}{\\mathrm{max}}\\,\\frac{1}{\\lambda_{d_{\\mathrm{on}}}\\left(\\mathbb{E}_{d_{h}^{\\pi}}\\left[(\\mathcal{P}_{\\mathrm{on}}\\phi_{h})(\\mathcal{P}_{\\mathrm{on}}\\phi_{h})^{\\top}\\right]\\right)}}\\\\ &{\\phantom{\\times}\\leqslant\\underset{\\pi}{\\mathrm{min}}\\,\\underset{h}{\\mathrm{max}}\\,\\frac{\\mathbb{1}_{\\mathcal{X}_{o n}}}{\\operatorname*{min}_{s,a}d_{h}^{\\pi}\\left(s,a\\right)\\mathbb{1}_{\\mathcal{X}_{o n}}}}\\\\ &{\\leqslant\\underset{\\pi}{\\mathrm{min}}\\,\\underset{h,s,a}{\\mathrm{max}}\\,\\frac{\\mathbb{1}_{\\mathcal{X}_{o n}}}{d_{h}^{\\pi}\\left(s,a\\right)\\mathbb{1}_{\\mathcal{X}_{o n}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By the same trick that Xie et al. (2022a) use in their Lemma 3, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}}{d_{h}^{\\pi}\\left(s,a\\right)\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}}\\leqslant\\frac{\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}}{\\operatorname*{sup}_{\\pi^{\\prime\\prime}}d_{h}^{\\pi^{\\prime\\prime}}\\left(s,a\\right)\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}/\\sum_{s^{\\prime},a^{\\prime}}\\operatorname*{sup}_{\\pi^{\\prime}}d_{h}^{\\pi^{\\prime}}\\left(s^{\\prime},a^{\\prime}\\right)\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}}}\\\\ &{\\leqslant\\frac{\\sum_{s,a}\\operatorname*{sup}_{\\pi}d_{h}^{\\pi}\\left(s,a\\right)\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}}{\\operatorname*{sup}_{\\pi}d_{h}^{\\pi}\\left(s,a\\right)\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}}}\\\\ &{\\leqslant d_{\\mathrm{on}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "E Proofs for Algorithm 2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "E.1 Setup ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We consider the same state-action space splitting framework of Tan and $\\mathrm{Xu}$ (2024). Let $\\mathcal{X}_{\\mathrm{on}}\\cup\\mathcal{X}_{\\mathrm{off}}=$ $[H]\\times S\\times A$ . Then, their images under the feature map $\\Phi_{\\mathrm{off}}\\,=\\,\\mathrm{Span}(\\phi(\\mathcal{X}_{\\mathrm{off},h}))_{h\\in[H]}\\,\\subseteq\\,\\mathbb{R}^{d}$ and $\\Phi_{\\mathrm{on}}\\,=\\,\\mathrm{Span}(\\phi(\\mathcal{X}_{\\mathrm{on},h}))_{h\\in[H]}\\,\\subseteq\\,\\mathbb{R}^{d}$ are subspaces of $\\mathcal{X}$ with dimension $d_{\\mathrm{off}}$ and $d_{\\mathrm{on}}$ , respectively. We denote $\\mathcal{P}_{\\mathrm{off}},\\mathcal{P}_{\\mathrm{on}}$ as the orthogonal projection operators onto these subspaces respectively. The partial offline all-policy concentrability coefficient ", "page_idx": 21}, {"type": "equation", "text": "$$\nc_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})=\\operatorname*{max}_{h}\\frac{1}{\\lambda_{d_{\\mathrm{off}}}(\\mathbb{E}_{\\mu_{h}}[(\\mathcal{P}_{\\mathrm{off}}\\phi_{h})(\\mathcal{P}_{\\mathrm{off}}\\phi_{h})^{\\top}])},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "is bounded by the inverse of the $d_{\\mathrm{off}}$ -th largest eigenvalue of the covariance matrix of the projected feature maps onto the offline partition, where $\\lambda_{k}$ is the $k$ -th largest eigenvalue. Write $\\mathbb{I}_{\\mathcal{X}_{\\mathrm{on}}}$ as shorthand for $\\mathbb{1}((s,a,h)\\in\\mathcal{X}_{\\mathrm{on}}\\big)$ , and similarly for $\\mathbb{1}_{\\mathcal{X}_{\\mathrm{off}}}$ . ", "page_idx": 21}, {"type": "text", "text": "Now, we work through the analysis of He et al. (2023) to ensure that their result holds in our setting, where the regret decomposes into online part $\\|\\Sigma_{t,h}^{-1/2}\\phi_{h}(s_{h}^{(t)},a_{h}^{(t)})\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\|_{2}$ and offline part $\\|\\Sigma_{t,h}^{-1/2}\\phi_{h}(s_{h}^{(t)},a_{h}^{(t)})\\mathbb{1}_{\\mathcal{X}_{\\mathrm{off}}}\\|_{2}$ respectively, instead of $\\|\\boldsymbol{\\Sigma}_{t,h}^{-1/2}\\phi_{h}(s_{h}^{(t)},a_{h}^{(t)})\\|_{2}$ . ", "page_idx": 21}, {"type": "text", "text": "E.2 High-probability events ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We define several \u201chigh probability\u201d events which are similar to those defined in He et al. (2023). ", "page_idx": 21}, {"type": "text", "text": "\u2022 We define $\\widetilde{w}_{t,h}$ as the solution of the weighted ridge regression problem for the squared value function ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widetilde{w}_{t,h}=\\Sigma_{t,h}^{-1}\\sum_{i=1}^{t-1}\\bar{\\sigma}_{i,h}^{-2}\\phi(s_{h}^{(i)},a_{h}^{(i)})V_{t,h+1}^{2}(s_{h+1}^{(i)}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "\u2022 We define $\\mathcal{E}$ as the event where the following inequalities hold for all $s,a,t,h\\in S\\times A\\times$ $[T]\\times[H]$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\widehat{\\mathbf{w}}_{t,h}^{\\top}\\phi(s,a)-\\left[\\mathbb{P}_{h}V_{t,h+1}\\right](s,a)\\right|\\leqslant\\bar{\\beta}\\sqrt{\\phi(s,a)^{\\top}{\\Sigma}_{t,h}^{-1}\\phi(s,a)},}\\\\ {\\left|\\widetilde{\\mathbf{w}}_{t,h}^{\\top}\\phi(s,a)-\\left[\\mathbb{P}_{h}V_{t,h+1}^{2}\\right](s,a)\\right|\\leqslant\\tilde{\\beta}\\sqrt{\\phi(s,a)^{\\top}{\\Sigma}_{t,h}^{-1}\\phi(s,a)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\vert\\check{\\mathbf{w}}_{t,h}^{\\top}\\phi(s,a)-\\left[{\\mathbb{P}}_{h}\\check{V}_{t,h+1}\\right](s,a)\\right\\vert\\leqslant\\bar{\\beta}\\sqrt{\\phi(s,a)^{\\top}\\Sigma_{t,h}^{-1}\\phi(s,a)},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\tilde{\\beta}=O\\left(H^{2}\\sqrt{d\\lambda}+\\sqrt{d^{3}H^{4}\\log^{2}(d H N/(\\delta\\lambda))}\\right),\\bar{\\beta}=O\\left(H\\sqrt{d\\lambda}+\\sqrt{d^{3}H^{2}\\log^{2}(d H N/(\\delta\\lambda))}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This is the \u201ccoarse event\u201d as mentioned in their paper, where concentration holds for the value and squared value function with all three estimators. ", "page_idx": 22}, {"type": "text", "text": "\u2022 We define $\\widetilde{\\mathcal{E}}_{h}$ as the event that for all episodes $t\\in[T]$ , stages $h\\leqslant h^{\\prime}\\leqslant H$ and state-action pairs $(s,a)\\in S\\times A$ , the weight vector $\\widehat{\\mathbf{w}}_{t,h}$ satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\vert\\widehat{\\mathbf{w}}_{t,h^{\\prime}}^{\\top}\\phi(s,a)-\\left[\\mathbb{P}_{h}V_{t,h^{\\prime}+1}\\right](s,a)\\right\\vert\\leqslant\\beta\\sqrt{\\phi(s,a)^{\\top}\\Sigma_{t,h^{\\prime}}^{-1}\\phi(s,a)},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\beta=O\\left(H\\sqrt{d\\lambda}+\\sqrt{d\\log^{2}(1+d N H/(\\delta\\lambda))}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Furthermore, let $\\tilde{\\mathcal{E}}=\\tilde{\\mathcal{E}}_{1}$ denotes the event that (19) holds for all stages $h\\in[H]$ . This is the fine event where concentration for $\\hat{\\bf w}$ is tighter than that required in (16) to (18). ", "page_idx": 22}, {"type": "text", "text": "Equipped with these definitions, we recall the following lemmas from He et al. (2023): ", "page_idx": 22}, {"type": "text", "text": "Lemma 7 (Lemma B.1, He et al. (2023)). $\\mathcal{E}$ holds with probability at least $1-7\\delta$ . ", "page_idx": 22}, {"type": "text", "text": "Lemma 8 (Lemma B.2, He et al. (2023)). On the event $\\mathcal{E}$ and $\\widetilde{\\mathcal{E}}_{h+1}$ , for each episode $t\\in[T]$ and stage $h,$ , the estimated variance satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left[\\overline{{\\mathbb{V}}}_{h}V_{t,h+1}\\right]\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)-\\left[\\mathbb{V}_{h}V_{t,h+1}\\right]\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)\\right|\\leqslant E_{t,h},}\\\\ &{\\left|\\left[\\overline{{\\mathbb{V}}}_{h}V_{t,h+1}\\right]\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)-\\left[\\mathbb{V}_{h}V_{h+1}^{*}\\right]\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)\\right|\\leqslant E_{t,h}+D_{t,h}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma 9 (Lemma B.3, He et al. (2023)). On the event $\\mathcal{E}$ and $\\tilde{\\mathcal{E}}_{h+1}$ , for any episode t and $i>t$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left[\\mathbb{V}_{h}\\left(V_{i,h+1}-V_{h+1}^{*}\\right)\\right]\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)\\leqslant D_{t,h}/\\left(d^{3}H\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma 10 (Lemma B.4, He et al. (2023)). On the event $\\mathcal{E}$ and $\\tilde{\\mathcal{E}}_{h}$ , for all episodes $t\\,\\in\\,[T]$ and stages $h\\leqslant h^{\\prime}\\leqslant H$ , we have $Q_{t,h}(s,a)\\geqslant Q_{h}^{*}(s,a)\\geqslant\\check{Q}_{t,h}(s,a)$ .r In addition, we have $V_{t,h}(s)\\geqslant$ $V_{h}^{*}(s)\\geqslant\\check{V}_{t,h}(s)$ . ", "page_idx": 22}, {"type": "text", "text": "Lemma 1 1q (Lemma B.5, He et al. (2023)). On event $\\mathcal{E}$ , event $\\tilde{\\mathcal{E}}$ holds with probability at least $1-$ $\\delta$ . ", "page_idx": 22}, {"type": "text", "text": "E.3 Regret decomposition ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "From He et al. (2023), based on Lemma B.4 of their paper, $Q_{t,h}(s_{h}^{(t)},a_{h}^{(t)})=V_{t,h}(s_{h}^{(t)})\\geqslant V_{h}^{*}(s_{h}^{(t)})$ , i.e. optimism holds for all episodes and timesteps. Therefore, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Reg}(T)\\lesssim\\displaystyle\\sum_{t=1}^{T}\\sum_{h=1}^{H}\\left\\{\\left[\\mathbb{P}_{h}\\left(V_{t,h+1}-V_{t,h+1}^{\\pi^{(t)}}\\right)\\right]\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)-\\left(V_{t,h+1}\\left(s_{h+1}^{(t)}\\right)-V_{t,h+1}^{\\pi^{(t)}}\\left(s_{h+1}^{(t)}\\right)\\right)\\right\\}}\\\\ &{\\qquad\\qquad+\\displaystyle\\beta\\sum_{t=1}^{T}\\sum_{h=1}^{H}\\|\\Sigma_{t,h}^{-1/2}\\phi_{h}(s_{h}^{(t)},a_{h}^{(t)})\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Accordingly, given a partition $\\mathcal{X}_{\\mathrm{off}},\\mathcal{X}_{\\mathrm{on}}$ of $[H]\\times S\\times A$ , we can further decompose this into the fraction of episodes where each partition is visited, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sum_{h=1}^{H}\\|\\Sigma_{t,h}^{-1/2}\\phi_{h}(s_{h}^{(t)},a_{h}^{(t)})\\|_{2}=\\sum_{h,t}\\|\\Sigma_{t,h}^{-1/2}\\phi_{h}(s_{h}^{(t)},a_{h}^{(t)})\\mathbb{1}_{\\mathcal{X}_{\\mathrm{off}}}\\|_{2}+\\sum_{h,t}\\|\\Sigma_{t,h}^{-1/2}\\phi_{h}(s_{h}^{(t)},a_{h}^{(t)})\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "He et al. (2023) define the events ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathcal{E}_{1}=\\left\\{\\forall h\\in[H],\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{h^{\\prime}=h}^{H}\\left[\\mathbb{P}_{h}\\left(V_{t,h+1}-V_{t,h+1}^{\\pi^{(t)}}\\right)\\right]\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)\\right.}\\\\ {\\quad\\left.\\qquad\\qquad-\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{h^{\\prime}=h}^{H}\\left(V_{t,h+1}\\left(s_{h+1}^{(t)}\\right)-V_{t,h+1}^{\\pi^{(t)}}\\left(s_{h+1}^{(t)}\\right)\\right)\\leqslant2\\sqrt{2H^{3}T\\log(H/\\delta)}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathcal{E}_{2}=\\left\\{\\forall h\\in[H],\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{h^{\\prime}=h}^{H}\\left[\\mathbb{P}_{h}\\left(V_{t,h+1}-\\check{V}_{t,h+1}\\right)\\right]\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)\\right.}\\\\ {\\quad\\left.\\qquad\\qquad-\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{h^{\\prime}=h}^{H}\\left(V_{t,h+1}\\left(s_{h+1}^{(t)}\\right)-\\check{V}_{t,h+1}\\left(s_{h+1}^{(t)}\\right)\\right)\\leqslant2\\sqrt{2H^{3}T\\log(H/\\delta)}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which they show that by Azuma-Hoeffding, both hold with probability $1-\\delta$ each. As such, we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\log(T)\\lesssim\\sqrt{H^{3}T\\log(H/\\delta)}+\\sum_{h,t}\\beta\\big\\|\\mathbf{\\Sigma}_{t,h}^{-1/2}\\phi_{h}(s_{h}^{(t)},a_{h}^{(t)})\\mathbb{1}_{\\mathcal{X}_{\\mathrm{off}}}\\big\\|_{2}+\\sum_{h,t}\\beta\\big\\|\\mathbf{\\Sigma}_{t,h}^{-1/2}\\phi_{h}(s_{h}^{(t)},a_{h}^{(t)})\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\big\\|_{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here, we denote ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathsf{l e g}_{\\mathrm{off}}(T)=\\sum_{h,t}\\beta\\big\\|\\Sigma_{t,h}^{-1/2}\\phi_{h}\\big(s_{h}^{(t)},a_{h}^{(t)}\\big)\\mathbb{1}_{\\mathcal{X}_{\\mathrm{off}}}\\big\\|_{2},\\qquad\\mathsf{R e g}_{\\mathrm{on}}(T)=\\sum_{h,t}\\beta\\big\\|\\Sigma_{t,h}^{-1/2}\\phi_{h}\\big(s_{h}^{(t)},a_{h}^{(t)}\\big)\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\big\\|_{2},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "as the offline regret and online regret, respectively. ", "page_idx": 23}, {"type": "text", "text": "E.4 Offline regret control ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Now, we bound the regret on the offline partition. We first perform a similar argument to that in Tan and $\\mathrm{Xu}$ (2024); Xie et al. (2022a) to show that the sum of bonuses can be controlled by the maximum eigenvalue of the inverse weighted average covariance matrix in Lemma 12. We will then show that the maximum eigenvalue can be nicely bounded in Lemma 13. ", "page_idx": 23}, {"type": "text", "text": "Lemma 12 (Sum of Bonuses on Offline Partition). For any partition $\\mathcal{X}_{\\mathrm{off}},\\mathcal{X}_{\\mathrm{on}}$ , we can bound the sum of bonuses on the offline partition with the following: ", "page_idx": 23}, {"type": "equation", "text": "$$\nR e g_{\\mathrm{off}}(T)\\lesssim\\sum_{h=1}^{H}\\sqrt{\\frac{d N_{\\mathrm{on}}^{2}}{N_{\\mathrm{off}}}\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{off}}}\\phi_{h}^{\\top}\\bar{\\Sigma}_{\\mathrm{off},h}^{-1}\\phi_{h}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\bar{\\Sigma}_{\\mathrm{off},h}=(\\Sigma_{\\mathrm{off},h}+\\lambda I)/N_{\\mathrm{off}}$ and $T=N_{\\mathrm{on}}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. It is sufficient to show the following holds true ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{t}\\beta\\|\\Sigma_{t,h}^{-1/2}\\phi_{h}(s_{h}^{(t)},a_{h}^{(t)})\\mathbb{1}_{\\mathcal{X}_{\\mathrm{off}}}\\|_{2}\\leqslant\\sqrt{\\frac{d N_{\\mathrm{on}}^{2}}{N_{\\mathrm{off}}}\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{off}}}\\phi_{h}^{\\top}\\bar{\\Sigma}_{\\mathrm{off},h}^{-1}\\phi_{h}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "then the desired inequality directly follows. With a direct calculation, one may observe that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Sigma_{t,h}^{-1/2}\\phi_{h}(s_{h}^{(t)},a_{h}^{(t)})\\mathbb{1}_{\\mathcal{X}_{\\mathrm{off}}}\\|_{2}=\\sqrt{\\phi_{h}^{\\top}(s_{h}^{(t)},a_{h}^{(t)})\\Sigma_{t,h}^{-1}\\phi_{h}(s_{h}^{(t)},a_{h}^{(t)})\\mathbb{1}_{\\mathcal{X}_{\\mathrm{off}}}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\lesssim\\sqrt{\\phi_{h}^{\\top}(s_{h}^{(t)},a_{h}^{(t)})(\\Sigma_{\\mathrm{off},h}+\\lambda\\mathbf{I})^{-1}\\phi_{h}(s_{h}^{(t)},a_{h}^{(t)})\\mathbb{1}_{\\mathcal{X}_{\\mathrm{off}}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last inequality holds as $\\Sigma_{\\mathrm{off},h}~\\leq~\\Sigma_{t,h}$ . As a result, we are able to bound the desired inequality with the maximum eigenvalue of the inverse weighted matrix, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t}\\|\\Sigma_{t,h}^{-1/2}\\phi_{h}(s_{h}^{(t)},a_{h}^{(t)})\\|_{\\mathcal{X}_{\\mathrm{off}}}\\|_{2}\\leqslant N_{\\mathrm{on}}\\sqrt{\\underset{\\phi_{h}\\in\\Phi_{\\mathrm{off}}}{\\operatorname*{max}}\\phi_{h}^{\\top}(\\Sigma_{\\mathrm{off},h}+\\lambda\\mathbf{I})^{-1}\\phi_{h}}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad=\\sqrt{N_{\\mathrm{on}}\\frac{N_{\\mathrm{on}}}{N_{\\mathrm{off}}}\\,\\underset{\\phi_{h}\\in\\Phi_{\\mathrm{off}}}{\\operatorname*{max}}\\phi_{h}^{\\top}\\bar{\\Sigma}_{\\mathrm{off},h}^{-1}\\phi_{h}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\bar{\\Sigma}_{\\mathrm{off},h}=(\\Sigma_{\\mathrm{off},h}+\\lambda\\mathbf{I})/N_{\\mathrm{off}}$ . As $\\beta=\\tilde{O}(\\sqrt{d})$ , we obtain the bound we desired: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t}\\beta\\|\\boldsymbol{\\Sigma}_{t,h}^{-1/2}\\phi_{h}(s_{h}^{(t)},a_{h}^{(t)})\\mathbb{1}_{\\mathcal{X}_{\\mathrm{off}}}\\|_{2}\\leqslant\\sqrt{d N_{\\mathrm{on}}\\frac{N_{\\mathrm{on}}}{N_{\\mathrm{off}}}\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{off}}}\\phi_{h}^{\\top}\\bar{\\boldsymbol{\\Sigma}}_{\\mathrm{off},h}^{-1}\\phi_{h}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma 13 (Partial Concentrability Bound). For any partition $\\mathcal{X}_{\\mathrm{off}},\\mathcal{X}_{\\mathrm{on}}$ , we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{h=1}^{H}\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{off}}}\\sqrt{\\phi_{h}^{\\top}\\bar{\\Sigma}_{\\mathrm{off},h}^{-1}\\phi_{h}}\\lesssim\\sqrt{c_{\\mathrm{off}}(\\chi_{\\mathrm{off}})^{2}H^{3}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "when $N_{\\mathrm{on}},N_{\\mathrm{off}}\\geqslant\\tilde{\\Omega}(d^{13}H^{14})$ , where we define $\\bar{\\Sigma}_{\\mathrm{off},h}=(\\Sigma_{\\mathrm{off},h}+\\lambda I)/N_{\\mathrm{off}}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. Similar to the definition of $\\bar{\\Sigma}_{\\mathrm{off},h}$ , we define $\\bar{\\mathbf{A}}_{\\mathrm{off},h}\\,=\\,(\\mathbf{A}_{\\mathrm{off},h}+\\lambda\\mathbf{I})/N_{\\mathrm{off}}$ in a similar way. Then, one may observe that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{off}}}\\left(\\phi_{h}^{\\top}\\bar{\\mathbf{A}}_{\\mathrm{off},h}^{-1}\\phi_{h}\\right)=\\displaystyle\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{off}}}\\left(\\phi_{h}^{\\top}\\left(\\frac{1}{N_{\\mathrm{off}}}\\left(\\displaystyle\\sum_{n=1}^{N_{\\mathrm{off}}}\\phi_{n,h}\\phi_{n,h}^{\\top}+\\lambda\\mathbf{I}\\right)\\right)^{-1}\\phi_{h}\\right)}\\\\ &{\\displaystyle\\leqslant\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{off}}}\\phi_{h}^{\\top}\\mathbb{E}_{\\mu_{h}}\\big[\\bar{\\mathbf{A}}_{\\mathrm{off},h}\\big]^{-1}\\phi_{h}\\left(1-\\sqrt{\\displaystyle\\frac{2}{N_{\\mathrm{off}}}\\log\\left(\\frac{4d}{\\delta}\\right)}\\right)^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last line holds by an application of the Matrix Chernoff inequality. Then, we may further bound the quantity with the partial offline all-policy concentrability coefficient, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\phi_{h}\\in\\Phi_{\\mathrm{off}}}{\\operatorname*{max}}\\left(\\phi_{h}^{\\top}\\bar{\\mathbf{A}}_{\\mathrm{off},h}^{-1}\\phi_{h}\\right)\\lesssim\\underset{x_{\\mathrm{off}},\\mathcal{X}_{\\mathrm{on}}}{\\operatorname*{inf}}\\underset{h}{\\operatorname*{max}}\\,\\frac{1}{\\lambda_{d_{\\mathrm{off}}}\\,\\big(\\mathbb{E}_{\\mu}(\\mathcal{P}_{\\mathrm{off}}\\phi_{h})(\\mathcal{P}_{\\mathrm{off}}\\phi_{h})^{\\top}\\big)}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\underset{x_{\\mathrm{off}},\\mathcal{X}_{\\mathrm{on}}}{\\operatorname*{inf}}\\underset{h}{\\operatorname*{max}}\\,\\frac{1}{\\lambda_{\\operatorname*{min}}\\,\\big(\\mathbb{E}_{\\mu}(U_{\\mathrm{off}}^{\\top}\\phi_{h})(U_{\\mathrm{off}}^{\\top}\\phi_{h})^{\\top}\\big)}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "To tighten the dependence of the regret of the offline partition on $H$ , we again employ a truncation argument that used in Lemma 4. Recall that in Section $\\mathbf{B}$ of the appendix in He et al. (2023), by the total variance lemma of Jin et al. (2019), it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sum_{h=1}^{H}\\sigma_{t,h}^{2}\\leqslant\\widetilde{O}\\left(H^{2}T+d^{10.5}H^{16}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Again, recall that we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{h,t}\\|\\Sigma_{t,h}^{-1/2}\\phi_{h}(s_{h}^{(t)},a_{h}^{(t)})\\mathbb{1}_{\\mathcal{X}_{\\mathrm{off}}}\\|_{2}}\\\\ &{\\lesssim_{\\sqrt{H^{2}N_{\\mathrm{on}}\\displaystyle\\frac{N_{\\mathrm{on}}}{N_{\\mathrm{off}}}\\displaystyle\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{off}}}\\left(\\phi_{h}^{\\top}\\left(\\displaystyle\\frac{1}{N_{\\mathrm{off}}}\\left(\\displaystyle\\sum_{n=1}^{N_{\\mathrm{off}}}\\bar{\\sigma}_{n,h}^{-2}\\phi_{n,h}\\phi_{n,h}^{\\top}+\\lambda\\mathbf{I}\\right)\\right)\\right)^{-1}\\phi_{h}}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "As \u03c3\u00af2 $\\bar{\\sigma}_{n,h}^{2}=\\operatorname*{max}\\left\\{\\sigma_{n,h}^{2},H,4d^{6}H^{4}||\\phi_{n,h}||_{\\Sigma_{n,h}^{-1}}\\right\\}$ . Consider the sets ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}_{1}=\\big\\{n\\in[N_{\\mathrm{off}}]:\\forall h:\\ \\bar{\\sigma}_{n,h}^{2}=\\operatorname*{max}(\\sigma_{n,h}^{2},H)\\big\\},\\qquad\\mathcal{Z}_{2}=\\mathbb{Z}_{1}^{c}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here, $\\mathcal{T}_{2}$ roughly correspond to the \u201cbad\u201d set of trajectories where there exists some timestep $h$ such that $\\bar{\\sigma}_{n,h}^{2}>\\bar{\\operatorname*{max}}\\{\\sigma_{n,h}^{2},\\bar{H}\\}$ , and $\\mathcal{T}_{1}$ to be the \u201cgood\u201d set of trajectories where the monotonic variance estimator is controlled. ", "page_idx": 24}, {"type": "text", "text": "We need to bound the cardinality of the latter before employing our truncation argument on the estimated variances. As we note that for all n P I2 we have that maxhPrHsb $\\begin{array}{r}{\\operatorname*{max}_{h\\in[H]}\\sqrt{\\phi_{n,h}^{\\top}{\\Sigma}_{n,h}^{-1}\\phi_{n,h}}\\ \\geqslant}\\end{array}$ $1/(4d^{6}H^{2})$ , which indicates that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{h=1}^{H}\\operatorname*{min}\\left\\{1,16d^{12}H^{4}\\phi_{n,h}^{\\top}\\Sigma_{n,h}^{-1}\\phi_{n,h}\\right\\}\\geqslant1,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and so we can conclude that ", "page_idx": 25}, {"type": "equation", "text": "$$\n|\\mathcal{Z}_{2}|\\leqslant\\sum_{h=1}^{H}\\sum_{n=1}^{N_{\\mathrm{off}}}\\operatorname*{min}\\big\\{1,16d^{12}H^{4}\\phi_{n,h}^{\\top}\\Sigma_{n,h}^{-1}\\phi_{n,h}\\big\\}\\lesssim d^{13}H^{5}\\log(1+N/d),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "by Lemma D.5 of Zhou and Gu (2022) and the fact that $\\vert\\vert\\phi_{n,h}/\\bar{\\sigma}_{n,h}\\vert\\vert^{2}\\leqslant1/H^{2}$ . As we require in Theorem 2 that $N_{\\mathrm{on}},N_{\\mathrm{off}}=\\tilde{\\Omega}(d^{13}H^{14})$ , we come to the following result ", "page_idx": 25}, {"type": "equation", "text": "$$\n|\\mathcal{Z}_{2}|/N_{\\mathrm{off}}\\lesssim8d^{13}H^{5}\\log(1+N/d)/N_{\\mathrm{off}}=\\tilde{o}(1),\\quad|\\mathcal{Z}_{1}|/N_{\\mathrm{off}}=1-\\tilde{o}(1).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Informally, this means that the proportion of trajectories in the \u201cbad set\u201d $\\mathcal{T}_{2}$ is asymptotically zero, and the proportion in the \u201cgood set\u201d $\\mathcal{T}_{1}$ is asymptotically one. As for every $n\\in\\mathcal{Z}_{1}$ we have that for any $h\\in\\bar{[H]}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{\\phi_{h}\\in\\hat{\\mathbb{R}}_{\\mathrm{eff}}}{\\operatorname*{max}}\\,\\,\\left(\\phi_{h}^{\\top}\\,\\overline{{\\Sigma}}_{\\mathrm{off},h}^{-1}\\phi_{h}\\right)}\\\\ &{=\\underset{\\phi_{h}\\in\\hat{\\mathbb{R}}_{\\mathrm{eff}}}{\\operatorname*{max}}\\,\\,\\left(\\phi_{h}^{\\top}\\,\\left(\\frac{1}{N_{\\mathrm{off}}}\\,\\left(\\underset{n=1}{\\overset{N_{\\mathrm{off}}}{\\sum}}\\frac{\\sigma_{n,h}^{-2}}{\\sigma_{n,h}^{-2}\\phi_{n,h}\\phi_{n,h}^{\\top}+\\lambda\\lambda}\\right)\\right)^{-1}\\phi_{h}\\right)}\\\\ &{=\\underset{\\phi_{h}\\in\\hat{\\mathbb{R}}_{\\mathrm{eff}}}{\\operatorname*{max}}\\,\\,N_{\\mathrm{off}}\\,\\left(\\phi_{h}^{\\top}\\,\\left(\\underset{n=1}{\\overset{N_{\\mathrm{off}}}{\\sum}}\\frac{\\sigma_{n,h}^{-2}}{\\sigma_{n,h}^{-2}\\phi_{n,h}\\phi_{n,h}^{\\top}+\\lambda\\lambda}\\right)^{-1}\\phi_{h}\\right)}\\\\ &{\\leqslant\\underset{\\phi_{h}\\in\\hat{\\mathbb{R}}_{\\mathrm{eff}}}{\\operatorname*{max}}\\,\\,N_{\\mathrm{off}}\\,\\,\\left(\\phi_{h}^{\\top}\\,\\left(\\underset{n\\in\\hat{\\mathbb{Z}}_{h}}{\\sum}\\frac{\\phi_{n,h}\\phi_{n,h}^{\\top}}{\\sigma_{n,h}^{2}+H}+\\lambda\\mathbf{I}\\right)^{-1}\\phi_{h}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now we invoke the total variance lemma. Recall that in Section $\\mathbf{B}$ of the appendix in He et al. (2023), by the total variance lemma of Jin et al. (2019), if $N_{\\mathrm{off}}\\geqslant\\tilde{\\Omega}(d^{10.5}H^{14})$ , it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{1}{N_{\\mathrm{off}}}\\sum_{n=1}^{N_{\\mathrm{off}}}\\sum_{h=1}^{H}\\sigma_{n,h}^{2}=\\widetilde{O}\\left(H^{2}+d^{10.5}H^{16}/N_{\\mathrm{off}}\\right)=\\widetilde{O}\\left(H^{2}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "With a direct application of Lemma 17, as we set $T=\\tilde{O}(H)$ and $\\gamma=c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}})/N_{\\mathrm{off}}$ , we will then get to ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{h=1}^{H}\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{off}}}\\sqrt{\\phi_{h}^{\\top}\\Sigma_{\\mathrm{off},h}^{-1}\\phi_{h}}\\lesssim\\frac{c_{\\mathrm{off}}(\\chi_{\\mathrm{off}})H}{N_{\\mathrm{off}}}\\sqrt{N_{\\mathrm{off}}H}=\\sqrt{\\frac{c_{\\mathrm{off}}(\\chi_{\\mathrm{off}})H^{3}}{N_{\\mathrm{off}}}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which indicates that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{h=1}^{H}\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{off}}}\\sqrt{\\phi_{h}^{\\top}\\bar{\\Sigma}_{\\mathrm{off},h}^{-1}\\phi_{h}}\\lesssim\\sqrt{c_{\\mathrm{off}}(\\chi_{\\mathrm{off}})^{2}H^{3}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now, from Lemmas 12 and 13, for any partition $\\mathcal{X}_{\\mathrm{off}},\\mathcal{X}_{\\mathrm{on}}$ , the offline regret satisfies ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{Reg}_{\\mathrm{off}}(T)\\lesssim\\sum_{h=1}^{H}\\sqrt{d N_{\\mathrm{on}}\\frac{N_{\\mathrm{on}}}{N_{\\mathrm{off}}}\\operatorname*{max}_{\\phi_{h}\\in\\Phi_{\\mathrm{off}}}\\phi_{h}^{\\top}\\bar{\\Sigma}_{\\mathrm{off},h}^{-1}\\phi_{h}}\\lesssim\\sqrt{c_{\\mathrm{off}}(\\chi_{\\mathrm{off}})^{2}d H^{3}N_{\\mathrm{on}}\\frac{N_{\\mathrm{on}}}{N_{\\mathrm{off}}}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "E.5 Online regret control ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We will then bound the online term, $\\mathrm{Reg}_{\\mathrm{on}}(T)$ . He et al. (2023) show in Lemma E.1 that it is possible to use Cauchy-Schwarz to bound this by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathsf{R e g}_{\\mathrm{on}}(T)=\\widetilde{O}\\left(d^{4}H^{8}+\\beta d^{7}H^{5}+\\beta\\sqrt{d H T+d H\\sum_{t=1}^{T}\\sum_{h=1}^{H}\\sigma_{t,h}^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and in Section $\\mathbf{B}$ of the appendix, state that by the total variance lemma of Jin et al. (2019), ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sum_{h=1}^{H}\\sigma_{t,h}^{2}\\leqslant\\tilde{O}\\left(H^{2}T+d^{10.5}H^{16}\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We will seek to use the online partition to tighten the dimensional dependence in the first result accordingly. ", "page_idx": 26}, {"type": "text", "text": "Lemma 14 (Modified Lemma E.1 in He et al. (2023)). For any parameters $\\beta^{\\prime}\\geqslant1$ and $C\\geqslant1$ , and any partition $\\mathcal{X}_{\\mathrm{off}},\\mathcal{X}_{\\mathrm{on}}$ , the summation of bonuses on the online partition is upper bounded by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\operatorname*{min}\\left(\\beta^{\\prime}\\sqrt{\\phi\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)^{\\top}\\Sigma_{t,h}^{-1}\\phi\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}},C\\right)}\\\\ &{\\displaystyle\\leqslant4d^{4}H^{6}C\\iota+10\\beta^{\\prime}d_{\\mathrm{on}}^{5}H^{4}\\iota+2\\beta^{\\prime}\\sqrt{2d_{\\mathrm{on}}\\iota\\sum_{t=1}^{T}\\left(\\sigma_{t,h}^{2}+H\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\iota=\\log(1+N/(d\\lambda))$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. For each horizon $h\\in[H]$ , we first note that the summation can be bounded by the sum of two terms, where the first term is tight-bounded and the second term stands for a tail event where $\\phi^{T}\\Sigma^{-1}\\phi$ gets large. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\operatorname*{min}\\left(\\beta^{\\prime}\\sqrt{\\phi\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)^{\\top}\\Sigma_{t,h}^{-1}\\phi\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}},C\\right)}\\\\ &{\\displaystyle\\leqslant\\sum_{t=1}^{T}\\beta^{\\prime}\\operatorname*{min}\\left(\\sqrt{\\phi\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)^{\\top}\\Sigma_{t,h}^{-1}\\phi\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}},1\\right)}\\\\ &{\\qquad+\\displaystyle C\\sum_{t=1}^{T}\\mathbb{1}\\left\\{\\sqrt{\\phi\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)^{\\top}\\Sigma_{t,h}^{-1}\\phi\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\geqslant1\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We first bound $\\begin{array}{r}{\\sum_{t=1}^{T}\\beta^{\\prime}\\operatorname*{min}\\left(\\sqrt{\\phi\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)^{\\top}\\Sigma_{t,h}^{-1}\\phi\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}},1\\right)}\\end{array}$ using a variant of Lemma B.1 from Zhou and Gu (2022) in Lemma 18. With this, we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{=1}^{T}\\beta^{\\prime}\\operatorname*{min}\\left(\\sqrt{\\phi\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)^{\\top}\\Sigma_{t,h}^{-1}\\phi\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}},1\\right)}\\\\ &{\\displaystyle\\leqslant\\sum_{t=1}^{T}\\beta^{\\prime}\\operatorname*{min}\\left(\\sqrt{\\phi\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)^{\\top}\\left(\\sum_{n=1}^{N_{\\mathrm{off}}+t}(\\phi_{n,h}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}})(\\phi_{n,h}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}})^{\\top}+\\lambda\\mathbf{I}_{d}\\right)^{-1}\\phi\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}},1\\right)}\\\\ &{\\displaystyle\\leqslant10\\beta^{\\prime}d_{\\mathrm{on}}^{5}H^{4}\\imath+2\\beta^{\\prime}\\sqrt{2d_{\\mathrm{on}}t\\sum_{k=1}^{K}\\left(\\sigma_{k,h}^{2}+H\\right)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\iota=\\log(1+N/(d\\lambda))$ . ", "page_idx": 26}, {"type": "text", "text": "From this, it suffices to follow the rest of the proof of Lemma E.1 from He et al. (2023) to bound the remaining term by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{1}\\left\\{\\sqrt{\\phi\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)^{\\top}\\Sigma_{t,h}^{-1}\\phi\\left(s_{h}^{(t)},a_{h}^{(t)}\\right)}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\geqslant1\\right\\}\\leqslant4d^{4}H^{6}C\\iota.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "As a result, we obtain the following bound for the online regret ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{Reg}_{\\mathrm{on}}(T)\\lesssim d^{7}H^{9}+\\beta\\sqrt{d_{\\mathrm{on}}d H^{3}T}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "E.6 Putting everything together ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Combining our results in E.4 and E.5, we come to the bound of total regret that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{Reg}(N_{\\mathrm{on}})\\lesssim\\sqrt{H^{3}N_{\\mathrm{on}}\\log(H/\\delta)}+\\sqrt{c_{\\mathrm{off}}(\\chi_{\\mathrm{off}})^{2}d H^{3}N_{\\mathrm{on}}\\frac{N_{\\mathrm{on}}}{N_{\\mathrm{off}}}}+\\sqrt{d_{\\mathrm{on}}d H^{3}N_{\\mathrm{on}}}+d^{7}H^{9}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "When we set $N_{\\mathrm{on}},N_{\\mathrm{off}}=\\tilde{\\Omega}(d^{13}H^{14})$ and choose $\\chi_{\\mathrm{off}}$ , $\\chi_{\\mathrm{on}}$ be the partition that minimize the right hand side, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{Reg}(N_{\\mathrm{on}})\\lesssim\\operatorname*{inf}_{\\chi_{\\mathrm{off}},\\chi_{\\mathrm{on}}}\\left(\\sqrt{c_{\\mathrm{off}}(\\chi_{\\mathrm{off}})^{2}d H^{3}N_{\\mathrm{on}}\\frac{N_{\\mathrm{on}}}{N_{\\mathrm{off}}}}+\\sqrt{d_{\\mathrm{on}}d H^{3}N_{\\mathrm{on}}}\\right),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "proving Theorem 2. ", "page_idx": 27}, {"type": "text", "text": "F OPTCOV from Wagenmaker and Jamieson (2023) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We lean on the OPTCOV algorithm from Wagenmaker and Pacchiano (2023) for reward-agnostic exploration , first proposed in Wagenmaker and Jamieson (2023), as well as the Frank-Wolfe subroutine used, for completeness. ", "page_idx": 27}, {"type": "text", "text": "Algorithm 5 Collection of Optimal Covariates (OPTCOV), Wagenmaker and Pacchiano (2023) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1: Input: functions to optimize $(f_{i})_{i}$ , constraint tolerance $\\epsilon$ , confidence $\\delta$ .   \n2: for $i=1,2,3,\\dots{\\bf d}$ o   \n3: Set the number of iterates $T_{i}\\gets2^{i}$ , episodes per iterate $K_{i}\\gets2^{i}$ .   \n4: Play any policy for $K_{i}$ episodes to collect covariates ${\\Gamma}_{0}$ and data $\\mathfrak{D}_{0}$ .   \n5: Initialize covariance matrix $\\mathbf{\\Lambda}_{\\mathbf{1}}\\gets\\mathbf{\\Gamma}_{\\mathbf{1}}/K$ .   \n6: for $t=1,...,T_{i}$ do   \n7: Run FORCE (Wagenmaker et al., 2022) or another regret-minimizing algorithm on the   \nexploration-focused synthetic reward $g_{h}^{(t)}(s,a)\\alpha\\mathrm{tr}(-\\nabla_{\\Lambda}f_{i}(\\Lambda)|_{\\Lambda=\\Lambda_{t}\\phi(s,a)\\phi(s,a)^{\\top}})$ .   \n8: Collect covariates $\\mathbf{\\Gamma}_{t}$ , data $\\mathfrak{D}_{t}$ .   \n9: Perform Frank-Wolfe update: $\\begin{array}{r}{\\mathbf{\\Gamma}_{t+1}\\leftarrow(1-\\frac{1}{t+1})\\mathbf{A}_{t}+\\frac{1}{t+1}\\mathbf{\\Gamma}_{t}/K_{i}}\\end{array}$ .   \n10: end for   \n11: Assign $\\widehat{\\mathbf{A}_{i}}\\gets\\mathbf{A}_{T_{i}+1},\\mathfrak{D}_{i}\\gets\\cup_{t=0}^{T_{i}}\\mathfrak{D}_{t}$ .   \n12: if $f_{i}(\\widehat{\\pmb{\\Lambda}_{i}})\\leqslant K_{i}T_{i}\\epsilon$ then   \n13: Rexturn: $\\hat{\\Lambda},K_{i}T_{i},\\mathfrak{D}_{i}$ .   \n14: end if   \n15: end for ", "page_idx": 27}, {"type": "text", "text": "The algorithm essentially performs the doubling trick to determine how many samples to collect, terminating when the minimum eigenvalue of the covariance matrix is above the set tolerance. ", "page_idx": 27}, {"type": "text", "text": "Wagenmaker and Pacchiano (2023) then prove the following guarantee for OPTCOV in the hybrid setting: ", "page_idx": 27}, {"type": "text", "text": "Lemma 15 (Termination of OPTCOV, Lemma C.2 (Wagenmaker and Pacchiano, 2023)). Let ", "page_idx": 28}, {"type": "equation", "text": "$$\nf_{i}(\\mathbf{A})=\\frac{1}{\\eta_{i}}\\log\\left(\\sum_{\\phi\\in\\Phi}e^{\\eta_{i}\\|\\phi\\|_{\\mathbf{A}_{i}(\\Lambda)}^{2}-1}\\right),\\quad\\mathbf{A}_{i}(\\Lambda)=\\mathbf{A}+\\frac{1}{T_{i}K_{i}}\\mathbf{A}_{0,i}+\\frac{1}{T_{i}K_{i}}\\mathbf{A}_{\\mathrm{off}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for some $\\mathbf{\\Lambda}_{\\Lambda_{0,i}}$ satisfying $\\mathbf{A}_{0,i}\\,\\geq\\,\\mathbf{A}_{0}$ for all $i$ , and $\\eta_{i}\\,=\\,2^{2i/5}$ . Let $(\\beta_{i},M_{i})$ denote the smoothness and magnitude constants for $f_{i}$ . Let $(\\beta,M)$ be some values such that $\\beta_{i}\\leqslant\\eta_{i}\\beta,M_{i}\\leqslant M$ for all $i$ . Then, if we run OPTCOV on $\\left(f_{i}\\right)_{i}$ with constraint tolerance $\\epsilon$ and confidence $\\delta$ , we have that with probability at least $1-\\delta$ , it will run for at most ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{max}\\left\\{\\underset{N}{\\mathrm{min}}\\,16N\\;}&{s.t.\\quad\\underset{\\Lambda\\in\\Omega}{\\mathrm{inf}}\\,\\underset{\\phi\\in\\Phi}{\\mathrm{max}}\\,\\phi^{\\top}\\left(N\\Lambda+\\Lambda_{0}+\\Lambda_{\\mathrm{off}}\\right)^{-1}\\phi\\leqslant\\frac{\\epsilon}{6},}\\\\ &{\\;\\;\\frac{\\mathrm{poly}\\left(\\beta,d,H,M,\\log1/\\delta\\right)}{\\epsilon^{4/5}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "episodes, and will return data $\\left\\{\\phi_{\\tau}\\right\\}_{\\tau=1}^{N}$ with covariance $\\begin{array}{r}{\\hat{\\Sigma}_{N}=\\sum_{\\tau=1}^{N}\\phi_{\\tau}\\phi_{\\tau}^{\\top}}\\end{array}$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\nf_{\\hat{i}}\\left(N^{-1}\\widehat{\\Sigma}_{N}\\right)\\leqslant N\\epsilon\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "wherei is the iteration on which OPTCOV terminates. ", "page_idx": 28}, {"type": "text", "text": "We use this to obtain a modified guarantee for OPTCOV that does not require a call to the CONDITIONEDCOV algorithm of Wagenmaker and Jamieson (2023). ", "page_idx": 28}, {"type": "text", "text": "Lemma 16 (Modified Bound on OPTCOV, Theorem 4, Wagenmaker and Pacchiano (2023)). Consider running OPTCOV with some $\\epsilon_{\\mathrm{exp}}>0$ and functions $f_{i}$ as defined in Lemma 15, instantiated with the regularization $\\bar{\\lambda}\\geqslant0$ . Then with probability $1-\\delta$ , this procedure will collect at most ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{nax}\\left\\{\\operatorname*{min}_{N}C\\cdot N\\ s.t.\\ \\operatorname*{inf}_{\\Lambda\\in\\Omega}\\operatorname*{max}_{\\phi\\in\\Phi}\\phi^{\\top}\\left(N(\\Lambda+\\bar{\\lambda}I)+\\Lambda_{\\mathrm{off}}\\right)^{-1}\\phi\\leqslant\\frac{\\epsilon_{\\mathrm{exp}}}{6},\\frac{\\mathrm{poly}\\left(d,H,c_{\\mathrm{on}}(\\mathcal{X}_{\\mathrm{on}}),\\log1/\\delta\\right)}{\\epsilon_{\\mathrm{exp}}^{4/5}},\\epsilon\\right\\}\\ ,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "episodes, and will produce covariates $\\hat{\\Sigma}$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\phi_{h}\\in\\Phi}\\phi_{h}\\left(\\widehat{\\Sigma}+\\bar{\\lambda}\\pmb{I}+\\mathbf{\\Lambda}_{\\mathrm{off}}\\right)^{-1}\\phi_{h}\\leqslant\\epsilon_{\\mathrm{exp}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. This is essentially the proof of Theorem 4 in Wagenmaker and Pacchiano (2023), except where we chase around a few terms that differ in the analysis. By Lemma D.5 of Wagenmaker and Jamieson (2023), it suffices to bound the smoothness constants of $f_{i}(\\mathbf{\\boldsymbol{A}})$ by ", "page_idx": 28}, {"type": "equation", "text": "$$\nL_{i}=\\frac{1}{\\bar{\\lambda}^{2}},\\quad\\beta_{i}=\\frac{2}{\\bar{\\lambda}^{3}}\\left(1+\\frac{\\eta_{i}}{\\bar{\\lambda}}\\right),\\quad M_{i}=\\frac{1}{\\bar{\\lambda}^{2}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Assume that the termination condition of OPTCOV is met for $\\hat{i}$ satisfying ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\widehat{i}\\leqslant\\log\\left(\\mathrm{poly}\\left(\\frac{1}{\\epsilon_{\\mathrm{exp}}},d,H,\\log1/\\delta,c_{\\mathrm{on}}(\\mathcal{X}_{\\mathrm{on}}),\\bar{\\lambda}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We assume this holds and justify it at the conclusion of the proof. For notational convenience, define ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\iota:=\\mathrm{poly}\\left(\\log\\frac{1}{\\epsilon_{\\mathrm{exp}}},d,H,\\log1/\\delta,c_{\\mathrm{on}}(\\mathcal{X}_{\\mathrm{on}}),\\bar{\\lambda}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Given this upper bound on $\\Hat{i}$ , set ", "page_idx": 28}, {"type": "equation", "text": "$$\nL=M:=\\frac{1}{\\bar{\\lambda}^{2}},\\quad\\beta:=\\iota.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "With this choice of $L,M,\\beta$ , we have $L_{i}\\leqslant L,M_{i}\\leqslant M,\\beta_{i}\\leqslant\\eta_{i}\\beta$ for all $i\\leqslant\\widehat{i}$ . ", "page_idx": 28}, {"type": "text", "text": "Now apply Lemma 15 with $\\mathbf{A}_{0}\\;=\\;{\\bar{\\lambda}}\\cdot\\mathbf{I}$ and get that, with probability at least $1-\\delta$ , OPTCOV terminates after at most ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{max}\\left\\{\\underset{N}{\\operatorname*{min}}\\,16N\\quad\\mathrm{~s.t.~}\\quad\\underset{\\mathbf{A}\\in\\Omega}{\\operatorname*{inf}}\\,\\underset{\\phi\\in\\Phi}{\\operatorname*{max}}\\,\\phi^{\\top}\\left(N\\mathbf{A}+\\bar{\\lambda}\\cdot I+\\mathbf{A}_{\\mathrm{off}}\\right)^{-1}\\phi\\leqslant\\frac{\\epsilon_{\\mathrm{exp}}}{6}\\right.}\\\\ &{\\left.\\quad\\quad\\quad\\frac{\\mathrm{poly}\\,(d,H,\\underline{{\\lambda}},c_{\\mathrm{off}}(\\mathcal{X}_{\\mathrm{off}}),\\log1/\\epsilon_{\\mathrm{exp}},\\log1/\\delta)}{\\epsilon_{\\mathrm{exp}}^{4/5}}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "episodes, and returns data $\\left\\{\\phi_{\\tau}\\right\\}_{\\tau=1}^{N}$ with covariance $\\begin{array}{r}{\\hat{\\Sigma}=\\sum_{\\tau=1}^{N}\\phi_{\\tau}\\boldsymbol{\\phi}_{\\tau}^{\\top}}\\end{array}$ such that ", "page_idx": 29}, {"type": "equation", "text": "$$\nf_{\\hat{i}}\\left(N^{-1}\\widehat{\\Sigma}\\right)\\leqslant N\\epsilon_{\\mathrm{exp}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\hat{i}$ is the iteration on which OPTCOV terminates. ", "page_idx": 29}, {"type": "text", "text": "By Lempma D.1 of Wagenmaker and Jamieson (2023) we have ", "page_idx": 29}, {"type": "equation", "text": "$$\nN\\cdot\\operatorname*{max}_{\\phi_{h}\\in\\Phi}\\phi_{h}\\left(\\widehat{\\Sigma}+\\mathbf{A}_{\\widehat{i},0}+\\mathbf{A}_{\\mathrm{off}}\\right)^{-1}\\phi_{h}\\leqslant f_{\\widehat{i}}\\left(N^{-1}\\widehat{\\Sigma}\\right),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and the upper bound on the tolerance follows from Lemma D.8 of Wagenmaker and Jamieson (2023). ", "page_idx": 29}, {"type": "text", "text": "It remains to justify the bound on $\\Hat{i}$ . We do so with the same argument that Wagenmaker and Pacchiano (2023) use. Note that by tphe definition of OPTCOV, if we run for a total of $\\bar{N}$ episodes, we can bound $\\begin{array}{r}{\\hat{i}\\,\\leqslant\\,\\frac{1}{4}\\log_{2}(\\bar{N})}\\end{array}$ . However, we see that the bound on $\\hat{i}$ given above upper bounds $\\textstyle{\\frac{1}{4}}\\log_{2}({\\bar{N}})$ for $\\bar{N}$ the upper bound on the number of samples collecte dp by OPTCOV stated above. Thus, the bound on $\\hat{i}$ is valid. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "G Miscellanous lemmas ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Lemma 17. Let $\\Phi\\,\\subset\\,\\mathbb{R}^{d}$ be a linear subspace. Suppose $\\{\\phi_{h,n}\\}_{h\\in[H],n\\in[N]}\\,\\in\\,\\Phi$ be a collection of unit vectors and $\\left\\{\\sigma_{h,n}\\right\\}_{h\\in[H],n\\in[N]}\\;\\in\\;\\mathbb{R}_{+}$ be a collection of positive real numbers with mean $\\begin{array}{r}{\\bar{\\sigma}\\ =\\ (N H)^{-1}\\sum_{h,n}\\sigma_{h,n}}\\end{array}$ . Suppose it holds that $\\begin{array}{r}{\\operatorname*{max}_{h\\in[H]}\\operatorname*{max}_{\\phi_{h}\\in\\Phi}(\\phi_{h}^{T}\\Lambda_{h}^{-1}\\phi_{h})\\ \\leqslant\\ \\gamma}\\end{array}$ , then the following result  \u0159satisfies ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{h=1}^{H}\\operatorname*{max}_{\\phi_{h}\\in\\Phi}\\sqrt{\\phi_{h}^{T}\\Sigma_{h}^{-1}\\phi_{h}}\\lesssim\\gamma H\\sqrt{N\\bar{\\sigma}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "with ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Lambda_{h}=\\sum_{n=1}^{N}\\phi_{h,n}\\phi_{h,n}^{T}+\\lambda I_{d},\\qquad\\Sigma_{h}=\\sum_{n=1}^{N}\\frac{\\phi_{h,n}\\phi_{h,n}^{T}}{\\sigma_{h,n}}+\\lambda I_{d}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. First, we denote $\\begin{array}{r}{\\bar{\\sigma}_{h}\\,=\\,N^{-1}\\sum_{n}\\sigma_{h,n}\\,}\\end{array}$ . Informally, this implies that most individuals of $\\sigma_{h,}$ \u00a8 is asymptotically on the order of $\\bar{\\sigma}_{h}$ , \u0159with only a small amount of individuals being higher in order. To rule out the effect of the \u201clarge\u201d ones, we group them into the following collection of sets: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{h}(C_{h})=\\{n\\in[N]:\\sigma_{h,n}\\geqslant C_{h}\\bar{\\sigma}_{h}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Here, we leave the choice of the truncation level $C_{h}$ open for now, but note that we allow the truncation levels $C_{h}$ vary across different timesteps $h$ and related to $\\bar{\\sigma}_{h}$ . It follows by definition that $\\begin{array}{r}{\\sum_{h=1}^{H}\\bar{\\sigma}_{h}=H\\bar{\\sigma}}\\end{array}$ . From an application of Markov\u2019s Inequality, the cardinality of set $\\mathcal{E}_{h}(C_{h})$ can be u\u0159pper bounded as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left|\\mathcal{E}_{h}(C_{h})\\right|\\leqslant\\frac{N}{C_{h}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We now choose the truncation level $C_{h}$ . To do so, we follow the steps below to quantify the effect induced by the trajectories with high variance (i.e. those that belong to $\\mathcal{E}_{h}(C_{h}),$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi_{h}\\in\\Phi}\\phi_{h}^{\\top}\\Sigma_{h}^{\\star}\\phi_{h}\\geqslant\\operatorname*{min}_{\\phi_{h}\\in\\Phi}\\phi_{h}^{\\top}\\left(\\sum_{n=1}^{N}\\frac{\\phi_{h,n}\\phi_{h,n}^{T}}{\\sigma_{h,n}}\\right)\\phi_{h}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\geqslant\\underset{\\phi_{h}\\in\\Phi}{\\operatorname*{min}}\\phi_{h}^{\\top}\\bigg(\\underset{n\\in[N]\\backslash\\mathcal{E}_{h}(C_{h})}{\\sum}\\frac{\\phi_{h,n}\\phi_{h,n}^{T}}{\\sigma_{h,n}}\\bigg)\\phi_{h}}\\\\ &{\\geqslant\\frac{1}{C_{h}\\bar{\\sigma}_{h}}\\underset{\\phi_{h}\\in\\Phi}{\\operatorname*{min}}\\phi_{h}^{\\top}\\bigg(\\underset{n\\in[N]\\backslash\\mathcal{E}_{h}(C_{h})}{\\sum}\\phi_{h,n}\\phi_{h,n}^{T}\\bigg)\\phi_{h}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We now utilize a basic matrix inequality that for any matrix $A,B$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi_{h}\\in\\Phi}\\phi_{h}^{\\top}A\\phi_{h}\\geqslant\\operatorname*{min}_{\\phi_{h}\\in\\Phi}\\phi_{h}^{\\top}(A+B)\\phi_{h}-\\operatorname*{max}_{\\phi_{h}\\in\\Phi}\\phi_{h}^{\\top}B\\phi_{h},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which allows us to further bound $\\operatorname*{min}_{\\phi_{h}\\in\\Phi}\\phi_{h}^{\\top}\\Sigma_{h}^{\\star}\\phi_{h}$ as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{\\phi_{h}\\in\\Phi}\\phi_{h}^{\\top}\\Sigma_{h}^{\\star}\\phi_{h}\\geqslant\\frac{1}{C_{h}\\bar{\\sigma}_{h}}\\operatorname*{min}_{\\phi_{h}\\in\\Phi}\\phi_{h}^{\\top}\\Bigg(\\sum_{n=1}^{N}\\phi_{h,n}\\phi_{h,n}^{T}+\\lambda I_{d}\\Bigg)\\phi_{h}}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\displaystyle\\frac{1}{C_{h}\\bar{\\sigma}_{h}}\\operatorname*{max}_{\\phi_{h}\\in\\Phi}\\phi_{h}^{\\top}\\Bigg(\\sum_{n\\in\\mathcal{E}_{h}(C_{h})}\\phi_{h,n}\\phi_{h,n}^{T}+\\lambda I_{d}\\Bigg)\\phi_{h}}\\\\ &{\\qquad\\qquad\\qquad\\gtrsim\\displaystyle\\frac{1}{C_{h}\\bar{\\sigma}_{h}}\\bigg(\\gamma^{-1}-\\frac{N}{C_{h}}-\\lambda\\bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This leads to the following result: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi_{h}\\in\\Phi}\\phi_{h}^{\\top}\\Lambda_{h}\\phi_{h}=\\operatorname*{min}_{\\phi_{h}\\in\\Phi}(\\phi_{h}^{\\top}\\Lambda_{h}^{-1}\\phi_{h})^{-1}\\gtrsim\\left\\{\\operatorname*{max}\\left(\\frac{c_{\\mathrm{off}}(\\chi_{\\mathrm{off}})}{N_{\\mathrm{off}}},\\frac{d_{\\mathrm{on}}}{N_{\\mathrm{on}}}\\right)\\right\\}^{-1}=\\gamma^{-1},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the first equality holds because $\\Lambda_{h}$ is a linear transformation on the subspace $\\Phi$ . Equivalently, this holds from the variational characterization of the eigenvalues and the fact that the largest absolute eigenvalue is equal to the inverse of the smallest absolute eigenvalue of the inverse. As a result, in order to rule out the effect of the \u201chigh variance trajectories\u201d, we select the truncation level $\\delta_{h}$ such that $N/C_{h}=\\Theta(\\gamma^{-1})$ , implying $\\bar{C_{h}}=\\Theta(N\\gamma)$ . Hence, we obtain the following lower bound: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi_{h}\\in\\Phi}\\phi_{h}^{\\top}\\Sigma_{h}^{\\star}\\phi_{h}\\gtrsim\\frac{1}{\\gamma^{2}N\\bar{\\sigma}_{h}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Finally, we note that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{h=1}^{H}\\operatorname*{max}_{\\phi_{h}\\in\\Phi}\\sqrt{\\phi_{h}^{\\top}\\Sigma_{h}^{\\star\\,-1}\\phi_{h}}=\\sum_{h=1}^{H}\\left(\\operatorname*{min}_{\\phi_{h}\\in\\Phi}\\sqrt{\\phi_{h}^{\\top}\\Sigma_{h}^{\\star}\\phi_{h}}\\right)^{-1}\\lesssim\\gamma\\sqrt{N}\\sum_{h=1}^{H}\\sqrt{\\bar{\\sigma}_{h}}\\leqslant\\gamma H\\sqrt{N\\bar{\\sigma}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Lemma 18 (Modified Lemma B.1 from Zhou and Gu (2022)). Let $\\chi_{\\mathrm{off}}$ , $\\chi_{\\mathrm{on}}$ be a partition of $S\\!\\times\\!A\\times$ $[H]$ , such that their images under the feature map, $\\Phi_{\\mathrm{off}},\\Phi_{\\mathrm{on}}$ are subspaces of dimension $d_{\\mathrm{off}},d_{\\mathrm{on}}$ respectively. Let $\\left\\{\\sigma_{k},\\beta_{k}\\right\\}_{k\\geqslant1}$ be a sequence of non-negative numbers, $\\alpha,\\gamma>0$ , $\\{\\mathbf{x}_{k}\\}_{k\\geqslant1}\\subset\\mathbb{R}^{d}$ and $\\|\\mathbf{x}_{k}\\|_{2}\\leqslant L$ . Let $\\{\\mathbf{Z}_{k}\\}_{k\\geqslant1}$ and $\\left\\{\\bar{\\sigma}_{k}\\right\\}_{k\\geqslant1}$ be recursively defined as follows: $\\mathbf{Z}_{1}=\\lambda\\mathbf{I}+\\mathbf{Z}_{\\mathrm{off}}$ for some symmetric matrix $\\mathbf{Z}_{\\mathrm{off}}$ , where $N=\\hat{N}_{\\mathrm{off}}+K$ , and we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\forall k\\geqslant1,\\bar{\\sigma}_{k}=\\operatorname*{max}\\left\\{\\sigma_{k},\\alpha,\\gamma\\left\\|\\mathbf{x}_{k}\\right\\|_{\\mathbf{z}_{k}^{-1}}^{1/2}\\right\\},\\mathbf{Z}_{k+1}=\\mathbf{Z}_{k}+\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\mathbf{x}_{k}\\mathbf{x}_{k}^{\\top}/\\bar{\\sigma}_{k}^{2}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Let $\\iota=\\log\\left(1+N L^{2}/\\left(d\\lambda\\alpha^{2}\\right)\\right)$ . Then we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\operatorname*{min}\\left\\{1,\\beta_{k}\\left\\|\\mathbf{x}_{k}\\right\\|_{\\mathbf{z}_{k}^{-1}}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\right\\}\\leqslant2d_{\\mathrm{on}}\\iota+2\\operatorname*{max}_{k\\in[K]}\\beta_{k}\\gamma^{2}d_{\\mathrm{on}}\\iota+2\\sqrt{d_{\\mathrm{on}}\\iota}\\sqrt{\\sum_{k=1}^{K}\\beta_{k}^{2}\\left(\\sigma_{k}^{2}+\\alpha^{2}\\right)}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. The proof roughly follows that of Lemma B.1 in Zhou and Gu (2022), except that we have to make modifications as necessary to tighten the dimension dependence to $d_{\\mathrm{on}}$ and incorporate the offline data. ", "page_idx": 30}, {"type": "text", "text": "Decompose the set $[K]$ into a union of two disjoint subsets $[K]=\\mathcal{Z}_{1}\\cup\\mathcal{Z}_{2}$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{Z}_{1}=\\left\\{k\\in[K]:\\|\\mathbf{x}_{k}/\\bar{\\sigma}_{k}\\|_{\\mathbf{Z}_{k}^{-1}}\\,\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\geqslant1\\right\\},\\mathbb{Z}_{2}=[K]\\backslash\\mathbb{Z}_{1}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then the following upper bound of $\\lvert\\mathcal{T}_{1}\\rvert$ holds, where the projector ${\\mathcal{P}}_{\\mathrm{on}}$ onto $\\Phi_{\\mathrm{on}}$ has the decomposition $\\mathcal{P}_{\\mathrm{on}}=U_{\\mathrm{on}}U_{\\mathrm{on}}^{\\top}$ by the thin SVD, and we write $\\mathbf{u}_{k}=U_{\\mathrm{on}}^{\\top}\\mathbf{x}_{k}$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{i=1}^{n}\\sum_{k=\\ell_{1}}^{\\operatorname*{min}\\big\\{1,|\\mathcal{K}_{i}|/\\mathcal{R}_{k}\\big\\}}\\frac{2^{k}}{\\mathcal{K}_{k}^{2}}1_{{\\boldsymbol k}=\\binom{n}{\\ell_{1}}}}}\\\\ &{\\lesssim\\underset{k=1}{\\overset{\\sum}{\\sum}}\\operatorname*{min}\\left\\{1,||\\mathcal{K}_{i}|/\\mathcal{R}_{k}|\\right\\}_{{\\boldsymbol k}_{\\ell_{1}}}^{2}1_{{\\boldsymbol k}=\\binom{n}{\\ell_{2}}}}\\\\ &{\\lesssim\\underset{k=1}{\\overset{\\sum}{\\sum}}\\operatorname*{min}\\left\\{1,|\\mathcal{K}_{i}|^{2}\\frac{2}{\\kappa^{2}}\\mathbf{x}_{k}^{\\top}\\mathbf{L}_{k}^{-1}\\mathbf{x}_{k}\\right\\}}\\\\ &{=\\underset{k=1}{\\overset{\\sum}{\\sum}}\\operatorname*{min}\\left\\{1,(U_{\\omega}U_{\\omega}^{\\top}\\mathbf{x}_{k}^{\\top})^{\\top}\\mathbf{Z}_{k}^{-1}\\left(U_{\\omega}U_{\\omega}^{\\top}\\mathbf{x}_{k}^{\\top})^{1}\\mathbf{L}_{\\kappa}\\right\\}}\\\\ &{=\\underset{k=1}{\\overset{\\sum}{\\sum}}\\operatorname*{min}\\left\\{1,\\mathbf{x}_{k}^{\\top}U_{\\omega}U_{\\omega}^{\\top}\\mathbf{Z}_{k}^{-1}U_{\\omega}U_{\\omega}^{\\top}\\mathbf{x}_{k}^{\\top}\\mathbf{L}_{\\kappa}\\right\\}}\\\\ &{=\\underset{k=1}{\\overset{\\sum}{\\sum}}\\operatorname*{min}\\left\\{1,\\mathbf{x}_{k}^{\\top}U_{\\omega}U_{\\omega}^{\\top}\\left(\\underset{k=1}{\\overset{k}{\\sum}}\\mathbf{1}_{{\\boldsymbol k}_{\\ell_{1}}}\\alpha_{n}^{-2}\\mathbf{x}_{k}\\mathbf{x}_{n}^{\\top}+\\mathrm{M}_{d}\\right)^{-1}U_{\\omega}U_{\\omega}^{\\top}\\mathbf{x}_{k}^{\\top}\\mathbf{L}_{\\kappa}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By Lemma 20, we can take the $U_{\\mathrm{on}}$ inside the inverse and conclude that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{K}\\operatorname*{min}\\left\\{1,\\mathbf{x}_{k}^{\\top}U_{\\mathrm{on}}U_{\\mathrm{on}}^{\\top}\\left(\\sum_{n=1}^{k}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\bar{\\sigma}_{n}^{-2}\\mathbf{x}_{n}\\mathbf{x}_{n}^{\\top}+\\lambda\\mathbf{I}_{d}\\right)^{-1}U_{\\mathrm{on}}U_{\\mathrm{on}}^{\\top}\\mathbf{x}_{k}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\right\\}}\\\\ &{\\displaystyle=\\sum_{k=1}^{K}\\operatorname*{min}\\left\\{1,\\mathbf{x}_{k}^{\\top}U_{\\mathrm{on}}\\left(\\sum_{n=1}^{k}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\bar{\\sigma}_{n}^{-2}U_{\\mathrm{on}}^{\\top}\\mathbf{x}_{n}\\mathbf{x}_{n}^{\\top}U_{\\mathrm{on}}+\\lambda\\mathbf{I}_{d_{\\mathrm{on}}}\\right)^{-1}U_{\\mathrm{on}}^{\\top}\\mathbf{x}_{k}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Intuitively, this is because all the $\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}U_{\\mathrm{on}}^{\\top}\\mathbf{x}_{n}$ and $\\mathbb{I}_{\\mathcal{X}_{\\mathrm{on}}}\\mathbf{x}_{n}$ are both in $\\Phi_{\\mathrm{on}}$ , and in that case the projection is just the identity. ", "page_idx": 31}, {"type": "text", "text": "Writing $\\mathbf{u}_{n}=U_{\\mathrm{on}}^{\\top}\\mathbf{x}_{n}$ , and invoking Lemma D.5 of Zhou and Gu (2022) (which is a restatement of Lemma 11 of Abbasi-yadkori et al. (2011)) and the fact that $\\|\\mathbf{x}_{k}/\\bar{\\sigma}_{k}\\|_{2}\\leqslant L/\\alpha$ , it holds that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{K}\\operatorname*{min}\\left\\{1,\\mathbf{x}_{k}^{\\top}U_{\\mathrm{on}}\\left(\\sum_{n=1}^{k}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\bar{\\sigma}^{-2}U_{\\mathrm{on}}^{\\top}\\mathbf{x}_{n}\\mathbf{x}_{n}^{\\top}U_{\\mathrm{on}}+\\lambda\\mathbf{I}_{d_{\\mathrm{on}}}\\right)^{-1}U_{\\mathrm{on}}^{\\top}\\mathbf{x}_{k}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\right\\}}\\\\ &{\\displaystyle\\sum_{k=1}^{K}\\operatorname*{min}\\left\\{1,\\mathbf{u}_{k}^{\\top}\\left(\\sum_{n=1}^{k}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\bar{\\sigma}^{-2}\\mathbf{u}_{n}\\mathbf{u}_{n}^{\\top}+\\lambda\\mathbf{I}_{d_{\\mathrm{on}}}\\right)^{-1}\\mathbf{u}_{k}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\right\\}}\\\\ &{\\displaystyle\\leqslant2d_{\\mathrm{on}}\\iota,}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "as desired, and conclude that $|\\mathcal{T}_{1}|\\leqslant2d_{\\mathrm{on}}\\iota$ . ", "page_idx": 31}, {"type": "text", "text": "The rest of the proof follows Zhou and Gu (2022) more closely. By the same argument that Zhou and Gu (2022) use, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{k\\in[K]}\\operatorname*{min}\\left\\{1,\\beta_{k}\\left\\|\\mathbf{x}_{k}\\right\\|_{\\mathbf{z}_{k}^{-1}}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\right\\}\\leqslant2d_{\\mathrm{on}}\\iota+\\sum_{k\\in\\mathbb{Z}_{2}}\\beta_{k}\\bar{\\sigma}_{k}\\left\\|\\mathbf{x}_{k}/\\bar{\\sigma}_{k}\\right\\|_{\\mathbf{z}_{k}^{-1}}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Decompose $\\mathcal{T}_{2}=\\mathcal{T}_{1}\\cup\\mathcal{I}_{2}$ , where ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{I}_{1}=\\left\\lbrace k\\in\\mathbb{Z}_{2}:\\bar{\\sigma}_{k}=\\sigma_{k}\\cup\\bar{\\sigma}_{k}=\\alpha\\right\\rbrace,\\mathcal{I}_{2}=\\left\\lbrace k\\in\\mathbb{Z}_{2}:\\bar{\\sigma}_{k}=\\gamma\\sqrt{\\left\\|\\mathbf{x}_{k}\\right\\|_{\\mathbf{z}_{k}^{-1}}}\\mathbb{I}_{\\mathcal{X}_{\\mathrm{on}}}\\right\\rbrace.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Similar to Zhou and Gu (2022), ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{k\\in\\mathcal{I}_{1}}\\beta_{k}\\bar{\\sigma}_{k}\\left\\|\\mathbf{x}_{k}/\\bar{\\sigma}_{k}\\right\\|_{\\mathbf{z}_{k}^{-1}}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\leqslant\\displaystyle\\sum_{k\\in\\mathcal{I}_{1}}\\beta_{k}\\left(\\sigma_{k}+\\alpha\\right)\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\operatorname*{min}\\left\\{1,\\left\\|\\mathbf{x}_{k}/\\bar{\\sigma}_{k}\\right\\|_{\\mathbf{z}_{k}^{-1}}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\right\\}}\\\\ {\\displaystyle}&{\\leqslant\\displaystyle\\sum_{k=1}^{K}\\beta_{k}\\left(\\sigma_{k}+\\alpha\\right)\\operatorname*{min}\\left\\{1,\\left\\|\\mathbf{x}_{k}/\\bar{\\sigma}_{k}\\right\\|_{\\mathbf{z}_{k}^{-1}}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\right\\}}\\\\ {\\displaystyle}&{\\leqslant\\displaystyle\\sqrt{2\\sum_{k=1}^{K}\\left(\\sigma_{k}^{2}+\\alpha^{2}\\right)\\beta_{k}^{2}}\\sqrt{\\displaystyle\\sum_{k=1}^{K}\\operatorname*{min}\\left\\{1,\\left\\|\\mathbf{x}_{k}/\\bar{\\sigma}_{k}\\right\\|_{\\mathbf{z}_{k}^{-1}}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\right\\}^{2}}}\\\\ {\\displaystyle}&{\\leqslant2\\sqrt{\\displaystyle\\sum_{k=1}^{K}\\beta_{k}^{2}\\left(\\sigma_{k}^{2}+\\alpha^{2}\\right)}\\sqrt{d_{\\mathrm{on}}t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and as for $k\\in\\mathcal{T}_{2}$ we have that $\\bar{\\sigma}_{k}=\\gamma^{2}\\,\\|\\mathbf{x}_{k}/\\bar{\\sigma}_{k}\\|_{\\mathbf{Z}_{k}^{-1}}\\,\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{k\\in\\mathcal{I}_{2}}\\beta_{k}\\bar{\\sigma}_{k}\\left\\|\\mathbf{x}_{k}/\\bar{\\sigma}_{k}\\right\\|_{\\mathbf{Z}_{k}^{-1}}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}=\\gamma^{2}\\cdot\\displaystyle\\sum_{k\\in\\mathcal{I}_{1}}\\beta_{k}\\left\\|\\mathbf{x}_{k}/\\bar{\\sigma}_{k}\\right\\|_{\\mathbf{Z}_{k}^{-1}}^{2}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}}}\\\\ &{}&{\\qquad=\\gamma^{2}\\cdot\\displaystyle\\sum_{k=1}^{K}\\beta_{k}\\operatorname*{min}\\left\\{1,\\|\\mathbf{x}_{k}/\\bar{\\sigma}_{k}\\|_{\\mathbf{Z}_{k}^{-1}}^{2}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\right\\}\\leqslant2\\operatorname*{max}_{k\\in[K]}\\beta_{k}\\gamma^{2}d_{\\mathrm{on}}\\iota.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\operatorname*{min}\\left\\{1,\\beta_{k}\\left\\|\\mathbf{x}_{k}\\right\\|_{\\mathbf{z}_{k}^{-1}}\\mathbb{1}_{\\mathcal{X}_{\\mathrm{on}}}\\right\\}\\leqslant2d_{\\mathrm{on}}\\iota+2\\operatorname*{max}_{k\\in[K]}\\beta_{k}\\gamma^{2}d_{\\mathrm{on}}\\iota+2\\sqrt{d_{\\mathrm{on}}\\iota}\\sqrt{\\sum_{k=1}^{K}\\beta_{k}^{2}\\left(\\sigma_{k}^{2}+\\alpha^{2}\\right)}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Lemma 19 (Modified Version of Theorem 4.3, Zhou and Gu (2022)). Let $\\left\\{\\mathcal{G}_{n}\\right\\}_{n=1}^{N}$ be a filtration, and $\\{\\mathbf{x}_{n},\\eta_{n}\\}_{n=1}^{N}$ be a stochastic process such that $\\mathbf{x}_{n}\\ \\in\\ \\mathbb{R}^{d}$ is $\\mathcal{G}_{n}$ -measurable and $\\eta_{n}~\\in~\\mathbb{R}$ is $\\mathcal{G}_{n+1}$ -measurable. Let $L,\\sigma,\\lambda,\\epsilon>0,\\pmb{\\mu}^{*}\\in\\mathbb{R}^{d}$ . Arrange the datapoints from the offline and online samples as follows, $1,...,N_{\\mathrm{off}},N_{\\mathrm{off}}+1,...,N_{\\mathrm{off}}+N_{\\mathrm{on}}$ . For $n=1,...,N$ , let $y_{n}=\\langle\\pmb{\\mu}^{*},\\mathbf{x}_{n}\\rangle+\\eta_{n}$ and suppose that $\\eta_{n},\\mathbf{x}_{n}$ also satisfy ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\eta_{n}\\ |\\ \\mathcal{G}_{n}\\right]=0,\\mathbb{E}\\left[\\eta_{n}^{2}\\ |\\ \\mathcal{G}_{n}\\right]\\leqslant\\sigma^{2},|\\eta_{n}|\\leqslant R,\\|\\mathbf{x}_{n}\\|_{2}\\leqslant L.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For $n=1,...,N$ , let $\\begin{array}{r}{\\mathbf{Z}_{n}=\\lambda\\mathbf{I}+\\sum_{i=1}^{n}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top},\\mathbf{b}_{n}=\\sum_{i=1}^{n}y_{i}\\mathbf{x}_{i},\\mu_{n}=\\mathbf{Z}_{n}^{-1}\\mathbf{b}_{n}}\\end{array}$ , and ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{n}=\\!12\\sqrt{\\sigma^{2}d\\log\\left(1+n L^{2}/(d\\lambda)\\right)\\log\\left(32(\\log(R/\\epsilon)+1)n^{2}/\\delta\\right)}}\\\\ &{\\qquad+\\,24\\log\\left(32(\\log(R/\\epsilon)+1)n^{2}/\\delta\\right)\\underset{1\\leqslant i\\leqslant n}{\\operatorname*{max}}\\left\\lbrace|\\eta_{i}|\\operatorname*{min}\\left\\lbrace1,\\|\\mathbf{x}_{i}\\|_{\\mathbf{z}_{i-1}^{-1}}\\right\\rbrace\\right\\rbrace}\\\\ &{\\qquad+\\,6\\log\\left(32(\\log(R/\\epsilon)+1)n^{2}/\\delta\\right)\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then, for any $0<\\delta<1$ , we have with probability at least $1-\\delta$ that, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\forall n=1,...,N,\\left\\|\\sum_{i=1}^{n}\\mathbf{x}_{i}\\eta_{i}\\right\\|_{\\mathbf{z}_{n}^{-1}}\\leqslant\\beta_{n},\\left\\|\\mu_{n}-\\mu^{*}\\right\\|_{\\mathbf{z}_{n}}\\leqslant\\beta_{n}+{\\sqrt{\\lambda}}\\left\\|\\mu^{*}\\right\\|_{\\mathbf{z}_{n}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. The proof is merely a small wrapper over Theorem 4.3 of Zhou and Gu (2022), where we adapt this to our setting in the same way that Tan and $\\mathrm{Xu}$ (2024) do in Lemma 1 of their paper. That is, we pre-append the offline data to the online data, and generate the $\\mathbf{Z_{n}},\\mathbf{b_{n}},\\mu_{\\mathbf{n}},\\beta_{\\mathbf{n}}$ accordingly. ", "page_idx": 32}, {"type": "text", "text": "As in Lemma 1 of Tan and $\\mathrm{Xu}$ (2024), let $N\\;=\\;N_{\\mathrm{off}}\\:+\\:N_{\\mathrm{on}}$ . Order the $N_{\\mathrm{off}}$ offline episodes arbitrarily, to form episodes $1,...,N_{\\mathrm{off}}$ , and then begin the online episodes from episode $N_{\\mathrm{off}}\\,+$ $1,...,N$ . Then, we can directly apply Theorem 4.3 of Zhou and Gu (2022) to recover the desired result. \u53e3 ", "page_idx": 32}, {"type": "text", "text": "Lemma 20. Suppose that $W\\,=\\,\\mathbb{R}^{m}$ and $V\\,=\\,\\mathbb{R}^{n}$ , where $n\\,<m$ . Let $U:W\\mapsto V$ be a linear transformation and that $S=(U^{\\top}U)W$ . As $v,v_{1},\\ldots,v_{n}\\in S$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\boldsymbol{v}^{\\top}\\boldsymbol{U}^{\\top}\\boldsymbol{U}\\Big(\\sum_{j=1}^{k}{v_{i}v_{i}^{\\top}+\\lambda I_{m}}\\Big)^{-1}\\boldsymbol{U}^{\\top}\\boldsymbol{U}\\boldsymbol{v}=\\boldsymbol{v}^{\\top}\\boldsymbol{U}^{\\top}\\Big(\\sum_{j=1}^{k}U\\boldsymbol{v}_{i}v_{i}^{\\top}\\boldsymbol{U}^{\\top}+\\lambda I_{n}\\Big)^{-1}\\boldsymbol{U}\\boldsymbol{v}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. For projection matrix $U$ , there exists orthogonal matrix $Q\\,\\in\\,\\mathbb{R}^{m\\times m}$ and diagonal matrix $D\\;=\\;\\left(I_{n},\\mathbf{0}_{n\\times(m-n)}\\right)$ such that $U\\,=\\,D Q$ . We further define ${\\textbf{\\em u}}=\\,U v,\\,{\\tilde{v}}\\;=\\,Q v,\\,u_{i}\\;=\\,U v_{i}$ and $\\tilde{\\pmb{v}}_{i}\\,=\\,Q\\pmb{v}_{i}$ for $1\\leqslant i\\leqslant n$ . Then, we note that as $\\pmb{v}\\in S$ , we have $\\pmb{v}\\,=\\,\\pmb{U}^{\\top}\\pmb{U}\\pmb{v}\\,=\\,\\pmb{Q}^{\\top}\\pmb{\\Lambda}\\pmb{Q}\\pmb{v},$ , where ${\\pmb{\\Lambda}}\\,=\\,\\mathrm{diag}({\\pmb I}_{n},{\\bf0}_{m-n})$ , which is equivalent to $\\tilde{\\pmb{v}}=\\Lambda\\tilde{\\pmb{v}}$ . As a result, we may conclude that v\u02dcJ \u201c puJ, 0m\u00b4nq. ", "page_idx": 33}, {"type": "text", "text": "Therefore, with a direct calculation, one will see that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bigg(\\displaystyle\\sum_{j=1}^{k}v_{i}v_{i}^{\\top}+\\lambda I_{m}\\bigg)^{-1}=}&{\\left(\\displaystyle\\sum_{j=1}^{k}Q^{\\top}\\bar{v}_{i}\\bar{v}_{i}^{\\top}Q+\\lambda I_{m}\\right)^{-1}}\\\\ &{\\!=Q^{\\top}\\bigg(\\displaystyle\\sum_{j=1}^{k}\\bar{v}_{i}\\bar{v}_{i}^{\\top}+\\lambda I_{m}\\bigg)^{-1}Q}\\\\ &{\\!=Q^{\\top}\\left(\\sum_{i=1}^{k}u_{i}u_{i}^{\\top}+\\lambda I_{n}\\quad\\underset{\\lambda I_{m-n}}{\\overset{0}{\\sum}}\\right)^{-1}Q}\\\\ &{\\!=Q^{\\top}\\left(\\left(\\sum_{j=1}^{k}u_{i}u_{i}^{\\top}+\\lambda I_{n}\\right)^{-1}\\quad\\underset{\\lambda^{-1}I_{m-n}}{\\overset{0}{\\sum}}\\right)Q.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This will establish our desired conclusion ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{LHS}={v^{\\top}}\\bigg(\\displaystyle\\sum_{j=1}^{k}v_{i}v_{i}^{\\top}+\\lambda I_{m}\\bigg)^{-1}v=\\tilde{v}^{\\top}\\left(\\left(\\sum_{j=1}^{k}u_{i}u_{i}^{\\top}+\\lambda I_{n}\\right)^{-1}\\begin{array}{c c}{\\ }&\\\\ {\\lambda^{-1}I_{m-n}}\\end{array}\\right)\\tilde{v}}\\\\ &{\\qquad={u^{\\top}}\\bigg(\\displaystyle\\sum_{j=1}^{k}u_{i}u_{i}^{\\top}+\\lambda I_{n}\\bigg)^{-1}u=\\mathrm{RHS}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "H Further details on the numerical experiments ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "The environment, as in Tan and Xu (2024), is a 6-piece wide Tetris board with pieces no larger than $2\\times2$ , where the action space consists of four actions, differentiated by the degree of rotation in 90 degree intervals and the reward is given by penalizing any increases in the height of the stack from a tolerance of 2 blocks. As in Tan and Xu (2024), we generate feature vectors by projecting the 640-dimensional one-hot state-action encoding onto a 60-dimensional subspace spanned by the top 60 eigenvectors of the feature covariance matrix under 200 trajectories from the uniform behavior policy. ", "page_idx": 33}, {"type": "text", "text": "All experiments were run on a single computer with an Intel i9-13900k CPU, 128 GB of RAM, and a NVIDIA RTX3090 GPU, in no more than a couple of hours. ", "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We claim that our algorithms achieve sharper error or regret bounds that are no worse than, and can improve on, (1) the optimal sample complexity in offline RL (the first algorithm, for PAC RL) and online RL (the second algorithm, for regret-minimizing RL) in linear Markov decide processes (MDPs), and (2) that this work establishes the tightest theoretical guarantees currently available for hybrid RL in linear MDPs. Theorems 1 and 2, and the accompanying discussion, support claim (1) by being no worse than the associated bounds from Xiong et al. (2023) and He et al. (2023) respectively. Table 2 supports claim (2) by showcasing our improvement over existing hybrid RL methods for linear MDPs. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We include a discussion of the limitations of our algorithms and analysis in Section 5 and in the discussion immediately after Theorems 1 and 2. The latter also contains a discussion on the computational efficiency of our algorithms. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our assumptions and setting are outlined in Assumptions 1 and 2, and Section 2. Any further conditions are outlined in the statement of our theorems and lemmas. Though we provide proof sketches in the main body, the proof of Theorem 1 is found in Appendix B, and the proof of Theorem 2 is found in Appendix E. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Justification: The code is available on GitHub at github.com/hetankevin/hybridlin. The full details are in Section 4 and Appendix H. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes]   \nJustification: The code is available on GitHub at github.com/hetankevin/hybridlin. Guidelines:   \n\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes]   \nJustification: The full details are in Section 4 and Appendix H. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Justification: One standard deviation error bars are displayed. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified. ", "page_idx": 36}, {"type": "text", "text": "\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). \u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: All experiments were run on a single computer with an Intel i9-13900k CPU, 128 GB of RAM, and a NVIDIA RTX3090 GPU, in no more than a couple of hours. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We have read and reviewed the code of ethics. Our paper is a theoretical contribution to the field of reinforcement learning theory, and as such conforms to the guidelines outlined. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: Our paper is a theoretical contribution to the field of reinforcement learning theory, and as such any societal impact will at the very least be second-order impacts not directly tied to our work. Having said that, we discuss that Algorithm 1 does not randomize over policies, allowing learned policies to be deployed in critical real-world applications, and Algorithm 2 minimizes regret over the online samples, allowing it to be used in situations where performance-agnostic online exploration is untenable, such as in medicine. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Justification: Our paper poses no such risks. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Justification: Our paper does not use existing assets. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?   \nAnswer: [NA]   \nJustification: Our paper does not release new assets. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used. ", "page_idx": 38}, {"type": "text", "text": "At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?   \nAnswer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]