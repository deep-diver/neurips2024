[{"figure_path": "YkJ5BuEXdD/figures/figures_4_1.jpg", "caption": "Figure 1: Both reference and preference-tuned models exhibit low ranking accuracy on most preference datasets. Each point represents the length-normalized or non-length-normalized ranking accuracy of individual (1a) reference models (pre-trained or fine-tuned), or (1b) preference-tuned models (trained with DPO or RLHF). The random chance accuracy for each dataset is indicated with a black 'X'. We sub-sample 1K examples from each dataset and use the test split when available. We describe datasets in B.2 and list all numbers in Tables 2, 3, and 4. For UltraFeedback, ranking accuracy is measured with exact match across all 4 outputs (see App. A.6).", "description": "This figure shows the low ranking accuracy of both reference and preference-tuned LLMs across several datasets.  The left panel displays the accuracy of various pre-trained or fine-tuned reference models, while the right panel shows the accuracy of models fine-tuned using preference learning algorithms (RLHF and DPO).  The 'X' represents the expected accuracy by random chance for each dataset.  The figure highlights that even state-of-the-art preference-tuned LLMs struggle to achieve high ranking accuracy, which is typically below 60%.", "section": "3.1 Existing Reference Models Rarely Have Correct Rankings"}, {"figure_path": "YkJ5BuEXdD/figures/figures_6_1.jpg", "caption": "Figure 1: Both reference and preference-tuned models exhibit low ranking accuracy on most preference datasets. Each point represents the length-normalized or non-length-normalized ranking accuracy of individual (1a) reference models (pre-trained or fine-tuned), or (1b) preference-tuned models (trained with DPO or RLHF). The random chance accuracy for each dataset is indicated with a black 'X'. We sub-sample 1K examples from each dataset and use the test split when available. We describe datasets in B.2 and list all numbers in Tables 2, 3, and 4. For UltraFeedback, ranking accuracy is measured with exact match across all 4 outputs (see App. A.6).", "description": "This figure shows that both reference and preference-tuned LLMs have low ranking accuracy (below 60%) across various datasets.  The length-normalized and non-length-normalized ranking accuracies are shown for multiple models and datasets. The random chance accuracy is provided for comparison.  The subplots show reference model accuracies and preference-tuned model accuracies separately.", "section": "3.1 Existing Reference Models Rarely Have Correct Rankings"}, {"figure_path": "YkJ5BuEXdD/figures/figures_6_2.jpg", "caption": "Figure 1: Both reference and preference-tuned models exhibit low ranking accuracy on most preference datasets. Each point represents the length-normalized or non-length-normalized ranking accuracy of individual (1a) reference models (pre-trained or fine-tuned), or (1b) preference-tuned models (trained with DPO or RLHF). The random chance accuracy for each dataset is indicated with a black 'X'. We sub-sample 1K examples from each dataset and use the test split when available. We describe datasets in B.2 and list all numbers in Tables 2, 3, and 4. For UltraFeedback, ranking accuracy is measured with exact match across all 4 outputs (see App. A.6).", "description": "This figure shows that both reference and preference-tuned LLMs achieve low ranking accuracy across various preference datasets.  The plots compare the ranking accuracy of different LLMs against random chance. It highlights the significant gap between the performance of existing models and the idealized ranking accuracy.", "section": "3.1 Existing Reference Models Rarely Have Correct Rankings"}, {"figure_path": "YkJ5BuEXdD/figures/figures_7_1.jpg", "caption": "Figure 1: Both reference and preference-tuned models exhibit low ranking accuracy on most preference datasets. Each point represents the length-normalized or non-length-normalized ranking accuracy of individual (1a) reference models (pre-trained or fine-tuned), or (1b) preference-tuned models (trained with DPO or RLHF). The random chance accuracy for each dataset is indicated with a black 'X'. We sub-sample 1K examples from each dataset and use the test split when available. We describe datasets in B.2 and list all numbers in Tables 2, 3, and 4. For UltraFeedback, ranking accuracy is measured with exact match across all 4 outputs (see App. A.6).", "description": "This figure shows that both reference and preference-tuned LLMs exhibit low ranking accuracy (<60%) across various datasets.  It highlights the significant gap between the observed accuracy and the idealized accuracy achievable under perfect conditions, emphasizing the difficulty for preference learning algorithms to learn high-quality preference rankings. The plot includes length-normalized and non-length-normalized accuracies.", "section": "3.1 Existing Reference Models Rarely Have Correct Rankings"}, {"figure_path": "YkJ5BuEXdD/figures/figures_8_1.jpg", "caption": "Figure 1: Both reference and preference-tuned models exhibit low ranking accuracy on most preference datasets. Each point represents the length-normalized or non-length-normalized ranking accuracy of individual (1a) reference models (pre-trained or fine-tuned), or (1b) preference-tuned models (trained with DPO or RLHF). The random chance accuracy for each dataset is indicated with a black \u2018X\u2019. We sub-sample 1K examples from each dataset and use the test split when available. We describe datasets in B.2 and list all numbers in Tables 2, 3, and 4. For UltraFeedback, ranking accuracy is measured with exact match across all 4 outputs (see App. A.6).", "description": "This figure shows that both reference and preference-tuned LLMs exhibit low ranking accuracy (below 60%) across various preference datasets.  The plots compare the performance of different models, highlighting the significant gap between their actual ranking accuracy and the idealized accuracy achievable under perfect conditions.", "section": "3.1 Existing Reference Models Rarely Have Correct Rankings"}, {"figure_path": "YkJ5BuEXdD/figures/figures_8_2.jpg", "caption": "Figure 1: Both reference and preference-tuned models exhibit low ranking accuracy on most preference datasets. Each point represents the length-normalized or non-length-normalized ranking accuracy of individual (1a) reference models (pre-trained or fine-tuned), or (1b) preference-tuned models (trained with DPO or RLHF). The random chance accuracy for each dataset is indicated with a black 'X'. We sub-sample 1K examples from each dataset and use the test split when available. We describe datasets in B.2 and list all numbers in Tables 2, 3, and 4. For UltraFeedback, ranking accuracy is measured with exact match across all 4 outputs (see App. A.6).", "description": "The figure shows that both reference and preference-tuned LLMs exhibit low ranking accuracy (below 60%) across various datasets.  The results highlight that even under idealized conditions where the model is perfectly optimized for the DPO or RLHF objective, there remains a significant gap between the achieved ranking accuracy and the theoretically achievable accuracy. This is because of the flaws of the DPO objective, which struggles to correct even mild ranking errors in the reference model. The random chance accuracy is shown as a reference point.", "section": "3.1 Existing Reference Models Rarely Have Correct Rankings"}, {"figure_path": "YkJ5BuEXdD/figures/figures_24_1.jpg", "caption": "Figure 1: Both reference and preference-tuned models exhibit low ranking accuracy on most preference datasets. Each point represents the length-normalized or non-length-normalized ranking accuracy of individual (1a) reference models (pre-trained or fine-tuned), or (1b) preference-tuned models (trained with DPO or RLHF). The random chance accuracy for each dataset is indicated with a black 'X'. We sub-sample 1K examples from each dataset and use the test split when available. We describe datasets in B.2 and list all numbers in Tables 2, 3, and 4. For UltraFeedback, ranking accuracy is measured with exact match across all 4 outputs (see App. A.6).", "description": "This figure shows that both reference and preference-tuned LLMs have low ranking accuracy across various datasets.  The plots compare the ranking accuracy of several LLMs, before and after preference tuning with RLHF and DPO methods.  The random chance accuracy is shown for reference.  The results highlight a significant gap between the accuracy achieved by real-world models and the theoretical maximum.", "section": "3.1 Existing Reference Models Rarely Have Correct Rankings"}, {"figure_path": "YkJ5BuEXdD/figures/figures_25_1.jpg", "caption": "Figure 1: Both reference and preference-tuned models exhibit low ranking accuracy on most preference datasets. Each point represents the length-normalized or non-length-normalized ranking accuracy of individual (1a) reference models (pre-trained or fine-tuned), or (1b) preference-tuned models (trained with DPO or RLHF). The random chance accuracy for each dataset is indicated with a black 'X'. We sub-sample 1K examples from each dataset and use the test split when available. We describe datasets in B.2 and list all numbers in Tables 2, 3, and 4. For UltraFeedback, ranking accuracy is measured with exact match across all 4 outputs (see App. A.6).", "description": "This figure shows the low ranking accuracy of both reference and preference-tuned LLMs on various preference datasets.  The left panel (a) displays the accuracy of various pre-trained or fine-tuned reference LLMs, while the right panel (b) shows the accuracy of LLMs further tuned using preference learning algorithms such as RLHF and DPO.  The results indicate that most models, even those specifically trained to improve ranking accuracy, perform poorly, achieving less than 60% accuracy in many cases.  The figure highlights the significant gap between the observed and idealized ranking accuracy.", "section": "3.1 Existing Reference Models Rarely Have Correct Rankings"}, {"figure_path": "YkJ5BuEXdD/figures/figures_26_1.jpg", "caption": "Figure 1: Both reference and preference-tuned models exhibit low ranking accuracy on most preference datasets. Each point represents the length-normalized or non-length-normalized ranking accuracy of individual (1a) reference models (pre-trained or fine-tuned), or (1b) preference-tuned models (trained with DPO or RLHF). The random chance accuracy for each dataset is indicated with a black 'X'. We sub-sample 1K examples from each dataset and use the test split when available. We describe datasets in B.2 and list all numbers in Tables 2, 3, and 4. For UltraFeedback, ranking accuracy is measured with exact match across all 4 outputs (see App. A.6).", "description": "This figure shows that both reference and preference-tuned LLMs exhibit low ranking accuracy across various preference datasets.  The left panel (a) displays the ranking accuracy of various reference models (before fine-tuning with preferences), while the right panel (b) shows the ranking accuracy after preference tuning with either RLHF or DPO. The 'X' marks indicate the random chance accuracy for each dataset.  The results highlight that even after preference tuning, the LLMs fail to achieve high ranking accuracy, suggesting a limitation in the effectiveness of current preference learning algorithms.", "section": "3.1 Existing Reference Models Rarely Have Correct Rankings"}, {"figure_path": "YkJ5BuEXdD/figures/figures_27_1.jpg", "caption": "Figure 1: Both reference and preference-tuned models exhibit low ranking accuracy on most preference datasets. Each point represents the length-normalized or non-length-normalized ranking accuracy of individual (1a) reference models (pre-trained or fine-tuned), or (1b) preference-tuned models (trained with DPO or RLHF). The random chance accuracy for each dataset is indicated with a black 'X'. We sub-sample 1K examples from each dataset and use the test split when available. We describe datasets in B.2 and list all numbers in Tables 2, 3, and 4. For UltraFeedback, ranking accuracy is measured with exact match across all 4 outputs (see App. A.6).", "description": "This figure shows that both reference and preference-tuned large language models (LLMs) achieve low ranking accuracy on various datasets.  The plots display the ranking accuracy of several LLMs before and after preference tuning using different methods (RLHF and DPO).  The low accuracy highlights that these models struggle to consistently rank preferred outputs above less preferred outputs, even when trained specifically for this purpose.  The figure also shows the random chance accuracy as a baseline for comparison.  This indicates a significant gap between current models and the ideal performance.", "section": "3.1 Existing Reference Models Rarely Have Correct Rankings"}, {"figure_path": "YkJ5BuEXdD/figures/figures_27_2.jpg", "caption": "Figure 1: Both reference and preference-tuned models exhibit low ranking accuracy on most preference datasets. Each point represents the length-normalized or non-length-normalized ranking accuracy of individual (1a) reference models (pre-trained or fine-tuned), or (1b) preference-tuned models (trained with DPO or RLHF). The random chance accuracy for each dataset is indicated with a black 'X'. We sub-sample 1K examples from each dataset and use the test split when available. We describe datasets in B.2 and list all numbers in Tables 2, 3, and 4. For UltraFeedback, ranking accuracy is measured with exact match across all 4 outputs (see App. A.6).", "description": "This figure shows that both reference and preference-tuned LLMs exhibit low ranking accuracy (less than 60%) on various preference datasets.  The plots compare the ranking accuracy of pre-trained and fine-tuned models (reference models) against models trained with preference learning algorithms (RLHF and DPO). The random chance accuracy is shown for comparison.  The datasets used include: UltraFeedback, HH-RLHF, SHP, Synthetic Instruct GPT-J Pairwise, and StackExchange Preferences.", "section": "3.1 Existing Reference Models Rarely Have Correct Rankings"}, {"figure_path": "YkJ5BuEXdD/figures/figures_27_3.jpg", "caption": "Figure 1: Both reference and preference-tuned models exhibit low ranking accuracy on most preference datasets. Each point represents the length-normalized or non-length-normalized ranking accuracy of individual (1a) reference models (pre-trained or fine-tuned), or (1b) preference-tuned models (trained with DPO or RLHF). The random chance accuracy for each dataset is indicated with a black 'X'. We sub-sample 1K examples from each dataset and use the test split when available. We describe datasets in B.2 and list all numbers in Tables 2, 3, and 4. For UltraFeedback, ranking accuracy is measured with exact match across all 4 outputs (see App. A.6).", "description": "This figure shows that both reference and preference-tuned LLMs exhibit low ranking accuracy across various datasets.  The low accuracy highlights a significant gap between the observed accuracy and the idealized accuracy achievable under perfect conditions.  This gap indicates that current preference learning methods struggle to effectively improve the ranking ability of LLMs, even when provided with perfect training data.", "section": "3.1 Existing Reference Models Rarely Have Correct Rankings"}, {"figure_path": "YkJ5BuEXdD/figures/figures_28_1.jpg", "caption": "Figure 1: Both reference and preference-tuned models exhibit low ranking accuracy on most preference datasets. Each point represents the length-normalized or non-length-normalized ranking accuracy of individual (1a) reference models (pre-trained or fine-tuned), or (1b) preference-tuned models (trained with DPO or RLHF). The random chance accuracy for each dataset is indicated with a black 'X'. We sub-sample 1K examples from each dataset and use the test split when available. We describe datasets in B.2 and list all numbers in Tables 2, 3, and 4. For UltraFeedback, ranking accuracy is measured with exact match across all 4 outputs (see App. A.6).", "description": "This figure shows that both reference and preference-tuned LLMs have low ranking accuracy (below 60%) across various datasets.  The left panel displays the ranking accuracy of various reference models, while the right panel shows the ranking accuracy of various preference-tuned models. The random chance accuracy is shown for comparison.", "section": "3.1 Existing Reference Models Rarely Have Correct Rankings"}, {"figure_path": "YkJ5BuEXdD/figures/figures_28_2.jpg", "caption": "Figure 1: Both reference and preference-tuned models exhibit low ranking accuracy on most preference datasets. Each point represents the length-normalized or non-length-normalized ranking accuracy of individual (1a) reference models (pre-trained or fine-tuned), or (1b) preference-tuned models (trained with DPO or RLHF). The random chance accuracy for each dataset is indicated with a black 'X'. We sub-sample 1K examples from each dataset and use the test split when available. We describe datasets in B.2 and list all numbers in Tables 2, 3, and 4. For UltraFeedback, ranking accuracy is measured with exact match across all 4 outputs (see App. A.6).", "description": "This figure shows the ranking accuracy of various reference and preference-tuned LLMs on several preference datasets.  The results reveal that most models, even those fine-tuned with preference learning algorithms (RLHF and DPO), achieve surprisingly low ranking accuracy (generally below 60%).  The figure highlights the significant gap between the observed and idealized ranking accuracies.", "section": "3.1 Existing Reference Models Rarely Have Correct Rankings"}, {"figure_path": "YkJ5BuEXdD/figures/figures_28_3.jpg", "caption": "Figure 1: Both reference and preference-tuned models exhibit low ranking accuracy on most preference datasets. Each point represents the length-normalized or non-length-normalized ranking accuracy of individual (1a) reference models (pre-trained or fine-tuned), or (1b) preference-tuned models (trained with DPO or RLHF). The random chance accuracy for each dataset is indicated with a black 'X'. We sub-sample 1K examples from each dataset and use the test split when available. We describe datasets in B.2 and list all numbers in Tables 2, 3, and 4. For UltraFeedback, ranking accuracy is measured with exact match across all 4 outputs (see App. A.6).", "description": "This figure shows the ranking accuracy of various reference and preference-tuned LLMs on several datasets.  It demonstrates that both reference models (before preference tuning) and state-of-the-art preference-tuned models achieve surprisingly low ranking accuracies (generally below 60%), highlighting a significant gap between the observed accuracy and the ideal achievable accuracy under perfect conditions.", "section": "3.1 Existing Reference Models Rarely Have Correct Rankings"}, {"figure_path": "YkJ5BuEXdD/figures/figures_28_4.jpg", "caption": "Figure 1: Both reference and preference-tuned models exhibit low ranking accuracy on most preference datasets. Each point represents the length-normalized or non-length-normalized ranking accuracy of individual (1a) reference models (pre-trained or fine-tuned), or (1b) preference-tuned models (trained with DPO or RLHF). The random chance accuracy for each dataset is indicated with a black 'X'. We sub-sample 1K examples from each dataset and use the test split when available. We describe datasets in B.2 and list all numbers in Tables 2, 3, and 4. For UltraFeedback, ranking accuracy is measured with exact match across all 4 outputs (see App. A.6).", "description": "This figure shows that both reference and preference-tuned LLMs exhibit low ranking accuracy (below 60%) across several datasets.  The left plot (a) shows ranking accuracy for various reference models (before preference tuning), while the right plot (b) displays the ranking accuracy after preference tuning using either RLHF or DPO.  The 'X' marks represent the chance accuracy, highlighting the poor performance relative to random guessing. The length-normalized and non-length-normalized accuracy are shown.", "section": "3.1 Existing Reference Models Rarely Have Correct Rankings"}, {"figure_path": "YkJ5BuEXdD/figures/figures_28_5.jpg", "caption": "Figure 1: Both reference and preference-tuned models exhibit low ranking accuracy on most preference datasets. Each point represents the length-normalized or non-length-normalized ranking accuracy of individual (1a) reference models (pre-trained or fine-tuned), or (1b) preference-tuned models (trained with DPO or RLHF). The random chance accuracy for each dataset is indicated with a black 'X'. We sub-sample 1K examples from each dataset and use the test split when available. We describe datasets in B.2 and list all numbers in Tables 2, 3, and 4. For UltraFeedback, ranking accuracy is measured with exact match across all 4 outputs (see App. A.6).", "description": "This figure shows that both reference and preference-tuned large language models (LLMs) achieve low ranking accuracy across various preference datasets.  The plots compare the ranking accuracy of different LLMs, highlighting the significant gap between the observed accuracy and the theoretically achievable accuracy. This discrepancy underscores the challenges faced by preference learning algorithms in effectively learning preference rankings.", "section": "3.1 Existing Reference Models Rarely Have Correct Rankings"}, {"figure_path": "YkJ5BuEXdD/figures/figures_28_6.jpg", "caption": "Figure 1: Both reference and preference-tuned models exhibit low ranking accuracy on most preference datasets. Each point represents the length-normalized or non-length-normalized ranking accuracy of individual (1a) reference models (pre-trained or fine-tuned), or (1b) preference-tuned models (trained with DPO or RLHF). The random chance accuracy for each dataset is indicated with a black 'X'. We sub-sample 1K examples from each dataset and use the test split when available. We describe datasets in B.2 and list all numbers in Tables 2, 3, and 4. For UltraFeedback, ranking accuracy is measured with exact match across all 4 outputs (see App. A.6).", "description": "This figure shows that both reference and preference-tuned LLMs exhibit low ranking accuracy (below 60%) across various datasets.  The plots compare length-normalized and non-length-normalized ranking accuracy, highlighting a significant gap between the achieved accuracy and the idealized accuracy achievable under perfect conditions.  The figure underscores a key finding of the paper: preference learning algorithms struggle to significantly improve ranking accuracy.", "section": "3.1 Existing Reference Models Rarely Have Correct Rankings"}]