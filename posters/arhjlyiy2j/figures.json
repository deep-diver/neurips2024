[{"figure_path": "arHJlYiY2J/figures/figures_2_1.jpg", "caption": "Figure 1: An illustration of pairwise collaborative video generation. Existing video diffusion models generate videos separately, which may result in inconsistent frame contents (e.g., geometries, objects, motions) across videos (Left); Collaborative video generation aims to produce videos sharing the same underlying content (Middle); In this work, we train our model on video pair datasets, and extend it to generate more collaborative videos (Right).", "description": "This figure illustrates the difference between conventional video generation and collaborative video diffusion.  Conventional methods generate videos independently, leading to inconsistencies in content and dynamics across videos.  Collaborative video diffusion, on the other hand, generates videos with shared underlying content, ensuring consistency despite different camera angles. The figure shows how this approach can be scaled from pairs of videos to multiple videos.", "section": "3 Collaborative Video Generation"}, {"figure_path": "arHJlYiY2J/figures/figures_3_1.jpg", "caption": "Figure 2: Architecture of collaborative video diffusion. Left: The model takes two (or more) noisy video features and camera trajectories as input and generates the noise prediction for both videos. Note that the image autoencoder of Stable Diffusion is omitted here; Right: Our Cross-View Synchronization Module takes the same frames from the two videos along with the corresponding fundamental matrix as input, and applies a masked cross-view attention between the frames.", "description": "This figure illustrates the architecture of the Collaborative Video Diffusion (CVD) model. The left panel shows an overview of the model's architecture, highlighting the input (noisy video features and camera trajectories), the processing steps (CVD model and Cross-View Synchronization Module), and the output (predicted noise for each video). The right panel zooms in on the Cross-View Synchronization Module, showing how it uses masked attention to align features from corresponding frames of different videos.  This module is a key component of CVD, enabling consistent multi-video generation.", "section": "4 Collaborative Video Diffusion with Camera Control"}, {"figure_path": "arHJlYiY2J/figures/figures_4_1.jpg", "caption": "Figure 3: Two-Phase Hybrid Training. We use different data processing schemes to handle the two datasets (Top) and apply separate model structures to train in corresponding phases (Bottom).", "description": "This figure illustrates the two-phase hybrid training strategy used in the paper.  The top half shows how the two datasets, RealEstate10K and WebVid10M, are processed differently.  RealEstate10K videos are folded to create synchronized video pairs, while WebVid10M videos undergo homography augmentation to simulate multiple views. The bottom half shows the distinct model structures used for each phase.  The RealEstate10K phase leverages CameraCtrl and its associated components, while the WebVid10M phase uses AnimateDiff and only includes the cross-view synchronization module (CVSM) with pseudo-epipolar masks, without the pose LoRA and camera embedding components used in the RealEstate10K phase.", "section": "4.2 Hybrid Training Strategy from Two Datasets"}, {"figure_path": "arHJlYiY2J/figures/figures_7_1.jpg", "caption": "Figure 4: Qualitative comparison. Our method maintains consistency across videos for static and dynamic scenes, while no prior work can generate synchronized different realizations. * Despite our best efforts, we are incapable of getting MotionCtrl [60]+SVD [5] to generate any motion beyond the simplest static camera zooming in-and-out. Please refer to our supplemental video for illustration.", "description": "This figure compares the results of CVD with several baselines on generating videos from the same prompts but with different camera trajectories. The left column shows results for prompts related to a coastline with stormy weather, while the right column shows results for prompts related to rabbits in a forest.  Each row represents a different method: MotionCtrl, MotionCtrl+SVD, CameraCtrl, CameraCtrl+SparseCtrl, and CVD (Ours).  The figure highlights CVD's ability to maintain consistency in content and dynamics across multiple videos, a significant improvement over the baselines, which struggle to maintain consistency, especially in terms of motion. Note that MotionCtrl+SVD failed to generate any significant motion besides simple zooming.", "section": "5.1 Qualitative Results"}, {"figure_path": "arHJlYiY2J/figures/figures_9_1.jpg", "caption": "Figure 5: Multi-view Video Generation. Left: The cameras move towards 4 directions, while all cameras are looking at the same 3D point; Right: The trajectories are interpolated from one trajectory (1st Row) to another (4th Row).", "description": "This figure shows examples of multi-view video generation results.  The left side demonstrates videos generated with camera trajectories moving in four different directions while focusing on a single 3D point. The right side showcases videos where trajectories are interpolated, starting from one path and smoothly transitioning to another.", "section": "5 Experiments"}, {"figure_path": "arHJlYiY2J/figures/figures_16_1.jpg", "caption": "Figure 2: Architecture of collaborative video diffusion. Left: The model takes two (or more) noisy video features and camera trajectories as input and generates the noise prediction for both videos. Note that the image autoencoder of Stable Diffusion is omitted here; Right: Our Cross-View Synchronization Module takes the same frames from the two videos along with the corresponding fundamental matrix as input, and applies a masked cross-view attention between the frames.", "description": "This figure illustrates the architecture of the Collaborative Video Diffusion (CVD) model.  The left panel shows an overview of the CVD model which takes as input multiple noisy video features and corresponding camera trajectories. These inputs are processed to generate noise predictions for each video. The image autoencoder from Stable Diffusion is not explicitly shown for simplification. The right panel zooms into the Cross-View Synchronization Module (CVSM), a crucial part of CVD.  The CVSM takes corresponding frames from different videos and their fundamental matrices as input.  It utilizes a masked cross-view attention mechanism to enhance consistency between the videos by aligning features based on epipolar geometry.", "section": "4 Collaborative Video Diffusion with Camera Control"}, {"figure_path": "arHJlYiY2J/figures/figures_17_1.jpg", "caption": "Figure 6: Exemplar visualization of epipolar-attention map.", "description": "This figure visualizes the attention mechanism used in the Cross-View Synchronization Module.  It shows attention maps for two example images, one of a fish tank and the other of a landscape. The green circles highlight specific pixels in the first image, and the corresponding attention weights are displayed in the second image.  The brighter colors indicate stronger attention to corresponding epipolar regions, demonstrating the model's ability to effectively correlate semantically consistent regions across videos from different camera viewpoints.", "section": "4.1 Cross-View Synchronization Module"}, {"figure_path": "arHJlYiY2J/figures/figures_17_2.jpg", "caption": "Figure 4: Qualitative comparison. Our method maintains consistency across videos for static and dynamic scenes, while no prior work can generate synchronized different realizations. * Despite our best efforts, we are incapable of getting MotionCtrl [60]+SVD [5] to generate any motion beyond the simplest static camera zooming in-and-out. Please refer to our supplemental video for illustration.", "description": "This figure compares the results of the proposed Collaborative Video Diffusion (CVD) model with several baseline models for generating videos from different camera viewpoints. The left column shows videos of a castle scene, and the right column shows videos of an aquarium scene. The top rows show the results of the baseline models: CameraCtrl, CameraCtrl+SparseCtrl, and MotionCtrl+SVD.  The bottom row shows the results of the proposed CVD model.  The CVD model produces significantly more consistent videos across different camera perspectives, especially in the dynamic aspects of the scenes, such as the movement of fish in the aquarium or the lightning in the castle scene.", "section": "5.1 Qualitative Results"}, {"figure_path": "arHJlYiY2J/figures/figures_18_1.jpg", "caption": "Figure 4: Qualitative comparison. Our method maintains consistency across videos for static and dynamic scenes, while no prior work can generate synchronized different realizations. * Despite our best efforts, we are incapable of getting MotionCtrl [60]+SVD [5] to generate any motion beyond the simplest static camera zooming in-and-out. Please refer to our supplemental video for illustration.", "description": "This figure shows a qualitative comparison of the proposed CVD model with several baselines on video generation.  The results demonstrate that CVD outperforms the baselines in maintaining consistency across videos generated with different camera trajectories, especially in dynamic scenes. The superior performance of CVD is highlighted by the consistent content and motion across multiple videos, unlike the baselines which often exhibit inconsistencies in geometries, objects, and motions.", "section": "5.1 Qualitative Results"}, {"figure_path": "arHJlYiY2J/figures/figures_19_1.jpg", "caption": "Figure 2: Architecture of collaborative video diffusion. Left: The model takes two (or more) noisy video features and camera trajectories as input and generates the noise prediction for both videos. Note that the image autoencoder of Stable Diffusion is omitted here; Right: Our Cross-View Synchronization Module takes the same frames from the two videos along with the corresponding fundamental matrix as input, and applies a masked cross-view attention between the frames.", "description": "This figure illustrates the architecture of the Collaborative Video Diffusion (CVD) model.  The left panel shows an overview of the CVD model's structure, highlighting its input (noisy video features and camera trajectories), its processing of these inputs, and its output (noise predictions for video generation). The right panel focuses specifically on the Cross-View Synchronization Module, detailing its input (frames from two videos and the fundamental matrix) and its operation (masked cross-view attention).  The figure emphasizes the key components involved in achieving consistent multi-video generation with camera control, showing the interaction between the main model and the synchronization module.", "section": "4 Collaborative Video Diffusion with Camera Control"}, {"figure_path": "arHJlYiY2J/figures/figures_20_1.jpg", "caption": "Figure 10: Additional Qualitative Results with different camera trajectories and realizations.", "description": "This figure shows additional qualitative results demonstrating the model's ability to generate multiple videos with different camera trajectories and realizations while maintaining consistent content and dynamics.  Each row represents a separate set of videos generated from the same text prompt, but with varying camera viewpoints. The consistency in content across different viewpoints highlights the effectiveness of the proposed Collaborative Video Diffusion (CVD) method in achieving multi-view video generation with camera control.", "section": "5.1 Qualitative Results"}, {"figure_path": "arHJlYiY2J/figures/figures_21_1.jpg", "caption": "Figure 11: Additional Qualitative Results with different camera trajectories and realizations.", "description": "This figure shows additional qualitative results generated by the model with different camera trajectories and realizations.  The prompt used was: \"an aerial view of a cyberpunk city, night time, neon lights, masterpiece, high quality.\" The results demonstrate the model's ability to generate consistent videos across multiple viewpoints, even when the camera movements vary significantly. Each row represents a different set of camera trajectories, and the images within each row show the generated video frames for that set of trajectories.", "section": "5.1 Qualitative Results"}, {"figure_path": "arHJlYiY2J/figures/figures_22_1.jpg", "caption": "Figure 12: Additional Qualitative Results with different camera trajectories and realizations.", "description": "This figure shows several sets of videos generated using different camera trajectories. Each set contains multiple videos, all sharing the same scene and content, but viewed from different angles and perspectives.  The consistency of the generated videos across different viewpoints demonstrates the effectiveness of the proposed CVD method in generating coherent multi-view video content.", "section": "5 Experiments"}, {"figure_path": "arHJlYiY2J/figures/figures_23_1.jpg", "caption": "Figure 4: Qualitative comparison. Our method maintains consistency across videos for static and dynamic scenes, while no prior work can generate synchronized different realizations. * Despite our best efforts, we are incapable of getting MotionCtrl [60]+SVD [5] to generate any motion beyond the simplest static camera zooming in-and-out. Please refer to our supplemental video for illustration.", "description": "This figure compares the results of the proposed CVD model with several baseline methods for generating videos from the same scene with different camera trajectories. The results show that the CVD model produces videos with higher consistency in terms of content and motion, even when compared to baselines that also incorporate camera control. Notably, existing methods struggle to create multiple videos of dynamic scenes from various viewpoints.", "section": "5.1 Qualitative Results"}, {"figure_path": "arHJlYiY2J/figures/figures_24_1.jpg", "caption": "Figure 1: An illustration of pairwise collaborative video generation. Existing video diffusion models generate videos separately, which may result in inconsistent frame contents (e.g., geometries, objects, motions) across videos (Left); Collaborative video generation aims to produce videos sharing the same underlying content (Middle); In this work, we train our model on video pair datasets, and extend it to generate more collaborative videos (Right).", "description": "This figure illustrates the difference between conventional video generation and collaborative video generation.  Conventional methods produce videos independently, leading to inconsistencies in content across different videos. In contrast, collaborative video generation aims to produce videos sharing the same underlying content and motion, ensuring consistency. The figure demonstrates how the authors' method extends pairwise video generation to create multiple collaborative videos from different camera angles.", "section": "3 Collaborative Video Generation"}, {"figure_path": "arHJlYiY2J/figures/figures_25_1.jpg", "caption": "Figure 15: Visualization of homography warping for WebVid10M data. For each video clip, the top row represents the original frames from the video, and the bottom row represents the frames warped by homography transformations.", "description": "This figure shows example video clips from the WebVid10M dataset before and after applying homography transformations. Homography warping is a technique used to transform an image into a new perspective by applying a homography matrix, which represents a projective transformation. In the context of this paper, homography warping is used to augment the WebVid10M dataset by creating additional video pairs for training a model that generates multiple consistent videos from different camera trajectories.", "section": "4.2 Hybrid Training Strategy from Two Datasets"}]