[{"type": "text", "text": "Be Confident in What You Know: Bayesian Parameter Efficient Fine-Tuning of Vision Foundation Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep Shankar Pandey \u2020 Spandan Pyakurel \u2020 Qi $\\mathbf{Y_{u}}^{*}$ Rochester Institute of Technology {dp7972,sp1468,qi.yu}@rit.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large transformer-based foundation models have been commonly used as pretrained models that can be adapted to different challenging datasets and settings with state-of-the-art generalization performance. Parameter efficient fine-tuning (PEFT) provides promising generalization performance in adaptation while incurring minimum computational overhead. However, adaptation of these foundation models through PEFT leads to accurate but severely underconfident models, especially in few-shot learning settings. Moreover, the adapted models lack accurate fine-grained uncertainty quantification capabilities limiting their broader applicability in critical domains. To flil out this critical gap, we develop a novel lightweight Bayesian Parameter Efficient Fine-Tuning (referred to as Bayesian-PEFT) framework for large transformer-based foundation models. The framework integrates state-of-the-art PEFT techniques with two Bayesian components to address the under-confidence issue while ensuring reliable prediction under challenging fewshot settings. The first component performs base rate adjustment to strengthen the prior belief corresponding to the knowledge gained through pre-training, making the model more confident in its predictions; the second component builds an evidential ensemble that leverages belief regularization to ensure diversity among different ensemble components. Our thorough theoretical analysis justifies that the Bayesian components can ensure reliable and accurate few-shot adaptations with well-calibrated uncertainty quantification. Extensive experiments across diverse datasets, few-shot learning scenarios, and multiple PEFT techniques demonstrate the outstanding prediction and calibration performance by Bayesian-PEFT. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformer-based foundation models have been developed as general models with state-of-the-art generalization performance [32, 66, 52, 28]. These models leverage the rich meta-knowledge acquired during the pre-training stage to effectively adapt to complex downstream tasks [32], where pretraining is usually performed on massive-scale annotated datasets (e.g., [35, 55]) through supervised learning[32, 66, 28] or self-supervised learning [52]. To achieve effective adaption, various parameterefficient fine-tuning (PEFT) approaches have been developed [38, 27, 9, 54] that introduce a small number of tunable parameters either within or outside of the backbone architecture to ensure good generalization capability while incurring little computational overhead because most parts of (or the entire) backbone architecture is frozen during fine-tuning [25, 59, 67]. Bias-fine tuning [9], a representative partial tuning-based PEFT, only fine-tunes the bias parameters to downstream tasks. Adapter [51] and side-tune [72] fine-tuning techniques are instances of extra module-based PEFT that introduce extra parameterized modules and fine-tune them based on the downstream tasks. Visual Prompt-tuning [32] (VPT) follows the popular prompt learning paradigm by introducing a learnable prompt that is fine-tuned on the downstream task knowledge keeping the pre-trained backbone frozen. ", "page_idx": 0}, {"type": "image", "img_path": "loQCk0qruU/tmp/045dd395d1916b47bdd22b022577f4d2d50b85ce9978fcb16256f58df639c6e6.jpg", "img_caption": ["Figure 1: Accuracy Vs. ECE on CIFAR100 few-shot adaptation from different PEFT methods "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite the attractive generalization performance, most foundation models adapted to downstream few-shot tasks through PEFT exhibit a somewhat surprising and undesirable behavior that may prevent them from being applied to many critical domains. Figure 1 (b) shows the predictive accuracy versus the Expected Calibration Error (ECE) of a foundation model after performing few-shot adaptation on CIFAR-100 using a series of representative PEFT methods, including VPT [32], Adapter [51], Bias Fine Tuning [71], and Side-tune [72]. It is clear that the adapted model is able to provide accurate predictions even after fine-tuned on limited training samples. For example, all PEFT methods help to boost the model\u2019s accuracy to over $75\\%$ using just 5-shot fine-tuning and the accuracy reaches $80\\%$ after 10-shot fine-tuning. However, the adapted model is very poorly calibrated as shown by the large ECE scores, which are consistently over 0.3 across all the fine-tuning methods. Increasing the fine-tuning size does not show clear improvement and sometimes even hurts the calibration performance. While one may expect the poor ECE to be caused by over-confidence as we fine-tune a large foundation model using very limited training samples, the detailed ECE plots as shown in Figure 2 (a)-(c) reveal that the model is in fact severely under-confident. For example, after adapting to the 1-shot training dataset, the VPT fine-tuned model can already achieve a test accuracy close to $50\\%$ but is under-confident in almost all its predictions leading to an ECE score over 0.45. The under-confidence issue is observed for all representative PEFT methods across different datasets and various few-shot learning settings as evidenced by our experiments. ", "page_idx": 1}, {"type": "text", "text": "The under-confident few-shot adaptation behavior of foundation models closely mimics how human experts with rich domain knowledge in their own disciplines tend to make conservative decisions when facing new tasks that deviate from their own expertise. Analogous to their human counterparts, the rich prior knowledge gained through the pre-training stage of foundational models overshadows the relatively limited knowledge obtained through few-shot fine-tuning, which prevents them from making more confident predictions in the downstream tasks. Unreliable uncertainty (i.e., confidence) quantification makes the predictions provided by these models less trustworthy, which may limit applying the promising \u201cpre-train-then-finetune\u201d paradigm to many critical domains. As shown in Figure 2 (a)-(c), the fine-tuned model seldom makes any predictions with confidence over $80\\%$ , making it difficult to leverage these predictions in any high-stakes decision-making process. ", "page_idx": 1}, {"type": "text", "text": "The need to balance between the rich prior knowledge gained through pre-training and the new knowledge obtained through few-shot adaptation inspires us to investigate the under-confidence issue from the Bayesian perspective. In particular, we propose to look into the predictive behavior of the few-shot adapted foundation model through the lenses of evidential learning [56], which is built upon Bayesian theorem and subjective logic (SL) theory [33]. As part of the recent advances in modern Bayesian modeling, evidential learning provides a cost-effective way to perform Bayesian inference with the capability to quantify fine-grained second-order uncertainty [57]. By leveraging the important theoretical connection between fine-grained uncertainty and model accuracy [46] , we unveil the underlying reason that supports the good predictive performance of a few-shot adapted foundation model and the root cause for the under-confident behavior. Drawing from this important insight as outlined above, we propose to integrate the modern PEFT techniques into a novel lightweight Bayesian framework, referred to as Bayesian-PEFT (B-PEFT), aiming to achieve highly reliable and accurate few-shot adaptations with well-calibrated and trustworthy uncertainty quantification. ", "page_idx": 1}, {"type": "image", "img_path": "loQCk0qruU/tmp/354ce4a345fda147172dfe0095fea1f6b45a2bf25816aaf1fa82e7be2df61a7e.jpg", "img_caption": ["Figure 2: PEFT on the 1-shot CIFAR100 dataset: all existing PEFT techniques exhibit severe underconfidence while the proposed B-PEFT reduces the ECE by almost an order of magnitude. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The proposed Bayesian framework offers two important components to address the under-confidence issue while ensuring reliable prediction under challenging few-shot settings. The first component makes novel adjustments to the base rates introduced by the SL theory to strengthen the prior belief corresponding to the knowledge gained through pre-training. Meanwhile, the adjustment does not change the relative order of the belief assigned to different classes, which ensures that the model accuracy is maintained. Our theoretical analysis shows that the proposed base rate adjustment strategy leads to more confident predictions (by increasing the gaps between the belief assigned to the groundtruth class and the rest) without compromising the model\u2019s accuracy. Figure 2d shows that B-PEFT significantly improves the model calibration. To further enhance the reliability of both prediction accuracy and uncertainty quantification when performing few-shot adaptation, the second component performs Bayesian model averaging by building a diversity-inducing evidential ensemble. In addition to using different random initialization of the PEFT components, diversity is further enhanced through incorrect belief regularization that penalizes a model for assigning a high belief to a non-ground-truth class. By controlling the strength of belief regularization, different ensemble components are guided to learn from diverse features in the data space, where a light penalty allows an ensemble component to learn the common discriminative features while a heavy one will force an ensemble component to learn rare features to avoid errors on the difficult data samples. A deeper theoretical analysis of the proposed diversity-induced evidential ensemble is equivalent to Stein Variational Gradient Descent (SVGD) based ensembles [13]. Experiments on multiple challenging few-shot learning tasks justify the superior performance of B-PEFT over state-of-the-art PEFT baselines, in terms of both prediction accuracy and uncertainty calibration performance. Our contributions can be summarized as follows: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We identify the severe under-confidence issue of pre-trained foundation models after performing parameter-efficient fine-tuning over few-shot datasets. The fine-grained uncertainty analysis through evidential learning and SL theory reveals the root cause for their good predictive performance while being under-confident. \u2022 We develop a novel lightweight Bayesian framework that integrates state-of-the-art PEFT techniques with two Bayesian components to address the under-confidence issue while ensuring reliable prediction under challenging few-shot settings. The first component performs base rate adjustment to strengthen the prior belief corresponding to the knowledge gained through pre-training, making the model more confident in its predictions; the second component builds an evidential ensemble that leverages belief regularization to ensure diversity among different ensemble components. \u2022 We perform thorough theoretical analysis to justify why the proposed Bayesian components can ensure reliable and accurate few-shot adaptations with well-calibrated uncertainty quantification. \u2022 We carry out experiments with 4 benchmark datasets, 5 different few-shot settings, and 3 parameter efficient fine-tuning techniques that demonstrate the effectiveness of the developed model. ", "page_idx": 2}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Parameter Efficient Fine Tuning of Foundation Models. Transformer-based foundation models [63, 15] have been developed as an improvement over traditional convolution-based architectures [29, 31] for computer vision tasks. The transformer-based models achieve strong generalization performance [40] after training on large datasets. Moreover, the pre-trained transformers can be fine-tuned in limited data settings leading to state-of-the-art performance [32, 27]. As a computation, memory, and parameter-efficient alternative to full fine-tuning of such large pre-trained transformers, different Parameter Efficient Fine Tuning (PEFT) approaches have been developed. PEFT techniques freeze most of the large transformer backbone, fine-tune the remaining backbone parameters and/or introduce lightweight extra modules for adapting to the downstream task. Existing approaches can broadly be categorized as extra-module-based [72, 54], partial-tuning-based [71, 9], and visual prompt tuning-based [68, 68, 28] methods. Extra-module-based methods (e.g., Adapter [51], side-tune[72]) introduce small additional learnable modules and keep the pre-rained backbone frozen. Partial-tuningbased methods (e.g., Bias [9]) keep a large portion of the backbone frozen, and fine-tune only part of the foundation model to downstream tasks. Visual prompt-based PEFT methods (VPT) introduce a learnable prompt variable along with a learnable classification head over the fixed pre-trained backbone to be adapted to the downstream task. VPT [32] has shown significant improvement over other PEFT techniques, and can even outperform full fine-tuning in multiple datasets/settings [28]. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Calibration in Deep Learning Models Calibration methods have been increasingly explored to achieve trustworthy deep-learning models. Post-hoc calibration methods [26, 43, 73] aim to learn a calibration map for a standard trained deep learning model such that the map can transform the poorly calibrated probabilities to calibrated probabilities. Regularization-based calibration approaches introduce explicit regularization (such as with $\\mathrm{L_{2}}$ regularization [26], entropy regularization [50]), or implicit regularization (such as with focal loss [39]) during training to ensure that the trained model is calibrated. Data augmentation methods such as Label smoothing [44], and mixup training [60][74] have also been explored for developing calibrated deep learning models. Recent survey [65] provides a discussion of the most relevant works towards developing calibrated deep learning models. Most existing calibration methods are designed to tackle the over-confidence issue, which is more commonly observed for large models trained from limited data due to overfitting. We observe that fine-tuned foundation models exhibit severe under-confidence in their predictions, where existing calibration techniques are less effective. To this end, we propose a lightweight Bayesian framework that fills this critical gap. ", "page_idx": 3}, {"type": "text", "text": "Few-Shot Adaptation and Relationship with Meta-Learning. In this work, we consider few-shot adaption with a focus on $N$ -way $K$ -shot classification [64, 22], where the model is presented with a few-shot training set with $N$ -class, each having $K$ examples. For instance, 1-shot Cifar100 training set consists of 1 sample from each of the 100 classes. The model is then evaluated on the test set, which is identical to the query set in the meta-testing tasks [58]. It is worth to note that meta-learning (e.g., matching networks [64], MAML [16], VERSA [23], PLATIPUS [17]) leverages an episodic learning paradigm to achieve few-shot adaptation, where both meta-training and meta-testing are done on the task level in an episodic fashion [69, 37] with a large number of $N$ -way $K$ -shot training tasks. In this work, we consider more challenging few-shot adaptation tasks (e.g., 100-way 1-shot in Cifar100 and 102-way 1-shot in Flowers102) compared to the commonly used 5-way 1-shot meta-learning tasks. We leverage the power of the pre-trained foundation models, which eliminates the need of task based episodic meta-training. From a meta-learning perspective, the pre-training phase for the foundation model could be viewed as performing meta-knowledge acquisition similar to meta-training. The pre-trained model can be seen as an expert equipped with the meta-knowledge, and parameter-efficient fine-tuning performs quick adaptation to the downstream tasks, analogous to the support-set based adaptation done in meta-testing. ", "page_idx": 3}, {"type": "text", "text": "3 Bayesian Parameter-Efficient Fine-Tuning of Foundation Models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We start by introducing some fundamental concepts from evidential learning, which will serve as key building blocks in the proposed Bayesian-PEFT framework. We then detail the two Bayesian components: base rate adjustment to address under-confidence and diversity-inducing evidential ensemble to improve the reliability on both prediction accuracy and uncertainty quantification. ", "page_idx": 3}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Evidential Deep Learning (EDL) [56] introduces a computationally efficient framework to transform deterministic deep learning (DL) models into uncertainty-aware models. The key idea is to introduce a higher-order conjugate prior distribution over the predicted likelihood distribution and train the DL model to output parameters of the higher-order distribution. Towards classification, EDL models [56, 11] introduce Dirichlet prior distribution for the multinomial likelihood distribution. Specifically, the output softmax layer of the DL model is replaced by a monotonic, non-negative transformation function (e.g., ReLU, SoftPlus, or exp) to obtain the evidence for different classes that are transformed into the Dirichlet parameters. Mathematically, for a DL model $f_{\\theta}(\\cdot)$ , and an input sample $\\mathbf{x}$ , we have ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\ne_{i}=\\mathcal{E}\\big(f_{\\theta}(\\mathbf{x})\\big)_{i}\\quad\\alpha_{i}=e_{i}+a_{i}\\times W\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $e_{i}$ is the output evidence for the $i^{\\mathrm{th}}$ class from the model $f_{\\theta}(\\cdot)$ and input sample $\\mathbf{x}$ , $a_{i}$ is the base rate for the $i^{\\mathrm{th}}$ class, $W$ is the non-informative prior weight usually set to the number of classes, $\\mathcal{E}$ is the non-negative transformation function, and $\\alpha_{i}$ parameterizes a Dirichlet distribution. Existing EDL works usually adopt a non-informative base rate of $\\begin{array}{r}{a_{i}=\\frac{1}{N}\\forall i\\in[1,N]}\\end{array}$ . Furthermore, a multinomial distribution $\\mathtt{M u l t}(y|\\mathbf{p})$ over labels is parameterized as $\\begin{array}{r}{\\dot{\\mathbb{E}}[p_{i}]=\\frac{\\alpha_{i}}{S}}\\end{array}$ , where the total Dirichlet Strength $\\begin{array}{r}{S=\\sum_{i=1}^{N}\\alpha_{i}}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "An evidential model can be trained via a Type-II Maximum Likelihood-based evidential loss $\\mathcal{L}^{\\mathrm{Log}}(x,y)$ [56] with KL regularization that penalizes evidence assigned to non-ground-truth classes: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{evid}}(x,y)=\\mathcal{L}^{\\mathrm{Log}}(x,y)+\\lambda\\mathrm{KL}\\big(\\mathtt{D i r}(p|\\Tilde{\\alpha})||\\mathtt{D i r}(p|\\mathbf{1})\\big)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\tilde{\\alpha}=\\pmb{y}+(\\mathbf{1}-\\pmb{y})\\odot\\pmb{\\alpha}$ . Once trained, the evidential model can predict an evidence vector $e=$ $(e_{1},e_{2},...e_{N})^{\\top}$ for a given test sample $\\textbf{\\em x}$ . From the predicted evidence, we obtain the model\u2019s belief (b) over different classes, the correct belief $b_{\\mathsf{c o r}}$ , incorrect belief $b_{\\mathrm{inc}}$ , and vacuity $u$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{b}=\\frac{\\mathbf{e}}{S}\\quad,\\quad b_{\\mathrm{cor}}=\\sum\\mathbf{y}\\odot\\mathbf{b}\\quad,\\quad b_{\\mathrm{inc}}=\\sum(\\mathbf{1}-\\mathbf{y})\\odot\\mathbf{b}\\quad,\\quad u=\\frac{N}{S},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where vacuity $u$ is a second-order uncertainty [33] that captures the model\u2019s lack of knowledge in its prediction; $\\mathbf{b}_{\\mathsf{c o r}}$ and $\\mathbf{b}_{\\mathrm{inc}}$ quantify model accuracy and error, respectively. However, neither $\\mathbf{b}_{\\mathsf{c o r}}$ nor $\\mathbf{b}_{\\mathrm{inc}}$ can be evaluated without the ground-truth label, which is not available in the testing phase. Existing theoretical work has established an important connection between $\\mathbf{b}_{\\mathrm{inc}}$ and dissonance [46], which is another second-order uncertainty [33] that can be quantified without the ground-truth label. More specifically, dissonance dis can be evaluated as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathtt{d i s}=\\sum_{n=1}^{N}\\Big(b_{n}\\frac{\\sum_{j\\neq n}b_{j}B a l(b_{j},b_{n})}{\\sum_{j\\neq n}b_{j}}\\Big),\\quad B a l(b_{j},b_{n})=\\left\\{2\\frac{\\operatorname*{min}(b_{j},b_{n})}{b_{j}+b_{n}},\\quad\\mathrm{if~}b_{i}b_{j}>0\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $B a l(\\cdot,\\cdot)$ is the relative mass balance function between two belief masses. The dissonance essentially captures the conflicting belief assigned to different classes [57]. ", "page_idx": 4}, {"type": "text", "text": "3.2 Strengthening the prior belief through base rate adjustment ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To gain a deeper understanding of the under-confident few-shot adaptation behavior of foundation models, we perform fine-grained uncertainty analysis using the predicted evidence from an evidential model. To this end, we replace the softmax layer in a VPT fine-tuned transformer model with an exponential-based evidential head that outputs non-negative evidence for different classes. We analyze the output evidence from the evidential model that reveals some interesting insights. ", "page_idx": 4}, {"type": "text", "text": "Why is the model accurate? First, we observe that the relative order of the evidence assigned to different classes is accurate. This implies that the model outputs relatively greater evidence for the correct class compared to all other classes that ensure the model\u2019s strong predictive performance. To more precisely quantify the model\u2019s accuracy, we adapt the lower bound of incorrect belief theorem developed for meta-learning [46] to evaluate the model accuracy through its predicted dissonance: ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Consider an evidential model that outputs incorrect belief of $b_{i n c}$ and the dissonance in the beliefs is dis. Then, the incorrect belief of the model will be at least half of the dissonance for all predictions from the evidential model. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac12d i s\\leq b_{i n c}\\quad w h e r e\\quad0\\leq d i s\\leq1\\quad\\&\\quad0\\leq b_{i n c}\\leq1\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Figure 3a shows the test accuracy vs. dissonance curve, which is aligned with the relationship between the incorrect belief and the dissonance given in the theorem above, where a low dissonance implies a low $\\boldsymbol{b}_{\\mathrm{inc}}$ (or a high accuracy). From all the testing samples, we observe relatively low dissonance and the highest is only slightly above 0.7 as shown in the figure. We further evaluate the Area Under the Curve of the Accuracy vs. $(1-{\\tt d i s})$ and obtain an AUC of 0.82 as shown in Figure 3b. This implies the model is able to clearly discriminate the ground-truth label from the rest without much confusion (i.e., low dissonance) which ensures its good prediction accuracy. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "loQCk0qruU/tmp/08ccd8fea93eb0dcd627a748bb9d95ca3f81586ba7c32143e21f5ca9acdfa630.jpg", "img_caption": ["Figure 3: 1-shot Cifar10 results and evidence vacuity trends "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Why is the model under-confident? Despite being able to assign relatively more evidence to the correct label over the rest, it is also interesting to observe that the model generally assigns very low evidence to all the labels, including the correct one. Figure 3c shows the evidence distribution of one representative test data sample from Cifar100. As can be seen, most classes are assigned very low evidence that is close to zero. The ground-truth class is assigned higher evidence, but it is far from sufficient to make the prediction confident. The resultant confidence is only 0.025 while the vacuity is extremely high at 0.875, implying that the model believes it has very limited knowledge of the data sample despite it correctly identifying the correct label. Figure 3d shows the predicted vacuity over all the test samples, most of which are assigned a very high vacuity. This confirms the overly conservative behavior of the model, where the low confidence is primarily due to the insufficient allocation of evidence in its predictions. On the other hand, since the model is fairly accurate, it is reasonable to believe that the model underestimates the contribution of the rich prior knowledge gained through pre-training. ", "page_idx": 5}, {"type": "text", "text": "Base rate adjustment to strength the prior belief. The fine-grained uncertainty analysis through the lenses of evidential learning not only explains the good predictive performance of the few-shot adapted model through PEFT but also unveils the root cause for its under-confident behavior, which is under-estimation of the contribution from the prior knowledge to the downstream task. While the classical Bayes\u2019 theorem offers a principal idea to address the issue, which is to strengthen the prior belief, there is a lack of practical way to achieve this. To this end, we propose to leverage the base rate introduced by the subjective logic theory [33] as an effective vehicle to adjust the prior belief gained through pre-training. According to (1), adjusting the base rate has the effect of changing the Dirichlet parameter $\\alpha$ , which will change the confidence for the prediction given by $\\operatorname*{max}_{i}\\mathbb{E}[p_{i}]$ . However, base rate adjustment needs to meet two key requirements: (1) the relative order of the Dirichlet parameters assigned to different classes should be preserved so that the predictive performance of the model remains unaffected, (2) the gap between the Dirichlet parameters for different classes is transformed such that the model becomes more confident in its predictions, making it well-calibrated. To meet these requirements, we we propose a transformation function $A_{m}$ to the model\u2019s output evidence such that the model is well calibrated without any compromise in the generalization performance: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\alpha={\\mathcal{A}}_{m}{\\big(}f_{\\theta}(\\mathbf{x_{i}}){\\big)}=\\mathbf{e}+W\\chi\\quad,\\quad\\chi_{i}=a_{i}^{\\mathbf{adj}}=\\left({\\frac{e_{i}-e_{\\operatorname*{min}}}{e_{\\operatorname*{min}}}}\\right)^{m}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\pmb{\\chi}=(\\chi_{1},\\chi_{2},...\\chi_{N})^{\\top}$ is the adjusted base rate, and $m\\geq1$ controls the base rate transformation. The adjusted base rate $x$ considers evidence of all classes as a reference via $e_{\\mathtt{m i n}}$ , and transforms the gap between different class evidences such that the model is well calibrated. ", "page_idx": 5}, {"type": "text", "text": "Lemma 2. The base-rate adjusted model that uses learnable base rate $\\pmb{\\chi}=(\\chi_{1},\\chi_{2},...,\\chi_{N})^{\\top}$ has the same generalization performance compared to the model using fixed base rate of $\\begin{array}{r}{a_{i}=\\frac{1}{N}\\forall i\\in[1,N]}\\end{array}$ Theorem 3. For any $m\\geq1$ , the transformation function $A_{m}$ transforms the base rate for the class with the highest evidence $e_{m a x}$ and class with the second highest evidence $e_{2n d}$ such that the gap in Dirichlet parameters between the two classes is non-decreasing. ", "page_idx": 5}, {"type": "text", "text": "Remark. Theorem 3 ensures that the expected probability $\\mathbb{E}[p_{i}]$ for the predicted class $i$ has an increased gap with the rest of the classes, which results in an increase of the model\u2019s confidence. Therefore, if the prediction is accurate, the model\u2019s calibration performance will be improved. Meanwhile, Lemma 2 ensures that the good prediction accuracy of the model is maintained by the proposed base rate adjustment strategy. The detailed proofs are given in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "3.3 Building A Diversity Induced Evidential Ensemble ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The second Bayesian component of the proposed B-PEFT framework aims to further improve the reliability of both prediction accuracy and uncertainty quantification when performing few-shot adaptation. It performs Bayesian model averaging by building a diversity-inducing evidential ensemble. The ensemble of deep learning models (i.e., deep ensemble) [20, 36] can effectively improve the generalization performance of deep learning models. Moreover, deep ensembles can capture the model uncertainty [36, 53] via the agreement-disagreement between the ensemble components. Model uncertainty essentially captures the uncertainty in the model parameters, which is denoted as $\\theta$ of the graphical model of B-PEFT as shown in Figure 4(b). The schematic diagram of B-PEFT is shown in Figure 4(a). The model uncertainty can be leveraged to evaluate the reliability of fine-grained uncertainty output by the evidential model. ", "page_idx": 5}, {"type": "image", "img_path": "loQCk0qruU/tmp/8b8969985e391c4ebcc079ee8db6788119dacbf6ce1299b91a55e98e2427453a.jpg", "img_caption": ["Figure 4: (a) Schematic diagram and (b) Graphical model of the B-PEFT model "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "The effectiveness of the ensembles has been empirically demonstrated across multiple datasets/settings [30] with theoretical guarantees [2]. However, standard deep ensembling leads to limited diversity among the ensemble components as it only considers the random initialization of components. We propose a novel diversity-inducing ensembling scheme for the evidential models. Similar to the deep ensemble [36], we also consider randomly initialized evidential models. We additionally train each ensemble component with different strengths for incorrect evidence regularization along with evidential loss objective. The overall objective for each ensemble component is identical to (2). ", "page_idx": 6}, {"type": "text", "text": "However, each ensemble component is trained with different incorrect evidence (or belief) regularization strengths (i.e., different components place different priorities for the minimization of incorrect evidence over the maximization of correct evidence) which leads to diversity among the components. Since each component\u2019s priority for minimizing the incorrect evidence is different, the components focus on different attributes/features in the data that help the model avoid overfitting to an identical set of discriminative features. As a result, the proposed evidential ensembling scheme implicitly pushes the ensemble components away from each other, making it equivalent to the repulsive force in the Stein Variational Gradient Descent (SVGD) [12, 13]. ", "page_idx": 6}, {"type": "text", "text": "Lemma 4. For given incorrect evidence regularization $\\mathcal{L}_{r e g}^{\\,i n c},$ and $E$ ensemble components with regularization strengths $\\lambda_{p},p\\in[1,P],$ , the ensemble components in the evidence space are implicitly pushed away from each other by a force $\\lambda_{p}\\nabla{\\mathcal{L}}_{r e g}^{i n c}$ that acts identical to the repulsive force in Stein Variational Gradient Descent (SVGD) based ensembles. ", "page_idx": 6}, {"type": "text", "text": "Remark. The detailed proof is given in the Appendix. We present an intuitive visualization of the update of the evidential ensemble model for different strengths $(\\lambda_{1}<\\lambda_{2}<\\ldots<\\lambda_{P})$ of incorrect evidence regularization for different seeds in Figure 5. Each ensemble component aims to maximize the likelihood (direction $\\vec{A}$ ) and minimize incorrect ", "page_idx": 6}, {"type": "image", "img_path": "loQCk0qruU/tmp/76807d28e036ff5c6652d972f9b5319719e9fa85004ee100c679a47e1b46c0cf.jpg", "img_caption": ["Figure 5: Illustration of ensemble diversity achieved through incorrect belief regularization with different strength "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "loQCk0qruU/tmp/3ff19191c43a74bff7104d9b2d082423e16a131af072ef8f1c54cadd062db769.jpg", "table_caption": ["Table 1: Prediction accuracy and ECE performance on few-shot adaptation "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "loQCk0qruU/tmp/925bd7a3b22c542f98453297fe1459b1f327aae8b4328fa9320d5ded2ad5296e.jpg", "table_caption": ["(d) B-PEFT Model (Ours) "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "evidence (direction $\\vec{B}$ ). The strengths of incorrect evidence regularization (direction $\\scriptstyle{\\overrightarrow{B}}$ ) are different for each ensemble component that acts as an implicit repulsive force among the ensemble components, ensuring that they are diverse from each other. Different from the SVGD-based ensemble, in our proposed model, the particles do not need to explicitly communicate with each other making our proposed approach computationally efficient, scalable, and generalizable. ", "page_idx": 7}, {"type": "text", "text": "4 Experiments and Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Experiment setup, datasets, and baselines. We consider $K$ -shot adaptation (i.e., the dataset has $K$ examples per class in the training set) with Cifar10 [1], Cifar100 [1], Food101 [8], and Flowers102 [45] datasets. For instance, the 2-shot Cifar100 dataset has 2 examples per class leading to a total of 200 labeled training samples. For all datasets and experiments, the training set is a few-shot dataset, and the evaluation is done on the standard test set available with benchmark datasets. Details of the few-shot training datasets along with additional experiment details are presented in the Appendix E. We consider large pre-trained vision transformer with ViT backbone [15] and consider Visual Prompt Tuning (VPT) [32], along with bias fine-tuning [9] and adapter fine-tuning [72] as the PEFT techniques (We use VPT as the representative PEFT where not specified due to its superior performance). We consider accuracy-preserving post-hoc calibration techniques including Temperature Scaling (TS) [26], Parameterized Temperature Scaling (PTS) [61], and Isotonic Regression (IR-MC) [6] as the baseline calibration techniques. ", "page_idx": 7}, {"type": "text", "text": "Prediction and calibration performance. We first consider standard cross-entropy (CE)-based PEFT of the supervised pre-trained ViT model on few-shot datasets. We present the accuracy and calibration results of VPT in Table 1 (a). We observe that the straightforward adaption of the models leads to accurate but under-confident models as indicated by a high ECE. The evidential models as shown in Table 1 (b) have comparable or better generalization performance across the datasets/settings. However, these models are also severely under-confident similar to CE-based models indicated by high ECE and accuracy-confidence trends (see Figure 11a, 11b in the Appendix). The overall performance of the calibrated evidential model using base-rate adjustment is presented in Table 1 (c). As can be seen, the accuracy remains the same as the adjusted base rate expands the gap between evidence of the class and preserves the relative order in the predicted class evidence. It effectively tackles the under-confidence issue, which leads to a significant improvement in the overall ECE performance across the datasets and settings (also see Figure 11c in the Appendix). Table 1 (d) shows the results of the proposed B-PEFT model that introduces a diversity-enforcing ensemble of calibrated evidential models trained with different strengths of incorrect evidence regularization. performance. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Such a learning signal helps the model avoid overftiting (addressing the potential overconfidence issue) and leads to further improvement in generalization and the calibration. ", "page_idx": 8}, {"type": "text", "text": "We further carry out experiments with additional PEFT techniques of bias and adapter fine-tuning and on the $K$ -shot Cifar100 dataset in Table 2. We observe that these PEFT techniques lead to a lower generalization performance (as indicated by a lower test accuracy) compared to the VPT-based technique. Nevertheless, the same underconfidence issue remains as shown in Table 2 (a) for standard cross-entropy trained model performance, and in Table 2 (b) for their evidential extensions. We then carry out experiments using base-rate adjusted evidential model and our proposed B-PEFT model. The results are shown in Table 2 (c) and Table 2 (d). Base-rate adjusted evidential model and B-PEFT are equally effective with these PEFT techniques, and they address the underconfidence issue, which leads to improvement in both generalization and calibration. ", "page_idx": 8}, {"type": "table", "img_path": "loQCk0qruU/tmp/3fe563ca2d64f9dcb29a32f9a81d592433fa1f108b508c76c17050df0800aa5a.jpg", "table_caption": ["Table 2: Adapter and bias fine tuning results "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Comparison with existing calibration techniques. We compare our proposed evidential calibra tion technique with existing calibration techniques using a representative Cifar100 dataset on different few-shot classification settings. We consider Isotonic Regression (IRMC)[6], Temperature Scaling (TS) [26], and Parameterized Temperature Scaling (PTS) [61] as baselines. Overall results are presented in Table 3. The baseline model and its evidential extension are poorly calibrated. All calibration ", "page_idx": 8}, {"type": "table", "img_path": "loQCk0qruU/tmp/47bb6e77786abdcb24990075261096f9d3e8cd90f104aa385eff629c8a0814d8.jpg", "table_caption": ["Table 3: ECE performance comparison "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "techniques transform the logits to address the under-confidence issue of the model. Our proposed Base-rate adjusted Evidential model (BR-Evid) addresses the under-confidence issue in the evidential model leading to significant improvement in the ECE. B-PEFT model further improves on the BREvid model as it effectively addresses both the under-confidence issue (via the base rate adjustment) and overconfidence issue (via the Bayesian ensembling) that leads to superior calibration results as indicated by the lowest ECE in all few-shot settings. ", "page_idx": 8}, {"type": "text", "text": "Uncertainty quantification results. We investigate the uncertainty quantification performance of our proposed calibrated evidential ensemble model. The developed model can reflect the model uncertainty (i.e., the model\u2019s confidence in its predictions) through the ensemble agreement/disagreement and the distributional uncertainty through the sharpness of evidential prior distribution (i.e., the vacuity). For the analysis, we use Cifar10 as in-distribution (ID) dataset for different shots, and Cifar100 as OOD dataset. As shown in Figure 6, we plot vacuity and variance distribution for 1 and 5 shots. A single model outputs the vacuity that can be used to detect OOD samples. We can see ID samples are in the lower vacuity region and OOD samples are in the higher vacuity region. As the number of shots increases, the region becomes more separate. However, for 1 shot, there are some OOD samples in the lower vacuity region as well. Since we can not trust vacuity alone for uncertainty, we utilize the variance of ensemble components to quantify model uncertainty. As we see in Figure 6(c), the variance of OOD samples mostly lies in the high variance region. As we increase the number of shots, the variance shifts towards the lower region. High variance indicates that model-predicted vacuity can not be totally trusted. This behavior is qualitatively shown in Figure ", "page_idx": 8}, {"type": "image", "img_path": "loQCk0qruU/tmp/2d9153996c38d8268463428b149df46a573e10f13478cecc818610b07e1f00ac.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 6: (a-b): Vacuity distribution of a single model and (c-d): variance distribution of ensemble models for 1/5 shots cifar10 as In-Distribution and Cifar100 as Out-of-Distribution dataset ", "page_idx": 9}, {"type": "image", "img_path": "loQCk0qruU/tmp/87fbc015478fcf41638963ba7d437db3c4c0ed2bf055649dfb7b851d642da0ba.jpg", "img_caption": ["Figure 7: Qualitative analysis of OOD samples for 1-shot adaptation of Cifar10 as ID dataset and Cifar100 as OOD dataset "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7. We observe that when only a single component outputs a low vacuity for the OOD samples in Figure 7 (a-b), the variance of the ensemble is high, implying a high model uncertainty. When all the components output a high vacuity to the OOD sample in Figure 7 (c), the variance is also low, implying a low model uncertainty. ", "page_idx": 9}, {"type": "text", "text": "Ablation study. We carry out ablation with 1-shot Cifar100 dataset to study the impact of the base rate transformation order $m$ (Section 3.2) for different strengths of incorrect evidence regularization on the calibration performance. As we increase $m$ , the probability gap between classes improves, leading to more confident predictions. However, the model starts to become overconfident for large $m$ values (see Figure 8).Moreover, with an increase in incorrect evidence regularization strength, the optimal $m$ value decreases to a smaller value (e.g., optimal $m$ $=2.0$ for $\\lambda=0.1$ , and optimal $m=1.5$ for $\\lambda=10$ ). Choosing a proper $m$ leads to the best-calibrated evidential model. We present additional results studying the impact of $m$ in Appendix F.1. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "image", "img_path": "loQCk0qruU/tmp/e00f906b450b0b3d303be30afbe98d9be37cef383bc5b0fda48520c085f4f0fb.jpg", "img_caption": ["Figure 8: Impact of $m$ "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Limited by space, we provide results comparing meta-learning methods on standard few-shot tasks in Appendix F.4, discuss impact of various components (number of classes, data size, and unfrozen parameters) in Appendix F.5, and include additional experiments and comparisons, including OOD settings, in Appendix F.6. Moreover, we carry out additional ablation experiments to study the impact of incorrect evidence regularization strength (Appendix F.2), and the impact of ensemble components (Appendix F.3). We further carry out experiments and discuss applying PEFT to foundation models pre-trained in a self-supervised fashion (Appendix G). We discuss the societal impact and limitations of the work in Appendices H and I, respectively. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we focus on transformer-based large vision foundation models and investigate different parameter-efficient fine-tuning techniques for effective few-shot adaptation. We observe that the existing models are severely under-confident, especially in challenging datasets and settings. Moreover, existing models lack fine-grained uncertainty quantification capabilities. We extend the models to uncertainty-aware evidential models, and resort to the evidential framework to develop a novel Bayesian parameter efficient fine-tuning (B-PEFT) framework that integrates evidence-based base rate adjustment to addresses the under-confidence and a diversity inducing evidential ensemble technique to further improve the reliability in model prediction and uncertainty quantification. The B-PEFT framework possesses theoretically sound properties to ensure its superior generalization capability and robust calibration behavior. We carry out intensive experiments across different benchmark datasets and diverse few-shot settings that demonstrate the outstanding performance of B-PEFT. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research was supported in part by an NSF IIS award IIS-1814450. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agency. We would like to thank the anonymous reviewers for their constructive comments. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Krizhevsky Alex. Learning multiple layers of features from tiny images. https://www. cs. toronto. edu/kriz/learning-features-2009-TR. pdf, 2009.   \n[2] Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. arXiv preprint arXiv:2012.09816, 2020.   \n[3] Alexander Amini, Wilko Schwarting, Ava Soleimany, and Daniela Rus. Deep evidential regression. Advances in Neural Information Processing Systems, 33:14927\u201314937, 2020.   \n[4] Arsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, and Dmitry Vetrov. Pitfalls of in-domain uncertainty estimation and ensembling in deep learning. arXiv preprint arXiv:2002.06470, 2020.   \n[5] Wentao Bao, Qi Yu, and Yu Kong. Evidential deep learning for open set action recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13349\u2013 13358, 2021.   \n[6] Eugene Berta, Francis Bach, and Michael Jordan. Classifier calibration with roc-regularized isotonic regression. In International Conference on Artificial Intelligence and Statistics, pages 1972\u20131980. PMLR, 2024.   \n[7] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In International conference on machine learning, pages 1613\u20131622. PMLR, 2015.   \n[8] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 \u2013 mining discriminative components with random forests. In European Conference on Computer Vision, 2014.   \n[9] Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. Tinytl: Reduce memory, not parameters for efficient on-device learning. Advances in Neural Information Processing Systems, 33:11285\u2013 11297, 2020.   \n[10] Bertrand Charpentier, Oliver Borchert, Daniel Z\u00fcgner, Simon Geisler, and Stephan G\u00fcnnemann. Natural posterior network: Deep bayesian predictive uncertainty for exponential family distributions. In International Conference on Learning Representations, 2022.   \n[11] Bertrand Charpentier, Daniel Z\u00fcgner, and Stephan G\u00fcnnemann. Posterior network: Uncertainty estimation without ood samples via density-based pseudo-counts. Advances in Neural Information Processing Systems, 33:1356\u20131367, 2020.   \n[12] Francesco D\u2019Angelo and Vincent Fortuin. Repulsive deep ensembles are bayesian. Advances in Neural Information Processing Systems, 34:3451\u20133465, 2021.   \n[13] Francesco D\u2019Angelo, Vincent Fortuin, and Florian Wenzel. On stein variational neural network ensembles. arXiv preprint arXiv:2106.10760, 2021.   \n[14] Erik Daxberger, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen, Matthias Bauer, and Philipp Hennig. Laplace redux-effortless bayesian deep learning. Advances in Neural Information Processing Systems, 34:20089\u201320103, 2021.   \n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[16] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1126\u20131135. JMLR. org, 2017.   \n[17] Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In Advances in Neural Information Processing Systems, pages 9516\u20139527, 2018.   \n[18] Gianni Franchi, Olivier Laurent, Maxence Legu\u00e9ry, Andrei Bursuc, Andrea Pilzer, and Angela Yao. Make me a bnn: A simple strategy for estimating bayesian uncertainty from pre-trained models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12194\u201312204, 2024.   \n[19] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pages 1050\u20131059. PMLR, 2016.   \n[20] Mudasir A Ganaie, Minghui Hu, Ashwani Kumar Malik, Muhammad Tanveer, and Ponnuthurai N Suganthan. Ensemble deep learning: A review. Engineering Applications of Artificial Intelligence, 115:105151, 2022.   \n[21] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. International Journal of Computer Vision, 132(2):581\u2013595, 2024.   \n[22] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4367\u20134375, 2018.   \n[23] Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin, and Richard E Turner. Meta-learning probabilistic inference for prediction. arXiv preprint arXiv:1805.09921, 2018.   \n[24] Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting gradient-based meta-learning as hierarchical bayes. In International Conference on Learning Representations, 2018.   \n[25] Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. Ppt: Pre-trained prompt tuning for few-shot learning. arXiv preprint arXiv:2109.04332, 2021.   \n[26] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 1321\u20131330. PMLR, 2017.   \n[27] Cheng Han, Qifan Wang, Yiming Cui, Zhiwen Cao, Wenguan Wang, Siyuan Qi, and Dongfang Liu. $\\mathbf{E}^{\\star}$ 2vpt: An effective and efficient approach for visual prompt tuning. arXiv preprint arXiv:2307.13770, 2023.   \n[28] Cheng Han, Qifan Wang, Yiming Cui, Wenguan Wang, Lifu Huang, Siyuan Qi, and Dongfang Liu. Facing the elephant in the room: Visual prompt tuning or full finetuning? arXiv preprint arXiv:2401.12902, 2024.   \n[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[30] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger. Snapshot ensembles: Train 1, get m for free. arXiv preprint arXiv:1704.00109, 2017.   \n[31] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700\u20134708, 2017.   \n[32] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pages 709\u2013727. Springer, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[33] Audun J\u00f8sang. Subjective logic, volume 3. Springer, 2016. ", "page_idx": 12}, {"type": "text", "text": "[34] Audun Josang, Jin-Hee Cho, and Feng Chen. Uncertainty characteristics of subjective opinions. In 2018 21st International Conference on Information Fusion (FUSION), pages 1998\u20132005. IEEE, 2018.   \n[35] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 128(7):1956\u20131981, 2020.   \n[36] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30, 2017.   \n[37] Hae Beom Lee, Hayeon Lee, Donghyun Na, Saehoon Kim, Minseop Park, Eunho Yang, and Sung Ju Hwang. Learning to balance: Bayesian meta-learning for imbalanced and out-ofdistribution tasks. In Eighth International Conference on Learning Representations, ICLR 2020. ICLR, 2020.   \n[38] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.   \n[39] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980\u20132988, 2017.   \n[40] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.   \n[41] Andrey Malinin and Mark Gales. Predictive uncertainty estimation via prior networks. Advances in neural information processing systems, 31, 2018.   \n[42] Aryan Mobiny, Pengyu Yuan, Supratik K Moulik, Naveen Garg, Carol C Wu, and Hien Van Nguyen. Dropconnect is effective in modeling uncertainty of bayesian deep networks. Scientific reports, 11(1):5458, 2021.   \n[43] Azadeh Sadat Mozafari, Hugo Siqueira Gomes, Wilson Le\u00e3o, Steeven Janny, and Christian Gagn\u00e9. Attended temperature scaling: a practical approach for calibrating deep neural networks. arXiv preprint arXiv:1810.11586, 2018.   \n[44] Rafael M\u00fcller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? Advances in neural information processing systems, 32, 2019.   \n[45] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian conference on computer vision, graphics & image processing, pages 722\u2013729. IEEE, 2008.   \n[46] Deep Shankar Pandey and Qi Yu. Multidimensional belief quantification for label-efficient meta-learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14391\u201314400, June 2022.   \n[47] Deep Shankar Pandey and Qi Yu. Evidential conditional neural processes. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 9389\u20139397, 2023.   \n[48] Deep Shankar Pandey and Qi Yu. Learn to accumulate evidence from all training samples: Theory and practice. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 26963\u201326989. PMLR, 23\u201329 Jul 2023.   \n[49] Tim Pearce, Felix Leibfried, and Alexandra Brintrup. Uncertainty in neural networks: Approximately bayesian ensembling. In International conference on artificial intelligence and statistics, pages 234\u2013244. PMLR, 2020.   \n[50] Gabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton. Regularizing neural networks by penalizing confident output distributions. arXiv preprint arXiv:1701.06548, 2017.   \n[51] Jonas Pfeiffer, Andreas R\u00fcckl\u00e9, Clifton Poth, Aishwarya Kamath, Ivan Vulic\u00b4, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych. Adapterhub: A framework for adapting transformers. arXiv preprint arXiv:2007.07779, 2020.   \n[52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[53] Rahul Rahaman et al. Uncertainty quantification and deep ensembles. Advances in Neural Information Processing Systems, 34:20063\u201320075, 2021.   \n[54] Sylvestre-Alvise Rebuff,i Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. Advances in neural information processing systems, 30, 2017.   \n[55] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. arXiv preprint arXiv:2104.10972, 2021.   \n[56] Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential deep learning to quantify classification uncertainty. Advances in neural information processing systems, 31, 2018.   \n[57] Weishi Shi, Xujiang Zhao, Feng Chen, and Qi Yu. Multifaceted uncertainty estimation for label-efficient deep learning. Advances in neural information processing systems, 33, 2020.   \n[58] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   \n[59] Zhao Song, Ke Yang, Naiyang Guan, Junjie Zhu, Peng Qiao, and Qingyong Hu. Vppt: Visual pre-trained prompt tuning framework for few-shot image classification. In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135, 2023.   \n[60] Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Michalak. On mixup training: Improved calibration and predictive uncertainty for deep neural networks. Advances in Neural Information Processing Systems, 32, 2019.   \n[61] Christian Tomani, Daniel Cremers, and Florian Buettner. Parameterized temperature scaling for boosting the expressive power in post-hoc uncertainty calibration. In In European Conference on Computer Vision (ECCV), 2022.   \n[62] Dennis Ulmer. A survey on evidential deep learning for single-pass uncertainty estimation. arXiv preprint arXiv:2110.03051, 2021.   \n[63] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[64] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. Advances in neural information processing systems, 29, 2016.   \n[65] Cheng Wang. Calibration in deep learning: A survey of the state-of-the-art. arXiv preprint arXiv:2308.01222, 2023.   \n[66] Xiao Wang, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-Yong Wei, Yaowei Wang, Yonghong Tian, and Wen Gao. Large-scale multi-modal pre-trained models: A comprehensive survey. Machine Intelligence Research, 20(4):447\u2013482, 2023.   \n[67] Jingyuan Wen, Yutian Luo, Nanyi Fei, Guoxing Yang, Zhiwu Lu, Hao Jiang, Jie Jiang, and Zhao Cao. Visual prompt tuning for few-shot text classification. In Proceedings of the 29th International Conference on Computational Linguistics, pages 5560\u20135570, 2022.   \n[68] Liqi Yan, Cheng Han, Zenglin Xu, Dongfang Liu, and Qifan Wang. Prompt learns prompt: exploring knowledge-aware generative prompt collaboration for video captioning. In Proceedings of international joint conference on artificial intelligence (IJCAI), pages 1622\u20131630, 2023.   \n[69] Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn. Bayesian model-agnostic meta-learning. In Advances in Neural Information Processing Systems, pages 7332\u20137342, 2018.   \n[70] Yu Yu, Chao-Han Huck Yang, Jari Kolehmainen, Prashanth G Shivakumar, Yile Gu, Sungho Ryu Roger Ren, Qi Luo, Aditya Gourav, I-Fan Chen, Yi-Chieh Liu, et al. Low-rank adaptation of large language model rescoring for parameter-efficient speech recognition. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 1\u20138. IEEE, 2023.   \n[71] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021.   \n[72] Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and Jitendra Malik. Side-tuning: a baseline for network adaptation via additive side networks. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part III 16, pages 698\u2013714. Springer, 2020.   \n[73] Jize Zhang, Bhavya Kailkhura, and T Yong-Jin Han. Mix-n-match: Ensemble and compositional methods for uncertainty calibration in deep learning. In International conference on machine learning, pages 11117\u201311128. PMLR, 2020.   \n[74] Linjun Zhang, Zhun Deng, Kenji Kawaguchi, and James Zou. When and how mixup improves calibration. In International Conference on Machine Learning, pages 26135\u201326160. PMLR, 2022.   \n[75] Xujiang Zhao, Feng Chen, Shu Hu, and Jin-Hee Cho. Uncertainty aware semi-supervised learning on graph data. Advances in Neural Information Processing Systems, 33:12827\u201312836, 2020.   \n[76] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16816\u201316825, 2022.   \n[77] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):2337\u20132348, 2022. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table of Contents ", "page_idx": 15}, {"type": "text", "text": "A Organization of the Appendix 17 ", "page_idx": 15}, {"type": "text", "text": "B Summary of the Symbols 17 ", "page_idx": 15}, {"type": "text", "text": "C Further Discussion on Uncertainty-Aware Deep Learning 17 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Existing Uncertainty Quantification Methods in Deep Learning . . 17   \nC.2 Evidential Deep Learning Models for Classification . . 18   \nC.3 Evidential models vs. Standard Bayesian Models . . 19 ", "page_idx": 15}, {"type": "text", "text": "D Proofs of Theoretical Results 19 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D.1 Proof of Lemma 2 19   \nD.2 Proof of Theorem 3 19   \nD.3 Connection with SVGD-based Bayesian Ensembling and Proof of Lemma 4 20 ", "page_idx": 15}, {"type": "text", "text": "E Dataset and Implementation Details 21 ", "page_idx": 15}, {"type": "text", "text": "F Additional Experiments 22 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "F.1 Impact of $m$ on Expected Calibration Error . . 222526262627 F.2 Impact of Incorrect Evidence Regularization Strength (\u03bb)   \nF.3 Impact of Ensemble Components . . .   \nF.4 Few Shot Learning Results . . .   \nF.5 Impact of Different Components   \nF.6 Additional Experiments and Comparison ", "page_idx": 15}, {"type": "text", "text": "G Calibration Behavior of Self-Supervised Model 28 ", "page_idx": 15}, {"type": "text", "text": "H Societal Impact 28 ", "page_idx": 15}, {"type": "text", "text": "I Limitations and Future Work 29 ", "page_idx": 15}, {"type": "text", "text": "A Organization of the Appendix ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u2022 In Section B, we present the table with summary of the key symbols used in this work.   \n\u2022 In Section C, we present related works in uncertainty-aware deep learning, describe evidential deep learning for classification in details, and discuss some key advantages of using evidential models to address the under-confidence issue over standard Bayesian models.   \n\u2022 In Section D, we present proof of all our theoretical claims.   \n\u2022 In Section E, we present the details of hyperparameters and additional experiment details.   \n\u2022 In Section F, we present additional experiment results including the impact of m for calibration performance, comparison results with meta-learning methods, the impact of the number of classes, data size, and unfrozen parameters, comparison in OOD settings, the impact of incorrect evidence regularization strength, the results of additional PEFT methods, and the impact of ensemble components.   \n\u2022 In Section G, we discuss the calibration and accuracy behavior for PEFT of large foundation models pre-trained in a self-supervised fashion.   \n\u2022 In Section H, we discuss societal impact of our work.   \n\u2022 In Section I, we discuss the limitations of our work and present potential future direction. ", "page_idx": 16}, {"type": "text", "text": "The source code for the experiments carried out in this work is attached in the supplementary materials and is available at the link: https://github.com/ritmininglab/B-PEFT ", "page_idx": 16}, {"type": "text", "text": "B Summary of the Symbols ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "loQCk0qruU/tmp/b2cd477466f9afdf89c04b65c108142af1c051f70d49cbec9933292e78106447.jpg", "table_caption": ["Table 4: Summary of the symbols and their definitions "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Further Discussion on Uncertainty-Aware Deep Learning ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Existing Uncertainty Quantification Methods in Deep Learning ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Accurate quantification of predictive uncertainty is essential for the development of trustworthy Deep Learning (DL) models. To this end, DL models have been augmented to become uncertaintyaware using a variety of approaches such as ensemble-based approaches [36, 49], bayesian neural networks based approaches [42, 19, 7], and deterministic neural network based approaches [56, 11, 3]. Deep ensemble techniques [36, 49] construct an ensemble of neural networks, and the agreement/disagreement across the ensemble components is used to quantify different uncertainties. Alternatively, Bayesian neural networks [19][7][42] have been developed that consider a Bayesian formalism (e.g., bayes-by-backprop [7], dropout during test [19]) to quantify different uncertainties. ", "page_idx": 16}, {"type": "text", "text": "ABNN [18] introduces Bayesian normalization layers after training of deep learning models, and requires additional training of these layers in a post-hoc manner. Deterministic neural network-based approaches [57, 41, 10] extend the existing neural network to become uncertainty-aware and enable the networks to quantify fine-grained uncertainties with a single forward pass through the network. Evidential deep learning models [56, 5, 75, 10, 10, 62], an instance of deterministic approaches, introduce a conjugate higher-order evidential prior for the likelihood distribution to enable the model to express the fine-grained uncertainties in both classification[56, 11] and regression problems [3, 47]. Towards classification, evidential models [56, 5, 75] introduce higher-order evidential Dirichlet prior to the multinomial likelihood that enables the deterministic neural network model to capture different uncertainty characteristics. In what follows, we first provide some additional details on using evidential learning model to perform classification. We then highlight some important advantage of using evidential models over standard Bayesian models in uncertainty quantification. ", "page_idx": 17}, {"type": "text", "text": "C.2 Evidential Deep Learning Models for Classification ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Evidential Deep Learning models, based on Subjective Logic theory [33], aim to train the model such that for any new input sample, the model can make predictions, as well as output fine-grained uncertainty information (via vacuity [56] and dissonance [34]). Towards capturing fine-grained uncertainty for classification problems, EDL models assume that the label for each sample is obtained from a generative process with a Dirichlet prior and a multinomial likelihood. The parameters for the Dirichlet prior express the vacuity and belief masses for uncertainty estimation. The conjugacy between the Dirichlet prior and the multinomial likelihood is explored, and different evidential losses are introduced for model training and inference [48]. In this work, we consider Type-II Maximum Likelihood-based evidential loss $\\mathcal{L}^{\\mathrm{Log}}(x,y)$ [56] with incorrect evidence regularization $\\mathcal{L}_{\\mathrm{reg}}^{\\mathrm{inc}}(x,y)$ given by [56] ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{evid}}(x,y)=\\mathcal{L}^{\\mathrm{Log}}(\\pmb{x},\\pmb{y})+\\lambda\\times\\mathcal{L}_{\\mathrm{reg}}^{\\mathrm{inc}}(\\pmb{x},\\pmb{y})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We replace the softmax layer in the head of the VPT model with exp activation function. To avoid zero evidence regions, we also include the correct evidence regularization $\\mathcal{L}_{\\mathsf{c o r}}(\\pmb{x},\\pmb{y})=-\\lambda_{\\mathsf{c o r}}\\log(\\alpha_{g t}\\!-\\!1)$ ( $\\lambda_{\\mathsf{c o r}}$ is the magnitude of the model\u2019s vacuity) in model training objective. The evidential model outputs evidence vector $\\boldsymbol{e}=\\left(e_{1},e_{2},...e_{N}\\right)$ for a given input $\\textbf{\\em x}$ and corresponding ground truth label of $\\textit{\\textbf{y}}$ . Based on the evidence, Dirichlet parameters are obtained as $\\alpha_{i}=e_{i}+1$ . The Type-II Maximum likelihood-based evidential loss is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\mathrm{Log}}(x,y)=-\\ln\\int\\mathrm{Mult}(y|p)\\mathrm{Dir}(p|\\alpha)\\mathrm{d}p=\\log S-\\sum_{k=1}^{K}y_{k}\\log\\alpha_{k}\\quad S=\\sum_{k=1}^{K}\\alpha_{k}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The incorrect evidence regularization guides the model to minimize the evidence for all classes other than the ground truth class and can take one of the following forms ", "page_idx": 17}, {"type": "text", "text": "1. KL-based incorrect evidence regularization term as in EDL [56] ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathcal{L}_{\\mathtt{r e g}}^{\\mathtt{E D L}}(x,y)=\\mathrm{KL}\\big(\\mathtt{D i r}(p|\\tilde{\\alpha})||\\mathtt{D i r}(p|\\mathbf{1})\\big)}\\\\ {\\qquad\\qquad=\\log\\Big(\\displaystyle\\frac{\\Gamma\\sum_{k=1}^{K}\\tilde{\\alpha}_{k}}{\\Gamma(K)\\prod_{k=1}^{K}\\Gamma\\tilde{\\alpha}_{k}}\\Big)+\\displaystyle\\sum_{k=1}^{K}(\\tilde{\\alpha}_{k}-1)\\bigg[\\psi(\\tilde{\\alpha}_{k})-\\psi\\Big(\\displaystyle\\sum_{j=1}^{K}\\tilde{\\alpha}_{j}\\Big)\\bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Where $\\tilde{\\alpha}\\,=\\,y+({\\bf1}-y)\\odot\\alpha\\,=\\,(\\tilde{\\alpha}_{1},\\tilde{\\alpha}_{2},...\\tilde{\\alpha}_{N})$ parameterize a dirichlet distribution, $\\tilde{\\alpha}_{i=g t}\\;=\\;1,\\tilde{\\alpha}_{i}\\;=\\;\\alpha_{i}\\forall i\\;\\neq\\;g t$ , and $\\odot$ represents element-wise product. Here, the KL regularization term encourages the Dirichlet distribution based on the incorrect evidence i.e., $\\operatorname{Dir}(p|\\tilde{\\alpha})$ to be flat which is possible when there is no incorrect evidence. ", "page_idx": 17}, {"type": "text", "text": "2. Incorrect evidence sum based regularization as in ADL [57] ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{reg}}^{\\mathrm{ADL}}(\\pmb{x},\\pmb{y})=\\sum_{k=1}^{K}\\left(\\mathbf{e}\\odot\\left(\\mathbf{1}-\\mathbf{y}\\right)\\right)_{k}=\\sum_{k=1}^{K}e_{k}\\times\\left(1-y_{k}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "3. Incorrect belief-sum based regularization as in Units-ML [46] ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{reg}}^{\\mathrm{Units}}(x,y)=\\sum_{k=1}^{K}\\left(\\frac{\\mathbf{e}}{S}\\odot\\left(\\mathbf{1}-\\mathbf{y}\\right)\\right)_{k}=\\sum_{k=1}^{K}\\frac{e_{k}}{S}\\times\\left(1-y_{k}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "All three regularizations guide the model to minimize the incorrect evidence(ideally close to zero). In our experiments, we consider KL-based incorrect evidence regularization. ", "page_idx": 18}, {"type": "text", "text": "C.3 Evidential models vs. Standard Bayesian Models ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As compared with the Bayesian-inspired models, evidential learning offers two key properties that allow us to formulate a principled solution to address the unique under-confident behavior of the PEFT methods. First, thanks to its evidence-based fine-grained uncertainty decomposition capability, we can separate two distinct sources of second-order uncertainty, including vacuity and dissonance. Different from the commonly used first-order uncertainty (e.g., entropy), these two second-order uncertainty serve as a key tool to understand why PEFT methods are both accurate (with a low dissonance) while being under-confident (with a high vacuity). This key insight suggests that these methods systematically under-estimate the contribution from the prior knowledge to the downstream task. While the classical Bayes\u2019 theorem offers a principal idea to address the issue, which is to strengthen the prior belief, there is a lack of practical way to achieve this. As the second key property, evidential learning allows us to leverage the base rate, which is rooted in the subjective logic theory as an effective vehicle to adjust the prior belief gained through pre-training. To this end, we propose a transformation function in Eq. (6) to adjust the base rate that leads to the increase of the model confidence while maintaining the predictive accuracy of the model as guaranteed by our theoretical results in Lemma 2 and Theorem 3. Furthermore, we develop belief-based diversity for ensemble of evidential models leading to the the B-PEFT model. In theory, evidential deep learning model could be augmented with the Bayesian normalization layers [18] or Bayesian neural networks [7] as an alternative to belief-based diversity of B-PEFT. We leave exploration of different techniques for diversity for Bayesian evidential model as a potential future work. ", "page_idx": 18}, {"type": "text", "text": "D Proofs of Theoretical Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we provide the proofs of the major theoretical results presented in the main paper. ", "page_idx": 18}, {"type": "text", "text": "D.1 Proof of Lemma 2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. Consider an input sample $\\textbf{\\em x}$ for which the model outputs the evidence $(e_{1},e_{2},...,e_{N})^{\\top}$ . Let $e_{\\mathtt{m a x}}=\\operatorname*{max}(e_{1},e_{2},...,e_{N})$ , and $e_{\\mathtt{m i n}}=\\operatorname*{min}(e_{1},e_{2},...,e_{N})$ . Here, $e_{\\mathtt{m a x}}\\geq e_{i}\\geq e_{\\mathtt{m i n}}\\forall i\\in[1,N]$ . For the evidential model with fixed base rate of $\\begin{array}{r}{a_{i}=\\frac{1}{N}\\forall i\\in[1,N]}\\end{array}$ , the model\u2019s predicted class is given by $c_{\\mathtt{p r e d}}=\\arg\\operatorname*{max}(e_{1}+a_{1}\\times W,e_{2}+a_{2}\\times W,...,e_{N}+a_{N}\\times W)=\\arg\\operatorname*{max}(e_{1}+1,e_{2}+a_{1}\\times W,e_{3}+a_{N}),$ $1,...,e_{N}+1)=\\mathtt{I n d e x}(e_{\\mathtt{m a x}})$ . For the calibrated model with learnable $\\pmb{\\chi}=(\\chi_{1},\\chi_{2},...,\\chi_{N})^{\\top}$ , the model\u2019s predicted class is given by $\\mathbf{\\Pi}_{\\mathrm{>red}}^{\\mathrm{new}}=\\arg\\operatorname*{max}(\\alpha_{1},\\alpha_{2},...,\\alpha_{N})=\\arg\\operatorname*{max}(e_{1}+$ \u03c71 \u00d7 W, e2 + \u03c72 \u00d7 $\\begin{array}{r}{\\dot{W_{,\\,\\cdots,\\,e_{N}}}+\\chi_{N}\\times\\dot{W_{}}\\dot{)}=\\arg\\underset{\\mathrm{max}}{\\dot{\\mathbf{r}_{\\scriptscriptstyle1}}}+\\dot{N}(\\frac{e_{1}}{e_{\\mathrm{min}}})-N,e_{2}+N(\\frac{e_{2}}{e_{\\mathrm{min}}})-\\dot{N},...,\\dot{e}_{N}+\\dot{N}(\\frac{e_{N}}{e_{\\mathrm{sin}}})-N)}\\end{array}$ . Since $e_{\\mathtt{m a x}}\\geq e_{i}\\geq e_{\\mathtt{m i n}}\\forall i\\in[1,N]$ , $\\alpha_{\\mathrm{max}}\\geq\\alpha_{i}\\geq\\alpha_{\\mathrm{min}}\\forall i\\in[1,N]$ , and $c_{\\mathtt{p r e d}}^{\\mathtt{n e w}}=c_{\\mathtt{p r e d}}$ \u53e3 ", "page_idx": 18}, {"type": "text", "text": "D.2 Proof of Theorem 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. Consider an input sample $\\mathbf{x}$ for which the model outputs the evidence $(e_{1},e_{2},...,e_{N})^{\\top}$ . Let $e_{\\mathtt{m a x}}=\\operatorname*{max}(e_{1},e_{2},...e_{N})$ , $e_{\\mathtt{m i n}}=\\operatorname*{min}(e_{1},e_{2},...e_{N})$ , and $e_{\\mathtt{m a x}}\\,\\geq\\,e_{2\\mathtt{n d}}\\,\\geq,...,\\geq\\,e_{\\mathtt{m i n}}$ . For the evidential model with a fixed base rate of $\\begin{array}{r}{a_{i}=\\frac{1}{N}\\forall i\\in[1,N]}\\end{array}$ , the difference between the Dirichlet parameters for class with maximum evidence and class with the second maximum evidence is given by $\\alpha_{\\sf m a x}-\\alpha_{\\sf m d}=e_{\\sf m a x}+a_{\\sf m a x}W-e_{\\sf m d}-a_{\\sf m d}W=e_{\\sf m a x}-e_{\\sf m d}$ as $\\begin{array}{r}{a_{i}=\\frac{1}{N}\\forall i\\in[1,N]}\\end{array}$ . ", "page_idx": 18}, {"type": "text", "text": "For the calibrated model with learnable $\\pmb{\\chi}=(\\chi_{1},\\chi_{2},...,\\chi_{N})$ , the difference between the Dirichlet parameters for class with maximum evidence and class with the second maximum evidence is given by $\\alpha_{\\mathrm{nax}}-\\alpha_{\\mathrm{2nd}}=e_{\\mathrm{nax}}+\\chi_{\\mathrm{nax}}W-e_{\\mathrm{2nd}}-\\chi_{\\mathrm{2nd}}W=(e_{\\mathrm{nax}}-e_{\\mathrm{2nd}})+(\\chi_{\\mathrm{nax}}-\\chi_{\\mathrm{2nd}})W$ . Now, $\\begin{array}{r l}&{\\chi_{\\mathrm{max}}=\\left(\\frac{e_{\\mathrm{max}}-e_{\\mathrm{min}}}{e_{\\mathrm{min}}}\\right)^{m}=\\left(\\frac{e_{\\mathrm{max}}}{e_{\\mathrm{min}}}-1\\right)^{m}\\quad\\&\\quad\\chi_{\\mathrm{2nd}}=\\left(\\frac{e_{\\mathrm{2nd}}-e_{\\mathrm{min}}}{e_{\\mathrm{min}}}\\right)^{m}=\\left(\\frac{e_{\\mathrm{2nd}}}{e_{\\mathrm{min}}}-1\\right)^{m}}\\\\ &{\\quad\\mathrm{Or,~}\\left(\\chi_{\\mathrm{max}}-\\chi_{\\mathrm{2nd}}\\right)=\\left(\\frac{e_{\\mathrm{max}}}{e_{\\mathrm{min}}}-1\\right)^{m}-\\left(\\frac{e_{\\mathrm{2nd}}}{e_{\\mathrm{min}}}-1\\right)^{m}}\\end{array}$ (12) (13) ", "page_idx": 18}, {"type": "text", "text": "Since $\\begin{array}{r}{\\frac{e_{i}}{e_{\\mathfrak{m i n}}}\\ \\geq\\ 1\\forall i\\ \\in\\ [1,N]}\\end{array}$ , and $e_{\\mathtt{m a x}}\\;\\geq\\;e_{2\\mathtt{n d}}\\;\\geq,...,\\geq\\;e_{\\mathtt{m i n}}$ , $\\left(\\chi_{\\tt m a x}\\,-\\,\\chi_{\\tt2n d}\\right)\\,\\geq\\,0\\forall m\\,>\\,0$ . For $e_{\\mathtt{m a x}}>e_{2\\mathtt{n d}}$ , $\\&m>0$ , $\\chi_{\\mathrm{max}}-\\chi_{2\\mathrm{nd}}>0$ . Thus, with the proposed learnable base rate, the gap between the two largest Dirichlet parameters is maintained whenever $m=0$ and/or $e_{\\mathtt{m a x}}=e_{2\\mathtt{n d}}$ . Moreover, whenever $m\\geq1$ and $e_{\\mathtt{m a x}}>e_{2\\mathtt{n d}}$ , the Dirichlet parameter gap between the two classes is increased by a factor of $\\begin{array}{r}{\\left(\\frac{e_{\\mathrm{max}}}{e_{\\mathrm{min}}}-1\\right)^{m}-\\left(\\frac{e_{2\\mathrm{nd}}}{e_{\\mathrm{min}}}-1\\right)^{m}}\\end{array}$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "D.3 Connection with SVGD-based Bayesian Ensembling and Proof of Lemma 4 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We first carry out an analysis of the update in Stein Variational Gradient Descent (SVGD) based ensembling [12, 13] that reveals the repulsive force acting among the ensemble components that pushes the particles away and introduces diversity. We then consider ensemble components with different strengths of incorrect evidence regularization $\\mathcal{L}_{\\mathrm{reg}}^{\\mathrm{inc}}$ and analyze the update to the evidential model in the evidence space that reveals a repulsive diversity-enforcing force acting identical to the SVGD based ensemble. ", "page_idx": 19}, {"type": "text", "text": "SVGD update involves randomly initializing the particles and iteratively updating the particles to match the target distribution, which is summarized below. ", "page_idx": 19}, {"type": "text", "text": "Algorithm 1 SVGD Update ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Input: $\\{x_{i}^{0}\\}_{i=1}^{N}$ : A set of initial parameters, and target distribution density function $p(x)$ For $L$ iterations, iteratively update the particles as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bullet\\ x_{i}^{l+1}=x_{i}^{l}+\\epsilon_{l}\\hat{\\phi}^{\\ast}(x_{i}^{l}),\\quad\\mathrm{where}\\quad\\hat{\\phi}^{\\ast}(x)=\\frac{1}{E}\\sum_{e=1}^{E}k(x_{e}^{l},x)\\nabla_{x_{e}^{l}}\\log p(x_{e}^{l})+\\nabla_{x_{e}^{l}}k(x_{e}^{l},x)\\nabla_{x_{e}^{l+1}}\\log p(x_{e}^{l}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here, $\\epsilon_{l}$ is the step size at iteration $l$ , and $k(\\cdot,\\cdot)$ is the kernel function that measures similarity. Output: $\\{x_{i}^{0}\\}_{i=1}^{N}$ : A set of initial parameters, and target distribution density function $p(x)$ ", "page_idx": 19}, {"type": "text", "text": "For given incorrect evidence regularization $\\mathcal{L}_{\\mathrm{reg}}^{\\mathrm{inc}}$ , and $\\mathbf{P}$ ensemble components with regularization strengths $\\lambda_{p},p\\in[1,P]$ , the ensemble components in the evidence space are implicitly pushed away from each other by a force $\\lambda_{p}\\nabla{\\mathcal{L}}_{\\mathtt{r e g}}^{\\mathtt{i n c}}$ that acts identical to the repulsive force in Stein Variational Gradient Descent (SVGD) based ensembles. ", "page_idx": 19}, {"type": "text", "text": "Proof. For simplicity, consider RBF kernel for $k(\\cdot,\\cdot)$ i.e., $\\begin{array}{r}{k(a,b)=\\exp\\left(\\frac{-1}{h}(a-b)^{2}\\right)}\\end{array}$ , two particles $x_{1},x_{2}$ , and analyze their updates in SVGD based ensembling. At iteration $l$ , the update to particle $x_{2}$ is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{\\varsigma}^{*}(x_{2}^{l})=\\frac12\\Big(k(x_{2}^{l},x_{1}^{l})\\nabla_{x_{1}^{l}}\\log p(x_{1}^{l})+k(x_{2}^{l},x_{2}^{l})\\nabla_{x_{2}^{l}}\\log p(x_{2}^{l})+\\nabla_{x_{1}^{l}}k(x_{1}^{l},x_{2}^{l})+\\nabla_{x_{2}^{l}}k(x_{1}^{l},x_{2}^{l})\\Big)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In the above update, $\\vec{Q}\\;=\\;k(x_{2}^{l},x_{1}^{l})\\nabla_{x_{1}^{l}}\\log p(x_{1}^{l})+k(x_{2}^{l},x_{2}^{l})\\nabla_{x_{2}^{l}}\\log p(x_{2}^{l})$ aims to guide the particles in the direction that maximizes the likelihood, and the update direction $\\vec{R}=\\nabla_{x_{1}^{l}}k(x_{1}^{l},x_{2}^{l})+$ $\\nabla_{x_{2}^{l}}k(x_{1}^{l},x_{2}^{l})$ acts as the repulsive force. Considering the repulsive force ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overrightarrow{R}=\\nabla_{x_{1}^{l}}k(x_{1}^{l},x_{2}^{l})+\\nabla_{x_{2}^{l}}k(x_{1}^{l},x_{2}^{l})=\\nabla_{x_{1}^{l}}\\exp\\Big(\\displaystyle\\frac{-1}{h}(x_{1}^{l}-x_{2}^{l})^{2}\\Big)+\\nabla_{x_{2}^{l}}\\exp\\Big(\\displaystyle\\frac{-1}{h}(x_{1}^{l}-x_{2}^{l})^{2}\\Big)}\\\\ &{\\quad=\\frac{2}{h}(x_{2}^{l}-x_{1}^{l})k(x_{1}^{l},x_{2}^{l})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "As can be seen, the repulsive force $\\vec{\\cal R}$ pushes the particle $\\boldsymbol{x}_{2}^{l}$ in the direction away from particle $\\boldsymbol{x}_{1}^{l}$ that introduces diversity. With more particles, each particle is updated in the direction that maximizes the likelihood, and the particle is pushed away from all other particles (by force $R$ ). ", "page_idx": 19}, {"type": "text", "text": "Next, consider ensemble components with different strengths of incorrect evidence regularization $\\mathcal{L}_{\\mathrm{reg}}^{\\mathrm{inc}}$ to analyze the update to the evidential model in the evidence space. For simplicity, we consider the incorrect evidence sum-based regularization similar to ADL without correct evidence regularization (The analysis is valid for all incorrect evidence regularization and for models with correct evidence regularization). For a model with incorrect evidence regularization, the overall evidential loss is given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{evid}}(x,y)=\\mathcal{L}^{\\mathrm{Log}}(x,y)+\\lambda\\times\\mathcal{L}_{\\mathrm{reg}}^{\\mathrm{ADL}}(x,y)=\\log S-\\sum_{k=1}^{K}y_{k}\\log\\alpha_{k}+\\lambda\\times\\sum_{k=1}^{K}e_{k}\\times(1-y_{k})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\operatorname{grad}_{k}={\\frac{\\partial{\\mathcal{L}}^{\\mathrm{Log}}(\\mathbf{x},\\mathbf{y})}{\\partial o_{k}}}+{\\frac{\\partial{\\mathcal{L}}_{r e g}^{\\mathrm{aDL}}(\\mathbf{x},\\mathbf{y})}{\\partial o_{k}}}={\\Big(}{\\frac{1}{S}}-{\\frac{y_{k}}{\\alpha_{k}}}{\\Big)}{\\frac{\\partial e_{k}}{\\partial o_{k}}}+\\lambda\\times(1-y_{k})\\times{\\frac{\\partial e_{k}}{\\partial o_{k}}}}\\\\ &{\\qquad={\\Big(}{\\frac{1}{S}}-{\\frac{y_{k}}{\\alpha_{k}}}+\\lambda(1-y_{k}){\\Big)}e_{k}={\\Big(}{\\frac{1}{S}}-{\\frac{y_{k}}{\\alpha_{k}}}+\\lambda(1-y_{k}){\\Big)}e_{k}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Consider $K$ class classification problem. The gradient update to the logit layer for the evidential model is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\begin{array}{r}{\\mathbf{grad}_{k}=\\!e_{k}\\times{\\left[\\begin{array}{l}{{\\frac{1}{S}}-{\\frac{y_{1}}{\\alpha_{1}}}}\\\\ {{\\frac{1}{S}}-{\\frac{y_{2}}{\\alpha_{2}}}}\\\\ {\\qquad\\cdots}\\\\ {{\\frac{1}{S}}-{\\frac{y_{K}}{\\alpha_{K}}}}\\end{array}\\right]}+e_{k}\\times\\lambda\\times{\\left[\\begin{array}{l}{1-y_{1}}\\\\ {1-y_{2}}\\\\ {\\qquad\\cdots}\\\\ {1-y_{K}}\\end{array}\\right]}}\\\\ {={\\overrightarrow{A}}+\\lambda\\times{\\overrightarrow{B}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here, $y_{k}\\in[0,1],y_{k}=1$ if $k=\\mathtt{g t}$ , and $y_{k}=0$ otherwise. Moreover, the $\\lambda$ value is varied, and different $\\lambda$ values lead to different evidential models. ", "page_idx": 20}, {"type": "text", "text": "The update force $\\vec{A}$ pushes the evidential model in the direction that maximizes the likelihood (similar to $\\vec{Q}$ in SVGD-based update), and the force $\\vec{B}$ implicitly pushes the ensemble components away from each other (similar to the repulsive force $\\vec{R}$ in SVGD-based update). Each component moves in $\\vec{B}$ direction with a different force determined by the incorrect evidence regularization strength $\\lambda$ that ensures that the ensemble components are diverse. Due to the different strengths of incorrect evidence regularization, each ensemble component places a different priority level for minimization of incorrect evidence over acquiring correct evidence, which ensures that the ensemble components remain diverse. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "E Dataset and Implementation Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We consider ViT model backbone [15] that is pre-trained in a supervised fashion, and 4 benchmark datasets of Cifar10 [1], Cifar100 [1], Food101 [8], and Flowers102 [45]. We consider few-shot adaptation with $K$ -shot classification problem (We experiment with $K$ values of 1, 2, 5, 10, and 20). The few-shot training set is constructed by randomly selecting $K$ samples per class from the training set of the benchmark datasets. We consider 2-shot validation set for all datasets and settings. We train the model on the few-shot training set, use the 2-shot validation set for hyperparameter tuning, and evaluate all models on the benchmark test set with all the test set samples. We augment the few-shot training set and the few-shot validation set with resize, random horizontal flip, and cropping. The dataset details are also presented in Table 5. We train all the models for 50 epochs on the few-shot training dataset with a batch size of 64 samples at each iteration and evaluate the model on the benchmark test set. The evidence is in the range [0, infinity], and some stability issues could potentially arise in extreme cases when the logit output is extremely low (i.e. close to negative infinity). In our experiments, we did not observe the stability issue. Still, the issue can arise in some extreme cases for which a small delta in the denominator could be introduced or the network\u2019s logits could be bounded to be greater than a small negative value. For the calibration baseline model of Parameterized Temperature Scaling (PTS) [61], we consider a 2-layer neural network with 128 nodes in the hidden layer (we carry out hyperparameter tuning with 1-layer, 2-layer, and 3-layer networks and select the best model), and train with a learning rate of 0.00001. For Temperature Scaling [26], we optimize for the temperature hyperparameter using Adam optimizer, and a learning rate of 0.01 (We also experiment with SGD optimizer, and learning rates of 0.1, 1.0, 0.01, and 0.0001 to select the best performing model). The evidential models use incorrect evidence regularization strength of $(0,0.1,1.0,10.0,100.0$ , and 1000.0). For Isotonic Regression [6], we consider multi-class setting and sklearn package. We use VPT [32] as the representative PEFT where not specified due to its superior performance. The key model performance results are averaged across 5 different runs to present the mean and the standard deviation. The experiments use Pytorch and are carried out on a workstation with NVIDIA RTX A6000 GPU. ", "page_idx": 20}, {"type": "table", "img_path": "loQCk0qruU/tmp/b5bf25c6a7d5e6aa55c61e3cc55819371bdd71a5a16c926bf65bf89dc607981a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "loQCk0qruU/tmp/aadc6aa297b6922c3edfeef89c05d2086bcbab972ff5d2844f3adce5943d4cb4.jpg", "img_caption": ["(d) 1 Shot, $\\mathrm{{KL}=100.0}$ , $\\boldsymbol{\\mathbf{m}}=(\\boldsymbol{\\mathbf{e}})$ 1 Shot, $\\mathrm{{KL}=100.0}$ , $\\mathbf{m}=(\\mathrm{f})\\ 1$ Shot, $\\mathrm{{KL}=100.0}$ , $\\mathbf{m}=$ 1.0 1.5 2.0 ", "Figure 9: Visualization of the impact of m for 1-Shot Cifar100 dataset using reliability plots "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "F Additional Experiments ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "F.1 Impact of $m$ on Expected Calibration Error ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The developed B-PEFT model introduces the post-hoc calibration technique that adjusts the base rate in the evidential model with one additional hyperparameter $m$ (see Section 3.2 for details). We carry out a grid search using the few-shot validation dataset and select the optimal $m$ for the evidential models. In this section, we carry out detailed ablation to study the impact of $m$ with few-shot Cifar100 datasets. We consider $m$ values of $(0.5,1.0,1.5,2.0,2.5,3.0)$ , incorrect evidence regularization strengths of $(0.0,0.1,1.0,10.0,100.0,1000.0)$ , and few-shot values of $(1,2,5,10,20)$ (see Table 6 and Figure 9, Figure 10). Across all the experiments, we observe that as we increase $m$ , the model transforms the evidence to maximize the evidence gap making the model more confident on its knowledge. With increased confidence, the model\u2019s calibration performance improves. However, a large increase in $m$ values starts to make the model overconfident leading to increased ECE and poor calibration. The strength of incorrect evidence regularization also impacts the optimal value of $m$ . For large values of incorrect evidence regularization, a smaller $m$ value suffices to make the model well-calibrated. The trend is seen across all few-shot classification settings. ", "page_idx": 21}, {"type": "table", "img_path": "loQCk0qruU/tmp/a76e4b026cbb9e1a25e71edb689ae46bec182d10fe7740225ca13b1865625242.jpg", "table_caption": ["Table 6: Impact of m "], "table_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "loQCk0qruU/tmp/11ed1910dfd368773e3bb3e4994862377b6fb1228bf94e12ca71bb13d8d4f503.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "(d) 5 Shot, $\\mathrm{{KL}=100.0}$ , $\\mathbf{m}=$ (e) 5 Shot, $\\mathrm{{KL}=100.0}$ , $\\mathbf{m}=(\\mathrm{f})\\ 1$ Shot, $\\mathrm{{KL}=100.0}$ , $\\mathbf{m}=$ 1.0 1.5 2.0 ", "page_idx": 23}, {"type": "text", "text": "Figure 10: Visualization of the impact of m for 5-Shot Cifar100 dataset using reliability plots (a) Standard CE model (b) Evidential Model (c) Calibrated Evidential (d) B-PEFT (Ours) Figure 11: Accuracy-Confidence trends in 1-shot Cifar100 Results ", "page_idx": 23}, {"type": "image", "img_path": "loQCk0qruU/tmp/341d1f762c879af84d02ac35d1b12ca18d1bca97742d0bf2f844ec20910c64ae.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "F.2 Impact of Incorrect Evidence Regularization Strength (\u03bb) ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Evidential deep learning models introduce incorrect evidence regularization to minimize the evidence of classes other than the ground truth class. In this work, we use KL divergence-based incorrect evidence regularization (see Section C.2), and introduce a hyperparameter $\\lambda$ that controls the priority the model places on minimizing the incorrect class evidence over maximizing the correct class evidence. In this section, we study the impact of the hyperparameter $\\lambda$ on the model performance with $K$ -shot Cifar100 and Flowers102 experiments (We experiment with $K$ values of $(1,2,5,10,20))$ . We observe that the model\u2019s calibration performance (the ECE) is optimal when no incorrect evidence regularization is used i.e., $\\lambda\\:=\\:0$ (see Table 7 and Table 8). However, no incorrect evidence regularization hurts the model\u2019s generalization performance. With the increase in incorrect evidence regularization, the model\u2019s generalization performance (indicated by accuracy) improves albeit with ECE tradeoff. However, very large incorrect evidence regularization misguides the model to only focus on minimizing incorrect class evidence hurting generalization. The optimal $\\lambda$ value leads to the best generalization performance while hurting the ECE performance. Moreover, with a larger number of shots in training, the optimal $\\lambda$ value is generally smaller (For instance, in 1-shot Cifar100, optimal $\\lambda=1000.0$ , in 2-shot Cifar100, optimal $\\lambda=10.0$ , and in 5-shot Cifar100, optimal $\\lambda=0.1$ ). Once the optimal $\\lambda$ value is determined, we can resort to our evidential base-rate adjustment that leads to a calibrated evidential model with good generalization performance. ", "page_idx": 24}, {"type": "table", "img_path": "loQCk0qruU/tmp/f5708182ee6a6fff04094b28eaed72422a5044e911f6d28932772f22ce6a724c.jpg", "table_caption": ["Table 7: Different Shot Classification \u2013 Accuracy and ECE in Cifar100 "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "loQCk0qruU/tmp/2efb9decd158120b42b440046b02499e58affd1e08c8f3c78c7dfd2ba4861165.jpg", "table_caption": ["Table 8: Different Shot Classification \u2013 Accuracy and ECE in Flowers102 "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "F.3 Impact of Ensemble Components ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We present the experiment to study the impact of ensemble components in Figure 12. The experiment is performed on 1 one-shot Cifar100 dataset using prompt-based adaption for vision transformer. There is a performance gain of both accuracy and ECE with the use of all ensemble components. We also note that as we increase the number of components from 1 to 3, both the generalization and calibration performance increase significantly. For instance, the ECE with a single ensemble component is 0.088 which improves to 0.077 with 3 ensemble components while the accuracy improves by almost $3\\%$ .. However, with a further increase in the number of ensemble components, the generalization/calibration performance improvement is not significant. Thus, for our B-PEFT model, we carry out an ensemble of 3 evidential models that is a good balance between the number of ensemble components and the gain in generalization/calibration performance. ", "page_idx": 25}, {"type": "image", "img_path": "loQCk0qruU/tmp/b57edeb11c4e57ee063f6d535db77efecb29d7c64e808ab144cc41614279c3c2.jpg", "img_caption": ["Figure 12: Impact of ensemble components as number of component increases "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "F.4 Few Shot Learning Results ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this set of experiments, we apply our model to the 5-way 1-shot mini-ImageNet testing tasks and compare it with representative meta-learning models. The results are summarized in Table 9. Benefiting from the knowledge acquired during pre-training, the proposed PEFT-based model outperforms the episodic meta-learning models by a large margin, demonstrating the potential of large vision foundation models for effective few-shot learning. However, the VPT model is underconfident as shown in Table 10 and Figure 13. Our B-PEFT model improves on the VPT model\u2019s generalization and calibration leading to promising few-shot adaptation results. ", "page_idx": 25}, {"type": "table", "img_path": "loQCk0qruU/tmp/37bdb90c5e96c4fcb1d678524034615b06241c82029e3d321e09b79a048c8d72.jpg", "table_caption": ["Table 9: 5-Way 1-Shot Mini-ImageNe "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "loQCk0qruU/tmp/533370fcfd5399f730b8db118b6b36897f564f285b1b9f0da26e8ae5474602b3.jpg", "table_caption": ["Table 10: Calibration Results "], "table_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "loQCk0qruU/tmp/5e7a790985ec7054ab1990091ae86d0c6a40080ec6282ca9557745bc601181cd.jpg", "img_caption": ["Figure 13: VPT reliability plot for 5-Way 1- Shot Mini-ImageNet "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "loQCk0qruU/tmp/d21cd21274ddc2c32d22ac09460ec9a2c0b290e680f01096128f6a5e7ca5d86c.jpg", "img_caption": ["Figure 14: Full fine tuning reliability plot for 100-way 1-Shot Cifar100 "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "F.5 Impact of Different Components ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we study the under-confidence behavior of PEFT methods w.r.t. data size, number of classes, and number of unfrozen parameters. ", "page_idx": 25}, {"type": "text", "text": "Data size. We observe that the model\u2019s accuracy increases with more training samples (see Table 1 where we vary shots from 1 to 20). The under-confidence remains, even with further increase in training samples. To this end, we conduct additional experiments on Cifar100 by increasing the training samples per class to 500 and observe the under-confidence issue despite the increase in the accuracy. The trend is summarized in Figure 15 (a-b). We see an increase in accuracy and a decrease in ECE. However, even with 500 samples per class, the under-confidence issue remains. Further, we report the accuracy and ECE of the fully fine-tuned model (fine-tuning of all the parameters) for 1 shot cifar100 in Table 11 where we observe a decrease in accuracy while the calibration issue remains. We observe that full fine-tuning leads to overconfidence behavior, hurting the generalization performance, as seen in Table 11 and reliability plot as presented in Figure 14. ", "page_idx": 26}, {"type": "image", "img_path": "loQCk0qruU/tmp/df25af8494550ccdf704f60a5409739cdafb5a8185e052bbd1a52e8b93d11774.jpg", "img_caption": ["Figure 15: (a-b) Accuracy-EEC trend with the number of parameters, (c-d): Accuracy-ECE Trend with data size "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Number of classes: To study the impact of number of classes, we formulate 5-way 1 shot, 10-way 1 shot, and 100-way 1 shot tasks using Cifar100. The results are presented in Table 12. As we decrease the number of shots from 100 to 5, we see an increase in accuracy and a decrease in ECE. We observe that the model is more accurate as tasks become easier (indicated by fewer classes i.e., lower $N$ value in Table 12). However, the under-confidence issue remains. ", "page_idx": 26}, {"type": "text", "text": "Number of unfrozen parameters: We conduct additional experiments on Cifar100 100-way 1-shot tasks by varying the number of prompts for 1) shallow prompt: prompt added to the input only and 2) deep prompt: prompt added to all Transformer encoder layers\u2019 input as well. The accuracy and ECE trends are presented in Figure 15 (c-d). As can be seen, with the increase in the number of prompts for both shallow and deep prompts, there are fluctuations in accuracy and ECE performance. However, the under-confidence issue persists for all the cases. ", "page_idx": 26}, {"type": "text", "text": "F.6 Additional Experiments and Comparison ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we carry out additional experiments to study the OOD performance of our model, and compare our model with additional methods present in literature. ", "page_idx": 26}, {"type": "text", "text": "OOD Performance: The current work, being an instance of fine-grained uncertainty quantification works, could potentially help in OOD detection. To this end, we present the OOD results of Cifar10 as in-distribution dataset and Cifar100 as out-of-distribution dataset with AUROC, FPR95, AUPR metrics for our model on 100-way 1-shot and 100-way 5-shot Cifar100 tasks in Table 13. As seen, B-PEFT performs better than PEFT, and with more training data, the model\u2019s OOD detection capabilities improve. Even with only 5 samples/class (i.e. 100-way 5-shot Cifar100 task), the model can achieve an AUROC of 92.58. ", "page_idx": 26}, {"type": "text", "text": "Model Comparison: We first carry out experiments with cosine classifier [22] without training for the 100-way 1-shot task on Cifar100. The cosine classifier has comparable generalization performance (accuracy) in comparison to VPT based model (see Table 14). However, looking at the ECE, the miscalibration issue is even higher than VPT based model. Hence, the simple solution (cosine classifier) does not ensure calibrated predictions. We also carry out experiments with test time augmentation [4] and VPT fine tuning with LoRA [70], on 100-way 1-shot Cifar100 dataset. Towards comparison with Bayesian inspired methods [14], we use Laplace approximation on last layer of the model using Kronecker Product and Diagonalization represented by KronLaplace and DiagLaplace in Table 14. As can be seen, these methods also suffer from the under-confidence issue when straightforwardly extended to the VPT. B-PEFT achieves much better generalization and calibration performance than these baselines. ", "page_idx": 26}, {"type": "table", "img_path": "loQCk0qruU/tmp/75b322cf4e214ca58136ef26dbe112e96cb96b2fed371663745b6a554daec83b.jpg", "table_caption": ["Table 11: Full fine-tuning results "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "loQCk0qruU/tmp/e87f51678bd62d1512bf77eaffd1879a1b10a006820abd4661f5afe7113820ba.jpg", "table_caption": ["Table 12: N-Way 1-Shot Cifar100 calibration"], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "loQCk0qruU/tmp/f690996f9907ff6b6d62d76675b12c49623a86b35dc6ba61b561c2f34c746e7a.jpg", "table_caption": ["Table 13: 100-way Cifar100 OOD experiments "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "loQCk0qruU/tmp/8cc8a9df3e2ba8f98bd87279aad74ba8384bcded002f7098b911005a21d9f642.jpg", "table_caption": ["Table 14: Comparison with baselines "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "G Calibration Behavior of Self-Supervised Model ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this work, we focus on vision foundation models that are pre-trained in a supervised learning paradigm. These models have shown remarkable effectiveness in a wide range of areas including image classification, video understanding, and visual recognition among others. Alternatively, foundation models have been developed that pre-train in a self-supervised fashion (e.g., CLIP [52]). These models demonstrate good zero-shot performance on various datasets. As a representative of the self-supervised models, we use CLIP in our experiments. Parameter-efficient methods [77, 76, 21] have been proposed to adapt the CLIP model for downstream tasks. We use the popular methods: adapter and prompt for few-shot adaptation to study the calibration behavior. The reliability plot of different shot adaptations (1,2 and 5 shots per class) for cifar100 along with accuracy and ECE is presented in Figure 17. The accuracy behavior as we increase the number of shots from 1 to 20 is shown in Figure 16. In both methods, we observe for few-shot adaptations, the accuracy is either lower or comparable to zero-shot performance. More specifically, for adapter-based adaptation, even with 5 shots per class, the accuracy does not reach zero-shot accuracy. On the contrary, the parameter-efficient fine-tuning of supervised foundation models shows consistent improvement in accuracy as we increase the number of shots. ", "page_idx": 27}, {"type": "image", "img_path": "loQCk0qruU/tmp/3d1ba882e26c13241fb4cb46061426137ed5c06bc9b5681fa7adf56353ef8c13.jpg", "img_caption": ["Figure 16: Accuracy trends of CLIP on fewshot adaptation "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "Towards calibration performance, we observe that prompt-based adaptation, along with zero-shot generally show good calibration performance with some degree of overfitting (see Figure 17). In contrast, prompt adaptation for supervised models that are severe, and prompt adaptation for CLIP models have no such issue. However, the adapter-based adaptation is mostly over-confident and hence has a higher ECE value than our proposed model. Considering these results for parameter-efficient few-shot adaption of pre-trained self-supervised models, the calibration and uncertainty behavior of such pre-trained models pose an interesting direction for further investigation. ", "page_idx": 27}, {"type": "text", "text": "H Societal Impact ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We study the parameter-efficient fine-tuning techniques for large pre-trained vision foundation models, where we identify two key issues: the under-confidence of the fine-tuned models in their predictions, and lack of fine-grained uncertainty quantification capabilities. We develop a novel Bayesian Evidential model: B-PEFT that addresses the weaknesses of existing PEFT for pre-trained foundation models. Being an instance of the PEFT, our developed model enables the large foundation models to be adapted to challenging few-shot problems in a parameter-efficient and computationally efficient manner with limited memory requirements and energy footprint. Moreover, the developed model improves the generalization performance and the model\u2019s predictions are calibrated ensuring trustworthiness. Finally, the model has fine-grained uncertainty quantification capabilities which are highly desirable when applying these models in real-world safety-critical scenarios. Overall, our developed B-PEFT is expected to have a strong positive societal impact. ", "page_idx": 27}, {"type": "image", "img_path": "loQCk0qruU/tmp/54205fbbe01e95011c1b111eefa9d8b940968720246f87bee5b9494f501ddb9e.jpg", "img_caption": ["Figure 17: Different Shot adaptation: Calibration performance of CLIP based model on cifar100 dataset using prompt and adapter-based fine-tuning "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "I Limitations and Future Work ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this work, we investigate the calibration of transformer-based foundation models under few-shot adaptation using various parameter-efficient fine-tuning methods. We focus on fine-tuning supervised pre-trained models for few-shot learning. We note that there are other self-supervised pre-trained models that show promising results for various benchmark datasets. We investigate the calibration of CLIP, a representative method, under few-shot adaptation using the two most popular parameterefficient fine-tuning methods: prompt and adapter. Our preliminary results demonstrate that the few-shot performance does not consistently increase in comparison to zero-shot. Similarly, promptbased fine-tuning has relatively better calibration than adapter-based fine-tuning. As an extension of this work, we will investigate the calibration performance of self-supervised foundation models. Additionally, it could be interesting to study the calibration performance of PEFT for tasks beyond image classification, e.g., to other modalities such as audio and language foundation models. For instance, the ideas developed in this work could potentially be used in situations where the PEFT leads to mis-calibrated models and the developed model requires trustworthy fine-grained uncertainty quantification capabilities. If these data modalities are also modeled using transformers and follow the parameter-efficient fine-tuning paradigm in performing downstream tasks, we expect the proposed approach can benefit them in a similar way. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our abstract and introduction present the paper\u2019s contribution and scope, and match the theoretical and empirical results in the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The limitations of the proposed work have been discussed in Section I. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Complete proof of all the theoretical claims is presented. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Details for reproducibility along with link to the code is provided. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Only benchmark datasets and publicly available models are used for training and evaluation. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Experimental setting and details are provided. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All the key results present the mean and standard deviation of five trials. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Details of all compute resources used in experiments is provided ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The work confirms to the NeurIPS Code of Ethics. ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The societal impact is discussed in Section H. ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] Justification: The work poses no obvious high risk for misuse. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: References and citations are provided as required. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The link to the source code and resources is provided. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: No crowdsourcing and research with human subjects is involved in this research work. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: No crowdsourcing and research with human subjects is involved in this research work. ", "page_idx": 30}]