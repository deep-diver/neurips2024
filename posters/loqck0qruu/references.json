{"references": [{"fullname_first_author": "Krizhevsky Alex", "paper_title": "Learning multiple layers of features from tiny images", "publication_date": "2009", "reason": "This paper introduced the CIFAR-10 and CIFAR-100 datasets, which are widely used in the paper's experiments to evaluate performance."}, {"fullname_first_author": "Dosovitskiy Alexey", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-10-26", "reason": "This paper introduced Vision Transformers (ViT), the backbone architecture used in the paper's experiments."}, {"fullname_first_author": "Radford Alec", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021", "reason": "This paper introduced CLIP, a model that is used as a baseline in the paper's analysis of the impact of self-supervised pre-training."}, {"fullname_first_author": "Guo Chuan", "paper_title": "On calibration of modern neural networks", "publication_date": "2017", "reason": "This paper provided the foundation for calibration techniques used to compare the proposed method's calibration performance."}, {"fullname_first_author": "Lakshminarayanan Balaji", "paper_title": "Simple and scalable predictive uncertainty estimation using deep ensembles", "publication_date": "2017", "reason": "This paper introduced deep ensembles, which are used in the proposed method to address under-confidence and improve uncertainty quantification."}]}