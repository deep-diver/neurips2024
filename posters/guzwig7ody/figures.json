[{"figure_path": "guzWIg7ody/figures/figures_5_1.jpg", "caption": "Figure 1: (a) Demonstration of the convolution operation W * z, where the input is z \u2208 RD\u00d7w, and the output is W * z \u2208 RD\u00d7w'. Here Wj,:,: is a D \u00d7 w matrix for the j-th output channel. (b) Demonstration of the ConvResNeXt. f1,1 \u00b7\u00b7\u00b7 fN,M are the building blocks, each building block is a convolution neural network.", "description": "Figure 1(a) shows a 1D convolution operation where a kernel W of size K convolves with an input vector z of length D and width w to produce an output vector of length D and width w'.  Figure 1(b) shows the architecture of a ConvResNeXt network consisting of N residual blocks. Each residual block contains M parallel blocks, each being a small convolutional neural network.", "section": "2 Preliminaries"}, {"figure_path": "guzWIg7ody/figures/figures_14_1.jpg", "caption": "Figure 2: Illustration of a Besov function on 1-dimensional manifold embedded in a 3-dimensional ambient space.", "description": "The figure visualizes a Besov function defined on a one-dimensional manifold that is embedded within a three-dimensional ambient space.  The manifold itself is a curve that resembles a twisted loop. The color coding represents the function's values along the manifold, ranging from negative (blue) to positive (red). The visualization demonstrates how a relatively simple function (one-dimensional) can have a complex appearance when viewed within a higher-dimensional setting.", "section": "B Numerical Simulation"}, {"figure_path": "guzWIg7ody/figures/figures_15_1.jpg", "caption": "Figure 3: MSE as a function of the effective degree of freedom (dof) of different methods.", "description": "This figure compares the mean squared error (MSE) achieved by various regression methods as a function of their effective degrees of freedom.  It shows that ConvResNeXt achieves the lowest MSE with a relatively smaller number of degrees of freedom compared to other methods like Lasso regression, kernel ridge regression, decision trees, XGBoost, and Gaussian Processes. This suggests that ConvResNeXt is more efficient and effective for this task.", "section": "B Numerical Simulation"}, {"figure_path": "guzWIg7ody/figures/figures_15_2.jpg", "caption": "Figure 4: MSE as a function of dimension D.", "description": "This figure shows the mean squared error (MSE) of different regression models plotted against the ambient dimension (D) of the data.  It demonstrates the effect of increasing data dimensionality on the accuracy of various methods, including ConvResNeXt (our proposed method), Kernel Ridge Regression, Lasso Regression, XGBoost, Decision Tree, and Gaussian Processes.  The results highlight the relative robustness of ConvResNeXt and PNN to increasing dimensionality compared to the other methods.  Specifically,  ConvResNeXt and PNN show much smaller increases in MSE as D increases compared to other baselines.", "section": "B Numerical Simulation"}, {"figure_path": "guzWIg7ody/figures/figures_15_3.jpg", "caption": "Figure 5: MSE as function of sample size n.", "description": "This figure displays the mean squared error (MSE) for various regression methods plotted against the number of data points (n).  It shows how the MSE changes as the amount of training data increases.  The different lines represent different regression techniques, illustrating their comparative performance and scalability with data size.", "section": "B Numerical Simulation"}]