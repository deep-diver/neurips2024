[{"type": "text", "text": "On conditional diffusion models for PDE simulations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Aliaksandra Shysheya\u2217 Cristiana Diaconu\u2217 Federico Bergamin\u2217 University of Cambridge University of Cambridge Technical University of Denmark as2975@cam.ac.uk cdd43@cam.ac.uk fedbe@dtu.dk ", "page_idx": 0}, {"type": "text", "text": "Paris Perdikaris Microsoft Research AI4Science paperdikaris@microsoft.com ", "page_idx": 0}, {"type": "text", "text": "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato University of Cambridge jmh233@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Richard E. Turner   \nUniversity of Cambridge   \nMicrosoft Research AI4Science   \nThe Alan Turing Institute   \nret23@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Emile Mathieu University of Cambridge ebm32@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Modelling partial differential equations (PDEs) is of crucial importance in science and engineering, and it includes tasks ranging from forecasting to inverse problems, such as data assimilation. However, most previous numerical and machine learning approaches that target forecasting cannot be applied out-of-the-box for data assimilation. Recently, diffusion models have emerged as a powerful tool for conditional generation, being able to flexibly incorporate observations without retraining. In this work, we perform a comparative study of score-based diffusion models for forecasting and assimilation of sparse observations. In particular, we focus on diffusion models that are either trained in a conditional manner, or conditioned after unconditional training. We address the shortcomings of existing models by proposing 1) an autoregressive sampling approach, that significantly improves performance in forecasting, 2) a new training strategy for conditional score-based models that achieves stable performance over a range of history lengths, and 3) a hybrid model which employs flexible pre-training conditioning on initial conditions and flexible posttraining conditioning to handle data assimilation. We empirically show that these modifications are crucial for successfully tackling the combination of forecasting and data assimilation, a task commonly encountered in real-world scenarios. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Partial differential equations (PDEs) are ubiquitous as they are a powerful mathematical framework for modelling physical phenomena, ranging from the motion of fluids to acoustics and thermodynamics. Applications are as varied as weather forecasting [4], train aerodynamics profiling [70], and concert hall design [76]. PDEs can either be solved through numerical methods like finite element methods [43], or, alternatively, there has been a rising interest in leveraging deep learning to learn neural approximations of PDE solutions [56]. These approaches have shown promise for generating accurate solutions in fields such as fluid dynamics and weather prediction [37, 52, 54]. ", "page_idx": 0}, {"type": "text", "text": "Some of the most common tasks within PDE modelling are 1) forecasting, where the goal is to generate accurate and long rollouts based on some initial observations, and 2) inverse problems, which aim to reconstruct a certain aspect of the PDE (i.e. coefficient, initial condition, full trajectory, etc.) given some partial observations of the solution to the PDE. Data assimilation (DA) refers to a particular class of inverse problems [62], with a goal to predict or refine a trajectory given some partial and potentially noisy observations. While in general these tasks are addressed separately, there are cases in which a model that could jointly tackle both tasks would be beneficial. For example, in weather prediction, a common task is to forecast future states, as well as update them after more observations from weather stations and satellites are available [29]. Currently, traditional numerical weather prediction systems tackle this in a two-stage process (a forecast model followed by a data assimilation system), both with a similarly high computational cost. A simpler option would be to have a model that is flexible enough to both produce accurate forecasts as well as condition on any incoming noisy observations. ", "page_idx": 0}, {"type": "table", "img_path": "nQl8EjyMzh/tmp/128421632e8a8c0e72aaaf94f180e8c7db66ff46040add08abb5ca4ec999affd.jpg", "table_caption": ["Table 1: Score-based methods considered in this work. Each method can be classified in terms of the score network (joint or conditional), the rollout strategy at sampling time, and the conditioning mechanism at inference time. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we investigate how a probabilistic treatment of PDE dynamics [9, 81] can achieve this goal, with a focus on score-based diffusion models [26, 64, 68] as they have shown remarkable performance in other challenging applications, ranging from image and video [27, 58] to protein generation [72, 78, 79]. We perform a comparative study between the diffusion model approaches in Tab. 1, and evaluate their performance on forecasting and DA in PDE modelling. The unconditionallytrained model that is conditioned post-training through reconstruction guidance [65] is referred to as the joint model, while the amortised model directly fits a conditional score during training. ", "page_idx": 1}, {"type": "text", "text": "The main contribution of this work is that we propose extensions to the joint and amortised models to make them successfully applicable to tasks that combine forecasting and DA: ", "page_idx": 1}, {"type": "text", "text": "Joint model. We propose an autoregressive (AR) sampling strategy to overcome the limitations of the sampling approach proposed in [60], which samples full PDE trajectories all-at-once (AAO). We provide theoretical justification for why AR outperforms AAO, and empirically show, through extensive experiments, that AR outperforms or performs on par with AAO on DA, and drastically improves upon AAO in forecasting, where the latter fails. ", "page_idx": 1}, {"type": "text", "text": "Amortised model. We introduce a novel training procedure for amortised models for PDEs, that, unlike previous work [42], allows their performance to remain stable over a variety of history lengths. Moreover, we propose a hybrid model that combines amortisation with reconstruction guidance that is especially well-suited to mixed tasks, that involve both a forecasting and DA component. ", "page_idx": 1}, {"type": "text", "text": "Finally, this work provides, to the best of our knowledge, the first quantitative comparison between joint and amortised models for PDE modelling, with all other factors such as training dataset and model architecture carefully controlled. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Continuous-time diffusion models. In this section we recall the main concepts underlying continuous diffusion models, and refer to Song et al. [68] for a thorough presentation. We consider a forward noising process $(x(t))_{t\\geq0}$ associated with the following stochastic differential equation (SDE) ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\boldsymbol{x}(t)=-\\frac{1}{2}\\boldsymbol{x}(t)\\mathrm{d}t+\\mathrm{d}\\boldsymbol{w}(t),\\;\\boldsymbol{x}(0)\\sim p_{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "with $w(t)$ an isotropic Wiener process, and $p_{0}$ the data distribution. The process defined by (1) is the well-known Ornstein\u2013Uhlenbeck process, which geometrically converges to the standard Gaussian distribution $\\mathcal{N}(0,\\mathrm{I})$ . Additionally, for any $t\\geq0$ , the noising kernel of (1) admits the following form $p_{t|0}(x(t)|x(0))=\\mathcal{N}(x(t)|\\mu_{t}x(0),\\sigma_{t}^{2}\\mathrm{I})$ , with $\\mu_{t}\\,=\\,e^{-t/2}$ and $\\sigma_{t}^{2}=1-e^{-t}$ . Importantly, under mild assumptions on $p_{0}$ , the time-reversal process $(\\overleftarrow{x}(t))_{t\\geq0}$ also satisfies an SDE [10, 24] which is ", "page_idx": 1}, {"type": "text", "text": "given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\overleftarrow{\\boldsymbol{x}}\\left(t\\right)=\\left\\{-\\frac{1}{2}\\overleftarrow{\\boldsymbol{x}}\\left(t\\right)-\\nabla\\log p_{t}(\\overleftarrow{\\boldsymbol{x}}\\left(t\\right))\\right\\}\\mathrm{d}t+\\mathrm{d}w(t),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $p_{t}$ denotes the density of $x(t)$ . In practice, the score $\\nabla\\log{p_{t}}$ is unavailable, and is approximated by a neural network $\\mathbf{s}_{\\theta}(t,\\cdot)\\approx\\nabla\\log p_{t}$ , referred to as the score network. The parameters $\\theta$ are learnt by minimising the denoising score matching (DSM) loss [30, 68, 73] ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta)=\\mathbb{E}[\\|\\mathbf{s}_{\\theta}(t,x(t))-\\nabla\\log p_{t}(x(t)|x(0))\\|^{2}],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the expectation is taken over the joint distribution of $t\\sim\\mathcal{U}([0,1])$ , $x(0)\\sim p_{0}$ and $x(t)\\sim$ $p_{t|0}(\\cdot|x(0))$ from the noising kernel. Sampling involves discretising and solving (2). Optionally, to avoid errors accumulating along the denoising discretisation, each denoising step (predictor) can be followed by a small number of Langevin Monte Carlo (corrector) steps [67]. ", "page_idx": 2}, {"type": "text", "text": "Conditioning with diffusion models. The methodology introduced above allows for (approximately) sampling from $x(0)\\sim p_{0}$ . However, in many settings, one is actually interested in sampling from the conditional $p(x(0)|y)$ given some observations $y\\in\\mathcal{V}$ with likelihood $p(\\boldsymbol{y}|\\boldsymbol{\\mathcal{A}}(\\boldsymbol{x}))$ and $A:\\,\\mathcal{X}\\ \\to\\ \\mathcal{Y}$ being a measurement operator. The conditional is given by Bayes\u2019 rule $p(x(0)|y)\\,=\\,p(y|x(0))p\\bar{(x}(0))/p(y)$ , yet it is not available in closed form. An alternative is to simulate the conditional denoising process [13, 15, 68] ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\overleftarrow{x}\\left(t\\right)=\\left\\{-\\frac{1}{2}\\overleftarrow{x}(t)-\\nabla\\log p_{t}(\\overleftarrow{x}(t)|y)\\right\\}\\mathrm{d}t+\\mathrm{d}w(t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In the following, we describe the two main ways to sample from this process. ", "page_idx": 2}, {"type": "text", "text": "Amortising over observations. The simplest approach directly learns the conditional score $\\nabla\\log p_{t}(\\overleftarrow{\\mathbf{\\boldsymbol{x}}}(t)|\\mathbf{\\boldsymbol{y}})$ [26, 68]. This can be achieved by additionally feeding $y$ to the score network $\\mathbf{s}_{\\theta}$ , i.e. ${\\bf s}_{\\theta}(t,x(t),y)$ , and optimising its parameters by minimising a loss akin to (3). ", "page_idx": 2}, {"type": "text", "text": "Reconstruction guidance. Alternatively, one can leverage Bayes\u2019 rule, which allows to express the conditional score w.r.t. $x(t)$ as the sum of the following two gradients ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla\\log p(x(t)|y)=\\underbrace{\\nabla\\log p(x(t))}_{\\mathrm{prior}}+\\underbrace{\\nabla\\log p(y|x(t))}_{\\mathrm{guidance}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\nabla\\log p(x(t))$ can be approximated by a score network ${\\mathbf{s}}_{\\theta}(t,x(t))\\approx\\nabla\\log p(x(t))$ trained on the prior data\u2014with no information about the observation $y$ . The conditioning comes into play via the $\\operatorname{\\bar{V}}\\log{p(y|x(t))}$ term which is referred to as reconstruction guidance [11, 13, 27, 48, 65, 66, 80]. Assuming a Gaussian likelihood $p(y|x(0))=\\mathcal{N}(y|\\mathrm{A}x(0),\\sigma_{y}^{2}\\tilde{\\mathrm{I}})$ with linear measurement ${\\mathcal{A}}(x)=$ $\\mathrm{A}x$ , we have: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{p(y|x(t))=\\displaystyle\\int p(y|x(0))p(x(0)|x(t))\\:\\mathrm{d}x(0)\\approx\\displaystyle\\int\\mathcal{N}(y|\\mathrm{A}x(0),\\sigma^{2})q(x(0)|x(t))\\:\\mathrm{d}x(0)}\\\\ {\\displaystyle=\\mathcal{N}\\left(y|\\mathrm{A}\\hat{x}(x(t)),(\\sigma_{y}^{2}+r_{t}^{2})\\mathrm{I}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $p(x(0)|x(t))$ is not available in closed form and is approximated by a Gaussian $\\begin{array}{r l r}{q(x(0)|x(t))}&{{}\\triangleq}&{\\mathcal{N}(x(0)|\\hat{x}(x(t)),r_{t}^{2}\\mathrm{I})}\\end{array}$ with mean given by Tweedie\u2019s formula [18, 57] $\\hat{x}(x(t))\\triangleq\\mathbb{E}[x(0)|x(t)]=(x(t)+\\sigma_{t}^{2}\\nabla\\log p_{t}(x(t)))/\\mu_{t}$ and variance $\\mathrm{Var}[x(0)|x(t)]\\approx r_{t}^{2}$ , where $r_{t}$ is the guidance schedule and is chosen as a monotonically increasing function [19, 53, 60, 65]. ", "page_idx": 2}, {"type": "text", "text": "3 PDE surrogates for forecasting and data assimilation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we discuss the design space of diffusion models, outlined in Tab. 1, for tackling forecasting and DA in the context of PDE modelling. ", "page_idx": 2}, {"type": "text", "text": "Problem setting. We denote by $\\mathcal{P}:\\mathcal{U}\\to\\mathcal{U}$ a differential operator taking functions $u:\\mathbb{R}_{+}\\times\\mathcal{Z}\\rightarrow$ $\\mathcal{X}\\in\\mathcal{U}$ as input. Along with some initial $u(0,\\cdot)=u_{0}$ and boundary $u(\\tau,\\bar{\\partial}\\mathcal{Z})=f$ conditions, these define a partial differential equation (PDE) $\\partial_{\\tau}u=\\mathcal{P}(u;\\alpha)=\\mathcal{P}\\left(\\partial_{z}u,\\partial_{z}^{2}u,\\dots;\\alpha\\right)$ with coefficients $\\alpha$ and where $\\partial_{\\tau}u$ is a shorthand for the partial derivative $\\partial u/\\partial\\tau$ , and $\\partial_{z}u,\\partial_{z}^{2}u$ denote the partial derivatives $\\partial u/\\partial z,\\partial^{2}u/\\partial z^{2}$ , respectively. We assume access to some accurate numerical solutions from conventional solvers, which are discretised both in space and time. Let\u2019s denote such trajectories by $x_{1:L}=(x_{1},\\dots,x_{L})\\in\\mathbb{R}^{D\\times L}\\sim p_{0}$ with $D$ and $L$ being the size of the discretised space and time domains, respectively. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "We are generally interested in the problem of generating realistic PDE trajectories given observations, i.e. sampling from conditionals $\\bar{p(x_{1:L}|y)}$ . Specifically, we consider tasks that involve conditioning on initial states, as well as on sparse space-time observations, reflective of some real-life scenarios such as weather prediction. Thus, we focus on two types of problems, and the combination thereof: (a) forecasting, which involves predicting future states $x_{H:L}$ given a length- $H$ past history $x_{1:H-1}$ , i.e. sampling from $p(x_{H:L}|x_{1:H-1})$ ; (b) data assimilation (DA), which is concerned with inferring the ground state of a dynamical system given some sparse observed states $x_{o}$ , i.e. sampling from $p(x_{1:L}|x_{o})$ . Then, the goal is to learn a flexible PDE surrogate, i.e. a machine learning model, that, unlike most numerical solvers [43], is able to solve the underlying PDE while accounting for various initial conditions and sparse observations. ", "page_idx": 3}, {"type": "text", "text": "3.1 Learning the score ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Learning a diffusion model over the full joint distribution $p(x_{1:L}(t))$ can become prohibitively expensive for long trajectories, as it requires a score network taking the full sequence as input\u2014 ${\\bf s}_{\\theta}(t,x_{1:L}(t))$ , with memory footprint scaling linearly with the length $L$ of the sequence. ", "page_idx": 3}, {"type": "text", "text": "One approach to alleviate this, as suggested by Rozet and Louppe [60], is to assume a Markov structure of order $k$ such that the joint over data trajectories can be factorised into a series of conditionals $\\begin{array}{r}{p(x_{1:L})=p(x_{1})p(x_{2}|x_{1})\\dots p(x_{k+1}|x_{1:k})\\prod_{i=k+2}^{L}p(x_{i}|x_{i-k:i-1})}\\end{array}$ . Here we are omitting the time dependency $x_{1:L}=x_{1:L}(0)$ for clarity\u2019s sake. The score w.r.t. $x_{i}$ can be written as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla_{x_{i}}\\log{p(x_{1:L})}=\\nabla_{x_{i}}\\log{p(x_{i}|x_{i-k:i-1})}+\\sum_{j=i+1}^{i+k}\\nabla_{x_{i}}\\log{p(x_{j}|x_{j-k:j-1})}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n=\\nabla_{{x}_{i}}\\log p(x_{i},x_{i+1:i+k}|x_{i-k:i-1})=\\nabla_{{x}_{i}}\\log p(x_{i-k:i+k})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with $k+1\\leq i\\leq L-k-1$ , whilst formulas for $i\\leq k$ and $i\\geq L-k$ can be found in App. B.1. In the case of modelling the conditional $p(x_{1:L}|y)$ , we use the same Markov assumption, such that learning $\\nabla_{x_{i}}\\log{p(x_{1:L}|y)}$ reduces to learning a local score $\\nabla_{x_{i}}\\log p(x_{i-k:i+k}|y)$ . The rest of Sec. 3.1 describes learning the local score of the joint distribution, but the same reasoning can easily be applied to fitting the local score of the conditional distribution. ", "page_idx": 3}, {"type": "text", "text": "When noising the sequence with the SDE (1), there is no guarantee that $x_{1:L}(t)$ will still be $k$ -order Markov. However, we still assume that $x_{1:L}(t)$ is Markov but with a larger order $k^{\\prime}>k$ , as motivated in Rozet and Louppe [60, Sec 3.1]. To simplify notations we use $k^{\\prime}=k$ in the rest of the paper. Consequently, instead of learning the entire joint score at once $\\nabla_{x_{1:L}(t)}\\log p(x_{1:L}(t))$ , we only need to learn the local scores ${\\bf s}_{\\theta}(t,x_{i-k:i+k}(t))\\approx\\nabla_{x_{i-k:i+k}(t)}\\log p_{t}(x_{i-k:i+k}(t))$ with a window size of $2k+1$ . The main benefit is that the network only takes as input sequences of size $2k+1$ instead of $L$ , with $k<<L$ . The local score network $\\mathbf{s}_{\\theta}$ is trained by minimising the following DSM loss ${\\mathcal{L}}(\\theta)$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}\\,\\|\\mathbf{s}_{\\theta}(t,x_{i-k:i+k}(t))-\\nabla\\log p_{t|0}(x_{i-k:i+k}(t)|x_{i-k:i+k})\\|^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the expectation is taken over the joint $i\\sim\\mathcal{U}([k+1,L-k])$ , $t\\sim\\mathcal{U}([0,1])$ , $x_{i-k:i+k}\\sim p_{0}$ and $x_{i-k:i+k}(t)\\stackrel{.}{\\sim}p_{t|0}(\\cdot|x_{i-k:i+k})$ given by the noising process. ", "page_idx": 3}, {"type": "text", "text": "3.2 Conditioning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Reconstruction guidance. We now describe how to tackle forecasting and DA by sampling from conditionals $p(x_{1:L}|x_{o})$ \u2014instead of the prior $p(x_{1:L})$ \u2014with the trained joint local score ${\\bf s}_{\\theta}(t,x_{i-k:i+k}(t))$ described in Sec. 3.1. For both tasks, the conditioning information $\\boldsymbol{y}=\\boldsymbol{x_{o}}\\in\\mathbb{R}^{O}$ is a measurement of a subset of variables in the space-time domain, i.e. $x_{o}=A(x)=\\mathrm{A}\\;\\mathrm{vec}(x)+\\eta$ with $\\mathrm{vec}(x)\\,\\in\\,\\mathbb{R}^{N}$ the space-time vectorised trajectory, $\\eta\\,\\sim\\,\\mathcal{N}(0,\\sigma_{y}^{2}\\mathrm{I})$ and a masking matrix $\\mathrm{A}=\\{0,1\\}^{O\\times N}$ where rows indicate observed variables with 1. Plugging this in (6) and computing the score we get the following reconstruction guidance term ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla\\log p(x_{o}|x(t))\\approx\\frac{1}{r_{t}^{2}+\\sigma_{y}^{2}}(y-\\mathrm{A}\\hat{x}(x(t)))^{\\top}\\mathrm{A}\\frac{\\partial\\hat{x}(x(t))}{\\partial x(t)}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Summing up this guidance with the trained score over the prior as in (5), we get an approximation of the conditional score $\\nabla\\log p(x_{1:L}(t)|x_{o})$ and can thus generate conditional trajectories by simulating the conditional denoising process. ", "page_idx": 4}, {"type": "text", "text": "Conditioning via architecture. Alternatively, instead of learning a score for the joint distribution and conditioning a posteriori, one can directly learn an approximation to the conditional score. Such a score network $\\mathbf{s}_{\\theta}\\bar{(t,x_{i:i+P}(t)|x_{i-C:i})}$ is trained to fit the conditional score given the states $x_{i-C:i}$ by minimising a DSM loss akin to (3). Here, we denote the number of observed states by $C$ , while $P$ stands for the length of the predictive horizon. Most of the existing works [35, 75] propose models, which we refer to as amortised models, where the number of conditioning frames $C$ is fixed and set before training. Thus, a separate model for each combination of $P$ and $C$ needs to be trained. To overcome this issue, similarly to [28], we propose a universal amortised model, which allows to use any $P$ and $C$ , such that $P+C=2k+1$ , during sampling. In particular, during training, instead of determining $C$ beforehand, we sample $C\\sim\\operatorname{Uniform}(\\{0,\\ldots,2k\\})$ for each batch and train the model to fit the conditional score $\\mathbf{s}_{\\theta}(t,x_{i-C:i+P}(t)|x_{i-C:i})$ , thus amortising over $C$ as well. ", "page_idx": 4}, {"type": "text", "text": "The conditioning on the previous states is achieved by feeding them to the score network as separate input channels. The conditioning dimension is fixed to the chosen window size of $2k+1$ . To indicate whether a particular channel is present we add binary masks of size $2k+1$ . Even though we condition on $x_{i-C:i}$ , the score for the whole window size $x_{i-C:i+P}(t)$ is predicted. In our experiments, we found that this training scheme works well for our tasks, although another parameterization as in [28], where the score is only predicted for the masked variables, could be used. ", "page_idx": 4}, {"type": "text", "text": "Architecture conditioning and reconstruction guidance. In the universal amortised model, conditioning on previous states happens naturally via the model architecture. However, this strategy is not practical for conditioning on partial observations (e.g. sparse space-time observations as in DA), as a new model needs to be trained every time new conditioning variables are introduced. To avoid this, we propose to simultaneously use reconstruction guidance (as estimated in (8)) to condition on other variables, as well as keep the conditioning on previous states through model architecture. ", "page_idx": 4}, {"type": "text", "text": "3.3 Rollout strategies ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we describe sampling approaches that can be used to generate PDE rollouts, depending on the choice of local score and conditioning. We provide in App. K pseudocode for each rollout strategy. ", "page_idx": 4}, {"type": "text", "text": "All-at-once (AAO). With a trained joint score ${\\mathbf s}_{\\theta}(t,x_{i-k:i+k}(t))$ we can generate entire trajectories $x_{1:L}$ in one go, as in Rozet and Louppe [60]. The full score $\\dot{\\mathbf{s}_{\\theta}(t,x_{1:L}(t))}\\approx\\nabla\\log p_{t}(x_{1:L}(t))$ is reconstructed from the local scores, as shown in App. B.1 and summarised in Rozet and Louppe [60, $\\mathrm{Alg}\\ 2.]$ . The ith component is given by ${\\bf s}_{i}={\\bf s}_{\\theta}(t,x_{i-k:i+k}(t))[k+1]$ which is the central output of the score network, except for the start and the end of the sequence for which the score is given by $\\mathbf{s}_{1:k}=\\mathbf{s}_{\\theta}(t,x_{1:2k+1}(t))[:k+1]$ and ${\\bf s}_{L-k:L}={\\bf s}_{\\theta}(t,x_{L-2\\bar{k}:L}(t))[k+1:],$ respectively. If we observe some values $x_{o}\\sim\\mathcal{N}(\\cdot|\\mathrm{A}x_{1:L},\\sigma_{y}^{2}\\mathrm{I})$ , the conditional sampling is done by summing the full score ${\\bf s}_{\\theta}(t,x_{1:L}(t))$ with the guidance term $\\nabla\\log p(x_{o}|x_{1:L}(t))$ estimated with (8). ", "page_idx": 4}, {"type": "text", "text": "Autoregressive (AR). Let us assume that we want to sample $x_{C+1:L}$ given $x_{1:C}$ initial states. An alternative approach to AAO is to factor the forecasting prediction problem as $p(x_{1:L}|x_{1:C})=$ $\\textstyle\\prod_{i}p(x_{i}|x_{i-C:i-1}{\\bar{\\mathbf{\\alpha}}})$ with $C\\leq k$ . As suggested in Ho et al. [27], it then follows that we can sample the full trajectory via ancestor sampling by iteratively sampling each conditional diffusion process. More generally, instead of sampling one state at a time, we can autoregressively generate $P$ states, conditioning on the previous $C$ ones, such that $P+C=2k+1$ . This procedure can be used both by the joint and the amortised conditional score. DA can similarly be tackled by further conditioning on additional observations appearing within the predictive horizon $P$ at each AR step of the rollout. ", "page_idx": 4}, {"type": "text", "text": "Scalability. Both sampling approaches involve a different number of neural function evaluations (NFEs). Assuming $p$ predictor steps and $c$ corrector steps to simulate (4), AAO sampling requires $(1+c)p$ NFEs. In contrast, the AR scheme is more computationally intensive as each autoregressive step also costs $(1+c)p$ NFEs but with $\\textstyle\\left(1+{\\frac{L-(2k+1)}{P}}\\right)$ AR steps. We stress, though, that this is assuming the full score network evaluation ${\\bf s}_{1:L}={\\bf s}_{\\theta}(t,x_{1:L}(t))$ is parallelised. In practice, due to memory constraints, the full score in AAO would have to be sequentially computed. Assuming only one local score evaluation fits in memory, it would require as many NFEs as AR steps. What\u2019s more, as discussed in the next paragraph, AAO in practice typically requires more corrector steps. See App. F.1 for further discussion on scalability. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Modelling capacity of the joint score. As highlighted in Tab. 1, when using a joint score, both AAO and AR rollout strategies can be used. We hypothesise there are two main reasons why AR outperforms AAO. First, for the same score network with an input window size of $2k+1$ , the AAO model has a Markov order $k$ whilst the AR model has an effective order of $2k$ . Indeed, as shown in (7), a process of Markov order $k$ yields a score for which each component depends on $2k+1$ inputs. Yet parameterising a score network taking $2k+1$ inputs, means that the AR model would condition on the previous $2k$ states at each step. This twice greater Markov order implies that the AR approach is able to model a strictly larger class of data processes than the AAO sampling. See App. B.1 for further discussion on this. ", "page_idx": 5}, {"type": "text", "text": "Second, with AAO sampling the denoising process must ensure the start of the generated sequence agrees with the end of the sequence. This requires information to be propagated between the two ends of the sequence, but at each step of denoising, the score network has a limited receptive field of $2k+1$ , limiting information propagation. Langevin corrector steps allow extra information flow, but come with additional computational cost. In contrast, in AR sampling, there is no limit in information flowing forward due to the nature of the autoregressive scheme. ", "page_idx": 5}, {"type": "text", "text": "4 Related work ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the following, we present works on diffusion models for PDE modelling since these are the models we focus on in this paper. We present an extended related work section in App. C, where we broadly discuss ML-based and classical solver-based techniques for tackling PDE modelling. ", "page_idx": 5}, {"type": "text", "text": "Diffusion models for PDEs. Recently, several works have leveraged diffusion models for solving PDEs, with a particular focus on fluid dynamics. Amortising the score network on the initial state, Kohl et al. [35] introduced an autoregressive scheme, whilst Yang and Sommer [81] suggested to directly predict future states. Recently, Cachay et al. [9] proposed to unify the denoising time and the physical time to improve scalability. Lippe et al. [42] built upon neural operators with an iterative denoising refinement, particularly effective to better capture higher frequencies and enabling long rollouts. In contrast to the above-mentioned work, others have tackled DA and super-resolution tasks. Shu et al. [63] and Jacobsen et al. [32] suggested using the underlying PDE to enforce adherence to physical laws. Huang et al. [29] used an amortised model and inpainting techniques [46] for conditioning at inference time to perform DA on weather data. Rozet and Louppe [60] decomposed the score of long trajectory into a series of local scores over short segments to be able to work with flexible lengths and to dramatically improve scalability w.r.t. memory use. Concurrently to our work, Qu et al. [55] extended this approach to latent diffusion models to perform DA on ERA5 weather data [25]. In this work, we show that such a trained score network can alternatively be sampled autoregressively, which is guaranteed to lead to a higher Markov order, and empirically produce accurate long-range forecasts. Concurrently, Ruhe et al. [61] proposed to use a local score, not justified by any Markov assumption, together with a frame-dependent noising process for video and fluid dynamics generation. Diffusion models for PDE modelling are indeed closely related to other sequence modelling tasks, such as video generation and indeed our universal amortised approach is influenced by H\u00f6ppe et al. [28], Voleti et al. [75]. ", "page_idx": 5}, {"type": "text", "text": "5 Experimental results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We study the performance of the diffusion-based models proposed in Tab. 1 on three different described in more detail below. The code to reproduce the experiments is publicly available at https://github.com/cambridge-mlg/pdediff. ", "page_idx": 5}, {"type": "text", "text": "Data. In this work, we consider the 1D Kuramoto-Sivashinsky (KS) and the 2D Kolmogorov flow equations, with the latter being a variant of the incompressible Navier-Stokes flow. KS is a fourth-order nonlinear 1D PDE describing flame fronts and solidification dynamics. The position of the flame $u$ evolves as $\\partial_{\\tau}u+u\\partial_{x}u+\\partial_{x}^{2}\\overline{{u}}+\\nu\\partial_{x}^{4}u=0$ where $\\nu>0$ is the viscosity. The equation is solved on a periodic domain with 256 points for the space discretisation and timestep $\\Delta\\tau=0.2$ . Training trajectories are of length $140\\Delta\\tau$ , while the length of validation and testing ones is set to $640\\Delta\\tau$ . The Kolmogorov flow is a 2D PDE that describes the dynamics of an incompressible fluid. The evolution of the PDE is given by $\\begin{array}{r}{\\partial_{\\tau}\\mathbf{u}+\\mathbf{u}\\cdot\\nabla\\mathbf{u}-\\nu\\nabla^{2}\\mathbf{u}+\\frac{\\tilde{1}}{\\rho}\\nabla p-f=0}\\end{array}$ and $\\nabla\\cdot\\mathbf{u}=0$ , where u represents the velocity field, $\\nu$ the viscosity, $\\rho$ the fluid density, $p$ the pressure, and $f$ is the external forcing. The considered trajectories have 64 states with $64\\times64$ resolution and $\\Delta\\tau\\,=\\,0.2$ . The evaluation metrics are measured with respect to the scalar vorticity field $\\omega=\\partial_{x}u_{y}-\\partial_{y}u_{x}$ . We focus on these since their dynamics are challenging, and have been investigated in prior work [17, 42]. We refer to App. D for more details on the data generation. In addition, in App. F.1 we present results for the simpler 1D Burgers\u2019 equation. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Models. For forecasting and DA, we train a single local score network on contiguous segments randomly sampled from training trajectories. We use a window 9 model for KS and a window 5 model for Kolmogorov, as these settings give a good trade-off between performance and memory requirements. We show in F.3 that increasing the window size in KS does lead to improved performance in forecasting, but the gains are relatively small. We parameterise the score network $\\mathbf{s}_{\\theta}$ with a modern U-Net architecture [22, 59] with residual connections and layer normalization. For sampling, we use the DPM solver [44] with 128 evenly spaced discretisation steps. As a final step, we return the posterior mean over the noise free data via Tweedie\u2019s formula. For the guidance schedule we set $r_{t}^{2}=\\gamma\\sigma_{t}^{2}/\\mu_{t}^{2}$ and tune $\\gamma$ via grid search. We refer to App. D.4 for more details. ", "page_idx": 6}, {"type": "text", "text": "Evaluation metrics. We compute two per time step metrics\u2014mean squared error $\\tt M S E_{1:L}\\;=$ E[(x1:L \u2212x\u02c61:L)2] and Pearson correlation \u03c11:L = VaCr(oxv1(:xL1):LV,ax\u02c6r(1:x\u02c6L1:)L) between model samples $\\hat{x}_{1:L}\\sim p_{\\theta}$ and ground truth trajectories $x_{1:L}\\sim p_{0}$ . We measure sample accuracy through $\\mathtt{R M S D}=$ $\\sqrt{\\frac{1}{L}\\sum_{l=1}^{L}{\\tt M S E}_{l}}$ and high correlation time $t_{\\operatorname*{max}}=l_{\\operatorname*{max}}\\times\\Delta t$ with $l_{\\mathrm{max}}=\\arg\\operatorname*{max}_{l\\in1:L}\\;\\{\\rho_{l}>0.8\\}$ . ", "page_idx": 6}, {"type": "text", "text": "5.1 Forecasting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As mentioned in Sec. 3, forecasting is a crucial component of the combined task we consider, so we investigate the ability of trained diffusion models to sample long rollouts. The models are evaluated on 128 and 50 test trajectories for KS and Kolmogorov, respectively. To understand the current state of score-based models for PDE forecasting we benchmark them against other ML-based models, including the state-of-the-art PDE-Refiner [42] and a MSE-trained U-Net and Fourier Neural Operator (FNO) [39]. We follow the setup in [42] for the architectures and training settings of the baselines. For the diffusion models and the MSE-trained U-Net baseline, we experiment with both an architecture inspired from [60] and one inspired from [42] and report the best results. The performance of both neural network architectures considered can be found in Tab. 5. More details about the different architectures can be found in App. D.4. ", "page_idx": 6}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/29b1ca51230f848749db145695f91263c0af4f16c1cfdb7dabdd104bb601c6d3.jpg", "img_caption": ["Figure 1: High correlation time $(\\uparrow)$ on the KS (left) and Kolmogorov (right) datasets for different $P~|~C$ conditioning scenarios of the joint, amortised and universal amortised models, where $P$ indicates the number of generated states and $C$ the number of states conditioned upon. We show mean $\\pm\\:3$ standard errors, as computed on the test trajectories. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Fig. 1 shows the performance of the joint, amortised and universal amortised models for different conditioning scenarios $P~|~C$ , with $P$ the number of states generated, and $C$ the number of states conditioned upon at each iteration. For the KS dataset, the universal amortised model outperforms the plain amortised one across all $P\\mid C$ ; for Kolmogorov their performance is comparable (i.e. within 3 standard errors). For both datasets, the best (plain and universal) amortised models outperform the best joint AR model, with the exact best high correlation times shown in Fig. 2. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Stability over history length. The most interesting comparison between the models lies in how they leverage past history in the context of forecasting. Previous work notes that longer history generally degrades the performance of amortised models [42]. We find that this behaviour is datasetdependent\u2014the observation holds for KS, whereas for Kolmogorov the plain amortised models with longer history (i.e. 1 | 4 and 2 | 3) tend to outperform the ones with shorter history (i.e. 3 | 2 and 4 | 1). Due to our novel training procedure for the universal amortised model (i.e. training over a variety of tasks), we obtain stable performance over all conditioning scenarios for the KS dataset. For Kolmogorov, the universal amortised model benefits from longer history, similarly to the plain amortised one. However, note that, unlike the plain amortised model, where a different model needs to be trained for each conditioning scenario, the universal amortised can tackle any scenario, allowing for a trade-off between accuracy and computational speed at sampling time. The joint model generally benefits from longer conditioning trajectories, but is also fairly stable for the majority of $P~|~C$ settings. We show more results in App. F.3. ", "page_idx": 7}, {"type": "text", "text": "For a complete comparison between the models in Tab. 1, we also perform AAO sampling for the joint model. However, as shown in App. F.1, the predicted samples rapidly diverge from the ground truth. Thus, the AR method of querying the joint model is crucial for forecasting. ", "page_idx": 7}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/0862a3460c5017c53d79d38acfddedeff427632e59709d63afbc713f3b807970.jpg", "img_caption": ["Figure 2: The best joint and amortised models compared against standard ML-based benchmarks for forecasting on the KS (left) and Kolmogorov (right) datasets. High correlation time (\u2191) is reported, showing the mean $\\pm\\:3$ standard errors, as computed on the test trajectories. We show in Tab. 5 the configurations for the best models. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Benchmarks. In Fig. 2, we report the comparison against popular methods for forecasting. The universal amortised model performs similarly to the MSE-trained U-Net on both datasets, while the joint model is comparable to the MSE-trained FNO, performing on par with it for KS and slightly outperforming it for Kolmogorov. However, we stress that, unlike these benchmarks, which can only be straight-forwardly applied to forecasting, our models are significantly more flexible\u2014they manage to achieve competitive performance with some of the MSE-trained benchmarks, yet can be applied to a wider variety of tasks (e.g. forecasting combined with DA). ", "page_idx": 7}, {"type": "text", "text": "Additional results. For a more thorough understanding of the models\u2019 behaviour, we perform further analysis on the frequency spectra of the generated trajectories in App. I. We show that, as noted in previous work [42], the generated KS samples tend to approximate the low-frequency components well, but fail to capture the higher-frequency components, while the Kolmogorov samples generally show good agreement with the ground truth spectrum. We also investigate the long-term behaviour of our proposed models in App. J. Even when generating trajectories that are significantly longer than those used during training, the models are capable of generating physically-plausible states. ", "page_idx": 7}, {"type": "text", "text": "5.2 Offline data assimilation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In offline DA, we assume we have access to some sparse observations. The experimental setting goes as follows. We first choose a number of observed variables, and then uniformly sample the associated indices. We assume to always observe some full initial states. More details can be found in App. G. We repeat this for different numbers of values observed, varying the sparsity of observations, from approximately a third of all data to almost exclusively conditioning on the initial states. We compute the RMSD of the generated trajectories for six values of the proportion of observed data, evenly spaced on a log scale from $10^{-3}$ to $10^{-0.5}$ . We show results for the joint AR model, joint AAO with 0 and 1 corrections (indicated between brackets), and the universal amortised model. To provide a consistent comparison with the AAO approach from [60], all models use the U-Net architecture inspired by [60]. We tune the guidance strength $\\gamma$ , and for AR we only report the $P\\mid C$ setting that gives the best trade-off between performance and computation time (see Fig. 28 for a comprehensive summary of performance depending on $\\gamma$ and $P\\mid C)$ . We show the results on 50 test trajectories for both KS and Kolmogorov. More results can be found in App. G. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/ba8736a364fbfae1001bbfe6c1b4bc602da2ee7c3586702f8569b94d1f677526.jpg", "img_caption": ["Figure 3: RMSD (mean $\\pm\\:3$ standard errors) for KS (left) and Kolmogorov (right) on the offline DA setting for varying sparsity levels (top), and the computational cost associated with each setting (bottom). The latter is the same for all sparsity settings for AAO, but differs for AR since it depends on the $P\\mid C$ setting that was used. The $c$ in AAO $(c)$ refers to the number of corrector steps used. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "More efficient sampling. We observe in Fig. 3 that AR sampling outperforms AAO in the sparse observation regimes, and performs on par with it in the dense observation regimes, provided corrections are used. However, Fig. 3 shows that corrections significantly increase the associated computational cost. We believe the difference in performance is mostly due to the limiting information propagation in the AAO scheme, combined with the lower effective Markov order of AAO vs AR ( $k$ vs $2k$ for a local score network with a window of $2k+1)$ , as discussed in Sec. 3.3. To further investigate this hypothesis, in App. G.2.1 we compare the results for a window 5 $\\mathbf{\\nabla}k=2$ ) AR model with a window 9 $[k=4,$ ) AAO model for KS. These should, in theory, have the same effective Markov order. We observe that for the sparsest observation regime, there is still a gap between AR $\\mathit{\\Delta}k=2$ ) and AAO $\\mathrm{[}k=4\\mathrm{]}$ ), potentially because the high sparsity of observations make the information flow in the AAO scheme inefficient. In fact, this is consistent with the findings from forecasting, where we observe that the AAO strategy is unable to produce coherent trajectories if it does not have access to enough future observations. In contrast, for the other sparsity scenarios the performance of the two schemes becomes similar provided corrections are used. However, this comes with a significant increase in computational cost. ", "page_idx": 8}, {"type": "text", "text": "Reconstruction guidance for universal amortised model. As outlined in Sec. 3.2, we can combine the universal amortised model with reconstruction guidance for conditioning on sparse observations. Amortised models do not straight-forwardly handle DA \u2014 this can be seen in Fig. 29a, where we show that conditioning just through the architecture gives poor results in DA. In KS, the universal amortised model achieves lower RMSD for the very sparse observation scenarios $[0^{-3}$ proportion observed), and for the very dense ones ( $\\mathrm{^{10^{0.5}}}$ proportion observed). However, the joint model outperforms it in the middle ranges. In Kolmogorov, the universal amortised model slightly outperforms the joint for all sparsity settings, with the results being within error bars. These findings indicate that they are both viable choices for offline DA, with the best performing choice depending on the application. ", "page_idx": 8}, {"type": "text", "text": "Interpolation baseline. In App. G.3 we also provide a quantitative and qualitative comparison to a simple interpolation baseline, proving that the studied models manage to capture the underlying physical behaviour better than such simple baselines. ", "page_idx": 8}, {"type": "text", "text": "5.3 Online DA: combining forecasting and data assimilation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Inspired by real-life applications such as weather prediction, we compare the models in the online DA setting, which represents the combination of forecasting and DA. We assume that some sparse and noisy variables are observed once every $s$ states (in blocks of length $s$ ), and whenever new observations arrive (i.e. at each DA step), we forecast the next $f$ states. At the first step, we perform forecasting assuming we have access to some randomly sampled indices, accounting for $10\\bar{\\%}$ of the first $s$ -length trajectory. At the second step (after $s$ time-steps), we refine this forecast with the newly observed set of variables. This repeats until we have the forecast for the entire length of the test trajectories. The performance is measured in terms of the mean RMSD between the forecasts and the ground truth (averaged over all the DA steps, each outputting a trajectory of length $f$ ). We set $s=20$ and $f=400$ for KS, and $s=5$ and $f=40$ for Kolmogorov, resulting in 13, and 6 DA steps, respectively. The setup is visualised in App. H, and the key findings from the results in Tab. 2 are ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "\u2022 AAO is unsuitable for tasks involving forecasting \u2014 As opposed to the offline setting, AAO fails at online DA, leading to a significantly higher RMSD on both datasets.   \n\u2022 The universal amortised model shows the best performance out of all the models, which is in line with the findings from the previous sections \u2014 it outperforms joint AR in forecasting and performs on par with it in DA. ", "page_idx": 9}, {"type": "table", "img_path": "nQl8EjyMzh/tmp/b79e74f027c9fae7bf8db15f81631a2b3915db00372c1e2324716c502ca9b981.jpg", "table_caption": ["Table 2: RMSD $\\left(\\downarrow\\right)$ (mean $\\pm\\:3$ standard errors over test trajectories) for online DA for the KS and Kolmogorov datasets. The $c$ in AAO $(c)$ refers to the number of corrector steps used. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we explore different ways to condition and sample diffusion models for forecasting and DA. In particular, we empirically demonstrate that diffusion models achieve comparable performance with MSE-trained models in forecasting, yet are significantly more flexible as they can effectively also tackle DA. We theoretically justify and empirically demonstrate the effectiveness of AR sampling for joint models, which is crucial for any task that contains a forecasting component. Moreover, we enhance the flexibility and robustness of amortised models by proposing a new training strategy which also amortises over the history length, and by combining them with reconstruction guidance. ", "page_idx": 9}, {"type": "text", "text": "Limitations and future work. Although achieving competitive results with MSE-trained baselines in forecasting, the models still lack state-of-the-art performance. Thus, they are most practical in settings where flexible conditioning is needed, rather than a standard forecasting task. Although we have explored different solvers and number of discretisation steps, the autoregressive sampling strategy requires simulating a denoising process at each step, which is computationally intensive. We believe that this can be further improved, perhaps by reusing previous sampled states. Additionally, the guided sampling strategy requires tuning the guidance strength, yet we believe there exist some simple heuristics that are able to decrease the sensitivity of this hyperparameter. Finally, we are interested in investigating how the PDE characteristics (i.e. spatial / time discretisation, data volume, frequency spectrum, etc.) influence the behaviour of the diffusion-based models. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "AS, RET and EM are supported by an EPSRC Prosperity Partnership EP/T005386/1 between the EPSRC, Microsoft Research and the University of Cambridge. CD is supported by the Cambridge Trust Scholarship. FB is supported by the Innovation Fund Denmark (0175-00014B) and the Novo Nordisk Foundation through the Center for Basic Machine Learning Research in Life Science (NNF20OC0062606) and NNF20OC0065611. JMHL acknowledges support from a Turing AI Fellowship under grant EP/V023756/1. RET is supported by the EPSRC Probabilistic AI Hub (EP/Y028783/1) and gifts from Google, Amazon, ARM, and Improbable. We thank the anonymous reviewers for their key suggestions and insightful questions that helped improved the quality of the paper. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] William F. Ames. Numerical Methods for Partial Differential Equations. Computer Science and Applied Mathematics. Academic Press, New York, 1977. ISBN 0120567601. URL http://www.worldcat.org/isbn/0120567601.   \n[2] Anima Anandkumar, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Nikola Kovachki, Zongyi Li, Burigede Liu, and Andrew Stuart. Neural operator: Graph kernel network for partial differential equations. In ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations, 2019.   \n[3] Simon Arridge, Peter Maass, Ozan \u00d6ktem, and Carola-Bibiane Sch\u00f6nlieb. Solving inverse problems using data-driven models. Acta Numerica, 28:1\u2013174, 2019. doi: 10.1017/ S0962492919000059.   \n[4] Peter Bauer, Alan Thorpe, and Gilbert Brunet. The quiet revolution of numerical weather prediction. Nature, 525(7567), 2015.   \n[5] Saakaar Bhatnagar, Yaser Afshar, Shaowu Pan, Karthik Duraisamy, and Shailendra Kaushik. Prediction of aerodynamic flow fields using convolutional neural networks. Computational Mechanics, 64(2), 2019.   \n[6] Kaushik Bhattacharya, Bamdad Hosseini, Nikola B. Kovachki, and Andrew M. Stuart. Model Reduction And Neural Networks For Parametric PDEs. The SMAI Journal of computational mathematics, 7:121\u2013157, 2021.   \n[7] Johannes Brandstetter, Max Welling, and Daniel E. Worrall. Lie Point Symmetry Data Augmentation for Neural PDE Solvers. In International Conference on Machine Learning. PMLR, 2022.   \n[8] Johannes Martinus Burgers. A mathematical model illustrating the theory of turbulence. Advances in applied mechanics, 1:171\u2013199, 1948.   \n[9] Salva R\u00fchling Cachay, Bo Zhao, Hailey Joren, and Rose Yu. DYffusion: a dynamics-informed diffusion model for spatiotemporal forecasting. In Advances in Neural Information Processing Systems (NeurIPS), 2023.   \n[10] Patrick Cattiaux, Giovanni Conforti, Ivan Gentil, and Christian L\u00e9onard. Time reversal of diffusion processes under a finite entropy condition, September 2022.   \n[11] Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving Diffusion Models for Inverse Problems using Manifold Constraints. Advances in Neural Information Processing Systems, 35, 2022.   \n[12] Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye. Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems through Stochastic Contraction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.   \n[13] Hyungjin Chung, Jeongsol Kim, Michael T. Mccann, Marc L. Klasky, and Jong Chul Ye. Diffusion Posterior Sampling for General Noisy Inverse Problems. In The Eleventh International Conference on Learning Representations, ICLR, 2023.   \n[14] Steven M Cox and Paul C Matthews. Exponential time differencing for stiff systems. Journal of Computational Physics, 176(2):430\u2013455, 2002.   \n[15] Kieran Didi, Francisco Vargas, Simon V. Mathis, Vincent Dutordoir, Emile Mathieu, Urszula Julia Komorowska, and Pietro Lio. A framework for conditional diffusion modelling with applications in motif scaffolding for protein design. 2023.   \n[16] Tobin A Driscoll, Nicholas Hale, and Lloyd N Trefethen. Chebfun guide, 2014.   \n[17] Yifan Du and Tamer A. Zaki. Evolutional Deep Neural Network. Physical Review E, 104(4), 2021.   \n[18] Bradley Efron. Tweedie\u2019s formula and selection bias. Journal of the American Statistical Association, 106, 2011.   \n[19] Marc Anton Finzi, Anudhyan Boral, Andrew Gordon Wilson, Fei Sha, and Leonardo ZepedaNunez. User-defined Event Sampling and Uncertainty Quantification in Diffusion Models for Physical Dynamical Systems. In International Conference on Machine Learning. PMLR, 2023.   \n[20] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.   \n[21] John Guibas, Morteza Mardani, Zongyi Li, Andrew Tao, Anima Anandkumar, and Bryan Catanzaro. Adaptive Fourier Neural Operators: Efficient Token Mixers for Transformers. In International Conference on Learning Representations, 2021.   \n[22] Jayesh K Gupta and Johannes Brandstetter. Towards multi-spatiotemporal-scale generalized PDE modeling, 2023.   \n[23] Ernst Hairer and Gerhard Wanner. Solving Ordinary Differential Equations II. Stiff and Differential-Algebraic Problems, volume 14. 01 1996. doi: 10.1007/978-3-662-09947-6.   \n[24] Ulrich G Haussmann and Etienne Pardoux. Time reversal of diffusions. The Annals of Probability, 1986.   \n[25] Hans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, Andr\u00e1s Hor\u00e1nyi, Joaqu\u00edn Mu\u00f1ozSabater, Julien Nicolas, Carole Peubey, Raluca Radu, Dinand Schepers, et al. The era5 global reanalysis. Quarterly Journal of the Royal Meteorological Society, 146(730):1999\u20132049, 2020.   \n[26] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, 2020.   \n[27] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. In Deep Generative Models for Highly Structured Data Workshop, ICLR, volume 10, 2022.   \n[28] Tobias H\u00f6ppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, and Andrea Dittadi. Diffusion models for video prediction and infilling. Transactions on Machine Learning Research, 2022. ISSN 2835-8856.   \n[29] Langwen Huang, Lukas Gianinazzi, Yuejiang Yu, Peter D Dueben, and Torsten Hoefler. Diffda: a diffusion model for weather-scale data assimilation. arXiv preprint arXiv:2401.05932, 2024.   \n[30] Aapo Hyv\u00e4rinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005.   \n[31] Victor Isakov. Inverse Problems for Partial Differential Equations. Springer Publishing Company, Incorporated, 3rd edition, 2018. ISBN 3319847104.   \n[32] Christian Jacobsen, Yilin Zhuang, and Karthik Duraisamy. CoCoGen: Physically-Consistent and Conditioned Score-based Generative Models for Forward and Inverse Problems, December 2023.   \n[33] Dmitrii Kochkov, Jamie A. Smith, Ayya Alieva, Qing Wang, Michael P. Brenner, and Stephan Hoyer. Machine learning\u2013accelerated computational fluid dynamics. Proceedings of the National Academy of Sciences, 118(21), 2021. ISSN 0027-8424. doi: 10.1073/pnas.2101784118. URL https://www.pnas.org/content/118/21/e2101784118.   \n[34] Dmitrii Kochkov, Jamie A Smith, Ayya Alieva, Qing Wang, Michael P Brenner, and Stephan Hoyer. Machine learning\u2013accelerated computational fluid dynamics. Proceedings of the National Academy of Sciences, 118(21), 2021.   \n[35] Georg Kohl, Li-Wei Chen, and Nils Thuerey. Turbulent flow simulation using autoregressive conditional diffusion models. arXiv preprint arXiv:2309.01745, 2023.   \n[36] Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural Operator: Learning Maps Between Function Spaces. Journal of Machine Learning Research, 24(89), 2023.   \n[37] Lizao Li, Robert Carver, Ignacio Lopez-Gomez, Fei Sha, and John Anderson. Generative emulation of weather forecast ensembles with diffusion models. Science Advances, 10(13): eadk4489, 2024. doi: 10.1126/sciadv.adk4489.   \n[38] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier Neural Operator for Parametric Partial Differential Equations. In 9th International Conference on Learning Representations, ICLR, 2021.   \n[39] Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. In International Conference on Learning Representations, 2021.   \n[40] Zongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede Liu, Kamyar Azizzadenesheli, and Anima Anandkumar. Physics-informed neural operator for learning partial differential equations. arXiv preprint arXiv:2111.03794, 2021.   \n[41] Wenyuan Liao, Mehdi Dehghan, and Akbar Mohebbi. Direct numerical method for an inverse problem of a parabolic partial differential equation. Journal of Computational and Applied Mathematics, 232(2):351\u2013360, 2009. ISSN 0377-0427. doi: https://doi.org/10. 1016/j.cam.2009.06.017. URL https://www.sciencedirect.com/science/article/ pii/S0377042709003628.   \n[42] Phillip Lippe, Bastiaan S. Veeling, Paris Perdikaris, Richard E. Turner, and Johannes Brandstetter. PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers. In Advances in Neural Information Processing Systems (NeurIPS), 2023.   \n[43] Wing Kam Liu, Shaofan Li, and Harold S. Park. Eighty Years of the Finite Element Method: Birth, Evolution, and Future. Archives of Computational Methods in Engineering, 29(6), 2022.   \n[44] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-Solver $^{++}$ : Fast Solver for Guided Sampling of Diffusion Probabilistic Models, 2023.   \n[45] Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. Nature Machine Intelligence, 3(3), 2021.   \n[46] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.   \n[47] Emile Mathieu, Vincent Dutordoir, Michael J. Hutchinson, Valentin De Bortoli, Yee Whye Teh, and Richard E. Turner. Geometric Neural Diffusion Processes. In Advances in Neural Information Processing Systems (NeurIPS), 2023.   \n[48] Xiangming Meng and Yoshiyuki Kabashima. Diffusion Model Based Posterior Sampling for Noisy Linear Inverse Problems, May 2023.   \n[49] Nicholas H Nelsen and Andrew M Stuart. The Random Feature Model for Input-Output Maps between Banach Spaces. SIAM Journal on Scientific Computing, 43(5), 2021.   \n[50] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32. 2019.   \n[51] Ravi G. Patel, Nathaniel A. Trask, Mitchell A. Wood, and Eric C. Cyr. A physics-informed operator regression framework for extracting data-driven continuum models. Computer Methods in Applied Mechanics and Engineering, 373, January 2021.   \n[52] Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, Pedram Hassanzadeh, Karthik Kashinath, and Animashree Anandkumar. FourCastNet: A Global Datadriven High-resolution Weather Model using Adaptive Fourier Neural Operators, February 2022.   \n[53] Ashwini Pokle, Matthew J. Muckley, Ricky T. Q. Chen, and Brian Karrer. Training-free Linear Image Inversion via Flows, September 2023.   \n[54] Ilan Price, Alvaro Sanchez-Gonzalez, Ferran Alet, Tom R. Andersson, Andrew El-Kadi, Dominic Masters, Timo Ewalds, Jacklynn Stott, Shakir Mohamed, Peter Battaglia, Remi Lam, and Matthew Willson. Gencast: Diffusion-based ensemble forecasting for medium-range weather, 2024.   \n[55] Yongquan Qu, Juan Nathaniel, Shuolin Li, and Pierre Gentine. Deep generative data assimilation in multimodal setting. arXiv preprint arXiv:2404.06665, 2024.   \n[56] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations. 2017.   \n[57] Herbert E Robbins. An empirical bayes approach to statistics. In Breakthroughs in Statistics: Foundations and basic theory, pages 388\u2013394. Springer, 1992.   \n[58] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.   \n[59] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for Biomedical Image Segmentation. In Medical Image Computing and Computer-Assisted Intervention, 18th International Conference,. Springer, 2015.   \n[60] Fran\u00e7ois Rozet and Gilles Louppe. Score-based Data Assimilation. In Advances in Neural Information Processing Systems (NeurIPS), 2023.   \n[61] David Ruhe, Jonathan Heek, Tim Salimans, and Emiel Hoogeboom. Rolling diffusion models. arXiv preprint arXiv:2402.09470, 2024.   \n[62] Daniel Sanz-Alonso, Andrew M Stuart, and Armeen Taeb. Inverse problems and data assimilation. arXiv preprint arXiv:1810.06191, 2018.   \n[63] Dule Shu, Zijie Li, and Amir Barati Farimani. A Physics-informed Diffusion Model for High-fidelity Flow Field Reconstruction. Journal of Computational Physics, 478, 2023.   \n[64] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, 2015.   \n[65] Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-Guided Diffusion Models for Inverse Problems. In International Conference on Learning Representations, 2022.   \n[66] Jiaming Song, Qinsheng Zhang, Hongxu Yin, Morteza Mardani, Ming-Yu Liu, Jan Kautz, Yongxin Chen, and Arash Vahdat. Loss-Guided Diffusion Models for Plug-and-Play Controllable Generation. 2023.   \n[67] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019.   \n[68] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.   \n[69] Antonio Stanziola, Simon Arridge, Ben T. Cox, and Bradley E. Treeby. A research framework for writing differentiable pde discretizations in jax. Differentiable Programming workshop at Neural Information Processing Systems 2021, 2021.   \n[70] Maciej Szudarek, Adam Piechna, Piotr Prusin\u00b4ski, and Leszek Rudniak. Cfd study of high-speed train in crosswinds for large yaw angles with rans-based turbulence models including geko tuning approach. Energies, 15(18), 2022.   \n[71] Jos Torge, Charles Harris, Simon V. Mathis, and Pietro Lio. Diffhopp: A graph diffusion model for novel drug design via scaffold hopping. arXiv preprint arXiv:2308.07416, 2023.   \n[72] Brian L Trippe, Jason Yim, Doug Tischer, Tamara Broderick, David Baker, Regina Barzilay, and Tommi Jaakkola. Diffusion probabilistic modeling of protein backbones in 3d for the motif-scaffolding problem. International Conference on Learning Representations (ICLR), 2023.   \n[73] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7):1661\u20131674, 2011.   \n[74] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St\u00e9fan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, \u02d9Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Ant\u00f4nio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261\u2013272, 2020.   \n[75] Vikram Voleti, Alexia Jolicoeur-Martineau, and Christopher Pal. Mcvd: Masked conditional video diffusion for prediction, generation, and interpolation. In (NeurIPS) Advances in Neural Information Processing Systems, 2022.   \n[76] Michael Vorlaender, Dirk Schr\u00f6der, S\u00f6nke Pelzer, and Frank Wefers. Virtual reality for architectural acoustics. Journal of Building Performance Simulation, 8, 01 2015.   \n[77] Sifan Wang, Hanwen Wang, and Paris Perdikaris. Learning the solution operator of parametric partial differential equations with physics-informed deeponets. Science advances, 7(40), 2021.   \n[78] Joseph L. Watson, David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, Andrew J. Borst, Robert J. Ragotte, Lukas F. Milles, Basile I. M. Wicky, Nikita Hanikel, Samuel J. Pellock, Alexis Courbet, William Sheffler, Jue Wang, Preetham Venkatesh, Isaac Sappington, Susana V\u00e1zquez Torres, Anna Lauko, Valentin De Bortoli, Emile Mathieu, Regina Barzilay, Tommi S. Jaakkola, Frank DiMaio, Minkyung Baek, and David Baker. Broadly applicable and accurate protein design by integrating structure prediction networks and diffusion generative models. bioRxiv, 2022.   \n[79] Joseph L. Watson, David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, Andrew J. Borst, Robert J. Ragotte, Lukas F. Milles, Basile I. M. Wicky, Nikita Hanikel, Samuel J. Pellock, Alexis Courbet, William Sheffler, Jue Wang, Preetham Venkatesh, Isaac Sappington, Susana V\u00e1zquez Torres, Anna Lauko, Valentin De Bortoli, Emile Mathieu, Sergey Ovchinnikov, Regina Barzilay, Tommi S. Jaakkola, Frank DiMaio, Minkyung Baek, and David Baker. De novo design of protein structure and function with RFdiffusion. Nature, 2023.   \n[80] Kevin E Wu, Kevin K Yang, Rianne van den Berg, James Y Zou, Alex X Lu, and Ava P Amini. Protein structure generation via folding diffusion. arXiv preprint arXiv:2209.15611, 2022.   \n[81] Gefan Yang and Stefan Sommer. A denoising diffusion model for fluid field prediction. arXiv e-prints, pages arXiv\u20132301, 2023.   \n[82] Qinsheng Zhang and Yongxin Chen. Fast Sampling of Diffusion Models with Exponential Integrator. In The Eleventh International Conference on Learning Representations, ICLR. OpenReview.net, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "[83] Yinhao Zhu and Nicholas Zabaras. Bayesian deep convolutional encoder\u2013decoder networks for surrogate modeling and uncertainty quantification. Journal of Computational Physics, 366, August 2018. ", "page_idx": 15}, {"type": "text", "text": "On conditional diffusion models for PDE simulations (Appendix) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A Organisation of the supplementary ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this supplementary, we first motivate in App. B the choice made regarding the (local) score network given the Markov assumption on the sequences. We then provide more reference to related work in App. C, followed by a description of the data generating process in App. D for the datasets used in the experiments in Sec. 5. We also provide further implementation details. After, in App. E, we assess the ability of the all-at-once sampling strategy\u2014which relies on reconstructing the full score from the local ones\u2014to perform accurate forecast, and in particular show the crucial role of correction steps in the discretisation. We then provide more results on forecasting in App. F, including a study of the influence of the guidance strength hyperparameter $\\gamma$ and of the conditioning scenario, a comparison with widely used benchmarks in forecasting, such as climatology and persistence, and results for different U-Net architectures. In App. G we show more experimental evidence on the offline DA task, including the influence of the guidance strength and of the conditioning scenarios, as well as perform a comparison between an AR model and an AAO model with twice higher Markov order. We also include a comparison with an interpolation baseline. In App. H we provide further experimental evidence for the online DA task for a range of conditioning scenarios for the joint and amortised models. In App. I we provide frequency spectra of the trajectories generated by our methods and the forecasting baselines, and in App. J we analyse the long-term behaviour of the diffusion models. Finally, we provide pseudo-code in App. K. ", "page_idx": 16}, {"type": "text", "text": "B Markov score ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we look at the structure of the score induced by a Markov time series. In particular, we lay out different ways to parameterise and train this score with a neural network. In what follows we assume an AR-1 Markov model for the sake of clarity, but the reasoning developed trivially generalises to AR- $k$ Markov models. Assuming an AR-1 Markov model, we have that the joint density factorise as ", "page_idx": 16}, {"type": "equation", "text": "$$\np(x_{1:L})=p(x_{1})\\prod_{t=2}^{T}p(x_{i}|x_{i-1}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Looking at the score of (9) for intermediary states we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\nabla_{x_{i}}\\log p(x_{1:L})=\\nabla_{x_{i}}\\log p(x_{i+1}|x_{i})+\\nabla_{x_{i}}\\log p(x_{i}|x_{i-1})}&{}\\\\ {=\\nabla_{x_{i}}\\log p(x_{i+1},x_{i}|x_{i-1})}&{}\\\\ {=\\nabla_{x_{i}}\\log p(x_{i+1},x_{i}|x_{i-1})+\\underbrace{\\nabla_{x_{i}}\\log p(x_{i-1})}_{0}}&{}\\\\ {=\\nabla_{x_{i}}\\log p(x_{i+1},x_{i},x_{i-1})}&{}\\\\ {=\\nabla_{x_{i}}\\log p(x_{i-1},x_{i},x_{i+1}).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similarly, for the first state, we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{x_{1}}\\log p(x_{1:L})=\\nabla_{x_{1}}\\log p(x_{1})+\\nabla_{x_{1}}\\log p(x_{2}|x_{1})}\\\\ &{\\qquad\\qquad\\qquad=\\nabla_{x_{1}}\\log p(x_{1},x_{2})}\\\\ &{\\qquad\\qquad\\qquad=\\nabla_{x_{1}}\\log p(x_{1},x_{2})+\\underbrace{\\nabla_{x_{1}}\\log p(x_{3}|x_{2})}_{0}}\\\\ &{\\qquad\\qquad\\qquad=\\nabla_{x_{1}}\\log p(x_{1},x_{2})+\\nabla_{x_{1}}\\log\\underbrace{p(x_{3}|x_{2},x_{1})}_{\\mathrm{since}=p(x_{3}|x_{2})}}\\\\ &{\\qquad\\qquad\\qquad=\\nabla_{x_{1}}\\log p(x_{1},x_{2},x_{3}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and for the last state ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{x_{L}}\\log p(x_{1:L})=\\nabla_{x_{L}}\\log p(x_{L}|x_{L-1})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\nabla_{x_{L}}\\log\\underbrace{p(x_{L}|x_{L-1},x_{L-2})}_{\\mathrm{since}=p(x_{L}|x_{L-1})}}\\\\ &{\\qquad\\qquad\\qquad=\\nabla_{x_{L}}\\log p(x_{L}|x_{L-1},x_{L-2})+\\underbrace{\\nabla_{x_{L}}\\log p(x_{L-1},x_{L-2})}_{0}}\\\\ &{\\qquad\\qquad\\qquad=\\nabla_{x_{L}}\\log p(x_{L},x_{L-1},x_{L-2})}\\\\ &{\\qquad\\qquad\\qquad=\\nabla_{x_{L}}\\log p(x_{L-2},x_{L-1},x_{L}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Noised sequence. Denoising diffusion requires learning the score of noised states. The sequence $x_{1:L}$ is noised with the process defined in (1) for an amount of time $t$ : $x_{1:L}(t)|x_{1:L}\\sim p_{t|0}$ . With $x_{1:L}$ an AR- $k$ Markov model there is no guarantee that $x_{1:L}(t)$ is still $k$ -Markov. Yet, we can assume that $x_{1:L}(t)$ is an AR- $k^{\\prime}$ Markov model where $k^{\\prime}>k$ , as in Rozet and Louppe [60]. Although it may seem in contradiction with the previous statement, in what follows we focus on the AR-1 assumption since derivations and the general reasoning is much easier to follow, yet reiterate that this generalises to the order $k$ . ", "page_idx": 17}, {"type": "text", "text": "Local score. Eqs. (11), (12) and (14) suggest training a score network to fit $\\begin{array}{r l r}{{\\bf s}_{\\theta}(t,x_{i-1}(t),x_{i}(t),\\bar{\\bf\\Delta}_{x_{i+1}}(t))}&{{}\\approx}&{\\nabla\\log p_{t}(x_{i-1}\\bar{(t)},x_{i}(t),x_{i+1}(t))}\\end{array}$ by sampling tuples $x_{i-1},x_{i},x_{i+1}\\sim p_{0}$ and noising them. ", "page_idx": 17}, {"type": "text", "text": "Full score. The first, intermediary and last elements of the score are then respectively given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\;\\nabla_{x_{1}(t)}\\log p_{t}(x_{1:L}(t))\\approx\\mathbf{s}_{\\theta}(t,x_{1}(t),x_{2}(t),x_{3}(t))[1]}\\\\ &{\\bullet\\;\\nabla_{x_{i}(t)}\\log p_{t}(x_{1:L}(t))\\approx\\mathbf{s}_{\\theta}(t,x_{i-1}(t),x_{i}(t),x_{i+1}(t))[2]}\\\\ &{\\bullet\\;\\nabla_{x_{L}(t)}\\log p_{t}(x_{1:L}(t))\\approx\\mathbf{s}_{\\theta}(t,x_{L-2}(t),x_{L-1}(t),x_{L}(t))[3]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Joint sampling. The full score over the sequence $x_{1:L}$ can be reconstructed via the trained local score network taking segments of length 3 as input. This allows for sampling from the joint. All of the above generalise to AR- $k$ models, where the local score network takes a segment of length $2k+1$ as input. ", "page_idx": 17}, {"type": "text", "text": "Guided AR. Alternatively for forecasting, having learnt a local score network ${\\bf s}_{\\theta}(t,x_{i-1}(t),x_{i}(t),x_{i+1}(t))$ , i.e. modelling joints $p(x_{i-1},x_{i},x_{i+1})$ , we can condition on $x_{i-1}$ , $x_{i}$ to sample $x_{i+1}$ leveraging reconstruction guidance\u2014as described in Sec. 3.3. Effectively, this induces an AR-2\u2014instead of AR-1\u2014 Markov model, or in general, an AR- $2k$ \u2014instead of $k$ \u2014Markov model. As such, this score parameterisation takes more input than strictly necessary to satisfy the $k$ Markov assumption. ", "page_idx": 17}, {"type": "text", "text": "Amortised AR. Obviously we note that, looking at (10) and (13), one could learn a conditional score network amortised on the previous value such that $\\begin{array}{r l}{\\mathbf{s}_{\\theta}(t,x_{i}(t)|x_{i-1}(0))}&{\\approx}\\end{array}$ $\\nabla_{x_{i}(t)}\\log{p(x_{i}(t)|x_{i-1}(0))}$ . This unsurprisingly allows forecasting with autoregressive rollout as in Sec. 3.3. Additionally learning a model over $p(x_{1})$ and summing pairwise the scores as in (10) would also allow for joint sampling. ", "page_idx": 17}, {"type": "text", "text": "In App. B.1, we started with an AR- $k$ model, but ended up with learning a local score taking $2k+1$ inputs. Below we derive an alternative score parameterisation. Still assuming an AR-1 model (9), we have that the score for intermediary states can be expressed as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\nabla_{x_{i}}\\log p(x_{1:L})=\\nabla_{x_{i}}\\log p(x_{i+1}|x_{i})+\\nabla_{x_{i}}\\log p(x_{i}|x_{i-1})}&{}\\\\ {=\\underbrace{\\nabla_{x_{i}}\\log p(x_{i}|x_{i+1})-\\nabla_{x_{i}}\\log p(x_{i})}_{\\mathrm{via~Baye~'neule}}+\\nabla_{x_{i}}\\log p(x_{i}|x_{i-1})+\\underbrace{\\nabla_{x_{i}}\\log p(x_{i-1})}_{\\mathrm{(0)}}}&{}\\\\ {=\\nabla_{x_{i}}\\log p(x_{i}|x_{i+1})+\\underbrace{\\nabla_{x_{i}}\\log p(x_{i+1})}_{\\mathrm{(0)}}-\\nabla_{x_{i}}\\log p(x_{i})+\\nabla_{x_{i}}\\log p(x_{i},x_{i-1})}&{}\\\\ {=\\nabla_{x_{i}}\\log p(x_{i},x_{i+1})-\\nabla_{x_{i}}\\log p(x_{i})+\\nabla_{x_{i}}\\log p(x_{i},x_{i-1}).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Local score. From (15), we notice that we could learn a pairwise score network such that $\\begin{array}{r}{\\tilde{\\mathbf{s}}_{\\theta}(t,x_{i-1}(t),x_{i}(t))\\approx\\nabla_{x_{i}(t)}\\log p(x_{i-1}(t),x_{i}(t))}\\end{array}$ , and $\\tilde{\\mathbf{s}}_{\\theta}(t,x_{i}^{\\bar{i}}(t))\\approx\\nabla_{x_{i}(t)}\\log p(x_{i}(t))$ , trained by sampling tuples $x_{i-1},x_{i},x_{i+1}\\sim p_{0}$ and noising them. Note that this requires 3 calls to the local score network instead of 1 as in App. B.1. ", "page_idx": 18}, {"type": "text", "text": "For $k>1$ . ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\nabla_{x_{i}}\\log p(x_{1:L})=\\nabla_{x_{i}}\\log p(x_{i-k},\\ldots,x_{i-1},x_{i},x_{i+1},\\ldots,x_{i+k})}}\\\\ &{=\\nabla_{x_{i}}\\log p(x_{i}|x_{i-k:i-1})+\\displaystyle\\sum_{j=i+1}^{i+k}\\nabla_{x_{i}}\\log p(x_{j}|x_{j-k:j-1})}\\\\ &{=\\nabla_{x_{i}}\\log p(x_{i-k:i})+\\displaystyle\\sum_{j=i+1}^{i+k}\\nabla_{x_{i}}\\log p(x_{j-k:j})-\\displaystyle\\sum_{m=i}^{i+k-1}\\nabla_{x_{i}}\\log p(x_{m-k+1:m})}\\\\ &{=\\displaystyle\\sum_{j=i}^{i+k}\\nabla_{x_{i}}\\log p(x_{j-k:j})-\\displaystyle\\sum_{m=i}^{i+k-1}\\nabla_{x_{i}}\\log p(x_{m-k+1:m})}\\end{array}\\quad(1<x).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "From (16), we obtained a $2k+1$ window size model, while having the scores with window size of $k$ and $k+1$ . This would require roughly twice more forward passes during AAO sampling, but will double the effective window size. ", "page_idx": 18}, {"type": "text", "text": "Full score. The first, intermediary and last elements of the score are then respectively given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{x_{1}(t)}\\log{p(x_{1:L}(t))}=\\nabla_{x_{1}(t)}\\log{p(x_{1}(t))}+\\nabla_{x_{1}(t)}\\log{p(x_{2}(t)|x_{1}(t))}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\nabla_{x_{1}(t)}\\log{p(x_{1}(t),x_{2}(t))}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\triangleq\\tilde{\\mathbf{s}}_{\\theta}(t,x_{1}(t),x_{2}(t))[1].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{x_{i}(t)}\\log{p(x_{1:L}(t))}=\\mathbf{s}_{\\theta}(t,x_{i-1}(t),x_{i}(t),x_{i+1}(t))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\triangleq\\tilde{\\mathbf{s}}_{\\theta}(t,x_{i}(t),x_{i+1}(t))[1]+\\tilde{\\mathbf{s}}_{\\theta}(t,x_{i-1}(t),x_{i}(t))[2]-\\tilde{\\mathbf{s}}_{\\theta}(t,x_{i}(t)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\nabla_{x_{L}(t)}\\log{p(x_{1:L}(t))}=\\nabla_{x_{L}(t)}\\log{p(x_{L}(t)|x_{L-1}(t))}}&{}\\\\ {=\\nabla_{x_{L}(t)}\\log{p(x_{L}(t),x_{L-1}(t))}}&{}\\\\ {=\\tilde{\\mathbf{s}}_{\\theta}(t,x_{L-1}(t),x_{L}(t))[2].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Joint sampling. The full score over $x_{1:L}(t)$ could therefore be reconstructed from these, enabling joint sampling. ", "page_idx": 18}, {"type": "text", "text": "AR. Additionally, autoregressive forecasting could be achieved with guidance by iterating over pairs $(x_{i-1},x_{i})$ , thus using a local score of window size $k+1$ , in contrast to App. B.1 which requires an inflated window of size $2k+1$ . ", "page_idx": 18}, {"type": "text", "text": "C More related work ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Sec. 4, we presented relevant works that focus on machine learning models and more specifically diffusion models for PDE modelling. Our work builds upon different conditional generation protocols used for conditional sampling in diffusion models. Sec. 2 presents the techniques we considered for our models. In the following, we summarise all the available methods for building a conditional diffusion model. ", "page_idx": 19}, {"type": "text", "text": "Conditional diffusion models. The development of conditional generation methods for diffusion models is an active area of research, whereby a principled approach is to frame this as the problem of simulating a conditional denoising process which converges to the conditional of interest when fully denoised [68, 65, 12]. One approach is to leverage Bayes\u2019s rule, and combine a pretrained score network, with an estimate of the conditional over observation given noised data\u2014referred to as a guidance term [13, 48, 66, 65, 27]. Several refinements over the estimation of this guidance have been suggested [60, 19]. Another line of work has focused on \u2018inpainting\u2019 tasks\u2014problems where the observations lie in a subspace of the diffused space\u2014and suggested \u2018freezing\u2019 the conditioning data when sampling [68, 72, 46, 47]. Lastly, another popular approach, sometimes referred to as classifier-free guidance, is to directly learn the conditional score amortised over the observation of interest [26, 79, 71]. This approach requires having access at training time to pairs of data and observation. ", "page_idx": 19}, {"type": "text", "text": "Machine learning models for PDEs. One class of ML-based methods that can learn to approximately solve PDEs given some initial state assumes a fixed finite-dimensional spatial discretisation (i.e. mesh), typically a regular grid, and, leveraging convolutional neural networks, learns to map initial values of the grid to future states [20, 83, 5]. These are data-driven methods, which are trained on trajectories generated by conventional solvers, typically at fine resolution but then downscaled. ", "page_idx": 19}, {"type": "text", "text": "Alternatively, mesh-free approaches learn infinite-dimensional operators via neural networks, coined neural operators [2, 36, 45, 51, 49, 6]. These methods are able to map parameters along with an initial state to a set of neural network parameters that can then be used for simulation at a different discretisation. Neural operators typically achieve this by leveraging the Fourier transform and working in the spectral domain [38, 21, 52]. Fourier neural operators (FNOs) can be used as the backbone of the diffusion model, but U-Net-based [59] models have been shown to outperform them [22]. ", "page_idx": 19}, {"type": "text", "text": "Classical solvers for PDEs. Conventional numerical methods, e.g. finite element methods [43], are commonly used to solve time-dependent PDEs in practice [1]. In contrast to ML-based methods, numerical methods have the advantage of providing theoretical guarantees of convergence and bounds on error, which depend on time and space discretisation. However, many real-world problems, e.g. fluid and weather modelling, are described by complex time-dependent PDEs, and thus to obtain accurate and reliable solutions, the space and time discretisations have to be very small [23]. As such, in this case, the conventional methods require significant computational and time resources. To tackle this issue, several recent works [33, 69] attempted to speed up numerical solvers using automatic differentiation and hardware accelerators (GPU/TPU). ", "page_idx": 19}, {"type": "text", "text": "While conventional numerical methods have been intensively studied and applied in forecasting, they are not suitable for solving inverse problems out-of-the-box. To overcome this, several works [31, 41] propose numerical approaches specifically developed for certain PDEs, while Arridge et al. [3] provides an overview on data-driven, regularisation-based solutions to inverse problems. Moreover, unlike diffusion models, numerical solvers do not offer a probabilistic treatment, which is crucial in certain applications of PDE modelling. For example, in weather and climate modelling, being able to predict a range of probable weather scenarios is key in decision-making, such as issuing public hazards warnings and planning renewable energy use [54]. ", "page_idx": 19}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/a0284450dcf409d987835327a5d3f72a95ddd6f5974dc837dc5919df13e40479.jpg", "img_caption": ["Figure 4: True test trajectories from solving the Burgers\u2019 equation following the setup of [77]. In this case, both training, validation, and test trajectories have the same length. We used $\\Delta\\tau=0.01s$ , so the trajectory contains states from time $\\tau=0s$ to $\\tau=1s$ . The spatial dimension is discretized in 128 evenly distributed states "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "D Data generation and implementation details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "D.1 Burgers\u2019 equation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Ground truth trajectories for the Burgers\u2019 equation [8] ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\partial u}{\\partial\\tau}+u\\frac{\\partial u}{\\partial z}=\\nu\\frac{\\partial^{2}u}{\\partial z^{2}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "are obtained from the open-source dataset made available by Li et al. [40], where they consider $(x,\\tau)\\in(0,1)\\times(0,1]$ . The dataset consists of 1200 trajectories of 101 timesteps, i.e. $\\Delta\\tau=0.01$ , and where the spatial discretisation used is 128. 800 trajectories were used as training set, 200 as validation set and the remaining 200 as test set. These trajectories were generated following the setup described in [77] using the code available in their public repository. Initial conditions are sampled from a Gaussian random field defined as $\\mathcal{N}(0,625^{\\circ}.(-\\Delta+5^{2}I)^{-4}\\overset{\\circ}{\\rightharpoondown}$ (although be aware that in the paper they state to be using $\\mathcal{N}(0,25^{2}(-\\Delta+5^{2}I)^{-4})$ , which is different from what the code suggests) and then they integrate the Burgers\u2019 equation using spectral methods up to $\\tau=1$ using a viscosity of 0.01. Specifically, they solve the equations using a spectral Fourier discretisation and a fourth-order stiff time-stepping scheme (ETDRK4) [14] with step-size $10^{-4}$ using the MATLAB Chebfun package [16]. Examples of training, validation, and test trajectories are shown in Fig. 4. ", "page_idx": 20}, {"type": "text", "text": "D.2 1D Kuramoto-Sivashinsky ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We generate the ground truth trajectories for the 1D Kuramoto-Sivashinsky equation ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\partial u}{\\partial\\tau}+u\\frac{\\partial u}{\\partial z}+\\frac{\\partial^{2}u}{\\partial z^{2}}+\\nu\\frac{\\partial^{4}u}{\\partial z^{4}}=0,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "following the setup of Lippe et al. [42], which is based on the setup defined by Brandstetter et al. [7]. In contrast to Lippe et al. [42], we generate the data by keeping $\\Delta x$ and $\\Delta\\tau$ fixed, i.e. using a constant time and spatial discretisation2. The public repository of Brandstetter et al. [7] can be used to generate our dataset by using the same arguments as the one defined in Lippe et al. [42, App D1.]. ", "page_idx": 20}, {"type": "text", "text": "Brandstetter et al. [7] solves the KS-equation with periodic boundaries using the method of lines, with adreef irnaendd oovme r ctoruefnfcicaiteendt sF oreuprireers esenrtiiensg,  ti.hee. $\\begin{array}{r}{u_{0}(x)=\\sum_{k=1}^{K}A_{k}\\sin(2\\pi l_{k}x/L+\\phi_{k})}\\end{array}$ ,e  wphhaesree $A_{k},l_{k},\\phi_{k}_{k}$ shift, and the space-dependent frequency. The viscosity parameter is set to $\\nu=1$ . Data is initially generated with float64 precision and then converted to float32 for training the different models. We generated 1024 trajectories of $140\\Delta\\tau$ , where $\\Delta\\tau=0.2s$ , as training set, while the validation and test set both contain 128 trajectories of length $640\\Delta\\tau$ . The spatial domain is discretised into 256 evenly spaced points. Trajectories used to train and evaluate the models can be seen in Fig. 5. ", "page_idx": 20}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/47ad1bd52eca275570530dff16127d8b3836554e69df434be4252236c34dd440.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 5: Examples from the Kuramoto-Sivashinsky dataset. Training trajectories are shorter than those used to evaluate the models. Training trajectories have 140 time steps generated every $\\Delta\\tau=0.2s$ , i.e. training trajectories contain examples from $0s$ to $28s$ . Validation and test trajectories, instead, show the evolution of the equation for 640 steps, i.e. from $0s$ to $128s$ . The spatial dimension is discretized in 256 evenly distributed states. ", "page_idx": 21}, {"type": "text", "text": "D.3 2D Kolmogorov-Flow equation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We use the setup described by [60] and their public repository to generate ground truth trajectories for the Kolmogorov-flow equation. Their implementation relies on the $\\mathrm{j}$ ax-cfd library [34] for solving the Navier-Stokes equation. As we have seen in Section 5, the Kolmogorov equation is defined as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\frac{\\partial{\\bf u}}{\\partial t}+{\\bf u}\\cdot\\nabla{\\bf u}-\\nu\\Delta^{2}{\\bf u}+\\frac{1}{\\rho}\\nabla p-f=0}}\\\\ {\\displaystyle{{\\nabla\\cdot\\bf u}=0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\nu>0$ is the viscosity also defined as $\\frac{1}{R e}$ with $R e$ being the Reynolds number, $\\rho$ is the fluid density, $p$ the pressure field, and $f$ is an external forcing term. For the data generation they follow Kochkov et al. [34] setup, which used $R e\\mathrm{~=~}1000$ , or $\\nu=0.001$ respectively, a constant density $\\rho=1$ and $f=\\sin(4y)\\hat{\\mathbf{x}}-0.1\\mathbf{u}$ where the first term can be seen as a forcing term and the second one as a dragging term. They solve the equation on a $256\\times256$ domain grid defined on $[0,2\\pi]^{2}$ with periodic boundaries conditions but then they coarsen the generated states into a $64\\times64$ resolution. While other works in the literature directly model the vorticity, i.e. the curl of the velocity field, they directly model the velocity field $\\mathbf{u}$ . They simulate Eq. (17) starting from different initial states for 128 steps and they keep only the last 64 states. The training set we used contains 819 trajectories, the validation set contains 102 trajectories, and we test all the different models on 50 test trajectories. We show in Fig. 6 some example states from the training, validation, and test sets, corresponding to a time step $\\bar{\\in}\\{\\Delta\\tau,8\\Delta\\tau,16\\bar{\\Delta}\\tau,24\\Delta\\tau,32\\Delta\\tau,40\\Delta\\tau,4\\bar{8}\\Delta\\tau,56\\Delta\\tau,64\\Delta\\tau\\}.$ . ", "page_idx": 21}, {"type": "text", "text": "D.4 Implementation details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Our implementation is built on PyTorch [50] and the codebase will soon be made publicly available. For generating samples we simulate the probabilistic ordinary differential equation (ODE) associated with the backward or denoising process which admits the same marginals as $\\overleftarrow{x}(t)$ , referred to as the probability flow ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\bar{x}(t)=\\left\\{-\\frac{1}{2}\\bar{x}(t)-\\frac{1}{2}\\nabla\\log p_{t}(\\bar{x}(t))\\right\\}\\mathrm{d}t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "using the DPM solver [44] in 128 evenly spaced discretisation steps. As a final step, we return the posterior mean over the noise-free data via Tweedie\u2019s formula. Unless specified otherwise, we set the guidance schedule to $r_{t}^{2}=\\gamma\\sigma_{t}^{2}/\\mu_{t}^{2}$ [19, 60], tune $\\gamma$ via grid-search and set $\\sigma_{y}=0.01$ . ", "page_idx": 21}, {"type": "text", "text": "In the following, we present all the hyperparameters used for the U-Net that parameterises the score function. For the forecasting task, we studied two different architectures for KS and Kolmogorov\u2014 one considered in [60] (SDA) and another one in [42] (PDE-Refiner). Tab. 3 contains the architecture and training parameters for the SDA architecture, while Tab. 4 contains the parameters used for the PDE-Refiner architecture. For KS, the SDA and PDE-Refiner architectures have very similar capacity in terms of the number of trainable parameters. The only difference is in the way they encode the diffusion time $t$ : SDA encodes it as a separate channel that is concatenated to the input, while PDE-Refiner encodes $t$ into the parameters of the LayerNorm. For Kolmogorov, the architectures considered were different in terms of the number of residual blocks per level and channels per level, resulting in the PDE-Refiner architecture having more capacity than the SDA architecture. ", "page_idx": 21}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/6dad4ac67ce78a57c32636c2304b59fbabde9da82fd3bef6a3c29e7c25a607ef.jpg", "img_caption": ["Figure 6: Examples from the Kolmogorov dataset. Training, validation, and test trajectories all have 64 time steps. For each example, we only show subset of the states, corresponding to $t\\in$ $\\{\\Delta\\tau,8\\Delta\\tau,16\\Delta\\bar{\\tau_{\\parallel}}24\\Delta\\tau,32\\Delta\\tau,40\\bar{\\Delta\\tau_{\\parallel}}48\\Delta\\tau,56\\Delta\\tau,64\\Delta\\tau\\}$ "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "The results in Figs. 1 and 2 correspond to the best-performing architecture for each model, but we show the full set of results in Tab. 5. The results for the data assimilation experiments are all obtained using the SDA architecture. ", "page_idx": 22}, {"type": "text", "text": "Baselines implementation details. In the forecasting experiment of Sec. 5, we tested the different score-based models against other ML-based models, including the state-of-the-art PDE-Refiner [42] and an MSE-trained U-Net and Fourier Neural Operator (FNO) [39]. For the implementation of the baselines, we used the following open-source code, all using the PDE-Refiner architecture from Tab. 4. In addition, for the MSE-trained U-Net, we also implemented a model using the SDA architecture, with results shown in Tab. 5. ", "page_idx": 22}, {"type": "text", "text": "Hardware. All models and the experiments we present in this paper were run on a single GPU. We used a GPU cluster with a mix of RTX 2080 Ti, RTX A5000, and Titan X GPUs. The majority of the GPUs we used have 11GB of memory. A direct map between experiments and a specific GPU was not tracked, therefore we cannot report these details. The results in Fig. 3 are computed on a RTX 2080 Ti GPU with 11GB of memory. ", "page_idx": 22}, {"type": "text", "text": "E All-at-once (AAO) joint sampling and study of solvers in forecasting ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "One approach for generating long rollouts conditioned on some observations using score-based generative modelling is to directly train a joint posterior score $s_{\\theta}(t,x_{1:L}(t)|y)$ to approximate $\\bar{\\nabla}_{x_{1:L}(t)}\\log p(x_{1:L}(t)|y)$ . The posterior can be composed from a prior score and a likelihood term, just as in the main paper. However, this approach cannot be used for generating long rollouts (large $L$ ), as the score network becomes impractically large. Moreover, depending on the dimensionality of each state, with longer rollouts the probability of running into memory issues increases significantly. To deal with this issue, in this section we investigate the approach proposed by Rozet and Louppe [60], whereby the prior score $(s_{\\theta}(t,x_{1:L}(t)))$ is approximated with a series of local scores $(s_{\\theta}(\\bar{t},x_{i-k:i+k}(t)))$ , which are easier to learn. The size of these local scores is in general significantly smaller than the length of the generated trajectory and will be denoted as the window of the model $W$ . ", "page_idx": 22}, {"type": "table", "img_path": "nQl8EjyMzh/tmp/fb677817e531e70a91af79a6c5eae7e7ed3f088c68c7d5d666776f94c1c379b8.jpg", "table_caption": ["Table 3: Hyperparameters used for the SDA U-Net architecture inspired by [60] in the three datasets considered in the paper. Blue refers to specific information about the joint model and red refers to the universal amortised model. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "nQl8EjyMzh/tmp/967e282c79ba7742ad22838e6d643ec2536c1825b8a8bfdac12fae8d11d2c438.jpg", "table_caption": ["Table 4: Hyperparameters used for the PDE-Refiner U-Net architecture inspired by [42] in the forecasting experiment for the KS and Kolmogorov datasets. Blue refers to specific information about the joint model and red refers to the universal amortised model. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "At sampling time, the entire trajectory is generated all-at-once, with the ability to condition on initial and/or intermediary states. We study two sampling strategies. The first one is based on the exponential integrator (EI) discretization scheme [82], while the second one uses the $\\mathrm{DPM++}$ solver [44]. For each strategy, the predictor steps can be followed by a few corrector steps (Langevin Monte Carlo) [68, 47]. For the results below, we consider the forecasting scenario for the Burgers\u2019 dataset. We forecast trajectories of 101 states, conditioned on a noisy version of the initial 6 states (with standard deviation $\\sigma_{y}=0.01)$ ). The results are computed based on 30 test trajectories, each with different initial conditions. The sampling procedure uses 128 diffusion steps and a varying number of corrector steps. ", "page_idx": 23}, {"type": "text", "text": "Influence of solver and time scheduling. We show the results using two solvers: EI [82] and $\\mathrm{DPM++}$ [44]. For each, we study two time scheduling settings, which determine the spacing between the time steps during the sampling process ", "page_idx": 23}, {"type": "text", "text": "\u2022 Linear/uniform spacing: $\\begin{array}{r}{t_{i}=t_{N}-(t_{N}-t_{0})\\frac{i}{N}}\\end{array}$ \u2022 Quadratic: $\\begin{array}{r}{t_{i}=(\\frac{N-i}{N}t_{0}^{1/\\kappa}+\\frac{i}{N}t_{N}^{1/\\kappa})^{\\kappa}}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "where $N$ indicates the number of diffusion steps, $t_{N}$ and $t_{0}$ are chosen to be 1 and $10^{-3}$ and $\\kappa$ is chosen to be 2. ", "page_idx": 24}, {"type": "text", "text": "Fig. 7 shows the RMSD between the generated samples and the true trajectories, averaged across the trajectory length. It indicates that for AAO sampling with 0 corrections, there is little difference between the two time schedules, and that $\\mathrm{DPM++}$ slightly outperforms EI in terms of RMSD. However, the differences are not significant when taking into account the error bars. As the number of corrections is increased, the difference in performance between the two solvers gets negligible. However, it does seem that with a large number of corrections (25), the quadratic time schedule leads to lower RMSD. Nevertheless, given that, in general, only $3-5$ corrector steps are used, we argue that the choice of solver and time schedule has a small impact on overall performance. ", "page_idx": 24}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/2da3eeb680289a8191c3b3b1883086301cbb2d2e7a7e7c8945c742a260a3b2a0.jpg", "img_caption": ["Figure 7: RMSD for Burgers\u2019 for the two solvers $\\scriptstyle\\left(\\mathrm{DPM}++\\right.\\kern-\\nulldelimiterspace}$ and EI), and the two time schedules (quadratic and linear) used. The results are shown for 0 (left), 5 (middle), and 25 (right) corrector steps. When using no/only a few corrector steps, the RMSD is not highly impacted by the choice of solver and time schedule. However, at 25 corrector steps, the quadratic time schedule seems to lead to lower RMSD. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Influence of corrector steps and window size. Figs. 8 and 9 illustrate the effect of the window size and number of corrector steps on the mean squared error (MSE) and the correlation coefficient (as compared to the ground truth samples). Based on the findings from above, here we used the $\\mathrm{DPM++}$ solver with quadratic time discretisation. ", "page_idx": 24}, {"type": "text", "text": "The corrector steps seem to be crucial to prevent the error accumulation, with performance generally improving with increasing corrections for both models (window 5 and 9). However, this comes at a significantly higher computational cost, as each correction requires one extra function evaluation. Unsurprisingly, the bigger model (window 9) is better at capturing time dependencies between states, but it comes with increased memory requirements. ", "page_idx": 24}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/5413bde35b79d6ac13237c88c25c9581ab6c1734f94621ef0bb480916a9a33a7.jpg", "img_caption": ["Figure 8: Evolution of the MSE for Burgers\u2019 along the trajectory length for a window of 5 (left) and 9 (right). The error bars indicate $\\pm\\,3$ standard errors. Each line corresponds to a different number of corrector steps. Unsurprisingly, the window 9 model performs better than the window 5 model (when using the same number of corrector steps). Performing corrector steps, alongside predictor steps, is crucial for preventing errors from accumulating. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Finally, the findings from above are also confirmed in Fig. 10, which illustrates that the predictive performance increases with the window size and the number of corrector steps. Both of these come at an increased computational cost; increasing the window size too much might lead to memory issues, while increasing the number of corrections has a negative impact on the sampling time. ", "page_idx": 24}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/1a81e315027b9ba475335a637b259536b6515ccebc0c735298c6f49bdb67e61c.jpg", "img_caption": ["Figure 9: Evolution of the Pearson correlation coefficient for Burgers\u2019 along the trajectory length for a window of 5 (left) and 9 (right). The error bars indicate $\\pm\\:3$ standard errors. The findings are entirely consistent with the ones suggested by the MSE results. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/24cb14381b8d2f8b75a7a65e65dcf579cc192f0d6073f13599e8894637629fcb.jpg", "img_caption": ["Figure 10: RMSD for Burgers\u2019 as a function of corrector steps for the two studied models (window 5 and window 9). Increasing both the window size and the number of corrections positively impacts the predictive performance. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "F Additional experimental results on forecasting ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "This section shows further experimental results in the context of PDE forecasting. We show results on the Burgers\u2019, KS, and Kolmogorov datasets. We study the difference between all-at-once (AAO) and autoregressive (AR) sampling in forecasting, the influence of the guidance strength, and of the conditioning scenario in AR sampling for the joint and amortised models. Moreover, we also provide a comparison with some widely used benchmarks in forecasting: climatology and persistence. Finally, we provide results for two U-Net architecture choices for the diffusion-based models and the MSE-trained U-Net baseline. ", "page_idx": 25}, {"type": "text", "text": "F.1 AAO vs AR ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Once the joint model $\\mathbf{s}_{\\theta}(t,x_{1:L}(t))\\approx\\nabla_{x_{1:L}(t)}\\log p(x_{1:L}(t))$ is trained, there are multiple ways in which it can be queried ", "page_idx": 25}, {"type": "text", "text": "\u2022 All-at-once (AAO): sampling the entire trajectory in one go (potentially conditioned on some initial and/or intermediary states); \u2022 Autoregressively (AR): sampling a few states at a time, conditioned on a few previously generated states (as well as potentially on some initial and/or intermediary states). ", "page_idx": 25}, {"type": "text", "text": "Let us denote the length of the generated trajectory with $L$ , the number of predictor steps used during the sampling process with $p$ , and the number of corrector steps with $c$ . Moreover, in the case of the AR procedure, we have two extra hyperparameters: the predictive horizon $P$ (the number of states generated at each iteration), and the number of conditioned states $C$ . Finally, let the window of the model be $W$ . Each sampling procedure has its advantages and disadvantages. ", "page_idx": 25}, {"type": "text", "text": "1. Sampling time: the AAO procedure (with no corrections) is faster than the AR one due to its non-recursive nature. We express the computational cost associated with each procedure in terms of the number of function evaluations (NFEs) (i.e. number of calls of the neural network) needed to generate the samples. \u2022 AAO: ${\\mathrm{NFE}}=(1+{\\mathrm{c}})\\times p$ \u2022 AR: $\\begin{array}{r}{\\mathrm{NFE}=(1+\\mathrm{c})\\times p\\times(1+\\frac{L-W}{P})}\\end{array}$   \n2. Memory cost: the AAO procedure is more memory-intensive than the AR one, as the entire generated trajectory length needs to fit in the memory when batching is not performed. If ", "page_idx": 25}, {"type": "text", "text": "batching is employed, then the NFEs (and hence, sampling time) increases. In the limit that only one local score evaluation fits in memory, the AAO and AR procedures require the same NFEs. ", "page_idx": 26}, {"type": "text", "text": "3. Redundancy of generated states: in the AR procedure, at each iteration the model regenerates the states it conditioned upon, leading to computational redundancy. ", "page_idx": 26}, {"type": "text", "text": "We show that the AAO procedure is not well-suited for forecasting tasks, by performing experiments on the Burgers\u2019 and KS datasets. ", "page_idx": 26}, {"type": "text", "text": "Burgers\u2019. As mentioned above, the AR method comes at an increased computational cost at sampling time, but generates samples of higher quality and manages to maintain temporal coherency even during long rollouts. However, as illustrated in App. E, the quality of the AAO procedure can be improved by using multiple corrector steps. In particular, to obtain the same NFE for AAO and AR sampling (assuming the same number of predictor steps $p$ and no batching for AAO), we can use $\\begin{array}{r}{c=\\frac{\\mathbf{\\nabla}\\cdot\\mathbf{\\bar{\\alpha}}-\\breve{W}}{P}}\\end{array}$ corrector steps. ", "page_idx": 26}, {"type": "text", "text": "Fig. 11 shows the results for a trajectory with $L=101$ , using a model with $W=5$ . The results are expressed in terms of the RMSD, calculated based on 30 test trajectories. For AAO sampling we set $c=48$ , while for AR sampling we set $c=0$ . In the AR sampling, we generated 2 states at a time conditioned on the 3 previously generated states. In both cases we conditioned on a noisy version of the initial 3 true states (with standard deviation $\\sigma_{y}=0.01)$ ). For all experiments, we used the EI discretization scheme, with a quadratic time schedule. The AR procedure is clearly superior to the ", "page_idx": 26}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/3373175730fb8a52ee9c18b52aa13efef4741007b7b27655d53c7a498ee35b36.jpg", "img_caption": ["Figure 11: RMSD for Burgers\u2019 for AAO and AR generation with a window of 5. The number of corrections for AAO generation was chosen to match the computational budget of the AR sampling technique, yet the AR technique is clearly superior. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "AAO, even with 48 corrector steps. This can be easily observed by comparing the generated samples to the ground truth as shown in Fig. 12. ", "page_idx": 26}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/bd06401f20bf858f3284ad27bad492ef4bfdd14adf0afac2f50efcd7992b3d96.jpg", "img_caption": ["Figure 12: Comparison between the true samples (top), AAO samples (middle), and AR samples (bottom) for Burgers\u2019. The AAO samples are only able to capture the initial states correctly, while the AR-generated samples maintain very good correlation with the ground truth even at long rollouts (101 time steps). "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "For a window of 9, we used the following number of corrector steps for each conditioning scenario: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\;P=2,C=7:c={\\frac{101-9}{2}}=46}\\\\ &{\\bullet\\;P=3,C=6:c={\\frac{101-9}{3}}=31}\\\\ &{\\bullet\\;P=4,C=5:c={\\frac{101-9}{4}}=23}\\\\ &{\\bullet\\;P=5,C=4:c={\\frac{101-9}{5}}=19}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Moreover, the number of states we initially condition upon is equal to $C$ (i.e., when comparing AAO with 46 corrections to AR with $P=2$ , $C=7$ (2 | 7), we condition on the first 7 initial noised up states). ", "page_idx": 27}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/9e7d97b91ba03d65450c71b4723f2abc9d5fd1b91f529a8fe525f35cc123f57f.jpg", "img_caption": ["Figure 13: RMSD for Burgers\u2019 for AAO and AR generation with a window of 9 and with a varying computational budget. In the AAO case the computational budget is dictated by the number of corrector steps (indicated on the $\\mathbf{X}_{\\mathrm{}}$ -axis through the number before the comma). In the AR case, the budget is determined by the number of states generated at each iteration $(P)$ . This is also indicated on the $\\mathbf{X}$ -axis in the format $P\\mid C$ (e.g., 2 | 7 implies we generate two states at a time and condition on 7). AR is clearly superior to AAO for forecasting. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Once again, the AR-generated trajectories achieve significantly better metrics, which also reflects in the quality of the generated samples. Fig. 14 shows examples for AAO: 46/AR: 2 | 7. ", "page_idx": 27}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/ede077e053c5d1835ff521c62b3355addaa0da82ae422b9bf0eda9de680558a7.jpg", "img_caption": ["Figure 14: Comparison for Burgers\u2019 between the true samples (top), AAO samples with 46 corrections (middle), and AR 2 | 7 samples (bottom). With 46 corrector steps, some AAO trajectories manage to maintain good correlation with some of the easier test trajectories. However, the quality of the AR samples is clearly superior to the AAO samples, managing to model all test trajectories well. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "KS. We show similar findings on the KS dataset, where we present the performance of AAO and AR sampling on 128 test trajectories of length $100\\Delta t$ (corresponding to 20 seconds) in terms of MSE along the trajectory length. For AAO we employ a varying number of corrections $(0,1,3$ , and 5) and for each setting we show the results for the best $\\gamma\\in\\{0.0\\bar{3},0.05,0.1,0.5\\}$ . Setting $\\gamma$ to lower values led to unstable results. For AR we set $\\gamma=0.1$ and show results for 8 | 1. We condition on the first 8 true states and show the results for the remaining 92 states. ", "page_idx": 27}, {"type": "text", "text": "Fig. 15a clearly shows that the AAO sampling has poor forecasting performance as compared to AR sampling. Employing corrections has a positive impact close to the initial states, but they no longer seem to help for longer rollouts. Even if we employ 5 Langevin corrector steps, the AAO metrics are significantly worse than the AR metrics. ", "page_idx": 27}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/eefe231199f6157a1e2d4921d52fae42354e920707dfbc621bb9a171e50017ab.jpg", "img_caption": ["(b) Example trajectories from the window 9 model. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/0dbd292e022a8f49ac46e65de711cc62ff822793948cdd5afbc1c54dc3b9afd5.jpg", "img_caption": ["(a) Evolution of the MSE for KS along the trajectory length for a window of 9. The error bars indicate $\\pm\\:3$ standard errors. For AAO we show the results for $0,1,3$ , and 5 corrector steps, indicated between brackets. Averaged over the entire trajectory, employing 1 correction gives best performance. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 15: AAO vs AR comparison for the KS dataset. ", "page_idx": 28}, {"type": "text", "text": "Fig. 15b shows an example trajectory of length 100, alongside the predictions from AAO with 1 correction and AR sampling (left column). The right column shows the difference between the predictions and the ground truth. The AR procedure leads to results that are very similar to the ground truth for the first 100 states, whereas the predictions from AAO sampling quickly diverge from the ground truth. ", "page_idx": 28}, {"type": "text", "text": "Kolmogorov. We also confirm the previous findings in the Kolmogorov dataset, with experimental evidence shown in Fig. 16. We compare the performance between AAO and AR sampling for a model with a window size of 5 in terms of MSE along the trajectory length. We use 50 test trajectories of length 64. For AAO we show the results for 0 and 1 corrections, and show the results for the best setting for the guidance strength between $\\gamma\\in\\{0.03,0.05,0.1,0.3\\}$ , corresponding to $\\gamma=0.05$ in both cases. For AR we show results for 1 | 4 and $\\gamma=0.1$ . We condition on the first 4 states and show the results for the remaining 60 states. ", "page_idx": 28}, {"type": "text", "text": "We plot in Fig. 16b the vorticity associated with the true, AAO-predicted, and AR-predicted states at eight different time points. The AAO-predicted states diverge from the ground truth quickly, while the AR-predicted states stay close to the ground truth for more than half of the trajectory length. ", "page_idx": 28}, {"type": "text", "text": "F.2 Influence of guidance strength in forecasting ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "KS. We studied three different settings for the parameter that controls the guidance strength $\\gamma\\in\\{0.01,0.03,0.1\\}$ for the KS dataset for joint AR sampling. We found $\\gamma=0.01$ to be unstable in the case of forecasting, so in Fig. 17 we show the results for $\\gamma\\in\\{0.03,0.1\\}$ . We consider a variety of window sizes $W$ and conditioning scenarios $P~|~C$ , with $P+C=W$ , where $P$ represents the number of states generated at each rollout iteration, and $C$ the number of states we condition upon. As shown in Fig. 17, $\\gamma=0.1$ is the better choice for low / middle-range values of $P$ , and decreasing it to $\\gamma=0.03$ can lead to unstable results. In contrast, $\\gamma=0.03$ gives enhanced performance for longer predictive horizons $P$ , as compared to $\\gamma=0.1$ . ", "page_idx": 28}, {"type": "text", "text": "Kolmogorov. We studied five different settings for the guidance strength $\\gamma$ in the Kolmogorov dataset $\\bar{\\gamma}\\in\\{0.01,0.03,0.05,0.1,0.2,0.3\\}$ . The lowest value 0.01 gave unstable results, so we are not showing it in Fig. 18. The findings are similar to those for the KS dataset, indicating that the guidance strength needs to be tuned depending on the conditioning scenario. For $P=3$ and $P=4$ , the best setting of $\\gamma$ is 0.03. However, for smaller $P$ , $\\gamma=0.03$ gives unstable results, and the best settings are $\\gamma=0.05$ for $P=2$ , and $\\gamma=0.1$ for $P=1$ . ", "page_idx": 28}, {"type": "text", "text": "F.3 Forecasting performance of different conditioning scenarios $P\\mid C$ ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The autoregressive sampling method offers some flexibility in terms of the number of states generated at each iteration $(P)$ . From a computational budget perspective, the higher $P$ , the fewer iterations (a) Evolution of the MSE for Kolmogorov along the trajectory length for a model with window 5. The error bars indicate $\\pm\\:3$ standard errors. For AAO we show the results for 0, and 1 corrector steps, indicated between brackets. Averaged over the entire trajectory, employing corrections does not improve performance. ", "page_idx": 28}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/2ac7d69020e0bc1e05a6c76912b8c87bde734fad1a87fafdb38b8e6a48897598.jpg", "img_caption": ["Trajectory length "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "ry need to be performed. However, this might come at the cost of a decrease in the quality of the generated samples. In this section, we investigate this trade-off for the Burgers\u2019 and KS datasets. The results for the Kolmogorov dataset are shown in the main paper in Fig. 1. ", "page_idx": 29}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/58aac52569ee495456c7c59ac47f6b60be4275d2ca702acc6a115d84c0cffb26.jpg", "img_caption": ["(b) The vorticity of the predicted states from the window 5 model. The top row shows the true vorticity, the middle one shows the vorticity of the AAO-predicted samples, and the bottom row, the vorticity of the AR-predicted states. We indicate by $t$ the time index of the state (i.e. $t=\\Delta\\tau$ means first forecast state). ", "Figure 16: AAO vs AR comparison for the Kolmogorov dataset. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/05f22b316f93ae9cc6c2198cf244fa38a614466df33e714017872a5aac096e94.jpg", "img_caption": ["Figure 17: Forecasting performance of the joint model in the KS dataset for $\\gamma\\in\\{0.03,0.1\\}$ for varying window sizes and $P\\mid C$ conditioning scenarios. The top row shows the RMSD, while the bottom one shows the high correlation time. Where we do not show results for $\\gamma=0.03$ it is because they were unstable. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "We present results for a window of 9 in the Burgers\u2019 dataset and study several window sizes $W$ for the KS dataset. We also vary the predictive horizons $P$ , and the number of conditioning frames $\\boldsymbol{C}=\\boldsymbol{W}-\\boldsymbol{P}$ . The trajectories are conditioned upon the first $W-P$ true states. ", "page_idx": 29}, {"type": "text", "text": "The results are presented in terms of RMSD, which is computed based on 200 test trajectories for the Burgers dataset, and on 128 test trajectories for the KS dataset. The number of diffusion steps used is 128. For all experiments, we used the $\\mathrm{DPM++}$ solver, with a linear time discretisation scheme. At the end, we also performed an additional denoising step, and adopted the dynamic thresholding method for the Burgers\u2019 dataset. The guidance strength is fixed at $\\gamma=0.1$ for the Burgers\u2019 dataset, while for KS we show the best setting out of $\\gamma\\in\\{0.03,0.1\\}$ . ", "page_idx": 29}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/1e8b4549b650677d2c41e9410d472de05207022a11998b26bbd00982a962741b.jpg", "img_caption": ["Figure 18: Forecasting performance of the joint model in the Kolmogorov dataset for $\\gamma\\ \\in$ $\\left\\lbrace0.03,0.05,0.1,0.2,0.3\\right\\rbrace$ for varying window sizes and $P~|~C$ conditioning scenarios. The top row shows the RMSD, while the bottom one shows the high correlation time. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "Burgers\u2019. Fig. 19 shows that the performance is not very sensitive to $P$ for low to medium values of $P$ , although the model seems to perform better with lower $P$ (and consequently, higher number of conditioning frames $C$ ). However, the performance quickly deteriorates for very high values of $P$ (i.e., $P=8$ ). ", "page_idx": 30}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/701f6648242032bf34b736eb065032cdd36a489d01417b78596240345ecd4561.jpg", "img_caption": ["Figure 19: Burgers\u2019 dataset. RMSD for different values of $P$ . The right plot is a zoomed in version of the left one, where we excluded the $8\\mid1$ setting. Outliers have been removed. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "KS. We repeat the same experiment for the KS dataset, but study a wider variety of window sizes $W$ and conditioning scenarios $P~|~C$ , where $P+C=W$ . Moreover, as mentioned in G.1, for each setting we test two values of the guidance parameter $\\gamma$ . In Fig. 20 we only show the results corresponding to the best guidance strength. We also show results for the amortised model introduced in Sec. 3.2 in Fig. 21, where a different model was trained for each $P\\mid C$ scenario. Unlike the joint model, which was trained for 4000 epochs, the amortised model was only trained for 1024 epochs, since after that it started overfitting. ", "page_idx": 30}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/a0e20f95ea1c086352f606901f2ca954fff67bb9260092cde346f5ac1b068aac.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 20: RMSD (top) and high correlation time (bottom) for the joint model on the KS dataset for varying window sizes $W$ and $P~|~C$ conditioning scenarios (where $P+C=W$ ). The model performs best for low values of $P$ ( $P=1$ ), and high values of the number of conditioning frames $C$ . ", "page_idx": 30}, {"type": "text", "text": "Joint model. As observed in Fig. 20, the best conditioning scenario for the joint model for each window corresponds to $P=1$ (i.e. a low number of predicted frames). We observe an increase in performance by increasing the window size, with the best performing setting corresponding to a model with window 13 (1 | 12). In this case the model achieves 77.3s high correlation time, as opposed to 73.2s for the window 9 model presented in the main paper. We believe further gains in performance could be achieved by increasing the window size further, but this would also come with higher memory requirements. ", "page_idx": 31}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/2597a2767fed975774061b6d1e6d7bec65e4213abd86b3ed3657b4ae2dce91a1.jpg", "img_caption": ["Figure 21: RMSD (top) and high correlation time (bottom) for the amortised model in the KS dataset with varying $P$ and $C$ . The model performs best when generating relatively large blocks $(P\\geq4)$ and conditioning on a lower number of frames $\\left(C\\leq4\\right)$ ). "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Amortised model. In contrast to the joint model, Fig. 21 shows that the amortised model underperforms for small $P=1$ , with high correlation time of less than 50s. We hypothesise that the reason for this might be the combination of the following: i) generating larger number of frames at the same time requires less steps for the same trajectory length, so there is less error accumulation; and ii) for larger $P$ the model is trained with a better learning task compared to $P=1$ , as larger $P$ provides more information to the model. For $P\\geq4$ , the model performs similarly for any $P$ , being slightly more sensitive to larger $C$ when $P$ is lower. Regardless of $P$ , the performance of the model deteriorates significantly with larger $C=8$ , similar to the behaviour shown in Lippe et al. [42]. ", "page_idx": 31}, {"type": "text", "text": "Universal amortised model. As indicated in Sec. 3.2, a more flexible alternative to the amortised model is the universal amortised model. Due to our novel training procedure, the latter does not need re-training for each $P\\mid C$ conditioning scenario. Moreover, as shown in Fig. 1 and Fig. 22, the performance of the universal amortised model is stable for all $P~|~C$ scenarios, and, unlike other models from the literature [42], is not negatively affected by longer previous history. ", "page_idx": 31}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/69d55494e29b2f72842f75b37a47a6cb0cf446c60d7b57a1a6858b1fce5999e5.jpg", "img_caption": ["Figure 22: RMSD for universal amortised model (left) and amortised model (right) in the KS dataset with varying $P$ and $C$ . Universal amortised model is more stable as well as outperforms the amortised model across all combinations of $P$ and $C$ . "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "F.4 Comparison with climatology and persistence ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We also compare the performance of our models (joint AR and universal amortised) to two common forecasting benchmarks\u2014climatology, determined by computing the average value over the entire training set, and persistence, obtained by repeating the last observed state for the entire trajectory. Rather than looking at RMSD or high correlation time, we look at a per time step criterion\u2014MSE. ", "page_idx": 31}, {"type": "text", "text": "This allows us to find out at which point predicting just the mean value is preferred to the predictions of the models (i.e. when climatology gives lower MSE than the models). ", "page_idx": 32}, {"type": "text", "text": "Fig. 23 shows that the joint and universal amortised models outperform persistence throughout the trajectory length. For KS, they significantly outperform climatology until approximately 400 time steps, corresponding to about 80s. As observed in the main paper through the high correlation time, the amortised model seems to be better at long-term prediction than the joint model. For Kolmogorov, they outperform climatology for more than 40 time steps, corresponding to 8s. ", "page_idx": 32}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/6edc38741df51a37ca98dd99bdc98328b9d05cac15f17597e91f1304ffa74288.jpg", "img_caption": ["Figure 23: MSE along the trajectory length for KS (left) and Kolmogorov (right) for 1) the joint AR model (blue), 2) the universal amortised model (orange), and the two forecasting benchmarks considered\u20143) persistence (black) and 4) climatology (grey). The error bars indicate $\\pm\\,3$ standard errors. Our models outperform climatology for about two thirds of the trajectory length. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "F.5 Example samples ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We show in Fig. 24 some example predicted trajectories using the joint 1 | 8 model and the universal amortised 8 | 1 model for the KS dataset, alongside the ground truth trajectories. We also show the error between the predictions and the true trajectories. ", "page_idx": 32}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/38ca277efb8c8d4c9d9419e51011fc8dc050ee8875c793f71379cc8996067c2b.jpg", "img_caption": ["Figure 24: Example true trajectories, and model predictions for the joint 1 | 8 (middle) and universal amortised 8 | 1 (bottom) model on the KS dataset. The top left/right example corresponds to one of the trajectories with the highest/lowest RMSD. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "For Kolmogorov, we show in Fig. 25 examples of the vorticity at different time steps $t$ corresponding to the true states (top), to the states predicted by the joint 1 | 4 model (middle), and to the states predicted by the universal amortised 1 | 4 model (bottom). ", "page_idx": 33}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/f7765d778490e1bc5839f420d06bd27f2075175949dfc0f417d9ed40efcc639a.jpg", "img_caption": ["Figure 25: Example vorticity fields at different time steps and for four different initial states. Each plot consists of three different rows corresponding to the true trajectories (top), to the trajectories predicted by the joint 1 | 4 model (middle), and to those predicted by the universal amortised 1 | 4 model (bottom). "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "F.6 Results for different architectures ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "For the diffusion models and the MSE-trained U-Net baseline outlined in Sec. 5, we study two different U-Net architectures and report the best results in Fig. 2. One follows [60] (SDA), while the other one follows [42] (PDE-Refiner). More details about the exact hyperparameters and training details can be found in App. D.4. ", "page_idx": 34}, {"type": "text", "text": "As illustrated in Tab. 5, we obtain better results with the SDA architecture for the joint AR model, while the PDE-Refiner architecture yields better performance for the amortised models. For the MSE-trained U-Net baseline, the SDA architecture gives better results for the KS dataset, but worse for Kolmogorov. ", "page_idx": 34}, {"type": "table", "img_path": "nQl8EjyMzh/tmp/26f0a8460bca7ce23b41bd7339bc4b1effad42e760ece7d5a5d1192f02e96f88.jpg", "table_caption": ["Table 5: The best high correlation time (mean $\\pm\\:3$ standard errors over test trajectories) achieved for two different architectures\u2014SDA, inspired by [60], and PDE-Refiner, inspired by [42]. For the diffusion-based models, the conditioning scenario $P\\mid C$ that corresponds to the best performance is indicated between brackets. "], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "G Additional experimental results on offline data assimilation ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this section, we provide more details about the offline DA setup. For KS, for each sparsity level we uniformly sample the indices associated with the observed variables in space-time. For Kolmogorov, we uniformly sample indices in space at each time step. For both datasets, we assume to always observe some full initial states. The number of such states for AR sampling depends on the chosen conditioning scenario and is equal to $C$ . For AAO we condition on $W-1$ initial states, where $W$ stands for the window size. ", "page_idx": 34}, {"type": "text", "text": "Furthermore, this section provides further experimental results for the offline data assimilation task for the KS and Kolmogorov datasets. We did not explore this for the Burgers\u2019 dataset since we decided to focus on PDEs with more challenging dynamics. We investigate the influence of the guidance strength, and the performance of the AR models with varying conditioning scenarios. We also perform a comparison between a joint model with Markov order of $k$ that is queried autoregressively and a $2k_{\\mathrm{{}}}$ -Markov order model queried all-at-once. This allows us to gain further insight into the differences between the two sampling schemes. Finally, we provide a quantitative and qualitative comparison to a simple interpolation baseline. ", "page_idx": 34}, {"type": "text", "text": "G.1 Influence of the guidance strength for the joint model ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "We study several settings of the guidance parameter $\\gamma$ for 1) the joint model sampled AAO, and 2) the joint model sampled autoregressively. We show the results in terms of RMSD evaluated on 50 test trajectories for varying levels of sparsity, as in the main paper. ", "page_idx": 34}, {"type": "text", "text": "Joint AAO. For the KS dataset, we consider a window 9 model, with $\\gamma\\quad\\in$ $\\left\\{0.01,0.03,0.05,0.1,0.5\\right\\}$ . For Kolmogorov, we show the results for a window 5 model and $\\gamma\\in\\{0.03,0.05,0.1\\}$ . We query them AAO, and employ 0 and 1 corrections (AAO (0) and AAO (1)). ", "page_idx": 34}, {"type": "text", "text": "For KS, using $\\gamma=0.01$ results in unstable rollouts, therefore we only report the results for the other four values of $\\gamma$ . For the sparse observation regimes, we find that the best results are achieved by employing the lowest $\\gamma$ value that still leads to stable results $(\\gamma=0.03)$ . For the denser regimes (i.e. proportion observed $\\geq10^{-1}$ for 0 corrections and $\\geq10^{-1.5}$ for 1 correction), the best setting corresponds to $\\gamma=0.05$ . ", "page_idx": 34}, {"type": "text", "text": "Fig. 27 shows the results on the Kolmogorov dataset. In contrast to the KS dataset, we found that the best $\\gamma$ value is consistent among all observation regimes, and corresponds to $\\gamma=0.05$ for both AAO ", "page_idx": 34}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/fca9b64e5ea6961f3624f419e3a837b7042d889704f668cc94a55c42c550e818.jpg", "img_caption": ["Proportion observed "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Figure 26: RMSD on the KS dataset for the joint model queried AAO with 0 (AAO (0) - left) and 1 (AAO (1) - right) corrections. We show results for a window of size 9. Confidence intervals indicate $\\pm\\:3$ standard errors. ", "page_idx": 35}, {"type": "text", "text": "(0) and AAO (1). In this case, decreasing the $\\gamma$ value (i.e. increasing the guidance strength) too much does not lead to improved performance and leads to a much higher standard error, as compared to the other studied settings. ", "page_idx": 35}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/fc83d9fbfaef409a2c2a12a92c8788e60eafa49935168a5f5e23a7c2bf2959f9.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Figure 27: RMSD on the Kolmogorov dataset for the joint model queried AAO with 0 (AAO (0) - left) and 1 (AAO (1) - right) corrections. We show results for a window of size 5. Confidence intervals indicate $\\pm\\:3$ standard errors. ", "page_idx": 35}, {"type": "text", "text": "Joint AR. For the joint AR scheme not only can we vary the guidance strength, but we can also change the conditioning scenario $P\\mid C$ . In Fig. 28 we show the RMSD for the KS dataset for two values of $\\gamma\\in\\{0.03,0.\\dot{1}\\}$ for a window 9 model. Each plot shows the results for a different sparsity level. ", "page_idx": 35}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/4c0ae8b1668ed753c967ee6c6a3148c5bb238f9c4c5bcdfaab50fe6611efa582.jpg", "img_caption": ["Figure 28: RMSD for the KS dataset on the offline DA task for varying conditioning scenarios and $\\gamma=0.03$ (light blue) and $\\gamma=0.1$ (dark blue). For each sparsity level (proportion observed), we consider all the $P~|~C$ scenarios for a model with window 9. When results are not presented for $\\gamma=0.03$ , it is because they were unstable. Error bars indicate $\\pm\\:3$ standard errors. Note the logarithmic y-scale for the bottom two rows of plots. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Fig. 28 shows that for the lower values of $P$ , i.e. settings where we predict only a few frames at a time, the results become unstable for low values of $\\gamma$ . However, especially in the sparser regimes, decreasing $\\gamma$ to 0.03 can lead to a significant improvement in performance for the larger values of $P$ , i.e. settings with a higher predictive horizon. We also tried decreasing $\\gamma$ further to 0.01 but the results were unstable for the majority of $P\\mid C$ settings. The only scenario where $\\gamma=0.01$ achieves a better performance than the settings presented in Fig. 28 is for $8\\mid1$ and proportion observed of $10^{-1.5}$ , and this is the scenario considered in Fig. 3 for the above-mentioned proportion of observed data. ", "page_idx": 36}, {"type": "text", "text": "For Kolmogorov, we studied $\\gamma\\,\\in\\,\\{0.03,0.05,0.1,0.2,0.3\\}$ for each conditioning scenario of a window 5 model $(P\\in\\{1,2,3,4\\})$ . We show the best $\\gamma$ setting for each conditioning setting in Tab. 6. Similarly to the KS dataset, we find that lower $\\gamma$ values give better results for larger $P$ , while they are unstable for lower $P$ . ", "page_idx": 36}, {"type": "table", "img_path": "nQl8EjyMzh/tmp/df7b451123958b187383c7b92d45959d4e7d7851869cb63e6d7ce5a5cf410f64.jpg", "table_caption": ["Table 6: Best $\\gamma$ corresponding to each conditioning scenario $P$ for different proportion of data observed on the Kolmogorov dataset for the joint model with window size 5. "], "table_footnote": [], "page_idx": 36}, {"type": "text", "text": "G.1.1 Conditioning for DA in the universal amortised model ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In our experiments, we found that to get better performance for the universal amortised model with reconstruction guidance, both $\\gamma$ and $\\sigma_{y}$ need to be tuned given a particular proportion of data observed. What is more, the universal amortised model is also more sensitive to the choice of the reconstruction guidance parameters than the joint one. Due to computation constraints and sensitivity of the universal amortised model to the choice of hyperparameters in reconstruction guidance, we limit our experiments to a model with window 9 for the KS dataset. ", "page_idx": 36}, {"type": "text", "text": "For each proportion of observed data we perform a hyperparameter sweep varying $\\gamma\\ \\in$ $\\{0.1,0.05,\\bar{0}.0\\bar{1}\\}$ and $\\sigma_{y}\\;\\in\\;\\{0.01,0.005,0.00\\bar{1}\\}$ . Tab. 7 shows the value for the best predictive horizon $P$ , $\\gamma$ and $\\sigma_{y}$ for each proportion of data observed for a model with window size of 9 on the KS dataset, and Tab. 8 shows the results for the Kolmogorov dataset for a model with window size 5. Empirically, we found that the more data are observed, the lower values of $\\gamma$ and $\\sigma_{y}$ are required for optimal performance. Lower values of $\\gamma$ and $\\sigma_{y}$ increase the strength of the guidance term, which indeed is expected to guide the process better when more data are available. ", "page_idx": 36}, {"type": "text", "text": "As described in the Sec. 3.2, for the universal amortised model we propose to use conditioning via architecture for past states and reconstruction guidance for sparse observations DA. However, there are a few other options that could be used for incorporating sparse observations in amortised models: i) resampling [46], where at each reverse diffusion time step more sampling iterations are introduced, which mix observed and unobserved variables, and ii) conditioning via architecture, which requires a separate model, taking not only previous states, but also sparse observations as input, to be trained. ", "page_idx": 36}, {"type": "table", "img_path": "nQl8EjyMzh/tmp/d13fddacab8a5faa589904dd6a7340e940f2f66cd78315676c8905d378fac610.jpg", "table_caption": ["Table 7: Best $\\gamma$ , $\\sigma_{y}$ and $P$ for different proportions of data observed on the KS dataset for universal amortised model with window size 9. "], "table_footnote": [], "page_idx": 36}, {"type": "table", "img_path": "nQl8EjyMzh/tmp/5a4e262c5c1e41cc63fc30bbbfdf603f76227c08895745bdbfb5e654e346d455.jpg", "table_caption": ["Table 8: Best $\\gamma$ , $\\sigma_{y}$ and $P$ for different proportions of data observed on the Kolmogorov dataset for universal amortised model with window size 5. "], "table_footnote": [], "page_idx": 37}, {"type": "text", "text": "We have not investigated the resampling strategy, as in their work [15] showed that it has worse performance compared to reconstruction guidance in image and protein design applications. Moreover, in practice resampling requires around 10 mixing sampling steps, with one step accounting for a single call to the score network, for each reverse diffusion iteration. Therefore, this makes the conditional sampling with resampling approach much more compute-intense and less effective in terms of sampling time. ", "page_idx": 37}, {"type": "text", "text": "We have studied the second approach for conditioning, where all the conditioning is happening via the architecture. For this, we trained a separate model, which takes in fully observed previous states and some random data observations with masks as an input to the score network. During training we sample the number of fully observed states $C$ as well as random masks for sparse observations within the rest of the $P$ frames for each batch. As a result, the model is trained to fit the following conditional score: $\\mathbf{s}_{\\theta}(t,x_{i-C:i+P}(t)|x_{i-C:i},x_{o})$ . Fig. 29a shows the performance of this model (green) for different proportions of observed data. We considered a larger proportion of observed data, as for the lower ones considered in the main text of the paper, the model was significantly underperforming. From the Fig. $29\\mathrm{a}$ , it can be clearly seen that the model with all the conditioning done through the architecture performs significantly worse compared to our proposed version of the universal amortised model with reconstruction guidance. More specifically, the level of the universal amortised model\u2019s performance with $10\\%$ of data observed is reached by the other model only when more than $70\\%$ of data is observed. We hypothesise that such poor model performance can be explained by the fact that at some point during the AR rollout, the input condition (previously generated $C$ states and partially observed sparse data within window of size $P$ ) that is provided to the model becomes out-of-distribution due to the mismatch between the previously generated states and the partially observed variable from the true trajectory. Fig. 29b shows inputs to the score network depending on the AR step on KS data. The model was trained with window 9 and uses 3 previously generated states for conditioning. The mismatch between previously generated states and observed trajectory data is visually noticeable at AR step 60, while becoming even more drastic with more AR steps. Therefore, there is a mismatch between input distribution used during training and sampling. ", "page_idx": 37}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/d05cb9d86cf75d0865d30738e9e66549d2991a4b3c244bd49e368d7f3ee4fb6d.jpg", "img_caption": ["(a) Conditioning via architecture "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/266835a720fc5b4d2f5e6d2396bfa49350a182007ce6b67acdfded0e29695a5d.jpg", "img_caption": ["(b) Input to the model with conditioning via architecture "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "Figure 29: Left: RMSD for the KS dataset on offline DA for the universal amortised model with all conditioning done via architecture. The model is compared against an identical universal amortised model that, in contrast, uses reconstruction guidance for conditioning on sparse observations. Right: The input that is used for model conditioning depending on the AR step (the longer the trajectory, the more AR steps are performed). The red line divides the previously generated $C$ states used for conditioning and the observations of the true trajectory within window $P$ . The observations of the true trajectory are not fully observed by the model, but are illustrated without masks for more clarity. The vertical axis stands for the state within the window size used in the score network (9 in this case), while the horizontal one illustrates space discretisation of the PDE. ", "page_idx": 37}, {"type": "text", "text": "G.2 Performance for varying window sizes ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Joint model. We compare the performance of a joint model with window 5 $\\left[k\\right.=\\left.2\\right]$ ) with that of a model with window 9 $\\mathrm{[}k=4,$ on the KS dataset. For both models, we show results for AR sampling, as well as AAO sampling with 0, and 1 corrections (AAO (0) and AAO (1)) in Fig. 30. The window 5 model is represented with dashed lines, while the window 9 model with solid lines. For each proportion of data observed, we show the results corresponding to a value of $\\gamma$ and $P\\mid C$ for AR sampling that gave the best performance. ", "page_idx": 38}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/606341703a2bdfa4b3a1f34b905980ec807d39ab367ae0a1964e68ba615aa397.jpg", "img_caption": ["Figure 30: RMSD for the KS dataset on the offline DA task for two models, one with a window of size 5 (dashed lines), and one with a window size of 9 (solid lines). The left plot shows the results for AR sampling, while the right plot shows the results for AAO sampling with 0 and 1 corrections. The error bars indicate $\\pm\\:3$ standard errors. Note the logarithmic y scale. "], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Fig. 30 shows that for both AAO and AR sampling the performance of the window 9 model is generally higher than that of the window 5 model, although for some sparsity settings the difference is not significant. The biggest improvement is seen for the middle range of sparsity (proportion of observed data between $\\overline{{10^{-2}}}$ and $10^{-1}$ ). We hypothesise that the reason why we do not see a significant improvement in performance between the two models in some settings is because a Markov order of $k\\,=\\,2$ (corresponding to a window 5 model) is enough to deal with the task at hand, and increasing the Markov order offers limited additional useful information, leading to only marginal improvements. ", "page_idx": 38}, {"type": "text", "text": "G.2.1 Comparison between AR $(k)$ and AAO $(2k)$ ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "As mentioned in Sec. 3.3, we hypothesise there are two reasons why the AR approach outperforms the AAO approach. The first reason has to do with modelling capacity\u2014for a window of size $2k+1$ , the effective Markov order of the AR model is $2k$ , while the AAO model has an effective Markov order of $k$ . Secondly, in AAO sampling information needs to be propagated from the start of the sequence to its end, in order for the trajectory to be coherent. However, the score network only has a limited receptive field of $2k+1$ , limiting information propagation. The way to tackle this is through Langevin corrector steps, but they come at an additional computational cost. To empirically assess these claims in the context of DA, we compare a window 5 model $\\mathit{\\Pi}[k=2]$ ) queried autoregressively with a window 9 model $[k=4,$ ) sampled AAO. For the latter, we apply 0, 1, and 2 corrector steps (AAO (0), AAO (1), and AAO (2)). ", "page_idx": 38}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/941777ef4d328aad6d7da5c0387cbe8ab1446402b92c2a2738eba661089f3490.jpg", "img_caption": ["Proportion observed "], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Figure 31: RMSD for the KS dataset on the offline DA task for two models, one with a window of size 5 queried autoregressively (dashed blue), and one with a window size of 9 and sampled AAO (solid purple). We show results for 0, 1, and 2 corrector steps, indicated with the $c$ in AAO $(c)$ . The error bars indicate $\\pm\\:3$ standard errors. The right plot shows the same results as the left one, but on a logarithmic y scale. ", "page_idx": 38}, {"type": "text", "text": "The findings shown in Fig. 31 are consistent with our theoretical justification. In general, increasing the number of corrections (and hence, enhancing information propagation) increases the performance of AAO. Indeed, for the majority of sparsity levels (proportion observed $\\geq10^{-2.5},$ ), the performance of the AR $k=2$ and AAO $k=4$ with at least 1 correction become similar. ", "page_idx": 39}, {"type": "text", "text": "The Langevin corrector steps do not seem to make a significant difference for the two extremes considered\u2014proportion observed of $10^{-3}$ and $10^{-0.5}$ . In the first case, there are so few data observed, that the task becomes similar to forecasting. We have already shown that AAO is inefficient in that setting, regardless of whether corrections are applied or not (see Fig. 15). For a proportion observed of $10^{\\overline{{-}}0.5}$ , the conditioning information is dense enough that corrections are not needed. ", "page_idx": 39}, {"type": "text", "text": "We can also corroborate the findings from App. F.1 regarding the performance of AAO vs. AR on forecasting with the findings from this section on DA to better understand the difference between AAO and AR sampling. The results on both tasks clearly show that a window 5 AR models $[k=2]$ ) does not lead to the same results as a window 9 AAO (0) model $k=4,$ ), although they have the same effective Markov order. In the context of DA, the performance of the AR model can be recovered by applying at least one corrector step, which allows for more efficient information propagation. In contrast, in forecasting, applying corrector steps helps in the vicinity of the observed states, but makes little difference for long rollouts. The lack of observations (after the initial states) leads to inefficient information flow, which results in the AAO procedure failing to produce coherent trajectories. For AR, this is naturally tackled through the autoregressive fashion of the rollout. ", "page_idx": 39}, {"type": "text", "text": "G.2.2 Performance for varying conditioning scenarios $P\\mid C$ ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "We study varying conditioning scenarios for the offline DA task. For the KS dataset we consider two joint models of different window sizes $W=5$ and $W=9$ ) and show in Fig. 32 the performance for each $P~|~C$ scenario (indicated in the legend), where $P+C=W$ . For each proportion of observed data and for each conditioning scenario $P\\mid C$ , we show the results corresponding to the best guidance strength $\\gamma$ . ", "page_idx": 39}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/a46360d80a8f07aae2dc1200f94905c28a04f0c8cf6554196f854bb253a770fa.jpg", "img_caption": ["Proportion observed "], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "Figure 32: RMSD for the KS dataset on the offline DA task for two models, one with a window of size 5 (left), and one with a window size of 9 (right) for varying $P~|~C$ scenarios (indicated in the legend). Confidence intervals indicate $\\pm\\:3$ standard errors. Note the logarithmic y scale. ", "page_idx": 39}, {"type": "text", "text": "We observe in Fig. 32 that for both window sizes the best performance is generally achieved for the larger values of $P$ . This is probably due to the increasing amount of observations within the generated frames. However, for a proportion of observed data of $\\bar{1}0^{-3}$ , the observations are so sparse, that the task becomes closer to forecasting, where, as previously noted, a lower value of $P$ is preferred. In that setting, the best performance is achieved by the 1 | 4 and $1\\mid8$ scenarios, respectively, although there are only small differences between the different scenarios. ", "page_idx": 39}, {"type": "text", "text": "In the main paper, in Fig. 3, we show the figures corresponding to the best $P~|~C$ scenario for each proportion of observed data, with the exception of $\\mathrm{\\dot{1}0^{-3}}$ . There, we found little difference between the performance of $P\\in\\{1,2,3,4\\}$ , and hence chose to show the scenario that offers the best trade-off between performance and computational complexity (i.e. $P=4$ )\u2014an increase in RMSD of $0.25(\\approx1.4\\%)$ for an almost four-fold decrease in sampling time. ", "page_idx": 39}, {"type": "text", "text": "In Fig. 33a we show the performance of the amortised model with window 9 for different $P$ and $C$ for KS. The best performance is achieved by the 6 | 3 model across most of the proportions of data observed. The universal amortised model is again more stable compared to the joint one when varying $P$ and $C$ , however it struggles to get the same level of performance for intermediate values of observed data considered. Fig. 33b shows the same for the Kolmogorov dataset. In contrast to KS, the best result is achieved by the model that has the longest history (1 | 4). The performance of the model is again more stable for different $P\\mid C$ scenarios compared to the Joint AR. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/adab3480c5b0167d0038d8de68d825ea035a4a1dc9f88bf8457de72db0291cec.jpg", "img_caption": ["Figure 33: RMSD for the KS (left) and Kolmogorov (right) datasets on the offline DA task for the universal amortised model for varying $P\\mid C$ scenarios (indicated in the legend). Confidence intervals indicate $\\pm\\:3$ standard errors. Note the logarithmic y scale. "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "We performed the same analysis for the Kolmogorov dataset and show the results in Fig. 34. In this case we only study a model of window 5 for both the joint and universal amortised models. We observe similar findings as in the KS dataset for the joint model, with lower $P$ values being preferred for lower proportions of observed data, and higher $P$ when the amount of observations increases. ", "page_idx": 40}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/29d121886709c02e24af6fa9bc4174500bb2fa8b490c5fc0d6a73a5499d2db3d.jpg", "img_caption": ["Figure 34: RMSD for the Kolmogorov dataset on the offline DA task for the joint model with window size 5 for varying $P~|~C$ scenarios (indicated in the legend). Confidence intervals indicate $\\pm\\nobreakspace3\\nobreakspace$ standard errors. Note the logarithmic y scale. ", "Proportion observed "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "G.3 Comparison to interpolation ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "In this section we perform a quantitative and qualitative comparison to a simple interpolation baseline. This was implemented using the interpolation functionality within the Scipy library [74]. For KS, we investigated three interpolation methods: linear, cubic, and nearest, and report the results for the method that gave the best results for each percentage of observed data. For Kolmogorov, we investigated linear and nearest interpolation, since the cubic method is not applicable to 3D data (2 spatial dimensions and 1 time dimension). ", "page_idx": 40}, {"type": "text", "text": "We present the results in Fig. 35. For the 1D KS dataset, all the methods studied in the main text (joint AR, AAO, and universal amortised) generally outperform the interpolation baseline for the sparse regimes (up to $10^{-1}$ proportion observed). The only exception is for universal amortised with $10^{-2}$ data observed, where the performance is on par. As more data is observed, the performance of all five methods becomes similar, indicating that the field can be easily reconstructed based on the observations alone. For the two sparsest regimes, linear interpolation gave the best results, while for the last four, we used cubic interpolation. ", "page_idx": 40}, {"type": "text", "text": "For Kolmogorov, there is a significant gap between the AR methods and the interpolation baseline, potentially indicating that interpolation is not as efficient when the number of data dimensions grows. The AAO methods also significantly outperform the interpolation baseline for all but the sparsest regimes. All results correspond to linear interpolation, since this gave the best performance. ", "page_idx": 40}, {"type": "text", "text": "For both datasets, we also provide a qualitative comparison between the joint AR predictions and the interpolation predictions, alongside the ground truth and the observed data. This is illustrated for two levels of sparsity for each dataset ( $\\mathrm{[0^{-2}}$ and $10^{-1}$ ). Fig. 36 shows the results for the KS dataset. For the smaller proportion of data observed, the interpolation baseline produces unphysical behaviour, possibly because it does not contain any prior knowledge about the dynamics of the dataset and the observations alone do not offer enough information for good trajectory reconstruction. In the case of the joint AR model, the learned dynamics from the prior score make the predictions more consistent with the dynamics describing the KS dataset. When the data is dense enough, even a simple interpolation baseline gives reasonable predictions (Fig. 36b). ", "page_idx": 40}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/fb61bf7b213f7788f05550b5e60ba211ebcf554e1a4c4ca96ddef9ae6b8098e9.jpg", "img_caption": ["Figure 35: RMSD (mean $\\pm3$ standard errors) for the KS (left) and Kolmogorov (right) datasets on the offline DA task for varying sparsity levels. We show the results in Fig. 3, alongside an interpolation baseline (red). Note the logarithmic x scale. "], "img_footnote": [], "page_idx": 41}, {"type": "text", "text": "", "page_idx": 41}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/ea3b288246628ef8307028c4e30c9d87120831c6835b333734ede54c5adcb873.jpg", "img_caption": ["Figure 36: Qualitative comparison between the true, predicted (with the joint AR model), and interpolated trajectories, as well as the observed values for the KS dataset. The left plot corresponds to a proportion of observed data of $10^{-2}$ , while the right corresponds to $10^{-1}$ . "], "img_footnote": [], "page_idx": 41}, {"type": "text", "text": "In contrast, for the Kolmogorov dataset, the interpolation results still show unphysical behaviour even for the higher percentage of data observed $\\bar{(10^{-1})}$ . This is probably because of the higher dimensionality of the data, as well as because we are plotting the vorticity (curl of velocity field), rather than the predicted velocity fields, which involves differentiating the predictions. Overall, the visual quality of the predictions outputted by the methods studied in the main text are clearly superior to the interpolation baseline. ", "page_idx": 41}, {"type": "text", "text": "H Additional experimental results on online data assimilation ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "In this section we provide a more detailed explanation of the setup from the online DA task and provide results for several conditioning scenarios for the joint and amortised models. ", "page_idx": 41}, {"type": "text", "text": "The online DA scenario goes as described in Sec. 5.3. In Fig. 38 we also present a diagram, showing an example of which observations would be available at each DA step in the KS dataset. The darker dots indicate observations, while the lighter background indicates that no observations are available in that region at that DA step. The example corresponds to a proportion of data observed of 0.1. ", "page_idx": 41}, {"type": "text", "text": "We study varying conditioning scenarios for the online DA task for the joint and universal amortised models. We do not show results for $P=1$ due to the excessive computational time required for sampling, and only study the results for one window size for each dataset: window 9 for KS and window 5 for Kolmogorov. ", "page_idx": 41}, {"type": "text", "text": "Joint model. For the window 9 joint model, we show the results on the KS dataset corresponding to the best guidance strength out of $\\gamma\\in\\{0.03,0.05,0.1\\}$ for AR sampling. For AAO sampling we studied $\\gamma\\in\\{0.01,0.03,\\bar{0.05},0.1,0.5\\}$ with 0 and 1 corrections. We found that the results did not significantly vary with $\\gamma$ , with $\\gamma=0.01$ giving marginally better performance, and lower values giving unstable results. In Tab. 2 we show the results corresponding to $\\gamma=0.01$ for AAO with 0 (AAO (0)) and 1 (AAO (1)) corrections, and for $\\gamma=0.05$ and 4 | 5 for AR. ", "page_idx": 41}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/7243bdf1b1efa8a9a2bcfe5fc095acadd3fc1378badca147a23424aa341cd311.jpg", "img_caption": ["Figure 37: Qualitative comparison between the true, predicted (with the joint AR model), and interpolated trajectories, as well as the observed values for the Kolmogorov dataset. The top plot corresponds to a proportion of observed data of $10^{-2}$ , while the bottom corresponds to $10^{-1}$ . "], "img_footnote": [], "page_idx": 42}, {"type": "text", "text": "Fig. 39a shows that the joint model has a similar performance for $P\\in\\{4,5,6\\}$ , with the lowest RMSD given by 6 | 3. Lower values of $P\\in\\{2,3\\}$ give poorer performance, just as in the offline DA task. Higher values of $P\\in\\{7,8\\}$ also perform worse, just as in the forecasting task. Thus, the fact that the middle values of $P$ perform best is probably due to the fact that the online DA task is a mix between DA and forecasting. ", "page_idx": 42}, {"type": "text", "text": "For the Kolmogorov dataset we studied $\\gamma\\in\\{0.03,0.05,0.1,0.3\\}$ for a model with window 5 queried AAO. We employed 0 (AAO (0)) and 1 (AAO (1)) corrections. Similarly to the KS dataset, we found little difference between the $\\gamma$ settings, and found that $\\gamma=0.03$ gave marginally better performance. For AR, we studied $\\gamma\\in\\{0.03,0.05,0.1,0.2\\}$ for each possible value of $P$ $\\dot{\\langle P\\in\\{1,2,3,4\\}\\rangle}$ . We found $\\gamma=0.03$ to be unstable for low values of $P\\in\\{1,2\\}$ . The best setting was achieved for 2 | 3 and $\\gamma=0.05$ . We show the results corresponding to the best $\\gamma$ setting in Fig. 39b. ", "page_idx": 42}, {"type": "text", "text": "Universal amortised model. For the universal amortised model, we use the best $\\gamma=0.05$ and $\\sigma_{y}=0.001$ parameters found for the offline DA for the same proportion of observed data, due to computational constraints. Fig. 40a shows the performance on the KS dataset for each $P\\mid C$ scenario, where $P+C=W$ . Fig. 40a indicates that the universal amortised model shows stable performance across different $P$ and $C$ , with the setting of 5 | 4 being slightly better than the other ones. Fig. 40b shows the performance on the Kolmogorov dataset for each $P~|~C$ scenario, where $P+C=W$ . Fig. 40b suggests that conditioning the model on larger history results in better performance. Even though 1 | 4 shows the best result, all other conditioning scenarios also have competitive performance compared to both AAO(1) and Joint AR models. ", "page_idx": 42}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/789d42efd6b11cc7aed10cd6008cc7c998fbd8df6277893e862345951160f649.jpg", "img_caption": ["Figure 38: Setup for the DA online task for the KS dataset for a proportion of observed data of 0.1. Every 20 time steps, we get access to sparse observations from blocks of 20 states. At the first DA step we predict the first 400 states based on the sparse observations from the first 20 states. At the next DA step, we use the previously-generated forecast, as well as the new sparse observations coming from the next block (time step 20-40) to predict the next trajectory of length 400 (from time step 20 to 420). This repeats until the end of the sequence (640 time steps). "], "img_footnote": [], "page_idx": 43}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/1996b061ed91d608bc77a6f2738e1bfa9d6ba1a0a1b706d62ba81ea2cb51fa20.jpg", "img_caption": ["Figure 39: RMSD for the joint model on the KS (left) and Kolmogorov (right) datasets for varying $P~|~C$ scenarios. For KS we use a window 9 model, whereas for Kolmogorov we use a window 5 model. "], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "", "page_idx": 43}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/8ac3760b946a42e9bd72c42e4820b222c8946c11f12b4ab146a45a17c8e88c3f.jpg", "img_caption": ["Figure 40: RMSD for the universal amortised model on the KS (left) and Kolmogorov (right) datasets for varying $P\\mid C$ scenarios. For KS we use a window 9 model, whereas for Kolmogorov we use a windw 5 model. "], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "I Frequency Analysis ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Lippe et al. [42] showed that capturing high-frequencies in the spectrum is crucial for accurate longterm rollouts. Following their analysis we compute the frequency spectra of a one-step prediction for all the methods considered in Sec. 5 for both KS and Kolmogorov. In Fig. 41, we compare the spectrum of a one-step prediction at the beginning and end of the trajectory. In the case of KS, the spectra of the states generated by our models do not vary significantly depending on how far away they are from the initial state. In contrast, PDE-Refiner significantly overestimates the amplitude of high frequencies as the states depart from the initial conditions. However, we can see that at the beginning of the trajectory, PDE-Refiner can capture higher frequencies more accurately than all the considered methods. ", "page_idx": 44}, {"type": "text", "text": "For Kolmogorov the frequency spectrum is not as varied as the one from KS. Possible reasons include the coarser spatial discretisation used or the nature of the underlying PDE. We compute and plot the average frequency spectrum for the two channels (i.e. the two velocity components). From Fig. 42, we do not see much difference between the spectra of different models\u2014all models are capable of capturing the ground truth spectrum well. ", "page_idx": 44}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/858d3bd4dc7fc159ed95e5dc8b7a9eba0c82a14917c722bed4390fb22d9a3603.jpg", "img_caption": ["Figure 41: Analysis of the spectra of two KS trajectories for different methods (Joint AR, universal amortised, PDE-Refiner, MSE-trained U-Net, MSE-trained FNO). Each row corresponds to a different trajectory. The first column shows the spectra of states closer to the initial state, while the second corresponds to states further away. "], "img_footnote": [], "page_idx": 44}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/a4adba31ae1625faf4413e49f829daf464322a77efcf1778943e96ee90855fe1.jpg", "img_caption": ["Figure 42: Analysis of the spectrum of a Kolmogorov trajectory for different methods (Joint AR, universal amortised, PDE-Refiner, MSE-trained U\u2013Net, MSE-trained FNO). Each row corresponds to a different time step (left - closer to the initial state, right - closer to end of trajectory), and each column to a different channel. "], "img_footnote": [], "page_idx": 45}, {"type": "text", "text": "J Stability of long rollouts ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "To investigate the long-term behaviour of the different diffusion models, we generate a very long trajectory $\\langle T=2000\\Delta\\tau$ , corresponding to 400 seconds) for the KS equation. We expect the samples to diverge from the ground truth after a certain number of states, but the question is whether our models still generate physically plausible predictions. We illustrate in Fig. 43 an example of a long trajectory generated by the joint AR (middle) and universal amortised (bottom) models, showing that the generated states remain visually plausible throughout the trajectory. We also analyse the spectra of different one-step predictions along the trajectories in Fig. 44. We can see that both models manage to capture the low frequencies well, but overestimate the high-frequency components. This holds for all the different steps we considered\u2014a state close to the initial (known) ones, a state in the middle of the long trajectory, and a state towards the end of the trajectory. This implies that the spectra do not change qualitatively along the trajectory length and hence, remain physically plausible throughout the trajectory. ", "page_idx": 45}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/f2198e407e29ef4bcff0899323c2b716a0887247dcd94a58b8c80c0189887a48.jpg", "img_caption": ["Figure 43: Example of a long trajectory $2000\\Delta\\tau$ steps, corresponding to 400s) generated by the joint AR (middle) and universal amortised (bottom) models. Note how, even if we are generating longer trajectories than what the models have been trained on, the states remain visually plausible. ", "Time (in seconds) "], "img_footnote": [], "page_idx": 45}, {"type": "image", "img_path": "nQl8EjyMzh/tmp/aba3287cf383b2fc2baeec6bb6af56cb8b893b23d9243d6328c61b364c9b14ae.jpg", "img_caption": ["Figure 44: Frequency spectra of a state close to the initial state (top left), in the middle of the trajectory (top right), and towards the end of the trajectory (bottom). In each plot, we show the ground truth spectrum (blue), as well as the spectra of the trajectories generated by the joint AR (blue) and universal amortised models (orange). "], "img_footnote": [], "page_idx": 46}, {"type": "text", "text": "K Algorithms ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Algorithm 1 Predictor-corrector (PC) sampling as implemented in [67, 60]. We will refer to this as the function predictor-corrector-step(N,CS) ", "page_idx": 46}, {"type": "text", "text": "Input: Discretisation steps $N$ for the reverse-time SDE, corrector steps CS   \nOutput: Trajectory $x(0)$   \nInitialise $x(T)\\sim p_{T}$   \nfor $i=N-1$ to 0 do $x(t_{i})\\gets$ Predictor $(x(t_{i+1}))$ \u25b7Using Eq. (5) as score for $j=1$ to $C S$ do $x(t_{i})\\gets\\mathrm{Corrector}(x(t_{i}))$ \u25b7Using Eq. (5) as score end for   \nend for   \nreturn $x(0)$ ", "page_idx": 46}, {"type": "text", "text": "Algorithm 2 All-at-once (AAO) rollout as in [60] ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Input: Trajectory length $L$ , conditioning states $x_{1:C}$ , discretisation steps $N$ for the reverse-time   \nSDE, corrector steps $C S$   \nOutput: Trajectory $x_{1:L}$   \n$x_{1:L}(0)\\gets$ predictor-corrector-step(N,CS) $\\triangleright$ Using Algorithm 2 from [60] to get an   \napproximation of the score of the full trajectory and using guidance   \nreturn $x(0)$ ", "page_idx": 46}, {"type": "text", "text": "Algorithm 3 AR rollout with a joint model ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Input: Trajectory length $L$ , Markov window $M$ , initial true conditioning states $x_{1:C}$ , prediction   \nstates $P=M-C$ , discretisation steps $N$ for the reverse-time SDE, corrector steps $C S$   \nOutput: Trajectory $x_{1:L}$   \nInitialize $x_{\\mathrm{gen}}\\leftarrow[x_{1:C}]$   \nwhile $\\mathsf{e n}(\\bar{x}_{\\mathsf{g e n}})\\leq L$ do Define conditioning states as $x_{\\mathrm{{gen}}}[-C$ :] \u25b7Selecting last $C$ states from $x_{\\mathrm{gen}}$ $x_{1:M}(0)\\gets$ predictor-corrector-step(N,CS) $\\triangleright$ This has length $\\bar{M}$ $x_{\\mathrm{gen}}\\leftarrow x_{\\mathrm{gen}}+[x_{1:M}(0)[C:]]$ \u25b7Selecting the last $P$ states   \nend while   \n$\\begin{array}{r}{x_{1:L}=x_{\\mathrm{gen}}}\\\\ {\\mathbf{return}\\;x_{1:L}}\\end{array}$ ", "page_idx": 47}, {"type": "text", "text": "Algorithm 4 AR rollout with a universal or classic amortised model ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Input: Trajectory length $L$ , Markov window $M$ , initial true conditioning states $x_{1:C}$ , prediction   \nstates $P=M-C$ , discretisation steps $N$ for the reverse-time SDE, corrector steps $C S$   \nOutput: Trajectory $x_{1:L}$   \nInitialize $x_{\\mathrm{gen}}\\leftarrow[x_{1:C}]$   \nwhile $\\mathsf{l e n}(\\bar{x}_{\\mathsf{g e n}})\\leq L$ do Define conditioning states as $x_{\\mathrm{{gen}}}[-C$ :] \u25b7Selecting last $C$ states from $x_{\\mathrm{gen}}$ $x_{1:P}(0)\\gets$ predictor-corrector-step(N,CS) $\\triangleright$ This has length $P$ $x_{\\mathrm{gen}}\\leftarrow x_{\\mathrm{gen}}+[x_{1:P}(0)]$   \nend while   \nx1:L = xgen   \nreturn x1:L ", "page_idx": 47}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: The experimental contributions are supported by Sec. 5 in the main text of the paper. We also provide extended results in App. G and App. F. The theoretical findings are discussed in App. B.1 and Sec. 3.3. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 48}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We discuss limitation of our work in the Sec. 6. In addition to that, we present a runtime analysis of the different rollout strategies both in the main paper, see Sec. 3.3 and in the supplementary material App. F.1. We report the computational cost in minutes for the DA task in Sec. 5. One possible additional limitation of our work is that the results are computed using a single trained model with one random seed. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 48}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: In our paper, we theoretically justified the Markov score assumption in App. B. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 49}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: We present a full description of the data generation process for all the three datasets considered in the paper in App. D. In App. D.4 we present all the architecture and training details for all three experiments. In the appendix, we also report results for different parameter settings. We include provisional code together with the submission and we plan to make the code publicly available upon acceptance. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 49}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 50}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: Together with the paper submission we are including a starting code to run some of the experiments contained in the paper. Due to memory constraints, we are not able to share the exact data used for KS and Kolmogorov. However, we are providing all necessary instructions for data generation. In the README file included in the code, we explain how to run the code. We plan to make the code publicly available upon acceptance. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 50}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Justification: We are reporting all the dataset and hyperparameters details in App. D and App. D.4. In addition to that, the submission code also contains these details. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 50}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: In all the plots and tables we present in the main paper we explain in the caption how the error bars are computed, i.e. we compute mean and standard error over all the test trajectories. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 51}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: In App. D.4, we summarised the hardware we used for training our models and running our experiments. Since we relied on a cluster with multiple GPUs, it is challenging to provide a precise mapping between the experiment and the specific GPU type used. However, everything was run on a single GPU. We did not track how many runs were used for the hyperparameters search. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 51}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: The experiments considered in this paper deal with PDE simulation. We used open-source code that we cited in the paper to generate the data and we believe the paper conforms to the Code of Ethics. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. ", "page_idx": 51}, {"type": "text", "text": "\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 52}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: This work tackles the task of generating realistic trajectories of dynamical systems with underlying physics described by partial differential equations (PDEs). PDEs are ubiquitous in Science and Engineering fields, as they are the foundation for describing and solving many practical problems. Fluid dynamics, which studies the flow of liquids and gases, is perhaps the field that relies the most on solving PDEs. Applications can range from profiling turbines with optimal performance when designing wind turbines or hydroelectric dams, to optimising the aerodynamics of airborne objects such as planes or missiles. Our work, however, does not engage with any of these tasks specifically. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 52}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: Since our work focuses on score-based models for PDE simulations, we believe it does not pose such risks. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ", "page_idx": 52}, {"type": "text", "text": "\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 53}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: In the paper, we use open-source code for both data generation and the baselines. We credited them both in the paper and in App. D and App. D.4. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 53}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: The assets introduced by this paper are in the form of new models, i.e. code. We described in detail the architecture choices and the training details of the different models in App. D.4 and the data generation process in App. D. In addition to that, we made the whole codebase publicly available. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 53}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 53}, {"type": "text", "text": "\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 54}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 54}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 54}]