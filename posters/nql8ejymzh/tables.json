[{"figure_path": "nQl8EjyMzh/tables/tables_1_1.jpg", "caption": "Table 1: Score-based methods considered in this work. Each method can be classified in terms of the score network (joint or conditional), the rollout strategy at sampling time, and the conditioning mechanism at inference time.", "description": "The table compares three different score-based diffusion models used in the paper for forecasting and data assimilation tasks.  It categorizes them based on three key characteristics: the type of score network (joint or conditional, indicating whether it is trained on the full joint distribution or conditioned on some observations), the rollout strategy used during sampling (all-at-once or autoregressive), and how conditioning is handled during inference (guidance or architectural).", "section": "1 Introduction"}, {"figure_path": "nQl8EjyMzh/tables/tables_9_1.jpg", "caption": "Table 2: RMSD (\u2193) (mean \u00b1 3 standard errors over test trajectories) for online DA for the KS and Kolmogorov datasets. The c in AAO (c) refers to the number of corrector steps used.", "description": "This table presents the results of the online data assimilation experiments.  It shows the root mean squared deviation (RMSD) for the Kuramoto-Sivashinsky (KS) and Kolmogorov datasets.  Different models are compared: Joint AR, Joint AAO (0), Joint AAO (1), and Universal Amortised. The number after the parentheses in 'Joint AAO' indicates the number of corrector steps used in the sampling process. Lower RMSD values indicate better performance.", "section": "5.3 Online DA: combining forecasting and data assimilation"}, {"figure_path": "nQl8EjyMzh/tables/tables_23_1.jpg", "caption": "Table 3: Hyperparameters used for the SDA U-Net architecture inspired by [60] in the three datasets considered in the paper. Blue refers to specific information about the joint model and red refers to the universal amortised model.", "description": "This table lists the hyperparameters used for training the score-based diffusion models. The hyperparameters are specific to each dataset (Burgers, KS, Kolmogorov) and model type (joint or universal amortised). The table shows choices made for the following: residual blocks per level, channels per level, kernel size, padding, activation function, normalization method, optimizer, weight decay, learning rate, scheduler, number of epochs, and batch size.", "section": "3.1 Learning the score"}, {"figure_path": "nQl8EjyMzh/tables/tables_23_2.jpg", "caption": "Table 4: Hyperparameters used for the PDE-Refiner U-Net architecture inspired by [42] in the forecasting experiment for the KS and Kolmogorov datasets. Blue refers to specific information about the joint model and red refers to the universal amortised model.", "description": "This table presents the hyperparameters used for training the PDE-Refiner U-Net architecture, inspired by the work of [42], in the forecasting experiments. It includes details about residual blocks per level, channels per level, kernel size, padding, activation, normalization, optimizer, weight decay, learning rate, scheduler, epochs and batch size for both Kuramoto-Sivashinsky (KS) and Kolmogorov datasets.  The table highlights the specific differences in hyperparameters between the joint and universal amortised models using blue and red color coding.", "section": "3.2 Conditioning"}, {"figure_path": "nQl8EjyMzh/tables/tables_34_1.jpg", "caption": "Table 5: The best high correlation time (mean \u00b1 3 standard errors over test trajectories) achieved for two different architectures-SDA, inspired by [60], and PDE-Refiner, inspired by [42]. For the diffusion-based models, the conditioning scenario P | C that corresponds to the best performance is indicated between brackets.", "description": "This table presents the best high correlation time achieved by four different models (Joint AR, Univ. Amortised, Amortised, and MSE U-Net) using two different architectures (SDA and PDE-Refiner) across two datasets (KS and Kolmogorov).  The best performance for each model/architecture combination, along with the corresponding conditioning parameters (P|C), is shown.  The results highlight the variation in performance between different models and architectures across the two datasets.", "section": "F.6 Results for different architectures"}, {"figure_path": "nQl8EjyMzh/tables/tables_36_1.jpg", "caption": "Table 6: Best \u03b3 corresponding to each conditioning scenario P for different proportion of data observed on the Kolmogorov dataset for the joint model with window size 5.", "description": "This table shows the best value of the guidance strength hyperparameter (\u03b3) for different conditioning scenarios (P) and varying proportions of observed data for the Kolmogorov dataset. The experiments were performed using a joint model with a window size of 5.  The optimal value of \u03b3 appears to be dependent on both the conditioning scenario and the proportion of data observed. ", "section": "G.1.1 Conditioning for DA in the universal amortised model"}, {"figure_path": "nQl8EjyMzh/tables/tables_36_2.jpg", "caption": "Table 7: Best \u03b3, \u03c3y and P for different proportions of data observed on the KS dataset for universal amortised model with window size 9.", "description": "This table shows the best hyperparameter settings (\u03b3, \u03c3y, P) for the universal amortised model with a window size of 9,  for different proportions of observed data in the offline data assimilation task on the Kuramoto-Sivashinsky (KS) dataset.  The parameters were tuned to optimize the model's performance for each sparsity level.  The best predictive horizon (P), guidance strength (\u03b3), and observation noise standard deviation (\u03c3y) are presented for each data sparsity level.", "section": "G.1.1 Conditioning for DA in the universal amortised model"}, {"figure_path": "nQl8EjyMzh/tables/tables_37_1.jpg", "caption": "Table 8: Best \u03b3, \u03c3y and P for different proportions of data observed on the Kolmogorov dataset for universal amortised model with window size 5.", "description": "This table shows the best hyperparameter settings for the universal amortised model on the Kolmogorov dataset for different levels of data sparsity. For each proportion of observed data, the table shows the best predictive horizon (P), guidance strength (\u03b3), and observation noise standard deviation (\u03c3y).  These settings were determined empirically through a hyperparameter sweep and aim to optimize performance in data assimilation tasks within the model.", "section": "G.1.1 Conditioning for DA in the universal amortised model"}]