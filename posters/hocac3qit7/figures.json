[{"figure_path": "hocAc3Qit7/figures/figures_1_1.jpg", "caption": "Figure 1: Conceptual understanding of learning velocities in abstract cognitive environments. a. Grid cells generate hexagonal tuning in spatial navigation experiments (illustration borrowed from [1]). b. Similar grid-like tuning has been found in auditory frequency space in rodents [2], (c.) in visual space in monkeys [3], and (d.) in an abstract cartoon bird space in human experiments. e. Example of an abstract, non-spatial domain called \u2018Stretchy Blob\u2019, a 2D Gaussian that can either stretch or shrink along two axes. f. SR, CSCG, and TEM [4\u20137] learn transition structures of this cognitive domain by traversing it. These models require one to build a set of representations for these encountered states as well as structures enabling transitions between them. g. Our approach learns a self-consistent movement (velocity) signal that is independent of the states traversed and encodes the global transition structure of the environment in a minimally low-dimensional representation. h. How can the brain initially extract a general notion of velocity within abstract, non-spatial domains? Solving this important prerequisite challenge can show how grid cells flexibly map and organize various abstract spaces. i. The actual movement signals in this particular example are low-dimensional. These velocities are independent from the states they connect.", "description": "This figure provides a conceptual overview of how the brain learns velocities in abstract cognitive environments. It contrasts traditional approaches that learn state representations and transitions with the proposed approach that extracts low-dimensional, state-independent velocity signals for efficient map generation. The figure uses examples like spatial navigation, auditory tone sequences, visual space, and abstract cartoon spaces to illustrate the concept, highlighting the flexibility of grid cells in mapping diverse abstract domains.", "section": "1 Introduction"}, {"figure_path": "hocAc3Qit7/figures/figures_4_1.jpg", "caption": "Figure 2: Task and fundamental problem setup. a. Two states in the \u2018Stretchy Blob\u2019 task that have a low-dimensional velocity representing the transition between them. b. If two consecutive images in this space i\u2081 and i2 are separated by a velocity v\u2081\u2192\u2082, can we learn a function f that estimates this velocity and a function g that \u2018applies\u2019 this quantity to i\u2081 to predict i\u2082? c. The key self-consistency constraint we introduce called \u2018loop-closure\u2019: estimated velocities along a closed-loop trajectory must sum to zero. This computation is performed by a neural integrator, such as grid cells. d. Our various procedurally generated abstract cognitive domains: Stretchy Blob (2D), Moving Blobs (2D), Stretchy Bird (2D), Stretchy Bird (3D), and Frequency Modulation (1D).", "description": "This figure illustrates the core concepts and tasks used in the paper's proposed model.  Panel (a) shows a simple example of a transition in the 2D Stretchy Blob abstract space. Panel (b) defines the core problem: learning a function (f) to estimate velocity from two consecutive images and another function (g) to predict the next image using the estimated velocity. Panel (c) introduces the \u2018loop closure\u2019 constraint, essential for self-supervised learning. Finally, panel (d) shows the five different abstract cognitive domains used to evaluate the model.", "section": "2 Self-supervision for velocity extraction"}, {"figure_path": "hocAc3Qit7/figures/figures_5_1.jpg", "caption": "Figure 3: Self-supervised learning framework. a. Model diagram consisting of an encoder, decoder, and an integrator which acts on the low-dimensional velocity latent space. The model takes in two consecutive input frames and predicts an unseen frame with the learned velocity. b. Our various self-supervised loss terms. The two critical loss terms (top) are \u2018next-state prediction\u2019 and \u2018loop-closure\u2019. The auxiliary loss terms (bottom) which further refine the solution space are \u2018shortcut estimation\u2019 and \u2018isotropy\u2019.", "description": "This figure illustrates the self-supervised learning framework proposed in the paper.  Panel (a) shows the architecture of the model, which consists of an encoder, a decoder, and an integrator. The encoder takes two consecutive input frames and outputs a low-dimensional velocity representation. The decoder takes the velocity and the previous frame as input and predicts the next frame. The integrator sums up the velocities to enforce the loop closure constraint. Panel (b) details the different loss functions used to train the model, including next-state prediction, loop closure, shortcut estimation, and isotropy losses. These losses ensure that the model learns a consistent and faithful representation of velocity in abstract cognitive domains.", "section": "2 Self-supervision for velocity extraction"}, {"figure_path": "hocAc3Qit7/figures/figures_6_1.jpg", "caption": "Figure 4: Model results. a. Our model produces low-dimensional velocity latents that are similar to the ground truth (g.t.) distribution without knowing this distribution across a variety of cognitive environments. b. In cases where the model's latent dimensionality is higher than the intrinsic velocity dimensionality of the environment, our model still identifies the lowest-dimensional representations embedded in higher-dimensional space.", "description": "This figure displays the results of the model's performance on various abstract cognitive tasks.  Panel (a) shows that the model accurately infers low-dimensional velocity representations that align well with the ground truth, even without prior knowledge of the underlying distribution.  Panel (b) demonstrates the model's robustness:  even when given a higher-dimensional latent space than the intrinsic dimensionality of the task, the model still effectively extracts the minimal, essential dimensions of the velocity.", "section": "3 Experimental results"}, {"figure_path": "hocAc3Qit7/figures/figures_7_1.jpg", "caption": "Figure 5: Comparison to baselines. We show our model comparisons to various dimensionality reduction and motion-prediction baselines in the a. 2D Moving Blobs and b. 1D Frequency Modulation tasks. Existing baselines cannot identify the low-dimensional velocity signals between arbitrary transitions in this space, even failing to do so in a simple one-dimensional domain. Meanwhile, our model produces results that closely match the true, underlying velocity distribution.", "description": "This figure compares the performance of the proposed model against several baselines for dimensionality reduction and motion prediction on two tasks: 2D Moving Blobs and 1D Frequency Modulation.  It demonstrates that existing methods struggle to identify low-dimensional velocity representations in these tasks, while the proposed model accurately recovers the underlying low-dimensional velocity distribution.", "section": "3.2 Comparison to existing dimensionality reduction methods"}, {"figure_path": "hocAc3Qit7/figures/figures_8_1.jpg", "caption": "Figure 6: Dimensionality reduction through our model, versus PCA. a. Schematic of the raw input states from a random trajectory of the 2D Moving Blob task, showing the states as points in a 16x16 dimensional space. b. We estimate 3-dimensional velocities between states, and integrate these estimated velocities to obtain a low-dimensional representation of the initial input states. A 2-dimensional plane is shown in gray for perspective, demonstrating our model-produced low-dimensional representations are approximately in a 2D subspace. c. Left: Computing PCA on the same dataset shows representations occupying a volume in a 3-dimensional space. Right: Around 24 dimensions are required for PCA to capture 95% of the variance in the data, indicating that PCA is unable to find a low-dimensional space describing the dataset.", "description": "This figure compares the dimensionality reduction performance of the proposed model against Principal Component Analysis (PCA).  Panel (a) shows high-dimensional input states as points in a 16x16 dimensional space. Panel (b) demonstrates how the model estimates 3D velocities between these states, integrating them to produce a low-dimensional (approximately 2D) representation. In contrast, Panel (c) shows that PCA requires many dimensions (around 24) to capture 95% of the variance, highlighting the model's superior ability to find a low-dimensional structure in high-dimensional data.", "section": "3 Experimental results"}, {"figure_path": "hocAc3Qit7/figures/figures_8_2.jpg", "caption": "Figure 2: Task and fundamental problem setup. a. Two states in the \u2018Stretchy Blob\u2019 task that have a low-dimensional velocity representing the transition between them. b. If two consecutive images in this space i\u2081 and i2 are separated by a velocity v1\u21922, can we learn a function f that estimates this velocity and a function g that \u2018applies\u2019 this quantity to i\u2081 to predict i2? c. The key self-consistency constraint we introduce called \u2018loop-closure\u2019: estimated velocities along a closed-loop trajectory must sum to zero. This computation is performed by a neural integrator, such as grid cells. d. Our various procedurally generated abstract cognitive domains: Stretchy Blob (2D), Moving Blobs (2D), Stretchy Bird (2D), Stretchy Bird (3D), and Frequency Modulation (1D).", "description": "This figure illustrates the task setup and fundamental problems addressed in the paper. It shows how a low-dimensional velocity can represent the transition between two states in an abstract cognitive domain. It also highlights the self-consistency constraint ('loop-closure') for efficient map generation and introduces several procedurally generated abstract cognitive domains used in the experiments.", "section": "2 Self-supervision for velocity extraction"}, {"figure_path": "hocAc3Qit7/figures/figures_13_1.jpg", "caption": "Figure 8: Continued, comparison to baselines. a. Model inferred velocity space in the 2D Stretchy Bird environment compared with baselines. b. Model inferred velocity space in the 3D Stretchy Bird environment compared with baselines. c. Model inferred velocity space in the 2D Stretchy Blob environment compared with baselines.", "description": "This figure compares the performance of the proposed model against several baseline dimensionality reduction and motion extraction methods across three different abstract cognitive domains.  Panels (a), (b), and (c) show the inferred velocity spaces generated by the proposed model and the baselines for the 2D Stretchy Bird, 3D Stretchy Bird, and 2D Stretchy Blob domains, respectively.  The color coding represents the underlying trajectory in each domain.  The goal is to demonstrate the superior performance of the proposed model in accurately capturing the low-dimensional velocity structure underlying transitions within these complex high-dimensional domains.", "section": "3 Experimental results"}, {"figure_path": "hocAc3Qit7/figures/figures_13_2.jpg", "caption": "Figure 9: Can baseline models produce faithful representations of velocity? We pipe the outputs of our baselines into the same synthetic grid cell network to observe grid firing rates along a trajectory. Qualitatively, our model produces the most hexagonal grid firing field in comparison to other baselines.", "description": "This figure compares the results of different dimensionality reduction methods, including the authors' proposed model, in generating grid-like firing fields.  The model's outputs were used as inputs to a synthetic grid cell network. The figure visually demonstrates that the authors' model performs better at producing hexagonal patterns characteristic of grid cells compared to traditional methods like PCA, Isomap, UMAP, autoencoder, and MCNet.  This showcases the superior ability of their method to extract a low-dimensional representation of velocity that is suitable for grid cell integration across diverse tasks and environments.", "section": "A Additional experiments"}, {"figure_path": "hocAc3Qit7/figures/figures_14_1.jpg", "caption": "Figure 10: Decoder respects boundaries inferred from the extent of the training data. a. Starting from a random point in the 2D Stretchy Bird environment, we use the decoder to estimate the state after applying a velocity v. We then iteratively apply this same velocity v to the generated state estimate from the previous step, generating a series of states obtained by traversing the environment while following this fixed velocity. We find that once the neck and leg length have maximally shrunk to the extent observed in the training data, the decoder arrives at a fixed point. Thus, the decoder performs state-dependent transformations: after reaching an inferred boundary of the training data, further velocities along the same direction do not continue to transform the data along that dimension.", "description": "This figure shows that the decoder in the model implicitly learns the boundaries of the training data manifold.  When given a velocity that would push the state outside the observed range of the training data (in this case, a bird with extreme neck and leg lengths), the decoder stops generating further changes in the state, effectively respecting the boundaries of the training data.", "section": "A.2 Decoder implicitly learns boundaries of training data manifold"}, {"figure_path": "hocAc3Qit7/figures/figures_14_2.jpg", "caption": "Figure 11: Loss Ablation studies. Loss ablation studies in the 2D Moving Blobs environment show that most of velocity estimation learning process comes from our two critical loss terms, \u2018next-state prediction\u2019 and \u2018loop-closure\u2019. Further loss terms refine the solution space. Grid cell firing rates show that model generated velocities are faithful representations of the true, underlying velocity distribution.", "description": "This figure presents the ablation study on the loss functions. It shows that the next-state prediction loss and loop closure loss are the most critical for accurate velocity extraction. Adding two auxiliary losses (shortcut and isotropy) further refines the solution and produces more faithful representations of the true, underlying velocity distribution, which is demonstrated using the grid cell firing rates.", "section": "B Ablation studies"}, {"figure_path": "hocAc3Qit7/figures/figures_16_1.jpg", "caption": "Figure 6: Dimensionality reduction through our model, versus PCA. a. Schematic of the raw input states from a random trajectory of the 2D Moving Blob task, showing the states as points in a 16x16 dimensional space. b. We estimate 3-dimensional velocities between states, and integrate these estimated velocities to obtain a low-dimensional representation of the initial input states. A 2-dimensional plane is shown in gray for perspective, demonstrating our model-produced low-dimensional representations are approximately in a 2D subspace. c. Left: Computing PCA on the same dataset shows representations occupying a volume in a 3-dimensional space. Right: Around 24 dimensions are required for PCA to capture 95% of the variance in the data, indicating that PCA is unable to find a low-dimensional space describing the dataset.", "description": "This figure compares the dimensionality reduction performance of the proposed model against PCA.  Panel (a) shows high-dimensional data points from a trajectory. Panel (b) demonstrates that the model effectively reduces these to a low-dimensional representation (approximately 2D in this case),  while panel (c) shows that PCA requires significantly more dimensions to capture a similar amount of variance, highlighting the model's superior dimensionality reduction capabilities.", "section": "3 Experimental results"}]