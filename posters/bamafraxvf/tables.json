[{"figure_path": "BAmAFraxvf/tables/tables_7_1.jpg", "caption": "Table 1: GazeFollow dataset. The best scores are given in bold, while the second best are underlined.", "description": "This table presents a comparison of different methods for gaze target localization and recognition on the GazeFollow dataset.  The methods include various baselines (using different conditioning techniques on the predicted gaze heatmap) and the proposed method from the paper.  The metrics used are Average Distance (Avg. Dist.), Minimum Distance (Min. Dist.), Accuracy@1 (Acc@1), Accuracy@3 (Acc@3), and MultiAccuracy@1 (MultiAcc@1).  Lower Avg. Dist. and Min. Dist. values indicate better localization, while higher Accuracy values show better recognition performance.  The best performing method for each metric is highlighted in bold, with the second-best underlined. The baseline models use the proposed gaze heatmap prediction model (except for the baseline which uses a model from Chong et al. [10]).", "section": "5.2 Comparison with the State-of-the-art"}, {"figure_path": "BAmAFraxvf/tables/tables_7_2.jpg", "caption": "Table 2: Results of our model and baselines on the Gaze-HOI dataset. The best scores are given in bold, while the second best are underlined. The \u2020 sign means the model was trained on GazeFollow and evaluated on GazeHOI without fine-tuning.", "description": "This table presents the performance comparison of different models on the Gaze-HOI dataset.  The models include a random baseline, a majority baseline, several variants of a proposed two-stage baseline, and the authors' proposed model.  Both localization (GazeAcc) and recognition (Acc@1 and Acc@3) metrics are reported.  The baseline models are variations of the same overall architecture that differ in how the predicted gaze heatmap is used to condition an image before being processed by CLIP to generate the gaze label embedding. One baseline model was additionally fine-tuned on the GazeFollow dataset. The authors' model is presented in two versions: one trained only on GazeHOI and another pretrained on GazeFollow and further fine-tuned on GazeHOI.  The table highlights the best performing model in each metric.", "section": "5.2 Comparison with the State-of-the-art"}, {"figure_path": "BAmAFraxvf/tables/tables_8_1.jpg", "caption": "Table 1: GazeFollow dataset. The best scores are given in bold, while the second best are underlined.", "description": "This table presents a comparison of different methods for gaze target localization and recognition on the GazeFollow dataset.  It shows the average and minimum Euclidean distances between predicted and ground truth gaze locations, as well as the accuracy of gaze target class prediction at different thresholds. The results highlight the superior performance of the proposed method compared to existing baselines and variants of the proposed model.", "section": "5.2 Comparison with the State-of-the-art"}, {"figure_path": "BAmAFraxvf/tables/tables_18_1.jpg", "caption": "Table 1: GazeFollow dataset. The best scores are given in bold, while the second best are underlined. All baselines use our own model for gaze heatmap prediction, except for Baseline\u2020 which uses [10].", "description": "This table presents a comparison of different methods for gaze target localization and recognition on the GazeFollow dataset.  The results are shown in terms of Average Distance (Avg. Dist.), Minimum Distance (Min. Dist.), Accuracy@1 (Acc@1), Accuracy@3 (Acc@3), and MultiAccuracy@1 (MultiAcc@1).  The best performance for each metric is highlighted in bold, and the second-best is underlined.  The table includes results for several baselines, which primarily differ in how the predicted gaze heatmap is used to condition the input image before feeding it into a classification model.  One baseline uses a pre-trained model from a different paper ([10]) for comparison. The table demonstrates the superior performance of the proposed method in this paper.", "section": "5.2 Comparison with the State-of-the-art"}]