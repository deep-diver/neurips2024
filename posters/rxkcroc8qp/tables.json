[{"figure_path": "RxkcroC8qP/tables/tables_5_1.jpg", "caption": "Table 1: Quantitative assessments of the reconstruction quality for EEG, MEG, and fMRI in Subject 8. For detailed explanations of the metrics.", "description": "This table presents a quantitative comparison of the reconstruction quality achieved using EEG, MEG, and fMRI data for Subject 8.  Several metrics are used to evaluate the quality of the reconstructed images, including Pixel Correlation (PixCorr), Structural Similarity Index (SSIM), and scores from AlexNet (layers 2 and 5), Inception, CLIP, and SwAV.  Higher scores generally indicate better reconstruction quality.", "section": "3.3 Image Generation Performance"}, {"figure_path": "RxkcroC8qP/tables/tables_14_1.jpg", "caption": "Table 2: Brain module configuration", "description": "This table details the architecture of the Adaptive Thinking Mapper (ATM), the EEG encoder used in the proposed framework.  It lists each layer of the ATM, including its input and output shapes and the number of parameters. The table provides a quantitative overview of the model's complexity and the computational resources required for its operation.", "section": "2.1 ATM for EEG Embedding"}, {"figure_path": "RxkcroC8qP/tables/tables_14_2.jpg", "caption": "Table 3: Ablation study on the ATM model's different components for THINGS-EEG retrieval.", "description": "This table presents the results of an ablation study conducted on the Adaptive Thinking Mapper (ATM) model, a tailored EEG encoder.  The study systematically removed different components of the ATM model to assess their individual contributions to the model's performance on the THINGS-EEG dataset's retrieval task.  The components tested are MLP, Temporal-Spatial Convolution (TSC), and Channel-wise Attention (CAL). The table shows the Top-1 and Top-5 accuracies achieved by different configurations of the ATM model. This helps to understand the relative importance of each component in the overall performance.", "section": "B.3 Architecture details"}, {"figure_path": "RxkcroC8qP/tables/tables_15_1.jpg", "caption": "Table 4: Impact of each module on the result in different configurations. The reported results represent the mean performance metrics of the ATM-S, calculated over the final 10 training epochs across all 10 subjects.", "description": "This table presents the ablation study results on the ATM model. It shows the impact of each module (Channel-wise attention layer, Token embedding, Feed Forward Network, Position encoding, Temporal spatial convolution) on the Top-1 and Top-5 retrieval accuracy. Different configurations of each module are tested and their corresponding accuracies are reported.  The results highlight the contribution of each component to the overall model's performance.", "section": "B.3 Architecture details"}, {"figure_path": "RxkcroC8qP/tables/tables_18_1.jpg", "caption": "Table 5: Latent VAE retrieval performance", "description": "This table presents the Top-1 and Top-5 retrieval accuracies achieved using latent variables from a Variational Autoencoder (VAE).  The results are compared against the ideal chance level, which represents the accuracy expected by random chance. The ATM-S (Ours) row shows the results obtained using the authors' proposed method.  The table demonstrates that the proposed method significantly outperforms random chance in image retrieval tasks, suggesting its effectiveness in extracting meaningful representations from EEG data.", "section": "C.3 Low-level pipeline"}, {"figure_path": "RxkcroC8qP/tables/tables_20_1.jpg", "caption": "Table 6: The classification performance of various methods are discussed. Due to differences in datasets and data modalities, we have specified unified metrics to objectively assess the performance of each method.", "description": "This table compares the classification performance of different models on two datasets: GOD-Wiki (fMRI) and THINGS (MEG/EEG).  The performance is evaluated using top-1 and top-5 accuracy across three different numbers of image categories (50-way, 100-way, and 200-way). The table highlights the superior performance of the proposed ATM model on the THINGS dataset, especially in the MEG modality.", "section": "D Performance comparison"}, {"figure_path": "RxkcroC8qP/tables/tables_20_2.jpg", "caption": "Table 1: Quantitative assessments of the reconstruction quality for EEG, MEG, and fMRI in Subject 8. For detailed explanations of the metrics.", "description": "This table presents a quantitative comparison of the reconstruction quality achieved using EEG, MEG, and fMRI data for Subject 8.  It compares various metrics including Pixel Correlation (PixCorr), Structural Similarity Index (SSIM), and the top-1 and top-5 accuracy using AlexNet (layers 2 and 5), Inception, CLIP, and SwAV. Higher values for most metrics generally indicate better reconstruction quality.", "section": "3.3 Image Generation Performance"}, {"figure_path": "RxkcroC8qP/tables/tables_35_1.jpg", "caption": "Table 8: Overall performance of zero-shot Retrieval on THINGS-EEG dataset. We divided the last batch from the original training set as the validation set, selected the best model according to the minimum validation loss in 40 epochs, and finally evaluated the performance on the test set. We showed in-subject and cross-subject retrieval task performance (Ave \u00b1 Std.%) under the condition of batch size=1024. We compared the 2-way, 4-way, 10-way, the Top-1 and Top-5 accuracy of 200-way from different EEG embedding methods.", "description": "This table presents a comprehensive comparison of the zero-shot retrieval performance using different EEG embedding methods on the THINGS-EEG dataset. It breaks down the results for both subject-dependent (training and testing on the same subject) and subject-independent (leave-one-subject-out) scenarios, across various retrieval tasks (2-way, 4-way, 10-way, Top-1, and Top-5) and with a batch size of 1024. The table highlights the superior performance of the proposed ATM-S and ATM-E methods compared to existing methods.", "section": "H Additional evaluation results"}]