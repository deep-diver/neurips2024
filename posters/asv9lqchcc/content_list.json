[{"type": "text", "text": "GoMatching: A Simple Baseline for Video Text Spotting via Long and Short Term Matching ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Haibin $\\mathbf{H}\\mathbf{e}^{1*}$ , Maoyuan $\\mathbf{Y}\\mathbf{e}^{1}\\colon$ \u2217, Jing Zhang1\u2020, Juhua ${\\bf L i u}^{1\\dagger}$ , $\\mathbf{Bo}\\,\\mathbf{Du}^{1}$ , Dacheng Tao2 ", "page_idx": 0}, {"type": "text", "text": "1 School of Computer Science, National Engineering Research Center for Multimedia Software, and Institute of Artificial Intelligence, Wuhan University, China 2 College of Computing & Data Science at Nanyang Technological University {haibinhe, yemaoyuan, liujuhua, dubo}@whu.edu.com jingzhang.cv@gmail.com, dacheng.tao@gmail.com https://github.com/Hxyz-123/GoMatching ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Beyond the text detection and recognition tasks in image text spotting, video text spotting presents an augmented challenge with the inclusion of tracking. While advanced end-to-end trainable methods have shown commendable performance, the pursuit of multi-task optimization may pose the risk of producing sub-optimal outcomes for individual tasks. In this paper, we identify a main bottleneck in the state-of-the-art video text spotter: the limited recognition capability. In response to this issue, we propose to efficiently turn an off-the-shelf query-based image text spotter into a specialist on video and present a simple baseline termed GoMatching, which focuses the training efforts on tracking while maintaining strong recognition performance. To adapt the image text spotter to video datasets, we add a rescoring head to rescore each detected instance\u2019s confidence via efficient tuning, leading to a better tracking candidate pool. Additionally, we design a long-short term matching module, termed LST-Matcher, to enhance the spotter\u2019s tracking capability by integrating both long- and short-term matching results via Transformer. Based on the above simple designs, GoMatching delivers new records on ICDAR15-video, DSText, BOVText, and our proposed novel test set with arbitrary-shaped text termed ArTVideo, which demonstrates GoMatching\u2019s capability to accommodate general, dense, small, arbitrary-shaped, Chinese and English text scenarios while saving considerable training budgets. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Text spotting has received increasing attention due to its various applications, such as video retrieval [1] and autonomous driving [2]. Recently, numerous image text spotting (ITS) methods [3\u20136] that simultaneously tackle text detection and recognition, have attained extraordinary accomplishment. In the video realm, video text spotting (VTS) involves a tracking task additionally. Although VTS methods [7\u201312] make significant progress, a substantial discrepancy persists when compared to ITS. We observe that the text recognition proficiency of VTS models is far inferior to ITS models. To investigate this issue, we compare the state-of-the-art (SOTA) VTS model TransDETR [12] and ITS model Deepsolo [5] for image-level text spotting performance on ICDAR15-video [13] and our established ArTVideo (i.e., Arbitrary-shaped Text in Video) test set (Sec.4.1) which comprises about $30\\%$ curved text. ", "page_idx": 0}, {"type": "text", "text": "As illustrated in Fig. 1(a), even when evaluating the image-level spotting performance on the VTS model\u2019s training set, the F1-score of TransDETR is only comparable to the zero-shot performance of ", "page_idx": 0}, {"type": "image", "img_path": "ASv9lQcHCc/tmp/d7fc85582d475dc85f7ab68b9f3a42e8ccfe73f8cb2018b62f95c34d29b94188.jpg", "img_caption": ["Figure 1: (a) \u2018Gap between Spot. & Det.\u2019: the gap between spotting and detection F1-score. As the spotting task involves recognizing the results of the detection process, the detection score is indeed the upper bound of spotting performance. The larger the gap, the poorer the recognition ability. Compared to the ITS model (Deepsolo [5]), the VTS model (TransDETR [12]) presents unsatisfactory image-level text spotting F1-scores, which lag far behind its detection performance, especially on ArTVideo with curved text. It indicates recognition capability is a main bottleneck in the VTS model. (b) GoMatching outperforms TransDETR by over 12 MOTA on ICDAR15-video while saving 197 training GPU hours and 10.8GB memory. Notice that since the pre-training strategies and settings vary between TransDETR and GoMatching, the comparison is focused on the fine-tuning stage. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Deepsolo. The performance of the VTS model on ArTVideo is much worse. Moreover, there is a huge gap between the spotting and detection-only performance of the VTS model, which indicates that the recognition capability is the main bottleneck. We attribute this discrepancy to two key aspects: 1) the model architecture and 2) the training data. First, in terms of model architecture, ITS studies [5, 6] have presented the advantages of employing advanced query formulation for text spotting in DETR frameworks [14, 15]. In contrast, existing Transformer-based VTS models still rely on Region of Interest (RoI) components or simply cropping detected text regions for recognition. On the other hand, some studies [16, 17] have indicated that there exists optimization conflict in detection and association during the end-to-end training of MOTR [18]. We hold that TransDETR [12], which further incorporates text recognition into MOTR-based architecture, may also suffer from optimization confilct. Second, regarding the training data, most text instances in current video datasets [13, 10, 19] are straight or oriented, and the bounding box labels are only quadrilateral, which constrains the data diversity and recognition performance as well. Overall, the limitations in model architecture and data probably lead to the unsatisfactory text spotting performance of the SOTA VTS model. Hence, leveraging model and data knowledge from ITS presents considerable value for VTS. ", "page_idx": 1}, {"type": "text", "text": "To achieve this, a straightforward approach is to take an off-the-shelf SOTA image text spotter and focus the training efforts on tracking across frames, akin to tracking-by-detection methods. An important question is how to efficiently incorporate a RoI-free image text spotter for VTS. In this paper, we propose a simple baseline via lonG and short term Matching, termed GoMatching, which leverages an off-the-shelf RoI-free image text spotter to identify text from each single frame and associates text instances across frames with a strong tracker. ", "page_idx": 1}, {"type": "text", "text": "Specifically, we select the state-of-the-art DeepSolo [5] as the image text spotter and design a LongShort Term Matching-based tracker termed LST-Matcher. Initially, to adapt the DeepSolo to video datasets while preserving its inherent knowledge, we freeze Deepsolo and introduce a rescoring mechanism. This mechanism entails training an additional lightweight text classifier called rescoring head via efficient tuning, and recalibrating confidence scores for detected instances to mitigate performance degradation caused by the image-video domain gap. The final score for each instance is determined by a fusion operation between the original score provided by the image text spotter and the calibrated score acquired from the rescoring head. The identified text instances are then sent to LST-Matcher for association. LST-Matcher can effectively harnesses both long- and short-term information, making it a highly capable tracker. As a result, our baseline significantly surpasses existing SOTA methods by a large margin with much lower training costs, as shown in Fig. 1(b). ", "page_idx": 1}, {"type": "text", "text": "In summary, the contribution of this paper is threefold. 1) We identify the limitations in current VTS methods and propose a novel and simple baseline, which leverages an off-the-shelf image text spotter with a strong customized tracker. 2) We introduce the rescoring mechanism and longshort term matching module to adapt image text spotter to video datasets and enhance the tracker\u2019s capabilities. 3) We establish the ArTVideo test set for addressing the absence of curved texts in current video datasets and evaluating the text spotters on videos with arbitrary-shape text. Extensive experiments on public challenging datasets and the established ArTVideo test set demonstrate the effectiveness of our baseline and its outstanding performance with less training budgets. For example, GoMatching achieves the highest ranking on ICDAR15-video and DSText. Especially on bilingual dataset BOVText, GoMatching obtains a $45\\%$ improvement on MOTA compared to the recorded best performance [11]. On curved text dataset ArTVideo, GoMatching also surpasses previous SOTA method [12] by a substantial margin. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Multi-Object Tracking ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Multi-object tracking methods follow the tracking-by-detection (TBD) or tracking-by-querypropagation (TBQP) pipeline. TBD methods [20\u201322] employ detectors for localization and then use association algorithms to get object trajectories. Different from extending tracks frame-by-frame, GTR [23] proposes to generate entire trajectories at once in Transformer. TBQP paradigm extends query-based object detectors[14, 15] to tracking. MOTR [18] detects object locations and serially updates its tracking queries for detecting the same items in the following frames, achieving an end-to-end solution. However, MOTR suffers from optimization conflict between detection and association [16, 17], resulting in inferior detection performance. For the VTS task which additionally involves text recognition, a naive way of training all modules end-to-end may also lead to optimization conflict. In contrast, we explore inheriting prior knowledge of text spotting from ITS models while focusing on the tracking task. ", "page_idx": 2}, {"type": "text", "text": "2.2 Image Text Spotting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Early approaches [24\u201326] crafted RoI-based modules to bridge text detection and recognition. However, these methods ignored one vital issue, i.e., the synergy problem between the two tasks. To overcome this dilemma, recent Transformer-based methods [27, 3, 28, 6] get rid of the fetters of RoI modules, and chase a better representation for the two tasks. For example, DETR-based TESTR [4] uses two decoders for each task in parallel. In contrast, DeepSolo [5] proposes a unified and explicit query form for the two tasks, without harnessing dual decoders. However, the above methods cannot perform tracking in the video. ", "page_idx": 2}, {"type": "text", "text": "2.3 Video Text Spotting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Compared to ITS, existing SOTA VTS methods still rely on RoI for recognition. CoText [11] adopts a lightweight text spotter with Masked-RoI, then uses several encoders to fuse features derived from the spotter, and finally feeds them to a tracking head with cosine similarity matching. TransDETR [12] performs detection and tracking under the MOTR paradigm and then uses Rotated-RoI to extract features for the subsequent recognizer. They pursue training all modules in an end-to-end manner. In comparison, we explore how to efficiently turn a RoI-free ITS model into a VTS one. We reveal the probability of freezing off-the-shelf ITS part and focusing on tracking, thereby saving training budgets while reaching SOTA performance. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The architecture of GoMatching is presented in Fig. 2. It consists of a frozen image text spotter, a rescoring head, and a Long-Short Term Matching module (LST-Matcher). We adopt an outstanding off-the-shelf image text spotter (i.e., DeepSolo) and freeze its parameters, with the aim of introducing strong text spotting capability into VTS while significantly reducing training cost. In DeepSolo, there are $p$ sequences of queries used for final predictions, with each storing comprehensive semantics for a text instance. To alleviate spotting performance degradation caused by the image-video domain gap, we devise a rescoring mechanism, which determines the confidence scores for text instances by considering both the scores from the image text spotter and a new trainable rescoring head. Finally, we design LST-Matcher to generate instance trajectories by leveraging long-short term information. ", "page_idx": 2}, {"type": "image", "img_path": "ASv9lQcHCc/tmp/ac4bb267b4c32239000433b50e5915fbce98f1bffd3fb68e463708db33a3283e.jpg", "img_caption": ["Figure 2: The overall architecture of GoMatching. The frozen image text spotter provides text spotting results for frames. The rescoring mechanism considers both instance scores from the image text spotter and a trainable rescoring head to reduce performance degradation due to the domain gap. Long-short term matching module (LST-Matcher) assigns IDs to text instances based on the queries in long-short term frames. The yellow star sign \u2018\u22c6\u2019 indicates the final output of GoMatching. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.2 Rescoring Mechanism ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Owing to the domain gap between image and video datasets, employing a frozen image text spotter for direct prediction may result in relative low recall due to low text confidence, further leading to a reduction in end-to-end spotting performance. To ease this issue, we devise a rescoring mechanism via a lightweight rescoring head and a simple score fusion operation. Specifically, the rescoring head is designed to recompute the score for each query from the decoder in the image text spotter. It consists of a simple linear layer and is initialized with the parameters of the image text spotter\u2019s classification head. The score fusion operation then decides the final scores by considering both the scores from the image text spotter and the rescoring head. Let $C_{o}^{t}=\\{c_{o_{1}}^{t},...,\\dot{c_{o_{p}}^{t}}\\}$ cto }be a set of original scores produced by image text spotter in frame $t$ . $C_{r}^{t}=\\{c_{r_{1}}^{t},...,c_{r_{p}}^{t}\\}$ is a set of recomputed scores obtained from the rescoring head. We obtain the maximum value for each query as the final score, denoted as $C_{f}^{t}=\\{c_{f_{1}}^{t}=\\bar{m}a x(c_{o_{1}}^{t},c_{r_{1}}^{t}),...,c_{f_{p}}^{t}=m a x(c_{o_{p}}^{t},c_{r_{p}}^{t})\\}$ . With final scores, the queries in frames are filtered by a threshold before being sent to LST-Matcher for association. ", "page_idx": 3}, {"type": "text", "text": "3.3 Long-Short Term Matching Module ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Long-short term matching module (LSTMatcher) consists of two sub-modules: the Short Term Matching module (ST-Matcher) and the Long Term Matching module (LT-Matcher), which own the same structure. ST-Matcher is steered to match simple instances between adjacent frames into trajectories, while LT-Matcher is responsible for using long term information to address the unmatched instances due to severe occlusions or strong appearance changes. Each of them contains a one-layer Transformer encoder and a one-layer Transformer decoder [23]. We use a simple multi-layer perceptron (MLP) to map the flitered text instance queries into embeddings as the input, getting rid of using RoI features as in most existing MOT methods. In the encoder, historical embeddings are enhanced by self-attention. The decoder takes embeddings in the current frame as query and enhanced his", "page_idx": 3}, {"type": "image", "img_path": "ASv9lQcHCc/tmp/fde1824c3e2d111a6a8ba1d879331a6884581b57e6c038cf889973afe446af29.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 3: The inference pipeline of LST-Matcher, which is a two-stage association process: (1) STMatcher associates the instances with trajectories in previous frames as denoted by blue lines. (2) LT-Matcher associates the remaining unmatched instances by utilizing other trajectories in history frames as denoted by red lines. ", "page_idx": 3}, {"type": "text", "text": "torical embeddings as key for cross-attention, and computes the association score matrix. The current instances are then linked to the existing trajectories composed of historical embeddings or generate new trajectories according to the association score matrix. ", "page_idx": 3}, {"type": "text", "text": "To be specific, supposing a given clip including $T$ frames and $N_{t}$ text instances in frame $t$ after threshold filtering. $Q^{t}\\,{\\doteq\\,}\\{\\bar{q_{1}^{t}},...,q_{N_{t}}^{t}\\}$ is the set of text instance queries in frame $t$ . Initially, we use a two-layer MLP to map these frozen queries into embeddings $E^{t}=\\{e_{1}^{t},...,e_{N_{t}}^{t}\\}$ . The set of embeddings in all frames is denoted as $E^{L}=E^{1}\\cup...\\cup E^{T}$ . Let the universal set of embeddings in adjacent frames of the input batch be denoted as $E^{S}=E^{S_{2}}\\cup E^{S_{3}}\\cup\\ldots\\cup E^{S_{T}}$ and $E^{S_{t}}=E^{t-1}\\bar{\\cup}E^{t}$ . Based on the predictions of image text spotter, we obtain their corresponding bounding boxes $B^{t}=\\{b_{1}^{t},...,\\bar{b_{N_{t}}^{t}}\\}$ . Let $\\bar{\\boldsymbol{\\tau}}=\\{\\tau_{1},...,\\tau_{K}\\}$ be the set of ground-truth (GT) trajectories of all instances in the clip, where $\\boldsymbol{\\tau}_{k}=\\{\\tau_{k}^{1},...,\\tau_{k}^{T}\\}$ describes a tube of instance locations $\\tau_{k}^{t}\\in\\mathbb{R}^{4}\\cup\\{\\varnothing\\}$ through time. $\\tau_{k}^{t}=\\emptyset$ means the absence of instance $k$ in frame $t$ . Let $\\hat{\\alpha}_{k}^{t}$ be the matched instance index for $\\tau_{k}^{t}$ according to the following equation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\alpha}_{k}^{t}=\\left\\{\\begin{array}{l l}{\\emptyset,\\quad\\mathrm{if~}\\tau_{k}^{t}=\\emptyset\\mathrm{~or~max}_{i}(I o U(b_{i}^{t},\\tau_{k}^{t}))<0.5}\\\\ {\\operatorname*{argmax}_{i}(I o U(b_{i}^{t},\\tau_{k}^{t})),\\quad\\mathrm{otherwise}}\\end{array}\\right..}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "ST-Matcher calculates a short-term association score $v_{i}^{t}(e_{\\hat{\\alpha}_{k}^{t}}^{t},E^{S_{t}})\\,\\in\\,\\mathbb{R}^{N_{S_{t}}}$ for $i$ -th instances in frame $t$ , where $e_{\\hat{\\alpha}_{k}^{t}}^{t}\\in\\mathbb{R}^{D}$ is a trajectory query and $N_{S_{t}}\\,=\\,N_{t}+N_{t-1}$ . LT-Matcher calculates a long-term trajectory-specific association score $u_{i}^{t}(e_{k},E^{L})\\in\\mathbb{R}^{N}$ for $i$ -th instances in frame $t$ , where $\\begin{array}{r}{\\boldsymbol{e}_{k}\\in\\{e_{\\hat{\\alpha}_{k}^{1}}^{1},e_{\\hat{\\alpha}_{k}^{2}}^{2},...,e_{\\hat{\\alpha}_{k}^{T}}^{T}\\},N=\\sum_{t=1}^{T}N_{t}}\\end{array}$ . Specifically, when $v_{i}^{t}(e_{\\hat{\\alpha}_{k}^{t}}^{t},E^{S_{t}})=0$ and $u_{i}^{t}(e_{k},E^{L})=0$ , it means no association at time $t$ . Then, ST-Matcher and LT-Matcher can predict distributions of short-term and long-term associations for all instance $i$ in frame $t$ which can be written as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nP_{s_{a}}(e_{\\hat{\\alpha}_{k}^{t}}^{t},E^{S_{t}})=\\frac{\\exp(v_{i}^{t}(e_{\\hat{\\alpha}_{k}^{t}}^{t},E^{S_{t}}))}{\\sum_{j\\in\\{\\varnothing,1,...,N_{t}\\}}\\exp(v_{j}^{t}(e_{\\hat{\\alpha}_{k}^{t}}^{t},E^{S_{t}}))},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\nP_{l_{a}}(e_{k},E^{L})=\\frac{\\exp(u_{i}^{t}(e_{k},E^{L}))}{\\sum_{j\\in\\{\\emptyset,1,...,N_{t}\\}}\\exp(u_{j}^{t}(e_{k},E^{L}))}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To ensure sufficient training of ST-Matcher and LT-Matcher, embeddings set $E^{S}$ and $E^{L}$ are fed into ST-Matcher and LT-Matcher during training, respectively. ", "page_idx": 4}, {"type": "text", "text": "During inference, we engage a memory bank to store the instance trajectories from $H$ history frames for long term association. All flitered instances in each frame are further processed by non-maximumsuppression (NMS) before being fed into LST-Matcher for association. Unlike the training phase, where ST-Matcher and LT-Matcheder are independent of each other, LST-Matcher comprises a two-stage associating procedure as described in Fig. 3. Concretely, ST-Matcher first matches the embedding $E^{t}$ in the current frame $t$ with the trajectories $\\tau^{t-1}$ in the previous frame $t-1$ . Then, LT-Matcher employs other trajectories \u03c4 oHth in the memory bank to associate the unmatched ones $E_{s_{-}u}^{t}$ with low association score in ST-Matcher caused by the heavy occlusion or strong appearance changes. If the association score with any trajectory calculated in ST-Matcher or LT-Matcher is higher than a threshold $\\theta$ , the instance is linked to the trajectory with the highest score. Otherwise, this instance is used to initiate a new trajectory. Finally, we combine the trajectories $\\tau_{s}$ and $\\tau_{l}$ predicted by ST-Matcher and LT-Matcher to obtain new trajectories \u03c4 Nfor tracking in the next frame. ", "page_idx": 4}, {"type": "text", "text": "3.4 Optimization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Rescoring Loss. To train the rescoring head, we following DETR [14] and use Hungarian algorithm [29] to find a bipartite matching $\\hat{\\sigma}$ between the prediction set $\\hat{Y}$ and the ground truth set $Y$ with minimum matching cost $\\mathcal{C}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\sigma}=\\arg\\operatorname*{min}_{\\sigma}\\sum_{i}^{N}\\mathcal{C}(Y_{i},\\hat{Y}_{\\sigma(i)}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $N$ is the number of ground truth instances per frame. The cost $\\mathcal{C}$ can be defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{C}(Y_{i},\\hat{Y}_{\\sigma(i)})=\\lambda_{c}\\mathcal{L}_{c l s}(\\hat{p}_{\\sigma(i)}(c_{i}))+\\lambda_{b}\\sum_{1}^{N}\\|b_{i}-\\hat{b}_{\\sigma(i)}\\|,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda_{c}$ and $\\lambda_{b}$ serve as the hyper-parameters to balance different tasks. $\\hat{p}_{\\sigma(i)}(c_{i})$ and $\\hat{b}_{\\sigma(i)}$ are the probability for ground truth class $c_{i}$ and the predicition of bounding box respectively, and $b_{i}$ ", "page_idx": 4}, {"type": "text", "text": "represents the ground truth bounding box. $\\mathcal{L}_{c l s}$ is the focal loss [30]. Specifically, the focal loss for training the rescoring head can be formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{r e s}=\\sum_{1}^{\\infty}[-1_{\\{c_{i}\\neq\\emptyset\\}}\\alpha(1-\\hat{p}_{\\{\\bar{\\sigma}(i)}}(c_{i}))^{\\gamma}\\log(\\hat{p}_{\\bar{\\sigma}(i)}(c_{i}))-1_{\\{c_{i}=\\bar{\\sigma}\\}}(1-\\alpha)(\\hat{p}_{\\bar{\\sigma}(i)}(c_{i}))^{\\gamma}\\log(1-\\hat{p}_{\\bar{\\sigma}(i)}(c_{i}))]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\alpha$ and $\\gamma$ are the hyper-parameters of focal loss. ", "page_idx": 5}, {"type": "text", "text": "Long-Short Association Loss. In ST-Matcher, we only consider each trajectory in the universal set of adjacent frames, while in LT-Matcher we consider each trajectory in all long term frames. For each trajectory, we optimize the log-likelihood of its assignments $\\hat{\\alpha}_{k}$ following GTR [23]: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\overset{\\smile}{\\mathcal{L}_{s_{-}a s s}}(E^{S},\\hat{\\tau}_{k})=-\\sum_{t=2}^{T}\\log P_{s_{a}}(\\hat{\\alpha}_{k}^{t}|e_{\\hat{\\alpha}_{k}^{t}}^{t},E^{S_{t}}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{l_{-}a s s}(E^{L},\\hat{\\tau}_{k})=-\\sum_{w}\\sum_{t=1}^{T}\\log P_{l_{a}}(\\hat{\\alpha}_{k}^{t}|E_{\\hat{\\alpha}_{k}^{w}}^{w},E^{L}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $w\\in\\{1,...,T|\\hat{\\alpha}_{k}^{w}\\neq\\varnothing\\}$ . ", "page_idx": 5}, {"type": "text", "text": "In ST-Matcher and LT-Matcher, empty trajectories would be generated for these unassociated queries, and their optimization goals can be defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\,}}\\\\ {{\\mathcal{L}_{s_{-}b g}(E^{S})=-\\displaystyle\\sum_{j:\\neq\\hat{\\pm}\\hat{\\alpha}_{k}^{t}=j}\\sum_{t=2}^{T}\\log P_{s_{a}}(\\alpha^{t}=\\emptyset|e_{j}^{t},E^{S_{t}}),}}\\\\ {{\\mathcal{L}_{l_{-}b g}(E^{L})=-\\displaystyle\\sum_{w=1}^{T}\\sum_{j:\\neq\\hat{\\alpha}_{k}^{w}=j}\\sum_{t=1}^{T}\\log P_{l_{a}}(\\alpha^{t}=\\emptyset|E_{j}^{w},E^{L}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Finally, we can get the long-short association loss as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{a s s o}=\\mathcal{L}_{s_{-}b g}+\\mathcal{L}_{l_{-}b g}+\\sum_{\\hat{\\tau}_{k}}(\\mathcal{L}_{s_{-}a s s}+\\mathcal{L}_{l_{-}a s s}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Overall Loss. Combined with the rescore loss $\\mathcal{L}_{r e s}$ in Eq. (6) and the long-short association loss $\\mathcal{L}_{a s s o}$ in Eq. (11), the final training loss can be defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\lambda_{r e s}\\mathcal{L}_{r e s}+\\lambda_{a s s o}\\mathcal{L}_{a s s o},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the hyper-parameters $\\lambda_{r e s}$ and $\\lambda_{a s s o}$ are the weights of $\\mathcal{L}_{r e s}$ and $\\mathcal{L}_{a s s o}$ , respectively. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Datasets and Evaluation Metrics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "ICDAR15-video [13] is a word-level video text reading benchmark annotated with quadrilateral bounding boxes, comprising a training set of 25 videos and a test set of 24 videos. It focuses on wild scenarios, such as driving on the road, exploring shopping streets, walking in a supermarket, etc. ", "page_idx": 5}, {"type": "text", "text": "BOVText [10] is a large-scale, bilingual, and open-world benchmark for video text spotting, encompassing English and Chinese. The dataset is meticulously collected from YouTube and KuaiShou with different scenarios. The text box annotations are represented as quadrilaterals at the textline level. ", "page_idx": 5}, {"type": "text", "text": "DSText [19] is a newly proposed dataset, and focuses on dense and small text reading challenges in the video. This dataset provides 50 training videos and 50 test videos. Compared with the previous datasets, DSText mainly includes the following three new challenges: dense video texts, high-proportioned small texts, and various new scenarios, e.g., \u2018Game\u2019, \u2018Sports\u2019, etc. Similar to ICDAR15-video, DSText adopts word-level annotations, which are labeled with quadrilaterals. ", "page_idx": 5}, {"type": "text", "text": "ArTVideo is a novel word-level test set established in this work to evaluate the performance of arbitrary-shaped video text, which is absent in the VTS community. It contains 20 videos with about $30\\%$ curved text instances. Straight text is annotated with quadrilaterals, while curved text is annotated with polygons. More details are provided in the Appendix A. ", "page_idx": 5}, {"type": "text", "text": "Evaluation Metrics. To evaluate performance, we adopt three evaluation metrics commonly used in ICDAR15-video competition and DSText competition, including MOTA [31], MOTP, and IDF1 [32]. ", "page_idx": 5}, {"type": "text", "text": "4.2 Implementation Details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In all experiments, we only use a single NVIDIA GeForce RTX 3090 (24G) GPU to train and evaluate GoMatching. As for the image text spotter in GoMatching, we apply the officially released DeepSolo [5]. During fine-tuning GoMatching on downstream video datasets, we only update the rescoring head and LST-Matcher, while keeping DeepSolo frozen. More inference settings can be seen in the Appendix B. ", "page_idx": 6}, {"type": "text", "text": "Training Setting. The text spotting part of GoMatching is initialized with off-the-shelf DeepSolo weights and kept frozen in all experiments. We optimize other modules on video datasets. We follow EfficientDet [33] to adopt the scale-and-crop augmentation strategy with a resolution of 1280. The batch size $T$ is 6. All frames in a batch are from the same video. Text instances with fusion scores higher than 0.3 are fed into the LST-Matcher during training. AdamW [34] is used as the optimizer. We adopt the warmup cosine annealing learning rate strategy with the initial learning rate being set to 0.00005. The loss weights $\\lambda_{r e s}$ and $\\lambda_{a s s o}$ are set to 1.0 and 0.5, respectively. For focal loss, $\\alpha$ is 0.25 and $\\gamma$ is 2.0 as in [14, 5]. The model is trained for $30\\mathbf{k}$ iterations on all downstream video datasets. ", "page_idx": 6}, {"type": "text", "text": "4.3 Comparison with State-of-the-art Methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Results on ICDAR15-video. To evaluate the effectiveness of GoMatching on oriented video text, we conduct a comparison with the state-of-the-art methods on ICDAR15-video presented in Table 1a. As can be seen, GoMatching ranks first in all metrics on the ICDAR15-video leaderboard. By effectively combining a robust image text spotter with a strong tracker, GoMatching improves the best performance by $5.08\\%$ MOTA, $0.75\\%$ MOTP, and $3.16\\%$ IDF1, respectively. Furthermore, owing to the substantial enhancement in recognition and tracking capabilities (details can be found in Sec. 4.4 and Appendix G), GoMatching outperforms the current SOTA single-model method TransDETR by $11.08\\%$ MOTA, $3.92\\%$ MOTP, and $7.31\\%$ IDF1, respectively. ", "page_idx": 6}, {"type": "text", "text": "Results on BOVText. Except for the English word recognition scenario, GoMatching can readily adapt to other video text recognition scenarios, such as Chinese text line recognition. For BOVText, which focuses on English and Chinese textline recognition, we employ the DeepSolo trained on bilingual textline datasets and then fine-tune GoMatching on BOVText. The results are presented in Table 1b. It is evident that GoMatching achieves a new record on the BOVText dataset and surpasses previous methods significantly. GoMatching exhibits superior performance over the previous SOTA method CoText [11], with improvements of $41.5\\%$ on MOTA, $6.9\\%$ on MOTP, and $14.3\\%$ on IDF1. Such exceptional performance of GoMatching on BOVText suggests its proficiency in spotting both Chinese and English text in videos. Moreover, it can be easily extended to other languages by adapting the image spotter. ", "page_idx": 6}, {"type": "text", "text": "Results on DSText. We further conduct experiments on DSText with dense and small video text scenarios. Results are presented in Table 1c. It is worth noting that most of the previous methods on the DSText leaderboard used an ensemble of multiple models and large public datasets to enhance their performance [19]. For example, TencentOCR integrates the detection results of DBNet [35] and Cascade MaskRCNN [36] built with multiple backbone architectures, combines the Parseq [37] text recognizer, and further improves the end-to-end tracking with ByteTrack [22]. DA adopts Mask R-CNN [38] and DBNet to detect text, then uses BotSORT [21] to replace the tracker in VideoTextSCM [39] and employs the Parseq model for recognition. As a single model with a frozen image text spotter, GoMatching also shows competitive performance compared to other ensembling methods on the leaderboard. GoMatching ranks first $(22.83\\%)$ on MOTA, second $(80.43\\%)$ on MOTP, and third $(46.09\\%)$ on IDF1. Moreover, compared to the SOTA single-model method, GoMatching achieves substantial improvements of $45.46\\%$ and $19.66\\%$ on MOTA and IDF1, respectively. ", "page_idx": 6}, {"type": "text", "text": "Results on ArTVideo. We test TransDETR and GoMatching on ArTVideo to compare the zero-shot text spotting capabilities for arbitrary-shaped text. For a fair comparison, both TransDETR and GoMatching are trained on ICDAR15-video. Unlike ICDAR15-video and DSText which only have straight text, ArTVideo has a substantial number of curved text, so we report results under four settings: tracking results on both straight and curved text, spotting results on both straight and curved text, tracking results on curved text only, and spotting results on curved text only. As shown in Table 1d, GoMatching outperforms TransDETR under all settings. Especially when involving an additional recognition task (end-to-end spotting) or only considering curved text, the performance advantages of GoMatching are more significant. This further confirms that the previous SOTA ", "page_idx": 6}, {"type": "text", "text": "Table 1: Comparison results with SOTA methods on four distinct datasets. \u2018\u2020\u2019 denotes that the results are collected from the official competition website. \u2018\\*\u2019: we use the officially released model for evaluation. \u2018M-ME\u2019 indicates whether multi-model ensembling is used. \u2018Y\u2019 and \u2018N\u2019 stand for yes and no. The best and second-best results are marked in bold and underlined, respectively. ", "page_idx": 7}, {"type": "table", "img_path": "ASv9lQcHCc/tmp/fcb549394c00f7bdbf1db630df9db043d1c2f03b7b5f3c38f239d18d354f0705.jpg", "table_caption": ["(a) Results on ICDAR15-video. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "ASv9lQcHCc/tmp/05e67d40901dd9a897dd6693e1bd1319695dc474a035d215de7583fae0a82b51.jpg", "table_caption": ["(b) Results on BOVText. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "ASv9lQcHCc/tmp/5640fbdf142e59947daad53c1c9ba2d1ff18fca5473e77f3077f29f9e9bfb18e.jpg", "table_caption": ["(c) Results on DSText. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "ASv9lQcHCc/tmp/a2e64ca82db572a74524e60f3eeff034043f3498fe1d864e139f29d5f80a2b12.jpg", "table_caption": ["(d) Results on ArTVideo. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "methods have unsatisfactory recognition capabilities and limited adaptability to complex scenarios. Furthermore, as shown in Fig. 1(b), GoMatching achieves excellent performance while significantly reducing the training budget. ", "page_idx": 7}, {"type": "text", "text": "Some visual results are provided in Fig. 4. It shows that GoMatching performs well on straight and curved text, and even more complex scene text. More visual results (including some failure cases) and analysis are provided in the Appendix G. ", "page_idx": 7}, {"type": "text", "text": "4.4 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We first conduct comprehensive ablation studies on ICDAR15-video to verify the effectiveness of each component. The experimental results are shown in Table 2. The impact of frame length on long-term association during inference is then studied, and the results are shown in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "Effectiveness of Utilizing Queries. Comparing the first two rows in Table 2, we can find that using queries from the decoder of image text spotter is more beneficial for tracking than RoI features. By leveraging the unified queries from frozen DeepSolo, $0.98\\%$ and $1.05\\%$ improvements on MOTA and IDF1 are achieved. This is because queries integrate more text instance information, i.e., unifying multi-scale features, text semantics, and position information, which has been proven effective in DeepSolo. Although position information is essential for tracking, it is ignored in RoI features. ", "page_idx": 7}, {"type": "text", "text": "Effectiveness of Rescoring Mechanism. To verify the effectiveness of the rescoring mechanism, we test three different scoring mechanisms: the original score from DeepSolo, the score recomputed by the rescoring head, and the fusion score from the rescoring mechanism. As shown in row 2 and row 3 of Table 2, the rescoring head can alleviate the performance degradation caused by the domain gap between ICDAR15-image and ICDAR15-video, and achieve gains of $1.25\\%$ and $0.97\\%$ on MOTA and IDF1, respectively. Moreover, as shown in row 4, we can observe that combining the knowledge of rescoring head learned from the new dataset with the prior knowledge of DeepSolo can further improve MOTA and IDF1 by $0.33\\%$ and $0.32\\%$ , respectively. Appendix F contains more results. ", "page_idx": 7}, {"type": "text", "text": "Effectiveness of LST-Matcher. In this part, we conduct three experiments to prove the effectiveness of the LST-Matcher. As shown in row 4 of Table 2, we only use LT-Matcher to associate high-score text instances in the current frame with trajectories in the tracking memory bank. In row 5, we only use ST-Matcher to associate high-score text instances in the current frame with trajectories of the previous frame. In addition, as shown in row 6, we employ both LT-Matcher and ST-Matcher to test LST-Matcher. We can easily observe that compared to LT-Matcher, LST-Matcher improves MOTA ", "page_idx": 7}, {"type": "image", "img_path": "ASv9lQcHCc/tmp/c6234a0476277ca72a36ec906411b32f0cce53f5fcdfbf74a7307b14ccec6043.jpg", "img_caption": ["Figure 4: Visual results of video text spotting. Images from top to bottom are the results on ICDAR15-video, BOVText, DSText, and ArTVideo, respectively. Text instances belonging to the same trajectory are assigned the same color. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 2: Impact of difference components in the proposed GoMatching. \u2018Query\u2019 indicates that LST-Matcher employs the queries of high-score text instances for association, otherwise RoI features. Column \u2018Scoring\u2019 indicates the employed scoring mechanism, in which \u2018O\u2019 means using the original scores from DeepSolo, \u2018R\u2019 means using the scores recomputed by the rescoring head, and \u2018F\u2019 means using the fusion scores obtained from the rescoring mechanism. ", "page_idx": 8}, {"type": "table", "img_path": "ASv9lQcHCc/tmp/dd2e2aa9680812d99398699fb56b2326b06a06c70ded23ba8887b561e2247aea.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "and IDF1 by $1.72\\%$ and $1.29\\%$ respectively, while compared to ST-Matcher, the improvement on MOTA and IDF1 are $1.12\\%$ and $5.1\\%$ , respectively. In Fig. 5, we also demonstrate that using LSTMatcher can effectively mitigate the issue of ID switches caused by the strong appearance changes due to motion blur. These results validate that combining short-term and long-term information leads to more robust tracking outcomes, thereby enhancing the performance of video text spotting. ", "page_idx": 8}, {"type": "text", "text": "Different training strategies. To investigate the impacts of different training strategies on GoMatching, we establish three distinct settings on the ICDAR15-video dataset, as shown in Table 3. 1) We only fine-tune the tracker while keeping the image text spotter frozen (the first row of Table 3). 2) We first fine-tune the image text spotter on images extracted from ICDAR15-video and then further fine-tune the tracker with the image text spotter fixed (the second row of Table 3). 3) We jointly train the spotter\u2019s decoder and tracker of GoMatching while trying different learning rates for the decoder (the last three rows of Table 3). As shown in the first two rows of Table 3, fine-tuning the image spotter on the downstream video dataset results in a performance decline compared to the default setting. This is due to two data-related factors: 1) minor variations in text content between frames in the same video lead to insufficient data diversity, causing the image text spotter to overfit more easily; and 2) image blurring from camera motion reduces the quality of data available for training the image text spotter. When the image text spotter and tracker are trained simultaneously (the last three rows), the model\u2019s performance significantly decreases. Even with the decoder\u2019s learning rate close to zero (i.e., 0.001), there is still a $1.13\\%$ drop in IDF1. As the decoder\u2019s learning rate increases, the performance decline becomes more pronounced. This indicates that naive joint optimization of text spotting and tracking is challenging, likely due to confilcts between the two tasks. In future work, ", "page_idx": 8}, {"type": "image", "img_path": "ASv9lQcHCc/tmp/87143d787adbbdb5c9973ed7d741524d9f6e8b04d59f1290e51644ec84049d68.jpg", "img_caption": ["Figure 5: Visualization results of ST-Matcher and LST-Matcher. The first row shows the failure case that suffers from the ID switches issue when using only ST-Matcher, caused by missed detection and erroneous matching. The second row shows that LST-Matcher effectively mitigates this issue via both long and short term matching. Text instances in the same color represent the same IDs. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 3: Results of GoMatching under various training settings on the ICDAR15-video dataset. \u2018Only Image Spotter\u2019 and \u2018Only Tracker\u2019 refer to fine-tuning either the image spotter or tracker with another module fixed. \u2018End-to-End\u2019 denotes that training the image spotter and tracker in an end-to-end manner. $\\bullet0.001^{\\bullet}$ , $\\bullet0.01^{\\bullet}$ and \u20180.1\u2019 correspond to the ratios of the learning rate employed by the decoder of the image text spotter relative to the base learning rate. Due to constraints in training resources, we only optimize the parameters of the decoder component for the image text spotter. ", "page_idx": 9}, {"type": "table", "img_path": "ASv9lQcHCc/tmp/b5ea5307f8be3ecedcd1e06ff03c0a11e51d6cef2840cc57de258f5fa77bd35e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "it is worth trying to establish larger, more diverse video text spotting datasets and to explore more effective multi-task optimization strategies. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a simple yet strong baseline, termed GoMatching, for video text spotting. GoMatching harnesses the talent of an off-the-shelf query-based image text spotter and only needs to tune a lightweight tracker, effectively addressing the limitations of previous SOTA methods in recognition. Specifically, we design the rescoring mechanism and LST-Matcher to adapt the image text spotter to unseen video datasets while empowering GoMatching with excellent tracking capability. Moreover, we establish a novel test set ArTVideo for the evaluation of video text spotting models on arbitrary-shaped text, filling the gap in this area. Experiments on public benchmarks and ArTVideo demonstrate the superiority of our GoMatching in terms of both spotting accuracy and training cost. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by the National Key Research and Development Program of China under Grant 2023YFC2705700, in part by the National Natural Science Foundation of China under Grant U23B2048, 62076186 and 62225113, in part by the Innovative Research Group Project of Hubei Province under Grant 2024AFA017, and in part by the Science and Technology Major Project of Hubei Province under Grant 2024BAB046. Dr Tao\u2019s research is partially supported by NTU RSR and Start Up Grants. The numerical calculations in this paper have been done on the supercomputing system in the Supercomputing Center of Wuhan University. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jianfeng Dong, Xirong Li, Chaoxi Xu, Xun Yang, Gang Yang, Xun Wang, and Meng Wang. Dual encoding for video retrieval by text. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(8):4065\u20134080, 2021.   \n[2] Chongsheng Zhang, Yuefeng Tao, Kai Du, Weiping Ding, Bin Wang, Ji Liu, and Wei Wang. Character-level street view text spotting based on deep multisegmentation network for smarter autonomous driving. IEEE Transactions on Artificial Intelligence, 3(2):297\u2013308, 2021.   \n[3] Yuliang Liu, Jiaxin Zhang, Dezhi Peng, Mingxin Huang, Xinyu Wang, Jingqun Tang, Can Huang, Dahua Lin, Chunhua Shen, Xiang Bai, et al. Spts v2: single-point scene text spotting. arXiv preprint arXiv:2301.01635, 2023.   \n[4] Xiang Zhang, Yongwen Su, Subarna Tripathi, and Zhuowen Tu. Text spotting transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9519\u20139528, 2022.   \n[5] Maoyuan Ye, Jing Zhang, Shanshan Zhao, Juhua Liu, Tongliang Liu, Bo Du, and Dacheng Tao. Deepsolo: Let transformer decoder with explicit points solo for text spotting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19348\u201319357, 2023.   \n[6] Mingxin Huang, Jiaxin Zhang, Dezhi Peng, Hao Lu, Can Huang, Yuliang Liu, Xiang Bai, and Lianwen Jin. Estextspotter: Towards better scene text spotting with explicit synergy in transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19495\u201319505, 2023.   \n[7] Xiaobing Wang, Yingying Jiang, Shuli Yang, Xiangyu Zhu, Wei Li, Pei Fu, Hua Wang, and Zhenbo Luo. End-to-end scene text recognition in videos based on multi frame tracking. In 2017 14th IAPR international conference on document analysis and recognition (ICDAR), volume 1, pages 1255\u20131260. IEEE, 2017.   \n[8] Zhanzhan Cheng, Jing Lu, Yi Niu, Shiliang Pu, Fei Wu, and Shuigeng Zhou. You only recognize once: Towards fast video text spotting. In Proceedings of the 27th ACM International Conference on Multimedia, pages 855\u2013863, 2019.   \n[9] Zhanzhan Cheng, Jing Lu, Baorui Zou, Liang Qiao, Yunlu Xu, Shiliang Pu, Yi Niu, Fei Wu, and Shuigeng Zhou. Free: A fast and robust end-to-end video text spotter. IEEE Transactions on Image Processing, 30:822\u2013837, 2020.   \n[10] Weijia Wu, Yuanqiang Cai, Debing Zhang, Sibo Wang, Zhuang Li, Jiahong Li, Yejun Tang, and Hong Zhou. A bilingual, openworld video text dataset and end-to-end video text spotter with transformer. arXiv preprint arXiv:2112.04888, 2021.   \n[11] Wejia Wu, Zhuang Li, Jiahong Li, Chunhua Shen, Hong Zhou, Size Li, Zhongyuan Wang, and Ping Luo. Real-time end-to-end video text spotter with contrastive representation learning. arXiv preprint arXiv:2207.08417, 2022.   \n[12] Weijia Wu, Yuanqiang Cai, Chunhua Shen, Debing Zhang, Ying Fu, Hong Zhou, and Ping Luo. End-to-end video text spotting with transformer. International Journal of Computer Vision, 132 (9):4019\u20134035, 2024.   \n[13] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chandrasekhar, Shijian Lu, et al. Icdar 2015 competition on robust reading. In 2015 13th international conference on document analysis and recognition (ICDAR), pages 1156\u20131160. IEEE, 2015.   \n[14] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213\u2013229. Springer, 2020.   \n[15] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. In ICLR, 2021.   \n[16] Yuang Zhang, Tiancai Wang, and Xiangyu Zhang. Motrv2: Bootstrapping end-to-end multiobject tracking by pretrained object detectors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22056\u201322065, 2023.   \n[17] En Yu, Tiancai Wang, Zhuoling Li, Yuang Zhang, Xiangyu Zhang, and Wenbing Tao. Motrv3: Release-fetch supervision for end-to-end multi-object tracking. arXiv preprint arXiv:2305.14298, 2023.   \n[18] Fangao Zeng, Bin Dong, Yuang Zhang, Tiancai Wang, Xiangyu Zhang, and Yichen Wei. Motr: End-to-end multiple-object tracking with transformer. In European Conference on Computer Vision, pages 659\u2013675. Springer, 2022.   \n[19] Weijia Wu, Yuzhong Zhao, Zhuang Li, Jiahong Li, Mike Zheng Shou, Umapada Pal, Dimosthenis Karatzas, and Xiang Bai. Icdar 2023 video text reading competition for dense and small text. arXiv preprint arXiv:2304.04376, 2023.   \n[20] Zhongdao Wang, Liang Zheng, Yixuan Liu, Yali Li, and Shengjin Wang. Towards real-time multi-object tracking. In European Conference on Computer Vision, pages 107\u2013122. Springer, 2020.   \n[21] Nir Aharon, Roy Orfaig, and Ben-Zion Bobrovsky. Bot-sort: Robust associations multipedestrian tracking. arXiv preprint arXiv:2206.14651, 2022.   \n[22] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng Weng, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang Wang. Bytetrack: Multi-object tracking by associating every detection box. In European Conference on Computer Vision, pages 1\u201321. Springer, 2022.   \n[23] Xingyi Zhou, Tianwei Yin, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Global tracking transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8771\u20138780, 2022.   \n[24] Minghui Liao, Guan Pang, Jing Huang, Tal Hassner, and Xiang Bai. Mask textspotter v3: Segmentation proposal network for robust scene text spotting. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XI 16, pages 706\u2013722. Springer, 2020.   \n[25] Wenhai Wang, Enze Xie, Xiang Li, Xuebo Liu, Ding Liang, Zhibo Yang, Tong Lu, and Chunhua Shen. Pan $^{++}$ : Towards efficient and accurate end-to-end spotting of arbitrarily-shaped text. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9):5349\u20135367, 2021.   \n[26] Yuliang Liu, Chunhua Shen, Lianwen Jin, Tong He, Peng Chen, Chongyu Liu, and Hao Chen. Abcnet v2: Adaptive bezier-curve network for real-time end-to-end text spotting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11):8048\u20138064, 2021.   \n[27] Yair Kittenplon, Inbal Lavi, Sharon Fogel, Yarin Bar, R Manmatha, and Pietro Perona. Towards weakly-supervised text spotting using a multi-task transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4604\u20134613, 2022.   \n[28] Maoyuan Ye, Jing Zhang, Shanshan Zhao, Juhua Liu, Tongliang Liu, Bo Du, and Dacheng Tao. Deepsolo $^{++}$ : Let transformer decoder with explicit points solo for multilingual text spotting. arXiv preprint arXiv:2305.19957, 2023.   \n[29] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83\u201397, 1955.   \n[30] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980\u20132988, 2017.   \n[31] Keni Bernardin and Rainer Stiefelhagen. Evaluating multiple object tracking performance: The clear mot metrics. EURASIP Journal on Image and Video Processing, 2008:1\u201310, 2008.   \n[32] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo Tomasi. Performance measures and a data set for multi-target, multi-camera tracking. In European conference on computer vision, pages 17\u201335. Springer, 2016.   \n[33] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efficientdet: Scalable and efficient object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10781\u201310790, 2020.   \n[34] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[35] Minghui Liao, Zhaoyi Wan, Cong Yao, Kai Chen, and Xiang Bai. Real-time scene text detection with differentiable binarization. In Proceedings of the AAAI conference on artificial intelligence, pages 11474\u201311481, 2020.   \n[36] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: High quality object detection and instance segmentation. IEEE transactions on pattern analysis and machine intelligence, pages 1483\u2013 1498, 2019.   \n[37] Darwin Bautista and Rowel Atienza. Scene text recognition with permuted autoregressive sequence models. In European Conference on Computer Vision, pages 178\u2013196. Springer, 2022.   \n[38] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017.   \n[39] Yuzhe Gao, Xing Li, Jiajian Zhang, Yu Zhou, Dian Jin, Jing Wang, Shenggao Zhu, and Xiang Bai. Video text tracking with a spatio-temporal complementary model. IEEE Transactions on Image Processing, pages 9321\u20139331, 2021.   \n[40] Yuliang Liu, Lianwen Jin, Shuaitao Zhang, Canjie Luo, and Sheng Zhang. Curved scene text detection via transverse and longitudinal sequence connection. Pattern Recognition, 90: 337\u2013345, 2019.   \n[41] Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida, Masakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles Mestre, Joan Mas, David Fernandez Mota, Jon Almazan Almazan, and Lluis Pere De Las Heras. Icdar 2013 robust reading competition. In 2013 12th international conference on document analysis and recognition, pages 1484\u20131493. IEEE, 2013. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A More Details of ArTVideo ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Due to the scarcity of curved text instances within existing video text spotting datasets, it is infeasible to evaluate the performance of video text spotting models on curved text. To flil this gap, we collected a test set named ArTVideo containing 20 video clips with a total of 884 frames, in which 18 videos were collected from YouTube and 2 videos from the BOVText test set. ArTVideo contains 6,526 text instances, including 4,632 straight text instances and 1,894 curved text instances, i.e., curved text accounts for about $30\\%$ . As shown in Fig. 6, we provide high-quality word-level annotations for both straight and curved text in two different annotated ways. The straight text is labeled with quadrilaterals, while for curved text, we follow the CTW1500 [40] and adopt a polygon with 14 points to annotate the text contour. More statistics about ArTVideo are shown in Fig. 7. ", "page_idx": 13}, {"type": "image", "img_path": "ASv9lQcHCc/tmp/aca0a5fd9e60e1a30776ab80e53f764ae20644d20eb5ce3e003b60841b212303.jpg", "img_caption": ["Figure 6: Visual examples from our ArTVideo. The straight and curved text are labeled with quadrilaterals and polygons, respectively. The same background color in different frames (columns) denotes the same instance. "], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "ASv9lQcHCc/tmp/404e4f1da52ea93d1546103195e71c3051732b975012981db93213546ec69acd.jpg", "img_caption": ["Figure 7: Statistics of ArTVideo. (a) and (b) show the distribution of text instance numbers in each frame and the distribution of the text length of each instance, respectively. (c) shows the word cloud of text annotations in ArTVideo. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Table 4: Ablation studies on the number of frames $(T)$ for long-term association in LT-Matcher, and the max number of history frames in tracking memory bank is $H=T-\\dot{1}$ ). Experiments are conducted on ICDAR15-video and the best results are marked in bold. ", "page_idx": 14}, {"type": "table", "img_path": "ASv9lQcHCc/tmp/51806a94937c835acef97f8d9d1e89cb050999495166f7f5bac914e2ac579faf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "ASv9lQcHCc/tmp/12f8d01d5dbaa1c68204af4fef1ed7f584e7f396b2b6f4d6b39d65c702441852.jpg", "table_caption": ["Table 6: Results of using different image sizes on ICDAR15-video. \u2018Size\u2019 means the size of the shorter side of the input image during inference. The best results are highlighted in bold. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "ASv9lQcHCc/tmp/80def4f6b95147cb015ee10c40fa51e3eaae8018f891b07e546b551abae93f59.jpg", "table_caption": ["Table 5: Results of different score fusion strategies on ICDAR5-video. \u2018Mean\u2019, \u2018Geomean\u2019, and \u2018Maximum\u2019 denote the arithmetic mean, geometric mean, and the maximum score fusion strategies, respectively. The best results are highlighted in bold. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "ASv9lQcHCc/tmp/df22b2015c37e2e28f020d12f1654d7282098e629975d0e3a068ed37d395fd84.jpg", "table_caption": ["Table 7: Comparison between TransDETR and GoMatching. \u2018T-Para.\u2019 and \u2018A-Para.\u2019 denote the number of all parameters and the trainable parameters in each model, respectively. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B More Details of Inference Settings ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Since there is no training set in ArTVideo, we directly use the model trained on ICDAR15-video to evaluate the generalization ability of GoMatching to arbitrary-shaped text. The association score threshold is set to 0.2. For ICDAR15-video, we set the shorter size of the input image to 800, 1000, and 1440, with 800 aligned with the setting in TransDETR and 1440 aligned with the setting in DeepSolo. As for BOVText, DSText, and ArTVideo, the shorter sizes are set to 1000, 1280, and 1440, respectively. All the ablation studies are conducted on the setting of 1440. ", "page_idx": 14}, {"type": "text", "text": "C Impact of the Frame Number in LT-Matcher ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Table 4, we further study and analyze the impact of the number of frames for long-term association in LT-Matcher during inference. For the text spotting task, since a single frame may have a large number of text instances, sometimes reaching hundreds, excessive historical frame information would weaken the discrimination of text instance features, resulting in erroneous matching results. Therefore, we conduct a hyper-parameter search and find that the optimal frame number is 6. ", "page_idx": 14}, {"type": "text", "text": "D Comparison of Different Score Fusion Strategies ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To investigate the impact of the score fusion strategy in the rescoring mechanism, we further evaluate two other strategies: (1) the arithmetic mean score fusion strategy and (2) the geometric mean score fusion strategy, denoted as mean and geo-mean, respectively. The arithmetic mean score fusion strategy takes the average of the scores from the image text spotter and the rescoring head as the final score for each query, while the geo-mean score fusion strategy uses the geometric mean. These two strategies can be formulated as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{c_{m e a n}=(c_{o}+c_{r})/2,}\\\\ {c_{g e o-m e a n}=\\sqrt{c_{o}*c_{r}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $c_{m e a n}$ and $c_{g e o-m e a n}$ denote the final score of the two strategies, respectively. $c_{o}$ and $c_{r}$ are the scores from the image text spotter and the rescoring head, respectively. ", "page_idx": 14}, {"type": "text", "text": "From Table 5, we can see that employing the maximum score fusion strategy achieves the best performance on MOTA and IDF1 among all the strategies. As for the other two strategies, extremely low confidence scores from the image text spotter may result in low final scores, probably leading to missed detections. Therefore, we adopt the maximum score fusion strategy in the rescoring mechanism of GoMatching by default. ", "page_idx": 14}, {"type": "image", "img_path": "ASv9lQcHCc/tmp/a355169cb60e0938865c6a37260f2b005ed63488d06b2787897ac253d6ab2d98.jpg", "img_caption": ["Figure 8: Visual comparison of GoMatching with and without the rescoring mechanism. The values in parentheses indicate the confidence scores of the detected text. $\\leftrightarrow\\,^{,}$ points to the filtered texts due to low confidence without using rescoring. $\\leftrightarrow\\,^{\\bullet}$ points to the text whose confidence score has been improved by the rescoring mechanism. The rescoring mechanism increases the confidence scores of small texts and blurry texts, preventing them from being filtered out by the threshold and thereby cultivating a better tracking candidate pool. "], "img_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "ASv9lQcHCc/tmp/c5d14a86d966cf0f0338af6daa9810805c33d925801fc6e0b59317d93c1eef6f.jpg", "table_caption": ["Table 8: Video text detection performance on ICDAR2013-video [41]. The best results are highlighted in bold. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "ASv9lQcHCc/tmp/9eae8a131f5466228be4e7e77dcf7f57f137a3577d7a45861c1ce4012b57dc3a.jpg", "table_caption": ["Table 9: Comparison results of detection AP on the ICDAR13-video between with and without the rescoring mechanism. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "E More Comparisons between TransDETR and GoMatching ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Table 6, we provide the results of using three different image sizes in GoMatching during inference on ICDAR15-video. In the first row of Table 6, the shorter side of the input image is set to 800, which is the same as the default setting of TransDETR. It is evident from Table 6 that GoMatching significantly outperforms TransDETR in all settings. Meanwhile, it also shows that GoMatching outperforms TransDETR under its default setting in terms of both inference speed and spotting accuracy. With the increase in image size (e.g., 1000), GoMatching offers better spotting performance at the cost of decreased inference speed. ", "page_idx": 15}, {"type": "image", "img_path": "ASv9lQcHCc/tmp/e9c5558373471b11de3ed94c6990b2dd30a4cf273953a6869653d09b7d49c342.jpg", "img_caption": ["Figure 9: More visualization results of TransDETR and GoMatching. Failure cases of text detection and recognition are highlighted with ellipses and rectangles, respectively. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Moreover, we compare the number of all parameters and the trainable parameters of GoMatching and TransDETR, as shown in Table 7. It is noteworthy that GoMatching requires a much smaller training budget than TransDETR owing to its simpler architecture design, making it a simple but strong baseline for video text spotting. ", "page_idx": 16}, {"type": "text", "text": "F More Results for the Rescoring Mechanism ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Table 8, we provide the video text detection results of GoMatching on ICDAR13-video [41] and compare them with Free and TransDETR. As shown in the table, without the rescoring mechanism, GoMatching relies on the original results from DeepSolo, resulting in a $4.5\\%$ decrease on Recall and only a $1.8\\%$ improvement on F-measure compare to TransDETR. This is due to the domain gap between image data and video data, directly using an image spotter leads to a low confidence and consequently low Recall on video data. When encompasses the rescoring mechanism, GoMatching achieves a 9.1 improvement on Recall compared to DeepSolo and a $6.5\\%$ F-measure enhancement compared to TransDETR. These improvements highlight the effectiveness of rescoring mechanism in alleviating the domain gap and leading to a better tracking candidate pool. The impressive results of GoMatching are not merely attributed to the introduction of a robust image text spotter. ", "page_idx": 16}, {"type": "text", "text": "To further explore how the rescoring mechanism eases the domain gap, we calculate the AP of detection results on ICDAR13-video, as shown in the Table 9. Incorporating the rescoring mechanism effectively improves the detection performance of DeepSolo on video datasets, particularly for small text, resulting in a $3.9\\%$ improvement on $\\mathrm{{AP}_{S}}$ . We also present more visual results to embody the potency of rescoring mechanism in Fig. 8. ", "page_idx": 16}, {"type": "text", "text": "G More Visualization Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We present more visualization results of TransDETR and GoMatching in Fig. 9, including some failure cases. It can be observed that GoMatching exhibits a significant improvement in text recognition performance compared to TransDETR. It should be noted that GoMatching may also experience failures due to the image-video domain gap and extreme cases, such as very small text instances and significant motion blur. These issues can be mitigated by employing a stronger text spotter with a more representative backbone and training on larger, more diverse datasets. ", "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: We discuss the limitations of GoMatching in Appendix G. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We provide detailed information about the training and inference settings in Section 4.2 and Appendix B, along with the release of the code and trained models, ensuring that all necessary information for reproducing the main experimental results is disclosed. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have provided the data and code, with sufficient instructions for open access in supplemental material. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We specify the training and test details in Section 4.2 and Appendix B. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: The experiments reported in the paper do not include error bars. Instead, we focus on providing comprehensive results and analyses to demonstrate the performance of our approach. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide relevant information in Section 4.2 and Appendix E. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: No ethical issues involved. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our paper aiming to improve performance and efficiency in text spotting from videos, and does not have any direct negative applications. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 20}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper poses no such risks Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: All data and models utilized in this paper are publicly available and appropriately cited. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We are submitting the code for GoMatching in the supplementary material and providing a \"readme.md\" file to reproduce our results. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}]