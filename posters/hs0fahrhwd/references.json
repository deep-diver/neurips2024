{"references": [{"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "publication_date": "2019-00-00", "reason": "This paper introduces BERT, a foundational language model that has significantly impacted NLP and is leveraged in this paper for cross-modality adaptation in time series forecasting."}, {"fullname_first_author": "Alec Radford", "paper_title": "Language Models are Unsupervised Multitask Learners", "publication_date": "2019-00-00", "reason": "This paper introduces GPT-2, a powerful language model whose architecture is used as the backbone of the proposed TIME-FFM model."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and Efficient Foundation Language Models", "publication_date": "2023-02-13", "reason": "This paper introduces LLaMA, another significant language model that, like GPT-2, is relevant to the cross-modality approach and foundational model concepts in TIME-FFM."}, {"fullname_first_author": "Brendan McMahan", "paper_title": "Communication-Efficient Learning of Deep Networks from Decentralized Data", "publication_date": "2017-00-00", "reason": "This is a foundational paper in federated learning, a crucial aspect of TIME-FFM's design, outlining efficient methods for training models on decentralized data without compromising privacy."}, {"fullname_first_author": "Xu Liu", "paper_title": "UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series Forecasting", "publication_date": "2024-00-00", "reason": "This is a closely related work that also uses language models for time series forecasting, providing a benchmark and comparison point for evaluating TIME-FFM's performance."}]}