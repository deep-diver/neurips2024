[{"figure_path": "HS0faHRhWD/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Specific prediction models are trained for diverse domains. (b) A unified model is trained for cross-domain time series. (c) The current in-modality adaption in FL setting fine-tunes LM for NLP tasks, with all the trained parameters are exchanged between clients and the server. (d) Our proposal investigates how to construct a FM by unlocking the potential of LM for cross-domain time series forecasting in FL paradigm.", "description": "This figure illustrates four different approaches to time series forecasting. (a) shows the traditional approach of training separate models for each domain. (b) presents a unified model trained on data from multiple domains. (c) depicts the current federated learning (FL) approach, where language models (LMs) are fine-tuned for specific tasks, and parameters are exchanged between clients and a central server. (d) introduces the proposed approach, TIME-FFM, which leverages pretrained LMs for cross-domain time series forecasting in a federated learning setting.", "section": "1 Introduction"}, {"figure_path": "HS0faHRhWD/figures/figures_3_1.jpg", "caption": "Figure 2: Overall architecture of TIME-FFM. Each round begins with downloading global parameters of modality alignment and prompt adaption modules. We \u2461 conduct modality alignment to generate patch tokens and \u2462 adaptively determine prompt tokens. \u2463 The two tokens are input into the LM backbone and \u2464 the outputs are projected to generate the prediction results. After local optimization, the updated parameters of modality alignment and prompt adaption modules are uploaded to the server for aggregation.", "description": "The figure illustrates the overall architecture of the TIME-FFM model.  It shows how the model processes time-series data in a federated learning setting, transforming it into a text-based representation using modality alignment and prompt adaption modules. These transformed data are then fed into a pre-trained language model (LM) backbone. The output from the LM is subsequently used to generate predictions, which are then fine-tuned using personalized prediction heads. The process involves the exchange of parameters between local clients and the central server in a federated manner.", "section": "3.2 Model Structure"}, {"figure_path": "HS0faHRhWD/figures/figures_8_1.jpg", "caption": "Figure 2: Overall architecture of TIME-FFM. Each round begins with downloading global parameters of modality alignment and prompt adaption modules. We \u2461 conduct modality alignment to generate patch tokens and \u2462 adaptively determine prompt tokens. \u2463 The two tokens are input into the LM backbone and \u2464 the outputs are projected to generate the prediction results. After local optimization, the updated parameters of modality alignment and prompt adaption modules are uploaded to the server for aggregation.", "description": "This figure illustrates the overall architecture of the TIME-FFM model, highlighting the interaction between clients and server during the federated learning process.  The model takes time series data as input and transforms it into text tokens using a modality alignment module.  A prompt adaption module then generates domain-specific prompts. These tokens are fed into a pre-trained LM backbone, and the resulting outputs are used to make predictions.  The process is iterative, with updates being aggregated on the server after local optimization on each client.", "section": "3.2 Model Structure"}, {"figure_path": "HS0faHRhWD/figures/figures_15_1.jpg", "caption": "Figure 2: Overall architecture of TIME-FFM. Each round begins with downloading global parameters of modality alignment and prompt adaption modules. We \u2461 conduct modality alignment to generate patch tokens and \u2462 adaptively determine prompt tokens. \u2463 The two tokens are input into the LM backbone and \u2464 the outputs are projected to generate the prediction results. After local optimization, the updated parameters of modality alignment and prompt adaption modules are uploaded to the server for aggregation.", "description": "This figure illustrates the overall architecture of the TIME-FFM model, highlighting the different stages involved in each round of federated learning. It shows how global parameters are downloaded, modality alignment and prompt adaption are performed, and how the resulting tokens are fed into the LM backbone for prediction. Finally, it illustrates how the updated parameters are uploaded to the server for aggregation.", "section": "3.2 Model Structure"}]