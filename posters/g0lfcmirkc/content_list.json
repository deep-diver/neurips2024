[{"type": "text", "text": "Linguistic Collapse: Neural Collapse in (Large) Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Robert Wu University of Toronto, Vector Institute rupert@cs.toronto.edu ", "page_idx": 0}, {"type": "text", "text": "Vardan Papyan University of Toronto, Vector Institute vardan.papyan@utoronto.ca ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Neural collapse $(\\mathcal{N C})$ is a phenomenon observed in classification tasks where top-layer representations collapse into their class means, which become equinorm, equiangular and aligned with the classifiers. These behaviors \u2013 associated with generalization and robustness \u2013 would manifest under specific conditions: models are trained towards zero loss, with noise-free labels belonging to balanced classes, which do not outnumber the model\u2019s hidden dimension. Recent studies have explored ${\\mathcal{N C}}$ in the absence of one or more of these conditions to extend and capitalize on the associated beneftis of ideal geometries. Language modeling presents a curious frontier, as training by token prediction constitutes a classification task where none of the conditions exist: the vocabulary is imbalanced and exceeds the embedding dimension; different tokens might correspond to similar contextual embeddings; and large language models (LLMs) in particular are typically only trained for a few epochs. This paper empirically investigates the impact of scaling the architectures and training of causal language models (CLMs) on their progression towards ${\\mathcal{N C}}$ . We find that ${\\mathcal{N C}}$ properties that develop with scale (and regularization) are linked to generalization. Moreover, there is evidence of some relationship between ${\\mathcal{N C}}$ and generalization independent of scale. Our work thereby underscores the generality of ${\\mathcal{N C}}$ as it extends to the novel and more challenging setting of language modeling. Downstream, we seek to inspire further research on the phenomenon to deepen our understanding of LLMs \u2013 and neural networks at large \u2013 and improve existing architectures based on ${\\mathcal{N C}}$ -related properties. Our code is hosted on GitHub: https://github.com/rhubarbwu/linguistic-collapse. ", "page_idx": 0}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/9e5cd4228319d351fa653e991c2bf0a9c5526199b27cb73e53a7668687811822.jpg", "img_caption": ["Figure 1: Simultaneous development of the four neural collapse $(\\mathcal{N}\\mathcal{C})$ [1] properties in 230 causal language models trained on TinyStories [2], alongside improvement in generalization (i.e. validation performance). Left to right: $\\mathcal{N C}_{1}}$ ) within-class (representation) variability collapse; $\\mathcal{G N C}_{2}}$ ) hyperspherical uniformity of class means; $\\mathcal{U}\\Lambda\\mathcal{C}_{3})$ ) uniform duality between class means and corresponding classifiers; and ${\\mathcal{N C}}_{4}$ ) agreement between token (maximum a prior) classifiers and implicit nearestclass center classifiers. Colored by model size and annotated with coefficient of determination $(R^{2})$ . "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "1.1 Neural Collapse ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A learning phenomenon known as neural collapse $(\\mathcal{N}\\mathcal{C})$ emerges during the terminal phase of training (TPT) deep neural networks with cross-entropy (CE) loss for classification tasks.[1] It was originally characterized as the co-occurrence of the following properties in a model\u2019s top-layer (also known as last-layer) representations (also known as features or embeddings) and linear classifier weights: ", "page_idx": 1}, {"type": "text", "text": "$(\\mathcal{N}\\mathcal{C}_{1})$ Within-class variability collapse: Top-layer representations collapse to their class means. ", "page_idx": 1}, {"type": "text", "text": "$(\\mathcal{N}\\mathscr{C}_{2})$ Convergence to a simplex ETF: Class means tend towards equinorm and equiangular vectors when centered about the global average. The resulting geometry \u2013 known as a simplex equiangular tight frame (ETF) \u2013 maximizes pairwise angles and distances.   \n$\\left(\\mathcal{N}\\mathcal{C}_{3}\\right)$ Convergence to self-duality: Linear classifier weight vectors converge to their corresponding top-layer class mean vectors, and thus also form a simplex ETF.   \n$(\\mathcal{N}\\mathcal{C}_{4})$ Nearest decision rule: Linear classifiers approximate a nearest-class center (NCC) classifier: top-layer representations predict the class with the closest mean (implied by $\\mathcal{N}\\mathcal{C}1{-}3\\$ ). ", "page_idx": 1}, {"type": "text", "text": "These behaviors, often associated with improved generalization and robustness [3\u20135] (among other benefits, such as those discussed in $\\S1.4)$ , traditionally manifest under the following conditions: ", "page_idx": 1}, {"type": "text", "text": "Rq1) Few classes: The number of classes is upper-bounded by the embedding dimension plus one: $C\\leq d+1$ ; this is required to construct a perfect simplex ETF.   \nRq2) Balanced classes: The number of samples is equal across classes: $N_{c}=N_{c^{\\prime}},\\forall c\\neq c^{\\prime}$ .   \n$\\mathtt{R q}3$ ) Noise-free labels: Identical (or very similar) embeddings should belong to the same class.   \n$\\mathtt{R q4})$ Sufficient training (TPT): The model is trained past zero error, towards zero loss. ", "page_idx": 1}, {"type": "text", "text": "Absent these conditions, one does not typically anticipate ${\\mathcal{N C}}$ . Since then, follow-up studies have extended beyond and proposed techniques of quantifying or achieving ${\\mathcal{N C}}$ (discussed in Section 2). ", "page_idx": 1}, {"type": "text", "text": "1.2 (Large) Language Models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "${\\mathcal{N C}}$ is a phenomenon observed specifically in classification tasks. While not traditionally thought of as classifiers, language models \u2013 including large language models (LLMs) \u2013 learn to model aleatoric uncertainty, which can be viewed as stochastic token prediction [6]. For instance, masked language models (MLMs) such as BERT [7] predict one or several masked tokens within an input sequence based on the surrounding context. Likewise, autoregressive or causal language models (CLMs) such as GPT [8] predict the next token in a sequence given the context of previous tokens. Most of these models are essentially (pre-)trained by token classification on their vocabularies. This parallel \u2013 also drawn by [9] \u2013 raises a few natural questions: ", "page_idx": 1}, {"type": "text", "text": "1. Does the pre-training stage of a language model exhibit NC? ", "page_idx": 1}, {"type": "text", "text": "2. How do scaling and other training configurations influence NC in (L)LMs? 3. To what extent is ${\\mathcal{N C}}$ in (L)LMs correlated with their generalization abilities? 4. Do such correlations, between ${\\mathcal{N C}}$ and improved generalization, persist independent of the (potential confounders of) model size and training? ", "page_idx": 1}, {"type": "text", "text": "To address these, we first examine the specific settings of training CLMs as they are opposed $(\\neg)$ to the traditional prerequisites (R1-4, $\\S1.1)$ for ${\\mathcal{N C}}$ to manifest. ", "page_idx": 1}, {"type": "text", "text": "\u00acRq1) Many classes: The unique tokens found in language modeling vocabularies are vast, usually numbering in the tens of thousands and far exceeding the hidden dimension [10]. ", "page_idx": 1}, {"type": "text", "text": "\u00acRq2) Imbalanced classes: The distribution of tokens in natural language is typically very imbalanced [11, 12], as is the case in TinyStories [2], the dataset we use (Appendix Figure 4). It has an average of 16K samples per class but a standard deviation of over 32K. ", "page_idx": 1}, {"type": "text", "text": "\u00acRq3) Ambiguous contexts: There may exist very similar or even identical contexts followed by different tokens in the natural language data [13]. For instance, over half of the sequences in TinyStories [2] lead with \u201cOnce upon a time\u201d, but only three-quarters of these follow with a comma (\u201cOnce upon a time,\u201d). In other words, there is almost certainly some ambiguity to contend with in our context embeddings.   \n\u00acRq4) Undertraining: Most practical language models (including LLMs) are not trained for ", "page_idx": 2}, {"type": "text", "text": "more than a few epochs [14, 15]. Further optimization typically renders diminishing returns in improving evaluation performance [16] long before any possible TPT. ", "page_idx": 2}, {"type": "text", "text": "1.3 Contributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We train a suite of Transformer-based [17] $\\mathrm{{CLMs}^{1}}$ across a grid of model widths, depths, and training epochs on the TinyStories dataset [2] to assess the degrees to which ${\\mathcal{N C}}$ properties develop and how they relate to generalization performance. Our findings (summarized in Figure 1) reveal: ", "page_idx": 2}, {"type": "text", "text": "\u2022 Emergence of ${\\mathcal{N C}}$ with scale: As we scale model size and training, the properties of ${\\mathcal{N C}}$ emerge; within-class variability $(\\mathcal{N}\\mathcal{C}_{1})$ and interference are reduced while hyperpsherical uniformity $(\\mathcal{G N C}_{2})$ , uniform duality $(\\mathcal{U}\\mathcal{N}\\mathcal{C}_{3})$ , and classifier agreement $(\\mathcal{N}\\mathcal{C}_{4})$ improve. \u2022 Progression towards hyperspherical uniformity: Class means, while unable to form a simplex ETF $(\\mathcal{N}\\mathscr{C}_{2})$ , nonetheless tend towards uniform dispersion on a hypersphere, a geometry theorized by [18] and formalized by [19] as hyperspherical uniformity $\\scriptstyle({\\bar{\\mathcal{G}}}{\\mathcal{N}}{\\mathcal{C}}2)$ . \u2022 Tendency towards uniform duality: Direct alignment (self-duality) between class means and classifiers $\\left(\\mathcal{N}\\mathcal{C}_{3}\\right)$ does not appear to develop with scale. Instead, the variability of (mis)alignment across classes decreases with width and training, suggesting its minimization \u2013 which we term uniform duality $(\\mathcal{U}\\mathcal{N}\\mathcal{C}_{3})$ \u2013 may be more cohesive with ${\\bar{N}}C$ . \u2022 Correlation between ${\\mathcal{N C}}$ and generalization: The developments of ${\\mathcal{N C}}$ properties are correlated with improved validation performance. We show these correlations to persist even when fixing the (potential confounders of) model architecture and training by simply varying the random seeds for initialization and data shuffilng. This suggests that ${\\mathcal{N C}}$ is not simply a side-effect of training, but possibly a factor of generalization in its own right. ", "page_idx": 2}, {"type": "text", "text": "1.4 Significance ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Recently, methods building on ${\\mathcal{N C}}$ have found use in deep learning at large. We highlight areas such as federated learning [20], graph neural networks [21], incremental/continual learning [22\u201324], metalearning [24, 25], out-of-distribution detection [26\u201328], privacy [29, 30], learning theory [31\u201336] transfer learning [3, 5, 37\u201340], and even LLM prompting [41]. With our results, we aim to extend insights from such contributions and other related works to the autoregressive language modeling domain and ultimately assist researchers in improving and interpreting their (L)LMs. ", "page_idx": 2}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "${\\mathcal{N C}}$ was initially observed in image classification tasks such as on CIFAR-10/100 [42] and ImageNet [43]. Since then, the phenomenon has been further studied both theoretically and empirically [5, 18, 35, 44\u201366], with several works venturing into settings without some of the traditional prerequisites $(\\neg\\mathrm{Rq}1\\neg4,\\,\\S1.2)$ and proposing adaptations of the analysis framework or optimization procedures: ", "page_idx": 2}, {"type": "text", "text": "A large number of classes $\\mathtt{(\\neg R q1)}$ ${\\mathcal{N C}}$ established the simplex ETF as an optimal configuration. However, a perfect simplex ETF $(\\mathcal{N}\\mathscr{C}_{2})$ requires that the number of classes $C$ not exceed $d+1$ where $d$ is the embedding dimension. This requirement that $d$ be sufficiently large is not practical when the classes number beyond the thousands.2For instance, GPT-2 [68] and Llama 3.1 [69] have vocabularies of around 50K and 128K tokens, respectively. ", "page_idx": 2}, {"type": "text", "text": "In such a scenario, one might still expect class means to be uniformly distributed on a $d_{\\cdot}$ -dimensional hypersphere [18]. [19] formalize this as hyperspherical uniformity within a generalized neural collapse $(\\mathcal{G N C})$ framework, which [9] then empirically demonstrate. These latter two works mention language modeling as applicable for $g\\mathcal{N C}$ ; [9] even framed token prediction as a classification task, just as we do. We remark however that both earlier works simulate a large number of classes by drastically shrinking the embedding dimension. In contrast, we study next-token prediction, using the full class space (vocabulary) with an imbalanced token distribution and ambiguous samples. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Class imbalance $\\operatorname{(-Rq2)}$ ${\\mathcal{N C}}$ traditionally assumed that classes were sample-balanced. Since then, follow-up works have investigated the effect of class imbalance on the prevalence of ${\\mathcal{N C}}$ . [47] studied the layer-peeled model (LPM) and discovered that minority collapse occurs when classes are imbalanced across two equally-sized groups of classes; a threshold for minority collapse was later characterized by [35]. [52] showed that ${\\mathcal{N C}}$ still occurs in such an LPM when the classifier is initialized as a fixed ETF. [70] introduced simplex-encoded-label interpolation (SELI) but noted that more severe imbalance worsens even this geometry. Recently, feature regularization has been employed to induce ${\\mathcal{N C}}$ and improve generalization in class-imbalanced settings [58, 61, 62]. ", "page_idx": 3}, {"type": "text", "text": "Multi-label classification $\\tt{(-R q3)}$ In some problems, one might encounter mixed or multi-label samples, be they natural or due to noise or augmentation [71, 72]. ${\\mathcal{N C}}$ was also recently studied for such data by [73], who observed that multi-label class means arrive at the average of their labels\u2019 respective means. They also devise an augmented CE loss function to accommodate such samples. ", "page_idx": 3}, {"type": "text", "text": "Likewise, most of our ambiguous samples could be considered multi- or soft-label: identical (or very similar) context samples with different hard token labels $\\mathtt{(-18q3)}$ . Under popular CLM pre-training (teacher-forcing with CE loss), this effectively precludes the prospect of achieving zero classification error and potentially introduces irreducible noise. ", "page_idx": 3}, {"type": "text", "text": "A recent study showed that memorization of noisy labels likely leads to degradation of ${\\mathcal{N C}}$ [60]. ", "page_idx": 3}, {"type": "text", "text": "Early stages of training $\\tt{(-7R q4)}$ [53] studied ${\\mathcal{N C}}$ in small ResNet [74] models that had not yet converged (similar to most LLMs). They show that within-class variability drops and \u201cplateaus\u201d $(\\mathcal{N}\\mathcal{C}_{1})$ earlier than other ${\\mathcal{N C}}$ metrics, a result that we also observe (\u00a74.1, Figures 6, 7). ", "page_idx": 3}, {"type": "text", "text": "Natural language processing (NLP) An earlier study reported that the ratio of within-class to between-class covariances of word embeddings increases with model depth [75, 76], seemingly at odds with ${\\mathcal{N C}}_{1}$ . It can however be distinguished from literature studying layer-wise ${\\mathcal{N C}}$ in that it does not center the mean representation vectors (i.e. subtract the global mean). ", "page_idx": 3}, {"type": "text", "text": "[19] fine-tuned BERT [7] using their hyperspherical uniformity gap (HUG) objective on binary classification tasks from the GLUE benchmark [77]. [78] conducted a tangentially-related investigation of convolutional neural networks for few-class semi-supervised clustering in which they identify ${\\mathcal{N C}}$ . Our work is distinct from these in several ways: a) our class space far exceeds our embedding dimension $(C\\gg d+1)$ ) because we classify on the full token vocabulary; b) we analyze embeddings at a token-level granularity rather than at the sequence level; and c) our next token prediction is causal (context-dependent) as opposed to the per-sample independence of their few-category classification. ", "page_idx": 3}, {"type": "text", "text": "A more related work studied feature collapse in individual word representations [79], but the authors note that their analysis is limited to shallow NLP modeling on more rigid and tabular text data. ", "page_idx": 3}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Below we describe the training setup for our CLMs (\u00a73.1), procedures3 for collecting top-layer embeddings (\u00a73.2, 3.7), and measurements of ${\\mathcal{N C}}$ and generalization (\u00a73.4, 3.5, 3.6, 3.7, 3.8). ", "page_idx": 3}, {"type": "text", "text": "3.1 Dataset and Language Models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "TinyStories [2] is a synthetic4 dataset generated by GPT-3.5 and GPT-4 using around 1500 English words a child might use. Next-token prediction is performed by sampling from the token vocabulary $\\mathbb{V}=[1,29233]$ , which for our purposes can therefore be framed as classification among $C=29{,}233$ class e s.5 Follow ing the GPT-style preprocessing regime [8], raw sequences are packed into $S$ chunks of size $T$ , providing $N=S(T-\\mathrm{\\bar{1}})$ token samples for training.6 Details are listed in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "We use $30\\,\\mathrm{CLM}$ architectures based on GPT Neo [80], configured similarly to [2]. They vary in width (embedding dimension) $d\\ \\in\\ \\{64,128,256,512,768,102\\bar{4}\\}$ and depth (number of self-attention layers) $L\\bar{\\in}\\,\\{1,2,4,8,12\\}$ . Our models were trained by teacher-forcing7 using CE loss. For each architecture, we trained multiple models for 1, 3, and 10 epochs ablated over weight decay factors $\\beta=0.0005$ [51] and $\\beta=0.1$ [81]. Further details are listed in Appendices B, C. ", "page_idx": 4}, {"type": "text", "text": "3.2 Context Embeddings ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Base CLMs perform next-token prediction: given a sequence of tokens $\\mathbf{\\boldsymbol{x}}_{1:t}\\in\\mathbb{V}^{t}$ , a top-layer context embedding $\\bar{h}(\\pmb{x}_{1:t})\\in\\mathbb{R}^{d}$ is used to predict the next token $x_{t+1}^{\\prime}\\in\\mathbb{V}$ where $1\\leq t\\leq T$ . A classifier for class $c$ with weights $\\pmb{w}_{c}$ and bias8 $b_{c}$ would make maximum a prior (MAP) predictions as ", "page_idx": 4}, {"type": "equation", "text": "$$\nx_{t+1}^{\\prime}:=\\underset{c\\in\\mathbb{V}}{\\operatorname{argmax}}\\left\\langle\\pmb{w}_{c},h(\\pmb{x}_{1:t})\\right\\rangle+b_{c}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Class embedding means For each token class $c$ , we are interested in the mean embedding $\\pmb{\\mu_{c}}\\in\\mathbb{R}^{d}$ across sequences $s$ and their contexts $\\pmb{x}_{1:t}^{(s)}$ , where the next token $x_{t+1}^{(s)}$ is ground-truth $(t<T)$ and equal to $c$ . To centre these means, we compute their unweighted9 average $\\bar{\\pmb{\\mu}}\\in\\mathbb{R}^{d}$ . Precisely, we take ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mu_{c}:=\\frac{1}{N_{c}}\\sum_{s=1}^{S}\\sum_{t=1}^{T-1}h\\left(x_{1:t}^{(s)}\\right)\\mathbb{I}\\left(x_{t+1}^{(s)}=c\\right),\\qquad\\bar{\\mu}:=\\frac{1}{C}\\sum_{c=1}^{C}\\mu_{c},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $N_{c}$ is the number of samples of class $c$ and $\\mathbb{I}$ is the (binary) indicator function. ", "page_idx": 4}, {"type": "text", "text": "Class embedding variances In a second pass, we accumulate the sample variance norms:10 ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sigma_{c}^{2}:=\\frac{1}{N_{c}}\\sum_{s=1}^{S}\\sum_{t=1}^{T-1}\\left\\|h\\left(x_{1:t}^{(s)}\\right)-\\pmb{\\mu}_{c}\\right\\|_{2}^{2}\\mathbb{I}\\left(x_{t+1}^{(s)}=c\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.3 Homogeneity and Variability ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For some collapse measures (such as $(\\mathcal{G})\\mathcal{N C}_{2}$ and $\\mathcal{N C}_{3}$ ), we are primarily interested in the variation rather than the average of pairwise relationships. To that end, we also include in our analysis the coefficient of variation $(\\mathrm{CoV})$ of several measurements, which is the ratio of their standard deviations to their means: $\\sigma(\\cdot)/\\mu(\\cdot)$ . We can interpret this as a normalized measure of variability. ", "page_idx": 4}, {"type": "text", "text": "3.4 Signal-to-Noise Ratio \u2013 NC1 ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The ability to disambiguate between classes depends on the ratio of within-class to between-class variabilities. Building upon foundational works [85, 86], ${\\mathcal{N C}}$ originally measured variability through an inverse signal-to-noise ratio (SNR), whose minimization constitutes within-class variability collapse $(\\mathcal{N C}_{1})$ . We instead employ a class-distance normalized variance (CDNV) similar to [3]: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\sigma}_{c,c^{\\prime}}:=\\frac{1}{\\|\\pmb{\\mu}_{c}-\\pmb{\\mu}_{c^{\\prime}}\\|_{2}^{k}}\\cdot\\frac{\\sigma_{c}^{2}+\\sigma_{c^{\\prime}}^{2}}{2\\|\\pmb{\\mu}_{c}-\\pmb{\\mu}_{c^{\\prime}}\\|_{2}^{2}},\\quad\\forall c\\neq c^{\\prime}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Our metric differs in that we divide by an additional power ${\\|\\pmb{\\mu}_{c}-\\pmb{\\mu}_{c^{\\prime}}\\|_{2}^{k}}$ of the mean distance norm. This further downweights the CDNV within well-separated class pairs in favour of emphasizing more mutually noisy pairs. We found this augmented CDNV with $k=2$ to be especially useful in our setting of many imbalanced and variably confusable token classes. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "These pairwise measures constitute the off-diagonal11 entries of a symmetric matrix in $\\mathbb{R}^{C\\times C}$ , whose average we use as an inverse SNR. Within-class variability collapse is then re-characterized by the minimization of this quantity: $\\hat{\\sigma}_{c,c^{\\prime}}\\rightarrow0,\\forall c\\neq c^{\\prime}$ . This alternative convergence is empirically faithful to ${\\mathcal{N C}}_{1}$ but more robust and numerically stable [3]. ", "page_idx": 5}, {"type": "text", "text": "3.5 Geometric Structures \u2013 (G)NC2 ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The separability of our representations also depends on the geometric structures found in our embeddings. [1] characterize $\\bar{\\mathcal{N}}\\bar{\\mathcal{C}}_{2}$ as convergence to a simplex equiangular tight frame (ETF) [87, 88]. ", "page_idx": 5}, {"type": "text", "text": "Equinormness Such a near-orthonormal configuration firstly implies that class means are equinorm: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\log\\|\\pmb{\\mu}_{c}-\\bar{\\pmb{\\mu}}\\|_{2}-\\log\\|\\pmb{\\mu}_{c^{\\prime}}-\\bar{\\pmb{\\mu}}\\|_{2}\\to0,\\quad\\forall c\\neq c^{\\prime}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We measure $\\mathrm{CoV}$ in the logarithms of class mean norms to assess \u201cequinormness\u201d. ", "page_idx": 5}, {"type": "text", "text": "Equiangularity $\\mathcal{N C}_{2}$ also entails that class means are equiangular about their center $\\bar{\\pmb{\\mu}}$ : pairwise distances and angles between their class means should be maximized and similar. Following [1], we measure interference (sometimes known as similarity or coherence [89, 90]). Its minimization, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left\\langle\\frac{\\mu_{c}-\\bar{\\mu}}{\\|\\mu_{c}-\\bar{\\mu}\\|_{2}},\\frac{\\mu_{c^{\\prime}}-\\bar{\\mu}}{\\|\\mu_{c^{\\prime}}-\\bar{\\mu}\\|_{2}}\\right\\rangle\\to\\frac{-1}{C-1},\\quad\\forall c\\neq c^{\\prime},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "together with equinormness (Equation 5) constitute convergence to a simplex ETF. Although this geometry is not ultimately attainable since there are too many classes $(C>d+1)$ , it can still be meaningful to measure a model\u2019s tendency towards one. As with CDNV noise (Equation 4), pairwise interferences form off-diagonal12 entries in a symmetric matrix in $\\mathbb{R}^{C\\times C}$ . The minimization of $\\mathrm{CoV}$ in interferences therefore expresses the degree of \u201cequiangularity\u201d. ", "page_idx": 5}, {"type": "text", "text": "Hyperspherical uniformity A relaxation from the ETF is hyperspherical uniformity $(\\mathcal{G N C}_{2})$ , with mean vectors $\\pmb{\\mu}_{c}$ uniformly distributed on the $d$ -dimensional hypersphere [18, 19]. We gauge the degree of this uniformity with pairwise interactions under a logarithmic inverse distance kernel:13 ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\log\\left\\|\\frac{\\pmb{\\mu}_{c}-\\bar{\\pmb{\\mu}}}{\\|\\pmb{\\mu}_{c}-\\bar{\\pmb{\\mu}}\\|_{2}}-\\frac{\\pmb{\\mu}_{c^{\\prime}}-\\bar{\\pmb{\\mu}}}{\\|\\pmb{\\mu}_{c^{\\prime}}-\\bar{\\pmb{\\mu}}\\|_{2}}\\right\\|^{-1},\\ \\forall c\\neq c^{\\prime}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.6 Alignment Between Classifiers and Class Mean Embeddings \u2013 $(\\mathcal{U})\\mathcal{N}\\mathcal{C}3$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The linear classifiers $\\{w_{c}\\}_{c=1}^{C}$ lie in a dual vector space to that of the class means $\\{\\pmb{\\mu}_{c}\\}_{c=1}^{C}$ . While convergence to self-duality $(\\mathcal{N}\\mathcal{C}_{3})$ was initially measured as distances [1] between class means and classifiers (Equation 11), we follow [5] to inspect class-wise cosine similarities:14 ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left\\langle\\frac{\\pmb{w}_{c}}{\\|\\pmb{w}_{c}\\|_{2}},\\frac{\\pmb{\\mu}_{c^{\\prime}}-\\bar{\\pmb{\\mu}}}{\\|\\pmb{\\mu}_{c^{\\prime}}-\\bar{\\pmb{\\mu}}\\|_{2}}\\right\\rangle\\rightarrow1,\\quad\\forall c\\in\\mathbb{V}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For intuition analogous to that for equinormness and equiangularity $(\\S3.5)$ , we also measure uniform duality $(\\mathcal{U}\\mathcal{N}\\mathcal{C}_{3})$ as the minimization of the $\\mathrm{CoV}$ of these similarities (Appendices N, O). ", "page_idx": 5}, {"type": "text", "text": "3.7 Agreement of the Classifiers \u2013 NC4 ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Finally, $\\mathcal{N C}_{4}$ is described as the simplification of the linear classifier\u2019s MAP predictions (Equation 1) for $t e s t^{15}$ points to those of the implicit nearest-class center (NCC) classifier: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\underset{c\\in\\mathbb{V}}{\\operatorname{argmax}}\\left<\\pmb{w}_{c},\\pmb{h}\\right>+b_{c}\\rightarrow\\underset{c\\in\\mathbb{V}}{\\operatorname{argmin}}\\,\\|\\pmb{h}-\\pmb{\\mu}_{c}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We calculate16 agreement as the proportion of samples on which the classifiers agree: 17 ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{N_{\\mathrm{val}}}\\sum_{s=1}^{S_{\\mathrm{val}}}\\sum_{t=1}^{T_{\\mathrm{val}}-1}\\mathbb{I}\\left(x_{t+1}^{\\prime}{}^{(s)}=\\underset{c\\in\\mathcal{V}}{\\operatorname{argmin}}\\left\\|h\\left(x_{1:t}^{(s)}\\right)-\\mu_{c}\\right\\|_{2}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "3.8 Probing a Relationship Between ${\\mathcal{N C}}$ and Generalization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To isolate the effect of ${\\mathcal{N C}}$ on generalization independent of model scaling and training (if it exists), we selected a two-layer 768-wide architecture of which to train twenty more instances with weight decay $\\beta=0.0005$ , each with a different data shuffling seed. We then followed the remainder of the pipeline described above to collect and analyze embeddings with respect to ${\\mathcal{N C}}$ . Finally, we performed a permutation test with $10^{4}$ trials to determine the statistical significance of any correlation between $\\mathcal{N C}$ and generalization that remains when we hold constant all factors but shuffilng seeds. ", "page_idx": 6}, {"type": "text", "text": "4 Experimental Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we present the results from our empirical study on scaling and generalization: ", "page_idx": 6}, {"type": "text", "text": "$(\\mathcal{N}\\mathcal{C}_{1})$ Within-class variability is reduced across model scale (more so by width than depth) and training (up to 6 epochs), and is tightly correlated with validation performance.   \n$(\\mathcal{N}\\mathscr{C}_{2})$ Equinormness/equiangularity shows some improvement with scale, training, and performance. Hyperspherical uniformity $(\\mathcal{G N C}_{2})$ also improves but more clearly and consistently.   \n$\\left(\\mathcal{N}\\mathcal{C}_{3}\\right)$ Our models fail to achieve self-duality: class means do not align with classifiers. However, uniform duality (UNC3) is correlated with model width, training, and performance.   \n$(\\mathcal{N}\\mathcal{C}_{4})$ Larger or more trained models exhibit closer agreement between their linear and implicit NCC classifiers. Agreement is also associated with validation performance. ", "page_idx": 6}, {"type": "text", "text": "4.1 Within-Class Variability Collapse \u2013 NC1 ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Scaling our models dramatically reduces normalized variance, which is further aided by more training epochs and stronger weight decay (Appendix Figs. 6, 7). These noise reductions tightly associate with generalization (Fig. 1, left, \u201c $\\mathbf{\\Psi}^{\\mathcal{N}}\\mathcal{C}_{1}\\mathbf{\\Psi}^{,,}$ ). The relationship is most apparent at larger model scale. ", "page_idx": 6}, {"type": "text", "text": "4.2 Geometric Structures \u2013 (G)NC2 ", "text_level": 1, "page_idx": 6}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/7150406bdac97df4ac060d7cace57eae8db32f25af2ffaf38001ebf702066a99.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 2: Validation loss shows negligible correlation with equinormness $\\mathcal{N C}_{2}$ , left), some relationship with equiangularity $\\mathscr{N C}_{2}$ , centre), and a stronger one with hyperspherical uniformity $(\\mathcal{G N C}_{2}$ , right). So, $\\bar{\\mathcal{G}}\\mathcal{N}\\bar{\\mathcal{C}}_{2}^{\\phantom{\\dagger}}$ develops with scale and correlates well with generalization, better than $\\mathcal{N C}_{2}$ . ", "page_idx": 6}, {"type": "text", "text": "Equinormness Class mean embedding norms grow with model width and training (Appendix Fig. 8), and subtly with depth (Appendix Fig. 9). Despite this growth, the variation of norms consistently decreases (improving equinormness) with scale (Appendix Figs. 10, 11). Both trends correlate with improved generalization (Appendix Fig. 20). ", "page_idx": 6}, {"type": "text", "text": "Equiangularity Scaling model dimensions reduces average interference (Appendix Figs. 12, 13) down to an empirical plateau of approximately $10^{-3}$ , which is more apparent in less-trained models. However, the variation of interference rises and persists when scaling (Appendix Figs. 14, 15), suggesting that interference becomes more concentrated between some pairs of classes. These results could be due to various factors, including but not limited to unfriendly conditions of language modeling $(\\S1.2)$ or the impossibility of a simplex ETF (\u00a73.5). ", "page_idx": 7}, {"type": "text", "text": "We only see a rough correlation between average interference and generalization and almost none with its variation, equiangularity (Appendix Fig. 21). In other words, the limited trends we observed toward a simplex ETF do not appear to be associated with generalization when $C>d+1$ . ", "page_idx": 7}, {"type": "text", "text": "Hyperspherical uniformity Logarithmic distances drop more gradually and consistently with scale (Appendix Figs. 16, 17), implying this quantity is more robust or may not face the same geometric barriers seen in conventional interference (Appendix Figs. 14, 15). Variation of these logarithmic distances is also consistently reduced with scale (Appendix Fig. 18, 19). ", "page_idx": 7}, {"type": "text", "text": "And finally, generalization has much stronger correlations with logarithmic distances than it has with regular interference (Fig. 2), validating the applicability of ${\\mathcal{G N C}}$ [19] when $C>d+1$ . ", "page_idx": 7}, {"type": "text", "text": "4.3 Classifier (Mis)alignment and Duality \u2013 $(\\mathcal{U})\\mathcal{N}\\mathcal{C}3$ ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Model width $(d)$ is correlated faintly with average similarity between class means and their respective classifiers (Appendix Fig. 23), but strongly with variation in similarity (Appendix Fig. 25). The relationships to generalization follow the same pattern (Fig. 3), suggesting that uniform duality $(\\mathcal{U}\\mathcal{N}\\mathcal{C}_{3})$ might serve as a better ${\\mathcal{N C}}$ property than self-duality $\\left(\\mathcal{N}\\mathcal{C}_{3}\\right)$ overall; we discuss this in $\\S5.1$ . ", "page_idx": 7}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/6d8c54f5f2af8969156066bf5eda53a6994f261ccaffc9564452cf69a3c06c9b.jpg", "img_caption": ["Figure 3: Validation loss shows a negligible relationship with self-duality $\\mathcal{N C}_{3}$ , left) and some correlation with uniform duality $(\\mathcal{U}\\bar{\\mathcal{N}}\\bar{\\mathcal{C}}_{3}^{\\;\\!-}$ , right). In other words, $\\mathcal{U}\\mathcal{N}\\mathcal{C}_{3}$ develops with scale and correlates with generalization much better than $\\mathcal{N C}_{3}$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.4 Classifier Agreement \u2013 NC4 ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The linear and NCC classifiers agree on far more examples than random chance. Scaling encourages agreement (Appendix Figs. 29, 30). Increasing width for certain depths happens to plateau or even regress the agreement rate, but this limitation is overcome with further training. And finally, agreement is a strong indicator of generalization (Fig. 1, right, ${\\mathcal{N C}}_{4}$ ). ", "page_idx": 7}, {"type": "text", "text": "5 Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We find that ${\\mathcal{N C}}$ is generally promoted by model size and training and correlated with generalization (validation performance). We also discern some of this correlation independent of scale (\u00a75.1). ", "page_idx": 7}, {"type": "text", "text": "Table 1: Permutation test of ${\\mathcal{N C}}$ measurements with respect to validation loss. Twenty-one (21) identical two-layer 768-wide models were trained with different data shuffling seeds and permuted with $10^{4}$ trials. The $p$ -values below 0.05 (bolded) show those properties to be statistically significant. ", "page_idx": 8}, {"type": "table", "img_path": "G0LfcMiRkc/tmp/9de36e325460aa847e0b7766c0adee4d514916ab0359dc879d1915fc2b2b448c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.1 Neural Collapse\u2019s Relationship with Generalization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Table 1 presents the correlation scores of ${\\mathcal{N C}}$ metrics with generalization alongside their associated $\\mathbf{p}$ -values from the permutation tests described in $\\S3.8$ . The majority of the correlations are statistically significant $p<0.05)$ independent of scale, affirming that $\\bar{\\mathcal{N C}}$ is correlated with generalization. ", "page_idx": 8}, {"type": "text", "text": "5.2 Duality of Duality ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The dichotomy of self-duality $\\left(\\mathcal{N}\\mathcal{C}_{3}\\right)$ and uniform duality $(\\mathcal{U}\\mathcal{N}\\mathcal{C}_{3})$ is rather conspicuous. Our main experiments find that $\\mathcal{N C}_{3}$ does not consistently develop with scale while $\\bar{\\mathcal{U}}\\bar{\\mathcal{N}}\\mathcal{C}_{3}$ does (Fig. 3). However, within fixed scale, the opposite is true, implying that $\\mathcal{U}\\Lambda\\mathcal{C}_{3}$ may be confounded by model capacity while $\\mathcal{N C}_{3}$ is a subtle and fine-grained indicator of generalization. ", "page_idx": 8}, {"type": "text", "text": "5.3 The Effect of Weight Regularization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our models trained with either weight decay factor exhibited very similar patterns in the emergence of ${\\mathcal{N C}}$ (or lack thereof), but the more aggressive factor $\\beta=0.1$ resulted in stronger development of ${\\mathcal{N C}}$ properties than with $\\beta=0.0005$ (Appendices E, F, G, H, I, J, K, M, N, P). These findings empirically affirm $\\beta=0.1$ weight decay as sensible for LLM pre-training [81], and concur with [56] on the pivotal role that appropriate regularization plays in the emergence of ${\\mathcal{N C}}$ . ", "page_idx": 8}, {"type": "text", "text": "6 Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Neural collapse While to the best of our knowledge, no previous work has studied realistic stochastic token prediction, it is possible that the quantities that we measure are not perfectly suited for ${\\mathcal{N C}}$ in language modeling. As we described in $\\S1.1$ , the ${\\mathcal{N C}}$ framework does not translate neatly to the language modeling space due to many adverse conditions, so full convergence to ${\\mathcal{N C}}$ in the TPT was highly improbable. This paper leaves much room for future work to better adapt ${\\mathcal{N C}}$ for next-token prediction, which we discuss further in Section 7. ", "page_idx": 8}, {"type": "text", "text": "Language modeling Our work focused on autoregressive pre-training in its most basic form. We did not conduct experiments into encoder, multi-modal, or instruction-tuned models. Post-training techniques such as supervised fine-tuning, reinforcement learning with human feedback [91] or direct preference optimization [92] are also out-of-scope. This paper uses validation CE loss as the sole indicator of performance, leaving out any downstream task evaluations. ", "page_idx": 8}, {"type": "text", "text": "Confounder of model scale The models that we used in our permutation test (\u00a75.1, Table 1) are only of a single small architecture trained for one epoch with relatively weak weight regularization $(\\beta\\,=\\,0.0005)$ ). Therefore, our experimental results on scale-independent links between ${\\mathcal{N C}}$ and generalization may not necessarily translate to larger models. Further investigation on (foundation) LLMs orders of magnitude larger than our CLMs trained with modern NLP methods would provide more robust insight into any direct correlations. ", "page_idx": 8}, {"type": "text", "text": "7 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Layer/depth-wise neural collapse Past works have established that properties resembling ${\\mathcal{N C}}$ evolve as a function of model depth [4, 36, 53, 59, 60, 93\u2013105]. Layer-wise ${\\mathcal{N C}}-$ sometimes dubbed deep neural collapse (DNC) [53, 59] \u2013 and related phenomena at intermediate layers remain an interesting subtopic. We leave their study and induction in CLMs (like [106]) as future work. ", "page_idx": 9}, {"type": "text", "text": "Learning to collapse Given the evidence for the development of ${\\mathcal{N C}}$ and associated generalization under various loss functions [19, 49, 60, 101, 107\u2013109] in other domains, NLP researchers may still benefti from analyzing, adapting or training towards ${\\mathcal{N C}}$ . As alluded to earlier, the simplex ETF and even the CE loss may not be truly optimal for this problem setting, so we anticipate future works to both construct more amenable geometries with better-suited objectives and then capitalize on their benefits downstream. As discussed in $\\S2$ , there is an abundance of literature in ${\\mathcal{N C}}$ , some of which could potentially adapt ${\\mathcal{N C}}$ to be useful for NLP; we hope to inspire such investigations. ", "page_idx": 9}, {"type": "text", "text": "LLM Evaluations Researchers in NLP and multimodal settings are ultimately interested in measuring model performance on practical tasks. Notable benchmarks include GLUE [77], MMLU [110], and BIG-bench [111]. However, several contemporaries [112\u2013114] have demonstrated that models\u2019 downstream capabilities are roughly correlated with their abilities to effectively compress their pretraining data. Based on their findings, our application of the ${\\mathcal{N C}}$ framework to the pre-training stage of CLMs against validation CE loss should be an appropriate first step in this intersection. Looking forward, we anticipate exciting analysis for language modeling tasks or benchmarks, especially on creativity and retrieval for natural language understanding and generation. ", "page_idx": 9}, {"type": "text", "text": "Conversely, some form of ${\\mathcal{N C}}$ could even serve as an alternative evaluation. Although it would be prohibitively expensive to measure ${\\mathcal{N C}}$ on the vast and sometimes obscure pre-training data of most frontier production LLMs, doing so on a small set of unseen data (i.e. test data) would be realistic. ", "page_idx": 9}, {"type": "text", "text": "Interpretability At a high level, the number and density of clusters for a token can reflect its learned meanings and uses. This would be particularly useful as LLMs adapt to ever-evolving language and further expansion into non-English domains. Our formulae (Section 3) and results (Section 4) expose the pairwise token class interactions in noise, interference, classifier duality, and classifier agreement in the top-level features. Similarly to works in other domains [21, 55, 72, 115], these ${\\mathcal{N C}}$ metrics can serve as a form of low-level interpretability to aid understanding certain behaviors of (L)LMs. Between tokens, one can often discern how related or interchangeable words are based on their pair-wise interactions, or how antithetical or unrelated they are based on orthogonality. For example, we present a rudimentary analysis of homonyms and English first names in Appendix Q. ", "page_idx": 9}, {"type": "text", "text": "Fairness Foundation models are ubiquitous for their comprehensive capabilities and adaptability. As previous work discussed class imbalance [35, 58, 61, 62], our work may extend these strategies to measure and perhaps promote fairness in foundation LLMs, some of which are designed to be multilingual or multicultural. For example, [116] contemporarily explores the use of UNC3 to mitigate biases in BERT-based [7] models. ", "page_idx": 9}, {"type": "text", "text": "While ${\\mathcal{N C}}$ itself would not lead to unfairness, its potential interpretability may, in theory, enable an (adversarial) agent to measure and optimize for (un)fairness as they manipulate an LLM. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we apply the neural collapse $(\\mathcal{N}\\mathcal{C})$ framework to the next-token prediction problem, where models are undertrained and next-tokens are variably drawn from numerous and imbalanced token classes. We leverage canonical and more recent metrics to demonstrate that ${\\mathcal{N C}}$ emerges as we scale the size and training of hundreds of causal language models. Our results show a correlation between ${\\mathcal{N C}}$ and generalization, a relationship that persists even when the model scale is fixed. ", "page_idx": 9}, {"type": "text", "text": "In the short term, our work presents rudimentary techniques to analyze and interpret token-level properties of (L)LMs. We anticipate future work to suitably adapt ${\\dot{N}}C$ (and related frameworks) to the still-fresh frontier of autoregressive language modeling. Researchers could then effectively capitalize on previous learnings from ${\\mathcal{N C}}$ to better understand and improve the pre/post-training processes of increasingly complex and large language (multimodal) models. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Elliot Creager, David Glukhov, Daniel Johnson, Jivan Waber, and Colin Raffel for their helpful feedback and stimulating discussions. Aditya Mehrotra and Yu Bo Gao provided technical assistance in our implementations. We acknowledge the support of Canada\u2019s Natural Sciences and Engineering Research Council (NSERC, www.nserc-crsng.gc.ca/). This research was enabled in part by resources from Calcul Qu\u00e9bec (www.calculquebec.ca), the Digital Research Alliance of Canada (www.alliancecan.ca), and the Vector Institute (www.vectorinstitute.ai). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Vardan Papyan, X. Y. Han, and David L. Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. Proceedings of the National Academy of Sciences, 117(40):24652\u201324663, sep 2020. doi: 10.1073/pnas.2015509117. URL https://www.pnas. org/doi/full/10.1073/pnas.2015509117.   \n[2] Ronen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent english?, 2023. URL https://arxiv.org/abs/2305.07759. [3] Tomer Galanti, Andr\u00e1s Gy\u00f6rgy, and Marcus Hutter. On the role of neural collapse in transfer learning, 2022. URL https://arxiv.org/abs/2112.15121v2. [4] Tomer Galanti, Liane Galanti, and Ido Ben-Shaul. On the implicit bias towards minimal depth of deep neural networks, 2022. URL https://arxiv.org/abs/2202.09028v9.   \n[5] Vignesh Kothapalli. Neural collapse: A review on modelling principles and generalization. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https:// openreview.net/forum?id=QTXocpAP9p. [6] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, page 610\u2013623, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445922. URL https://doi.org/10.1145/3442188.3445922.   \n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019. URL https://arxiv. org/abs/1810.04805.   \n[8] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. [9] Jiachen Jiang, Jinxin Zhou, Peng Wang, Qing Qu, Dustin Mixon, Chong You, and Zhihui Zhu. Generalized neural collapse for a large number of classes, 2023. URL https://arxiv.org/ abs/2310.05351.   \n[10] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen. Breaking the softmax bottleneck: A high-rank rnn language model, 2018. URL https://arxiv.org/abs/1711. 03953.   \n[11] C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal, 27(3):379\u2013423, 1948. doi: 10.1002/j.1538-7305.1948.tb01338.x.   \n[12] P Sargant Florence. Human behaviour and the principle of least effort. The Economic Journal, 60(240):808\u2013810, 1950.   \n[13] Daniel Jurafsky and James H Martin. Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition, 2009.   \n[14] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001.08361v1.   \n[15] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. URL https: //arxiv.org/abs/2203.15556v1.   \n[16] Niklas Muennighoff, Alexander M. Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models, 2023. URL https://arxiv.org/abs/2305.16264v4.   \n[17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[18] Jianfeng Lu and Stefan Steinerberger. Neural collapse with cross-entropy loss, 2021. URL https://arxiv.org/abs/2012.08465v2.   \n[19] Weiyang Liu, Longhui Yu, Adrian Weller, and Bernhard Sch\u00f6lkopf. Generalizing and decoupling neural collapse via hyperspherical uniformity gap, 2023. URL https://arxiv.org/ abs/2303.06484v2.   \n[20] Zexi Li, Xinyi Shang, Rui He, Tao Lin, and Chao Wu. No fear of classifier biases: Neural collapse inspired federated learning with synthetic and fixed classifier. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5319\u20135329, 2023.   \n[21] Vignesh Kothapalli, Tom Tirer, and Joan Bruna. A neural collapse perspective on feature evolution in graph neural networks. Advances in Neural Information Processing Systems, 36, 2024.   \n[22] Yibo Yang, Haobo Yuan, Xiangtai Li, Jianlong Wu, Lefei Zhang, Zhouchen Lin, Philip Torr, Dacheng Tao, and Bernard Ghanem. Neural collapse terminus: A unified solution for class incremental learning and its variants, 2023. URL https://arxiv.org/abs/2308. 01746v1.   \n[23] Qinhao Zhou, Xiang Xiang, and Jing Ma. Hierarchical task-incremental learning with featurespace initialization inspired by neural collapse. Neural Processing Letters, 55(8):10811\u201310827, 2023.   \n[24] Hang Ran, Weijun Li, Lusi Li, Songsong Tian, Xin Ning, and Prayag Tiwari. Learning optimal inter-class margin adaptively for few-shot class-incremental learning via neural collapse-based meta-learning. Information Processing & Management, 61(3):103664, 2024. ISSN 0306-4573. doi: https://doi.org/10.1016/j.ipm.2024.103664. URL https://www.sciencedirect.com/ science/article/pii/S0306457324000244.   \n[25] Saaketh Medepalli and Naren Doraiswamy. On the role of neural collapse in meta learning models for few-shot learning, 2023. URL https://arxiv.org/abs/2310.00451v2.   \n[26] Jarrod Haas, William Yolland, and Bernhard Rabus. Linking neural collapse and l2 normalization with improved out-of-distribution detection in deep neural networks, 2023. URL https://arxiv.org/abs/2209.08378v3.   \n[27] Mou\u00efn Ben Ammar, Nacim Belkhir, Sebastian Popescu, Antoine Manzanera, and Gianni Franchi. Neco: Neural collapse based out-of-distribution detection, 2024. URL https: //arxiv.org/abs/2310.06823.   \n[28] Jiawei Zhang, Yufan Chen, Cheng Jin, Lei Zhu, and Yuantao Gu. Epa: Neural collapse inspired robust out-of-distribution detector, 2024. URL https://arxiv.org/abs/2401.01710v1.   \n[29] Donghao Li, Yang Cao, and Yuan Yao. Neuromixgdp: A neural collapse-inspired random mixup for private data release. In Conference on Parsimony and Learning, pages 480\u2013514. PMLR, 2024.   \n[30] Chendi Wang, Yuqing Zhu, Weijie J. Su, and Yu-Xiang Wang. Neural collapse meets differential privacy: Curious behaviors of noisygd with near-perfect representation learning, 2024. URL https://arxiv.org/abs/2405.08920v2.   \n[31] Tolga Ergen, Arda Sahiner, Batu Ozturkler, John Pauly, Morteza Mardani, and Mert Pilanci. Demystifying batch normalization in relu networks: Equivalent convex optimization models and implicit regularization, 2022. URL https://arxiv.org/abs/2103.01499v3.   \n[32] Tolga Ergen and Mert Pilanci. Revealing the structure of deep neural networks via convex duality. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 3004\u20133014. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/ v139/ergen21b.html.   \n[33] Mariia Seleznova, Dana Weitzner, Raja Giryes, Gitta Kutyniok, and Hung-Hsu Chou. Neural (tangent kernel) collapse. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 16240\u201316270. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 3477ca0ce484aa2fa42c1361ab601c25-Paper-Conference.pdf.   \n[34] Matus Telgarsky. Feature selection with gradient descent on two-layer networks in low-rotation regimes, 2022. URL https://arxiv.org/abs/2208.02789v1.   \n[35] Wanli Hong and Shuyang Ling. Neural collapse for unconstrained feature model under crossentropy loss with imbalanced data, 2023. URL https://arxiv.org/abs/2309.09725v2.   \n[36] Gerard Ben Arous, Reza Gheissari, Jiaoyang Huang, and Aukosh Jagannath. High-dimensional sgd aligns with emerging outlier eigenspaces, 2023. URL https://arxiv.org/abs/2310. 03010v1.   \n[37] Like Hui, Mikhail Belkin, and Preetum Nakkiran. Limitations of neural collapse for understanding generalization in deep learning, 2022. URL https://arxiv.org/abs/2202.08384v1.   \n[38] Tomer Galanti, Andr\u00e1s Gy\u00f6rgy, and Marcus Hutter. Improved generalization bounds for transfer learning via neural collapse. In First Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward at ICML 2022, 2022. URL https://openreview.net/forum?id $=$ VrK7pKwOhT_.   \n[39] Zijian Wang, Yadan Luo, Liang Zheng, Zi Huang, and Mahsa Baktashmotlagh. How far pretrained models are from neural collapse on the target dataset informs their transferability. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5549\u20135558, 2023.   \n[40] Xiao Li, Sheng Liu, Jinxin Zhou, Xinyu Lu, Carlos Fernandez-Granda, Zhihui Zhu, and Qing Qu. Understanding and improving transfer learning of deep models via neural collapse. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https:// openreview.net/forum?id $\\equiv$ o8r84MzTQB.   \n[41] Didi Zhu, Zexi Li, Min Zhang, Junkun Yuan, Yunfeng Shao, Jiashuo Liu, Kun Kuang, Yinchuan Li, and Chao Wu. Understanding prompt tuning for v-l models through the lens of neural collapse, 2023. URL https://arxiv.org/abs/2306.15955v3.   \n[42] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report 0, University of Toronto, Toronto, Ontario, 2009. URL https://www.cs. toronto.edu/\\~kriz/learning-features-2009-TR.pdf.   \n[43] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[44] Dustin G. Mixon, Hans Parshall, and Jianzong Pi. Neural collapse with unconstrained features, 2020. URL https://arxiv.org/abs/2011.11619v1.   \n[45] Tomaso Poggio and Qianli Liao. Explicit regularization and implicit bias in deep network classifiers trained with the square loss, 2020. URL https://arxiv.org/abs/2101.00072v1.   \n[46] Weinan E and Stephan Wojtowytsch. On the emergence of simplex symmetry in the final and penultimate layers of neural network classifiers, 2021. URL https://arxiv.org/abs/ 2012.05420v3.   \n[47] Cong Fang, Hangfeng He, Qi Long, and Weijie J Su. Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training. Proceedings of the National Academy of Sciences, 118(43):e2103091118, 2021.   \n[48] Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geometric analysis of neural collapse with unconstrained features, 2021. URL https: //arxiv.org/abs/2105.02375v1.   \n[49] X.Y. Han, Vardan Papyan, and David L. Donoho. Neural collapse under MSE loss: Proximity to and dynamics on the central path. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id $=$ w1UbdvWH_R3.   \n[50] Can Yaras, Peng Wang, Zhihui Zhu, Laura Balzano, and Qing Qu. Neural collapse with normalized features: A geometric analysis over the riemannian manifold. Advances in neural information processing systems, 35:11547\u201311560, 2022.   \n[51] Akshay Rangamani and Andrzej Banburski-Fahey. Neural collapse in deep homogeneous classifiers and the role of weight decay. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4243\u20134247, 2022. doi: 10.1109/ ICASSP43922.2022.9746778.   \n[52] Yibo Yang, Shixiang Chen, Xiangtai Li, Liang Xie, Zhouchen Lin, and Dacheng Tao. Inducing neural collapse in imbalanced learning: Do we really need a learnable classifier at the end of deep neural network?, 2022. URL https://arxiv.org/abs/2203.09081v3.   \n[53] Tom Tirer and Joan Bruna. Extended unconstrained features model for exploring deep neural collapse. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 21478\u201321505. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/tirer22a.html.   \n[54] Peng Wang, Huikang Liu, Can Yaras, Laura Balzano, and Qing Qu. Linear convergence analysis of neural collapse with unconstrained features. In OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop), 2022.   \n[55] Jinxin Zhou, Chong You, Xiao Li, Kangning Liu, Sheng Liu, Qing Qu, and Zhihui Zhu. Are all losses created equal: A neural collapse perspective, 2022. URL https://arxiv.org/ abs/2210.02192v2.   \n[56] Akshay Rangamani, Marius Lindegaard, Tomer Galanti, and Tomaso A Poggio. Feature learning in deep classifiers through intermediate neural collapse. In International Conference on Machine Learning, pages 28729\u201328745. PMLR, 2023.   \n[57] Tom Tirer, Haoxiang Huang, and Jonathan Niles-Weed. Perturbation analysis of neural collapse. In International Conference on Machine Learning, pages 34301\u201334329. PMLR, 2023.   \n[58] Hien Dang, Tho Tran, Stanley Osher, Hung Tran-The, Nhat Ho, and Tan Nguyen. Neural collapse in deep linear networks: From balanced to imbalanced data, 2023. URL https: //arxiv.org/abs/2301.00437v5.   \n[59] Peter S\u00faken\u00edk, Marco Mondelli, and Christoph H Lampert. Deep neural collapse is provably optimal for the deep unconstrained features model. Advances in Neural Information Processing Systems, 36, 2024.   \n[60] Duc Anh Nguyen, Ron Levie, Julian Lienen, Eyke H\u00fcllermeier, and Gitta Kutyniok. Memorization-dilation: Modeling neural collapse under noise. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id $=$ cJWxqmmDL2b.   \n[61] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang, and Jiaya Jia. Understanding imbalanced semantic segmentation through neural collapse. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19550\u201319560, 2023.   \n[62] Xuantong Liu, Jianfeng Zhang, Tianyang Hu, He Cao, Yuan Yao, and Lujia Pan. Inducing neural collapse in deep long-tailed learning. In International Conference on Artificial Intelligence and Statistics, pages 11534\u201311544. PMLR, 2023.   \n[63] Peifeng Gao, Qianqian Xu, Peisong Wen, Huiyang Shao, Zhiyong Yang, and Qingming Huang. A study of neural collapse phenomenon: Grassmannian frame, symmetry and generalization, 2023. URL https://arxiv.org/abs/2304.08914v2.   \n[64] Mufan Bill Li, Mihai Nica, and Daniel M. Roy. The neural covariance sde: Shaped infinite depth-and-width networks at initialization, 2023. URL https://arxiv.org/abs/2206. 02768v3.   \n[65] Zhanxuan Hu, Yichen Wang, Hailong Ning, Yonghang Tai, and Feiping Nie. Neural collapse inspired semi-supervised learning with fixed classifier. Information Sciences, 667:120469, 2024.   \n[66] Gao Peifeng, Qianqian Xu, Yibo Yang, Peisong Wen, Huiyang Shao, Zhiyong Yang, Bernard Ghanem, and Qingming Huang. Towards demystifying the generalization behaviors when neural collapse emerges, 2024. URL https://openreview.net/forum?id $\\cdot^{=}$ XVv4S6LnMk.   \n[67] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy models of superposition. Transformer Circuits Thread, 2022. URL https: //transformer-circuits.pub/2022/toy_model/index.html.   \n[68] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.   \n[69] A. Dubey et al. (101 additional authors). The llama 3 herd of models. 2024. URL https: //arxiv.org/abs/2407.21783. All authors were affiliated with Meta.   \n[70] Christos Thrampoulidis, Ganesh Ramachandra Kini, Vala Vakilian, and Tina Behnia. Imbalance trouble: Revisiting neural-collapse geometry. Advances in Neural Information Processing Systems, 35:27225\u201327238, 2022.   \n[71] Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with noisy labels. Advances in neural information processing systems, 26, 2013.   \n[72] Quinn Fisher, Haoming Meng, and Vardan Papyan. Pushing boundaries: Mixup\u2019s influence on neural collapse, 2024. URL https://arxiv.org/abs/2402.06171v1.   \n[73] Pengyu Li, Xiao Li, Yutong Wang, and Qing Qu. Neural collapse in multi-label learning with pick-all-label loss, 2024. URL https://arxiv.org/abs/2310.15903v4.   \n[74] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[75] David Mimno and Laure Thompson. The strange geometry of skip-gram with negative sampling. In Martha Palmer, Rebecca Hwa, and Sebastian Riedel, editors, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2873\u20132878, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1308. URL https://aclanthology.org/D17-1308.   \n[76] Kawin Ethayarajh. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings, 2019. URL https://arxiv.org/abs/1909. 00512.   \n[77] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding, 2019. URL https://arxiv.org/abs/1804.07461v3.   \n[78] Jia Hui Feng, Edmund M-K Lai, and Weihua Li. A study of neural collapse for text classification. In International Conference on Deep Learning Theory and Applications, pages 126\u2013142. Springer, 2023.   \n[79] Thomas Laurent, James H. von Brecht, and Xavier Bresson. Feature collapse, 2023. URL https://arxiv.org/abs/2305.16162v1.   \n[80] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https: //doi.org/10.5281/zenodo.5297715. If you use this software, please cite it using these metadata.   \n[81] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u2013 1901, 2020.   \n[82] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022. URL https: //arxiv.org/abs/2205.01068v4.   \n[83] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. URL https://arxiv.org/abs/2302.13971v1.   \n[84] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7b, 2023. URL https://arxiv.org/abs/2310.06825v1.   \n[85] Ronald A Fisher. The use of multiple measurements in taxonomic problems. Annals of eugenics, 7(2):179\u2013188, 1936.   \n[86] C Radhakrishna Rao. The utilization of multiple measurements in problems of biological classification. Journal of the Royal Statistical Society. Series B (Methodological), 10(2): 159\u2013203, 1948.   \n[87] Thomas Strohmer and Robert W Heath Jr. Grassmannian frames with applications to coding and communication. Applied and computational harmonic analysis, 14(3):257\u2013275, 2003.   \n[88] Shayne FD Waldron. An introduction to finite tight frames. Springer, 2018.   \n[89] David L Donoho, Michael Elad, and Vladimir N Temlyakov. Stable recovery of sparse overcomplete representations in the presence of noise. IEEE Transactions on information theory, 52(1):6\u201318, 2005.   \n[90] Joel A Tropp. Just relax: Convex programming methods for identifying sparse signals in noise. IEEE transactions on information theory, 52(3):1030\u20131051, 2006.   \n[91] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2020. URL https://arxiv.org/abs/1909.08593.   \n[92] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 53728\u201353741. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/ 2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf.   \n[93] Vardan Papyan. Traces of class/cross-class structure pervade deep learning spectra, 2020. URL https://arxiv.org/abs/2008.11865v1. [94] Christopher R. Hoyt and Art B. Owen. Probing neural networks with t-sne, class-specific projections and a guided tour, 2021. URL https://arxiv.org/abs/2107.12547v1.   \n[95] John Zarka, Florentin Guth, and St\u00e9phane Mallat. Separation and concentration in deep networks, 2021. URL https://arxiv.org/abs/2012.10424v2.   \n[96] Ido Ben-Shaul and Shai Dekel. Nearest class-center simplification through intermediate layers. In Alexander Cloninger, Timothy Doster, Tegan Emerson, Manohar Kaul, Ira Ktena, Henry Kvinge, Nina Miolane, Bastian Rieck, Sarah Tymochko, and Guy Wolf, editors, Proceedings of Topological, Algebraic, and Geometric Learning Workshops 2022, volume 196 of Proceedings of Machine Learning Research, pages 37\u201347. PMLR, 25 Feb\u201322 Jul 2022. URL https: //proceedings.mlr.press/v196/ben-shaul22a.html. [97] Hangfeng He and Weijie J. Su. A law of data separation in deep learning, 2022. URL https://arxiv.org/abs/2210.17020v2.   \n[98] Liam Parker, Emre Onal, Anton Stengel, and Jake Intrater. Neural collapse in the intermediate hidden layers of classification neural networks, 2023. URL https://arxiv.org/abs/2308. 02760v1.   \n[99] Akshay Rangamani, Marius Lindegaard, Tomer Galanti, and Tomaso A Poggio. Feature learning in deep classifiers through intermediate neural collapse. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 28729\u201328745. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/rangamani23a.html.   \n[100] Wojciech Masarczyk, Mateusz Ostaszewski, Ehsan Imani, Razvan Pascanu, Piotr Mi\u0142 o\u00b4s, and Tomasz Trzcinski. The tunnel effect: Building data representations in deep neural networks. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 76772\u201376805. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/ 2023/file/f249db9ab5975586f36df46f8958c008-Paper-Conference.pdf.   \n[101] Daniel Beaglehole, Peter S\u00faken\u00edk, Marco Mondelli, and Mikhail Belkin. Average gradient outer product as a mechanism for deep neural collapse, 2024. URL https://arxiv.org/ abs/2402.13728v2.   \n[102] Peng Wang, Xiao Li, Can Yaras, Zhihui Zhu, Laura Balzano, Wei Hu, and Qing Qu. Understanding deep representation learning via layerwise feature compression and discrimination, 2024. URL https://arxiv.org/abs/2311.02960v2.   \n[103] Sicong Wang, Kuo Gai, and Shihua Zhang. Progressive feedforward collapse of resnet training, 2024. URL https://arxiv.org/abs/2405.00985v1.   \n[104] Connall Garrod and Jonathan P. Keating. Unifying low dimensional observations in deep learning through the deep linear unconstrained feature model, 2024. URL https://arxiv. org/abs/2404.06106v1.   \n[105] Emanuele Zangrando, Piero Deidda, Simone Brugiapaglia, Nicola Guglielmi, and Francesco Tudisco. Neural rank collapse: Weight decay and small within-class variability yield low-rank bias, 2024. URL https://arxiv.org/abs/2402.03991v1.   \n[106] Jiachen Jiang, Jinxin Zhou, and Zhihui Zhu. On layer-wise representation similarity: Application for multi-exit models with a single classifier, 2024. URL https://arxiv.org/abs/ 2406.14479.   \n[107] Mengjia Xu, Akshay Rangamani, Qianli Liao, Tomer Galanti, and Tomaso Poggio. Dynamics in deep classifiers trained with the square loss: Normalization, low rank, neural collapse, and generalization bounds. Research, 6:0024, 2023.   \n[108] Tong Liang and Jim Davis. Inducing neural collapse to a fixed hierarchy-aware frame for reducing mistake severity. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1443\u20131452, 2023.   \n[109] Guglielmo Bonifazi, Iason Chalas, Gian Hess, and Jakub \u0141ucki. Can we understand plasticity through neural collapse?, 2024. URL https://arxiv.org/abs/2404.02719v1.   \n[110] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021.   \n[111] BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id $=$ uyTL5Bvosj.   \n[112] Zhengxiao Du, Aohan Zeng, Yuxiao Dong, and Jie Tang. Understanding emergent abilities of language models from the loss perspective, 2024. URL https://arxiv.org/abs/2403. 15796.   \n[113] Yuzhen Huang, Jinghan Zhang, Zifei Shan, and Junxian He. Compression represents intelligence linearly. In First Conference on Language Modeling, 2024. URL https: //openreview.net/forum?id $\\cdot$ SHMj84U5SH.   \n[114] Mingjia Yin, Chuhan Wu, Yufei Wang, Hao Wang, Wei Guo, Yasheng Wang, Yong Liu, Ruiming Tang, Defu Lian, and Enhong Chen. Entropy law: The story behind data compression and llm performance, 2024.   \n[115] Li Guo, Keith Ross, Zifan Zhao, George Andriopoulos, Shuyang Ling, Yufeng Xu, and Zixuan Dong. Cross entropy versus label smoothing: A neural collapse perspective, 2024. URL https://arxiv.org/abs/2402.03979v2.   \n[116] Jingxuan Xu, Wuyang Chen, Linyi Li, Yao Zhao, and Yunchao Wei. Collapsed language models promote fairness, 2024. URL https://arxiv.org/abs/2410.04472.   \n[117] Stephen Merity, Ilya Sutskever, Simon Kornblith, and Nikhil Goyal. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.   \n[118] Yao Zhu, Zhen Yu, Chao Zhang, Yijia Wu, et al. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. arXiv preprint arXiv:1506.05829, 2015.   \n[119] Common Crawl. Common crawl, 2023. URL https://commoncrawl.org. Accessed: 2023-10-30.   \n[120] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2020. URL https: //arxiv.org/abs/2101.00027.   \n[121] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. The curse of recursion: Training on generated data makes models forget, 2024. URL https://arxiv.org/abs/2305.17493.   \n[122] Matthias Gerstgrasser, Rylan Schaeffer, Apratim Dey, Rafael Rafailov, Henry Sleight, John Hughes, Tomasz Korbak, Rajashree Agrawal, Dhruv Pai, Andrey Gromov, Daniel A. Roberts, Diyi Yang, David L. Donoho, and Sanmi Koyejo. Is model collapse inevitable? breaking the curse of recursion by accumulating real and synthetic data, 2024. URL https://arxiv. org/abs/2404.01413.   \n[123] Neil Burgess, Jelena Milanovic, Nigel Stephens, Konstantinos Monachopoulos, and David Mansell. Bfloat16 processing for neural networks. In 2019 IEEE 26th Symposium on Computer Arithmetic (ARITH), pages 88\u201391, 2019. doi: 10.1109/ARITH.2019.00022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "A Dataset ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "TinyStories is a dataset of short children\u2019s stories generated by GPT-3.5 and GPT-4 [2], released with the CDLA-Sharing-1.0 licence. We trained and evaluated on the first version, as described: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The 2,141,709 stories are split into 2,119,719 train and 21,990 validation stories.   \n\u2022 Their experimental setup [2] called for the GPT-2 [68] tokenizer, of which only a subset vocabulary $\\mathbb{V}=[1,29233]$ appears in TinyStories.   \n\u2022 Following the GPT-style teacher-forcing regime for training/evaluation [8], raw sequences (stories) from the train set are packed (by two preprocessing workers) into 229,367 $(S)$ chunks of 2048 $(T)$ tokens each. This setup provides 469,514,249 $(N)$ ground-truth18 token samples for training. ", "page_idx": 19}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/357276f7f25f4dd652ebd111476f2540edda6ca5165ab9c578556fa41db9a8d0.jpg", "img_caption": ["Figure 4: The $500\\;\\mathrm{most}$ frequent classes from TinyStories [2] exhibit significant sample imbalance. Despite the synthetic nature of TinyStories, such a distribution is typical of natural language [11, 12]. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "A.1 Alternative (Real) Datasets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The study of ${\\mathcal{N C}}$ in causal language modeling at the token level would be very expensive, so the motivation to use a small dataset is clear. However, most commonly used text datasets such as WikiText [117], BookCorpus [118], CommonCrawl [119], or most subsets from the Pile [120] are much too complex and broad to be effectively compressed by CLMs of the scale that we work with. ", "page_idx": 19}, {"type": "text", "text": "WikiText-2 and WikiText-103 present significant drawbacks for our experiments. Both datasets contain a considerable amount of low-quality data that does not concentrate on essential linguistic structures such as grammar, vocabulary, facts, and reasoning. WikiText-2 has a similar empirical vocabulary to TinyStories under the GPT-Neo [80] tokenizer (27K vs. 29K) but only has around 34K rows of training data compared to 2.1M in TinyStories. Our small-scale NC experiment on WikiText-2 revealed that the models were very brittle and prone to overfitting. On the other hand, WikiText-103 is comparably sized to TinyStories but utilizes around 44K unique tokens. Our CLMs trained on WikiText-103 struggled to produce coherent sentences, likely due to the excessive breadth and information, as noted by the authors of TinyStories. Beyond these two, we were unable to find any real datasets that both followed established scaling laws [14, 15] for CLMs at our scale and are simple enough to suit the analysis of ${\\mathcal{N C}}$ . ", "page_idx": 19}, {"type": "text", "text": "A.2 On the Use of TinyStories ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "According to its authors, TinyStories [2] is explicitly designed to preserve the essential elements of natural language, such as grammar, vocabulary, facts, and reasoning, while being smaller and more refined in terms of its breadth and diversity. Unlike large corpora that can overwhelm small language models (SLMs) due to their excessive breadth and diversity, TinyStories offers a concentrated dataset that hones in on core linguistic structures and reasoning capabilities. This is evident in its small word vocabulary, consisting of approximately 1500 words that a child would use, and in its 29K empirical vocabulary under the GPT-Neo tokenizer. ", "page_idx": 19}, {"type": "text", "text": "Despite its concentrated nature, TinyStories enables models trained on it to produce grammatically correct, factual, and reasonable stories. Additionally, these models can be finetuned on specific instructions found in the TinyStories-Instruct dataset. The authors of TinyStories also demonstrate that their models can creatively produce stories that are dissimilar enough to their training data, indicating a balanced capability for generalization and creativity. ", "page_idx": 20}, {"type": "text", "text": "One particular advantage of TinyStories is the small vocabulary relative to total training tokens, rendering a reasonable number of classes with higher average token counts. This is relevant because the possibility of ${\\mathcal{N C}}$ and a CLM\u2019s ability to compress language data into distinct geometries depend partially on the ratios between embedding dimension, vocabulary size, and average token frequency. Conveniently, frequency analysis of the overall dataset produced a distribution (Figure 4) similar to real human language, so TinyStories should provide a good balance for an initial study of this ${\\mathcal{N C}}$ . ", "page_idx": 20}, {"type": "text", "text": "Additionally, TinyStories has a more regular structure as GPT- $.3.5/4$ was instructed to produce children\u2019s stories with certain themes and forms with a conservative vocabulary. We believe this would reduce the amount of clustering noise from the breadth of information and structures in real general data, and allow our smaller CLMs to exhibit some clear trends toward ${\\mathcal{N C}}$ . ", "page_idx": 20}, {"type": "text", "text": "Furthermore, TinyStories was created using GPT $3.5/4$ , which are advanced language models with significantly larger architectures trained on orders of magnitude more tokens, helping minimize the effect of the synthetic nature of the generated dataset. We also considered a possible effect of model collapse as a result of training on synthetic data [121] and follow-up work [122] suggest that a single iteration of data generation (as generated TinyStories) has a very negligible model collapse. ", "page_idx": 20}, {"type": "text", "text": "B Model Architectural Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Table 2: Sample architectural configuration for a 12-layer 1024-dimensional causal language model (CLM) based on [2] and GPT-Neo [80]. Shallower models have configurations with attention_layers and attention_types truncated. Narrower models adjust hidden_size to their width $(d)$ . All other configuration values are the same across models. ", "page_idx": 20}, {"type": "table", "img_path": "G0LfcMiRkc/tmp/a654ef69856ef696c2b1e8096103ec8917be47ccecf4de1d3bdc1f620d8754d7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "C Optimization ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Training was performed using an adaptation of an open-source causal language modeling script from Huggingface: https://github.com/huggingface/transformers/blob/main/ examples/pytorch/language-modeling/run_clm.py ", "page_idx": 21}, {"type": "text", "text": "\u2022 Each model was trained on a single NVIDIA A100 (40GB) GPU for up to 8 hours per epoch.   \n\u2022 Learning rates were set by a linear schedule based on the number of steps with no warm-up.   \n\u2022 Training was performed in bfloat16 [123] mixed precision.   \n\u2022 The results presented in this work are from two sets of models trained with weight decay $\\beta=0.0005$ [51] and $\\beta=0.1$ [81]. A previous set of models was trained without weight decay and the results are very similar to $\\beta=0.0005$ . ", "page_idx": 21}, {"type": "text", "text": "Table 3: Batch sizes used to train models on a single NVIDIA A100 (40GB) GPU. Width $(d)$ and depth $(L)$ correspond to hidden_size and length of attention_layers, respectively, in Table 2. ", "page_idx": 21}, {"type": "table", "img_path": "G0LfcMiRkc/tmp/6ae7cc1cd73bbcd3581972ace8192601a36cf7b0fef0b4e5e6641c4de2be95f2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/b9eb58a50331651d5e3c89ff47cf76518448078bd733da40bbb876cc1a6da760.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 5: Average (logarithmic) class-distance normalized variance (CDNV, $\\mathcal{N}\\mathcal{C}_{1}$ ) (left) and validation (cross-entropy) loss (right) with respect to training epochs. ", "page_idx": 21}, {"type": "text", "text": "D Embeddings Collection & NC Analysis ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Codes for (post-)training analysis are hosted on GitHub: ", "page_idx": 21}, {"type": "text", "text": "\u2022 Main code https://github.com/rhubarbwu/linguistic-collapse \u2022 Auxillary package: https://github.com/rhubarbwu/neural-collapse ", "page_idx": 21}, {"type": "text", "text": "One pass over the train set for embeddings collection can take up to 6 hours on a single NVIDIA A100 (40GB) GPU. Analysis of a single metric for a given model takes no longer than 5 minutes. ", "page_idx": 21}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/b7b0e4bce8f28e162ab95326a9742da6c4c961cc078195f6c8044716670c35ba.jpg", "img_caption": ["E Within-Class Variability Collapse with Scale \u2013 ${\\mathcal{N C}}_{1}$ ", "Figure 6: Average (logarithmic) class-distance normalized variance (CDNV) is reduced $(\\mathcal{N}\\mathcal{C}_{1})$ when scaling width $(d)$ and across training for 1 (left) through 10 (right) epochs with weight decays $\\beta=0.0005$ (top) and $\\beta=0.1$ (bottom). "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/b289376854d1b8a5f0aa694287db07311eaef6405603195a8886f27877c04ff0.jpg", "img_caption": ["Figure 7: Average (logarithmic) class-distance normalized variance (CDNV) is reduced $(\\mathcal{N}\\mathcal{C}_{1})$ when scaling depth $(L)$ and across training for 1 (left) through 10 (right) epochs with weight decays $\\beta=0.0005$ (top) and $\\beta=0.1$ (bottom). "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/5c3f0f2de6a0f1c20291e919f30676b076de961c143173753997cd5b0418da71.jpg", "img_caption": ["F Mean Norms Growth with Scale \u2013 (Related to ${\\mathcal{N}}{\\mathcal{C}}_{2}$ ) "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 8: Class embedding mean norms grow when scaling width $(d)$ and across training for 1 (left) through 10 (right) epochs with weight decays $\\beta=0.0005$ (top) and $\\beta=0.1$ (bottom). ", "page_idx": 23}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/66f4f2bce556b5ad610d990d8b25d8fc227ce9434b46f797b26b3cc0434d93a7.jpg", "img_caption": ["Figure 9: Class embedding mean norms grow when scaling depth $(L)$ and across training for 1 (left) through 10 (right) epochs with weight decays $\\beta=0.0005$ (top) and $\\beta=0.1$ (bottom). "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/4b058b7dcfa77292061f0212b708c24e885b2d102a136936bb417b90e5e476f6.jpg", "img_caption": ["G Equinormness with Scale \u2013 $(\\mathcal{G})\\mathcal{N C}_{2}$ "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 10: Variation in embedding norms decreases $(\\mathcal{N}\\mathscr{C}_{2})$ when scaling width $(d)$ in models trained for 1 (left) through 10 (right) with weight decays $\\beta=0.0005$ (top) and $\\beta=0.1$ (bottom). Note that the degree of equinormness eventually plateaus for sufficiently deep and trained models. ", "page_idx": 24}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/366e83c8c98516e8be370eef5a86f3cf86566694e2d66fd7c894e9a87eff69e2.jpg", "img_caption": ["Figure 11: Variation in embedding norms decreases $(\\mathcal{N}\\mathscr{C}_{2})$ when scaling depth $(L)$ in models trained for 1 (left) through 10 (right) with weight decays $\\beta=0.0005$ (top) and $\\beta=0.1$ (bottom). "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/7a4fedaf0244d06048a61783663f01ed46bf0f28e886261c700decdbd76e09ac.jpg", "img_caption": ["H Interference with Scale \u2013 (Related to $\\mathcal{N}\\ensuremath{\\mathcal{C}_{2}}^{\\phantom{\\dagger}}$ ) ", "Figure 12: Average interference decreases (to some extent) when scaling width $(d)$ in models trained for 1 (left) through 10 (right) with weight decays $\\beta=0.0005$ (top) and $\\beta=0.1$ (bottom). "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/02bd38f2e076536714d29366d94235a2ff753699871416b1d2f72e4cdb643b65.jpg", "img_caption": ["Figure 13: Average interference decreases (to some extent) when scaling depth $(L)$ in models trained for 1 (left) through 10 (right) with weight decays $\\beta=0.0005$ (top) and $\\beta=0.1$ (bottom). "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/f162932c5c851dfdd97437a3e216a02ff870923ddbb74fd0f63ce41da0a0da6a.jpg", "img_caption": ["I Equiangularity with Scale \u2013 (Against $\\mathcal{N}\\ensuremath{\\mathcal{C}_{2}}^{\\phantom{\\dagger}}$ ) ", "Figure 14: Variation in interference roughly increases when scaling width $(d)$ in models trained for 1 (left) through 10 (right) with weight decays $\\beta=0.0005$ (top) and $\\beta=0.1$ (bottom). Note this trend is against equiangularity, affirming the traditional $\\mathcal{N C}2$ to be less useful than $\\mathcal{G N C}2$ [19]. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/edd10b8df3b1dd7c1cbdf3f338f94651c2b78247010effd6b380beb67b969e8e.jpg", "img_caption": ["Figure 15: Variation in interference increases when scaling depth $(L)$ in models trained for 1 (left) through 10 (right) with weight decays $\\beta=0.0005$ (top) and $\\beta=0.1$ (bottom). Note this trend is against equiangularity, affirming the traditional $\\mathcal{N C}_{2}$ to be less useful than $\\mathcal{G N C_{2}}$ [19]. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/494305c7d7b8cfa02cc987498dfa983573a77870ff53ac4a78d77a63bd8e1451.jpg", "img_caption": ["J Logarithmic Distances with Scale \u2013 (Related to $\\mathcal{G N C}_{2}$ ) ", "Figure 16: Average logarithmic distance decreases when scaling width $(d)$ in models trained for 1 (left) through 10 (right) with weight decays $\\beta=0.0005$ (top) and $\\beta=0.1$ (bottom). "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/6daaef6f2145f14a61f3c30065613d96c02b6bb20fc05c4cf8c23bd7dc1fdc20.jpg", "img_caption": ["Figure 17: Average logarithmic distance decreases when scaling depth $(L)$ in models trained for 1 (left) through 10 (right) with weight decays $\\beta=0.0005$ (top) and $\\beta=0.1$ (bottom). "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/86a8546c2f52e6511185617717b80ce1eced9cc4093bf2c64b3af46f35e981d8.jpg", "img_caption": ["K Hyperspherical Uniformity with Scale \u2013 GNC2 ", "Figure 18: Variation in logarithmic distances decreases when scaling width $(d)$ in models trained for 1 (left) through 10 (right) with weight decays $\\beta=0.0005$ (top) and $\\beta=0.1$ (bottom). This consistent trend towards hyperspherical uniformity affirms that $\\mathcal{G N C}2$ [19] is more useful than $\\mathcal{N C}2$ . "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/a02dc4e1497d45e962e2e4e101bf2069cf192ee2c81b2b88ed8623289d397931.jpg", "img_caption": ["Figure 19: Variation in logarithmic distances decreases when scaling depth $(L)$ in models trained for 1 (left) through 10 (right) with weight decays $\\beta=0.0005$ (top) and $\\beta=0.1$ (bottom). This consistent trend towards hyperspherical uniformity affirms that $\\mathcal{G N C}2$ [19] is more useful than $\\mathcal{N C}2$ . "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/d3e05f4d0ebdd88075c930cb830d66ef7b81036e03c10b9a6f428612fbfce72e.jpg", "img_caption": ["L Correlations of $(\\mathcal{G})\\mathcal{N C}_{2}$ with Generalization Performance "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 20: Generalization (validation loss) shows some correlation with average mean norms (left) and almost none with their variations (i.e. equinormness, $\\mathcal{N C}_{2}$ ) (right). ", "page_idx": 29}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/a912ad006f6ad1ebb02814df512dd98b8750cb1bca9bb779137d5d0f403d0a7a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 21: Generalization (validation loss) correlated with average interference (left) and its variation (i.e. equiangularity, $\\mathcal{N C}_{2}$ ) (centre). We also computed the empirical measure from [5] (right). ", "page_idx": 29}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/f047a756b869cdd5de6562b1e42bd19bc24db24a637ec2b459e32deb9370dfe5.jpg", "img_caption": ["Figure 22: Validation loss shows some correlation with average (logarithmic) kernel distances and with their variation (i.e. hyperspherical uniformity, $\\mathcal{G N C}_{2})$ ) (right). "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "M Self-Duality with Scale \u2013 (Against $\\mathcal{N C}_{3}$ ) ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Self-duality $(\\mathcal{N}\\mathcal{C}_{3})$ was originally the convergence of classifiers to means up to rescaling [1]: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\pmb{w}_{c}}{\\|\\pmb{w}_{c}\\|_{2}}-\\frac{\\pmb{\\mu}_{c}-\\bar{\\pmb{\\mu}}}{\\|\\pmb{\\mu}_{c}-\\bar{\\pmb{\\mu}}\\|_{2}}\\right\\|_{2}\\rightarrow0,\\quad\\forall c.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Instead, we use class-wise cosine similarity (Equation 8) and its variation $(\\mathcal{U}\\mathcal{N}\\mathcal{C}3)$ . ", "page_idx": 30}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/c440c5e7c5b951f21b1ff8d94f06b13ec53616093e5666e525ae98010bda2db6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 23: Average classifier alignment increases $\\mathcal{N C}3$ when training for 1 (left) through 10 (right) with weight decays $\\beta=0.0005$ (top) and $\\beta=0.1$ (bottom). However, we see no meaningful trend when scaling width $d$ , suggesting that $\\mathcal{N C}3$ does not coalesce with language modeling training. ", "page_idx": 30}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/5020a2612d531c59c9dffa224498dce8c9d98398170a760e556b7618d7d76655.jpg", "img_caption": ["Figure 24: Average classifier alignment increases $\\mathcal{N C}3$ when training for 1 (left) through 10 (right) with weight decays $\\beta=0.0005$ (top) and $\\beta=0.1$ (bottom). However, we see no meaningful trend when scaling depth $L$ , suggesting that $\\mathcal{N C}3$ does not coalesce with language modeling training. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/7ad9e97e656a1e39b7ce961ee36655cf09a642f6c3dc3f3fd5e158fcf176a48e.jpg", "img_caption": ["N Uniformity Duality with Scale \u2013 UNC3 ", "Figure 25: Variation in classifier alignment decreases when scaling width $(d)$ in models trained for 1 (left) through 10 (right) with weight decays $\\beta=0.0005$ (top) and $\\beta=0.1$ (bottom). "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/bce33d4cd8d9657102a673806fcf6af5b279c97eb138a6d772be09b916b1579d.jpg", "img_caption": ["Figure 26: Variation in classifier alignment increases when scaling depth $(L)$ in models trained for 1 (left) through 10 (right) with weight decays $\\beta=0.0005$ (top) and $\\beta=0.1$ (bottom). This negative trend of UNC3 in more learnt models (right) suggests that the link of $(\\mathcal{U})\\mathcal{N}\\mathcal{C}3$ with scale and performance is still weak. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "O Correlations of $(\\mathcal{U})\\mathcal{N C}_{3}$ with Generalization Performance ", "text_level": 1, "page_idx": 32}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/b1f33bc16b3f657fcf9f4090a1b14a36af912e69bdb5f3bec6c31ead2aeb63ec.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Figure 27: Generalization (validation loss) correlated with average dot-product similarity (for interpretability purposes only) (left) and cosine similarity (classifier alignment, $\\mathcal{N C}_{3}$ ) (centre). We also computed the empirical measure from [5] (right). ", "page_idx": 32}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/7a216f6d46437f1763bf31233a44610a54568b01330e036d550fa7fd89f01e47.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Figure 28: Generalization (validation loss) correlated with variation in dot-product similarity (for interpretability purposes only) (left) and cosine similarity (uniform duality, ${\\bar{\\mathcal{U}}}{\\bar{\\mathcal{N}}}{\\mathcal{C}}_{3}\\mathrm{.}$ ) (right). ", "page_idx": 32}, {"type": "text", "text": "P Classifier Agreement \u2013 ${\\mathcal{N C}}_{4}$ ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "For computational reasons, we compute Equations 9, 10 using a simple decomposition: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{argmin}_{c\\in\\mathbb{V}}\\|h_{b}-\\pmb{\\mu}_{c}\\|_{2}=\\operatorname*{argmin}_{c\\in\\mathbb{V}}\\left(\\|h_{b}\\|^{2}+\\|\\pmb{\\mu}_{c}\\|^{2}-2h_{b}^{\\top}\\pmb{\\mu}_{c}\\right)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $b\\in[1,B]$ and $c\\in\\mathbb{V}$ with batch size $B$ and vocabulary $\\mathbb{V}$ . ", "page_idx": 33}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/be1a76775a4fe1ca7f1f5c04c0036f42b0ce75590dc037fd8e542882248ec729.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 29: Classifier agreement improves when scaling width $(d)$ in models trained for 1 (left) through 10 (right) with weight decays $\\beta=0.0005$ (top) and $\\beta=0.1$ (bottom). ", "page_idx": 33}, {"type": "image", "img_path": "G0LfcMiRkc/tmp/e44c2fd15e1c643184fb8701478cc5c8ac630a128a05a482c50250f67ae5569e.jpg", "img_caption": ["Figure 30: Classifier agreement improves when scaling depth $(L)$ in models trained for 1 (left) through 10 (right) with weight decays $\\beta=0.0005$ (top) and $\\beta=0.1$ (bottom). "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Q Examples for Interpretability ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "This section presents token-wise interpretability results found in top-layer embeddings from our most learned model (https://huggingface.co/rhubarbwu/TinyStories-12x1024_10L). ", "page_idx": 34}, {"type": "text", "text": "Table 4: Under TinyStories-12x1024_10L, these fifteen homonyms have much shorter mean embedding norms (i.e. closer to the global centre) than the average token. This is expected since homonyms typically present conflicts and interference. ", "page_idx": 34}, {"type": "table", "img_path": "G0LfcMiRkc/tmp/f3510e9f4442aa38d65d2ca7b1ab13b21ccca9cb66c66630d1408a657030b09d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "Table 5: Under TinyStories-12x1024_10L, the variability and interference of some English first names were far below those of the average token. This might be because names are distinct from one another and are not typically used in the same contexts as other words (aside from articles). The only names to have CDNV close to that of the average token are \u201cAnna\u201d and \u201cTim\u201d. Note that the positive interference of the last row (average token) is not a typo. ", "page_idx": 35}, {"type": "table", "img_path": "G0LfcMiRkc/tmp/0fe745611547bc87395dcf600af0c5ea37952898b0feeb0c7f5c9fbd5c97dc5e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: The claims of this paper listed in the abstract and introduction are simply the emergence of the ${\\mathcal{N C}}$ phenomena and its relationship with generalization in CLMs, which are the scope and results of the paper. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: We discuss the limitations of the experiments themselves (e.g. small models, limited sample size) as well as the possibility that our formulations could be improved in the future (there could be more suitable metrics). See Sections 6, 7. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: The equations and equivalent decompositions used are all clear and numbered in the paper. Their sources are all provided as references. We do not introduce new proofs or non-trivial expressions. See Section 3. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our methods for training the models, collecting embeddings, and analyzing metrics are clearly described in the paper, with supporting details in Appendices A, B, C. Links to code are provided in the Abstract and Section 3. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 37}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 38}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Links to the main code and auxiliary library are provided in the Abstract and Section 3, respectively. Main code repository includes training, collection, and analysis scripts, as well as a README briefly documenting how to use them. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: Appendices A, B, C, D provide the necessary information on model, data, training, and collection specifications to reproduce our experiments. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We trained multiple copies of the same models and observed patterns consistent with the ones presented in our paper. We also state the number of data points and permutation trials we performed for our significance test. See Figure 1 and Sections 4 and 5. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We provide details on the number of dataloader/preprocessing works, the specific GPU model used and the amount of RAM and training/collection time required. See Appendices A, B, C, D. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We have read the Code of Ethics and determined that our paper does not violate the guidelines. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: The paper includes a discussion on how ${\\mathcal{N C}}$ might be extended to measure fairness in LLMs. Naturally this may inform positive or adversarial manipulations of LLMs. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: We do not use pretrained models or scraped data. Our dataset is synthetic, public and transparent with no risks. The training procedure we use is the standard causal language modeling regime. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The pre-training dataset and its licence are provided in Appendix A and the authors of basic GPT Neo architectures are credited in Appendix B. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: Timestamped CSV flies of results that we reported in our paper are included in the repository linked in the Abstract. A link to a reference model is provided in Appendix Q. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: We do not work with human subjects. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: We do not work with human subjects. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 42}]