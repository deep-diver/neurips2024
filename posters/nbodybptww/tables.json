[{"figure_path": "nBOdYBptWW/tables/tables_6_1.jpg", "caption": "Table 1: Single-task comparison with existing methods on forecasting, classification, anomaly detection, and imputation tasks where each model is separately trained on each dataset. Full results are shown in Table 30, Table 31, Table 32, and Table 33.", "description": "This table presents a comparison of the UniTS model's performance against various baseline methods across four common time series tasks: forecasting, classification, anomaly detection, and imputation.  Each model was trained independently on individual datasets, providing a clear assessment of the model's single-task capabilities.  Detailed results for each task are referenced in subsequent tables (30-33).", "section": "5.1 Benchmarking UniTS on Single-Task Learning"}, {"figure_path": "nBOdYBptWW/tables/tables_7_1.jpg", "caption": "Table 1: Single-task comparison with existing methods on forecasting, classification, anomaly detection, and imputation tasks where each model is separately trained on each dataset. Full results are shown in Table 30, Table 31, Table 32, and Table 33.", "description": "This table presents a comparison of UNITS's performance against various existing models for four time series tasks: forecasting, classification, anomaly detection, and imputation.  Each model is trained separately on each dataset, which allows for a direct comparison of performance on individual tasks and datasets. The full results for each task are detailed in Tables 30, 31, 32, and 33.", "section": "5.1 Benchmarking UNITS on Single-Task Learning"}, {"figure_path": "nBOdYBptWW/tables/tables_8_1.jpg", "caption": "Table 3: Few-shot multi-task learning on 9 forecasting and 6 classification tasks on out-of-domain datasets. Ratio is the data ratio of the dataset used for training. Full results in Table 29.", "description": "This table presents the results of a few-shot learning experiment on 9 forecasting and 6 classification tasks using out-of-domain datasets.  Three different training data ratios (5%, 15%, and 20%) were tested for two models, iTransformer-FT and UniTS (both PMT and FT versions). The table shows the accuracy (Acc\u2191), Mean Squared Error (MSE\u2193), and Mean Absolute Error (MAE\u2193) for each model and data ratio.  The \"Best Count\" column indicates how many times each model achieved the best performance across all tasks for a given data ratio.  The \"Shared\" column shows whether the model uses shared weights (UniTS) or task-specific heads (iTransformer).  This table demonstrates UniTS's ability to perform well even with limited training data, surpassing iTransformer in most cases.", "section": "5.4 UNITS for Few-Shot Learning on New Datasets and Tasks"}, {"figure_path": "nBOdYBptWW/tables/tables_9_1.jpg", "caption": "Table 4: Few-shot multi-task learning for block-wise imputation on 6 datasets. Full results are in Table 28.", "description": "This table shows the results of few-shot learning experiments for block-wise imputation on six datasets.  The models were fine-tuned using 25% and 50% of the training data for each dataset. The table compares the performance of UNITS-PMT and UNITS-FT (fully fine-tuned) against several baselines: TimesNet-FT, PatchTST-FT, and iTrans-FT.  The metrics used are MSE for each dataset and the average MSE across all datasets.", "section": "5.4 UNITS for Few-Shot Learning on New Datasets and Tasks"}, {"figure_path": "nBOdYBptWW/tables/tables_9_2.jpg", "caption": "Table 5: Few-shot multi-task learning on anomaly detection tasks on 5 datasets.", "description": "This table presents the results of a few-shot learning experiment on anomaly detection tasks using five different datasets.  It compares the performance of several models, including UNITS-PMT (prompt-tuned) and UNITS-FT (fully fine-tuned), against various baselines. The performance is measured by the F1-score, and the results are broken down for each dataset to show the model's performance on individual datasets. This experiment aims to evaluate the models' ability to adapt to new tasks and datasets with limited training data.", "section": "5.4 UNITS for Few-Shot Learning on New Datasets and Tasks"}, {"figure_path": "nBOdYBptWW/tables/tables_18_1.jpg", "caption": "Table 6: Key features of a unified multi-task time series model include the capability to handle heterogeneous time series samples with different numbers of variables and time lengths. Additionally, it should support both generative and predictive time series tasks within the same model.", "description": "This table compares several existing time series models against three key desiderata for a unified multi-task time series model: the ability to handle heterogeneous time series data, the use of a universal task specification, and the use of one shared model.  A checkmark indicates that a model satisfies a desideratum, while an \"X\" indicates it does not.", "section": "A Extended Related Work"}, {"figure_path": "nBOdYBptWW/tables/tables_19_1.jpg", "caption": "Table 7: Multi-task datasets for classification and forecasting. Prediction length or number of classes are indicated in parenthesis for Forecast and Classification respectively.", "description": "This table lists 38 datasets used in the paper for multi-task learning experiments on forecasting and classification. Each dataset is characterized by its name, the number of training samples, sequence length, number of variables, the type of task (forecasting or classification), and the number of classes for classification tasks or the prediction length for forecasting tasks. The datasets cover diverse domains, including finance, healthcare, and human activity.", "section": "5 Experiments"}, {"figure_path": "nBOdYBptWW/tables/tables_19_2.jpg", "caption": "Table 7: Multi-task datasets for classification and forecasting. Prediction length or number of classes are indicated in parenthesis for Forecast and Classification respectively.", "description": "This table lists the 38 datasets used in the paper for multi-task learning experiments on forecasting and classification tasks.  For each dataset, it provides the name, the number of training samples, the length of each time series sequence, the number of variables, the type of task (forecasting or classification), and the number of classes for classification tasks.  The prediction length for forecasting tasks is also indicated in parentheses.  The datasets encompass various domains such as finance, human activity, healthcare, and electricity, showcasing the model's ability to handle heterogeneous data.", "section": "5 Experiments"}, {"figure_path": "nBOdYBptWW/tables/tables_20_1.jpg", "caption": "Table 9: Datasets for zero-shot forecasting. Prediction length is indicated in parenthesis. Note that only the first 500 variables are used for the Web Traffic and Temperature Rain datasets.", "description": "This table lists five datasets used for zero-shot forecasting experiments.  It shows the name of the dataset, the sequence length of the time series, the number of variables, the type of task (forecasting), and the class or category of the dataset (Electricity, Weather, Healthcare, Web, Weather).  The note indicates a limitation that only the first 500 variables are used in the Web Traffic and Temperature Rain datasets.", "section": "5 Experiments"}, {"figure_path": "nBOdYBptWW/tables/tables_20_2.jpg", "caption": "Table 10: Datasets for imputation tasks.", "description": "This table presents four datasets used for imputation tasks in the paper.  Each dataset is characterized by its name, sequence length, number of variables, the imputation task itself, the mask ratio (representing the percentage of missing values), and the class or domain the data belongs to. The mask ratio is varied (12.5%, 25%, 37.5%, 50%) to evaluate performance under different levels of missing data.", "section": "5 Experiments"}, {"figure_path": "nBOdYBptWW/tables/tables_20_3.jpg", "caption": "Table 11: Datasets for anomaly detection tasks.", "description": "This table lists the datasets used for anomaly detection experiments in the paper.  It provides the dataset name, the sequence length used in the multi-task and single-task settings, the number of variables, the type of task (anomaly detection), and the specific class or domain of each dataset (Machine, Spacecraft, Infrastructure).", "section": "5 Experiments"}, {"figure_path": "nBOdYBptWW/tables/tables_20_4.jpg", "caption": "Table 1: Single-task comparison with existing methods on forecasting, classification, anomaly detection, and imputation tasks where each model is separately trained on each dataset. Full results are shown in Table 30, Table 31, Table 32, and Table 33.", "description": "This table presents a comparison of the UNITS model's performance against various baseline methods for four common time series tasks: forecasting, classification, anomaly detection, and imputation.  Each model was trained separately on each individual dataset.  The table highlights UNITS's superior performance across all four tasks and provides references to tables containing the full results for each task.", "section": "5 Experiments"}, {"figure_path": "nBOdYBptWW/tables/tables_23_1.jpg", "caption": "Table 13: Baseline methods used for comparison in this paper.", "description": "This table lists various baseline methods used in the paper for comparison purposes across different time series tasks (forecasting, classification, imputation, and anomaly detection).  Each task has a range of methods listed, categorized by their underlying architecture type (e.g., LLM-reprogrammed, transformer-based, etc.).  This is crucial to allow readers to evaluate the performance of the proposed UNITS model against well-established and state-of-the-art techniques.", "section": "5 Experiments"}, {"figure_path": "nBOdYBptWW/tables/tables_24_1.jpg", "caption": "Table 14: Enhancing prompt learning capability of pre-trained UniTS through model scaling. Average performance on 20 forecasting tasks and 18 classification tasks are reported.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of scaling the UniTS model on its prompt learning capabilities.  It shows average performance across 20 forecasting tasks and 18 classification tasks for different model sizes (parameter counts). The goal is to demonstrate how increasing the model's size affects its ability to leverage prompt learning for improved performance.", "section": "5.2 Benchmarking UniTS for Multi-Task Learning"}, {"figure_path": "nBOdYBptWW/tables/tables_24_2.jpg", "caption": "Table 15: Ablation on the number of prompt tokens.", "description": "This table presents the results of an ablation study conducted to determine the optimal number of prompt tokens to use in the UniTS model.  The study varied the number of prompt tokens (0, 5, and 10) and measured the impact on the average classification accuracy (AccAvg\u2191) and the average Mean Squared Error (MSEAvg\u2193) and Mean Absolute Error (MAEAvg\u2193) for forecasting tasks. The results show that increasing the number of prompt tokens from 5 to 10 led to a slight improvement in performance, suggesting that 10 prompt tokens is the optimal number to use.", "section": "5 Experiments"}, {"figure_path": "nBOdYBptWW/tables/tables_24_3.jpg", "caption": "Table 16: Ablation on using shared/unshared prompt tokens in UNITS network.", "description": "This ablation study investigates the impact of using shared versus unshared prompt tokens within the UNITS network architecture.  The results compare the average accuracy, mean squared error (MSE), and mean absolute error (MAE) across all tasks when using shared prompt tokens (one set of tokens used for all tasks) against unshared prompt tokens (separate tokens for each task). The goal is to determine if the efficiency and performance benefits of having prompt tokens outweigh the negative effects of sharing tokens between disparate tasks.", "section": "Additional Results: Prompt Learning and Pre-training"}, {"figure_path": "nBOdYBptWW/tables/tables_25_1.jpg", "caption": "Table 17: Ablation on the pre-training scheme.", "description": "This table presents the ablation study on the unified masked reconstruction pre-training scheme of the UNITS model. It shows the average performance (accuracy for classification tasks, MSE and MAE for forecasting tasks) achieved under three different pre-training settings: 1) with both CLS token-based and prompt token-based reconstruction loss, 2) without CLS token-based reconstruction loss, and 3) without prompt token-based reconstruction loss. The results demonstrate the importance of both loss terms in achieving high performance in both classification and forecasting tasks.", "section": "4.3 UNITS Model Training"}, {"figure_path": "nBOdYBptWW/tables/tables_25_2.jpg", "caption": "Table 1: Single-task comparison with existing methods on forecasting, classification, anomaly detection, and imputation tasks where each model is separately trained on each dataset. Full results are shown in Table 30, Table 31, Table 32, and Table 33.", "description": "This table presents a comparison of the UniTS model's performance against various baseline methods for four distinct time series tasks: forecasting, classification, anomaly detection, and imputation.  It highlights UniTS's performance in single-task settings where each model is trained independently on a single dataset, demonstrating its competitive advantage across different tasks and datasets. The table shows key metrics such as MSE and MAE for forecasting and imputation, accuracy for classification, and F1-score for anomaly detection.  Complete results for each task are available in Tables 30, 31, 32, and 33.", "section": "5.1 Benchmarking UniTS on Single-Task Learning"}, {"figure_path": "nBOdYBptWW/tables/tables_25_3.jpg", "caption": "Table 19: Performance of UniTS under different pre-training data sizes, average performance on 20 forecasting and 18 classification are reported. Pre-training data size refers to the proportion of the total training set used.", "description": "This table shows the performance of the UniTS model under different pre-training data sizes.  It demonstrates how the model's accuracy in classification and mean squared error/mean absolute error in forecasting tasks improve with larger pre-training datasets. The table uses metrics like  Acc Avg\u2191 (Cls.), MSE Avg\u2193 (Fore.), and MAE Avg\u2193 (Fore.) to represent the classification accuracy and forecasting error metrics.", "section": "5 Experiments"}, {"figure_path": "nBOdYBptWW/tables/tables_26_1.jpg", "caption": "Table 2: Multi-task benchmarking across 20 forecasting tasks and 18 classification tasks. Both UNITS-SUP and UNITS-PMT process all 38 tasks using a single model. GPT4TS reprograms a pre-trained LLM (GPT-2) to time series and has dataset/task-specific modules, thus, it is excluded from best count evaluations to ensure fair comparisons. \"p\" is forecasting length. \"Class./Num.\" denotes the \u201cnumber of classes in each task/\"number of datasets\".", "description": "This table presents the results of a multi-task benchmarking experiment comparing the performance of the proposed model (UNITS) against other state-of-the-art methods on 38 datasets encompassing 20 forecasting and 18 classification tasks.  The table highlights UNITS's ability to handle multiple tasks using a single unified model, in contrast to baselines requiring task-specific components.  The results show UNITS's superior performance across a range of forecasting lengths and dataset complexities.", "section": "5 Experiments"}, {"figure_path": "nBOdYBptWW/tables/tables_26_2.jpg", "caption": "Table 21: Cross-domain pre-training evaluation on UniTS, average performance on 4 Weather or Traffic dataset domains are reported.", "description": "This table presents the results of a cross-domain pre-training experiment conducted on the UniTS model.  The experiment evaluated the model's performance on forecasting tasks (using MSE and MAE metrics) across four datasets from both weather and traffic domains. It compares the performance when the model is pre-trained exclusively on weather data, on traffic data, and when it is trained jointly on both. The goal is to assess how the choice of pre-training data affects the model's ability to generalize across different domains.", "section": "Additional Results: Ablation Studies of UNITS"}, {"figure_path": "nBOdYBptWW/tables/tables_26_3.jpg", "caption": "Table 22: Ablation on the MHSA in UNITS.", "description": "This ablation study evaluates the impact of removing either the Time MHSA or the Variable MHSA from the UNITS architecture.  The results show a decrease in performance (Accuracy, MSE, and MAE) when either component is removed, highlighting their importance in the model's effectiveness.", "section": "F Additional Results: Ablation Studies of UNITS"}, {"figure_path": "nBOdYBptWW/tables/tables_27_1.jpg", "caption": "Table 23: Ablation on the MLP layer in UNITS network.", "description": "This table presents the ablation study on replacing the Dynamic FFN layer in the UNITS network with a standard MLP layer or removing it entirely.  It compares the average accuracy (Acc_Avg), Mean Squared Error (MSE_Avg), and Mean Absolute Error (MAE_Avg) across various tasks for the three model configurations: the original UNITS-SUP model with Dynamic FFN, a model replacing Dynamic FFN with MLP, and a model without Dynamic FFN. The results demonstrate the importance of the Dynamic FFN for improved performance.", "section": "F Additional Results: Ablation Studies of UNITS"}, {"figure_path": "nBOdYBptWW/tables/tables_27_2.jpg", "caption": "Table 2: Multi-task benchmarking across 20 forecasting tasks and 18 classification tasks. Both UNITS-SUP and UNITS-PMT process all 38 tasks using a single model. GPT4TS reprograms a pre-trained LLM (GPT-2) to time series and has dataset/task-specific modules, thus, it is excluded from best count evaluations to ensure fair comparisons. \"p\" is forecasting length. \"Class./Num.\" denotes the \u201cnumber of classes in each task\"/\"number of datasets\".", "description": "This table presents the results of multi-task benchmarking experiments using the proposed UNITS model, comparing its performance with other state-of-the-art methods.  The table shows the performance across 20 forecasting tasks and 18 classification tasks, using two variations of the UNITS model (UNITS-SUP and UNITS-PMT).  It also includes a comparison to GPT4TS, a method that repurposes a pre-trained large language model for time series tasks. Note that GPT4TS is excluded from the \"best count\" comparison because it uses dataset/task-specific modules, making it not directly comparable to the unified UNITS model.", "section": "5 Experiments"}, {"figure_path": "nBOdYBptWW/tables/tables_27_3.jpg", "caption": "Table 25: Zero-shot multi-task learning on forecasting tasks on 5 out-of-domain data with new forecasting length and new number of variables. We set shared prompt tokens and GEN tokens for UNITS. One sample from each dataset is used following [81].", "description": "This table compares the performance of the proposed UniTS model against the LLMTime baseline on five different zero-shot forecasting tasks. Each task uses a different dataset with varying numbers of variables and forecasting lengths, none of which were seen during the model's training.  The results showcase UniTS's ability to generalize and perform well in zero-shot scenarios, as it achieves lower Mean Squared Error (MSE) and faster inference times than LLMTime on most tasks.", "section": "5.4 UNITS for Few-Shot Learning on New Datasets and Tasks"}, {"figure_path": "nBOdYBptWW/tables/tables_28_1.jpg", "caption": "Table 2: Multi-task benchmarking across 20 forecasting tasks and 18 classification tasks. Both UNITS-SUP and UNITS-PMT process all 38 tasks using a single model. GPT4TS reprograms a pre-trained LLM (GPT-2) to time series and has dataset/task-specific modules, thus, it is excluded from best count evaluations to ensure fair comparisons. \"p\" is forecasting length. \"Class./Num.\" denotes the \u201cnumber of classes in each task\"/\"number of datasets\".", "description": "This table presents the results of a multi-task benchmarking experiment comparing the performance of UNITS-SUP and UNITS-PMT models against other state-of-the-art models on 38 tasks which includes 20 forecasting tasks and 18 classification tasks. The table shows that both UNITS models effectively handle all 38 tasks using a single model architecture, and the performance of the UNITS models surpasses that of other existing methods on most tasks.  It highlights the performance gains achieved by using a unified model for both generative (forecasting) and predictive (classification) tasks.", "section": "5.2 Benchmarking UNITS for Multi-Task Learning"}, {"figure_path": "nBOdYBptWW/tables/tables_28_2.jpg", "caption": "Table 2: Multi-task benchmarking across 20 forecasting tasks and 18 classification tasks. Both UNITS-SUP and UNITS-PMT process all 38 tasks using a single model. GPT4TS reprograms a pre-trained LLM (GPT-2) to time series and has dataset/task-specific modules, thus, it is excluded from best count evaluations to ensure fair comparisons. \"p\" is forecasting length. \"Class./Num.\" denotes the \u201cnumber of classes in each task\"/\"number of datasets\".", "description": "This table presents the results of a multi-task benchmarking experiment comparing the performance of UNITS (both supervised and prompt-tuned versions) against several other state-of-the-art time series models across 20 forecasting and 18 classification tasks.  The results are summarized showing the number of times each model achieved the best performance (best count) for each task, and the overall average performance across all tasks. It highlights UNITS's ability to handle multiple diverse tasks with a single model, contrasting with GPT4TS that uses a pre-trained LLM and additional task-specific modules.", "section": "5.2 Benchmarking UNITS for Multi-Task Learning"}, {"figure_path": "nBOdYBptWW/tables/tables_29_1.jpg", "caption": "Table 1: Single-task comparison with existing methods on forecasting, classification, anomaly detection, and imputation tasks where each model is separately trained on each dataset. Full results are shown in Table 30, Table 31, Table 32, and Table 33.", "description": "This table presents a comparison of the UniTS model's performance against various baseline methods across four different time series tasks: forecasting, classification, anomaly detection, and imputation.  Each model is trained individually on each dataset.  The table provides a summary of results, with full details available in other tables referenced within the paper.", "section": "5 Experiments"}, {"figure_path": "nBOdYBptWW/tables/tables_29_2.jpg", "caption": "Table 1: Single-task comparison with existing methods on forecasting, classification, anomaly detection, and imputation tasks where each model is separately trained on each dataset. Full results are shown in Table 30, Table 31, Table 32, and Table 33.", "description": "This table presents a comparison of UNITS's single-task performance against various baseline models for four time series tasks: forecasting, classification, anomaly detection, and imputation.  Each model was trained and evaluated separately on each dataset. The full results for each task are detailed in Tables 30, 31, 32 and 33, respectively.", "section": "5.1 Benchmarking UNITS on Single-Task Learning"}, {"figure_path": "nBOdYBptWW/tables/tables_30_1.jpg", "caption": "Table 1: Single-task comparison with existing methods on forecasting, classification, anomaly detection, and imputation tasks where each model is separately trained on each dataset. Full results are shown in Table 30, Table 31, Table 32, and Table 33.", "description": "This table presents a comparison of the UniTS model's performance against various state-of-the-art time series models on four common tasks: forecasting, classification, anomaly detection, and imputation.  Each model is trained independently on each dataset, providing a comprehensive evaluation of UniTS's capabilities in a single-task setting.  Detailed results are further presented in Tables 30, 31, 32, and 33.", "section": "5.1 Benchmarking UNITS on Single-Task Learning"}, {"figure_path": "nBOdYBptWW/tables/tables_31_1.jpg", "caption": "Table 1: Single-task comparison with existing methods on forecasting, classification, anomaly detection, and imputation tasks where each model is separately trained on each dataset. Full results are shown in Table 30, Table 31, Table 32, and Table 33.", "description": "This table presents a comparison of the UniTS model's performance against various existing time series models across four common time series tasks (forecasting, classification, anomaly detection, and imputation).  Each model is trained individually for each dataset.  The table shows metrics (e.g., MSE, MAE, Accuracy, F1) indicating performance on a selection of datasets. Detailed results for each task are available in tables 30, 31, 32 and 33 in the paper.", "section": "5.1 Benchmarking UNITS on Single-Task Learning"}, {"figure_path": "nBOdYBptWW/tables/tables_31_2.jpg", "caption": "Table 1: Single-task comparison with existing methods on forecasting, classification, anomaly detection, and imputation tasks where each model is separately trained on each dataset. Full results are shown in Table 30, Table 31, Table 32, and Table 33.", "description": "This table presents a comparison of the UNITS model's performance against 12 forecasting models, 20 classification models, 18 anomaly detection models, and 16 imputation models on 38 datasets.  Each model was trained individually for each dataset (single-task setting).  The table shows results for each task type, highlighting the superior performance of UNITS across various metrics.", "section": "5.1 Benchmarking UNITS on Single-Task Learning"}, {"figure_path": "nBOdYBptWW/tables/tables_32_1.jpg", "caption": "Table 1: Single-task comparison with existing methods on forecasting, classification, anomaly detection, and imputation tasks where each model is separately trained on each dataset. Full results are shown in Table 30, Table 31, Table 32, and Table 33.", "description": "This table presents a comparison of the UNITS model's performance against 12 forecasting methods, 20 classification models, 18 anomaly detection models, and 16 imputation models across 38 datasets.  Each model is trained individually for each task and dataset (single-task setting).  The table highlights the superior performance of UNITS compared to state-of-the-art methods in each task.", "section": "5.1 Benchmarking UNITS on Single-Task Learning"}, {"figure_path": "nBOdYBptWW/tables/tables_33_1.jpg", "caption": "Table 1: Single-task comparison with existing methods on forecasting, classification, anomaly detection, and imputation tasks where each model is separately trained on each dataset. Full results are shown in Table 30, Table 31, Table 32, and Table 33.", "description": "This table presents a comparison of the UniTS model's performance against other state-of-the-art models for four distinct time series tasks: forecasting, classification, anomaly detection, and imputation.  Crucially, each model in this comparison was trained on a single task and dataset, ensuring a fair comparison with the UniTS model, which also was trained under the same single task condition.  The full results for each task can be found in other tables referenced in the caption.", "section": "5.1 Benchmarking UNITS on Single-Task Learning"}, {"figure_path": "nBOdYBptWW/tables/tables_33_2.jpg", "caption": "Table 35: Compare UNITS trained by multi-task learning with that trained by single-task learning under same hyper-parameters.", "description": "This table compares the performance of the UniTS model trained using multi-task learning versus single-task learning.  The comparison uses the same hyperparameters for both training methods.  The results show that multi-task learning leads to significantly better performance on both classification and forecasting tasks, highlighting the effectiveness of the multi-task approach.", "section": "Additional Results: Multi-task versus Single-task Learning"}]