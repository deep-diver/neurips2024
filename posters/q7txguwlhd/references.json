{"references": [{"fullname_first_author": "Stefano V. Albrecht", "paper_title": "Multi-Agent Reinforcement Learning: Foundations and Modern Approaches", "publication_date": "2024", "reason": "This book provides a comprehensive overview of the field of multi-agent reinforcement learning, which is the foundational context for the current paper's work on N-agent ad hoc teamwork."}, {"fullname_first_author": "Tabish Rashid", "paper_title": "QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning", "publication_date": "2018", "reason": "This paper introduces QMIX, a key algorithm in cooperative multi-agent reinforcement learning that the current paper builds upon and compares against in the NAHT setting."}, {"fullname_first_author": "Jakob Foerster", "paper_title": "Learning with opponent-learning awareness", "publication_date": "2018", "reason": "This work introduces opponent-learning awareness, a concept crucial to the NAHT setting's need for agents to account for the actions and adaptive strategies of teammates."}, {"fullname_first_author": "Ryan Lowe", "paper_title": "Multi-agent actor-critic for mixed cooperative-competitive environments", "publication_date": "2017", "reason": "This paper introduces MADDPG, a fundamental multi-agent reinforcement learning algorithm used in the experimental evaluation and comparison within the NAHT framework."}, {"fullname_first_author": "Georgios Papoudakis", "paper_title": "Agent modelling under partial observability for deep reinforcement learning", "publication_date": "2021", "reason": "This paper introduces a key concept of agent modelling under partial observability, which directly relates to the challenges faced by agents in the NAHT setting."}]}