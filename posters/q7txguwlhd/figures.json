[{"figure_path": "q7TxGUWlhD/figures/figures_1_1.jpg", "caption": "Figure 1: Left: CMARL algorithms assume full control over all M agents in a cooperative scenario. Center: AHT algorithms assume that only a single agent is controlled by the learning algorithm, while the other M \u2212 1 agents are uncontrolled and can have a diverse, unknown set of behaviors. Right: NAHT, the paradigm proposed by this paper, assumes that a potentially varying N agents are controlled by the learning algorithm, while the remaining M \u2212 N agents are uncontrolled.", "description": "This figure illustrates the differences between three multi-agent reinforcement learning paradigms: CMARL (Cooperative Multi-Agent Reinforcement Learning), AHT (Ad Hoc Teamwork), and NAHT (N-Agent Ad Hoc Teamwork). In CMARL, all agents are controlled by the learning algorithm. In AHT, only one agent is controlled, and the others have unknown behaviors.  NAHT generalizes both, with a variable number (N) of agents being controlled, while the rest (M-N) are uncontrolled.  This highlights the increasing complexity and realism moving from CMARL to AHT and finally NAHT.", "section": "Introduction"}, {"figure_path": "q7TxGUWlhD/figures/figures_3_1.jpg", "caption": "Figure 2: POAM trains a single policy network \u03c0\u03b8\u03c1, which characterizes the behavior of all controlled agents (green), while uncontrolled agents (yellow) are drawn from U. Data from both controlled and uncontrolled agents is used to train the value network, Ve, while the policy is trained on data from the controlled agents only. The policy and value function are both conditioned on a learned team embedding vector, et.", "description": "The figure illustrates the architecture of the Policy Optimization with Agent Modeling (POAM) algorithm.  POAM trains a single policy network (\u03c0\u03b8\u03c1) for all controlled agents, using a learned embedding vector (et) representing the observed behaviors of both controlled and uncontrolled teammates. This embedding vector is generated by an agent modeling network (encoder-decoder). A value network (Ve) is trained using data from both controlled and uncontrolled agents, providing a baseline for policy gradient updates. Importantly, the policy network is only trained using data from controlled agents, making it adaptable to varying numbers and types of uncontrolled teammates.", "section": "5 Policy Optimization with Agent Modeling (POAM)"}, {"figure_path": "q7TxGUWlhD/figures/figures_6_1.jpg", "caption": "Figure 3: POAM consistently improves over the baselines of IPPO-NAHT, POAM-AHT, and the best naive MARL baseline in all tasks, in either sample efficiency or asymptotic return.", "description": "This figure compares the learning curves of POAM and several baseline methods across five different multi-agent tasks.  The x-axis represents training timesteps, and the y-axis shows the mean test return.  POAM consistently outperforms all baselines either in terms of sample efficiency (how quickly it reaches a high return) or asymptotic return (the highest return achieved after sufficient training).  The baselines include a naive MARL approach (using various well-known multi-agent RL algorithms), IPPO-NAHT (Independent PPO in the NAHT setting), and POAM-AHT (POAM applied to the AHT setting). The results indicate POAM's superior ability to learn effective cooperative strategies in the presence of uncontrolled teammates.", "section": "6 Experiments and Results"}, {"figure_path": "q7TxGUWlhD/figures/figures_7_1.jpg", "caption": "Figure 3: POAM consistently improves over the baselines of IPPO-NAHT, POAM-AHT, and the best naive MARL baseline in all tasks, in either sample efficiency or asymptotic return.", "description": "This figure compares the performance of POAM against three baselines across five different tasks.  The x-axis represents training timesteps, and the y-axis represents mean test return.  POAM consistently outperforms the baselines (IPPO-NAHT, POAM-AHT, and the best naive MARL) in terms of either sample efficiency (reaching higher return sooner) or asymptotic return (achieving a higher final return).  Each plot shows the performance on a single task: mpe-pp, 5v6, 8v9, 3s5z, and 10v11.", "section": "6.2 Main Results"}, {"figure_path": "q7TxGUWlhD/figures/figures_8_1.jpg", "caption": "Figure 5: Evolution of a POAM agent's within-episode mean squared error (left) and within-episode probability of actions of modeled teammates (right), over the course of training on mpe-pp.", "description": "This figure shows the evolution of a POAM agent's performance in predicting the actions and observations of its teammates over the course of training. The left panel shows the mean squared error (MSE) of the agent's predictions for its teammates' observations, while the right panel shows the probability of the agent correctly predicting its teammates' actions. The results show that the agent's ability to predict its teammates' behavior improves significantly over time, suggesting that the agent is learning to model its teammates' behavior effectively. The results are presented for the mpe-pp task.", "section": "6.4 A Closer Look at POAM"}, {"figure_path": "q7TxGUWlhD/figures/figures_8_2.jpg", "caption": "Figure 3: POAM consistently improves over the baselines of IPPO-NAHT, POAM-AHT, and the best naive MARL baseline in all tasks, in either sample efficiency or asymptotic return.", "description": "The figure shows learning curves for four algorithms across four different tasks.  The x-axis represents training timesteps and the y-axis represents the mean test return.  POAM consistently outperforms other algorithms (IPPO-NAHT, POAM-AHT, and Naive MARL) across all four tasks, showing superior sample efficiency and/or asymptotic return.  The shaded areas represent 95% confidence intervals.", "section": "6.2 Main Results"}, {"figure_path": "q7TxGUWlhD/figures/figures_16_1.jpg", "caption": "Figure 7: A practical instantiation of the NAHT problem.", "description": "This figure illustrates the team sampling procedure used in the N-agent ad hoc teamwork (NAHT) problem.  It shows two sets of agents: controlled agents (C) and uncontrolled agents (U). A sampling process (X) selects a subset of agents (N) from the controlled set and the remaining agents (M-N) from the uncontrolled set to form a team.  The number of controlled agents (N) is sampled uniformly from 1 to M-1. The figure clearly shows the process of assembling a team of M agents for a task, where the algorithm controls N of them, while the rest are uncontrolled and have potentially unknown behaviors.", "section": "6.1 Experimental Design"}, {"figure_path": "q7TxGUWlhD/figures/figures_18_1.jpg", "caption": "Figure 8: Evolution of the within-episode mean squared error (left) and within-episode probability of actions of modeled teammates (right), over the course of training on mpe-pp.", "description": "This figure shows the evolution of the within-episode mean squared error (MSE) and the within-episode probability of actions predicted by POAM's agent modeling network for the multi-agent particle environment (MPE) predator-prey task. The left plot shows how the average MSE decreases over the course of training, while the right plot shows how the average probability of actions increases.  The plots show that the average MSE decreases within the episode and the average probability increases as the agent modeling network becomes more confident in its predictions. These results suggest that POAM is able to learn accurate teammate models and adapt to their behavior.", "section": "A.4 Supplemental Figures"}, {"figure_path": "q7TxGUWlhD/figures/figures_19_1.jpg", "caption": "Figure 3: POAM consistently improves over the baselines of IPPO-NAHT, POAM-AHT, and the best naive MARL baseline in all tasks, in either sample efficiency or asymptotic return.", "description": "This figure shows the learning curves for the proposed POAM algorithm and its baseline algorithms (IPPO-NAHT, POAM-AHT, and the best naive MARL baseline) on different tasks (mpe-pp, 5v6, 8v9, 3s5z, 10v11).  The x-axis represents the number of training timesteps, and the y-axis represents the mean test return across five trials with 95% confidence intervals shown as shaded regions. The figure demonstrates that POAM generally outperforms the baselines in terms of either sample efficiency or asymptotic return, indicating its effectiveness in handling uncontrolled teammates in various cooperative scenarios.", "section": "6.2 Main Results"}, {"figure_path": "q7TxGUWlhD/figures/figures_20_1.jpg", "caption": "Figure 3: POAM consistently improves over the baselines of IPPO-NAHT, POAM-AHT, and the best naive MARL baseline in all tasks, in either sample efficiency or asymptotic return.", "description": "This figure displays the learning curves for POAM, IPPO-NAHT, POAM-AHT and the best naive MARL baseline across five different tasks (mpe-pp, 5v6, 8v9, 3s5z, 10v11). The results show that POAM outperforms baselines across all tasks, either in terms of sample efficiency or asymptotic return.  The x-axis represents training timesteps, and the y-axis represents the mean test return.", "section": "6.2 Main Results"}, {"figure_path": "q7TxGUWlhD/figures/figures_20_2.jpg", "caption": "Figure 3: POAM consistently improves over the baselines of IPPO-NAHT, POAM-AHT, and the best naive MARL baseline in all tasks, in either sample efficiency or asymptotic return.", "description": "The figure shows the learning curves of different algorithms (POAM, IPPO-NAHT, POAM-AHT, and the best naive MARL baseline) on four different tasks (mpe-pp, 5v6, 8v9, 3s5z).  The x-axis represents training timesteps, and the y-axis represents the mean test return.  Error bars representing the 95% confidence interval are included.  The results demonstrate that POAM generally outperforms the baselines in terms of sample efficiency and/or asymptotic return, indicating its effectiveness in learning cooperative multi-agent behaviors in the presence of unknown teammates.", "section": "6.2 Main Results"}, {"figure_path": "q7TxGUWlhD/figures/figures_20_3.jpg", "caption": "Figure 3: POAM consistently improves over the baselines of IPPO-NAHT, POAM-AHT, and the best naive MARL baseline in all tasks, in either sample efficiency or asymptotic return.", "description": "This figure shows the learning curves for the different methods (POAM, IPPO-NAHT, POAM-AHT, and Naive MARL) on four different tasks (mpe-pp, 5v6, 8v9, 3s5z).  It demonstrates that POAM generally outperforms the baseline methods in terms of either sample efficiency or asymptotic return. The shaded areas represent 95% confidence intervals.  The x-axis represents training timesteps (in millions), and the y-axis represents the average test return.", "section": "6.2 Main Results"}, {"figure_path": "q7TxGUWlhD/figures/figures_20_4.jpg", "caption": "Figure 3: POAM consistently improves over the baselines of IPPO-NAHT, POAM-AHT, and the best naive MARL baseline in all tasks, in either sample efficiency or asymptotic return.", "description": "This figure displays the learning curves for the mpe-pp, 5v6, 8v9, 10v11, and 3s5z tasks.  It compares the performance of the proposed POAM algorithm against three baselines: IPPO-NAHT (Independent PPO in the NAHT setting), POAM-AHT (POAM in the AHT setting), and the best-performing naive MARL algorithm.  The x-axis represents training timesteps, and the y-axis represents the mean test return.  The figure shows that POAM generally outperforms the baselines in terms of both sample efficiency and asymptotic return, demonstrating its effectiveness in handling uncontrolled teammates.", "section": "6 Experiments and Results"}, {"figure_path": "q7TxGUWlhD/figures/figures_20_5.jpg", "caption": "Figure 3: POAM consistently improves over the baselines of IPPO-NAHT, POAM-AHT, and the best naive MARL baseline in all tasks, in either sample efficiency or asymptotic return.", "description": "The figure shows the learning curves for four different algorithms across four different tasks (mpe-pp, 5v6, 8v9, and 3s5z). Each curve represents the average test return over five trials, with shaded regions representing the 95% confidence intervals.  The algorithms compared are POAM (the proposed algorithm), IPPO-NAHT (a baseline using IPPO adapted for the NAHT problem), POAM-AHT (using the POAM algorithm with only a single agent to represent the controlled agents), and the best performing baseline among several naive MARL algorithms. The figure demonstrates that POAM outperforms the other three methods in terms of sample efficiency and asymptotic return across these experiments.", "section": "6 Experiments and Results"}, {"figure_path": "q7TxGUWlhD/figures/figures_21_1.jpg", "caption": "Figure 3: POAM consistently improves over the baselines of IPPO-NAHT, POAM-AHT, and the best naive MARL baseline in all tasks, in either sample efficiency or asymptotic return.", "description": "The figure shows the learning curves of POAM and IPPO-NAHT across four different tasks (mpe-pp, 5v6, 8v9, 3s5z).  It also displays the asymptotic test returns achieved by the best naive MARL baseline and POAM-AHT across these tasks.  The results demonstrate that POAM generally outperforms all the baseline methods in either sample efficiency or asymptotic return.", "section": "6.2 Main Results"}, {"figure_path": "q7TxGUWlhD/figures/figures_21_2.jpg", "caption": "Figure 3: POAM consistently improves over the baselines of IPPO-NAHT, POAM-AHT, and the best naive MARL baseline in all tasks, in either sample efficiency or asymptotic return.", "description": "The figure shows the learning curves of POAM and IPPO-NAHT, and the test returns achieved by the best naive MARL baseline and POAM-AHT, on various tasks (mpe-pp, 5v6, 8v9, 10v11, 3s5z).  It visually demonstrates that POAM outperforms the baselines in terms of sample efficiency and/or asymptotic return across multiple multi-agent cooperative tasks. The y-axis represents the mean test return, and the x-axis represents the number of training timesteps.  Shaded regions indicate 95% confidence intervals.", "section": "6.2 Main Results"}, {"figure_path": "q7TxGUWlhD/figures/figures_22_1.jpg", "caption": "Figure 5: Evolution of a POAM agent's within-episode mean squared error (left) and within-episode probability of actions of modeled teammates (right), over the course of training on mpe-pp.", "description": "This figure shows two plots that visualize how the performance of the agent modeling module within POAM changes over time during training. The left plot displays the mean squared error (MSE) between the actual observations of the teammates and the predictions made by POAM's agent modeling network. The right plot shows the average probability that the actions predicted by POAM's model for the teammates actually match the actions that the teammates choose. The plots illustrate that, as the training progresses, the MSE decreases, and the probability of accurate predictions increases, indicating that POAM's agent modeling module improves its ability to predict the behavior of the teammates over time.", "section": "6.4 A Closer Look at POAM"}, {"figure_path": "q7TxGUWlhD/figures/figures_22_2.jpg", "caption": "Figure 14: Comparing the performance of POAM versus POAM-AHT, as the number of controlled agents varies. POAM and POAM-AHT agents are evaluated with the following set of uncontrolled agents: QMIX, VDN, IQL, MAPPO, IPPO.", "description": "This figure shows how the performance of POAM and POAM-AHT changes as the number of controlled agents varies.  The x-axis represents the number of controlled agents, and the y-axis represents the mean test return. The different lines represent different tasks (mpe-pp, 5v6, 8v9, 10v11, 3s5z). The horizontal dashed lines show the self-play performance of each algorithm (i.e., when all agents are controlled).  POAM consistently outperforms POAM-AHT when more than one agent is controlled.  The performance of POAM-AHT decreases as the number of controlled agents increases, because its evaluation setting becomes further from the training setting (N=1).", "section": "6.2 Main Results"}]