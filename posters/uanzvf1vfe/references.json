{"references": [{"fullname_first_author": "Y. Arjevani", "paper_title": "Lower bounds for non-convex stochastic optimization", "publication_date": "2023-00-00", "reason": "This paper provides crucial lower bounds for non-convex stochastic optimization, which helps to contextualize the efficiency of the proposed methods."}, {"fullname_first_author": "J. Bernstein", "paper_title": "signSGD: Compressed optimization for non-convex problems", "publication_date": "2018-00-00", "reason": "This is a foundational paper on signSGD, which is the core algorithm being improved upon in this work."}, {"fullname_first_author": "C. Fang", "paper_title": "SPIDER: Near-optimal non-convex optimization via stochastic path-integrated differential estimator", "publication_date": "2018-00-00", "reason": "This paper introduces the SPIDER algorithm, a key variance reduction technique that the current work builds upon and adapts to the sign-based setting."}, {"fullname_first_author": "S. Ghadimi", "paper_title": "Stochastic first- and zeroth-order methods for nonconvex stochastic programming", "publication_date": "2013-00-00", "reason": "This paper establishes fundamental convergence rates for stochastic gradient descent (SGD) in non-convex settings, providing a baseline against which to compare improvements."}, {"fullname_first_author": "L. Zhang", "paper_title": "Linear convergence with condition number independent access of full gradients", "publication_date": "2013-00-00", "reason": "This paper introduces the SVRG algorithm, a foundational variance reduction technique that is relevant to the finite-sum problems the current paper addresses."}]}