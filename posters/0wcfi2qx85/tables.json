[{"figure_path": "0WCFI2Qx85/tables/tables_5_1.jpg", "caption": "Table 1: Pilot experiments on cross architecture distillation with ScaleKD and FD. si denotes the distillation is conducted on stage-i. To clearly show the performance gain, experiments in this table are conducted without Lkd.", "description": "This table presents the results of pilot experiments comparing ScaleKD and traditional Feature Distillation (FD) methods on cross-architecture knowledge distillation tasks.  It shows the top-1 accuracy achieved on ResNet-50 and Mixer-S student models when using Swin-S as the teacher model.  The results demonstrate that ScaleKD significantly outperforms FD, highlighting its effectiveness in bridging architectural differences for knowledge transfer.", "section": "3.1 Pilot Experiments under Basic Settings"}, {"figure_path": "0WCFI2Qx85/tables/tables_5_2.jpg", "caption": "Table 2: Pilot experiments on scaling up the teacher size. The advanced training strategy uses more sophisticated data augmentation and optimizer, and longer training epochs, as shown in Table 10.", "description": "This table presents the results of pilot experiments designed to evaluate the impact of scaling up the size of the teacher model on the performance of the ScaleKD method.  Two training strategies are compared: a traditional strategy and an advanced strategy (details in Table 10). The table shows the baseline performance, the teacher-student model parameter ratio, and the resulting top-1 accuracy gains achieved by ScaleKD with both strategies. The results demonstrate the scalability of the ScaleKD approach, showing improved gains as the teacher model size is increased.", "section": "3.1 Pilot Experiments under Basic Settings"}, {"figure_path": "0WCFI2Qx85/tables/tables_5_3.jpg", "caption": "Table 3: Main results of ScaleKD on 11 teacher-student network pairs. \u2020 denotes the model pre-trained on IN-22K [45] and \u2021 denotes the model pre-trained by EVA [41], which has the learned knowledge of LAION-2B [48].", "description": "This table presents the main results of the ScaleKD method. It shows the top-1 accuracy and the improvement achieved by ScaleKD compared to individually trained counterparts for 11 different teacher-student network pairs.  The teacher models are large vision transformers (ViTs), while the student models represent a variety of architectures including CNNs, MLPs, and other ViTs. The table also indicates which models were pre-trained on larger datasets (IN-22K or LAION-2B).", "section": "3 Main Experiments"}, {"figure_path": "0WCFI2Qx85/tables/tables_6_1.jpg", "caption": "Table 5: Transfer learning results (%) on MS-COCO.", "description": "This table presents the transfer learning results on the MS-COCO dataset.  It shows the performance of different models (ResNet-50, Swin-T, and ViT-B/16) pre-trained using various methods (baselines and the proposed ScaleKD) on three downstream tasks: classification, object detection, and instance segmentation. The results demonstrate the generalization capability of models trained by ScaleKD.", "section": "3.4 Transferring to Downstream Tasks"}, {"figure_path": "0WCFI2Qx85/tables/tables_6_2.jpg", "caption": "Table 5: Transfer learning results (%) on MS-COCO.", "description": "This table presents the transfer learning results on the MS-COCO dataset.  It compares the performance of baseline models (ResNet-50 and Swin-T) with those trained using the ScaleKD method. The results are shown for classification (IN-1K), object detection, and instance segmentation tasks, indicating the improvements achieved by ScaleKD in downstream tasks.  The metrics shown include Top-1 accuracy for classification and Average Precision (AP) and its variants for object detection and instance segmentation.", "section": "3.4 Transferring to Downstream Tasks"}, {"figure_path": "0WCFI2Qx85/tables/tables_7_1.jpg", "caption": "Table 7: Performance comparison with recent top-performing KD methods. Following the settings of them, the students are trained under the advanced training strategy. Best results are bolded.", "description": "This table compares the performance of ScaleKD against other state-of-the-art knowledge distillation (KD) methods.  The comparison uses ResNet-50 and Swin-T as student models, trained with the advanced training strategy. The table highlights ScaleKD's superior performance, even with fewer training epochs compared to other methods, showcasing its efficiency in transferring knowledge from a strong pre-trained Vision Transformer (ViT) teacher.", "section": "3.5 Comparison with Recent Top-Performing KD Methods"}, {"figure_path": "0WCFI2Qx85/tables/tables_7_2.jpg", "caption": "Table 3: Main results of ScaleKD on 11 teacher-student network pairs. \u2020 denotes the model pre-trained on IN-22K [45] and \u2021 denotes the model pre-trained by EVA [41], which has the learned knowledge of LAION-2B [48].", "description": "This table presents the main results of the ScaleKD knowledge distillation method. It shows the top-1 accuracy and the improvement achieved by ScaleKD on ImageNet-1K for 11 different teacher-student network pairs.  The table includes various architectures (CNN, MLP, and ViT) and model sizes.  It highlights the significant performance gains obtained using ScaleKD, especially when compared to individually training the student models from scratch.", "section": "3 Main Experiments"}, {"figure_path": "0WCFI2Qx85/tables/tables_7_3.jpg", "caption": "Table 5: Transfer learning results (%) on MS-COCO.", "description": "This table presents the transfer learning results on the MS-COCO dataset.  It compares the performance of baseline models (ResNet-50 and Swin-T) against models trained using the ScaleKD method.  The results are shown for image classification (top-1 accuracy), object detection (average precision - AP), and instance segmentation (average precision - AP). The numbers in parentheses indicate the improvement in performance achieved by using ScaleKD.", "section": "3.4 Transferring to Downstream Tasks"}, {"figure_path": "0WCFI2Qx85/tables/tables_8_1.jpg", "caption": "Table 9: Ablation studies. Experiments in (b)-(d) are performed on Swin-S\u2192ResNet-50. As DFM and TPP are designed based on CAP, CAP is added by default when choosing DFM and TPP in (a). Because of this, we treat CAP as another baseline method, when analyzing DFM and TPP in (c)-(d).", "description": "This table presents the ablation study results for the three core components of ScaleKD: CAP, DFM, and TPP.  It shows the impact of each component individually and in combination on the model's performance.  The experiment was conducted on the Swin-S to ResNet-50 teacher-student pair. The table systematically evaluates different combinations to reveal the individual contributions and the synergistic effects of the three core components.", "section": "4 Ablation Study"}, {"figure_path": "0WCFI2Qx85/tables/tables_8_2.jpg", "caption": "Table 9: Ablation studies. Experiments in (b)-(d) are performed on Swin-S\u2192ResNet-50. As DFM and TPP are designed based on CAP, CAP is added by default when choosing DFM and TPP in (a). Because of this, we treat CAP as another baseline method, when analyzing DFM and TPP in (c)-(d).", "description": "This table presents the ablation study results for the three core components of ScaleKD: Cross Attention Projector (CAP), Dual-view Feature Mimicking (DFM), and Teacher Parameter Perception (TPP).  It shows the impact of each component individually and in combination on the overall performance, demonstrating their complementary nature.  The experiments are conducted using a Swin-S teacher and ResNet-50 student model.", "section": "4 Ablation Study"}, {"figure_path": "0WCFI2Qx85/tables/tables_15_1.jpg", "caption": "Table 10: Detailed settings of traditional training strategy and advanced training strategy on IN-1K.", "description": "This table details the configurations used for training the models on the ImageNet-1K dataset.  It compares two training strategies: a traditional one, commonly used in previous knowledge distillation research and an advanced strategy used for more recently developed CNNs, MLPs, and Vision Transformers (ViTs). The table lists various hyperparameters such as batch size, learning rate, learning rate schedule, optimizer, weight decay, and data augmentation techniques for both strategies.", "section": "B Experimental Setups"}, {"figure_path": "0WCFI2Qx85/tables/tables_15_2.jpg", "caption": "Table 11: Detailed settings of transfer learning strategies on MS-COCO and ADE20K.", "description": "This table details the configurations used for transfer learning experiments on the MS-COCO and ADE20K datasets.  It specifies settings for both datasets, including weight initialization, batch size, learning rate and its decay schedule, optimizer, hyper-parameters, weight decay, training epochs (or iterations), crop size, and drop path rate.  These settings are crucial for replicating and understanding the results of the transfer learning experiments reported in the paper.  The table showcases two different experimental set ups, one for the MS-COCO dataset and another one for the ADE20K dataset.  The differences highlight the adaptation of hyper-parameters based on dataset requirements and the different tasks involved (object detection/instance segmentation vs. semantic segmentation).", "section": "3.4 Transferring to Downstream Tasks"}, {"figure_path": "0WCFI2Qx85/tables/tables_16_1.jpg", "caption": "Table 4: Experiments on exploring scalable properties from the teacher's pre-training data. We use the best reported models with different pre-training methods as our baselines to examine whether our student model has learned the teacher's pre-training knowledge. We use Swin-L as the teacher for the first two experiments and BEiT-L/14 as the teacher for the rest two experiments. \u21d2 denotes transfer learning and * denotes the model is trained and tested with 384\u00d7 384 sample resolution.", "description": "This table presents the results of experiments designed to investigate whether ScaleKD enables student models to inherit the scalability properties of their teacher models, specifically focusing on the impact of the teacher's pre-training data.  The table compares the performance of student models trained using ScaleKD against baselines representing various pre-training methods (supervised, self-supervised, cross-modal, and hybrid). The results show that ScaleKD consistently achieves better performance than the baselines, even when the student model only sees data from ImageNet-1k, demonstrating the transfer of pre-training knowledge.", "section": "3.3 The Scalable Properties from Teacher's Pre-training Data"}, {"figure_path": "0WCFI2Qx85/tables/tables_16_2.jpg", "caption": "Table 3: Main results of ScaleKD on 11 teacher-student network pairs. \u2020 denotes the model pre-trained on IN-22K [45] and \u2021 denotes the model pre-trained by EVA [41], which has the learned knowledge of LAION-2B [48].", "description": "This table presents the main results of the ScaleKD method on ImageNet-1K. It compares the top-1 accuracy of 11 different student models trained using ScaleKD with their respective baselines (individually trained models). The student models represent diverse architectures, including MobileNet-V1, ResNet-50, ConvNeXt-T, Mixer-S/16, Mixer-B/16, ViT-S/16, Swin-T, and ViT-B/16.  The teacher models used are Swin-L and BEIT-L/14.  The table highlights the improvement in top-1 accuracy achieved by ScaleKD for each student model compared to its individually trained counterpart. It also shows the model size (parameters and FLOPs) for both teacher and student models.  Additionally, the table notes which teacher models were pre-trained on larger datasets (IN-22K or LAION-2B).", "section": "3 Main Experiments"}, {"figure_path": "0WCFI2Qx85/tables/tables_17_1.jpg", "caption": "Table 3: Main results of ScaleKD on 11 teacher-student network pairs. \u2020 denotes the model pre-trained on IN-22K [45] and \u2021 denotes the model pre-trained by EVA [41], which has the learned knowledge of LAION-2B [48].", "description": "This table presents the main results of the ScaleKD method on eleven different teacher-student network pairs.  It shows the top-1 accuracy achieved by each student model when trained using ScaleKD, along with the parameters and FLOPs for both the teacher and student models. Some teacher models were pre-trained on larger datasets (indicated by \u2020 and \u2021).  The table demonstrates the effectiveness of ScaleKD across various architectures and pre-training scenarios.", "section": "3 Main Experiments"}, {"figure_path": "0WCFI2Qx85/tables/tables_18_1.jpg", "caption": "Table 15: Experiments on the training efficiency of ScaleKD. The student model in all experiments is ResNet-50. In (a), we compare ScaleKD with traditional FD using three teachers with different model scales. In (b), we conduct the experiments based on Swin-S\u2192ResNet-50 teacher-student network pair to illustrate the training costs (memory and time) introduced by each component of ScaleKD. Experiments are conducted on 8 \u00d7 NVIDIA Tesla-V100 GPUs.", "description": "This table presents an ablation study on the training efficiency of ScaleKD, comparing it with traditional feature distillation (FD). It shows the GPU memory usage and training time in days for different teacher models (Swin-S, Swin-B, and Swin-L) using both FD and ScaleKD.  A further breakdown examines the impact of each ScaleKD component (CAP, DFM, TPP) on training efficiency.", "section": "D More Ablation Studies"}, {"figure_path": "0WCFI2Qx85/tables/tables_18_2.jpg", "caption": "Table 15: Ablation study on training efficiency of ScaleKD. The student model in all experiments is ResNet-50. In (a), we compare ScaleKD with traditional FD using three teachers with different model scales. In (b), we conduct the experiments based on Swin-S\u2192ResNet-50 teacher-student network pair to illustrate the training costs (memory and time) introduced by each component of ScaleKD. Experiments are conducted on 8 \u00d7 NVIDIA Tesla-V100 GPUs.", "description": "This table presents an ablation study on the training efficiency of the ScaleKD method. It compares the training costs (GPU memory and time) of ScaleKD against traditional Feature Distillation (FD) using three different teacher models with varying sizes. It also breaks down the training costs of ScaleKD into its individual components (CAP, DFM, TPP, KD) to assess their contribution to the overall cost.", "section": "4.1 Tightly Coupled Design Properties of Three Core Components"}, {"figure_path": "0WCFI2Qx85/tables/tables_18_3.jpg", "caption": "Table 16: Ablation study on pre-training and distillation.", "description": "This ablation study compares the performance of training a ViT-S/16 model from scratch on ImageNet-1K, with and without knowledge distillation (KD), and compares those results with pre-training on ImageNet-22K, with and without KD.  It demonstrates that ScaleKD significantly improves performance compared to other methods. ", "section": "4.1 Tightly Coupled Design Properties of Three Core Components"}, {"figure_path": "0WCFI2Qx85/tables/tables_18_4.jpg", "caption": "Table 9: Ablation studies. Experiments in (b)-(d) are performed on Swin-S\u2192ResNet-50. As DFM and TPP are designed based on CAP, CAP is added by default when choosing DFM and TPP in (a). Because of this, we treat CAP as another baseline method, when analyzing DFM and TPP in (c)-(d).", "description": "This table presents ablation study results evaluating the contribution of each component (CAP, DFM, TPP) of the ScaleKD method.  Experiments were conducted on the Swin-S to ResNet-50 teacher-student pair.  The table shows the impact of each component on the final Top-1 accuracy.  CAP is a baseline, with DFM and TPP progressively added to assess their individual and combined contributions.", "section": "4 Ablation Study"}]