[{"figure_path": "0WCFI2Qx85/figures/figures_2_1.jpg", "caption": "Figure 1: Overview of three core components in our ScaleKD, which are (a) cross attention projector, (b) dual-view feature mimicking, and (c) teacher parameter perception. Note that the teacher model is frozen in the distillation process and there is no modification to the student's model at inference.", "description": "This figure illustrates the three core components of the ScaleKD method: Cross Attention Projector (CAP), Dual-view Feature Mimicking (DFM), and Teacher Parameter Perception (TPP).  CAP aligns the feature computing paradigms of the teacher and student models by using positional embeddings and a cross-attention mechanism. DFM mimics the teacher's features in both the original and frequency domains to address model scale differences. TPP aligns the parameter spaces of the teacher and student by creating a proxy feature processing path.  Importantly, the teacher model remains frozen during the distillation process, and no modifications are made to the student model during inference.", "section": "2 Method"}, {"figure_path": "0WCFI2Qx85/figures/figures_3_1.jpg", "caption": "Figure 2: Feature distribution of BEIT-L/14 [41] in the frequency domain, where the direct component response is dominant. Details on drawing this figure are shown in Figure 5.", "description": "This figure shows the frequency distribution of features from a BEIT-L/14 model in the frequency domain.  The dominant response is concentrated in the direct component (zero frequency), indicating an imbalance in the feature distribution.  More detailed explanation of how this visualization was generated is found in Figure 5.", "section": "Problem Analysis"}, {"figure_path": "0WCFI2Qx85/figures/figures_8_1.jpg", "caption": "Figure 6: Feature distance distributions of alternative components for the last stage features between teacher and student on IN-1K. We obtain 64,000 feature pairs on Swin-L\u2192ResNet-50 network pair from 64,000 samples. After calculating the distance between teacher and student, we project the high-dimension distances into a two-dimension space for illustration. Finally, we randomly select 6,400 data points for 8 times to draw the scatters. Blue points denote the distances without DFM, while orange points denote the distances with DFM.", "description": "This figure shows the feature distance distributions between teacher and student models with and without DFM applied.  The high-dimensional feature distances are projected into a 2D space for visualization. The results demonstrate that DFM effectively reduces the distance between the teacher and student features, particularly for the alternative components, which are often neglected in traditional knowledge distillation methods.", "section": "More Visualization Results"}, {"figure_path": "0WCFI2Qx85/figures/figures_18_1.jpg", "caption": "Figure 4: Ablation study on the hyper-parameter \u03b2.", "description": "This ablation study shows the impact of the hyperparameter \u03b2 on the performance of ScaleKD.  \u03b2 controls the balance between two feature mimicking paths in DFM. The x-axis represents the value of \u03b2, ranging from 0.0 (only using alternative features) to 1.0 (only using direct component). The y-axis shows the top-1 accuracy on the ResNet-50 student model.  The results indicate that a balance between both paths (around \u03b2 = 0.6) leads to optimal performance, outperforming using either path alone.", "section": "4.2 Role of Each of Three Core Components"}, {"figure_path": "0WCFI2Qx85/figures/figures_19_1.jpg", "caption": "Figure 5: More illustrative feature distributions of large pre-trained ViTs in the frequency domain. We first collect the output feature maps of 1600 samples from IN-1K, then conduct DCT on each channel, and finally take the average value across these samples after converting all responses into absolute values.", "description": "This figure shows the frequency distributions of feature maps from three different large pre-trained vision transformers (ViTs): ViT-L/14, Swin-L, and BEIT-L/14.  The data used is a subset of ImageNet-1K (IN-1K). Each subfigure represents a 3D plot showing frequency distribution on a per-channel basis.  The process involves extracting feature maps, applying a Discrete Cosine Transform (DCT) to each channel, averaging across samples, and displaying the magnitude of the frequency response. This visualization helps to illustrate the differences in feature distributions between these pre-trained ViTs, highlighting the relative importance of different frequency components in their feature representations.", "section": "More Visualization Results"}, {"figure_path": "0WCFI2Qx85/figures/figures_19_2.jpg", "caption": "Figure 1: Overview of three core components in our ScaleKD, which are (a) cross attention projector, (b) dual-view feature mimicking, and (c) teacher parameter perception. Note that the teacher model is frozen in the distillation process and there is no modification to the student's model at inference.", "description": "This figure illustrates the three main components of the ScaleKD method: the Cross Attention Projector (CAP), Dual-view Feature Mimicking (DFM), and Teacher Parameter Perception (TPP).  CAP aligns the feature computing paradigms of the teacher and student models. DFM mimics features from two perspectives: the original space and a frequency-filtered space to address model scale and knowledge density differences. TPP bridges the parameter spaces of teacher and student by using feature mimicking in a proxy path.  Importantly, the teacher model remains frozen during the distillation process, and no modifications are made to the student model for inference.", "section": "2 Method"}]