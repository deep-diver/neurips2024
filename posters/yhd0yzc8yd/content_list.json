[{"type": "text", "text": "Linearly Decomposing and Recomposing Vision Transformers for Diverse-Scale Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shuxia $\\mathbf{Lin}^{1,2^{*}}$ , Miaosen Zhang1,2\\*, Ruiming Chen1,2\\*, Qiufeng Wang1,2, $\\mathbf{X}\\mathbf{u}\\:\\mathbf{Y}\\mathbf{a}\\mathbf{n}\\mathbf{g}^{1,2^{\\dagger}}$ , and Xin Geng1,2\u2020 ", "page_idx": 0}, {"type": "text", "text": "1School of Computer Science and Engineering, Southeast University, Nanjing 210096, China 2Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications, Southeast University, Ministry of Education, China {shuxialin, 230228501, 220232251, qfwang, xuyang_palm, xgeng}@seu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vision Transformers (ViTs) are widely used in a variety of applications, while they usually have a fixed architecture that may not match the varying computational resources of different deployment environments. Thus, it is necessary to adapt ViT architectures to devices with diverse computational overheads to achieve an accuracy-efficient trade-off. This concept is consistent with the motivation behind Learngene. To achieve this, inspired by polynomial decomposition in calculus, where a function can be approximated by linearly combining several basic components, we propose to linearly decompose the ViT model into a set of components called learngenes during element-wise training. These learngenes can then be recomposed into differently scaled, pre-initialized models to satisfy different computational resource constraints. Such a decomposition-recomposition strategy provides an economical and flexible approach to generating different scales of ViT models for different deployment scenarios. Compared to model compression or training from scratch, which require to repeatedly train on large datasets for diverse-scale models, such strategy reduces computational costs since it only requires to train on large datasets once. Extensive experiments are used to validate the effectiveness of our method: ViTs can be decomposed and the decomposed learngenes can be recomposed into diverse-scale ViTs, which can achieve comparable or better performance compared to traditional model compression and pre-training methods. The code for our experiments is available in the supplemental material. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The pre-training Vision Transformers (ViTs) [13] have become fundamental to various applications, including image classification [59, 7], object detection [29, 52], semantic segmentation [46, 67], and multimodal tasks [63, 58]. However, these ViTs typically have a standard and relatively fixed architecture, which poses challenges for deployment in diverse real-world settings, i.e., devices in different application scenarios have different computational capabilities, making standard, fixed-size ViTs unsuitable for direct deployment. Furthermore, the storage demands of a traditional ViT may exceed the capabilities of certain devices. For instance, the memory size for the ViT-B/16 model [13] with 86M parameters is approximately $320~\\mathrm{MB}$ , while some devices may find this prohibitive due to inherent physical constraints, e.g., the limited RAM capacity of the Raspberry $\\mathrm{Pi}\\ 1$ with 256MB-512MB of RAM. ", "page_idx": 0}, {"type": "image", "img_path": "Yhd0yzC8yD/tmp/a6d32a825dc80124c62990ada01d713f86c450c6a5d16647164863ed3cc1409e.jpg", "img_caption": ["Figure 1: Comparison between Knowledge Distillation (KD) (left) and our method (right). KD requires $N$ times of training with the teacher model on data to produce $N$ different scale models for $N$ clients. In contrast, our method requires only a single training to decompose the teacher model into different learngenes that can be economically and flexibly recomposed into models with diverse layers to meet different client needs. "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "Yhd0yzC8yD/tmp/0b9bd6dae72351e901f5c9cc391f945ff6c78dd80c612c775142311aafcb8ed8.jpg", "img_caption": ["Figure 2: After using PCA to reduce the dimension of the parameters of a well-trained ViT, we find that the parameters of the most layers have an approximately linear correlation with their layer position. More details are given in the supplementary material. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "To address this limitation, researchers have developed various compression techniques [6, 62, 55, 28] to reduce the model size to make them suitable for the application scenarios while meantime maximum preserving the model ability. Typical approaches including micro-architecture design[23], model pruning[14, 68], quantization[43], and knowledge distillation[62]. However, in these approaches, the architecture of the smaller model, such as the number of layers, is predetermined and thus lacks the flexibility to meet the diverse needs of real-world deployment scenarios. Consider the scenario depicted in Figure 1: to obtain $N$ models of different sizes for different devices, the knowledge distillation methods need to go through $N$ separate trainings. Striking a balance between flexibility and accuracy is challenging and often requires complex training strategies to ensure satisfactory model performance. ", "page_idx": 1}, {"type": "text", "text": "Then we may ask, is there a way to more flexibly and economically generate $N$ smaller models from a large one, while maintaining the performance of the smaller ones? In calculus, we know that a function can be approximated by linearly combining several basic components with corresponding weighted coefficients at various levels of precision, e.g., Taylor series [45], Chebyshev Polynomial of the First Kind [31], Fourier series [47] and so on. Such a mathematical theorem induces two engineering insights: (1) a function can be decomposed into a series of basic components, i.e., the polynomial terms, and (2) we can combine an appropriate number of the basic components to approximate a function with a certain degree of precision. Motivated by these insights, we explore whether a large ViT can be incrementally decomposed into basic components, which we term \u201clearngenes\u201d, mimicking the behavior of organismal genes as proposed by [49, 51]. In their innovative learning framework, critical knowledge is continuously condensed as learngenes during the evolution of the ancestry model, which are then inherited to initialize descendant models of varying sizes. Similarly, in our approach, we propose to decompose a large ViT into a set of learngenes, encapsulating critical knowledge from the original model. These learngenes can then be flexibly recomposed to initialize ViT models of different scales, adapting to diverse deployment scenarios. ", "page_idx": 1}, {"type": "text", "text": "Specifically, to achieve this, we propose that each layer $W$ of the ViT can be decomposed into some basic learngenes: $A_{1},...,A_{N}$ , where each $A_{n}$ includes all submodules in $W$ , such as Multiheaded Self-Attention (MSA), Multi-Layer Perceptron (MLP), Feed Forward Networks (FFN), and so on. However, a ViT contains many layers and if we specify a learngene space for each layer, then an $L$ -layer ViT requires a total of $N\\times L$ basic learngenes, which still requires a large amount of computation to decompose. Interestingly, as shown in Figure 2 of [54], the parameters of most layers in the well-trained ViT of [13] have an approximately non-decreasing trend after PCA dimension reduction. Among several possible ftiting functions, the linear function is used here for its simplicity and effectiveness in approximating this trend. Thus we further assume that different layers of a ViT can also be linearly decomposed into the same learngene space, which means that we can also share the decomposed learngene across different layers in recomposition. ", "page_idx": 1}, {"type": "text", "text": "To summarize, we assume that the parameters of the $l$ -th layer can be got by ", "page_idx": 2}, {"type": "equation", "text": "$$\nW_{l}=\\sum_{n=1}^{N}a(l,n)\\times{\\bf A}_{n},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $a(l,n)$ are the linear combination coefficients, which are pre-defined and depend on the layer and learngene. Here, \u201cpre-defined\u201d denotes that $a(l,n)$ satisfies a pre-defined role, e.g., we find that the first-kind Chebyshev polynomial formula is a suitable one, and does not require adjustment during training. Also, when recomposing, given the layer rank $l$ and which learngenes should be used, we can first compute the values of $a(l,n)$ and then directly use the Equation 1 to construct the parameters of a ViT with any layer. ", "page_idx": 2}, {"type": "text", "text": "Such decomposition-recomposition strategy has the following two distinctive characteristics. First, such a decomposition strategy provides us with a novel training mechanism that allows us to iteratively train each learngene $\\textstyle A_{n}$ in an incremental way. Specifically, to train the first learngene $A_{1}$ , we train a ViT where the parameter of the $l$ -th layer is $\\dot{\\boldsymbol{W_{l}}}=\\boldsymbol{a}(\\boldsymbol{l},\\boldsymbol{1})\\dot{\\times}\\boldsymbol{A}_{1}$ . Then we begin to train the second learngene $A_{2}$ by constructing a ViT where $W_{l}=a(l,1)\\times{\\bf A}_{1}+a(l,2)\\times{\\bf A}_{2}$ . During training, $A_{1}$ is fixed and only the parameters of $A_{2}$ are updated. Then such a process is iterated a few times until all learngenes are trained. Compared to traditional pre-training methods, the decomposition approach not only generates a series of learngenes, but also yields a variety of pre-trained models with different scales during training. Importantly, the decomposed model achieves comparable performance to the pre-training method under the same parameters and training epochs. ", "page_idx": 2}, {"type": "text", "text": "Second, after decomposition, the decomposed learngenes can be flexibly recomposed into diverse scale ViTs with different depths without training. Meanwhile, we can use only an appropriate number of learngenes instead of all of them for recomposition to achieve dynamic accuracy-efficiency tradeoffs. For example, we can use the previous 6 learngenes $\\{A_{1},...,A_{6}\\}$ to recompose a 4-layer ViT where the k-th layer is initialized as Wk =  6n=1 a(k, n) \u00d7 An, where a(k, n) is still pre-defined by satisfying a specific role, e.g., the first-kind Chebyshev polynomial coefficients. In contrast to model compression methods where each training only targets a specific model size, which is not flexible enough to meet the needs of real-world deployment scenarios. ", "page_idx": 2}, {"type": "text", "text": "Finally, our experiments demonstrate the viability of linear decomposition and subsequent linear recomposition for ViT models. During decomposition, we observe that incrementally increasing the number of learngenes allows the performance to match that of a classically trained, integral ViT model. In recomposition, we can flexibly use an appropriate number of decomposed learngenes to construct ViTs of different layers. This method achieves performance comparable to other model compression methods, while facilitating the economical generation of models for diverse client requirements, e.g., reducing the training cost by $80\\times$ when generating the same number of models with diverse scales and initializations. ", "page_idx": 2}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this paper, we present a method that decomposes the parameters of the ViT model into a series of learngenes combined by polynomial coefficients. Then, the decomposed learngenes are used to flexibly recompose new ViT models with diverse scales. Thus, we discuss related works including model initialization and model compression which have similarities with the decomposition and recomposition of our method, respectively. ", "page_idx": 2}, {"type": "text", "text": "Model Initialization. Model initialization plays an important role in the training of deep neural networks, affecting both the rate of convergence and the quality of generalization. Fundamental methods, such as Xavier initialization [15] and Kaiming initialization [21], have been seminal in this domain. However, a recent trend is to use pre-trained models for initialization, as shown by studies such as [20, 5, 57, 32, 50]. This practice revolves around using these pre-trained models as a starting point, and then fine-tuning them for specific tasks. While such pre-trained models offer a superior initialization point, often outperforming the likes of Xavier and Kaiming initialization and ensuring faster training convergence, they come with their own challenges. Their large architectural footprint makes them unsuitable for direct deployment across various application scenarios, especially given the varying computing capabilities of devices. Furthermore, these pre-training methods often require large amounts of data and computational resources, making it difficult to build specialized models for individual tasks. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Model Compression Model compression [9, 55, 34] is a key area in deep learning, especially for deployment on resource-constrained devices. Micro-architecture requires the design of specific model architectures to meet different computational resource requirements, but the optimal architecture often varies depending on the required model size. Model pruning [64, 65] is a method that involves iteratively training the model, trimming less critical parameters, and subsequently fine-tuning for various model sizes. Quantization [18, 56, 14], which modulates the precision of model parameters,also requires unique iterative adjustments and potential retuning for each accuracy target. Meanwhile, knowledge distillation [33, 2], a concept popularized by [22], uses a more comprehensive teacher model to guide the training of a smaller student model. If we want to obtain $N$ student models, we need to train the model for $N$ times. A common drawback of these techniques is the lack of flexibility: generating $N$ models of different sizes typically requires $N$ times the computational and time cost. ", "page_idx": 3}, {"type": "text", "text": "Prompt Tuning Visual Prompt Tuning (VPT) have also been proposed to improve parameter efficiency. For example, $\\mathrm{E}^{\\mathord{\\sim}2\\mathrm{VPT}}$ [17], which reduces tunable parameters through learnable prompts and pruning, and by works like Facing the Elephant in the Room [16] that examine optimal conditions for VPT based on task and data distribution. Methods such as AdapterFusion [41] and PrefixTuning [27] use small, task-specific modules or tunable prefixes to allow efficient model adaptation without full retraining. In multimodal tasks, Learnable In-Context Vector (L-ICV) [40] addresses in-context learning (ICL) challenges by improving VQA performance with reduced computational costs by distilling task information into a single learnable vector. While these approaches reduce the number of tunable parameters, they still lack flexibility in deployment. For instance, generating $N$ models of different sizes often requires $N$ separate tuning or retraining processes, leading to significant computational overhead. ", "page_idx": 3}, {"type": "text", "text": "Therefore, we introduce a novel training approach where we decompose the ViT model into different learngenes. These decomposed learngenes can be recomposed to generate models of diverse scales, making them adaptable to downstream tasks with different computational resource requirements. Additionally, by using these learngenes, the recomposed models also provide a good initialization. This results in improved performance and faster convergence in downstream tasks compared to learning from scratch. ", "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We propose to decompose a Vision Transformer (ViT) into a series basic learngenes that each layer of the ViT can be linearly recomposed by these learngenes. Figure 3 outlines the whole pipeline of our method, where Section 3.1 shows how to decompose the ViT into the basic learngenes and in Section 3.2, we detail how to get these learngenes during training. After decomposition, Section 3.3 demonstrates how to flexibly recompose these learngenes into diverse ViTs with different depths to achieve a the balance between parameter efficiency and model performance. ", "page_idx": 3}, {"type": "text", "text": "3.1 Decomposing the ViT ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As discussed in Section 1, motivated by calculus, we propose to decompose a ViT into different basic learngenes where each layer of the ViT is the linear composition of these learngenes. Formally, given the basic learngenes $A_{1},...,A_{N}$ , the parameters of the $l$ -th layer $W_{l}$ can be got by Equation 1. Note that $W_{l}$ represents the general term of the parameters in the $l.$ -th layer of a ViT, which can be the modules such as the Multi-head Self Attention (MSA), Multi-layer Perceptron (MLP), and Layer Normalization (LN). In other words, each module of a ViT layer can be decomposed into a series of basic learngenes. For the coefficients $a(l,n)$ , we assume that they satisfy a pre-defined polynomial such as the Taylor series, the first kind Chebyshev polynomial, or the Fourier series. Through preliminary experiments presented in Section 4.2.1, we find that the first kind Chebyshev polynomial is particularly well suited for our purposes. Therefore, we use it as an example for the decomposition coefficients in the following content. The first kind Chebyshev polynomial, represented by $\\bar{T_{n}}(x)$ , is ", "page_idx": 3}, {"type": "image", "img_path": "Yhd0yzC8yD/tmp/b138216a065a89952a1f5adbf023935e86f915c26a2b4bcd10dde70917b6e1d1.jpg", "img_caption": ["Figure 3: Decomposition and Recomposition. The upper part illustrates the decomposition process, where a ViT model is gradually recomposed into several learngenes. At each stage, only the newly added learngenes are trained, while the previously trained learngenes remain frozen. The lower part shows the recomposition process with two examples. The first initializes a 2-layer ViT with three learngenes trained by \u201cwithout constraints\" , while the second initializes a 3-layer ViT with four learngenes trained by \u201cwith constraints\". Note that the flame icons indicate that the parameters of the layer are trained, while the snowflake icons indicates that the parameters are frozen. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "defined recursively as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{0}(x)=1,}\\\\ &{T_{1}(x)=x,}\\\\ &{T_{n+1}(x)=2x T_{n}(x)-T_{n-1}(x)\\quad\\mathrm{for}\\;n\\geq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Figure 2, it was observed that the parameters of each layer in the ViT have an approximately linear increasing trend. We assume that different layers of a ViT share the same learngene space, and the polynomial coefficients used to compose the learngenes are layer-dependent. This means that for $T_{n}=\\left(x\\right)$ , where $\\begin{array}{r}{x=\\frac{l-1}{L}}\\end{array}$ , and the coefficient $a(l,n)$ is defined as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\na(l,n)=T_{n}(\\frac{l-1}{L}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To avoid confusion, we use an example to show how to calculate these coefficients. Suppose the ViT model we decompose is a 12-layer ViT, in the other words, $L$ is equal to 12. For the second layer and the third order learngene, the formula to calculate $a(2,3)$ is as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\na(2,3)=2\\times{\\frac{2-1}{12}}\\times T_{2}({\\frac{2-1}{12}})-T_{1}({\\frac{2-1}{12}}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "And for the fourth layer and the third-order learngene, the formula to calculate $a(4,3)$ is ", "page_idx": 4}, {"type": "equation", "text": "$$\na(4,3)=2\\times{\\frac{4-1}{12}}\\times T_{2}({\\frac{4-1}{12}})-T_{1}({\\frac{4-1}{12}}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that the first kind Chebyshev polynomial is recursive, so in each layer the coefficients of the subsequent learngenes are influenced by the coefficients of the previous learngenes. ", "page_idx": 4}, {"type": "text", "text": "3.2 Learning the Basic Learngenes ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Here we introduce how to obtain the basic learngenes during training. Suppose we want to decompose a $L$ -th layer ViT into $N$ learngenes. We iteratively use more learngenes, following Equation 1, to construct and train the ViT. During training, in each iteration, only the newly added learngenes are trained, and all the previously used learngenes are fixed. More specifically, in the first iteration, we only use the first learngene $A_{1}$ to build the ViT following Equation 1, i.e., $W_{l}=a(l,1)A_{1}$ , and train this ViT. After the first iteration, we add the second learngene $A_{2}$ into the Equation 1, i.e., $W_{l}=a(l,1)A_{1}+a(l,2)A_{2}$ , and then continue training this ViT on classification again. However, in this iteration, only the second learngene $A_{2}$ will be trained, while $A_{1}$ will be fixed. Then, we will iterate this process until all $N$ learngenes are trained in turn. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "When training each learngene, we use the training objectives following MiniViT [62], where the total loss $\\mathcal{L}$ contains two terms: the classification cross-entropy loss $\\mathcal{L}_{c e}$ and the distillation loss $\\mathcal{L}_{d l}$ . The classification cross-entropy loss $\\mathcal{L}_{c e}$ is as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c e}=-\\sum_{j}\\sum_{i}y_{i j}\\log(\\phi_{i}(z_{j})).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, $y_{i j}$ denotes the ground truth label for the $i$ -th class of the $j$ -th data instance, expressed in a one-hot encoded format. The function $\\phi$ represents the softmax operation, and $z_{j}$ is the logit output corresponding to the $j$ -th data instance from the model comprising with learngenes. Next, we define the distillation loss as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{d l}=K L(z_{t}||\\boldsymbol{z}_{s}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $z_{t}$ denotes the logits output of the teacher model, $z_{s}$ denotes the logits output of the model comprising with learngenes and $K L$ represents the Kullback-Leibler divergence loss. ", "page_idx": 5}, {"type": "text", "text": "Thus, the loss function for learngene-wise training integrates both cross entropy and distillation losses as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{a l l}=\\mathcal{L}_{c e}+\\mathcal{L}_{d l}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.3 Recomposing the ViTs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "After the training process, the ViT model is decomposed into several learngenes. Consequently, these learngenes linearly composed by polynomial coefficients are serve as initialization for ViT models for diverse downstream tasks. It is important to note that, the linear coefficients are pre-defined by Equation 3, which means that these coefficients will not be updated during fine-tuning the recomposed models. The initialization of ViT models with layers of different depth use a suitable number of learngenes from Equation 3, with the number of layers $L$ adapted to fit the available computational resources. Based on the above decomposition formulas, the ViT models consisting of $L$ layers can be initialized as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nW_{l}=a(l,1)A_{1}+a(l,2)A_{2}+\\ldots+a(l,k)A_{k},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $W_{l}$ denotes the parameters of the $l_{\\cdot}$ -th layer in the ViT models, $k$ is the number of learngenes used for initialization. ", "page_idx": 5}, {"type": "text", "text": "After initializing the ViT models, there are two ways to train the model as shown in the lower part of Figure 3. The first one is that Equation 9 merely provides an initialization for the ViT models. In this way, the ViT models are trained without being constrained by Equation 9, the parameters of each layer $W_{l}$ are updated independently, i.e., the number of trainable parameters correlates with the layer number of the recomposed models. The second one is that the parameters of each layer of the ViT models are still constrained by Equation 9. In other words, during training, the ViT models update the parameters of the $k$ learngenes used for initialization, not the parameters of $W_{l}$ . Consequently, the number of trainable parameters in the model depends on the number of learngenes used. Therefore, not only can we choose an appropriate number of learngenes to initialize models with an appropriate number of layers, but we can also choose different training methods based on computational resources as discussed in Section 4.2.3. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we first describe the implementation details of the decomposition and recomposition processes of ViT in Section 4.1. Followed by experiments to validate that the ViT model can be decomposed into a series of learngenes and the feasibility of the ViT models that are flexibly composed of these learngenes in Section 4.2. ", "page_idx": 5}, {"type": "text", "text": "4.1 Implementation Details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1.1 Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To train each learngene, we use ImageNet-1K [12], which contains approximately 1.2M training images across 1000 classes and 50K validation images. After recomposing ViT models of different layers with learngenes, we adapt them on 9 diverse downstream datasets, which include 3 object classification tasks: CIFAR-10 [26], CIFAR-100 [26], and Tiny-ImageNet [1]; 5 fine-grained classification tasks: iNaturalist-2019 [66], Food-101 [4], Oxford Flowers-102 [35], Stanford Cars [25], and Oxford-IIIT Pets [38]; 1 texture classification task: DTD [10]. Each dataset presents unique challenges, ranging from basic object recognition to more specialized classification based on fine-grained visual differences and texture patterns. ", "page_idx": 6}, {"type": "text", "text": "4.1.2 Details of Decomposition and Recomposition ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Decomposition. Our approach to decompose the ViT model into different learngenes is based on the DeiT architecture [48]. To train the learngenes, we use a distillation strategy with the RegNet-16GF [42] as the teacher model. We train a total of 12 learngenes because there are 12 layers in DeiT. Constrained by computational resources, instead of training each learngene sequentially, we divided the training process into five phases, with learngene counts of $\\{1,2,4,8,12\\}$ . For example, during the third training phase, only the two newly added learngenes are trained, while the two previously trained learngenes are frozen. In each phase, the model is trained for 100 epochs with an initial 5 warmup epochs. ", "page_idx": 6}, {"type": "text", "text": "Recomposition. In the recomposition, we can choose different number of learngenes to flexibly generate the ViT models with different layers, such as $\\{2,4,6,8,10,12\\}$ layers. First, we determine the appropriate number of learngenes and number of layers of the ViT models based on computing resources. Then we use the pre-defined Equation 3 to calculate the polynomial coefficients. Following the Equation 9 can be used to initialize the parameters of the ViT models. After the models are initialized,they can be trained in one of two ways as mentioned in Section 4.2.3. ", "page_idx": 6}, {"type": "text", "text": "Computing resources. The resource cost of our method includes the decomposition and recomposition. In decomposition, it takes 500 epochs to obtain 12 learngenes, while recomposition requires no additional training to use different number of decomposed learngenes to initialize different layerbased models. Since the number of layers of ViTs ranges from 1 to 12, and each layer can be initialized with 12 different numbers of learngenes, there are a total of $12\\!\\times\\!12$ combinations of ViTs with different sizes and different initializations. Then if we try to get all these student ViTs by pre-training/distillation, we need to respectively train each one individually. Taking [62] as an example, training one student takes 300 epochs, then ${\\bf12}\\times{\\bf12}\\times{\\bf300{=}}{\\bf43.2K}$ epochs are required in total. ", "page_idx": 6}, {"type": "text", "text": "4.2 Results and Analyses ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this paper, we have two elementary assumptions of Equation 2. Firstly, a ViT can be decomposed into a series of basic learngenes. Secondly, ViTs with diverse depths can be flexibly recomposed by parts or all of these learngenes. Here, we first determine the polynomial coefficients for decomposition and recomposition based on the experiments in Section 4.2.1. Then we implement experiments to validate these two major assumptions in Section 4.2.2 and Section 4.2.3, respectively. ", "page_idx": 6}, {"type": "text", "text": "4.2.1 Choosing Suitable Polynomial Coefficients ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the decomposition and recomposition, there are several choices of the polynomial coefficients for Equation 3, e.g., Taylor series, the first kind Chebyshev polynomial, Fourier series and Legendre series. To select the appropriate polynomial coefficients, we perform a simple experiment in which the ViT model is decomposed ", "page_idx": 6}, {"type": "text", "text": "Table 1: Results of ViTs with different decomposition coefficients. ", "page_idx": 6}, {"type": "table", "img_path": "Yhd0yzC8yD/tmp/9dbe5ce24d4ae440b8b2e07131d525241bec88fe6f8e0003d1c514175def41c5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "into 12 learngenes. We then use these polynomial coefficients to compose these learngenes and train them simultaneously for 100 epochs. The results are presented in Section 4.2.1, it can be observed that when using the first kind Chebyshev Polynomial coefficients, we have the highest performance: ", "page_idx": 6}, {"type": "text", "text": "$77.73\\%$ accuracy. Therefore, we use the first kind Chebyshev polynomial coefficients in Equations 1 and 9 in the subsequent decomposition and recomposition experiments. ", "page_idx": 7}, {"type": "text", "text": "4.2.2 Decomposing the ViT ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Section 3.2, we show that the basic learngenes decomposed from the ViT model, i.e., DeiT-Base, can be trained incrementally, a method named ICT. Figure 4(a) also shows the result of training all learngenes simultaneously under the same training settings, a method called SCT. The process of incremental training is visually represented in Figure 4, where it is evident that as the number of learn", "page_idx": 7}, {"type": "image", "img_path": "Yhd0yzC8yD/tmp/b68a3f1816edeba30ab8205052a9ceecd5c15470c3535e616aafdc7240d35568.jpg", "img_caption": ["(a) Compare with Deit-Base(b) Compare with SN-Net ", "Figure 4: The accuracy of our method is compared with that of DeiT-B and SN-Net on ImageNet-1K. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "genes increases, the accuracy of the ViT model composed of these learngenes not only improves, but also surpasses the results of SCT. ", "page_idx": 7}, {"type": "text", "text": "In particular, at 12 learngenes, the accuracy of the ViT model reaches $83.70\\%$ , which is comparable to the performance of DeiT-Base, where the parameters in different layers are initialized independently. The results shown by DeiT in Figure 4(a), which are the same as the training method used in the decomposition, are obtained by training with the loss function in Equation 8, with 500 epochs of training. Thus, after separately learning each linear learngene of the ViT, the performance can achieve the results of the classical ViT which is trained as a whole, confirming the earlier assumption in Section 1 that the parameters of ViT can be linearly decomposed. ", "page_idx": 7}, {"type": "text", "text": "Moreover, we also compare the decomposition results with SN-Net [37] in Figure 4(b), which describes the generation of differently scaled ViTs on ImageNet-1k. The results show that the models obtained by decomposition outperform those stitched by SN-Net from DeiT-Tiny, DeiT-Small, and DeiT-Base. The computational cost for both approaches is as follows: our method totals 800 epochs, consisting of 300 epochs for training DeiT-Base and 500 for the decomposition. In contrast, SN-Net incurs 1150 epochs, with each of DeiT-Tiny/Small/Base requiring $300\\times3$ epochs, plus an additional $50\\times5$ epochs for five stitched models. Thus, our method demonstrates not only better performance but also greater efficiency in computational cost. ", "page_idx": 7}, {"type": "text", "text": "4.2.3 Recomposing the ViTs ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "After training, these learngenes can be flexibly recomposed into ViT models with different layers according to Equation 9. To validate the effectiveness of recomposing ViTs with diverse depths by using different numbers of learngenes, we perform the following experiments. ", "page_idx": 7}, {"type": "text", "text": "Evaluating Learngene Number and Layer Depth Effects on Model Performance. As mentioned in Section 1, the decomposed learngenes can be used to initialize ViT models with different layers to satisfy the computational resource requirements. Additionally, Section 3.3 introduces two ways for training the reconstituted ViT models. The first method, named \u201cw/o const\u201d means that $W_{l}$ calculated by Equation 9 only provides an initialization, and the parameters of each layer are independent. The second method, named \u201cw/ const\u201d is that the parameters of each layer still satisfy Equation 9. Due to space limitations, some experimental results are presented in the Appendix. ", "page_idx": 7}, {"type": "text", "text": "In Figure 5, we have two dimensions: the depth of the network and the number of the learngenes used to recompose. From these figures, first, we can find that when recomposing a deeper ViT or using more basic learngenes to recompose ViTs, the performance will also improve in most cases, whether \u201cw/ const\u201d or \u201cw/o const\u201d are used. In addition, increasing the number of layers tends to have a greater impact on the preform of the models. Second, on each graph, there is a boxed area bounded by red contours that represents $95\\%$ of the maximum value on each graph. Therefore, if the computer resources of clients are limited, they can choose to build VITs with fewer layers and initialize with fewer learngenes to achieve better resource-performance trade-off. For example, in Figure $5(\\mathrm{g)}$ , the 6-layer ViT model initialized by only 1 learngene can achieve $95\\%$ of the best performance. ", "page_idx": 7}, {"type": "text", "text": "Figures 5(f) and $5(\\mathrm{g)}$ also show a comparison between the \u201cw/ const\u201d and \u201cw/o const\u201d training methods. However, it should be noted that when the number of layers exceeds the number of learngenes, the model trained by the \u201cw/o const\u201d method consumes more memory than the \u201cw/ const\u201d. Therefore, for downstream clients with limited computational resources who want better performance. They can construct a ViT model with a higher number of layers, initialize it with a smaller number of learngenes, and then use \u201cw/ const\u201d training method. For example, a 12-layer ViT model initialized with two learngenes requires 86MB of memory using the \u201cw/o const\u201d method, but only 14.3MB using the \u201cw/ const\u201d method. On Food-101, it achieves $89.34\\%$ performance with \u201cw/ const\u201d and $90.44\\%$ with \u201cw/o const\u201d. Despite the slightly lower performance, the \u201cw/ const\u201d method significantly reduces memory usage by about 12 times. ", "page_idx": 7}, {"type": "image", "img_path": "Yhd0yzC8yD/tmp/9196a41f317789d139605be8015a66f8a8a23ce1bc567bd187ad2f45541cd9a4.jpg", "img_caption": ["Figure 5: Performance of ViTs with different layers initialized with different learngenes on downstream datasets. The first row shows the results with the \u201cw/o const\u201d training method, the second row is trained with \u201cw/ const\u201d. The second row also shows the differences between the two, where \u201c $\\mathrel{\\phantom{i}}+\\mathrel{\\phantom{i}}$ \u201d indicates improvement and \u201c\u2212\u201d indicates degradation. The boxed areas bounded by the red contours represent regions where the accuracy is within $95\\%$ of the maximum value of each graph. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "Yhd0yzC8yD/tmp/f2c0197166e820ac20964c2c1e964defad9b56536ce0665763a46dd27b8c6dd6.jpg", "img_caption": ["Figure 6: Comparative results. The ViTs are initialized with two different recomposition cases and trained by corresponding methods, i.e., one is trained with constraint and the other is trained without constraint, and then compared with the performance of differently scaled pre-trained models and models obtained by model compression methods. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Comparative Analysis. We compare our method with pre-training & fine-tuning (Pre-Fin) and model compression methods. In particular, (1) Pre-Fin: We use the pre-trained DeiT models including DeiT-Tiny, DeiT-Small and DeiT-Base in [48], and then fine-tune the model for downstream tasks. (2) Model compression: The methods used include Mini-DeiT-B in [62] and Efficient-ViT [30]. ", "page_idx": 8}, {"type": "text", "text": "For two training strategies in recomposition, we use different learngene configurations to recompose models with different numbers of trainable parameters. For the \u201cw/ const\u201d, the parameters of the model depend on the number of learngenes as in Section 3.3. Therefore, we use the 12-layer ViT model recomposed by $\\{1,2,4,6,8,10,^{\\circ}12\\}$ learngenes. For the \u201cw/o const\u201d, the trainable parameters correlate with the layer number of the model. Here, the number of learngenes is kept at 12 and ViT models are initialized over {2, 4, 6, 8, 10, 12} layers. Additional dataset results and corresponding numerical data are in the Appendix due to space limitations. ", "page_idx": 8}, {"type": "text", "text": "From Figure 6, we find that when different trainable parameters are used, \u201cw/ const\u201d outperforms the other methods such as pre-fine and model compression on downstream datasets. For example, in ", "page_idx": 8}, {"type": "text", "text": "Figure 6(a), when the model with 7.8M parameters is recomposed from a single learngene and trained with constraints, the accuracy is $72.3\\%$ . This performance exceeds that of Efficient ViT, which has 8.8M parameters, by $6.3\\%$ . Moreover, \u201cw/o const\u201d performs slightly worse than other methods with less trainable parameters because the networks have fewer layers. As in Figure 6(b), a 4-layer model with 28M parameters trained with \u201cw/o const\u201d performs over $9\\%$ worse than DeiT-Small with 22M parameters. However, as the number of layers increases, the performance of models trained with \u201cw/o const\u201d improves. For example, a 6-layer model with the same number of parameters as MiniViT, which is a 12-layer model, shows a performance difference of only $1.4\\%$ , indicating competitive results. These comparisons validate that when using suitable training strategy, the recomposed diverse-scale ViTs have good initialization that by simply training, they can achieve comparable or better performance than Pre-Fin and model compression methods. Furthermore, by our strategy, an appropriate number of decomposed learngenes can be selected based on available computational resources to initialize the models without training from scratch, achieving flexible accuracy and efficiency trade-offs. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In summary, this paper proposes a novel training method for efficiently generating ViTs of varied sizes to meet diverse computational needs. By employing linear decomposition, a ViT can be decomposed into basic learngenes, termed as \u201clearngenes\u201d, which encapsulate critical knowledge from the original model. These learngenes can then be selectively and linearly recomposed to form ViTs of various layers and sizes. This flexible recomposition provides an economical and adaptive solution for creating a series of small or medium ViTs tailored to different deployment environments. Our experiments confirm the effectiveness of this learngene-based decomposition-recomposition method and show that these recomposed ViTs maintain performance comparable to traditional model compression techniques while offering greater flexibility and efficiency. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was supported by the National Science Foundation of China (62125602, 62076063), the Key Program of Jiangsu Science Foundation (BK20243012), the Fundamental Research Funds for the Central Universities (2242024k30035) and the Big Data Computing Center of Southeast University. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Tiny ImageNet Visual Recognition Challenge. http://cs231n.stanford.edu/. Accessed: YYYYMM-DD.   \n[2] Nima Aghli and Eraldo Ribeiro. Combining weight pruning and knowledge distillation for cnn compression. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3191\u20133198, 2021.   \n[3] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021.   \n[4] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13, pages 446\u2013461. Springer, 2014.   \n[5] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.   \n[6] Cristian Bucilu\u02c7a, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535\u2013541, 2006.   \n[7] Chun-Fu Richard Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision transformer for image classification. In Proceedings of the IEEE/CVF international conference on computer vision, pages 357\u2013366, 2021.   \n[8] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9640\u20139649, 2021.   \n[9] Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration for deep neural networks. arXiv preprint arXiv:1710.09282, 2017.   \n[10] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3606\u20133613, 2014.   \n[11] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 702\u2013703, 2020.   \n[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[14] Elias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning. Advances in Neural Information Processing Systems, 35:4475\u20134488, 2022.   \n[15] Xavier Glorot and Yoshua Bengio. Xavier initialization. J. Mach. Learn. Res, 2010.   \n[16] C Han, Q Wang, Y Cui, W Wang, L Huang, S Qi, and D Liu. Facing the elephant in the room: visual prompt tuning or full finetuning? arxiv. Preprint], 2024.   \n[17] Cheng Han, Qifan Wang, Yiming Cui, Zhiwen Cao, Wenguan Wang, Siyuan Qi, and Dongfang Liu. E\u02c6 2vpt: An effective and efficient approach for visual prompt tuning. arXiv preprint arXiv:2307.13770, 2023.   \n[18] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.   \n[19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000\u201316009, 2022.   \n[20] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729\u20139738, 2020.   \n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[22] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.   \n[23] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.   \n[24] Sasan Karamizadeh, Shahidan M Abdullah, Azizah A Manaf, Mazdak Zamani, and Alireza Hooman. An overview of principal component analysis. Journal of Signal and Information Processing, 4(3B):173, 2013.   \n[25] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages 554\u2013561, 2013.   \n[26] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[27] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021.   \n[28] Yawei Li, Kamil Adamczewski, Wen Li, Shuhang Gu, Radu Timofte, and Luc Van Gool. Revisiting random channel pruning for neural network compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 191\u2013201, 2022.   \n[29] Jingyu Lin, Yan Yan, and Hanzi Wang. A dual-path transformer network for scene text detection. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135. IEEE, 2023.   \n[30] Xinyu Liu, Houwen Peng, Ningxin Zheng, Yuqing Yang, Han Hu, and Yixuan Yuan. Efficientvit: Memory efficient vision transformer with cascaded group attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.   \n[31] John C Mason and David C Handscomb. Chebyshev polynomials. CRC press, 2002.   \n[32] Xuran Meng, Difan Zou, and Yuan Cao. Benign overftiting in two-layer relu convolutional neural networks for xor data. arXiv preprint arXiv:2310.01975, 2023.   \n[33] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Improved knowledge distillation via teacher assistant. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 5191\u20135198, 2020.   \n[34] James O\u2019 Neill. An overview of neural network compression. arXiv preprint arXiv:2006.03669, 2020.   \n[35] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian conference on computer vision, graphics & image processing, pages 722\u2013729. IEEE, 2008.   \n[36] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.   \n[37] Zizheng Pan, Jianfei Cai, and Bohan Zhuang. Stitchable neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16102\u201316112, 2023.   \n[38] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE conference on computer vision and pattern recognition, pages 3498\u20133505. IEEE, 2012.   \n[39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \n[40] Yingzhe Peng, Chenduo Hao, Xu Yang, Jiawei Peng, Xinting Hu, and Xin Geng. Learnable in-context vector for visual question answering. arXiv preprint arXiv:2406.13185, 2024.   \n[41] Jonas Pfeiffer, Aishwarya Kamath, Andreas R\u00fcckl\u00e9, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020.   \n[42] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Designing network design spaces. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10428\u201310436, 2020.   \n[43] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In European conference on computer vision, pages 525\u2013542. Springer, 2016.   \n[44] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pages 618\u2013626, 2017.   \n[45] James Stewart. Essential calculus: Early transcendentals. Brooks/Cole, a part of the Thomson Corporation, 2007.   \n[46] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 7262\u20137272, 2021.   \n[47] Georgi P Tolstov. Fourier series. Courier Corporation, 2012.   \n[48] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pages 10347\u201310357. PMLR, 2021.   \n[49] Qiu-Feng Wang, Xin Geng, Shu-Xia Lin, Shi-Yu Xia, Lei Qi, and Ning Xu. Learngene: From open-world to your learning task. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8557\u20138565, 2022.   \n[50] Qiufeng Wang, Xu Yang, Haokun Chen, and Xin Geng. Vision transformers as probabilistic expansion from learngene. In Forty-first International Conference on Machine Learning.   \n[51] Qiufeng Wang, Xu Yang, Shuxia Lin, Jing Wang, and Xin Geng. Learngene: Inheriting condensed knowledge from the ancestry model to descendant models. arXiv preprint arXiv:2305.02279, 2023.   \n[52] Zhehui Wang, Tao Luo, Miqing Li, Joey Tianyi Zhou, Rick Siow Mong Goh, and Liangli Zhen. Evolutionary multi-objective model compression for deep neural networks. IEEE Computational Intelligence Magazine, 16(3):10\u201321, 2021.   \n[53] Ross Wightman et al. Pytorch image models, 2019.   \n[54] Shiyu Xia, Miaosen Zhang, Xu Yang, Ruiming Chen, Haokun Chen, and Xin Geng. Transformer as linear expansion of learngene. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 16014\u201316022, 2024.   \n[55] Canwen Xu and Julian McAuley. A survey on model compression and acceleration for pretrained language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 10566\u201310575, 2023.   \n[56] Jiwei Yang, Xu Shen, Jun Xing, Xinmei Tian, Houqiang Li, Bing Deng, Jianqiang Huang, and Xian-sheng Hua. Quantization networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7308\u20137316, 2019.   \n[57] Xingyi Yang, Daquan Zhou, Songhua Liu, Jingwen Ye, and Xinchao Wang. Deep model reassembly. Advances in neural information processing systems, 35:25739\u201325753, 2022.   \n[58] Chao Yi, De-Chuan Zhan, and Han-Jia Ye. Bridge the modality and capacity gaps in vision-language model selection. arXiv preprint arXiv:2403.13797, 2024.   \n[59] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF international conference on computer vision, pages 558\u2013567, 2021.   \n[60] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6023\u20136032, 2019.   \n[61] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.   \n[62] Jinnian Zhang, Houwen Peng, Kan Wu, Mengchen Liu, Bin Xiao, Jianlong Fu, and Lu Yuan. Minivit: Compressing vision transformers with weight multiplexing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12145\u201312154, 2022.   \n[63] Yi-Kai Zhang, Shiyin Lu, Yang Li, Yanqing Ma, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, and Han-Jia Ye. Wings: Learning multimodal llms without text-only forgetting. arXiv preprint arXiv:2406.03496, 2024.   \n[64] Yihua Zhang, Yuguang Yao, Parikshit Ram, Pu Zhao, Tianlong Chen, Mingyi Hong, Yanzhi Wang, and Sijia Liu. Advancing model pruning via bi-level optimization. Advances in Neural Information Processing Systems, 35:18309\u201318326, 2022.   \n[65] Kaiqi Zhao, Animesh Jain, and Ming Zhao. Automatic attention pruning: Improving and automating model pruning using attentions. In International Conference on Artificial Intelligence and Statistics, pages 10470\u201310486. PMLR, 2023.   \n[66] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9719\u20139728, 2020.   \n[67] Hongyuan Zhu, Fanman Meng, Jianfei Cai, and Shijian Lu. Beyond pixels: A comprehensive survey from bottom-up to semantic image segmentation and cosegmentation. Journal of Visual Communication and Image Representation, 34:12\u201327, 2016.   \n[68] Weiqi Zou, Yang Wang, Xueyang Fu, and Yang Cao. Dreaming to prune image deraining networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6023\u20136032, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "image", "img_path": "Yhd0yzC8yD/tmp/36a2aa9034cd2bbf23a041164252bae019eaf74f873ea91f079619ad7badf75c.jpg", "img_caption": ["Figure 7: The parameters of each layer of the pre-trained model and their corresponding layer position relationships. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In the following section, we present the process of dimensionality reduction applied to the parameters of each layer in the Vision Transformer (ViT) models in order to observe the interrelationships between the parameters across different layers. We then supplement the experimental content presented in the main text, including the experimental results on decomposition and recomposition, as well as implementation details. Finally, this section concludes with a discussion of the limitations and societal implications of our method. ", "page_idx": 13}, {"type": "text", "text": "A.1 Dimensionality Reduction and Inter-Layer Parameter Relationships in ViTs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Our analysis starts with the aggregation of parameter matrices from the Multihead Self-Attention (MSA), Multilayer Perceptron (MLP), and Layer Normalization (LN) modules of an extensively trained Transformer model. Specifically, within the MSA module and given an input $x$ , we first perform the computation of $(x\\bar{W}^{Q})(x W^{\\bar{K}})^{T}$ to determine the attentional outputs for each head. This then facilitates the derivation of $W^{A}$ for the MSA as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\nW^{A}=W^{Q}W^{K^{T}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $W^{K}$ and $W^{Q}$ , belonging to $\\mathbb{R}^{D\\times D}$ , act as transformation parameter matrices for keys and queries, respectively, where $D$ denotes the dimensionality of the intermediate embeddings. Subsequent operations involve the bias vectors for keys and queries, proceeding according to Equation 10. Together with other parameter matrices in the MSA module, we reshape these into vectors and subject them to L2 normalization. ", "page_idx": 13}, {"type": "text", "text": "In the MLP module, we flatten and normalize weight matrices and bias vectors from two linear transformations. A similar approach is employed in the LN module, where weight matrices and bias vectors associated with transformation parameters are normalized. Normalized vectors, specific to each parameter type, are concatenated in alignment with their corresponding layer, culminating in a synthesized matrix $\\boldsymbol{S}\\in\\mathbb{R}^{N\\times D_{L}}$ , where $D_{L}$ symbolizes the normalized vector dimension for each parameter type, and $N$ signifies the depth of the Transformer. ", "page_idx": 13}, {"type": "text", "text": "Principal Component Analysis (PCA) [24] is then utilized to reduce the dimension of each vector in $S$ to a one-dimensional scalar, primarily for analytical convenience. These reduced vectors are then combined to form the matrix $\\dot{H^{\\prime}}\\in\\mathbb{R}^{N\\times K}$ , with $K$ indicative of the number of parameter types. Each row of $H$ is interpreted as the representative vector of the corresponding layer. Another iteration of PCA is performed to compress the rows of $H$ into a one-dimensional domain, resulting in a vector $U\\in\\mathbb{R}^{N}$ . This final vector is used to illustrate the relationship between the sequential positioning of the layers in the transformer and their respective parameter values, thus illustrating the nuanced interplay of parameters across the architectural layers. ", "page_idx": 13}, {"type": "text", "text": "We also show the graphs for MoCov3 [8], DINOv2 [36], MAE-B [19] and BEITv2-B [3] in Figures 7(a) to 7(d), which show that the parameters of the layers have an approximately linear correlation with the layer position. ", "page_idx": 13}, {"type": "text", "text": "A.2 Experiment ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.2.1 Decomposition ", "text_level": 1, "page_idx": 13}, {"type": "image", "img_path": "Yhd0yzC8yD/tmp/fdde03f25a7fff4ece3139f19624bc2d4bd034ecdc324db19e1bed1a9777ac9d.jpg", "img_caption": ["(a) Qualitative Visualization of Each learngene. Uses each of the first six learngenes to initialize the model separately, showing different characteristics and areas of focus for each learngene. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "Yhd0yzC8yD/tmp/e9ed72a30532c06a19037fbe6047e67b8acac57becc66e52bbb7539fe8e28ba3.jpg", "img_caption": ["Figure 8: Qualitative visualization of decomposed learngenes "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Qualitative Visualization of Learngenes. After training, we obtain the decomposed learngenes of the Vision Transformer (ViT). To analyze their characteristics, we employ Gradient-weighted Class Activation Mapping (Grad-CAM) [44], which is pivotal in the classification process, to visualize the feature maps in two scenarios: one where each learngene is used independently to initialize a ViT, and the other where learngenes are added incrementally to initialize the ViT model. ", "page_idx": 14}, {"type": "text", "text": "From Figure 8(a), we find that each learngene focuses on uniquely specific regions, indicating that the functionalities of the ViT model are decomposed into distinct learngenes. Specifically, the first learngene focuses on basic shapes and contours. The second learngene detects textures and colors, such as highlighting leaf textures and bird feather details. The third learngene identifies spatial features such as insect wings, while the fourth learngene detects finer details such as bird eyes or insect antennae. The fifth learngene emphasizes image contrast, and the sixth distinguishes between foreground and background, focusing on depth perception. As shown in Figure 8(b), early learngenes capture basic shapes and edges, while later learngenes refine the focus on textures, colors, and specific details such as leaf veins or bird feathers. As learngenes are added, the attention of model shifts to finer features such as bird beaks, insect wings, and antennae, indicating a progression from general to detailed feature recognition that is likely to lead to more accurate image classification. ", "page_idx": 14}, {"type": "text", "text": "A.2.2 Recomposition ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Evaluating Learngene Number and Layer Depth Effects on Model Performance. The experimental results in Figure 9 on these two datasets further verify the findings in the text. The results in the appendix confirm the observation that deeper network configurations or a larger number of ", "page_idx": 14}, {"type": "image", "img_path": "Yhd0yzC8yD/tmp/7054a199731f2516959e6a6ee6744f7346594977a2b58fb582a6c6a22fca5043.jpg", "img_caption": ["(a) CIFAR100 /\u201cw/o(b) INAT / \u201cw/o(c) Pets /\u201cw/o const\u201d (d) Cars /\u201cw/o const\u201d(e) DTD /\u201cw/o const\u201d const\u201d const\u201d "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "Yhd0yzC8yD/tmp/39e9f61816b98d30a98e9fabfdb8a7a149dc2f7f733ce14a869cef0ab9c5a79c.jpg", "img_caption": ["(f) CIFAR100 /\u201cw/(g) INAT /\u201cw/ const\u201d (h) Pets /\u201cw/ const\u201d (i) Cars /\u201cw/ const\u201d (j) DTD /\u201cw/ const\u201d const\u201d "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 9: Performance of ViTs with different layers initialized with different learngenes on downstream datasets. ", "page_idx": 15}, {"type": "image", "img_path": "Yhd0yzC8yD/tmp/8c699f1620b38f9a6ff05a68b0b86f350c57250c5cfa3a8e1c3002e672b92659.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 10: Comparative results. The ViTs are initialized with two different recomposition cases and trained by corresponding methods, i.e., one is trained with constraint and the other is trained without constraint, and then compared with the performance of differently scaled pre-trained models and models obtained by model compression methods. ", "page_idx": 15}, {"type": "text", "text": "decomposed learngenes generally improve ViT performance, whether trained by \u201cw/o const\u201d or \u201cw/ const\u201d. The results also show stable performance beyond certain learngene and layer thresholds, and provide strategies for optimizing ViT models under resource constraints, such as using fewer layers and learngenes to achieve near-optimal performance. We also compare the \u201cw/const\u201d and \u201cw/o const\u201d training methods and find that performance differences decrease as model complexity increases. In particular, models with higher layers show minimal performance variance between the two methods, but differ in memory consumption, with \u201cw/o const\u201d requiring more. We recommend \u201cw/o const\u201d for efficiency in resource-constrained scenarios, as shown by the 12-layer ViT model initialized with fewer learngenes, which achieves comparable performance with significantly less memory consumption. ", "page_idx": 15}, {"type": "text", "text": "Comparative Results. The experimental results presented in Figure 10 show that a 12-layer Vision Transformer (ViT) initialized with different numbers of learngenes and trained using the \u201cwith constraint\u201d method, represented by the orange lines in the graphs, outperforms traditional approaches such as pre-fine tuning and model compression under the same parameter constraints. In addition, as the number of layers in the model increases to a certain threshold, models initialized with 12 learngenes and trained using the \u201cwithout constraint\u201d method also outperform these baselines. Thus, our method strikes a balance between flexibility and model efficiency. ", "page_idx": 15}, {"type": "text", "text": "Numerical Results for Comparative Results. Table 2 provide the numerical performance on 9 downstream tasks to compare our methods with pre-training fine-tuning and model compression methods. The same results are also shown in Figures 6 and 10 of the main paper. ", "page_idx": 15}, {"type": "table", "img_path": "Yhd0yzC8yD/tmp/79328d1aae67658ada0a57d4372cedd3adfa1b9fe5702a707a799b8f72ad7f36.jpg", "table_caption": ["Table 2: Comparative experiments. Two cases of initializing models with learngenes and their corresponding training approaches, compared with the pre-training fine-tuning and model compression methods. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "Yhd0yzC8yD/tmp/a6141c056d3b3a40d7d8efffd5525552dced4382a91669d4204a18bcefb77ec5.jpg", "table_caption": ["Table 3: Overview of Classification Datasets "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.3 Implementation Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.3.1 Code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We implement the model using PyTorch [39] and the Timm library [53]. The decomposition process is trained over 500 epochs on four NVIDIA RTX 3090 GPUs, and the recomposed models are trained over 100 epochs on two NVIDIA RTX 3090 GPUs for each downstream task. ", "page_idx": 16}, {"type": "text", "text": "A.3.2 Datasets and Pre-processing ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Datasets We adapt the recomposed models on 9 diverse downstream datasets, covering a wide range of classification challenges. These datasets include 3 object classification tasks: CIFAR-10 [26], CIFAR-100 [26], and Tiny-ImageNet [1]; 5 fine-grained classification tasks: iNaturalist-2019 [66], Food-101 [4], Oxford Flowers-102 [35], Stanford Cars [25], and Oxford-IIIT Pets [38]; 1 texture classification task: DTD [10]. The details of these datasets are in Table 3. ", "page_idx": 16}, {"type": "text", "text": "Data Process Following previous works [62], we train and evaluate the decomposition and recomposition processes on all datasets at a resolution of $224\\times224$ . The data augmentation techniques employed include RandAugment [11], Cutmix [60], Mixup [61], and random erasing. ", "page_idx": 16}, {"type": "text", "text": "A.3.3 Hyper-parameter ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Decomposition We train each learngene using the following setting: ", "page_idx": 16}, {"type": "text", "text": "\u2022 Batch Size: We employ distributed training across 4 GPUs, with each GPU handling 128 data instances, resulting in an overall batch size of 512.   \n\u2022 Optimizer: The training of each learngene is optimized using AdamW, with an initial learning rate of 0.0008 and a weight decay of 0.05.   \n\u2022 Learning Rate Schedule: We apply a cosine learning rate decay, with a warm-up period of 5 epochs. ", "page_idx": 17}, {"type": "text", "text": "Recomposition We train the recomposed models on downstream tasks using the following setting: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Batch Size: We employ distributed training across 2 GPUs, with each GPU handling 128 data instances, resulting in an overall batch size of 256.   \n\u2022 Optimizer: The training of each learngene is optimized using AdamW and a weight decay of 0.05.   \n\u2022 Learning Rate Schedule: We apply a cosine learning rate decay, with a warm-up period of 5 epochs. ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: The claims stated in the abstract and introduction are fully supported by the experimental results presented in the paper, which confirm the effectiveness of our methodology in adapting Vision Transformers to diverse computational settings. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We discussed the limitations of our method in the Appendix ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our method is inspired by polynomial decomposition in calculus. This paper primarily investigates the feasibility of the approach, and at this stage we are unable to provide a comprehensive theoretical underpinning. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our paper provides code in the Supplementary Material and describes the datasets and hyperparameters used in the Experiments section of the main text and in the Appendix. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide code in the Supplementary Material, and the datasets used in the paper are all public datasets, which are described in the Appendix. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide experimental setting in Experiment section and the hyperparameters in Appendix. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: Due to the large size of the datasets, significance tests were not performed, but consistency was maintained by fixing the random seed during training and evaluation. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The appendix details the types and quantities of machines used, although specific execution times are not provided due to the numerous experiments conducted. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The research strictly conforms to the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper discusses the societal impact of our work in the Appendix. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 21}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve the release of models or data, hence there are no associated risks requiring safeguards. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The experimental section and appendix provide detailed references to the datasets used, citing the original sources. In addition, all experimental settings are properly referenced, ensuring full compliance with the licensing requirements of the assets used. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: No release of new assets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: No crowdsourcing or human subjects research. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: No crowdsourcing or human subjects research. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]