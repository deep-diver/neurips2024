{"references": [{"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-10-26", "reason": "This paper is foundational for the use of Vision Transformers (ViTs) in image recognition, a core concept of the current paper."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Training data-efficient image transformers & distillation through attention", "publication_date": "2021-07-01", "reason": "This paper introduces DeiT, a data-efficient training approach for ViTs which is used as the basis for the decomposition strategy in the current work."}, {"fullname_first_author": "Geoffrey Hinton", "paper_title": "Distilling the knowledge in a neural network", "publication_date": "2015-03-16", "reason": "This paper introduces the concept of knowledge distillation, a key technique used in the decomposition process to train the learngenes."}, {"fullname_first_author": "Jinnian Zhang", "paper_title": "Minivit: Compressing vision transformers with weight multiplexing", "publication_date": "2022-06-01", "reason": "This paper provides a model compression method for ViTs, which is directly compared to in the experimental results of this paper."}, {"fullname_first_author": "Qiu-Feng Wang", "paper_title": "Learngene: From open-world to your learning task", "publication_date": "2022-01-01", "reason": "This paper introduces the concept of learngenes and their use in model initialization, a concept that forms the theoretical foundation of the current paper."}]}