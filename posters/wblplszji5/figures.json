[{"figure_path": "WBLPlszJI5/figures/figures_4_1.jpg", "caption": "Figure 1: Impact of the heterogeneity (\u03c3<sub>h</sub>), number of Byzantine adversaries f and the task complexity (\u03c3). (Top) The average error for different values of \u03bb, computed using 20 random experiments. (Bottom) Comparison of theoretical \u03bb* and empirical minimizer of the error. We fixed the following default values: n = 600, f = 100, m = 20, \u03c3 = 15, \u03c3<sub>h</sub> = 2", "description": "This figure displays the results of experiments on Byzantine-robust federated mean estimation. The top row shows how the average error of the mean estimation changes with different levels of collaboration (\u03bb) under varying conditions of data heterogeneity (\u03c3h), number of Byzantine adversaries (f), and task complexity (\u03c3). The bottom row compares the theoretical optimal collaboration level (\u03bb*) with the empirically observed optimal level. The results show that the optimal collaboration level decreases with increasing heterogeneity and number of Byzantine adversaries. Also, the optimal level of collaboration is affected by task complexity, with higher complexity leading to higher collaboration levels.", "section": "Warm-up: Mean Estimation with Adversaries"}, {"figure_path": "WBLPlszJI5/figures/figures_7_1.jpg", "caption": "Figure 1: Impact of the heterogeneity (\u03c3\u043d), number of Byzantine adversaries f and the task complexity (\u03c3). (Top) The average error for different values of \u03bb, computed using 20 random experiments. (Bottom) Comparison of theoretical \u03bb* and empirical minimizer of the error. We fixed the following default values: n = 600, f = 100, m = 20, \u03c3 = 15, \u03c3\u043d = 2", "description": "This figure shows the impact of data heterogeneity, the number of Byzantine adversaries, and task complexity on the performance of the federated mean estimation algorithm. The top row displays the average error for different collaboration levels (\u03bb) under varying conditions.  The bottom row compares the theoretical optimal collaboration level (\u03bb*) with the empirically observed optimal level. The results illustrate how the optimal level of collaboration depends on these factors.", "section": "2 Warm-up: Mean Estimation with Adversaries"}, {"figure_path": "WBLPlszJI5/figures/figures_19_1.jpg", "caption": "Figure 2: Effect of adversarial fraction and heterogeneity and local sample size. (Top) Phishing dataset with logistic regression with n = 20, \u03b1 = 3. (Bottom) MNIST with a Convolutional Neural Network (details in Appendix D) n = 20. \u03b1 = \u221e refers to the homogeneous setting.", "description": "This figure analyzes the impact of adversarial clients and data heterogeneity on the performance of the proposed personalized federated learning algorithm for two datasets: Phishing and MNIST. The top row shows results for the Phishing dataset using logistic regression, while the bottom row presents results for the MNIST dataset using a convolutional neural network.  Each row displays test accuracy on local datasets for various collaboration levels (\u03bb), with different numbers of adversarial clients (f) and levels of heterogeneity (\u03b1).  The \u03b1 = \u221e case represents a homogeneous setting (no heterogeneity). The figure demonstrates how the optimal collaboration level changes depending on the adversarial fraction and heterogeneity, highlighting situations where full collaboration is not optimal.", "section": "3.3 Experimental Validation"}, {"figure_path": "WBLPlszJI5/figures/figures_20_1.jpg", "caption": "Figure 4: Effect of adversarial fraction and heterogeneity. Binary MNSIT dataset with logistic regression. n = 200, m = 32.", "description": "This figure shows the impact of both the fraction of Byzantine adversaries and the level of heterogeneity on the test accuracy for the binary MNIST dataset using logistic regression.  The x-axis represents the collaboration level (\u03bb), while the y-axis shows the test accuracy. Different lines represent different fractions of Byzantine clients (f). The subfigures (a) and (b) correspond to different levels of heterogeneity (\u03b1).  The results show that the optimal collaboration level depends on both the number of adversarial clients and heterogeneity, and that in some cases, not collaborating at all is better than full collaboration.", "section": "3.3 Experimental Validation"}, {"figure_path": "WBLPlszJI5/figures/figures_20_2.jpg", "caption": "Figure 5: Local Vs FL performance on local test dataset. Phishing dataset with n = 20, a = 3. As the number of local samples increases, the Byzantine fraction threshold above which local learning performs better than Robust Federated Learning gets smaller.", "description": "This figure compares the performance of local learning and robust federated learning on a phishing dataset for different numbers of Byzantine (adversarial) clients.  The x-axis shows the number of Byzantine clients, and the y-axis represents the test accuracy. Three different local dataset sizes (m=16, 32, 48) are shown.  The figure demonstrates that as the amount of local data increases, the point at which local learning outperforms robust federated learning shifts towards fewer Byzantine clients. This highlights the importance of local data size in mitigating the negative effects of adversarial clients in federated learning scenarios.", "section": "D.4 Phishing"}, {"figure_path": "WBLPlszJI5/figures/figures_20_3.jpg", "caption": "Figure 2: Effect of adversarial fraction and heterogeneity and local sample size. (Top) Phishing dataset with logistic regression with n = 20, \u03b1 = 3. (Bottom) MNIST with a Convolutional Neural Network (details in Appendix D) n = 20. \u03b1 = \u221e refers to the homogeneous setting.", "description": "The figure displays the impact of different factors (adversarial fraction, heterogeneity, and local sample size) on the test accuracy of two datasets: Phishing and MNIST. The top row shows the results for the Phishing dataset using logistic regression with 20 clients and a heterogeneity parameter \u03b1 of 3. The bottom row shows the results for the MNIST dataset using a Convolutional Neural Network with 20 clients and a heterogeneity parameter \u03b1 of \u221e (homogeneous setting). Each subfigure presents the test accuracy against the collaboration parameter \u03bb for various levels of adversarial clients (f).", "section": "3.3 Experimental Validation"}, {"figure_path": "WBLPlszJI5/figures/figures_21_1.jpg", "caption": "Figure 7: Effect of the adversarial fraction and the data size. Phishing with logistic regression. n = 20, Auto-FOE attack, (top) \u03b1 = 3, (bottom) \u03b1 = 10.", "description": "This figure shows the results of experiments conducted on a phishing dataset using logistic regression.  The experiments vary the number of adversarial clients (f), the size of the local dataset (m), and the level of data heterogeneity (\u03b1). The top row shows results for \u03b1 = 3, while the bottom row shows results for \u03b1 = 10.  Each subplot displays the test accuracy on the local dataset as a function of the collaboration level (\u03bb) for different values of f and m. The results illustrate how the optimal collaboration level and the impact of adversarial clients change depending on dataset size and heterogeneity.", "section": "3.3 Experimental Validation"}]