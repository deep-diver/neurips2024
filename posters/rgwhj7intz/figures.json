[{"figure_path": "rgwhJ7INtZ/figures/figures_1_1.jpg", "caption": "Figure 1: Top row. Under \u00b5P, (left) the sharpness dynamics are largely identical for the whole training dynamics across different widths, phenomenon that we call Super Consistency. The dashed horizontal lines are the Edge of Stability thresholds. Center: The loss dynamics are similar early in training, but accumulate finite-size effects over time, thus violating Super Consistency. Right: The learning rate transfers from small to large model, suggesting that the loss landscape is Super Consistent across different model sizes. Bottom row. Under NTK parameterization (NTP), the sharpness dynamics show large discrepancies. Also, the learning rate does not transfer. The architecture is a two-layer convolutional network trained on CIFAR-10 with data augmentation, where the width corresponds to the number of filters in the convolution. (See App. J). Other parameters: B = 128, epochs = 50.", "description": "The figure shows the training dynamics of sharpness and loss under two different parameterizations, \u00b5P and NTK.  Under \u00b5P scaling, sharpness remains consistent across different network widths, a phenomenon termed \"Super Consistency.\" The loss curves show slight differences at later stages. Importantly, the learning rate transfers successfully from smaller to larger models under \u00b5P. In contrast, under NTK parameterization, sharpness dynamics differ significantly across widths, and learning rate transfer fails.  The experiment is performed on a two-layer convolutional network trained on CIFAR-10.", "section": "Super Consistency of the Optimization Landscape"}, {"figure_path": "rgwhJ7INtZ/figures/figures_3_1.jpg", "caption": "Figure 2: (a) The top Hessian eigenvalues exhibit a progressive increase to a threshold, with larger eigenvalues showing precise Super Consistency, while lower eigenvalues show finite-size accumulation at small width in the initial phase of training. (b) Top eigenvalues of the NTK matrix \u0398. As opposed to the top eigenvalues of the Hessian, these exhibit evident finite-size accumulation during training. Model: 3-layer ConvNet, \u03c4 = 0, \u03bf = 0.7 (optimal). Details in Sec. J.", "description": "This figure visualizes the training dynamics of the top Hessian and NTK eigenvalues under the \u00b5P parametrization.  The top row (a) shows that the largest Hessian eigenvalues exhibit Super Consistency, converging to a width-independent threshold. This contrasts with the bottom row (b), which shows the top NTK eigenvalues accumulating significant finite-size effects during training. This difference highlights the distinct behavior of the Hessian and NTK spectra under \u00b5P and suggests that the observed Super Consistency in the Hessian is linked to feature learning.", "section": "Super Consistency of the Optimization Landscape"}, {"figure_path": "rgwhJ7INtZ/figures/figures_3_2.jpg", "caption": "Figure 23: (a) Convergence rate of the largest NTK eigenvalue in width at multiple steps during training.(b) Convergence rate of the sharpness in width at multiple steps during training. Note that the largest NTK eigenvalue starts width independent for both learning rates, but becomes width dependent during training, as opposed to the sharpness which maintains a width independent dynamic throughout the whole optimization process (see Fig. 24). (c) Loss convergence rate in width; note that the loss accumulates finite-size effects during time, exhibiting a wider is better effect during training. (d) Hessian trace evolution during training at various widths. Unlike the sharpness, the trace has a width dependent period at the beginning of training, but approaches a width-indepedent threshold. Details: Residual ConvNet trained on CIFAR10 with cross-entropy loss.", "description": "This figure shows the convergence rates of the largest NTK eigenvalue and sharpness with respect to width at multiple steps during training.  It also shows the loss convergence rate and Hessian trace evolution.  The key takeaway is that the sharpness exhibits width-independent behavior throughout training, unlike the NTK eigenvalue and loss, which show width dependence. This difference highlights the unique consistency of the sharpness across different model sizes.", "section": "Time Evolution of other Spectral Quantities"}, {"figure_path": "rgwhJ7INtZ/figures/figures_4_1.jpg", "caption": "Figure 1: Top row. Under \u00b5P, (left) the sharpness dynamics are largely identical for the whole training dynamics across different widths, phenomenon that we call Super Consistency. The dashed horizontal lines are the Edge of Stability thresholds. Center: The loss dynamics are similar early in training, but accumulate finite-size effects over time, thus violating Super Consistency. Right: The learning rate transfers from small to large model, suggesting that the loss landscape is Super Consistent across different model sizes. Bottom row. Under NTK parameterization (NTP), the sharpness dynamics show large discrepancies. Also, the learning rate does not transfer. The architecture is a two-layer convolutional network trained on CIFAR-10 with data augmentation, where the width corresponds to the number of filters in the convolution. (See App. J). Other parameters: B = 128, epochs = 50.", "description": "This figure compares the training dynamics of sharpness (largest eigenvalue of the Hessian), loss, and learning rate across different model widths under two different parameterizations: \u00b5P and NTK.  The \u00b5P parameterization shows consistent behavior across different widths (Super Consistency), with learning rate transfer. In contrast, the NTK parameterization shows significant discrepancies across widths, and no learning rate transfer.  The results suggest a connection between Super Consistency of the loss landscape and learning rate transfer.", "section": "1 Introduction"}, {"figure_path": "rgwhJ7INtZ/figures/figures_6_1.jpg", "caption": "Figure 4: Depth-\u00b5P extensions with top row showing transfer plots and bottom row the sharpness evolution. (a) ConvNets with 1 layer per block exhibit both hyperparameter transfer and sharpness Super Consistency. (b) ConvNets with 2 layers per block. The model has a lazy behavior within each block, and no transfer. The sharpness starts accumulating finite-size effects during training, violating Super Consistency. (c) ViTs also have k > 2 blocks per layer by design, and thus have a similar behaviour. Details: (a), (b) are trained with SGD, with widths 128 and 32 respectively; (c) is trained with Adam, with the learning rate scaled by 1/\u221aL [8]. See Fig. 22 for convergence rates.", "description": "This figure compares the performance of three different network architectures (ConvNets with 1 layer per block, ConvNets with 2 layers per block, and ViTs) under the Depth-\u00b5P parameterization.  The top row shows learning rate transfer plots, while the bottom row illustrates the sharpness evolution. The results demonstrate that networks with a single layer per block exhibit both hyperparameter transfer and sharpness Super Consistency (consistent sharpness dynamics across different model sizes). In contrast, networks with multiple layers per block (ConvNets with 2 layers per block and ViTs) show a lack of transfer and violate Super Consistency, indicating that sharpness dynamics are not consistent across different model sizes.", "section": "4 Super Consistency and Learning Rate Transfer"}, {"figure_path": "rgwhJ7INtZ/figures/figures_6_2.jpg", "caption": "Figure 5: Evolution of the top eigenvalues of the Hessian components G and R for a two-layer linear network trained on random data under MSE loss. The vector field characterizes the evolution during training for a fixed learning rate. Top: \u00b5P. Note how G drives the initial change super consistently. Bottom: NTP. For wider networks the sharpening phase reduces, since the network is approaching the limit where the NTK is fixed to its value at initialization.", "description": "This figure shows the evolution of the top eigenvalues of the Hessian components G and R for a two-layer linear network trained on random data under MSE loss.  The left panel shows the \u00b5P parameterization, where the sharpness evolution is largely dominated by the G matrix consistently across different widths. The right panel shows the NTP parameterization, where the sharpness evolution slows down when increasing the width. The results indicate that under \u00b5P, feature learning causes the progressive sharpening, while the lack of feature learning in NTP prevents the Hessian from adapting and its largest eigenvalue from reaching the convergence threshold. ", "section": "4 Super Consistency and Learning Rate Transfer"}, {"figure_path": "rgwhJ7INtZ/figures/figures_17_1.jpg", "caption": "Figure 1: Top row. Under \u00b5P, (left) the sharpness dynamics are largely identical for the whole training dynamics across different widths, phenomenon that we call Super Consistency. The dashed horizontal lines are the Edge of Stability thresholds. Center: The loss dynamics are similar early in training, but accumulate finite-size effects over time, thus violating Super Consistency. Right: The learning rate transfers from small to large model, suggesting that the loss landscape is Super Consistent across different model sizes. Bottom row. Under NTK parameterization (NTP), the sharpness dynamics show large discrepancies. Also, the learning rate does not transfer. The architecture is a two-layer convolutional network trained on CIFAR-10 with data augmentation, where the width corresponds to the number of filters in the convolution. (See App. J). Other parameters: B = 128, epochs = 50.", "description": "This figure compares the training dynamics of sharpness and loss under two different parameterizations: \u00b5P and NTK.  The top row shows that under \u00b5P scaling, the sharpness remains consistent across different model widths throughout training (a phenomenon the authors term \"Super Consistency\"), while the loss shows slight deviations at later stages.  Importantly, the learning rate transfers effectively from smaller to larger models under \u00b5P. The bottom row demonstrates that under NTK parametrization, sharpness dynamics vary significantly across different widths, and the learning rate does not transfer. This highlights the relationship between Super Consistency and learning rate transfer.", "section": "1 Introduction"}, {"figure_path": "rgwhJ7INtZ/figures/figures_18_1.jpg", "caption": "Figure 1: Top row. Under \u00b5P, (left) the sharpness dynamics are largely identical for the whole training dynamics across different widths, phenomenon that we call Super Consistency. The dashed horizontal lines are the Edge of Stability thresholds. Center: The loss dynamics are similar early in training, but accumulate finite-size effects over time, thus violating Super Consistency. Right: The learning rate transfers from small to large model, suggesting that the loss landscape is Super Consistent across different model sizes. Bottom row. Under NTK parameterization (NTP), the sharpness dynamics show large discrepancies. Also, the learning rate does not transfer. The architecture is a two-layer convolutional network trained on CIFAR-10 with data augmentation, where the width corresponds to the number of filters in the convolution. (See App. J). Other parameters: B = 128, epochs = 50.", "description": "This figure compares the training dynamics of sharpness and loss under \u00b5P and NTP parameterizations.  The top row shows that under \u00b5P, sharpness remains consistent across different network widths (super consistency), while the loss eventually shows discrepancies due to finite-size effects. Learning rate transfer is observed under \u00b5P.  The bottom row shows that under NTP, sharpness dynamics vary significantly across widths, and learning rate transfer does not occur.  The experiment uses a two-layer convolutional neural network trained on CIFAR-10.", "section": "1 Introduction"}, {"figure_path": "rgwhJ7INtZ/figures/figures_18_2.jpg", "caption": "Figure 1: Top row. Under \u00b5P, (left) the sharpness dynamics are largely identical for the whole training dynamics across different widths, phenomenon that we call Super Consistency. The dashed horizontal lines are the Edge of Stability thresholds. Center: The loss dynamics are similar early in training, but accumulate finite-size effects over time, thus violating Super Consistency. Right: The learning rate transfers from small to large model, suggesting that the loss landscape is Super Consistent across different model sizes. Bottom row. Under NTK parameterization (NTP), the sharpness dynamics show large discrepancies. Also, the learning rate does not transfer. The architecture is a two-layer convolutional network trained on CIFAR-10 with data augmentation, where the width corresponds to the number of filters in the convolution. (See App. J). Other parameters: B = 128, epochs = 50.", "description": "This figure compares the training dynamics of a two-layer convolutional neural network under two different parameterizations: \u00b5P and NTK.  The top row shows the results under \u00b5P scaling, demonstrating that the sharpness (largest eigenvalue of the Hessian) remains consistent across different network widths throughout training (Super Consistency), while the training loss shows minor deviations with increasing width, and the optimal learning rate transfers from smaller to larger models. The bottom row illustrates the NTK parameterization, which shows significant differences in sharpness dynamics across different widths and a lack of learning rate transfer.", "section": "1 Introduction"}, {"figure_path": "rgwhJ7INtZ/figures/figures_19_1.jpg", "caption": "Figure 1: Top row. Under \u00b5P, (left) the sharpness dynamics are largely identical for the whole training dynamics across different widths, phenomenon that we call Super Consistency. The dashed horizontal lines are the Edge of Stability thresholds. Center: The loss dynamics are similar early in training, but accumulate finite-size effects over time, thus violating Super Consistency. Right: The learning rate transfers from small to large model, suggesting that the loss landscape is Super Consistent across different model sizes. Bottom row. Under NTK parameterization (NTP), the sharpness dynamics show large discrepancies. Also, the learning rate does not transfer. The architecture is a two-layer convolutional network trained on CIFAR-10 with data augmentation, where the width corresponds to the number of filters in the convolution. (See App. J). Other parameters: B = 128, epochs = 50.", "description": "This figure compares the training dynamics of two-layer convolutional neural networks under two different parameterizations: \u00b5P and NTK.  The top row shows results for \u00b5P, demonstrating that sharpness (a measure of landscape curvature) remains consistent across different network widths ('Super Consistency'), while training loss shows some divergence with increasing width.  Importantly, the learning rate transfers effectively between small and large models.  In contrast, the bottom row shows results for NTK, where sharpness dynamics vary significantly with width, indicating inconsistent landscape behavior, and the learning rate does not transfer. This illustrates a key finding of the paper: learning rate transfer correlates with Super Consistency of the loss landscape.", "section": "1 Introduction"}, {"figure_path": "rgwhJ7INtZ/figures/figures_19_2.jpg", "caption": "Figure 10: Evolution of loss under \u00b5P and NTP for the toy example of Section 5: ||VNDEV \u2212 w*||2, where w* = 1 \u2208 RD, D = 100. This is a minimal example of transfer captured by our theory: \u00b5P trajectories align. Different linestyles correspond to different values of \u03b7o (grid is different for \u00b5P and NTP).", "description": "This figure shows the loss curves for a simple two-layer linear network with linear activation trained on a dataset with 100 dimensions. The network is trained under two different parameterizations: \u00b5P and NTP. The figure demonstrates the alignment of the loss curves across different learning rates under \u00b5P parameterization, illustrating the learning rate transfer phenomenon. Conversely, under NTP parameterization the curves do not align.", "section": "5 Case study: Two-Layer Linear Network"}, {"figure_path": "rgwhJ7INtZ/figures/figures_28_1.jpg", "caption": "Figure 2: (a) The top Hessian eigenvalues exhibit a progressive increase to a threshold, with larger eigenvalues showing precise Super Consistency, while lower eigenvalues show finite-size accumulation at small width in the initial phase of training. (b) Top eigenvalues of the NTK matrix \u0398. As opposed to the top eigenvalues of the Hessian, these exhibit evident finite-size accumulation during training. Model: 3-layer ConvNet, \u03c4 = 0, \u03bf = 0.7 (optimal). Details in Sec. J.", "description": "This figure visualizes the dynamics of the top Hessian eigenvalues and the top NTK eigenvalues during training of a 3-layer convolutional neural network.  Panel (a) shows that the top Hessian eigenvalues progressively increase towards a threshold, and remain consistent across different network widths (super consistency).  In contrast, panel (b) shows that the top NTK eigenvalues accumulate significant finite-size effects during training, with significant discrepancies across different widths. This difference highlights a key distinction between the \u00b5P and NTK parameterizations discussed in the paper.", "section": "Super Consistency of the Optimization Landscape"}, {"figure_path": "rgwhJ7INtZ/figures/figures_29_1.jpg", "caption": "Figure 1: Top row. Under \u00b5P, (left) the sharpness dynamics are largely identical for the whole training dynamics across different widths, phenomenon that we call Super Consistency. The dashed horizontal lines are the Edge of Stability thresholds. Center: The loss dynamics are similar early in training, but accumulate finite-size effects over time, thus violating Super Consistency. Right: The learning rate transfers from small to large model, suggesting that the loss landscape is Super Consistent across different model sizes. Bottom row. Under NTK parameterization (NTP), the sharpness dynamics show large discrepancies. Also, the learning rate does not transfer. The architecture is a two-layer convolutional network trained on CIFAR-10 with data augmentation, where the width corresponds to the number of filters in the convolution. (See App. J). Other parameters: B = 128, epochs = 50.", "description": "This figure compares the training dynamics of sharpness and loss under \u00b5P (feature learning) and NTP (kernel regime) parameterizations.  Under \u00b5P, the sharpness remains consistent across different network widths (Super Consistency),  and the learning rate transfers from smaller to larger models.  In contrast, under NTP, sharpness dynamics vary significantly with width, and learning rate transfer doesn't occur. The experiment uses a two-layer convolutional network trained on CIFAR-10.", "section": "Super Consistency of the Optimization Landscape"}, {"figure_path": "rgwhJ7INtZ/figures/figures_29_2.jpg", "caption": "Figure 1: Top row. Under \u00b5P, (left) the sharpness dynamics are largely identical for the whole training dynamics across different widths, phenomenon that we call Super Consistency. The dashed horizontal lines are the Edge of Stability. The finite-size effects accumulate, thus violating Super Consistency. The dashed horizontal lines are the Edge of Stability. The finite-size effects accumulate, thus violating Super Consistency. The dashed horizontal lines are the Edge of Stability. The finite-size effects accumulate, thus violating Super Consistency. The dashed horizontal lines are the Edge of Stability. The finite-size effects accumulate, thus violating Super Consistency. Right: The learning rate transfers from small to large models, suggesting that the loss landscape is Super Consistent across different model sizes. Bottom row. Under NTK, the sharpness dynamics across different model sizes. Bottom row. Under NTK, the sharpness dynamics show large discrepancies. Also, note that the loss landscape is Super Consistent across different model sizes. Bottom row. Under NTK, the sharpness dynamics show large discrepancies. Also, note that the loss landscape is Super Consistent across different model sizes. Bottom row. Under NTK, the sharpness dynamics show large discrepancies. Also, the training loss does not transfer. The architecture is a two-layer convolutional network trained on CIFAR-10 with a varying number of filters in the convolutional layers.", "description": "This figure demonstrates the key finding of the paper: Super Consistency.  The top row shows that under the \u00b5P parameterization, the sharpness of the loss landscape (largest eigenvalue of the Hessian) remains largely consistent across different network widths throughout training. This is in contrast to the NTK parameterization (bottom row), where sharpness dynamics differ significantly across widths.  The figure also highlights the relationship between Super Consistency and learning rate transfer; learning rate transfer is observed under \u00b5P but not NTK.  The results are shown for a specific network architecture trained on CIFAR-10.", "section": "Super Consistency of the Optimization Landscape"}, {"figure_path": "rgwhJ7INtZ/figures/figures_30_1.jpg", "caption": "Figure 1: Top row. Under \u00b5P, (left) the sharpness dynamics are largely identical for the whole training dynamics across different widths, phenomenon that we call Super Consistency. The dashed horizontal lines are the Edge of Stability thresholds. Center: The loss dynamics are similar early in training, but accumulate finite-size effects over time, thus violating Super Consistency. Right: The learning rate transfers from small to large model, suggesting that the loss landscape is Super Consistent across different model sizes. Bottom row. Under NTK parameterization (NTP), the sharpness dynamics show large discrepancies. Also, the learning rate does not transfer. The architecture is a two-layer convolutional network trained on CIFAR-10 with data augmentation, where the width corresponds to the number of filters in the convolution. (See App. J). Other parameters: B = 128, epochs = 50.", "description": "This figure compares the training dynamics of sharpness and loss under two different parameterizations: \u00b5P and NTK.  The top row shows that under \u00b5P scaling, the sharpness remains consistent across different network widths throughout training (Super Consistency), while the loss shows some finite-size effects that accumulate over time.  Importantly, the learning rate transfers successfully from smaller to larger models, indicating a consistent optimization landscape. In contrast, the bottom row shows the NTK parameterization, where the sharpness exhibits significant discrepancies across different widths and the learning rate does not transfer. This highlights the distinct behavior of the loss landscape under different scaling regimes and the crucial role of feature learning.", "section": "1 Introduction"}, {"figure_path": "rgwhJ7INtZ/figures/figures_30_2.jpg", "caption": "Figure 1: Top row. Under \u00b5P, (left) the sharpness dynamics are largely identical for the whole training dynamics across different widths, phenomenon that we call Super Consistency. The dashed horizontal lines are the Edge of Stability thresholds. Center: The loss dynamics are similar early in training, but accumulate finite-size effects over time, thus violating Super Consistency. Right: The learning rate transfers from small to large model, suggesting that the loss landscape is Super Consistent across different model sizes. Bottom row. Under NTK parameterization (NTP), the sharpness dynamics show large discrepancies. Also, the learning rate does not transfer. The architecture is a two-layer convolutional network trained on CIFAR-10 with data augmentation, where the width corresponds to the number of filters in the convolution. (See App. J). Other parameters: B = 128, epochs = 50.", "description": "This figure compares the training dynamics of sharpness and loss under two different parameterizations: \u00b5P and NTK.  The top row shows results for \u00b5P, demonstrating the consistent sharpness across different network widths (super consistency). The middle panel shows that the loss curves, while initially similar, diverge over time due to finite-size effects, violating super consistency. The right panel displays learning rate transfer from smaller to larger models under \u00b5P. The bottom row illustrates the NTK parameterization, revealing significant differences in sharpness dynamics and the absence of learning rate transfer.  The experiment involves a two-layer convolutional network trained on CIFAR-10 with data augmentation.", "section": "Super Consistency of the Optimization Landscape"}, {"figure_path": "rgwhJ7INtZ/figures/figures_31_1.jpg", "caption": "Figure 1: Top row. Under \u00b5P, (left) the sharpness dynamics are largely identical for the whole training dynamics across different widths, phenomenon that we call Super Consistency. The dashed horizontal lines are the Edge of Stability thresholds. Center: The loss dynamics are similar early in training, but accumulate finite-size effects over time, thus violating Super Consistency. Right: The learning rate transfers from small to large model, suggesting that the loss landscape is Super Consistent across different model sizes. Bottom row. Under NTK parameterization (NTP), the sharpness dynamics show large discrepancies. Also, the learning rate does not transfer. The architecture is a two-layer convolutional network trained on CIFAR-10 with data augmentation, where the width corresponds to the number of filters in the convolution. (See App. J). Other parameters: B = 128, epochs = 50.", "description": "This figure compares the training dynamics of a two-layer convolutional neural network under two different parameterizations: \u00b5P and NTK. The top row shows the results for \u00b5P, demonstrating that the sharpness (largest eigenvalue of the Hessian) remains consistent across different network widths throughout training, a phenomenon the authors term \"Super Consistency.\"  The training loss shows initial consistency but deviates over time due to finite-size effects.  Importantly, the learning rate transfers effectively from smaller to larger networks. The bottom row illustrates the NTK parameterization, revealing significant discrepancies in sharpness dynamics across widths and a failure of learning rate transfer.  This contrast highlights the key role of feature learning in achieving Super Consistency and learning rate transfer.", "section": "1 Introduction"}, {"figure_path": "rgwhJ7INtZ/figures/figures_31_2.jpg", "caption": "Figure 1: Top row. Under \u00b5P, (left) the sharpness dynamics are largely identical for the whole training dynamics across different widths, phenomenon that we call Super Consistency. The dashed horizontal lines are the Edge of Stability thresholds. Center: The loss dynamics are similar early in training, but accumulate finite-size effects over time, thus violating Super Consistency. Right: The learning rate transfers from small to large model, suggesting that the loss landscape is Super Consistent across different model sizes. Bottom row. Under NTK parameterization (NTP), the sharpness dynamics show large discrepancies. Also, the learning rate does not transfer. The architecture is a two-layer convolutional network trained on CIFAR-10 with data augmentation, where the width corresponds to the number of filters in the convolution. (See App. J). Other parameters: B = 128, epochs = 50.", "description": "This figure compares the training dynamics of sharpness and loss under two different parameterizations (muP and NTK) across various network widths.  The top row shows that under muP scaling, sharpness remains consistent across different widths (Super Consistency), while the loss eventually diverges.  Learning rate transfers successfully from smaller to larger models under muP. The bottom row shows that under NTK scaling, sharpness dynamics differ significantly across widths and learning rate does not transfer. This highlights the importance of feature learning in achieving Super Consistency and learning rate transfer.", "section": "3 Super Consistency of the Optimization Landscape"}, {"figure_path": "rgwhJ7INtZ/figures/figures_31_3.jpg", "caption": "Figure 1: Top row. Under \u00b5P, (left) the sharpness dynamics are largely identical for the whole training dynamics across different widths, phenomenon that we call Super Consistency. The dashed horizontal lines are the Edge of Stability thresholds. Center: The loss dynamics are similar early in training, but accumulate finite-size effects over time, thus violating Super Consistency. Right: The learning rate transfers from small to large model, suggesting that the loss landscape is Super Consistent across different model sizes. Bottom row. Under NTK parameterization (NTP), the sharpness dynamics show large discrepancies. Also, the learning rate does not transfer. The architecture is a two-layer convolutional network trained on CIFAR-10 with data augmentation, where the width corresponds to the number of filters in the convolution. (See App. J). Other parameters: B = 128, epochs = 50.", "description": "The figure compares the training dynamics of a two-layer convolutional neural network under two different parameterizations: \u00b5P (mu-P) and NTK (Neural Tangent Kernel).  The top row shows results for \u00b5P.  The leftmost plot demonstrates that the sharpness (largest eigenvalue of the Hessian) remains consistent across different network widths throughout training, a phenomenon the authors call \"Super Consistency.\" The center plot shows that while the training loss is initially similar across widths, it diverges over time. The rightmost plot shows that the optimal learning rate transfers from small to large models under \u00b5P. The bottom row shows the analogous results for the NTK parameterization. In contrast to \u00b5P, the sharpness dynamics are vastly different at different scales, and the learning rate does not transfer.", "section": "1 Introduction"}, {"figure_path": "rgwhJ7INtZ/figures/figures_32_1.jpg", "caption": "Figure 1: Top row. Under \u00b5P, (left) the sharpness dynamics are largely identical for the whole training dynamics across different widths, phenomenon that we call Super Consistency. The dashed horizontal lines are the Edge of Stability thresholds. Center: The loss dynamics are similar early in training, but accumulate finite-size effects over time, thus violating Super Consistency. Right: The learning rate transfers from small to large model, suggesting that the loss landscape is Super Consistent across different model sizes. Bottom row. Under NTK parameterization (NTP), the sharpness dynamics show large discrepancies. Also, the learning rate does not transfer. The architecture is a two-layer convolutional network trained on CIFAR-10 with data augmentation, where the width corresponds to the number of filters in the convolution. (See App. J). Other parameters: B = 128, epochs = 50.", "description": "This figure compares the training dynamics of two-layer convolutional neural networks under two different parameterizations: \u00b5P (muP) and Neural Tangent Kernel (NTK).  The top row shows that under \u00b5P scaling, the sharpness (largest eigenvalue of the Hessian) remains consistent across different network widths throughout training, a phenomenon the authors term \"Super Consistency.\"  The training loss shows some initial similarity but diverges over time, and the learning rate transfers successfully from smaller to larger models. The bottom row demonstrates that under NTK scaling, the sharpness dynamics differ significantly across widths and the learning rate does not transfer.", "section": "Super Consistency of the Optimization Landscape"}, {"figure_path": "rgwhJ7INtZ/figures/figures_32_2.jpg", "caption": "Figure 4: Depth-\u00b5P extensions with top row showing transfer plots and bottom row the sharpness evolution. (a) ConvNets with 1 layer per block exhibit both hyperparameter transfer and sharpness Super Consistency. (b) ConvNets with 2 layers per block. The model has a lazy behavior within each block, and no transfer. The sharpness starts accumulating finite-size effects during training, violating Super Consistency. (c) ViTs also have k > 2 blocks per layer by design, and thus have a similar behavior. Details: (a), (b) are trained with SGD, with widths 128 and 32 respectively; (c) is trained with Adam, with the learning rate scaled by 1/\u221aL [8]. See Fig. 22 for convergence rates.", "description": "This figure shows the results of experiments using Depth-\u00b5P scaling with different numbers of layers per residual block.  The top row displays learning rate transfer plots and the bottom row shows sharpness evolution across different model widths and depths.  Part (a) demonstrates both hyperparameter transfer and Super Consistency with one layer per block. In contrast, (b) and (c), which employ two or more layers per block, respectively, do not exhibit transfer, and their sharpness shows increasing deviations with width and depth, indicating a lack of Super Consistency. This suggests that the Super Consistency property of the loss landscape is crucial for learning rate transfer across different model sizes. The specific architectures include convolutional networks and Vision Transformers.", "section": "4 Super Consistency and Learning Rate Transfer"}, {"figure_path": "rgwhJ7INtZ/figures/figures_33_1.jpg", "caption": "Figure 1: Top row. Under \u00b5P, (left) the sharpness dynamics are largely identical for the whole training dynamics across different widths, phenomenon that we call Super Consistency. The dashed horizontal lines are the Edge of Stability thresholds. Center: The loss dynamics are similar early in training, but accumulate finite-size effects over time, thus violating Super Consistency. Right: The learning rate transfers from small to large model, suggesting that the loss landscape is Super Consistent across different model sizes. Bottom row. Under NTK parameterization (NTP), the sharpness dynamics show large discrepancies. Also, the learning rate does not transfer. The architecture is a two-layer convolutional network trained on CIFAR-10 with data augmentation, where the width corresponds to the number of filters in the convolution. (See App. J). Other parameters: B = 128, epochs = 50.", "description": "This figure compares the training dynamics of sharpness and loss under two different parameterizations: \u00b5P and NTK.  \u00b5P exhibits \"Super Consistency,\" meaning the sharpness remains consistent across different network widths throughout training.  The learning rate also transfers between models of different sizes. In contrast, the NTK parametrization shows significantly different sharpness dynamics at different scales, and the learning rate does not transfer.", "section": "1 Introduction"}, {"figure_path": "rgwhJ7INtZ/figures/figures_33_2.jpg", "caption": "Figure 22: Convergence rates with respect to time for the losses and sharpnesses shown in Figure 4.", "description": "This figure shows the convergence rates of losses and sharpnesses over time for three different network architectures: Depth-\u00b5P ConvNet with k=1, Depth-\u00b5P ConvNet with k=2, and Depth-\u00b5P ViT.  Each plot shows the convergence rate (fitted power law y=at\u03b2) of the loss and sharpness (largest eigenvalue of the preconditioned Hessian) for various network depths. The results illustrate differences in how the consistency of the sharpness and loss varies depending on the architecture and scaling regime.", "section": "G Time Evolution of other Spectral Quantities"}, {"figure_path": "rgwhJ7INtZ/figures/figures_34_1.jpg", "caption": "Figure 23: (a) Convergence rate of the largest NTK eigenvalue in width at multiple steps during training.(b) Convergence rate of the sharpness in width at multiple steps during training. Note that the largest NTK eigenvalue starts width independent for both learning rates, but becomes width dependent during training, as opposed to the sharpness which maintains a width independent dynamic throughout the whole optimization process (see Fig. 24). (c) Loss convergence rate in width; note that the loss accumulates finite-size effects during time, exhibiting a wider is better effect during training. (d) Hessian trace evolution during training at various widths. Unlike the sharpness, the trace has a width dependent period at the beginning of training, but approaches a width-indepedent threshold. Details: Residual ConvNet trained on CIFAR10 with cross-entropy loss.", "description": "This figure analyzes the convergence rate of different properties of the loss landscape (largest NTK eigenvalue, sharpness, loss, and Hessian trace) with respect to the network width.  It shows that while the sharpness remains consistent across different widths, other quantities like the largest NTK eigenvalue and the loss accumulate finite-size effects as width increases. The Hessian trace also exhibits width dependence initially but converges to a width-independent value.", "section": "G Time Evolution of other Spectral Quantities"}, {"figure_path": "rgwhJ7INtZ/figures/figures_34_2.jpg", "caption": "Figure 1: Top row. Under \u00b5P, (left) the sharpness dynamics are largely identical for the whole training dynamics across different widths, phenomenon that we call Super Consistency. The dashed horizontal lines are the Edge of Stability thresholds. Center: The loss dynamics are similar early in training, but accumulate finite-size effects over time, thus violating Super Consistency. Right: The learning rate transfers from small to large model, suggesting that the loss landscape is Super Consistent across different model sizes. Bottom row. Under NTK parameterization (NTP), the sharpness dynamics show large discrepancies. Also, the learning rate does not transfer. The architecture is a two-layer convolutional network trained on CIFAR-10 with data augmentation, where the width corresponds to the number of filters in the convolution. (See App. J). Other parameters: B = 128, epochs = 50.", "description": "The figure shows the training dynamics of sharpness and loss for both \u00b5P and NTK parameterizations across various model widths.  Under \u00b5P, sharpness remains consistent regardless of width, exhibiting a phenomenon called 'Super Consistency', while the loss shows some deviation.  Learning rate transfer is also observed under \u00b5P, but not under NTK, where sharpness displays significant variation across different widths. This suggests a link between Super Consistency and learning rate transfer.", "section": "3 Super Consistency of the Optimization Landscape"}, {"figure_path": "rgwhJ7INtZ/figures/figures_35_1.jpg", "caption": "Figure 2: (a) The top Hessian eigenvalues exhibit a progressive increase to a threshold, with larger eigenvalues showing precise Super Consistency, while lower eigenvalues show finite-size accumulation at small width in the initial phase of training. (b) Top eigenvalues of the NTK matrix \u0398. As opposed to the top eigenvalues of the Hessian, these exhibit evident finite-size accumulation during training. Model: 3-layer ConvNet, \u03c4 = 0, \u03bf = 0.7 (optimal). Details in Sec. J.", "description": "This figure shows the training dynamics of the top eigenvalues of the Hessian and Neural Tangent Kernel (NTK) matrices under \u00b5P parametrization.  The top row (a) shows that the largest Hessian eigenvalues exhibit Super Consistency, converging to a width-independent threshold, while the smaller eigenvalues show finite-size effects. The bottom row (b) shows that the top NTK eigenvalues show significant finite-size effects during training, in contrast to the Hessian eigenvalues. This difference highlights the impact of feature learning on the landscape's properties.", "section": "Super Consistency of the Optimization Landscape"}, {"figure_path": "rgwhJ7INtZ/figures/figures_35_2.jpg", "caption": "Figure 1: Top row. Under \u00b5P, (left) the sharpness dynamics are largely identical for the whole training dynamics across different widths, phenomenon that we call Super Consistency. The dashed horizontal lines are the Edge of Stability thresholds. Center: The loss dynamics are similar early in training, but accumulate finite-size effects over time, thus violating Super Consistency. Right: The learning rate transfers from small to large model, suggesting that the loss landscape is Super Consistent across different model sizes. Bottom row. Under NTK parameterization (NTP), the sharpness dynamics show large discrepancies. Also, the learning rate does not transfer. The architecture is a two-layer convolutional network trained on CIFAR-10 with data augmentation, where the width corresponds to the number of filters in the convolution. (See App. J). Other parameters: B = 128, epochs = 50.", "description": "This figure compares the training dynamics of a two-layer convolutional neural network under two different parameterizations: \u00b5P (mu-P) and NTK (Neural Tangent Kernel).  The top row shows results for the \u00b5P parameterization.  The left plot demonstrates that the sharpness (largest eigenvalue of the Hessian) remains consistent across different network widths throughout training, a phenomenon the authors term \"Super Consistency.\" The center plot shows that while training loss is initially similar, finite-size effects accumulate over time, violating Super Consistency. The right plot shows that the optimal learning rate transfers from small to large models, further supporting the idea of Super Consistent loss landscapes. The bottom row shows results for the NTK parameterization.  In contrast to \u00b5P, the sharpness dynamics exhibit significant discrepancies across different widths, and the learning rate does not transfer. This suggests that Super Consistency is linked to learning rate transfer.", "section": "1 Introduction"}, {"figure_path": "rgwhJ7INtZ/figures/figures_36_1.jpg", "caption": "Figure 27. Convolutional networks trained with SGD on CIFAR-10 parameterized with \u00b5P (top) and Depth-\u00b5P (bottom) showing learning transfer (a) and super consistent Hessian-gradient alignment during training (b, d). HPs: (top) 100 warmup steps, batch size 256, 20 epochs, no residual connections (bottom) 200 warmup steps, batch size 128, 40 epochs, scaling (both) 6 layers, ReLU.", "description": "This figure shows the results of training convolutional neural networks on CIFAR-10 using two different parameterizations: \u00b5P (top) and Depth-\u00b5P (bottom).  Both parameterizations aim to achieve learning rate transfer, meaning the optimal learning rate remains consistent across different model sizes (widths for \u00b5P, depths for Depth-\u00b5P). The left panels (a and c) illustrate this learning rate transfer, with each line representing a different model size.  The right panels (b and d) demonstrate Super Consistency of the landscape, a property the authors define where the alignment between the gradient and the Hessian's largest eigenvalues remains consistent throughout the training process.  The dashed lines in (b and d) represent the Edge of Stability (EoS) threshold.", "section": "4 Super Consistency and Learning Rate Transfer"}, {"figure_path": "rgwhJ7INtZ/figures/figures_36_2.jpg", "caption": "Figure 1: Top row. Under \u00b5P, (left) the sharpness dynamics are largely identical for the whole training dynamics across different widths, phenomenon that we call Super Consistency. The dashed horizontal lines are the Edge of Stability thresholds. Center: The loss dynamics are similar early in training, but accumulate finite-size effects over time, thus violating Super Consistency. Right: The learning rate transfers from small to large model, suggesting that the loss landscape is Super Consistent across different model sizes. Bottom row. Under NTK parameterization (NTP), the sharpness dynamics show large discrepancies. Also, the learning rate does not transfer. The architecture is a two-layer convolutional network trained on CIFAR-10 with data augmentation, where the width corresponds to the number of filters in the convolution. (See App. J). Other parameters: B = 128, epochs = 50.", "description": "This figure compares the training dynamics of sharpness (largest eigenvalue of the Hessian), training loss, and learning rate for two different model scaling methods: \u00b5P and NTK.  Under \u00b5P scaling, the sharpness remains consistent across different model widths, showing a phenomenon the authors call \"Super Consistency.\"  The training loss shows some consistency early on but diverges over time, while the learning rate transfers effectively between small and large models. In contrast, under NTK scaling, the sharpness dynamics differ significantly across widths, and learning rate transfer does not occur. The experiment uses a two-layer convolutional network trained on the CIFAR-10 dataset.", "section": "Super Consistency of the Optimization Landscape"}]