[{"type": "text", "text": "Super Consistency of Neural Network Landscapes and Learning Rate Transfer ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Lorenzo Noci\u22171 Alexandru Meterez\u22173 4 5 Thomas Hofmann 1 Antonio Orvieto 2 3 4 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, there has been growing evidence that if the width and depth of a neural network are scaled toward the so-called rich feature learning limit $\\textstyle\\mu\\mathbf{P}$ and its depth extension), then some hyperparameters \u2014 such as the learning rate \u2014 exhibit transfer from small to very large models. From an optimization perspective, this phenomenon is puzzling, as it implies that the loss landscape is consistently similar across very different model sizes. In this work, we study the landscape through the lens of the loss Hessian, with a focus on its largest eigenvalue (i.e. the sharpness), and find that certain spectral properties under $\\mu\\mathrm{P}$ are largely independent of the size of the network, and remain consistent as training progresses. We name this property Super Consistency of the landscape. On the other hand, we show that in the Neural Tangent Kernel (NTK) and other scaling regimes, the sharpness exhibits very different dynamics at different scales. But what causes these differences in the sharpness dynamics? Through a connection between the Hessian\u2019s and the NTK\u2019s spectrum, we argue that the cause lies in the presence (for $\\mu\\mathrm{P}_{.}$ ) or progressive absence (for the NTK scaling) of feature learning. We corroborate our claims with a substantial suite of experiments, covering a wide range of datasets and architectures: from ResNets and Vision Transformers trained on benchmark vision datasets to Transformers-based language models trained on WikiText. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent trends in deep learning research have unmistakably shifted towards an increase in model sizes, with networks comprising of billions of parameters emerging as the standard [1]. However, as models enlarge, so does the cost incurred in hyperparameter tuning which has led researchers to look for ways to scale up the architecture \u2014 both in terms of width and depth \u2014 while preserving the optimal hyperparameters (such as the learning rate). ", "page_idx": 0}, {"type": "text", "text": "While there exist several ways (a.k.a parametrizations) to scale up the width and depth of the network, not all of them facilitate learning rate transfer. For standard deep learning practices, such as networks parametrized with LeCun/Kaiming initializations [2, 3], a significant shift in the optimal learning rate is usually observed as the width and the depth of the model are increased. Similarly, under the Neural Tangent Kernel (NTK) parametrization [4], which provides theoretical insights into the behavior of very wide neural networks during training, the optimal learning rate also varies as the width and depth of the network change. Alternatively, Yang and $\\operatorname{Hu}$ [5] and Yang et al. [6] propose the $\\mu\\mathrm{P}$ framework, designed to maximize the gradient update of the representations of the intermediate layers (i.e. feature learning) as the width increases. Under $\\mu\\mathrm{P}$ scaling, and its depth extension for residual networks Depth- $\\mu\\mathrm{P}$ [7, 8], it has been empirically demonstrated that the learning rate transfers across both width and depth. In Vyas et al. [9] it is observed that in feature learning parametrizations (e.g. $\\mu\\mathrm{P})$ ", "page_idx": 0}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/9fac308edb239d9a4c1c3747385a983744669397750743873b31cb736ae6220d.jpg", "img_caption": ["Figure 1: Top row. Under $\\mu\\mathrm{P}$ , (left) the sharpness dynamics are largely identical for the whole training dynamics across different widths, phenomenon that we call Super Consistency. The dashed horizontal lines are the Edge of Stability thresholds. Center: The loss dynamics are similar early in training, but accumulate finite-size effects over time, thus violating Super Consistency. Right: The learning rate transfers from small to large model, suggesting that the loss landscape is Super Consistent across different model sizes. Bottom row. Under NTK parameterization (NTP), the sharpness dynamics show large discrepancies. Also, the learning rate does not transfer. The architecture is a two-layer convolutional network trained on CIFAR-10 with data augmentation, where the width corresponds to the number of filters in the convolution. (See App. J). Other parameters: $B=128$ , epochs $=50$ . "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "the model\u2019s dynamics are consistent across model sizes, but for harder tasks or longer training times there are progressive and significant deviations across different model sizes. We give an example in Figure 1 (top center), where the training losses exhibits an increasing gap. The fact that the learning rate is exactly preserved, however, suggests that some properties of the landscape do not exhibit these finite-size deviations, and must be precisely preserved across different model sizes for the whole training trajectory. ", "page_idx": 1}, {"type": "text", "text": "Motivated by this, in the present work we identify the notion of Super Consistency to describe the properties of the neural network\u2019s loss landscape that are preserved across training as a function of the model width and depth, thus not accumulating finite-size effects. In particular, we analyze the landscape through the lens of the loss Hessian. It provides insights into the landscape\u2019s local curvature, and its structure for neural networks has been studied in several works [10\u201314]. Of great interest in optimization theory is the sharpness, i.e. the top Hessian eigenvalue, which for neural networks exhibit a rapid increase (progressive sharpening) towards a threshold called Edge of Stability $(E o S)$ [15, 16]. However, although a few works have provided early insights [10, 16, 9], the scaling properties of the sharpness and Hessian\u2019s dynamics under different scaling limits remain unexplored. In this work we first present evidence of Super Consistency in the Hessian\u2019s largest eigenvalues, which have been shown to control the curvature along the optimization subspace [17]. We then focus on the sharpness dynamics, and find that the presence (resp. absence) of Super Consistency correlates well with presence (resp. absence) of learning rate transfer under $\\mu\\mathrm{P}$ , NTP and other scaling limits. These results suggest that learning rate transfer happens in super consistent landscapes, as the geometry of the landscape does not significantly change with the network\u2019s size. ", "page_idx": 1}, {"type": "text", "text": "Then, we investigate the role of feature learning in the progressive sharpening phase, and argue that while in $\\mu\\mathrm{P}$ feature learning causes progressive sharpening to reach a width-independent sharpness, in the NTK regime the progressive lack of feature learning when the width is increased prevents the Hessian from adapting, and its largest eigenvalue from reaching the convergence threshold. ", "page_idx": 1}, {"type": "text", "text": "More concretely: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We define Super Consistency, and show that under $\\mu\\mathrm{P}$ and Depth- $\\cdot\\mu\\mathrm{P}$ it holds for the largest eigenvalues of the loss Hessian (Fig. 2), which converge to a largely width-independent threshold and remains there for the rest of training. On the other hand, we show that other quantities, such as the training loss and the NTK eigenvalues accumulate significant finite-size effects. We quantify the rate of divergence of these quantities with power law fits (Fig. 3). ", "page_idx": 2}, {"type": "text", "text": "\u2022 We analyze the relationship between Super Consistency of the sharpness and learning rate transfer across $\\mu\\mathrm{P}$ , Depth- $\\cdot\\mu\\mathrm{P}$ , NTP and other parametrizations (Fig. 1, Fig. 4 and Sec. B). For $\\mu\\mathrm{P}$ and Depth- $\\mathbf{\\nabla}\\mu\\mathbf{P}$ , which do transfer, the sharpness stays super consistent, stabilizing to a threshold (Fig. 1, top left), which in some cases corresponds Edge of Stability [16], and oscillates around it for a sustained period of training time. On the other hand, under NTP, Standard Parametrization (SP), or Depth- $\\mathbf{\\nabla}\\mu\\mathbf{P}$ with multiple layers per residual block, the sharpness dynamics significantly separate during training for different widths, albeit in different ways. Also, here we do not observe transfer. \u2022 We reproduce some of these results at realistic scale, including ResNets and Vision Transformers (ViTs) trained on Imagenet and GPT-2 on text data. Also, we analyze the effect of batch size, learning rate warm-up, and long training times. \u2022 In Sec. 4.1 we show that the progressive sharpening phase is mainly driven by the NTK\u2019s largest eigenvalue, which is asymptotically fixed to its initial value for NTP, while it evolves at any width under $\\mu\\mathrm{P}$ . In Sec. 5 we provide intuition with a theoretical analysis on a two-layer linear network. ", "page_idx": 2}, {"type": "text", "text": "Finally, in Sec. 6 we discuss to what extent Super Consistency of these properties explains learning rate transfer, and the relevance of our results in the existing literature on optimization and scaling limits. Due to page limitations, we defer the discussion on related work to the appendix (App. A). ", "page_idx": 2}, {"type": "text", "text": "2 Background and Definitions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider a neural network with residual connections, defined by the following recursive equations over the layer indexes $\\ell\\in[L]$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nh^{\\ell+1}(x)=\\tau h^{\\ell}(x)+\\frac{1}{\\sqrt{N}L^{\\alpha}}W^{\\ell}\\phi(h^{\\ell}(x)),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $N$ and $L$ are the width and depth of the network, $W^{\\ell}\\in\\mathbb{R}^{N\\times N}$ for $\\ell=1,\\ldots,L-1$ , and $\\tau$ is a factor that enables $\\left.\\tau\\right.=1)$ ) or disables $\\tau\\:=\\:0)$ ) the skip branch. We denote the output with $\\begin{array}{r}{f(x)=\\frac{1}{\\gamma}W^{L}\\phi(h^{L}(x))}\\end{array}$ , where $W^{L}\\in\\mathbb{R}^{1\\times N}$ and $\\gamma$ scales the network output. Similarly, $\\alpha$ has the role of interpolating between different depth limit regimes. At the first layer, we define $\\begin{array}{r}{h^{1}(x)=\\frac{1}{\\sqrt{D}}W^{0}x}\\end{array}$ , where $\\breve{W}^{0}\\in\\mathbb R^{N\\times D}$ . All the weights $\\theta=\\{\\overline{{W}}^{\\ell}\\}_{l=0}^{L}$ are initialized independently from ${\\mathcal{N}}(0,1)$ and we denote with $P$ the total number of parameters. We stress that the fully connected layer can be replaced with any type of layer (our experiments include convolutional and attention layers). Given a dataset $\\bar{D}=\\bar{\\{(x_{\\mu},y_{\\mu})\\}}_{\\mu=1}^{|\\bar{D}|}$ of datapoints $\\boldsymbol{x}_{\\mu}\\in\\mathbb{R}^{D}$ and labels $y_{\\mu}\\in\\mathbb{R}$ , we train the network with stochastic gradient descent (SGD) with batch size $B$ and learning rate $\\eta\\in\\mathbb R$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\theta_{t+1}=\\theta_{t}-\\eta\\sum_{\\mu=1}^{B}\\nabla_{\\theta}\\mathcal{L}(f_{t}(x_{\\mu})),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the loss $\\mathcal{L}$ is a twice differentiable loss function. Defining $f_{t}:=\\,(f_{t}(x_{\\mu}))_{\\mu\\in[|\\mathcal{D}|]}\\,\\in\\,\\mathbb{R}^{|\\mathcal{D}|}$ to be the vector of network\u2019s outputs at time $t$ , if one considers continuous time, the corresponding gradient descent dynamics in function space $d f_{t}/d t$ take the following form [18]: $\\begin{array}{r}{\\frac{d f_{t}}{d t}\\;=\\;-\\Theta(f_{t})\\Delta(f_{t})}\\end{array}$ , where $\\Delta(f_{t})_{i}\\;:=\\;\\partial\\mathcal{L}(f_{t}(x_{i}))/\\partial f_{t}(x_{i}),\\;i\\;\\in\\;[|\\mathcal{D}|]$ is the vector of residuals, and $\\Theta(f_{t})_{i j}\\;:=\\;\\langle\\nabla_{\\theta}f_{t}(x_{i}),\\nabla_{\\theta}f_{t}(x_{j})\\rangle$ for $i,j\\,\\in\\,[|\\mathcal{D}|]$ is the Neural Tangent Kernel (NTK). ", "page_idx": 2}, {"type": "text", "text": "Infinite Width. The parameters $\\gamma,\\alpha,\\eta\\,\\in\\,\\mathbb{R}$ determine the nature of the scaling limit. If $\\gamma=$ $\\gamma_{0},\\eta=\\eta_{0}$ are $\\mathcal{O}(1)$ constants with respect to $N,L$ (neural tangent parameterization, or NTP), then the network enters the NTK regime [4]. Here, in the limit of infinite width, the NTK remains constant to its value at initialization throughout training, i.e. $\\Theta(f_{t})=\\Theta(f_{0})$ for all $t\\geq0$ . Thus, the network\u2019s dynamics become equivalent to a linear model trained on the first order term of the Taylor expansion of the model at initialization [19]. The fact that the NTK is fixed to its value at initializatio\u221an is associated with the lack of feature learning of the model in the large width limit. If $\\gamma=\\gamma_{0}\\sqrt{N}$ , and $\\eta=\\eta_{0}\\gamma^{2}$ ${\\big/}\\mu\\mathrm{P},$ or mean-field parameterization), the features evolve in the limit (i.e. the NTK $\\Theta(f_{t})$ evolves), and the richer model\u2019s dynamics can be described using either Tensor Programs [5] or dynamical mean field theory [20]. Under $\\mu\\mathrm{P}$ , Yang et al. [6] show that the learning rate $\\eta_{0}$ as well as other hyperparameters transfer across width, in contrast to kernel limits, which we reproduce for our residual network in Fig. 1. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Infinite Depth. If on top of the $\\mu\\mathrm{P}$ framework, the residual branches are scaled with $\\alpha=1/2$ (Depth- $\\mathbf{\\nabla}\\mu\\mathbf{P}_{,}$ ), then Bordelon et al. [7] and Yang et al. [8] show that the infinite width dynamics also admit a feature-learning infinite depth limit. Under Depth- $\\mu\\mathrm{P}$ , the learning rate $\\eta_{0}$ transfers with both width and depth. In this paper, we compare NTP and $\\mu\\mathrm{P}$ regimes as the width is increased, and show that our results extend to depth-scaling using the Depth- $\\mathbf{\\nabla}\\cdot\\mu\\mathbf{P}$ model. We summarize the feature learning parametrizations and report Depth- $\\cdot\\mu\\mathrm{P}$ for Adam in Appendix K. ", "page_idx": 3}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/e678e8e46a2c5bd80ee35c7c4e5b859b7f1a3586ec82f82c2987520b3a0dd27d.jpg", "img_caption": ["(a) Largest Hessian eigenvalues - $\\mu\\mathrm{P}$ "], "img_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/a69689e8e23d001065b0a02a5e22ea2f64b535b367f80aefd605df8b18ef8be4.jpg", "img_caption": ["(b) Largest NTK eigenvalues - $\\mu\\mathrm{P}$ "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: (a) The top Hessian eigenvalues exhibit a progressive increase to a threshold, with larger eigenvalues showing precise Super Consistency, while lower eigenvalues show finite-size accumulation at small width in the initial phase of training. (b) Top eigenvalues of the NTK matrix $\\Theta$ . As opposed to the top eigenvalues of the Hessian, these exhibit evident finite-size accumulation during training. Model: 3-layer ConvNet, $\\tau=0$ , $\\eta_{0}=0.7$ (optimal). Details in Sec. J. ", "page_idx": 3}, {"type": "text", "text": "3 Super Consistency of the Optimization Landscape ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this work, we analyze the landscape through the lens of the preconditioned Hessian $\\gamma^{2}H_{t}$ , where $\\begin{array}{r}{H_{t}:=\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta_{t}):=\\dot{\\sum}_{\\mu}\\,\\nabla_{\\theta}^{2}\\mathcal{L}(f_{t}(x_{\\mu}))\\in\\mathbb{R}^{P\\times P}}\\end{array}$ , as $\\theta_{t}$ evolves with gradient descent. The Hessian is a key object in optimization theory [21], information geometry [22, 23], and deep learning theory [17, 24, 16, 13] and its relation to optimal step sizes is often used to design second-order optimizers [23, 25\u201327]. In Figure 1, we can observe that learning rate transfer correlates with strong alignment across model sizes of the Hessian top eigenvalue dynamics, a property which we name Super Consistency. The choice of the preconditioning factor $\\gamma^{\\underline{{\\breve{2}}}}$ ensures the right scaling with respect to the width $N$ , as the theory will justify. We also provide an intuition and an extension to Adam in Appendix. J.1. Unless stated otherwise, every experiment is conducted with the this preconditioning factor $\\gamma^{2}$ set according to the corresponding parametrization. ", "page_idx": 3}, {"type": "text", "text": "More concretely, Super Consistency refers to when certain aspects of the loss landscape and of the predictor $S_{N}(t)$ (in this paper $S_{N}(\\dot{t})$ refers to the NTK\u2019s and loss Hessian\u2019s eigenvalues or the loss itself) exhibit the following two properties: ", "page_idx": 3}, {"type": "text", "text": "\u2022 At realistically large $N$ , $S_{N}(t)$ does not deviate significantly from its limit $S_{\\infty}(t):=$ $\\scriptstyle\\operatorname*{lim}_{N\\to\\infty}S_{N}({\\acute{t}})$ . This is what is referred to as consistency in Vyas et al. [9]. \u2022 $S_{N}(t)$ does not accumulate significant finite-size effects over time, i.e. the curves of $S_{N}(t)$ and $S_{\\infty}(t)$ remain close over a sustained period of training. ", "page_idx": 3}, {"type": "text", "text": "With respect to the experiment illustrated in Fig. 1, notice that the curves of the loss (center) at different widths show progressive and significant deviations, thus violating Super Consistency. On the other hand, the sharpness dynamics for $\\mu\\mathrm{P}$ qualitatively exhibit little-to-no deviations. Also, notice that we assume existence of the limit $\\begin{array}{r}{\\operatorname*{lim}_{N\\to\\infty}S_{N}(t)}\\end{array}$ . For those parametrizations (e.g. standard parametrization[6]) that do not have a well-defined limit, $S_{N}(t)$ diverges at large $N$ and Super Consistency is trivially violated. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "We now turn to the analysis of the Hessian spectrum and observe the following: ", "page_idx": 4}, {"type": "text", "text": "Observation: in $\\mu P$ (and Depth- $\\mu P$ ), Hessian eigenvalues are super consistent along the optimisation trajectory. Smaller eigenvalues have progressively different dynamics. ", "page_idx": 4}, {"type": "text", "text": "In Fig. 2 (a), we train a residual network on CIFAR-10 (a 10 classes image classification task) using cross-entropy loss, and show super consistent dynamics of three of the top $k\\,=\\,10$ eigenvalues. The choice of $k=10$ is guided by Gur-Ari et al. [17], where it is observed that stochastic gradient descent happens in a small subspace where the gradient lies in the space spanned by top $k$ Hessian eigenvectors (where $k$ is the number of classes). Thus, our results show that the curvature along the training trajectory is preserved super consistently at different scales, thus suggesting that the geometry of the landscape across the trajectory is preserved across model size. Lower order eigenvalues tend to accumulate finite-size effects in the first phase of training, and stabilize at lower thresholds for smaller width models. We discuss the effect of lower order eigenvalues through the Hessian trace in Appendix G. Finally, to make sure that Super Consistency holds along the training trajectory regardless of the tiny-subspace assumption, in Appendix I we track the directional sharpness, that measures the curvature along the gradient direction. ", "page_idx": 4}, {"type": "text", "text": "To give a quantitative measure to the finite-size accumulation property, we measure deviations over time by estimating the following quantity: ", "page_idx": 4}, {"type": "equation", "text": "$$\ng(t):=|S_{N}(t)-S_{\\infty}(t)|.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "When $g(t)$ increases over time (up to fluctuations), Super Consistency is violated. We illustrate this in Fig. 3 $(\\flat,\\mathtt{c})$ , where we compute the left hand side of Eq. 3 for the loss $\\mathcal{L}(f_{t})$ and the NTK\u2019s largest eigenvalue $\\lambda_{m a x}(\\Theta)$ . To estimate the infinite width limit, we use a very large-width model as a proxy. Notice how under $\\mu\\mathrm{P}$ the the loss dynamics progressively diverge from the infinite width model, indicating a finite-size accumulation over time. The same holds for $\\lambda_{m a x}(\\Theta)$ . To study the rate of divergence $g(t)$ , we fit a power law of the form $y=a t^{\\beta}$ to the observations. A larger $\\beta$ indicates a higher divergence rate. Notice how $\\beta\\,>\\,0.6$ for the loss, and $\\beta\\approx2$ for $\\lambda_{m a x}(\\Theta)$ , indicating quadratic divergence. In comparison, in Fig. 3 (left), we show Super Consistency of the sharpness, in that finite-size effects are not accumulated over time (10 epochs). Finally, both in Fig. 2 and 3 notice how the sharpness is not just converging in width, but in fact width-independent. This might be due to the fact that the threshold is a stable attractor of the dynamics [28]. ", "page_idx": 4}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/9dfb9a4d17141ede019166bc2b83b0e76a44c73d6371030a41fafe196f46fd72.jpg", "img_caption": ["Figure 3: (a) Convergence rate of the sharpness at finite width $N$ to the infinite limit proxy. Note that the distance approaches 0 as the training time increases. (b) Convergence rate of the loss at finite width $N$ to the infinite limit proxy. Note that the loss accumulates finite-size effects over time and the distance to the proxy increases. (c) Convergence rate of the top NTK eigenvalues over time to the infinite limit proxy. Similar to the loss, this also accumulates finite-size effects over time. Details: infinite limit proxy is width 4096, model is ConvNet, $\\tau=0$ , $\\eta_{0}=0.7$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4 Super Consistency and Learning Rate Transfer ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Sharpness and Edge of Stability. We now focus on the sharpness $\\lambda:=\\lambda_{\\operatorname*{max}}(\\gamma^{2}H)$ , defined as the largest eigenvalue of the Hessian. In the theory of smooth convex [21], nonconvex [29], and stochastic [30] optimization, the sharpness plays a crucial role in in the guarantees of convergence of gradient methods and selection of the optimal step size. For instance, for a quadratic objective, $\\lambda_{t}=\\lambda$ is constant and gradient descent would diverge if the learning rate satisfies $\\eta_{0}>2\\bar{/}\\lambda$ , and training speed is maximized for $\\eta_{0}=1/\\lambda$ (LeCun et al. [2], page 28). Beyond this classical example, which assumes constant Hessian, the descent lemma [21] states that $\\dot{\\mathcal{L}}(\\theta_{t+1})\\,\\leq\\,\\mathcal{L}(\\theta_{t})$ if $\\begin{array}{r}{\\eta\\le\\frac{2}{\\beta}}\\end{array}$ where $\\beta:=\\operatorname*{sup}_{\\theta}\\|\\nabla^{2}L(\\theta)\\|_{2}$ , and $\\lVert\\nabla^{2}L({\\boldsymbol{\\theta}})\\rVert_{2}$ is the sharpness at $\\theta$ . When it comes to deep neural networks, $\\lambda_{t}$ is generally observed to increase during training (progressive sharpening): in the early phase of training it increases [31, 32] and then it decreases close to convergence [10]. Under full batch gradient descent training, the sharpness consistently rises above the $E o S$ threshold of $2/\\eta_{0}$ [16]. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Conditions for hyperparameter transfer. The empirical success of hyperparameter transfer crucially relies on the following two observations, constituing a \u201ctheoretical puzzle\u201d [8]. ", "page_idx": 5}, {"type": "text", "text": "1. The optimal learning rate is preserved across widths/depths, indicating very fast convergence with respect to the scaling quantity. 2. The models show consistent improvement in training speed with respect to the scaling quantity (i.e. there is a clear \u201cwider/deeper is better\u201d effect), indicating that the loss dynamics have not yet converged to the limiting behaviour predicted by the theory. ", "page_idx": 5}, {"type": "text", "text": "In this section, we study the role of Super Consistency in learning rate transfer. We focus on the dynamics of sharpness $\\lambda_{\\mathrm{max}}$ across training, due to its well-established connection to optimization theory and step size selection, as well as better computational tractability than the full Hessian spectrum. We provide extensive studies of other relevant spectral quantities (i.e. Hessian and NTK eigenvalues) in Appendix G. ", "page_idx": 5}, {"type": "text", "text": "Observation: in $\\mu P$ (and Depth- $\\cdot\\mu P_{\\cdot}$ ), the sharpness $\\lambda$ is super consistent along the training trajectory, while for NTP the sharpness decreases in width. This correlates with presence/absence of hyperparameter transfer. ", "page_idx": 5}, {"type": "text", "text": "In Fig. 1 we train a two-layer convolutional network under the $\\mu\\mathrm{P}$ and NTP scalings with cross entropy loss, while keeping track of the sharpness at fixed gradient step intervals. The top row shows the dynamics of $\\lambda$ . Notice how the sharpness\u2019 behaviour is qualitatively different in the two parameterizations: in $\\mu\\mathrm{P}$ it reaches a width-independent value which is close to the EoS threshold of $2/\\eta_{0}$ . On the other hand, in NTP we observe a progressive diminishing of the sharpness with width, as previously observed for Mean-Square-Error loss by Cohen et al. [16]. ", "page_idx": 5}, {"type": "text", "text": "We then study the effect of depth under the Depth- $\\mathbf{\\nabla}\\mu\\mathbf{P}$ model of Eq. 1. In Fig. 4 (left), we show that the sharpness\u2019 dynamics are also super consistent across depth, although progressively diminishing from the EoS threshold. This suggests that EoS is not necessary for the learning rate to transfer, but the consistency of the sharpness dynamics is. ", "page_idx": 5}, {"type": "text", "text": "Other feature learning parameterizations. Finally, we study the effect of other feature parameterizations that do not exhibit learning rate transfer. In particular, we study the Depth- $\\mathcal{\\mu}\\mathrm{P}$ scaling of the residual branches in residual networks with multiple layers per residual branch - denoted by $k$ (i.e. each branch has multiple weight matrices and non linearities). A typical example is the Transformer architecture, which has multiple layers per block in both the attention and fully connected blocks. This parameterization, although it learns features in the infinite depth limit, it is lazy within each residual branch [8, 7]. The results are in Fig. 4. Notice how the sharpness dynamics are not super consistent, in that they accumulate finite-size effects over time. We study other parametrizations, including those without a stable limit in Appendix B, showing compatible results with those presented here. The observation that sharpness dynamics exhibit greater consistency compared to loss dynamics suggests that under $\\mu\\mathrm{P}$ scaling, although models with larger capacity fit the data faster, the paths taken by various models through the optimization landscape show a surprisingly uniform curvature. ", "page_idx": 5}, {"type": "text", "text": "4.1 Feature Learning and Progressive Sharpening ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now study the effect of feature learning in the sharpness dynamics. Following the Gauss-Newton decomposition [25, 12], the Hessian can be decomposed as a sum of two matrices $H=\\mathcal{G}+\\mathcal{R}$ , where $\\mathcal{G}$ is the Gauss-Newton (GN) matrix and $\\mathcal{R}$ depends on the Hessian of the model. For MSE loss, ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\mathcal{G}}=\\sum_{i=1}^{|\\mathcal{D}|}{\\nabla}_{\\theta}f(x_{i}){\\nabla}_{\\theta}f(x_{i})^{\\top}=K^{\\top}K\\quad{\\mathrm{~and~}}\\quad{\\mathcal{R}}=\\sum_{i=1}^{|\\mathcal{D}|}{\\nabla}_{\\theta}^{2}f(x_{i})(y_{i}-f(x_{i})),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $K\\in\\mathbb{R}^{|\\mathcal{D}|\\times P}$ is a matrix where each row is $\\nabla_{\\theta}f(x_{i})$ (i.e. the Jacobian of $f(x_{i}))$ , and $y_{i}\\in\\mathbb{R}$ is the label. One can readily see that the NTK matrix can be written as $\\Theta(f_{\\theta})=K K^{\\top}$ , thus the ", "page_idx": 5}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/beda20667049166fcaaf5e8127dd704fe2a882d1e26b7bf441089e53bd2af63b.jpg", "img_caption": ["Figure 4: Depth- $\\mathbf{\\nabla}\\mu\\mathbf{P}$ extensions with top row showing transfer plots and bottom row the sharpness evolution. (a) ConvNets with 1 layer per block exhibit both hyperparameter transfer and sharpness Super Consistency. (b) ConvNets with 2 layers per block. The model has a lazy behavior within each block, and no transfer. The sharpness starts accumulating finite-size effects during training, violating Super Consistency. (c) ViTs also have $k>2$ blocks per layer by design, and thus have a similar behaviour. Details: (a), (b) are trained with SGD\u221a, with widths 128 and 32 respectively; (c) is trained with Adam, with the learning rate scaled by $1/\\sqrt{L}$ [8]. See Fig. 22 for convergence rates. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "NTK and $\\mathcal{G}$ share the same nonzero eigenvalues. In Figure 5, we show that under $\\mu\\mathrm{P}$ the sharpness evolution is dominated by the $\\mathcal{G}$ matrix consistently across different widths, while for NTP the sharpness evolution slows down when increasing the width. Since in the limit the NTK matrix is fixed for NTP, while it evolves with time for $\\mu\\mathrm{P},$ , these results provide further supporting evidence for the role of feature learning in the evolution of the hessian. While this argument strictly holds for MSE loss, it can be generalized to any twice differentiable loss function, albeit with some caveats. In Appendix D, we generalize the setting, analyze the cross-entropy loss and perform validating experiments, confirming the conclusions drawn here. Finally, in Appendix H, we show that our results remains valid in a random feature model, where the NTK matrix is fixed at initialization at any finite width. In Section 5 we revisit the above claims more precisely in a simplified setting, providing further intuition on the sharpness dynamics and learning rate transfer. ", "page_idx": 6}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/575c1450b7b829aeef4c86fd19d8353731035211b3dffe2f52970a2509765c0b.jpg", "img_caption": ["Figure 5: Evolution of the top eigenvalues of the Hessian components $\\mathcal{G}$ and $\\mathcal{R}$ for a two-layer linear network trained on random data under MSE loss. The vector field characterizes the evolution during training for a fixed learning rate. Top: $\\mu\\mathrm{P}.$ . Note how $\\mathcal{G}$ drives the initial change super consistently. Bottom: NTP. For wider networks the sharpening phase reduces, since the network is approaching the limit where the NTK is fixed to its value at initialization. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Large scale experiments. In App. F, we perform more experiments to validate the connection between the consistency of the sharpness\u2019 dynamics and learning rate transfers across datasets (TinyImageNet, Wikitext), architectures (ViT, GPT-2 [33]), and optimizers (Adam [34] and AdamW [35]). We find these results to be consistent with those in the main text. ", "page_idx": 6}, {"type": "text", "text": "End of training dynamics. In App. E.1 (Fig. 12), we study the width dependence of the sharpness at the late phase of training. It is well-known that for cross-entropy loss, a phase transition happens where the sharpness starts to decrease [16]. We found that even for $\\mu\\mathrm{P}$ this transition point is widthdependent, with a consequent slight shift in optimal learning rates during this late phase. Again, these results are in line with our results that super consistent sharpness facilitates transfer. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Batch size ablation. We repeat the experiment in Fig. 1 with increasing batch size, observing that the threshold is reached across all the tested batch sizes, thus not affecting learning rate transfers. For larger batches, a close-to-EoS threshold is reached across more learning rates. Results are summarized in Fig. 13 and 14 in App. E. ", "page_idx": 7}, {"type": "text", "text": "5 Case study: Two-Layer Linear Network ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now revisit and validate our intuition and empirical findings in Sec. 4 through the lens of a two-layer neural network with linear activations and $L2$ loss. Our purpose is to characterize the dynamics of $\\mu P$ and NTP at the edge of stability through the lens of a simple example that shares a similar phenomenology with the more complex scenarios observed in the last section (see Fig. 10, App. C). In particular, the theory justifies the preconditioned Hessian $\\gamma^{2}H\\propto N H$ as the right object of study when it comes to the sharpness computations (see Prop. 5.3). Also, it provides an intuition to the width-independent evolution of the sharpness. Our setting is similar to the one leading to the insightful analysis of $E o S$ in [28, 36]. Compared to these works, we do not limit the analysis to a single datapoint or to vanishing targets 1. ", "page_idx": 7}, {"type": "text", "text": "Notation and assumptions. Consider a dataset of $|\\mathcal D|$ datapoints in $D$ dimensions $X\\ \\ \\in$ $\\mathbb{R}^{|\\mathcal{D}|\\times D}$ ( $\\left\\lvert\\mathcal{D}\\right\\rvert\\ge D)$ , and labels generated through a latent ground-truth vector $\\boldsymbol{w}_{\\ast}\\,\\in\\,\\mathbb{R}^{D}$ , that is $Y=X w_{*}$ . The neural network we use here is parametrized by weights $W^{0}\\in\\mathbb{R}^{D\\times N}$ , $W^{1}\\in\\mathbb{R}^{N\\times1}$ , where $N$ is the width. To simplify the notation in our setting, we name $E:=W^{0}$ and $V:=W^{1}$ : $\\begin{array}{r}{f(X)=\\frac{1}{\\gamma\\sqrt{N D}}X E V,\\mathcal{L}(E,\\hat{V})\\overset{\\cdot}{=}\\frac{1}{2}\\|f(X)-Y\\|^{2}}\\end{array}$ . We initialize each entry of $E,V$ i.i.d. Gaussian with mean zero and variance 1. Recall that $\\gamma_{\\mathrm{NTP}}\\,=\\,1$ , $\\gamma_{\\mu P}\\,\\,=\\,\\sqrt{N}$ . We train with gradient descent (GD) with a learning rate $\\eta=\\eta_{0}\\gamma^{2}$ . Empirically, we observed (Fig. 10, App. C) that picking $|\\mathcal{D}|=D$ and data $X=I_{D}$ ( $I_{D}$ is the $D\\times D$ identity matrix) is sufficient to track most of the crucial features of $\\mu P/$ NTP explored in this paper, except the \u201cwider is better\u201d effect which here is less apparent due to the simple hypothesis class. The loss function reduces to: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\_E(E,V)={\\frac{1}{2}}\\left\\|w-w_{*}\\right\\|^{2},\\quad{\\mathrm{with}}\\quad w:={\\frac{1}{\\gamma{\\sqrt{N D}}}}E V.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Finally, we reparametrize the model by defining: ", "page_idx": 7}, {"type": "equation", "text": "$$\ne:=\\frac{1}{N D}E E^{\\top}\\in\\mathbb{R}^{D\\times D},\\;\\;v:=\\frac{1}{N D}V^{\\top}V\\in\\mathbb{R}_{\\geq0}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Note that using a learning rate $\\eta_{0}\\gamma^{2}$ when optimizing $\\mathcal{L}$ is equivalent to using a learning rate $\\eta_{0}$ when optimizing $\\gamma^{2}\\bar{\\mathcal{L}}$ . Next, we characterize how $e,v$ evolve through time, and give conclusions for $\\mu\\mathrm{P}.$ . ", "page_idx": 7}, {"type": "text", "text": "5.1 Dynamics and Edge of Stability in Latent Space ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now show that at any value of the width $N$ , under GD on the original network parameters $(E,V)$ , the dynamics of $w,e,v$ , can be described completely through a self-contained dynamical system in $(1+D+D^{2})$ dimensions. This property is surprising because the original dynamical system described by GD on the variables $E,V$ lives in $N(D+1)$ dimensions. Concretely, this means we can study the Hessian dynamics at different network widths in the same space. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.1 (Evolution Laws). Let $(E,V)$ evolve with $G D$ at stepsize $\\eta=\\eta_{0}\\gamma^{2}$ on the loss of Eq. 4. The evolution of $(w,e,v)$ is completely described by the following self-contained equation: let the $^+$ denote updated quantities, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle w^{+}=w-\\eta_{0}(v\\cdot I_{D}+e)(w-w_{*})+\\frac{\\eta_{0}^{2}\\gamma^{2}}{N D}(w w^{\\top}-w_{*}w^{\\top})(w-w_{*}).}}\\\\ {{\\displaystyle e^{+}=e+\\frac{\\eta_{0}\\gamma^{2}}{N D}\\left[-2w w^{\\top}+w_{*}w^{\\top}+w w_{*}^{\\top}\\right]+\\frac{\\eta_{0}^{2}\\gamma^{2}}{N D}\\left[v w w^{\\top}-v w_{*}w^{\\top}-v w w_{*}^{\\top}+v w_{*}w_{*}^{\\top}\\right].}}\\\\ {{\\displaystyle v^{+}=v+\\frac{\\eta_{0}\\gamma^{2}}{N D}\\left[-2w^{\\top}w+2w_{*}^{\\top}w\\right]+\\frac{\\eta_{0}^{2}\\gamma^{2}}{N D}\\left[w^{\\top}e w-2w_{*}^{\\top}e w+w_{*}^{\\top}e w_{*}\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "1If our dataset has cardinality 1, then the NTK is a scalar. If targets vanish, for 2-layer linear networks with $L2$ loss, NTP and $\\mu P$ induce the same loss on the parameters ( $\\ensuremath{\\gamma}$ cancels). ", "page_idx": 7}, {"type": "text", "text": "While the system above describes the evolution laws $(w_{k},e_{k},v_{k})\\,\\to\\,(w_{k+1},e_{k+1},v_{k+1})$ , the dynamics are influenced also by initialization. In Prop. C.1 in the Appendix, we show that the only dependence in width in the evolutions laws are in the initial conditions. ", "page_idx": 8}, {"type": "text", "text": "Last, by analyzing the stability of the dynamical system in Theorem 5.1, we can characterize the edge of stability using tools from dynamical systems [37]. First of all, we need the following Lemma, which implies that at the minimizer $\\boldsymbol{w}=\\boldsymbol{w}_{*})$ ), the Hessian has the same non-zero eigenvalues as the NTK $\\Theta$ , which only depends on $e$ and $v$ . ", "page_idx": 8}, {"type": "text", "text": "Lemma 5.2 (GN bound). Let $\\gamma^{2}\\nabla^{2}{\\mathcal{L}}={\\mathcal{G}}+{\\mathcal{R}}$ be Gauss-Newton decomposition2 (see Sec. 4.1) of the Hessian for the loss in Eq. 4, with $\\mathcal{G}=K^{\\top}K$ , where $K\\in\\mathbb{R}^{D\\times(N D+\\mathbf{\\hat{N}})}$ . Let us denote the NTK matrix $\\Theta=K K^{\\top}\\in\\mathbb{R}^{D\\times D}$ . Then ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\Theta(E,V)=e+v\\cdot I_{D}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and ", "page_idx": 8}, {"type": "equation", "text": "$$\n|\\lambda_{\\operatorname*{max}}[\\gamma^{2}\\nabla^{2}\\mathcal{L}(E,V)]-\\lambda_{\\operatorname*{max}}[\\Theta(E,V)]|\\leq\\sqrt{\\frac{\\gamma^{2}}{N D}}\\|w-w_{*}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We stress that this result implies that evolution of the NTK (i.e. feature learning) goes hand in hand with the evolution of the sharpness, as we empirically show in Sec. 4.1. We are now ready to state the result on the sharpness at convergence. ", "page_idx": 8}, {"type": "text", "text": "Proposition 5.3 (EoS). Let $(E,V)$ evolve with GD with stepsize $\\eta\\,=\\,\\eta_{0}\\gamma^{2}$ on the loss of Eq. 4 towards a minimizer $(E_{*},V_{*})$ . Assume the corresponding solution in latent space $(w_{*},e_{*},v_{*})$ is marginally stable 3. Then, $\\begin{array}{r}{\\lambda_{\\operatorname*{max}}[\\gamma^{2}\\nabla^{2}\\mathcal{L}(E_{*},V_{*})]=\\frac{2}{\\eta_{0}}\\pm\\frac{\\eta_{0}\\gamma^{2}\\|w_{*}\\|^{2}}{N D}.}\\end{array}$ . ", "page_idx": 8}, {"type": "text", "text": "Implications for NTP. Consider $\\gamma=1$ in Thm. 5.1. The dynamics of $(w,e,v)$ are width-dependent. Let us take $N\\rightarrow\\infty$ in the equation above to amplify this effect: the system becomes linear ", "page_idx": 8}, {"type": "equation", "text": "$$\nw^{+}=w-\\eta_{0}(v\\cdot I_{D}+e)(w-w_{*}),\\;\\;e^{+}=e,\\;\\;v^{+}=v.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "While $w$ evolves from $w_{0}$ as expected from standard NTK theory [4], $e,v$ stay clamped at initialization. Applying Lemma 5.2 with $\\gamma=\\mathcal{O}(1)$ , the Hessian and the NTK have the same largest eigenvalue at large width (at rate $O({\\sqrt{N}}))$ . This makes sense, as under NTP the predictor converges to a linear model in the large $N$ limit, and thus $\\mathcal{R}$ vanishes. Also, this proves that he sharpness has no dependency on the learning rate in the width limit (we observe this e.g. in Fig. 1 and throughout all our experiments). This derivation is also in line with our discussion in Sec. 4.1: we only have sharpening under feature learning, and for the same reason we cannot observe NTP at the edge of stability as $N\\rightarrow\\infty$ (see stepsize dependency in Prop. 5.3), as also noted empirically by [16]. ", "page_idx": 8}, {"type": "text", "text": "Implications for $\\mu\\mathrm{P}.$ The following result immediately follows by inspection of the equations in Thm 5.1, combined with Prop. C.1. ", "page_idx": 8}, {"type": "text", "text": "Corollary 5.4. Consider $\\mu P$ $\\langle\\gamma=\\sqrt{N})$ and let $(E,V)$ evolve with GD with stepsize $\\eta=\\eta_{0}\\gamma^{2}$ on the loss of Eq. 4. Then, the equations governing the evolution of $(w,e,v)$ (defined in Thm. 5.1) in latent space have no width dependency \u2013 this holds at any finite width and not just at the limit. Initialization of $(w,e,v)$ is instead width-dependent, yet the error from $N\\rightarrow\\infty$ case scales in expectation like $1/\\sqrt{N}$ . ", "page_idx": 8}, {"type": "text", "text": "The corollary shows that $\\mu\\mathrm{P}$ trajectories at different widths align in the latent space $(w,e,v)$ , albeit with a vanishing perturbation in the initial condition (see Prop. C.1). While NTP\u2019s dynamics for $e$ and $v$ become slower as the width increases, for $\\mu\\mathrm{P}$ their evolution laws are width independent. This implies that if the dynamics converge towards a minimizer for $e$ and $v$ , this will be at the sharpness value predicted by Prop. 5.3. Under $\\mu\\mathrm{P}$ , where $\\gamma^{2}\\propto N$ , this value will be width-independent, as Super Consistency would suggest. We stress that Prop. 5.3 characterizes the sharpness at convergence (i.e. at infinite time). At finite time, the\u221are is still a discrepancy between the $\\lambda_{\\operatorname*{max}}(\\Theta)$ and the sharpness of the order of the residual term $1/\\sqrt{D}\\|w-w^{*}\\|$ (Lemma 5.2). Finally, we stress that Prop. 5.3 prescribes the right scaling for the Hessian by including the preconditioning factor of $\\gamma^{2}$ . Thus, we do not prove that at any finite time, the whole sharpness trajectory is width-independent, nor we are estimating converge rates in $N$ at finite time. Indeed, there will be a finite-size dependence coming from the initial conditions. We leave a precise characterization of the whole sharpness dynamics across the training trajectory for future work. ", "page_idx": 8}, {"type": "text", "text": "6 Discussion & Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "On Feature Learning Parametrizations. In this paper, we have shown how certain properties of the loss Hessian evolve almost identically across training for different model sizes, and named this property Super Consistency. We have also compared the sharpness dynamics under different scaling limits and parameterizations, and related Super Consistency of the landscape to learning rate transfer. Beyond being able to distinguish feature learning (rich) and kernel (lazy) parametrization, we have also shown how other suboptimal feature learning parametrizations have sharpness dynamics violating Super Consistency through finite-size accumulations. This seems to suggest that Super Consistency of the landscape is an important discriminant when it comes to hyperparameter transfer beyond the rich/lazy regimes. We foresee that our paper could spark further research interest at the intersection between the scaling limits of neural networks and optimization theory. ", "page_idx": 9}, {"type": "text", "text": "On the NTK and Hessian dynamics. In Section 4.1 we have drawn the connection between progressive sharpening and NTK evolution in the early phase of training. However, Figure 3 (b), shows how the NTK eigenvalues at different widths accumulate finite-size effects over time and diverge from each other, while the Hessian eigenvalues are Super Consistent. This suggests that other forces are at play after progressive sharpening, such as Self-Stabilization [38]. In fact, progressive sharpening on one hand, and Self-Stabilization on the other, make the stability threshold a stable attractor of the sharpness dynamics. Gaining theoretical understanding for these complex interactions in the context of scaling limits is an exciting area of future research. ", "page_idx": 9}, {"type": "text", "text": "Design of Step-size Tuners. In most of our experiments, we rely on a constant step size $\\eta_{0}$ . However, an alternative is to use a step-size tuner, i.e. to automatically choose $\\eta_{0}$ based on some criteria of the local landscape [39]. Our results open directions into some possible investigations and design choices for new step size tuners. For instance, do step size tuners transfer with the width and depth of the architecture? Given our results on the role of warmup schedule to improve transfer, it seems plausible to design step size tuners that use EoS results to achieve optimal learning rate transfer under different parameterizations. ", "page_idx": 9}, {"type": "text", "text": "Limitations. One of the underlying assumptions of the argument presented here is that the sharpness is an important property of the landscape when it comes to step size selection. Indeed, the results in Cohen et al. [16] establish a more intricate relationship between sharpness and learning rate. We discuss this in Sec. A.2. Overall, our theory on Super Consistency does not exclude the existence of other factors that might influence the optimal learning rate. Hyperparameter transfer requires Super Consistency of the landscape, thus we expect other potential factors to have this property. Finally, we note that due to the high computational cost of Hessian estimation, we do not perform experiments at a larger scale than presented here. It would be interesting to see if Super Consistency still holds at an even larger scale. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors would like to thank Bobby He, Imanol Schlag, Dayal Kalra, Tiago Pimentel and Gregor Bachmann for providing insightful feedback on early versions of this manuscript. LN would also like to thank Blake Bordelon, Boris Hanin and Mufan Li for the stimulating discussions on the topic of scaling limits that helped inspiring this work. AO acknowledges the financial support of the Hector Foundation. LN acknowledges the support of a Google PhD fellowship. AM acknowledges the support of a Kempner Fellowship. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, Y. Du, C. Yang, Y. Chen, Z. Chen, J. Jiang, R. Ren, Y. Li, X. Tang, Z. Liu, P. Liu, J.-Y. Nie, and J.-R. Wen. \u201cA Survey of Large Language Models\u201d. In: arXiv preprint arXiv:2303.18223 (2023). URL: http://arxiv.org/abs/2303.18223.   \n[2] Y. LeCun, L. Bottou, G. B. Orr, and K.-R. M\u00fcller. \u201cEfficient backprop\u201d. In: Neural networks: Tricks of the trade. Springer, 2002, pp. 9\u201350.   \n[3] K. He, X. Zhang, S. Ren, and J. Sun. \u201cDelving deep into rectifiers: Surpassing human-level performance on imagenet classification\u201d. In: Proceedings of the IEEE international conference on computer vision. 2015, pp. 1026\u20131034.   \n[4] A. Jacot, F. Gabriel, and C. Hongler. \u201cNeural tangent kernel: Convergence and generalization in neural networks\u201d. In: Advances in neural information processing systems 31 (2018).   \n[5] G. Yang and E. J. Hu. \u201cTensor programs iv: Feature learning in infinite-width neural networks\u201d. In: International Conference on Machine Learning. PMLR. 2021, pp. 11727\u201311737.   \n[6] G. Yang, E. J. Hu, I. Babuschkin, S. Sidor, X. Liu, D. Farhi, N. Ryder, J. Pachocki, W. Chen, and J. Gao. \u201cTensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer\u201d. In: arXiv preprint arXiv:2203.03466 (2022).   \n[7] B. Bordelon, L. Noci, M. B. Li, B. Hanin, and C. Pehlevan. \u201cDepthwise hyperparameter transfer in residual networks: Dynamics and scaling limit\u201d. In: arXiv preprint arXiv:2309.16620 (2023).   \n[8] G. Yang, D. Yu, C. Zhu, and S. Hayou. \u201cTensor Programs VI: Feature Learning in InfiniteDepth Neural Networks\u201d. In: arXiv preprint arXiv:2310.02244 (2023).   \n[9] N. Vyas, A. Atanasov, B. Bordelon, D. Morwani, S. Sainathan, and C. Pehlevan. \u201cFeatureLearning Networks Are Consistent Across Widths At Realistic Scales\u201d. In: arXiv preprint arXiv:2305.18411 (2023).   \n[10] L. Sagun, L. Bottou, and Y. LeCun. \u201cEigenvalues of the hessian in deep learning: Singularity and beyond\u201d. In: arXiv preprint arXiv:1611.07476 (2016).   \n[11] L. Sagun, U. Evci, V. U. Guney, Y. Dauphin, and L. Bottou. \u201cEmpirical analysis of the hessian of over-parametrized neural networks\u201d. In: arXiv preprint arXiv:1706.04454 (2017).   \n[12] J. Martens. \u201cNew insights and perspectives on the natural gradient method\u201d. In: The Journal of Machine Learning Research 21.1 (2020), pp. 5776\u20135851.   \n[13] S. P. Singh, G. Bachmann, and T. Hofmann. \u201cAnalytic insights into structure and rank of neural network hessian maps\u201d. In: Advances in Neural Information Processing Systems 34 (2021), pp. 23914\u201323927.   \n[14] A. Orvieto, J. Kohler, D. Pavllo, T. Hofmann, and A. Lucchi. \u201cVanishing curvature and the power of adaptive methods in randomly initialized deep networks\u201d. In: arXiv preprint arXiv:2106.03763 (2021).   \n[15] A. Lewkowycz, Y. Bahri, E. Dyer, J. Sohl-Dickstein, and G. Gur-Ari. \u201cThe large learning rate phase of deep learning: the catapult mechanism\u201d. In: arXiv preprint arXiv:2003.02218 (2020).   \n[16] J. M. Cohen, S. Kaur, Y. Li, J. Z. Kolter, and A. Talwalkar. \u201cGradient descent on neural networks typically occurs at the edge of stability\u201d. In: arXiv preprint arXiv:2103.00065 (2021).   \n[17] G. Gur-Ari, D. A. Roberts, and E. Dyer. \u201cGradient descent happens in a tiny subspace\u201d. In: arXiv preprint arXiv:1812.04754 (2018).   \n[18] S. Arora, S. S. Du, W. Hu, Z. Li, R. R. Salakhutdinov, and R. Wang. \u201cOn exact computation with an infinitely wide neural net\u201d. In: Advances in neural information processing systems 32 (2019).   \n[19] J. Lee, L. Xiao, S. Schoenholz, Y. Bahri, R. Novak, J. Sohl-Dickstein, and J. Pennington. \u201cWide neural networks of any depth evolve as linear models under gradient descent\u201d. In: Advances in neural information processing systems 32 (2019).   \n[20] B. Bordelon and C. Pehlevan. \u201cSelf-consistent dynamical field theory of kernel evolution in wide neural networks\u201d. In: Advances in Neural Information Processing Systems 35 (2022), pp. 32240\u201332256.   \n[21] Y. Nesterov. Introductory lectures on convex optimization: A basic course. Vol. 87. Springer Science & Business Media, 2013.   \n[22] S. Amari. \u201cInformation geometry\u201d. In: Contemporary Mathematics 203 (1997), pp. 81\u201396.   \n[23] S.-I. Amari. \u201cNatural gradient works efficiently in learning\u201d. In: Neural computation 10.2 (1998), pp. 251\u2013276.   \n[24] B. Ghorbani, S. Krishnan, and Y. Xiao. \u201cAn investigation into neural net optimization via hessian eigenvalue density\u201d. In: International Conference on Machine Learning. PMLR. 2019, pp. 2232\u20132241.   \n[25] J. Martens. Second-order optimization for neural networks. University of Toronto (Canada), 2016.   \n[26] J. Martens and R. Grosse. \u201cOptimizing neural networks with kronecker-factored approximate curvature\u201d. In: International conference on machine learning. PMLR. 2015, pp. 2408\u20132417.   \n[27] A. Botev, H. Ritter, and D. Barber. \u201cPractical Gauss-Newton optimisation for deep learning\u201d. In: International Conference on Machine Learning. PMLR. 2017, pp. 557\u2013565.   \n[28] D. S. Kalra, T. He, and M. Barkeshli. \u201cUniversal Sharpness Dynamics in Neural Network Training: Fixed Point Analysis, Edge of Stability, and Route to Chaos\u201d. In: arXiv preprint arXiv:2311.02076 (2023).   \n[29] G. Garrigos and R. M. Gower. \u201cHandbook of convergence theorems for (stochastic) gradient methods\u201d. In: arXiv preprint arXiv:2301.11235 (2023).   \n[30] R. M. Gower, N. Loizou, X. Qian, A. Sailanbayev, E. Shulgin, and P. Richt\u00e1rik. \u201cSGD: General analysis and improved rates\u201d. In: International conference on machine learning. PMLR. 2019, pp. 5200\u20135209.   \n[31] S. Jastrzkebski, Z. Kenton, N. Ballas, A. Fischer, Y. Bengio, and A. Storkey. \u201cOn the relation between the sharpest directions of DNN loss and the SGD step length\u201d. In: arXiv preprint arXiv:1807.05031 (2018).   \n[32] S. Jastrzebski, M. Szymczak, S. Fort, D. Arpit, J. Tabor, K. Cho, and K. Geras. \u201cThe break-even point on optimization trajectories of deep neural networks\u201d. In: arXiv preprint arXiv:2002.09572 (2020).   \n[33] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. \u201cLanguage models are unsupervised multitask learners\u201d. In: OpenAI blog 1.8 (2019), p. 9.   \n[34] D. P. Kingma and J. Ba. \u201cAdam: A method for stochastic optimization\u201d. In: arXiv preprint arXiv:1412.6980 (2014).   \n[35] I. Loshchilov. \u201cDecoupled weight decay regularization\u201d. In: arXiv preprint arXiv:1711.05101 (2017).   \n[36] M. Song and C. Yun. \u201cTrajectory alignment: understanding the edge of stability phenomenon via bifurcation theory\u201d. In: arXiv preprint arXiv:2307.04204 (2023).   \n[37] E. Ott. Chaos in dynamical systems. Cambridge university press, 2002.   \n[38] A. Damian, E. Nichani, and J. D. Lee. \u201cSelf-stabilization: The implicit bias of gradient descent at the edge of stability\u201d. In: arXiv preprint arXiv:2209.15594 (2022).   \n[39] V. Roulet, A. Agarwala, and F. Pedregosa. \u201cOn the Interplay Between Stepsize Tuning and Progressive Sharpening\u201d. In: OPT 2023: Optimization for Machine Learning. 2023.   \n[40] M. Claesen and B. D. Moor. Hyperparameter Search in Machine Learning. 2015. arXiv: 1502.02127 [cs.LG].   \n[41] J. Bergstra and Y. Bengio. \u201cRandom search for hyper-parameter optimization.\u201d In: Journal of machine learning research 13.2 (2012).   \n[42] K. Jamieson and A. Talwalkar. \u201cNon-stochastic best arm identification and hyperparameter optimization\u201d. In: Artificial intelligence and statistics. PMLR. 2016, pp. 240\u2013248.   \n[43] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar. \u201cHyperband: A novel bandit-based approach to hyperparameter optimization\u201d. In: Journal of Machine Learning Research 18.185 (2018), pp. 1\u201352.   \n[44] J. Snoek, H. Larochelle, and R. P. Adams. \u201cPractical bayesian optimization of machine learning algorithms\u201d. In: Advances in neural information processing systems 25 (2012).   \n[45] J. Snoek, O. Rippel, K. Swersky, R. Kiros, N. Satish, N. Sundaram, M. Patwary, M. Prabhat, and R. Adams. \u201cScalable bayesian optimization using deep neural networks\u201d. In: International conference on machine learning. PMLR. 2015, pp. 2171\u20132180.   \n[46] A. Shaban, C.-A. Cheng, N. Hatch, and B. Boots. \u201cTruncated back-propagation for bilevel optimization\u201d. In: The 22nd International Conference on Artificial Intelligence and Statistics. PMLR. 2019, pp. 1723\u20131732.   \n[47] L. Franceschi, M. Donini, P. Frasconi, and M. Pontil. \u201cForward and reverse gradient-based hyperparameter optimization\u201d. In: International Conference on Machine Learning. PMLR. 2017, pp. 1165\u20131173.   \n[48] D. Maclaurin, D. Duvenaud, and R. Adams. \u201cGradient-based hyperparameter optimization through reversible learning\u201d. In: International conference on machine learning. PMLR. 2015, pp. 2113\u20132122.   \n[49] D. Yogatama and G. Mann. \u201cEfficient transfer learning method for automatic hyperparameter tuning\u201d. In: Artificial intelligence and statistics. PMLR. 2014, pp. 1077\u20131085.   \n[50] D. Stoll, J. K. Franke, D. Wagner, S. Selg, and F. Hutter. \u201cHyperparameter transfer across developer adjustments\u201d. In: arXiv preprint arXiv:2010.13117 (2020).   \n[51] V. Perrone, R. Jenatton, M. W. Seeger, and C. Archambeau. \u201cScalable hyperparameter transfer learning\u201d. In: Advances in neural information processing systems 31 (2018).   \n[52] S. Horv\u00e1th, A. Klein, P. Richt\u00e1rik, and C. Archambeau. \u201cHyperparameter transfer learning with adaptive complexity\u201d. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2021, pp. 1378\u20131386.   \n[53] G. Yang and E. Littwin. \u201cTensor programs ivb: Adaptive optimization in the infinite-width limit\u201d. In: arXiv preprint arXiv:2308.01814 (2023).   \n[54] B. Bordelon and C. Pehlevan. \u201cDynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks\u201d. In: arXiv preprint arXiv:2304.03408 (2023).   \n[55] S. Jelassi, B. Hanin, Z. Ji, S. J. Reddi, S. Bhojanapalli, and S. Kumar. \u201cDepth Dependence of muP Learning Rates in ReLU MLPs\u201d. In: arXiv preprint arXiv:2305.07810 (2023).   \n[56] S. Yaida. \u201cMeta-Principled Family of Hyperparameter Scaling Strategies\u201d. In: arXiv preprint arXiv:2210.04909 (2022).   \n[57] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. http : / / www . deeplearningbook.org. MIT Press, 2016.   \n[58] X. Zhu, Z. Wang, X. Wang, M. Zhou, and R. Ge. \u201cUnderstanding edge-of-stability training dynamics with a minimalist example\u201d. In: arXiv preprint arXiv:2210.03294 (2022).   \n[59] S. Arora, Z. Li, and A. Panigrahi. \u201cUnderstanding gradient descent on the edge of stability in deep learning\u201d. In: International Conference on Machine Learning. PMLR. 2022, pp. 948\u2013 1024.   \n[60] K. Ahn, J. Zhang, and S. Sra. \u201cUnderstanding the unstable convergence of gradient descent\u201d. In: International Conference on Machine Learning. PMLR. 2022, pp. 247\u2013257.   \n[61] J. M. Cohen, B. Ghorbani, S. Krishnan, N. Agarwal, S. Medapati, M. Badura, D. Suo, D. Cardoze, Z. Nado, G. E. Dahl, et al. \u201cAdaptive gradient methods at the edge of stability\u201d. In: arXiv preprint arXiv:2207.14484 (2022).   \n[62] G. Iyer, B. Hanin, and D. Rolnick. \u201cMaximal initial learning rates in deep relu networks\u201d. In: International Conference on Machine Learning. PMLR. 2023, pp. 14500\u201314530.   \n[63] L. N. Smith and N. Topin. \u201cSuper-convergence: Very fast training of neural networks using large learning rates\u201d. In: Artificial intelligence and machine learning for multi-domain operations applications. Vol. 11006. SPIE. 2019, pp. 369\u2013386.   \n[64] Y. Li, C. Wei, and T. Ma. \u201cTowards explaining the regularization effect of initial large learning rate in training neural networks\u201d. In: Advances in Neural Information Processing Systems 32 (2019).   \n[65] D. S. Kalra and M. Barkeshli. \u201cPhase diagram of early training dynamics in deep neural networks: effect of the learning rate, depth, and width\u201d. In: Thirty-seventh Conference on Neural Information Processing Systems. 2023.   \n[66] R. M. Neal. \u201cBayesian Learning for Neural Networks\u201d. PhD thesis. University of Toronto, 1995.   \n[67] J. Lee, Y. Bahri, R. Novak, S. S. Schoenholz, J. Pennington, and J. Sohl-Dickstein. \u201cDeep neural networks as gaussian processes\u201d. In: arXiv preprint arXiv:1711.00165 (2017).   \n[68] A. Garriga-Alonso, C. E. Rasmussen, and L. Aitchison. \u201cDeep convolutional networks as shallow gaussian processes\u201d. In: arXiv preprint arXiv:1808.05587 (2018).   \n[69] J. Hron, Y. Bahri, J. Sohl-Dickstein, and R. Novak. \u201cInfinite attention: NNGP and NTK for deep attention networks\u201d. In: International Conference on Machine Learning. PMLR. 2020, pp. 4376\u20134386.   \n[70] G. Yang. \u201cTensor programs ii: Neural tangent kernel for any architecture\u201d. In: arXiv preprint arXiv:2006.14548 (2020).   \n[71] L. Chizat, E. Oyallon, and F. Bach. \u201cOn lazy training in differentiable programming\u201d. In: Advances in neural information processing systems 32 (2019).   \n[72] L. Chizat and F. Bach. \u201cOn the global convergence of gradient descent for over-parameterized models using optimal transport\u201d. In: Advances in neural information processing systems 31 (2018).   \n[73] L. Chizat and F. Bach. \u201cImplicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss\u201d. In: Conference on Learning Theory. PMLR. 2020, pp. 1305\u2013 1338.   \n[74] S. Mei, T. Misiakiewicz, and A. Montanari. \u201cMean-field theory of two-layers neural networks: dimension-free bounds and kernel limit\u201d. In: Conference on Learning Theory. PMLR. 2019, pp. 2388\u20132464.   \n[75] S. Hayou, E. Clerico, B. He, G. Deligiannidis, A. Doucet, and J. Rousseau. \u201cStable resnet\u201d. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2021, pp. 1324\u20131332.   \n[76] L. Noci, S. Anagnostidis, L. Biggio, A. Orvieto, S. P. Singh, and A. Lucchi. \u201cSignal propagation in transformers: Theoretical perspectives and the role of rank collapse\u201d. In: Advances in Neural Information Processing Systems 35 (2022), pp. 27198\u201327211.   \n[77] S. Hayou and G. Yang. \u201cWidth and Depth Limits Commute in Residual Networks\u201d. In: arXiv preprint arXiv:2302.00453 (2023).   \n[78] S. Hayou. \u201cOn the infinite-depth limit of finite-width neural networks\u201d. In: arXiv preprint arXiv:2210.00688 (2022).   \n[79] B. Hanin and M. Nica. \u201cProducts of many large random matrices and gradients in deep neural networks\u201d. In: Communications in Mathematical Physics 376.1 (2020), pp. 287\u2013322.   \n[80] L. Noci, G. Bachmann, K. Roth, S. Nowozin, and T. Hofmann. \u201cPrecise characterization of the prior predictive distribution of deep ReLU networks\u201d. In: Advances in Neural Information Processing Systems 34 (2021), pp. 20851\u201320862.   \n[81] M. Li, M. Nica, and D. Roy. \u201cThe future is log-Gaussian: ResNets and their infinite-depthand-width limit at initialization\u201d. In: Advances in Neural Information Processing Systems 34 (2021), pp. 7852\u20137864.   \n[82] M. Li, M. Nica, and D. Roy. \u201cThe neural covariance SDE: Shaped infinite depth-and-width networks at initialization\u201d. In: Advances in Neural Information Processing Systems 35 (2022), pp. 10795\u201310808.   \n[83] L. Noci, C. Li, M. B. Li, B. He, T. Hofmann, C. Maddison, and D. M. Roy. \u201cThe shaped transformer: Attention models in the infinite depth-and-width limit\u201d. In: arXiv preprint arXiv:2306.17759 (2023).   \n[84] M. B. Li and M. Nica. \u201cDifferential Equation Scaling Limits of Shaped and Unshaped Neural Networks\u201d. In: arXiv preprint arXiv:2310.12079 (2023).   \n[85] B. Hanin and M. Nica. \u201cFinite depth and width corrections to the neural tangent kernel\u201d. In: arXiv preprint arXiv:1909.05989 (2019).   \n[86] S. Yaida. \u201cNon-Gaussian processes and neural networks at finite widths\u201d. In: Mathematical and Scientific Machine Learning. PMLR. 2020, pp. 165\u2013192.   \n[87] J. Zavatone-Veth, A. Canatar, B. Ruben, and C. Pehlevan. \u201cAsymptotics of representation learning in finite Bayesian neural networks\u201d. In: Advances in neural information processing systems 34 (2021), pp. 24765\u201324777.   \n[88] L. Chizat and P. Netrapalli. \u201cSteering Deep Feature Learning with Backward Aligned Feature Updates\u201d. In: arXiv preprint arXiv:2311.18718 (2023).   \n[89] A. Agarwala, F. Pedregosa, and J. Pennington. \u201cSecond-order regression models exhibit progressive sharpening to the edge of stability\u201d. In: arXiv preprint arXiv:2210.04860 (2022).   \n[90] J. Gilmer, B. Ghorbani, A. Garg, S. Kudugunta, B. Neyshabur, D. Cardoze, G. E. Dahl, Z. Nado, and O. Firat. \u201cA loss curvature perspective on training instabilities of deep learning models\u201d. In: International Conference on Learning Representations. 2021.   \n[91] K. Osawa, S. Ishikawa, R. Yokota, S. Li, and T. Hoefler. \u201cASDL: A Unified Interface for Gradient Preconditioning in PyTorch\u201d. In: arXiv preprint arXiv:2305.04684 (2023).   \n[92] Z. Yao, A. Gholami, K. Keutzer, and M. W. Mahoney. \u201cPyhessian: Neural networks through the lens of the hessian\u201d. In: 2020 IEEE international conference on big data (Big data). IEEE. 2020, pp. 581\u2013590.   \n[93] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. \u201cAn image is worth 16x16 words: Transformers for image recognition at scale\u201d. In: arXiv preprint arXiv:2010.11929 (2020).   \n[94] B. Bordelon, H. T. Chaudhry, and C. Pehlevan. \u201cInfinite Limits of Multi-head Transformer Dynamics\u201d. In: arXiv preprint arXiv:2405.15712 (2024). ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Related works 16   \nA.1 Learning Rate Transfer and Scaling Limits 17   \nA.2 Sharpness and Optimal Step Size 17 ", "page_idx": 14}, {"type": "text", "text": "B Experiments in the absence of a valid scaling limit 17 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 $\\mu\\mathrm{P}$ without $1/{\\sqrt{\\mathrm{depth}}}$ scaling of the residual branches 17   \nB.2 Standard Parameterization (SP) experiments 18   \nB.3 $\\mu\\mathrm{P}^{-}$ disabling residuals 18   \nB.4 $\\mu\\mathrm{P}$ experiments for full batch GD 19   \nC Analysis of a Two-Layer Linear Network 20   \nC.1 Relationship between sharpness and residual, and Gauss-Newton 20   \nC.2 Proof of Thm.5.1 22   \nC.3 Theory for Standard Parametrization (SP) 27 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "D Connection between the eigenvalues of the Hessian and NTK matrix 29 ", "page_idx": 14}, {"type": "text", "text": "E Late-time dynamics and batch size ablations 30   \nE.1 Late-time dynamics . 30   \nE.2 The effect of Batch Size and Data Augmentation 30   \nF Large-Scale experiments, more Datasets and Optimizers 31   \nF.1 GPT-2 experiments on WikiText . . 31   \nF.2 ConvNets Experiments on Larger Datasets and Adam(W) 32 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "G Time Evolution of other Spectral Quantities 33 ", "page_idx": 14}, {"type": "text", "text": "H Sharpness evolution in Random Feature Models 34 ", "page_idx": 14}, {"type": "text", "text": "I Directional sharpness 36 ", "page_idx": 14}, {"type": "text", "text": "J.1 Hessian Computation 37   \nJ.2 GPT-2 38   \nJ.3 Vision Transformers (ViTs) 38   \nJ.4 ResNet . 38   \nJ.5 Coordinate check for $\\mu\\mathrm{P}$ 38 ", "page_idx": 14}, {"type": "text", "text": "K Summary of Feature Learning Parametrizations 39 ", "page_idx": 14}, {"type": "text", "text": "A Related works ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Hyperparameter search. Hyperparameter tuning [40] has been paramount in order to obtain good performance when training deep learning models. With the emergence of large language models, finding the optimal hyperparameters has become unfeasible in terms of computational resources. Classical approaches based on grid searching [41] over a range of learning rates, even with improvements such as successive halving [42, 43] require training times on the order of weeks on large datasets, making them largely impractical. Bayesian optimization [44, 45] methods aim to reduce the search space for the optimal HPs by choosing what parameters to tune over in the next iteration, based on the previous iterations. A different approach for HP search involves formulating the problem as an optimization over the HP space and solving it through gradient descent [46\u201348]. ", "page_idx": 15}, {"type": "text", "text": "Learning rate transfer. While meta-learning and neural architecture search (NAS) literature provide methods for finding the optimal learning rate in neural networks, these are still dependent on the model size and can become costly for larger architectures. Parameter transfer methods have been studied in the literature, for learning rate transfer across datasets [49], and in the context of reusing previous hyperparameter optimizations for new tasks [50]. Perrone et al. [51] and Horv\u00e1th et al. [52] proposed methods based on Bayesian optimization for hyperparameter transfer in various regimes. Yang and Hu [5], Yang et al. [6, 8], and Yang and Littwin [53] used the tensor programs framework to derive a model parameterization technique which leads to feature learning and learning rate transfer through width and depth, termed $\\mu\\mathrm{P}.$ Bordelon et al. [7] used the DMFT framework [20, 54] to derive the Depth- $\\cdot\\mu\\mathrm{P}$ limit, leading to optimal learning rate transfer across depth is models with residual connections. For MLPs, Jelassi et al. [55] the depth dependence of the $\\mu\\mathrm{P}$ learning rates. Finally, conditions on the networks and its dynamics to achieve hyperparameter transfer have been analyzed in Yaida [56]. ", "page_idx": 15}, {"type": "text", "text": "Training on the Edge of Stability. The choice of learning rate has been coined as one of the important aspects of training deep neural networks [57]. One phenomenon studied in the optimization literature is the fact that under gradient descent (GD), neural networks have sharpness close to 2/step size, termed the Edge of Stability (EoS) [16, 38, 58\u201360], with the extension to Adam being introduced by Cohen et al. [61]. Iyer et al. [62] study the maximal learning rate for ReLU networks and establish a relationship between this learning rate and the width and depth of the model. Smith and Topin [63] and Y. Li et al. [64] study the effect of large initial learning rates on neural network training. Lewkowycz et al. [15], Kalra and Barkeshli [65], and Kalra et al. [28] show empirically that the learning rate at initialization can lead the \u201ccatapult\u201d phenomena. Song and Yun [36] analyze the trajectory of gradient descent in a two-layer fully connected network, showing that the initialization has a crucial role in controlling the evolution of the optimization. Finally, early evidence of Super Consistency was shown in Fig. 3 of Sagun et al. [11], where it was shown that at end of training the Hessian spectrum is similar across model sizes. ", "page_idx": 15}, {"type": "text", "text": "Scaling limits The study of scaling limits for neural network was pioneered by the seminal work of Neal [66] on the equivalence between infinite width networks and Gaussian Processes, and more recently extended in different settings and architectures [67\u201369] and under gradient descent training, leading to the Neural Tangent Kernel (NTK) [4, 19, 18, 70] or \"lazy\" limit [71]. The rich feature learning infinite width limit has been studied using different frameworks, either Tensor programs [5] or DMFT [20, 54]. The main motivation behind these works is to maximize feature learning as the width is scaled up. In the two-layer case, the network\u2019s infinite-width dynamics have also been studied using other t\u221aools, such as optimal transport [72, 73] or mean-field theory [74]. The infinite depth analysis of $1/{\\sqrt{\\mathrm{depth}}}$ -scaled residual networks was introduced in [75], and later applied to the Transformers (used here) in [76]. The infinite width-and-depth limit of this class of residual networks have been studied in Hayou and Yang [77] and Hayou [78]\u221a at initialization and in Bordelon et al. [7], Yang et al. [8] for the training dynamics. Without the $1/{\\sqrt{\\mathrm{depth}}}$ -scaling, the joint limit has mainly been studied at initialization [79\u201384]. Deviations from the infinite dynamics can also be studied using perturbative approaches [85\u201387]. Finally, it is worth mentioning that a method to control and measure feature learning have been recently proposed in Chizat and Netrapalli [88]. ", "page_idx": 15}, {"type": "text", "text": "Scaling limits and Hyperparameter transfer It is worth mentioning that while for width limits feature learning is a clear discriminant between NTK and mean-field limits, the issue becomes more subtle with depth limits. In fact, there exist a family of infinite depth limits that admit feature learning $(\\alpha\\in[1/2,1]$ in Eq. 1, with an appropriate depth correction to the learning rate). [8] classifies the depth limits in terms of the feature diversity exponent, a measure that quantifies the diversity between the features of different layers. With respect to this measure, $\\alpha=1/2$ is the one that maximizes it. Bordelon et al. [7] try to quantify the finite-size approximation to the infinite (continuous) model, arguing that hyperparameter transfer is achieved faster for models that have lower discretization error to the infinite model\u2019s dynamics. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "A.1 Learning Rate Transfer and Scaling Limits ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Our results on the early training dynamics complement the analysis of Vyas et al. [9] and Kalra et al. [28] on the consistency of the loss and the sharpness in the first few steps of training. However, we further extend it, showing how while the loss curves depart later in training (with \u201cwider/deeper being better\"), the consistency of the sharpness\u2019 dynamics is maintained longer in time and does not accumulate finite-size effects over time. Furthermore, our work explains some of the experimental results on the lack of progressive sharpening under the NTP parameterization [16] (Appendix H), by relating it to the lack of feature learning and the consequent absence of hyperparameter transfer. Our results on the role of feature learning are also compatible with [89], where it is theoretically shown that the non linear dynamics exhibits progressive sharpening in a simple non linear model. Our results are also in line with the recent experiments on the evolution of the sharpness for ReLU MLPs trained with gradient descent under $\\mu\\mathrm{P}$ [28] (e.g. Figure 1). We extend these results to include the width (in)-dependence behaviour of the convergent stability point, a crucial aspect for successful hyperparameter transfer. ", "page_idx": 16}, {"type": "text", "text": "Finally, It is worth mentioning that there exist a family of infinite depth limits that admit feature learning $\\left(\\alpha\\,\\in\\,[1/2,1\\right]$ in Eq. 1, with an appropriate depth correction to the learning rate). Most of our experiments focus on mod\u221aels with a single layer per residual block, which exhibit transfer more consistently under the $\\mu\\mathrm{P}\\cdot$ depth setting (i.e. $\\alpha=1/2$ ) and it is considered optimal in terms of feature diversity across blocks [8]. We adopt the same depth scaling with our experiments with Transformers \u2014 which have multiple layers per block. Under this setting, both Bordelon et al. [7] (e.g. Fig. 3) and Yang et al. [8] (Fig. 16) show good (albeit at time slightly worse) transfer across depth with Adam. More broadly, we expect that a scaling limit that admits learning rate transfer would find a corresponding width/depth-independent behaviour in the sharpness dynamics. ", "page_idx": 16}, {"type": "text", "text": "A.2 Sharpness and Optimal Step Size ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "One of the underlying assumptions to explain why super consistent sharpness causes hyperparameter transfer is that the sharpness influences the optimal learning rate. Although this is well-established in classic optimization [2], Edge of Stability tells a story of a more intricate relationship: the choice of the learning rate also influences the sharpness. Also recent works on step size tuners show evidence of a more complex interaction in the joint dynamics of step size and sharpness [39], a relation that still has to be fully understood and could be leveraged to design better step size tuners. However, the sharpness is still arguably a very good proxy for understanding the trainability of the model at large learning rates. For instance, Gilmer et al. [90] argue that maintaining a small sharpness in neural network optimization favors training at large learning rates. Interestingly, in Cohen et al. [16] (Appendix F) it is shown that choosing the step size as $1/\\lambda_{t}$ at any step is suboptimal. The reason could be that gradient directions are not aligned with the largest curvature. Alternatively, one could claim there is another, more sophisticated, functional relationship between sharpness and the maximum step size allowed. Indeed, the \u201ccatapult mechanism\u201d[15] shows the maximum stable learning rate is $c_{a r c h}/\\lambda_{0}$ , where $c_{a r c h}$ depends on the architecture. On a related note, Gur-Ari et al. [17] shows that after the first few steps, the gradient direction is aligned with the top Hessian eigenvectors, underlying once again the importance of the sharpness and the first few eigenvalues in this phase of training, which we show here to be super consistent (Fig. 2). ", "page_idx": 16}, {"type": "text", "text": "B Experiments in the absence of a valid scaling limit ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 $\\mu\\mathbf{P}$ without $1/{\\sqrt{\\mathbf{depth}}}$ scaling of the residual branches ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We first study the effect of increasing the depth in a model without the $1/\\sqrt{L}$ -scaling of the residual branches, which results in exploding activations (and sharpness) as the depth increases. See Figure ", "page_idx": 16}, {"type": "text", "text": "6, where we first show that (top row) there is no learning rate transfer, and there is no consistent sharpness either. For instance, notice how the sharpness for the model of depth 24 quickly reaches its EoS value at its optimal learning rate of 0.068 (and larger models, e.g. depth 36 diverge). On the other hand, for the same learning rate the smaller depth models struggle to reach EoS at the same speed. This suggests that for the smaller-depth model, the learning rate can be safely increased. Indeed, the optimal learning rate for the smaller depth model is significantly larger, where the model reaches the EoS value very fast (optimal lr: 0.53). The larger depth models are not trainable at these larger learning rates. ", "page_idx": 17}, {"type": "text", "text": "In the bottom row of Figure 6, we show that by adding a linear warmup scheduler in the first phase of training, the network is progressively more trainable at larger step-sizes, and the sharpness reaches its stability threshold at any width/depth that is trainable [90]. This suggests that the diverging initial sharpness can be counteracted with initial small learning rates. Also, we notice a better transfer, indicating that a sustained period of depth-independent sharpness can help learning rate transfer. On the other hand we do not observe good transfer after the first epoch, which correlates with the fact that the sharpness is not consistent due to a blow up with depth at initialization. ", "page_idx": 17}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/f7a0e73bbb7c5f8f04daef2fb913c1e34816dc02115d8db80616d89ee35747c0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 6: $\\mu\\mathrm{P}$ without $1/{\\sqrt{\\mathrm{depth}}}$ scaling of the residual branches for ConvNets trained using SGD on CIFAR10. (Top row): no warmup. (Bottom row): 200 steps of linear warmup. Right column: sharpness dynamics in the first epoch for the optimal learning rates for the models of depth 3 and depth 24. Notice how in the first epoch (Left column) there is no learning rate transfer in both cases. This correlates with the fact that that the training starts at an increasing sharpness with depth, which causes no consistency the dynamics (and divergent behaviour at large learning rates). Adding warmup alleviates this issue, allowing the model to reach edge of stability and improve learning rate transfer (middle column). One step in the sharpness plot corresponds to 2 batches. Parameters: batch size $\\phantom{-}128$ , epochs $=20$ , using data augmentation. ", "page_idx": 17}, {"type": "text", "text": "B.2 Standard Parameterization (SP) experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Following the SP formulation introduced in [5], we analyze the sharpness and learning rate transfer under this regime in Figure 7. Note that our experiments use a fixed learning rate $\\eta$ . While Yang and Hu [5] use a width scaled learning rate in order to parameterize SP as a kernel limit in infinite width, the SP definition that we use does not have a well defined limit and thus diverges as the width is increased. For more details, we refer the reader to (Yang and Hu [5], Appendix J.3). ", "page_idx": 17}, {"type": "text", "text": "B.3 $\\mu\\mathbf{P}$ - disabling residuals ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We also provide training runs of models parameterized with $\\mu\\mathrm{P}$ , while disabling the residuals. Note that the models quickly become untrainable under this regime when increasing the depth. This phenomenon is due to the vanishing curvature experienced by these models, as shown in Figure 8, which leads to vanishing signal. ", "page_idx": 17}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/0cce03991aa08424480aeb9071563122f3cee75f55c395d6557941a16b3c9d9c.jpg", "img_caption": ["Figure 7: Standard Parameterization (SP) for ConvNets trained using SGD on CIFAR10, for varying number of learning rate warmup steps. (Left column) No warmup, (Middle column) 1000 warmup steps and (Right column) 2000 warmup steps. Note that under SP, in the beginning the training starts from a high curvature, which means that large step sizes would lead to divergent behaviour. Adding warmup alleviates this issue, allowing the model to reach edge of stability and improve learning rate transfer. One step corresponds to 10 batches. Parameters: batch size $\\mathrm{=}256$ , epochs $=\\!20$ , using data augmentation. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/89546db73d2b2b754f6b71bdb0116870ee8cfe3b01156f2f4f5bda9e6c8cf78d.jpg", "img_caption": ["Figure 8: $\\mu\\mathrm{P}$ parameterization on ConvNets with residuals disabled $\\left(\\tau=0\\right)$ ) trained on CIFAR10. (Left) Learning rate transfer plot, showing that under this setting, the optimal learning rate transfers, but with increasingly larger shifts when increasing the depth due to the vanishing signal. (Right) Sharpness evolution during training, showing that the dynamics are following a depth independent trajectory for the optimal learning rate (0.5275). Note that deeper models become much harder to train when residuals are disabled, motivated by the observation that the the depth 24 model suffers from vanishing curvature at larger learning rates. The spikes in the plot are due to the curvature approaching 0 in log-scale. One step corresponds to 10 batches. Parameters: batch size $=256$ , epochs $=\\!20$ , using data augmentation. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.4 $\\mu\\mathbf{P}$ experiments for full batch GD ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we investigate the effect of $\\mu\\mathrm{P}$ and on learning rate transfer and sharpness, when trained with full batch gradient descent. We subsampled 5000 sampled from CIFAR10 in a stratified fashion (i.e. 500 sampled from each of the 10 classes) and proceeded to train residual ConvNets under the $\\mu\\mathrm{P}$ parameterization with GD, following a similar procedure as [16]. Our findings show that under $\\mu\\mathrm{P}$ , when using a large enough learning rate, the models are able to achieve edge of stability, as well as have learning rate transfer. This is empirically shown in Figure 9, where we can see that while the sharpness has the typical oscillations around the EoS threshold studied by [16], it still does maintain a width-independent trend during training. ", "page_idx": 18}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/0228f5bb8727012ccc3b25d4b90adbd85b6b24813180ef93870df418b725f06f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 9: Residual ConvNets trained using (full batch) GD on a 5000 sample subset of CIFAR10. (Left) Learning rate transfer plot, showing that the optimal learning rate transfers across different widths; the slight shift in the transfer plot is due to the oscillations around the EoS threshold seen in (middle) and (right). (Middle) Sharpness dynamics during training, showing a consistent width independent dynamic throughout the whole training procedure; dashed line represents $2/\\eta$ . (Right) Training loss dynamic during time for the optimal learning rate (0.76), showing the wider-isbetter behaviour of the $\\mu\\mathrm{P}$ parameterization, as well as the oscillations induced by the EoS regime. Parameters: batch size $\\mathrm{\\Theta=256}$ , no warmup, using data augmentation. ", "page_idx": 19}, {"type": "text", "text": "C Analysis of a Two-Layer Linear Network ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Recall the definition of our model: ", "page_idx": 19}, {"type": "equation", "text": "$$\nL(E,V)=\\frac{1}{2}\\left\\Vert\\frac{1}{\\gamma\\sqrt{N D}}X E V-Y\\right\\Vert^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/4058b5ff9a6a50826793f9eee9b7d28c71e2ef9df1a27d83dc759e0a85e68781.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 10: Evolution of loss under $\\mu P$ and NTP for the toy example of Section 5: $\\begin{array}{r}{\\frac{1}{2}\\|\\frac{1}{\\sqrt{N D}\\gamma}E V-}\\end{array}$ $w_{*}\\Vert^{2}$ , where $w_{\\ast}=1\\in\\mathbb{R}^{D}$ , $D=100$ . This is a minimal example of transfer captured by our theory: $\\mu P$ trajectories align. Different linestyles correspond to different values of $\\eta_{0}$ (grid is different for $\\mu P$ and NTP). ", "page_idx": 19}, {"type": "text", "text": "Under the previous assumptions regarding the data, we have that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial_{E}=\\frac{1}{\\gamma^{2}N D}E V^{\\top}-\\frac{1}{\\gamma\\sqrt{N D}}w_{*}V^{\\top}.}\\\\ {\\partial_{V}=\\frac{1}{\\gamma^{2}N D}E^{\\top}E V-\\frac{1}{\\gamma\\sqrt{N D}}E^{\\top}w_{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "C.1 Relationship between sharpness and residual, and Gauss-Newton ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The following bound leverages a Gauss-Newton decomposition and leads analytical insights supporting Sec. 4.1. Proof of Lemma 5.2, which we restate here. ", "page_idx": 19}, {"type": "text", "text": "Lemma (GN bound). Let $\\gamma^{2}\\nabla^{2}{\\mathcal{L}}={\\mathcal{G}}+{\\mathcal{R}}$ be Gauss-Newton decomposition4 (see Sec. 4.1) of the Hessian for the loss in Eq. 4, with $\\mathcal{G}=K^{\\top}K$ , where $K\\in\\mathbb{R}^{D\\times(N D+\\mathbf{\\hat{N}})}$ .   \nLet us denote the NTK matrix $\\Theta=K K^{\\top}\\in\\mathbb{R}^{D\\times D}$ . Then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Theta(E,V)=e+v\\cdot I_{D}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\lambda_{\\operatorname*{max}}[\\gamma^{2}\\nabla^{2}\\mathcal{L}(E,V)]-\\lambda_{\\operatorname*{max}}[\\Theta(E,V)]|\\leq\\sqrt{\\frac{\\gamma^{2}}{N D}}\\|w-w_{*}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The result above is actually more general: it indicates that the eigenvalues of the positive semidefinite portion of the Hessian $\\dot{\\mathcal{G}}=\\kappa^{\\top}\\\\kappa$ are fully5 characterized by the eigenvalues a smaller matrix $\\dot{\\Theta}=K K^{\\top}$ . Furthermore, in our model $\\Theta$ has a closed form that depends only on the variables $e,v$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. The Hessian blocks become: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{E E}=\\cfrac{1}{\\gamma^{2}N D}I_{D}\\otimes V V^{\\top}\\in\\mathbb R^{N D\\times N D}}\\\\ &{\\partial_{E V}=\\cfrac{1}{\\gamma^{2}N D}E\\otimes V+\\cfrac{1}{\\gamma\\sqrt{N D}}\\left(\\cfrac{1}{\\gamma\\sqrt{N D}}E V-w_{*}\\right)\\otimes I_{N}\\in\\mathbb R^{N D\\times N}}\\\\ &{\\partial_{V E}=\\cfrac{1}{\\gamma^{2}N D}E^{\\top}\\otimes V^{\\top}+\\cfrac{1}{\\gamma\\sqrt{N D}}\\left(\\cfrac{1}{\\gamma\\sqrt{N D}}V^{\\top}E^{\\top}-w_{*}^{\\top}\\right)\\otimes I_{N}\\in\\mathbb R^{N\\times N D}}\\\\ &{\\partial_{V V}=\\cfrac{1}{\\gamma^{2}N D}E^{\\top}E\\in\\mathbb R^{N\\times N}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using these definitions, we can separate the Hessian $H$ into a sum of 2 matrices, where one depends on the residual and one does not. ", "page_idx": 20}, {"type": "equation", "text": "$$\n7^{2}\\mathcal{L}(E,V)=\\underbrace{\\frac{1}{\\gamma^{2}N D}\\left(I_{D}\\otimes V V^{\\top}\\quad E\\otimes V\\right)}_{\\mathcal{G}(E,V)}+\\underbrace{\\frac{1}{\\gamma\\sqrt{N D}}\\left(I_{N}\\otimes(w-w_{*})^{\\top}\\quad I_{N}\\otimes(w-w_{*})\\right)}_{\\mathcal{R}(E,V)}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "hence our quantity of interest: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\gamma^{2}\\nabla^{2}\\mathcal{L}(E,V)=\\underbrace{\\frac{1}{N D}\\left(I_{D}\\otimes V V^{\\top}\\quad E\\otimes V\\right)}_{\\mathcal{G}(E,V)}+\\underbrace{\\sqrt{\\frac{\\gamma^{2}}{N D}}\\left(I_{N}\\otimes(w-w_{*})^{\\top}\\quad I_{N}\\otimes(w-w_{*})\\right)}_{\\mathcal{R}(E,V)}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\begin{array}{r}{w=\\frac{1}{\\gamma\\sqrt{N D}}E V}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Study of $\\boldsymbol{\\mathcal{G}}$ . Note that ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{G}(E,V)=K^{\\top}K,\\qquad K^{\\top}=\\frac{1}{\\sqrt{N D}}\\left(\\!\\!\\begin{array}{c}{{I_{D}\\otimes V}}\\\\ {{E^{\\top}}}\\end{array}\\!\\!\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By an SVD decomposition, it is easy to see that, the nonzero eigenvalues of $\\mathcal{G}=K^{\\top}K$ are the same as the nonzero eigenvalues of $\\Theta=\\dot{K}K^{\\top}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Im(E,V)=\\frac{1}{N D}\\left(I_{D}\\otimes V^{\\top}\\quad E\\right)\\left(\\frac{I_{D}\\otimes V}{E^{\\top}}\\right)=\\frac{1}{N D}E E^{\\top}+\\frac{1}{N D}V^{\\top}V I_{D}=e+v\\cdot I_{D}\\in\\mathbb{R}^{D\\times D}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $e,v$ are the quantities found in the main paper, and we therefore have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{max}}[\\mathcal{G}(E,V)]=\\lambda_{\\operatorname*{max}}[\\Theta(E,V)]=\\lambda_{\\operatorname*{max}}\\left[\\frac{1}{N D}E E^{\\top}\\right]+\\frac{1}{N D}V^{\\top}V.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "5Simple application of the SVD decomposition. ", "page_idx": 20}, {"type": "text", "text": "Study of $\\mathcal{R}$ and of the residual. Note that $\\mathcal{R}(E,V)$ has both positive and negative eigenvalues, with spectrum symmetric along the real line. It is easy to show that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{max}}[\\mathcal{R}(E,V)]=-\\lambda_{\\operatorname*{min}}[\\mathcal{R}(E,V)]=\\sqrt{\\frac{\\gamma^{2}}{N D}}\\|w-w^{*}\\|.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using the fact that $\\mathcal{G}$ is Hermitian, we can apply Weyl\u2019s inequality to obtain a bound on the deviation of the sharpness from the maximum eigenvalue of $\\mathcal{G}$ in terms of the residual: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname{t}_{\\operatorname*{max}}[\\Theta(E,V)]-\\sqrt{\\frac{\\gamma^{2}}{N D}}\\|w-w_{*}\\|_{2}\\leq\\lambda_{\\operatorname*{max}}\\big[\\gamma^{2}\\nabla^{2}\\mathcal{L}(E,V)\\big]\\leq\\lambda_{\\operatorname*{max}}[\\Theta(E,V)]+\\sqrt{\\frac{\\gamma^{2}}{N D}}\\|w-w_{*}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Which finally yields: ", "page_idx": 21}, {"type": "equation", "text": "$$\n|\\lambda_{\\operatorname*{max}}[\\gamma^{2}\\nabla^{2}\\mathcal{L}(E,V)]-\\lambda_{\\operatorname*{max}}[\\Theta(E,V)]|\\leq\\sqrt{\\frac{\\gamma^{2}}{N D}}\\|w-w_{*}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "C.2 Proof of Thm.5.1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We divide the proof into two parts. In the first part, we study the evolution of the unnormalized quantities $E E^{\\dagger},V^{\\top}V$ and $E V$ . Then, we study how normalization affects the dynamics. ", "page_idx": 21}, {"type": "text", "text": "Part one: dynamics in a smaller space. We go step by step recalling the gradient descent equations at the beginning of this section. ", "page_idx": 21}, {"type": "text", "text": "Dynamics of $E V$ . We have : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{G}_{+}V_{+}=(E-\\eta\\partial_{E})(V-\\eta\\partial_{V})}\\\\ &{\\qquad=E V-\\eta\\partial_{E}V-\\eta E\\partial_{V}+\\eta^{2}\\partial_{E}\\partial_{V}}\\\\ &{\\qquad=E V-\\frac{\\eta_{0}}{N D}E V^{\\top}V+\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}w_{*}V^{\\top}V-\\frac{\\eta_{0}}{N D}E E^{\\top}E V+\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}E E^{\\top}w_{*}}\\\\ &{\\qquad\\qquad+\\left(\\frac{\\eta_{0}}{N D}E V^{\\top}-\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}w_{*}V^{\\top}\\right)\\left(\\frac{\\eta_{0}}{N D}E^{\\top}E V-\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}E^{\\top}w_{*}\\right)}\\\\ &{\\qquad=E V-\\frac{\\eta_{0}}{N D}E V V^{\\top}V+\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}w_{*}V^{\\top}V-\\frac{\\eta_{0}}{N D}E E^{\\top}E V+\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}E E^{\\top}w_{*}}\\\\ &{\\qquad\\qquad+\\frac{\\eta_{0}^{2}}{N^{2}D^{2}}E V V^{\\top}E V-\\frac{\\eta_{0}^{2}\\gamma}{N^{2}D^{2}}E V V^{\\top}E^{\\top}w_{*}-\\frac{\\eta_{0}^{2}\\gamma}{N^{2}D^{2}}w_{*}V^{\\top}E^{\\top}E V+\\frac{\\gamma^{2}\\eta_{0}^{2}}{N D}w_{*}V}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let us rename ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\tilde{w}=E V,\\qquad\\tilde{v}=V^{\\top}V,\\qquad\\tilde{e}=E E^{\\top}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "then the equation becomes more compact: ", "page_idx": 21}, {"type": "table", "img_path": "rgwhJ7INtZ/tmp/234e7a8ccdc4dfb4934a9fba9c0a95df1171b60625390380b7a6a4e2c258a250.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Note that no quantities appear in the equation for $\\tilde{w}^{+}$ besides $\\tilde{w},\\tilde{v},\\tilde{e}$ . We will see that these do not appear also in the equations for $\\tilde{v},\\tilde{e}$ . ", "page_idx": 21}, {"type": "text", "text": "Dynamics of $V^{\\top}V$ . Let us write down here the equations for $-\\eta\\partial_{V}$ and $-\\eta\\partial_{V}^{\\top}$ for ease of reference: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle-\\,\\eta\\partial_{V}^{\\top}=-\\frac{\\eta_{0}}{N D}V^{\\top}E^{\\top}E+\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}w_{*}^{\\top}E}}\\\\ {{\\displaystyle-\\,\\eta\\partial_{V}=-\\frac{\\eta_{0}}{N D}E^{\\top}E V+\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}E^{\\top}w_{*}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{+}^{\\top}V_{+}=(V-\\eta\\partial_{V})^{\\top}(V-\\eta\\partial_{V})}\\\\ &{\\qquad=V^{\\top}V-\\eta\\partial_{V}^{\\top}V-\\eta V^{\\top}\\partial_{V}+\\eta^{2}\\partial_{V}^{\\top}\\partial_{V}}\\\\ &{\\qquad=V^{\\top}V-\\frac{\\eta_{0}}{N D}V^{\\top}E^{\\top}E V+\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}w_{*}^{\\top}E V-\\frac{\\eta_{0}}{N D}V^{\\top}E^{\\top}E V+\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}V^{\\top}E^{\\top}w_{*}}\\\\ &{\\qquad\\qquad+\\left(-\\frac{\\eta_{0}}{N D}V^{\\top}E^{\\top}E+\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}w_{*}^{\\top}E\\right)\\left(-\\frac{\\eta_{0}}{N D}E^{\\top}E V+\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}E^{\\top}w_{*}\\right)}\\\\ &{\\qquad=V^{\\top}V-\\frac{\\eta_{0}}{N D}V^{\\top}E^{\\top}E V+\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}w_{*}^{\\top}E V-\\frac{\\eta_{0}}{N D}V^{\\top}E^{\\top}E V+\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}V^{\\top}E^{\\top}w_{*}}\\\\ &{\\qquad\\qquad+\\frac{\\eta_{0}^{2}}{N^{2}D^{2}}V^{\\top}E^{\\top}E V-\\frac{\\eta_{0}^{2}\\gamma}{N^{3}D^{3}}w_{*}^{\\top}E E^{\\top}E V-\\frac{\\eta_{0}^{2}\\gamma}{N^{3}D^{3}}V^{\\top}E^{\\top}E E^{\\top}w_{*}+\\frac{\\gamma^{2}\\eta_{0}^{2}}{N D}w_{*}^{\\top}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using our notation, equations get yet again simpler: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\tilde{v}^{+}=\\tilde{v}-2\\frac{\\eta_{0}}{N D}\\tilde{w}^{\\top}\\tilde{w}+2\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}w_{*}^{\\top}\\tilde{w}+\\frac{\\eta_{0}^{2}}{N^{2}D^{2}}\\tilde{w}^{\\top}\\tilde{e}\\tilde{w}-2\\frac{\\eta_{0}^{2}\\gamma}{N^{\\frac{3}{2}}D^{\\frac{3}{2}}}w_{*}^{\\top}\\tilde{e}\\tilde{w}+\\frac{\\gamma^{2}\\eta_{0}^{2}}{N D}w_{*}^{\\top}\\tilde{e}w_{*}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that again no quantities appear in the equation for $\\tilde{v}^{+}$ besides $\\tilde{w},\\tilde{v},\\tilde{e}$ . ", "page_idx": 22}, {"type": "text", "text": "Dynamics of $E^{\\top}E$ . For convenience, recall: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\eta\\partial_{E}=-\\frac{\\eta_{0}}{N D}E V V^{\\top}+\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}w_{*}V^{\\top}}\\\\ &{-\\eta\\partial_{E}^{\\top}=-\\frac{\\eta_{0}}{N D}V V^{\\top}E^{\\top}+\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}V w_{*}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\mathfrak{L}}_{+}E_{+}^{\\top}=(E-\\eta\\partial_{E})(E-\\eta\\partial_{E})^{\\top}}\\\\ &{\\qquad=E E^{\\top}-\\eta\\partial_{E}E^{\\top}-\\eta E\\partial_{E}^{\\top}+\\eta^{2}\\partial_{E}\\partial_{E}^{\\top}}\\\\ &{\\qquad=E E^{\\top}-\\frac{\\eta_{0}}{N D}E V^{\\top}E^{\\top}+\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}w_{*}V^{\\top}E^{\\top}-\\frac{\\eta_{0}}{N D}E V^{\\top}E^{\\top}+\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}E V w_{*}^{\\top}}\\\\ &{\\qquad\\qquad+\\left(-\\frac{\\eta_{0}}{N D}E V^{\\top}+\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}w_{*}V^{\\top}\\right)\\left(-\\frac{\\eta_{0}}{N D}V V^{\\top}E^{\\top}+\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}V w_{*}^{\\top}\\right)}\\\\ &{\\qquad=E E^{\\top}-\\frac{\\eta_{0}}{N D}E V V^{\\top}E^{\\top}+\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}w_{*}V^{\\top}E^{\\top}-\\frac{\\eta_{0}}{N D}E V V^{\\top}E^{\\top}+\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}E V w_{*}^{\\top}}\\\\ &{\\qquad\\qquad+\\frac{\\eta_{0}^{2}}{N^{2}D^{2}}E V V^{\\top}V V^{\\top}E^{\\top}-\\frac{\\gamma\\eta_{0}^{2}}{N^{3}D^{3}}w_{*}V^{\\top}V V^{\\top}E^{\\top}-\\frac{\\gamma\\eta_{0}^{2}}{N^{3}D^{3}}E V V^{\\top}V w_{*}^{\\top}+\\frac{\\gamma^{2}\\eta_{0}^{2}}{N^{2}D^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using our notation, equations get yet again simpler: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\ {\\tilde{e}}^{+}=\\tilde{e}-2\\frac{\\eta_{0}}{N D}\\tilde{w}{\\tilde{w}}^{\\top}+\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}w_{*}{\\tilde{w}}^{\\top}+\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}\\tilde{w}w_{*}^{\\top}\\right.\\ \\ \\ \\ \\ \\ \\ }\\\\ {\\left.\\ \\ \\ \\ \\ \\ \\ +\\left.\\frac{\\eta_{0}^{2}}{N^{2}D^{2}}\\tilde{v}\\tilde{w}{\\tilde{w}}^{\\top}-\\frac{\\gamma\\eta_{0}^{2}}{N^{\\frac{3}{2}}D^{\\frac{3}{2}}}\\tilde{v}w_{*}{\\tilde{w}}^{\\top}-\\frac{\\gamma\\eta_{0}^{2}}{N^{\\frac{3}{2}}D^{\\frac{3}{2}}}\\tilde{v}\\tilde{w}w_{*}^{\\top}+\\frac{\\eta_{0}^{2}\\gamma^{2}}{N D}\\tilde{v}w_{*}w_{*}^{\\top}\\right..}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Part two: Normalization. Consider scalar reparameterizations. ", "page_idx": 22}, {"type": "equation", "text": "$$\nw=\\alpha_{w}\\tilde{w},\\qquad e=\\alpha_{e}\\tilde{e},\\qquad v=\\alpha_{v}\\tilde{v}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "While we gave the form of these normalizers already in the paper, we keep it more general here to real numbers and show that the right normalizers arise directly. ", "page_idx": 22}, {"type": "text", "text": "Reparameterization of $E V$ . We have ", "page_idx": 22}, {"type": "text", "text": "w+ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\alpha_{w}\\tilde{w}^{+}}\\\\ &{=(\\alpha_{w}\\tilde{w})-\\frac{\\eta_{0}}{N D}\\tilde{v}(\\alpha_{w}\\tilde{w})+\\frac{\\gamma\\eta_{0}\\alpha_{w}}{\\sqrt{N D}}\\tilde{v}w_{*}-\\frac{\\eta_{0}}{N D}\\tilde{\\epsilon}(\\alpha_{w}\\tilde{w})+\\frac{\\gamma\\eta_{0}\\alpha_{w}}{\\sqrt{N D}}\\tilde{\\epsilon}w_{*}}\\\\ &{\\qquad+\\frac{\\eta_{0}^{2}}{N^{2}D^{2}}(\\tilde{w}\\tilde{w}^{\\top})(\\alpha_{w}\\tilde{w})-\\frac{\\eta_{0}^{2}\\gamma}{N^{2}D^{3}}(\\alpha_{w}\\tilde{w}\\tilde{w}^{\\top})w_{*}-\\frac{\\eta_{0}^{2}\\gamma}{N^{3}D^{2}}(w_{*}\\tilde{w}^{\\top})(\\alpha_{w}\\tilde{w})+\\frac{\\gamma^{2}\\eta_{0}^{2}}{N D}(w_{*}(\\alpha_{w}\\tilde{w})^{\\top}}\\\\ &{=(\\alpha_{w}\\tilde{w})-\\frac{\\eta_{0}}{N D\\alpha_{w}}(\\alpha_{w}\\tilde{v})(\\alpha_{w}\\tilde{w})+\\frac{\\gamma\\eta_{0}\\alpha_{w}}{\\sqrt{N D}\\alpha_{w}}(\\alpha_{v}\\tilde{w})w_{*}-\\frac{\\eta_{0}}{N D\\alpha_{w}}(\\alpha_{w}\\tilde{v})(\\alpha_{w}\\tilde{w})+\\frac{\\gamma\\eta_{0}\\alpha_{w}}{\\sqrt{N D}\\alpha_{w}}(\\alpha_{\\epsilon}\\tilde{v}w)w_{*}}\\\\ &{\\qquad+\\frac{\\eta_{0}}{N^{2}D^{2}}(\\alpha_{w}^{2}\\tilde{w}^{\\top})(\\alpha_{w}\\tilde{w})-\\frac{\\eta_{0}^{2}\\gamma}{N^{2}D^{3}}(\\alpha_{w}^{2}\\tilde{w}^{\\top})w_{*}-\\frac{\\eta_{0}^{2}\\gamma}{N^{2}D^{3}}(\\alpha_{w}w_{*}\\tilde{w}^{\\top})(\\alpha_{w}\\tilde{w})+\\frac{\\gamma}{2}}\\\\ &{=w-\\frac{\\eta_{0}}{N D\\alpha_{w}}w+\\frac{\\gamma\\eta_{0}\\alpha_{w}}{\\sqrt{N D}\\alpha \n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We would like $\\alpha_{w},\\alpha_{e},\\alpha_{v}$ to be such that on the right-hand side we have no width dependency. To do that we need (first and second line refer to first and second lines in the equation) ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\alpha_{v}\\propto\\frac{1}{N},\\qquad\\alpha_{w}\\propto\\alpha_{v}\\frac{\\sqrt{N}}{\\gamma},\\qquad\\alpha_{e}\\propto\\frac{1}{N},\\qquad\\alpha_{w}\\propto\\alpha_{e}\\frac{\\sqrt{N}}{\\gamma}}\\\\ {\\displaystyle\\alpha_{w}\\propto\\frac{1}{N},\\qquad\\alpha_{w}\\propto\\frac{\\gamma}{N^{\\frac{3}{2}}D^{\\frac{3}{2}}},\\qquad\\gamma^{2}\\propto N}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where proportionality can \u221adepend on any factor (e.g. $d)$ except of course $N$ . Crucially note that the equations require $\\gamma\\propto\\overbar{\\sqrt{N}}$ . So this can be done for $\\pmb{\\mu}P$ but not for NTP (except if $d\\simeq N.$ ). Further, we need ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\alpha_{w},\\alpha_{e},\\alpha_{v}\\propto\\frac{1}{N}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "To get that $w\\to w_{*}$ , we choose (as in the main paper) ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\alpha_{w}={\\frac{1}{\\gamma{\\sqrt{N D}}}}={\\frac{1}{N{\\sqrt{D}}}}\\;(\\mathrm{for}\\;\\mu P),\\qquad\\alpha_{e}={\\frac{1}{N D}},\\qquad\\alpha_{v}={\\frac{1}{N D}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "So, we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v^{+}=w-\\frac{\\eta_{0}}{N D\\alpha_{v}}v w+\\frac{\\gamma\\eta_{0}\\alpha_{w}}{\\sqrt{N D}\\alpha_{v}}v w_{\\ast}-\\frac{\\eta_{0}}{N D\\alpha_{e}}e w+\\frac{\\gamma\\eta_{0}\\alpha_{w}}{\\sqrt{N D}\\alpha_{e}}e w_{\\ast}}\\\\ &{\\qquad\\quad+\\frac{\\eta_{0}^{2}}{N^{2}D^{2}\\alpha_{w}^{2}}w w^{\\top}w-\\frac{\\eta_{0}^{2}\\gamma}{N^{3}D^{3}\\alpha_{w}}(w w^{\\top})w_{\\ast}-\\frac{\\eta_{0}^{2}\\gamma}{N^{\\frac{3}{2}}D^{\\frac{3}{2}}\\alpha_{w}}(w_{\\ast}w^{\\top})w+\\frac{\\gamma^{2}\\eta_{0}^{2}}{N D}(w_{\\ast}w^{\\top})w_{\\ast}}\\\\ &{\\qquad=w-\\eta_{0}v w+\\eta_{0}v w_{\\ast}-\\eta_{0}e w+\\eta_{0}e w_{\\ast}}\\\\ &{\\qquad\\quad+\\frac{\\eta_{0}^{2}\\gamma^{2}}{N D}w w^{\\top}w-\\frac{\\eta_{0}^{2}\\gamma^{2}}{N D}(w w^{\\top})w_{\\ast}-\\frac{\\eta_{0}^{2}\\gamma^{2}}{N D}(w_{\\ast}w^{\\top})w+\\frac{\\gamma^{2}\\eta_{0}^{2}}{N D}(w_{\\ast}w^{\\top})w_{\\ast}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Reparameterization of $V^{\\top}V$ . Recall that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\tilde{v}^{+}=\\tilde{v}-2\\frac{\\eta_{0}}{N D}\\tilde{w}^{\\top}\\tilde{w}+2\\frac{\\gamma\\eta_{0}}{\\sqrt{N D}}w_{*}^{\\top}\\tilde{w}+\\frac{\\eta_{0}^{2}}{N^{2}D^{2}}\\tilde{w}^{\\top}\\tilde{e}\\tilde{w}-2\\frac{\\eta_{0}^{2}\\gamma}{N^{\\frac{3}{2}}D^{\\frac{3}{2}}}w_{*}^{\\top}\\tilde{e}\\tilde{w}+\\frac{\\gamma^{2}\\eta_{0}^{2}}{N D}w_{*}^{\\top}\\tilde{e}w_{*}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This implies, after scaling ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v^{+}=\\alpha_{v}\\tilde{v}^{+}}\\\\ &{\\quad=(\\alpha_{v}\\tilde{v})-2\\frac{\\eta_{0}\\alpha_{v}}{N D}\\tilde{w}^{\\top}\\tilde{w}+2\\frac{\\gamma\\eta_{0}\\alpha_{v}}{\\sqrt{N D}}w_{*}^{\\top}\\tilde{w}}\\\\ &{\\qquad\\quad+\\;\\frac{\\eta_{0}^{2}\\alpha_{v}}{N^{2}D^{2}}\\tilde{w}^{\\top}\\tilde{e}w-2\\frac{\\eta_{0}^{2}\\gamma\\alpha_{v}}{N^{3}D^{\\frac{3}{2}}}w_{*}^{\\top}\\tilde{e}\\tilde{w}+\\frac{\\gamma^{2}\\eta_{0}^{2}\\alpha_{v}}{N D}w_{*}^{\\top}\\tilde{e}w_{*}}\\\\ &{\\quad=v-2\\frac{\\eta_{0}\\alpha_{v}}{N D\\alpha_{w}^{2}}w^{\\top}w+2\\frac{\\gamma\\eta_{0}\\alpha_{v}}{\\sqrt{N D}\\alpha_{w}}w_{*}^{\\top}w}\\\\ &{\\qquad\\quad+\\;\\frac{\\eta_{0}^{2}\\alpha_{v}}{N^{2}D^{2}\\alpha_{w}^{2}\\alpha_{e}}w^{\\top}e w-2\\frac{\\eta_{0}^{2}\\gamma\\alpha_{v}}{N^{\\frac{3}{2}}D^{\\frac{3}{2}}\\alpha_{v}\\alpha_{e}}w_{*}^{\\top}e w+\\frac{\\gamma^{2}\\eta_{0}^{2}\\alpha_{v}}{N D\\alpha_{e}}w_{*}^{\\top}e w_{*}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "It is easy to see that the choice $\\begin{array}{r}{\\alpha_{w},\\alpha_{e},\\alpha_{v}\\propto\\frac{1}{N}}\\end{array}$ in addition with $\\gamma=\\sqrt{N}$ gives independence of the RHS to width. ", "page_idx": 24}, {"type": "text", "text": "Under our choices ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\alpha_{w}={\\frac{1}{\\gamma{\\sqrt{N D}}}}={\\frac{1}{N{\\sqrt{D}}}}\\;(\\mathrm{for}\\;\\mu P),\\qquad\\alpha_{e}={\\frac{1}{N D}},\\qquad\\alpha_{v}={\\frac{1}{N D}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "we get ", "page_idx": 24}, {"type": "equation", "text": "$$\nv^{+}=v+\\frac{\\eta_{0}\\gamma^{2}}{N D}\\left[-2w^{\\top}w+2w_{*}^{\\top}w\\right]+\\frac{\\eta_{0}^{2}\\gamma^{2}}{N D}\\left[w^{\\top}e w-2w_{*}^{\\top}e w+w_{*}^{\\top}e w_{*}\\right]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Reparameterization of $E E^{\\top}$ . Let\u2019s substitute the scaled version in the equations ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\varepsilon}^{+}=\\alpha_{e}\\tilde{e}^{+}}\\\\ &{\\quad=e-2\\frac{\\eta_{0}\\alpha_{e}}{N D\\alpha_{w}^{2}}w w^{\\top}+\\frac{\\gamma\\eta_{0}\\alpha_{e}}{\\sqrt{N D}\\alpha_{w}}w_{*}w^{\\top}+\\frac{\\gamma\\eta_{0}\\alpha_{e}}{\\sqrt{N D}\\alpha_{w}}w w_{*}^{\\top}}\\\\ &{\\qquad\\quad+\\frac{\\eta_{0}^{2}\\alpha_{e}}{N^{2}D^{2}\\alpha_{w}^{2}\\alpha_{v}}v w w^{\\top}-\\frac{\\gamma\\eta_{0}^{2}\\alpha_{e}}{N^{\\frac{3}{2}}D^{\\frac{3}{2}}\\alpha_{v}\\alpha_{w}}v w_{*}w^{\\top}-\\frac{\\gamma\\eta_{0}^{2}\\alpha_{e}}{N^{\\frac{3}{2}}D^{\\frac{3}{2}}\\alpha_{v}\\alpha_{w}}v w w_{*}^{\\top}+\\frac{\\eta_{0}^{2}\\gamma^{2}\\alpha_{e}}{N D\\alpha_{v}}v w_{*}w_{*}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "With our choices ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\alpha_{w}={\\frac{1}{\\gamma{\\sqrt{N D}}}}={\\frac{1}{N{\\sqrt{D}}}}\\;(\\mathrm{for}\\;\\mu P),\\qquad\\alpha_{e}={\\frac{1}{N D}},\\qquad\\alpha_{v}={\\frac{1}{N D}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "we get ", "page_idx": 24}, {"type": "equation", "text": "$$\ne^{+}=e+\\frac{\\eta_{0}\\gamma^{2}}{N D}\\left[-2w w^{\\top}+w_{*}w^{\\top}+w w_{*}^{\\top}\\right]+\\frac{\\eta_{0}^{2}\\gamma^{2}}{N D}\\left[v w w^{\\top}-v w_{*}w^{\\top}-v w w_{*}^{\\top}+v w_{*}w_{*}^{\\top}\\right]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "C.2.1 Initialization ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proposition C.1. At initialization, as $N\\rightarrow\\infty$ , $\\begin{array}{r}{e\\stackrel{\\mathbb{P}}{\\rightarrow}e^{\\infty}:=\\frac{1}{D}I_{D}}\\end{array}$ and $\\begin{array}{r}{v\\stackrel{\\mathbb{P}}{\\rightarrow}v^{\\infty}:={\\frac{1}{D}}}\\end{array}$ . Moreover, errors from $\\infty-$ initialization scale as $\\textstyle{\\frac{1}{\\sqrt{N}}}$ in expectation: $\\begin{array}{r}{\\mathbb{E}|v-v^{\\infty}|^{2},\\mathbb{E}|e_{i j}-e_{i,j}^{\\infty}|^{2}\\leq\\frac{2}{N D}}\\end{array}$ , $\\forall i,j\\in$ $[D]$ . While for $\\gamma=1$ (NTP) $w$ at initialization is in the limit Gaussian with elementwise variance $1/D$ , for $\\gamma\\,=\\,\\sqrt{N}\\;(\\mu P)$ we have $w\\ {\\overset{\\mathbb{P}}{\\to}}\\ w^{\\infty}:=0,$ , with elementwise variations scaling as $\\frac{1}{\\sqrt{N}}$ : $\\begin{array}{r}{\\mathbb{E}|w_{i}-w_{i}^{\\infty}|^{2}=\\frac{1}{N D}}\\end{array}$ , $\\forall i\\in[D]$ . ", "page_idx": 24}, {"type": "text", "text": "First, note that trivially ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}[w_{i}]=0,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}[w_{i}]^{2}=\\frac{1}{\\gamma^{2}N D}\\sum_{j,j^{\\prime}=1}^{N}\\mathbb{E}[E_{i j}E_{i j^{\\prime}}V_{j}V_{j^{\\prime}}]=\\frac{1}{D\\gamma^{2}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Next: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[e_{i j}]=\\frac{1}{N D}\\sum_{k=1}^{N}\\mathbb{E}[E_{i k}E_{j k}]=\\frac{1}{D}\\delta_{i j},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[e_{i j}^{2}]=\\frac{1}{N^{2}D^{2}}\\sum_{k,k^{\\prime}=1}^{N}\\mathbb{E}[E_{i k}E_{j k}E_{i k^{\\prime}}E_{j k^{\\prime}}].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For $i\\neq j$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[e_{i j}^{2}]=\\frac{1}{N^{2}D^{2}}\\sum_{k,k^{\\prime}=1}^{N}\\mathbb{E}[E_{i k}E_{i k^{\\prime}}]\\mathbb{E}[E_{j k}E_{j k^{\\prime}}]=\\frac{1}{N D^{2}}\\qquad(i\\neq j)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For $i=j$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}[e_{i j}^{2}]=\\frac{1}{N^{2}D^{2}}\\sum_{k,k^{\\prime}=1}^{N}\\mathbb{E}[E_{i k}^{2}E_{i k^{\\prime}}^{2}]}\\\\ {\\displaystyle=\\frac{1}{N^{2}D^{2}}\\sum_{k\\neq k^{\\prime}}\\mathbb{E}[E_{i k}^{2}]\\mathbb{E}[E_{i k^{\\prime}}^{2}]+\\frac{1}{N^{2}D^{2}}\\sum_{k=1}^{N}\\mathbb{E}[E_{i k}^{4}]}\\\\ {\\displaystyle=\\frac{N(N-1)}{N^{2}D^{2}}+\\frac{3}{N D^{2}}}\\\\ {\\displaystyle=\\frac{N+2}{N D^{2}}\\sum_{k\\neq k^{\\prime}}(i=j).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "So ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{Var}[e_{i j}]=\\frac{N+2}{N D^{2}}-\\frac{1}{D^{2}}=\\frac{2}{N D^{2}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Finally: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[v]=\\frac{1}{N D}\\sum_{i=1}^{N}\\mathbb{E}[V_{i}V_{i}]=\\frac{1}{D},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathfrak{L}[v^{2}]=\\frac{1}{N^{2}D^{2}}\\sum_{i,i^{\\prime}=1}^{N}\\mathbb{E}[V_{i}V_{i}V_{i^{\\prime}}V_{i^{\\prime}}]=\\frac{1}{N^{2}D^{2}}\\sum_{i\\neq i^{\\prime}}^{N}\\mathbb{E}[V_{i}^{2}]\\mathbb{E}[V_{i^{\\prime}}^{2}]+\\frac{1}{N^{2}D^{2}}\\sum_{i=1}^{N}\\mathbb{E}[V_{i}^{4}]=\\frac{N-1}{N D^{2}}+\\frac{3}{N D^{2}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "So ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{Var}[v]=\\frac{2}{N D^{2}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "C.2.2 Proof of Prop. 5.3 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We simply need to compute the Jacobian for the dynamical system $G:(w,e,v)\\to(w^{+},e^{+},v^{+})$ . Note that \u2013 specifically at $w=w^{*}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\displaystyle\\frac{\\partial w^{+}}{\\partial w}\\big|_{w=w^{*}}=I_{D}-\\eta_{0}(e+v I)+\\frac{\\eta_{0}^{2}\\gamma^{2}}{N D}w_{*}\\otimes w_{*}^{\\top},}&&{\\displaystyle\\frac{\\partial w^{+}}{\\partial e}\\big|_{w=w^{*}}=0,\\quad\\frac{\\partial w^{+}}{\\partial v}\\big|_{w=w^{*}}=0,}\\\\ &{\\displaystyle\\frac{\\partial e^{+}}{\\partial w}\\big|_{w=w^{*}}=\\star,}&&{\\displaystyle\\frac{\\partial e^{+}}{\\partial e}\\big|_{w=w^{*}}=I_{D^{2}},\\quad\\frac{\\partial e^{+}}{\\partial v}\\big|_{w=w^{*}}=0,}\\\\ &{\\displaystyle\\frac{\\partial v^{+}}{\\partial w}\\big|_{w=w^{*}}=\\star,}&&{\\displaystyle\\frac{\\partial v^{+}}{\\partial v}\\big|_{w=w^{*}}=\\star,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we do not care about the values with a $\\star$ because anyway the resulting matrix is lower-triangular: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{J_{G}(w_{*},e_{*},v_{*})=\\left(\\begin{array}{c c c}{I-\\eta_{0}(v_{*}I+e_{*})+\\frac{\\eta_{0}^{2}\\gamma^{2}}{N D}w_{*}\\otimes w_{*}^{\\top}}&{0}&{0}\\\\ {\\star}&{I}&{0}\\\\ {\\star}&{\\star}&{I}\\end{array}\\right)}\\\\ {=I+\\left(\\begin{array}{c c c}{-\\eta_{0}(v_{*}I+e_{*})+\\frac{\\eta_{0}^{2}\\gamma^{2}}{N D}w_{*}\\otimes w_{*}^{\\top}}&{0}&{0}\\\\ {\\star}&{\\star}&{0}\\end{array}\\right)}\\\\ {\\star}&{}&{\\star}&{0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To be at the edge of stability, we need all eigenvalues of $J_{G}(w_{*},e_{*},v_{*})$ to have absolute value 1. Since $\\eta_{0}>0$ , this implies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{t}_{\\mathrm{max}}\\left[-\\eta_{0}\\big(v_{*}I+e_{*}\\big)+\\frac{\\eta_{0}\\gamma^{2}}{N D}w_{*}\\otimes w_{*}^{\\top}\\right]=-2\\implies\\lambda_{\\mathrm{max}}\\left[\\big(v_{*}I+e_{*}\\big)+\\frac{\\eta_{0}^{2}\\gamma^{2}}{N D}w_{*}\\otimes w_{*}^{\\top}\\right]=\\frac{2}{\\eta_{0}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Next, note that $w_{*}\\otimes w_{*}^{\\top}$ is rank 1, hence its maximum eigenvalue is equal to the trace, that is equal to $\\lVert\\boldsymbol{w}^{*}\\rVert^{2}$ . Finally, note that by Lemma 5.2 the eigenvalues of $\\left(v_{*}I+e_{*}\\right)$ coincide with the eigenvalues of the Hessian for the original simplified loss $\\nabla{\\mathcal{L}}$ . This implies the result. ", "page_idx": 26}, {"type": "text", "text": "C.3 Theory for Standard Parametrization (SP) ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "As in $\\mu\\mathrm{P}$ and NTP, we consider the simplified loss ", "page_idx": 26}, {"type": "equation", "text": "$$\nL(E,V)=\\frac{1}{2}\\left\\|E V-w^{*}\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $E\\in\\mathbb{R}^{D\\times N},V\\in\\mathbb{R}^{N\\times1}$ . Compared to $\\mu\\mathrm{P}$ and NTP, normalizing factors appear in the initialization: $E_{i j}\\sim\\mathcal{N}(0,1/D)$ , $V_{j}\\sim\\mathcal{N}(0,1/N)$ , $\\forall i,j$ . Gradients are $\\partial\\bar{E_{\\L}}=E V V^{\\dagger^{\\star}}-w_{\\ast}V^{\\top}$ , $\\partial_{V}=$ $E^{\\top}E V-E^{\\top}w_{*}$ . ", "page_idx": 26}, {"type": "text", "text": "C.3.1 Dynamics equations, same form as $\\mu P$ and NTP ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Let us rename $w=E V,\\;v=V^{\\top}V,\\;e=E E^{\\top}$ , then the dynamics gradient descent with stepsize $\\eta$ becomes, in $(w,e,v)\\in\\mathbb{R}^{D+D^{2}+1}$ space: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w^{+}=w-\\eta(v\\cdot I_{D}+e)(w-w_{*})+\\eta^{2}(w w^{\\top}-w_{*}w^{\\top})(w-w_{*}).}\\\\ &{e^{+}=e+\\eta\\left[-2w w^{\\top}+w_{*}w^{\\top}+w w_{*}^{\\top}\\right]+\\eta^{2}\\left[v w w^{\\top}-v w_{*}w^{\\top}-v w w_{*}^{\\top}+v w_{*}w_{*}^{\\top}\\right].}\\\\ &{v^{+}=v+\\eta\\left[-2w^{\\top}w+2w_{*}^{\\top}w\\right]+\\eta^{2}\\left[w^{\\top}e w-2w_{*}^{\\top}e w+w_{*}^{\\top}e w_{*}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Hence, under SP, $(w,e,v)$ at different widths have width-independent evolution laws in $\\mathbb{R}^{D+D^{2}+1}-$ same happens under $\\mu$ P. However, in contrast to $\\mu\\mathrm{P}_{i}$ initialization of this system does not converge as $N\\rightarrow\\infty$ . Hence, actual dynamics across widths are drastically different, as we will see next. ", "page_idx": 26}, {"type": "text", "text": "C.3.2 Initialization ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "First, note that trivially $\\mathbb{E}[w_{i}]=0$ , and $\\begin{array}{r}{\\mathbb{E}[w_{i}]^{2}=\\sum_{j,j^{\\prime}=1}^{N}\\mathbb{E}[E_{i j}E_{i j^{\\prime}}V_{j}V_{j^{\\prime}}]=\\sum_{j=1}^{N}\\mathbb{E}[E_{i j}^{2}V_{j}^{2}]=}\\end{array}$ $\\textstyle{\\frac{1}{D}}$ . This makes sense since the forward pass is normalized. However, at initialization: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}[e_{i j}]=\\sum_{k=1}^{N}\\mathbb{E}[E_{i k}E_{j k}]=\\frac{N}{D}\\delta_{i j},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "that already includes a dependency on the width $N$ , further: $\\begin{array}{r}{\\mathbb{E}[e_{i j}^{2}]=\\sum_{k,k^{\\prime}=1}^{N}\\mathbb{E}[E_{i k}E_{j k}E_{i k^{\\prime}}E_{j k^{\\prime}}].}\\end{array}$ Hence ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}[e_{i j}^{2}]=\\sum_{k,k^{\\prime}=1}^{N}\\mathbb{E}[E_{i k}E_{i k^{\\prime}}]\\mathbb{E}[E_{j k}E_{j k^{\\prime}}]=\\sum_{k=1}^{N}\\mathbb{E}[E_{i k}^{2}]\\mathbb{E}[E_{j k}^{2}]=\\frac{N}{D^{2}}\\qquad(i\\neq j),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}[e_{i j}^{2}]=\\sum_{k,k^{\\prime}=1}^{N}\\mathbb{E}[E_{i k}^{2}E_{i k^{\\prime}}^{2}]=\\sum_{k\\neq k^{\\prime}}\\mathbb{E}[E_{i k}^{2}]\\mathbb{E}[E_{i k^{\\prime}}^{2}]+\\sum_{k=1}^{N}\\mathbb{E}[E_{i k}^{4}]=\\frac{N(N-1)}{D^{2}}+\\frac{3N}{D^{2}}=\\frac{N(N+2)}{D^{2}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "So $\\begin{array}{r}{\\mathrm{Var}[e_{i j}]=O\\left(\\frac{N}{D^{2}}\\right)}\\end{array}$ . Finally: ", "page_idx": 27}, {"type": "text", "text": "$\\mathbb{E}[v]=\\sum_{i=1}^{N}\\mathbb{E}[V_{i}V_{i}]=1,$ $\\mathbb{E}[v^{2}]=\\sum_{i,i^{\\prime}=1}^{N}\\mathbb{E}[V_{i}V_{i}V_{i^{\\prime}}V_{i^{\\prime}}]=\\sum_{i\\neq i^{\\prime}}^{N}\\mathbb{E}[V_{i}^{2}]\\mathbb{E}[V_{i^{\\prime}}^{2}]+\\sum_{i=1}^{N}\\mathbb{E}[V_{i}^{4}]=\\frac{N(N-1)}{N^{2}}+\\frac{3N}{N^{2}}=\\frac{N(N+2)}{N^{2}}.$ So $\\begin{array}{r}{\\mathrm{Var}[v]=O\\left(\\frac{1}{N^{2}}\\right)}\\end{array}$ . ", "page_idx": 27}, {"type": "text", "text": "C.3.3 Conclusion ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "While the laws $(w,e,v)\\to(w^{+},e^{+},v^{+})$ are not width dependent in SP, $e$ has initialization of scale $O(N)$ . As such, while the forward pass (i.e. $w$ ) is $O(1)$ at initialization, the trajectory of $(w,e,v)$ under gradient descent starts at a width-dependent point $(O(1),O(N),O(1))$ ; hence it is drastically different at different widths. Further, the NTK $(v\\cdot I_{D}+e)$ is also $O(N)$ , and this controls the evolution of the forward pass $w$ : $w^{+}=w-\\eta(v\\cdot I_{D}+e)(w-w_{*})+O(\\eta^{2})$ . We therefore validate also in this simple setting the derivation by [5]: if $\\eta=O(1)$ , forward pass of SP blows up after one step of gradient descent. ", "page_idx": 27}, {"type": "text", "text": "D Connection between the eigenvalues of the Hessian and NTK matrix ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Recall that for MSE loss and one-dimensional output $f(x)$ , the Gauss-Newton decomposition of the Hessian $H$ reads: ", "page_idx": 28}, {"type": "equation", "text": "$$\nH=\\mathcal{G}+\\mathcal{R}=\\sum_{i=1}^{|\\mathcal{D}|}\\nabla_{\\theta}f(x_{i})\\nabla_{\\theta}f(x_{i})^{\\top}+\\sum_{i=1}^{|\\mathcal{D}|}\\nabla_{\\theta}^{2}f(x_{i})(y_{i}-f(x_{i})),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\mathcal{G}$ is the Gaussian-Newton matrix. This can be generalized to (1) different loss functions and (2) multidimensional output $f(x)\\in\\mathbb{R}^{k}$ , where $k$ is the dimension of the logits (i.e. the number of classes in classification problems). Here we notice that in (2) we exactly preserve the connection between GGN and NTK\u2019s spectra, while in (1) we have an extra term in $\\mathcal{G}$ that causes a deviation from the exact correspondence. However, we show that this deviation is largely negligible in practice, in the sense that $\\mathcal{G}$ will have the same spectrum as the NTK as training progresses, and $\\mathcal{G}$ still dominates $\\mathcal{R}$ , as one would expect from the experiments in the main text (e.g Fig. 1). We begin by defining the Gauss-Newton matrix (GN) in the case of cross-entropy loss, following [12]: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{G}=\\sum_{i=1}^{|\\mathcal{D}|}\\nabla_{\\theta}f(x_{i})\\bar{H}_{\\mathcal{L}}\\nabla_{\\theta}f(x_{i})^{\\top}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where now $\\nabla_{\\theta}f(x_{i})\\in\\mathbb{R}^{k P}$ , and $\\bar{H}_{\\mathcal{L}}\\in\\mathbb{R}^{k P\\times k P}$ is a block-diagonal matrix where the $k\\times k$ Hessian of the loss $(H_{\\mathcal{L}})$ with respect to model output is repeated $P$ times. Again, we can stack the Jacobian vectors $\\nabla_{\\theta}f(x_{i})$ for all the datapoints into $K\\in\\mathbb{R}^{|\\mathcal{D}|\\times k P}$ , thus: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{G}=K^{\\top}\\bar{H}_{\\mathcal{L}}K\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For MSE loss, $\\bar{H}_{\\mathcal{L}}$ is the identity, hence the correspondence to the NTK matrix is maintained (same sharpness). However, for the cross-entropy loss, the first derivative of the loss with respect to the model output $\\Delta:=\\nabla_{f(x)}\\mathcal{L}$ can be shown to be $\\Delta=\\sigma(f(x))-y$ , where $y$ is the one hot vector encoding the true classes, and $\\sigma(\\cdot)$ denotes the softmax activation. Hence, for the Hessian, we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n[H_{\\mathcal{L}}]_{i j}=\\delta_{i j}\\sigma(f(x))_{i}-\\sigma(f(x))_{i}\\sigma(f(x))_{j},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which in general deviates from the identity. However, during training the model increase the probability of correct predictions, and thus $H_{\\mathcal{L}}$ gets closer to the identity, thus having an asymptotically negligible effect on the Gauss-Newton matrix $\\mathcal{G}$ and its correspondence to the NTK. ", "page_idx": 28}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/e70fbdfd038a2b6a691c9771047f71301b6e9c55bb8293b813a3e6de172ce3e3.jpg", "img_caption": ["Figure 11: Norm of the residual and top eigenvalue of the GN matrix, where the vector field shows the evolution of these quantities during training for a fixed learning rate. Left: $\\mu\\mathrm{P}-$ note that all curves have a sharpening phase, after which the residual continues to decrease. Right: NTP - Increasing the width reduces the sharpening, since it approaches the infinite width limit where the NTK matrix becomes asymptotically constant. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "We now perform experiments to test whether $\\mathcal{G}$ dominates the residual for cross-entropy loss, in order to support our claim on the connection between feature learning and optimization. We plot the evolution of the largest eigenvalue of the GN matrix and the residual norm through time in Figure 11 for $\\mu\\mathrm{P}$ and NTP. The largest eigenvalue of this matrix is computed using a power iteration algorithm based on the implementation provided in [91]. Note that while for $\\mu\\mathrm{P}$ we observe a large amount of progressive sharpening during training, for NTP the sharpness becomes asymptotically constant. The architecture used for the plot is a convolutional neural network as described in J, trained on CIFAR-10 for 10 epochs using cross-entropy loss. These experiments confirm the results obtained for MSE loss in the main text (Fig. 5). ", "page_idx": 28}, {"type": "text", "text": "E Late-time dynamics and batch size ablations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "E.1 Late-time dynamics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "It was noted in [16] (Appendix C) that with cross-entropy loss (adopted here), the sharpness decreases towards the end of training. Here in Fig. 12, we show that while the dynamics are remarkably consistent during the first part of training, they diverge during the phase transition in which the sharpness begins to drop, with wider models starting to exhibit the sharpness drop earlier. Hence, this transition point is highly width-dependent, and it coincides with a slight shift in the optimal learning rate. This late phase happens when the classifier maximizes the margin between classes [16], and can be largely prevented by using data augmentation techniques, as we exemplify in Sec.E.2. ", "page_idx": 29}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/b539a03e3c3253131b2364a3a0ddaeda0669b18417e90ac7b13e1e38789c8742.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 12: Late-time dynamics. We study the same setting as in Fig. 26 (for $\\mu\\mathrm{P}_{.}$ ), under longer training time. Notice how when training longer, the sharpness decreases once the model gets closer to convergence. In this phase, there is a shift of the optimal learning rate, as the bottom row shows. ", "page_idx": 29}, {"type": "text", "text": "E.2 The effect of Batch Size and Data Augmentation ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Batch Size We test what happens to the sharpness and optimal learning rate when the batch size is increased. The sharpness is well-known to increase with the batch size, as it is shown also in Cohen et al. [16] and Jastrzkebski et al. [31]. ", "page_idx": 29}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/91d950f15a0a6bf507c7848a24dc71745dcb8dfa72a5f3e24c15d5c2273e149b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 13: Batch size ablation on a three layer convolutional network. The red dashed line indicates the sharpness of $4/\\eta_{0}$ , only shown for the largest learning rate where the sharpness rises above the EoS threshold. Parameters: dataset: CIFAR-10, with data augmentation, epochs $=50$ . The learning rate shown above are: (0.32, 0.88, 2.45) ", "page_idx": 29}, {"type": "text", "text": "Here we add that under $\\mu\\mathrm{P}_{:}$ , the batch size we tested (128, 256, 512) do not influence significantly the width-independence phenomenon of the sharpness, as we summarize in Fig. 13. We observe good learning rate transfer across all the tested batch sizes. We also observe that the optimal learning rate increases with the batch sizes by roughly a factor of 2. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "Data Augmentation We repeat the same experiment, varying the batch size, but this time we turn on data augmentation using standard transformations (random crops, horizontal flips, 10-degrees rotations). The results are in Fig. 14 (to compare with Fig. 13). Notice how data augmentation has a stabilizing effect on the sharpness, delaying the late phase of training where the sharpness drops, as analyzed in Sec. E.1 [16, 28]. On the other hand, Thus, under regularization techniques such as data augmentation, we should expect better hyperparameter transfer. ", "page_idx": 30}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/47496c73828afc267749b549924154fdaca5e88f23cf78493240195dabae5f18.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 14: Batch size ablation. The red dashed line indicates the sharpness of $4/\\eta_{0}$ , only shown for the largest learning rate where the sharpness rises above the EoS threshold. Parameters: dataset: CIFAR-10, without data augmentation, epochs $=50$ . The learning rate shown above are: (0.32, 0.88, 2.45) ", "page_idx": 30}, {"type": "text", "text": "F Large-Scale experiments, more Datasets and Optimizers ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We provide empirical validation of our findings and show that in realistic architectures, such as Transformers (ViTs, GPT-2) and ResNets, we achieve width (or depth, respectively) independent sharpness. These results empirically demonstrate that our findings extend to different modalities and architectures. ", "page_idx": 30}, {"type": "text", "text": "F.1 GPT-2 experiments on WikiText ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Figure 15 shows the transfer of a GPT-2 model trained on WikiText for 40 epochs with Depth- $\\cdot\\mu\\mathrm{P}$ and Adam, without learning rate scaling. A similar plot but for $\\mu\\mathrm{P}$ (without the depth parameterization) is presented in Figure 16. Note that the sharpness exhibits a width independent behaviour. ", "page_idx": 30}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/7cbd5b6a3b03af796520bad997dad3551f259f52a40315cde2780da532e13ee9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 15: GPT-2 trained on WikiText using Adam, with fixed learning rate, width 512. Note that due to the structure of the transformer block containing $k\\geq2$ layers per block, the transfer is imperfect and thus the sharpness is also not super consisten\u221at. Here, in contrast to Depth- $\\mathbf{\\nabla}\\mu\\mathbf{P}^{\\dagger}$ \u2019s prescription in Table 1, we do not rescale the learning rate by $1/\\sqrt{L}$ . See also App. J. ", "page_idx": 30}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/995049606f09c8e6f1bfe79427567ffd2b3e5d8dfe5e43af3efe809721b3f7ad.jpg", "img_caption": ["Figure 16: Post-LN Transformers (similar to GPT-2) trained with Adam on WikiText-2 parameterized with $\\mu\\mathrm{P}$ showing learning transfer in width (a) and super consistent sharpness evolution (b). HPs: 2 layers, 2 heads, 20 epochs, batch size 512, 100 warmup steps, sequence length 35. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "F.2 ConvNets Experiments on Larger Datasets and Adam(W) ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Figures 17 and 18 show residual ConvNets parameterized with $\\mu\\mathrm{P}$ and Depth- $\\mathbf{\\nabla}\\mu\\mathbf{P}$ respectively trained on TinyImagenet, and Figure 19 shows the evolution of a similar model parameterized with $\\mu\\mathrm{P}$ trained on Imagenet. Similarly, we study the evolution of residual ConvNets when trained with Adam on CIFAR-10 with 1 and 2 layers per block in Figure 20. Finally, we show the results of training residual ConvNets under $\\mu\\mathrm{P}$ with Adam and AdamW in Figure 21. ", "page_idx": 31}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/6e59f56a156aee497bdc9a8041338ee7d321b715b9446632f18ff8d61e720765.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 17: Residual convolutional networks (ResNets) trained on Tiny-ImageNet with stochastic gradient descent. Left figure shows the learning rate transfers across width in ResNets parameterized with $\\mu\\mathrm{P}.$ Right figure shows that for a fixed learning rate, the sharpness becomes width independent during training. Parameters: batch size 64, epochs 10. ", "page_idx": 31}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/0754a99b69b49390592ee2ec60f9e9475e6ebb80b2f19b6b1f1feed89dbbd20c.jpg", "img_caption": ["Figure 18: Residual convolutional networks trained on Tiny-ImageNet with stochastic gradient descent. Left figure shows the learning rate transfers across depth in ResNets parameterized with Depth $-\\mu\\mathrm{P}.$ . Right figure shows that for a fixed learning rate, the sharpness becomes depth independent during training. Parameters: batch size 64, epochs 10. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/460820103bdbc0f5beb392e9deb237f433eb3b757488ea15b0aa9ecfd542de1a.jpg", "img_caption": ["Figure 19: Residual convolutional networks (ResNets) trained on ImageNet with stochastic gradient descent. Left figure shows the learning rate transfers across width in ResNets parameterized with $\\mu\\mathrm{P}.$ Right figure shows that for a fixed learning rate, the sharpness becomes width independent during training. Parameters: batch size 128, epochs 1. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/79637b00b427e0a2eda8cdde1b38cc369021cf119c17d484a9840ea0565c7695.jpg", "img_caption": ["Figure 20: ConvNets trained with Adam with fixed learning rate on CIFAR-10. (a) One layer per block, showing that the learning rate transfers and we have sharpness Super Consistency. (b) When training with $k=2$ layers per block, notice that the transfer worsens. While the effect is subtle in this setting, it translates to the sharpness accumulating finite-size effects during training. Here, we use the Depth- $\\mu\\mathrm{P}$ prescription for Adam reported in Table 1. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "G Time Evolution of other Spectral Quantities ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section we present convergence rates for various other spectral quantities of interest. In Figure 22, we show the same Super Consistency of the landscape in the case of Depth- $\\mathbf{\\nabla}\\mu\\mathbf{P}$ , as exhibited in the width case in Figure 2. In Figure 23, we present the convergence rate of the largest NTK eigenvalue, sharpness and loss respectively, as well as the evolution of the Hessian trace during time. Figure 24 (a) shows the loss curve evolution under $\\mu\\mathrm{P}$ and NTP regime. Note in Figure 24 (b) and (c) how the sharpness achieves a near EoS value in the case of $\\mu\\mathrm{P}$ , whereas for NTP wider networks have a sharpness that remains closer to initialization. Finally, in Figure 25 we show that in the case of having multiple linear layers per block, this leads to violations of Super Consistency, and thus imperfect learning rate transfer. Note that this is the same case as in the GPT-2 plots illustrated in Figure 15. ", "page_idx": 32}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/0f7377131b713834724cbc013d23b02af41a39851917df0988c769dc12ff452f.jpg", "img_caption": ["Figure 21: Convolutional networks trained with Adam (top) and AdamW (bottom, weight decay 0.001) on CIFAR-10 parameterized with $\\mu\\mathrm{P}$ showing learning transfer in width (a) and super consistent sharpness evolution (b). HPs: 20 epochs, 3 layers, no skips, batch size 256, 200 warmup steps. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/a6337dbdc97d732360f18c56f87880317b84a2aed4c31f19cf68551b9cec23f2.jpg", "img_caption": ["Figure 22: Convergence rates with respect to time for the losses and sharpnesses shown in Figure 4. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "H Sharpness evolution in Random Feature Models ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this section, we compare the NTP and $\\mu\\mathrm{P}$ parameterizations to a random feature model, i.e. a model where we all the weights of the intermediate layers are frozen to their value at initialization, and only the final readout layer is trained. Crucially, this model does not learn features by construction for any learning rate and any width. The results are shown in Fig. 26. Notice how the transfer in the random feature model is achieved only at very large widths compared to $\\mu\\mathrm{P}.$ However, the transfer is better than in NTP. This is in line with our claim, as under a random feature model increasing the learning rate does not induce more feature learning, as is the case in NTP. ", "page_idx": 33}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/592c9a08306feceaa6787bcfc532ee761f20bcf2c9bca5e9ce9eb1f8ff349711.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 23: (a) Convergence rate of the largest NTK eigenvalue in width at multiple steps during training.(b) Convergence rate of the sharpness in width at multiple steps during training. Note that the largest NTK eigenvalue starts width independent for both learning rates, but becomes width dependent during training, as opposed to the sharpness which maintains a width independent dynamic throughout the whole optimization process (see Fig. 24). (c) Loss convergence rate in width; note that the loss accumulates finite-size effects during time, exhibiting a wider is better effect during training. (d) Hessian trace evolution during training at various widths. Unlike the sharpness, the trace has a width dependent period at the beginning of training, but approaches a width-indepedent threshold. Details: Residual ConvNet trained on CIFAR10 with cross-entropy loss. ", "page_idx": 34}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/11e32fbccea1b38b503a3f4f4e827df8f9a32e13d1204e824ec52d2791d98703.jpg", "img_caption": ["Figure 24: Early training dynamics in $\\mu\\mathrm{P}$ (top row) and NTP parameterization (bottom row). (a) Loss curves. (b): Sharpness dynamics. Notice how for $\\mu\\mathrm{P}$ progressive sharpening until EoS (black dashed line) is achieved at any width and with comparable speed, while for NTP the time to EoS progressively increase with width. Also, the loss curves start to depart from each other as training progresses, while $\\lambda$ stays at EoS for a more sustained period ot time. (c) Sharpness vs width at selected time steps. For $\\mu\\mathrm{P}$ , $\\lambda$ converges very fast in width, while in NTP it diminishes. Other parameters: architecture: Three-layer convolutional network. Dataset: CIFAR-10, without data augmentation. $B=128.$ , epochs $=1$ , learning rate $\\eta_{0}=0.8$ for $\\mu\\mathrm{P}$ and 8 for NTP. The reported width is the size of the readout layer. "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/81fe37031fa904740904787854a3329814dd3ccce8dd115b7ded33a3684e3c23.jpg", "img_caption": ["Figure 25: Dynamics of some of the eigenvalues of the Hessian (left) and NTK (right) under Depth- $\\mathbf{\\nabla}\\mu\\mathbf{P}$ scaling with $k=2$ layers per residual block. We observe violations on Super Consistency in both cases. The model is the same as in Fig. 4 (center). This is compatible with absence of learning rate transfer. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/ede7c7a34bb438bc472b2ad1e502109b07bd70e636c01d6d57de7e3d9ea8ed05.jpg", "img_caption": [], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Figure 26: Learning rate transfer plot (top row) and sharpness dynamics (bottom row) for a three-layer convolutional network and three different settings. (a) random feature model (only the readout layer is trained), (b) and (c) correspond to $\\mu\\mathrm{P}$ and NTP parameterizations, respectively. In random feature learning, the absence of feature learning prevents the sharpness\u2019 evolution at any width, thus learning rate transfer coincides with the convergence of the sharpness $\\lambda$ at initialization. Also notice how for NTP, the progressive sharpening converges to $\\lambda=2/\\eta_{0}$ at a much lower speed as the width increases, in line with the early dynamics reported in Fig. 24. Other parameters: $B=128$ , epochs $=20$ for the $\\mu\\mathrm{P/NTP}$ models and 10 for the random feature model, dataset: CIFAR-10, without data augmentation. ", "page_idx": 35}, {"type": "text", "text": "I Directional sharpness ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In Figure 27 we provide the evolution of the directional sharpness, which captures the curvature along the gradient direction during training under $\\mu\\mathrm{P}$ and Depth- $\\mu\\mathrm{P}$ respectively in ConvNets. Note that, similar to the sharpness plots, the directional sharpness, defined as $\\frac{\\nabla_{\\theta}\\mathcal{L}^{\\top}H\\nabla_{\\theta}\\mathcal{L}}{||\\nabla_{\\theta}\\mathcal{L}||^{2}}$ also follows a width independent trajectory during training. This measure has been used, for instance, in Gur-Ari et al. [17]. ", "page_idx": 35}, {"type": "text", "text": "J Experimental details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "The experiments were ran on A100 and H100 GPUs, with 80GB VRAM. Each experiment averaged less than 24 hours total execution time. Unless stated otherwise, we use data augmentations, where the random transformations are compositions of crops, horizontal flips, and 10-degree rotations. Additionally, we provide further details on the models used in our experiments and the modifications we have introduced. ", "page_idx": 35}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/8cbfacaea532514091a50400f32b31cb738105606a21815d8506d22359de4880.jpg", "img_caption": ["Figure 27: Convolutional networks trained with SGD on CIFAR-10 parameterized with $\\mu\\mathrm{P}$ (top) and Depth- $\\cdot\\mu\\mathrm{P}$ (bottom) showing learning transfer (a) and super consistent Hessian-gradient alignment during training (b, d). HPs: (top) 100 warmup steps, batch size 256, 20 epochs, no residual connections (bottom) 200 warmup steps, batch size 128, 40 epochs, \u221a1L scaling (both) 6 layers, ReLU. "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "rgwhJ7INtZ/tmp/e3b005644d5245ace7c5df1a3fcbdd5cc344158fecb7670df0d5dd65dbe24db5.jpg", "img_caption": ["Figure 28: Coordinate check in $\\mu\\mathrm{P}$ . "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "J.1 Hessian Computation ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "The implementations of our models are done in PyTorch. For the Hessian measurements, we use PyHessian [92]. In particular, the library adopts the Power Iteration method for the top $k$ Hessian eigenvalues (i.e. including the sharpness) and the Hutchinson method for trace computation. Both methods adopts Hessian vector products to avoid computing the whole Hessian. This reduces the time complexity from quadratic to linear in the number of parameters. In both algorithms, we fix the number of iterations and tolerance between consecutive eigenvalue computation to the default values of 100 and 0.001, respectively. We measure the sharpness on the same fixed batch throughout training. ", "page_idx": 36}, {"type": "text", "text": "SGD Among the equivalent ways of parametrizing then network with $\\mu\\mathrm{P},$ we opt for the one that rescales the learning rate by the width, i.e. $\\eta=\\eta_{0}\\gamma^{2}=\\eta_{0}N$ . This effectively sets the EoS threshold to the width-dependent value of $2/(N\\eta_{0})$ . In our plot, we take this scaling difference into account by computing the eigenvalues of the scaled Hessian $\\dot{\\gamma}^{2}H=N H$ . With respect to learning rate transfer, such a rescaling makes intuitive sense, as it is $\\eta_{0}$ that is transferring, and not $\\eta$ . ", "page_idx": 36}, {"type": "text", "text": "Adam Adam updates are of the form $\\theta_{t+1}=\\theta_{t}-\\eta P_{t+1}^{-1}m_{t+1}$ , where $m_{t}$ is a momentum vector computed in the exponential moving average fashion: $m_{t+1}=\\beta_{1}m_{t}+(1-\\beta_{1})g_{t+1}$ , where $g_{t}$ are the gradient of the loss at time $t$ . $P_{t+1}$ is a diagonal preconditioner of the form ", "page_idx": 36}, {"type": "equation", "text": "$$\nP_{t}=(1-\\beta_{1}^{t})\\left[\\mathrm{diag}\\left(\\sqrt{\\frac{\\nu_{t}}{1-\\beta_{2}^{t}}}\\right)+\\epsilon I\\right],\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\nu_{t}:=\\beta_{2}\\nu_{t-1}+(1-\\beta_{2})g_{t}^{2}$ , where $g_{t}^{2}$ is the element-wise squared gradients. In the experiments of this paper, we consider the preconditioned Hessian $P^{-1}H$ , for which it is shown that its largest eigenvalue converges to the EoS threshold of \u03b72((11+\u2212\u03b2\u03b21)) in Cohen et al. [61]. Furthermore, we use the Adam $\\mu\\mathrm{P}$ parametrization in Table 8 of Yang et al. [6], which rescales the learning rates of the hidden layers (i.e. with both input and output dimensions that scale with $N$ ) by $1/N$ . To account for this, we further adjust the Hessian by computing $\\tilde{H}=D P^{-1}H$ , where $D$ is a diagonal matrix containing the learning rate for each parameter. We always report spectral quantities of $\\tilde{H}$ in the experiments with Adam. With these modifications, we expect $\\begin{array}{r}{\\lambda_{\\operatorname*{max}}(\\tilde{H})=\\frac{2(1+\\beta_{1})}{(1-\\beta_{1})}}\\end{array}$ , which is 38 for the full-batch case and the default $\\beta_{1}=0.9$ . Indeed, this is what we observe in our experiments. We stress that we expect Super Consistency of this preconditioned Hessian, where in $\\mu\\mathrm{P}$ different layers may have a different dependence on th\u221ae width [6]. Finally, we point out that in the Depth- $\\mu\\mathrm{P}$ experiments, we reported the hessian of $N/\\sqrt{L}H$ . Although this produces results that deviate from the prescription of Cohen et al. [61], it is sufficient to capture Super Consistency in depth, as each layer has the same depth-dependence. ", "page_idx": 37}, {"type": "text", "text": "J.2 GPT-2 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "The backbone architecture in the experiments presented on Wikitext is the standard GPT-2 transformer introduced by [33], with the Depth- $\\cdot\\mu\\mathrm{P}$ parameterization changes presented in [6, 8, 53]. Crucially, the following modifications are introduced by the $\\mu\\mathrm{P}$ parameterization: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The attention map is rescaled byd1Q , as opposed to\u221a1dQ \u2022 The residual branch is downscaled by $\\frac{1}{\\sqrt{L}}$ , where $L$ is the number of layers in the model ", "page_idx": 37}, {"type": "text", "text": "Our implementation is based on the implementation provided by Yang et al. [6], with the addition of the residual scaling. This uses a different parametrization from the one reported in Table. 1 but equivalent dynamics, obtainable using their \u201cabc-rule\". Similar to the experiments performed on ViTs, the GPT-2 models are trained using Adam, where the base width is fixed and the depth is varied (and vice versa for the Depth- $\\mu\\mathrm{P}$ case). In addition, for the fixed width, increasing depth experiments, we place the layer normalization layer in front of the residual, following the Pre-LN architecture, unless stated otherw\u221aise, and we do not use any learning rate warmup. Note that in this setting we also train without the $1\\sqrt{L}$ scaling of the learning rate that the theory would prescribe (summarized in Table 1). This follows following the heuristic prescription of Bordelon et al. [7]. As in their case, we empirically observed that we did not get learning rate transfer if we used the scaling. In the fixed depth, increasing width case, we use a linear rate warmup, with no decay, and we use a standard Post-LN GPT-2 style architecture. ", "page_idx": 37}, {"type": "text", "text": "J.3 Vision Transformers (ViTs) ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "The ViT implementation is based on the work of [93] and follows the same tokenization and training protocol. In order to follow the Depth- $\\mu\\mathrm{P}$ parameterization, we make the same modifications as in J.2. The models are trained with Adam. ", "page_idx": 37}, {"type": "text", "text": "J.4 ResNet ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "We use convolutional neural networks with skip connections, with $3\\times3$ kernels and stride 1. We apply pooling after every 3rd layer, followed by a subsequent convolutional \u221alayer. Following Bordelon et al. [7] and Yang et al. [8], we downscale the residual branches by $1/\\sqrt{L}$ . Note that in the Depth- $\\mu\\mathrm{P}$ setting we also scale the learning rate as $1\\sqrt{L}$ in these experiments. ", "page_idx": 37}, {"type": "text", "text": "J.5 Coordinate check for $\\mu\\mathbf{P}$ ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In Figure 28 we check the coordinatewise evolution of the activations at hidden layers within a ConvNet. Note that the evolution is width independent, as predicted by $\\mu\\mathrm{P}$ theory. ", "page_idx": 37}, {"type": "text", "text": "K Summary of Feature Learning Parametrizations ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Here we summarize the parametrizations used. In Table 1, we report the scaling of the learning rate, output multiplier $\\gamma$ , depth-scaling exponents $\\alpha$ for the residual branches for Depth- $\\mathbf{\\nabla}\\mu\\mathbf{P}$ (both SGD and Adam) [7, 8, 94]. $\\mu\\mathrm{P}$ is recovered as a special case, where the depth dependence is ignored. Notice that this version of Depth- $\\cdot\\mu\\mathrm{P}$ for Adam is obtained with a simplification, setting Adam\u2019s $\\epsilon$ parameter to zero, where Sign-GD is recovered. ", "page_idx": 38}, {"type": "table", "img_path": "rgwhJ7INtZ/tmp/485fc5887631f34e1bb18bfc60d90053ed8fce5b49ab366a9debc17e019b6c89.jpg", "table_caption": [], "table_footnote": [], "page_idx": 38}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We provide extensive evaluations for the claims in the paper on Super Consistency and its relation to learning rate transfer. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 39}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: We provide a discussion on the limitations of this work. In particular, when it comes to the link between sharpness and optimal learning rate, and on the computational resources required to get Hessian, which prevents larger scale experiments. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 39}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We explicitly state all the assumption and simplifications, as well as an organized appendix with all the proofs. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 40}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We provide a detailed experimental details section, as well as formulas and choice of hyperparameters. We will release the code upone acceptance. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 40}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We will release the code upon acceptance ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 41}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We have a dedicated experimental details Section. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 41}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We have run the experiments with multiple seeds, and provided error bars in the plots (confidence intervals given by the Matplotlib Python library) ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We provide this info. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 42}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We are conform with the code Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 42}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: No societal impact for this largely theoretical work. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: We do not pose such risks, and only use public data. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 43}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: we provide info and citations for the packages that we use. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 43}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 44}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] Justification: We do not release new assets. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 44}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: Not applicable ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 44}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: not applicable. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 44}]