{"references": [{"fullname_first_author": "A. Jacot", "paper_title": "Neural tangent kernel: Convergence and generalization in neural networks", "publication_date": "2018-12-01", "reason": "This paper introduced the Neural Tangent Kernel (NTK), a crucial concept for understanding the behavior of infinitely wide neural networks, which is used extensively in the current paper's theoretical analysis."}, {"fullname_first_author": "G. Yang", "paper_title": "Tensor programs iv: Feature learning in infinite-width neural networks", "publication_date": "2021-07-01", "reason": "This paper introduced the \u00b5P framework, a parameterization that facilitates learning rate transfer across different model sizes, a key phenomenon investigated and explained by this paper."}, {"fullname_first_author": "G. Yang", "paper_title": "Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer", "publication_date": "2022-03-01", "reason": "This paper extends the \u00b5P framework to show its effectiveness for transfer of hyperparameters, such as learning rate, across larger, more realistic neural network models, aligning with the current paper's empirical findings."}, {"fullname_first_author": "B. Bordelon", "paper_title": "Depthwise hyperparameter transfer in residual networks: Dynamics and scaling limit", "publication_date": "2023-09-01", "reason": "This paper studies the Depth-\u00b5P framework, extending \u00b5P to deep networks and further supporting the theoretical analysis of the learning rate transfer phenomenon central to the main paper."}, {"fullname_first_author": "N. Vyas", "paper_title": "Feature-Learning Networks Are Consistent Across Widths At Realistic Scales", "publication_date": "2023-05-01", "reason": "This paper empirically demonstrates the consistency of neural network landscapes across different model sizes, supporting the main claims of the current paper regarding Super Consistency and its connection to hyperparameter transfer."}]}