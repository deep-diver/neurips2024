[{"type": "text", "text": "An Analytical Study of Utility Functions in Multi-Objective Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Manel Rodriguez-Soto ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Juan A. Rodriguez-Aguilar ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Artificial Intelligence Research Institute (IIIA-CSIC) Bellaterra, Spain manel.rodriguez@iiia.csic.es ", "page_idx": 0}, {"type": "text", "text": "Artificial Intelligence Research Institute (IIIA-CSIC) Bellaterra, Spain jar@iiia.csic.es ", "page_idx": 0}, {"type": "text", "text": "Maite Lopez-Sanchez Universitat de Barcelona (UB) Barcelona, Spain maite_lopez@ub.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multi-objective reinforcement learning (MORL) is an excellent framework for multi-objective sequential decision-making. MORL employs a utility function to aggregate multiple objectives into one that expresses a user\u2019s preferences. However, MORL still misses two crucial theoretical analyses of the properties of utility functions: (1) a characterisation of the utility functions for which an associated optimal policy exists, and (2) a characterisation of the types of preferences that can be expressed as utility functions. In this paper, we contribute to both theoretical analyses. As a result, we formally characterise the families of preferences and utility functions that MORL should focus on: those for which an optimal policy is guaranteed to exist. We expect our theoretical results to foster the development of novel MORL algorithms that exploit our theoretical findings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sequential decision-making problems are ubiquitous, impacting areas like autonomous driving [6], robotics [35], finance [3] and healthcare [4], to name a few. Recently, Reinforcement Learning (RL) has emerged as a pivotal framework for addressing sequential decision-making tasks [11, 13]. Most of the RL literature has focused on problems for which an agent deals with a single objective (e.g. get rich in finance, win a race). However, real-world scenarios often present multiple, conflicting objectives [32] (e.g., a self-driving car must ensure safety, efficiency, and passenger comfort). ", "page_idx": 0}, {"type": "text", "text": "Multi-Objective Reinforcement Learning (MORL) has developed as one of the most promising frameworks for addressing multi-objective decision-making [19, 18, 22]. Despite its novelty compared to single-objective RL, the current state of the art in MORL shows promising results for tackling real-world problems that are inherently multi-objective [10, 32]. Most MORL approaches are utilitybased and assume that there exists a utility function [10] that combines all objectives into a single one, allowing the learning agent to ponder between them. However, deciding the most appropriate utility function is a problem in itself. Given that, the literature on MORL focuses on learning a set of candidate policies, called the undominated set [10], which maximise all possible utility functions. In that way, once a utility function is decided, the decision-maker can directly select the policy from the undominated set that maximises it. ", "page_idx": 0}, {"type": "text", "text": "Thus, utility functions are widely considered a fundamental concept of MORL [31]. Utility functions capture a user\u2019s preferences over different objectives and drive the learning [22]. Hence, both concepts (utilities and preferences) are at the core of state-of-the-art MORL. The state-of-art approach of considering the undominated set as the most general solution concept of MORL relies on two assumptions: ", "page_idx": 1}, {"type": "text", "text": "1. On utilities: It assumes that for every utility function, there exists a policy optimising it.   \n2. On preferences: It assumes that any user preference can be expressed as a utility function. ", "page_idx": 1}, {"type": "text", "text": "Unfortunately, none of the assumptions is correct. There are many examples of preferences that cannot be expressed with a utility function (e.g., the lexicographic order [34], as proved in [5]). Likewise, even for problems with a finite amount of possible states and actions, there are many utility functions for which there is no optimal policy for any state (we provide an explicit example in the later sections). ", "page_idx": 1}, {"type": "text", "text": "These two counterexamples raise the need for answering the following two main theoretical questions: (1) for which type of utility functions are optimal policies guaranteed to exist? (2) what types of preferences can be represented as utility functions? The state of the art on MORL has not addressed these fundamental research questions so far. ", "page_idx": 1}, {"type": "text", "text": "Against this background, we propose an in-depth analysis of utility functions in MORL by means of the following three contributions: ", "page_idx": 1}, {"type": "text", "text": "1. We provide two novel MORL fundamental theoretical concepts. We introduce the first formal definition of preferences between policies in MORL and the first formal definition of utility maximisation in MORL.   \n2. Given a utility function in MORL, we characterise the sufficient conditions that guarantee the existence of an optimal policy maximising it.   \n3. We characterise under which conditions we can express preferences between policies as utility functions. These are represented by a type of function called quasi-representative utility function, which preserves the most preferred policies as its maximal points. ", "page_idx": 1}, {"type": "text", "text": "We expect that our theoretical results will lead to novel MORL algorithms that can exploit the analytical properties of the utility functions introduced here. ", "page_idx": 1}, {"type": "text", "text": "The remainder of this paper is organised as follows. Section 2 provides the necessary background in multi-objective reinforcement learning. Then, Section 3 provides sufficient conditions to guarantee the existence of utility-maximising policies. Next Section 4 characterises the family of preferences that can be represented with utility functions. Thereafter, Section 5 presents the related work. Finally, Section 6 summarises our main theoretical findings and sets paths for future research. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Single-objective reinforcement learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In single-objective reinforcement learning (RL), sequential decision-making problems are formalised as Markov decision process (MDP) [11, 27]. An MDP represents an environment in which an agent is capable of repeatedly acting upon it to make it to transition it to a different state, and immediately receive a scalar reward (representing the agent\u2019s objective) after each action: ", "page_idx": 1}, {"type": "text", "text": "Definition 1 (Markov Decision Process). A (single-objective)1 Markov Decision Process (MDP) is defined as a tuple $\\langle S,A,R,T\\rangle$ of two sets and two functions: the set of states $\\boldsymbol{S}$ , the set of actions $\\boldsymbol{\\mathcal{A}}(s)$ available at each state $s$ , the reward function $R:S\\times\\mathcal{A}\\times S\\rightarrow\\mathbb{R}$ , and the transition function $T:S\\times\\mathcal{A}\\times\\mathcal{S}\\rightarrow[0,1]$ specifying the probability $T(s,a,s^{\\prime})=\\mathbb{P}(s^{\\prime}\\mid s,a)$ that the next state is $s^{\\prime}\\,i f$ an action a is performed upon the state $s$ . ", "page_idx": 1}, {"type": "text", "text": "An agent\u2019s behaviour in an MDP is called a policy $\\pi$ . A policy $\\pi(s,a)$ describes how likely an agent will perform action $a$ if the agent is currently in state $s$ . The agent\u2019s objective is to learn the policy that accumulates the maximum sum of discounted rewards. Thus, to evaluate a given policy, we need to compute the (expected) discounted sum of rewards that an agent obtains by following it. This operation is formalised by means the so-called value function $V:S\\rightarrow\\mathbb{R}$ , defined as: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nV^{\\pi}(s)\\triangleq\\!\\!\\mathbb{E}[\\sum_{k=0}^{\\infty}\\gamma^{k}R(S_{t+k},A_{t+k},S_{t+k+1})\\mid S_{t}=s,\\pi],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\gamma\\in[0,1)$ is the discount factor, indicating how much we care about future rewards. Value functions allow us to partially order policies [27]. Hence, they allow to formalise the agent\u2019s objective as learning the policy that maximises the value function. This policy is defined as the optimal policy: ", "page_idx": 2}, {"type": "text", "text": "Definition 2 (Optimal policy). Given an MDP $\\mathcal{M}$ , its optimal policy $\\pi_{*}$ is the policy that maximises the value function $V^{\\pi}$ . Formally: ", "page_idx": 2}, {"type": "equation", "text": "$$\nV^{\\pi_{*}}(s)\\geq V^{\\pi}(s),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for every state s of the MDP $\\mathcal{M}$ , and every policy $\\pi$ of $\\mathcal{M}$ . ", "page_idx": 2}, {"type": "text", "text": "The optimal policy is the solution concept in single-objective RL. For any MDP with a finite state and action space, at least one optimal policy exists, which is also deterministic and stationary [7]. ", "page_idx": 2}, {"type": "text", "text": "2.2 Multi-objective reinforcement learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Multi-objective reinforcement learning (MORL) deals with environments in which an agent pursues multiple objectives simultaneously (for example, in a healthcare context, the health and the autonomy of a patient). Recall that in single-objective RL, the reward function represents the agent\u2019s objective. Thus, MORL considers environments with multiple reward functions, called Multi-Objective Markov Decision Processes [19, 10]. Formally: ", "page_idx": 2}, {"type": "text", "text": "Definition 3 (Multi-Objective MDP). An n-objective Markov Decision Process (MOMDP) is defined as a tuple $\\langle S,A,{\\vec{R}},T\\rangle$ where $\\boldsymbol{S}$ , $\\boldsymbol{\\mathcal{A}}$ and $T$ are the same as in an MDP, and ${\\vec{R}}:S\\times{\\mathcal{A}}\\times{\\mathcal{S}}\\to{\\mathbb{R}}^{n}$ is a vector of reward functions, providing a reward function $R_{i}$ for each objective $i\\in\\{1,\\ldots,n\\}$ . ", "page_idx": 2}, {"type": "text", "text": "Policies in a MOMDP are evaluated by means of a value function vector $\\vec{V}$ , defined as the vector of all value functions per objective $\\vec{V}(s)=(V_{1}(s),\\dotsc,V_{n}(s))$ . ", "page_idx": 2}, {"type": "text", "text": "If not all objectives can be fully fulfliled simultaneously, the agent needs to prioritise between them. To represent an agent\u2019s preferences with respect to multiple objectives, most approaches in MORL assume that the value function of each objective can be aggregated into a single function. That way, the agent\u2019s goal becomes to maximise this aggregated value function.This aggregation is performed by means of a utility function $u$ (also called a scalarisation function)[18, 22]. In MORL, a utility function $u$ is defined as a function mapping the domain of all value functions (a subset of the real coordinate space $\\mathbb{R}^{n}$ ) to the real space $\\mathbb{R}$ . With $u$ , the agent\u2019s goal can be expressed as learning a policy that maximises the function $(u\\circ\\vec{V})(s)=u(\\vec{V}(s))$ . Formally2: ", "page_idx": 2}, {"type": "text", "text": "Definition 4 (Utility function). Let $\\mathcal{M}$ be a MOMDP of n objectives. Any function $u:\\mathbb{R}^{n}\\to\\mathbb{R}$ is a utility function of $\\mathcal{M}$ . ", "page_idx": 2}, {"type": "text", "text": "The family of linear utility functions is especially notable. Any linear utility function $l$ returns a weighted sum of value functions $(l\\circ{\\vec{V}})(s)={\\vec{w}}\\cdot{\\vec{V}}(s)$ . For linear utility functions, the scalarised problem of maximising $l\\circ\\vec{V}$ can be solved with single-objective reinforcement learning algorithms3. ", "page_idx": 2}, {"type": "text", "text": "While in single-objective RL there is a clear definition of the solution concept (a deterministic and stationary optimal policy), there is no equivalent for MORL. Instead, the utility function is typically assumed to be unknown, and that we only have minor assumptions about it (e.g., that it is linear or that it is monotonically increasing) [19, 22, 10]. With that in mind, the solution concept in MORL is to learn a set of candidate policies $\\pi$ , with each of them optimising a possible utility function $u$ . The next Section explores typical solution concepts in MORL. ", "page_idx": 2}, {"type": "text", "text": "2.3 Solution concepts of MOMDPs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The solution concepts in MORL depend on how much is assumed about the utility function. If nothing is assumed, the goal is to learn the set of maximal policies for any utility function. It is important to remark what it means for a policy to be maximal for a utility function. By far, the majority of the MORL community follow the so-called state-independent (SI) criterion to define optimality [19, 18, 22, 10]. Given a MOMDP, this criterion considers that a policy $\\pi_{*}$ maximises a utility function if and only if it is maximal among the expectation of possible initial states (for some value function $\\vec{V}$ and the random variable of possible initial states $S_{0}$ ). We denote this expectation with $\\vec{V}_{S I}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\vec{V}_{S I}^{\\pi}\\doteq\\mathbb E[\\vec{V}^{\\pi}(S_{0})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Due to its simplicity, the state-independent (SI) criterion is widely used in MORL and RL in general. While it is generally innocuous in single-objective RL, it can generate contradictory policies in multi-objective RL, as we will show in Example 4 below. ", "page_idx": 3}, {"type": "text", "text": "Considering this state-independent criterion, all solution concepts for MOMDPs have been formalised exclusively for it. Thus, the state-of-the-art general solution, the undominated set, is defined as the set of policies that are maximal for at least one utility function. Formally [10]: ", "page_idx": 3}, {"type": "text", "text": "Definition 5 (Undominated set). Given a MOMDP $\\mathcal{M}$ , its undominated set $U(\\mathcal{M})$ is defined as the set of policies for which there exists a utility function u with a maximal scalarised value. ", "page_idx": 3}, {"type": "equation", "text": "$$\nU(\\mathcal{M})\\doteq\\{\\pi\\in\\Pi(\\mathcal{M})\\mid\\exists u:\\forall\\pi^{\\prime}\\in\\Pi(\\mathcal{M}),\\,u(\\vec{V}_{S I}^{\\pi})\\ge u(\\vec{V}_{S I}^{\\pi^{\\prime}})\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Pi(\\mathcal{M})$ is the set of all possible policies of an MOMDP $\\mathcal{M}$ . ", "page_idx": 3}, {"type": "text", "text": "We recall that the definition of undominated set makes no assumption on the structure of the utility function. If we constrain it to be a linear function, then the solution concept becomes the convex hull. The convex hull of a MOMDP contains all policies that are maximal for at least one linear utility function (again, according to the SI criterion). Formally [10]: ", "page_idx": 3}, {"type": "text", "text": "Definition 6 (Convex hull). Given an MOMDP $\\mathcal{M}$ , its convex hull $C H({\\mathcal{M}})$ is the subset of policies $\\pi_{*}$ that are optimal for some weight vector $\\vec{w}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nC H(\\mathcal{M})\\doteq\\{\\pi\\in\\Pi(\\mathcal{M})\\mid\\exists\\vec{w}\\in\\mathbb{R}^{n}:\\forall\\pi^{\\prime}\\in\\Pi(\\mathcal{M}),\\ \\vec{w}\\cdot\\vec{V}_{S I}^{\\pi}\\ge\\vec{w}\\cdot\\vec{V}_{S I}^{\\pi^{\\prime}}\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Pi(\\mathcal{M})$ is the set of policies of $\\mathcal{M}$ . ", "page_idx": 3}, {"type": "text", "text": "3 Utility optimal policies ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Recall that the MORL literature defines solution concepts following the state-independent criterion. However, for a proper analysis of utility functions in MORL, we require more precise definitions considering each and every state of a MOMDP. ", "page_idx": 3}, {"type": "text", "text": "Furthermore, recall that, in single-objective MDPs, value functions impose a partial order over policies of an MDP [27]. However, thanks to Banach\u2019s fixed point theorem, we know that a deterministic and stationary optimal policy always exists for any finite MDP (and, thus, for every state) [7]. These theoretical properties become much weaker in multi-objective MDPs. In particular, the Banach fixed point theorem does not generalise even for finite MOMDPs. Thus, we may have finite MOMDPs for which no optimal policy exists for any state. These \u201cmore precarious\" theoretical results motivate even more the need for studying the existence of optimal policies in a MOMDP at two different levels: one at a single-state level (i.e., considering a single state), and another one at the all-states level (considering all states). ", "page_idx": 3}, {"type": "text", "text": "We begin by defining utility optimality at the state level: a given policy $\\pi$ is optimal at state $s$ with respect to a given utility function $u$ if and only if it obtains more scalarised discounted returns than any other policy at $s$ . Formally: ", "page_idx": 3}, {"type": "text", "text": "Definition 7 (utility optimal policy at a state). Let $\\mathcal{M}$ be a MOMDP with state set $\\boldsymbol{S}$ . Let $\\Pi^{\\mathcal{M}}$ be the set of policies of $\\mathcal{M}$ . Let u be a utility function. Then, a policy $\\pi_{*}\\in\\Pi^{\\mathcal{M}}$ is optimal with respect to utility function $u$ at state $s\\in S$ if and only $i f$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(u\\circ\\vec{V}^{\\pi_{*}})(s)\\geq(u\\circ\\vec{V}^{\\pi})(s),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for every policy $\\pi\\in\\Pi^{\\mathcal{M}}$ . We say that $\\pi_{*}$ is $\\langle u,s\\rangle$ -optimal for short. ", "page_idx": 3}, {"type": "text", "text": "Example 1. Consider a MOMDP $\\mathcal{M}$ with two states: an initial state $s_{1}$ and a terminal state $s_{2}$ . An agent can perform two actions $(a_{1},\\ a_{2})$ in this environment, with rewards $\\vec{R}(s_{1},a_{1})\\,=\\,(1,0)$ , $\\vec{R}(s_{1},a_{2})=(0,1)$ respectively. Consider the utility function $u(x,y)=x\\!+\\!s i n(y)$ . The deterministic policy $\\pi(s_{1})\\,=\\,a_{1}$ that obtains vectorial value $\\vec{V}^{\\pi}(s_{1})\\,=\\,(1,0)$ is clearly $\\langle u,s_{1}\\rangle$ -optimal since $s i n(y)<y$ for any $y\\in[0,1]$ . ", "page_idx": 4}, {"type": "text", "text": "In the same vein as in single-objective RL, given a MOMDP, we define a policy as utility optimal at the all-states level (or simply utility optimal) as a utility optimal policy in every state in the MOMDP. Formally: ", "page_idx": 4}, {"type": "text", "text": "Definition 8 (utility optimal policy). Let M be a MOMDP with state set $\\boldsymbol{S}$ . Let $\\Pi^{\\mathcal{M}}$ be the set of policies of $\\mathcal{M}$ . Let u be a utility function. Then, a policy $\\pi_{*}\\in\\Pi^{\\mathcal{M}}$ is optimal with respect to utility function $u$ if and only if: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(u\\circ\\vec{V}^{\\pi_{*}})(s)\\geq(u\\circ\\vec{V}^{\\pi})(s),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for every policy $\\pi\\in\\Pi^{\\mathcal{M}}$ , and every state $s\\in S$ . We say that $\\pi_{*}$ is $u$ -optimal for short. ", "page_idx": 4}, {"type": "text", "text": "Example 2. In the MOMDP in Example $^{\\,I}$ , policy $\\pi(s_{1})=a_{1}$ is $u$ -optimal since there are only two states, and the second one is terminal. ", "page_idx": 4}, {"type": "text", "text": "We know that a (deterministic) $u$ -optimal policy always exists for any linear utility function, as shown in Section 2.2. However, this is not always the case for arbitrary utility functions. The following three examples illustrate conditions that are not enough to guarantee the existence of neither deterministic nor stochastic utility optimal policies in finite MOMDPs. These conditions are: ", "page_idx": 4}, {"type": "text", "text": "1. That the utility function is monotonically increasing (a family of utility functions specially studied in MORL [18, 10]. Example 3 illustrates how this condition is not enough to guarantee a deterministic $u$ -optimal policy.   \n2. That the utility function is strictly monotonically increasing. Example 5 shows how this condition is not enough to guarantee a stochastic $\\langle u,s\\rangle$ -optimal policy for any given state $s$ .   \n3. That the utility function is both strictly monotonically increasing and continuously differentiable. Example 4 shows how even assuming both conditions there are utility functions without stochastic $u$ -optimal policies. ", "page_idx": 4}, {"type": "text", "text": "In the following Example 3 we consider the Chebyshev function (also called Tchebycheff, a wellknown utility function in MORL [17, 19, 18, 10]. The Chebyshev function returns more scalar value the nearest a given input value $x$ is to a reference value $\\vec{r}$ . Moreover, the Chebyshev function is also monotonically increasing [18]. ", "page_idx": 4}, {"type": "text", "text": "Example 3. Let $\\epsilon>0$ be a small real number, $\\vec{r}\\in\\mathbb{R}^{n}$ a reference value, and $\\vec{w}\\,\\in\\,\\mathbb{R}^{n}$ a weight vector such that each $w_{i}\\geq0$ . The Chebyshev function $\\psi_{\\vec{r},\\epsilon,w}:\\mathbb{R}^{n}\\to\\mathbb{R}$ is defined as $I I7J$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\psi_{\\vec{r},\\epsilon,\\vec{w}}(x)\\doteq-(\\operatorname*{max}_{i}w_{i}\\cdot|r_{i}-x_{i}|+\\epsilon\\cdot\\sum_{i}w_{i}\\cdot|r_{i}-x_{i}|).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Let $\\mathcal{M}$ be a 2-objective deterministic MDP with three states $s_{1},\\ s_{2}$ , and $s_{3}$ such that $s_{3}$ is the terminal state. Regarding actions, there is one possible action in $s_{1}$ , which has associated rewards $\\vec{R}(s_{1},a_{1})=(1,0)$ . Action $a_{1}$ transitions the state to $s_{2}$ . Then, in state $s_{2}$ , there are two possible actions with associated rewards $\\vec{R}(s_{2},a_{2})\\,=\\,(2,20)$ and $\\vec{R}(s_{2},a_{3})\\:=\\:(3,1)$ . All actions in $s_{2}$ transition to terminal state $s_{3}$ . ", "page_idx": 4}, {"type": "text", "text": "This environment has two possible deterministic policies. The first policy is $\\pi_{1}(s_{1})=a_{1},\\pi_{1}(s_{2})=a_{2}$ . This policy obtains values $\\vec{V}^{\\pi_{1}}(s_{1})=(3,20),\\vec{V}^{\\pi_{1}}(s_{2})=(2,20)$ . The second policy is $\\pi_{2}(s_{1})=$ $a_{1}$ $_{1},\\pi_{2}(s_{2})\\,=\\,a_{3}$ . This policy obtains values $\\vec{V}^{\\pi_{2}}(s_{1})\\,=\\,(4,1),\\vec{V}^{\\pi_{2}}(s_{2})\\,=\\,(3,1)$ . We select as reference point $\\vec{r}=(3.5,20)$ , associated weights $\\vec{w}=(1,1/19)$ , and $\\epsilon=0$ . For this configuration we have: for policy $\\pi_{1}$ , $\\psi(\\vec{V}^{\\pi_{1}}(s_{1}))=-0.5$ , $\\psi(\\vec{V}^{\\pi_{1}}(s_{2}))=-1.5.$ . For policy $\\pi_{2}$ , $\\psi(\\vec{V}^{\\pi_{2}}(s_{1}))=$ $\\psi(\\vec{V}^{\\pi_{2}}(s_{2}))=-1$ . Clearly, $\\pi_{1}$ is the only deterministic $\\langle\\psi,s_{1}\\rangle$ -optimal policy, while $\\pi_{2}$ is the only deterministic $\\langle\\psi,s_{2}\\rangle$ -optimal policy. Thus, no deterministic $\\psi$ -optimal policy exists. ", "page_idx": 4}, {"type": "text", "text": "Example 3 showed that deterministic $u$ -optimal policies do not necessarily exist in finite MOMDPs, a fact that was already known in the MORL literature [18]. But we can go one step further: the next Example 4 shows that being finite is not enough for an MOMDP to guarantee the existence of stochastic $u$ -optimal policies. Moreover, the utility function from Example 4 is both strictly monotonically increasing and continuously differentiable: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Example 4. Consider the utility function $\\begin{array}{r}{u(x,y)=\\sqrt{x^{2}+1}+\\frac{y}{20},}\\end{array}$ , which is strictly monotonically increasing for any $(x,y)\\in\\mathbb{R}^{+}\\times\\mathbb{R},$ , and continuously differentiable in $\\mathbb{R}^{2}$ . Let $\\mathcal{M}$ be the same 2-objective deterministic MDP from Example 3. ", "page_idx": 5}, {"type": "text", "text": "This environment has the same two deterministic policies from Example 3. The first policy is $\\pi_{1}(s_{1})=a_{1},\\pi_{1}(s_{2})=a_{2}$ , which obtains scalarised values $u(\\vec{V}^{\\pi_{1}}(s_{1}))\\stackrel{\\leftarrow}{\\approx}4.16,u(\\vec{V}^{\\pi_{1}}(s_{2}))\\approx3.24$ . The second policy is $\\pi_{2}(s_{1})\\,=\\,a_{1},\\pi_{2}(s_{2})\\,=\\,a_{3}$ , which obtains scalarised values $u(\\vec{V}^{\\pi_{2}}(s_{1}))\\approx$ 4.17, $u(\\vec{V}^{\\pi_{2}}(s_{2}))\\approx3.21$ . ", "page_idx": 5}, {"type": "text", "text": "It is easy to check that $\\pi_{1}$ is the absolute $\\langle u,s_{2}\\rangle$ -optimal policy, while $\\pi_{2}$ is the absolute $\\langle u,s_{1}\\rangle$ - optimal policy. We leave the details at Appendix A.1. Thus, no stochastic $u$ -optimal policy exists. ", "page_idx": 5}, {"type": "text", "text": "Our third example is a finite MOMDP and a strictly monotonically increasing utility function $u$ for which no stochastic $\\langle u,s\\rangle$ -optimal policy exists for any state $s$ . ", "page_idx": 5}, {"type": "text", "text": "Example 5. Consider the utility function u such that if $x\\,=\\,y_{\\mathrm{~\\,~}}$ , then $u(x,x)=0,$ , and otherwise $\\begin{array}{r}{u(x,y)=\\frac{1}{|x-y|}}\\end{array}$ . Let $\\mathcal{M}$ be the 2-objective deterministic MDP from Example $^{\\,l}$ with two states $s_{1}$ and $s_{2}$ such that $s_{1}$ is the initial state and $s_{2}$ is the terminal state. There are two possible actions in $s_{1}$ with associated rewards $\\vec{R}(s_{1},a_{1})=(1,0)$ and $\\vec{R}(s_{1},a_{2})=(0,1)$ . ", "page_idx": 5}, {"type": "text", "text": "Every policy of $\\mathcal{M}$ will be of the form $\\pi(s_{1},a_{1})=p$ and $\\pi(s_{1},a_{2})=1-p$ for some $p\\in[0,1]$ . The vectorial value of such policy at state $s_{1}$ will be $\\vec{V}^{\\pi}(s_{1})=(p,1-p)$ . Notice how every possible value belongs to the Pareto Front of $\\mathcal{M}$ . Thus, any utility function is strictly monotonically increasing in $\\mathcal{M}$ , including the one defined in this example. ", "page_idx": 5}, {"type": "text", "text": "In particular, for any policy, its scalarised value will be $\\begin{array}{r}{u(1,1-p)=\\frac{1}{|2p-1|}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "If for any policy $\\pi$ we have $\\pi(s_{1},a_{1})\\,=\\,p\\,<\\,{\\textstyle{\\frac{1}{2}}}$ , then the policy $\\pi^{\\prime}$ such that $\\pi(s_{1},a_{1})\\,=\\,p+\\epsilon,$ , with $\\epsilon>0$ small enough so that $\\begin{array}{r}{p+\\epsilon\\,<\\,\\frac{1}{2}}\\end{array}$ obtains more scalarised value than $\\pi$ . Similarly, if $\\pi(s_{1},a_{1})=p>\\textstyle{\\frac{1}{2}}$ , we can find an alternative policy $\\pi^{\\prime}$ such that $\\pi(s_{1},a_{1})=p-\\epsilon$ that obtains more scalarised value than $\\pi$ . Thus, no $\\langle u,s_{1}\\rangle$ -optimal policy exists in this MOMDP. ", "page_idx": 5}, {"type": "text", "text": "The result of Example 5 is specially significant because most MORL literature (with its stateindependent criterion that only considers initial states $S_{0}$ ), focuses on computing $\\langle u,S_{0}\\rangle$ -optimal policies on strictly monotonically increasing utility functions [19, 10]. As we have shown in Example 5, such optimal policies are not guaranteed to exist. ", "page_idx": 5}, {"type": "text", "text": "Therefore, the logical next question after these three examples is to ask for which families of utility functions there exists at least one global utility optimal policy or at least one utility optimal policy for every state. In particular, we focus on stationary policies, like in single-objective RL. Formally: ", "page_idx": 5}, {"type": "text", "text": "Problem 1. For which families of utility functions is guaranteed that a stationary $\\langle u,s\\rangle$ -optimal policy will exist for every state s of every possible finite MOMDP? ", "page_idx": 5}, {"type": "text", "text": "Problem 2. For which families of utility functions is guaranteed that a stationary u-optimal policy will exist for every possible finite MOMDP? ", "page_idx": 5}, {"type": "text", "text": "Next, Sections 3.1 and 3.2 focus on providing sufficient conditions to guarantee the existence of utility optimal policies in a state and utility optimal policies in general, respectively. ", "page_idx": 5}, {"type": "text", "text": "3.1 Utility optimal policy at a state existence ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "This Section introduces a family of utility functions that solve Problem 1. In particular, we offer a sufficient condition to guarantee the existence of a stationary $\\langle u,s\\rangle$ -optimal policy for every state $s$ of a finite MOMDP. This sufficient condition is that the utility function is continuous. Formally: ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Let M be a finite MOMDP. Let u be a continuous utility function for all value functions of all policies $\\Pi(\\mathcal{M})$ of $\\mathcal{M}$ . Then, for every state s of $\\mathcal{M}$ , at least one stationary $\\langle u,s\\rangle$ -optimal policy exists. ", "page_idx": 5}, {"type": "text", "text": "Proof 1. See Appendix A.3. ", "page_idx": 5}, {"type": "text", "text": "Continuous utility functions are one the most extensively studied and applied family of functions due to their well-behaved properties (e.g., existence of absolute maximum and minimum). Nevertheless, recall that Theorem 1 only provides sufficient conditions, and thus there might exist $\\langle u,s\\rangle$ -optimal policies for discontinuous utility functions. We offer such an example in the proof of Theorem 3. ", "page_idx": 6}, {"type": "text", "text": "3.2 Utility optimal policy existence ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Demanding that the same policy is $u$ -optimal for some utility function $u$ for every state of the MOMDP is a much harder problem than demanding it for a given state. Thus, in this case, it is not enough that the utility function is continuously differentiable (i.e., continuous and all partial derivatives also continuous), and it is not enough that the utility function is also strictly monotonically increasing (as seen in Example 4). ", "page_idx": 6}, {"type": "text", "text": "It is already known that, for linear utility functions, we can obtain a $u$ -optimal policy. So, the question is if we can find at least another family of utility functions for which a $u$ -optimal policy exists. Theorem 2 presents such a family: utility functions that result from composing an affine function together with a strictly monotonically increasing function. Formally: ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Let $\\mathcal{M}$ be a finite multi-objective MDP M. Let u be a utility function decomposable as $u(x)=h(g(x))$ , with $g(x):\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ being an affine function, and $h(x):\\mathbb{R}\\to\\mathbb{R}$ being a strictly monotonically increasing function for all value functions of all policies $\\Pi(\\mathcal{M})$ of $\\mathcal{M}$ . At least one deterministic and stationary $u$ -optimal policy exists. ", "page_idx": 6}, {"type": "text", "text": "Proof 2. See Appendix A.4. ", "page_idx": 6}, {"type": "text", "text": "Notice that, in particular, Theorem 2 also covers linear utility functions. Linear utility functions are one of the most widely applied families of utility functions in MORL [18, 10]. To finish this Section, we show an example of a function composed by an affine and a strictly monotonically increasing function (that hence satisfies Theorem 2) that produces a non-linear (and non-affine) utility function for which a $u$ -optimal policy exists. ", "page_idx": 6}, {"type": "text", "text": "Example 6. Consider any 2-objective MDP $\\mathcal{M}$ where all rewards are positive (i.e., $\\vec{R}(s,a)\\in$ $\\mathbb{R}^{+}\\times\\bar{\\mathbb{R}}^{+}$ for all $s,a)$ , and a utility function $u$ defined as ", "page_idx": 6}, {"type": "equation", "text": "$$\nu(x,y)=-\\frac{1}{x+y+3+s i n(x+y+3)}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "obseie $u(x,y)$ lay s $u(x,y)=h(g(x,y))$ inwgit. h B $g(x,y)=x+y+3$ tibmeianl gp aoflifcinye ,e xainstds $h(x)=$ $-{\\frac{1}{x+s i n(x)}}$ ", "page_idx": 6}, {"type": "text", "text": "Notice that Theorem 2 only provides sufficient conditions of utility functions $u$ for guaranteeing the existence of $u$ -optimal policies. In fact, Example 2 shows a non-affine utility function for which an $u$ -optimal policy exists in a particular MOMDP. ", "page_idx": 6}, {"type": "text", "text": "4 Preference relations in MORL ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the previous section, we characterised the utility functions for which we can compute a utility optimal policy. However, as mentioned in the Introduction, a more fundamental question remains unanswered: Which user\u2019s preferences can be expressed as utility functions in a given MOMDP? ", "page_idx": 6}, {"type": "text", "text": "We require formalising preference relations and their maximal elements in MOMDPs to answer this last question. Preference relations, also known as binary relations, allow us to express, among two elements of a set, which one we prefer [25, 14]. While the state of the art in MORL makes no distinction between preference relations and utility functions [10], it is important to maintain them as two separate concepts. First of all, let us provide a formal definition of preference relations in MORL, inspired by [25]: ", "page_idx": 6}, {"type": "text", "text": "Definition 9 (Preference relation in a MOMDP). Let M be a MOMDP of n objectives. We define a preference relation in M as any binary relation $\\succeq$ over at least one pair of value vectors of $\\mathbb{R}^{n}$ . $I n$ particular, we say that: ", "page_idx": 6}, {"type": "text", "text": "\u2022 a value function $\\vec{V}_{1}\\in\\mathcal{V}$ is weakly preferred to another value function $\\vec{V}_{2}\\in\\mathcal{V}$ if and only if ${\\vec{V}}_{1}(s)\\succeq{\\vec{V}}_{2}(s)$ for every state s of $\\mathcal{M}$ . In short, we denote $\\vec{V_{1}}\\succeq\\!\\!_{\\mathscr{M}}\\vec{V_{2}}$ . \u2022 a value function $\\vec{V}_{1}\\in\\mathcal{V}$ is strictly preferred to another value function $\\vec{V}_{2}\\in\\mathcal{V}$ if and only if ${\\vec{V}}_{1}(s)\\succeq{\\vec{V}}_{2}(s)$ for every state s of $\\mathcal{M}$ and not $\\vec{V}_{2}(s^{\\prime})\\succeq\\vec{V}_{1}(s^{\\prime})$ for at least one state $s^{\\prime}$ of $\\mathcal{M}$ . In short, we denote $\\vec{V}_{1}\\succ_{\\mathcal{M}}\\vec{V}_{2}$ . ", "page_idx": 7}, {"type": "text", "text": "If for two value vectors we have that ${\\vec{V}}_{1}(s)\\succeq{\\vec{V}}_{2}(s)$ and $\\vec{V}_{2}\\succeq\\vec{V}_{1}(s)$ , we say that they are indifferent, and we denote it with the $\\approx$ symbol. Notice that this definition makes no assumption over the preference relation. We do not impose that this preference relation is a pre-order, a partial order, or a total order [9]. Considering that the MORL literature applies utility functions of all kinds, we did not want to restrict our definition. ", "page_idx": 7}, {"type": "text", "text": "Following the game theory literature, humans have preferences, and we (sometimes) can represent them as utility functions, but not the other way around [14]. In fact, sometimes, a utility function that fully represents our preferences may not exist. If such a utility function exists, we call it the representative utility function of preference relation $\\succeq$ . Formally: ", "page_idx": 7}, {"type": "text", "text": "Definition 10 (Representative utility function). Let $\\mathcal{M}$ be a MOMDP, and l $e t\\succeq b e$ a preference relation in $\\mathcal{M}$ . Then, we define a utility function u as representative of the preference relation $\\succeq i f$ and only $i f,$ for every pair of possible value functions $\\vec{V}_{1},\\vec{V}_{2}$ , and every state s of $\\mathcal{M}$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\vec{V}_{1}(s)\\succeq\\vec{V}_{2}(s)\\iff(u\\circ\\vec{V}_{1})(s)\\geq(u\\circ\\vec{V}_{2})(s).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Some (but not all) preference relations have representative utility functions. However, any utility function $u$ , is representative of some preference relation $\\succeq_{u}$ defined as exactly fulfliling Equation 10. ", "page_idx": 7}, {"type": "text", "text": "In order theory, for any quasi-order (i.e., a preference relation that is at least reflexive and transitive), we can define the concept of maximal elements [9]. In our case, given a preference relation $\\succeq$ between the value functions of a MOMDP, its maximal elements would be the value functions associated with the policy that we expect the agent to learn. Formally: ", "page_idx": 7}, {"type": "text", "text": "Definition 11 (Maximal element). Let M be a MOMDP. $L e t\\succeq b e$ a preference relation in $\\mathcal{M}$ that is at least reflexive and transitive (a quasi-order). Then, the value vector $\\vec{V}_{*}(s)$ of value function $\\vec{V}_{*}$ is $a$ maximal element in state $s$ if and only if for every other possible value function $\\vec{V}$ of $\\mathcal{M}$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\vec{V}(s)\\succeq\\vec{V}_{*}(s)\\implies\\vec{V}_{*}(s)\\succeq\\vec{V}(s).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Example 7. In finite single-objective MDPs, the optimal value $V_{*}(s)$ is a maximal element for every state s, for the preference relation $\\succeq$ defined as $V(s)\\succeq V^{\\prime}(s)\\iff V(s)\\geq V^{\\prime}(s)$ . ", "page_idx": 7}, {"type": "text", "text": "As mentioned in the introduction above, not all preference orders in MORL can be represented as a utility function. One of the most well-known cases is the lexicographic order [5, 2]. Although Lexicographic MORL has been studied in detail [8, 29, 12, 30, 24], almost no work in MORL (with the exception of [23]) has noticed that an associated utility function does not exist in general. Let us see through an example why the lexicographic order cannot be represented as a linear utility function. For utility functions in general, we refer to Corollary 2 of [23]. ", "page_idx": 7}, {"type": "text", "text": "Example 8. Consider a MOMDP $\\mathcal{M}$ with two states: an initial state $s_{1}$ and a terminal state $s_{2}$ . An agent can perform three actions in this environment $(a_{1},a_{2},a_{3})$ , with rewards $\\vec{R}(s_{1},a_{1})=(1,0)$ , $\\vec{R}(s_{1},a_{2})=(0,1)$ , and $\\vec{R}(s_{1},a_{3})=(1,1)$ , respectively. Consider now the lexicographic order $\\succeq$ such that objective $^{\\,I}$ is always preferred to objective 2. Hence, $\\vec{R}(s_{1},a_{3})\\succ\\vec{R}(s_{1},\\bar{a}_{1})\\succ\\vec{R}(s_{1},a_{2})$ . Any linear utility function here will be of the form $u_{\\alpha,\\beta}(x,y)=\\alpha\\cdot x+\\beta\\cdot y$ . For $u_{w}$ to represent $\\succ$ , it must satisfy $u_{\\alpha,\\beta}(1,1)>u_{\\alpha,\\beta}(1,0)$ and $u_{\\alpha,\\beta}(1,0)>u_{\\alpha,\\beta}(0,1),$ , for example, $u_{10,1}(x,y)=$ 10x + y. ", "page_idx": 7}, {"type": "text", "text": "However, policies can be stochastic, and thus, we can have for instance any policy $\\pi$ such that $\\pi(s_{1},a_{1})=p,\\pi(s_{1},a_{2})=1-p$ , with $1\\geq p\\geq0,$ , which has associated value $\\vec{V}^{\\pi}(s_{1})=(p,1-p)$ . Hence, the utility function must also satisfy $u(1,0)>u(0.9,0.1)>\\cdots>u(0.1,0.9)>u(0,1)$ . And it needs to be absolutely precise: $u(p+\\epsilon,1-p-\\epsilon)>u(p,1-p)$ for every $\\epsilon>0$ arbitrarily small. Thus, it is impossible to represent the lexicographic order as a linear utility function. ", "page_idx": 7}, {"type": "text", "text": "While lexicographic orders cannot be represented with utility functions in MOMDPs, they do have maximal elements among finite MOMDPs. A utility function that shares the exact same maximal elements as a lexicographic order would be very helpful. With such a utility function, we could still find the policies that maximise a lexicographic order with state-of-the-art MORL algorithms. Having formalised maximal elements in MOMDPs for any quasi-order, we can introduce utility functions that at least preserve maximal elements. We call this kind of utility function quasi-representative. Formally: ", "page_idx": 8}, {"type": "text", "text": "Definition 12. Let M be an MOMDP. $L e t\\succeq b e$ a preference relation $\\succeq i n\\:\\mathcal{M}$ that is at least reflexive and transitive (a quasi-order). Let u be a utility function such that for every state s of $\\mathcal{M}$ : ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\vec{V}_{*}(s)\\;i s\\;a\\;m a x i m a l\\;e l e m e n t\\;o f\\succeq a t\\;s t a t e\\;s\\;\\iff\\;\\vec{V}_{*}(s)\\in\\arg\\operatorname*{max}_{\\vec{V}}[(u\\circ\\vec{V})(s)].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Then, we say that u is quasi-representative $o f\\succeq i n\\:\\mathcal{M}$ . ", "page_idx": 8}, {"type": "text", "text": "Example 9. Consider the MOMDP $\\mathcal{M}$ in Example 8. The utility function $u(x,y)=10x+y$ is quasi-representative of the lexicographic order because $u(1,1)>u(1,0)$ and $u(1,1)>u(0,1)$ . ", "page_idx": 8}, {"type": "text", "text": "In fact, quasi-representative utility functions allow us to define an equivalence relation between utility functions. Hence, by abuse of notation, we will also say that two utility functions are quasirepresentative for a given MOMDP if and only if they share the same maximum elements for every state $s$ of this MOMDP. ", "page_idx": 8}, {"type": "text", "text": "Example 10. Consider the same MOMDP $\\mathcal{M}$ and the lexicographic order $\\succeq$ from Example 8. For example, utility functions $u(x,y)=10x+y$ and $u^{\\prime}(x,y)=15x+y+30$ share the same utility optimal policy (which is $\\pi(s_{1})=a_{3},$ ), and hence are quasi-representative for ${\\mathcal{M}}\\,a n d\\succeq$ . ", "page_idx": 8}, {"type": "text", "text": "Notice that if a utility function $u$ is representative of some preference relation $\\succeq$ , it is also quasirepresentative of $\\succeq$ . Notice also that a utility function may be representative or quasi-representative of a given preference order for some MOMDP but not for other MOMDPs. ", "page_idx": 8}, {"type": "text", "text": "Now, given a MOMDP, what conditions must a preference order meet to be represented by a quasirepresentative utility function? Essentially, it is sufficient to have a maximal element for every state of the MOMDP. We present now a family of preference orders for which a quasi-representative utility function always exists for every finite MOMDP: ", "page_idx": 8}, {"type": "text", "text": "Theorem 3. $L e t\\succeq b e$ a preference relation and $\\mathcal{M}$ any finite MOMDP. Assume that $\\succeq i s$ : $(I)$ complete (either $a\\succeq b$ or $b\\succeq a$ or $a\\approx b$ for every two possible value vectors $a,b$ of $\\mathcal{M}$ ); (2) transitive (if $a\\succeq b,$ , and $b\\succeq c,$ then $a\\succeq c,$ ); and (3) at least one maximal element $\\vec{V}(s)$ exists for every state s of $\\mathcal{M}$ . Then, a quasi-representative utility function exists fo $\\cdot r\\succeq i n\\:\\mathcal{M}$ . ", "page_idx": 8}, {"type": "text", "text": "Proof 3. We offer a constructive proof. For every state $s$ , consider its set of maximal elements $\\vec{\\nu}_{*}(s)$ according to $\\succeq$ , which is non-empty for every state due to Condition (3). Since the preference relation is complete, for every state $s$ all its maximal elements will share the same value (i.e., $\\vec{V}_{1}(s)=\\vec{V}_{2}(s)$ for every two $\\vec{V}_{1},\\vec{V}_{2}\\in\\vec{\\mathcal{V}}_{*}(s))$ . Hence, without loss of generality, we consider that there is a single maximal element per state. Then, the number of maximal elements is at most $|S|$ , and we can order them according to $\\succeq$ (we can order them because $\\succeq$ is total and transitive). Then, set a number between 1 and $|S|$ for each of these elements, ordered by $\\succeq$ . For every other vector $x\\,\\in\\mathbb{R}^{n}$ , set $u(x)=0$ . Now, by construction, for every state $s$ we have that $\\mathrm{max}_{\\vec{V}}(u\\circ\\vec{V})(s)\\in\\vec{\\mathcal{V}}_{*}(s)$ . In other words, $u$ is a quasi-representative utility function, such that it returns the most preferred value vector for each state $s$ according to $\\succeq$ . \u53e3 ", "page_idx": 8}, {"type": "text", "text": "Completeness and transitivity are very common conditions for preference relations in game theory [14]. The third condition is required for MOMDPs since we are dealing with an infinite amount of policies. For instance, Example 5 showed a finite MOMDP for which there is no maximum element for any environment state. ", "page_idx": 8}, {"type": "text", "text": "The main takeaway from Theorem 3 is that MORL algorithms should focus on the family of preference relations that fulfils all its conditions. Such conditions are sufficient to guarantee the existence of a quasi-representative utility function, as we just proved. ", "page_idx": 8}, {"type": "text", "text": "The family of preference relations satisfying the conditions of Theorem 3 has examples aplenty, such as the previously mentioned family of lexicographic orders. Moreover, any continuous utility function is representative of a total order that satisfies all conditions of Theorem 3. ", "page_idx": 8}, {"type": "text", "text": "5 Related work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Most of the literature in MORL focuses on creating novel solution concepts in MORL and algorithmic methods to solve them (e.g., [28, 33, 20, 24, 21]). Instead, we focus on characterising for which families of utility functions these solutions exist, a largely overlooked theoretical problem despite its relevant implications. Take for instance the work in [33], where Van Moffaert et al. present a method for computing the Pareto front of a given MOMDP. They implicitly assume that this Pareto front will always include the solution policy (a $u\\rangle$ -optimal policy in our terms), but as we have proven in Example 4, this is not always the case. ", "page_idx": 9}, {"type": "text", "text": "Then, regarding the study of preference relations in MORL, to the best of our knowledge the only other works in the literature apart from ours are [23, 26]. Skalse et al.\u2019s theoretical results in [23] complement ours by stating that, for every so-called objective (a preorder between policies), a $u$ - optimal policy exists if and only if this objective can be represented with a linear utility function. This aligns with our results in Theorem 2. However, they do not establish whether there may be more families of utility functions for which a $u$ -optimal policy exists, as we do with Theorem 2. Moreover, our preference definition allows for ordering policies in each state of the environment, providing more granularity than their objective definition. This difference is also significant, because it allows us to identify issues in the solution concepts of MORL as we have tried to illustrate with Examples 5 and 4. ", "page_idx": 9}, {"type": "text", "text": "Next, Subramani et al. in [26] follow on the work in [23], but they tackle a different problem than us. Their focus is on compare the expressivity of the MORL framework with other frameworks. They aim to know which objectives (defined identically to [23]) can be represented on each framework. However, like [23], their objective definition does not allow them to order policies differently per state, unlike our preference definition. ", "page_idx": 9}, {"type": "text", "text": "To finish, closely related to our work, Miura in [15] tackles the problem of characterising preferences and their properties in constrained MDPs [1]. They define preferences as sets of acceptable policies and aim to find for which environments they can set the constraints and reward functions of a constrained MDP (CMDP) for which the acceptable policies are optimal. While CMDPs and MOMDPs share many similarities, they belong to separate research area. The major difference between them is that a CMDP has constraints, while an MOMDP has a utility function. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Multi-objective reinforcement learning (MORL) is the most promising framework for dealing with sequential decision-making problems with multiple objectives. In MORL, the learning agent ponders between the multiple objectives by means of a utility function aligned with the user\u2019s preferences. However, the state of the art in MORL has disregarded two fundamental theoretical problems related to utility functions: (1) for which utility functions an associated optimal policy is guaranteed to exist? and (2) which preference relations can be expressed as a utility function? ", "page_idx": 9}, {"type": "text", "text": "In this paper, we contributed to the state of the art in MORL by formalising both problems for the first time and by analysing each one. For utility functions, we first formalised the concept of utility optimality in MORL. Then, we provided sufficient and insufficient conditions for such a policy to exist for any finite MOMDP. For preference relations, we first formalise them for MOMDPs, and we also provide the minimal conditions to guarantee that they can be expressed as a particular type of utility function, the so-called quasi-representative utility functions. We expect our theoretical contributions to spark interest in both theoretical and practical MORL research. In fact, our results have direct practical consequences: to avoid contradictory policies, the MORL community needs to design algorithms that check that their learned policies are utility optimal. ", "page_idx": 9}, {"type": "text", "text": "We envision many directions for future research. On the theoretical side, a generalisation of the presented theoretical results to multi-agent multi-objective environments would be of great interest in the MORL literature [22]. On the algorithmic side, we expect to see the development of algorithms that exploit our Theorems to compute utility optimal policies. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The research presented in this paper was supported by the EU-funded VALAWAI (# 101070930) project, the Spanish-funded VAE (# TED2021-131295B-C31) and Rhymas (# PID2020- 113594RB-100) projects. This work was supported by grant PID2022-136787NB-I00 funded by MCIN/AEI/10.13039/501100011033. It was also funded by GUARDEN (101060693), Fairtrans (PID2021-124361OB-C33), AUTODEMO (SR21-00329), TAILOR (H2020-952215), PrepParticip2.0 ( 24S03545-001), grants 2021 SGR 00313 and 2021 SGR 00754. Maite Lopez-Sanchez belongs to the WAI research group (University of Barcelona) associated unit to CSIC by IIIA. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] E. Altman. Constrained markov decision processes, 1999.   \n[2] S. Barbera, P. J. Hammond, and C. Seidl, editors. Handbook of Utility Theory: Volume 1: Principles. Kluwer Academic Publishers, 1998.   \n[3] A. Charpentier, R. \u00c9lie, and C. Remlinger. Reinforcement learning in economics and finance. Comput. Econ., 62(1):425\u2013462, apr 2021. [4] A. Coronato, M. Naeem, G. De Pietro, and G. Paragliola. Reinforcement learning for intelligent healthcare applications: A survey. Artificial Intelligence in Medicine, 109:101964, 2020.   \n[5] G. Debreu. On the preferences characterization of additively separable utility. In A. Tangian and J. Gruber, editors, Constructing Scalar-Valued Objective Functions, pages 25\u201338, Berlin, Heidelberg, 1997. Springer Berlin Heidelberg.   \n[6] B. B. Elallid, N. Benamar, A. S. Hafid, T. Rachidi, and N. Mrani. A comprehensive survey on the application of deep and reinforcement learning approaches in autonomous driving. Journal of King Saud University - Computer and Information Sciences, 34(9):7366\u20137390, 2022.   \n[7] E. A. Feinberg. Total Expected Discounted Reward MDPS: Existence of Optimal Policies. John Wiley and Sons, Ltd, 2011.   \n[8] Z. G\u00e1bor, Z. Kalm\u00e1r, and C. Szepesv\u00e1ri. Multi-criteria reinforcement learning. pages 197\u2013205, 01 1998.   \n[9] E. Harzheim. Ordered sets, volume 7. Springer Science & Business Media, 2005.   \n[10] C. F. Hayes, R. R\u02d8adulescu, E. Bargiacchi, J. K\u00e4llstr\u00f6m, M. Macfarlane, M. Reymond, T. Verstraeten, L. M. Zintgraf, R. Dazeley, F. Heintz, E. Howley, A. A. Irissappane, P. Mannion, A. Now\u00e9, G. Ramos, M. Restelli, P. Vamplew, and D. M. Roijers. A practical guide to multiobjective reinforcement learning and planning. Autonomous Agents and Multi-Agent Systems, 36, 2022.   \n[11] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: A survey. J. Artif. Int. Res., 4(1):237\u2013285, May 1996.   \n[12] C. Li and K. Czarnecki. Urban driving with multi-objective deep reinforcement learning. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS \u201919, page 359\u2013367, Richland, SC, 2019. International Foundation for Autonomous Agents and Multiagent Systems.   \n[13] Y. Li. Deep reinforcement learning: An overview, 2017. cite arxiv:1701.07274.   \n[14] M. Maschler, E. Solan, and S. Zamir. Game Theory, 2nd Edition. Cambridge University Press, 2013.   \n[15] S. Miura. On the expressivity of multidimensional markov reward. In Proceedings of the 2022 Conference on Reinforcement Learning and Decision Making., 07 2023.   \n[16] E. L. Pennec. Reinforcement Learning Book of Proofs. 2023.   \n[17] P. Perny and P. Weng. On finding compromise solutions in multiobjective markov decision processes. volume 215, pages 969\u2013970, 01 2010.   \n[18] D. Roijers and S. Whiteson. Multi-Objective Decision Making. Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan and Claypool, California, USA, 2017. doi:10.2200/S00765ED1V01Y201704AIM034.   \n[19] D. M. Roijers, P. Vamplew, S. Whiteson, and R. Dazeley. A survey of multi-objective sequential decision-making. J. Artif. Int. Res., 48(1):67\u2013113, Oct. 2013.   \n[20] D. M. Roijers, S. Whiteson, and F. A. Oliehoek. Computing convex coverage sets for faster multi-objective coordination. J. Artif. Intell. Res., 52:399\u2013443, 2015.   \n[21] W. R\u00f6pke, C. Hayes, P. Mannion, E. Howley, A. Nowe, and D. Roijers. Distributional multiobjective decision making, 05 2023.   \n[22] R. Ra\u02d8dulescu, P. Mannion, D. M. Roijers, and A. Now\u00e9. Multi-objective multi-agent decision making: a utility-based analysis and survey. Autonomous Agents and Multi-Agent Systems, 34:1\u201352, 2019.   \n[23] J. Skalse and A. Abate. On the limitations of markovian rewards to express multi-objective, risk-sensitive, and modal tasks. In Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence, UAI \u201923. JMLR.org, 2023.   \n[24] J. Skalse, L. Hammond, C. Griffin, and A. Abate. Lexicographic multi-objective reinforcement learning. pages 3405\u20133411, 07 2022.   \n[25] B. L. Slantchev. Game theory $:$ Preferences and expected utility. 20012.   \n[26] R. Subramani, M. Williams, M. Heitmann, H. Holm, C. Griffin, and J. Skalse. On the expressivity of objective-specification formalisms in reinforcement learning. In Eleventh International Conference on Learning Representation., 2023.   \n[27] R. S. Sutton and A. G. Barto. Reinforcement learning - an introduction. Adaptive computation and machine learning. MIT Press, 1998.   \n[28] P. Vamplew, R. Dazeley, E. Barker, and A. Kelarev. Constructing stochastic mixture policies for episodic multiobjective reinforcement learning tasks. In A. Nicholson and X. Li, editors, AI 2009: Advances in Artificial Intelligence, pages 340\u2013349, Berlin, Heidelberg, 2009. Springer Berlin Heidelberg.   \n[29] P. Vamplew, R. Dazeley, C. Foale, S. Firmin, and J. Mummery. Human-aligned artificial intelligence is a multiobjective problem. Ethics and Information Technology, 20, 03 2018.   \n[30] P. Vamplew, C. Foale, R. Dazeley, and A. Bignold. Potential-based multiobjective reinforcement learning approaches to low-impact agents for ai safety. Engineering Applications of Artificial Intelligence, 100, 04 2021.   \n[31] P. Vamplew, C. Foale, C. F. Hayes, P. Mannion, E. Howley, R. Dazeley, S. Johnson, J. K\u00e4llstr\u00f6m, G. Ramos, R. Radulescu, W. R\u00f6pke, and D. M. Roijers. Utility-based reinforcement learning: Unifying single-objective and multi-objective reinforcement learning. In Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems, AAMAS \u201924, page 2717\u20132721, Richland, SC, 2024. International Foundation for Autonomous Agents and Multiagent Systems.   \n[32] P. Vamplew, B. J. Smith, J. K\u00e4llstr\u00f6m, G. Ramos, R. R\u02d8adulescu, D. M. Roijers, C. F. Hayes, F. Heintz, P. Mannion, P. J. K. Libin, R. Dazeley, and C. Foale. Scalar reward is not enough: a response to silver, singh, precup and sutton (2021). Autonomous Agents and Multi-Agent Systems, 36(2), oct 2022.   \n[33] K. Van Moffaert and A. Now\u00e9. Multi-objective reinforcement learning using sets of pareto dominating policies. J. Mach. Learn. Res., 15(1):3483\u20133512, Jan. 2014.   \n[34] K. Wray, S. Zilberstein, and A.-i. Mouaddib. Multi-objective mdps with conditional lexicographic reward preferences. Proceedings of the AAAI Conference on Artificial Intelligence, 29, 01 2015.   \n[35] W. Zhao, J. P. Queralta, and T. Westerlund. Sim-to-real transfer in deep reinforcement learning for robotics: a survey. In 2020 IEEE Symposium Series on Computational Intelligence (SSCI), pages 737\u2013744, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Next, we provide the proofs of all theoretical results of An Analytical Study of Utility Functions in Multi-Objective Reinforcement Learning that could not fit in the main paper. ", "page_idx": 13}, {"type": "text", "text": "A.1 Last part of Example 4 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Example 11. Recall the utility function $\\begin{array}{r}{u(x,y)=\\sqrt{x^{2}+1}+\\frac{y}{20}}\\end{array}$ . ", "page_idx": 13}, {"type": "text", "text": "As previously mentioned This environment has two possible deterministic policies. The first policy is $\\pi_{1}(s_{1})=a_{1},\\pi_{1}(s_{2})=a_{2}$ . This policy obtains values $\\vec{V}^{\\pi_{1}}(s_{1})=(3,2\\bar{0}),\\vec{V}^{\\pi_{1}}(s_{2})=(2,\\bar{2}0)$ , and scalarised values $u(\\vec{V}^{\\pi_{1}}(s_{1}))\\approx4.16,u(\\vec{V}^{\\pi_{1}}(s_{2}))\\approx3.24.$ . ", "page_idx": 13}, {"type": "text", "text": "The second policy is $\\pi_{2}(s_{1})\\;\\;=\\;\\;a_{1},\\pi_{2}(s_{2})\\;\\;=\\;\\;a_{3}$ . This policy obtains values $\\begin{array}{r l}{\\vec{V}^{\\pi_{2}}\\big(s_{1}\\big)}&{{}=}\\end{array}$ $(4,1),\\vec{V}^{\\pi_{2}}(s_{2})=(3,1)$ , and scalarised values $u(\\vec{V}^{\\pi_{2}}(s_{1}))\\approx4.17,u(\\vec{V}^{\\pi_{2}}(s_{2}))\\approx3.21$ . ", "page_idx": 13}, {"type": "text", "text": "Any stochastic policy $\\pi$ will be of the form $p\\pi_{1}+(1-p)\\pi_{2}$ with $1\\geq p\\geq0$ . That means that: ", "page_idx": 13}, {"type": "text", "text": "\u2022 At state $s_{1}$ it will obtain value $\\vec{V}^{\\pi}(s_{1})=(3p+4(1-p),20p+(1-p))=(4-p,19p+1),$   \nand scalarised value $\\begin{array}{r}{u(\\vec{V}^{\\pi}(s_{1}))=\\sqrt{(4-p)^{2}+1}+\\frac{19p+1}{20}}\\end{array}$ .   \n\u2022 At state $s_{2}$ it will obtain value $\\vec{V}^{\\pi}(s_{2})=(2p+3(1-p),20p+(1-p))=(3-p,19p+1),$ and scalarised value $\\begin{array}{r}{u(\\vec{V}^{\\pi}(s_{2}))=\\sqrt{(3-p)^{2}+1}+\\frac{19p+1}{20}}\\end{array}$ ", "page_idx": 13}, {"type": "text", "text": "Consider now the scalarised value of the stochastic policy $\\pi$ as a function $u^{\\prime}(p,s)$ depending on the real variable $p$ . Its derivative is ", "page_idx": 13}, {"type": "equation", "text": "$$\nu^{\\prime}(p,s)=\\frac{p-\\alpha_{s}}{\\sqrt{p^{2}-2\\alpha_{s}p+\\alpha_{s}^{2}+1}}+\\frac{19}{20},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\alpha_{s_{1}}=4_{\\cdot}$ , $\\alpha_{s_{2}}=3$ . The derivative $u^{\\prime}(p,s_{1})$ has a root $r_{1}\\approx0.958,$ , and the derivative $u^{\\prime}(p,s_{2})$ has a root $r_{2}\\approx-0.042$ . Both roots are global minima, and thus $u^{\\prime}(0,s_{1})$ is a global maximum for $[0,1]$ at state $s_{1}$ , and $u^{\\prime}(1,s_{2})$ is a global maximum for $[0,1]$ at state $s_{2}$ . In other words, $\\pi_{1}$ is the absolute $\\langle u,s_{2}\\rangle$ -optimal policy, while $\\pi_{2}$ is the absolute $\\bar{\\langle u,s_{1}\\rangle}$ -optimal policy. Thus, no stochastic $u$ -optimal policy exists. ", "page_idx": 13}, {"type": "text", "text": "A.2 Preliminaries for Theorems ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This Section is devoted to prove an indispensable theorem: that our search for the utility optimal policy can be reduced to searching only among stationary policies. ", "page_idx": 13}, {"type": "text", "text": "Lemma 4. Let $\\mathcal{M}$ be any finite MOMDP. Then, for every policy $\\pi$ , there exists another stationary policy $\\pi^{\\prime}$ such that, for every state s of $\\mathcal{M}$ , it obtains the same expected returns: $\\vec{V}^{\\pi}(s)=\\vec{V}^{\\pi^{\\prime}}(s)$ for every state s. ", "page_idx": 13}, {"type": "text", "text": "Proof 4. Direct generalisation from single-objective MDPs, in which for any policy $\\pi$ , there is another stationary policy $\\pi^{\\prime}$ such that for every state $s_{0}$ it achieves the same value $V^{\\pi}(s_{0})=V^{\\pi^{\\prime}}(s_{0})$ . See Proposition 1.1. of [16] for a full proof for single-objective MDPs. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Theorem 5. Let $\\mathcal{M}$ be any finite MOMDP. Let u be a utility function under the SER criterion. Then: ", "page_idx": 13}, {"type": "text", "text": "\u2022 If an $\\langle u,s\\rangle$ -optimal policy $\\pi_{*}$ exists for a state s of $\\mathcal{M}$ , there is at least another stationary policy $\\pi_{*}^{\\prime}$ such that $\\pi_{*}^{\\prime}$ is also $\\langle u,s\\rangle$ -optimal.   \n\u2022 If an u-optimal policy $\\pi_{*}$ exists for $\\mathcal{M}$ , there is at least another stationary policy $\\pi_{*}^{\\prime}$ such that $\\pi_{*}^{\\prime}$ is also $u$ -optimal. ", "page_idx": 13}, {"type": "text", "text": "Proof 5. We only cover the first case, with the second one being analogous. Let $\\pi_{*}$ be such that $u(\\vec{V}^{\\pi_{*}})(s)\\geq u(\\vec{V}^{\\pi})(s)$ for every state $s$ . Then, by Lemma 4, there exists another stationary policy $\\pi_{*}^{\\prime}$ such that $\\vec{V}^{\\pi_{*}}(s)=\\vec{V}^{\\pi_{*}^{\\prime}}(s)$ for every state $s$ . Thus, $u(\\vec{V}^{\\pi_{*}^{\\prime}}(s))=u(\\vec{V}^{\\pi_{*}}(s))\\geq u(\\vec{V}^{\\pi}(s))$ , and so $\\pi_{*}^{\\prime}$ is also $u$ -optimal. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Notice that Theorem 5 need not be true for utility functions under the ESR criterion. ", "page_idx": 14}, {"type": "text", "text": "A.3 Proof of Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Theorem 6. Let $\\mathcal{M}$ be a finite MOMDP. Let u be a continuous utility function for all value functions of all policies $\\Pi(\\mathcal{M})$ of $\\mathcal{M}$ . Then, for every state s of $\\mathcal{M}$ , at least one stationary $\\langle u,s\\rangle$ -optimal policy exists. ", "page_idx": 14}, {"type": "text", "text": "Proof 6. Without loss of generalisation we only consider stationary policies thanks to Theorem 5. ", "page_idx": 14}, {"type": "text", "text": "Given $\\mathcal{M}$ , consider the polytope (i.e., $n$ -dimensional bounded polyhedron) formed by a convex coverage set $C C S$ of $\\mathcal{M}$ at state $s$ (which has a finite amount of deterministic stationary policies as vertices) [18]. ", "page_idx": 14}, {"type": "text", "text": "Such polytope, by definition of $C C S$ , envelops all images of all possible value functions at state $s$ for $\\mathcal{M}$ . Moreover, the image of any value function at state $s$ can be expressed as a convex combination of the deterministic stationary policies of $C C S$ at that state [18]. ", "page_idx": 14}, {"type": "text", "text": "Since $u$ is a continuous function, and the polytope formed by $C C S$ is closed and bounded, by the Extreme Value Theorem there exists a maximum value vector $\\vec{V}_{*}(s)$ for $u$ in the polytope. ", "page_idx": 14}, {"type": "text", "text": "Since for any value vector $\\vec{V}_{*}(s)$ in the polytope there is an associated stochastic stationary policy [18], we can find the policy $\\pi_{*}$ associated with $\\vec{V}_{*}(s)$ that achieves the maximum value in the polytope. ", "page_idx": 14}, {"type": "text", "text": "Thus, there exists a $\\langle u,s\\rangle$ -optimal policy for every state $s$ of the MOMDP. ", "page_idx": 14}, {"type": "text", "text": "A.4 Proof of Theorem 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma 7. For every finite single-objective MDP $\\mathcal{M}$ , any utility function u that is strictly monotonically increasing preserves the ordering between policies. That is, for every two value functions $V_{1}$ and $V_{2}$ , for every state $s$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(u\\circ V_{1})(s)>(u\\circ V_{2})(s)\\iff V_{1}(s)>V_{2}(s),}\\\\ &{(u\\circ V_{1})(s)=(u\\circ V_{2})(s)\\iff V_{1}(s)=V_{2}(s).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In particular, any optimal policy is also u-optimal in $\\mathcal{M}$ and vice-versa. ", "page_idx": 14}, {"type": "text", "text": "Proof 7. Direct from the definition of strictly monotonic function. ", "page_idx": 14}, {"type": "text", "text": "Lemma 8. For every finite single-objective MDP $\\mathcal{M}$ , for any affine utility function u, there exists a deterministic and stationary u-optimal policy in $\\mathcal{M}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof 8. Any affine function $u$ is quasi-representative of another linear utility function $l$ defined as $l(x)\\,\\doteq\\,f(x)-f(0)$ . For any linear utility function $l$ , there always exists a deterministic and stationary $l$ -optimal policy. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "With these two Lemmas, we can prove that there exists a family of non-linear utility functions for which an $u$ -optimal policy exists (and moreover, the policy is deterministic and stationary): utility functions product of composing an affine function together with a strictly montonically increasing function. ", "page_idx": 14}, {"type": "text", "text": "Theorem 9. Let $\\mathcal{M}$ be a finite multi-objective MDP M. Let u be a utility function decomposable as $u(x)=h(g(x))$ , with $g(x):\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ being an affine function, and $h(x):\\mathbb{R}\\to\\mathbb{R}$ being a strictly monotonically increasing function for all value functions of all policies $\\Pi(\\mathcal{M})$ of $\\mathcal{M}$ . At least one deterministic and stationary $u$ -optimal policy exists. ", "page_idx": 14}, {"type": "text", "text": "Proof 9. Direct consequence of combining Lemma 7 and Lemma 8. ", "page_idx": 14}, {"type": "text", "text": "We divide the proof in two steps. First, we prove that a function decomposable in a linear function and a strictly monotonically increasing function has $u$ -optimal policies: ", "page_idx": 14}, {"type": "text", "text": "(i) First, as Lemma 7 states, applying a strictly monotonically increasing utility function to a single-objective MDP does not modify its set of deterministic and stationary optimal policies.   \n(ii) Second, every linear utility function $l u$ can transform a MOMDP $\\mathcal{M}$ into a single-objective MDP $\\mathcal{M}^{\\prime}$ with the scalarised reward function $l u\\dot{\\pi}$ . Of course, all optimal policies of the single-objective MDP $\\mathcal{M}^{\\prime}$ are precisely the $l u$ -optimal policies of $\\mathcal{M}$ (for the technical proof of this see Section 2.2 of the main paper).   \n(iii) These two facts together tell us: given a utility function $f$ that can be decomposed into a strictly monotonically increasing function smi, and a linear function $l u$ , then $f(x)=$ $s m i(l u(x))$ will have deterministic and stationary $f$ -optimal policies (which will be exactly the deterministic and stationary $l u$ -optimal policies). ", "page_idx": 15}, {"type": "text", "text": "Next, we prove that the following two functions are quasi-representative: a function decomposable in an affine function and a strictly monotonically increasing function, and another decomposable in a linear function and a strictly monotonically increasing function. ", "page_idx": 15}, {"type": "text", "text": "(iv) Next, Lemma 8 proves that any affine utility function $a f$ is quasi-representative of another linear utility function $l u$ . More precisely, in every MOMDP, all $a f$ -optimal policies are also $l u$ -optimal policies and vice-versa for the linear utility function $l u$ defined as $l u(x)=$ $a f(x)-a f(0)$ . This is because any affine function $a f(x)$ can be decomposed as $a f(x)=$ $A(x)+b$ , with $A(x)$ being a linear function and $b=a f(0)$ , a constant.   \n(v) Now, consider a utility function $f(x)=s m i(a f(x))$ that can be decomposed as a product of a strictly monotonically increasing utility smi function and an affine utility function $a f$ . Consider also the utility function $f^{\\prime}(x)=s m i(a f(x)-a f(0))$ . This second function $f^{\\prime}(x)$ is composed by a strictly monotonically increasing function and a linear function, so as previously proven in $(i i i)$ , there are deterministic and stationary $f^{\\prime}$ -optimal policies.   \n(vi) Then, recall that by $(i v)$ , utility functions $a f(x)$ and $a f(x)-a f(0)$ are quasi-representative (i.e., they share the same optimal policies). Thus, it is clear that $s m i(a f(x))$ and $s m i(a f(\\stackrel{.}{x})\\!-\\!a f(0))$ are also quasi-representative, because strictly monotonically increasing functions preserve the ordering by definition. ", "page_idx": 15}, {"type": "text", "text": "Finally, since $f^{\\prime}(x)=s m i(a f(x)-a f(0))$ has deterministic and stationary $f^{\\prime}$ -optimal policies, and $f(x)=s m i(a f(x))$ and $f^{\\prime}(x)$ are quasi-representative, we conclude that there are also deterministic and stationary $f$ -optimal policies. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We characterised preferences that can be expressed as utility functions with Theorem 3, and we characterised sufficient conditions for utility functions to have associated optimal policies with Theorems 1 (per state), and 2 (for the whole MOMDP). ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: As a fully theoretical paper, all limitations of our theoretical results are transparent by looking at their necessary conditions. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Every Theorem and Lemma has its respective proof. Each proof lists all necessary previous theoretical results to be proved. Every Theorem clearly states its assumptions. Every Theorem is numbered and has its respective proof inmediately below. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: As a fully theoretical paper, the paper does not include experiments. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: As a fully theoretical paper, the paper does not include experiments. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: As a fully theoretical paper, the paper does not include experiments. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: As a fully theoretical paper, the paper does not include experiments. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: Justification: As a fully theoretical paper, it does not include any experiment. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: As a fully theoretical paper, it did not include human subjects or participants, there is no dataset involved, and we envision no potential harmful consequences in society of our theorems. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: As a fully theoretical paper, it poses no societal impact on itself. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: As a fully theoretical paper, it poses no such risks. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: Justification: As a fully theoretical paper, it does not use existing assets. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 20}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: Justification: As a fully theoretical paper, it does not release new assets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: Justification: As a fully theoretical paper, it does not involve crowdsourcing nor research with human subjects. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: As a fully theoretical paper, it does not involve crowdsourcing nor research with human subjects. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]