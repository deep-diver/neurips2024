[{"figure_path": "8hBc843g1p/figures/figures_3_1.jpg", "caption": "Figure 2: How (a) the loss and (b) the match rate changes with attack iterations. The attacks are performed against Llama-2-7B-Chat model to generate query-specific adversarial suffixes on AdvBench. Best viewed in color.", "description": "This figure shows the impact of Language Skip Gradient Method (LSGM) on the performance of Gradient-based attacks against LLMs.  It presents two graphs: (a) Loss and (b) Match Rate. The x-axis represents the number of attack iterations. The y-axis of (a) Loss shows the value of the loss function, while the y-axis of (b) Match Rate represents the percentage of successful attacks where the model output exactly matches the target string. The figure compares the performance of four attack methods: GCG, GCG-LSGM, AutoPrompt, and AutoPrompt-LSGM. The shaded area around the lines represents the standard deviation. Overall, it demonstrates that incorporating LSGM into GCG and AutoPrompt improves the efficiency and success rate of adversarial attacks.", "section": "3.2 Reducing the Gradients from Residual Modules"}, {"figure_path": "8hBc843g1p/figures/figures_4_1.jpg", "caption": "Figure 3: The cosine similarities between the gradients from residual modules and the gradients from skip connections in different residual blocks.", "description": "This figure visualizes the cosine similarity between gradients from residual modules and skip connections within different residual blocks of a neural network.  The x-axis represents the residual block number, and the y-axis shows the cosine similarity.  The plot reveals a negative correlation in most blocks, indicating that gradients from residual modules and skip connections often pull in opposite directions during the optimization process. This negative correlation is a key observation that supports the effectiveness of the Language Skip Gradient Method (LSGM) technique presented in the paper.", "section": "3.2 Reducing the Gradients from Residual Modules"}, {"figure_path": "8hBc843g1p/figures/figures_4_2.jpg", "caption": "Figure 4: Comparing the average effects of residual modules and the average effects of skip connections on the change in adversarial loss varies with different residual blocks. Best viewed in color.", "description": "This figure shows a bar chart comparing the average effects of residual modules and skip connections on adversarial loss reduction across different residual blocks in a neural network.  The x-axis represents the m-th residual block, and the y-axis shows the effect on loss reduction. Each bar is split into two parts, representing the effect of the residual module (MLP/attention) and the skip connection, respectively.  The chart illustrates the relative contribution of each component to adversarial loss reduction within different layers of the network, highlighting the relative importance of skip connections in this process.", "section": "3.2 Reducing the Gradients from Residual Modules"}, {"figure_path": "8hBc843g1p/figures/figures_5_1.jpg", "caption": "Figure 5: (a) The PCCs computed on the entire intermediate representations, i.e., PCC(hFVhL(x), L(x)) and PCC(hFvr, L(x)), at different layers of Llama-2-7B-Chat. (b) The PCCs computed on the o-th token intermediate representations, i.e., PCC(hr,o\u2207hr,,L(x), L(x)) and PCC(hour,o, L(x)), at the mid-layer of Llama-2-7B-Chat. Best viewed in color.", "description": "This figure displays the Pearson's correlation coefficient (PCC) between the scalar projection of intermediate representations and the adversarial loss at different layers of the Llama-2-7B-Chat model.  (a) shows the PCCs for the entire intermediate representations, while (b) focuses on the PCCs for individual tokens in the mid-layer. The results help to evaluate the effectiveness of the Intermediate Level Attack (ILA) strategy in improving gradient-based adversarial prompt generation.", "section": "3.3 Adapting Intermediate Level Attack for Gradient-base Attacks Against LLMs"}, {"figure_path": "8hBc843g1p/figures/figures_6_1.jpg", "caption": "Figure 7: How (a) the loss and (b) the match rate changes with attack iterations. The attacks are performed against Llama-2-7B-Chat model to generate query-specific adversarial suffixes on AdvBench. Best viewed in color.", "description": "This figure shows the results of the experiments for query-specific adversarial suffix generation against the Llama-2-7B-Chat model on AdvBench.  The left panel (a) displays the adversarial loss over the iterations of the attack, showing how the loss changes as the attacks progress for different methods (GCG, GCG-LILA, AutoPrompt, and AutoPrompt+LILA). The shaded areas represent the standard deviations. The right panel (b) illustrates the match rate (percentage of attacks achieving exact target string matches) over iterations, offering a comparison of attack success among the four methods. This visualization allows for an assessment of the effectiveness of the different attack methods, showing that both LILA and LSGM improve on the baseline methods.", "section": "3.3 Adapting Intermediate Level Attack for Gradient-base Attacks Against LLMs"}, {"figure_path": "8hBc843g1p/figures/figures_12_1.jpg", "caption": "Figure 8: How the match rate and attack success rate change with (a) the choice of \u03b3 for GCG-LSGM, (b) the choice of layer for GCG-LILA. Best viewed in color.", "description": "This figure presents the ablation study results for the two proposed methods: Language Skip Gradient Method (LSGM) and Language Intermediate Level Attack (LILA).  The left subfigure (a) shows how the match rate and attack success rate of GCG-LSGM vary with different values of \u03b3 (the decay factor used in LSGM to reduce gradients from residual modules). The right subfigure (b) shows the impact of different intermediate layers selected for applying LILA on GCG's performance.", "section": "B Ablation Study"}]