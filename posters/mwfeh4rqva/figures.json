[{"figure_path": "MwFeh4RqvA/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of key components of our proposed methodology.", "description": "This figure provides a visual representation of the proposed methodology, including the VAE latent representation and training, the diffusion model inference with mutual conditioning, and the scene composition process compared to state-of-the-art multi-layer methods.  Panel (a) shows the VAE architecture and training process, highlighting the disentanglement of RGB and alpha channels for transparency preservation. Panel (b) illustrates the diffusion model inference process, showing the mutual conditioning between RGB and alpha channels during denoising. Panel (c) visually compares the proposed multi-stage scene composition approach with existing multi-layer methods, demonstrating its iterative integration of pre-generated RGBA instances and its ability to achieve fine-grained control over object attributes and scene layout.", "section": "3 Methods"}, {"figure_path": "MwFeh4RqvA/figures/figures_6_1.jpg", "caption": "Figure 2: Our model can generalise to different styles and to follow detailed instructions. Top row: \u2018a cartoon style frog\u2019, \u2018a digital artwork of an anime-style character with long, flowing white hair and large and expressive purple eyes in a white attire\u2019, \u2018a stylised character with a traditional Asian hat, with a red and green pattern\u2019, \u2018a man with a contemplative expression and a neatly trimmed beard\u2019, Bottom row: \u2018a woman with a classic, vintage style, curly hair, red lipstick, fair skin in a dark attire\u2019, \u2018a bird mid-flight with brown and white feathers and orange head\u2019, \u2018a hand-painted ceramic vase in blue and yellow colours and with a floral pattern\u2019, \u2018a woman with short, blonde hair, vivid green eyes, in a white blouse, with a gold necklace featuring a pendant with a gemstone\u2019.", "description": "This figure showcases the ability of the RGBA generator to produce diverse and high-quality images while adhering to detailed instructions embedded in the prompts. Each image demonstrates mastery over style and fine-grained attributes. The top row shows a variety of styles from cartoonish to anime-style, and the bottom row demonstrates realism, precise attributes, and even complex object descriptions.", "section": "4 Experiments"}, {"figure_path": "MwFeh4RqvA/figures/figures_6_2.jpg", "caption": "Figure 3: Instances generated with the captions: \u2018a majestic brown bear with dark brown fur, its head slightly tilted to the left and its mouth slightly open\u2019, \u2018an Impressionist portrait of a woman\u2019, \u2018a portrait of a young man, depicted in a blend of blue and red tones\u2019.", "description": "This figure shows qualitative comparison results of instances generated by different methods. The compared methods are PixArt-a, Stable Diffusion v1.5, the two combined with Matte Anything matting, Text2Layer, LayerDiffusion and the proposed method. The captions used to generate the images are provided in the caption of the figure. The figure demonstrates that the proposed method is capable of generating realistic images following detailed instructions in the caption.", "section": "Experiments"}, {"figure_path": "MwFeh4RqvA/figures/figures_7_1.jpg", "caption": "Figure 1: Overview of key components of our proposed methodology.", "description": "This figure shows a high-level overview of the proposed methodology for generating compositional scenes via text-to-image RGBA instance generation. It consists of three main parts: (a) VAE latent representation and training, illustrating how the variational autoencoder (VAE) is trained to learn a disentangled representation of RGB and alpha channels for RGBA images; (b) Diffusion model inference with mutual conditioning, showing how the diffusion model is used to generate RGBA instances by sequentially denoising RGB and alpha latents with mutual conditioning; and (c) Our scene composition process compared to state-of-the-art multi-layer methods, highlighting how the proposed multi-layer composite generation process smoothly assembles pre-generated RGBA instances into complex images with realistic scenes.", "section": "3 Methods"}, {"figure_path": "MwFeh4RqvA/figures/figures_8_1.jpg", "caption": "Figure 1: Overview of key components of our proposed methodology.", "description": "This figure provides a visual overview of the proposed methodology for generating compositional scenes via text-to-image RGBA instance generation. It consists of three main parts:\n(a) VAE latent representation and training: Illustrates the variational autoencoder (VAE) used for latent representation learning and training, highlighting the disentanglement of RGB and alpha channels for transparency preservation.\n(b) Diffusion model inference with mutual conditioning: Shows the diffusion model inference process with mutual conditioning between RGB and alpha channels latents for sequential denoising and improved transparency control.\n(c) Our scene composition process compared to state-of-the-art multi-layer methods: Contrasts the proposed multi-layer scene composition process with existing methods, emphasizing the sequential integration of pre-generated instances for improved control and realism.", "section": "3 Methods"}, {"figure_path": "MwFeh4RqvA/figures/figures_8_2.jpg", "caption": "Figure 1: Overview of key components of our proposed methodology.", "description": "This figure provides a visual summary of the proposed methodology for generating compositional scenes. It shows three main stages: 1) VAE latent representation and training for generating RGBA instances, 2) Diffusion model inference with mutual conditioning for generating RGBA instances, and 3) Scene composition process that compares the proposed method to other state-of-the-art multi-layer methods.", "section": "3 Methods"}, {"figure_path": "MwFeh4RqvA/figures/figures_8_3.jpg", "caption": "Figure 1: Overview of key components of our proposed methodology.", "description": "This figure provides a visual overview of the proposed methodology for generating compositional scenes via text-to-image RGBA instance generation. It consists of three main parts:\n\n(a) VAE latent representation and training: Illustrates the process of training a variational autoencoder (VAE) to generate a latent representation of images with transparency information.\n(b) Diffusion model inference with mutual conditioning: Shows how the diffusion model is used to generate images by iteratively denoising the latent representation with mutual conditioning between RGB and alpha channels.\n(c) Our scene composition process compared to state of the art multi-layer methods: Compares the proposed multi-layer scene composition process with existing state-of-the-art methods. This part highlights the sequential integration of instances in intermediate layered representations, allowing for fine-grained control over attributes and layout.", "section": "3 Methods"}, {"figure_path": "MwFeh4RqvA/figures/figures_9_1.jpg", "caption": "Figure 6: Visual examples of scene manipulations compared to Instance Diffusion. Our layer-based approach allows to replace instances or modify their positions.", "description": "This figure compares the scene manipulation capabilities of the proposed method with the Instance Diffusion method.  It shows several examples of modifications made to a scene, including replacing objects and changing their position.  The results demonstrate the proposed method's superior ability to maintain scene consistency during these edits, unlike Instance Diffusion, which produces more significant changes to the overall scene. This highlights the advantages of using the proposed layer-based approach for fine-grained control and scene manipulation.", "section": "Experiments"}, {"figure_path": "MwFeh4RqvA/figures/figures_15_1.jpg", "caption": "Figure 7: Images generated with our LDM fine-tuned in the latent space of VAEs that were trained with a single KL loss with weight 10e \u2013 6 (a), 2 separate KL losses each with weight 10e \u2013 6 (b), and 2 KL losses with weight 1 (c).", "description": "This figure shows the impact of different training paradigms for the RGBA VAE on generated samples. Three images generated with a diffusion model fine-tuned on different VAE latent spaces are presented.  (a) shows images generated using a single KL loss with weight 10e-6, (b) uses 2 separate KL losses each with weight 10e-6, and (c) uses 2 KL losses with weight 1.  The differences illustrate how the choice of VAE training affects the quality and contrast of the generated images.", "section": "A.3 VAE Ablation"}, {"figure_path": "MwFeh4RqvA/figures/figures_18_1.jpg", "caption": "Figure 1: Overview of key components of our proposed methodology.", "description": "This figure shows a diagram that illustrates the three main components of the proposed methodology: (a) VAE latent representation and training; (b) Diffusion model inference with mutual conditioning; (c) Our scene composition process compared to state-of-the-art multi-layer methods.  It provides a visual overview of the system architecture, highlighting the different steps and how they interact with each other. The multi-layer scene composition process is compared against other state-of-the-art methods to highlight its unique characteristics.", "section": "Methods"}, {"figure_path": "MwFeh4RqvA/figures/figures_19_1.jpg", "caption": "Figure 1: Overview of key components of our proposed methodology.", "description": "The figure provides a visual representation of the proposed methodology, showcasing three key components:\n(a) VAE latent representation and training:  Illustrates the Variational Autoencoder (VAE) used for latent space representation and its training process, emphasizing disentanglement of RGB and alpha channels for transparency handling. \n(b) Diffusion model inference with mutual conditioning: Shows the diffusion model inference process with mutual conditioning between RGB and alpha channels during sequential denoising. \n(c) Our scene composition process compared to state-of-the-art multi-layer methods: Compares the proposed multi-layer scene composition approach with existing methods, highlighting the sequential integration of pre-generated RGBA instances. This contrasts with existing methods that often process all layers simultaneously, showcasing a key differentiator of the proposed approach.", "section": "3 Methods"}, {"figure_path": "MwFeh4RqvA/figures/figures_19_2.jpg", "caption": "Figure 1: Overview of key components of our proposed methodology.", "description": "This figure provides a visual overview of the proposed methodology, showing three key components: (a) VAE latent representation and training; (b) diffusion model inference with mutual conditioning; and (c) scene composition process compared to state-of-the-art multi-layer methods.  It illustrates the two-stage process of the approach, first generating individual instances as RGBA images and then integrating them in a multi-layer composite image. The figure contrasts the proposed method with existing layer-wise methods, highlighting the differences in the generation and composition processes. It visually represents the key concepts, training process, and the overall workflow of the proposed multi-stage generation paradigm.", "section": "3 Methods"}, {"figure_path": "MwFeh4RqvA/figures/figures_19_3.jpg", "caption": "Figure 1: Overview of key components of our proposed methodology.", "description": "This figure provides a visual overview of the methodology used in the paper. Panel (a) shows the VAE latent representation and training process, highlighting the separation of RGB and alpha channels and the mutual conditioning during inference. Panel (b) illustrates the diffusion model inference with mutual conditioning between RGB and alpha latents, emphasizing the sequential denoising process. Panel (c) compares the proposed scene composition process to state-of-the-art multi-layer methods, showcasing the sequential integration of pre-generated RGBA instances in a multi-layer composite image, highlighting the benefits of this approach in terms of flexibility and control.", "section": "3 Methods"}, {"figure_path": "MwFeh4RqvA/figures/figures_19_4.jpg", "caption": "Figure 1: Overview of key components of our proposed methodology.", "description": "This figure provides a visual overview of the proposed methodology's key components. It showcases the VAE latent representation and training, the diffusion model inference with mutual conditioning, and the scene composition process. The scene composition process is compared with state-of-the-art multi-layer methods, highlighting the differences in approach and workflow. Overall, it presents a high-level visual summary of the key steps involved in the novel multi-stage generation paradigm.", "section": "3 Methods"}, {"figure_path": "MwFeh4RqvA/figures/figures_19_5.jpg", "caption": "Figure 1: Overview of key components of our proposed methodology.", "description": "This figure provides a high-level overview of the proposed methodology for generating compositional scenes.  It shows three key stages: (a) VAE latent representation and training which focuses on disentangling RGB and alpha channels; (b) Diffusion model inference with mutual conditioning that leverages this disentanglement to generate individual objects as RGBA images; and (c) Scene composition process that contrasts the authors' approach with other state-of-the-art methods, showcasing the iterative integration of RGBA instances into a multi-layer composite image, achieving fine-grained control over attributes and layout.", "section": "3 Methods"}, {"figure_path": "MwFeh4RqvA/figures/figures_20_1.jpg", "caption": "Figure 6: Visual examples of scene manipulations compared to Instance Diffusion. Our layer-based approach allows to replace instances or modify their positions.", "description": "This figure compares the results of scene manipulations using the proposed multi-layer approach versus the Instance Diffusion method.  The goal is to showcase the ability to easily replace or reposition objects in the generated scene, maintaining overall scene consistency.  The top row demonstrates changes to object attributes and the addition/removal of objects, while the bottom row demonstrates moving and resizing objects. The proposed approach is shown to maintain higher fidelity and better scene coherence compared to the baseline method.", "section": "Experiments"}, {"figure_path": "MwFeh4RqvA/figures/figures_20_2.jpg", "caption": "Figure 1: Overview of key components of our proposed methodology.", "description": "This figure shows a schematic overview of the proposed methodology, which consists of three main components: VAE latent representation and training, diffusion model inference with mutual conditioning, and the scene composition process.  The VAE is used to create a latent representation of the images, the diffusion model is used to generate images from the latent representation, and the scene composition process is used to combine multiple images into a single composite image. The figure also shows how the proposed methodology compares to state-of-the-art multi-layer methods.", "section": "Methods"}, {"figure_path": "MwFeh4RqvA/figures/figures_20_3.jpg", "caption": "Figure 1: Overview of key components of our proposed methodology.", "description": "This figure shows the key components of the proposed methodology. Panel (a) illustrates the VAE latent representation and training process, panel (b) shows the diffusion model inference with mutual conditioning of RGB and alpha channels, and panel (c) compares the scene composition process used in the proposed methodology with state-of-the-art multi-layer methods.  The proposed method uses RGBA images for generating isolated scene components and then iteratively integrating them into a multi-layer composite image to build complex scenes.", "section": "3 Methods"}, {"figure_path": "MwFeh4RqvA/figures/figures_20_4.jpg", "caption": "Figure 6: Visual examples of scene manipulations compared to Instance Diffusion. Our layer-based approach allows to replace instances or modify their positions.", "description": "This figure compares the results of scene manipulation experiments using the proposed multi-layer approach and the Instance Diffusion method.  The images demonstrate the ability to modify scene elements, such as replacing objects or repositioning them, while maintaining overall scene consistency. The proposed approach shows a greater ability to preserve the original scene context during manipulations compared to the Instance Diffusion method.", "section": "Experiments"}, {"figure_path": "MwFeh4RqvA/figures/figures_20_5.jpg", "caption": "Figure 1: Overview of key components of our proposed methodology.", "description": "This figure shows a schematic overview of the proposed methodology. (a) shows the VAE latent representation and training. (b) shows the diffusion model inference with mutual conditioning. (c) shows the scene composition process compared to state-of-the-art multi-layer methods. The proposed method uses a multi-stage generation process that first generates individual instances as RGBA images with transparency information, then integrates these instances into a multi-layer composite image according to a specific layout.", "section": "3 Methods"}, {"figure_path": "MwFeh4RqvA/figures/figures_20_6.jpg", "caption": "Figure 6: Visual examples of scene manipulations compared to Instance Diffusion. Our layer-based approach allows to replace instances or modify their positions.", "description": "This figure compares the results of scene manipulation using the proposed multi-layer approach against the Instance Diffusion method.  It shows that the proposed approach allows for easier replacement of instances and modification of their positions while maintaining a higher degree of scene consistency. The images demonstrate various changes including adding a new item (a ball of yarn), changing an item (the expression of a mug), moving items, and rescaling items.", "section": "Experiments"}, {"figure_path": "MwFeh4RqvA/figures/figures_20_7.jpg", "caption": "Figure 1: Overview of key components of our proposed methodology.", "description": "This figure provides a high-level overview of the proposed methodology for generating compositional scenes using RGBA instance generation. It shows three main stages:\n(a) VAE latent representation and training: This stage focuses on training a Variational Autoencoder (VAE) to learn a latent representation of RGBA images, which are images with an alpha channel representing transparency information. This allows for generating images with fine-grained control over object attributes, including color, pattern, and pose. \n(b) Diffusion model inference with mutual conditioning: This stage describes the fine-tuning of a diffusion model to generate RGBA images by leveraging mutual conditioning of the RGB and alpha channels in the latent space. This ensures accurate and realistic generation of transparent images.\n(c) Our scene composition process compared to state-of-the-art multi-layer methods: This stage presents the multi-layer composite generation process that iteratively integrates pre-generated RGBA instances into a composite image. This approach allows for fine-grained control over object appearance, location, and the layout of the scene, resulting in a high degree of control and flexibility compared to existing methods. The figure highlights that the authors' proposed method offers advantages in terms of controllability, flexibility, and interactivity in scene composition compared to state-of-the-art approaches.", "section": "3 Methods"}, {"figure_path": "MwFeh4RqvA/figures/figures_21_1.jpg", "caption": "Figure 1: Overview of key components of our proposed methodology.", "description": "This figure provides a high-level overview of the proposed methodology in the paper, illustrating the three main components: VAE latent representation and training, diffusion model inference with mutual conditioning, and the scene composition process.  The VAE section shows how it learns to represent images in a latent space, disentangling the RGB and alpha channels. The diffusion model section illustrates the inference process with mutual conditioning for generating RGBA images. Finally, the scene composition section compares the proposed multi-layer process with existing state-of-the-art methods, highlighting the advantages of the proposed approach in generating complex scenes with precise control over instance attributes and locations.", "section": "3 Methods"}, {"figure_path": "MwFeh4RqvA/figures/figures_22_1.jpg", "caption": "Figure 1: Overview of key components of our proposed methodology.", "description": "This figure shows an overview of the proposed methodology, which consists of three main components: VAE latent representation and training, Diffusion model inference with mutual conditioning, and scene composition process.  The VAE is used to learn a latent representation of RGBA images, which are then used to train a diffusion model. The diffusion model is used to generate images by iteratively denoising the latent representation. The scene composition process is used to combine multiple RGBA instances into a single image.", "section": "3 Methods"}, {"figure_path": "MwFeh4RqvA/figures/figures_22_2.jpg", "caption": "Figure 1: Overview of key components of our proposed methodology.", "description": "This figure provides a visual representation of the three key components of the proposed methodology. (a) shows the VAE latent representation and training process for generating images with transparency information. (b) illustrates the diffusion model inference process with mutual conditioning of RGB and alpha channels. (c) compares the authors' scene composition process with state-of-the-art multi-layer methods, highlighting the difference in generating and integrating pre-generated instances as RGBA images into the final scene.", "section": "3 Methods"}, {"figure_path": "MwFeh4RqvA/figures/figures_22_3.jpg", "caption": "Figure 1: Overview of key components of our proposed methodology.", "description": "This figure provides a visual overview of the proposed methodology for generating compositional scenes. It highlights three key components: (a) The VAE latent representation and training which generates the individual instances as RGBA images. (b) The diffusion model inference with mutual conditioning which ensures control over instance attributes. (c) The scene composition process that smoothly assembles components in realistic scenes, compared to other state-of-the-art multi-layer methods. The figure showcases the multi-stage generation process with the steps involved and how the components are integrated into a final image.", "section": "3 Methods"}, {"figure_path": "MwFeh4RqvA/figures/figures_22_4.jpg", "caption": "Figure 1: Overview of key components of our proposed methodology.", "description": "This figure provides a visual summary of the paper's methodology, broken down into three stages: (a) VAE latent representation and training, (b) Diffusion model inference with mutual conditioning, and (c) Our scene composition process compared to state-of-the-art multi-layer methods.  The diagram shows the steps involved in generating RGBA images, incorporating transparency information, and using a multi-layer composite generation process for building complex scenes.  It highlights the differences between the proposed method and existing multi-layer approaches.", "section": "3 Methods"}]