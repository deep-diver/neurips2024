[{"type": "text", "text": "Invertible Consistency Distillation for Text-Guided Image Editing in Around 7 Steps ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Nikita Starodubcev1 2 Mikhail Khoroshikh1 2 Artem Babenko1 Dmitry Baranchuk1 1Yandex Research 2HSE University ", "page_idx": 0}, {"type": "text", "text": "https://yandex-research.github.io/invertible-cd ", "page_idx": 0}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/4769a6ec83f0ddf44ce9bba9efb3e2dd640b640aa5bfa795fba99038830d18a5.jpg", "img_caption": ["Figure 1: Invertible Consistency Distillation (iCD) enables both fast image editing and strong generation performance in a few model evaluations. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion distillation represents a highly promising direction for achieving faithful text-to-image generation in a few sampling steps. However, despite recent successes, existing distilled models still do not provide the full spectrum of diffusion abilities, such as real image inversion, which enables many precise image manipulation methods. This work aims to enrich distilled text-to-image diffusion models with the ability to effectively encode real images into their latent space. To this end, we introduce invertible Consistency Distillation (iCD), a generalized consistency distillation framework that facilitates both high-quality image synthesis and accurate image encoding in only 3 4 inference steps. Though the inversion problem for text-to-image diffusion models gets exacerbated by high classifier-free guidance scales, we notice that dynamic guidance significantly reduces reconstruction errors without noticeable degradation in generation performance. As a result, we demonstrate that iCD equipped with dynamic guidance may serve as a highly effective tool for zero-shot text-guided image editing, competing with more expensive state-of-the-art alternatives. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Recently, text-to-image diffusion models [1, 2, 3, 4, 5, 6] have become a dominant paradigm in image generation based on user-provided textual prompts. The exceptional quality of these models makes them a valuable tool for graphics editors, especially for various image manipulation tasks [7, 8, 9]. In practice, however, the applicability of diffusion models is often hindered by their slow inference, which stems from a sequential sampling procedure, gradually recovering images from pure noise. ", "page_idx": 1}, {"type": "text", "text": "To speed-up the inference, many recent works aim to reduce the number of diffusion steps via diffusion distillation [10, 11, 12, 13, 14, 15, 16, 17] that has provided significant progress in highquality generation in 1\u22124 steps and has already been successfully scaled to the state-of-the-art textto-image diffusion models [18, 19, 20, 21, 22, 23, 24]. Though the existing distillation approaches still often trade either mode coverage or image quality for few-step inference, the proposed models can already be feasible for practical applications, such as text-driven image editing [25, 26, 27]. ", "page_idx": 1}, {"type": "text", "text": "The most effective diffusion-based editing methods typically require encoding real images into the latent space of a diffusion model. For \u201cundistilled\u201d models, this encoding is possible by virtue of the connection of diffusion modeling [28] with denoising score matching [29] through SDE and probability flow ODE (PF ODE) [30]. The ODE perspective of diffusion models reveals their reversibility, i.e., the ability to encode a real image into the model latent space and closely reconstruct it with minimal changes. This ability is successfully exploited in various applications, such as text-driven image editing [31, 32, 33], domain translation [34, 9], style transfer [35]. ", "page_idx": 1}, {"type": "text", "text": "Nevertheless, it remains unclear if distilled models can be enriched with such reversibility since existing diffusion distillation methods primarily focus on achieving efficient generation. This work positively answers this question by proposing invertible Consistency Distillation (iCD), a generalized consistency modeling framework [10, 12, 13] enabling both high-quality image generation and accurate inversion in a few sampling steps. ", "page_idx": 1}, {"type": "text", "text": "In practice, text-to-image models leverage classifier-free guidance (CFG) [36], which is crucial for high-fidelity text-to-image generation [1, 3] and text-guided editing [32, 33]. However, the guided diffusion processes yield significant challenges for inversion-based editing methods [32]. Previous approaches [32, 37, 38, 39, 40, 41, 42, 27, 25, 26, 43, 44] have extensively addressed these challenges but often necessitate high computational budget to achieve both strong image manipulations and faithful content preservation. While some of these techniques are applicable to the distilled models [25, 26, 27], they still dilute the primary advantage of distilled diffusion models: efficient inference. ", "page_idx": 1}, {"type": "text", "text": "One of the main ingredients of the iCD framework is how it operates with guided diffusion processes. Recently, dynamic guidance has been proposed to improve distribution diversity without noticeable loss in image quality [45, 46]. The key idea is to deactivate CFG for high diffusion noise levels to stimulate exploration at earlier sampling steps. In this work, we notice that dynamic CFG can facilitate image inversion while preserving the editability of the text-to-image diffusion models. Notably, dynamic CFG yields no computational overhead, entirely leveraging the efficiency gains from diffusion distillation. In our experiments, we demonstrate that invertible distilled models equipped with dynamic guidance are a highly effective inversion-based image editing tool. ", "page_idx": 1}, {"type": "text", "text": "To sum up, our contributions can be formulated as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a generalized consistency distillation framework, invertible Consistency Distillation (iCD), enabling both high-fidelity text-to-image generation and accurate image encoding in around 3\u22124 sampling steps.   \n\u2022 We investigate dynamic classifier-free guidance in the context of image inversion and textguided editing. We demonstrate that it preserves editability of the text-to-image diffusion models while significantly increasing the inversion quality for free.   \n\u2022 We apply iCD to large-scale text-to-image models such Stable Diffusion 1.5 [4] and XL [1] and extensively evaluate them for image editing problems. According to automated and human studies, we confirm that iCD unlocks faithful text-guided image editing for 6\u22128 steps and is comparable to state-of-the-art text-driven image manipulation methods while being multiple times faster. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion probabilistic models DPMs [29, 28, 47] are a class of generative models producing samples from a simple, typically standard normal, distribution by solving the underlying Probability Flow ODE [30, 48], involving iterative score function estimation. DPMs are trained to approximate the score function and employ dedicated diffusion ODE solvers [49, 50, 48] for sampling. DDIM [49] is a simple yet effective solver, widely used in text-to-image models and operating in around 50 steps. A single DDIM step from $\\pmb{x}_{t}$ to $\\pmb{x}_{s}$ can be formulated as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{x}_{s}^{w}=\\mathrm{DDIM}(\\pmb{x}_{t},t,s,c,w)=\\sqrt{\\frac{\\alpha_{s}}{\\alpha_{t}}}\\pmb{x}_{t}+\\epsilon_{\\theta}^{w}(\\pmb{x}_{t},t,c)\\left(\\sqrt{1-\\alpha_{s}}-\\sqrt{\\frac{\\alpha_{s}}{\\alpha_{t}}}\\sqrt{1-\\alpha_{t}}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\alpha_{s},\\,\\alpha_{t}$ are defined according to the diffusion schedule [47], and $\\epsilon_{\\theta}^{w}(\\mathbf{\\boldsymbol{x}}_{t},t,\\mathbf{\\boldsymbol{c}})=\\epsilon_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t},t,\\mathcal{O})+$ $w(\\epsilon_{\\theta}(x_{t},t,c)-\\epsilon_{\\theta}(x_{t},t,\\mathcal{O}))$ is a linear combination of conditional and unconditional noise predictions called as Classifier-Free Guidance (CFG) [51], used to improve the image quality and context alignment in conditional generation. In the following, we omit the condition $^c$ for simplicity. ", "page_idx": 2}, {"type": "text", "text": "Due to reversibility, the PF ODE can be solved in both directions: encoding data into the noise space and decoding it back without additional optimization procedures. We refer to this process as inversion [32] where encoding and decoding correspond to forward and reverse processes, respectively. ", "page_idx": 2}, {"type": "text", "text": "Consistency Distillation CD [10, 12, 13] is the recent state-of-the-art diffusion distillation approach for few-step image generation, which learns to integrate the PF ODE induced with a pretrained diffusion model. In more detail, the model $\\scriptstyle f_{\\theta}$ is trained to satisfy the self-consistency property: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{CD}}(\\pmb{\\theta})=\\mathbb{E}\\left[d(\\pmb{f}_{\\pmb{\\theta}}(\\pmb{x}_{t_{n-1}}^{w},t_{n-1}),\\pmb{f}_{\\pmb{\\theta}}(\\pmb{x}_{t_{n}},t_{n}))\\right]\\rightarrow\\operatorname*{min}_{\\pmb{\\theta}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $t_{n}\\;\\in\\;\\{t_{0},...,t_{N}\\}$ is a discrete time step, $d(\\cdot,\\cdot)$ denotes a distance function and $\\pmb{x}_{t_{n-1}}^{w}$ is obtained with a single step of the DDIM solver from $t_{n}$ to $t_{n-1}$ using the teacher diffusion model. The optimum of (2) is defined by the boundary condition, $f_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t_{0}},t_{0})=\\mathbf{\\boldsymbol{x}}_{t_{0}}$ . Therefore, consistency models (CMs) learn the transition from any trajectory point to the starting one: $f_{\\theta}(x_{t_{n}},t_{n})\\,=$ $x_{t_{0}},\\forall\\,t_{n}\\in\\{t_{0},...,t_{N}\\}$ . Consequently, CMs imply a single step generation. However, approximating the entire trajectory using only one step remains highly challenging, leading to unsatisfactory results in practice. To address this, [10] proposes stochastic multistep consistency sampling that iteratively predicts $\\pmb{x}_{t_{0}}$ using $\\scriptstyle f_{\\theta}$ and goes back to the intermediate points using the forward diffusion process. ", "page_idx": 2}, {"type": "text", "text": "The competitive performance of consistency models has stimulated their rapid adoption for text-toimage generation [52, 18, 53]. Nevertheless, we believe that CMs have not yet fully realized their potential in downstream applications, where DPMs excel. One of the reasons is that, unlike DPMs, CMs do not support the inversion process. This work aims to unlock this ability for CMs. ", "page_idx": 2}, {"type": "text", "text": "Dynamic guidance State-of-the-art text-toimage models employ large CFG scales to achieve high image quality and textual alignment. However, it often leads to the reduced diversity of generated images. To address this, dynamic classifier-free guidance [45, 46, 54] has recently been proposed to improve distribution diversity without noticeable loss in generative performance. CADS [45] gradually increases the guidance scale from zero to the initial high value over the sampling process, Figure 2a. Alternatively, [46] proposes deactivating the guidance for low and high noise levels and using it ", "page_idx": 2}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/e97b04009c87795d74d74802091a6eb4aac7e052c9d1cf7c5c9be61b3fe4ae5e.jpg", "img_caption": ["Figure 2: Dynamic CFG strategies. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "only on the middle time step interval, Figure 2b. Both strategies suggest that the unguided process at high noise levels is responsible for better distribution diversity without compromising sampling quality. In addition, the authors [46] demonstrate that guidance at low noise levels has a minor effect on the performance and can be omitted to avoid extra model evaluations for guidance calculation. Both dynamic techniques are controlled by two hyperparameters: $\\tau_{1}$ and $\\tau_{2}$ , which are responsible for the value of dynamic CFG $w(t)$ . In our work, we focus on the CADS formulation. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section introduces the invertible Consistency Distillation (iCD) framework, which comprises forward and reverse consistency models. First, we formulate the forward CD procedure that encodes images into latent noise. Then, we describe multi-boundary generalization of iCD to enable deterministic multistep inversion. Finally, we investigate the dynamic guidance technique from the inversion perspective. ", "page_idx": 3}, {"type": "text", "text": "3.1 Forward Consistency Distillation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Forward Consistency Distillation (fCD) works in the opposite way to CD. That is, it aims to map any point on the PF ODE trajectory to the latent noise (the last trajectory point). ", "page_idx": 3}, {"type": "text", "text": "The transition from CD to the forward counterpart is quite straightforward: the only thing that should be modified is the boundary condition. Precisely, the forward consistency model is constrained to be an identity function for the last trajectory point: $f_{\\theta}(x_{t_{N}},t_{N})=x_{t_{N}}$ . Thus, fCD inherits the same consistency distillation loss (2) without incurring extra training costs. This way, ", "page_idx": 3}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/5b50bd28bdffab2193256077e5e156cf19390b97f4e257b2d56779eba542e4df.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 3: The proposed invertible Consistency Distillation framework consists of two models: the forward $m$ -boundary model, $\\mathrm{fCD}^{m}$ , and the reverse model, $\\mathbf{CD}^{m}$ . (a) For $m\\,=\\,1$ , the reverse model corresponds to CD. More boundary points unlock the deterministic multistep inversion, e.g., (b) shows the case for $m=2$ . ", "page_idx": 3}, {"type": "text", "text": "the distilled model can transform any trajectory point to the last one: $\\pmb{f}_{\\theta}(\\pmb{x}_{t_{n}},t_{n})\\,=\\,\\pmb{x}_{t_{N}},\\forall\\;t_{n}$ . To perform inversion, first, fCD encodes an image into noise and then CD decodes it back. The comparison between CD and fCD is shown in Figure 3a. ", "page_idx": 3}, {"type": "text", "text": "3.2 Multi-boundary Consistency Distillation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In practice, the encoding with fCD faces two challenges. Firstly, like in CD, a single-step prediction with fCD can be highly inaccurate. However, this cannot be easily addressed since the multistep consistency sampling [10] is not applicable to fCD. Concretely, intermediate points cannot be obtained from the latent noise using the forward diffusion process. Secondly, even if fCD is accurate, the multistep sampling is not suitable for decoding, as its stochastic nature prevents the reconstruction of real images. So, to improve the prediction accuracy of fCD and reduce the reconstruction error of CD, it is necessary to formulate a deterministic multistep procedure for both models. ", "page_idx": 3}, {"type": "text", "text": "Recent approaches [53, 13] generalize the CD framework to a multistep regime and allow approximating arbitrary trajectory intervals in the reverse direction. However, these methods focus solely on the generation quality, without supporting the inversion. Thus, inspired by these works, we propose a multi-boundary CD, that unlocks deterministic multistep inversion with the distilled models and carries similar training costs as the classical CD methods. ", "page_idx": 3}, {"type": "text", "text": "Specifically, we divide the solution interval, $\\{t_{0},...,t_{N}\\}$ , into $m$ segments and perform the distillation on each of these segments separately. This way, we obtain a set of single-step consistency models operating on different intervals and boundary points. This formulation is valid for both CD and fCD and hence can enable deterministic multistep inversion. We provide an illustration of 2-boundary CD and fCD in Figure 3b. We denote the multi-boundary reverse and forward models as $\\mathbf{CD}^{m}$ and $\\mathrm{fCD}^{m}$ ", "page_idx": 3}, {"type": "text", "text": "Formally, we consider $\\mathbf{CD}^{m}$ and $\\mathrm{fCD}^{m}$ using the following parametrization, inspired by [53, 13]. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{x}_{s_{t}^{m}}=\\pmb{f}_{\\pmb{\\theta}}^{m}(\\pmb{x}_{t},t,s_{t}^{m},w)=\\mathrm{DDIM}(\\pmb{x}_{t},t,s_{t}^{m},w),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $s_{t}^{m}$ is the boundary time step depending on the number of boundaries, $m$ , and the current time step $t$ . For instance, let $m=1$ , then $\\bar{s}_{t}^{1}=t_{0}^{\\bar{}}$ for $\\mathrm{CD^{1}}$ and $s_{t}^{1}=t_{N}$ for $\\mathrm{fCD^{1}}$ . Note that we learn a single model, the multistep sampling is achieved by varying $s_{t}^{m}$ during inference. The training objective remains the same as (2), avoiding additional training costs compared to CD. The only limitation is that the number of segments and the corresponding boundary time steps must be set before the training. ", "page_idx": 3}, {"type": "text", "text": "3.3 Training $\\mathbf{fCD}^{m}$ and $\\mathbf{CD}^{m}$ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We train $\\mathrm{fCD}^{m}$ and $\\mathrm{CD}^{m}$ separately, initializing both with the same teacher model. We use the same loss with a difference only in boundary time steps. However, a notable difference is the CFG scale, $w$ . For ${\\mathbf{}}{\\mathbf{C}}{\\mathbf{D}}^{m}$ , we preliminarily embed the model on guidance, following [20], to use various $w$ during sampling and avoid extra model evaluations. For $\\mathbf{fCD}^{m}$ , we consider an unguided model with a constant $w=1$ . The reason is that the guided encoding $(w>1)$ ) leads to out-of-distribution latent noise [32], and as a result, to poor image reconstruction. We confirm this intuition in Section 3.4. Finally, we find that $m{=}3{-}4$ provides competitive generation and inversion quality for large-scale text-to-image models [4, 1]. ", "page_idx": 4}, {"type": "text", "text": "Preservation losses The procedure described above already provides decent inversion quality but still does not match the teacher inversion performance. To reduce the gap between them, we propose the forward and reverse preservation losses aimed at making $\\mathrm{CD}^{m}$ and $\\mathrm{fCD}^{m}$ more consistent with each other and improve the inversion accuracy. These losses can additionally be turned on during training. Below, we denote the parameters of $\\mathrm{CD}^{m}$ , $\\mathrm{fCD}^{m}$ as $\\theta^{+},\\theta^{-}$ , respectively. ", "page_idx": 4}, {"type": "text", "text": "The forward preservation loss modifies only $\\mathbf{fCD}^{m}$ and is described as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{f}}(\\pmb{\\theta}^{-},\\pmb{\\theta}^{+})=\\mathbb{E}\\left[d\\left(\\pmb{f}_{\\pmb{\\theta}^{-}}^{m}(\\pmb{f}_{\\pmb{\\theta}^{+}}^{m}(\\pmb{x}_{s_{t}^{m}})),\\pmb{x}_{s_{t}^{m}}\\right)\\right]\\rightarrow\\operatorname*{min}_{\\pmb{\\theta}^{-}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For simplicity, we omit some notation. In a nutshell, we sample a noisy image $\\pmb{x}_{s_{t}^{m}}$ for a boundary time step $s_{t}^{m}$ , then make a prediction using $\\mathrm{CD}^{m}$ and force $\\mathbf{fCD}^{m}$ to predict the same $\\pmb{x}_{s_{t}^{m}}$ . This approach encourages $\\mathrm{CD}^{m}$ and $\\mathrm{fCD}^{m}$ to be consistent with each other. ", "page_idx": 4}, {"type": "text", "text": "The reverse preservation loss provides the same intuition but with a difference in the optimized model $\\mathbf{\\nabla}\\mathbf{CD}^{m}$ instead of $\\mathrm{f}\\mathbf{C}\\mathbf{D}^{m}.$ ) and prediction sequence. That is, we first make a prediction using $\\mathbf{fCD}^{m}$ and then use $\\mathbf{CD}^{m}$ . We denote it as $\\mathcal{L}_{\\mathrm{r}}(\\pmb{\\theta}^{-},\\pmb{\\theta}^{+})$ . In our experiments, we calculate the preservation losses only for the unguided reverse process ( $w=1$ ). ", "page_idx": 4}, {"type": "text", "text": "Putting it all together We present our final pipeline for the case where $\\mathrm{fCD}^{m}$ and $\\mathrm{CD}^{m}$ are trained jointly starting from the pretrained diffusion model. However, it is possible to learn them by one or take an already pretrained consistency model and learn the rest one. The final objective consists of two consistency losses with the proposed multi-boundary modification and two preservation losses: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{iCD}}(\\boldsymbol{\\theta}^{+},\\boldsymbol{\\theta}^{-})=\\mathcal{L}_{\\mathrm{CD}}(\\boldsymbol{\\theta}^{+})+\\mathcal{L}_{\\mathrm{CD}}(\\boldsymbol{\\theta}^{-})+\\lambda_{\\mathrm{f}}\\mathcal{L}_{\\mathrm{f}}(\\boldsymbol{\\theta}^{-},\\boldsymbol{\\theta}^{+})+\\lambda_{\\mathrm{r}}\\mathcal{L}_{\\mathrm{r}}(\\boldsymbol{\\theta}^{+},\\boldsymbol{\\theta}^{-})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In this way, the proposed approach can compete with the state-of-the-art inversion methods using heavyweight diffusion models. We present technical details about the training in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "3.4 Dynamic Classifier-Free Guidance Facilitates Inversion ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As previously discussed, dynamic guidance [45, 46] provides promising results for both faithful and diverse text-to-image generation. In this work, we reveal that dynamic CFG is also an effective technique for improving inversion accuracy as shown in Figure 4b. Below, we delve into the questions when and why dynamic guidance might facilitate image inversion while preserving the generative performance. To answer these questions, we conduct experiments using Stable Diffusion 1.5 with DDIM solver for 50 steps and maximum CFG scale set to 8.0. ", "page_idx": 4}, {"type": "text", "text": "Dynamic guidance for decoding We start with the dynamic CFG analysis at the decoding stage using the unguided encoding process following the prior work [32]. First, we wonder at which time steps the guidance has the most significant impact on reconstruction quality. To this end, we evaluate MSE between real and reconstructed images for different CFG turn-on thresholds T. If $t>\\mathbf{T}$ , we set $w\\,=\\,1.0$ , otherwise, the CFG scale is set to its initial value 8.0. In Figure 4a, we observe an exponential decrease in reconstruction error, implying that the absence of CFG at higher noise levels is essential for achieving more accurate inversion. Figure 4b confirms this intuition qualitatively. These results are consistent with [45, 46], which also suggest turning off the guidance at high noise levels but motivating this from the perspective of diversity improvement. ", "page_idx": 4}, {"type": "text", "text": "Then, we investigate the influence of various $\\tau_{1},\\tau_{2}$ from the CADS dynamic (Figure 2a) on the inversion and generation performance. We aim to identify an operating point providing both strong generation performance and faithful image inversion. Thus, we evaluate generation performance using the ImageReward [55] (IR) on top of randomly generated samples for 1000 COCO2014 prompts [56]. The inversion accuracy is estimated in terms of MSE between original and reconstructed samples. Figure 5a presents the results for varying $\\tau_{1}$ and $\\tau_{2}$ . It can be seen that several points for $\\tau_{1}\\geq0.7$ offer slightly lower text-to-image performance but exhibit significantly better reconstruction quality compared to the constant CFG scale, 8.0. Moreover, we notice that the settings where $\\tau_{1}\\,=\\,\\tau_{2}$ perform similarly to those where $\\tau_{1}<\\tau_{2}$ . Consequently, in all our experiments, we consider a single $\\tau$ representing the case where $\\tau_{1}=\\tau_{2}$ and use $\\tau=0.7$ and $\\tau=0.8$ . This means that $\\mathrm{CD}^{m}$ follows unguided sampling for $t>\\tau$ and sets the initial CFG scale for $t\\leq\\tau$ . ", "page_idx": 4}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/19254f427fd6a903245b746b8d569e42a4151428739356cc2a580beb9875e39d.jpg", "img_caption": ["Figure 4: (a) Reconstruction error of the decoding process for different CFG turn-on thresholds. (b) Image inversion examples for different CFG turn-on thresholds T. Guidance at high noise levels $\\mathbf{T}=1.0)$ ) drastically degrades the inversion quality. "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/e9ce948322b8abea45f6d1de1a3dc1a3915ac9be028143ec515e3c09beb1b91a.jpg", "img_caption": ["Figure 5: (a) Trade-off between generation performance (IR) and reconstruction quality (MSE) provided by different $\\tau_{1},\\tau_{2}$ . (b) Generation examples for dynamic and constant CFG scales. The points around $\\tau_{1}=\\tau_{2}=0.8$ provide preferable trade-off between generation and inversion performance. ", "Table 1: FID-5k for SD1.5 starting from the noise latents obtained using different encoding strategies, and NLL for these latents. Though encoding with dynamic CFG produces consistently more plausible latents than constant CFG, the unguided encoding remains preferable. "], "img_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "b1XPHC7MQB/tmp/d12f80c1fb8de31f34e66ab061cc8d54ecc0cb6e517321290014a0aa2eef9564.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Note that the setting with $\\tau{=}\\tau_{1}{=}\\tau_{2}$ corresponds to a step CFG function $w(t)$ , which yields a distinct advantage for distilled models. The linearly changing CFG scales are not applicable to the processes with large discretization steps, typical for distilled diffusion models. Therefore, such a CFG schedule needs to be distilled into the model during training, making it less flexible for different generation and editing settings. In contrast, the step CFG function enables dynamic CFG for already pretrained distilled models, operating with different constant CFG scales. ", "page_idx": 5}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/448c227d3ce492d082d21ab87c855329e106c8489e880e6f305fdc9441abb4e7.jpg", "img_caption": ["Figure 6: Few examples of text-to-image generation using the iCD-XL model for 4 steps. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Dynamic guidance for encoding Next, we investigate the guidance role in tackling the encoding problem. In Table 1, we compare noise latents encoded using various guidance strategies. The quality of the noise latents is estimated by evaluating the generation performance starting from these latents with a fixed CFG scale of 8.0. As the performance measure, we calculate FID for 5000 image-text pairs from COCO [56]. ", "page_idx": 6}, {"type": "text", "text": "We observe that the latents obtained with a consistently high CFG scale exhibit the worst generative performance, indicating their out-of-domain nature. While dynamic guidance produces significantly more plausible latents, it still falls short of the unguided encoding in most cases. To further validate these results, we estimate the negative log-likelihood (NLL) of the encoded latents under different CFG settings in Table 1 (Bottom). NLL is calculated with respect to the standard normal distribution. While NLL decreases for dynamic CFG with lower $\\tau$ , the encoding without guidance $\\scriptstyle{w=1}$ ) provides the highest likelihood value. Therefore, in all our experiments, we maintain $w{=}1$ for the encoding and train the forward distilled models (fCD) on the unguided teacher process. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the following experiments, we apply our approach to text-to-image diffusion models of different scales: SD1.5 [57] and SDXL [1], and denote them iCD-SD1.5 and iCD-XL, respectively. We provide the training details in Appendix A. ", "page_idx": 6}, {"type": "text", "text": "Initially, we illustrate the inversion capability of the proposed framework. Then, we consider the text-guided image editing problem and demonstrate that our approach outperforms or is comparable to significantly more expensive baselines. ", "page_idx": 6}, {"type": "text", "text": "Before diving into the main experiments, we present a few generated samples using iCD-XL for 4 steps in Figure 6. Additional quantitative and qualitative results are provided in Appendix B. The results confirm that the distilled model demonstrates strong text-to-image generation performance. ", "page_idx": 6}, {"type": "text", "text": "4.1 Inversion quality of iCD ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Here, we analyze the reconstruction capabilities of iCD-SD1.5 under various configurations. Specifically, we explore the contribution of the different pipeline components, such as the number of steps, preservation losses, and dynamic CFG, to inversion performance. Our forward model is run without ", "page_idx": 6}, {"type": "table", "img_path": "b1XPHC7MQB/tmp/2f4a40d5a23a97844a4f048a8ce409af2e52112e67030be4f3687540af917ff9.jpg", "table_caption": ["Table 2: Exploration of iCD-SD1.5 configurations in terms of image inversion performance. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/c94e0f86b46d762a6807ec4139d6150ae2328322309bf6717b32e3559d8fd36c.jpg", "img_caption": ["Figure 7: Influence of the dynamic guidance and preservation losses on image inversion with iCD. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "CFG $w=1$ ), while for the reverse model, we consider two settings: unguided $w=1$ ) and guided $w=8$ ), both of which are important in practice. ", "page_idx": 7}, {"type": "text", "text": "Configuration To evaluate the inversion quality, we consider 5K images and the corresponding prompts from the MS-COCO dataset [56]. We measure the reconstruction quality using LPIPS [58], PSNR and cosine distance in the DinoV2 [59] feature space. As for the reference, the teacher inversion with a disabled CFG scale is considered. For the dynamic guidance, we use $\\tau=0.7$ . The coefficients for the preservation losses are equal to $\\lambda_{\\mathrm{f}}=1.5$ and $\\lambda_{\\mathrm{r}}=1.5$ . ", "page_idx": 7}, {"type": "text", "text": "Results The results are presented in Table 2. First, configurations (A-C) evaluate the number of the forward and inverse models inference steps. We observe that the reconstruction quality improves as the number of steps increases. In our main experiments, we consider 3 and 4 steps. ", "page_idx": 7}, {"type": "text", "text": "Then, (E-G) examine the preservation losses. In $(\\mathbf{E})$ , we learn the forward model in the encoder [60] regime using the forward preservation loss only. This experiment reveals that the consistency loss contributes significantly to inversion performance. $(\\mathbf{F},\\mathbf{G})$ show that both losses improve the inversion, with the latter approaching the quality of the teacher model. ", "page_idx": 7}, {"type": "text", "text": "Finally, we explore the dynamic CFG and preservation losses under the guided decoding setting (I-K) and compare them to the setting $(\\mathbf{H})$ , which does not employ any boosting techniques. From the configurations $(\\mathbf{I},\\mathbf{J},\\mathbf{K})$ , we can see that all techniques provide significant contribution to the reconstruction quality. In Figure 7, we visualize their influence on inversion. It can be seen that the dynamic CFG (I) is rather responsible for global object preservation, while the preservation losses (J,K) rather improve fine-grained details. We note that the final configuration $(\\mathbf{K})$ provides comparable inversion quality to the unguided process while preserving the editing capabilities due to the activated guidance. More visual examples of inversion and quantitative results are in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "4.2 Text-guided image editing ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we apply the proposed iCD to the text-guided image editing problem. For the SD1.5 model, we use the Prompt-to-Prompt (P2P) method [61]. We vary two hyperparameters: the cross-attention and self-attention steps balancing between editing strength and preservation of the reference image. We also apply our approach to MasaCTRL [62] in Appendix D. For the SDXL model, we follow the ReNoise [25] evaluation setting and just change the source prompt during decoding according to [63]. ", "page_idx": 7}, {"type": "text", "text": "Metrics We measure editing performance using both automatic metrics and human-study. The former uses two metrics: 1) to estimate the preservation of the reference image, we calculate the cosine distance between images in the DinoV2 feature space; 2) as an editing quality measure, we use the CLIP score between the edited image and the target prompt. For human evaluation, we employ professional assessors who successfully completed assessment tasks. We show them the source and target prompts, reference image and two images produced with the methods under the comparison and ask the question: which of the edited images do you prefer more taking into account the editing strength and reference preservation? ", "page_idx": 7}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/41b82b00661597d31affcc2459be4c92cdcfc29ef3b45dcd894e09db7a3df9fe.jpg", "img_caption": ["Figure 8: Image editing examples produced by our method (iCD-SD1.5) and the baseline approaches. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/3ed903c041d1e6f55af3bf93d90b268c8ef00b15e902557d944383dbe800535b.jpg", "img_caption": ["Figure 9: Quantitative comparisons between different editing approaches based on SD1.5: automatic metrics (left) and human preference study (right). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Benchmarks In our experiments, we consider two benchmarks: PieBench [31] and a manually created COCO evaluation set based on text-paired images from the MS-COCO dataset [56]. PieBench [31] consists of various types of editing, for example, replacement, addition, or deletion. We take 420 examples of realistic images, including all types of editing. COCO focuses solely on the replacement task as one of the most popular among practitioners. This benchmark contains 140 image-text pairs. ", "page_idx": 8}, {"type": "text", "text": "4.2.1 Text-guided image editing with iCD-SD1.5 ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Configuration To provide the editing with the SD1.5 model, we consider iCD using 4 forward and 4 reverse steps trained with both preservation losses $\\lambda_{\\mathrm{f}}=1.5$ , $\\lambda_{\\mathrm{r}}=1.5)$ ). In Appendix D, we also present the results for iCD using 3 steps, which is not much worse than the 4 step model. We set the hyperparameters of the dynamic CFG to $\\tau=0.8$ and maximum CFG scale to 19.0. ", "page_idx": 8}, {"type": "table", "img_path": "b1XPHC7MQB/tmp/e58e5c459cd1dc1e5859457289c9142b5edef23242fa308a7098b783dd7acbb5.jpg", "table_caption": [], "table_footnote": ["Table 3: Automatic metrics (top) and human evaluation (bottom) for iCD-XL and ReNoise [25]. "], "page_idx": 9}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/959d37504ee09175d3c7f192c13f4d06bbd61e6564a49fc5751d6da7fb37b01e.jpg", "img_caption": ["Figure 10: Editing examples using XL models. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Baselines We compare our approach with four baseline approaches, which provide state-of-the-art editing performance: Null-text Inversion (NTI), Negative-prompt Inversion (NPI) [32], InfEdit [26] and Edit-friendly DDPM [43]. All methods except InfEdit are diffusion-based approaches using more than 100 steps. Moreover, NTI employs the additional high-cost optimization procedure to improve inversion quality. InfEdit operates with the distilled diffusion model, latent consistency distillation [18], but utilizing virtual inversion. All methods use P2P and we vary all possible hyperparameter values to find the configurations that provide the best editing-preservation trade-off. ", "page_idx": 9}, {"type": "text", "text": "Results Figure 9 provides quantitative results for both benchmarks. We observe that the proposed iCD is comparable to the baseline approaches in most cases. Moreover, sometimes it can even outperform them while being multiple times faster. For instance, according to human preference on the COCO benchmark, our approach surpasses the InfEdit, NPI, NTI and Edit-friendly DDPM (50 steps). On the PieBench, it outperforms the InfEdit and Edit-friendly DDPM (50 steps). In addition, we provide qualitative results in Figure 8, which confirm the competitiveness of the proposed method. Additional visual examples can be found in Appendix D. ", "page_idx": 9}, {"type": "text", "text": "Notably, the performance of the proposed method is weaker on the PieBench benchmark compared to COCO, according to both automatic and human-based metrics. We attribute this to the increased complexity of editing tasks, which probably require more steps. ", "page_idx": 9}, {"type": "text", "text": "4.2.2 Text-guided image editing with iCD-XL ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Configuration We consider the configuration using 4 steps, $\\tau=0.7$ and CFG scale equals 8.0. ", "page_idx": 9}, {"type": "text", "text": "Baselines We compare our approach to the recently proposed ReNoise [25], accurately following its guidelines. This method works with both distilled models (LCM-SDXL [18], SDXL-Turbo [19]) and original diffusion model, SDXL [1]. However, even for the distilled models, a significant number of steps is required to achieve decent performance. ", "page_idx": 9}, {"type": "text", "text": "Results The quantitative and qualitative comparisons are presented in Table 3 and Figure 10, respectively. According to the human evaluation, iCD-XL outperforms all ReNoise configurations. Based on the automatic evaluation, our approach provides better reference preservation (DinoV2 and CLIP score, I) while maintaining strong editing capabilities, as indicated by the CLIP score (T). We provide more visual examples in Appendix D. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The paper proposes a generalized consistency distillation framework that enables both accurate image inversion and solid generation performance using a few inference steps. Accompanied by the recently proposed dynamic guidance, the distilled models demonstrate highly efficient and accurate image manipulations, making a significant step towards real-time text-driven image editing. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024.   \n[2] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024.   \n[3] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[4] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models, 2021.   \n[5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart- $\\alpha$ : Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023.   \n[6] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart- $\\sigma$ : Weak-to-strong training of diffusion transformer for 4k text-to-image generation, 2024.   \n[7] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18392\u201318402, 2023.   \n[8] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. arXiv preprint arXiv:2311.10089, 2023.   \n[9] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In ACM SIGGRAPH 2023 Conference Proceedings, pages 1\u201311, 2023.   \n[10] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023.   \n[11] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022.   \n[12] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. arXiv preprint arXiv:2310.14189, 2023.   \n[13] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023.   \n[14] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diffinstruct: A universal approach for transferring knowledge from pre-trained diffusion models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[15] David Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap, Shuangfei Zhai, Siyuan Hu, Daniel Zheng, Walter Talbott, and Eric Gu. Tract: Denoising diffusion models with transitive closure time-distillation, 2023.   \n[16] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022.   \n[17] Liangchen Li and Jiajun He. Bidirectional consistency models. arXiv preprint arXiv:2403.18035, 2024.   \n[18] Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolin\u00e1rio Passos, Longbo Huang, Jian Li, and Hang Zhao. Lcm-lora: A universal stable-diffusion acceleration module. arXiv preprint arXiv:2311.05556, 2023.   \n[19] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation, 2023.   \n[20] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14297\u201314306, 2023.   \n[21] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and Qiang Liu. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation. In International Conference on Learning Representations, 2024.   \n[22] Shanchuan Lin, Anran Wang, and Xiao Yang. Sdxl-lightning: Progressive adversarial diffusion distillation, 2024.   \n[23] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation, 2024.   \n[24] Tianwei Yin, Micha\u00ebl Gharbi, Richard Zhang, Eli Shechtman, Fr\u00e9do Durand, William T Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In CVPR, 2024.   \n[25] Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, and Daniel Cohen-Or. Renoise: Real image inversion through iterative noising, 2024.   \n[26] Sihan Xu, Yidong Huang, Jiayi Pan, Ziqiao Ma, and Joyce Chai. Inversion-free image editing with natural language. 2024.   \n[27] Barak Meiri, Dvir Samuel, Nir Darshan, Gal Chechik, Shai Avidan, and Rami Ben-Ari. Fixedpoint inversion for text-to-image diffusion models, 2023.   \n[28] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[29] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \n[30] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[31] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Pnp inversion: Boosting diffusion-based editing with 3 lines of code. International Conference on Learning Representations (ICLR), 2024.   \n[32] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6038\u20136047, 2023.   \n[33] Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki Tanaka. Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models, 2024.   \n[34] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2426\u20132435, 2022.   \n[35] Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang, Chongyang Ma, Weiming Dong, and Changsheng Xu. Inversion-based style transfer with diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10146\u201310156, 2023.   \n[36] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021.   \n[37] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transformations. arXiv preprint arXiv:2211.12446, 2022.   \n[38] Senmao Li, Joost van de Weijer, Taihang Hu, Fahad Shahbaz Khan, Qibin Hou, Yaxing Wang, and Jian Yang. Stylediffusion: Prompt-embedding inversion for text-based editing. arXiv preprint arXiv:2303.15649, 2023.   \n[39] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6007\u20136017, 2023.   \n[40] Wenkai Dong, Song Xue, Xiaoyue Duan, and Shumin Han. Prompt tuning inversion for textdriven image editing using diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7430\u20137440, 2023.   \n[41] Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki Tanaka. Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models. arXiv preprint arXiv:2305.16807, 2023.   \n[42] Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, Kunpeng Song, Mengwei Ren, Ruijiang Gao, Yuxiao Chen, Di Liu, Qilong Zhangli, et al. Improving negative-prompt inversion via proximal guidance. arXiv preprint arXiv:2306.05414, 2023.   \n[43] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly ddpm noise space: Inversion and manipulations. arXiv preprint arXiv:2304.06140, 2023.   \n[44] Manuel Brack, Felix Friedrich, Katharina Kornmeier, Linoy Tsaban, Patrick Schramowski, Kristian Kersting, and Apolinaros Passos. Ledits $^{++}$ : Limitless image editing using text-toimage models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.   \n[45] Seyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, and Romann M. Weber. CADS: Unleashing the diversity of diffusion models through condition-annealed sampling. In The Twelfth International Conference on Learning Representations, 2024.   \n[46] Tuomas Kynk\u00e4\u00e4nniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in a limited interval improves sample and distribution quality in diffusion models, 2024.   \n[47] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 8162\u20138171. PMLR, 2021.   \n[48] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35:26565\u201326577, 2022.   \n[49] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \n[50] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775\u20135787, 2022.   \n[51] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.   \n[52] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023.   \n[53] Jonathan Heek, Emiel Hoogeboom, and Tim Salimans. Multistep consistency models. arXiv preprint arXiv:2403.06807, 2024.   \n[54] Angela Castillo, Jonas Kohler, Juan C P\u00e9rez, Juan Pablo P\u00e9rez, Albert Pumarola, Bernard Ghanem, Pablo Arbel\u00e1ez, and Ali Thabet. Adaptive guidance: Training-free acceleration of conditional diffusion models. arXiv preprint arXiv:2312.12487, 2023.   \n[55] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation, 2023.   \n[56] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Doll\u00e1r. Microsoft coco: Common objects in context, 2015.   \n[57] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684\u201310695, June 2022.   \n[58] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018.   \n[59] Maxime Oquab, Timoth\u00e9e Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023.   \n[60] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding in style: A stylegan encoder for image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2287\u20132296, June 2021.   \n[61] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022.   \n[62] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22560\u2013 22570, 2023.   \n[63] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022.   \n[64] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/04e197ab7845a46ba9e9b5fde02ac75531643efab849c5949e24b214fe52681c.jpg", "img_caption": ["Figure 11: Training dynamics of iCD-SD1.5 in terms of FID (a) and reconstruction loss (b). "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "First, following [20], we preliminary distill classifier-free guidance using a conditional embedding added to the time step embedding. The goal is to use different $w$ and apply dynamic techniques during inference after consistency distillation. We perform CFG distillation for the following guidance scales: 1, 8, 12, 16, 20 for SD1.5 and 1, 4, 6, 9, 10, 12, 13, 16, 18, 20 for SDXL. At this stage, the model successfully approximates the guided teacher without hurting its performance. ", "page_idx": 14}, {"type": "text", "text": "Then, we perform multi-boundary consistency distillation using LoRA adapters with a rank of 64 following [52]. We train forward and reverse adapters in parallel starting from the same teacher: CFG distilled SD1.5 or SDXL. For the SD1.5 model, we use a global batch size of 512, and for the SDXL 128. All models converge relatively fast, requiring about 6K iterations with a learning rate $8e{-6}$ . ", "page_idx": 14}, {"type": "text", "text": "We find that the forward and reverse models provide promising generation and inversion quality for 3 or 4 steps. The regularization coefficients for the forward and reverse preservation losses are $\\lambda_{f}{=}1.5$ and $\\lambda_{r}{=}1.5$ , respectively. ", "page_idx": 14}, {"type": "text", "text": "The iCD-SD1.5 models are trained for ${\\sim}36\\mathrm{h}$ and the iCD-XL ones for ${\\sim}68\\mathrm{h}$ on 8 NVIDIA A100 GPUs. We present the training dynamics in terms of FID and reconstruction MSE for iCD-SD1.5 in Figure 11. ", "page_idx": 14}, {"type": "text", "text": "For SD1.5 distillation, we use a ${\\sim}20M$ subset of LAION 2B, roughly flitered using CLIP score [64]. For SDXL, we collect ${\\sim}7M$ images with resolution $\\geq1024$ , also curated to avoid poorly aligned text-image pairs and low quality images. ", "page_idx": 14}, {"type": "text", "text": "We set the following time steps for our configurations: ", "page_idx": 14}, {"type": "text", "text": "\u2022 4 steps, $\\tau=0.8$ : reverse model [259, 519, 779, 999]; forward model [19, 259, 519, 779];   \n\u2022 4 steps, $\\tau=0.7$ : reverse model [259, 519, 699, 999]; forward model [19, 259, 519, 699];   \n\u2022 3 steps $\\tau=0.7$ : reverse model [339, 699, 999]; forward model [19, 339, 699]; ", "page_idx": 14}, {"type": "text", "text": "B Image generation with iCD ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We provide the generation performance of our distilled model in Table 4. The dynamic CFG $\\tau=0.8$ , $\\tau=0.7$ ) degrades in terms of ImageReward, while improving FID due to the increased diversity [45, 46]. The visual examples are presented in Figures 16, 17. ", "page_idx": 14}, {"type": "text", "text": "C Image inversion ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Table 5, we provide comparisons between the 3- and 4-step configurations of iCD-SD1.5, which perform similarly. Figure 14 shows the image inversions provided by our approach, NTI and NPI. ", "page_idx": 14}, {"type": "table", "img_path": "b1XPHC7MQB/tmp/4eb809cfdf2445bf682be51bce01a909bcdab36333b13578ebdc3d5efee5daf3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 4: Text-to-image performance of the SD1.5 model in terms of FID-5K, CLIP score and ImageReward for $w=8$ using 5K prompts from the MS-COCO dataset. ", "page_idx": 15}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/a6a1acbe37a3c7b858ee4d1aae58074438440a64bf98b7b2d1e57f86b95678f7.jpg", "img_caption": ["Figure 12: Quantitative (a) and qualitative (b) editing results using 3- and 4-step iCD-SD1.5 configurations on PieBench. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/788ec20e8c0842fb82343964be88f4cbe0b7f6f89e7d239b720b0f33b2c53ea0.jpg", "img_caption": ["Figure 13: Comparison between Prompt2Prompt and MasaCTRL applied to our approach "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 15 shows the image inversions compared to the ReNoise method. In Table 6, we present the time required to invert a single image for different methods. ", "page_idx": 15}, {"type": "table", "img_path": "b1XPHC7MQB/tmp/f020e0c5a209fd802e9af2d1fa4c8b00079dc1dd7d5917489ecfe6dfc4ec6e04.jpg", "table_caption": [], "table_footnote": ["Table 5: More image inversion results for 3- and 4-step iCD-SD1.5. Dynamic CFG uses $\\tau=0.7$ . "], "page_idx": 16}, {"type": "table", "img_path": "b1XPHC7MQB/tmp/e000eb396a0260096f4a527038b06165bf0729937675d46192bb290e61a1f305.jpg", "table_caption": ["Table 6: The time required to invert a single image for different approaches. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "D Text-guided image editing ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Figure 12 compares two iCD configurations (3 and 4 steps). We observe that both configurations perform similarly, with a slight preference for the 4-step configuration. We present additional visual results on image editing using the iCD-SD1.5 model in Figure 18 and the iCD-XL model in Figure 19. ", "page_idx": 16}, {"type": "text", "text": "Figure 13 provides a few examples of our approach combined with MasaCTRL [62] for non-rigid editing. The results demonstrate that our method can be used with various editing methods. ", "page_idx": 16}, {"type": "text", "text": "We also present the comparison with SDXL-Turbo using SDEdit [63] in Figure 20 (upper). We observe that SDXL Turbo significantly hurts reference image preservation due to stochasticity. This highlights the importance of accurate image inversion for editing. Moreover, we compare our approach with Intruct-Pix2Pix [7], Figure 20 (bottom), and observe that it outperforms Intruct-Pix2Pix in terms of both content preservation and editing strength while being training-free. ", "page_idx": 16}, {"type": "text", "text": "We present the annotation interface for the assessors in Figure 22. We calculate the confidence interval using bootstrap methods, splitting the human votes into $1,000$ subsets and then averaging the results and calculating the standard deviation. ", "page_idx": 16}, {"type": "text", "text": "E Failure cases ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We present some inversion failure cases in Figure 21. Our method sometimes oversaturates images for high guidance scales and struggles to reconstruct complex details like human faces and hands ", "page_idx": 16}, {"type": "text", "text": "F Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The iCD limitations include the requirement to predetermine boundary time steps before distillation, which may restrict the model flexibility. Additionally, the extra preservation losses necessitate increased computational resources during training. Furthermore, the editing method is susceptible to hyperparameter values, leading to inconsistent performance across different prompts. Finally, the quality of the current distillation techniques itself requires significant improvement, as it does not consistently achieve high standards across various scenarios. ", "page_idx": 16}, {"type": "text", "text": "G Broader impacts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Our work can significantly enhance tools for artists, designers, and content creators, allowing for more precise and efficient manipulation of images based on textual inputs. This can democratize high-quality digital art creation, making it accessible to those without extensive technical skills. On the other hand, the ability to edit images easily and realistically can be misused to create misleading information or fake images, which can be particularly harmful and potentially influence public opinion and elections. ", "page_idx": 17}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/7caa9a3600ada12a305f8b6fa29f32092da3f88cfe38d52602776161233eeeb2.jpg", "img_caption": ["Figure 14: Inversion examples produced by the SD1.5-based models. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/badd95cdb881bc38882c2968a303ef83c454107a0a8a27923e1948205a2ce92c.jpg", "img_caption": ["Figure 15: Inversion examples produced by the SDXL-based models. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Distillation ", "page_idx": 20}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/5603efd90c2cdb32e777b36d80a2a2fe8278a8d5e8831597c68656049851ede2.jpg", "img_caption": ["Teacher ", "A grey bird in the middle of a snow-covered ground ", "Distillation ", "Distillation $\\scriptstyle\\mathtt{\\mathtt{T}=0.7}$ "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/0f352b9ccce31e04664300308dfaa72a5b797d3df9b6574eeaa8b3546ab2a9a3.jpg", "img_caption": ["A brown stuffed teddy bear sitting on top of a couch "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/918319e4dda9767441eae82d6ef92f1f89e5551d6f13066cc68d58b5c246d31b.jpg", "img_caption": ["An elephant at a zoo on a sunny day "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/9e3b752581d219e43ed89be6918b9524c30c4daa63534e79c13c941aa3ed830d.jpg", "img_caption": ["Man and woman sitting next to each other "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/0fc84ea954be000802c0841794a0677f53b069fe51e7a80f1aa24c0f4cd8d0ee.jpg", "img_caption": ["A panoramic view of a cabin with stone fireplace. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/58f0d535ce3e89cd69cbbef4c7ffdcfc648548c27141f2e3fd406b36f1e8d575.jpg", "img_caption": ["A plate with steak, gravy, baked apples, and broccoli. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 16: Generation examples using SD1.5, the proposed distilled method using 4 steps and dynamic CFG. ", "page_idx": 20}, {"type": "text", "text": "Distillation ", "page_idx": 21}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/b52f7cc15daa6ad9e5c2297226a1d53c61effa459f50d4bfac0b86a9472a2c02.jpg", "img_caption": ["Teacher ", "a barred owl peeking out from dense treebranches ", "Distillation "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/4eebf811c9ce5e16368004d1f7eb5e8aaf89560e08f51dc5db1a6cdde14389e0.jpg", "img_caption": ["a dutch baroque painting of a horse in a field of flowers "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/a891f0101243648c52f0bf40cdd93c67b16b5484ef01f74aa69d828e4936cfa8.jpg", "img_caption": ["a bloody mary cocktail next to a napkin "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/9e95732182c0ffa2bdc5c47f023f7a9b8d1dfe18ec2cf6e778b640ecf0d6f3ba.jpg", "img_caption": ["a cute illustration of a horned owl with a graduation cap and diploma "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/6a52970b99a45815bdf5da847c994e0f5badffd93744600aba2167f61390e90b.jpg", "img_caption": ["a peaceful lakeside landscape with migrating herd of sauropods "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/cb99acd1e73fbc9988341c4b305a1750274a93f44e97f293e3341d2db587ba51.jpg", "img_caption": ["A blue Ford F-150 coming around a curve in a mountain road "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 17: Generation examples using SDXL, the proposed distilled method using 4 steps and dynamic CFG. ", "page_idx": 21}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/97e1284ca59d73a04a69db21cc6a8430b473896eae2edd08e3f560edc56d0b61.jpg", "img_caption": ["Figure 18: Additional editing examples produced by the SD1.5-based models. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/717d4695f61e41b9c7942f629db5ff9480716aa709955f5f234b5ce059d811b7.jpg", "img_caption": ["Figure 19: Additional editing examples using the SDXL-based models. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/405a83e5d84644003d32fe9483fed9ae7c57485d0ed6199a5342b7598e7411d5.jpg", "img_caption": ["Figure 20: Image editing results produced by our method and SDXL-turbo using SDEdit (upper) and Instruct-P2P (bottom). "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/50b5a96a44e48d0b122a506dcd45b4ac004c0719b1a9e5ba11f77a1651ac40dd.jpg", "img_caption": ["Figure 21: Failure cases. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "b1XPHC7MQB/tmp/c7fbcaaf17c1305f0e190b03c2335f29fef1d6f6b96a9bb374d486a1bcb1072d.jpg", "img_caption": ["Figure 22: The human evaluation interface for the text-guided image editing problem. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The contributions mentioned in the abstract and introduction sections are carefully aligned with the methods and experimental sections. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: the limitations are discussed in the supplementary material. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not provide theoretical results. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We provide the technical details and include the code in the supplementary materials. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We attach the code in the supplementary material and will release it upon acceptance. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: To the best of our judgment, we provide the details sufficient to reproduce the results. However, we do not discuss some low-level implementation details that are not critical for understanding, and refer the readers to our code. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide the confidence intervals for the main experiments and and explain their nature in the supplementary material. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide the configurations in each section of the experiments. Moreover, we include additional information about the training in the supplementary material. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have carefully read the NeurIPS Code of Ethics and conducted our research in accordance with it. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We discuss potential positive societal impacts and negative societal impacts in the supplementary material. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We will provide guidelines for users upon acceptance of a paper to control the use of our models. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We plan to release our code and models upon acceptance under the CC-by 4.0 license. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: Human evaluation is performed by professional assessors. They are officially hired, paid competitive salary and informed about potential risks. We include the instruction in the supplementary material. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Before involving participants in the evaluation, we provide them with detailed instructions addressing potential risks. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]