[{"figure_path": "T1lFrYwtf7/tables/tables_6_1.jpg", "caption": "Table 2: Experimental results on datasets with synthetic documents. trained with n sents means that latent paraphrasers are trained with the dataset containing n sentences. For ours, we report the average performance of three runs. \u2020 denotes the method that uses 10 times more additional data (paraphrases).", "description": "This table presents the experimental results of different knowledge injection methods on three question answering datasets with synthetic documents.  The results compare standard fine-tuning,  noise-based methods (FreeLB and NEFTune), and the proposed LaPael method, both with and without additional data-level paraphrasing. The table shows the Exact Match (EM), Recall, and F1 scores for each method on each dataset.", "section": "5.1 Experimental Setting"}, {"figure_path": "T1lFrYwtf7/tables/tables_6_2.jpg", "caption": "Table 2: Experimental results on datasets with synthetic documents. trained with n sents means that latent paraphrasers are trained with the dataset containing n sentences. For ours, we report the average performance of three runs. \u2020 denotes the method that uses 10 times more additional data (paraphrases).", "description": "This table presents the experimental results obtained using synthetic datasets for evaluating the knowledge injection performance of different methods.  The table compares the performance of several methods, including standard fine-tuning, noise-based methods like FreeLB and NEFTune, and the proposed LaPael method.  Results are shown for Exact Match (EM), Recall, and F1 scores on three different question-answering datasets (SQUAD-syn, StreamingQA-syn, and ArchivalQA-syn). The impact of training the latent paraphrasers with different numbers of sentences is also investigated.  A comparison is made with a fine-tuning approach that also incorporates data-level paraphrasing. ", "section": "5.1 Experimental Setting"}, {"figure_path": "T1lFrYwtf7/tables/tables_7_1.jpg", "caption": "Table 3: Experimental results on datasets with raw documents. For Ours, we use the latent paraphraser used in the SQUAD-syn experiment. Rec. denotes recall.", "description": "This table presents the experimental results obtained from using raw documents instead of synthetic documents. The results are shown for different question answering datasets (SQUAD-raw, StreamingQA-raw, Films 2024-raw, and Events 2024-raw).  The performance of various methods is compared: No Injection (no knowledge injection), Fine-Tuning (standard fine-tuning), FreeLB [57], NEFTune [16], and the proposed method (Ours). The metrics used to evaluate performance are Exact Match (EM), Recall (Rec.), and F1 score. The \"Ours\" method uses the latent paraphraser trained on the SQUAD-syn dataset (synthetic).", "section": "5.2 Experimental Results"}, {"figure_path": "T1lFrYwtf7/tables/tables_7_2.jpg", "caption": "Table 4: Experimental results on cross-domain transfer experiments. For ours, (X \u2192) denotes that latent paraphrasers are trained on Dtrain from the X dataset. Rec. denotes recall.", "description": "This table presents the results of cross-domain transfer experiments, showing how well the model generalizes across different datasets. The model trains latent paraphrasers on one dataset (source domain, indicated by X) and then fine-tunes the LLM on another dataset (target domain).  The table compares the performance of LaPael against several baseline methods in terms of Exact Match (EM), Recall (Rec.), and F1 scores for four different datasets: SQUAD-syn, StreamingQA-syn, MedMCQA-syn, and NovelQA-syn. The results demonstrate the ability of LaPael to successfully transfer knowledge learned from one domain to another, even for datasets with quite different characteristics.", "section": "5.1 Experimental Setting"}, {"figure_path": "T1lFrYwtf7/tables/tables_9_1.jpg", "caption": "Table 2: Experimental results on datasets with synthetic documents. trained with n sents means that latent paraphrasers are trained with the dataset containing n sentences. For ours, we report the average performance of three runs. \u2020 denotes the method that uses 10 times more additional data (paraphrases).", "description": "This table presents the experimental results obtained using synthetic datasets for evaluating the performance of different knowledge injection methods.  The table compares the performance of several methods: No Injection (baseline), Fine-Tuning (standard fine-tuning), Fine-Tuning (seq.) (fine-tuning with paraphrased sequences), FreeLB, NEFTune, and the proposed method (Ours).  Results are shown for three datasets (SQUAD-syn, StreamingQA-syn, ArchivalQA-syn) using different evaluation metrics (EM, Recall, F1).  The \"Ours\" results indicate the average performance over three runs, while the \"\u2020\" symbol indicates methods using significantly more paraphrased data.  The number of sentences used to train the latent paraphrasers is also specified for the \"Ours\" method.", "section": "5.1 Experimental Setting"}, {"figure_path": "T1lFrYwtf7/tables/tables_16_1.jpg", "caption": "Table 10: Comparison to Retrieval Augmented Generation (RAG) on Events 2024-raw.", "description": "This table compares the performance of the proposed LaPael method against a Retrieval Augmented Generation (RAG) method and baseline fine-tuning methods on the Events 2024-raw dataset.  It shows the Exact Match (EM), Recall (Rec.), and F1 scores for each method, highlighting the relative strengths and weaknesses of different knowledge injection techniques.", "section": "5.2 Experimental Results"}, {"figure_path": "T1lFrYwtf7/tables/tables_17_1.jpg", "caption": "Table 11: Dataset statistics. We report the size of  Dtrain, DK, and DQA used in our experiments.", "description": "This table presents the sizes of three different datasets used in the experiments.  Dtrain is the training dataset for the latent paraphrasers. Dk contains the knowledge to be injected into the language model. DQA is the question-answering dataset used for evaluating the knowledge injection performance. The table shows the number of documents or question-answer pairs in each dataset for both synthetic and raw document settings across different question answering benchmarks (SQUAD, StreamingQA, ArchivalQA, NovelQA, and MedMCQA).", "section": "5.1 Experimental Setting"}, {"figure_path": "T1lFrYwtf7/tables/tables_20_1.jpg", "caption": "Table 2: Experimental results on datasets with synthetic documents. trained with n sents means that latent paraphrasers are trained with the dataset containing n sentences. For ours, we report the average performance of three runs. \u2020 denotes the method that uses 10 times more additional data (paraphrases).", "description": "This table presents the experimental results of different knowledge injection methods on three question answering datasets (SQUAD-syn, StreamingQA-syn, ArchivalQA-syn) with synthetic documents.  The results compare standard fine-tuning, several noise-based baselines (FreeLB and NEFTune), and the proposed Latent Paraphrasing (LaPael) method. The table shows the Exact Match (EM), Recall, and F1 scores for each method.  The effect of training the latent paraphrasers on different numbers of sentences (50 or 1000) is also evaluated.", "section": "5.1 Experimental Setting"}, {"figure_path": "T1lFrYwtf7/tables/tables_21_1.jpg", "caption": "Table 2: Experimental results on datasets with synthetic documents. trained with n sents means that latent paraphrasers are trained with the dataset containing n sentences. For ours, we report the average performance of three runs. \u2020 denotes the method that uses 10 times more additional data (paraphrases).", "description": "This table presents the experimental results obtained using synthetic datasets for evaluating the performance of different knowledge injection methods.  The results are broken down by dataset (SQUAD-syn, StreamingQA-syn, ArchivalQA-syn), metric (EM, Recall, F1), and method (No Injection, Fine-Tuning, FreeLB, NEFTune, Ours). The \"Ours\" method refers to the proposed approach of latent paraphrasing, which is tested with different numbers of training sentences to show its sensitivity to training data size. The table aims to compare the effectiveness of the proposed approach with existing state-of-the-art methods and standard fine-tuning techniques in scenarios where the training data is augmented.", "section": "5.1 Experimental Setting"}]