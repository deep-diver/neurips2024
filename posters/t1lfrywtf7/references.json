{"references": [{"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-06", "reason": "This paper is foundational to the field of large language models (LLMs) and their ability to perform various tasks with minimal fine-tuning, providing critical background for the current paper\u2019s work on knowledge injection."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-04", "reason": "This paper introduced the transformer architecture, a fundamental building block of many modern LLMs, including those used in the current paper, making it essential context for the work."}, {"fullname_first_author": "Oded Ovadia", "paper_title": "Fine-tuning or retrieval? Comparing knowledge injection in LLMs", "publication_date": "2023-12-21", "reason": "This directly addresses knowledge injection in LLMs, a core topic of this paper, providing a relevant comparison to the authors' proposed LaPael approach."}, {"fullname_first_author": "Suchin Gururangan", "paper_title": "Don't stop pretraining: Adapt language models to domains and tasks", "publication_date": "2020-07-05", "reason": "This paper explores adapting pre-trained language models to specific domains and tasks, a related area to the current paper\u2019s focus on efficient knowledge injection."}, {"fullname_first_author": "Jason W. Wei", "paper_title": "EDA: easy data augmentation techniques for boosting performance on text classification tasks", "publication_date": "2019-11-03", "reason": "This paper presents data augmentation techniques for improving the performance of text classification models, offering a comparison point for data augmentation methods in the context of knowledge injection."}]}