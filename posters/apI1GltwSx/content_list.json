[{"type": "text", "text": "DELT: A Simple Diversity-driven EarlyLate Training for Dataset Distillation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAfilfiation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Recent advances in dataset distillation have led to solutions in two main directions.   \n2 The conventional batch-to-batch matching mechanism is ideal for small-scale   \n3 datasets and includes bi-level optimization methods on models and syntheses,   \n4 such as FRePo, RCIG, and RaT-BPTT, as well as other methods like distribution   \n5 matching, gradient matching, and weight trajectory matching. Conversely, batch-to  \n6 global matching typifies decoupled methods, which are particularly advantageous   \n7 for large-scale datasets. This approach has garnered substantial interest within the   \n8 community, as seen in $\\mathrm{SRe}^{2}\\mathrm{L}$ , G-VBSM, WMDD, and CDA. A primary challenge   \n9 with the second approach is the lack of diversity among syntheses within each   \n10 class since samples are optimized independently and the same global supervision   \n11 signals are reused across different synthetic images. In this study, we propose a   \n12 new EarlyLate training scheme to enhance the diversity of images in batch-to  \n13 global matching with less computation. Our approach is conceptually simple yet   \n4 effective, it partitions predefined IPC samples into smaller subtasks and employs   \n15 local optimizations to distill each subset into distributions from distinct phases,   \n16 reducing the uniformity induced by the unified optimization process. These distilled   \n17 images from the subtasks demonstrate effective generalization when applied to   \n18 the entire task. We conducted extensive experiments on CIFAR, Tiny-ImageNet,   \n19 ImageNet-1K, and its sub-datasets. Our empirical results demonstrate that the   \n20 proposed approach significantly improves over previous state-of-the-art methods   \n21 under various $\\mathrm{IPC}s^{1}$ . ", "page_idx": 0}, {"type": "text", "text": "22 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "23 In the era of large models and large datasets, dataset   \n24 distillation has emerged as a crucial strategy to en  \n25 hance training efifciency and make AI technologies   \n26 more accessible and affordable for the general public.   \n27 Previous approaches [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]   \n28 primarily employ a batch-to-batch matching tech  \n29 nique, where information like features, gradients,   \n30 and trajectories from a local original data batch are   \n31 used to supervise and train a corresponding batch   \n32 of generated data. This method\u2019s strength lies in its   \n33 ability to capture fine-grained information from the   \n34 original data, as each batch\u2019s supervision signals vary.   \n35 However, the downside is the necessity to repeatedly   \n36 input both original and generated data for each training iteration, which significantly increases memory   \n37 usage and computational costs. Recently, a new decoupled method [11, 12, 13] has been proposed   \n38 to separate the model training and data synthesis, also it leverages the batch-to-global matching to   \n39 avoid inputting original data during distilled data generation. This solution has demonstrated great   \n40 advantage on large-scale datasets like ImageNet-1K [11, 14] and ImageNet-21K [12]. However, as   \n41 shown in Fig. 2 right subfigure, a significant limitation of this method is its strategy of synthesizing   \n42 each data point individually, where supervision is repetitively applied across various synthetic images.   \n43 For instance, $\\mathrm{SRe}^{2}\\mathrm{L}[11]$ utilizes globally-counted layer-wise running means and variances from the   \n44 pre-trained model for supervising different intra-class image synthesis. This methodology results in a   \n45 pronounced lack of diversity within the same category of generated images.   \n46 To address this issue, previous studies such as G-VBSM [14] and RDED [15] have been conducted.   \n47 Specifically, G-VBSM [14] introduces a framework that utilizes a diverse set of local-match-global   \n48 matching signals derived from multiple backbones and statistical metrics, offering more precise and   \n49 effective matching than the singular model. However, as the diversity of matching models grows, the   \n50 overall complexity of the framework also increases, thus diminishing its conciseness. RDED [15]   \n51 crops each original image into multiple patches and ranks these using realism scores generated by an   \n52 observer model. Then it amalgamates every four chosen patches from previous stage into a single new   \n53 image, maintaining the resolution of the original images, and produce IPC-numbered distilled images   \n54 for each class. While RDED is effective for selecting and combining data, it does not enhance or   \n55 optimize the visual content within the distilled dataset. Thus, the diversity and richness of information   \n56 it encapsulates largely dependent on the distribution of the original dataset.   \n57 Our solution, termed the EarlyLate training scheme, is straightforward and also orthogonal to   \n58 these prior methods: by initializing each image in the same category at a different starting point for   \n59 optimization, we ensure that the final optimized results vary across images. We also use teacher-ranked   \n60 real image patches to initialize the synthetic images. This prevents some images from being short  \n61 optimized and ensures they provide sufifcient information. As shown in Fig. 1 of the computation   \n62 comparison, our approach not only enhances intra-class diversity but also significantly reduces the   \n63 computational load of the training process. Specifically, while conventional training requires $T$   \n64 optimization iterations per image or batch, in our EarlyLate scheme, the first image undergoes $T_{1}$   \n65 iterations (where $T_{1}=T$ ). Subsequent batches are processed with progressively fewer iterations, such   \n66 as $T_{2}$ $(T_{2}=T_{1}-\\mathrm{RI}^{2})$ ) for the next set, and so forth. The iterations for the final batch are reduced   \n67 to RI which is $1/j$ of the standard count (where typically $j=4$ or 8), meaning the total number of   \n68 optimization iterations required is just about $2/3$ of prior batch-to-global matching methods, such as   \n69 $\\mathrm{SRe}^{2}\\mathrm{L}$ and CDA. We further visualize the average cosine similarity between each sample of 50 IPCs   \n70 with the associated cluster centroid within the same class on ImageNet-1K, as shown in Fig. 2 left   \n71 subfigure, DELT shows significantly better diversity than other counterpart methods across all classes.   \n72 We perform extensive experiments on datasets of CIFAR-10, Tiny-ImageNet, ImageNet-1K and its   \n73 subsets. On ImageNet-1K, our proposed approach achieves $66.1\\%$ under IPC 50 with ResNet-101,   \n74 outperforming previous state-of-the-art RDED by $4.9\\%$ . On small-scale datasets of CIFAR-10, our   \n75 approach also obtains $2.5\\%$ and $19.2\\%$ improvement over RDED and $\\mathrm{SRe}^{2}\\mathrm{L}$ using ResNet-101. ", "page_idx": 0}, {"type": "image", "img_path": "apI1GltwSx/tmp/e97f4eeb79b758530993c64c864c7bc64db0676fa12d16b05f07b9a67002c768.jpg", "img_caption": ["Figure 1: Distill datasets to $\\mathrm{IPC}_{N}$ requires $N{*}T$ iterations in traditional distillation processes "], "img_footnote": [], "page_idx": 0}, {"type": "image", "img_path": "apI1GltwSx/tmp/6b50cb8951dd517151489048e35d12bd8ff50f3a8327baefd6edeee22b7350b2.jpg", "img_caption": ["Figure 2: Left: Intra-class semantic cosine similarity after a pretrained ResNet-18 model on ImageNet1K dataset, lower values are better. Right: Synthetic images from $\\mathrm{SRe}^{2}\\mathrm{L}$ , CDA and our DELT. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "76 Our main contributions in this work are as follows: ", "page_idx": 1}, {"type": "text", "text": "77 \u2022 We propose a simple yet effective EarlyLate training scheme for dataset distillation to   \n78 enhance the intra-class diversity of synthetic images from batch-to-global matching.   \n79 \u2022 We demonstrate empirically that the proposed method can generate optimized images at   \n80 different distances from their initializations, to enlarge informativeness among generations.   \n81 \u2022 We conducted extensive experiments and ablations on various datasets across different scales   \n82 to prove the effectiveness of the proposed approach3. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "83 2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "84 Dataset Distillation. Dataset distillation or condensation [1] focuses on creating a compact yet   \n85 representative subset from a large original dataset. This enables more efifcient model training while   \n86 maintaining the ability to evaluate on the original test data distribution and achieve satisfactory   \n87 performance. Previous works [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] mainly designed how to better match   \n88 the distribution between original data and generated data in a batch-to-batch manner, such as the   \n89 distribution of features [6], gradients [2], or the model weight trajectories [4, 8]. The primary   \n90 optimization method used is bi-level optimization [16, 17], which involves optimizing model   \n91 parameters and updating images simultaneously. For instance, using gradient matching, the process   \n92 can be formulated as to minimize the gradient distance: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{S\\in\\mathbb{R}^{N\\times d}}D\\left(\\nabla_{\\theta}\\ell(S;\\theta),\\nabla_{\\theta}\\ell(\\mathcal{T};\\theta)\\right)=D(S,\\mathcal{T};\\theta)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "93 where the function $D(\\cdot,\\cdot)$ is defined as a distance metric such as MSE [18], $\\theta$ denotes the model   \n94 parameters, and $\\nabla_{\\theta}\\ell(\\cdot;\\theta)$ represents the gradient, utilizing either the original dataset $\\mathcal{T}$ or its synthetic   \n95 version $s.\\ N$ is the number of $d$ -dimensional synthetic data. During distillation, the synthetic dataset   \n96 $s$ and model $\\theta$ are updated alternatively, ", "page_idx": 2}, {"type": "equation", "text": "$$\nS\\gets S-\\lambda\\nabla_{S}D(S,\\mathcal{T};\\theta),\\quad\\theta\\gets\\theta-\\eta\\nabla_{\\theta}\\ell(\\theta;S),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "97 where $\\lambda$ and $\\eta$ are learning rates designated for $s$ and $\\theta$ , respectively. ", "page_idx": 2}, {"type": "text", "text": "98 Batch-to-global matching used in [11, 14, 12, 13] tracks the distribution of BN statistics derived from   \n99 the original dataset for the local batch synthetic data, the formulation can be: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{S\\in\\mathbb{R}^{N\\times d}}(\\sum_{l}\\left\\|\\mu_{l}(S)-\\mathbf{B}\\mathbf{N}_{l}^{\\mathrm{{RM}}}\\right\\|_{2}+\\sum_{l}\\left\\|\\sigma_{l}^{2}(S)-\\mathbf{B}\\mathbf{N}_{l}^{\\mathrm{{RV}}}\\right\\|_{2})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "100 where $l$ is the index of BN layer, $\\mu_{l}(S)$ and $\\sigma_{l}^{2}(S)$ are mean and variance. $\\mathbf{B}\\mathbf{N}_{l}^{\\mathrm{RM}}$ and $\\mathbf{B}\\mathbf{N}_{l}^{\\mathrm{RV}}$ are   \n101 running mean and running variance in the pre-trained model at $l$ -th layer, which are globally counted.   \n102 Fig. 3 illustrates the difference of batch-to-batch and batch-to-global matching mechanisms, where $^b$   \n103 represents a local batch in data $\\mathcal{T}$ and $s$ . ", "page_idx": 2}, {"type": "image", "img_path": "apI1GltwSx/tmp/f6166759c61125ee2dbcccb7a30483f414c7c7cb53cbf222d409ebb8403f5313.jpg", "img_caption": ["116 Figure 3: Batch\u2013to-batch vs. batch-to-global 117 matching in dataset distillation. $\\theta_{f}$ indicates 118 weights are pretrained and frozen in this stage. 119 "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Moreover, for the recent advances of multi-stage dataset distillation methods, MDC [10] proposes to compress multiple condensation processes into a single one by including an adaptive subset loss on top of the basic condensation loss, so that to obtain datasets with multiple sizes. PDD [9] generates multiple small batches of synthetic images, each batch is conditioned on the accumulated data from previous batches. Unlike PDD, our current synthetic batch is independent with different operation iterations and not relevant to any previous batches. D3 [19] partitions large datasets into smaller subtasks and employs locally trained experts to distill each subset into distributions. These distilled distributions from the subtasks demonstrate effective generalization when applied to the entire task. ", "page_idx": 2}, {"type": "text", "text": "120 Initialization. Weight initialization [20, 21, 22, 23] is pivotal in training neural networks, significantly   \n121 influencing their optimization process. Proper initialization is essential for ensuring model convergence   \n122 and mitigating issues such as gradient vanishing. Recently, weight selection [24] introduces a strategy   \n123 for initializing smaller models by selecting a subset of weights from a pretrained larger model. This   \n124 method facilitates the transfer of learned attributes from the pretrained weights, enhancing the smaller   \n125 model\u2019s performance. Weight subcloning [25] involves manipulating the pretrained model to derive a   \n126 correspondingly scaled-down version with equivalent initialization. This involves two main steps:   \n127 initially, it applies a neuron importance ranking to reduce the embedding dimension per layer within   \n128 the pretrained model. Subsequently, it eliminates blocks from the transformer model to align with the   \n129 layer count of the scaled-down network.   \n130 This work focuses on data initialization for generation processes. Few studies have examined this   \n131 angle. While, PCA-K [26] appears to be the most relevant. It employs an initialization method that   \n132 involves drawing samples from a distribution that accurately mirrors and is easily sampled from the   \n133 training distribution. During training, it is possible to retrieve some details from the original image   \n134 using the initial noisy sample, which at best provides a blurred representation of the original image. ", "page_idx": 2}, {"type": "image", "img_path": "apI1GltwSx/tmp/ad1001f43c779868b5c80b1a38c9bdca50d6a8f9e850810d75a4e31156011d26.jpg", "img_caption": ["Figure 4: The proposed DELT learning procedure via a multi-round EarlyLate scheme. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "135 3 Our Approach ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "136 Preliminaries. The objective of a regular dataset distillation task is to generate a compact synthetic   \n137 dataset $\\boldsymbol{S}=\\{(\\hat{\\pmb{x}}_{1},\\hat{\\pmb{y}}_{1})\\,,\\dots,\\hat{(\\pmb{x}}_{|S|},\\hat{\\pmb{y}}_{|\\bar{S}|})\\}$ as a student dataset that captures a substantial amount of   \n138 the information from a larger labeled dataset $\\mathcal{T}=\\left\\{\\left(\\pmb{x}_{1},\\pmb{y}_{1}\\right),\\dots,\\left(\\pmb{x}_{|\\mathcal{T}|},\\pmb{y}_{|\\mathcal{T}|}\\right)\\right\\}$ , which serves as the   \n139 teacher dataset. Here, $\\hat{\\mathbf{y}}$ represents the soft label for the synthetic sample $\\hat{x}$ , and the size of $s$ is much   \n140 smaller than $\\mathcal{T}$ , yet it retains the essential information of the original dataset $\\mathcal{T}$ . The learning goal   \n141 using this distilled dataset is to train a post-validation model with parameters $\\pmb{\\theta}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\theta_{S}=\\arg\\operatorname*{min}_{\\theta}\\mathcal{L}_{S}(\\pmb{\\theta}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "142 ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{S}(\\pmb{\\theta})=\\mathbb{E}_{(\\hat{x},\\hat{y})\\in S}\\left[\\ell(\\phi_{\\theta_{S}}(\\hat{x}),\\hat{y};\\pmb{\\theta})\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "143 where $\\ell$ is a standard loss function such as soft cross-entropy and $\\phi_{\\theta_{S}}$ represents the model. ", "page_idx": 3}, {"type": "text", "text": "144 The primary aim of dataset distillation is to produce synthetic data that ensures minimal performance   \n145 difference between models trained on the synthetic dataset $s$ and those trained on the original dataset   \n146 $\\mathcal{T}$ using validation data $V$ . The optimization procedure for generating $s$ is given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\underset{S,|S|}{\\arg\\operatorname*{min}}\\left(\\operatorname*{sup}_{\\left\\{\\left|\\ell\\left(\\phi_{\\theta_{\\mathcal{T}}}\\left(x_{\\nu a l}\\right),\\,y_{\\nu a l}\\right)-\\ell\\left(\\phi_{\\theta_{S}}\\left(x_{\\nu a l}\\right),\\,y_{\\nu a l}\\right)\\right|\\right\\}_{\\left(x_{\\nu a l},y_{\\nu a l}\\right)\\sim V}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "147 where $({\\pmb x}_{\\nu a l},{\\pmb y}_{\\nu a l})$ are the sample and label pairs in the validation set of the real dataset $\\mathcal{T}$ . The   \n148 learning task then focuses on the <data, label> pairs within $s$ , maintaining a balanced representation   \n149 of distilled data across each class.   \n150 Initialization. Previous dataset distillation methods [11, 14,   \n151 12] on large-scale datasets like ImageNet-1K and 21K employ   \n152 Gaussian noise by default for data initialization in the synthesis   \n153 phase. However, Gaussian noise is random and lacks any   \n154 semantic information. Intuitively, using real images provide a   \n155 more meaningful and structured starting point, and this struc  \n156 tured start can lead to quicker convergence during optimization   \n157 because the initial data already contains useful features and   \n158 patterns that are closer to the target distribution, which further   \n159 enhances realism, quality, and generalization of the synthesized images. As shown in Fig. 2 right   \n160 subfigure, our generated images exhibit both diversity and a high degree of realism in some cases.   \n161 Selection Criteria. Here, we introduce how to select real image patches to initialize the synthetic   \n162 images. In our final syntheses, a significant fraction of our data has been subject to limited optimization   \n163 iterations, making effective initialization crucial. A proper initialization also dramatically minimizes   \n164 the overall computational load required for the updating on data. Prior approach [15] has demonstrated   \n165 that choosing representative data patches from the original dataset without training can yield favorable   \n166 performance without any additional training. Our observation, however, underscores that applying   \n167 iterative refinement to original patches can lead to markedly improved results. As illustrated in Fig. 1,   \n168 our selection criterion is based on a pretrained teacher model as a ranker, we calculate all patches\u2019   \n169 probabilities and sort them as the initialization pool. Then, we choose lowest, medium, or highest   \n170 probability patches as the initialization for our optimization.   \n171 Diversity-driven IPC Concatenation Training. As shown in Fig. 4, to further emphasize diversity   \n172 and avoid potential distribution bias from initialization, we optimize the initialized images starting   \n173 from different points. The motivation behind this design is that different data samples require varying   \n174 numbers of iterations to converge which is similar to the early stopping idea [27]. Importantly, as   \n175 images become easier to predict with more updates by class labels, training primarily on easy data   \n176 points can hinder model generalization. Therefore, our method enhances generalization by generating   \n177 data samples with varying dififculty levels, acting as a regularizer by limiting the optimization process   \n178 to a smaller volume of image pixel space. Previous work [28] studies how to perform early stopping   \n179 training on different layers\u2019 weights of the model with progressive retraining to mitigate noisy labels.   \n180 We are pioneering to study how to leverage early-late training when optimizing data. Moreover, we   \n181 improve the efifciency of our approach by performing gradient updates in a single scan. Initially, we   \n182 conduct a single gradient loop, continually introducing new data for distillation by concatenating them   \n183 at different time stamps. Consequently, the $M$ batch receives the synthetic images of all preceding   \n184 batches, $I P C_{0:M k-1}$ , as final generations. This process can be simplified as follows: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "apI1GltwSx/tmp/96fa21c90dc4c26a89cf151e4e206b4901678b8134d8ed4511049f0aacec0e75.jpg", "img_caption": ["Figure 5: Selection criteria with a teach ranker. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{I P C_{0:M k-1}=\\underbrace{\\bigl[\\underbrace{\\hat{\\mathbf{x}}_{0},\\hat{\\mathbf{x}}_{1},\\dots,\\hat{\\mathbf{x}}_{k-1}}_{I P C_{0:k-1}},\\dots,\\hat{\\mathbf{x}}_{M k-1}\\bigr]}_{\\dots}}\\\\ {I P C_{0:M k-1}\\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "185 where $[\\hat{\\pmb{x}}_{0},\\hat{\\pmb{x}}_{1},\\dots,\\hat{\\pmb{x}}_{M k-1}]$ refers to the concatenation of the generated image. \ud835\udc40is the number   \n186 of batches, $k$ is the number of generated images in each batch. We train these different batches at   \n187 different starting points, each batch goes through a completed learning phase, but the total number of   \n188 iterations varies. Then, the multiple IPCs of $\\hat{x}$ are concatenated into a simple batch. Because of its   \n189 early-late training property, we refer to this simple training scheme as EarlyLate training.   \n190 Training Procedure. As illustrated in Fig. 4, our learning procedure is extremely simple using an   \n191 incremental learning process: We split the total IPCs to be learned into multiple batches. The training   \n192 begins with the first batch. Following a predefined number of iterations, the second batch commences   \n193 its iterative training, and this process continues sequentially with subsequent batches. Batch-to-global   \n194 matching algorithm [12] of Eq. 3 has been utilized between each round. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "195 4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "196 4.1 Datasets and Results Details ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "197 We first run DELT on five standard benchmark tests including CIFAR-10 (10 classes) [29], Tiny  \n198 ImageNet (200 classes) [30], ImageNet-1K (1,000 classes) [31] and it variants of ImageNette (10   \n199 classes) [32], and ImageNet-100 (100 classes) [33] with performances reported in Table 1 and Table 2.   \n200 The evaluation protocol is following prior works [15, 11]. We compare DELT to six baseline dataset   \n201 distillation algorithms including Matching Training Trajectories (MTT) [4], Improved Distribution   \n202 Matching (IDM) [34], TrajEctory Matching with Constant Memory (TESLA) [8], Squeeze-Recover  \n203 Relabel $\\mathrm{(SRe^{2}L)}$ [11], Dififculty-Aligned Trajectory-Matching (DATM) [35], Realistic-Diverse  \n204 Efifcient Dataset Distillation (RDED) [15]. Following previous dataset distillation methods [2, 15,   \n205 11], we use ConvNet [36], ResNet-18/ResNet-101 [37], EfifcientNet-B0 [38], MobileNet-V2 [39],   \n206 MnasNet1_3 [40], and RegNet-Y-8GF [41], as our backbone for training or post-validation. All our   \n207 experiments are conducted on 4 NVIDIA RTX 4090 GPUs. ", "page_idx": 4}, {"type": "table", "img_path": "apI1GltwSx/tmp/eb56c1b6052d9301bc627cf5b8c999b0fe3f9f286fa2e798f79b2306ee560edb.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison with SOTA dataset distillation methods using relatively large-scale backbones on five benchmarks across different scales. MobileNet-v2 is modified to match the low resolutions of CIFAR-10 and Tiny-ImageNet following [42]. Due to the table space limitation, some other methods that are weaker than RDED are not listed, such as CDA and G-VBSM. Since IPC 1 is not applicable to use EarlyLate strategy and the single image in each class is optimized with a constant iteration. "], "page_idx": 5}, {"type": "table", "img_path": "apI1GltwSx/tmp/0e11f466439ae879743c39ca65efd55c6f03dac90b3ec30fedbb3f424b72a7da.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Table 2: Comparison with SOTA dataset distillation methods using small-scale backbone architecture on four benchmark datasets. Following [4, 34, 15], Conv-3 is used for CIFAR-10, Conv-4 for TinyImageNet and ImageNet-1K, Conv-5 for ImageNette, and Conv-6 for ImageNet-100 and ImageNet-1K. Entries marked with \u201c-\u201d are missing due to scalability issue. ", "page_idx": 5}, {"type": "text", "text": "208 As shown in Table 1, our approach establishes the new state-of-the-art accuracy in 13 out of 15 of   \n209 the configurations on five datasets from small-scale CIFAR-10 to large-scale ImageNet-1K using   \n210 relatively large backbone architecture of ResNet-101, in many cases with significant margins of   \n211 improvement. The results using small-scale architecture ConvNet are shown in Table 2, our approach   \n212 also achieves the state-of-the-art accuracy in 8 out of 12 of the configurations on four datasets. ", "page_idx": 5}, {"type": "text", "text": "213 4.2 Cross-architecture generalization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "214 An important characteristic of distilled datasets is their effectiveness in generalizing to novel training   \n215 architectures. In this context, we assess the transferability of DELT\u2019s distilled datasets tailored for   \n216 ImageNet-1K with 10 images per class. Following previous studies [11, 15], we test our models   \n217 using five distinct architectures: ResNet-18 [37], MobileNet-V2 [39], MnasNet1_3 [40], EfifcientNet  \n218 B0 [38], and RegNet-Y-8GF [41]. As shown in Table 4, our proposed approach demonstrates   \n219 significant better performance than other competitive methods on all these architectures. ", "page_idx": 5}, {"type": "text", "text": "220 4.3 Ablation Study ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "221 Mosaic splicing pattern. Mosaic stitching method [43] in RDED selects four crops from the train set   \n222 as the optimal hyper-parameter, and puts the contents of the four crops into a synthetic image that is   \n223 directly used for post-validation. In this work, considering that we use different dififculty levels of   \n224 selection for initialization, we examine different strategies of the Mosaic splicing patterns, including   \n225 $1\\times1$ , $2\\times2$ , $3\\times3$ , $4\\times4$ , and $5\\times5$ patches, as illustrated in Fig. 11. The ablation results are shown in   \n226 Table 3, it can be observed that $1\\times1$ achieves the best accuracy.   \n227 Initialization. We examine how different initialization strategies affect final performance, including:   \n228 choosing lowest probability crops, medium probability crops and highest probability crops. Our results   \n229 are shown in Table 3. Overall, the performance gap between different strategies is not significant, and   \n230 selecting the medium probability crops as the initialization achieves the best accuracy.   \n231 Optimization iterations. We examine two types of optimization iterations: maximum iteration   \n232 (MI) for the earliest batch training and round iteration (RI). MI presents the number of optimization   \n233 iterations that the earliest batch goes through. RI represents the number of iterations used for each   \n234 round in Fig. 4. It essentially indicates the iteration gap between the optimization of two adjacent   \n235 batches. As shown in Table 3, we test MI values of 1K, 2K, and 4K, using 500 and 1K iterations for   \n236 each RI. Note that when MI is set to 1K, it is not feasible to use 1K as RI. The results show that 4K   \n237 (same as [11, 12]) MI and 500 RI achieves the best accuracy.   \n238 Early-only vs. EarlyLate. Early-only is equivalent to using constant MI to optimize each image. The   \n239 method will transform to baseline batch-to-global matching of CDA [12] $^+$ real image initialization.   \n240 Our results in Table 3 clearly show that the EarlyLate training bring a significant improvement on   \n241 final performance. More importantly, this strategy is the key factor in enhancing generation diversity.   \n242 Real image stitching vs. Minimax diffusion vs. Ours. We further compare the performance of our   \n243 approach with real image stitching [15] and diffusion generation [44]. The results are presented in   \n244 Table 3d. While the first two methods produce more realistic images, each image contains limited   \n245 information. In contrast, our method achieves the best final performance. ", "page_idx": 5}, {"type": "image", "img_path": "apI1GltwSx/tmp/0d5af00d4f6acec33917c5ef6dbf67672eddd456fc43a8d964e2de085918df4c.jpg", "img_caption": ["Figure 6: Mosaic splicing patterns on ImageNet-1K using real image patches as the initialization. In each block, the left column is the starting real image initialized samples and right is the final optimized syntheses. From top to bottom are images generated by early training and late training. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "246 4.4 Computational Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "247 For image optimization-based methods like $S\\mathrm{Re}^{2}\\mathrm{L}$ and CDA, the total computational cost is calculated   \n248 as $N\\times T$ , where $N$ is the MI. In our EarlyLate scheme, the first batch images undergo $T_{1}$ iterations   \n249 (where $T_{1}=T$ ). Subsequent batches are processed with progressively fewer iterations, such as $T_{2}$   \n250 $(T_{2}=T_{1}-\\mathrm{RI})$ for the next set, and so forth. The iterations for the final batch are reduced to RI which   \n251 iits e $1/j$ oonf st hree qsutairnedda rids $\\begin{array}{r}{N\\times T-\\frac{j(j-1)}{2}\\mathbf{R}\\mathbf{I}}\\end{array}$ $j=4$ owr h8i cihn  iosu rr oaubglahtliyo $2/3$ hoef t optrailo rn ubmatbcehr- toof- goluor boapl timmaitzcahtiinogn   \n253 methods. Our real time consumptions for data generation are shown in Table 5, note that the smaller   \n254 the dataset like CIFAR, the more time is spent on loading and processing the data, rather than training. ", "page_idx": 6}, {"type": "text", "text": "255 4.5 Visualization of DELT ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "256 Fig. 7 illustrates a comprehensive visual comparison between randomly selected synthetic images from   \n257 our distilled dataset and those from the real image patches [15], MinimaxDiffusion [44], MTT [4],   \n258 IDC [45], $\\mathrm{SRe}^{2}\\mathrm{L}$ [11], SCDD [46], CDA [12] and G-VBSM [14] distilled data. It can be observed that ", "page_idx": 6}, {"type": "text", "text": "Table 3: Ablation experiments on various aspects of our framework with ResNet-18 on ImageNet-1K. ", "page_idx": 7}, {"type": "table", "img_path": "apI1GltwSx/tmp/4ef9e0d5855a84b000c90220d4643eb0bc787c903abcf8483497ff891cbb7116.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "apI1GltwSx/tmp/889d0abe99646c54bee0ea355e597ab8555a92af36e1c5f0bf72aa40f8532337.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "apI1GltwSx/tmp/d35321479a0a984bf893e444093cd33e9012deacba825335c6dc70e87ed9294c.jpg", "table_caption": [], "table_footnote": ["(a) Number of patches. Ablation on initializing different numbers of scoring patches. Results are from ResNet-18 on ImageNet-1K for 500 iterations to synthesize 50 IPCs. "], "page_idx": 7}, {"type": "table", "img_path": "apI1GltwSx/tmp/71e4938471dce877defe374c686387f787c01eb8c545aea6cf75fd8434ea5ea7.jpg", "table_caption": ["(b) Selection criteria. Initializing $1\\times1$ images selected according to teacher model\u2019s probability "], "table_footnote": ["(c) Round Iterations. Top-1 acc. of our method for IPC(e) Comparison with real and diffusion generated data. 10 using different round iterations with ResNet-18. "], "page_idx": 7}, {"type": "table", "img_path": "apI1GltwSx/tmp/ce8f4686c7161c8332e597301712f986d596e60f2403cf564cd532210e3e2665.jpg", "table_caption": ["Table 4: Cross-architecture generalization. Results are evaluated on IPC 10. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "apI1GltwSx/tmp/07c341c899a57310198127c028cd383dd870341cbef40f52a1ce093f9236ccd6.jpg", "table_caption": ["Table 5: Actual computational consumption and analysis (hours under IPC 50) in data synthesis with image optimization-based methods on a single NVIDIA 4090 GPU. \u201cRI\u201d represents round iterations. A total 4K iterations are used for all methods and datasets to ensure fair comparisons. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "259 the images generated by each method have their own characteristics. MinimaxDiffusion leverages the   \n260 diffusion model to synthesize images which is close to the real ones. However, as in our above ablation,   \n261 both real and diffusion-generated data are inferior to ours. MTT results show noticeable artifacts   \n262 and distortions, the objects in all images are located in the middle of the generations, the diversity is   \n263 limited. IDC results also show distorted and less recognizable dog images, but diversity is increased.   \n264 $\\mathrm{SRe}^{2}\\mathrm{L}$ exhibits some dog features but with significant distortions and similar simple background.   \n265 SCDD shows more recognizable dog features but still the color is simple and monochromatic, the   \n266 same situation happens in CDA. G-VBSM shows more colorful patterns, possibly due to recovery   \n267 from multiple different networks, but all generations are in the same pattern and the diversity is   \n268 not large. Our approach\u2019s synthetic images exhibit a higher degree of diversity, including both   \n269 compressed distorted images from long-optimized initializations and clear, recognizable dog images   \n270 from short-optimized initializations, a unique capability not present in other methods. ", "page_idx": 7}, {"type": "text", "text": "271 4.6 Application I: Data-free Network Pruning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "272 Our distilled dataset acts as a multifunctional training tool and boosts the adaptability for diverse   \n273 downstream applications. We validate its utility in the scenario of data-free network pruning [47].   \n274 Table 6 shows the applicability of our dataset in this task when pruning $50\\%$ weights, where it   \n275 significantly surpasses previous methods such as $\\mathrm{SRe}^{2}\\mathrm{L}$ and RDED under IPC 10 and 50. ", "page_idx": 7}, {"type": "image", "img_path": "apI1GltwSx/tmp/b7cfbc2e5bd8ee572392a7e3badf34563bfdc8c6f7093fb1c7caed073cfd9cf5.jpg", "img_caption": ["Real M-Diffusion MTT SRe2L SCDD CDA G-VBSM Ours Figure 7: Distilled dataset visualization compared with other image optimization-based methods. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 6: Accuracy of data-free network pruning using slimming [48] on VGG11-BN [49]. ", "page_idx": 8}, {"type": "table", "img_path": "apI1GltwSx/tmp/8c5afdf66e6c64274699ec0b6c21cb630af4fb470a134cb9e7a808806c256189.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "276 4.7 Application II: Continual Learning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "277 We examine the effectiveness of DELT generated images   \n278 in the continual learning scenario. Following the setup in   \n279 prior studies [11, 6], we perform 100-step class-incremental   \n280 experiments on ImageNet-1K, comparing our results with   \n281 the baselines G-VBSM and $\\mathrm{SRe}^{2}\\dot{\\mathrm{L}}$ . As shown in Fig. 8,   \n282 our DELT distilled dataset significantly outperforms G  \n283 VBSM, with an average improvement of about $10\\%$ in   \n284 100-step class-incremental learning task. This highlights   \n285 the significant beneftis of deploying DELT, particularly in   \n286 mitigating the challenges of continual learning. ", "page_idx": 8}, {"type": "image", "img_path": "apI1GltwSx/tmp/1b17f6fd3e17c3ba04f2884adc2e55ff798afd66ad9ec90de99bdc90dc47cf91.jpg", "img_caption": ["Figure 8: Continual learning results. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "287 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "288 We have introduced a new training strategy, EarlyLate, to improve image diversity in batch-to-global   \n289 matching scenarios for dataset distillation. The proposed approach organizes predefined IPC samples   \n290 into smaller, manageable subtasks and utilizes local optimizations. This strategy helps in refining   \n291 each subset into distributions characteristic of different phases, thereby mitigating the homogeneity   \n292 typically caused by a singular optimization process. The images refined through this method exhibit   \n293 robust generalization across the entire task. We have extensively evaluated this approach on CIFAR-10   \n294 and 100, Tiny-ImageNet, ImageNet-1K, and its variants. Our empirical findings indicate that our   \n295 approach significantly outperforms prior state-of-the-art methods across various IPC configurations.   \n296 Limitations. Our method effectively avoids the issue of insufifcient data diversity generated by   \n297 batch-to-global methods and reduces the computational cost of the generation process. However,   \n298 there is still a performance gap when training the model on our generated data compared to training   \n299 on the original dataset. Additionally, our short-optimized data exhibits similar semantic information   \n300 to the original images, which may potentially leak the privacy of the original dataset. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "301 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "302 [1] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. Dataset distillation. arXiv preprint   \n303 arXiv:1811.10959, 2018.   \n304 [2] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with gradient matching. arXiv   \n305 preprint arXiv:2006.05929, 2020.   \n306 [3] Yongchao Zhou, Ehsan Nezhadarya, and Jimmy Ba. Dataset distillation using neural feature regression.   \n307 Advances in Neural Information Processing Systems, 35:9813\u20139827, 2022.   \n308 [4] George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A Efros, and Jun-Yan Zhu. Dataset   \n309 distillation by matching training trajectories. In Proceedings of the IEEE/CVF Conference on Computer   \n310 Vision and Pattern Recognition, pages 4750\u20134759, 2022.   \n311 [5] Saehyung Lee, Sanghyuk Chun, Sangwon Jung, Sangdoo Yun, and Sungroh Yoon. Dataset condensation   \n312 with contrastive signals. In International Conference on Machine Learning, pages 12352\u201312364. PMLR,   \n313 2022.   \n314 [6] Bo Zhao and Hakan Bilen. Dataset condensation with distribution matching. In IEEE/CVF Winter   \n315 Conference on Applications of Computer Vision, WACV 2023, Waikoloa, HI, USA, January 2-7, 2023,   \n316 2023.   \n317 [7] Songhua Liu, Kai Wang, Xingyi Yang, Jingwen Ye, and Xinchao Wang. Dataset distillation via factorization.   \n318 Advances in Neural Information Processing Systems, 35:1100\u20131113, 2022.   \n319 [8] Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh. Scaling up dataset distillation to imagenet-1k with   \n320 constant memory. In International Conference on Machine Learning, pages 6565\u20136590. PMLR, 2023.   \n321 [9] Xuxi Chen, Yu Yang, Zhangyang Wang, and Baharan Mirzasoleiman. Data distillation can be like   \n322 vodka: Distilling more times for better quality. In The Twelfth International Conference on Learning   \n323 Representations, 2024.   \n324 [10] Yang He, Lingao Xiao, Joey Tianyi Zhou, and Ivor Tsang. Multisize dataset condensation. ICLR, 2024.   \n325 [11] Zeyuan Yin, Eric Xing, and Zhiqiang Shen. Squeeze, recover and relabel: Dataset condensation at imagenet   \n326 scale from a new perspective. In NeurIPS, 2023.   \n327 [12] Zeyuan Yin and Zhiqiang Shen. Dataset distillation in large data era. arXiv preprint arXiv:2311.18838,   \n328 2023.   \n329 [13] Haoyang Liu, Tiancheng Xing, Luwei Li, Vibhu Dalal, Jingrui He, and Haohan Wang. Dataset distillation   \n330 via the wasserstein metric. arXiv preprint arXiv:2311.18531, 2023.   \n331 [14] Shitong Shao, Zeyuan Yin, Muxin Zhou, Xindong Zhang, and Zhiqiang Shen. Generalized large-scale data   \n332 condensation via various backbone and statistical matching. In CVPR, 2024.   \n333 [15] Peng Sun, Bei Shi, Daiwei Yu, and Tao Lin. On the diversity and realism of distilled dataset: An efifcient   \n334 dataset distillation paradigm. In CVPR, 2024.   \n335 [16] Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, and Zhouchen Lin. Investigating bi-level optimization   \n336 for learning and vision from a unified perspective: A survey and beyond. IEEE Transactions on Pattern   \n337 Analysis and Machine Intelligence, 44(12):10045\u201310067, 2021.   \n338 [17] Yihua Zhang, Prashant Khanduri, Ioannis Tsaknakis, Yuguang Yao, Mingyi Hong, and S\u0133ia Liu. An   \n339 introduction to bi-level optimization: Foundations and applications in signal processing and machine   \n340 learning. arXiv preprint arXiv:2308.00788, 2023.   \n341 [18] Zhou Wang and Alan C Bovik. Mean squared error: Love it or leave it? a new look at signal fidelity   \n342 measures. IEEE signal processing magazine, 26(1):98\u2013117, 2009.   \n343 [19] Tian Qin, Zhiwei Deng, and David Alvarez-Melis. Distributional dataset distillation with subtask   \n344 decomposition. arXiv preprint arXiv:2403.00999, 2024.   \n345 [20] Xavier Glorot and Yoshua Bengio. Understanding the dififculty of training deep feedforward neural   \n346 networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics,   \n347 pages 249\u2013256. JMLR Workshop and Conference Proceedings, 2010.   \n348 [21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing   \n349 human-level performance on imagenet classification. In Proceedings of the IEEE international conference   \n350 on computer vision, pages 1026\u20131034, 2015.   \n351 [22] Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint arXiv:1511.06422, 2015.   \n352 [23] Trieu H Trinh, Minh-Thang Luong, and Quoc V Le. Selfie: Self-supervised pretraining for image   \n353 embedding. arXiv preprint arXiv:1906.02940, 2019.   \n354 [24] Zhiqiu Xu, Yanjie Chen, Kirill Vishniakov, Yida Yin, Zhiqiang Shen, Trevor Darrell, Lingjie Liu, and   \n355 Zhuang Liu. Initializing models with larger ones. In The Twelfth International Conference on Learning   \n356 Representations, 2024.   \n357 [25] Mohammad Samragh, Mehrdad Farajtabar, Sachin Mehta, Raviteja Vemulapalli, Fartash Faghri, Devang   \n358 Naik, Oncel Tuzel, and Mohammad Rastegari. Weight subcloning: direct initialization of transformers   \n359 using larger pretrained ones. arXiv preprint arXiv:2312.09299, 2023.   \n360 [26] Jeffrey Zhang, Shao-Yu Chang, Kedan Li, and David Forsyth. Preserving image properties through   \n361 initializations in diffusion models. In Proceedings of the IEEE/CVF Winter Conference on Applications of   \n362 Computer Vision, pages 5242\u20135250, 2024.   \n363 [27] Lutz Prechelt. Early stopping-but when? In Neural Networks: Tricks of the trade, pages 55\u201369. Springer,   \n364 2002.   \n365 [28] Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, and Tongliang Liu.   \n366 Understanding and improving early stopping for learning with noisy labels. Advances in Neural Information   \n367 Processing Systems, 34:24392\u201324403, 2021.   \n368 [29] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n369 [30] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.   \n370 [31] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical   \n371 image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255.   \n372 Ieee, 2009.   \n373 [32] Fastai. Fastai/imagenette: A smaller subset of 10 easily classified classes from imagenet, and a little more   \n374 french.   \n375 [33] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In Computer Vision\u2013ECCV   \n376 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XI 16, pages   \n377 776\u2013794. Springer, 2020.   \n378 [34] Ganlong Zhao, Guanbin Li, Yipeng Qin, and Yizhou Yu. Improved distribution matching for dataset   \n379 condensation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,   \n380 pages 7856\u20137865, 2023.   \n381 [35] Ziyao Guo, Kai Wang, George Cazenavette, Hui Li, Kaipeng Zhang, and Yang You. Towards lossless   \n382 dataset distillation via dififculty-aligned trajectory matching. In The Twelfth International Conference on   \n383 Learning Representations, 2024.   \n384 [36] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In Proceedings   \n385 of the IEEE conference on computer vision and pattern recognition, pages 4367\u20134375, 2018.   \n386 [37] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.   \n387 In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n388 [38] Mingxing Tan and Quoc Le. Efifcientnet: Rethinking model scaling for convolutional neural networks. In   \n389 International conference on machine learning, pages 6105\u20136114. PMLR, 2019.   \n390 [39] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2:   \n391 Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and   \n392 pattern recognition, pages 4510\u20134520, 2018.   \n393 [40] Mingxing Tan, Bo Chen, Ruoming Pang, V\u0133ay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V   \n394 Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE/CVF   \n395 conference on computer vision and pattern recognition, pages 2820\u20132828, 2019.   \n396 [41] Il\u0133a Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Designing network   \n397 design spaces. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,   \n398 pages 10428\u201310436, 2020.   \n399 [42] Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and Jiajun Liang. Decoupled knowledge distillation. In   \n400 Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 11953\u201311962,   \n401 2022.   \n402 [43] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed and accuracy   \n403 of object detection. arXiv preprint arXiv:2004.10934, 2020.   \n404 [44] Jianyang Gu, Saeed Vahidian, Vyacheslav Kungurtsev, Haonan Wang, Wei Jiang, Yang You, and Yiran   \n405 Chen. Efifcient dataset distillation via minimax diffusion. In CVPR, 2024.   \n406 [45] Jang-Hyun Kim, Jinuk Kim, Seong Joon Oh, Sangdoo Yun, Hwanjun Song, Joonhyun Jeong, Jung-Woo   \n407 Ha, and Hyun Oh Song. Dataset condensation via efifcient synthetic-data parameterization. In Proceedings   \n408 of the 39th International Conference on Machine Learning, 2022.   \n409 [46] Muxin Zhou, Zeyuan Yin, Shitong Shao, and Zhiqiang Shen. Self-supervised dataset distillation: A good   \n410 compression is all you need. arXiv preprint arXiv:2404.07976, 2024.   \n411 [47] Suraj Srinivas and R Venkatesh Babu. Data-free parameter pruning for deep neural networks. arXiv   \n412 preprint arXiv:1507.06149, 2015.   \n413 [48] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning   \n414 efifcient convolutional networks through network slimming. In Proceedings of the IEEE international   \n415 conference on computer vision, pages 2736\u20132744, 2017.   \n416 [49] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image   \n417 recognition. arXiv preprint arXiv:1409.1556, 2014.   \n418 [50] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,   \n419 Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep   \n420 learning library. Advances in neural information processing systems, 32, 2019.   \n421 [51] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data   \n422 augmentation with a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision   \n423 and pattern recognition workshops, pages 702\u2013703, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "424 Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "425 A Broader Impacts ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "426 Our dataset distillation framework can significantly reduce the computational resources required for   \n427 training machine learning models. This leads to lower energy consumption and cost, making AI   \n428 more accessible and sustainable. By generating smaller, more manageable datasets, researchers and   \n429 developers can iterate and experiment more quickly, accelerating the pace of innovation in various   \n430 AI applications. However, condensed datasets might inadvertently amplify biases present in the   \n431 original data. If the distillation process does not adequately address bias, it could lead to unfair   \n432 or discriminatory AI systems. Also, simplifying datasets may lead to a loss of important nuances   \n433 and context, potentially degrading the performance of models in real-world applications where such   \n434 details are crucial. Moreover, the models may overfti to condensed data, indicating that models trained   \n435 on distilled datasets might perform well on the condensed data but poorly on more diverse real-world   \n436 data, limiting their generalizability and robustness. ", "page_idx": 12}, {"type": "text", "text": "437 B Training Details ", "text_level": 1, "page_idx": 12}, {"type": "table", "img_path": "apI1GltwSx/tmp/05f08de984590bae132da7505859e61e1655ef2fd568d89e20c9de8505c6a682.jpg", "table_caption": ["Table 7: Hyper-parameter settings. "], "table_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "apI1GltwSx/tmp/2a50ef8e9e86d87f366eb5286b398bf7a954d7795e48c9e9dd4fc590091d350a.jpg", "table_caption": ["(c) Dataset-specific settings in recovery "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "438 For reproducibility, we provide all our hyper-parameter settings used in our experiments in Table 7,   \n439 we outline such details below.   \n440 Squeezing and Pre-trained models. Following the previous works [11, 12, 15], we use the ofifcial   \n441 PyTorch [50] pre-trained ResNet-18 model for ImageNet-1K, and we use the same ofifcial Torchvision   \n442 [50] code to produce our pre-trained models, ResNet-18 and ConvNet, for the other datasets.   \n443 Ranking. A crucial part of our method is initialization, we simply use ResNet-18 pre-trained models   \n444 to rank and select the top-medium images as initialization for all our datasets, except for ImageNet-100   \n445 where we simply extracted the top-medium images based on the rankings of the original ImageNet-1K.   \n446 Recovery. For our synthesis, we provide the details of the general hyper-parameters used for different   \n447 datasets, including ImageNet-1K, ImageNet-100, ImageNette, Tiny-ImageNet, and CIFAR10, in   \n448 Table 7b. Because synthesizing a single image per class, i.e., IPC 1, is quite special as we cannot use   \n449 rounds, we apply different numbers of iterations based on both the dataset scale and the validation   \n450 teacher model as outlined in Table 7c.   \n451 Validation. This includes both the soft-label generation, Relabel in $\\mathrm{SRe}^{2}\\mathrm{L}$ , and evaluation, or   \n452 post-training. We outline such details in Table 7a. We use timm\u2019s version of RandAugment [51] with   \n453 different settings depending on the synthesized dataset being validated as outlined in Table 7c. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "image", "img_path": "apI1GltwSx/tmp/5e80d192cde29bab5a1494f177a3c6f49b061699de948e439c38a17d3619aee0.jpg", "img_caption": ["Figure 9: Synthetic image visualizations on Tiny-ImageNet generated by our DELT. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "454 C More Visualizations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "455 We provide more visualizations on synthetic Tiny-ImageNet, ImageNette and CIFAR-10 datasets. In   \n456 each figure, each column represents a different class, with images progressing from long optimization   \n457 at the top to short optimization at the bottom. ", "page_idx": 13}, {"type": "image", "img_path": "apI1GltwSx/tmp/37a1dc385c443f6b7aa664f243e8145e9470e077026315a6a0d0cd7a20b2901c.jpg", "img_caption": ["Figure 10: Synthetic image visualizations on ImageNette generated by our DELT. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "apI1GltwSx/tmp/d2ee78fcb7132b07863b5cdd4824ea9b81ebcc24d100a65a5e239b4f6ee2ffce.jpg", "img_caption": ["Figure 11: Synthetic image visualizations on CIFAR-10 generated by our DELT. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "458 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 16}, {"type": "text", "text": "6 Answer: [Yes] Justification: The limitations have been discussed in the conclusion section.   \n8 Guidelines:   \n9 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n0 the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n2 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n3 violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors   \n5 should reflect on how these assumptions might be violated in practice and what the implications would be. \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n8 only tested on a few datasets or with a few runs. In general, empirical results often 9 depend on implicit assumptions, which should be articulated.   \n0 \u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution   \n2 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n3 used reliably to provide closed captions for online lectures because it fails to handle   \n4 technical jargon.   \n5 \u2022 The authors should discuss the computational efifciency of the proposed algorithms   \n6 and how they scale with dataset size. \u2022 If applicable, the authors should discuss possible limitations of their approach to address   \n8 problems of privacy and fairness.   \n9 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n0 reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n2 judgment and recognize that individual actions in favor of transparency play an important   \n3 role in developing norms that preserve the integrity of the community. Reviewers will   \n4 be specifically instructed to not penalize honesty concerning limitations.   \n505 3. Theory Assumptions and Proofs ", "page_idx": 16}, {"type": "text", "text": "506 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n507 a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "09 Justification: The paper does not include theoretical assumptions.   \n10 Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "521 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "522 Question: Does the paper fully disclose all the information needed to reproduce the main   \n523 experimental results of the paper to the extent that it affects the main claims and/or conclusions   \n524 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We have provided all the experimental details to reproduce the results. Code is also available in the supplemental materials. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might sufifce, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "560 5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "561 Question: Does the paper provide open access to the data and code, with sufifcient instructions   \n562 to faithfully reproduce the main experimental results, as described in supplemental material?   \n563 Answer: [Yes]   \n564 Justification: We have included the code in the supplemental materials and shared the data   \n565 link anonymously in the main paper.   \n566 Guidelines:   \n567 \u2022 The answer NA means that paper does not include experiments requiring code.   \n568 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n569 public/guides/CodeSubmissionPolicy) for more details.   \n570 \u2022 While we encourage the release of code and data, we understand that this might not be   \n571 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n572 including code, unless this is central to the contribution (e.g., for a new open-source   \n573 benchmark).   \n574 \u2022 The instructions should contain the exact command and environment needed to run   \n575 to reproduce the results. See the NeurIPS code and data submission guidelines   \n576 (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n577 \u2022 The authors should provide instructions on data access and preparation, including how   \n578 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n579 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n580 proposed method and baselines. If only a subset of experiments are reproducible, they   \n581 should state which ones are omitted from the script and why.   \n582 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n583 versions (if applicable).   \n584 \u2022 Providing as much information as possible in supplemental material (appended to the   \n585 paper) is recommended, but including URLs to data and code is permitted.   \n586 6. Experimental Setting/Details   \n587 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n588 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n589 results?   \n590 Answer: [Yes]   \n591 Justification: We have specified all the training and test details to understand the results.   \n592 Guidelines:   \n593 \u2022 The answer NA means that the paper does not include experiments.   \n594 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n595 that is necessary to appreciate the results and make sense of them.   \n596 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n597 material.   \n598 7. Experiment Statistical Significance   \n599 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n600 information about the statistical significance of the experiments?   \n601 Answer: [Yes]   \n602 Justification: We have performed our experiments three times for each to provide the mean   \n603 and variance accuracy suitably and correctly in our tables.   \n604 Guidelines:   \n605 \u2022 The answer NA means that the paper does not include experiments.   \n606 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence   \n607 intervals, or statistical significance tests, at least for the experiments that support the   \n608 main claims of the paper.   \n609 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n610 example, train/test split, initialization, random drawing of some parameter, or overall   \n611 run with given experimental conditions).   \n612 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n613 call to a library function, bootstrap, etc.)   \n614 \u2022 The assumptions made should be given (e.g., Normally distributed errors). ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "625 8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "626 Question: For each experiment, does the paper provide sufifcient information on the computer   \n627 resources (type of compute workers, memory, time of execution) needed to reproduce the   \n628 experiments? ", "page_idx": 19}, {"type": "text", "text": "0 Justification: We have provided the details of computer resources in the experimental section. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "40 9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: This research conducted in the paper conforms in every respect with the NeurIPS Code of Ethics. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "652 10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "653 Question: Does the paper discuss both potential positive societal impacts and negative   \n654 societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "56 Justification: We have discussed both potential positive societal impacts and negative societal   \n57 impacts in Sec. A. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 19}, {"type": "text", "text": "666 \u2022 The conference expects that many papers will be foundational research and not tied   \n667 to particular applications, let alone deployments. However, if there is a direct path to   \n668 any negative applications, the authors should point it out. For example, it is legitimate   \n669 to point out that an improvement in the quality of generative models could be used to   \n670 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n671 that a generic algorithm for optimizing neural networks could enable people to train   \n672 models that generate Deepfakes faster.   \n673 \u2022 The authors should consider possible harms that could arise when the technology is   \n674 being used as intended and functioning correctly, harms that could arise when the   \n675 technology is being used as intended but gives incorrect results, and harms following   \n676 from (intentional or unintentional) misuse of the technology.   \n677 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n678 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n679 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n680 feedback over time, improving the efifciency and accessibility of ML).   \n681 11. Safeguards   \n682 Question: Does the paper describe safeguards that have been put in place for responsible   \n683 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n684 image generators, or scraped datasets)?   \n685 Answer: [NA]   \n686 Justification: We believe this paper poses no such risks.   \n687 Guidelines:   \n688 \u2022 The answer NA means that the paper poses no such risks.   \n689 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n690 necessary safeguards to allow for controlled use of the model, for example by requiring   \n691 that users adhere to usage guidelines or restrictions to access the model or implementing   \n692 safety filters.   \n693 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n694 should describe how they avoided releasing unsafe images.   \n695 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n696 not require this, but we encourage authors to take this into account and make a best   \n697 faith effort.   \n698 12. Licenses for existing assets   \n699 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n700 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n701 properly respected?   \n702 Answer: [Yes]   \n703 Justification: We have cited all papers and credited all code we utilized in this work.   \n704 Guidelines:   \n705 \u2022 The answer NA means that the paper does not use existing assets.   \n706 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n707 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n708 URL.   \n709 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n710 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n711 service of that source should be provided.   \n712 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n713 package should be provided. For popular datasets, paperswithcode.com/datasets   \n714 has curated licenses for some datasets. Their licensing guide can help determine the   \n715 license of a dataset.   \n716 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n717 the derived asset (if it has changed) should be provided.   \n718 \u2022 If this information is not available online, the authors are encouraged to reach out to the   \n719 asset\u2019s creators.   \n720 13. New Assets   \n721 Question: Are new assets introduced in the paper well documented and is the documentation   \n722 provided alongside the assets?   \n723 Answer: [Yes]   \n724 Justification: Our code has been included in the supplemental materials and is well   \n725 documented, we have also shared the synthetic data in the main paper.   \n726 Guidelines:   \n727 \u2022 The answer NA means that the paper does not release new assets.   \n728 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n729 submissions via structured templates. This includes details about training, license,   \n730 limitations, etc.   \n731 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n732 asset is used.   \n733 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n734 create an anonymized URL or include an anonymized zip file.   \n735 14. Crowdsourcing and Research with Human Subjects   \n736 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n737 include the full text of instructions given to participants and screenshots, if applicable, as   \n738 well as details about compensation (if any)?   \n739 Answer: [NA]   \n740 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n741 Guidelines:   \n742 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n743 human subjects.   \n744 \u2022 Including this information in the supplemental material is fine, but if the main   \n745 contribution of the paper involves human subjects, then as much detail as possible   \n746 should be included in the main paper.   \n747 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n748 or other labor should be paid at least the minimum wage in the country of the data   \n749 collector.   \n750 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n751 Subjects   \n752 Question: Does the paper describe potential risks incurred by study participants, whether   \n753 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n754 approvals (or an equivalent approval/review based on the requirements of your country or   \n755 institution) were obtained?   \n756 Answer: [NA]   \n757 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n758 Guidelines:   \n759 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n760 human subjects.   \n761 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n762 may be required for any human subjects research. If you obtained IRB approval, you   \n763 should clearly state this in the paper.   \n764 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n765 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n766 guidelines for their institution.   \n767 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n768 applicable), such as the institution conducting the review. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}]