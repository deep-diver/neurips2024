[{"figure_path": "zWuHSIALBh/tables/tables_3_1.jpg", "caption": "Table 2: Experimental results of supervised fine-tuning on Open Assistant dataset. PT denotes pre-trained Llama2 70B with 5-shot demonstration. SFTfact denotes the variant which only optimizes factuality. FS denotes FACTSCORE.", "description": "This table presents the experimental results of supervised fine-tuning on the Open Assistant dataset.  It compares the performance of several models: a pre-trained Llama 2 70B model (PT), a standard supervised fine-tuned model (SFT), a factuality-focused fine-tuned model (SFTfact), and a combined model (SFT). The evaluation metrics include win rate on the Alpaca Eval benchmark and FACTSCORE (FS) for factuality on three datasets: Biography, Alpaca Fact, and FAVA.  The table showcases the impact of focusing solely on factuality during the fine-tuning process.", "section": "5.1 Evaluation Datasets and Metrics"}, {"figure_path": "zWuHSIALBh/tables/tables_3_2.jpg", "caption": "Table 1: Pilot study on bio generation. Pos. denotes the positives for SFT or DPO. Neg. denotes the negatives for DPO. FS denotes FACTSCORE.", "description": "This table presents the results of a pilot study on biography generation.  It compares different approaches to fine-tuning a language model (Llama-2 7B) for factuality, specifically focusing on supervised fine-tuning (SFT) and direct preference optimization (DPO).  The table shows the source of supervision used for training (e.g., PTRAG, which leverages retrieval-augmented generation), the positive and negative examples used in DPO, the resulting FACTSCORE (a metric for factuality), and the number of correct and erroneous facts generated by the model.", "section": "3 A Pilot Study on Factual Alignment"}, {"figure_path": "zWuHSIALBh/tables/tables_6_1.jpg", "caption": "Table 2: Experimental results of supervised fine-tuning on Open Assistant dataset. PT denotes pre-trained Llama2 70B with 5-shot demonstration. SFTfact denotes the variant which only optimizes factuality. FS denotes FACTSCORE.", "description": "This table presents the results of supervised fine-tuning experiments conducted on the Open Assistant dataset using three different models: the pre-trained Llama 2 70B model (PT), a supervised fine-tuned model (SFT), and a variant of the SFT model that focuses specifically on factuality (SFTfact).  The table compares the performance of these models across four metrics: Alpaca Eval win rate (a measure of helpfulness), Biography FACTSCORE (a measure of factuality in biography generation), Alpaca Fact FACTSCORE (a measure of factuality in Alpaca Fact generation), and FAVA FACTSCORE (a measure of factuality in FAVA generation). The '# Corr. / Err.' columns indicate the number of correct and erroneous facts generated by each model.  This data allows researchers to assess the effectiveness of different training approaches on enhancing both the helpfulness and factual accuracy of LLMs.", "section": "5 Experiments"}, {"figure_path": "zWuHSIALBh/tables/tables_6_2.jpg", "caption": "Table 3: Experiments of direct preference optimization (DPO). IF. and Fact. denote instruction following (x, y+, y-) and factuality (x \u2208 Xfact, Ytrue, Yfalse) preference data, where Xfact denotes the set of fact-based instructions. DPOfact denotes the variant which only optimizes factuality. The preference data statistics is listed in Appendix, Table 11.", "description": "This table presents the results of experiments using direct preference optimization (DPO) for fine-tuning language models.  It compares different configurations of DPO, including variations that optimize for instruction following alone, factuality alone, and both simultaneously. The table shows the performance of each model variant on four metrics: Alpaca Eval win rate, Bio FACTSCORE, Alpaca Fact FACTSCORE, and FAVA FACTSCORE.  The number of correct and erroneous facts is also reported for the three FACTSCORE metrics.  Additional details on the preference data used are referenced.", "section": "5 Experiments"}, {"figure_path": "zWuHSIALBh/tables/tables_7_1.jpg", "caption": "Table 4: Results on TruthfulQA.", "description": "This table presents the results of the TruthfulQA evaluation for various models.  It compares the performance of different model variants on the TruthfulQA benchmark, focusing on the BLEU and ROUGE scores, which are common metrics for evaluating the quality of generated text. The models compared include Llama-2 7B Chat, different SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization) variants, with and without factuality-aware training. This allows assessing the impact of factuality-aware alignment on the truthfulness of the generated responses.", "section": "5.4 Results on TruthfulQA"}, {"figure_path": "zWuHSIALBh/tables/tables_7_2.jpg", "caption": "Table 3: Experiments of direct preference optimization (DPO). IF. and Fact. denote instruction following (x, y+, y-) and factuality (x \u2208 Xfact, Ytrue, Yfalse) preference data, where Xfact denotes the set of fact-based instructions. DPOfact denotes the variant which only optimizes factuality. The preference data statistics is listed in Appendix, Table 11.", "description": "This table presents the results of experiments using direct preference optimization (DPO) for fine-tuning language models. It compares different DPO training methods, focusing on instruction following and factuality, to evaluate the effectiveness of factuality-aware alignment.  The table includes the results for several metrics, such as the win rate on Alpaca Eval and FACTSCORE (FS) across different datasets.  The various rows show results for different combinations of instruction-following data and factuality preference data, allowing for analysis of how each factor contributes to the overall performance of the model.", "section": "5 Experiments"}, {"figure_path": "zWuHSIALBh/tables/tables_8_1.jpg", "caption": "Table 6: Effects of fact-based classification.", "description": "This table presents the ablation study results on the impact of fact-based instruction and sentence classification.  Rows 1 and 2 compare the performance of SFT models with and without fact-based instruction classification, showing that the latter leads to better instruction-following and factuality scores. Rows 3 and 4 compare the effects of adding factuality classification in the DPO stage, with and without instruction-level classification, showing that the combination of both classifications leads to slight improvements in both factuality and instruction following scores.  Overall, the table demonstrates the importance of correctly identifying fact-based instructions in factual alignment.", "section": "6.2 Effects of Fact-Based Sentence Classification"}, {"figure_path": "zWuHSIALBh/tables/tables_8_2.jpg", "caption": "Table 7: Ablation on factuality preference data.", "description": "This ablation study compares three different methods for creating factuality preference data for the factuality-aware DPO. The first method selects the responses with the maximum and minimum factuality rewards as positive and negative samples, respectively. The second method enumerates all possible response pairs and selects the pair with the highest and lowest factuality rewards. The third method uses a weighted combination of instruction-following and factuality rewards to select pairs. The table shows the performance of each method on Alpaca Eval (win rate) and Bio (FACTSCORE).", "section": "6.3 Ablations on Factuality Preference Data Creation"}, {"figure_path": "zWuHSIALBh/tables/tables_8_3.jpg", "caption": "Table 8: Effects of DPO on response length.", "description": "This table presents the average length of model responses generated by different models across four datasets: Alpaca Eval, Bio, Alpaca Fact, and FAVA.  The models compared include the baseline SFT model and three DPO variants: a standard DPO model, a DPO model focusing solely on factuality (DPOfact), and the full factuality-aware DPO model. The table shows how different alignment strategies impact the length of generated responses, illustrating a trade-off between instruction-following capability (encouraging longer responses) and factuality (potentially leading to shorter responses).", "section": "5.3 Comparisons of DPO"}, {"figure_path": "zWuHSIALBh/tables/tables_15_1.jpg", "caption": "Table 9: Results on MMLU and GSM8K.", "description": "This table compares the performance of different fine-tuned language models on two standard benchmarks: MMLU and GSM8K.  The models are compared based on their accuracy.  Model (1) is the baseline model, while Model (2) incorporates the factuality-aware approach proposed in the paper. The results show a slight decrease in accuracy when the factuality-aware approach is added, suggesting potential tradeoffs between instruction following and factuality.", "section": "5 Experiments"}, {"figure_path": "zWuHSIALBh/tables/tables_15_2.jpg", "caption": "Table 10: A comparison of factuality reward models.  denotes the correlation between human annotation.", "description": "This table presents the results of an ablation study comparing different factuality reward models.  The study evaluates the correlation between human annotations of factuality and the predictions of various models. The models differ in the underlying fact-checking method (Instruct Llama 7B vs. SFT (Llama-2 70B)), the number of supporting facts used, and whether the facts are atomic or sentence-level. The results show that Instruct Llama 7B with 10 atomic support facts achieves the highest correlation with human annotations.", "section": "5. Experiments"}, {"figure_path": "zWuHSIALBh/tables/tables_16_1.jpg", "caption": "Table 11: Training data statistics for different variants. IF. and Fact. denote instruction following (x, y+, y\u2212) and factuality (x \u2208 Xfact, Ytrue, Yfalse) preference data, where Xfact denotes the set of fact-based instructions.", "description": "This table presents the number of instructions and preference pairs used for training different model variants.  The variants are distinguished by whether they used instruction-following data only, factuality data only, or a combination of both. The number of fact-based instructions is also shown, indicating which instructions were used to train the factuality-aware aspect of the models. The table is crucial for understanding the differences in training data used and subsequently the variations in model performance across the different experimental settings reported in the paper.", "section": "4.2 Our Approach"}, {"figure_path": "zWuHSIALBh/tables/tables_17_1.jpg", "caption": "Table 2: Experimental results of supervised fine-tuning on Open Assistant dataset. PT denotes pre-trained Llama2 70B with 5-shot demonstration. SFTfact denotes the variant which only optimizes factuality. FS denotes FACTSCORE.", "description": "This table presents the experimental results of supervised fine-tuning on the Open Assistant dataset.  It compares different variations of the supervised fine-tuning (SFT) method, including a baseline using a pre-trained Llama 2 70B model with 5-shot demonstrations (PT), a standard SFT approach, and a variation focusing solely on factuality (SFTfact). The results are evaluated across four metrics: win rate on Alpaca Eval, FACTSCORE (FS) and number of correct/erroneous facts on three datasets: Biography, Alpaca Fact, and FAVA. The table helps to show how focusing specifically on factuality affects instruction-following capabilities and the overall factual accuracy of the generated text.", "section": "5.1 Evaluation Datasets and Metrics"}, {"figure_path": "zWuHSIALBh/tables/tables_19_1.jpg", "caption": "Table 2: Experimental results of supervised fine-tuning on Open Assistant dataset. PT denotes pre-trained Llama2 70B with 5-shot demonstration. SFTfact denotes the variant which only optimizes factuality. FS denotes FACTSCORE.", "description": "This table presents the experimental results of supervised fine-tuning on the Open Assistant dataset. It compares the performance of different models, including a pre-trained Llama 2 70B model (PT), a standard supervised fine-tuned model (SFT), and a factuality-focused variant (SFTfact).  The evaluation metrics include the win rate on Alpaca Eval, the FACTSCORE (FS) and the number of correct and erroneous facts on three datasets: Biography, Alpaca Fact, and FAVA. The table helps to analyze the impact of different fine-tuning approaches on both instruction following and factuality.", "section": "5. Experiments"}, {"figure_path": "zWuHSIALBh/tables/tables_19_2.jpg", "caption": "Table 2: Experimental results of supervised fine-tuning on Open Assistant dataset. PT denotes pre-trained Llama2 70B with 5-shot demonstration. SFTfact denotes the variant which only optimizes factuality. FS denotes FACTSCORE.", "description": "This table presents the experimental results obtained from supervised fine-tuning on the Open Assistant dataset.  It compares several different models, including a pre-trained Llama 2 70B model (PT) and different versions of the supervised fine-tuning model (SFT).  A key comparison is made between SFT and SFTfact, which specifically focuses on factuality optimization. The table shows the results for various metrics, including the win rate on Alpaca Eval (a measure of instruction-following capability), and FACTSCORE (FS) on Biography, Alpaca Fact, and FAVA datasets (all evaluating factuality). The results highlight the impact of different fine-tuning strategies and data sources on both instruction-following ability and factual accuracy.", "section": "5 Experiments"}]