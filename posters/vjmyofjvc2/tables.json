[{"figure_path": "VJMYOfJVC2/tables/tables_2_1.jpg", "caption": "Table 1: Comparison of current model editing methods. \u201c\u221a\u201d refers to \u201cyes\u201d and \u201cwell-supported\u201d, X refers to \u201cno\u201d or \u201cbadly-supported\u201d, and \u201c\u25cb\u201d refers to \u201cless-supported\u201d. The three metrics of Reliability, Generalization, and Locality denote the performances on lifelong (continual) editing.", "description": "This table compares different existing model editing methods based on their use of long-term vs. working memory, whether they use parametric or retrieval-based knowledge, if they support lifelong editing, and their performance on reliability, generalization, and locality metrics in continual editing scenarios.", "section": "Rethinking the Memory Design of Lifelong Model Editing"}, {"figure_path": "VJMYOfJVC2/tables/tables_5_1.jpg", "caption": "Table 2: Main editing results for QA setting (ZsRE dataset). T: Num Edits.", "description": "This table presents the main results of the question answering task using the ZsRE dataset.  It compares different methods for lifelong model editing across various metrics. The metrics evaluated are Reliability (Rel.), Generalization (Gen.), and Locality (Loc.),  averaged across different numbers of continual edits (T=1, 10, 100, 1000).  The results are shown for two different LLM architectures: LLAMA-2-7B and Mistral-7B.", "section": "3 Experiments"}, {"figure_path": "VJMYOfJVC2/tables/tables_5_2.jpg", "caption": "Table 2: Main editing results for QA setting (ZsRE dataset). T: Num Edits.", "description": "This table presents the main results of the experiments conducted for the question answering task using the ZsRE dataset.  It compares different model editing methods (FT-L, FT-EWC, MEND, ROME, MEMIT, MEMIT-MASS, DEFER, GRACE, and WISE) across four different numbers of continual edits (T=1, T=10, T=100, T=1000). For each method and number of edits, the table shows the reliability, generalization, and locality metrics, as well as the average score across these three metrics.  Results are provided for two different Large Language Models: LLaMA-2-7B and Mistral-7B.", "section": "Experiments"}, {"figure_path": "VJMYOfJVC2/tables/tables_6_1.jpg", "caption": "Table 4: Main editing results for Hallucination setting (SelfCheckGPT dataset). T: Num Edits.", "description": "This table presents the main results of the Hallucination setting using the SelfCheckGPT dataset.  It compares various methods for continual learning and model editing, evaluating their performance across different numbers of edits (T). The metrics used are Reliability (PPL for perplexity), Locality (an increase indicates better locality), and shows the average performance across these metrics.  The goal is to assess how well each method can correct hallucinations while maintaining the model's original knowledge and generalization abilities.", "section": "3.2 Main Results"}, {"figure_path": "VJMYOfJVC2/tables/tables_6_2.jpg", "caption": "Table 2: Main editing results for QA setting (ZsRE dataset). T: Num Edits.", "description": "This table presents the main results of the proposed WISE model and several baseline methods on a question-answering task using the ZsRE dataset.  The performance is evaluated across four different numbers of continual edits (T = 1, 10, 100, and 1000) and three key metrics: Reliability (Rel.), Generalization (Gen.), and Locality (Loc.). The average score across the three metrics is also reported for each method.", "section": "Experiments"}, {"figure_path": "VJMYOfJVC2/tables/tables_8_1.jpg", "caption": "Table 6: Scaling to 3K edits of ZsRE. LLAMA-2-7B.", "description": "This table presents the results of scaling the number of continual edits to 3000 for the ZsRE dataset using the LLaMA-2-7B model.  It compares the performance of WISE-Merge (using a single side memory and multi-time merging), WISE-Retrieve (keeping several side memories and retrieval), and a theoretical upper bound for WISE-Retrieve (Oracle).  The table shows that WISE consistently outperforms existing baselines, highlighting its scalability for handling extremely long continual edits. The 'oracle' version provides insight into the potential of WISE with significantly improved retrieval of side memories.", "section": "3 Experiments"}, {"figure_path": "VJMYOfJVC2/tables/tables_8_2.jpg", "caption": "Table 7: Ablation study of Router (compared with Table 2). LlaMA.", "description": "This table presents the ablation study results of the Router component in the WISE model. It compares the performance of WISE with and without the router, showing the impact of the router on the model's ability to maintain reliability, generalization, and locality during lifelong model editing. The results demonstrate that removing the router leads to a significant decrease in locality, highlighting the router's importance in identifying editing scopes and minimizing side effects.", "section": "2.3 WISE: Side Memory with Knowledge Sharding, Merging, and Routing"}, {"figure_path": "VJMYOfJVC2/tables/tables_18_1.jpg", "caption": "Table 1: Comparison of current model editing methods. \u201c\u221a\u201d refers to \u201cyes\u201d and \u201cwell-supported\u201d, X refers to \u201cno\u201d or \u201cbadly-supported\u201d, and \u201c\u25cb\u201d refers to \u201cless-supported\u201d. The three metrics of Reliability, Generalization, and Locality denote the performances on lifelong (continual) editing.", "description": "This table compares several existing lifelong model editing methods across three key metrics: reliability, generalization, and locality.  It shows whether each method edits long-term memory (model parameters) or working memory (activations/representations), and how well they perform on each metric in a continual editing scenario (multiple edits over time).  The results highlight the trade-offs between these three desired properties for different memory editing approaches.", "section": "2.2 Rethinking the Memory Design of Lifelong Model Editing"}, {"figure_path": "VJMYOfJVC2/tables/tables_21_1.jpg", "caption": "Table 2: Main editing results for QA setting (ZsRE dataset). T: Num Edits.", "description": "This table presents the main results of the experiment conducted for the question answering task using the ZsRE dataset.  It compares the performance of various model editing methods (FT-L, FT-EWC, MEND, ROME, MEMIT, MEMIT-MASS, DEFER, GRACE, and WISE) across different numbers of continual edits (T = 1, 10, 100, 1000). The performance is evaluated using three metrics: Reliability (Rel.), Generalization (Gen.), and Locality (Loc.).  The results are shown separately for two different LLM models: LLaMA-2-7B and Mistral-7B.", "section": "3 Experiments"}, {"figure_path": "VJMYOfJVC2/tables/tables_22_1.jpg", "caption": "Table 1: Comparison of current model editing methods. \u201c\u221a\u201d refers to \u201cyes\u201d and \u201cwell-supported\u201d, X refers to \u201cno\u201d or \u201cbadly-supported\u201d, and \u201c\u25cb\u201d refers to \u201cless-supported\u201d. The three metrics of Reliability, Generalization, and Locality denote the performances on lifelong (continual) editing.", "description": "This table compares several existing lifelong model editing methods across three key metrics: Reliability, Generalization, and Locality.  It highlights whether each method utilizes long-term or working memory and assesses their performance in maintaining previously learned knowledge while adapting to new information over many sequential edits.", "section": "2 Rethinking the Memory Design of Lifelong Model Editing"}, {"figure_path": "VJMYOfJVC2/tables/tables_23_1.jpg", "caption": "Table 1: Comparison of current model editing methods. \u201c\u221a\u201d refers to \u201cyes\u201d and \u201cwell-supported\u201d, X refers to \u201cno\u201d or \u201cbadly-supported\u201d, and \u201c\u25cb\u201d refers to \u201cless-supported\u201d. The three metrics of Reliability, Generalization, and Locality denote the performances on lifelong (continual) editing.", "description": "This table compares several existing lifelong model editing methods across three key criteria: reliability, generalization, and locality.  It highlights the strengths and weaknesses of approaches that focus on long-term memory (directly editing model parameters) versus those that rely on working memory (retrieving and modifying activations).  The table uses a visual shorthand (\u221a, X, \u25cb) to quickly communicate the level of support each method provides for each property in continual editing scenarios.", "section": "2.2 Rethinking the Memory Design of Lifelong Model Editing"}, {"figure_path": "VJMYOfJVC2/tables/tables_25_1.jpg", "caption": "Table 2: Main editing results for QA setting (ZsRE dataset). T: Num Edits.", "description": "This table presents the main results of the experiments conducted on the question answering task using the ZsRE dataset. It compares the performance of various methods in terms of reliability, generalization, and locality across different numbers of continual edits (T = 1, 10, 100, 1000). The results are shown separately for two different large language models: LLaMA-2-7B and Mistral-7B.  The metrics indicate how well each model can remember both current and previous edits (reliability), generalize to new queries (generalization), and avoid interference between edits and pre-trained knowledge (locality).", "section": "Experiments"}, {"figure_path": "VJMYOfJVC2/tables/tables_25_2.jpg", "caption": "Table 1: Comparison of current model editing methods. \u201c\u221a\u201d refers to \u201cyes\u201d and \u201cwell-supported\u201d, X refers to \u201cno\u201d or \u201cbadly-supported\u201d, and \u201c\u25cb\u201d refers to \u201cless-supported\u201d. The three metrics of Reliability, Generalization, and Locality denote the performances on lifelong (continual) editing.", "description": "This table compares several existing lifelong model editing methods across three key metrics: reliability (ability to remember past edits), generalization (ability to handle unseen queries), and locality (avoiding unintended edits to unrelated knowledge).  It highlights the trade-offs between different approaches that use either long-term memory (directly editing model parameters) or working memory (using retrieval-based methods).", "section": "2 Rethinking the Memory Design of Lifelong Model Editing"}]