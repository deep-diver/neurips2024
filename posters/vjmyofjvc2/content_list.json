[{"type": "text", "text": "WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Peng Wang1\u2217 Zexi Li1\u2217 Ningyu Zhang1\u2020 Ziwen $\\mathbf{X}\\mathbf{u}^{1}$ Yunzhi Yao1 Yong Jiang2 Pengjun Xie2 Fei Huang2 Huajun Chen1\u2020 1 Zhejiang University 2 Alibaba Group {peng2001,zexi.li,zhangningyu}@zju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) need knowledge updates to meet the ever-growing world facts and correct the hallucinated responses, facilitating the methods of lifelong model editing. Where the updated knowledge resides in memories is a fundamental question for model editing. In this paper, we find that editing either long-term memory (direct model parameters) or working memory (nonparametric knowledge of neural network activations/representations by retrieval) will result in an impossible triangle\u2014reliability, generalization, and locality can not be realized together in the lifelong editing settings. For long-term memory, directly editing the parameters will cause confilcts with irrelevant pretrained knowledge or previous edits (poor reliability and locality). For working memory, retrieval-based activations can hardly make the model understand the edits and generalize (poor generalization). Therefore, we propose WISE to bridge the gap between memories. In WISE, we design a dual parametric memory scheme, which consists of the main memory for the pretrained knowledge and a side memory for the edited knowledge. We only edit the knowledge in the side memory and train a router to decide which memory to go through when given a query. For continual editing, we devise a knowledge-sharding mechanism where different sets of edits reside in distinct subspaces of parameters and are subsequently merged into a shared memory without confilcts. Extensive experiments show that WISE can outperform previous model editing methods and overcome the impossible triangle under lifelong model editing of question answering, hallucination, and out-of-distribution settings across trending LLM architectures, e.g., GPT, LLaMA, and Mistral\u2021. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) show emergent intelligence when scaling the number of parameters and data [1\u20134], which reveals the sparks of artificial general intelligence [5]. However, when deployed, LLMs still make mistakes [6], generating responses with hallucinations [7], bias [8], and factual decays [9]. On the other hand, the world\u2019s knowledge is ever-growing, so the up-to-date knowledge is usually different from the one during LLMs\u2019 pretraining [10]. Many such errors and emerging facts will arise sequentially in deployment, some of which have to be addressed timely and efficiently without waiting for retraining or finetuning [11, 12]. Also, retraining or finetuning is often too computationally expensive [13, 10], which is not sustainable for lifelong growing knowledge. Therefore, lifelong model editing [10] was proposed to remedy the continual knowledge updates and injections for LLMs in a cheap and timely manner. ", "page_idx": 0}, {"type": "text", "text": "An effective lifelong model editing approach should satisfy the following properties [14, 15, 11, 16, 17]: i) reliability, the model can remember both current and previous edits after sequential editing; ii) locality, model editing will not influence inherent pretrained knowledge which is irrelevant to the edited knowledge; iii) generalization, the model is not just merely memorizing the query-target pairs; ", "page_idx": 1}, {"type": "text", "text": "instead, it should understand and generalize when given other forms of queries with the same knowledge. We compare existing model editing and continual learning methods on the three metrics in Figure $1$ and find that it seems to be an impossible triangle\u2014reliability, generalization, and locality can not be realized at the same time in the continual editing settings. We find that where the updated knowledge resides in memories affects editing performances, and previous methods can be generally divided into editing either long-term memory, e.g., ROME [18], MEMIT [19], and FT-EWC (Finetuning with Elastic Weight Consolidation [20], a continual learning method), or working memory, e.g., GRACE [10]. Note that the categorization of long-term and working memories is derived from human recognition [21, 22] and neuroscience [23] which has recently been adopted in the study of LLMs [24\u201327]. Model editing of long-term memory refers to directly editing the model parameters, which contain generalizable parametric knowledge [28, 24]. However, editing long-term memory will cause conflicts with previous pretrained knowledge, resulting in poor locality (e.g., ROME and FT-EWC in Figure 1). Working memory refers to the non-parametric knowledge of neural network activations/representations by retrieval, and it does not change the network parameters [24]; instead, it replaces the representations by retrieval at working (inference) time, like GRACE. GRACE\u2019s working memory shows promising results in reliability and locality, but in our experiments, it shows poor generalization since retrieval-based representations can hardly make the model understand the edits and generalize to different queries. It reveals that long-term memory and working memory both have drawbacks for lifelong model editing, though there were some special memory designs for LLM architectures, like MemorryLLM [28], SPALM [27], and Memoria [25], they change the architectures and cannot be directly applied for different LLMs. Intuitively, there is a gap between editing working and long-term memories, thus, in this paper, we study: ", "page_idx": 1}, {"type": "image", "img_path": "VJMYOfJVC2/tmp/051c3c03b12172e317d03dab412bb25105a2318d71d99b5f1e6cd02b792ab799.jpg", "img_caption": ["Figure 1: Metric triangle among reliability, generalization, and locality. ZsRE dataset, number of continual edits $T=100$ , LLaMA-2-7B. Editing methods based on long-term memory (ROME and FT-EWC) and working memory (DEFER and GRACE) show the impossible triangle in metrics, while our WISE is leading in all three metrics. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "2 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Preliminaries: Lifelong Model Editing ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We focus on lifelong model editing problem [10, 11], which can ensure hundreds or even thousands of sequential edits on LLMs to make the outputs of target queries align with human expectations while maintaining LLMs\u2019 previous knowledge and capability. Let $f_{\\Theta}:\\mathbb{X}\\mapsto\\mathbb{Y}$ , parameterized by $\\Theta$ , denote a model function mapping an input $\\mathbf{x}$ to the prediction $f_{\\Theta}(\\mathbf{x})$ . The initial model before editing is $\\Theta_{0}$ , which is trained on a large corpus $\\mathcal{D}_{\\mathrm{train}}$ . When the LLM makes mistakes or requires injections of new knowledge, it needs model editing with a time-evolving editing dataset as $\\bar{\\mathcal{D}_{\\mathrm{edit}}}=\\{(\\bar{\\mathcal{X}}_{e},\\mathcal{Y}_{e})|(\\mathbf{x}_{1},\\mathbf{y}_{1}),...,(\\bar{\\mathbf{x}_{T}},\\mathbf{y}_{T})\\}$ . At the time step $T$ , a model editor (ME) takes the $T$ -th edit and the LLM of the $T-1$ time step $f_{\\Theta_{T-1}}$ as inputs and produce the revised LLM model $f_{\\Theta_{T}}$ following the equation below: ", "page_idx": 2}, {"type": "equation", "text": "$$\nf_{\\Theta_{T}}=\\mathrm{ME}(f_{\\Theta_{T-1}},\\mathbf{x}_{T},\\mathbf{y}_{T}),\\quad\\mathrm{s.t.}~f_{\\Theta_{T}}(\\mathbf{x})=\\left\\{\\begin{array}{l l}{\\mathbf{y}_{e}}&{\\mathrm{if}\\,\\mathbf{x}\\in\\mathcal{X}_{e},}\\\\ {f_{\\Theta_{0}}(\\mathbf{x})}&{\\mathrm{if}\\,\\mathbf{x}\\not\\in\\mathcal{X}_{e}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Equation 1 describes that after model editing, the LLM should make the correct prediction on the current edit as $f_{\\Theta_{T}}(\\mathbf{x}_{T})=\\mathbf{y}_{T}$ , while also preserving knowledge from past editing instances $\\left(\\mathbf{x}{\\mathrm{<}}T,\\mathbf{y}{\\mathrm{<}}T\\right)\\in\\mathcal{D}_{\\mathrm{edit}}$ as well as maintaining capability of $f_{\\Theta_{0}}$ on the irrelevant data when $x\\notin\\mathcal{X}_{e}$ , especially for general training corpus $\\ensuremath{\\mathcal{D}}_{\\mathrm{train}}$ . ", "page_idx": 2}, {"type": "text", "text": "2.2 Rethinking the Memory Design of Lifelong Model Editing ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Table 1: Comparison of current model editing methods. $\\surd\\sqrt{\\phantom{2}},$ refers to \u201cyes\u201d and \u201cwell-supported\u201d, $\\boldsymbol{x}$ refers to \u201cno\u201d or \u201cbadly-supported\u201d, and $\\hookrightarrow$ \u201d refers to \u201cless-supported\u201d. The three metrics of Reliability, Generalization, and Locality denote  the performances on lifelong (continual) editing. ", "page_idx": 2}, {"type": "table", "img_path": "VJMYOfJVC2/tmp/a6943aece5b494c0685aa39c988d6163e2ec24a394cf5d7704e8e5d70e8c77a2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "In Table 1, we compare current model editing methods in terms of memory types and lifelong editing abilities. FT-EWC [20], ROME [18], MEMIT [19], and MEND [31] edit the long-term memory stored in the LLMs\u2019 model parameters, but they either do not support continual editing or have negative effects on irrelevant knowledge (poor locality). GRACE [10] is designed for lifelong editing via retrieval-based working memory. The retrieval codebook can avoid the conflicts of irrelevant knowledge, but GRACE fails to generalize due to its codebook being a non-parametric knowledge representation that solely memorizes queries without comprehension. It is worth noting that SERAC [32]/DEFER [10] uses working memory that is stored in additional small models: a scope classifier and a counterfactual model, whose knowledge is parametric. However, the small counterfactual model cannot match the expressiveness and generalization capabilities of LLM itself, making it challenging for the edited knowledge to generalize effectively. ", "page_idx": 2}, {"type": "text", "text": "To enable effective lifelong model editing, the method should take advantage of both LLM parameters\u2019 long-term memory and retrieval-based working memory. Therefore, we propose WISE as follows. ", "page_idx": 2}, {"type": "text", "text": "2.3 WISE: Side Memory with Knowledge Sharding, Merging, and Routing ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As illustrated in Figure 2, WISE comprises two key components: 1) Side Memory Design: i) side memory: side memory is a memory container that is initialized as a copy of LLM\u2019s certain FFN layer, storing the stream of edits; ii) memory routing mechanism: similar to retrieval, a routing activation component is adopted to identify the scope of edits, routing the main (original) or side memories during inference; 2) Knowledge Sharding and Merging: i) knowledge in random memory subspaces: to make the edits in appropriate knowledge density and avoid forgetting, we shard the side memory into several random subspaces for editing; ii) knowledge merging: we leverage model merging techniques to merge different memory shards into one side memory without loss of knowledge. ", "page_idx": 2}, {"type": "image", "img_path": "VJMYOfJVC2/tmp/b8d58e00c6d81097e6d0d85ab3997179abcabb22e65464dd3d3de9d55128fd4d.jpg", "img_caption": ["Figure 2: Overview of WISE. Side memory (in blue) and main memory (in green) store edited and pretrained knowledge, respectively. Note: during inference, if WISE-Retrieve, the activation routing will retrieve and select one side memory with maximal activation score. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "2.3.1 Side Memory Design ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Side memory in FFN\u2019s value matrix. Each layer in a Transformer contains a multi-head self-attention (MHA) mechanism and a feed-forward network (FFN), where the FFN constitutes two-thirds of the model parameters [33]. The question of how Transformers retrieve and utilize stored knowledge remains unresolved [18, 34], yet past works [31, 33] have demonstrated that editing the weights of the FFN is consistently more effective for LLMs. The FFN typically consists of key-value linear matrices: ${\\bf W}_{k},{\\bf W}_{v}$ , i.e., two multi-layer perceptron (MLP) layers. For the output of attention feature $\\mathbf{f}$ , the computation of the feed-forward network, omitting the bias terms, can be represented as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{FFN}(\\mathbf{f})=\\mathbf{a}\\cdot\\mathbf{W}_{v}=\\sigma(\\mathbf{f}^{\\top}\\cdot\\mathbf{W}_{k})\\cdot\\mathbf{W}_{v},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\sigma$ is a nonlinear activation function (e.g. SwiGLU, GeLU), and a represents the activation values of the first MLP layer. Following previous works [18, 33], we edit the value matrix $\\mathbf{W}_{v}$ of the chosen FFN layer. ", "page_idx": 3}, {"type": "text", "text": "However, directly editing the value matrix may cause forgetting and side effects in a lifelong setting. Thus, we copy a value matrix as side memory and edit the side memory instead of the original matrix (main memory). Specifically, the side memory is initialized with the copy of main memory as $\\mathbf{W}_{v^{\\prime}}\\gets\\mathbf{W}_{v}$ . Given the side memory, the new output is expressed as $\\mathrm{FFN}_{s}(\\bar{\\mathbf{f}})=\\mathbf{a}\\cdot\\mathbf{W}_{v^{\\prime}}$ . We will introduce how to update the side memory in Section 2.3.2. ", "page_idx": 3}, {"type": "text", "text": "Locating side memory\u2019s FFN layer. Transformer LLMs have been widely demonstrated to encode \u201clower-level\u201d information (e.g., parts of speech) in earlier layers while processing more advanced linguistic phenomena like anaphora and coreference in later layers [35\u201337]. Representations in later hidden layers propagate through residual connections without drastic changes [38, 18], enabling effective early exit in LLMs [39, 40]. Therefore, to minimize the side effects of editing and adjust advanced linguistic phenomena, we target mid-to-late layers (e.g. 27) for side memory. Further analysis of layer selection is provided in Section 3.3. ", "page_idx": 3}, {"type": "text", "text": "Routing between side memories and main memory. Similar to the retrieval-based methods [10, 32], during inference, it is needed to decide whether the main memory or the side memory is used. If a given query is within the scope of previous edits, the side memory is used; otherwise, the main memory. Inspired by [11], we introduce a routing activation indicator, given an input $\\mathbf{x}$ , it is formulated: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta_{\\mathrm{act}}(\\mathbf{x})=\\|\\boldsymbol{\\mathcal{A}}(\\mathbf{x})\\cdot(\\mathbf{W}_{v^{\\prime}}-\\mathbf{W}_{v})\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{\\mathcal{A}}(\\cdot)=\\mathbf{a}$ is the activation of the side memory\u2019s corresponding FFN layer in Equation 2. We want the activation indicators of editing queries to be larger than the ones of irrelevant queries by a large margin, which is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{min}\\{\\Delta_{\\mathrm{act}}(\\mathbf{x}_{e})|\\mathbf{x}_{e}\\in\\mathcal{D}_{\\mathrm{edit}}\\}\\gg\\mathrm{max}\\{\\Delta_{\\mathrm{act}}(\\mathbf{x}_{i})|\\mathbf{x}_{i}\\in\\mathcal{D}_{\\mathrm{irr}}\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{D}_{\\mathrm{irr}}$ is the irrelevant dataset which includes $\\mathcal{D}_{\\mathrm{train}}$ . ", "page_idx": 3}, {"type": "text", "text": "To achieve the above objective, we design a margin-based loss function during editing training, similar to contrastive [41] or triplet loss [42]. The margin-based loss function for routing activation is: ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{a}=\\operatorname*{min}_{\\mathbf{W}_{v^{\\prime}}}\\{\\operatorname*{max}(0,\\Delta_{\\mathrm{act}}(\\mathbf{x}_{i})-\\alpha)+\\operatorname*{max}(0,\\beta-\\Delta_{\\mathrm{act}}(\\mathbf{x}_{e}))+\\operatorname*{max}(0,\\gamma-(\\Delta_{\\mathrm{act}}(\\mathbf{x}_{e})-\\Delta_{\\mathrm{act}}(\\mathbf{x}_{i})))\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{x}_{e}\\in\\mathcal{D}_{\\mathrm{edit}},\\mathbf{x}_{i}\\in\\mathcal{D}_{\\mathrm{irr}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Equation 5 aims that for all queries of irrelevant examples $\\mathbf{x}_{i}$ , the activation indicators should be less than threshold $\\alpha$ , and for the edit samples $\\mathbf{x}_{e}$ , the activations should be larger than threshold $\\beta$ , with a certain distance $\\gamma$ between $\\Delta_{\\mathrm{act}}(\\mathbf{x}_{e})$ and $\\Delta_{\\mathrm{act}}(\\mathbf{x}_{i})$ . ", "page_idx": 4}, {"type": "text", "text": "In the continual stream of incoming edits, the smallest activation indicator within the edits is updated and saved: $\\epsilon=\\operatorname*{min}\\{\\Delta_{\\mathrm{act}}(\\mathbf{x}_{e})|\\mathbf{x}_{e}\\in\\mathcal{D}_{\\mathrm{edit}}\\}$ . We aim to recognize the local scope of edits in this form. During inference, if the activation indicator of a new input is greater than $\\epsilon$ , WISE will use the side memory $\\mathbf{W}_{v^{\\prime}}$ ; otherwise, using the main memory $\\mathbf{W}_{v}$ . Thus, given the query $\\mathbf{x}$ , the output of the targeted FFN in Equation 2 is replaced by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{FFN}_{\\mathrm{out}}(\\mathbf{x})=\\left\\{\\overset{A(\\mathbf{x})\\cdot\\mathbf{W}_{v^{\\prime}}}{A(\\mathbf{x})\\cdot\\mathbf{W}_{v}}\\right.\\quad\\mathrm{if}\\ \\lVert A(\\mathbf{x})\\cdot(\\mathbf{W}_{v^{\\prime}}-\\mathbf{W}_{v})\\rVert_{2}>\\epsilon,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "2.3.2 Knowledge Sharding and Merging ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "How to effectively and efficiently store continual knowledge in model parameters is important for lifelong editing. We introduce the notion of \u201cknowledge density\u201d (similar to knowledge capacity [43]) that describes how many pieces of knowledge are stored per parameter on average. There is an editing dilemma w.r.t. knowledge density: i) If only a few edits are made for full fine-tuning or editing the entire memory, the knowledge density is low, which may lead to overfitting. ii) If numerous edits are made within a common and limited parameter space, the knowledge density is high, resulting in confilcts within the edited knowledge and potentially causing catastrophic forgetting. To remedy this dilemma, we propose a knowledge sharding and merging mechanism to divide the edits into several shards, store them in different parameter subspaces, and merge them into a common side memory. ", "page_idx": 4}, {"type": "text", "text": "Knowledge in random memory subspaces. We edit the side memory $\\mathbf{W}_{v^{\\prime}}$ . We divide $n$ edits into $k$ shards, copy the side memory for $k$ times, and generate $k$ random gradient mask with mask ratio $\\rho$ for each copy of side memory. A random gradient mask $\\mathbf{M}_{i}\\in\\{0,1\\}^{|\\mathbf{W}_{v^{\\prime}}|},i\\in[k]$ is a binary mask whose proportion of 1 is $\\rho$ [44]. For edit shard $i,i\\in[k]$ , we edit the knowledge into the subspace $\\mathbf{M}_{i}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{W}_{v^{\\prime}}^{i}\\leftarrow\\mathbf{W}_{v^{\\prime}}^{i}-\\eta(\\mathbf{M}_{i}\\odot\\mathbf{g}_{i}(\\mathbf{W}_{v^{\\prime}}^{i})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{W}_{v^{\\prime}}^{i}$ is the $i$ -th copy of the side memory, $\\eta$ is the learning rate, $\\mathbf{g}_{i}(\\cdot)$ is the gradient of the $i$ -th shard of edits, and the gradient is the autoregressive loss plus the routing activation loss $L_{a}$ (Equation 5): $L_{\\mathrm{edit}}=-\\log P_{W_{v^{\\prime}}}(\\mathbf{y}_{e}|\\mathbf{x}_{e})+L_{a}$ . ", "page_idx": 4}, {"type": "text", "text": "The random mask of gradients freezes the parameters intact when the elements are 0 and updates the weights when the elements are 1. It is superior to pruning because it does not harm the network performance while regularizing optimization in a subspace [44]. In addition, the $\\rho$ subspace will have higher knowledge density when $k\\cdot\\rho<1$ , resulting in higher generalization (e.g., Figure 5). Also, different shards of edits have different random masks, and due to the (sub)orthogonality of random masks, different shards will not confilct with each other. Therefore, we can non-destructively merge the $k$ copies of side memory into one. ", "page_idx": 4}, {"type": "text", "text": "Knowledge merging. We merge the $k$ subspace pieces of side memory into one. Because we randomly generate the subspace masks, different random masks will have some overlapping elements and some disjoint elements, following the theorem below: ", "page_idx": 4}, {"type": "text", "text": "Theorem 2.1 Subspace Overlap. Generate $k$ memory subspaces $\\mathbf{W}_{v^{\\prime}}^{i}$ , $i\\in[k]$ by random mask with 1\u2019s ratio $\\rho_{;}$ , so each memory has $\\rho\\cdot\\left|\\mathbf{W}_{v^{\\prime}}\\right|$ active trained parameters. For any two subspaces $\\mathbf{W}_{v^{\\prime}}^{i}$ and $\\mathbf{W}_{v^{\\prime}}^{j}\\;i\\neq j;i,j\\in[k]$ , there are $\\rho^{2}\\cdot|\\mathbf{W}_{v^{\\prime}}|$ active parameters that are overlapped. For all $k$ subspaces, there are $\\rho^{k}\\cdot|\\mathbf{W}_{v^{\\prime}}|$ overlapped active parameters. ", "page_idx": 4}, {"type": "text", "text": "The theorem shows that larger $\\rho$ will cause more overlap of subspace parameters, and the proof is in Appendix C. We find that this overlap is helpful in playing the role of \u201canchors\u201d for knowledge merging (See Figure 5 and Appendix B.5). However, knowledge confilcts also exist in the overlapped parameters, so we leverage the recent task arithmetic model merging technique Ties-Merge [45] to relieve the confilcts. First, we compute the edit weight shift vectors $\\mathrm{T}_{e}=\\{\\tau_{e}^{i}=\\mathbf{W}_{v^{\\prime}}^{i}-\\mathbf{W}_{v}|i\\in[k]\\}$ . Then, we use Ties-Merge to merge the edit vectors into one: ", "page_idx": 4}, {"type": "table", "img_path": "VJMYOfJVC2/tmp/48de598c9778a70858e8e8ce97d80a9fc837477943b02c31f89e1c504dfa4115.jpg", "table_caption": ["Table 2: Main editing results for QA setting (ZsRE dataset). $T$ : Num Edits. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{W}_{v^{\\prime}}\\gets\\mathbf{W}_{v}+\\mathrm{Ties}(\\mathrm{T}_{e};\\mathbf{W}_{v}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Ties-Merge consists of three steps: i) trim: trim the redundant parameters for each task vector; ii) elect the sign: elect the signs of each parameter; ii) disjoint merge: compute the disjoint mean for each parameter which has the same and correct signs [45]. By Ties-Merge, different subspaces of knowledge are integrated into one with fewer conflicts. We study the effects of different merging techniques in Table 11 of Appendix B.2. ", "page_idx": 5}, {"type": "text", "text": "Routing and retrieving among several side memories. One single side memory has its limited knowledge capacity [43]. For the lifelong editing stream, we can produce several side memories and retrieve them via activation score routing. We compute different activation indicator scores of side memories and retrieve the top-1 during inference. This design is named WISE-Retrieve, which enables a more challenging lifelong editing scenario. For WISE with only one side memory, it is notated as WISE-Merge. For most of the experiments, we use WISE-Merge by default, and we compare WISE-Retrieve in Table 6 and Figure 6. ", "page_idx": 5}, {"type": "text", "text": "The pseudo-code of our method can be found in Algorithms 1 and 2. ", "page_idx": 5}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "3.1 Experimental Settings and Evaluation Metrics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the experiments, we compare the performance of different baselines and WISE in sequentially editing LLM models hundreds to thousands of times. In practice, we augment ${\\bf x}_{e}$ by generating 10 random token sequences of length 10 using $f_{\\Theta}$ , enhancing editing generalization/adaptation to diverse contexts. We ensure that this augmentation with random tokens is applied across all baselines (See Appendix B.6, we ablate the contribution of Random Token). ", "page_idx": 5}, {"type": "text", "text": "Datasets and Models. We choose trending autoregressive LLM models LLaMA-2-7B [13], Mistral-7B [52], and GPT-J-6B [53, 54] for evaluation. The dataset details are in Table 3. Following [10], we evaluate WISE on the closed-book question-answering (QA) dataset ZsRE [46], and also evaluate its ability to correct Hallucination in SelfCheckGPT [48]. The ", "page_idx": 5}, {"type": "text", "text": "Table 3: Dataset statistics for main results. Locality Data is the irrelevant data of the editing process. $T$ is the number of samples. Pre-edit is the unedited model\u2019s performance on each dataset. ", "page_idx": 5}, {"type": "table", "img_path": "VJMYOfJVC2/tmp/87e163815b50ce79b51e6b3fbba984a84a6d5cc532b7d44643124efafe38a0de.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Temporal dataset [50] is employed to test the out-of-distribution (OOD) generalization of editing. Since Temporal comprises emerging entities post-2019, we avoid using the latest LLMs in OOD experiments. Instead, we follow the original literature of the Temporal dataset [50] and adopt GPT-J6B as the base model, which is pretrained on the Pile [51] with a cutoff in 2020. Implementation details and editing examples for each dataset and can be found in Appendix A. ", "page_idx": 5}, {"type": "table", "img_path": "VJMYOfJVC2/tmp/820a9d6a964fa682d0942d044ec4914b3c1c824c02b241ea1a485601f2571389.jpg", "table_caption": ["Table 4: Main editing results for Hallucination setting (SelfCheckGPT dataset). $T$ : Num Edits. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Baselines. The baselines include methods of continual learning and model editing. We compare WISE against direct fine-tuning FT-L with an additional KL divergence loss [18], and continual learning fine-tuning based on Elastic Weight Consolidation (FT-EWC) [20]. We also compare WISE to other model editors, including 1) GPT-style editors based on causal tracing: ROME [18], MEMIT [19], and MEMIT-MASS (a batch-editing version of MEMIT); 2) hypernetwork-based editors: MEND [31]; and 3) the latest memory-based editors: DEFER (inspired by SERAC [32] for inference routing) and GRACE [10]. Details on all comparisons are found in Appendix A.2. ", "page_idx": 6}, {"type": "text", "text": "Metrics. Each edit example includes an edit descriptor (i.e., query) $\\mathbf{x}_{e}$ , its paraphrase prompts $\\mathbf{x}_{e^{\\prime}}$ (if available) for testing generalization, and an unrelated statement $\\mathbf{x}_{\\mathrm{loc}}$ for testing locality. For the editing dataset $\\mathcal{D}_{\\mathrm{edit}}=\\overline{{\\{(\\lambda_{e},y_{e})\\}}}$ with $T$ edits, we evaluate the final post-edit model $f_{\\Theta_{T}}$ after the $T$ -th edit example $(\\mathbf{x}_{T},\\mathbf{y}_{T})$ . We evaluate the model editor\u2019s reliability and generalization using the metrics Rel. (a.k.a Edit Success Rate [10]) and Gen. (Generalization Success Rate [55]), while Loc. (Localization Success Rate [55]), defined as the post-edit model should not change the output of the irrelevant examples $\\mathbf{x}_{\\mathrm{loc}}$ , assesses specificity. We report these metrics and their mean scores, which are formally defined as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{Rel.}=\\frac{1}{T}\\sum_{t=1}^{T}\\mathbf{1}(f_{\\Theta_{T}}(\\mathbf{x}_{e}^{t})=\\mathbf{y}_{e}^{t}),\\;\\mathrm{Gen.}=\\frac{1}{T}\\sum_{t=1}^{T}\\mathbf{1}(f_{\\Theta_{T}}(\\mathbf{x}_{e^{t}}^{t})=\\mathbf{y}_{e}^{t}),\\;\\mathrm{Loc.}=\\frac{1}{T}\\sum_{t=1}^{T}\\mathbf{1}(f_{\\Theta_{T}}(\\mathbf{x}_{\\mathrm{i}\\mathrm{o}}^{t})=f_{\\Theta_{0}}(\\mathbf{x}_{\\mathrm{i}\\mathrm{o}}^{t})),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathbb{1}(\\cdot)$ is the indicator function. Notably, for the Hallucination dataset, following [10], we use the perplexity (PPL) to verify the locality, and there is no proper metric for generalization. ", "page_idx": 6}, {"type": "text", "text": "3.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Competitive Performance of WISE. The competitive performance of WISE is evident in Table 2 and 4, which compare its results with eight baselines on the QA (ZsRE) and Hallucination (SelfCheckGPT) settings. In general, we observe the followings: $\\pmb{\\mathrm{\\Sigma}}$ WISE outperforms existing methods on multiple tasks after long editing sequences; $\\pmb{\\varphi}$ direct editing of long-term memory (ROME, MEMIT, etc.) creates confilcts with prior pretraining knowledge, resulting in poor locality; and $\\pmb{\\otimes}$ retrieving working memory and modifying activations (GRACE, DEFER, etc) struggle to generalize to diverse queries. ", "page_idx": 6}, {"type": "text", "text": "In the QA setting, with $T=1000$ , WISE achieves average scores of 0.83 and 0.79 on LLaMA and Mistral, respectively, reflecting improvements of $18\\%$ and $11\\%$ over the nearest competitor. This demonstrates WISE\u2019s outstanding stability and effective management of long-sequential edits. While methods like MEND and ROME are competitive early in editing, they show clear shortcomings as the edit sequence extends. Directly editing long-term memory (e.g., MEMIT, FT-EWC, MEND) results in a significant decline in Loc. When $T\\in\\{100,1000\\}$ , this indicates that these methods cannot preserve LLMs\u2019 knowledge structure and significantly impair the model\u2019s generalization ability. GRACE excels in Loc. and Rel. (close to 1.00), however, it sacrifices generalization in continual editing. A possible reason is that token representation may not be suitable for measuring semantic similarity in autoregressive LMs, leading to paraphrase $\\mathbf{x}_{e^{\\prime}}$ failing to achieve similarity matching with any CodeBook Key in GRACE (detailed in Appendix B.1). Overemphasis on preserving and precisely adapting training data (working memory) hampers adaptability to new contexts. In a nutshell, most previous methods struggle to balance Rel., Gen., and Loc., particularly in long-form editing tasks. In addition, the results of GPT-J-6B can be found in Figure 9 in the Appendix. ", "page_idx": 6}, {"type": "table", "img_path": "VJMYOfJVC2/tmp/21167ce7240a2003b52ddb03030bb4acf7e5b5bd202d7352b3e4fb12ee68f789.jpg", "table_caption": ["Table 5: OOD results for Temporal dataset. GPT-J-6B is used. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "WISE also surpasses the baselines on the Hallucination dataset, maintaining the lowest perplexity scores of 3.12 and 5.21 at $T\\;=\\;600$ , with Loc. remaining above 0.93. We similarly observe significant $P P L$ increases for FT-L, MEND, and ROME in long-context editing tasks, while GRACE\u2019s performance is lackluster in LLM long texts (possibly due to the limited fitting capacity of the very small active trained parameters $\\left|h^{l}\\right|$ of GRACE). ", "page_idx": 6}, {"type": "image", "img_path": "VJMYOfJVC2/tmp/48bb7f18a72f400c9135fc43a546cd8fc616006af3e509cf90f718a7c67ec86b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "VJMYOfJVC2/tmp/ccfef29d166a1f6a3eca6413820ea8e9ed6bc555d6aab5a66d16aaa7911687ae.jpg", "img_caption": ["Figure 4: Analysis of locating FFN layer of side memory for WISE. ZsRE, LLaMA-2-7B. ", "Figure 5: Analysis of different mask ratios $\\rho$ and subspaces $k$ for WISE. Left: Avg. performance of Rel., Gen., and Loc.; Right: the subspace overlap probability in Theorem 2.1. ZsRE, LLaMA-2-7B. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Out-of-Distribution Evaluation. Ideally, model editing needs to generalize distributionally from formulaic editing examples to natural texts [50], where the distributional shift involves complexity rather than conventional domain shift [56]. Following [50], we evaluate the OOD generalization of editing methods on emerging entities using the temporal updating dataset, Temporal. Editing examples and evaluation metrics are provided in Appendix A.1. As shown in Table 5, WISE effectively handles out-of-distribution generalization tasks (achieving the best OOD Gen. and overall performance). DEFER delivers mediocre performance on OOD Gen. due to the limited capacity of the auxiliary model[14]. During the fine-tuning phase, GRACE and MEMIT focus on the representation $v*$ of a single input token after $\\mathbf{W}_{v}$ (GRACE: last token, MEMIT: last subject token). However, regarding $v*$ the editing carrier encounters two problems: 1) the training objective is not aligned with the pretraining phase, and 2) the single representation limits the search scope of gradient descent, making it difficult to handle OOD generalization. WISE, on the other hand, avoids these challenges. ", "page_idx": 7}, {"type": "text", "text": "3.3 Further Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Visualization of WISE\u2019s Routing Activation. To demonstrate the effectiveness of memory routing, we record the activation values $\\Delta_{\\mathrm{{act}}}(\\mathbf{x})$ of 1000 (QA, ZsRE)/600 (Halluc.) queries during the inference stage via knowledge merging into a single side memory. As shown in Figure 3, the purple horizontal line represents the activation threshold $\\epsilon$ recorded during the editing phase. Almost all unrelated queries show low activations with values less than 10 in ZsRE and less than 20 in Halluc.; meanwhile, WISE accurately routes the editing prompt and unseen paraphrases into the side memory. This ensures editing locality and prevents excessive shifts from the pre-training distribution during lifelong editing. ", "page_idx": 7}, {"type": "text", "text": "Localization Analysis of WISE\u2019s Side Memory. To validate the benefits of editing mid-to-late layers, we select decoder layers from early, intermediate, mid-to-late, and late stages. As shown in Figure 4, the ablation results reveal that editing critical layers like the early and final layers (0, 1, 31) is ineffective, even resulting in a very low Loc. value of 0.096, which indicates a failure to recognize the editing scope. This may occur because the early layers represent fundamental grammatical information, and the final layer directly controls the decoding procedure, leading to poor editing of advanced language functions. Editing in the intermediate layers is suboptimal but still shows a markable improvement compared to early layers, possibly because intermediate layers start to integrate basic grammatical information with more complex semantic data. Notably, the mid-to-late layers demonstrate exceptional editing performance; for instance, selecting layer 26 results in an $80\\%$ success rate and generalization while maintaining $100\\%$ locality. This empirically supports our claim in Section 2.3.1 that the redundant mid-to-late layers [39] are ideal side memory layers and confirms the hierarchical nature of information processing in Transformer LLMs [57, 58]. ", "page_idx": 7}, {"type": "image", "img_path": "VJMYOfJVC2/tmp/aa990a2bce5264d60044df38f71919c847d7868ef9ee4f9ea9558094df2904bc.jpg", "img_caption": ["Figure 3: Activations of the memory routing module of WISE when varying T. X-axis: Num edits. LLaMA-7B. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Analysis of $\\rho$ and $k$ for WISE. We analyze the important hyperparameters of WISE: the mask ratio $\\rho$ and the number of subspaces $k$ in Figure 5. On the left figure, for $k=2$ , the best $\\rho$ is 0.2, satisfying $k*\\rho\\,=\\,0.4\\,<\\,1$ , which implies the effectiveness of our subspace design that higher knowledge density will cause better generalization. When scaling $k$ , we observe an increasing demand of $\\rho$ . From Theorem 2.1, the probability of subspace overlap is $\\rho^{k}$ , and we hypothesize that this overlap is important as an anchor for model merging. Interestingly, from the right figure, it can be observed that the optimal cases always have the $\\rho^{k}$ closest to 0.03. This shows an inherent tradeoff between merge anchor and merge conflicts, and the subspace overlaps around 0.03 are optimal for the best performances. Such experiments indicate that $20\\%$ FFN parameters can accommodate at least 500 edited samples. When \"mask memory exhaustion\" occurs, we can allocate new mask parameters to store new knowledge. Using retrieve when knowledge isn\u2019t full and merging as needed to save memory, achieves true lifelong model editing. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Scale Up to 3K of Edits. We scale the number of continual edits to 3K in Table 6. We compare WISE-Merge, keeping one side memory by multi-time merging, and WISE-Retrieve, keeping several side memories by routing and retrieving among different side memories. For WISE-Retrieve, we show an upper bound \u201coracle\u201d, which always identifies the correct routing ", "page_idx": 8}, {"type": "table", "img_path": "VJMYOfJVC2/tmp/3568de5564740b513493b222e6b1d99681c290b724dac1df85f603633b791fdf.jpg", "table_caption": ["Table 6: Scaling to 3K edits of ZsRE. LLaMA-2-7B. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "path. We observe that the WISE series maintains high scalability, consistently outperforming the strongest baselines including MEMIT-MASS and GRACE. WISE-Retrieve based on top-1 activation retrieval demonstrates the best results in 3K edits, showing the effectiveness of well-organized memory subspaces and routing strategies during editing. We note that the \u201coracle\u201d exhibits marginal performance decline when scaling the edits from 2K to 3K, yet it demonstrates remarkable performance across all metrics. This underscores the potential of WISE to handle extremely long continual edits, contingent upon substantial improvement in the retrieval of side memories. Additionally, an appropriate replay of edits can further improve retrieval accuracy, as detailed in Appendix B.3. ", "page_idx": 8}, {"type": "text", "text": "Contribution of Router designs in WISE. Without the router strategy, all inputs either pass solely through the main or side memory. To further validate its effectiveness, we conduct additional ablations with $L_{a}$ . WISE\u2019s performance on ZsRE is shown in Table 7. We observe the expected decrease in Loc. w.o. $L_{a}$ , such as dropping from 1.00 to 0.72 at $\\scriptstyle\\mathrm{T}=1000$ , reveals the router\u2019s effectiveness in identifying editing scopes, minimizing side effects, and retaining a substantial amount of pre-training knowledge. ", "page_idx": 8}, {"type": "table", "img_path": "VJMYOfJVC2/tmp/06eb711a96f809b3efc078d4cf4fcdea48b125a4214b8b45ac9e32b563464a23.jpg", "table_caption": ["Table 7: Ablation study of Router (compared with Table 2). LlaMA. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Inference Time Analysis of WISE. Figure 6 shows the inference time of a single instance for LLaMA after $t\\in[0,3000]$ editing steps, measured across 10 trials of each setting. Consistent with our expectations, we find that WISE-Merge incurs a constant inference delay (about $3\\%$ ) as the editing stream expands. WISE-Retrieve, due to the introduction of retrieval routing, shows an increase in inference time as the number of edits increases, with a time cost increment of about $7\\%$ after 3K edits. Knowledge merging ensures that WISE-Merge only brings constant additional costs $0.64\\%$ extra parameters and $4\\%$ extra GPU VRAM, as detailed in Appendix B.7), contrasting with past memory-based works that continuously demand more available memory [10, 32]. ", "page_idx": 8}, {"type": "image", "img_path": "VJMYOfJVC2/tmp/672360b70ac7e2a82f7eeb4c07cefc942dcb5aacbcd7c9965285cb23e2226582.jpg", "img_caption": ["Figure 6: Inference time of WISE when varying T. ZsRE, LLaMA-2-7B. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4 Related Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Memory and Knowledge Injection of LLMs. LLMs have long-term (episodic) and working memory [24, 25, 27]. Long-term memory is stored in model parameters, updatable via (re)pretraining [53], finetuning [59], and model editing [14]. Working memory resides in neuron activations, utilized during inference [24]. In-context learning and retrieval-based editing methods like GRACE contribute to working memory [60, 10]. However, whether finetuning or retrieval is debated [61, 62]. Also, current knowledge injection methods often suffer from computational overhead [13, 10], catastrophic forgetting [63], and overfitting [64]. Methods like MemorryLLM [28], SPALM [27], NKB [65], and Memoria [25] are proposed to improve the memories from the architecture design perspective. ", "page_idx": 8}, {"type": "text", "text": "Model Editing of LLMs. Model editing encompasses constrained finetuning, locating-and-editing, meta-learning, and retrieval-based methods. ROME identifies factual associations and edits efficiently using MLP-based memories [18], extended by MEMIT for mass-editing [19]. T-Patcher adds neurons for edits in LLMs\u2019 feed-forward layers [11]. Meta-learning methods like MEND decouple finetuning gradients to generalize edits [31], complemented by MALMEN addressing cancellation effects [15]. Retrieval-based methods like SERAC and GRACE improve working memory for editing [32, 10]. From single to mass editing and static to lifelong editing, model editing evolves to meet realistic demands. The latest efforts in lifelong editing such as LTE [66], MALMEN [15], and RECIPE [67] require extensive training with domain-specific edits before specific editing, yet we cannot predict the domain of upcoming edits in the editing flow and accessing these data is often impractical or unrealistic. It potentially increases the risks associated with retraining. ", "page_idx": 9}, {"type": "text", "text": "Model Merging Model merging [68], also known as model fusion [69, 70], studies how to aggregate different models\u2019 knowledge into one by parameter merging. However, in the research of linear mode connectivity, it is found that different minima of neural networks can hardly be merged into a generalized one even if trained on the same datasets from the same initialization (but with different random seeds) [71, 72]. The main reason is considered to be the permutation invariance property of deep neural networks, which means that the positions of neurons can be permuted without affecting the network function [71]; as a result, different minima reside in different loss basins [72]. To improve linear mode connectivity and model merging, methods like optimal transport [70, 73], re-basin [72], and training-time alignment [44] are developed. For the applications, model merging techniques can help to improve the generalization of federated learning [74, 75] and enable knowledge aggregation of different-task models in a task arithmetic way [76, 77]. Recently, methods like task arithmetic in tangent space [77], TIES-Merging [45], ZipIt! [78], and ColD fusion [79] have been proposed for deep model fusion of pretrained foundation models, such as CLIP, ViT, and large language models. Specifically, TIES-Merging [45] consists of trim, elect sign & merge pipeline, which inspires the merge process of side memories in our paper.   \nFor detailed related works, please refer to Appendix D. ", "page_idx": 9}, {"type": "text", "text": "5 Limitations and Broader Impacts ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Although WISE shows promising results in lifelong editing, it also has some limitations. One limitation is addressed in Table 6 that the side memory retrieval has room for improvement to reach the oracle. Also, in Figure 6, the inference time of WISE-Retrieve increases with ever-growing editing streams. However, the current limitations cannot outweigh the merits of WISE in that it currently reaches better performance in general for lifelong model editing. ", "page_idx": 9}, {"type": "text", "text": "We bridge the gap between long-term and working memory, it may inspire further work on memory design for model editing or even LLM architecture. However, the application of such technologies should be guided by ethical considerations. Malicious users may attempt to edit LLMs to propagate hate, highlighting the need for safeguards to prevent abuse and mitigate harmful outcomes. Some current model editors update the model\u2019s weights directly, making edits hard to trace and withdraw. WISE uses a modular and non-destructive side memory, allowing users to discard it if edits are unnecessary or harmful, without modifications to the main LLMs. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we point out the impossible triangle of current lifelong modeling editing approaches that reliability, generalization, and locality can hardly be achieved simultaneously. We find the reason behind this is the gap between working and long-term memory. Therefore, we propose WISE, consisting of side memory and model merging, to remedy the gap. Extensive results show WISE is promising to reach high metrics at once on various datasets and LLM models. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to express gratitude to the anonymous reviewers for their kind comments. This work was supported by the National Natural Science Foundation of China (No. 62206246, No. NSFCU23B2055, No. NSFCU19B2027), the Fundamental Research Funds for the Central Universities (226-2023-00138), Zhejiang Provincial Natural Science Foundation of China (No. LGG22F030011), Yongjiang Talent Introduction Programme (2021A-156-G), CIPSC-SMP-Zhipu Large Model CrossDisciplinary Fund, Information Technology Center and State Key Lab of CAD&CG, Zhejiang University. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [2] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. Advances in Neural Information Processing Systems, 35:19523\u201319536, 2022. [3] Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling laws in language and vision. Advances in Neural Information Processing Systems, 35:22300\u2013 22312, 2022. [4] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models. CoRR, abs/2303.18223, 2023. [5] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. [6] Vidhisha Balachandran, Hannaneh Hajishirzi, William Cohen, and Yulia Tsvetkov. Correcting diverse factual errors in abstractive summarization via post-editing and language model infliling. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9818\u20139830, 2022.   \n[7] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1\u201338, 2023.   \n[8] Emilio Ferrara. Should chatgpt be biased? challenges and risks of bias in large language models. Challenges and Risks of Bias in Large Language Models (October 26, 2023), 2023. [9] Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6491\u20136506, 2021.   \n[10] Tom Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, and Marzyeh Ghassemi. Aging with grace: Lifelong model editing with discrete key-value adaptors. Advances in Neural Information Processing Systems, 36, 2023.   \n[11] Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, and Zhang Xiong. Transformer-patcher: One mistake worth one neuron. In The Eleventh International Conference on Learning Representations, 2023.   \n[12] Angeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson d\u2019Autume, Tomas Kocisky, Sebastian Ruder, et al. Mind the gap: Assessing temporal generalization in neural language models. Advances in Neural Information Processing Systems, 34:29348\u201329363, 2021.   \n[13] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[14] Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. Editing large language models: Problems, methods, and opportunities. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 10222\u201310240, 2023.   \n[15] Chenmien Tan, Ge Zhang, and Jie Fu. Massive editing for large language model via meta learning. In The Twelfth International Conference on Learning Representations, 2023.   \n[16] Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy V. Pyrkin, Sergei Popov, and Artem Babenko. Editable neural networks. CoRR, abs/2004.00345, 2020.   \n[17] Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6491\u20136506, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.   \n[18] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:17359\u201317372, 2022.   \n[19] Kevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. Massediting memory in a transformer. In The Eleventh International Conference on Learning Representations, 2023.   \n[20] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.   \n[21] George A Miller, Galanter Eugene, and Karl H Pribram. Plans and the structure of behaviour. In Systems Research for Behavioral Science, pages 369\u2013382. Routledge, 2017.   \n[22] Alan Baddeley. Working memory and language: An overview. Journal of communication disorders, 36(3):189\u2013208, 2003.   \n[23] Keisuke Fukuda and Geoffrey F Woodman. Visual working memory buffers information retrieved from visual long-term memory. Proceedings of the National Academy of Sciences, 114(20):5306\u20135311, 2017.   \n[24] Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and Sanjiv Kumar. Large language models with controllable working memory. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1774\u20131793, 2023.   \n[25] Sangjun Park and JinYeong Bak. Memoria: Hebbian memory architecture for human-like sequential processing. arXiv preprint arXiv:2310.03052, 2023.   \n[26] Charles Packer, Vivian Fang, Shishir G Patil, Kevin Lin, Sarah Wooders, and Joseph E Gonzalez. Memgpt: Towards llms as operating systems. arXiv preprint arXiv:2310.08560, 2023.   \n[27] Dani Yogatama, Cyprien de Masson d\u2019Autume, and Lingpeng Kong. Adaptive semiparametric language models. Transactions of the Association for Computational Linguistics, 9:362\u2013373, 2021.   \n[28] Yu Wang, Xiusi Chen, Jingbo Shang, and Julian McAuley. Memoryllm: Towards self-updatable large language models. arXiv preprint arXiv:2402.04624, 2024.   \n[29] Joseph B Hellige. Hemispheric asymmetry: What\u2019s right and what\u2019s left, volume 6. Harvard University Press, 2001.   \n[30] Richard B Ivry and Lynn C Robertson. The two sides of perception. MIT press, 1998.   \n[31] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. Fast model editing at scale. In International Conference on Learning Representations, 2022.   \n[32] Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn. Memory-based model editing at scale. In International Conference on Machine Learning, pages 15817\u201315831. PMLR, 2022.   \n[33] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5484\u20135495, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.   \n[34] Jingcheng Niu, Andrew Liu, Zining Zhu, and Gerald Penn. What does the knowledge neuron thesis have to do with knowledge? In The Twelfth International Conference on Learning Representations, 2024.   \n[35] Ganesh Jawahar, Beno\u00eet Sagot, and Djam\u00e9 Seddah. What does BERT learn about the structure of language? In Anna Korhonen, David Traum, and Llu\u00eds M\u00e0rquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3651\u20133657, Florence, Italy, July 2019. Association for Computational Linguistics.   \n[36] Yulia Otmakhova, Karin Verspoor, and Jey Han Lau. Cross-linguistic comparison of linguistic feature encoding in BERT models for typologically different languages. In Ekaterina Vylomova, Edoardo Ponti, and Ryan Cotterell, editors, Proceedings of the 4th Workshop on Research in Computational Linguistic Typology and Multilingual NLP, pages 27\u201335, Seattle, Washington, July 2022. Association for Computational Linguistics.   \n[37] Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In Anna Korhonen, David Traum, and Llu\u00eds M\u00e0rquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593\u20134601, Florence, Italy, July 2019. Association for Computational Linguistics.   \n[38] Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James R. Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models. In The Twelfth International Conference on Learning Representations, 2024.   \n[39] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect. arXiv preprint arXiv:2403.03853, 2024.   \n[40] Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay, and Donald Metzler. Confident adaptive language modeling. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 17456\u201317472. Curran Associates, Inc., 2022.   \n[41] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597\u20131607. PMLR, 2020.   \n[42] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815\u2013823, 2015.   \n[43] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.3, knowledge capacity scaling laws. 2024.   \n[44] Zexi Li, Zhiqi Li, Jie Lin, Tao Shen, Tao Lin, and Chao Wu. Training-time neuron alignment through permutation subspace for improving linear mode connectivity and model fusion. arXiv preprint arXiv:2402.01342, 2024.   \n[45] Prateek Yadav, Derek Tam, Leshem Choshen, Colin A Raffel, and Mohit Bansal. Ties-merging: Resolving interference when merging models. Advances in Neural Information Processing Systems, 36, 2023.   \n[46] Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction via reading comprehension. In Roger Levy and Lucia Specia, editors, Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 333\u2013342, Vancouver, Canada, August 2017. Association for Computational Linguistics.   \n[47] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452\u2013466, 2019.   \n[48] Potsawee Manakul, Adian Liusie, and Mark Gales. SelfCheckGPT: Zero-resource blackbox hallucination detection for generative large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9004\u20139017, Singapore, December 2023. Association for Computational Linguistics.   \n[49] Together Computer. Redpajama: an open dataset for training large language models. 2023.   \n[50] John Hewitt, Sarah Chen, Lanruo Lora Xie, Edward Adams, Percy Liang, and Christopher D. Manning. Model editing with canonical examples, 2024.   \n[51] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2020.   \n[52] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7b, 2023.   \n[53] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training.   \n[54] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.   \n[55] Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, et al. A comprehensive study of knowledge editing for large language models. arXiv preprint arXiv:2401.01286, 2024.   \n[56] Yonatan Oren, Shiori Sagawa, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust language modeling. In Kentaro Inui, Jing Jiang, Vincent $\\mathrm{Ng}$ , and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4227\u20134237, Hong Kong, China, November 2019. Association for Computational Linguistics.   \n[57] Aaron Mueller, Robert Frank, Tal Linzen, Luheng Wang, and Sebastian Schuster. Coloring the blank slate: Pre-training imparts a hierarchical inductive bias to sequence-to-sequence models. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Findings of the Association for Computational Linguistics: ACL 2022, pages 1352\u20131368, Dublin, Ireland, May 2022. Association for Computational Linguistics.   \n[58] Shikhar Murty, Pratyusha Sharma, Jacob Andreas, and Christopher Manning. Grokking of hierarchical structure in vanilla transformers. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 439\u2013448, Toronto, Canada, July 2023. Association for Computational Linguistics.   \n[59] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021.   \n[60] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837, 2022.   \n[61] Oded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. Fine-tuning or retrieval? comparing knowledge injection in llms. arXiv preprint arXiv:2312.05934, 2023.   \n[62] Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, and Yanai Elazar. Fewshot fine-tuning vs. in-context learning: A fair comparison and evaluation. In Findings of the Association for Computational Linguistics: ACL 2023, pages 12284\u201312314, 2023.   \n[63] Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. arXiv preprint arXiv:2308.08747, 2023.   \n[64] Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. Memorization without overfitting: Analyzing the training dynamics of large language models. Advances in Neural Information Processing Systems, 35:38274\u201338290, 2022.   \n[65] Damai Dai, Wenbin Jiang, Qingxiu Dong, Yajuan Lyu, and Zhifang Sui. Neural knowledge bank for pretrained transformers. In Natural Language Processing and Chinese Computing: 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12\u201315, 2023, Proceedings, Part II, page 772\u2013783, Berlin, Heidelberg, 2023. Springer-Verlag.   \n[66] Yuxin Jiang, Yufei Wang, Chuhan Wu, Wanjun Zhong, Xingshan Zeng, Jiahui Gao, Liangyou Li, Xin Jiang, Lifeng Shang, Ruiming Tang, Qun Liu, and Wei Wang. Learning to edit: Aligning llms with knowledge editing, 2024.   \n[67] Qizhou Chen, Taolin Zhang, Xiaofeng He, Dongyang Li, Chengyu Wang, Longtao Huang, and Hui Xue. Lifelong knowledge editing for llms with retrieval-augmented continuous prompt learning, 2024.   \n[68] Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, and Jacob Solawetz. Arcee\u2019s mergekit: A toolkit for merging large language models. arXiv preprint arXiv:2403.13257, 2024.   \n[69] Weishi Li, Yong Peng, Miao Zhang, Liang Ding, Han Hu, and Li Shen. Deep model fusion: A survey. arXiv preprint arXiv:2309.15698, 2023.   \n[70] Sidak Pal Singh and Martin Jaggi. Model fusion via optimal transport. Advances in Neural Information Processing Systems, 33:22045\u201322055, 2020.   \n[71] Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur. The role of permutation invariance in linear mode connectivity of neural networks. In International Conference on Learning Representations, 2022.   \n[72] Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models modulo permutation symmetries. In The Eleventh International Conference on Learning Representations, 2023.   \n[73] Moritz Imfeld, Jacopo Graldi, Marco Giordano, Thomas Hofmann, Sotiris Anagnostidis, and Sidak Pal Singh. Transformer fusion with optimal transport. In The Twelfth International Conference on Learning Representations, 2024.   \n[74] Zexi Li, Tao Lin, Xinyi Shang, and Chao Wu. Revisiting weighted aggregation in federated learning with neural networks. In International Conference on Machine Learning, pages 19767\u201319788. PMLR, 2023.   \n[75] Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated learning with matched averaging. In International Conference on Learning Representations, 2020.   \n[76] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. In The Eleventh International Conference on Learning Representations, 2023.   \n[77] Guillermo Ortiz-Jimenez, Alessandro Favero, and Pascal Frossard. Task arithmetic in the tangent space: Improved editing of pre-trained models. Advances in Neural Information Processing Systems, 36, 2024.   \n[78] George Stoica, Daniel Bolya, Jakob Brandt Bjorner, Pratik Ramesh, Taylor Hearn, and Judy Hoffman. Zipit! merging models from different tasks without training. In The Twelfth International Conference on Learning Representations, 2024.   \n[79] Shachar Don-Yehiya, Elad Venezian, Colin Raffel, Noam Slonim, and Leshem Choshen. Cold fusion: Collaborative descent for distributed multitask finetuning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 788\u2013806, 2023.   \n[80] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u2013 1901, 2020.   \n[81] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[82] OpenAI and the Co-authors. Gpt-4 technical report, 2024.   \n[83] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), San Diega, CA, USA, 2015.   \n[84] Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In Sanjoy Dasgupta and David McAllester, editors, Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 71\u201379, Atlanta, Georgia, USA, 17\u201319 Jun 2013. PMLR.   \n[85] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.   \n[86] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Kentaro Inui, Jing Jiang, Vincent $\\mathrm{Ng}$ , and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982\u20133992, Hong Kong, China, November 2019. Association for Computational Linguistics.   \n[87] Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.   \n[88] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020.   \n[89] Tian Yu Liu, Matthew Trager, Alessandro Achille, Pramuditha Perera, Luca Zancato, and Stefano Soatto. Meaning representations from trajectories in autoregressive models. In The Twelfth International Conference on Learning Representations, 2024.   \n[90] Afra Feyza Aky\u00fcrek, Ekin Aky\u00fcrek, Derry Wijaya, and Jacob Andreas. Subspace regularizers for few-shot class incremental learning. In International Conference on Learning Representations, 2022.   \n[91] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023.   \n[92] Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context behind: Efficient infinite context transformers with infini-attention. arXiv preprint arXiv:2404.07143, 2024.   \n[93] Matthew Sotoudeh and A Thakur. Correcting deep neural networks with small, generalizing patches. In Workshop on safety and robustness in decision making, 2019.   \n[94] Ankit Singh Rawat, Chen Zhu, Daliang Li, Felix Yu, Manzil Zaheer, Sanjiv Kumar, and Srinadh Bhojanapalli. Modifying memories in transformer models. In International Conference on Machine Learning (ICML), volume 2020, 2021.   \n[95] Shuaiyi Li, Yang Deng, Deng Cai, Hongyuan Lu, Liang Chen, and Wai Lam. Consecutive model editing with batch alongside hook layers, 2024.   \n[96] Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, and Baobao Chang. Can we edit factual knowledge by in-context learning? In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4862\u20134876, Singapore, December 2023. Association for Computational Linguistics.   \n[97] Baolong Bi, Shenghua Liu, Lingrui Mei, Yiwei Wang, Pengliang Ji, and Xueqi Cheng. Decoding by contrasting knowledge: Enhancing llms\u2019 confidence on edited facts, 2024.   \n[98] Haizhou Shi, Zihao Xu, Hengyi Wang, Weiyi Qin, Wenyuan Wang, Yibin Wang, and Hao Wang. Continual learning of large language models: A comprehensive survey, 2024.   \n[99] Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, and Gholamreza Haffari. Continual learning for large language models: A survey, 2024.   \n[100] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale\u0161 Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366\u20133385, 2021.   \n[101] Bill Yuchen Lin, Sida I Wang, Xi Lin, Robin Jia, Lin Xiao, Xiang Ren, and Scott Yih. On continual model refinement in out-of-distribution data streams. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3128\u20133139, 2022.   \n[102] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience replay for continual learning. Advances in neural information processing systems, 32, 2019.   \n[103] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, and Lucas Page-Caccia. Online continual learning with maximal interfered retrieval. Advances in neural information processing systems, 32, 2019.   \n[104] Thomas Henn, Yasukazu Sakamoto, Cl\u00e9ment Jacquet, Shunsuke Yoshizawa, Masamichi Andou, Stephen Tchen, Ryosuke Saga, Hiroyuki Ishihara, Katsuhiko Shimizu, Yingzhen Li, et al. A principled approach to failure analysis and model repairment: Demonstration in medical imaging. In Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2021: 24th International Conference, Strasbourg, France, September 27\u2013October 1, 2021, Proceedings, Part III 24, pages 509\u2013518. Springer, 2021.   \n[105] Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and Wen Gao. Post-training quantization for vision transformer. Advances in Neural Information Processing Systems, 34:28092\u201328103, 2021.   \n[106] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017.   \n[107] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continual learning. In European Conference on Computer Vision, pages 631\u2013648. Springer, 2022.   \n[108] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 139\u2013149, 2022.   \n[109] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808, 2020.   \n[110] Frederik Tr\u00e4uble, Anirudh Goyal, Nasim Rahaman, Michael Curtis Mozer, Kenji Kawaguchi, Yoshua Bengio, and Bernhard Sch\u00f6lkopf. Discrete key-value bottleneck. In International Conference on Machine Learning, pages 34431\u201334455. PMLR, 2023.   \n[111] Yi Dai, Hao Lang, Yinhe Zheng, Fei Huang, Luo Si, and Yongbin Li. Lifelong learning for question answering with hierarchical prompts. arXiv e-prints, pages arXiv\u20132208, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In the Appendix, we introduce more details along with additional experimental results, discussions, and related works: ", "page_idx": 18}, {"type": "text", "text": "\u2022 Appendix A: Experimental setups (cf. Section 3).   \n\u2022 Appendix B: More experimental results (cf. Section 2 and 3).   \n\u2022 Appendix C: Proof of the Theorem 2.1 (cf. Section 2).   \n\u2022 Appendix D: Additional discussions and more related works (cf. Section 4). ", "page_idx": 18}, {"type": "text", "text": "A Implementation Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "A.1 Description of Datasets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Table 8: Bolded text refers to the edit labels ${\\bf y}_{e}$ . Locality example $\\mathbf{x}_{\\mathrm{loc}}$ is an unrelated query. ", "page_idx": 18}, {"type": "text", "text": "(a) ZsRE, question-answering (b) Hallucination editing dataset example. In the original data [10], there editing dataset example. is no paraphrase $x_{e^{\\prime}}$ so the measurement of Gen. metric is ignored here. ", "page_idx": 18}, {"type": "table", "img_path": "VJMYOfJVC2/tmp/38048895e54535806d4a9b953a2db2776edadd7f0288e196a9468db0d4c62fa0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "ZsRE The ZsRE question-answering task [46] is extensively studied within the model editing literature [18, 19, 31, 15, 11], where each record contains an editing statement $\\mathbf{x}_{e}$ , a paraphrase prompt ${\\bf x}_{e}^{\\prime}$ , and a locality prompt $\\mathbf{x}_{\\mathrm{loc}}$ . We use the same train/test split as [31] (163196/19086). Notably, only MEND requires fitting a hypernetwork on the training set; other methods discard the training set and perform edits and evaluations on the test set. In practice, we randomly sample 1K and 3K records from the test set to form the edit sets in Section 3.2 and 3.3. ", "page_idx": 18}, {"type": "text", "text": "Hallucination We utilize the same dataset as GRACE, SelfCheckGPT [48], to assess the ability of Model Editors to mitigate hallucinations in autoregressive LMs. This setting involves editing highly inaccurate sentences (sourced from GPT-3 [80]) and replacing them with corresponding sentences from actual Wikipedia entries. This dataset aligns more closely with real-world deployment scenarios where models trigger \"unexpected behaviors,\" and the token length of edits is significantly longer than in past datasets, making it a more challenging editing setting. Unlike GRACE, which used GPT2-XL (1.5B) [81], our main experiments deploy larger LLMs, LLaMA and Mistral, Figure 7: Hallucination length statistics. both with 7B parameters, we measure retention of pretraining data $(\\mathbf{x}_{\\mathrm{loc}})$ from the base model: RedPajama [49], a public version of LLaMA\u2019s pretraining data. Some of the exceptionally long editing samples cannot even be accommodated on an NVIDIA A800 (80GB) due to resource limitations. As shown in Figure 7, the original dataset provided by GRACE, after tokenization with LLAMATOKENIZER, has length distributions ranging from [17,390]. The dimension of a single MLP layer in llama-2-7b-hf is (11008, 4096) \u00a7. Theoretically, fine-tuning an input of length 390 with default ", "page_idx": 18}, {"type": "image", "img_path": "VJMYOfJVC2/tmp/c13c94a304835b788bf3d3030e1e7aea82e672d0f630a815d2e95ac3add0f1f3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "$\\mathbf{x}_{e},\\mathbf{y}_{e}$ Self-driving cars, also known as autonomous vehicles, are vehicles that are capable of navigating and operating without human intervention. These innovative vehicles rely on a combination of advanced sensors, artificial intelligence, and computer algorithms to interpret their environment and make real-time decisions. With the potential to significantly impact numerous industries and sectors, self-driving cars have the ability to revolutionize transportation by enhancing safety, improving traffic flow, and increasing energy efficiency. However, challenges related to regulatory frameworks, ethical considerations, and public acceptance still need to be addressed before widespread adoption becomes a reality.   \nxloc Apple has a new peach with the release of its 3.0GHz, 8-core Intel Xeon-based Mac Pro. The 8-core Mac Pro is powered bu two quad-core Intel Xeon \u00a8Clov ertown\u00a8processors running at 3.0GHz. Apple also released a quad-core Mac Pro featuring two Dual-Core Intel Xeon W\u00a8oodcrestp\u00a8rocessors.   \n$\\mathbf{x}_{e}$ , yood Self-driving cars, also known as autonomous cars or driverless cars, are vehicles capable of traveling without human input. These cars utilize a range of sensors, including optical and thermographic cameras, radar, lidar, ultrasound/sonar, GPS, odometry, and inertial measurement units, to perceive their surroundings. By interpreting sensory information, control systems in the car are able to create a three-dimensional model of its environment. Using this model, the car can then identify the best navigation path and develop strategies for managing traffic controls and obstacles. As self-driving car technology continues to advance, it is expected to have a significant impact on various fields such as the automotive industry, health, welfare, urban planning, traffic, insurance, and the labor market. The regulation of autonomous vehicles is also becoming an increasingly important topic of discussion. ", "page_idx": 19}, {"type": "text", "text": "full precision and the Adam optimizer would require $(390+4+4+4)*(11008*4096*4)+4*7\\mathrm{B}=$ 100.36GB of VRAM (for activations, gradients, first-order, and second-order optimizers), exceeding the memory capacity of the NVIDIA A800. Consequently, we excluded excessively long samples (limiting tokenized lengths to 254) and ultimately retained 906 editing instances (compared to 1392 in GRACE). To facilitate a fair comparison with MEND, we specifically allocated a training set for MEND, with a final train/test split of 306/600. All methods were edited and evaluated on the test set. ", "page_idx": 19}, {"type": "text", "text": "Temporal [50] sources the prefix $\\mathbf{x}_{e}$ from the first paragraph of an entity\u2019s Wikipedia page and samples a paragraph ${\\bf y}_{e}$ discussed by GPT-4 [82] about the emerging entity $\\mathbf{x}_{e}$ , which is usually noisy but may contain helpful information. These are presented as editing prompts to Model Editors. For out-of-distribution (OOD) generalization to complex natural contexts (not ftited), $\\mathbf{y}_{\\mathrm{ood}}$ is taken from the actual Wikipedia suffix of $\\mathbf{x}_{e}$ . This setup is utilized to evaluate the OOD generalization of Model Editors centered around a single canonical example. Consistent with previous work [10], the out-ofscope data $\\mathbf{x}_{\\mathrm{loc}}$ is derived from the Pile [51], the pretraining corpus of GPT-J-6B. Examples from the dataset can be seen in Table 9. To measure the OOD generalization of editing methods for emerging entities, we perform model editing using standardized simple examples and then evaluate this behavior on more complex instances. Following [50], in a natural setting, no single correct continuation exists. Thus, we also use probability threshold-based evaluations, such as $80\\%$ , where the editing success rate evaluates whether the loss $L_{\\mathbf{x}_{e},\\mathbf{y}_{\\mathrm{ood}}}$ for an example falls below $\\delta=-\\mathrm{log}(0.8)$ , as indicated in the formula below. The intuition behind this is that many other plausible alternative continuations may exist. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname{OOD}\\operatorname{Gen.}=\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{1}\\{(L_{\\Theta_{T}}(\\mathbf{x}_{e},\\mathbf{y}_{\\mathrm{ood}})<\\delta)\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "A.2 Descriptions of Compared Model Editors ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "FT-L. All other layers of the LLMs remain frozen, and only a single MLP layer is fine-tuned through autoregressive loss [18]. Additionally, we impose an $\\mathrm{L}_{\\infty}$ norm constraint to prevent the parameters from deviating too far from the pretrained distribution. ", "page_idx": 19}, {"type": "text", "text": "FT-EWC. Elastic Weight Consolidation (EWC) has been demonstrated to mitigate catastrophic forgetting by updating weights using a Fisher information matrix, which is computed from past edits, multiplied by a scaling factor $\\lambda$ [20]. Following [10], we omit the constraints of the $\\mathrm{L}_{\\infty}$ norm in this implementation. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "MEND. MEND [31] transforms the gradients obtained from standard fine-tuning using a hypernetwork that converts gradients decomposed into low rank (rank $=1$ ) into new gradients, which are then applied to the target layer for parameter updates. During the training phase, a small auxiliary hypernetwork receives editing examples $(\\mathbf{x}_{e},\\mathbf{y}_{e})$ , and $\\mathbf{x}_{\\mathrm{loc}}$ . MEND\u2019s training loss comprises the standard autoregressive loss combined with the KL divergence loss of the model\u2019s output on $\\mathbf{x}_{\\mathrm{loc}}$ before and after editing. This hypernetwork plays a crucial role during the editing procedure. ", "page_idx": 20}, {"type": "text", "text": "ROME. ROME [18] uses causal analysis to pinpoint knowledge within specific MLP layers and modifies the entire matrix through least squares approximation. It operates under the strong assumption that the MLP is the primary module for storing knowledge [33], and it injects a single piece of knowledge into the MLP at each iteration using a Lagrangian remainder. ", "page_idx": 20}, {"type": "text", "text": "MEMIT. Similarly, based on the assumption that the FFN serves as a knowledge key-value store, MEMIT [19] manipulates parameters of specific layers directly through least squares approximation. Unlike ROME, which updates a single layer, MEMIT is a multi-layer updating algorithm that supports simultaneous updates of hundreds or thousands of facts. For sequential model editing tasks, MEMIT requires immediate on-the-fly repairs when the model makes errors, expressed as $f_{\\Theta_{T}}=\\mathrm{MEMIT}(f_{\\Theta_{T-1}},\\mathbf{x}_{T},\\mathbf{y}_{T})$ , involving multiple operations on the original model. ", "page_idx": 20}, {"type": "text", "text": "MEMIT-MASS. Unlike sequential editing, MEMIT supports modification of multiple knowledge fragments in a batch mode, named MEMIT-MASS. Suppose we collect streaming errors as $\\left(\\mathcal{X},\\mathcal{Y}\\right)=$ $\\{(\\bar{\\mathbf{x}}_{0},\\mathbf{y}_{0}),(\\mathbf{x}_{1},\\mathbf{y}_{1}),...,(\\mathbf{x}_{T},\\mathbf{y}_{T})\\}$ and inject them collectively into the MLP, it only involves a single editing operation on the original model as $f_{\\Theta_{T}}\\,=\\,\\mathrm{MEMIT}(f_{\\Theta_{0}},\\chi,y)$ . Although this approach loses the capability for on-the-fly repairs, we still include this baseline in our experiments. ", "page_idx": 20}, {"type": "text", "text": "DEFER. In GRACE, a reimplementation of SERAC [32] is utilized, denoted as DEFER. For new inputs, DEFER includes a network $g$ (corresponding to the scope classifier in SERAC) that predicts whether to: 1) trust the prediction of the LLMs, or 2) trust the prediction of the new model. Here, the new model is configured as a single-layer linear network $o$ with a sigmoid activation function, corresponding to the counterfactual model in SERAC. During the editing process, $g$ and $o$ are fine-tuned jointly. ", "page_idx": 20}, {"type": "text", "text": "GRACE. GRACE [10] utilizes a discrete KEY-VALUE codebook and maintains the codebook throughout the editing flow by adding, expanding, and splitting KEYs. During the inference phase, it retrieves the nearest KEY and determines whether to replace the activation of the hidden layer output. ", "page_idx": 20}, {"type": "text", "text": "A.3 Training Details and Hyperparameters ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Except for MEMIT-MASS, the batch size for all methods is consistently 1 in sequential editing scenarios. All experiments are conducted using 3 NVIDIA A800 GPUs, with all tasks reproducible on a single A800. Editing ZsRE takes approximately 4 hours, while Hallucination requires around 6 hours. To ensure fair comparisons, unless otherwise specified (for some methods like MEND, ROME, and MEMIT, we follow the original literature by selecting the last few layers or using causal analysis to identify the target layers), the default target layers for editing on LLaMA, Mistral, and GPT-J are model.layers[27].mlp.down_proj.weight, model.layers[27].mlp.down_proj.weight, and transformer.h[21].mlp.c_fc, respectively. ", "page_idx": 20}, {"type": "text", "text": "For FT-L, we utilize a reimplementation from ROME \u00b6, employing the Adam [83] optimizer with consideration of learning rates at 1e-5, 1e-4, and 5e-4, and conducting gradient descents for 50 iterations, ultimately reporting the best results at a learning rate of 5e-4. ", "page_idx": 20}, {"type": "text", "text": "For FT-EWC, we follow the reimplementation in GRACE and its default settings, setting the learning rate at 1e-2, the $\\lambda_{\\mathrm{ewc}}$ penalty factor at 0.1, and the number of replay instances at 10. ", "page_idx": 20}, {"type": "text", "text": "For the training phase of MEND, we adhere to the original paper, setting the learning rate at 1e-4, iterating 100K times, and employing early stopping at 30K, ultimately achieving an accuracy of 0.95 on the training set. Notably, we target the last few MLP layers as per the original literature, such as model.layers[i].mlp.down_proj.weight, model.layers[i].mlp.gate_proj.weight, model.layers[i].mlp.up_proj.weight in LLaMA, where $i\\in[29,30,31]$ . ", "page_idx": 20}, {"type": "text", "text": "For ROME and MEMIT, we follow the original literature on GPT-J using the default configurations, specifically the fifth layer and layers [3,4,5,6,7,8]. In LLaMA and Mistral, additional causal analysis is conducted to pinpoint the layers storing knowledge. As shown in Figure 8, an increasing trend in the Average Indirect Effect of the MLP is observed across layers [4,5,6,7,8], suggesting that the model recalls factual knowledge here and passes the matured token distribution via residual connections to the last MLP. Thus, in LLaMA and Mistral, ROME edits the fifth layer, while MEMIT edits layers [4,5,6,7,8]. ", "page_idx": 20}, {"type": "image", "img_path": "VJMYOfJVC2/tmp/9b0df21ab8a2cb901cd4bac1086680bb31683f74864d6fbdcd1307d374156c1a.jpg", "img_caption": ["Figure 8: Mid-layer MLPs play a crucial mediating role in LLaMA-2-7B and Mistral-7B. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "VJMYOfJVC2/tmp/c6eefb6d6a3eef26264a4cf8f3dfd2a2132a5fc69d6b9d0a9032a66aa95f8374.jpg", "img_caption": ["Figure 9: GPT-J-6B, ZsRE, continual editing. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "For DEFER, the original literature uses a learning rate of 1.0; however, we found it unfti for LLaMA and Mistral, with severe fluctuations in model loss. Therefore, we experiment with learning rates of 7e-5, 7e-4, and 1e-3, and ultimately report using 7e-5 (optimal). ", "page_idx": 21}, {"type": "table", "img_path": "VJMYOfJVC2/tmp/4f79c0c2bfab3e108693824bdce5a095b1fed0d10a2b6658a391832c7acecc03.jpg", "table_caption": ["Table 10: WISE hyper-parameters during editing and merging. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "For GRACE, we strictly follow the original literature, setting the learning rate at 1.0, and using replace_last to only replace the activation of the last token in autoregressive scenarios. After observing failures in generalization, we adjust various $\\epsilon_{\\mathrm{init}}$ values and discuss this more in Appendix B.1. ", "page_idx": 21}, {"type": "text", "text": "For WISE, the hyperparameters for the QA and Hallucination tasks are identical. We find that a learning rate of 1.0 with the SGD [84] optimizer is a good approach for stable training. The hyperparameters designed in the knowledge editing phase include the random masking probability $\\rho$ and the routing threshold guidance $\\alpha,\\beta,\\gamma$ . In the knowledge merging phase, hyperparameters include the number of merges $k$ and the merging weights $\\lambda$ for each MLP (we discuss the impact of $\\rho$ and $k$ in Section 3.3). Theoretically, as the importance of knowledge in any MLP is considerable, we always average with $\\lambda=1/k$ across all experiments. These are shown in Table 10. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "A.4 Pseudo Code of WISE ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The pseudo-code of the WISE editing stage is in Algorithm 1, and the one of the WISE inference stage is Algorithm 2. ", "page_idx": 21}, {"type": "text", "text": "B More Experimental Results and Analyses ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "B.1 On the Pitfall of GRACE: Generalization Collapses in Decoder-only LLMs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Here, we discuss why GRACE exhibits poor generalization when editing decoder-only LMs. As shown in Figure 10, we continuously edit 15 samples $(\\mathbf{x}_{e},\\mathbf{y}_{e})$ using GRACE and observe the nearest codebook Key for their paraphrases $\\mathbf{x}_{e^{\\prime}}$ and unrelated queries $\\mathbf{x}_{\\mathrm{loc}}$ , as well as the governed Deferral radii $\\epsilon$ of those Keys. When overlapping Keys exist, GRACE reduces the Deferral radii to split this Keys and then adds a new codebook entry, resulting in exponentially decaying of radii $\\epsilon$ during the editing process. Though $\\epsilon$ is initialized from a high $\\epsilon_{\\mathrm{init}}$ , it will be small and ineffective after continuous edits. From Figure 10, we observe that GRACE is more likely to have a conservative ", "page_idx": 21}, {"type": "table", "img_path": "VJMYOfJVC2/tmp/a8d73022f3df5331960832124fe6b99d3b9bcbfea75ce9f71392f6259d548885.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Algorithm 2: WISE Inference Stage Input: The edited LLM model $f_{\\Theta_{T}}$ , the activation threshold $\\epsilon$ , the test dataset $\\mathcal{D}_{\\mathrm{test}}$ , whether WISE-Retrieve. Output: The model\u2019s output. 1: for each query $\\mathbf{x}_{i}\\in\\mathcal{D}_{\\mathrm{test}}$ do 2: if WISE-Retrieve then 3: Get the value of activation $\\Delta_{\\mathrm{act}}=\\|\\mathcal{A}(\\mathbf{x}_{i})\\cdot(\\mathbf{W}_{v^{\\prime}}-\\mathbf{W}_{v})\\|_{2}$ for each side memory and select the one with the maximal value of $\\Delta_{\\mathrm{{act}}}$ ; 4: else 5: Get the value of activation $\\Delta_{\\mathrm{act}}=\\|\\mathcal{A}(\\mathbf{x}_{i})\\cdot(\\mathbf{W}_{v^{\\prime}}-\\mathbf{W}_{v})\\|_{2}$ ; 6: end if 7: if $\\Delta_{\\mathrm{act}}>\\epsilon$ then 8: Use the side memory $\\mathbf{W}_{v^{\\prime}}$ to generate the output as in Equation 6; 9: else 10: Use the main memory $\\mathbf{W}_{v}$ to generate the output as in Equation 6. 11: end if 12: end for ", "page_idx": 22}, {"type": "text", "text": "strategy that sets smaller Deferral radii during editing. Smaller Deferral radii will cause $\\mathbf{x}_{e^{\\prime}}$ to fail to hit the codebook (the distance to the nearest Key is farther than its Deferral radii) but let $\\mathbf{x}_{\\mathrm{loc}}$ successfully far away from the radii, resulting low generalization and high locality. Also, we observe that the Deferral radii method is not effective under any $\\epsilon_{\\mathrm{init}}$ ; for all tested $\\epsilon_{\\mathrm{init}}$ values of 1.0, 3.0, 10.0, and 500.0, they all have low generalization and high locality. ", "page_idx": 22}, {"type": "text", "text": "This suggests that in autoregressive LMs, the distribution of the last token cannot effectively represent semantics; whereas in encoder-only and encoder-decoder architectures, capturing semantic information through vector representation has been extensively studied [85\u201387]. This is consistent with the degree of generalization shown by GRACE when anchoring the T5 [88] Encoder layer. Some related works [89] also indicate that in autoregressive models, semantic similarity measures based on averages of output tokens underperform, recommending the use of score distributions over text continuations to represent semantic distances. ", "page_idx": 22}, {"type": "text", "text": "B.2 Impact of Knowledge Merging Strategies for WISE ", "text_level": 1, "page_idx": 22}, {"type": "image", "img_path": "VJMYOfJVC2/tmp/6de44319a7388e1a753fe3d3aff9717360886bd91870caee47d7b59bfd3ea972.jpg", "img_caption": ["Figure 10: Investigation on the query $\\mathbf{x}$ and its distance to the nearest Key $k$ , as well as the deferral radius $\\epsilon$ of that Key. Red and Blue respectively represent the paraphrase query $\\mathbf{x}_{e^{\\prime}}$ and the unrelated query $\\mathbf{x}_{\\mathrm{loc}}$ , with the hatch representing the radius of the nearest Key. We observe that when confilcts occur (hit the codebook Key but with different Edit Target $\\mathbf{y}_{e}^{},$ ), the deferral radius $\\epsilon$ decays exponentially. This results in GRACE being unable to encompass the paraphrase $\\mathbf{x}_{e^{\\prime}}$ and maintain high locality, regardless of how $\\epsilon_{\\mathrm{init}}$ is adjusted. ZsRE, LLaMA-2-7B. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Here, we conduct a more in-depth study of the knowledge merging strategies for WISE, exploring various merging approaches including $(i)$ Linear, which uses a simple weighted average; (ii) Slerp, which spherically interpolates the parameters of two models; (iii) Ties, a component used in the main experiments of this paper that resolves merging disturbances through TRIM ELECT SIGN; (iv) Dare: which follows a Bernoulli distribution to delete redundant parameters and rescale the remaining ones; (v) Dare_Ties, which combines dare and the sign consensus algorithm of TIES; and (vi) Sign, an ablation component of Ties that addresses directional confilcts\u2014all utilizing the official implementation from MergeKit [68] ||. We randomly sample 100 edits from ZsRE, retaining a fine-tuned MLP every 50 edits (merging 2 MLPs). As shown in Table 11, we observe that ignoring the direction of parameter updates (Linear, Slerp, Dare) leads to a significant decline in editing performance, underscoring the importance of addressing knowledge conflicts in overlapping parameters. The success of Sign also reaffirms this point. Meanwhile, the randomly masked knowledge shards exhibit a non-redundancy, indivisible nature. This is demonstrated by the significantly weaker performance of Dare_Ties compared to Ties/Sign, indicating that removing parameter updates can lead to the loss of edited knowledge or even potential \"anchors\". ", "page_idx": 23}, {"type": "table", "img_path": "VJMYOfJVC2/tmp/44d4d7e50b976102592d628aeb61975bebca6235ff9e579d5a8094a5984f4311.jpg", "table_caption": ["Table 11: Varying Merging Strategy. ZsRE. LLaMA-2-7B. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "B.3 Analysis of Retrieving Top-1 Activation ", "text_level": 1, "page_idx": 24}, {"type": "image", "img_path": "VJMYOfJVC2/tmp/10d1c7a7f661c3152931efc5cf682955890dcfa066f9f70af059d754e25d5440.jpg", "img_caption": ["Figure 11: Comparing editing results of WISE-{Retrieve, Retrieveoracle, Retrieve w. $\\operatorname{L}_{\\mathtt{m e m o}}\\}$ when varying $T$ . (a) shows the simple average of Rel. and Gen. (ES.), while (b) shows retrieval accuracy, i.e., whether the Top-1 Activation routes to the correct MLP (prec $\\ Uiriririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririririr$ ). $\\mathtt{X}$ -axis: Num edits. ZsRE. LlaMA-2-7B. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "WISE-Retrieve retains each knowledge-sharding memory and retrieves through Top-1 Activation. However, as shown in Table 6 and Figure 11b, the retrieval accuracy still has significant room for improvement; specifically, when $T$ reaches 3K, the accuracy of routing to the correct MLP drops to around $60\\%$ , indicating the specificity between side memories is insufficient. One possible reason is that when sampling the edits from a single dataset (ZsRE), the editing instances $(\\mathbf{x}_{e},\\mathbf{y}_{e})$ all belong to the same domain. This leads to some very similar instances being captured by multiple expert side memories (resulting in high activations for all side memories), introducing more retrieval failures. ", "page_idx": 24}, {"type": "text", "text": "Therefore, to improve the specificity of side memory and reduce the probability of routing errors, we attempt to add a new constraint $L_{\\mathrm{memo}}$ to Equation 5. For knowledge-sharding memory $\\mathbf{W}_{i}$ , we randomly replay instances $\\left(\\mathbf{x}_{\\mathrm{m}},\\mathbf{y}_{\\mathrm{m}}\\right)$ from the edit set $\\mathcal{D}\\mathbf{w}_{j}$ of past shard $\\mathbf{W}_{j,\\,j\\in[0,i-1]}$ , ensuring that $\\mathbf{W}_{i}$ remains inactive for $\\mathbf{x}_{\\mathrm{m}}$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\nL_{a}^{\\prime}=L_{a}+\\underbrace{\\operatorname*{max}(0,\\Delta_{\\mathrm{act}}(\\mathbf{x}_{\\mathrm{m}})-\\alpha)}_{L_{\\mathrm{memo}}},\\quad\\mathrm{s.t.~}\\mathbf{x}_{\\mathrm{m}}\\in\\mathcal{D}_{\\mathbf{W}_{j}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "As shown in Figure 11b, this replay behavior increases the specificity between side memories, maintaining nearly ${\\bf{88\\%}}$ retrieval accuracy at $T=3K$ . Figure 11a also shows that WISE-Retrieve $w_{\\ast}$ . $L_{\\mathrm{memo}}$ improves Edit Success (ES.) by ${\\bf8.39\\,\\%}$ compared to WISE-Retrieve, providing a promising direction for future work. With finer-grained activation management, we might be able to bridge the performance gap between Retrieve and Oracle. ", "page_idx": 24}, {"type": "text", "text": "B.4 Case Study ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In Table 12, we present bad cases of using WISE to edit the LLaMA-2-7B on the ZsRE dataset and mitigating these failures is critical for future work in model editing. We observe that in $i$ ) errors occur only in part of the tokens, and these errors constitute a large proportion of the bad cases, indicating that the edits have not been sufficiently ftited. ii) displays cases where the entire output is incorrect, and factual failures indicate difficulties in retaining memory of parameters for some rare entities (such as Persian iia, iib). iv) presents cases of generalization failure, for example in ivd), where the model answered \u201cEnglish\u201d but did not fully follow the ground truth, indicating significant room for improvement in the accuracy of generalized edits. Meanwhile, in iii) we surprisingly find that even when WISE errs on the Edit Prompt, it can correctly answer its paraphrase iiib) \u201cThe kind of voice of Gemma Bosini is what?\u201d. This indicates that WISE can handle contextual information correctly in some cases but falls short in specific editing instructions, suggesting that optimizing editing instructions (modifying the editing context) may be a direction for improvement. ", "page_idx": 24}, {"type": "table", "img_path": "VJMYOfJVC2/tmp/45b0ce5af50d9edda8d871111e61f83911632d2bd4f7dd68df3fc36a80b3cbc8.jpg", "table_caption": ["Table 12: Failure cases of using WISE to edit LLaMA-2-7B. \u2714represents errors in part of the tokens, \u2717represents complete output errors (i.e., factual failures), and \u2713indicates the expected exact match. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "B.5 Importance of Knowledge Anchor When Merging Models ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Table 13: Analysis of Merging $w.o$ . and $w_{\\ast}$ .   \n\"knowledge anchor\" (KA). $T=1000$ . ZsRE.   \nLLaMA-2-7B. ", "page_idx": 25}, {"type": "table", "img_path": "VJMYOfJVC2/tmp/1f3e02bb443f321896e688e2d3a84a05f4a34faa78f6360ff21740c5aa1747bf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Here, we discuss the effects of independent (ensured by non-overlapping masks) vs partially overlapping parameters within MLP subspaces on editing performance, as shown in Table 13. It is observable that, despite varying mask ratios $\\rho$ and the number of subspaces $k$ , partial overlap (w. KA) consistently outperforms independent configurations (w.o. KA) in terms of Reliability (Rel.) and Generalization (Gen.). For example, at $\\rho/k$ of $5/0.20$ , there is a relative improvement of $9\\%$ and $7\\%$ respectively. ", "page_idx": 25}, {"type": "text", "text": "This demonstrates that the overlapping regions contribute as \u201canchors\u201d for knowledge fusion, facilitating information transfer across different subspaces. Moreover, the shared parameters provide a natural regularization [90] mechanism, helping synchronize model behavior across different subspaces. ", "page_idx": 25}, {"type": "text", "text": "B.6 Ablation Study of Random Prefix Token ", "text_level": 1, "page_idx": 25}, {"type": "image", "img_path": "VJMYOfJVC2/tmp/1ba1db6f010503eae2943a55ca00d3440aba82db29e889681abe04ec391e6158.jpg", "img_caption": ["Figure 12: Ablation studies on Random Prefix Token (PT) of WISE. Light/Dark colors indicate the Editing Sucess w.o./w. PT addition. ZsRE. LlaMA-2-7B "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "As described in Section 3.1, we employ random prefix token augmentation to enable the editing knowledge to cope with various contexts. That is, for a single $\\mathbf{x}_{e}$ , it expands into $(\\mathrm{prefix}_{i},\\mathbf{x}_{e})$ . The prefix is derived from tokens that are randomly generated by the original LM $f_{\\Theta}$ , serving as an economical data augmentation method. We observe that the editing success rate is compromised (Figure 12). Specifically, for instance, at $\\mathrm{T}{=}1000$ , Rel. and Gen. decreased by 0.15 and 0.17, respectively. By utilizing randomly generated prefix tokens, the model is able to learn a broader range of linguistic features, thereby exhibiting greater robustness in practical applications. We believe that access to the \"data generator\" can deepen the model\u2019s memory of editing samples. ", "page_idx": 26}, {"type": "text", "text": "B.7 Parameter Efficiency ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The key to lifelong model editing is maintaining constant or slowly increasing computational costs as the number of edits expands. Here, we provide a quantitative analysis using LLaMA-2-7B as an example. Suppose we select model.layers[27].mlp.down_proj.weight as side memory. In that case, the theoretically added parameters are 11008 \u00d7 4096 \u00d7 4 = 0.18 GB, which accounts for ", "page_idx": 26}, {"type": "image", "img_path": "VJMYOfJVC2/tmp/3f6fb2d73199c560d51a2a4fa199e1c07ad03afae956ab8f5166e6a845266528.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 13: Computational costs. $0.64\\%$ of the original LLaMA\u2019s $7B\\times4=28$ GB (ignoring the VRAM required for input activations). As shown in Figure 13, in practice, WISE-Merge increases VRAM by $4\\%$ compared to the original LLaMA and remains constant over time. WISE-Retrieve, instead of merging, uses retrieval routing, meaning the computational cost increases over time, but this increase is gradual and can easily handle thousands or tens of thousands of inputs. Additionally, if we partially merge side MLPs (combining WISE-Retrieve and WISE-Merge), we can further reduce the computational demands of WISE-Retrieve. ", "page_idx": 26}, {"type": "text", "text": "C Proof of Theorem 2.1 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Theorem C.1 Subspace Overlap. Generate $k$ memory subspaces $\\mathbf{W}_{v^{\\prime}}^{i},i\\,\\in\\,[k]$ by random mask with 1\u2019s ratio $\\rho_{!}$ , so each memory has $\\rho\\cdot\\left|\\mathbf{W}_{v^{\\prime}}\\right|$ active trained parameters. For any two subspaces $\\mathbf{W}_{v^{\\prime}}^{i}$ and $\\mathbf{W}_{v^{\\prime}}^{j}\\;i\\neq j;i,j\\in[k].$ , there are $\\rho^{2}\\cdot|\\mathbf{W}_{v^{\\prime}}|$ active parameters that are overlapped. For all $k$ subspaces, there are $\\rho^{k}\\cdot|\\mathbf{W}_{v^{\\prime}}|$ overlapped active parameters. ", "page_idx": 26}, {"type": "text", "text": "Proof: We aim to prove the Subspace Overlap theorem by induction. ", "page_idx": 26}, {"type": "text", "text": "Let $\\mathbf{W}_{v^{\\prime}}^{i}$ represent the $i$ -th memory subspace generated by a random mask with a sparsity ratio of $\\rho$ , where $i\\in[k]$ . Each memory subspace $\\mathbf{W}_{v^{\\prime}}^{i}$ contains $\\rho\\cdot|\\mathbf{W}_{v^{\\prime}}|$ active trained parameters. ", "page_idx": 26}, {"type": "text", "text": "We start by considering the case of two memory subspaces, $\\mathbf{W}_{v^{\\prime}}^{i}$ and $\\mathbf{W}_{v^{\\prime}}^{j}$ , where $i\\neq j$ and $i,j\\in[k]$ . Let $P({\\mathrm{parameter~sampled}})=\\rho$ be the probability that a parameter is sampled in one mask generation event. ", "page_idx": 26}, {"type": "text", "text": "1. For a single mask generation, the probability that a specific parameter is sampled is $\\rho$ . We denote this probability as $P({\\mathrm{sampled}})=\\rho$ .   \n2. Considering two independent mask generation events, the probability that the same parameter is sampled in both masks is the product of their individual probabilities, i.e., $\\rho^{\\dot{2}}$ . This is derived from the independence of the events. Mathematically: $P({\\mathrm{sampled~in~both~masks}})=P({\\mathrm{sampled}})\\times P({\\mathrm{sampled}})=\\rho\\times\\rho=\\rho^{2}.$ ", "page_idx": 26}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "3. Extending this logic, for $k$ independent mask generation events, the probability that a specific parameter is sampled in all $k$ masks is $\\rho^{k}$ . Mathematically: ", "page_idx": 26}, {"type": "equation", "text": "$$\nP({\\mathrm{sampled~in~all~}}k{\\mathrm{~masks}})=\\underbrace{P({\\mathrm{sampled}})\\times P({\\mathrm{sampled}})\\times\\cdots\\times P({\\mathrm{sampled}})}_{k{\\mathrm{~times}}}=\\rho^{k}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now, let\u2019s calculate the number of parameters overlapped in two random masks: The total number of parameters in $\\mathbf{W}_{v^{\\prime}}$ is $|\\mathbf{W}_{v^{\\prime}}|$ . ", "page_idx": 26}, {"type": "text", "text": "Thus, the number of parameters overlapped in two random masks, $\\mathbf{W}_{v^{\\prime}}^{i}$ and $\\mathbf{W}_{v^{\\prime}}^{j}$ , is $\\rho^{2}\\cdot|\\mathbf{W}_{v^{\\prime}}|$ .   \nExtending this to $k$ random masks, the number of parameters overlapped in all $k$ masks is $\\rho^{k}\\cdot|\\mathbf{W}_{v^{\\prime}}|$ .   \nThis concludes the proof. ", "page_idx": 26}, {"type": "text", "text": "D Detailed Related Works ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Memory and Knowledge Injection of LLMs The memories of LLMs can be divided into longterm (episodic) memory and working memory (short-term) [24, 25, 27]. Long-term memory refers to the knowledge stored in the model\u2019s parameters, which can be updated by (re)pretraining [53], finetuning [59], and model editing [14]. Working memory is stored in sustained activations/representations of neurons, which will be awakened during inference time [24]. In-context learning (ICL) is a kind of working memory [60], also along with retrieval-based editing methods like GRACE [10]. How to reinforce memory and inject/update knowledge for LLMs is a fundamental question [28, 61, 62]. ICL or finetuning? Different works show different conclusions. In [62], the authors find that few-shot finetuning is more generalizable than ICL, especially for out-of-distribution data. In [61], the authors contrast finetuning with retrieval-augmented generation (RAG) in terms of knowledge injection and find that RAG is better in most cases, and combining both will produce the best results. However, finetuning and pretraining are computation-expensive [13, 10] and usually suffer from catastrophic forgetting [63] and overfitting [64]. For ICL and RAG, the working memory is sometimes not controllable, the model may not follow the information of the contexts [24], and the context window is limited [91, 92], and there are works addressing these issues by training controllable ICL [24], long-context [91, 92], and recurrent memory architecture design [28]. SPALM is proposed to add language models with storage modules that resemble both working and long-term memories [27]. ", "page_idx": 27}, {"type": "text", "text": "Model Editing of LLMs Model editing can be summarized as the following lines of research. Constrained finetuning: Preliminary model editing uses constrained finetuning to update parameters based on new examples [93, 94]. Locate-and-edit: ROME [18] locates the factual associations in autoregressive LLMs and conducts accurate and efficient edits by taking MLPs as key-value memories. Then, MEMIT [19] extends ROME from single-editing to mass-editing. COMEBA-HK [95] identifies the Local Editing Scope and extends MEMIT for sequential editing. In addition, T-Patcher [11] targets the last feed-forward layer of LLMs, adding an additional neuron for each edit. Meta learning: Recent meta-learning methods use hypernetworks for aiding editing. MEND [31] learns a hypernetwork that can decouple the finetuning gradients into the gradient updates that generalize the edits and won\u2019t damage the performances on unrelated inputs. To remedy the cancellation effect of MEND, MALMEN [15] uses hypernetwork to produce the weight shifts of editing and formulates the weight shift aggregation as the least square problem. Retrieval-based methods: Instead of directly editing the model parameters, retrieval-based methods aim to improve the working memory of LLMs to enable model editing. IKE [96] uses context-edit facts to guide the model when generating edited facts. DeCK [97] employs contrasting knowledge decoding, which enhances the confidence of in-context-based editors in the edited facts. SERAC [32] (a modified version dubbed as DEFER [10]) records edit items in a file and trains additional scope classifier and counterfactual model to detect, retrieve, and generate the edit-related results. Though the editing retriever and generator are neural networks, they are too small to have the power of LLMs. GRACE [10] adopts a discrete codebook of edits for retrieving and replacing the edits\u2019 layer representations during inference. From single editing [18] to mass editing [15, 19], and from static editing to sequential [11] (continual) or lifelong editing [10], model editing is developing to meet more realistic demands. ", "page_idx": 27}, {"type": "text", "text": "Continual Learning Continual learning [98, 99] tackles the catastrophic forgetting problem in deep learning models with new knowledge [100], and recent research has focused on various methods in this area. One such method is continual finetuning, where LLMs are refined over time with the arrival of new instances. For instance, a comprehensive study by [101] explores continual finetuning extensively. However, it has been observed that regularizing finetuning with continual learning techniques such as Elastic Weight Consolidation [20], Experience Replay [102], and Maximally Interfered Replay [103] can lead to a rapid decay in performance on previous tasks, although it aids in retaining some memory of past inputs. This suggests that editing, as opposed to vanilla continual finetuning, presents unique challenges, especially considering that edits are unlikely to be evenly distributed [104]. One promising direction within the realm of continual learning is the adoption of key-value methods, inspired by advancements in computer vision [105, 106]. Recent studies have showcased the effectiveness of continual prompt-learning for NLP [107, 108], particularly in applications like text retrieval [109]. Notably, discrete key-value methods have been shown to excel in handling shifting distributions [110], with some recent efforts extending their application to question answering [111]. These methods cache values to ensure that inputs remain within the distribution for downstream encoders, thus facilitating the incorporation of longer-term memory, provided there are adequate computational resources. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Abstract and Section 1 Introduction ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Section 5 Limitations ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Assumptions in Section 2.3.2 and Proofs in Appendix C Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We report the setup throughout the paper as well as in the Section 3.1 and Appendix A ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We use publicly available datasets (Appendix A), Code and Data are also provided in supplemental material. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: In Appendix A, we provide detailed descriptions of data splits and the proposed method\u2019s hyperparameters and baselines\u2019 hyperparameters. Additionally, in Section 3.3, we discuss how to select them for the proposed method. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The LLMs only have one checkpoint of the corresponding size, e.g., LLaMA-2-7B, so we only edit once for each setting. But we test our method and baselines under various models, settings, and datasets, therefore, the statistical significance of the experiments can be verified and supported. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: In Appendix A.3, B.7 and Section 3.3 ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We use publicly standard datasets that do not contain information about individual people or offensive context to our knowledge. Ethical considerations are discussed in Section 5. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: In Section 5 ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 31}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Section 3.1 and Appendix A. We use publicly available artifacts. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]