[{"figure_path": "dfiXFbECSZ/tables/tables_4_1.jpg", "caption": "Table 1: Test accuracy of LOFIT using Gemma-7B, Llama 2-7B, and Llama 2-13B against representation intervention baselines. Results are averaged over 2 random seeds and the best results are bolded. For ITI and LOFIT, we select 3% attention heads for each model. LOFIT outperforms baselines by a large margin across all settings on all models.", "description": "This table presents the test accuracy results of the LOFIT model compared to other representation intervention baselines (0-shot, ITI, RepE) across three different language models (Gemma-7B, Llama 2-7B, Llama 2-13B) and three different tasks (TruthfulQA, MQuAKE, CLUTRR).  The results are averaged over two random seeds, and the best-performing model for each setting is bolded.  For both ITI and LOFIT, only 3% of the attention heads were selected for intervention.", "section": "Results: Effectiveness of Localization"}, {"figure_path": "dfiXFbECSZ/tables/tables_5_1.jpg", "caption": "Table 2: Bias tuning accuracy using attention heads from LOFIT against other head selection methods. For TruthfulQA, we report MC1 accuracy. Best results are bolded. Fine-tuning the representations of LOFIT heads leads to consistently better performance than other head selection methods.", "description": "This table compares the accuracy of bias tuning using attention heads selected by different methods: LOFIT, random sampling, probing layers, bias-based selection, and ITI-heads.  It shows the accuracy achieved on three tasks: TruthfulQA (MC1 accuracy reported), MQUAKE (EM), and CLUTRR (EM), for three different language models: Gemma-7B, Llama 2-7B, and Llama 2-13B.  The results demonstrate that LOFIT consistently outperforms other head selection methods across all tasks and models.", "section": "5.1 Importance of LOFIT Heads"}, {"figure_path": "dfiXFbECSZ/tables/tables_7_1.jpg", "caption": "Table 1: Test accuracy of LOFIT using Gemma-7B, Llama 2-7B, and Llama 2-13B against representation intervention baselines. Results are averaged over 2 random seeds and the best results are bolded. For ITI and LOFIT, we select 3% attention heads for each model. LOFIT outperforms baselines by a large margin across all settings on all models.", "description": "This table presents the test accuracy results of the LOFIT model, along with three comparison baselines (0-shot, ITI, RepE) on three different language models (Gemma-7B, Llama 2-7B, Llama 2-13B).  The results are averaged over two random seeds, and the best-performing method for each metric is highlighted in bold.  For ITI and LOFIT, only 3% of the attention heads were modified. The table demonstrates that LOFIT significantly outperforms the baseline methods across all models and evaluation metrics.", "section": "5 Results: Effectiveness of Localization"}, {"figure_path": "dfiXFbECSZ/tables/tables_8_1.jpg", "caption": "Table 1: Test accuracy of LOFIT using Gemma-7B, Llama 2-7B, and Llama 2-13B against representation intervention baselines. Results are averaged over 2 random seeds and the best results are bolded. For ITI and LOFIT, we select 3% attention heads for each model. LOFIT outperforms baselines by a large margin across all settings on all models.", "description": "This table presents the test accuracy results for the LOFIT model compared to various representation intervention baselines on three different language models: Gemma-7B, Llama 2-7B, and Llama 2-13B.  The results are averaged across two runs, and the best-performing model is highlighted in bold.  For both Inference-Time Intervention (ITI) and LOFIT, only 3% of attention heads were selected for modification.  The table demonstrates that LOFIT consistently outperforms all baselines across various tasks and model sizes.", "section": "Results: Effectiveness of Localization"}, {"figure_path": "dfiXFbECSZ/tables/tables_9_1.jpg", "caption": "Table 5: Out-of-domain generalization performance of LOFIT on Llama 2-7B-Chat after fine-tuning on TruthfulQA. 0-shot prompts are used for OOD evaluation. \"No-FT\" represents the base model without being fine-tuned on TruthfulQA. In-domain evaluation results on TruthfulQA are also included for reference. Compared to PEFT methods, LOFIT better preserves the existing capabilities of the base model after being fine-tuned across all settings.", "description": "This table shows the out-of-domain generalization performance of LOFIT on Llama 2-7B-Chat after fine-tuning on TruthfulQA.  It compares LOFIT's performance on several out-of-domain question answering benchmarks (TriviaQA, NQ, MMLU) using zero-shot prompting, against its in-domain performance (TruthfulQA) and the performance of other methods (ITI, RED, LORA). The \"No-FT\" row shows the baseline performance of the model without any fine-tuning. The results highlight LOFIT's ability to maintain good performance on unseen tasks after fine-tuning on a specific task, unlike other methods that might suffer performance degradation on out-of-domain tasks.", "section": "5 Results: Effectiveness of Localization"}, {"figure_path": "dfiXFbECSZ/tables/tables_18_1.jpg", "caption": "Table 1: Test accuracy of LOFIT using Gemma-7B, Llama 2-7B, and Llama 2-13B against representation intervention baselines. Results are averaged over 2 random seeds and the best results are bolded. For ITI and LOFIT, we select 3% attention heads for each model. LOFIT outperforms baselines by a large margin across all settings on all models.", "description": "This table presents the test accuracy results for three different language models (Gemma-7B, Llama 2-7B, and Llama 2-13B) across three different tasks (TruthfulQA, MQuAKE, and CLUTRR).  The performance of LOFIT is compared against three baselines: 0-shot, ITI (Inference-time Intervention), and RepE (Representation Engineering). The results show that LOFIT significantly outperforms the baselines in all cases, demonstrating its effectiveness.", "section": "Results: Effectiveness of Localization"}, {"figure_path": "dfiXFbECSZ/tables/tables_19_1.jpg", "caption": "Table 1: Test accuracy of LOFIT using Gemma-7B, Llama 2-7B, and Llama 2-13B against representation intervention baselines. Results are averaged over 2 random seeds and the best results are bolded. For ITI and LOFIT, we select 3% attention heads for each model. LOFIT outperforms baselines by a large margin across all settings on all models.", "description": "This table compares the performance of LOFIT against three baselines (0-shot, ITI, and RepE) across three different language models (Gemma-7B, Llama 2-7B, and Llama 2-13B) on three different tasks (TruthfulQA, MQuAKE, and CLUTRR).  The results show that LOFIT significantly outperforms the baselines, demonstrating its effectiveness in improving LLM performance.", "section": "5 Results: Effectiveness of Localization"}, {"figure_path": "dfiXFbECSZ/tables/tables_19_2.jpg", "caption": "Table 1: Test accuracy of LOFIT using Gemma-7B, Llama 2-7B, and Llama 2-13B against representation intervention baselines. Results are averaged over 2 random seeds and the best results are bolded. For ITI and LOFIT, we select 3% attention heads for each model. LOFIT outperforms baselines by a large margin across all settings on all models.", "description": "This table presents the test accuracy results of the LOFIT model, compared against other representation intervention baselines (0-shot, ITI, RepE), across three different large language models (Gemma-7B, Llama 2-7B, Llama 2-13B) and three different downstream tasks (TruthfulQA, MQUAKE, CLUTRR).  Results are averaged over two random seeds for each model and task, with the best performance highlighted in bold.  A key aspect of the comparison is the consistent superior performance of LOFIT which uses only 3% of the attention heads in each model.", "section": "Results: Effectiveness of Localization"}, {"figure_path": "dfiXFbECSZ/tables/tables_21_1.jpg", "caption": "Table 1: Test accuracy of LOFIT using Gemma-7B, Llama 2-7B, and Llama 2-13B against representation intervention baselines. Results are averaged over 2 random seeds and the best results are bolded. For ITI and LOFIT, we select 3% attention heads for each model. LOFIT outperforms baselines by a large margin across all settings on all models.", "description": "This table presents the test accuracy results of the LOFIT model compared to other representation intervention baselines across three different language models (Gemma-7B, Llama 2-7B, and Llama 2-13B) on three tasks: TruthfulQA, MQuAKE, and CLUTRR.  The results are averaged over two random seeds, and the best-performing model is highlighted in bold.  For the ITI and LOFIT models, only 3% of the attention heads were selected for each language model. The table clearly shows that LOFIT significantly outperforms the baselines on all tasks and models.", "section": "5 Results: Effectiveness of Localization"}, {"figure_path": "dfiXFbECSZ/tables/tables_21_2.jpg", "caption": "Table 3: Test accuracy of LOFIT and state-of-the-art PEFT methods. Results are averaged over 2 random seeds and the best results are bolded. For LOFIT, we select 10% attention heads. With 20x-200x fewer learned parameters, LOFIT is comparable to PEFT models across models and even outperforms them in some settings.", "description": "This table compares the performance of LOFIT against other parameter-efficient fine-tuning (PEFT) methods such as LoRA, RED, and ReFT across various tasks and language models.  It highlights LOFIT's comparable performance despite using significantly fewer parameters (20x-200x fewer). The results demonstrate LOFIT's competitiveness even with substantially reduced parameter modifications.", "section": "6 Comparison with PEFT Methods"}, {"figure_path": "dfiXFbECSZ/tables/tables_22_1.jpg", "caption": "Table 1: Test accuracy of LOFIT using Gemma-7B, Llama 2-7B, and Llama 2-13B against representation intervention baselines. Results are averaged over 2 random seeds and the best results are bolded. For ITI and LOFIT, we select 3% attention heads for each model. LOFIT outperforms baselines by a large margin across all settings on all models.", "description": "This table compares the performance of LOFIT against other representation intervention methods (ITI and RepE) across three different language models (Gemma-7B, Llama 2-7B, and Llama 2-13B) on three different downstream tasks (TruthfulQA, MQuAKE, and CLUTRR).  The results show that LOFIT significantly outperforms the baseline methods on all tasks and across all models, even when using only 3% of the attention heads.", "section": "5 Results: Effectiveness of Localization"}, {"figure_path": "dfiXFbECSZ/tables/tables_24_1.jpg", "caption": "Table 1: Test accuracy of LOFIT using Gemma-7B, Llama 2-7B, and Llama 2-13B against representation intervention baselines. Results are averaged over 2 random seeds and the best results are bolded. For ITI and LOFIT, we select 3% attention heads for each model. LOFIT outperforms baselines by a large margin across all settings on all models.", "description": "This table presents the test accuracy results of the LOFIT model and its baselines (O-shot, ITI, and RepE) on three different datasets (TruthfulQA, MQUAKE, and CLUTRR) using three different language models (Gemma-7B, Llama 2-7B, and Llama 2-13B).  The results show that LOFIT significantly outperforms all the baselines across all datasets and models.  The performance is averaged over two random seeds, and the best results are highlighted in bold. For fair comparison, both LOFIT and ITI use 3% of the attention heads.", "section": "5 Results: Effectiveness of Localization"}]