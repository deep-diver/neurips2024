[{"figure_path": "wlLjYl0Gi6/figures/figures_2_1.jpg", "caption": "Figure 2: (a): HOL blocking was evaluated by comparing FCFS and SRTF scheduling policies across 1K requests. (b): Analysis revealed that higher Kendall\u2019s Tau correlation coefficients were associated with reduced latency. This finding was validated using the ShareGPT dataset with the Llama-3-8B model.", "description": "This figure shows the results of an experiment comparing different scheduling policies for LLM requests.  Subfigure (a) demonstrates the reduction in HOL blocking achieved by using SRTF (Shortest Remaining Time First) compared to FCFS (First Come First Served). Subfigure (b) displays the correlation between Kendall\u2019s Tau (a measure of rank correlation) and the latency. The higher the Kendall\u2019s Tau, the closer the predicted schedule is to the optimal SRTF schedule, resulting in lower latency. This demonstrates the effectiveness of using learning to rank for LLM scheduling.", "section": "1 Introduction"}, {"figure_path": "wlLjYl0Gi6/figures/figures_6_1.jpg", "caption": "Figure 3: Mean latency of different schedulers with Llama-3 models on real workloads.", "description": "This figure compares the mean latency of different request scheduling methods (FCFS, MLFQ, Perception Only, Classification, and the proposed Ranking method) using Llama-3 language models (8B and 70B parameters) on two real-world datasets (ShareGPT and LMSYS-Chat-1M).  The x-axis represents the request rate (requests per second), and the y-axis shows the mean latency in seconds per token.  The results demonstrate the effectiveness of the proposed ranking method in reducing latency compared to the baselines, especially at higher request rates where head-of-line blocking becomes more significant. The 70B parameter model shows greater improvements compared to the 8B model, which is expected given the increased capacity.", "section": "5.2 Chatbot Serving Scheduling"}, {"figure_path": "wlLjYl0Gi6/figures/figures_8_1.jpg", "caption": "Figure 4: Average max_waiting_time across all requests with different scheduling method", "description": "This figure compares the average maximum waiting time experienced by users across different scheduling methods (FCFS, Ranking, and Ranking with Starvation Prevention) under varying request rates.  It visualizes the impact of each scheduling strategy on user-perceived latency, particularly highlighting the effectiveness of the proposed ranking method, especially when combined with starvation prevention, in mitigating longer wait times for individual requests.", "section": "5.5 Effectiveness Analysis"}, {"figure_path": "wlLjYl0Gi6/figures/figures_8_2.jpg", "caption": "Figure 3: Mean latency of different schedulers with Llama-3 models on real workloads.", "description": "This figure compares the mean latency of four different request scheduling methods (FCFS, MLFQ, Perception Only, and Ranking (the proposed method)) using two different Llama-3 language models (8B and 70B parameters) and two real-world datasets (ShareGPT and LMSYS-Chat-1M). The x-axis represents the request rate (requests per second), and the y-axis shows the mean latency (seconds per token).  The results show that the proposed ranking method significantly outperforms the other methods in terms of latency, especially at higher request rates, demonstrating its effectiveness in reducing latency in real-world LLM serving scenarios.", "section": "5.2 Chatbot Serving Scheduling"}, {"figure_path": "wlLjYl0Gi6/figures/figures_13_1.jpg", "caption": "Figure 6: Finish Time of Requests with MLFQ Scheduler.", "description": "This figure shows the relationship between the finish time and output lengths of requests when using the MLFQ scheduler in the vLLM system. The x-axis represents the output lengths, and the y-axis represents the finish times. The plot shows distinct rectangular blocks, where the lengths of the blocks grow exponentially with the quantum growth rate. These blocks represent requests that were completed within queues of different priorities. When requests from higher-priority queues fail to occupy the entire sliding window, requests from lower-priority queues are then processed, resulting in the adjacent blocks. The plot also shows a linear increase in output lengths over time, representing requests that finished and left the system. Additionally, there are clear horizontal lines in the figure, which illustrate when batches of requests time out and are demoted simultaneously due to reaching multiples of the base quantum.", "section": "A Implementation of MLFQ"}, {"figure_path": "wlLjYl0Gi6/figures/figures_14_1.jpg", "caption": "Figure 2: (a): HOL blocking was evaluated by comparing FCFS and SRTF scheduling policies across 1K requests. (b): Analysis revealed that higher Kendall\u2019s Tau correlation coefficients were associated with reduced latency. This finding was validated using the ShareGPT dataset with the Llama-3-8B model.", "description": "Figure 2(a) compares the Head-of-Line (HOL) blocking of FCFS and SRTF scheduling policies across 1000 requests, showing that SRTF significantly reduces blocking.  Figure 2(b) shows a strong correlation between Kendall\u2019s Tau (a measure of rank correlation) and latency using Llama-3-8B on the ShareGPT dataset.  Higher Kendall\u2019s Tau values, indicating better alignment with an optimal (SRTF-like) schedule, result in lower latency.", "section": "1 Introduction"}]