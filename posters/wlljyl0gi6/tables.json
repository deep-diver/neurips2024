[{"figure_path": "wlLjYl0Gi6/tables/tables_7_1.jpg", "caption": "Table 1: Latency (s/token) with Burst of 2K requests", "description": "This table presents the mean and p90 latency in seconds per token for different scheduling methods (FCFS, MLFQ, PO, Classification, Ours) under a burst of 2000 requests.  The latency is measured for two different Llama-3 models (8B and 70B parameters) on two different datasets (ShareGPT and LMSYS-Chat-1M).  The \"Ours\" column represents the results of the proposed ranking-based scheduling method.", "section": "5.2 Chatbot Serving Scheduling"}, {"figure_path": "wlLjYl0Gi6/tables/tables_7_2.jpg", "caption": "Table 2: Throughput Improvement with Proposed Ranking Method", "description": "This table presents the results of experiments comparing the throughput of the proposed ranking-based scheduling method with the FCFS (First-Come, First-Served) method for synthetic data generation.  It shows the time taken to generate 1000 samples and the total number of samples generated within a 5-minute time limit for different LLM models (Llama-3-8B and Llama-3-70B) and datasets (ShareGPT and LMSYS-Chat-1M). The results demonstrate a significant improvement in throughput for the ranking-based method compared to FCFS, highlighting its effectiveness in optimizing the generation of shorter LLM responses.", "section": "5.3 Synthetic Data Generation Scheduling"}, {"figure_path": "wlLjYl0Gi6/tables/tables_8_1.jpg", "caption": "Table 3: Ranking prediction ability with different classification (Class. in table) settings (i.e., different bucket sizes) for Llama-3-70B. Lat. column shows the mean latency processing a burst of 2k requests for chatbot serving. Time column shows the time to generate 1k requests for synthetic data generation. Optimal Prediction is using the generation length of one random seed to predict the length of another seed. Note that the p-values of Kendall\u2019s Tau are below a given significance level (i.e., 1e-3) in all settings.", "description": "This table compares the performance of different methods for predicting the ranking of LLM generation lengths, using various bucket sizes for classification and the proposed ranking method. It shows the accuracy, Kendall's Tau correlation, mean latency in chatbot serving (processing 2k burst requests), and time to generate 1k synthetic data samples for each method. Optimal Prediction serves as a baseline representing the best achievable performance with perfect generation length knowledge.", "section": "5.4 Comparing Ranking Predictors"}, {"figure_path": "wlLjYl0Gi6/tables/tables_9_1.jpg", "caption": "Table 4: Overhead of Predictor Model", "description": "This table presents the overhead introduced by the ranking predictor model in processing 1000 requests.  It compares the overall time to process the requests to the time spent on the prefill stage (without the predictor) and the time taken by the predictor alone.  The final column shows the percentage of overhead introduced by the predictor relative to the overall processing time.", "section": "5.4 Comparing Ranking Predictors"}, {"figure_path": "wlLjYl0Gi6/tables/tables_15_1.jpg", "caption": "Table 5: Predictor's Sensitivity to Batch Size", "description": "This table shows the mean and variance of Kendall's Tau values obtained using different batch sizes for the ranking predictor model. The results indicate that the predictor's performance is relatively insensitive to the batch size, maintaining consistent accuracy across various batch sizes.", "section": "C Predictor's Sensitivity to Batch Size"}, {"figure_path": "wlLjYl0Gi6/tables/tables_15_2.jpg", "caption": "Table 6: Relationship Between the ListMLE Loss and the Kendall's Tau", "description": "This table shows the correlation between the ListMLE loss and Kendall's Tau during the training process of the ranking predictor model.  It demonstrates that as the ListMLE loss decreases (indicating improved model performance), Kendall's Tau tends to increase (indicating better correlation between predicted and actual rankings). This supports the claim that minimizing ListMLE loss leads to a better approximation of the ideal SJF/SRTF schedule.", "section": "5.4 Comparing Ranking Predictors"}, {"figure_path": "wlLjYl0Gi6/tables/tables_16_1.jpg", "caption": "Table 7: Relationship Between ListMLE Loss and Kendall's Tau", "description": "This table compares the performance of the proposed ranking method against the Oracle and the FCFS baseline.  The Oracle represents the ideal scenario with perfect knowledge of generation lengths.  The table shows Kendall's Tau, a measure of ranking correlation, and the resulting latency (in seconds per token).  The results demonstrate that the proposed method achieves a high correlation with the Oracle's ranking and significantly lower latency than FCFS.", "section": "5.2 Chatbot Serving Scheduling"}, {"figure_path": "wlLjYl0Gi6/tables/tables_16_2.jpg", "caption": "Table 8: Relationship Between ListMLE Loss and Kendall's Tau", "description": "This table shows the Kendall's Tau correlation coefficient achieved by using two different sizes of OPT models (125 million and 350 million parameters) as ranking predictors for the ShareGPT and LMSYS-Chat-1M datasets.  The Kendall's Tau score reflects the correlation between the predicted ranking of generation lengths and the actual ranking. A higher score indicates a better prediction.", "section": "5.4 Comparing Ranking Predictors"}, {"figure_path": "wlLjYl0Gi6/tables/tables_17_1.jpg", "caption": "Table 9: Influence of Correcting Mispredictions", "description": "This table compares the latency (in seconds per token) of the proposed ranking-based scheduling method with and without dynamically correcting mispredictions.  The results show minimal improvement from re-prediction, suggesting that the initial prediction is sufficiently accurate and that the overhead of re-prediction does not outweigh the benefit.", "section": "5.5 Effectiveness Analysis"}]