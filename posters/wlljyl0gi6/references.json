{"references": [{"fullname_first_author": "Woosuk Kwon", "paper_title": "Efficient memory management for large language model serving with pagedattention", "publication_date": "2023", "reason": "This paper introduces pagedattention, a key technique used in the current work's LLM serving system, and is directly relevant to the scheduler's performance."}, {"fullname_first_author": "Gyeong-In Yu", "paper_title": "Orca: A distributed serving system for {Transformer-Based} generative models", "publication_date": "2022", "reason": "Orca is a state-of-the-art LLM serving system that the authors integrate their scheduler with, providing a strong baseline for comparison and highlighting the practical implications of their work."}, {"fullname_first_author": "Tie-Yan Liu", "paper_title": "Learning to rank for information retrieval", "publication_date": "2009", "reason": "This foundational paper in learning to rank provides the theoretical background and context for the authors' approach, justifying their choice of method and demonstrating the relevance of their work to a broader field."}, {"fullname_first_author": "Susan Zhang", "paper_title": "OPT: Open pre-trained transformer language models", "publication_date": "2022", "reason": "The authors use an OPT model as their ranking predictor; hence, understanding OPT's properties and capabilities is crucial to evaluate the methodology and results."}, {"fullname_first_author": "Zangwei Zheng", "paper_title": "Response length perception and sequence scheduling: An Ilm-empowered Ilm inference pipeline", "publication_date": "2023", "reason": "This paper tackles the problem of predicting LLM generation length, which is closely related to the authors' learning-to-rank approach; comparing and contrasting these methods is essential to the analysis"}]}