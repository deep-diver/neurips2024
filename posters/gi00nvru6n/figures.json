[{"figure_path": "Gi00NVru6n/figures/figures_3_1.jpg", "caption": "Figure 1: An overall illustration of our proposed method. We perform singular value decomposition oriented by the covariance matrix to aggregate task context into the principle components (up), which are frozen for maintaining world knowledge (down left) or utilized to initialize the learnable adapter for better fine-tuning performance (down right). The dark-colored adapter refers to the components with the largest r singular values, while the light one is composed of the smallest r components.", "description": "This figure illustrates the CorDA method.  The top part shows the singular value decomposition (SVD) process using the covariance matrix to capture task context. The bottom left shows the knowledge-preserved adaptation, where the components with the smallest singular values are used to initialize a learnable adapter, while others are frozen to preserve world knowledge. The bottom right shows the instruction-previewed adaptation, where the components with the largest singular values are used to initialize a learnable adapter, focusing on enhancing fine-tuning performance. The color-coding of adapters highlights the distinction between frozen and learnable components.", "section": "Method"}, {"figure_path": "Gi00NVru6n/figures/figures_5_1.jpg", "caption": "Figure 2: Perplexity (lower is better) on (a) Wikitext-2 and (b) Penn TreeBank (PTB) after decomposing the LLaMA-2-7B weights and reconstruction discarding the smallest r singular values and their singular vectors. We compare our context-oriented decomposition (CO-SVD) with plain SVD and ASVD. The perplexity of plain SVD on PTB at r = 1024 is 763.4, which is out of the shown range.", "description": "This figure shows the perplexity results on Wikitext-2 and Penn TreeBank (PTB) datasets after applying different decomposition methods.  The x-axis represents the number of smallest singular values discarded during reconstruction of the LLaMA-2-7B model weights. The y-axis shows the perplexity. Three methods are compared: Plain SVD, ASVD, and the proposed CO-SVD. CO-SVD shows significantly better performance in maintaining low perplexity even after discarding a large number of singular values, demonstrating its ability to preserve important information during weight decomposition.  Note the scale difference between the two subplots.", "section": "4.1 Analysis of the Ability to Capture Context"}, {"figure_path": "Gi00NVru6n/figures/figures_5_2.jpg", "caption": "Figure 2: Perplexity (lower is better) on (a) Wikitext-2 and (b) Penn TreeBank (PTB) after decomposing the LLaMA-2-7B weights and reconstruction discarding the smallest r singular values and their singular vectors. We compare our context-oriented decomposition (CO-SVD) with plain SVD and ASVD. The perplexity of plain SVD on PTB at r = 1024 is 763.4, which is out of the shown range.", "description": "This figure compares three different methods (Plain SVD, ASVD, and CO-SVD) for decomposing the weights of a large language model (LLaMA-2-7B) and then reconstructing the model after discarding the smallest r singular values and their corresponding vectors.  The perplexity, a measure of how well the model predicts the next word in a sequence, is evaluated on two datasets (Wikitext-2 and Penn TreeBank).  The results show that CO-SVD is significantly more robust to the removal of singular values than the other methods, maintaining lower perplexity even when a large number of values are discarded.", "section": "4.1 Analysis of the Ability to Capture Context"}, {"figure_path": "Gi00NVru6n/figures/figures_7_1.jpg", "caption": "Figure 3: The training loss curves on MetaMath of full fine-tuning, LoRA, PISSA, and CorDA with (a) rank 128 and (b) rank 32. The corresponding accuracies on GSM8k and Math are reported on the legends. Smoothing is performed for the loss curves.", "description": "This figure shows the training loss curves for different parameter-efficient fine-tuning (PEFT) methods (LoRA, PiSSA, CorDA) and full fine-tuning on the MetaMath dataset.  Two different rank values (r=128 and r=32) for the low-rank adapters are compared.  The plot visualizes the convergence speed and final loss achieved by each method, with corresponding accuracies on GSM8k and Math datasets shown in the legend for better context.", "section": "4.3 Instruction-Previewed Adaptation Results"}, {"figure_path": "Gi00NVru6n/figures/figures_15_1.jpg", "caption": "Figure 4: Covariance visualization results for \u201cself_attn.k_proj\u201d, \u201cself_attn.o_proj\u201d, \u201cmlp.down_proj\u201d, and \u201cmlp.gate_proj\u201d weights in the 0-th layer. Please zoom in for a better view.", "description": "This figure visualizes the covariance matrices for four different weight matrices (self_attn.k_proj, self_attn.o_proj, mlp.down_proj, and mlp.gate_proj) within the 0th layer of the model.  The visualizations are heatmaps showing the covariance between different input dimensions.  The heatmaps are displayed for three different tasks: Math, NQ open, and TriviaQA.  Red circles highlight outlier patterns that are similar for question answering tasks (NQ open and TriviaQA) but different for the Math task, indicating that the covariance matrices capture task-specific context.", "section": "3.2 Context-Oriented Decomposition"}, {"figure_path": "Gi00NVru6n/figures/figures_16_1.jpg", "caption": "Figure 4: Covariance visualization results for \u201cself_attn.k_proj\u201d, \u201cself_attn.o_proj\u201d, \u201cmlp.down_proj", "description": "This figure visualizes the covariance matrices for different layers and weight types within a language model, using data from three tasks: Math, NQ open, and Trivia QA. The heatmaps show the covariance between different dimensions of the input activations.  The red circles highlight similar patterns observed in NQ open and Trivia QA, both question-answering tasks, indicating that the covariance matrices capture task-specific context. This context is leveraged in CorDA to guide weight decomposition and adapter initialization.", "section": "3.2 Context-Oriented Decomposition"}]