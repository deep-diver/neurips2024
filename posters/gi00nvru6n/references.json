{"references": [{"fullname_first_author": "J. Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2018-10-04", "reason": "BERT is a foundational large language model (LLM) that significantly influenced the field and is frequently used as a pre-trained model for parameter-efficient fine-tuning methods."}, {"fullname_first_author": "E. J. Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2022-00-00", "reason": "LoRA is a highly influential parameter-efficient fine-tuning method that this paper builds upon and aims to improve, hence its importance to the current work."}, {"fullname_first_author": "T. Kwiatkowski", "paper_title": "Natural questions: a benchmark for question answering research", "publication_date": "2019-00-00", "reason": "Natural Questions is a key benchmark dataset for question answering used in the paper to evaluate the preservation of world knowledge in the fine-tuned models."}, {"fullname_first_author": "K. Cobbe", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-03", "reason": "This paper introduces a benchmark dataset for evaluating LLMs' code generation capabilities, which is one of the tasks evaluated in the current paper."}, {"fullname_first_author": "M. Joshi", "paper_title": "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension", "publication_date": "2017-00-00", "reason": "TriviaQA serves as a significant dataset for evaluating the knowledge preservation capabilities of fine-tuned LLMs, thus highlighting its importance in the context of this research."}]}