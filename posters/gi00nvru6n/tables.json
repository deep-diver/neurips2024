[{"figure_path": "Gi00NVru6n/tables/tables_6_1.jpg", "caption": "Table 1: The experimental results of CorDA in the knowledge-preserved adaptation mode and comparison with full fine-tuning, LoRA, and PiSSA. LLaMA-2-7B is used to fine-tune on (a) Math, (b) Code, and (c) Instruction Following tasks. The rank r of LoRA, PiSSA, and CorDA is 128. CorDA is initialized with the NQ open samples to collect the covariance matrices. All methods are implemented by us under the same training and evaluation settings. The row of \u201cLLaMA-2-7B", "description": "This table presents the experimental results of CorDA (Context-oriented Decomposition Adaptation), a parameter-efficient fine-tuning method, compared against full fine-tuning, LoRA, and PiSSA. The experiments are performed on three tasks: Math, Code, and Instruction Following, using the LLaMA-2-7B language model.  The table shows the performance on these tasks, measured by metrics like accuracy and exact match scores, and also assesses the preservation of world knowledge using metrics from TriviaQA, NQ open, and WebQS.  The rank parameter (r) in LoRA, PiSSA, and CorDA is set to 128, and CorDA is initialized using samples from the NQ open dataset to generate covariance matrices. All experiments are conducted under the same training and evaluation parameters for a fair comparison.", "section": "4.2 Knowledge-Preserved Adaptation Results"}, {"figure_path": "Gi00NVru6n/tables/tables_6_2.jpg", "caption": "Table 1: The experimental results of CorDA in the knowledge-preserved adaptation mode and comparison with full fine-tuning, LoRA, and PiSSA. LLaMA-2-7B is used to fine-tune on (a) Math, (b) Code, and (c) Instruction Following tasks. The rank r of LoRA, PiSSA, and CorDA is 128. CorDA is initialized with the NQ open samples to collect the covariance matrices. All methods are implemented by us under the same training and evaluation settings. The row of \u201cLLaMA-2-7B", "description": "This table presents a comparison of the performance of different parameter-efficient fine-tuning (PEFT) methods on Math, Code, and Instruction Following tasks.  The methods compared are full fine-tuning, LoRA, PiSSA, and the proposed CorDA method. The table shows the number of trainable parameters for each method, as well as the performance on several metrics, including those measuring world knowledge (Trivia QA, NQ open, WebQS), and task-specific performance (GSM8k, Math, HumanEval, MBPP, MTBench). The CorDA model uses the knowledge-preserved adaptation mode, where it is initialized using samples from the NQ open dataset. The results reveal that CorDA achieves better performance on world knowledge preservation while maintaining good performance on task-specific benchmarks. The rank of LoRA, PiSSA and CorDA is fixed at 128.", "section": "4.2 Knowledge-Preserved Adaptation Results"}, {"figure_path": "Gi00NVru6n/tables/tables_6_3.jpg", "caption": "Table 1: The experimental results of CorDA in the knowledge-preserved adaptation mode and comparison with full fine-tuning, LoRA, and PiSSA. LLaMA-2-7B is used to fine-tune on (a) Math, (b) Code, and (c) Instruction Following tasks. The rank r of LoRA, PiSSA, and CorDA is 128. CorDA is initialized with the NQ open samples to collect the covariance matrices. All methods are implemented by us under the same training and evaluation settings. The row of \u201cLLaMA-2-7B", "description": "This table presents a comparison of the performance of CorDA (in knowledge-preserved mode) against full fine-tuning, LoRA, and PiSSA on three tasks: Math, Code, and Instruction Following.  It shows the performance (average score across different metrics) and the number of parameters used for each method.  The results highlight the trade-off between performance and the number of trainable parameters, which is a key aspect of parameter-efficient fine-tuning.", "section": "4.2 Knowledge-Preserved Adaptation Results"}, {"figure_path": "Gi00NVru6n/tables/tables_7_1.jpg", "caption": "Table 2: The experimental results of CorDA in the instruction-previewed adaptation mode on Math, Code, and Instruction Following tasks using LLaMA-2-7B. CorDA is initialized with samples from each of the fine-tuning datasets (MetaMathQA, CodeFeedback, and WizardLM-Evol-Instruct) for the three tasks, respectively. The rank r of LoRA, DORA, PiSSA, and CorDA is 128. All methods are implemented by us under the same training and evaluation settings.", "description": "This table presents a comparison of the performance of various parameter-efficient fine-tuning methods (PEFT) on three tasks: Math, Code, and Instruction Following.  The methods compared are full fine-tuning, LoRA, DORA, PiSSA, and the proposed CorDA.  The table shows the number of parameters used by each method, along with the performance metrics for each task. CorDA uses a context-oriented decomposition adaptation method, and results show it outperforms other methods on instruction following tasks.", "section": "4.3 Instruction-Previewed Adaptation Results"}, {"figure_path": "Gi00NVru6n/tables/tables_8_1.jpg", "caption": "Table 3: The experimental results of CorDA in the instruction-previewed adaptation mode on the GLUE benchmark using RoBERTabase. CorDA is initialized with samples from each of the fine-tuning datasets. The rank r of LoRA, DORA, and CorDA is 128. All methods are implemented by us under the same training and evaluation settings. Matthew\u2019s correlation and Pearson\u2019s correlation are the metrics of CoLA and STS-B, respectively. The metric of the other tasks is accuracy.", "description": "This table compares the performance of CorDA with full fine-tuning, LoRA, and DORA on the GLUE benchmark using RoBERTabase.  The instruction-previewed adaptation mode of CorDA was used, initialized with samples from each fine-tuning dataset.  The rank of all low-rank methods was 128.  The results show accuracy for most tasks, and Matthew's and Pearson's correlations for CoLA and STS-B, respectively.", "section": "4.3 Instruction-Previewed Adaptation Results"}, {"figure_path": "Gi00NVru6n/tables/tables_8_2.jpg", "caption": "Table 4: Ablation experiments of the data choice used to collect covariance matrices and the adapter building manner in the knowledge-preserved adaptation mode. \u2020: corresponds to the result of PiSSA that performs plain SVD and uses the largest r components to initialize the adapter.", "description": "This table presents ablation study results on the knowledge-preserved adaptation mode of CorDA. It compares the performance of CorDA using different data sources (Wikitext-2, TriviaQA, NQ open) to construct covariance matrices and different adapter building methods (plain SVD using largest or smallest r components, CO-SVD using smallest r components).  The results are evaluated based on TriviaQA, NQ open, WebQS, GSM8k, and Math datasets, providing insights into how different choices affect the model's performance on knowledge preservation and new task learning.", "section": "4.4 Discussions"}, {"figure_path": "Gi00NVru6n/tables/tables_8_3.jpg", "caption": "Table 5: The instruction following performance of CorDA using WizardLM-Evol-Instruct and Alpaca data to collect covariance matrices in the instruction-previewed adaptation mode.", "description": "This table presents the performance of the CorDA model on the MTBench benchmark for instruction following.  Two versions of CorDA are shown, one initialized using data from the WizardLM-Evol-Instruct dataset and the other using data from the Alpaca dataset. The table highlights the impact of different training data on the model's performance in this specific task.", "section": "4.3 Instruction-Previewed Adaptation Results"}, {"figure_path": "Gi00NVru6n/tables/tables_14_1.jpg", "caption": "Table 1: The experimental results of CorDA in the knowledge-preserved adaptation mode and comparison with full fine-tuning, LoRA, and PiSSA. LLaMA-2-7B is used to fine-tune on (a) Math, (b) Code, and (c) Instruction Following tasks. The rank r of LoRA, PiSSA, and CorDA is 128. CorDA is initialized with the NQ open samples to collect the covariance matrices. All methods are implemented by us under the same training and evaluation settings. The row of \u201cLLaMA-2-7B\u201d shows the world knowledge performance of the original pre-trained model.", "description": "This table presents a comparison of the performance of CorDA (in knowledge-preserved mode), LoRA, PiSSA, and full fine-tuning on three tasks: Math, Code, and Instruction Following.  It shows the performance on several metrics, including accuracy on the specific task and performance on world knowledge benchmarks (TriviaQA, NQ open, WebQS). The table highlights CorDA's ability to achieve comparable or better performance while maintaining world knowledge.", "section": "4.2 Knowledge-Preserved Adaptation Results"}]