{"references": [{"fullname_first_author": "Naman Agarwal", "paper_title": "Disentangling adaptive gradient methods from learning rates", "publication_date": "2020-02-11", "reason": "This paper provides foundational insights into adaptive gradient methods, which are crucial for understanding the behavior and performance of second-order optimizers like Shampoo."}, {"fullname_first_author": "Rohan Anil", "paper_title": "Scalable second-order optimization for deep learning", "publication_date": "2020-02-20", "reason": "This work introduces practical techniques for making second-order optimization more scalable, directly addressing the memory challenges tackled by the current paper."}, {"fullname_first_author": "Vineet Gupta", "paper_title": "Shampoo: Preconditioned stochastic tensor optimization", "publication_date": "2018-01-01", "reason": "This paper introduces the Shampoo optimizer, the core algorithm that the current work improves by using lower-bit precision."}, {"fullname_first_author": "Tim Dettmers", "paper_title": "8-bit optimizers via block-wise quantization", "publication_date": "2022-01-01", "reason": "This paper proposes quantization techniques for first-order optimizers, providing a foundation for the current paper's extension to second-order optimizers."}, {"fullname_first_author": "\u00c5ke Bj\u00f6rck", "paper_title": "An iterative algorithm for computing the best estimate of an orthogonal matrix", "publication_date": "1971-01-01", "reason": "This paper provides the Bj\u00f6rck orthonormalization algorithm, which is a key technique used in the current work to improve the approximation accuracy of the quantized eigenvector matrix."}]}