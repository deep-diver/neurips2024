[{"figure_path": "ASqdVeifn7/tables/tables_3_1.jpg", "caption": "Table 1: Quantization errors in A<sup>-1/4</sup> of different quantization schemes at a PD matrix A. We employ block-wise normalization with a block size of 64. U is the eigenvector matrix of A, QM = quantized matrix, and OR = orthogonal rectification.", "description": "This table presents a comparison of quantization errors for different methods of quantizing positive definite (PD) matrices and their inverses.  It shows the normwise relative error (NRE) and angle error (AE) resulting from quantizing either the full matrix A or its eigenvector matrix U, for both real-world and synthetic data. The results are shown for both 4-bit and 8-bit quantization.  Orthogonal rectification (OR) is also evaluated to further improve the accuracy of the quantized inverse.", "section": "3.1 Quantizing the Eigenvector Matrices"}, {"figure_path": "ASqdVeifn7/tables/tables_5_1.jpg", "caption": "Table 4: Performance and memory cost of training Swin-Tiny on CIFAR-100. TA = test accuracy and TMC = total GPU memory cost.", "description": "This table compares the test accuracy and total GPU memory cost of training the Swin-Tiny model on the CIFAR-100 dataset using different optimizers.  It specifically shows the performance of 32-bit and 4-bit versions of AdamW combined with K-FAC, AdaBK, and CASPR, highlighting the memory efficiency achieved by the 4-bit versions.", "section": "6 Related Work"}, {"figure_path": "ASqdVeifn7/tables/tables_7_1.jpg", "caption": "Table 2: Performance, wall-clock time and memory cost on various image classification tasks. TA = test accuracy, WCT = wall-clock time, and TMC = total GPU memory cost.", "description": "This table presents the results of image classification experiments using various models (VGG19, ResNet34, ViT-Small, Swin-Tiny, ResNet50, ViT-Base/32) and optimizers (SGDM, AdamW, and their 4-bit Shampoo counterparts) on three datasets (CIFAR-100, Tiny-ImageNet, ImageNet-1k). For each combination, it reports the test accuracy (TA), wall-clock time (WCT), and total GPU memory cost (TMC).  It allows for comparing the performance, speed, and memory efficiency of different optimizers, including the impact of 4-bit quantization.", "section": "5 Experiments"}, {"figure_path": "ASqdVeifn7/tables/tables_8_1.jpg", "caption": "Table 3: Ablation study on the impact of different quantization techniques to Swin-Tiny training on the CIFAR-100 dataset. U is the eigenvector matrix of a preconditioner A. QM = quantized matrix, OR = orthogonal rectification in Algorithm 1, TL = training loss, and TA = test accuracy.", "description": "This table presents an ablation study evaluating the effect of different quantization techniques on the performance of Swin-Tiny model training using the CIFAR-100 dataset. It compares the impact of quantizing the entire preconditioner matrix (A) versus only its eigenvector matrix (U), with and without orthogonal rectification (OR).  The results show training loss (TL) and test accuracy (TA) for both 4-bit and 3-bit quantization using different mapping methods (Linear-2 and DT).", "section": "3.1 Quantizing the Eigenvector Matrices"}, {"figure_path": "ASqdVeifn7/tables/tables_8_2.jpg", "caption": "Table 4: Performance and memory cost of training Swin-Tiny on CIFAR-100. TA = test accuracy and TMC = total GPU memory cost.", "description": "This table compares the test accuracy (TA) and total GPU memory cost (TMC) of training the Swin-Tiny model on the CIFAR-100 dataset using different optimizers. It shows the results for AdamW with 32-bit and 4-bit K-FAC, AdamW with 32-bit and 4-bit AdaBK, and AdamW with 32-bit and 4-bit CASPR.  The comparison highlights the memory efficiency achieved by using 4-bit optimizers while maintaining comparable performance to their 32-bit counterparts.", "section": "6 Related Work"}, {"figure_path": "ASqdVeifn7/tables/tables_14_1.jpg", "caption": "Table 1: Quantization errors in A-1/4 of different quantization schemes at a PD matrix A. We employ block-wise normalization with a block size of 64. U is the eigenvector matrix of A, QM = quantized matrix, and OR = orthogonal rectification.", "description": "This table presents a comparison of quantization errors for different methods of quantizing positive definite (PD) matrices.  It shows the normwise relative error (NRE) and angle error (AE) when using different quantization methods (DT, Linear-2) on both the original matrix (A) and its eigenvector matrix (U). Results are shown for 4-bit and 8-bit quantization, with and without orthogonal rectification (OR).  The goal is to evaluate which method minimizes errors when reducing the memory footprint of second-order optimizer states.", "section": "3.1 Quantizing the Eigenvector Matrices"}, {"figure_path": "ASqdVeifn7/tables/tables_14_2.jpg", "caption": "Table 1: Quantization errors in A<sup>-1/4</sup> of different quantization schemes at a PD matrix A. We employ block-wise normalization with a block size of 64. U is the eigenvector matrix of A, QM = quantized matrix, and OR = orthogonal rectification.", "description": "This table presents a comparison of quantization errors for different methods of quantizing a positive definite (PD) matrix and its eigenvector matrix.  The errors are evaluated using two metrics: normwise relative error (NRE) and angle error (AE).  Different quantization mappings (DT and Linear-2) and bit depths (4 and 8 bits) are compared. The results show that quantizing the eigenvector matrix (U) generally produces significantly lower errors compared to directly quantizing the PD matrix (A).  Orthogonal rectification (OR) further improves the results.", "section": "3.1 Quantizing the Eigenvector Matrices"}, {"figure_path": "ASqdVeifn7/tables/tables_15_1.jpg", "caption": "Table 1: Quantization errors in A-1/4 of different quantization schemes at a PD matrix A. We employ block-wise normalization with a block size of 64. U is the eigenvector matrix of A, QM = quantized matrix, and OR = orthogonal rectification.", "description": "This table presents a comparison of quantization errors for different methods applied to a positive definite (PD) matrix A.  The goal is to approximate the inverse fourth root of A (A\u207b\u00b9/\u2074) using different quantization techniques with varying bit precision and normalization schemes. The table evaluates the normwise relative error (NRE) and angle error (AE) for different quantization approaches, including quantizing the entire matrix A, its eigenvector matrix U, or combinations of both. The results are shown for both a real-world matrix and a synthetic matrix. It highlights the impact of quantizing eigenvectors versus quantizing the full matrix. Orthogonal rectification is also tested to see its effect on reducing errors.", "section": "3.1 Quantizing the Eigenvector Matrices"}, {"figure_path": "ASqdVeifn7/tables/tables_16_1.jpg", "caption": "Table 4: Performance and memory cost of training Swin-Tiny on CIFAR-100. TA = test accuracy and TMC = total GPU memory cost.", "description": "This table presents the test accuracy (TA) and total GPU memory cost (TMC) for training the Swin-Tiny model on the CIFAR-100 dataset using different optimizers.  It compares the performance of AdamW with 32-bit and 4-bit versions of K-FAC, AdaBK, and CASPR, highlighting the memory efficiency gains achieved by using 4-bit quantization.", "section": "6 Related Work"}, {"figure_path": "ASqdVeifn7/tables/tables_23_1.jpg", "caption": "Table 8: Performance and wall-clock time of training ResNet34 on the CIFAR-100 dataset with cosine learning rate decay. TA = test accuracy, and WCT = wall-clock time.", "description": "This table presents a comparison of the performance (test accuracy) and training time (wall-clock time) for training the ResNet34 model on the CIFAR-100 dataset using different optimizers. The optimizers compared include SGDM (Stochastic Gradient Descent with Momentum) with varying numbers of epochs, SGDM combined with 32-bit Shampoo, and SGDM combined with the proposed 4-bit Shampoo.  The results showcase the impact of the different optimizers and the number of training epochs on both accuracy and training efficiency.", "section": "H.1 Image Classification"}, {"figure_path": "ASqdVeifn7/tables/tables_24_1.jpg", "caption": "Table 9: Performance and wall-clock time of training on the CIFAR-100 dataset with cosine learning rate decay and schedule-free approach. ResNet34 is trained for 300 epochs and Swin-Tiny is trained for 150 epochs. TA = test accuracy, and WCT = wall-clock time.", "description": "This table compares the test accuracy (TA) and wall-clock time (WCT) of training ResNet34 and Swin-Tiny models on the CIFAR-100 dataset using different optimizers.  The optimizers compared are standard SGDM and AdamW, as well as versions employing schedule-free optimization techniques (SGDScheduleFree and AdamWScheduleFree). The results highlight the performance differences between these optimizers under the cosine learning rate decay schedule.", "section": "H.1 Image Classification"}, {"figure_path": "ASqdVeifn7/tables/tables_24_2.jpg", "caption": "Table 10: Performance, wall-clock time, and memory cost of training Swin-Tiny on the CIFAR-100 dataset. TA = test accuracy, WCT = wall-clock time, and TMC = total GPU memory cost.", "description": "This table presents the test accuracy (TA), wall-clock time (WCT), and total GPU memory cost (TMC) for training the Swin-Tiny model on the CIFAR-100 dataset using different optimizers: NadamW, AdamW + 32-bit Shampoo, AdamW + 4-bit Shampoo (the proposed method), Adagrad, Adagrad + 32-bit Shampoo, and Adagrad + 4-bit Shampoo.  It compares the performance and resource usage of different optimizers, highlighting the efficiency gains achieved by the 4-bit Shampoo.", "section": "H.1 Image Classification"}, {"figure_path": "ASqdVeifn7/tables/tables_25_1.jpg", "caption": "Table 11: Performance and memory cost of training ResNet34 on the CIFAR-100 dataset with cosine learning rate decay. All the optimizers are run for 200 epochs. TA = test accuracy, and TMC = total GPU memory cost.", "description": "This table shows the test accuracy (TA) and total GPU memory cost (TMC) of training ResNet34 on the CIFAR-100 dataset using different optimizers for 200 epochs. The optimizers compared are SGDM, M-FAC (m=32), SGDM + 32-bit Shampoo, and SGDM + 4-bit Shampoo (our). The table highlights the memory efficiency of the proposed 4-bit Shampoo.", "section": "5 Experiments"}, {"figure_path": "ASqdVeifn7/tables/tables_25_2.jpg", "caption": "Table 12: Performance, wall-clock time, and memory usage per GPU on natural language modeling tasks. VL = validation loss, WCT = wall-clock time, and TMC = total GPU memory cost.", "description": "This table presents the performance of different optimizers on natural language modeling tasks using LLAMA-130M and LLAMA-350M models on the C4 dataset and GPT2-124M on the OWT dataset. The metrics reported are validation loss (VL), wall-clock time (WCT), and total GPU memory cost (TMC).  The table compares the performance of AdamW with 32-bit and 4-bit versions of Shampoo, including a naive 4-bit implementation and the authors' improved 4-bit implementation.", "section": "H.2 Natural Language Modeling"}, {"figure_path": "ASqdVeifn7/tables/tables_26_1.jpg", "caption": "Table 13: Memory cost of training LLAMA2-7B on the C4 dataset with different optimizers. One A800 GPU with a maximum memory of 81,920 MB is enabled. TMC = total GPU memory cost, and OOM = out of memory.", "description": "This table presents the results of an experiment evaluating memory efficiency when training the large language model LLAMA2-7B using different optimizers. The experiment varied the batch size used in training and measured the total GPU memory consumption. The table compares the memory usage of 8-bit AdamW, 8-bit AdamW with 32-bit Shampoo, and 8-bit AdamW with 4-bit Shampoo (both the naive and improved versions from the paper).  The results show that the 4-bit Shampoo significantly reduces memory consumption compared to the 32-bit version, enabling training with larger batch sizes.", "section": "H.2 Natural Language Modeling"}]