[{"figure_path": "FOkKndty5B/tables/tables_6_1.jpg", "caption": "Table 1: Main results on FineAction-CGR benchmark. The column LoRA represents whether the LLM is fine-tuned fully or using LoRA. \u2020: Model is re-trained on the stage 3's data. B: B@4. M: METEOR. R: ROUGE. C: CIDEr.", "description": "This table presents the main results of the proposed SlowFocus method on the FineAction-CGR benchmark.  It compares the performance of various Video LLMs (VideoLLaMA, Video-ChatGPT, LLaMA-VID, VTimeLLM) against the proposed method, indicating whether LoRA (low-rank adaptation) was used for fine-tuning.  Metrics include temporal grounding (mIoU, R@0.3, R@0.5, R@0.7), temporal captioning (B, M, R, C), and temporal reasoning (Acc, Score). The results showcase the significant improvement achieved by the SlowFocus approach.", "section": "5.2 Main results"}, {"figure_path": "FOkKndty5B/tables/tables_6_2.jpg", "caption": "Table 2: Comparison with existing methods on coarse-grained video understanding benchmarks. Our method achieve on par performance with state-of-the-art models.", "description": "This table compares the performance of the proposed SlowFocus method against other state-of-the-art models on three widely used coarse-grained video understanding benchmarks: MSVD-QA, MSRVTT-QA, and ActivityNet-QA.  The metrics used for evaluation include Accuracy and Score, reflecting overall performance and detailed aspects like correctness, detail, context, temporal aspects and consistency.  The results show that SlowFocus achieves comparable performance to the best existing models, highlighting its effectiveness even on tasks that don't specifically target fine-grained temporal understanding.", "section": "4 FineAction-CGR benchmark"}, {"figure_path": "FOkKndty5B/tables/tables_7_1.jpg", "caption": "Table 3: Comparison with existing methods on MovieChat-1K.Table 4: Comparison with existing methods on EgoSchema.", "description": "This table presents a comparison of the proposed SlowFocus model's performance against other state-of-the-art models on two long video understanding benchmarks: MovieChat-1K and EgoSchema.  The results show that even without specific training on these long video datasets, SlowFocus achieves competitive performance, highlighting its robustness and generalizability for video understanding.", "section": "5 Experiments"}, {"figure_path": "FOkKndty5B/tables/tables_7_2.jpg", "caption": "Table 5: Components analysis. V<sub>L</sub> means only low-frequency frames are sampled. V<sub>L</sub>+V<sub>H</sub> represents performing mixed-frequency sampling and N<sub>H</sub> denotes the number of high-frequency frames.", "description": "This table presents the ablation study on different components of the SlowFocus mechanism.  It shows the impact of using only low-frequency sampling (V<sub>L</sub>), adding high-frequency sampling (V<sub>L</sub> + V<sub>H</sub>) with varying numbers of high-frequency frames (N<sub>H</sub>), the effect of the temporal encoder, and the impact of the multi-frequency mixing attention (MMA). The results are evaluated using mIoU, R@0.3, R@0.5, R@0.7 for temporal grounding and Accuracy and Score for temporal reasoning on the FineAction-CGR benchmark.", "section": "5 Experiments"}, {"figure_path": "FOkKndty5B/tables/tables_8_1.jpg", "caption": "Table 6: Ablation of training stages. The first stage itself yields poor result. Integrating these stages together results in the optimal performance.", "description": "This table shows the results of an ablation study on the training stages of the proposed SlowFocus method. It demonstrates that combining all three stages (modality alignment, boundary enhancement, and SlowFocus adaptation) yields the best performance, significantly outperforming models trained with only a subset of the stages.", "section": "5 Experiments"}, {"figure_path": "FOkKndty5B/tables/tables_9_1.jpg", "caption": "Table 8: Ablation study on temporal token space N.", "description": "This table presents the results of an ablation study on the size of the temporal token space (N) used in the SlowFocus mechanism.  It shows how changes in N affect the performance of the model across different metrics for temporal grounding, temporal captioning, and temporal reasoning.  The results demonstrate the optimal size for N to achieve best performance on fine-grained video understanding tasks. ", "section": "5.3 Ablations"}]