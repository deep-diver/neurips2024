[{"type": "text", "text": "On the Stability and Generalization of Meta-Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yunjuan Wang   \nDepartment of Computer Science   \nJohns Hopkins University   \nBaltimore, MD, 21218   \nywang509@jhu.edu   \nRaman Arora   \nDepartment of Computer Science   \nJohns Hopkins University   \nBaltimore, MD, 21218   \narora@cs.jhu.edu ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We focus on developing a theoretical understanding of meta-learning. Given multiple tasks drawn i.i.d. from some (unknown) task distribution, the goal is to find a good pre-trained model that can be adapted to a new, previously unseen, task with little computational and statistical overhead. We introduce a novel notion of stability for meta-learning algorithms, namely uniform meta-stability. We instantiate two uniformly meta-stable learning algorithms based on regularized empirical risk minimization and gradient descent and give explicit generalization bounds for convex learning problems with smooth losses and for weakly convex learning problems with non-smooth losses. Finally, we extend our results to stochastic and adversarially robust variants of our meta-learning algorithm. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Traditional machine learning algorithms excel at generalizing, but they often require extensive training data and assume that both training and test data come from the same distribution or task. In real-world scenarios, large sets of training data from a single task are often lacking. Instead, training data may stem from diverse tasks with shared similarities, while test data come from entirely new tasks. The challenge is to rapidly adapt to these unseen tasks without the need to train from scratch. ", "page_idx": 0}, {"type": "text", "text": "To address this challenge, meta-learning, also referred to as learning-to-learn, has emerged as an effective approach. Meta-learning has gained significant attention recently [Hospedales et al., 2021], with applications spanning across various domains including computer vision [Nichol et al., 2018] and robotics [Al-Shedivat et al., 2017], ranging from few-shot classification [Snell et al., 2017], hyperparameter optimization [Franceschi et al., 2018], to personalized recommendation systems [Wang et al., 2022]. ", "page_idx": 0}, {"type": "text", "text": "As the name suggests, meta-learning operates on two levels of abstraction to enhance learning over time. On an intra-task level, the learner needs to find models that perform well on individual tasks. On a meta-level, the learner needs to figure out useful meta-information, perhaps a prior over tasks, that relates different tasks and allows transferring and adaptation of knowledge to new unseen tasks efficiently (both in terms of statistical as well computational overhead). It is typical to represent such meta-information in the form of a pre-trained model, which we can represent using certain meta-parameters. Distinct from a standard setting, meta-learning involves training on a diverse set of tasks. At test time, we evaluate the performance of the pre-trained model on new unseen tasks while allowing it to adapt using a small sample on the test task. ", "page_idx": 0}, {"type": "text", "text": "An increasing body of empirical research is dedicated to advancing meta-learning algorithms, among which model-agnostic meta-learning (MAML) [Finn et al., 2017] stands out as a prominent approach. MAML is designed to find a good meta-parameter w which facilitates the learning of task-specific parameters through a single step of gradient descent. In particular, given a set of $m$ tasks denoted $\\{\\mathcal{D}_{j}\\}_{j=1}^{m}$ ,MAML estimatesthe meta-parameter as $\\begin{array}{r}{\\dot{\\mathbf{w}}\\,=\\,\\mathrm{argmin}_{\\mathrm{w}}\\,\\frac{1}{m}\\sum_{j=1}^{m}{\\cal L}(\\mathbf{u}_{j},\\mathcal{D}_{j}),}\\end{array}$ where task-specific parameters are computed as $\\mathbf{u}_{j}=\\mathbf{w}-\\eta\\nabla L(\\mathbf{w},\\mathcal{D}_{j})$ ", "page_idx": 0}, {"type": "text", "text": "However, a notable limitation of MAML is that it requires computing second-order derivatives, which is computationally demanding for deep neural networks in practical applications. This computational complexity also poses a challenge for a theoretical understanding of MAML, an aspect that remains largely under-explored. To mitigate this challenge, several MAML variants have been proposed, including first-order MAML [Finn et al., 2017], Reptile [Nichol et al., 2018], and iMAML [Rajeswaran et al., 2019]. Owing to its success, MAML has been used for robust adversarial meta-learning [Yin et al., 2018, Goldblum et al., 2020, Wang et al., 2021, Collins et al., 2020], differential private meta-learning [Li et al., 2019], and personalized federated learning [Chen et al., 2018, Fallah et al., 2020]. ", "page_idx": 1}, {"type": "text", "text": "Another popular framework for meta-learning is based on a \u201cproximal\u2019 update, wherein the task-specific parameter are iteratively learned by minimizing the empirical loss and an $\\ell_{2}$ regularizer [Denevi et al., 2018, Zhou et al., 2019, Denevi et al., 2019a, 2020, Jiang et al., 2021]. Given a task $\\mathcal{D}$ and a meta-parameter w, the task-specific parameter u are defined as $\\mathrm{~\\bf~u~}=$ $\\begin{array}{r}{\\mathrm{argmin}_{\\mathrm{u}}\\,L(\\mathbf{u};\\mathcal{D})+\\frac{\\lambda}{2}\\left\\lVert\\mathbf{u}-\\mathbf{w}\\right\\rVert^{2}}\\end{array}$ . This regularization strategy ensures that the task-specific parameter remains close to the meta-parameter. A similar strategy has been explored in other contexts. For example, Kuzborskij and Orabona [2017] study the problem of hypothesis transfer learning and show a fast rate on the generalization error of a task-specific parameter u returned by regularized empirical risk minimization conditioned on a good meta-parameter w. Yet, it remains unclear how to ensure finding such a good meta-parameter, provably. Relatedly, Denevi et al. [2019b] study stochastic gradient descent with biased regularization for linear model and incrementally update the bias (meta-parameter). Concurrently, Zhou et al. [2019] proposed the Meta-Prox algorithm as a generic stochastic metalearning approach. Specifically, given a set of meta-training tasks $\\mathcal{D}_{1},\\ldots,\\mathcal{D}_{m}$ , the meta-parameter wis estimated by solving $\\mathrm{min}_{\\mathrm{w}}$ $\\begin{array}{r}{\\sum_{j=1}^{m}\\operatorname*{min}_{\\mathbf{u}}L(\\mathbf{u},\\mathcal{D}_{j})+\\frac{\\lambda}{2}\\left\\|\\mathbf{u}-\\mathbf{w}\\right\\|^{2}}\\end{array}$ Zhouetal[2019 argue that Meta-Prox is a generalization of MAML since the gradient descent update in MAML can be viewed as taking the first-order Taylor expansion of the objective, [Zhou et al., 2019, Section 3.1]. ", "page_idx": 1}, {"type": "text", "text": "In this work, we adopt the framework of Zhou et al. [2019] to study meta-learning from a theoretical perspective.Given $m$ tasks drawn i.i.d. from some (unknown) task distribution $\\mu$ our goal is to find a good pre-trained model (the meta-parameter) which can be adapted to a new unseen task, drawn i.i.d. from $\\mu$ , at test time, using gradient descent. Our key contributions are as follows. ", "page_idx": 1}, {"type": "text", "text": "1. We introduce a_ novel notion  of stability for meta-learning algorithms, namely uniform meta-stability.For $\\bar{\\beta}$ uniformly meta-stable algorithm, we bound the generalization gapby $\\mathcal{O}(\\bar{\\beta}\\log\\left(m n/\\delta\\right)+\\sqrt{\\log\\left(1/\\delta\\right)/(m n)})$ ", "page_idx": 1}, {"type": "text", "text": "2. We consider two variants of task-specific learning - based on regularized empirical risk minimization (RERM) and gradient descent (GD) - within our meta-learning framework. We apply our stability-based analysis to these variants to learning problems with convex, smooth losses and weakly convex, non-smooth losses. Our results are summarized in Table 1. ", "page_idx": 1}, {"type": "table", "img_path": "J8rOw29df2/tmp/ae4db6199e423d3fb671364cafb9f34681dc32255d4778c324a5717095cf5574.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "Table 1: Bounds on uniform meta-stability $\\bar{\\beta}$ for different families of learning problems. Here, $\\eta$ is the step-size for GD for task-specific learning, $\\gamma$ is the step-size for GD for meta-parameter learning, $m$ is the number of tasks during training, $n$ is the number of training data for the task at test time. ", "page_idx": 1}, {"type": "text", "text": "3. We extend our results to stochastic and adversarially robust variants of our meta-learning algorithm. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Algorithmic Stability Analysis. In many machine learning problems, standard learning theoretic tools, such as uniform convergence, do not apply since the associated complexity measures are unbounded or undefined (e.g., nearest neighbor classification), or yield guarantees that are not meaningful. Stability-based analysis is an alternative approach for obtaining generalization bounds in such settings, introduced by Bousquet and Elisseeff [2002] and further developed in a long line of influential works [Elisseeff et al., 2005, Mukherjee et al., 2006, Shalev-Shwartz et al., 2010, Liu et al., 2017]. More recently, there have been significant breakthroughs in this field, with the work of Feldman and Vondrak [2018, 2019], Bousquet et al. [2020], Klochkov and Zhivotovskiy [2021], thereby improving the high probability bounds for uniformly stable learning algorithms beyond those established by Bousquet and Elisseeff [2002]. These results are complemented by Hardt et al. [2016], who provide the generalization bounds via algorithmic stability analysis of stochastic gradient for stochastic convex optimization with smooth loss functions. Subsequent work by Bassily et al. [2020] improves upon these results by removing the smoothness assumption, while Zhou et al. [2022], Lei [2023] advance the state-of-the-art by relaxing the convexity assumption. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Theoretical Guarantees for Meta-Learning.  There has been significant progress in understanding the theoretical aspects of meta-learning, both in terms of convergence guarantees [Fallah et al., 2019, Ji et al., 2020, Mishchenko et al., 2023] and the generalization guarantees. The first generalization analysis can be traced back to Baxter [2000], who assumed that all tasks are sampled i.i.d. from the same task distribution. Subsequent works have enriched the guarantees through various learning theoretic constructs, including VC theory [Ben-David and Schuller, 2003, Maurer, 2009, Maurer et al., 2016], information-theoretic tools [Chen et al., 2021, Jose and Simeone, 2021, Jose et al., 2021, Rezazadeh et al., 2021, Hellstrom and Durisi, 2022], PAC-Bayes framework [Pentina and Lampert, 2014, Amit and Meir, 2018, Rothfuss et al., 2021, Farid and Majumdar, 2021, Liu et al., 2021, Ding et al., 2021, Rezazadeh, 2022, Riou et al., 2023, Zakerinia et al., 2024], etc. Other works that do not rely on the task distribution assumption instead choose to get a handle on the bound by defining certain metrics to measure either the task similarity [Du et al., 2020, Tripuraneni et al., 2020, Guan and Lu, 2021] or the divergence between the new tasks and the training sample for the training tasks [Fallah et al., 2021]. Finally, several works focus on the online meta-learning setting, also referred to as the lifelong learning [Pentina and Lampert, 2014, Balcan et al., 2019, Denevi et al., 2019a,b, Meunier and Alquier, 2021]. ", "page_idx": 2}, {"type": "text", "text": "A prominent line of work, starting with that of Maurer [2005], focuses on giving theoretical guarantees for meta-learning via algorithmic stability analysis. More recently, Chen et al. [2020] establish connections between single-task learning with support/query (episodic) meta-learning algorithms, providing generalization gap of $\\mathcal{O}(1/\\sqrt{m})$ (where $m$ is the number of tasks) for smooth functions that is independent of the sample size $n$ - this was shown to be nearly optimal in Guan et al. [2022]. Subsequently, Fallah et al. [2021] show a bound of $\\mathcal{O}(1/m n)$ for strongly convex functions and by leveraging a new notion of stability. Al-Shedivat et al. [2021] extend the result of Maurer [2005] to practical meta-learning algorithms for Lipschitz and smooth losses. Farid and Majumdar [2021] derive a PAC-Bayes bound to address the qualitatively different challenges of generalization within the task compared to that at the meta-level. Other relevant work includes analyzing the stability of bilevel optimization [Bao et al., 2021] and federated learning [Sun et al., 2024] for smooth functions. ", "page_idx": 2}, {"type": "text", "text": "2 Problem Setup and Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation. Throughout the paper, we denote scalars and vectors with lowercase italics and lowercase bold Roman letters, respectively; e.g., $u$ , u. We work in a Euclidean space and use $\\left\\Vert\\cdot\\right\\Vert$ and $\\left\\|\\cdot\\right\\|_{2}$ to denote the $\\ell_{2}$ norm. We use $[n]$ to represent the set $\\{1,2,\\ldots,n\\}$ , and define ${\\mathrm{U}}[n]$ to be the uniform distribution over $[n]$ . Let $\\Pi_{\\mathcal{W}}$ be the Euclidean projection onto $\\mathcal{W}$ . We adopt the standard O-notation and use $\\lesssim$ and $\\scriptscriptstyle\\mathcal{O}$ interchangeably. We use O to hide poly-logarithmic dependence on the parameters. ", "page_idx": 2}, {"type": "text", "text": "Let $\\mathcal{X},\\mathcal{Y}$ denote the input and output spaces, respectively. Consider a supervised learning setting where each data point is denoted by $\\mathbf{z}\\,=\\,(\\mathbf{x},y)$ drawn from some unknown distribution $\\mathcal{D}$ over $\\mathcal{Z}\\,=\\,\\mathcal{X}\\,\\times\\,\\mathcal{Y}$ .We consider a hypothesis space $\\mathcal{H}$ (maps from $\\mathcal{X}\\rightarrow\\mathcal{Y}$ ) parameterized by w $\\in$ $\\mathcal{W}$ \uff0cwhere $\\boldsymbol{\\mathscr{W}}\\subseteq\\ \\mathbb{R}^{d}$ is a closed set with radius $D$ .Let $\\ell:\\,\\mathbb{R}^{d}\\,\\times\\,\\mathcal{Z}\\,\\,\\stackrel{}{\\to}\\,\\mathbb{R}^{+}$ denote the loss function. We say that a loss function $\\ell$ is $M$ -bounded if $\\forall\\mathbf{w}\\;\\in\\;\\mathcal{W},\\forall\\mathbf{z}\\;\\in\\;\\mathcal{D},\\ell(\\mathbf{w},\\mathbf{z})\\;\\leq\\;M;\\;\\ell$ is $\\mu$ -strongly convex if $\\forall\\mathbf{w}_{1},\\mathbf{w}_{2}\\;\\in\\;\\mathcal{W},\\forall\\mathbf{z}\\;\\in\\;\\mathcal{D}$ \uff0c $\\ell(\\mathbf{w}_{1},\\mathbf{z})\\;\\geq\\;\\ell(\\mathbf{w}_{2},\\mathbf{z})\\,+\\,\\langle\\nabla\\ell(\\mathbf{w}_{2},\\mathbf{z}),\\mathbf{w}_{1}-\\mathbf{w}_{2}\\rangle\\,+$ $\\frac{\\mu}{2}\\left|\\overline{{\\left|\\mathbf{w}_{1}-\\mathbf{w}_{2}\\right|}}\\right|_{2}^{2}$ ;if $\\mu=0$ , we say $\\ell(\\cdot,\\mathbf{z})$ is convex. We say $\\ell$ is $G$ -Lipschitz continuous if $\\forall\\mathbf{w}_{1},\\mathbf{w}_{2}\\in$ $\\begin{array}{r}{\\mathcal{W},\\forall\\mathbf{z}\\,\\in\\,\\mathcal{D},\\|\\ell(\\mathbf{w}_{1},\\mathbf{z})-\\ell(\\mathbf{w}_{2},\\mathbf{z})\\|_{2}\\,\\leq\\,G\\,\\|\\mathbf{w}_{1}-\\mathbf{w}_{2}\\|_{2};}\\end{array}$ $\\ell$ is $\\overline{{H}}$ -smooth if , $\\overline{{\\forall\\mathbf{w}_{1},\\mathbf{w}_{2}}}\\;\\in\\;\\mathcal{W},\\forall\\mathbf{z}\\;\\in\\;\\$ $D,\\|\\nabla\\ell(\\mathbf{w}_{1},\\mathbf{z})-\\nabla\\ell(\\mathbf{w}_{2},\\mathbf{z})\\|_{2}\\leq H^{-}\\|\\mathbf{w}_{1}-\\mathbf{w}_{2}\\|_{2}$ ", "page_idx": 2}, {"type": "text", "text": "In a standard (single-task) learning setup, given a model w, the expected loss on task $\\mathcal{D}$ and the empirical loss on a training sample $\\boldsymbol{S}$ drawn i.i.d. from $\\mathcal{D}$ , are defined, respectively, as follows. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L(\\mathbf{w},\\mathcal{D})=\\mathbb{E}_{\\mathbf{z}\\sim\\mathcal{D}}\\,[\\ell(\\mathbf{w},\\mathbf{z})];\\quad L(\\mathbf{w},S)=\\frac{1}{n}\\sum_{z\\in S}\\ell(\\mathbf{w},\\mathbf{z}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In a meta-learning framework, we consider distributions $\\{{\\mathcal{D}}_{j}\\}_{j=1}^{m}$ associated with $m$ different tasks that are drawn from some (unknown) task distribution $\\mu$ . For each task $j$ , we assume that the learner has access to n training examples dawn i.d. from D, ie., S, = {1 . We denote the cumulative training data as $\\mathbf{S}=\\left\\{S_{j}\\right\\}_{j=1}^{m}$ , and refer to it as the meta-sample. ", "page_idx": 3}, {"type": "text", "text": "A meta-learning algorithm $\\boldsymbol{\\mathcal{A}}$ takes the meta-sample $\\mathbf{S}$ as input and outputs an algorithm $A(\\mathbf{S})$ $(\\mathcal{X}\\overline{{\\times\\mathcal{Y})^{n}\\to\\mathcal{H}}}$ . The performance of the meta-algorithm $\\boldsymbol{\\mathcal{A}}$ is measured in terms of its ability to generalize w.r.t. loss $\\ell(\\cdot)$ to a new (previously unseen) task from the task distribution $\\mu$ ; we also refer to it as the transfer risk: ", "page_idx": 3}, {"type": "equation", "text": "$L(\\mathbf{\\cal{A}}(\\mathbf{S}),\\mu)=\\mathbb{E}_{\\mathbf{\\cal{D}}\\sim\\mu}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}L(A(\\mathbf{S})(S),\\mathcal{D}).$ ", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The goal of meta-learning is to learn a useful prior over tasks to help with rapid adaptation to new tasks. Formally, we pose the problem as learning a meta-model, parameterized by what we will refer to as meta-parameter w, that performs well on a variety of tasks. The hope is that the meta-parameter w can be adapted easily to a new task $\\mathcal{D}\\sim\\mu$ in particular, that a task-specific model u can be quickly learned from a task-specific training set $S\\sim{\\mathcal{D}}^{n}$ of size $n$ using the following proximal update: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{u}=\\operatorname*{argmin}_{\\mathbf{u}\\in\\mathcal{W}}L(\\mathbf{u},\\mathcal{S})+\\frac{\\lambda}{2}\\left\\|\\mathbf{u}-\\mathbf{w}\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\lambda>0$ is a regularization parameter. ", "page_idx": 3}, {"type": "table", "img_path": "J8rOw29df2/tmp/c8819f0a8f2829abf20dacc7e29731643b5d0da93034fe6f53ad7bd6a6574d9b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "The meta-parameter w itself is learned on the given meta-sample S by minimizing a regularized empirical loss averaged over tasks, where the regularization term penalizes the task-specific models in proportionto the $\\ell_{2}$ distance from the meta-parameter [Zhou et al., 2019]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{w}}=\\mathop{\\mathrm{argmin}}_{\\mathbf{w}\\in\\mathcal{W}}\\frac{1}{m}\\sum_{j=1}^{m}\\operatorname*{min}_{\\mathbf{u}\\in\\mathcal{W}}F_{\\mathcal{S}_{j}}(\\mathbf{u},\\mathbf{w}):=\\mathop{\\mathrm{argmin}}_{\\mathbf{w}\\in\\mathcal{W}}\\frac{1}{m}\\sum_{j=1}^{m}\\operatorname*{min}_{\\mathbf{u}\\in\\mathcal{W}}\\left[L(\\mathbf{u},\\mathcal{S}_{j})+\\frac{\\lambda}{2}\\left\\lVert\\mathbf{u}-\\mathbf{w}\\right\\rVert^{2}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The formulation above involves a bi-level optimization problem. The upper-level optimization involves finding the meta-parameter w which requires solving the lower-level optimization problem of finding task-specific model parameters $\\mathbf{u}$ _We consider both Gradient Descent (GD) as well Regularized Empirical Risk Minimization (RERM) for task-specific learning (see Algorithm 2 for more details); for meta-learning we employ a gradient descent method (see Algorithm 1). ", "page_idx": 3}, {"type": "text", "text": "We would like to bound the transfer risk in terms of the empirical multi-task risk: ", "page_idx": 3}, {"type": "text", "text": "To do so, we rely on the stability of the meta-learning algorithm. ", "page_idx": 3}, {"type": "text", "text": "Stability of Meta-Learning Algorithm. Given a meta-sample $\\mathbf{S}=\\left\\{S_{j}\\right\\}_{j=1}^{m}$ , define $\\mathbf{S}^{(j)}$ to be the meta-sample obtained by replacing the training samples ${\\mathbf{}}S_{j}$ for the $j$ -th task, in S, by another i.i.d. sample $S_{j}^{\\prime}\\sim{\\mathcal{D}}_{j}^{n}$ . We refer to ${\\bf S},{\\bf S}^{(j)}$ as neighboring meta-samples. For a task-specific training sample ${\\cal S}=\\left\\{{\\bf z}^{i}\\right\\}_{i=1}^{n}$ , let $S^{(i)}$ denote the training data obtained by replacing the $i$ -th example $\\boldsymbol{z}^{i}\\in\\mathcal{S}$ by another example $z^{\\prime}\\sim\\mathcal{D}$ drawn independently; we refer to $\\mathcal{S},\\mathcal{S}^{\\left(i\\right)}$ as neighboring samples. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.1 (Maurer [2005]). Suppose the meta-algorithm $\\boldsymbol{\\mathcal{A}}$ satisfies: ", "page_idx": 3}, {"type": "text", "text": "1. (Uniform Stability of Single-Task Learning) For any meta-sample S and any $\\mathcal{S},\\mathcal{S}^{\\left(i\\right)}$ ", "page_idx": 4}, {"type": "text", "text": "2. (Uniform Stability of Meta-Learning) For any ${\\bf S},{\\bf S}^{(j)}$ and any given training set $S\\sim{\\mathcal{D}}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|L(\\mathcal{A}(\\mathbf{S})(\\mathcal{S}),\\mathcal{S})-L(\\mathcal{A}(\\mathbf{S}^{(j)})(\\mathcal{S}),\\mathcal{S})\\right|\\le\\beta^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, for $M$ -bounded loss $\\ell$ , with probability at least $1-\\delta$ , we have that ", "page_idx": 4}, {"type": "equation", "text": "$$\nL(\\mathbf{\\boldsymbol{\\calA}}(\\mathbf{\\boldsymbol{\\mathsf{S}}}),\\mu)\\lesssim L(\\mathbf{\\boldsymbol{\\calA}}(\\mathbf{\\boldsymbol{\\mathsf{S}}}),\\mathbf{\\boldsymbol{\\mathsf{S}}})+(m\\beta^{\\prime}+M)\\sqrt{\\log\\left(1/\\delta\\right)/m}+\\beta.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Theorem 2.1 follows using a simple extension of arguments in Bousquet and Elisseeff [2002]. By utilizing sharper bounds tailored for uniformly stable algorithms [Bousquet et al., 2020], a tighter bound can be achieved, as demonstrated in Theorem 2.2 below. A similar result was shown in Guan et al. [2022] for episodic training algorithms (except there is no $\\beta$ ", "page_idx": 4}, {"type": "text", "text": "Theorem 2.2. Suppose the meta-algorithm $\\boldsymbol{\\mathcal{A}}$ satisfies the same conditions as shown in Theorem 2.1. Then for $M$ -bounded loss $\\ell$ , with probability at least $1-\\delta$ , we have that ", "page_idx": 4}, {"type": "equation", "text": "$$\nL(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mu)\\lesssim L(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mathbf{S})+\\beta^{\\prime}\\log{(m)}\\log{(1/\\delta)}+M\\sqrt{\\log{(1/\\delta)}/m}+\\beta.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3  Uniform Meta-Stability ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Motivated by prior work (i.e., Theorem 2.1 and the definitions therein), we introduce a new notion of stability which measures the sensitivity of the learning algorithm as we replace both a task in the meta-sample as well as a single training example available for the task at test time. ", "page_idx": 4}, {"type": "text", "text": "Definition (Uniform Meta-Stability). We say that a meta-learning algorithm $\\boldsymbol{\\mathcal{A}}$ is $\\bar{\\beta}$ uniformly meta-stable if for any neighbouring meta-samples S , $\\mathbf{S}^{(j)}$ , and neighboring samples $\\mathcal{S},\\mathcal{S}^{\\left(i\\right)}$ \uff0c for any task $\\overline{{D\\sim\\mu}}$ and any $z\\sim\\mathcal{D}$ , we have that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\ell(A(\\mathbf{S})(S),\\mathbf{z})-\\ell(A(\\mathbf{S}^{(j)})(S^{(i)}),\\mathbf{z})\\right|\\leq\\bar{\\beta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The definition above is rather natural. Intuitively, for a meta-learning algorithm to transfer well, we require that the learning algorithms, i.e., $A(\\mathbf{S})$ and $A(\\mathbf{S}^{\\prime})$ , returned on two neighboring meta-samples, when trained on two neighboring samples return models that predict similarly. Our first result bounds the generalization gap in terms of the uniform meta-stability parameter. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. Consider a meta-learning problem for some $M$ -bounded loss function $\\ell$ and task distribution $\\mu$ . Let S be a meta-sample consisting of training samples on $m$ tasks each of size $n$ , and let $s\\sim\\mathcal{D}$ be a sample of size $n$ on a previously unseen task $\\mathcal{D}\\sim\\mu$ . Then, for any $\\beta$ -uniformly meta-stable learning algorithm $\\boldsymbol{\\mathcal{A}}$ , we have that with probability $1-\\delta$ \uff0c ", "page_idx": 4}, {"type": "equation", "text": "$$\nL(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mu)\\lesssim L(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mathbf{S})+\\bar{\\beta}\\log\\left(m n\\right)\\log\\left(1/\\delta\\right)+M\\sqrt{\\log\\left(1/\\delta\\right)/\\left(m n\\right)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The result above is a direct analogue of Theorem 2.1 with stability parameters $\\beta,\\beta^{\\prime}$ bothsubsumed into a single meta-stability parameter. We do obtain a faster rate of convergence - as we instantiate concrete algorithms and specialize our results to specific problems in Section 4.1, we will see a notable improvement in rates from $1/\\sqrt{m}$ to $1/m$ ,for $n>m$ ", "page_idx": 4}, {"type": "text", "text": "We conclude the section by presenting an alternate notion of algorithmic meta-stability and a basic result that directly bounds the generalization gap for the meta-learning problem. ", "page_idx": 4}, {"type": "text", "text": "Definition (On-Average Meta-Stability). Let $\\mu$ be an (unknown) underlying task distribution. We say that a meta-learning algorithm $\\boldsymbol{\\mathcal{A}}$ is $\\bar{\\beta}$ -on-average-replace-one-meta-stable if ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathscr{E}_{\\mathbf{S}\\sim\\{\\mathscr{D}_{j}^{n}\\}_{j=1}^{m},(S_{j}^{\\prime},z_{j}^{\\prime})\\sim\\mathscr{D}_{j}^{n+1},\\{\\mathscr{D}_{j}\\}_{j=1}^{m}\\sim\\mu^{m},j\\sim\\mathrm{U}[m],i\\sim\\mathrm{U}[n]\\,\\left|\\ell(\\mathcal{A}(\\mathbf{S})(S_{j}),z_{j}^{i})-\\ell(\\mathcal{A}(\\mathbf{S}^{(j)})(S_{j}^{(i)}),z_{j}^{i})\\right|\\leq\\bar{\\beta}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2. Let $\\mu$ be an underlying task distribution. Given a meta-sample S, test task $\\mathcal{D}\\sim\\mu$ ,and $S\\sim{\\mathcal{D}}^{n}$ ,for any $\\bar{\\beta}$ -on-average-replace-one-meta-stable meta-learning algorithm $\\boldsymbol{\\mathcal{A}}$ we have that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{S}\\sim\\left\\{\\mathscr{D}_{j}^{n}\\right\\}_{j=1}^{m},\\left\\{\\mathscr{D}_{j}\\right\\}_{j=1}^{m}\\sim\\mu^{m}\\big[L(\\mathbf{\\mathscr{A}}(\\mathbf{S}),\\mu)-L(\\mathbf{\\mathscr{A}}(\\mathbf{S}),\\mathbf{S})\\big]\\le\\bar{\\beta}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4  Bounding Transfer Risk ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we consider a concrete meta-learning algorithm given in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "4.1 Convex and Smooth Losses ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We begin with meta-learning problems with convex, Lipschitz (and potentially smooth) losses. ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.1. Assume that the loss function $\\ell$ is convex and $G$ -Lipschitz loss. Let S, $\\mathbf{S}^{(j)}$ denote neighboring meta-samples and $\\boldsymbol{S}$ $S^{(i)}$ the neighboring samples on a test task. Then, the following holds for Algorithm 1 with RERM for task-specific learning (i.e., Option 1 for Algorithm 2) $\\forall T\\geq1$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbf{S},S,j\\in[m],i\\in[n]}\\left\\|A(\\mathbf{S})(S)-A(\\mathbf{S}^{(j)})(S^{(i)})\\right\\|\\leq\\frac{G}{\\lambda m}+\\frac{2G}{\\lambda n}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Further, if $\\ell$ is convex, $M$ -bounded and $H$ -smooth, then setting $\\begin{array}{r}{\\lambda\\geq H,\\gamma\\leq\\frac{1}{\\lambda}}\\end{array}$ , we have $\\forall T\\geq1$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbf{S},S,j\\in[m],i\\in[n]}\\left\\|\\mathcal{A}(\\mathbf{S})(S)-\\mathcal{A}(\\mathbf{S}^{(j)})(S^{(i)})\\right\\|\\leq\\frac{2\\sqrt{2H M}}{2\\lambda n-H}+\\frac{n}{2\\lambda n-H}\\frac{4\\sqrt{2H M}}{(m+1)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We can now use the result above with Theorem 3.1 to get the following bound on the transfer risk. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.2. The following holds for Algorithm 1 with step-size $\\gamma\\leq\\frac{1}{\\lambda}$ on a given meta-sample S, and RERM for task-specific learning (i.e., Option 1 for Algorithm 2), for all $T\\geq1$ ", "page_idx": 5}, {"type": "text", "text": "1. For convex, $M$ -bounded, and $G$ -Lipschitz loss functions, with probability at least $1-\\delta$ ", "page_idx": 5}, {"type": "equation", "text": "$$\nL(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mu)\\lesssim L(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mathbf{S})+\\left(\\frac{G^{2}}{\\lambda n}+\\frac{G^{2}}{\\lambda m}\\right)\\log\\left(m n\\right)\\log\\left(1/\\delta\\right)+\\frac{M\\sqrt{\\log\\left(1/\\delta\\right)}}{\\sqrt{m n}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "2. For convex, $M$ bounded, and $H$ -smooth lossfunctions $(H\\leq\\lambda)$ , with probability at least $1-\\delta$ ", "page_idx": 5}, {"type": "equation", "text": "$$\nL(\\boldsymbol{A}(\\mathbf{S}),\\mu)\\!\\lesssim\\!L(\\boldsymbol{A}(\\mathbf{S}),\\mathbf{S})\\!+\\!\\left(\\!\\frac{H M}{(2n-1)\\lambda}\\!+\\!\\frac{H M}{(m+1)\\lambda}\\right)\\log{(m n)}\\log{(1/\\delta)}\\!+\\!\\frac{M\\!\\sqrt{\\log{(1/\\delta)}}}{\\sqrt{m n}}\\!.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Next, we give analogous results for GD for task-specific learning (i.e., Option 2 for Algorithm 2), albeit for smooth loss functions. Lemma 4.3 bounds the output sensitivity of the meta-learning algorithm. We use it with Theorem 3.1 to give the generalization guarantee in Theorem 4.4. ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.3. Assume that the loss function is convex, $G$ -Lipschitz and $H$ -smooth. Let S, $\\mathbf{S}^{(j)}$ denote neighboring meta-samples and $\\boldsymbol{S}$ \uff0c $S^{(i)}$ the neighboring samples on a test task. Then the following holds for Algorithm 1 with GD for task-specific learning (i.e., Option 2 for Algorithm 2) with $\\begin{array}{r}{\\eta\\le\\frac{2}{H+2\\lambda}}\\end{array}$ ,forall $T\\geq1$ as long as we set $\\begin{array}{r}{\\gamma\\leq\\frac{1}{\\lambda T}}\\end{array}$ \uff0c ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbf{S},S,j\\in[m],i\\in[n]}\\left\\|A(\\mathbf{S})(S)-A(\\mathbf{S}^{(j)})(S^{(i)})\\right\\|\\leq\\frac{4e G}{\\lambda m}+\\frac{2G}{\\lambda n}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 4.4. Assume that the loss function is convex, $M$ -bounded, $G$ -Lipschitz and $H$ -smooth. Suppsewerun lgorihm 1 for $T$ iteraions with $\\begin{array}{r}{\\gamma\\leq\\frac{1}{\\lambda T}}\\end{array}$ on a given meta-sample $\\mathbf{S}$ and GD for task-specic laning (Optio 2 Algorithm 2) with $\\begin{array}{r}{\\eta\\le\\frac{2}{H+2\\lambda}}\\end{array}$ Then, wih probability a least $1-\\delta$ \uff0c ", "page_idx": 5}, {"type": "equation", "text": "$$\nL(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mu)\\lesssim L(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mathbf{S})+\\left(\\frac{G^{2}}{\\lambda m}+\\frac{G^{2}}{\\lambda n}\\right)\\log{(m n)}\\log{(1/\\delta)}+\\frac{M\\sqrt{\\log{(1/\\delta)}}}{\\sqrt{m n}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The results above show that meta-stable learning algorithms do not overfit. The bound on the generalization gap of $\\begin{array}{r}{\\tilde{\\mathcal{O}}(\\frac{1}{m}+\\frac{1}{n}+\\frac{1}{\\sqrt{m n}})}\\end{array}$ is tigte than what we would obtan using pror work. Indeed, we show that Theorem 2.2 yields a rate of $\\begin{array}{r}{\\tilde{\\mathcal O}(\\frac{1}{m}+\\frac{1}{n}+\\frac{1}{\\sqrt{m}})}\\end{array}$ (see Theorems C.2 and C.3 in Appendix), which is worse for all $m\\,\\leq\\,n^{2}$ : Notably, the bounds on the generalization gap are independent of the number of iterations of the meta learning Algorithm 1 and the number of iterations of GD for Algorithm 2. This holds since the objective we are minimizing is strongly convex (given the strongly convex regularizer), which ensures that the output sensitivity (in Lemmas 4.3 and 4.1 are independent of $T$ and $K$ . In itself, this should not be surprising since we only bound the generalization error in terms of the empirical error - the latter may not be small unless the algorithms have converged. To get a better handle on the generalization error we focus on excess (transfer) risk bounds in Section 4.3. But first we give a similar development for another important problem class. ", "page_idx": 5}, {"type": "text", "text": "Here, we focus on a more practical setting of learning problems with loss functions that are weakly convex and non-smooth. The notion of weak convexity is often used in non-convex optimization literature in a variety of problems including robust phase retrieval [Davis et al., 2020] and dictionary learning [Davis and Drusvyatskiy, 2019]; see Drusvyatskiy [2017] for an extended discussion. ", "page_idx": 6}, {"type": "text", "text": "Definition. A function $f(\\mathbf{w})$ is $\\rho$ -weakly convex w.r.t. $\\lVert\\cdot\\rVert$ if $\\begin{array}{r}{f(\\mathbf{w})+\\frac{\\rho}{2}\\left\\|\\mathbf{w}\\right\\|^{2}}\\end{array}$ is convex in w. ", "page_idx": 6}, {"type": "text", "text": "The class of weakly convex functions is contained within the larger class of non-smooth functions and semi-smooth functions [Mifflin, 1977]. It includes convex functions and smooth functions with Lipschitz continuous gradient as special cases; $\\rho<0$ implies that the function is strongly convex. An important example from a practical perspective is that of training over-parameterized two-layer neural networks with smooth activation functions using a smooth loss [Richards and Rabbat, 2021]. We first bound the sensitivity of Algorithm 1 for weakly convex and non-smooth losses. ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.5. Assume that the loss function is $\\rho$ -weakly convex and $G$ -Lipschitz. Let S, $\\mathbf{S}^{(j)}$ denote neighboring meta-samples and $\\boldsymbol{S}$ $S^{(i)}$ the neighboring samples on a test task. Then the following holds for Algorithm 1 with $\\lambda\\geq2\\rho$ , and GD for task-specific learning (i.e., Option 2 for Algorithm 2) with $\\begin{array}{r}{\\eta\\le\\frac{1}{\\lambda}}\\end{array}$ , for all $T\\geq1$ as long as we set $\\begin{array}{r}{\\gamma\\leq\\frac{1}{\\lambda T}}\\end{array}$ \uff0c ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda,\\mathbf{\\mu}_{\\operatorname{sut}}^{\\operatorname{av}\\operatorname{an}\\mathtt{a n}\\mathtt{\\mu}_{2}}\\subset\\frac{\\mathbf{\\mu}_{\\operatorname{av}\\operatorname{sun}\\mathtt{g}}\\mathbf{\\mu}_{\\operatorname{av}}\\operatorname{wer}\\mathbf{\\mu}_{1}}{\\operatorname{sup}}\\overset{,}{\\sim}\\lambda^{T},}\\\\ &{\\qquad\\operatorname*{sup}_{\\mathbf{S},\\mathcal{S},\\mathcal{J}\\in[m],i\\in[n]}\\left\\|\\mathcal{A}(\\mathbf{S})(S)-\\mathcal{A}(\\mathbf{S}^{(j)})(\\mathcal{S}^{(i)})\\right\\|\\le(8e G+2G)\\sqrt{\\frac{\\eta}{\\lambda}}+\\frac{8e G}{\\lambda m}+\\frac{8G}{\\lambda n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Using the result above in conjunction with $\\operatorname{Thm}3.1$ gives the following bound on the transfer risk. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.6. Assume that the loss function is $\\rho$ -weakly convex, $M$ -bounded, and $G$ -Lipschitz. Suppse werunAlgorithm for $T$ itertions with $\\begin{array}{r}{\\gamma\\le\\frac{1}{\\lambda T},\\lambda\\ge2\\rho}\\end{array}$ on a meta-sample S,andGD for task-specific learning (Option 2, Algorithm 2) with $\\begin{array}{r}{\\eta\\le\\frac{1}{\\lambda}}\\end{array}$ , Then, with probability at least $1-\\delta$ \uff0c ", "page_idx": 6}, {"type": "equation", "text": "$$\nL(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mu)\\lesssim L(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mathbf{S})+\\left(G^{2}\\sqrt{\\frac{\\eta}{\\lambda}}+\\frac{G^{2}}{\\lambda m}+\\frac{G^{2}}{\\lambda n}\\right)\\log\\left(m n\\right)\\log\\left(1/\\delta\\right)+\\frac{M\\sqrt{\\log\\left(1/\\delta\\right)}}{\\sqrt{m n}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proof of Theorem 4.6 follows from Lemma 4.5 and Theorem 3.1. A few remarks are in order. ", "page_idx": 6}, {"type": "text", "text": "For learning rate $\\gamma\\leq{\\frac{1}{\\lambda T}}$ , Theorem 4.6 gives arate of $\\begin{array}{r}{\\tilde{\\mathcal{O}}(\\sqrt{\\eta}+\\frac{1}{m}+\\frac{1}{n}+\\frac{1}{\\sqrt{m n}})}\\end{array}$ on the generalization gap,.This naturally sugests setting $\\begin{array}{r}{\\eta=\\frac{1}{\\lambda K}}\\end{array}$ ,where $K\\geq\\operatorname*{min}\\left\\{m,n\\right\\}$ isthe numberofiterationsof GD in task-specific learning. Then, similar to the discussion in Section 4.1, Theorem 4.6 gives a tighter bound, when $n>m$ , than those derived using prior work (Theorem 2.2); we refer the reader to Theorem D.4 in the appendix for further details. ", "page_idx": 6}, {"type": "text", "text": "Our proof technique shares similarities with Bassily et al. [2020]. However, our result is not a straightforward application of theirs as we deal with a bi-level optimization problem and focus on weakly convex functions. It is worth noting that our results for weakly convex non-smooth losses requireregularization parameter $\\lambda\\geq2\\rho$ , which can be chosen in practice using cross-validation. ", "page_idx": 6}, {"type": "text", "text": "The work most related to ours is that of Guan et al. [2022]. However, our results are fundamentally different from theirs in several aspects. Firstly, the algorithms we study are different. Guan et al. [2022] focus on support/query (S/Q) training strategies (aka episodic training) where each task ${\\mathcal{S}}_{j}$ is split into two non-overlapping parts - the support set $S_{j}^{t r}$ for training the task-specific parameter and the query set $\\mathcal{S}_{j}^{t s}$ for measuring the algorithm's performance [Vinyals et al., 2016]. The metaparameter is learned by minimizing the loss computed over the query set._ Such S/Q training strategy is popular for modern gradient-based meta-learning algorithm such as MAML for few-shot learning [Fin et al, 2017], where the optimization objective can be writen as $\\begin{array}{r}{\\operatorname*{min}_{\\mathrm{w}}\\frac{1}{m}\\sum_{j=1}^{m}L(\\mathrm{w}-}\\end{array}$ $\\nabla L(\\mathbf{w},S_{j}^{t r}),S_{j}^{t s})$ . One notable limitation is that Guan et al. [2022] assume that the loss function on the task level, e.g.. $R(\\mathbf{w},S_{j})_{.}=L(\\mathbf{w}\\!-\\!\\nabla L(\\mathbf{w},S_{j}^{t r}),S_{j}^{t s})$ , is convex or (Holder) smooth. Such an assumption is highly impractical, as demonstrated by [Mishchenko et al., 2023, Theorem 1, Theorem 2], which provides several counterexamples where $L$ is convex and smooth but $R$ is neither convex nor smooth. In contrast, we directly deal with $L$ being weakly convex and nonsmooth. Our approach requires a more involved proof that deals with stability of bi-level optimization. This is in stark contrast with Guan et al. [2022] who directly reduce the meta-learning problem to a single-task learning problem without considering the bi-level structure of the problem. ", "page_idx": 6}, {"type": "text", "text": "The work of Fallah et al. [2021] proposed a notion of stability similar to ours. The difference is that they consider S/Q training and define the stability by changing a mini-batch of samples in $S_{j}^{t r}$ as well as a single sample in $S_{j}^{t s}$ . Moreover, their focus is primarily on strongly convex losses. They discuss generalization to training tasks and unseen tasks separately, as they do not assume all tasks are sampled from the same task distribution. Another related work of Guan and Lu [2021] present a generalization bound of $O({\\sqrt{C/m n}})$ under a task relatedness assumption, where $C$ captures the logarithm of the covering number of hypothesis class that possibly depends on the dimension $d$ More recently, Riou et al. [2023] provide generalization bounds with a fast rate of $\\textstyle{\\mathcal{O}}({\\frac{1}{m}}+{\\frac{1}{n}})$ , albeit under an additional extended Bernstein's condition. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.3  Excess Transfer Risk ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In the previous sections, we focused on establishing that meta-stable rules do not overfit to the meta-sample. In this Section, we focus on the question of whether meta-learning Algorithm 1 can achieve a small generalization error, i.e., are they guaranteed to transfer well on unseen tasks? We show that by focusing on the computational aspects, i.e., by bounding the optimization error in terms of the number of iterations. Furthermore, we give bounds on excess risk, wherein the benchmark is the performance of the best possible in-class predictor. ", "page_idx": 7}, {"type": "text", "text": "Let $\\begin{array}{r}{\\mathbf{u}_{*}=\\mathop{\\mathrm{argmin}}_{\\mathbf{u}\\in\\mathcal{W}}L(\\mathbf{u},\\mathcal{D})}\\end{array}$ $\\begin{array}{r}{\\mathbf{u}_{j}^{*}=\\operatorname*{argmin}_{\\mathbf{u}\\in\\mathcal{W}}L(\\mathbf{u},S_{j}),}\\end{array}$ $\\forall j\\in[m]$ be the optimal task-specific hypotheses for the unseen task and the given training tasks, respectively. Given a meta-algorithm $\\boldsymbol{\\mathcal{A}}$ the excess transfer risk can be decomposed as follows: ", "page_idx": 7}, {"type": "image", "img_path": "J8rOw29df2/tmp/f450c8b232922ebd26e27dc8f8d58fc068d257699b6af168b6a0f7ea24625ea6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "To control excess risk, we need to bound $\\mathcal{E}_{\\mathrm{gen}}(A)$ and $\\mathcal E_{\\mathrm{opt+app}}(A)$ simultaneously. The bounds on the first term are presented in the previous section. Here, we focus on analyzing the second term. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.7. Assume that the loss $\\ell$ is convex and $G$ -Lipschitz. Define $\\begin{array}{r}{\\mathbf{u}_{j}^{*}\\!=\\!\\operatorname*{argmin}_{\\mathbf{u}}L(\\mathbf{u},S_{j}),\\forall j\\in}\\end{array}$ $[m]$ . Suppose we run Algorithm 1 for $T$ iterations with step-size $\\begin{array}{r}{\\gamma=\\frac{1}{\\lambda T}}\\end{array}$ , and using GD for taskspecific learning (i.e., Option 2 for Algorithm 2), to find an algorithm $\\mathcal{A}(\\ddot{\\mathbf{S}})=\\mathcal{A}_{\\mathrm{task}}(\\mathbf{w}_{T+1},\\cdot)$ which is then run on ${\\mathcal{S}}_{j}$ for $K$ iterations with step-size $\\begin{array}{r}{\\eta\\le\\frac{1}{2\\lambda}}\\end{array}$ . Then, we have that ", "page_idx": 7}, {"type": "equation", "text": "$$\nL(\\boldsymbol{\\mathcal{A}}(\\mathbf{S})(S_{j}),S_{j})-\\operatorname*{inf}_{\\mathbf{u}}L(\\mathbf{u},S_{j})\\lesssim\\frac{D^{2}}{\\eta K}+G^{2}\\eta+G D\\eta\\lambda+\\lambda\\left\\|\\mathbf{w}_{T+1}-\\widehat{\\mathbf{w}}\\right\\|^{2}+\\lambda\\sigma^{2}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\widehat{\\bf w}$ is defined in Equation (1). Here $\\begin{array}{r}{\\sigma^{2}:=\\frac{1}{m}\\sum_{j=1}^{m}\\left\\|\\widehat{\\mathbf{w}}\\!-\\!\\mathbf{u}_{j}^{*}\\right\\|^{2}}\\end{array}$ is the approximation error, and $\\begin{array}{r}{\\left\\|\\mathbf{w}_{T+1}-\\widehat{\\mathbf{w}}\\right\\|^{2}\\lesssim\\frac{1}{T}(D^{2}+\\frac{D^{2}}{\\lambda\\eta K}+\\frac{\\eta(G+2\\lambda D)^{2}}{\\lambda})}\\end{array}$ m(G+2)D)2) is the optimization error. ", "page_idx": 7}, {"type": "text", "text": "Finally, to bound the excess transfer risk for convex and non-smooth losses, we use Theorem 4.6 with Theorem 4.7 to get that in expectation over the sampling of data (meta-sample S and sample $\\boldsymbol{S}$ ", "page_idx": 7}, {"type": "text", "text": "$\\operatorname{g}[\\mathcal{E}_{\\mathrm{mix}}(A)]\\leq\\operatorname{\\mathbb{E}}[\\mathcal{E}_{\\mathrm{gen}}(A)]+\\operatorname{\\mathbb{E}}[\\mathcal{E}_{\\mathrm{opt+app}}(A)]\\lesssim G^{2}\\sqrt{\\frac{\\eta}{\\lambda}}+\\frac{G^{2}}{\\lambda m}+\\frac{G^{2}}{\\lambda n}+\\frac{D^{2}}{\\eta K}+G^{2}\\eta+G D\\eta\\lambda+\\frac{\\lambda D^{2}}{T}+\\eta(G+2\\lambda D)^{2}+1$ \u5165\u3002\u00b2.", "page_idx": 7}, {"type": "text", "text": "By properly choosing step size $\\begin{array}{r}{\\eta=\\mathcal{O}\\left(\\frac{1}{\\lambda K^{2/3}}\\right)}\\end{array}$ , we obtain that the expected excesstransfersk decays at a rate of $\\begin{array}{r}{\\mathcal{O}(\\frac{1}{\\lambda K^{1/3}}+\\frac{1}{\\lambda m}+\\frac{1}{\\lambda n}+\\frac{\\lambda}{T}+\\lambda\\sigma^{2})}\\end{array}$ . Similarly, for convex, Lipschitz and smooth losses, applying Theorm 4.4with Thorm 4.7and selg $\\begin{array}{r}{\\eta=\\mathcal{O}(\\frac{1}{\\lambda\\sqrt{K}})}\\end{array}$ results i an expected excess ransfe rsk of $\\begin{array}{r}{\\mathcal{O}(\\frac{1}{\\lambda\\sqrt{K}}+\\frac{1}{\\lambda m}+\\frac{1}{\\lambda n}+\\frac{\\lambda}{T}+\\lambda\\sigma^{2})}\\end{array}$ . Therefore, as $K,T,m,n$ tend to infinity, the excess risk converges to $\\sigma^{2}$ .As $\\sigma$ represents the average distance between the optimal task-specific parameters $\\mathbf{u}_{j}$ 's and the optimal estimated meta-parameter $\\widehat{\\bf w}$ , the excess risk is small when $\\sigma$ is small. It is also typical to set the regularization parameter $\\lambda$ inversely proportional to the sample size $n$ (e.g., $\\lambda=\\mathcal{O}(1/\\bar{\\sqrt{n}}))$ ", "page_idx": 7}, {"type": "text", "text": "Denevi et al. [2019a] study the same algorithm as ours except in the online setting. However, the function classes they consider are limited to compositions of linear hypothesis classes with convex and closed losses. In contrast, our work considers a broader range of functions, encompassing not only convex, Lipschitz, and smooth functions but also weakly-convex and non-smooth functions. The bound onexpctdexcess rsk shwn inDenevit al 2019al takes thfm $\\begin{array}{r}{{\\mathcal{O}}(\\frac{\\operatorname{Var}_{m}}{\\sqrt{n}}+\\frac{1}{\\sqrt{m}})}\\end{array}$ , where $\\mathrm{Var}_{m}$ captures the relatedness among the tasks sampled from the task environment. Unfortunately, this bound relies on a specific choice of $\\begin{array}{r}{\\lambda\\,=\\,\\mathcal{O}\\left(\\frac{1}{\\mathrm{Var}_{m}}\\sqrt{\\frac{\\log(n)}{n}}\\right)}\\end{array}$ which depends on $\\mathrm{Var}_{m}\\mathrm{~-~}\\mathbf{a}$ quantity that is often not known a priori in practice. To compare with our work, set $K=n$ $T=m$ $\\bar{\\eta}=\\mathcal{O}(1/\\sqrt{n})$ , and $\\lambda={\\mathcal{O}}(1/{\\sqrt{n}})$ .Then, applying Theorem 4.4 with Theorem 4.7, we obtain that $\\begin{array}{r}{\\mathbb{E}[\\mathcal{E}_{\\mathrm{risk}}(A)]\\lesssim\\frac{\\sqrt{n}}{m}+\\frac{\\operatorname*{max}(1,\\sigma^{2})}{\\sqrt{n}}}\\end{array}$ Considering both $\\mathrm{Var}_{m}$ and $\\sigma$ as costats the houndon cxpeted excess risk based on our analysis is tighter than that of Denevi et al. [2019a] when $n\\lesssim m$ , a common setting studied in meta-learning framework. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We also conduct a simple experiment to empirically verify the tightness of our generalization bounds, which we defer to Appendix A due to space limitations. ", "page_idx": 8}, {"type": "text", "text": "5   Implications of the Generalization Bounds ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Next, we present stochastic and adversarially robust variants of the meta-learning Algorithm 1. ", "page_idx": 8}, {"type": "text", "text": "5.1  Proximal Meta-Learning with Stochastic Optimization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We adapt Algorithm 1 to utilize sampling-with-replacement where at each iteration we process the training set of a single task; see Algorithm 3 for more details. We show that with high probability the sensitivity of this stochastic meta-learning algorithm is bounded. ", "page_idx": 8}, {"type": "text", "text": "Lemma 5.1. Assume that the loss function is $\\rho$ -weakly convex and $G$ -Lipschitz. Let S, $\\mathbf{S}^{(j)}$ denote neighboring meta-samples and $\\boldsymbol{S}$ $S^{(i)}$ the neighboring samples on a test task. Then, with probability at least $1\\!-\\!\\exp{\\left(-T^{2}\\dot{e}^{2}/m^{2}\\right)}$ , the following holds for Algorithm 3 with $\\lambda\\geq2\\rho$ , and GD for taskspecific learning(i.e., Option 2 for Algorithm 2) with $\\begin{array}{r}{\\eta\\le\\frac{1}{\\lambda}}\\end{array}$ , for all $T\\geq1$ as long as we set $\\begin{array}{r}{\\gamma\\leq\\frac{1}{\\lambda T}}\\end{array}$ \uff0c ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbf{S},S,i\\in[n],j\\in[m]}\\Big\\|A(\\mathbf{S})(S)-A(\\mathbf{S}^{(j)})(S^{(i)})\\Big\\|\\leq(8e G+2G)\\sqrt{\\frac{\\eta}{\\lambda}}+\\frac{8e G}{\\lambda m}+\\frac{8G}{\\lambda n}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "5.2  Robust Adversarial Proximal Meta-Learning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We consider inference-time adversarial attacks with a general threat model $B:\\mathcal X\\rightarrow$ $2^{\\mathcal{X}}$ :Specifically, given an input example $\\mathbf{\\boldsymbol{x}}\\in\\dot{\\mathcal{X}},\\dot{\\mathcal{B}}(\\mathbf{\\boldsymbol{x}})\\subseteq\\mathbf{\\dot{\\mathbb{R}}}^{d}$ represents the set of all possible perturbations of $\\mathbf{X}$ that an adversary can choose from. This includes the typical examples such as the $L_{p}$ threat models that are often considered in practice, or a discrete set of designed transformations. ", "page_idx": 8}, {"type": "text", "text": "Given a model parameter w, let $\\tilde{\\ell}(\\mathbf{w},\\mathbf{z})\\,=$ $\\mathrm{max}_{\\widetilde{\\mathbf{z}}\\in B(\\mathbf{z})}\\,\\ell(\\mathbf{w},\\widetilde{\\mathbf{z}})$ denote the adversarial loss. We adapt the standard meta-learning framework simply by considering the robust variant, $\\tilde{\\ell}$ .of the standard loss $\\ell$ We denote the robust transfer risk and empirical robust multi-task risk as $L_{\\mathrm{rob}}(\\mathcal{A}(\\bar{\\mathbf{S}}),\\mu)$ and ", "page_idx": 8}, {"type": "table", "img_path": "J8rOw29df2/tmp/b568dcf46aa539dacede75c2f2446d17f9aceb5840cd9ba41a7510163800ab92.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "$L_{\\mathrm{rob}}(\\mathcal{A}(\\mathbf{S}),\\mathbf{S})$ . Now, given meta-sample S, the goal is to learn a robust prior (e.g., a pre-trained model) for rapid adaptation to and robust generalization on new tasks. We adopt the framework presented in Section 2 except we use robust loss for task-specific training; indeed, using GD (Option 2) on robust loss in Algorithm 2 yields adversarial training. We use Algorithm 1 for meta-learning. We now relate a loss function with its adversarially robust counterpart. ", "page_idx": 8}, {"type": "text", "text": "Proposition 5.2. Given a loss function $\\ell(\\cdot,\\mathbf{z})$ and its adversarial counterpart $\\tilde{\\ell}(\\cdot,z)$ , the following holds: (1) If $\\ell$ is $G$ -Lipschitz (in its first argument), then $\\tilde{\\ell}$ is $G$ -Lipschitz. (2) $\\tilde{\\ell}$ is not $H$ -smooth even $\\ell$ .s $H$ -smooth. (3) If $\\ell$ .s $H$ -smooth in w, then $\\tilde{\\ell}$ .s $H$ -weakly convex in w. ", "page_idx": 8}, {"type": "text", "text": "Using the result above with Theorem 3.1 yields the following bound on robust (transfer) risk. ", "page_idx": 8}, {"type": "text", "text": "Corollary 5.3. Assume that the loss $\\ell$ is $M$ -bounded and $H$ -smooth. Suppose we run Algorithm 1 for $T$ iterations with $\\begin{array}{r}{\\gamma\\le\\frac{1}{\\lambda T},\\eta\\le\\frac{1}{\\lambda},\\lambda>2H.}\\end{array}$ and wherein task-specifc learning Agorithm 2 (GD) is invoked with robust loss $\\tilde{\\ell}$ , we have that with probability at least $1-\\delta$ $L_{\\mathrm{rob}}(\\boldsymbol{A}(\\mathbf{S}),\\mu)\\!\\lesssim\\!L_{\\mathrm{rob}}(\\boldsymbol{A}(\\mathbf{S}),\\mathbf{S})\\!+\\!\\left(G^{2}\\sqrt{\\frac{\\eta}{\\lambda}}\\!+\\!\\frac{G^{2}}{\\lambda m}\\!+\\!\\frac{G^{2}}{\\lambda n}\\right)\\!\\log\\left(m n\\right)\\log\\left(1/\\delta\\right)\\!+\\!\\frac{M\\sqrt{\\log\\left(1/\\delta\\right)}}{\\sqrt{m n}}.$ ", "page_idx": 9}, {"type": "text", "text": "Note that prior work on robust adversarial meta-learning [Yin et al., 2018, Goldblum et al., 2020, Wang et al., 2021] focuses on empirical study of the problem; we present first theoretical guarantees. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce a novel notion of stability for meta-learning algorithms, namely uniform meta-stability, and offer a tighter bound on the generalization gap for the meta-learning problem compared to existing literature. We instantiate uniformly meta-stable learning algorithms and give generalization guarantees for both convex, smooth losses as well as weakly convex and non-smooth losses. Several avenues for further exciting research remain. For instance, it remains to be seen if our bounds are tight. Can we show lower bounds on the generalization error for meta-learning? Additionally, understanding how meta-learning relates to federated learning may offer insights on how to extend the theory to broader applications and inform the design of new algorithms. Finally, motivated by data privacy considerations, it would be interesting to extend our setup to privacypreserving meta-learning, similar in spirit to the recent work of Zhou and Bassily [2022]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was supported, in part, by the DARPA GARD award HR00112020004, NSF CAREER award HIS-1943251, funding from the Institute for Assured Autonomy (IAA) at JHU, and the Spring\\*22 workshop on \u201cLearning and Games\"\u201d at the Simons Institute for the Theory of Computing. YW acknowledges the support of Amazon Fellowship. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Maruan Al-Shedivat, Trapit Bansal, Yuri Burda, Iya Sutskever, Igor Mordatch, and Pieter Abbeel. Continuous adaptation via meta-learning in nonstationary and competitive environments. arXiv preprint arXiv:1710.03641, 2017.   \nMaruan Al-Shedivat, Liam Li, Eric Xing, and Ameet Talwalkar. On data efficiency of meta-learning. In International Conference on Artificial Intelligence and Statistics, pages 1369-1377. PMLR, 2021.   \nRon Amit and Ron Meir. Meta-learning by adjusting priors based on extended pac-bayes theory. In International Conference on Machine Learning, pages 205-214. PMLR, 2018.   \nMaria-Florina Balcan, Mikhail Khodak, and Ameet Talwalkar. Provable guarantees for gradient-based meta-learning. In International Conference on Machine Learning, pages 424 433. PMLR, 2019.   \nFan Bao, Guoqiang Wu, Chongxuan Li, Jun Zhu, and Bo Zhang. Stability and generalization of bilevel programming in hyperparameter optimization. Advances in neural information processing systems, 34:4529-4541, 2021.   \nRaef Bassily, Vitaly Feldman, Cristobal Guzman, and Kunal Talwar. Stability of stochastic gradient descent on nonsmooth convex losses. Advances in Neural Information Processing Systems, 33: 4381-4391, 2020.   \nJonathan Baxter. A model of inductive bias learning. Journal of artificial intelligence research, 12: 149-198, 2000.   \nShai Ben-David and Reba Schuller. Exploiting task relatedness for multiple task learning. In Learning Theory and Kernel Machines: 16th Annual Conference on Learning Theory and 7th Kernel Workshop, COLT/Kernel 2003, Washington, DC, USA, August 24-27, 2003. Proceedings, pages 567-580. Springer, 2003.   \nOlivier Bousquet and Andre Elisseeff. Stability and generalization. The Journal of Machine Learning Research, 2:499-526, 2002.   \nOlivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharper bounds for uniformly stable algorithms. In Conference on Learning Theory, pages 610-626. PMLR, 2020.   \nFei Chen, Mi Luo, Zhenhua Dong, Zhenguo Li, and Xiuqiang He. Federated meta-learning with fast convergence and efficient communication. arXiv preprint arXiv: 1802.07876, 2018.   \nJiaxin Chen, Xiao-Ming Wu, Yanke Li, Qimai Li, Li-Ming Zhan, and Fu-lai Chung. A closer look at the training strategy for modern meta-learning. Advances in Neural Information Processing Systems, 33:396-406, 2020.   \nQi Chen, Changjian Shui, and Mario Marchand. Generalization bounds for meta-learning: An information-theoretic analysis. Advances in Neural Information Processing Systems, 34:25878- 25890, 2021.   \nLiam Collins, Aryan Mokhtari, and Sanjay Shakkottai Taskrobust mdel-agnostic metaleaing Advances in Neural Information Processing Systems, 33:18860-18871, 2020.   \nDamek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of weakly convex functions. SIAM Journal on Optimization, 29(1):207-239, 2019.   \nDamek Davis and Benjamin Grimmer. Proximally guided stochastic subgradient method for nonsmooth, nonconvex problems. SIAM Journal on Optimization, 29(3): 1908-1930, 2019.   \nDamek Davis, Dmitriy Drusvyatskiy, and Courtney Paquette. The nonsmooth landscape of phase retrieval. IMA Journal of Numerical Analysis, 40(4):2652-2695, 2020.   \nGiulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Learning to learn around a common mean. Advances in neural information processing systems, 31, 2018.   \nGiulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil. Learning-to-learn stochastic gradient descent with biased regularization. In International Conference on Machine Learning, pages 1566-1575. PMLR, 2019a.   \nGiulia Denevi, Dimitris Stamos, Carlo Ciliberto, and Massimiliano Pontil. Online-within-online meta-learning. Advances in Neural Information Processing Systems, 32, 2019b.   \nGiulia Denevi, Massimiliano Pontil, and Carlo Ciliberto. The advantage of conditional meta-learning for biased regularization and fine tuning. Advances in Neural Information Processing Systems, 33: 964-974, 2020.   \nNan Ding, Xi Chen, Tomer Levinboim, Sebastian Godman, and Radu Soricut. Bridging the gap between practice and pac-bayes theory in few-shot meta-learning. Advances in Neural Information Processing Systems, 34:29506-29516, 2021.   \nDmitriy Drusvyatskiy. The proximal point method revisited. arXiv preprint arXiv: 1712.06038, 2017.   \nSimon S Du, Wei Hu, Sham M Kakade, Jason D Lee, and Qi Lei. Few-shot learning via learning the representation, provably. arXiv preprint arXiv:2002.09434, 2020.   \nAndre Elisseeff, Theodoros Evgeniou, Massimiliano Pontil, and Leslie Pack Kaelbing. Stability of randomized learning algorithms. Journal of Machine Learning Research, 6(1), 2005.   \nAlireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. On the convergence theory of gradient-based model-agnostic meta-learning algorithms. arxiv preprint: 1908.10400, 2019.   \nAlireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning: A metalearning approach. arXiv preprint arXiv:2002.07948, 2020.   \nAlireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Generalization of model-agnostic metalearning algorithms: Recurring and unseen tasks. Advances in Neural Information Processing Systems, 34:5469-5480, 2021.   \nAlec Farid and Anirudha Majumdar. Generalization bounds for meta-learning via pac-bayes and uniform stability. Advances in neural information processing systems, 34:2173-2186, 2021.   \nVitaly Feldman and Jan Vondrak. Generalization bounds for uniformly stable algorithms. Advances in Neural Information Processing Systems, 31, 2018.   \nVitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. In Conference on Learning Theory, pages 1270-1279. PMLR, 2019.   \nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 1126-1135. PMLR, 2017.   \nLuca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In International conference on machine learning, pages 1568-1577. PMLR, 2018.   \nMicah Goldblum, Liam Fowl, and Tom Goldstein. Adversarially robust few-shot learning: A meta-learning approach. Advances in Neural Information Processing Systems, 33:17886-17895, 2020.   \nJiechao Guan and Zhiwu Lu. Task relatedness-based generalization bounds for meta learning. In International Conference on Learning Representations, 2021.   \nJiechao Guan, Yong Liu, and Zhiwu Lu. Fine-grained analysis of stability and generalization for modern meta learning algorithms. Advances in Neural Information Processing Systems, 35: 18487-18500, 2022.   \nMoritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In International conference on machine learning, pages 1225-1234. PMLR, 2016.   \nFredrik Hellstrom and Giuseppe Durisi. Evaluated cmi bounds for meta learning: Tightness and expressiveness. Advances in Neural Information Processing Systems, 35:20648-20660, 2022.   \nTimothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural networks: A survey. IEEE transactions on pattern analysis and machine intelligence, 44(9): 5149-5169, 2021.   \nKaiyi Ji, Jason D Lee, Yingbin Liang, and H Vincent Poor. Convergence of meta-learning with task-specific adaptationover parial parameters. Advances in Neural Information Processing Systems, 33:11490-11500, 2020.   \nWeisen Jiang, James Kwok, and Yu Zhang. Effective meta-regularization by kermelized proximal regularization. Advances in Neural Information Processing Systems, 34:26212-26222, 2021.   \nSharu Theresa Jose and Osvaldo Simeone. Information-theoretic generalization bounds for metalearning and applications. Entropy, 23(1):126, 2021.   \nSharuThreJsalSne nd Giuspurisiransfmetaeang: Ira theoretic bounds and information meta-risk minimization. IEEE Transactions on Information Theory, 68(1):474-501, 2021.   \nYegor Klochkov and Nikita Zhivotovskiy. Stability and deviation optimal risk bounds with convergence rate $o(1/n)$ . Advances in Neural Information Processing Systems, 34:5065-5076, 2021.   \nIja Kuzborskijand FrancecoOrabona. Fast rates by transferring from auxiliary hypotheses. Machine Learning, 106:171-195, 2017.   \nYunwen Lei. Stability and generalization of stochastic optimization with nonconvex and nonsmooth problems. In The Thirty Sixth Annual Conference on Learning Theory, pages 191-227. PMLR, 2023.   \nJeffrey Li, Mikhail Khodak, Sebastian Caldas, and Ameet Talwalkar. Differentially private metalearning. arXiv preprint arXiv:1909.05830, 2019.   \nTianyu Liu, Jie Lu, Zheng Yan, and Guangquan Zhang. Pac-bayes bounds for meta-learning with data-dependent prior. arXiv preprint arXiv:2102.03748, 2021.   \nTongliang Liu, Gabor Lugosi, Gergely Neu, and Dacheng Tao. Algorithmic stability and hypothesis complexity. In International Conference on Machine Learning, pages 2159-2167. PMLR, 2017.   \nAndreas Maurer. Algorithmic stability and meta-learning. Journal of Machine Learning Research, 6 (6), 2005.   \nAndreas Maurer. Transfer bounds for linear feature learning. Machine learning, 75(3):327-350, 2009.   \nAndreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask representation learning. Journal of Machine Learning Research, 17(81):1-32, 2016.   \nDimitr Meunier and Pierre Alquier Meta-strategy for learning tuning parameters with guarantees. Entropy, 23(10):1257, 2021.   \nRobert Miffin. Semismooth and semiconvex functions in constrained optimization. SIAM Journal on Control and 0ptimization, 15(6):959-972, 1977.   \nKonstantin Mishchenko, lavomi Hanzely, and Peter Richtarik Convergenceof first-order algorithms for meta-learning with moreau envelopes. arXiv preprint arXiv:2301.06806, 2023.   \nSayan Mukherjee, Partha Niyogi, Tomaso Poggio, and Ryan Rifkin. Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization. Advances in Computational Mathematics, 25:161-193, 2006.   \nAlex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv preprint arXiv:1803.02999, 2018.   \nAnastasia Pentina and Christoph Lampert. A pac-bayesian bound for lifelong learning. In International Conference on Machine Learning, pages 991-999. PMLR, 2014.   \nAravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with implicit gradients. Advances in neural information processing systems, 32, 2019.   \nYao-Feng Ren and Han-Ying Liang. On the best constant in marcinkiewicz-Zygmund inequality. Statistics & probability letters, 53(3):227-233, 2001.   \nArezou Rezazadeh. A unifed view on pac-bayes bounds for meta-learning. In International Conference on Machine Learning, pages 18576-18595. PMLR, 2022.   \nArezou Rezazadeh, Sharu Theresa Jose, Giuseppe Durisi, and Osvaldo Simeone. Conditional mutual information-based generalization bound for meta learning. In 2021 IEEE International Symposium on Information Theory (ISIT), pages 1176-1181. IEEE, 2021.   \nDominic Richards and Mike Rabbat. Learning with gradient descent and weakly convex losses. In International Conference on Artificial Intelligence and Statistics, pages 1990-1998. PMLR, 2021.   \nCharles Riou, Piere Alquier, and Badr-Eddine Cherief-Abdellatif. Bayes meets berstein at the meta level: an analysis of fast rates in meta-learning with pac-bayes. arXiv preprint arXiv:2302.11709, 2023.   \nJonas Rothfuss, Vincent Fortuin, Martin Josifoski, and Andreas Krause. Pacoh: Bayes-optimal meta-learningwith pac-guarantees. In International Conference on Machine Learning, pages 9116-9126. PMLR, 2021.   \nShai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.   \nShai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability and uniform convergence. The Journal of Machine Learning Research, 11:2635-2670, 2010.   \nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in neural information processing systems, 30, 2017.   \nZhenyu Sun, Xiaochun Niu, and Ermin Wei. Understanding generalization of federated learning via stability: Heterogeneity matters. In International Conference on Artijficial Intelligence and Statistics, pages 676-684. PMLR, 2024.   \nNilesh Tripuraneni, Michael Jordan, and Chi Jin. On the theory of transfer learning: The importance of task diversity. Advances in neural information processing systems, 33:7852-7862, 2020.   \nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. Advances in neural information processing systems, 29, 2016.   \nChunyang Wang, Yanmin Zhu, Haobing Liu, Tianzi Zang, Jiadi Yu, and Feilong Tang. Deep meta-learning in recommendation systems: A survey. arXiv preprint arXiv:2206.04415, 2022.   \nRen Wang, Kaidi Xu, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Chuang Gan, and Meng Wang. On fast adversarial robustness adaptation in model-agnostic meta-learning. arXiv preprint arXiv:2102.10454,2021.   \nJiancong Xiao, Yanbo Fan, Ruoyu Sun, Jue Wang, and Zhi-Quan Luo. Stability analysis and generalization bounds of adversarial training. Advances in Neural Information Processing Systems, 35:15446-15459, 2022.   \nYue Xing, Qifan Song, and Guang Cheng. On the algorithmic stability of adversarial training. Advances in neural information processing systems, 34:26523-26535, 2021.   \nChengxiang Yin, Jian Tang, Zhiyuan Xu, and Yanzhi Wang. Adversarial meta-learning. arXiv preprint arXiv:1806.03316, 2018.   \nHossein Zakerinia, Amin Behjati, and Christoph H Lampert. More fexible pac-bayesian metalearning by learning learning algorithms. arXiv preprint arXiv:2402.04054, 2024.   \nPan Zhou, Xiaotong Yuan, Huan Xu, Shuicheng Yan, and Jiashi Feng. Efficient meta learning via minibatch proximal update. Advances in Neural Information Processing Systems, 32, 2019.   \nXinyu Zhou and Raef Bassily. Task-level differentially private meta learning. Advances in Neural Information Processing Systems, 35:20947-20959, 2022.   \nYi Zhou, Yingbin Liang, and Huishuai Zhang. Understanding generalization error of sgd in nonconvex optimization. Machine Learning, pages 1-31, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we conduct a simple experiment to empirically verify our generalization bounds. ", "page_idx": 14}, {"type": "text", "text": "Setting. Following the experimental setting in Nichol et al. [2018] and Zhou et al. [2019], we consider a synthetic one-dimensional sine wave regression problem. The goal is to approximate the distribution of parameters of function $f(x;\\alpha,\\beta)=\\alpha\\sin(x+\\beta)$ . The task environment $\\mu$ is a joint distribution $\\mathcal{D}(\\alpha,\\beta)$ of the parameters $\\alpha$ and $\\beta$ .We take $\\mathcal{D}(\\alpha,\\beta)$ to be a product distribution of $\\mathcal{D}(\\alpha)=\\mathbf{U}([-5,5]),\\mathcal{D}(\\beta)=\\mathbf{U}([0,\\pi])$ . We generate the meta-sample by first sampling $m$ training tasks, i.e., $m$ pairs of $(\\alpha,\\beta)$ sampled independently from $\\mathcal{D}(\\alpha,\\beta)$ : For each of these $m$ tasks, we sample $n\\,=\\,10$ points, $x_{1},\\ldots,x_{10}$ uniformly on $[-5,5]$ and label them as $y_{i}\\,=\\,f(x_{i};\\alpha,\\beta)$ Similarly, at test time we generate a new task from the task distribution and generate a training sample of size $n$ (by sampling $x$ 's uniformly on the interval $[-5,5]$ and labeling them using $f(x;\\alpha,\\beta)$ .We sample 1000 new tasks at test time. For each of the test task, we also generate an evaluation set of size 200, and use it to estimate the mean-squared error between the predictions of the learned model and the true labels. Our hypothesis class is a two layer network of width 40 and $t a n h(\\cdot)$ activation function. We run Algorithm 1 for $T=100$ iterations with a step size of $\\gamma=0.1$ and regularization parameter $\\lambda=0.5$ . Algorithm 2 (GD) is run for $K=15$ iterations with step size $\\eta=0.02$ . The experiment is conducted on a T4 GPU. ", "page_idx": 14}, {"type": "text", "text": "Results. We report the transfer risk, the average empirical risk (over tasks), and the generalization gap for different values of $m$ and $n$ in Figure 1. In the plot on the left, we fix $n=10$ , and vary the number of tasks $m$ from 10 to 5000. In the plot in the middle, we fix $m=1000$ , and change the number of samples $n$ from 5 to 1000. In the plot on the right, we choose $m\\,=\\,n$ , and scale $m$ and $n$ simultaneously from 10 to 1000. We observe that in all of these three scenarios, as $m$ (and/or $n$ ) increase, both the generalization gap as well as the transfer risk decrease. Moreover, the generalization gap decreases at rates approximately $\\mathcal{O}(1/m+1/n+1/\\sqrt{m n})$ for these three scenarios as suggested by our theoretical result. ", "page_idx": 14}, {"type": "image", "img_path": "J8rOw29df2/tmp/d3587883ad16f78f8529f9f437189764bc0c9e044cb7de9dd262a4aacb0e8d18.jpg", "img_caption": ["Figure 1: Error plots as a function of: (left) the number of tasks $m$ for fixed $n=10$ ; (middle) the number of training samples $n$ on a test task for fixed $m=1\\mathrm{K}$ ; (right) both $m$ and $n$ "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Missing Proofs of Section 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The following Lemmas and Theorems are used for proving Theorem 3.1. ", "page_idx": 14}, {"type": "text", "text": "Lemma B.1 (Bounded differences/McDiarmid's inequality). Consider a function $f$ of independent random variables $z_{1},\\ldots,z_{n}$ that take their value in $\\mathcal{Z}$ . Suppose that $f$ satisfies the bounded differences property, namely, for any $i=1,\\hdots,n$ and any $z_{1},\\dots,z_{n},z_{i}^{\\prime}\\in{\\mathcal{Z}}$ , it holds that ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(z_{1},\\dots,z_{\\mathrm{n}})-f(z_{1},\\dots,z_{i-1},z_{i}^{\\prime},z_{i+1},\\dots,z_{n})\\le\\beta.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then we have for any $p\\geq2$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|f(z_{1},\\dots,z_{\\mathrm{n}})-\\mathbb{E}f(z_{1},\\dots,z_{\\mathrm{n}})\\right\\|_{p}\\leq2{\\sqrt{n p}}\\beta.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Theorem B.2 (Marcinkiewicz-Zygmund's inequality Ren and Liang [2001]). Let $x_{1},\\ldots,x_{n}$ be independent centered random variables with a finite $p$ -th moment for $p\\geq2$ .Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{i=1}^{n}x_{i}\\right\\|_{p}\\leq3{\\sqrt{2n p}}\\left({\\frac{1}{n}}\\sum_{i=1}^{n}\\left\\|x_{i}\\right\\|_{p}^{p}\\right)^{\\frac{1}{p}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Theorem B.3. Let $\\mathsf{Z}=(\\mathbf{z}_{1},\\ldots,\\mathbf{z}_{n})$ be a vector of independent random variables each taking values in $\\mathcal{Z}$ . Let $\\mathbf{Z}=(Z_{1},\\hdots,Z_{m})$ be a vector of independent random vectors each taking values in ${\\mathcal{Z}}^{n}$ Let $g_{j,i}:(\\mathcal{Z}^{n})^{m}\\times\\mathcal{Z}^{n}\\rightarrow\\mathbb{R}$ be some functions such that the following holds for any $i^{\\bar{\\bf{\\Delta}}}\\in[n],j\\in[m]$ ", "page_idx": 15}, {"type": "text", "text": "(1). $|\\mathbb{E}[g_{j,i}(\\mathbf{Z},\\mathbf{Z})|\\mathbf{Z}_{j},\\mathbf{z}_{i}]|\\leq M\\;\\mathbf{a}.\\mathrm{~}$ S.,   \n(2). $\\mathbb{E}\\left[g_{j,i}(\\mathbf{Z},\\mathbf{Z})\\vert\\mathsf{Z}_{[m]\\backslash\\{j\\}},\\mathsf{z}_{[n]\\backslash\\{i\\}}\\right]=0\\,\\mathrm{a.s.},$   \n(3). $g_{j,i}$ has a bounded difference $\\bar{\\beta}$ W.r.t. all variables except the $(j,i)$ -th variable. ", "page_idx": 15}, {"type": "text", "text": "Then we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{j=1}^{m}\\sum_{i=1}^{n}g_{j,i}(\\mathbf{Z},\\mathbf{Z})\\right\\|\\lesssim m n\\bar{\\beta}\\log\\left(m n\\right)+M\\sqrt{m n}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Theorem B.3. The proof is an extension of [Bousquet et al., 2020, Theorem 4]. ", "page_idx": 15}, {"type": "text", "text": "Without loss of generality, we suppose that $n=2^{k},m=2^{r}$ . Otherwise, we can add extra functions that equal to zero. Consider a sequence of partitions $\\mathcal{C}_{0},\\ldots,\\mathcal{C}_{k}$ with $\\mathcal{C}_{0}\\,=\\,\\{\\{i\\}:i\\in[n]\\}$ \uff0c $\\mathcal{C}_{k}=$ $\\{[n]\\}$ , and to get $\\mathcal{C}_{l}$ from $\\mathcal{C}_{l+1}$ we split each subset into $\\mathcal{C}_{l+1}$ into two equal parts. We have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{C}_{0}=\\left\\{\\left\\{1\\right\\},\\ldots,\\left\\{2^{k}\\right\\}\\right\\},\\mathcal{C}_{1}=\\left\\{\\left\\{1,2\\right\\},\\left\\{3,4\\right\\},\\ldots,\\left\\{2^{k}-1,2^{k}\\right\\}\\right\\},\\mathcal{C}_{k}=\\left\\{\\left\\{1,\\ldots,2^{k}\\right\\}\\right\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By construction, we have $|\\mathcal{C}_{l}|\\;=\\;2^{k-l}$ and $|C|\\,=\\,2^{l}$ for each $C\\,\\in\\,{\\mathcal{C}}_{l}$ . For each $i~\\in~[n]$ and $l=0,\\ldots,k$ , denote by $C^{l}(i)\\in\\mathcal{C}_{l}$ the only set from $\\mathcal{C}_{l}$ that contains $i$ . In particular, $C^{0}(i)=\\{i\\}$ and $C^{k}(i)=[n]$ ", "page_idx": 15}, {"type": "text", "text": "Similarly, we consider a sequence of partitions $\\mathcal{E}_{0},\\ldots,\\mathcal{E}_{r}$ with $\\mathcal{E}_{0}=\\{\\{j\\}:j\\in[m]\\}$ $\\mathcal{E}_{r}=\\{[m]\\}$ \uff0c and to get $\\mathcal{E}_{q}$ from ${\\mathcal{E}}_{q+1}$ we split each subset in $\\mathcal{E}_{q+1}$ into two equal parts. We have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{E}_{0}=\\left\\{\\left\\{1\\right\\},\\ldots,\\left\\{2^{r}\\right\\}\\right\\},\\mathcal{E}_{1}=\\left\\{\\left\\{1,2\\right\\},\\left\\{3,4\\right\\},\\ldots,\\left\\{2^{r}-1,2^{r}\\right\\}\\right\\},\\mathcal{E}_{r}=\\left\\{\\left\\{1,\\ldots,2^{r}\\right\\}\\right\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By construction, we have $|\\mathcal{E}_{q}|\\,=\\,2^{r-q}$ and $|E|\\,=\\,2^{q}$ for each $E\\,\\in\\,{\\mathcal{E}}_{q}$ . For each $j\\,\\in\\,[m]$ and $q=0,\\ldots,r$ , denote by $E^{q}(j)\\in\\mathcal{E}_{q}$ the only set from $\\mathcal{E}_{q}$ that contains $j$ . In particular, $E^{0}(j)=\\{j\\}$ and $E^{r}(j)=[m]$ ", "page_idx": 15}, {"type": "text", "text": "For each $i\\in[n],j\\in[m]$ and every $l=0,\\ldots,k,q=0,\\ldots,r$ . consider the random variables ", "page_idx": 15}, {"type": "equation", "text": "$$\ng_{j,i}^{q,l}=g_{j,i}^{q,l}({\\sf Z}_{j},{\\sf Z}_{[m]\\backslash E^{q}(j)},{\\sf z}_{i},{\\sf z}_{[n]\\backslash C^{l}(i)}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "i.e., conditioned on ${\\bf Z}_{j},{\\bf z}_{i}$ and all the vectors that are not in the same set as ${\\mathbf{Z}}_{j}$ in the partition $\\mathcal{E}_{q}$ and all the variables tha are no n the same set as $\\mathbf{Z}_{i}$ in the partion $\\mathcal{C}_{l}$ . In particular, $g_{j,i}^{0,0}=g_{j,i}$ \uff0c $g_{j,i}^{r,k}=\\mathbb{E}[g_{j,i}|Z_{j},\\mathbf{z}_{i}]$ We can write elesopie suma folows ", "page_idx": 15}, {"type": "equation", "text": "$$\ng_{j,i}-\\mathbb{E}[g_{j,i}|\\mathsf{Z}_{j},\\mathsf{z}_{i}]=\\sum_{q=0}^{r-1}g_{j,i}^{q,0}-g_{j,i}^{q+1,0}+\\sum_{l=0}^{k-1}g_{j,i}^{r,l}-g_{j,i}^{r,l+1},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and the total sum of interest satisfies by the triangle inequality ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left|\\sum_{j=1}^{m}\\sum_{i=1}^{n}g_{j,i}\\right|\\right|\\leq\\left\\|\\sum_{j=1}^{m}\\sum_{i=1}^{n}\\mathbb{E}\\left[g_{j,i}|Z_{j},z_{i}\\right]\\right\\|+\\sum_{q=0}^{r-1}\\left\\|\\sum_{j=1}^{m}\\sum_{i=1}^{n}g_{j,i}^{q,0}-g_{j,i}^{q+1,0}\\right\\|+\\sum_{l=0}^{k-1}\\left\\|\\sum_{j=1}^{m}\\sum_{i=1}^{n}g_{j,i}^{r,l}-g_{j,i}^{r,l+1}\\right\\|\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $|\\mathbb{E}[g_{j,i}|Z_{j},\\mathbf{z}_{i}]|~\\le~M$ and $\\mathbb{E}\\left(\\mathbb{E}\\left[g_{j,i}\\vert Z_{j},\\mathbf{z}_{i}\\right]\\right)~=~0$ , by applying McDiarmid inequality in Lemma B.1, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{j=1}^{m}\\sum_{i=1}^{n}\\mathbb{E}\\left[g_{j,i}|Z_{j},\\mathbf{z}_{i}\\right]\\right\\|\\leq4\\sqrt{2m n}M.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We observe that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{j,i}^{q+1,l+1}({Z}_{j},{Z}_{[m]\\backslash E^{q+1}(j)},{\\mathrm{z}}_{i},{\\mathrm{z}}_{[n]\\backslash C^{l+1}(i)})}\\\\ &{=\\mathbb{E}\\left[g_{j,i}^{q+1,l}({Z}_{j},{Z}_{[m]\\backslash E^{q+1}(j)},{\\mathrm{z}}_{i},{\\mathrm{z}}_{[n]\\backslash C^{l}(i)})\\big|{\\mathrm{z}}_{i},{\\mathrm{z}}_{[n]\\backslash C^{l+1}(i)}\\right]}\\\\ &{\\qquad\\qquad\\qquad(\\mathrm{The~expectation~is~take~w.r.t.~the~variable~}{\\mathrm{z}}_{s},s\\in{\\mathrm{G}}^{r})}\\\\ &{=\\mathbb{E}\\left[g_{j,i}^{q,l+1}({Z}_{j},{Z}_{[m]\\backslash E^{q}(j)},{\\mathrm{z}}_{i},{\\mathrm{z}}_{[n]\\backslash C^{l+1}(i)})\\big|{Z}_{j},{Z}_{[m]\\backslash E^{q+1}(j)}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Asfunction $g_{j,i}^{q,l}$ conditioned on $Z_{j},Z_{[m]\\backslash E^{q+1}(j)},\\mathbf{Z}_{i},\\mathbf{Z}_{[n]\\backslash C^{l+1}(i)}$ we obtain a uniform bound ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|g_{j,i}^{q,0}-g_{j,i}^{q+1,0}\\right\\|\\left({\\mathsf{Z}}_{j},{\\mathsf{Z}}_{[m]\\setminus E^{q+1}(j)},{\\mathsf{Z}}_{i},{\\mathsf{Z}}_{[n]\\setminus C^{0}(i)}\\right)\\leq2\\sqrt{2^{q+1}}\\bar{\\beta}}\\\\ {\\left\\|g_{j,i}^{r,l}-g_{j,i}^{r,l+1}\\right\\|\\left({\\mathsf{Z}}_{j},{\\mathsf{Z}}_{[m]\\setminus E^{r}(j)},{\\mathsf{Z}}_{i},{\\mathsf{Z}}_{[n]\\setminus C^{l+1}(i)}\\right)\\leq2\\sqrt{2^{l+1}}\\bar{\\beta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "as there are $2^{l}$ indices in $C^{l+1}(i)\\backslash C^{l}(i)$ and $2^{q}$ indices in $E^{q+1}(j)\\backslash E^{q}(j)$ ", "page_idx": 16}, {"type": "text", "text": "Now we focus on $\\begin{array}{r}{\\sum_{j\\in E^{q}}\\sum_{i\\in C^{0}}g_{j,i}^{q,0}-g_{j,i}^{q+1,0}}\\end{array}$ gt1,0 forEa E &g andjeeriecu 9-9 gr,l+1 for Cl E $\\mathcal{C}_{l}$ , respectively. Since $g_{j,i}^{q,0}-g_{j,i}^{q+1,0}$ for $j\\in E^{q},i\\in C^{0}$ depends on $Z_{j},{\\bf Z}_{[m]\\backslash E^{q}(j)},{\\bf z}_{i},{\\bf z}_{[n]\\backslash C^{0}(i)}$ the terms are independent and zero mean conditioned on $Z_{[m]\\setminus E^{q}(j)}$ . Applying Theorem B.2, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left\\|\\sum_{j\\in E^{q}}\\sum_{i\\in C^{0}}g_{j,i}^{q,0}-g_{j,i}^{q+1,0}\\right\\|^{2}\\left({\\mathbf{Z}}_{[m]\\setminus E^{q}}\\right)}}\\\\ &{}&{\\leq36\\cdot2^{q}\\frac{1}{2^{q}}\\sum_{j\\in E^{q}}\\sum_{i\\in C^{0}}\\left\\|g_{j,i}^{q,0}-g_{j,i}^{q+1,0}\\right\\|^{2}\\left({\\mathbf{Z}}_{[m]\\setminus E^{q}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Integratng with espeto $(Z_{[m]\\backslash E^{q}})$ and using $\\begin{array}{r}{\\left\\lVert g_{j,i}^{q,0}-g_{j,i}^{q+1,0}\\right\\rVert\\leq2\\sqrt{2^{q+1}}\\bar{\\beta}}\\end{array}$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{j\\in E^{q}}\\sum_{i\\in C^{0}}g_{j,i}^{q,0}-g_{j,i}^{q+1,0}\\right\\|\\leq6\\sqrt{2^{q}}\\times2\\sqrt{2^{q+1}}\\bar{\\beta}=12\\sqrt{2}\\cdot2^{q}\\bar{\\beta}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Applying triangle inequality over all sets $C^{0}\\in\\mathcal{C}_{0},E^{q}\\in\\mathcal{E}_{q}$ gives us that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left\\|\\sum_{j\\in[m]}\\sum_{i\\in[n]}g_{j,i}^{q,0}-g_{j,i}^{q+1,0}\\right\\|\\leq\\sum_{\\scriptstyle E^{q}\\in\\mathcal{E}_{q},C^{0}\\in\\mathcal{C}_{0}}\\left\\|\\sum_{\\scriptstyle j\\in E^{q},i\\in C^{0}}g_{j,i}^{q,0}-g_{j,i}^{q+1,0}\\right\\|}}\\\\ &{}&{\\leq2^{r+k-q}\\times12\\sqrt{2}\\cdot2^{q}\\bar{\\beta}}\\\\ &{}&{=12\\sqrt{2}\\cdot2^{r+k}\\bar{\\beta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similarly, 9j, $g_{j,i}^{r,l}-g_{j,i}^{r,l+1}$ $j\\in E^{r},i\\in C^{l}$ depens on $z_{i},z_{[n]\\backslash C^{l+1}(i)}$ thetermnsare ndependentand zero mean conditioned on $Z[n]\\backslash C^{l+1}(i)$ . Applying Theorem B'.2, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left\\|\\sum_{j\\in E^{r}}\\sum_{i\\in C^{l}}g_{j,i}^{r,l}-g_{j,i}^{r,l+1}\\right\\|^{2}\\left(\\mathbf{z}_{[n]\\setminus C^{l}}\\right)}}\\\\ &{}&{\\leq36\\cdot2^{l+r}\\frac{1}{2^{l+r}}\\sum_{j\\in E^{r}}\\sum_{i\\in C^{l}}\\left\\|g_{j,i}^{r,l}-g_{j,i}^{r,l+1}\\right\\|^{2}\\left(\\mathbf{z}_{[n]\\setminus C^{l}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Integrating with respect to $\\left(\\mathbf{z}_{[n]\\setminus C^{l}}\\right)$ and using $\\left\\|g_{j,i}^{r,l}-g_{j,i}^{r,l+1}\\right\\|\\leq2\\sqrt{2^{l+1}}\\bar{\\beta}$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{j\\in E^{r}}\\sum_{i\\in C^{l}}g_{j,i}^{r,l}-g_{j,i}^{r,l+1}\\right\\|\\leq6\\sqrt{2^{l+r}}\\times2\\sqrt{2^{l+1}}\\bar{\\beta}=12\\sqrt{2}\\cdot2^{l+0.5r}\\bar{\\beta}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Applying triangle inequality over all sets $C^{l}\\in\\mathcal{C}_{l},E^{r}\\in\\mathcal{E}_{r}$ gives us that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\displaystyle\\sum_{j\\in[m]}\\displaystyle\\sum_{i\\in[n]}g_{j,i}^{r,l}-g_{j,i}^{r,l+1}\\right\\|\\leq\\displaystyle\\sum_{E^{r}\\in\\mathcal{E}_{r},C^{l}\\in\\mathcal{C}_{l}}\\left\\|\\displaystyle\\sum_{j\\in E^{r},i\\in C^{l}}g_{j,i}^{r,l}-g_{j,i}^{r,l+1}\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq2^{k-l}\\times12\\sqrt{2}\\cdot2^{l+0.5r}\\bar{\\beta}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq12\\sqrt{2}\\cdot2^{r+k}\\bar{\\beta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Recall that $2^{k}<2n,2^{r}<2m$ due to the possible extension of the sample. Therefore we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{q=0}^{r-1}\\left\\|\\sum_{j=1}^{m}\\sum_{i=1}^{n}g_{j,i}^{q,0}-g_{j,i}^{q+1,0}\\right\\|+\\sum_{l=0}^{k-1}\\left\\|\\sum_{j=1}^{m}\\sum_{i=1}^{n}g_{j,i}^{r,l}-g_{j,i}^{r,l+1}\\right\\|\\leq48\\sqrt{2}m n\\bar{\\beta}(\\left[\\log\\left(m\\right)\\right]+\\left[\\log\\left(n\\right)\\right])\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combined with Equation (2) get the required bound. ", "page_idx": 17}, {"type": "text", "text": "We now restate and prove Theorem 3.1. ", "page_idx": 17}, {"type": "text", "text": "Theorem 3.1. Consider a meta-learning problem for some $M$ -bounded loss function $\\ell$ and task distribution $\\mu$ . Let $\\mathbf{S}$ be a meta-sample consisting of training samples on $m$ tasks each of size $n$ , and let $s\\sim\\mathcal{D}$ be a sample of size $n$ on a previously unseen task $\\mathcal{D}\\sim\\mu$ .Then, for any $\\beta$ -uniformly meta-stable learning algorithm $\\boldsymbol{\\mathcal{A}}$ , we have that with probability $1-\\delta$ \uff0c ", "page_idx": 17}, {"type": "equation", "text": "$$\nL(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mu)\\lesssim L(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mathbf{S})+\\bar{\\beta}\\log\\left(m n\\right)\\log\\left(1/\\delta\\right)+M\\sqrt{\\log\\left(1/\\delta\\right)/\\left(m n\\right)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 3.1. In order to make use of Theorem B.3, we consider the following functions: ", "page_idx": 17}, {"type": "equation", "text": "$$\ng_{j,i}=g_{j,i}(\\mathbf{Z},\\mathbf{Z})=\\mathbb{E}_{(S_{j}^{\\prime},z_{j}^{\\prime})\\sim\\mathcal{D}_{j}^{n+1},\\mathcal{D}_{j}\\sim\\mu}\\mathbb{E}_{(S,z)\\sim\\mathcal{D}^{n+1},\\mathcal{D}\\sim\\mu}\\ell(A(\\mathbf{S}^{(j)})(S),\\mathbf{z})-\\ell(A(\\mathbf{S}^{(j)})(S_{j}^{(i)}),\\mathbf{z}_{j}^{i})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By the definition of uniform meta stability, we can write the following decomposition: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname{\\displaystyle\\left\\lvert{\\calZ}\\left(A\\left({\\cal A}\\left({\\cal A}\\left({\\cal S}\\right),\\beta\\right)\\right)-L\\left(A\\left({\\cal S}\\right),\\beta\\right)\\right\\rvert}}\\\\ &{=\\left\\lvert\\displaystyle\\left\\lvert\\sum_{j=1}^{m}\\sum_{\\Omega\\leq\\nu_{j}>-\\nu^{-1},\\,\\mathcal{D}\\neq\\mu}\\!\\left\\{A\\left({\\cal S}\\left({\\cal S}\\right),\\beta\\right)-\\ell\\left(A\\left({\\cal S}\\right)\\left({\\cal S}_{j}\\right),z_{j}^{\\star}\\right)\\right\\}\\right\\rvert}\\\\ &{=\\left\\lvert\\displaystyle\\left\\lvert\\sum_{j=1}^{m}\\sum_{\\lambda\\leq\\nu_{j}>-\\mu}\\sum_{\\gamma=\\mu}\\!\\!\\!\\!\\{\\sum_{\\bar{\\alpha}}\\!_{{\\beta},\\gamma>\\mu^{\\prime}}\\!\\!\\!\\!\\left(\\!\\!\\bar{\\alpha}\\left({\\cal S}\\right)\\left({\\cal S}\\right)\\!\\cdot\\!{\\cal Z}\\right)\\!\\!\\!+\\!\\!\\!\\left(\\!\\!\\bar{\\alpha}\\left({\\cal S}\\right)\\left({\\cal S}\\right)\\!\\cdot\\!{\\cal Z}\\right)\\!\\!-\\!\\!\\bar{\\alpha}\\left(A\\left({\\cal S}^{(j)}\\right)\\left({\\cal S}_{j}^{(j)}\\right)\\!\\cdot\\!\\!z_{j}^{\\star}\\!\\right)}\\\\ &{\\qquad+\\ell\\left(A\\left({\\cal S}^{(j)}\\right)\\left({\\cal S}_{j}^{(j)}\\right)\\!\\cdot\\!\\!z_{j}^{\\star}\\!\\right)-\\ell\\left(A\\left({\\cal S}\\right)\\left({\\cal S}\\right),{\\cal Z}_{j}^{\\star}\\right)\\!\\right)\\right\\}\\!\\Bigg\\rvert}\\\\ &{\\leq\\left\\lVert\\displaystyle\\sum_{j=1}^{m}\\sum_{i=1}^{m}\\mathbb{E}_{\\delta_{j}\\sim{\\cal P}_{j}^{\\alpha+\\frac{1}{2}}}\\!\\!\\!\\!\\!+\\!\\!\\frac{\\mathbb{E}_{\\delta_{j}}\\left({\\cal S}_{j}^{\\star}\\right)\\cdot{\\cal D}_{j}^{\\star}\\left({\\cal S}_{j}^{+1}\\right)\\cdot\\!\\sigma_{j}^{-1}}{p_{j}\\nu_{j}}\\left(\\ell\\left(A\\left({\\cal S}\\right)\\left({\\cal S}\\right),{\\cal Z}\\right)-\\ell\\left(A\\left({\\cal S}^{(j)}\\right)\\left({\\cal S}_{j}^{(j)}\\right),z_{j}^{\\star}\\right)\\right)\\right\\rvert+m m\\\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Moreover, we have $\\mathbb{E}\\left[g_{j,i}\\middle|S_{1},\\ldots,S_{j-1},S_{j+1},\\ldots,S_{m},\\mathbf{z}_{1},\\ldots,\\mathbf{z}_{i-1},\\mathbf{z}_{i+1},\\ldots,\\mathbf{z}_{n}\\right]=0$ and $|g_{j,i}|\\leq$ $2M$ a.s.for $i\\,\\in\\,[n],j\\,\\in\\,[m]$ . Applying Theorem B.3 as well as [Bousquet and Elisseeff, 2002, Lemma 1] achieves the results. ", "page_idx": 17}, {"type": "text", "text": "Theorem 3.2 can be directly proved by the definition of uniform meta-stability. ", "page_idx": 17}, {"type": "text", "text": "Theorem 3.2. Let $\\mu$ be an underlying task distribution. Given a meta-sample S, test task $\\mathcal{D}\\sim\\mu$ ,and $S\\sim{\\mathcal{D}}^{n}$ ,for any $\\bar{\\beta}$ -on-average-replace-one-meta-stable meta-learning algorithm $\\boldsymbol{\\mathcal{A}}$ we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{S}\\sim\\left\\{\\mathscr{D}_{j}^{n}\\right\\}_{j=1}^{m},\\left\\{\\mathscr{D}_{j}\\right\\}_{j=1}^{m}\\sim\\mu^{m}\\big[L(\\mathbf{\\mathscr{A}}(\\mathbf{S}),\\mu)-L(\\mathbf{\\mathscr{A}}(\\mathbf{S}),\\mathbf{S})\\big]\\le\\bar{\\beta}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof of Theorem 3.2. Since $\\boldsymbol{S}$ and $z^{\\prime}$ are both drawn i.i.d. from $\\mathcal{D}$ , and $\\mathbf{S}$ and $\\ensuremath{\\mathcal{S}}_{j}^{\\prime}$ are both drawn i.i.d. from $\\mu$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{S}\\sim\\left\\{\\mathcal{D}_{j}^{n}\\right\\}_{j=1}^{m},\\left\\{\\mathcal{D}_{j}\\right\\}_{j=1}^{m}\\sim\\mu^{m}}\\mathbb{E}_{S\\sim\\mathcal{D}^{n},\\mathcal{D}\\sim\\mu}L(\\boldsymbol A(\\mathbf{S})(S),\\mathcal{D})}\\\\ &{\\ =\\mathbb{E}_{\\mathbf{S}\\sim\\left\\{\\mathcal{D}_{j}^{n}\\right\\}_{j=1}^{m},(S_{j}^{\\prime})\\sim\\mathcal{D}_{j}^{n},\\left\\{\\mathcal{D}_{j}\\right\\}_{j=1}^{m}\\sim\\mu^{m},j\\sim\\mathrm{U}[m]}L(\\boldsymbol A(\\mathbf{S}^{(j)})(S_{j}),\\mathcal{D}_{j})}\\\\ &{\\ =\\mathbb{E}_{\\mathbf{S}\\sim\\left\\{\\mathcal{D}_{j}^{n}\\right\\}_{j=1}^{m},(S_{j}^{\\prime},z_{j}^{\\prime})\\sim\\mathcal{D}_{j}^{n+1},\\left\\{\\mathcal{D}_{j}\\right\\}_{j=1}^{m}\\sim\\mu^{m},j\\sim\\mathrm{U}[m],i\\sim\\mathrm{U}[n]}\\ell(\\boldsymbol A(\\mathbf{S}^{(j)})(S_{j}^{(i)}),\\mathbf{z}_{j}^{i})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "as well as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{S}\\sim\\left\\{{\\cal D}_{j}^{n}\\right\\}_{j=1}^{m},\\{{\\cal D}_{j}\\}_{j=1}^{m}\\sim\\mu^{m}}\\left[\\displaystyle\\frac{1}{m}\\sum_{j=1}^{m}L({\\cal A}(\\mathbf{S})({\\cal S}_{j}),{\\cal S}_{j})\\right]}\\\\ &{\\ =\\mathbb{E}_{\\mathbf{S}\\sim\\left\\{{\\cal D}_{j}^{n}\\right\\}_{j=1}^{m},\\{{\\cal D}_{j}\\}_{j=1}^{m}\\sim\\mu^{m},j\\sim\\mathrm{U}[m]}\\left[L({\\cal A}(\\mathbf{S})({\\cal S}_{j}),{\\cal S}_{j})\\right]}\\\\ &{\\ =\\mathbb{E}_{\\mathbf{S}\\sim\\left\\{{\\cal D}_{j}^{n}\\right\\}_{j=1}^{m},({\\cal S}_{j}^{\\prime},z_{j}^{\\prime})\\sim{\\cal D}_{j}^{n+1},\\{{\\cal D}_{j}\\}_{j=1}^{m}\\sim\\mu^{m},j\\sim\\mathrm{U}[m],i\\sim\\mathrm{U}[n]}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\ell({\\cal A}(\\mathbf{S})({\\cal S}_{j}),{\\cal z}_{j}^{i})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As a result, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{S}\\sim\\left\\{\\mathcal{D}_{j}^{n}\\right\\}_{j=1}^{m}},\\{\\mathcal{D}_{j}\\}_{j=1}^{m}\\sim\\mu^{m}\\left[L(A(\\mathbf{S}),\\mu)-L(A(\\mathbf{S}),\\mathbf{S})\\right]}\\\\ &{\\ =\\mathbb{E}_{\\mathbf{S}\\sim\\left\\{\\mathcal{D}_{j}^{n}\\right\\}_{j=1}^{m},(S_{j}^{\\prime},z_{j}^{\\prime})\\sim\\mathcal{D}_{j}^{n+1},\\{\\mathcal{D}_{j}\\}_{j=1}^{m}\\sim\\mu^{m},j\\sim\\cup[m],i\\sim\\cup[n]}\\left|\\ell(A(\\mathbf{S}^{(j)})(S_{j}^{(i)}),z_{j}^{i})-\\ell(A(\\mathbf{S})(S_{j}),z_{j}^{i})\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "(By definition of $\\bar{\\beta}$ -on-average-replace-one-meta-stable) ", "page_idx": 18}, {"type": "text", "text": "C Missing Proofs of Section 4.1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Lemma C.1 (Shalev-Shwartz and Ben-David [2014]). Given $\\boldsymbol{S}$ and $S^{(i)}$ , for a fixed w, define $\\mathbf{u}(\\mathbf{w},{\\cal S})$ and $\\mathbf{u}(\\mathbf{w},S^{(i)})$ is achieved via Algo. 1 with Option 1 RERM. Then if $\\ell$ is convex, $G$ -Lipschitz, we have $\\begin{array}{r}{\\operatorname*{sup}_{S,i\\in[n]}\\left\\|\\mathbf{u}(\\mathbf{w},S)-\\mathbf{u}(\\mathbf{w},S^{(i)})\\right\\|\\le\\frac{4G}{\\lambda n}}\\end{array}$ If $\\ell$ is convex and $H$ smooth $\\begin{array}{r}{(H\\leq\\frac{\\lambda n}{2})}\\end{array}$ , we have $\\begin{array}{r}{\\left\\|\\mathbf{u}(\\mathbf{w},S)-\\mathbf{u}(\\mathbf{w},S^{(i)})\\right\\|\\le\\frac{\\sqrt{8H}}{\\lambda n}(\\sqrt{\\ell(\\mathbf{w},\\mathbf{z}_{i})}+\\sqrt{\\ell(\\mathbf{w},\\mathbf{z}^{\\prime})}).}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Lemma 4.1. Assume that the loss function $\\ell$ is convex and $G$ -Lipschitz loss. Let S, $\\mathbf{S}^{(j)}$ denote neighboring meta-samples and $s,s^{(i)}$ the neighboring samples on a test task. Then, the following holds for Algorithm 1 with RERM for task-specific learning (i.e., Option 1 for Algorithm 2) $\\forall T\\geq1$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbf{S},S,j\\in[m],i\\in[n]}\\left\\|A(\\mathbf{S})(S)-A(\\mathbf{S}^{(j)})(S^{(i)})\\right\\|\\leq\\frac{G}{\\lambda m}+\\frac{2G}{\\lambda n}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Further, if $\\ell$ is convex, $M$ -bounded and $H$ -smooth, then setting $\\begin{array}{r}{\\lambda\\geq H,\\gamma\\leq\\frac{1}{\\lambda}}\\end{array}$ , we have $\\forall T\\geq1$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbf{S},S,j\\in[m],i\\in[n]}\\left\\|\\mathcal{A}(\\mathbf{S})(S)-\\mathcal{A}(\\mathbf{S}^{(j)})(S^{(i)})\\right\\|\\leq\\frac{2\\sqrt{2H M}}{2\\lambda n-H}+\\frac{n}{2\\lambda n-H}\\frac{4\\sqrt{2H M}}{(m+1)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma 4.1. We slightly abuse the notation, at iteration $t$ define $\\mathbf{w}_{t}=\\mathcal{A}(\\mathbf{S}),\\mathbf{w}_{t}^{\\prime}=\\mathcal{A}(\\mathbf{S}^{(j)})$ Given $\\mathrm{w}_{T+1}$ define $\\mathbf{u}(\\mathbf{w}_{T+1},S)=\\mathcal{A}(\\mathbf{S})(S),\\mathbf{u}(\\mathbf{w}_{T+1}^{\\prime},S^{(i)})=\\mathcal{A}(\\mathbf{S}^{(j)})(S^{(i)})$ ", "page_idx": 18}, {"type": "text", "text": "We first consider the setting where the loss $\\ell$ is convex, $G$ -Lipschitz. Recall that $F_{S}(\\mathbf{u},\\mathbf{w})\\ =$ $\\begin{array}{r}{L(\\mathbf{u},S)+\\frac{\\lambda}{2}\\left\\|\\mathbf{u}-\\mathbf{w}\\right\\|^{2}}\\end{array}$ If $\\ell$ is convex, then $F_{S}(\\mathbf{u},\\mathbf{w})$ .s $\\lambda$ -strongly-convex w.r.t u. Define $\\mathbf{u}(\\mathbf{w},S)=$ ${\\mathrm{argmin}}_{\\mathbf{u}\\in{\\mathcal{W}}}\\,F_{S}(\\mathbf{u},\\mathbf{w})$ $\\begin{array}{r}{\\mathbf{u}(\\mathbf{w}^{\\prime},\\boldsymbol{S})=\\operatorname*{argmin}_{\\mathbf{u}\\in\\boldsymbol{\\mathcal{W}}}F_{\\boldsymbol{S}}(\\mathbf{u},\\mathbf{w}^{\\prime})}\\end{array}$ . We have the following: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F_{S}(\\mathbf{u}(\\mathbf{w}^{\\prime},S),\\mathbf{w})-F_{S}(\\mathbf{u}(\\mathbf{w},S),\\mathbf{w})\\geq\\lambda\\left\\lVert\\mathbf{u}(\\mathbf{w},S)-\\mathbf{u}(\\mathbf{w}^{\\prime},S)\\right\\rVert^{2}}\\\\ {F_{S}(\\mathbf{u}(\\mathbf{w},S),\\mathbf{w}^{\\prime})-F_{S}(\\mathbf{u}(\\mathbf{w}^{\\prime},S),\\mathbf{w}^{\\prime})\\geq\\lambda\\left\\lVert\\mathbf{u}(\\mathbf{w},S)-\\mathbf{u}(\\mathbf{w}^{\\prime},S)\\right\\rVert^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Sum the above gives us that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\lambda\\left\\|\\mathbf{u}(\\mathbf{w},S)-\\mathbf{u}(\\mathbf{w}^{\\prime},S)\\right\\|^{2}}\\\\ &{\\le F_{S}(\\mathbf{u}(\\mathbf{w}^{\\prime},S),\\mathbf{w})-F_{S}(\\mathbf{u}(\\mathbf{w},S),\\mathbf{w})+F_{S}(\\mathbf{u}(\\mathbf{w},S),\\mathbf{w}^{\\prime})-F_{S}(\\mathbf{u}(\\mathbf{w}^{\\prime},S),\\mathbf{w}^{\\prime})}\\\\ &{=\\frac{\\lambda}{2}\\left(\\left\\|\\mathbf{u}(\\mathbf{w}^{\\prime},S)-\\mathbf{w}\\right\\|^{2}-\\left\\|\\mathbf{u}(\\mathbf{w},S)-\\mathbf{w}\\right\\|^{2}+\\left\\|\\mathbf{u}(\\mathbf{w},S)-\\mathbf{w}^{\\prime}\\right\\|^{2}-\\left\\|\\mathbf{u}(\\mathbf{w}^{\\prime},S)-\\mathbf{w}^{\\prime}\\right\\|^{2}\\right)}\\\\ &{=\\lambda\\left\\langle\\mathbf{u}(\\mathbf{w},S)-\\mathbf{u}(\\mathbf{w}^{\\prime},S),\\mathbf{w}-\\mathbf{w}^{\\prime}\\right\\rangle}\\\\ &{\\le\\lambda\\left\\|\\mathbf{u}(\\mathbf{w},S)-\\mathbf{u}(\\mathbf{w}^{\\prime},S)\\right\\|\\left\\|\\mathbf{w}-\\mathbf{w}^{\\prime}\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This gives us that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\mathbf{u}(\\mathbf{w},S)-\\mathbf{u}(\\mathbf{w}^{\\prime},S)\\|\\leq\\frac{1}{2}\\left\\|\\mathbf{w}-\\mathbf{w}^{\\prime}\\right\\|.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similarly, define $\\begin{array}{r}{\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime})=\\operatorname*{argmin}_{\\mathbf{u}\\in\\mathcal{W}}F_{S^{\\prime}}(\\mathbf{u},\\mathbf{w}^{\\prime})}\\end{array}$ we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F_{S}(\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime}),\\mathbf{w})-F_{S}(\\mathbf{u}(\\mathbf{w},S),\\mathbf{w})\\geq\\lambda\\left\\lVert\\mathbf{u}(\\mathbf{w},S)-\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime})\\right\\rVert^{2}}\\\\ {F_{S^{\\prime}}(\\mathbf{u}(\\mathbf{w},S),\\mathbf{w}^{\\prime})-F_{S^{\\prime}}(\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime}),\\mathbf{w}^{\\prime})\\geq\\lambda\\left\\lVert\\mathbf{u}(\\mathbf{w},S)-\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime})\\right\\rVert^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Sum the above gives us that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\lambda\\left\\|\\mathbf{u}(\\mathbf{w},S)-\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime})\\right\\|^{2}}\\\\ &{\\leq F_{S}(\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime}),\\mathbf{w})-F_{S}(\\mathbf{u}(\\mathbf{w},S),\\mathbf{w})+F_{S^{\\prime}}(\\mathbf{u}(\\mathbf{w},S),\\mathbf{w}^{\\prime})-F_{S^{\\prime}}(\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime}),\\mathbf{w}^{\\prime})}\\\\ &{=L(\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime}),S)-L(\\mathbf{u}(\\mathbf{w},S),S)+L(\\mathbf{u}(\\mathbf{w},S),S^{\\prime})-L(\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime}),S^{\\prime})}\\\\ &{\\qquad+\\frac{\\lambda}{2}\\left(\\left\\|\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime})-\\mathbf{w}\\right\\|^{2}-\\left\\|\\mathbf{u}(\\mathbf{w},S)-\\mathbf{w}\\right\\|^{2}+\\left\\|\\mathbf{u}(\\mathbf{w},S)-\\mathbf{w}^{\\prime}\\right\\|^{2}-\\left\\|\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime})-\\mathbf{w}^{\\prime}\\right\\|^{2}\\right)}\\\\ &{\\leq2G\\left\\|\\mathbf{u}(\\mathbf{w},S)-\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime})\\right\\|+\\lambda\\left\\langle\\mathbf{u}(\\mathbf{w},S)-\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime}),\\mathbf{w}-\\mathbf{w}^{\\prime}\\right\\rangle\\qquad\\qquad(\\ell\\,\\mathrm{is\\}G\\mathbf{-Lipschit})}\\\\ &{\\leq2G\\left\\|\\mathbf{u}(\\mathbf{w},S)-\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime})\\right\\|+\\lambda\\left\\|\\mathbf{u}(\\mathbf{w},S)-\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime})\\right\\|\\left\\|\\mathbf{w}-\\mathbf{w}^{\\prime}\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This gives us that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lVert\\mathbf{u}(\\mathbf{w},S)-\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime})\\rVert\\leq\\frac{1}{2}\\left\\lVert\\mathbf{w}-\\mathbf{w}^{\\prime}\\right\\rVert+\\frac{G}{\\lambda}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Finally, at iteration $t$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\left\\|{\\bf{w}}_{t+1}-{\\bf{w}}_{t+1}^{\\prime}\\right\\|}\\le\\left\\|{\\bf{w}}_{t}-\\gamma\\lambda\\left({\\bf{w}}_{t}-\\frac{1}{m}\\sum_{j=1}^{m}{\\bf{u}}({\\bf{w}}_{t},\\mathcal{S}_{j})\\right)-{\\bf{w}}_{t}^{\\prime}+\\gamma\\lambda\\left({\\bf{w}}_{t}^{\\prime}-\\frac{1}{m}\\sum_{j=1}^{m}{\\bf{u}}({\\bf{w}}_{t}^{\\prime},\\mathcal{S}_{j}^{\\prime})\\right)\\right\\|\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "(Projection is non-expansive) ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\left\\|(1-\\gamma\\lambda)(\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime})+\\gamma\\lambda\\displaystyle\\frac{1}{m}\\sum_{j=1}^{m}\\left(\\mathbf{u}(\\mathbf{w}_{t},S_{j})-\\mathbf{u}(\\mathbf{w}_{t}^{\\prime},S_{j}^{\\prime})\\right)\\right\\|}\\\\ &{\\leq(1-\\gamma\\lambda)\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+\\gamma\\lambda\\left(\\displaystyle\\frac{m-1}{m}\\displaystyle\\frac{1}{2}\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+\\displaystyle\\frac{1}{m}\\left(\\frac{1}{2}\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+\\displaystyle\\frac{G}{\\lambda}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(\\mathrm{Equation}\\left(3\\right),\\left(4\\right))}\\\\ &{=(1-\\frac{\\gamma\\lambda}{2})\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+\\frac{\\gamma G}{m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Choose $\\textstyle\\gamma\\leq{\\frac{1}{\\lambda}},\\forall t$ . Rearrange gives us that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\left\\|\\mathbf{w}_{t+1}-\\mathbf{w}_{t+1}^{\\prime}\\right\\|}{\\left(1-\\gamma\\lambda/2\\right)^{t+1}}\\leq\\frac{\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|}{\\left(1-\\gamma\\lambda/2\\right)^{t}}+\\frac{\\gamma G}{m}\\frac{1}{\\left(1-\\gamma\\lambda/2\\right)^{t+1}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that at initialization when $t=1$ wehave $\\|\\mathbf{w}_{1}-\\mathbf{w}_{1}^{\\prime}\\|=0$ Telescoping from $t=1$ to $T+1$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\left\\|\\mathbf{w}_{T+1}-\\mathbf{w}_{T+1}^{\\prime}\\right\\|}{\\left(1-\\gamma\\lambda/2\\right)^{T}}\\leq\\frac{\\gamma G}{m}\\sum_{t=1}^{T-1}\\frac{1}{\\left(1-\\gamma\\lambda/2\\right)^{t+1}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Calculate gives us that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{w}_{T+1}-\\mathbf{w}_{T+1}^{\\prime}\\right\\|\\leq{\\frac{2G}{\\lambda m}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similarly, define $\\begin{array}{r}{\\mathbf{u}(\\mathbf{w}_{T+1},S)=\\operatorname*{argmin}_{\\mathbf{u}\\in\\mathcal{W}}F_{S}(\\mathbf{u},\\mathbf{w}_{T+1}^{\\prime}),}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "$\\begin{array}{r}{\\mathbf{u}(\\mathbf{w}_{T+1}^{\\prime},S^{(i)})=\\operatorname*{argmin}_{\\mathbf{u}\\in\\mathcal{W}}F_{S^{(i)}}(\\mathbf{u},\\mathbf{w}_{T}^{\\prime})}\\end{array}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F_{S}(\\mathbf{u}(\\mathbf{w}_{T+1}^{\\prime},S^{(i)}),\\mathbf{w}_{T+1})-F_{S}(\\mathbf{u}(\\mathbf{w}_{T+1},S),\\mathbf{w}_{T+1})\\geq\\lambda\\left\\|\\mathbf{u}(\\mathbf{w}_{T+1},S)-\\mathbf{u}(\\mathbf{w}_{T+1}^{\\prime},S^{(i)})\\right\\|^{2}}\\\\ {F_{S^{(i)}}(\\mathbf{u}(\\mathbf{w}_{T+1},S),\\mathbf{w}_{T+1}^{\\prime})-F_{S^{(i)}}(\\mathbf{u}(\\mathbf{w}_{T+1}^{\\prime},S^{(i)}),\\mathbf{w}_{T+1}^{\\prime})\\geq\\lambda\\left\\|\\mathbf{u}(\\mathbf{w}_{T+1},S)-\\mathbf{u}(\\mathbf{w}_{T+1}^{\\prime},S^{(i)})\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Sum the above gives us that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\mathbb{I}\\Bigg\\|\\mathbf{u}\\Bigg(\\mathbf{w}_{T+1,S})-\\mathbf{u}(\\mathbf{w}_{T+1,S}^{\\prime},S^{(i)})\\Bigg\\|^{2}}\\\\ &{\\leq F_{\\mathcal{S}}(\\mathbf{u}(\\mathbf{w}_{T+1,S}^{\\prime},S),\\mathbf{w}_{T+1}^{\\prime})-F_{\\mathcal{S}}(\\mathbf{u}(\\mathbf{w}_{T+1,S}^{\\prime},S),\\mathbf{w}_{T+1})}\\\\ &{\\qquad+F_{\\mathcal{S},\\tau_{0}}(\\mathbf{u}(\\mathbf{w}_{T+1,S}^{\\prime},\\mathbf{w}_{T+1}^{\\prime})-F_{\\mathcal{S},\\tau_{0}}(\\mathbf{u}(\\mathbf{w}_{T+1,S}^{\\prime},S^{(i)}),\\mathbf{w}_{T+1}^{\\prime}))}\\\\ &{=L(\\mathbf{u}(\\mathbf{w}_{T+1,S}^{\\prime},S^{(i)}),S)-L(\\mathbf{u}(\\mathbf{w}_{T+1,S}^{\\prime},S),S)+L(\\mathbf{u}(\\mathbf{w}_{T+1,S}^{\\prime},S),S^{(i)})-L(\\mathbf{u}(\\mathbf{w}_{T+1,S}^{\\prime},S^{(i)}),S^{(i)})}\\\\ &{\\qquad+\\frac{\\lambda}{2}\\Bigg\\|\\bigg(\\mathbf{u}(\\mathbf{w}_{T+1,S}^{\\prime},S)-\\mathbf{w}_{T+1}\\bigg)^{2}-\\mathbb{I}\\Bigg\\|\\mathbf{u}(\\mathbf{w}_{T+1,S})-\\mathbf{w}_{T+1}\\bigg\\|^{2}}\\\\ &{\\qquad\\qquad+\\frac{\\lambda}{2}\\Bigg\\|\\mathbf{u}(\\mathbf{w}_{T+1,S})-\\mathbf{w}_{T+1}^{\\prime}\\Bigg\\|^{2}-\\mathbb{I}\\Bigg\\|\\mathbf{u}(\\mathbf{w}_{T+1,S}^{\\prime},S^{(i)})-\\mathbf{w}_{T+1}^{\\prime}\\Bigg\\|^{2}\\Bigg\\rangle}\\\\ &{\\leq\\frac{2C}{n}\\left\\|\\mathbf{u}(\\mathbf{w}_{T+1,S}^{\\prime})-\\mathbf{u}(\\mathbf{w}_{T+1,S}^{\\prime},S^{(i)})\\right\\|+\\lambda\\bigg\\langle\\mathbf{u}(\\mathbf{w}_{T+1,S}^{\\prime},S^{(i)})-\\mathbf{u}(\\mathbf{w}_{T+1,S}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This gives us that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{u}(\\mathbf{w}_{T+1},S)-\\mathbf{u}(\\mathbf{w}_{T+1}^{\\prime},S^{(i)})\\right\\|\\leq\\frac{1}{2}\\left\\|\\mathbf{w}_{T+1}-\\mathbf{w}_{T+1}^{\\prime}\\right\\|+\\frac{2G}{\\lambda n}\\leq\\frac{G}{\\lambda m}+\\frac{2G}{\\lambda n}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We now consider the surrogate loss $\\ell$ is convex, non-negative and $H$ -smooth. Note that such loss is also self-bounded. From a similar argument, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\mathbf{u}(\\mathbf{w},S)-\\mathbf{u}(\\mathbf{w}^{\\prime},S)\\|\\leq\\frac{1}{2}\\left\\|\\mathbf{w}-\\mathbf{w}^{\\prime}\\right\\|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Moreover, ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\lambda\\left\\lVert\\mathbf{u}(\\mathbf{w},S)-\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime})\\right\\rVert^{2}}\\\\ &{\\le E_{S}(\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime}),\\mathbf{w})-E_{S}(\\mathbf{u}(\\mathbf{w},S),\\mathbf{w})+F_{S^{\\prime}}(\\mathbf{u}(\\mathbf{w},S),\\mathbf{w}^{\\prime})-F_{S^{\\prime}}(\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime}),\\mathbf{w}^{\\prime})}\\\\ &{=L(\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime}),S)-L(\\mathbf{u}(\\mathbf{w},S),S)+L(\\mathbf{u}(\\mathbf{w},S),S^{\\prime})-L(\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime}),S^{\\prime})}\\\\ &{\\qquad+\\frac{\\lambda}{2}\\left(\\left\\lVert\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime})-\\mathbf{w}\\right\\rVert^{2}-\\left\\lVert\\mathbf{u}(\\mathbf{w},S)-\\mathbf{w}\\right\\rVert^{2}+\\left\\lVert\\mathbf{u}(\\mathbf{w},S)-\\mathbf{w}^{\\prime}\\right\\rVert^{2}-\\left\\lVert\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime})-\\mathbf{w}^{\\prime}\\right\\rVert^{2}\\right)}\\\\ &{\\le(\\left\\lVert\\nabla L(\\mathbf{u}(\\mathbf{w},S),S)\\right\\rVert+\\left\\lVert\\nabla L(\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime}),S^{\\prime})\\right\\rVert)\\left\\lVert\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime})\\!-\\!\\mathbf{u}(\\mathbf{w},S)\\right\\rVert\\!+\\!H\\left\\lVert\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime})-\\mathbf{u}(\\mathbf{w},S)\\right\\rVert^{2}}\\\\ &{\\qquad+\\lambda\\left\\langle\\mathbf{u}(\\mathbf{w},S)-\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime}),\\mathbf{w}-\\mathbf{w}^{\\prime}\\right\\rangle}\\\\ &{\\le\\left(\\sqrt{2H L(\\mathbf{u}(\\mathbf{w},S),S)}+\\sqrt{2H L(\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime}),S^{\\prime})}\\right)\\left\\lVert\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime})-\\mathbf{u}(\\mathbf{w},S)\\right\\rVert}\\\\ &{\\qquad+H\\left\\lVert\\mathbf{u}(\\mathbf \n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which is equivalent as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\|\\mathbf{u}(\\mathbf{w},S)-\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime})\\|\\leq\\frac{\\sqrt{2H L}\\left(\\mathbf{u}(\\mathbf{w},S),S\\right)+\\sqrt{2H L}\\left(\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime}),S^{\\prime}\\right)+\\lambda\\left\\|\\mathbf{w}-\\mathbf{w}^{\\prime}\\right\\|}{2\\lambda-H}\\;\\;(\\lambda\\geq H)}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\sqrt{2H}}{\\lambda}\\left(\\sqrt{L(\\mathbf{u}(\\mathbf{w},S),S)}+\\sqrt{L(\\mathbf{u}(\\mathbf{w}^{\\prime},S^{\\prime}),S^{\\prime})}\\right)+\\left\\|\\mathbf{w}-\\mathbf{w}^{\\prime}\\right\\|\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(6)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, at iteration $t$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|\\mathbf{w}_{t+1}-\\mathbf{w}_{t+1}^{\\prime}\\right\\|}\\\\ {\\displaystyle\\leq\\left\\|\\mathbf{w}_{t}-\\gamma\\lambda\\left(\\mathbf{w}_{t}-\\frac{1}{m}\\sum_{j=1}^{m}\\mathbf{u}(\\mathbf{w}_{t},S_{j})\\right)-\\mathbf{w}_{t}^{\\prime}+\\gamma\\lambda\\left(\\mathbf{w}_{t}^{\\prime}-\\frac{1}{m}\\sum_{j=1}^{m}\\mathbf{u}(\\mathbf{w}_{t}^{\\prime},S_{j}^{\\prime})\\right)\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "(Projection is non-expansive) ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle=\\left\\|(1-\\gamma\\lambda)(\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime})+\\gamma\\lambda\\frac{1}{m}\\sum_{j=1}^{m}\\left(\\mathbf{u}(\\mathbf{w}_{t},S_{j})-\\mathbf{u}(\\mathbf{w}_{t}^{\\prime},S_{j}^{\\prime})\\right)\\right\\|}\\\\ {\\displaystyle\\leq(1-\\gamma\\lambda)\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+\\gamma\\lambda\\Biggl(\\frac{m-1}{m}\\frac{1}{2}\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|}\\\\ {\\displaystyle\\qquad+\\,\\frac{1}{m}\\left(\\frac{\\sqrt{2H}}{\\lambda}\\left(\\sqrt{L(\\mathbf{u}(\\mathbf{w}_{t},S_{j}),S_{j})}+\\sqrt{L(\\mathbf{u}(\\mathbf{w}_{t}^{\\prime},S_{j}^{\\prime}),S_{j}^{\\prime})}\\right)+\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|\\right)\\Biggr)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n=\\left(1-\\frac{m+1}{2m}\\gamma\\lambda\\right)\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\|+\\frac{\\gamma\\sqrt{2H}}{m}\\left(\\sqrt{L(\\mathbf{u}(\\mathbf{w}_{t},S_{j}),S_{j})}+\\sqrt{L(\\mathbf{u}(\\mathbf{w}_{t}^{\\prime},S_{j}^{\\prime}),S_{j}^{\\prime})}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Telescope gives us that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\left|\\mathbf{w}_{T+1}-\\mathbf{w}_{T+1}^{\\prime}\\right|\\le\\displaystyle\\frac{\\gamma\\sqrt{2H\\lambda}}{m}\\sum_{t=1}^{T}\\left(1-\\frac{m+1}{2m}\\gamma\\lambda\\right)^{T-t}\\!\\!\\left(\\sqrt{L(\\mathbf{u}(\\mathbf{w}_{t},S_{j}),S_{j})}\\!+\\!\\sqrt{L(\\mathbf{u}(\\mathbf{w}_{t}^{\\prime},S_{j}^{\\prime}),S_{j}^{\\prime})}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\le\\displaystyle\\frac{4\\sqrt{2H M}}{\\lambda(m+1)},}&{(7)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last line holds if we consider $M$ -bounded loss. Otherwise, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\big\\|\\mathbf{w}_{T+1}-\\mathbf{w}_{T+1}^{\\prime}\\big\\|\\leq\\frac{2\\sqrt{2H}}{\\lambda(m+1)}\\left(\\sqrt{\\operatorname*{max}_{t\\in[T]}L(\\mathbf{u}(\\mathbf{w}_{t},\\mathcal{S}_{j}),\\mathcal{S}_{j})}+\\sqrt{\\operatorname*{max}_{t\\in[T]}L(\\mathbf{u}(\\mathbf{w}_{t}^{\\prime},\\mathcal{S}_{j}^{\\prime}),\\mathcal{S}_{j}^{\\prime})}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\lambda\\left\\|\\mathfrak{u}(\\mathbf{w},S)-\\mathfrak{u}(\\mathbf{w}^{\\prime},S^{(i)})\\right\\|^{2}}\\\\ &{\\le F_{0}(\\mathbf{w}^{\\prime},S^{(i)}),\\mathbf{w})-F_{0}(\\mathbf{w}(\\mathbf{w},S),\\mathbf{w})+F_{S^{(i)}}(\\mathbf{u}(\\mathbf{w},S),\\mathbf{w}^{\\prime})-F_{S^{(i)}}(\\mathbf{u}(\\mathbf{w}^{\\prime},S^{(i)}),\\mathbf{w}^{\\prime})}\\\\ &{=L(\\mathfrak{w}^{\\prime},S^{(i)}),S)-L(\\mathfrak{u}(\\mathbf{w},S),S)+L(\\mathfrak{u}(\\mathbf{w},S),S^{(i)})-L(\\mathfrak{u}(\\mathbf{w}^{\\prime},S^{(i)}),S^{(i)})}\\\\ &{\\qquad+\\frac{\\lambda}{2}\\left(\\left\\|\\mathfrak{u}(\\mathbf{w}^{\\prime},S^{(i)})-\\mathfrak{w}\\right\\|^{2}-\\left\\|\\mathfrak{u}(\\mathbf{w},S)-\\mathbf{w}\\right\\|^{2}+\\left\\|\\mathfrak{u}(\\mathbf{w},S)-\\mathbf{w}^{\\prime}\\right\\|^{2}-\\left\\|\\mathfrak{u}(\\mathbf{w}^{\\prime},S^{(i)})-\\mathbf{w}^{\\prime}\\right\\|^{2}\\right)}\\\\ &{\\le\\frac{1}{n}\\left(\\varepsilon(\\mathfrak{w}^{\\prime},S^{(i)}),z^{i}-\\varepsilon(\\mathfrak{u}(\\mathbf{w}^{\\prime},S^{(i)}),z^{\\prime})+\\varepsilon(\\mathfrak{u}(\\mathbf{w}^{\\prime},S),z^{\\prime})-\\ell(\\mathfrak{u}(\\mathbf{w}^{\\prime},S),z^{\\prime})\\right)}\\\\ &{\\qquad+\\lambda\\left\\langle\\mathfrak{u}(\\mathbf{w},S)-\\mathfrak{u}(\\mathbf{w}^{\\prime},S^{(i)}),\\mathbf{w}-\\mathbf{w}^{\\prime}\\right\\rangle}\\\\ &{\\le\\frac{1}{n}\\left(\\sqrt{2H\\ell(\\mathfrak{u}(\\mathbf{w},S),z^{\\prime})}+\\sqrt{2H\\ell(\\mathfrak{w}(\\mathbf{w}^{\\prime},S^{(i)}),z^{\\prime})}\\right)\\left\\|\\mathfrak{u}(\\mathbf{w}^{\\prime},S^{(i)})-\\mathfrak{u}(\\mathbf{w},S)\\right\\|}\\\\ &{\\qquad+\\frac{H}{n}\\left\\|\\mathfrak{u}(\\mathbf{w}^\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Rearrange gives us, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{u}(\\mathbf{w},S)-\\mathbf{u}(\\mathbf{w}^{\\prime},S^{(i)})\\right\\|\\leq\\displaystyle\\frac{1}{2\\lambda n-H}\\left(\\sqrt{2H\\ell(\\mathbf{u}(\\mathbf{w},S),\\mathbf{z}^{i})}+\\sqrt{2H\\ell(\\mathbf{u}(\\mathbf{w}^{\\prime},S^{(i)}),\\mathbf{z}^{\\prime})}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\frac{\\lambda n}{2\\lambda n-H}\\left\\|\\mathbf{w}-\\mathbf{w}^{\\prime}\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Plug in $\\mathrm{w}_{T+1}$ and $\\mathrm{w}_{T+1}^{\\prime}$ gives us that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{u}(\\mathbf{w}_{T+1},S)-\\mathbf{u}(\\mathbf{w}_{T+1}^{\\prime},S^{(i)})\\right\\|\\leq\\frac{2\\sqrt{2H M}}{2\\lambda n-H}+\\frac{n}{2\\lambda n-H}\\frac{4\\sqrt{2H M}}{(m+1)}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "If we apply Lemma 4.1 and Lemma C.1 with Theorem 2.2, we have the following theorem. ", "page_idx": 22}, {"type": "text", "text": "Theorem C.2. The following holds for Algorithm 1 with step-size $\\gamma\\le\\frac{1}{\\lambda}$ on a given meta-sample S, and RERM for task-specific learning (i.e., Option 1 for Algorithm 2), for all $T\\geq1$ ", "page_idx": 22}, {"type": "text", "text": "1. For convex, $M$ -bounded, and $G$ -Lipschitz loss functions, with probability at least $1-\\delta$ ", "page_idx": 22}, {"type": "equation", "text": "$$\nL(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mu)\\lesssim L(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mathbf{S})+\\frac{G^{2}}{\\lambda m}\\log\\left(m\\right)\\log\\left(1/\\delta\\right)+\\frac{M}{\\sqrt{m}}\\sqrt{\\log\\left(1/\\delta\\right)}+\\frac{G^{2}}{\\lambda n}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "2. For convex, $M$ bounded, and $H$ -smooth loss functions $(H\\leq\\lambda)$ , with probability at least $1-\\delta$ ", "page_idx": 22}, {"type": "equation", "text": "$$\nL(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mu)\\lesssim L(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mathbf{S})+\\frac{H M}{(m+1)\\lambda}\\log{(m)}\\log{(1/\\delta)}+\\frac{M}{\\sqrt{m}}\\sqrt{\\log{(1/\\delta)}}+\\frac{H M}{\\lambda n}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof of Theorem C.2. We slightly abuse the notation, at iteration $t$ ,define $\\mathbf{w}_{t}\\,=\\,A(\\mathbf{S})$ $\\mathbf{w}_{t}^{\\prime}\\,=\\,\\,$ $A(\\mathbf{S}^{(j)}),\\mathbf{u}(\\mathbf{w}_{T+1},S)=A(\\mathbf{S})(S),\\mathbf{u}(\\mathbf{w}_{T+1}^{\\prime},S^{(i)})=A(\\mathbf{S}^{(j)})(S^{(i)}).$ Apply Lemma 4.1 gives us that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\Big|L(\\boldsymbol{\\mathcal{A}}(\\mathbf{S})(\\mathcal{S}),\\mathcal{S})-L(\\boldsymbol{\\mathcal{A}}(\\mathbf{S}^{(j)})(\\mathcal{S}),\\mathcal{S})\\Big|=\\big|L(\\mathbf{u}(\\mathbf{w}_{T+1},\\mathcal{S}),\\mathcal{S})-L(\\mathbf{u}(\\mathbf{w}_{T+1}^{\\prime},\\mathcal{S}),\\mathcal{S})\\big|}}\\\\ &{}&{\\quad\\le G\\left\\|\\mathbf{u}(\\mathbf{w}_{T+1},\\mathcal{S})-\\mathbf{u}(\\mathbf{w}_{T+1}^{\\prime},\\mathcal{S})\\right\\|~~~~~}&{(G\\mathrm{-Lipschitz})}\\\\ &{}&{\\quad\\le\\displaystyle\\frac{G}{2}\\left\\|\\mathbf{w}_{T+1}-\\mathbf{w}_{T+1}^{\\prime}\\right\\|~~~~~}&{(\\mathrm{Equation}~(3))}\\\\ &{}&{\\quad\\le\\displaystyle\\frac{G^{2}}{\\lambda m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Apply Lemma C.1 gives us that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Big|\\ell(\\boldsymbol{\\mathcal{A}}(\\mathbf{S})(S),\\mathbf{z})-\\ell(\\boldsymbol{\\mathcal{A}}(\\mathbf{S})(S^{(i)}),\\mathbf{z})\\Big|=\\Big|\\ell(\\mathbf{u}(\\mathbf{w}_{T+1},S),\\mathbf{z})-\\ell(\\mathbf{u}(\\mathbf{w}_{T+1},S^{(i)}),\\mathbf{z})\\Big|}&{}\\\\ {\\le G\\left\\|\\mathbf{u}(\\mathbf{w}_{T+1},S)-\\mathbf{u}(\\mathbf{w}_{T+1},S^{(i)})\\right\\|}&{}\\\\ {\\le\\displaystyle\\frac{4G^{2}}{\\lambda n}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Apply Theorem 2.2 with $\\begin{array}{r}{\\beta^{\\prime}=\\frac{G^{2}}{\\lambda m}}\\end{array}$ \uff0c $\\begin{array}{r}{\\beta=\\frac{4G^{2}}{\\lambda n}}\\end{array}$ achieves the results. ", "page_idx": 22}, {"type": "text", "text": "Similarly, if the loss is $M$ -bounded, convex, non-negative and $H$ -smooth, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\lefteqn{L(A(\\mathbf{S})(S),S)-L(A(\\mathbf{S}^{(j)})(S),S)\\big|}}\\\\ &{=\\left|L(\\mathbf{u}(\\mathbf{w}_{T+1},S),S)-L(\\mathbf{u}(\\mathbf{w}_{T+1}^{\\prime},S),S)\\right|}\\\\ &{\\leq\\sqrt{2H L(\\mathbf{u}(\\mathbf{w}_{T+1},S),S)}\\left|\\|\\mathbf{u}(\\mathbf{w}_{T+1},S)-\\mathbf{u}(\\mathbf{w}_{T+1}^{\\prime},S)\\|+\\frac{H}{2}\\left\\|\\mathbf{u}(\\mathbf{w}_{T+1},S)-\\mathbf{u}(\\mathbf{w}_{T+1}^{\\prime},S)\\right\\|^{2}}\\\\ &{}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\leq\\sqrt{2H M}\\frac{1}{2}\\left\\|\\mathbf{w}_{T+1}-\\mathbf{w}_{T+1}^{\\prime}\\right\\|+\\frac{H}{8}\\left\\|\\mathbf{w}_{T+1}-\\mathbf{w}_{T+1}^{\\prime}\\right\\|^{2}}\\\\ &{\\leq\\frac{4H M}{(m+1)\\lambda}+\\frac{4H^{2}M}{(m+1)^{2}\\lambda^{2}}}\\\\ &{\\leq\\frac{8H M}{(m+1)\\lambda}\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(\\lambda\\geq H)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Apply Lemma C.1 gives us that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\lefteqn{\\frac{\\d\\ell(A(\\mathbf{S})(S),\\mathbf{z})-\\ell(A(\\mathbf{S})(S^{(i)}),\\mathbf{z})}{\\d t}}\\Big|}\\\\ &{=\\left|\\ell(\\mathbf{u}(\\mathbf{w}_{T+1},S),\\mathbf{z})-\\ell(\\mathbf{u}(\\mathbf{w}_{T+1},S^{(i)}),\\mathbf{z})\\right|}\\\\ &{\\leq\\sqrt{2H\\ell(\\mathbf{u}(\\mathbf{w}_{T+1},S),\\mathbf{z})}\\left\\|\\mathbf{u}(\\mathbf{w}_{T+1},S)-\\mathbf{u}(\\mathbf{w}_{T+1},S^{(i)})\\right\\|+\\frac{H}{2}\\left\\|\\mathbf{u}(\\mathbf{w}_{T+1},S)-\\mathbf{u}(\\mathbf{w}_{T+1},S^{(i)})\\right\\|^{2}}\\\\ &{\\leq\\frac{8H M}{\\lambda n}+\\frac{16H^{2}M}{\\lambda^{2}n^{2}}}\\\\ &{\\leq\\frac{24H M}{\\lambda n}\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad(\\lambda\\geq H)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Apply Theorem 2.2 with $\\begin{array}{r}{\\beta^{\\prime}=\\frac{8H M}{(m+1)\\lambda}}\\end{array}$ (m+1)x, \u03b2= 2 $\\beta={\\frac{24H M}{\\lambda n}}$ achieves the results. ", "page_idx": 23}, {"type": "text", "text": "Theorem 4.2. The following holds for Algorithm 1 with step-size $\\gamma\\leq\\frac{1}{\\lambda}$ on a given meta-sample S, and RERM for task-specific learning (i.e., Option 1 for Algorithm 2), for all $T\\geq1$ ", "page_idx": 23}, {"type": "text", "text": "1.For convex, $M$ -bounded, and $G$ -Lipschitz loss functions, with probability at least $1-\\delta$ ", "page_idx": 23}, {"type": "equation", "text": "$$\nL(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mu)\\lesssim L(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mathbf{S})+\\left(\\frac{G^{2}}{\\lambda n}+\\frac{G^{2}}{\\lambda m}\\right)\\log\\left(m n\\right)\\log\\left(1/\\delta\\right)+\\frac{M\\sqrt{\\log\\left(1/\\delta\\right)}}{\\sqrt{m n}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "2.For convex, $M$ bounded, and $H$ smooth loss functions $(H\\leq\\lambda)$ , with probability at least $1-\\delta$ ", "page_idx": 23}, {"type": "equation", "text": "$$\nL(\\boldsymbol{A}(\\mathbf{S}),\\mu)\\!\\lesssim\\!L(\\boldsymbol{A}(\\mathbf{S}),\\mathbf{S})\\!+\\!\\left(\\!\\frac{H M}{(2n-1)\\lambda}\\!+\\!\\frac{H M}{(m+1)\\lambda}\\right)\\log{(m n)}\\log{(1/\\delta)}\\!+\\!\\frac{M\\!\\sqrt{\\log{(1/\\delta)}}}{\\sqrt{m n}}\\!.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Theorem 4.2. We slightly abuse the notation, at iteration $t$ define $\\mathbf{w}_{t}\\;=\\;\\mathcal{A}(\\mathbf{S}),\\;\\mathbf{w}_{t}^{\\prime}\\;=$ $A(\\mathbf{S}^{(j)}),\\,\\mathbf{u}(\\mathbf{w}_{T+1},S)\\,=\\,A(\\mathbf{S})(S),\\mathbf{u}(\\mathbf{w}_{T+1}^{\\prime},S^{(i)})\\,=\\,A(\\mathbf{S}^{(j)})(S^{(i)})$ .For $\\ell$ to be convex and $G$ Lipschitz, applying Lemma 4.1 gives us that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\ell(\\boldsymbol{\\mathcal{A}}(\\mathbf{S})(\\mathcal{S}),\\mathbf{z})-\\ell(\\boldsymbol{\\mathcal{A}}(\\mathbf{S}^{(j)})(\\mathcal{S}^{(i)}),\\mathbf{z})\\right|=\\left|\\ell(\\mathbf{u}(\\mathbf{w}_{T+1},\\mathcal{S}),\\mathbf{z})-\\ell(\\mathbf{u}(\\mathbf{w}_{T+1}^{\\prime},\\mathcal{S}^{(i)}),\\mathbf{z})\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq G\\left\\|\\mathbf{u}(\\mathbf{w}_{T+1},\\mathcal{S})-\\mathbf{u}(\\mathbf{w}_{T+1}^{\\prime},\\mathcal{S}^{(i)})\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{G^{2}}{\\lambda m}+\\frac{2G^{2}}{\\lambda n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Further apply Theorem 3.1 gives us the result. For $\\ell$ to be convex, $M$ -bounded and $H$ -smooth, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\ell(A(\\mathbf{S})(\\mathcal{S}),z)-\\ell(A(\\mathbf{S}^{(j)})(\\mathcal{S}^{(i)}),z)\\Big|}}\\\\ &{=\\left|\\ell(\\mathfrak{u}(\\mathbf{w}_{T+1},\\mathcal{S}),z)-\\ell(\\mathfrak{u}(\\mathbf{w}_{T+1}^{\\prime},\\mathcal{S}^{(i)}),z)\\right|}\\\\ &{\\leq\\sqrt{2H\\ell(\\mathfrak{u}(\\mathbf{w}_{T+1},\\mathcal{S}),z)}\\left\\|\\mathfrak{u}(\\mathbf{w}_{T+1},\\mathcal{S})-\\mathfrak{u}(\\mathbf{w}_{T+1}^{\\prime},\\mathcal{S}^{(i)})\\right\\|+\\frac{H}{2}\\left\\|\\mathfrak{u}(\\mathbf{w}_{T+1},\\mathcal{S})-\\mathfrak{u}(\\mathbf{w}_{T+1}^{\\prime},\\mathcal{S}^{(i)})\\right\\|^{2}}\\\\ &{\\leq\\sqrt{2H M}\\left(\\frac{2\\sqrt{2H M}}{2\\lambda n-H}+\\frac{n}{2\\lambda n-H}\\frac{4\\sqrt{2H M}}{(m+1)}\\right)+\\frac{H}{2}\\left(\\frac{2\\sqrt{2H M}}{2\\lambda n-H}+\\frac{n}{2\\lambda n-H}\\frac{4\\sqrt{2H M}}{(m+1)}\\right)^{2}}\\\\ &{\\leq\\frac{4H M}{(2n-1)\\lambda}+\\frac{8H M}{(m+1)\\lambda}+\\frac{8H^{2}M}{(2n-1)^{2}\\lambda^{2}}+\\frac{16H^{2}M}{(m+1)^{2}\\lambda^{2}}}\\\\ &{\\leq\\frac{12H M}{(2n-1)\\lambda}+\\frac{24H M}{(m+1)\\lambda}}&{(H\\leq\\lambda)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Apply Theorem 3.1 gives the results. ", "page_idx": 23}, {"type": "text", "text": "Lemma 4.3. Assume that the loss function is convex, $G$ -Lipschitz and $H$ -smooth. Let S, $\\mathbf{S}^{(j)}$ denote neighboring meta-samples and $\\boldsymbol{S}$ \uff0c $S^{(i)}$ the neighboring samples on a test task. Then the following holds for Algorithm 1 with GD for task-specific learning (i.e., Option 2 for Algorithm 2) with $\\begin{array}{r}{\\eta\\le\\frac{2}{H+2\\lambda}}\\end{array}$ for ali $T\\geq1$ as long as we set $\\begin{array}{r}{\\gamma\\leq\\frac{1}{\\lambda T}}\\end{array}$ \uff0c ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbf{S},S,j\\in[m],i\\in[n]}\\left\\|A(\\mathbf{S})(S)-A(\\mathbf{S}^{(j)})(S^{(i)})\\right\\|\\leq\\frac{4e G}{\\lambda m}+\\frac{2G}{\\lambda n}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof of Lemma 4.3. We slightly abuse the notation, at iteration $t$ , define $\\mathbf{w}_{t}=\\mathcal{A}(\\mathbf{S})$ \uff0c $\\mathbf{w}_{t}^{\\prime}=\\mathcal{A}(\\mathbf{S}^{(j)})$ $\\mathbf{u}(\\mathbf{w}_{T+1},S)\\;=\\;{\\boldsymbol{A}}(\\mathbf{S})(S),\\mathbf{u}(\\mathbf{w}_{T+1}^{\\prime},S^{(i)})\\;=\\;{\\boldsymbol{A}}(\\mathbf{S}^{(j)})(S^{(i)})$ .  Recall that $F_{S}(\\mathbf{u},\\mathbf{w})\\ =\\ L(\\mathbf{u},S)\\ +$ $\\frac{\\lambda}{2}\\left\\|\\mathbf{u}-\\mathbf{w}\\right\\|^{2}$ If $\\ell$ is convex, then $F_{S}(\\mathbf{u},\\mathbf{w})$ .s $\\lambda$ strongly-convex w.r.t u. If $\\ell$ .s $H$ smooth, then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\langle\\nabla L(\\mathbf{u},S)-\\nabla L(\\mathbf{v},S),\\mathbf{u}-\\mathbf{v}\\rangle\\ge\\frac{1}{H}\\left\\lVert\\nabla L(\\mathbf{u},S)-\\nabla L(\\mathbf{v},S)\\right\\rVert^{2}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Given $S,S^{\\prime}$ , for any $\\mathbf{w},\\mathbf{w}^{\\prime}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{u}^{(k+1)}(\\mathbf{w},S)-\\mathbf{u}^{(k+1)}(\\mathbf{w}^{\\prime},S^{\\prime})\\right\\|}\\\\ &{\\leq\\bigg\\|\\mathbf{u}^{(k)}(\\mathbf{w},S)-\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},S^{\\prime})-\\eta\\Big(\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w},S),S)+\\lambda\\left(\\mathbf{u}^{(k)}(\\mathbf{w},S)-\\mathbf{w}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "(Projection is non-expansive) ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\,\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},S^{\\prime}),S^{\\prime})-\\lambda\\left(\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},S^{\\prime})-\\mathbf{w}^{\\prime}\\right)\\right)\\Bigg\\Vert}\\\\ &{\\quad\\quad-\\,\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w},S)-\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},S^{\\prime})\\Big\\Vert+\\eta\\lambda\\left\\Vert\\mathbf{w}-\\mathbf{w}^{\\prime}\\right\\Vert+2\\eta G}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Given $\\boldsymbol{S}$ , for any $\\mathbf{w},\\mathbf{w}^{\\prime}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|u^{k+1}(u,\\hat{S},\\cdot)-u^{k+1}(u^{k},\\hat{S},\\cdot)\\right|}\\\\ &{\\leq\\left|u^{k}(u,\\hat{S},\\cdot)u^{k}(u^{k},\\hat{S})\\right|}\\\\ &{\\qquad-\\eta\\left(\\nabla L(u^{k}(u,\\hat{S}),\\cdot)+\\lambda(u^{k}(u,\\hat{S})-u)\\cdot\\nabla L(u^{k}(u^{k},\\hat{S}),\\cdot)-\\lambda(u^{k}(u^{k},\\hat{S})-u)\\right|}\\\\ &{=\\left|u^{k}(u^{k},\\hat{S},\\cdot)u^{k}(u^{k},\\hat{S})+\\lambda(u^{k}-u^{k})\\right|}\\\\ &{\\qquad-\\eta\\left(\\nabla L(u^{k}(u,\\hat{S}),\\cdot)-\\nabla L(u^{k},\\hat{S})+\\lambda(u^{k}(u,\\hat{S})-u^{k})\\right|\\Big|}\\\\ &{\\qquad\\qquad\\times\\left|\\left(u^{k},\\frac{\\rho}{\\sqrt{n}}\\right)+\\frac{\\lambda}{2}\\left(\\nabla L^{2}u^{k},\\frac{\\rho}{\\sqrt{n}}\\right)\\times\\left|\\left(u^{k},\\frac{\\rho}{\\sqrt{n}}\\right)\\right|}\\\\ &{\\leq\\lambda\\eta\\left|\\nabla u^{k}-u^{k}\\right|+\\left(\\frac{\\lambda(u^{k})}{n}(u,\\hat{S})-\\alpha^{k}u^{k}\\right)(u^{k},\\hat{S})}\\\\ &{\\qquad-\\eta\\left(\\nabla L(u^{k}(u,\\hat{S}),\\cdot)-\\nabla L(u^{k})(u^{k},\\hat{S})+\\lambda(u^{k}(u,\\hat{S})-u^{k})(u^{k},\\hat{S})\\right)\\Big|\\left|\\right)^{1/1}}\\\\ &{\\leq\\lambda\\eta\\left|\\nabla u^{k}-u^{k}\\right|+\\left(1-\\lambda\\eta\\right)\\left|\\left|u^{k}(u,\\hat{S},\\cdot)u^{k}(u^{k},\\hat{S})\\right|^{2}}\\\\ &{\\qquad+\\left(\\rho^{-2}-\\frac{2(1-u^{k})}{n}\\right)\\left|\\nabla u^{k}(u^{k},\\hat{S})-\\alpha^{k}u^{k}(u^{k},\\hat{S})\\right|^{2}\\right)}\\\\ &{\\qquad+\\left(\\rho^{-2}-\\frac{2(1-u^{k})}{n}\\right)\\left|\\left|\\nabla u^{k}(u^{k},\\hat{S})-\\beta\\right| \n$$sive) ", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\leq(1-\\lambda\\eta)\\left\\|\\mathbf{u}^{(k)}(\\mathbf{w},S)-\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},S)\\right\\|+\\lambda\\eta\\left\\|\\mathbf{w}-\\mathbf{w}^{\\prime}\\right\\|\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combine the above two cases gives us that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{m}\\sum_{j=1}^{m}\\left\\|\\mathbf{u}^{(k+1)}(\\mathbf{w},S_{j})-\\mathbf{u}^{(k+1)}(\\mathbf{w}^{\\prime},S_{j}^{\\prime})\\right\\|}\\\\ {\\displaystyle\\leq\\frac{1}{m}\\sum_{j\\neq i}^{m}\\left\\|\\mathbf{u}^{(k)}(\\mathbf{w},S_{j})-\\mathbf{u}^{(k)}(\\mathbf{w},S_{j}^{\\prime})\\right\\|+\\displaystyle\\frac{1}{m}\\left\\|\\mathbf{u}^{(k)}(\\mathbf{w},S_{i})-\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},S_{i}^{\\prime})\\right\\|}\\\\ {\\displaystyle\\leq(1-\\lambda\\eta)\\,\\frac{1}{m}\\sum_{j=1}^{m}\\left\\|\\mathbf{u}^{(k)}(\\mathbf{w},S_{j})-\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},S_{j}^{\\prime})\\right\\|+\\lambda\\eta\\left\\|\\mathbf{w}-\\mathbf{w}^{\\prime}\\right\\|+\\frac{2\\eta G}{m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Given $\\mathbf{w}_{t},\\mathbf{w}_{t}^{\\prime}$ , when $k=1$ \uff0c $\\mathbf{u}^{(1)}(\\mathbf{w}_{t},S_{j}^{\\prime})=\\mathbf{w}_{t}$ \uff0c $\\mathbf{u}^{(1)}(\\mathbf{w}_{t}^{\\prime},S_{j})=\\mathbf{u}^{(1)}(\\mathbf{w}_{t}^{\\prime},S_{j}^{\\prime})=\\mathbf{w}_{t}^{\\prime}$ Telescoping gives us that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{1}{m}\\sum_{j=1}^{m}\\Big\\|\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S_{j})-\\mathbf{u}^{(k)}(\\mathbf{w}_{t}^{\\prime},S_{j}^{\\prime})\\Big\\|\\le\\big(1+(1-\\lambda\\eta)^{k-1}\\big)\\,\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\|+\\frac{2G}{\\lambda m}}}\\\\ &{}&{\\qquad\\le2\\,\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\|+\\frac{2G}{\\lambda m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Finally, at iteration $t$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left||\\mathbf{w}_{t+1}-\\mathbf{w}_{t+1}^{\\prime}|\\right|}\\\\ &{\\leq\\left\\|\\mathbf{w}_{t}-\\gamma\\lambda\\left(\\mathbf{w}_{t}-\\displaystyle\\frac{1}{m}\\sum_{j=1}^{m}\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S_{j})\\right)-\\mathbf{w}_{t}^{\\prime}+\\gamma\\lambda\\left(\\mathbf{w}_{t}^{\\prime}-\\displaystyle\\frac{1}{m}\\sum_{j=1}^{m}\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{t}^{\\prime},S_{j}^{\\prime})\\right)\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left(\\mathrm{Projection~is~non-expansive}\\right)}\\\\ &{=\\left\\|(1-\\gamma\\lambda)(\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime})+\\gamma\\lambda\\displaystyle\\frac{1}{m}\\sum_{j=1}^{m}\\frac{1}{K}\\sum_{k=1}^{K}\\left(\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S_{j})-\\mathbf{u}^{(k)}(\\mathbf{w}_{t}^{\\prime},S_{j}^{\\prime})\\right)\\right\\|}\\\\ &{\\leq(1-\\gamma\\lambda)\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+\\gamma\\lambda\\left(2\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+\\displaystyle\\frac{2G}{\\lambda m}\\right)}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left(\\mathrm{Equaion~(11)}\\right)}\\\\ &{=(1+\\gamma\\lambda)\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+\\frac{2\\gamma G}{m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that $\\mathbf{w}_{1}=\\mathbf{w}_{1}^{\\prime}=0$ . Choosing $\\begin{array}{r}{\\gamma\\leq\\frac{1}{\\lambda T}}\\end{array}$ and telescoping gives us that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{w}_{T+1}-\\mathbf{w}_{T+1}^{\\prime}\\right\\|\\leq\\left(1+{\\frac{1}{T}}\\right)\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+{\\frac{2G}{m\\lambda T}}\\leq{\\frac{2e G}{\\lambda m}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the inequality holds because $(1+\\frac{1}{T})^{T}\\leq e$ . Similarly, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{u}^{(k+1)}(\\mathbf{w},\\mathcal{S})-\\mathbf{u}^{(k+1)}(\\mathbf{w}^{\\prime},\\mathcal{S}^{(i)})\\right\\|}\\\\ &{\\leq\\bigg\\|\\mathbf{u}^{(k)}(\\mathbf{w},\\mathcal{S})-\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},\\mathcal{S}^{(i)})-\\eta\\Big(\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w},\\mathcal{S}),\\mathcal{S})+\\lambda\\left(\\mathbf{u}^{(k)}(\\mathbf{w},\\mathcal{S})-\\mathbf{w}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "(Projection is non-expansive) ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\left.\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},S^{(i)}),S^{(i)})-\\lambda\\left(\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},S^{(i)})-\\mathbf{w}^{\\prime}\\right)\\right)\\right|}\\\\ &{\\quad\\quad+\\left.\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},S^{(i)})-\\lambda)\\left(\\mathbf{u}^{(k)}(\\mathbf{w},S)-\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},S^{(i)})\\right)\\right.}\\\\ &{\\quad\\quad\\left.+\\eta\\left(\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w},S),S)-\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},S^{(i)}),S)\\right)\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\eta\\left(\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},S^{(i)}),S)-\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},S^{(i)}),S^{(i)})\\right)\\bigg\\|}\\\\ &{\\leq\\eta\\lambda\\left\\|\\mathbf{w}-\\mathbf{w}^{\\prime}\\right\\|+\\frac{2G\\eta}{n}+\\left(\\left\\|(1-\\eta\\lambda)\\left(\\mathbf{u}^{(k)}(\\mathbf{w},S)-\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},S^{(i)})\\right)\\right.}\\\\ &{\\qquad+\\left.\\eta\\left(\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w},S),S)-\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},S^{(i)}),S)\\right)\\right\\|^{2}\\right)^{1/2}}\\\\ &{\\leq\\eta\\lambda\\left\\|\\mathbf{w}-\\mathbf{w}^{\\prime}\\right\\|+\\frac{2G\\eta}{n}+\\left((1-\\lambda\\eta)^{2}\\left\\|\\mathbf{u}^{(k)}(\\mathbf{w},S)-\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},S^{(i)})\\right\\|^{2}\\right.}\\\\ &{\\qquad+\\left.\\left(\\eta^{2}-\\frac{2\\eta(1-\\eta\\lambda)}{H}\\right)\\right\\|\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w},S),S)-\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},S),S^{(i)})\\right\\|^{2}\\right)^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\leq(1-\\lambda\\eta)\\left\\|\\mathbf{u}^{(k)}(\\mathbf{w},S)-\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},S^{(i)})\\right\\|+\\eta\\lambda\\left\\|\\mathbf{w}-\\mathbf{w}^{\\prime}\\right\\|+{\\frac{2G\\eta}{n}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore we have $\\forall k\\in[K-1]$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{u}^{(k+1)}(\\mathbf{w}_{T+1},S)-\\mathbf{u}^{(k+1)}(\\mathbf{w}_{T+1}^{\\prime},S^{(i)})\\right\\|\\leq\\left(1+(1-\\lambda\\eta)^{k-1}\\right)\\left\\|\\mathbf{w}_{T+1}-\\mathbf{w}_{T+1}^{\\prime}\\right\\|+\\frac{2G}{\\lambda n}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "As $\\mathbf{u}^{(1)}(\\mathbf{w}_{T+1},S)=\\mathbf{w}_{T+1},\\mathbf{u}^{(1)}(\\mathbf{w}_{T+1}^{\\prime},S)=\\mathbf{w}_{T+1}^{\\prime}$ , plug in Equation (12), we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle\\left|\\frac{1}{K}\\sum_{k=1}^{K}\\mathsf{u}^{(k)}(\\mathbf{w}_{T+1},S)-\\frac{1}{K}\\sum_{k=1}^{K}\\mathsf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S^{(i)})\\right|\\right|}\\\\ &{\\le\\operatorname*{min}\\left(2,1+\\displaystyle\\frac{1}{\\lambda\\eta K}\\right)\\left\\|\\mathbf{w}_{T+1}-\\mathbf{w}_{T+1}^{\\prime}\\right\\|+\\frac{2G}{\\lambda n}}\\\\ &{\\le\\operatorname*{min}\\left(2,1+\\displaystyle\\frac{1}{\\lambda\\eta K}\\right)\\frac{2e G}{\\lambda m}+\\frac{2G}{\\lambda n}}\\\\ &{\\le\\frac{4e G}{\\lambda m}+\\frac{2G}{\\lambda n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The following theorem can be derived via Theorem 2.2 and Lemma 4.3. ", "page_idx": 26}, {"type": "text", "text": "Theorem C.3. Consider a meta-learning problem with convex, $M$ -bounded, $G$ -Lipschitz and $H$ smooth lossfunetion Then, after $T$ iterations of Algorithm 1 with $\\begin{array}{r}{\\gamma\\leq\\frac{1}{\\lambda T}}\\end{array}$ on a given meta-sample S, and GD for task-specifi learning (ie., Option 2 for Algorithm 2) with $\\begin{array}{r}{\\bar{\\eta}\\le\\frac{2}{H+2\\lambda}}\\end{array}$ , we have with probability at least $1-\\delta$ ", "page_idx": 26}, {"type": "equation", "text": "$$\nL(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mu)\\lesssim L(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mathbf{S})+\\frac{G^{2}}{\\lambda m}\\log\\left(m\\right)\\log\\left(1/\\delta\\right)+\\frac{M}{\\sqrt{m}}\\sqrt{\\log\\left(1/\\delta\\right)}+\\frac{G^{2}}{\\lambda n}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof of Theorem C.3. We slightly abuse the notation, at iteration $t$ define $\\mathbf{w}_{t}\\,=\\,A(\\mathbf{S})$ $\\mathbf{w}_{t}^{\\prime}\\,=\\,\\,$ $\\begin{array}{r}{\\boldsymbol{A}(\\mathbf{S}^{(j)}),\\mathbf{u}^{(K)}(\\mathbf{w}_{T+1},\\mathcal{S})=\\boldsymbol{A}(\\mathbf{S})(\\mathcal{S}),\\mathbf{u}^{(K)}(\\mathbf{w}_{T+1}^{\\prime},\\mathcal{S}^{(i)})=\\boldsymbol{A}(\\mathbf{S}^{(j)})(\\mathcal{S}^{(i)}).}\\end{array}$ Ifthelos is $M$ bounded, ", "page_idx": 26}, {"type": "text", "text": "convex and $G$ -Lipschitz, apply Equation (10) gives us ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\left|L(A(\\mathbf{S})(S),S)-L(A(\\mathbf{S}^{(j)})(S),S)\\right|}\\\\ &{=\\left|L(\\mathbf{u}^{(K)}(\\mathbf{w}_{T+1},S),S)-L(\\mathbf{u}^{(K)}(\\mathbf{w}_{T+1}^{\\prime},S),S)\\right|}\\\\ &{\\leq G\\left\\|\\mathbf{u}^{(K)}(\\mathbf{w}_{T+1},S)-\\mathbf{u}^{(K)}(\\mathbf{w}_{T+1}^{\\prime},S)\\right\\|}&{(G\\mathrm{-Lipschitz})}\\\\ &{\\leq G\\left((1-\\lambda\\eta)\\left\\|\\mathbf{u}^{(K)}(\\mathbf{w}_{T+1},S)-\\mathbf{u}^{(K)}(\\mathbf{w}_{T+1}^{\\prime},S)\\right\\|+\\lambda\\eta\\left\\|\\mathbf{w}_{T+1}-\\mathbf{w}_{T+1}^{\\prime}\\right\\|\\right)}&{(\\mathrm{Equation~}(10))}\\\\ &{\\leq G\\left\\|\\mathbf{w}_{T+1}-\\mathbf{w}_{T+1}^{\\prime}\\right\\|}\\\\ &{\\leq\\frac{2e G^{2}}{\\lambda m}}&{(\\mathrm{Equation~}(12))}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Given $\\mathcal{S},\\mathcal{S}^{\\left(i\\right)}$ . For any w, by Equation (13), for all $k\\in[K-1]$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n{\\left\\|{\\mathbf{u}}^{(k+1)}(\\mathbf{w},{\\mathcal{S}})-{\\mathbf{u}}^{(k+1)}(\\mathbf{w},{\\mathcal{S}}^{(i)})\\right\\|}\\leq\\frac{2G\\eta}{n}+(1-\\lambda\\eta)\\left\\|{\\mathbf{u}}^{(k)}(\\mathbf{w},{\\mathcal{S}})-{\\mathbf{u}}^{(k)}(\\mathbf{w},{\\mathcal{S}}^{(i)})\\right\\|\\leq\\frac{2G\\eta}{\\lambda n}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell(\\boldsymbol{\\mathcal{A}}(\\mathbf{S})(\\mathcal{S}),{\\mathbf z})-\\ell(\\boldsymbol{\\mathcal{A}}(\\mathbf{S})(\\mathcal{S}^{(i)}),{\\mathbf z})\\Big|}\\\\ &{=\\Bigg|\\ell\\left(\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1},\\mathcal{S}),{\\mathbf z}\\right)-\\ell\\left(\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1},\\mathcal{S}^{(i)}),{\\mathbf z}\\right)\\Bigg|}\\\\ &{\\le G\\left\\|\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1},\\mathcal{S})-\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1},\\mathcal{S}^{(i)})\\right\\|}\\\\ &{\\le\\displaystyle\\frac{2G^{2}}{\\lambda n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Apply Theorem 2.2 with \u03b23' = 2e\u21032, ,with \u03b2 = 2G\u00b2 2 gives us the result. ", "page_idx": 27}, {"type": "text", "text": "Theorem 4.4. Assume that the loss function is convex, $M$ -bounded, $G$ -Lipschitz and $H$ -smooth. Suppose we run Algorithm 1 for $T$ iterations with $\\begin{array}{r}{\\gamma\\leq\\frac{1}{\\lambda T}}\\end{array}$ on a given meta-sample S, and GD for task-specific eaning Option , Algorithm 2) with $\\begin{array}{r}{\\eta\\le\\frac{2}{H+2\\lambda}}\\end{array}$ Then, with probability a last $1-\\delta$ \uff0c ", "page_idx": 27}, {"type": "equation", "text": "$$\nL(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mu)\\lesssim L(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mathbf{S})+\\left(\\frac{G^{2}}{\\lambda m}+\\frac{G^{2}}{\\lambda n}\\right)\\log{(m n)}\\log{(1/\\delta)}+\\frac{M\\sqrt{\\log{(1/\\delta)}}}{\\sqrt{m n}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof of Theorem 4.4. We denote $\\begin{array}{r l r}{\\mathrm{~\\boldmath~\\Gamma~}=}&{{}\\mathcal{A}({\\bf S}),~{\\bf w}_{t}^{\\prime}\\;\\;=\\;\\;\\mathcal{A}({\\bf S}^{(j)}),~{\\bf u}({\\bf w}_{T+1},\\mathcal{S})}&{=\\;\\;\\mathcal{A}({\\bf S})(\\mathcal{S})}\\end{array}$ $\\mathbf{u}(\\mathbf{w}_{T+1},S^{(i)})\\,=\\,\\mathcal{A}(\\mathbf{S})(S^{(i)})$ .For $\\ell$ to be convex and $G$ -Lipschitz, applying Lemma 4.3 gives us that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\ell(\\mathcal{A}(\\mathbf{S})(\\mathcal{S}),{\\mathbf{z}})-\\ell(\\mathcal{A}(\\mathbf{S}^{(j)})(\\mathcal{S}^{(i)}),{\\mathbf{z}})\\Big|}\\\\ &{\\displaystyle=\\left|\\ell\\left(\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1},\\mathcal{S}),{\\mathbf{z}}\\right)-\\ell\\left(\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}(\\mathbf{w}_{T+1}^{\\prime},\\mathcal{S}^{(i)}),{\\mathbf{z}}\\right)\\right|}\\\\ &{\\displaystyle\\leq G\\left\\|\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1},\\mathcal{S}),{\\mathbf{z}})-\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},\\mathcal{S}^{(i)})\\right\\|}\\\\ &{\\displaystyle\\leq\\operatorname*{min}\\left(2,1+\\frac{1}{\\lambda\\eta K}\\right)\\frac{2e G^{2}}{\\lambda m}+\\frac{2G^{2}}{\\lambda m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Further apply Theorem 3.1 gives us the result ", "page_idx": 27}, {"type": "text", "text": "D Missing Proofs of Section 4.2 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We start with a proposition that provide some equivalent characterizations of weak convexity. ", "page_idx": 28}, {"type": "text", "text": "Proposition D.1 (Proposition 2.1 in Davis and Grimmer [2019]). Suppose $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}\\cup\\{\\infty\\}$ is a closed function and $\\rho>0$ , then the following are equivalent: ", "page_idx": 28}, {"type": "text", "text": "1. For any $\\begin{array}{r}{\\mathbf{w}_{1}\\in\\mathbb{R}^{d},\\,f(\\cdot)+\\frac{\\rho}{2}\\left\\|\\cdot-\\mathbf{w}_{1}\\right\\|}\\end{array}$ is convex. ", "page_idx": 28}, {"type": "text", "text": "2. For any $\\mathbf{w}_{1},\\mathbf{w}_{2}\\in\\mathbb{R}^{d}$ with $g(\\mathbf{w}_{1})\\in\\partial f(\\mathbf{w}_{1})$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\nf(\\mathbf{w}_{2})\\geq f(\\mathbf{w}_{1})+\\langle g(\\mathbf{w}_{1}),\\mathbf{w}_{2}-\\mathbf{w}_{1}\\rangle-\\frac{\\rho}{2}\\left\\|\\mathbf{w}_{2}-\\mathbf{w}_{1}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "3. For any $\\mathbf{w}_{1},\\mathbf{w}_{2}\\in\\mathbb{R}^{d}$ and $\\lambda>0$ ", "page_idx": 28}, {"type": "equation", "text": "$$\nf(\\lambda\\mathbf{w}_{1}+(1-\\lambda)\\mathbf{w}_{2})\\leq\\lambda f(\\mathbf{w}_{1})+(1-\\lambda)f(\\mathbf{w}_{2})+{\\frac{\\rho\\lambda(1-\\lambda)}{2}}\\left\\|\\mathbf{w}_{1}-\\mathbf{w}_{2}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Lemma D.2. [Bassily et al. [2020]] Given $\\boldsymbol{S}$ and $S^{(i)}$ , for a fixed w, consider $\\mathbf{u}(\\mathbf{w},S)$ and $\\mathbf{u}(\\mathbf{w},S^{(i)})$ are achieved via Algo. 1 with gradient descent for $K$ iterations. Then if $\\ell$ is convex and $G$ -Lipschitz, Iwehave $\\begin{array}{r}{\\operatorname*{sup}_{i\\in[n]}\\left\\|\\frac{-}{K}\\sum_{k=1}^{K}\\bar{\\mathbf{u^{(k)}}}(\\mathbf{w},S)-\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w},S^{(i)})\\right\\|\\leq\\frac{4G K\\eta}{n}+4G\\eta\\sqrt{K}.}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "Below we provide our key Lemma D.3 for the stability analysis. ", "page_idx": 28}, {"type": "text", "text": "Lemma D.3. Consider a meta-learning problem with $\\rho$ -weakly convex and $G$ -Lipschitz loss function. Let S, $\\mathbf{S}^{(j)}$ denote neighboring meta-samples and $\\boldsymbol{S}$ \uff0c $S^{(i)}$ the neighboring samples on a test task. Then, after $T$ iterations of Algorithm 1 with $\\begin{array}{r}{\\gamma\\le\\frac{1}{\\lambda T},\\lambda\\ge2\\rho}\\end{array}$ , and GD for task-specific learning (i., Option 2 for Algorithm 2) with $\\begin{array}{r}{\\eta\\le\\frac{1}{\\lambda}}\\end{array}$ \uff0c ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{S},j\\in[m]}{\\operatorname*{sup}}\\left\\|\\mathbf{w}_{T+1}-\\mathbf{w}_{T+1}^{\\prime}\\right\\|\\leq2e G\\sqrt{\\frac{\\eta}{\\lambda}}+\\frac{2e G}{\\lambda m}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proofof Lemma $D.3$ .If $\\ell$ is $\\rho$ -weakly convex, then from Proposition D.1 we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\langle\\nabla\\ell({\\mathbf u})-\\nabla\\ell({\\mathbf v}),{\\mathbf u}-{\\mathbf v}\\rangle\\geq-\\rho\\left\\|{\\mathbf u}-{\\mathbf v}\\right\\|^{2},\\forall{\\mathbf u},{\\mathbf v}\\in\\mathbb{R}^{d}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Given $\\boldsymbol{S}$ and $S^{\\prime}$ , for any w and $\\mathbf{w}^{\\prime}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\forall k\\in[K-1],\\quad\\Big\\|\\mathbf{u}^{(k+1)}(\\mathbf{w},S)-\\mathbf{u}^{(k+1)}(\\mathbf{w}^{\\prime},S^{\\prime})\\Big\\|}\\\\ &{\\leq\\bigg\\|\\mathbf{u}^{(k)}(\\mathbf{w},S)-\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},S^{\\prime})-\\eta\\bigg(\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w},S),S)+\\lambda\\left(\\mathbf{u}^{(k)}(\\mathbf{w},S)-\\mathbf{w}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "(Projection is non-expansive) ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad-\\,\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},{\\cal S}^{\\prime}),{\\cal S})-\\lambda\\left(\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},{\\cal S}^{\\prime})-\\mathbf{w}^{\\prime}\\right)\\Bigg)\\Bigg\\|}\\\\ &{=(1-\\eta\\lambda)\\left\\|\\mathbf{u}^{(k)}(\\mathbf{w},{\\cal S})-\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},{\\cal S}^{\\prime})\\right\\|+\\eta\\lambda\\left\\|\\mathbf{w}-\\mathbf{w}^{\\prime}\\right\\|+2\\eta{\\cal G}}\\\\ &{\\le\\left(1+(1-\\eta\\lambda)^{k}\\right)\\left\\|\\mathbf{w}-\\mathbf{w}^{\\prime}\\right\\|+\\displaystyle\\frac{2{\\cal G}}{\\lambda}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "And therefore ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w},\\mathcal{S})-\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}^{\\prime},\\mathcal{S}^{\\prime})\\right\\|\\leq\\operatorname*{min}\\left(2,1+\\frac{1}{\\lambda\\eta K}\\right)\\|\\mathbf{w}-\\mathbf{w}^{\\prime}\\|+\\frac{2G}{\\lambda}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We now focus on the situation where we give $\\boldsymbol{S}$ and $S^{\\prime}$ with a fix w. For simplicity, we define $\\delta_{k}=\\left\\|\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S)-\\mathbf{u}^{(k)}(\\mathbf{w}_{t}^{\\prime},S)\\right\\|$ .Note that $\\delta_{1}=\\lVert\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\rVert$ Wehave ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\delta_{k+1}=\\left\\lVert\\mathbf{u}^{(k+1)}(\\mathbf{w}_{t},S)-\\mathbf{u}^{(k+1)}(\\mathbf{w}_{t}^{\\prime},S)\\right\\rVert\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\leq\\left\\|\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S)\\!-\\!\\mathbf{u}^{(k)}(\\mathbf{w}_{t}^{\\prime},S)\\!-\\!\\eta\\!\\left(\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S),S)\\!+\\!\\lambda\\left(\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S)\\!-\\!\\mathbf{w}_{t}\\right)\\right)\\right\\|_{2},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "(Projection is non-expansive) ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\left.\\nabla L(\\mathbf u^{(k)}(\\mathbf w_{t}^{\\prime},\\mathcal S),\\mathcal S)-\\lambda\\left(\\mathbf u^{(k)}(\\mathbf w_{t}^{\\prime},\\mathcal S)-\\mathbf w_{t}^{\\prime}\\right)\\right)\\right\\|}\\\\ &{\\leq\\lambda\\eta\\left\\|\\mathbf w_{t}-\\mathbf w_{t}^{\\prime}\\right\\|+\\left\\|\\left(1-\\eta\\lambda\\right)\\left(\\mathbf u^{(k)}(\\mathbf w_{t},\\mathcal S)\\!-\\!\\mathbf u^{(k)}(\\mathbf w_{t}^{\\prime},\\mathcal S)\\right)}\\\\ &{\\qquad-\\eta\\left(\\nabla L(\\mathbf u^{(k)}(\\mathbf w_{t},\\mathcal S),\\mathcal S)\\!-\\!\\nabla L(\\mathbf u^{(k)}(\\mathbf w_{t}^{\\prime},\\mathcal S),\\mathcal S)\\right)\\right\\|}\\\\ &{=\\lambda\\eta\\left\\|\\mathbf w_{t}-\\mathbf w_{t}^{\\prime}\\right\\|+\\Delta_{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we define ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Delta_{k}\\!=\\!\\Bigg\\|\\big(1\\!-\\!\\eta\\lambda\\big)\\Big(\\mathbf{u}^{(k)}\\big(\\mathbf{w}_{t},\\mathcal{S}\\big)\\!-\\!\\mathbf{u}^{(k)}\\big(\\mathbf{w}_{t}^{\\prime},\\mathcal{S}\\big)\\Big)-\\eta\\Big(\\nabla L\\big(\\mathbf{u}^{(k)}\\big(\\mathbf{w}_{t},\\mathcal{S}\\big),\\mathcal{S}\\big)\\!-\\!\\nabla L\\big(\\mathbf{u}^{(k)}\\big(\\mathbf{w}_{t}^{\\prime},\\mathcal{S}\\big),\\mathcal{S}\\big)\\Big)\\Bigg\\|.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We have that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{k}^{2}=(1-\\eta\\lambda)^{2}\\delta_{k}^{2}+4\\eta^{2}G^{2}}\\\\ &{\\qquad-\\,2\\eta(1-\\eta\\lambda)\\Big\\langle\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S)-\\mathbf{u}^{(k)}(\\mathbf{w}_{t}^{\\prime},S),\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S),S)-\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{t}^{\\prime},S),S)\\Big\\rangle}\\\\ &{\\qquad\\leq(1-\\eta\\lambda)^{2}\\delta_{k}^{2}+4\\eta^{2}G^{2}+2\\eta(1-\\eta\\lambda)\\rho\\delta_{k}^{2}}\\\\ &{\\qquad\\leq(1-\\eta\\lambda)\\delta_{k}^{2}+4\\eta^{2}G^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Therefore, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\delta_{k+1}^{2}+\\lambda^{2}\\eta^{2}\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|^{2}-2\\lambda\\eta\\delta_{k+1}\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|\\leq\\Delta_{k}^{2}\\leq(1-\\eta\\lambda)\\delta_{k}^{2}+4\\eta^{2}G^{2}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Rearrange it gives us that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\delta_{k+1}^{2}}{\\left(1-\\eta\\lambda\\right)^{k+1}}+\\frac{\\lambda^{2}\\eta^{2}\\left\\Vert\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\Vert^{2}}{\\left(1-\\eta\\lambda\\right)^{k+1}}-\\frac{2\\lambda\\eta\\left\\Vert\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\Vert\\delta_{k+1}}{\\left(1-\\eta\\lambda\\right)^{k+1}}\\leq\\frac{\\delta_{k}^{2}}{\\left(1-\\eta\\lambda\\right)^{k}}+\\frac{4\\eta^{2}G^{2}}{\\left(1-\\eta\\lambda\\right)^{k+1}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Telescoping from $k=1$ to $K$ gives us that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\delta_{K+1}^{2}}{\\left(1-\\eta\\lambda\\right)^{K+1}}+\\sum_{k=1}^{K}\\frac{\\lambda^{2}\\eta^{2}\\left\\Vert\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\Vert^{2}}{\\left(1-\\eta\\lambda\\right)^{k+1}}\\leq\\sum_{k=1}^{K}\\frac{2\\lambda\\eta\\left\\Vert\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\Vert\\delta_{k+1}}{\\left(1-\\eta\\lambda\\right)^{k+1}}+\\sum_{k=1}^{K}\\frac{4\\eta^{2}G^{2}}{\\left(1-\\eta\\lambda\\right)^{k+1}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\delta_{K+1}^{2}+\\lambda^{2}\\eta^{2}\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|^{2}\\sum_{k=1}^{K}\\left(1-\\eta\\lambda\\right)^{K-k}-2\\lambda\\eta\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|\\delta_{K+1}}\\\\ {\\displaystyle}\\\\ {\\displaystyle\\leq\\frac{4\\eta G^{2}}{\\lambda}+2\\lambda\\eta\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|\\sum_{k=1}^{K-1}\\delta_{k+1}(1-\\eta\\lambda)^{K-k}}\\\\ {\\displaystyle}\\\\ {\\displaystyle\\leq\\frac{4\\eta G^{2}}{\\lambda}+2\\lambda\\eta(1-\\eta\\lambda)\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|\\sum_{k=1}^{K}\\delta_{k}(1-\\eta\\lambda)^{K-k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now we start proving the following bound by induction: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\delta_{K}\\leq2\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+2G\\sqrt{\\frac{\\eta}{\\lambda}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This claim holds when $k=1$ . For the inductive step, we assume it holds for some $k\\,\\in\\,[K]$ and prove the result for $k+1$ .We consider the following two cases. If $\\delta_{k+1}\\leq\\operatorname*{max}_{s\\in[k]}\\delta_{k}$ induction automatically holds. Otherwise, $\\delta_{k+1}>\\operatorname*{max}_{s\\in[k]}\\delta_{s}$ . Applying Equation (15) gives us that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\delta_{k+1}^{2}+\\lambda^{2}\\eta^{2}\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|^{2}\\sum_{j=1}^{k}\\left(1-\\eta\\lambda\\right)^{k-j}-2\\lambda\\eta\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|\\delta_{k+1}}\\\\ {\\displaystyle\\leq\\frac{4\\eta G^{2}}{\\lambda}+2\\lambda\\eta(1-\\eta\\lambda)\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|\\sum_{j=1}^{k}\\delta_{k}(1-\\eta\\lambda)^{k-j}}\\\\ {\\displaystyle\\leq\\frac{4\\eta G^{2}}{\\lambda}+2\\lambda\\eta(1-\\eta\\lambda)\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|\\delta_{k+1}\\sum_{j=1}^{k}(1-\\eta\\lambda)^{k-j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which is equivalent to ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\delta_{k+1}^{2}+\\lambda^{2}\\eta^{2}\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|^{2}\\sum_{j=1}^{k}\\left(1-\\eta\\lambda\\right)^{k-j}}\\\\ {\\displaystyle\\leq\\frac{4\\eta G^{2}}{\\lambda}+2\\lambda\\eta\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|\\delta_{k+1}\\left(1+(1-\\eta\\lambda)\\sum_{j=1}^{k}(1-\\eta\\lambda)^{k-j}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Rearrange gives us that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\delta_{k+1}-\\lambda\\eta\\left\\Vert\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\Vert\\left(1+(1-\\eta\\lambda)\\displaystyle\\sum_{j=1}^{k}(1-\\eta\\lambda)^{k-j}\\right)\\right)^{2}}\\\\ &{\\quad\\leq\\left(\\lambda\\eta\\left\\Vert\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\Vert\\left(1+(1-\\eta\\lambda)\\displaystyle\\sum_{j=1}^{k}(1-\\eta\\lambda)^{k-j}\\right)\\right)^{2}+\\frac{4\\eta G^{2}}{\\lambda}}\\\\ &{\\quad-\\,\\lambda\\eta\\left\\Vert\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\Vert^{2}\\left(1-(1-\\eta\\lambda)^{k+1}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\forall k\\in[K-1]}&{\\Big\\|\\mathbf{u}^{(k+1)}(\\mathbf{w}_{t},S)-\\mathbf{u}^{(k+1)}(\\mathbf{w}_{t}^{\\prime},S)\\Big\\|}\\\\ &{\\leq2G\\sqrt{\\frac{\\eta}{\\lambda}}+2\\left(\\lambda\\eta\\,\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\|\\left(1+(1-\\eta\\lambda)\\displaystyle\\sum_{j=1}^{k}(1-\\eta\\lambda)^{k-j}\\right)\\right)}\\\\ &{\\leq2\\,\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\|+2G\\sqrt{\\frac{\\eta}{\\lambda}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "And therefore ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left\\lVert\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{t},\\mathcal{S})-\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{t}^{\\prime},\\mathcal{S})\\right\\rVert\\leq2\\left\\lVert\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\rVert+2G\\sqrt{\\frac{\\eta}{\\lambda}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "As a result, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\lVert\\mathbf{w}_{t+1}-\\mathbf{w}_{t+1}^{\\prime}\\right\\rVert}\\\\ &{\\le\\left\\lVert\\mathbf{w}_{t}-\\gamma\\lambda\\left(\\mathbf{w}_{t}-\\cfrac{1}{m}\\displaystyle\\sum_{j=1}^{m}\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S_{j})\\right)-\\mathbf{w}_{t}^{\\prime}+\\gamma\\lambda\\left(\\mathbf{w}_{t}^{\\prime}-\\cfrac{1}{m}\\displaystyle\\sum_{j=1}^{m}\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{t}^{\\prime},S_{j}^{\\prime})\\right)\\right\\rVert}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "(Projection is non-expansive) ", "page_idx": 30}, {"type": "equation", "text": "$$\n=(1-\\gamma\\lambda)\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+\\gamma\\lambda\\left\\|\\frac{1}{m}\\sum_{j=1}^{m}\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S_{j})-\\frac{1}{m}\\sum_{j=1}^{m}\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{t}^{\\prime},S_{j}^{\\prime})\\right\\|\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\leq\\left(1-\\gamma\\lambda\\right)\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\|+\\displaystyle\\frac{m-1}{m}\\gamma\\lambda\\left(2\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+2G\\sqrt{\\frac{\\eta}{\\lambda}}\\right)+\\displaystyle\\frac{\\gamma\\lambda}{m}\\left(2\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+\\frac{2G}{\\lambda}\\right)}\\\\ {\\leq\\left(1+\\gamma\\lambda\\right)\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\|+2G\\gamma\\sqrt{\\eta\\lambda}+\\displaystyle\\frac{2G\\gamma}{m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Telescoping gives us that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{w}_{T+1}-\\mathbf{w}_{T+1}^{\\prime}\\right\\|\\leq(1+\\gamma\\lambda)^{T}\\left(2G\\sqrt{\\frac{\\eta}{\\lambda}}+\\frac{2G}{\\lambda m}\\right)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Choosing $\\begin{array}{r}{\\gamma\\leq\\frac{1}{\\lambda T}}\\end{array}$ gives us that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{w}_{T+1}-\\mathbf{w}_{T+1}^{\\prime}\\right\\|\\leq(1+\\frac{1}{T})^{T}\\left(2G\\sqrt{\\frac{\\eta}{\\lambda}}+\\frac{2G}{\\lambda m}\\right)\\leq2e G\\sqrt{\\frac{\\eta}{\\lambda}}+\\frac{2e G}{\\lambda m}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We remark that if we consider convex and non-smooth loss function by setting $\\rho=0$ , then follow a similar argument, Equation (14) can be replaced by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\delta_{k+1}^{2}+\\lambda^{2}\\eta^{2}\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|^{2}-2\\lambda\\eta\\delta_{k+1}\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|\\leq\\Delta_{k}^{2}\\leq(1-\\eta\\lambda)^{2}\\delta_{k}^{2}+4\\eta^{2}G^{2}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "And therefore Equation (16) can be replaced by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{\\mathbf u}^{(k+1)}({\\mathbf w}_{t},S)-{\\mathbf u}^{(k+1)}({\\mathbf w}_{t}^{\\prime},S)\\right\\|}\\\\ &{\\le2G\\sqrt{\\frac{\\eta}{\\lambda}}+2\\left(\\lambda\\eta\\left\\|{\\mathbf w}_{t}-{\\mathbf w}_{t}^{\\prime}\\right\\|\\left(1+(1-\\eta\\lambda)^{2}\\displaystyle\\sum_{j=1}^{k}(1-\\eta\\lambda)^{2k-2j}\\right)\\right)}\\\\ &{\\le\\displaystyle\\frac{2}{2-\\eta\\lambda}\\left\\|{\\mathbf w}_{t}-{\\mathbf w}_{t}^{\\prime}\\right\\|+2G\\sqrt{\\frac{\\eta}{\\lambda}}}\\\\ &{\\le2\\left\\|{\\mathbf w}_{t}-{\\mathbf w}_{t}^{\\prime}\\right\\|+2G\\sqrt{\\frac{\\eta}{\\lambda}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n(\\eta\\lambda\\leq1)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and the rest follows. ", "page_idx": 31}, {"type": "text", "text": "Theorem D.4. Consider a meta-learning problem with $\\rho$ -weakly convex, $M$ -bounded, $G$ -Lipschitz loss function. Then, after $T$ iterations of Algorithm 1 with $\\gamma\\leq{\\frac{\\mathfrak{z}}{\\lambda T}},\\lambda\\geq2\\rho$ , and GD for task-specific learning (i.e., Option 2 for Algorithm 2) with $\\begin{array}{r}{\\eta\\le\\frac{1}{\\lambda}}\\end{array}$ , we have with probability at least $1-\\delta$ \uff0c ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By setting $\\begin{array}{r}{\\eta=\\frac{1}{\\lambda K}}\\end{array}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\nL_{(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\,\\mu)}\\!\\lesssim\\!L(\\mathcal{A}(\\mathbf{S}),\\mathbf{S})\\!+\\!\\left(\\!\\frac{G^{2}}{\\lambda\\sqrt{K}}\\!+\\!\\frac{G^{2}}{\\lambda m}\\right)\\log{(m)}\\log{(1/\\delta)}\\!+\\!\\frac{M}{\\sqrt{m}}\\sqrt{\\log{(1/\\delta)}}\\!+\\!\\frac{G^{2}}{\\lambda n}\\!+\\!\\frac{G^{2}}{\\lambda\\sqrt{K}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof of Theorem D.4. If the loss is $M$ -bounded and $G$ -Lipschitz, apply Lemma D.3 gives us ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left\\lvert L(A(\\mathbf{S})(\\mathcal{S}),S)-L(A(\\mathbf{S}^{(j)})(\\mathcal{S}),S)\\right\\rvert}\\\\ &{=\\displaystyle\\left\\lvert L\\left(\\frac{1}{K}\\sum_{k=1}^{K}\\mathfrak{u}^{(k)}(\\mathbf{w}_{T+1},S),S\\right)-L\\left(\\frac{1}{K}\\sum_{k=1}^{K}\\mathfrak{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S),S\\right)\\right\\rvert}\\\\ &{\\le\\displaystyle\\alpha\\left\\lVert\\frac{1}{K}\\sum_{k=1}^{K}\\mathfrak{u}^{(k)}(\\mathbf{w}_{T+1},S)-\\frac{1}{K}\\sum_{k=1}^{K}\\mathfrak{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S)\\right\\rVert}\\\\ &{\\leq2G\\left\\lVert\\mathbf{w}_{T+1}-\\mathbf{w}_{T+1}^{\\prime}\\right\\rVert+2G^{2}\\sqrt{\\frac{\\eta}{\\lambda}}}\\\\ &{\\le\\left(4\\epsilon G^{2}+2G^{2}\\right)\\sqrt{\\frac{\\eta}{\\lambda}}+\\frac{4\\epsilon G^{2}}{\\lambda m}}\\\\ &{\\le\\left(4\\epsilon G^{2}+2G^{2}\\right)\\frac{1}{\\lambda\\sqrt{K}}+\\frac{4\\epsilon G^{2}}{\\lambda m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "On the other hand, applying Lemma D.2 gives us that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\Big|\\ell(\\boldsymbol{\\mathcal{A}}(\\mathbf{S})(S),\\mathbf{z})-\\ell(\\boldsymbol{\\mathcal{A}}(\\mathbf{S})(S^{(i)}),\\mathbf{z})\\Big|=\\Big|\\ell(\\mathbf{u}^{(K+1)}(\\mathbf{w}_{T+1},S),\\mathbf{z})-\\ell(\\mathbf{u}^{(K+1)}(\\mathbf{w}_{T+1},S^{(i)}),\\mathbf{z})\\Big|}}\\\\ &{}&{\\leq G\\left\\|\\mathbf{u}^{(K+1)}(\\mathbf{w}_{T},S)-\\mathbf{u}^{(K+1)}(\\mathbf{w}_{T},S^{(i)})\\right\\|}\\\\ &{}&{\\leq\\frac{4G^{2}K\\eta}{n}+4G^{2}\\eta\\sqrt{K}}\\\\ &{}&{\\leq\\frac{4G^{2}}{\\lambda n}+\\frac{4G^{2}}{\\lambda\\sqrt{K}}\\qquad\\qquad\\qquad\\qquad\\quad(\\mathrm{Set}~\\eta\\leq\\frac{1}{\\lambda K})}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Plug back into Theorem 2.2 gives the result ", "page_idx": 32}, {"type": "text", "text": "Lemma 4.5. Assume that the loss function is $\\rho$ -weakly convex and $G$ -Lipschitz. Let S, $\\mathbf{S}^{(j)}$ denote neighboring meta-samples and $\\boldsymbol{S}$ $S^{(i)}$ the neighboring samples on a test task. Then the following holds for Algorithm 1 with $\\lambda\\geq2\\rho$ , and GD for task-specific learning (i.e., Option 2 for Algorithm 2) with $\\begin{array}{r}{\\eta\\le\\frac{1}{\\lambda}}\\end{array}$ , for all $T\\geq1$ as long as we set $\\gamma\\leq{\\frac{1}{\\lambda T}}$ \uff0c ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda^{,~\\star\\bullet\\star\\star\\star\\star}\\cdots\\cdots\\cdots\\xrightarrow{\\star\\warrow\\cdots\\nrightarrow\\cdots\\cdots\\ w\\cdots\\ \\ i\\ -\\ \\lambda T^{\\star}}}\\\\ &{\\quad\\quad\\underset{{\\bf S},{\\cal S},{\\cal j}\\in[m],i\\in[n]}{\\operatorname*{sup}}\\left\\|\\mathcal A({\\bf S})({\\cal S})-\\mathcal A({\\bf S}^{(j)})({\\cal S}^{(i)})\\right\\|\\le(8e G+2G)\\sqrt{\\frac{\\eta}{\\lambda}}+\\frac{8e G}{\\lambda m}+\\frac{8G}{\\lambda n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof of Lemma 4.5. We slightly_ abuse the  notation,  at  outer  iteration $t$ define $\\mathbf{W}_{t}\\quad=$ $A(\\mathbf{S})$ \uff0c $\\begin{array}{r l r}{\\mathrm{w}_{t}^{\\prime}}&{{}=}&{\\mathcal{A}(\\mathbf{S}^{(j)})}\\end{array}$ Given $\\mathbf{W}_{t}$ , at  inner  iteration $k$ define $\\begin{array}{r l}{\\mathbf{u}^{(k)}(\\mathbf{w}_{t},\\boldsymbol{S})}&{{}=}\\end{array}$ $A(\\mathbf{S})(S)$ \uff0c $\\begin{array}{r l r}{\\mathbf{u}^{(k)}(\\mathbf{w}_{t}^{\\prime},\\mathcal{S}^{(i)})}&{{}=}&{\\mathcal{A}(\\mathbf{S}^{(j)})(\\mathcal{S}^{(i)})}\\end{array}$ We now provide the upper bound on $\\begin{array}{r}{\\left|\\left|\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1},S)-\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S^{(i)})\\right|\\right|}\\end{array}$ Recall that if $\\ell$ .s $\\rho$ -weakly convex, then we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\langle\\nabla\\ell({\\mathbf{u}})-\\nabla\\ell({\\mathbf{v}}),{\\mathbf{u}}-{\\mathbf{v}}\\rangle\\geq-\\rho\\left\\|{\\mathbf{u}}-{\\mathbf{v}}\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We   apply _ a  similar  procedure  as  Lemma  D.3.   For  simplicity,  we  define $\\delta_{k}\\quad=$ $\\left\\|\\mathbf{u}^{(k)}\\(\\mathbf{\\bar{w}}_{t},\\bar{S})-\\mathbf{u}^{(k)}\\big(\\mathbf{w}_{t}^{\\prime},\\bar{S}^{(i)}\\big)\\right\\|$ .Notethat $\\delta_{1}=\\lVert\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\rVert$ We have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{k+1}=\\bigg\\|\\mathbf{u}^{(k+1)}(\\mathbf{w}_{t},S)-\\mathbf{u}^{(k+1)}(\\mathbf{w}_{t}^{\\prime},S^{(i)})\\bigg\\|}\\\\ &{\\qquad=\\bigg\\|\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S)-\\mathbf{u}^{(k)}(\\mathbf{w}_{t}^{\\prime},S^{(i)})-\\eta\\bigg(\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S),S)+\\lambda\\left(\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S)-\\mathbf{w}_{t}\\right)}\\\\ &{\\qquad\\qquad-\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{t}^{\\prime},S^{(i)}),S^{(i)})-\\lambda\\left(\\mathbf{u}^{(k)}(\\mathbf{w}_{t}^{\\prime},S^{(i)})-\\mathbf{w}_{t}^{\\prime}\\right)\\bigg)\\bigg\\|}\\\\ &{\\qquad\\leq\\lambda\\eta\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+\\bigg\\|(1-\\eta\\lambda)\\left(\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S)-\\mathbf{u}^{(k)}(\\mathbf{w}_{t}^{\\prime},S^{(i)})\\right)}\\\\ &{\\qquad\\qquad-\\eta\\left(\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S),S)-\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{t}^{\\prime},S^{(i)}),S^{(i)})\\right)\\bigg\\|}\\\\ &{\\qquad=\\lambda\\eta\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+\\Delta_{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where we define ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{k}\\!=\\!\\Bigg\\|\\!\\left(1\\!-\\!\\eta\\lambda\\right)\\!\\left(\\!\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S)\\!-\\!\\mathbf{u}^{(k)}(\\mathbf{w}_{t}^{\\prime},S^{(i)})\\!\\right)\\!}\\\\ &{\\qquad\\!-\\!\\eta\\Big(\\!\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S),S)\\!-\\!\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{t}^{\\prime},S^{(i)}),S^{(i)})\\!\\Big)\\!\\Bigg\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We have that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\Lambda}_{k}^{2}=(1-\\eta\\lambda)^{2}\\delta_{k}^{2}+4\\eta^{2}G^{2}}\\\\ &{\\qquad-\\,2\\eta(1-\\eta\\lambda)\\Big\\langle\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1},S)-\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S^{(i)}),\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1},S),S)-\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S^{(i)}),\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1},S),S))\\Big\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=(1-\\eta\\lambda)^{2}\\delta_{k}^{2}+4\\eta^{2}G^{2}}\\\\ &{\\phantom{2p}-2\\eta(1-\\eta\\lambda)\\Big\\langle\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1},S)-\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S^{(i)}),\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1},S),S)-\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S^{(i)}),\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S),\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S),\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S),\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S),\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S),\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S),\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S),\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S),\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S),\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S),\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S),\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S),\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S),\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S),\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S),\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S),\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S),\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S),\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1} \n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Therefore, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\delta_{k+1}^{2}+\\lambda^{2}\\eta^{2}\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|^{2}-2\\lambda\\eta\\delta_{k+1}\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|\\leq\\Delta_{k}^{2}\\leq(1-\\eta\\lambda)\\delta_{k}^{2}+4\\eta^{2}G^{2}+\\frac{4G\\eta(1-\\eta\\lambda)}{n}\\delta_{k}\\left(\\frac{4\\eta}{\\lambda}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Rearrange it gives us that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\frac{\\delta_{k+1}^{2}}{\\left(1-\\eta\\lambda\\right)^{k+1}}+\\frac{\\lambda^{2}\\eta^{2}\\left\\Vert\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\Vert^{2}}{\\left(1-\\eta\\lambda\\right)^{k+1}}-\\frac{2\\lambda\\eta\\left\\Vert\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\Vert\\delta_{k+1}}{\\left(1-\\eta\\lambda\\right)^{k+1}}}\\\\ {\\displaystyle\\leq\\frac{\\delta_{k}^{2}}{\\left(1-\\eta\\lambda\\right)^{k}}+\\frac{4\\eta^{2}G^{2}}{\\left(1-\\eta\\lambda\\right)^{k+1}}+\\frac{4G\\eta(1-\\eta\\lambda)\\delta_{k}}{n\\left(1-\\eta\\lambda\\right)^{k+1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Telescoping from $k=1$ to $K$ gives us that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\delta_{K+1}^{2}}{\\left(1-\\eta\\lambda\\right)^{K+1}}+\\sum_{k=1}^{K}\\frac{\\lambda^{2}\\eta^{2}\\left\\Vert\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\Vert^{2}}{\\left(1-\\eta\\lambda\\right)^{k+1}}}}\\\\ &{}&{\\leq\\displaystyle\\sum_{k=1}^{K}\\frac{2\\lambda\\eta\\left\\Vert\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\Vert\\delta_{k+1}}{\\left(1-\\eta\\lambda\\right)^{k+1}}+\\sum_{k=1}^{K}\\frac{4\\eta^{2}G^{2}}{\\left(1-\\eta\\lambda\\right)^{k+1}}+\\sum_{k=1}^{K}\\frac{4G\\eta(1-\\eta\\lambda)\\delta_{k}}{n\\left(1-\\eta\\lambda\\right)^{k+1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\delta_{K+1}^{2}+\\lambda^{2}\\eta^{2}\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|^{2}\\sum_{k=1}^{K}\\left(1-\\eta\\lambda\\right)^{K-k}-2\\lambda\\eta\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|\\delta_{K+1}}\\\\ {\\displaystyle\\leq\\frac{4\\eta G^{2}}{\\lambda}+2\\lambda\\eta\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|\\sum_{k=1}^{K-1}\\delta_{k+1}(1-\\eta\\lambda)^{K-k}+\\frac{4G\\eta(1-\\eta\\lambda)}{n}\\sum_{k=1}^{K}\\delta_{k}(1-\\eta\\lambda)^{K-k}}\\\\ {\\displaystyle\\leq\\frac{4\\eta G^{2}}{\\lambda}+\\left(2\\lambda\\eta(1-\\eta\\lambda)\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+\\frac{4G\\eta(1-\\eta\\lambda)}{n}\\right)\\sum_{k=1}^{K}\\delta_{k}(1-\\eta\\lambda)^{K-k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Now we start proving the following bound by induction: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\delta_{K}\\leq2G\\sqrt{\\frac{\\eta}{\\lambda}}+4\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+\\frac{8G(1-\\eta\\lambda)}{\\lambda n}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This claim holds when $k=1$ . For the inductive step, we assume it holds for some $k\\,\\in\\,[K]$ and prove the result for $k+1$ . We consider the following two cases. If $\\delta_{k+1}\\leq\\operatorname*{max}_{s\\in[k]}\\delta_{k}$ , induction automatically holds. Otherwise, $\\delta_{k+1}>\\operatorname*{max}_{s\\in[k]}\\delta_{s}$ . Applying Equation (17) gives us that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\delta_{k+1}^{2}+\\lambda^{2}\\eta^{2}\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|^{2}\\sum_{j=1}^{k}\\left(1-\\eta\\lambda\\right)^{k-j}-2\\lambda\\eta\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|\\delta_{k+1}}\\\\ {\\displaystyle\\leq\\frac{4\\eta G^{2}}{\\lambda}+\\left(2\\lambda\\eta(1-\\eta\\lambda)\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+\\frac{4G\\eta(1-\\eta\\lambda)}{n}\\right)\\delta_{k+1}\\sum_{j=1}^{k}(1-\\eta\\lambda)^{k-j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which is equivalent to ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left(\\delta_{k+1}\\!-\\!\\left(\\!2\\lambda\\eta||\\mathbf{w}_{t}\\!-\\!\\mathbf{w}_{t}^{\\prime}||+\\!2\\lambda\\eta(1\\!-\\!\\lambda\\eta)||\\mathbf{w}_{t}\\!-\\!\\mathbf{w}_{t}^{\\prime}||\\sum_{j=1}^{k}\\!(1\\!-\\!\\eta\\lambda)^{k\\!-\\!j}\\!+\\!\\frac{4G\\eta(1\\!-\\!\\eta\\lambda)}{n}\\sum_{j=1}^{k}\\!(1\\!-\\!\\lambda\\eta)^{k\\!-\\!j}\\!\\right)\\!\\right)^{2}}\\\\ {\\displaystyle\\leq\\frac{4\\eta G^{2}}{\\lambda}\\!+\\!\\left(\\!2\\lambda\\eta||\\mathbf{w}_{t}\\!-\\!\\mathbf{w}_{t}^{\\prime}||\\!+\\!2\\lambda\\eta(1\\!-\\!\\lambda\\eta)||\\mathbf{w}_{t}\\!-\\!\\mathbf{w}_{t}^{\\prime}||\\sum_{j=1}^{k}\\!(1\\!-\\!\\eta\\lambda)^{k\\!-\\!j}\\!+\\!\\frac{4G\\eta(1\\!-\\!\\eta\\lambda)}{n}\\sum_{j=1}^{k}\\!(1\\!-\\!\\lambda\\eta)^{k\\!-\\!j}\\!\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{k+1}\\!\\!\\leq\\!2G\\sqrt{\\frac{\\eta}{\\lambda}}\\!+\\!2\\Bigg(\\!2\\lambda\\eta\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|\\!+\\!2\\lambda\\eta(1\\!-\\!\\lambda\\eta)\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|\\displaystyle\\sum_{j=1}^{k}(1-\\eta\\lambda)^{k-j}}\\\\ &{\\qquad\\qquad+\\frac{4G\\eta(1-\\eta\\lambda)}{n}\\displaystyle\\sum_{j=1}^{k}(1\\!-\\!\\lambda\\eta)^{k-j}\\Bigg)}\\\\ &{\\leq\\!2G\\sqrt{\\frac{\\eta}{\\lambda}}+4\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+\\frac{8G(1-\\eta\\lambda)}{\\lambda n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Plug in Lemma D.3 gives us that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\delta_{k+1}\\leq(8e G+2G)\\sqrt{\\frac{\\eta}{\\lambda}}+\\frac{8e G}{\\lambda m}+\\frac{8G}{\\lambda n}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{K}{\\sum_{k=1}^{K}{\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1},S)}}-\\frac{1}{K}{\\sum_{k=1}^{K}{\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S^{(i)})}}\\right\\|\\leq(8e G+2G)\\sqrt{\\frac{\\eta}{\\lambda}}+\\frac{8e G}{\\lambda m}+\\frac{8G^{2}}{\\lambda n}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Moreover, setting $\\begin{array}{r}{\\eta=\\frac{1}{\\lambda K}}\\end{array}$ gives us that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{K}{\\sum_{k=1}^{K}{\\bf u}^{(k)}({\\bf w}_{T+1},S)}-\\frac{1}{K}{\\sum_{k=1}^{K}{\\bf u}^{(k)}({\\bf w}_{T+1}^{\\prime},S^{(i)})}\\right\\|\\leq\\frac{8e G+2G}{\\lambda\\sqrt{K}}+\\frac{8e G}{\\lambda m}+\\frac{8G}{\\lambda m}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Theorem 4.6. Assume that the loss function is $\\rho$ -weakly convex, $M$ -bounded, and $G$ -Lipschitz. Suppose we run Algorithm 1 for $T$ iterations with $\\begin{array}{r}{\\gamma\\le\\frac{1}{\\lambda T},\\lambda\\ge2\\rho}\\end{array}$ on a meta-sample S, and GD for task-specific learning (Option 2, Algorithm 2) with $\\begin{array}{r}{\\eta\\le\\frac{1}{\\lambda}}\\end{array}$ , Then, with probability at least $1-\\delta$ \uff0c ", "page_idx": 34}, {"type": "equation", "text": "$$\nL(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mu)\\lesssim L(\\mathbf{\\mathcal{A}}(\\mathbf{S}),\\mathbf{S})+\\left(G^{2}\\sqrt{\\frac{\\eta}{\\lambda}}+\\frac{G^{2}}{\\lambda m}+\\frac{G^{2}}{\\lambda n}\\right)\\log\\left(m n\\right)\\log\\left(1/\\delta\\right)+\\frac{M\\sqrt{\\log\\left(1/\\delta\\right)}}{\\sqrt{m n}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof of Theorem 4.6. For $\\ell$ tobe $G$ -Lipschitz, applying Lemma 4.1 gives us that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell(A(\\mathbf{S})(\\mathcal{S}),z)-\\ell(A(\\mathbf{S}^{(j)})(\\mathcal{S}^{(i)}),z)\\bigg|}\\\\ &{=\\bigg|\\ell\\left(\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1},\\mathcal{S}),z\\right)-\\ell\\left(\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},\\mathcal{S}^{(i)}),z\\right)\\bigg|}\\\\ &{\\le G\\displaystyle\\left\\|\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1},\\mathcal{S})-\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},\\mathcal{S}^{(i)})\\right\\|}\\\\ &{\\le(8e G^{2}+2G^{2})\\sqrt{\\frac{\\eta}{\\lambda}}+\\displaystyle\\frac{8e G^{2}}{\\lambda m}+\\frac{8G^{2}}{\\lambda m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Plug it back into Theorem 3.1 gives the result ", "page_idx": 34}, {"type": "text", "text": "Theorem D.5 (Restatement of Theorem 4.7). Assume the loss $\\ell$ is convex and $G$ -Lipschitz. Define $\\mathsf{u}_{j}^{*}\\!=\\!\\operatorname{argmin}_{\\mathsf{u}}L(\\mathsf{u},S_{j}),\\forall j\\in[m]$ . Suppose we run Algorithm 1 with GD for task-specific learning with $\\begin{array}{r}{\\gamma=\\frac{1}{\\lambda T}}\\end{array}$ to find an algorithm $\\begin{array}{r}{\\mathcal{A}(\\mathbf{S})=\\mathcal{A}_{\\operatorname{task}}\\big(\\mathbf{w}_{T+1},\\cdot\\big)}\\end{array}$ which is then run on ${\\mathbf{}}S_{j}$ for $K$ iterations with step-size $\\begin{array}{r}{\\eta\\le\\frac{1}{\\lambda}}\\end{array}$ . Then, we have that ", "page_idx": 35}, {"type": "text", "text": "$L(\\mathcal{A}(\\mathbf{S})(\\mathcal{S}_{j}),\\mathcal{S}_{j})-\\operatorname*{inf}_{\\mathfrak{u}}L(\\mathbf{u},\\mathcal{S}_{j})\\leq\\frac{D^{2}}{2\\eta(1-\\eta\\lambda)K}+\\frac{G^{2}\\eta}{2(1-\\eta\\lambda)}+\\frac{G D\\eta\\lambda}{1-\\eta\\lambda}+\\frac{\\lambda\\left\\Vert\\mathbf{w}_{T+1}-\\widehat{\\mathbf{w}}\\right\\Vert^{2}+\\lambda\\sigma^{2}}{(1-\\eta\\lambda)(2-\\eta\\lambda)}$ where $\\begin{array}{r l r}{\\sigma^{2}}&{:=}&{\\frac{1}{K}\\sum_{j=1}^{K}\\left\\|\\widehat{\\mathbf{w}}\\!-\\!\\mathbf{u}_{j}^{*}\\right\\|^{2}}\\end{array}$ \uff0cwith $\\widehat{\\bf w}$ as defned in Equation (1). $\\left\\|\\mathbf{w}_{T+1}-\\widehat{\\mathbf{w}}\\right\\|^{2}\\ \\leq$ $\\begin{array}{r}{\\frac{1}{T}\\left(8D^{2}+\\frac{4D^{2}}{\\eta\\lambda K}+\\frac{\\eta(G+2\\lambda D)^{2}}{\\lambda}\\right)+\\frac{2D^{2}}{\\eta\\lambda K}+\\frac{\\eta(G+2\\lambda D)^{2}}{2\\lambda}}\\end{array}$ ", "page_idx": 35}, {"type": "text", "text": "ProofofTheorem $D.5$ Recal thedefnition $\\begin{array}{r}{\\widehat{\\mathbf{w}}\\!=\\!\\arg\\!\\operatorname*{min}_{\\mathbf{w}\\in\\mathcal{W}}\\frac{1}{m}{\\sum_{j=1}^{m}\\!\\operatorname*{min}_{\\mathbf{u}}}\\Big[L\\big(\\mathbf{u};\\mathcal{S}_{j}\\big)\\!+\\!\\frac{\\lambda}{2}\\left\\|\\mathbf{u}-\\mathbf{w}\\right\\|^{2}\\Big],}\\end{array}$ $\\begin{array}{r}{\\mathbf{u}^{*}(\\mathbf{w},S)\\ =\\ \\mathrm{argmin}_{\\mathbf{u}\\in\\mathcal{W}}\\left[L(\\mathbf{u};S)+\\frac{\\lambda}{2}\\left\\|\\mathbf{u}-\\mathbf{w}\\right\\|^{2}\\right]}\\end{array}$ \uff0c $\\begin{array}{r}{\\mathbf{u}_{j}^{*}\\ =\\ \\mathrm{argmin}_{\\mathbf{u}\\in\\mathcal{W}}\\,L(\\mathbf{u},\\mathcal{S}_{j}),\\forall j\\ \\in\\ [m]}\\end{array}$ .We slightly abuse the notation by defining $\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S_{j})\\,=\\,A(\\mathbf{S})(S_{j})$ at inner iteration $k$ for given $\\mathbf{W}_{t}$ . Then we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mu^{(n+1)}(w,S)-w\\right\\|^{2}}\\\\ &{=\\left\\|\\operatorname*{lim}_{\\mathcal{N}}\\left(\\mu^{(n)}(w,S)\\right)-\\eta\\left(\\nabla L(u^{(n)}(w,S),S)+\\lambda(u^{(n)}(w,S))-w_{0}\\right)\\right)-w_{0}^{1}\\right\\|^{2}}\\\\ &{\\leq\\left\\|(1-\\eta)\\left(u^{(n)}(w,S)-u_{x}^{\\prime})+\\eta\\left(x-u_{x}^{\\prime}\\right)-\\eta\\nabla L(u^{(n)}(w,S),S)\\right)\\right\\|^{2}}\\\\ &{\\leq(1-\\eta)^{3}\\left\\|\\overline{{\\mu}}^{(n)}(w,S)-u_{x}^{\\prime}\\right\\|^{2}+\\eta^{2}\\left\\|\\nabla L(u^{(n)}(w,S),S)\\right\\|^{2}+\\eta^{2}\\lambda^{2}\\left\\|w_{0}-w_{0}^{\\prime}\\right\\|^{2}}\\\\ &{\\qquad+2\\eta\\lambda(1-\\eta)\\left\\|u^{(n)}(w,S)-u_{x}^{\\prime}\\right\\|^{2}\\left\\|\\Pi_{R}-u_{x}^{\\prime}\\right\\|^{2}+\\eta^{2}\\lambda^{2}\\alpha\\left\\|w_{0}-w_{0}^{\\prime}\\right\\|}\\\\ &{\\qquad-2\\eta(1-\\eta)\\sqrt{\\lambda}\\left\\|\\nabla L(u^{(n)}(w,S),S),\\eta\\right\\|^{2}+\\eta^{2}\\left\\|\\nabla L(u^{(n)}(w,S),S)\\right\\|^{2}}\\\\ &{=\\left(1-\\eta\\right)\\left\\|\\overline{{\\mu}}^{(n)}(w,S)-u_{x}^{\\prime}\\right\\|^{2}+\\eta^{2}\\left\\|\\Pi_{R}-u_{y}^{\\prime}\\right\\|^{2}+^{2}\\eta^{2}\\left\\|\\nabla L(u^{(n)}(w,S),S)\\right\\|^{2}}\\\\ &{\\qquad+\\eta^{2}\\lambda\\alpha\\left\\|w_{0}-w_{0}^{\\prime}\\right\\|-2\\eta(1-\\eta)\\sqrt{\\lambda}\\left\\|\\nabla L(u^{(n)}(w,S),S),\\eta\\right\\|^{2}}\\\\ &{\\leq\\left\\|u^{(n)}(w,S)-u_{x}^{\\prime}\\right\\|^{2}+\\frac{2\\eta-1}{\\lambda^{2}\\alpha\\left\\|\\overline{{\\mu}}^{(n)}(w,S)\\right\\|^{2}}+ \n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Rearrange it and telescope it gives us that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\displaystyle\\cal L\\left(\\frac{1}{K}\\sum_{k=1}^{K}\\mathrm{u}^{(k)}(\\mathbf w_{t},S_{j}),S_{j}\\right)-\\cal L(\\mathbf u_{j}^{*},S_{j})}\\\\ &{\\le\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\cal L(\\mathbf u^{(k)}(\\mathbf w_{t},S_{j}),S_{j})-\\cal L(\\mathbf u_{j}^{*},S_{j})}&{\\mathrm{(fenser~\\bar{s}~i n e q u a l i t y)}}\\\\ &{\\le\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\left\\langle\\nabla{\\cal L}(\\mathbf u^{(k)}(\\mathbf w_{t},S_{j}),S_{j}),\\mathbf u^{(k)}(\\mathbf w_{t},S_{j})-\\mathbf u_{j}^{*}\\right\\rangle}&{\\mathrm{(Convexity)}}\\\\ &{\\le\\displaystyle\\frac{\\|\\mathbf u_{j}^{*}\\|^{2}+\\eta^{2}\\cal L(\\tau^{2}K+\\eta^{2}\\lambda G\\sum_{j=1}^{K}\\left\\|\\mathbf w_{t}-\\mathbf u_{j}^{*}\\right\\|+\\frac{\\eta\\lambda}{2-\\eta\\lambda}\\sum_{j=1}^{K}\\left\\|\\mathbf w_{t}-\\mathbf u_{j}^{*}\\right\\|^{2}}{2\\eta(1-\\eta\\lambda)K}}&\\\\ &{\\le\\displaystyle\\frac{D^{2}}{2\\eta(1-\\eta\\lambda)K}+\\frac{G^{2}\\eta}{2(1-\\eta\\lambda)}+\\frac{2G D\\eta\\lambda}{1-\\eta\\lambda}+\\frac{\\lambda\\|\\mathbf w_{t}-\\bar{\\mathbf w}\\|^{2}+\\lambda\\sigma^{2}}{(1-\\eta\\lambda)(2-\\eta\\lambda)}{(\\sigma^{2}-\\eta\\lambda)}}&{(\\sigma^{2}=\\frac{1}{K}\\sum_{k=1}^{K}\\left\\|\\mathbf u_{j}^{*}-\\hat{\\mathbf w}\\right\\|^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Follow from [Zhou et al., 2019, Theorem 1], we now control $\\left|\\left|\\mathbf{w}_{t+1}-\\widehat{\\mathbf{w}}\\right|\\right|^{2}$ .Define $\\mathbf{u}^{*}(\\mathbf{w}_{t},\\boldsymbol{S}_{j})=$ $\\begin{array}{r}{\\mathrm{argmin}_{\\mathbf{u}}\\,F_{S_{j}}(\\mathbf{u},\\mathbf{w}_{t})=\\mathrm{argmin}_{\\mathbf{u}}\\,L(\\mathbf{u},S_{j})+\\frac{\\lambda}{2}\\left\\|\\mathbf{u}-\\mathbf{w}_{t}\\right\\|^{2}}\\end{array}$ We start with the folowing: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\left\\|\\mathbf{w}_{t+1}-\\hat{\\mathbf{w}}\\right\\|^{2}=\\left\\|\\Pi_{W}\\left(\\mathbf{w}_{t}-\\gamma\\lambda\\left(\\mathbf{w}_{t}-\\frac{1}{m}\\displaystyle\\sum_{j=1}^{m}\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S_{j})\\right)\\right)-\\hat{\\mathbf{w}}\\right\\|^{2}}\\\\ &{\\le\\left\\|\\mathbf{w}_{t}-\\hat{\\mathbf{w}}-\\gamma\\lambda\\left(\\mathbf{w}_{t}-\\frac{1}{m}\\displaystyle\\sum_{j=1}^{m}\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S_{j})\\right)\\right\\|^{2}}\\\\ &{\\le\\left\\|\\mathbf{w}_{t}-\\hat{\\mathbf{w}}\\right\\|^{2}-2\\gamma\\lambda\\left\\langle\\mathbf{w}_{t}-\\hat{\\mathbf{w}},\\mathbf{w}_{t}-\\frac{1}{m}\\displaystyle\\sum_{j=1}^{m}\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S_{j})\\right\\rangle}\\\\ &{\\phantom{=\\;}+\\gamma^{2}\\lambda^{2}\\left\\|\\mathbf{w}_{t}-\\frac{1}{m}\\displaystyle\\sum_{j=1}^{m}\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S_{j})\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We now bound the latter two terms separately as follows: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|\\mathbf{w}_{t}-\\frac{1}{m}\\sum_{j=1}^{m}\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S_{j})\\right\\|^{2}}\\\\ {\\displaystyle=\\left\\|\\mathbf{w}_{t}-\\frac{1}{m}\\sum_{j=1}^{m}\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{*}(\\mathbf{w}_{t},S_{j})+\\frac{1}{m}\\sum_{j=1}^{m}\\frac{1}{K}\\sum_{k=1}^{K}\\left(\\mathbf{u}^{*}(\\mathbf{w}_{t},S_{j})-\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S_{j})\\right)\\right\\|^{2}}\\\\ {\\displaystyle\\leq2\\left\\|\\mathbf{w}_{t}-\\frac{1}{m}\\sum_{j=1}^{m}\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{*}(\\mathbf{w}_{t},S_{j})\\right\\|^{2}+\\displaystyle\\frac{2}{m}\\sum_{j=1}^{m}\\frac{1}{K}\\sum_{k=1}^{K}\\left\\|\\left(\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S_{j})-\\mathbf{u}^{*}(\\mathbf{w}_{t},S_{j})\\right)\\right\\|^{2}}\\\\ {\\displaystyle\\leq8D^{2}+\\frac{2}{m}\\sum_{j=1}^{m}\\frac{1}{K}\\sum_{k=1}^{K}\\left\\|\\left(\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S_{j})-\\mathbf{u}^{*}(\\mathbf{w}_{t},S_{j})\\right)\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "as well as ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle w_{1}-\\hat{w}_{1},\\sum_{n=1}^{\\infty}\\frac{1}{m_{n}!}\\frac{1}{m_{n}!}\\sum_{i=1}^{m}(w_{1},\\hat{x}_{i})\\right\\rangle}\\\\ &{=\\frac{1}{m_{n}!}\\frac{1}{m_{n}!}\\sum_{i=1}^{M}\\frac{\\hat{x}_{i}}{\\Delta x_{i}!}w_{i}-\\hat{w}_{n},w_{i}-w^{*}(w_{1},\\hat{x}_{i}))}\\\\ &{\\qquad-\\frac{1}{m_{n}!}\\sum_{i=1}^{m}\\frac{1}{\\Delta x_{i}!}\\sum_{\\binom{n}{i}=-1}^{m}w_{i}-\\hat{w}_{n}\\epsilon_{i}w_{1}(w_{1},\\hat{x}_{i})-w^{*}(w_{1},\\hat{x}_{i}))}\\\\ &{\\geq\\frac{1}{m_{n}!}\\frac{1}{m_{n}!}\\sum_{i=1}^{M}\\frac{1}{\\Delta x_{i}!}\\left\\langle w_{1}-\\hat{w}_{1},\\frac{1}{\\Delta x_{i}!}\\nabla_{E_{i}}(w_{1},\\hat{x}_{i}),w_{1}\\right\\rangle-\\frac{1}{2}|w_{1}-\\hat{w}_{1}|^{2}}\\\\ &{\\qquad-\\frac{1}{2m_{n}!}\\frac{1}{m_{n}!}\\sum_{i=1}^{M}\\left\\langle w_{1}^{*}\\otimes(w_{1},\\hat{x}_{i})-w^{*}(w_{1},\\hat{x}_{i})\\right\\rangle^{2}}\\\\ &{\\geq[w_{1}-\\hat{w}_{1}^{2}-\\frac{1}{2}]w_{1}w_{1}^{2}\\epsilon_{i}(w_{1},\\hat{x}_{i})-\\frac{1}{2m_{n}!}\\frac{1}{m_{n}!}\\sum_{i=1}^{N}\\frac{1}{\\Delta x_{i}!}\\left\\|w^{*}(w_{1},\\hat{x}_{i})-w^{*}(w_{1},\\hat{x}_{i})\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\epsilon_{i}\\sum_{j=1}^{m}F_{i}\\sum_{i=1}^{N}\\hat{w}_{j}(w \n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the common term $\\left\\|\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S_{j})-\\mathbf{u}^{*}\\big(\\mathbf{w}_{t},S_{j})\\right\\|^{2}$ can be controlled as follows: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbf{u}^{(k+1)}(\\mathbf{w}_{t},S_{j})-\\mathbf{u}^{*}(\\mathbf{w}_{t},S_{j})\\right|^{2}}\\\\ &{\\leq\\left|\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S_{j})-\\eta\\nabla F_{S_{j}}(\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S_{j}),\\mathbf{w}_{t})-\\mathbf{u}^{*}(\\mathbf{w}_{t},S_{j})\\right|^{2}}\\\\ &{\\leq\\left|\\mathbf{u}^{(k+1)}(\\mathbf{w}_{t},S_{j})-\\mathbf{u}^{*}(\\mathbf{w}_{t},S_{j})\\right|^{2}-2\\eta\\left\\langle\\nabla F_{S_{j}}(\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S_{j}),\\mathbf{w}_{t}),\\mathbf{u}^{(k+1)}(\\mathbf{w}_{t},S_{j})-\\mathbf{u}^{*}(\\mathbf{w}_{t},S_{j})\\right\\rangle}\\\\ &{\\qquad+\\eta^{2}\\left\\|\\nabla F_{S_{j}}(\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S_{j}),\\mathbf{w}_{t})\\right\\|^{2}}\\\\ &{\\leq(1-2\\eta\\lambda)\\left\\|\\mathbf{u}^{(k+1)}(\\mathbf{w}_{t},S_{j})-\\mathbf{u}^{*}(\\mathbf{w}_{t},S_{j})\\right\\|^{2}+\\eta^{2}\\left\\|\\nabla L(\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S_{j}),S_{j})+\\lambda(\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S_{j})-\\mathbf{w}_{t})\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(F_{S_{j}}(\\mathbf{u},\\mathbf{w})\\mathrm{~is~.~stromgly~convex~w.t~.})}\\\\ &{\\leq(1-2\\eta\\lambda)\\left\\|\\mathbf{u}^{(k+1)}(\\mathbf{w}_{t},S_{j})-\\mathbf{u}^{*}(\\mathbf{w}_{t},S_{j})\\right\\|^{2}+\\eta^{2}(G+2\\lambda D)^{2}}\\\\ &{\\leq4(1-2\\eta\\lambda)^{k}D^{2}+\\frac{\\eta(G+2\\lambda D)^{2}}{2\\lambda}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Plug back into Equation (19) gives us that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{w}_{t+1}-\\widehat{\\mathbf{w}}\\right\\|^{2}}\\\\ &{\\leq\\|\\mathbf{w}_{t}-\\widehat{\\mathbf{w}}\\|^{2}-2\\gamma\\lambda\\left(\\frac{1}{2}\\left\\|\\mathbf{w}_{t}-\\widehat{\\mathbf{w}}\\right\\|^{2}-\\frac{1}{2m}\\displaystyle\\sum_{j=1}^{m}\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\left\\|\\mathbf{u}^{(k)}(\\mathbf{w}_{t},\\mathcal{S}_{j})-\\mathbf{u}^{*}(\\mathbf{w}_{t},\\mathcal{S}_{j})\\right\\|^{2}\\right)}\\\\ &{\\qquad+\\gamma^{2}\\lambda^{2}\\left(8D^{2}+\\frac{2}{m}\\displaystyle\\sum_{j=1}^{m}\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\left\\|\\Big(\\mathbf{u}^{(k)}(\\mathbf{w}_{t},\\mathcal{S}_{j})-\\mathbf{u}^{*}(\\mathbf{w}_{t},\\mathcal{S}_{j})\\Big)\\Big\\|^{2}\\right)}\\\\ &{\\leq(1-\\gamma\\lambda)\\left\\|\\mathbf{w}_{t}-\\widehat{\\mathbf{w}}\\right\\|^{2}+\\gamma\\lambda\\left(\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}4(1-2\\eta\\lambda)^{k}D^{2}+\\frac{\\eta(G+2\\lambda D)^{2}}{2\\lambda}\\right)}\\\\ &{\\qquad+\\gamma^{2}\\lambda^{2}\\left(8D^{2}+\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}8(1-2\\eta\\lambda)^{k}D^{2}+\\frac{\\eta(G+2\\lambda D)^{2}}{\\lambda}\\right)}\\\\ &{\\leq(1-\\gamma\\lambda)\\left\\|\\mathbf{w}_{t}-\\widehat{\\mathbf{w}}\\right\\|^{2}+\\gamma\\lambda\\left(\\displaystyle\\frac{2D^{2}}{2K\\lambda}+\\displaystyle\\frac{\\eta(G+2\\lambda D)^{2}}{2\\lambda}\\right)+\\gamma^{2}\\lambda^{2}\\left(8D^{2}+\\frac{4D^{2}}{\\lambda\\lambda}+\\displaystyle\\frac{\\eta(G+2\\lambda D)^{2}}{\\lambda}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Choosing $\\begin{array}{r}{\\gamma=\\frac{1}{\\lambda T}}\\end{array}$ gives us that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{w}_{t+1}-\\widehat{\\mathbf{w}}\\right\\|^{2}}\\\\ &{\\leq(1-\\displaystyle\\frac{1}{T})\\left\\|\\mathbf{w}_{t}-\\widehat{\\mathbf{w}}\\right\\|^{2}+\\frac{1}{T}\\left(\\frac{2D^{2}}{\\eta\\lambda K}+\\frac{\\eta(G+2\\lambda D)^{2}}{2\\lambda}\\right)+\\frac{1}{T^{2}}\\left(8D^{2}+\\frac{4D^{2}}{\\eta\\lambda K}+\\frac{\\eta(G+2\\lambda D)^{2}}{\\lambda}\\right)}\\\\ &{\\leq\\displaystyle\\frac{\\left(8D^{2}+\\frac{4D^{2}}{\\eta\\lambda K}+\\frac{\\eta(G+2\\lambda D)^{2}}{\\lambda}\\right)}{T}+\\frac{2D^{2}}{\\eta\\lambda K}+\\frac{\\eta(G+2\\lambda D)^{2}}{2\\lambda}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "E Missing Proofs in Section 5 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Theorem E.1 (Bennett's inequality). Let $x_{1},\\ldots,x_{n}$ be independent r.v. with finite variance. Further assume $|x_{i}-\\mathbb{E}x_{i}|\\ \\leq\\ a$ a.s. for all $i$ \u201cDefine $\\begin{array}{r}{S_{n}\\ \\ {\\stackrel{\\cdot}{=}}\\ \\sum_{i=1}^{n}\\left[x_{i}-\\mathbb{E}[x_{i}]\\right]}\\end{array}$ and $\\sigma^{2}\\mathbf{\\Sigma}=\\mathbf{\\Sigma}$ $\\begin{array}{r}{\\sum_{i=1}^{n}\\mathbb{E}\\left(x_{i}-\\mathbb{E}[x_{i}]\\right)^{2}}\\end{array}$ . Then for any $t\\geq0$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathrm{P}\\left(S_{n}>t\\right)\\leq\\exp\\left(-\\frac{\\sigma^{2}}{a^{2}}h\\left(\\frac{a t}{\\sigma^{2}}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $h(u)=(1+u)\\log{(1+u)}-u$ ", "page_idx": 37}, {"type": "text", "text": "Lemma 5.1. Assume that the loss function is $\\rho$ -weakly convex and $G$ -Lipschitz. Let S, $\\mathbf{S}^{(j)}$ denote neighboring meta-samples and $\\boldsymbol{S}$ $S^{(i)}$ the neighboring samples on a test task. Then, with probability at least $1\\!-\\!\\exp{\\left(-T^{2}\\dot{e}^{2}/m^{2}\\right)}$ , the following holds for Algorithm 3 with $\\lambda\\geq2\\rho$ , and GD for taskspecific learning(i.e., Option 2 for Algorithm 2) with $\\begin{array}{r}{\\eta\\le\\frac{1}{\\lambda}}\\end{array}$ for all $T\\geq1$ as long as we set $\\begin{array}{r}{\\gamma\\leq\\frac{1}{\\lambda T}}\\end{array}$ \uff0c ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbf{S},S,i\\in[n],j\\in[m]}\\Big\\|A(\\mathbf{S})(S)-A(\\mathbf{S}^{(j)})(S^{(i)})\\Big\\|\\leq(8e G+2G)\\sqrt{\\frac{\\eta}{\\lambda}}+\\frac{8e G}{\\lambda m}+\\frac{8G}{\\lambda n}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof of Lemma 5.1. We slightly abuse the notation, at outer iteration $t$ ,define $\\mathbf{w}_{t}~=~\\mathcal{A}(\\mathbf{S})$ $\\mathbf{w}_{t}^{\\prime}\\,=\\,\\mathcal{A}(\\mathbf{S}^{(j)})$ ._Given $\\mathbf{W}_{t}$ , at inner iteration $k$ , define $\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S)\\,=\\,\\mathcal{A}(\\mathbf{S})(S),\\,\\mathbf{u}^{(k)}(\\mathbf{w}_{t}^{\\prime},S^{(i)})\\,=$ $A(\\mathbf{S}^{(j)})(S^{(i)})$ . From a similar argument as Lemma D.3, $\\forall k\\in[K-1]$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{u}^{(k+1)}(\\mathbf{w}_{t},S)-\\mathbf{u}^{(k+1)}(\\mathbf{w}_{t}^{\\prime},S^{\\prime})\\right\\|\\leq2\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+\\frac{2G}{\\lambda}}\\\\ &{\\left\\|\\mathbf{u}^{(k+1)}(\\mathbf{w}_{t},S)-\\mathbf{u}^{(k+1)}(\\mathbf{w}_{t}^{\\prime},S)\\right\\|\\leq2\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+2G\\sqrt{\\frac{\\eta}{\\lambda}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Let us define $r_{t}\\,=\\,\\mathbb{1}(S_{j_{t}}\\neq S_{j_{t}})$ . Note that at every step $t$ $\\mathbb{E}_{A}(r_{t})\\,=\\,\\frac{1}{m}$ . Moreover, note that $\\{r_{t}:t\\in[T]\\}$ is an independent sequence of Bernoulli random variables. As a result, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left\\|\\mathbf{w}_{t+1}-\\mathbf{w}_{t+1}^{\\prime}\\right\\|}\\\\ &{\\leq\\displaystyle\\left\\|\\mathbf{w}_{t}-\\gamma\\lambda\\left(\\mathbf{w}_{t}-\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S_{j_{t}})\\right)-\\mathbf{w}_{t}^{\\prime}+\\gamma\\lambda\\left(\\mathbf{w}_{t}^{\\prime}-\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{t}^{\\prime},S_{j_{t}}^{\\prime})\\right)\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "(Projection is non-expansive) ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle=(1-\\gamma\\lambda)\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+\\gamma\\lambda\\left\\|\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{t},S_{j_{t}})-\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{t}^{\\prime},S_{j_{t}}^{\\prime})\\right\\|}\\\\ {\\displaystyle\\leq(1-\\gamma\\lambda)\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+\\gamma\\lambda(1-r_{t})\\!\\Bigg(2\\left\\|\\mathbf{w}_{t}\\!-\\!\\mathbf{w}_{t}^{\\prime}\\right\\|\\!+\\!2G\\sqrt{\\frac{\\eta}{\\lambda}}\\Bigg)\\!+\\!\\gamma\\lambda r_{t}\\!\\left(\\left\\|\\mathbf{w}_{t}\\!-\\!\\mathbf{w}_{t}^{\\prime}\\right\\|\\!+\\!\\frac{2G}{\\lambda}\\right)}\\\\ {\\displaystyle\\leq(1+(1-r_{t})\\gamma\\lambda)\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+2G\\gamma\\sqrt{\\eta\\lambda}+2G\\gamma r_{t}}\\\\ {\\displaystyle\\leq(1+\\gamma\\lambda)\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+2G\\gamma\\sqrt{\\eta\\lambda}+2G\\gamma r_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Telescoping gives us that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left\\Vert\\mathbf{w}_{T+1}-\\mathbf{w}_{T+1}^{\\prime}\\right\\Vert\\leq2G\\sqrt{\\frac{\\eta}{\\lambda}}{\\left(1+\\gamma\\lambda\\right)}^{T}+2G\\gamma\\sum_{t=1}^{T}\\left(1+\\gamma\\lambda\\right)^{t-1}r_{t}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Further taking expectation w.r.t the randomness of the algorithm and gives us that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\cal A}\\left\\|\\mathbf{w}_{T+1}-\\mathbf{w}_{T+1}^{\\prime}\\right\\|\\leq\\left(1+\\gamma\\lambda\\right)^{T}\\left(2G\\sqrt{\\frac{\\eta}{\\lambda}}+\\frac{2G}{\\lambda m}\\right)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Choosing $\\gamma\\leq{\\frac{1}{\\lambda T}}$ gives us that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}_{A}\\left\\|\\mathbf{w}_{T+1}-\\mathbf{w}_{T+1}^{\\prime}\\right\\|\\leq(1+\\frac{1}{T})^{T}\\left(2G\\sqrt{\\frac{\\eta}{\\lambda}}+\\frac{2G}{\\lambda m}\\right)\\leq2e G\\sqrt{\\frac{\\eta}{\\lambda}}+\\frac{2e G}{\\lambda m}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Plug this back into Equation (18) gives us that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\boldsymbol{A}}\\left\\|\\mathbf{u}^{(K+1)}(\\mathbf{w}_{T+1},S)\\!-\\!\\mathbf{u}^{(K+1)}(\\mathbf{w}_{T+1}^{\\prime},S^{(i)})\\right\\|\\leq2G\\sqrt{\\frac{\\eta}{\\lambda}}+4\\mathbb{E}_{\\boldsymbol{A}}\\left\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\right\\|+\\frac{8G(1-\\eta\\lambda)}{\\lambda n}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq(8e G+2G)\\sqrt{\\frac{\\eta}{\\lambda}}+\\frac{8e G}{\\lambda m}+\\frac{8G}{\\lambda n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Seting $\\gamma\\leq\\frac{1}{\\lambda T}$ We note that for each $r_{t}$ has ariance smallr than $\\textstyle{\\frac{1}{m}}$ . Define random variable $\\begin{array}{r}{x_{t}:=(1+\\frac{1}{T})^{t-1}r_{t}}\\end{array}$ .We have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle|x_{t}-\\mathbb{E}[x_{t}]|=(1+\\frac{1}{T})^{t-1}\\left(r_{t}-\\mathbb{E}[r_{t}]\\right)\\leq e\\left|x_{t}-\\mathbb{E}[x_{t}]\\right|\\leq(1+\\frac{1}{T})^{t-1}\\left(1-\\frac{1}{m}\\right)\\leq e}\\\\ &{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left(x_{i}-\\mathbb{E}[x_{t}]\\right)^{2}\\leq\\displaystyle\\sum_{t=1}^{T}(1+\\frac{1}{T})^{2t-2}\\frac{1}{m}\\left(1-\\frac{1}{m}\\right)<\\frac{T e^{2}}{m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Hence by Bennett's inequality Theorem E.1, we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathsf{P}\\left[\\sum_{t=1}^{T}(1+\\frac{1}{T})^{t-1}r_{t}\\geq\\frac{1}{m}\\sum_{t=1}^{T}(1+\\frac{1}{T})^{t-1}\\right]\\leq\\exp\\left(-\\frac{T^{2}e^{2}}{m^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Therefore, with probability at least $1-\\exp\\left(-T^{2}e^{2}/m^{2}\\right)$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left\\Vert\\mathbf{w}_{T+1}-\\mathbf{w}_{T+1}^{\\prime}\\right\\Vert\\leq2G\\sqrt{\\frac{\\eta}{\\lambda}}{\\left(1+\\frac{1}{T}\\right)}^{T}+\\frac{2G}{\\lambda m T}\\sum_{t=1}^{T}\\left(1+\\frac{1}{T}\\right)^{t-1}\\leq2e G\\sqrt{\\frac{\\eta}{\\lambda}}+\\frac{2e G}{\\lambda m}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and therefore with probability at least $1-\\exp\\left(-T^{2}e^{2}/m^{2}\\right)$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall k\\in[K-1],\\Big\\|\\mathbf{u}^{(K+1)}(\\mathbf{w}_{T+1},S)-\\mathbf{u}^{(K+1)}(\\mathbf{w}_{T+1}^{\\prime},S^{(i)})\\Big\\|\\leq2G\\sqrt{\\frac{\\eta}{\\lambda}}+4\\,\\|\\mathbf{w}_{t}-\\mathbf{w}_{t}^{\\prime}\\|+\\frac{8G(1-\\eta\\lambda)}{\\lambda n}}\\\\ {\\leq(8e G+2G)\\sqrt{\\frac{\\eta}{\\lambda}}+\\frac{8e G}{\\lambda m}+\\frac{8G}{\\lambda n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "By triangle inequality, we have with probability at least $1-\\exp\\left(-T^{2}e^{2}/m^{2}\\right)$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1},S)-\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{u}^{(k)}(\\mathbf{w}_{T+1}^{\\prime},S^{(i)})\\right\\|\\leq(8e G+2G)\\sqrt{\\frac{\\eta}{\\lambda}}+\\frac{8e G}{\\lambda m}+\\frac{8G^{2}}{\\lambda n}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proposition 5.2. Given a loss function $\\ell(\\cdot,\\mathbf{z})$ and its adversarial counterpart $\\tilde{\\ell}(\\cdot,z)$ , the following holds: (1) If $\\ell$ is $G$ -Lipschitz (in its first argument), then $\\tilde{\\ell}$ is $G$ -Lipschitz. (2) $\\tilde{\\ell}$ is not $H$ -smooth even $\\ell$ .s $H$ -smooth. (3) If $\\ell$ is $H$ -smooth in w, then $\\tilde{\\ell}$ is $H$ -weakly convex in w. ", "page_idx": 39}, {"type": "text", "text": "Proof of Proposition 5.2. Given $\\mathbf{w}_{1},\\mathbf{w}_{2}$ , define ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\tilde{\\mathbf{z}}_{1}\\in\\underset{\\tilde{\\mathbf{z}}\\in B(\\mathbf{z})}{\\mathrm{argmax}}\\,\\ell\\big(\\mathbf{w}_{1},\\tilde{\\mathbf{z}}\\big)}\\\\ &{}&{\\tilde{\\mathbf{z}}_{2}\\in\\underset{\\tilde{\\mathbf{z}}\\in B(\\mathbf{z})}{\\mathrm{argmax}}\\,\\ell\\big(\\mathbf{w}_{2},\\tilde{\\mathbf{z}}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "For the first item, it holds as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\tilde{\\ell}(\\mathbf{w}_{1},\\mathbf{z})-\\tilde{\\ell}(\\mathbf{w}_{2},\\mathbf{z})\\right\\|=\\|\\ell(\\mathbf{w}_{1},\\tilde{\\mathbf{z}}_{1})-\\ell(\\mathbf{w}_{2},\\tilde{\\mathbf{z}}_{2})\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\operatorname*{max}\\left\\{|\\ell(\\mathbf{w}_{1},\\tilde{\\mathbf{z}}_{1})-\\ell(\\mathbf{w}_{2},\\tilde{\\mathbf{z}}_{1})|\\,,|\\ell(\\mathbf{w}_{1},\\tilde{\\mathbf{z}}_{2})-\\ell(\\mathbf{w}_{2},\\tilde{\\mathbf{z}}_{2})|\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\leq G\\left\\|\\mathbf{w}_{1}-\\mathbf{w}_{2}\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "For the second item, the non-smoothness of the adversarial loss has been verified in Xing et al. [2021], Xia0 et al. [2022]. For the third item, $\\ell(\\mathbf{w},\\mathbf{z})$ is $H$ -smooth implies that $\\ell(\\mathbf{w},\\mathbf{z})$ is $H$ -weakly convex, and further derive that $\\tilde{\\ell}(\\mathbf{w},\\mathbf{z})$ is $H$ weaklyconvexbecause ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\tilde{\\ell}(\\mathbf{w}_{2},\\mathbf{z})=\\ell(\\mathbf{w}_{2},\\tilde{\\mathbf{z}}_{2})}}\\\\ &{}&{\\geq\\ell(\\mathbf{w}_{2},\\tilde{\\mathbf{z}}_{1})}\\\\ &{}&{\\geq\\ell(\\mathbf{w}_{1},\\tilde{\\mathbf{z}}_{1})+\\langle g(\\mathbf{w}_{1},\\tilde{\\mathbf{z}}_{1}),\\mathbf{w}_{2}-\\mathbf{w}_{1}\\rangle-\\frac{\\rho}{2}\\left\\|\\mathbf{w}_{2}-\\mathbf{w}_{1}\\right\\|^{2}}\\\\ &{}&{\\qquad\\left(g(\\mathbf{w}_{1},\\tilde{\\mathbf{z}}_{1})\\in\\partial\\ell(\\mathbf{w}_{2},\\tilde{\\mathbf{z}}_{1}),\\mathrm{apply~Proposition~D.1}\\right)}\\\\ &{}&{=\\tilde{\\ell}(\\mathbf{w}_{1},\\mathbf{z})+\\langle\\tilde{g}(\\mathbf{w}_{1},\\mathbf{z}),\\mathbf{w}_{2}-\\mathbf{w}_{1}\\rangle-\\frac{\\rho}{2}\\left\\|\\mathbf{w}_{2}-\\mathbf{w}_{1}\\right\\|^{2}\\quad(\\mathrm{Redefine~}\\tilde{g}(\\mathbf{w}_{1},\\mathbf{z})\\in\\partial\\tilde{\\ell}(\\mathbf{w}_{1},\\mathbf{z}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "A. Claims ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We further expand on the claims made in abstract and introduction in Section 3 and 4. The detailed proofs are provided in the Appendix. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 40}, {"type": "text", "text": "B. Limitations ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: We list the required assumptions in the statement of each theorems. Several limitations are described together with future work in Section 6. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 40}, {"type": "text", "text": "C. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: All the proofs are provided in the Appendix. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 41}, {"type": "text", "text": "D. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: Please see Section A in the Appendix ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (t) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 41}, {"type": "text", "text": "E. Open access to data and code ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: Please see Section A in the Appendix. The code is provided in the supplementary file. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https : //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so ^No\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 42}, {"type": "text", "text": "F. Experimental Setting/Details ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: Please see Section A in the Appendix. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 42}, {"type": "text", "text": "G. Experiment Statistical Significance ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: Please see Section A in the Appendix. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "H. Experiments Compute Resources ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: Please see Section A in the Appendix. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 43}, {"type": "text", "text": "1. Code Of Ethics ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: The theoretical nature of the results means there are minimal ethical concerns. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 43}, {"type": "text", "text": "J. Broader Impacts ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: See Section 5.2 and 6. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "K. Safeguards ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 44}, {"type": "text", "text": "L. Licenses for existing assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 44}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 45}, {"type": "text", "text": "M. New Assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 45}, {"type": "text", "text": "N. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 45}, {"type": "text", "text": "O. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 45}]