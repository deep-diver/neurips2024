[{"figure_path": "j7sw0nXLjZ/figures/figures_1_1.jpg", "caption": "Figure 1: Four mainstream paradigms to ameliorate the scarcity of labeled biochemical data. (A) Self-supervised pretraining tasks include masked component modeling, contrastive learning, and auto-encoding. (B) Active learning involves the iterative selection of the most informative data, in which the molecular models are the most uncertain. These samples are then subjected to laboratory testing to determine their labels. This process is repeated with newly labeled data added to the training set. (C) Knowledge graphs are introduced to provide structured relations among multiple drugs and unstructured semantic relations associated with different drug molecules. (D) In SSL, the unlabeled data is used to create a smooth decision boundary between different classes or to estimate the distribution of the input data, while the labeled data is used to provide specific examples of the correct output.", "description": "This figure illustrates four common approaches to address the challenge of limited labeled data in biochemical machine learning.  (A) shows self-supervised pre-training, leveraging unlabeled data to learn representations. (B) depicts active learning, iteratively selecting data points for manual labeling based on model uncertainty. (C) highlights the use of domain knowledge, such as drug knowledge graphs, to improve model performance. (D) presents semi-supervised learning, combining labeled and unlabeled data to learn a more robust model.", "section": "1 Introduction"}, {"figure_path": "j7sw0nXLjZ/figures/figures_2_1.jpg", "caption": "Figure 2: The outline of InstructMol. We utilize a pre-trained target molecular model to forecast the properties of unlabeled examples as pseudo-labels. Then, an instructor model predicts the confidence of those pseudo-annotations, which are leveraged to guide the target molecular model to distribute different attention in inferring different data points.", "description": "This figure illustrates the two phases of the InstructMol algorithm.  In Phase 1, a pre-trained molecular model assigns pseudo-labels to unlabeled data.  Then, in Phase 2, an instructor model evaluates the reliability of these pseudo-labels. This information guides the target model in its learning process, weighting the importance of different data points (labeled and pseudo-labeled) to improve performance in predicting molecular properties and out-of-distribution (OOD) benchmarks.  The instructor model helps to mitigate the impact of potentially noisy pseudo-labels.", "section": "4 Method"}, {"figure_path": "j7sw0nXLjZ/figures/figures_7_1.jpg", "caption": "Figure 3: The scatter plot of the distributions of LogP predictions for unlabeled data with and without InstructMol. The first row includes predictions before instructive learning, and the second row includes predictions after instructive learning.", "description": "This figure shows the scatter plots of the predicted LogP values against the actual LogP values for unlabeled data, comparing the performance of InstructMol with and without instructive learning.  The top row displays the predictions before InstructMol's instructive learning phase, while the bottom row shows the predictions *after* this phase.  Each subplot represents a different training data size (14, 73, 146, 730, and 1326 molecules), demonstrating how the model's accuracy improves with instructive learning, even with limited training data.  The RMSE (Root Mean Squared Error) value is provided for each subplot, quantifying the prediction error. The plots visually illustrate the improved accuracy and reduced error of InstructMol's LogP predictions after instructive learning.", "section": "5.4 Discussion, Ablation, and Other Applications"}, {"figure_path": "j7sw0nXLjZ/figures/figures_8_1.jpg", "caption": "Figure 5: The distributions of confidence scores given by the instructor model during the training process.", "description": "This figure shows the distributions of confidence scores produced by the instructor model for both real and fake labels throughout the training process.  The x-axis represents the confidence scores, ranging from 0.0 to 1.0, with 1.0 indicating high confidence. The y-axis shows the probability density.  Separate distributions are shown for real labels (those assigned manually) and fake labels (those generated by the target model). The figure displays how the instructor model's ability to distinguish between real and fake labels improves over training iterations (0K to 10K). Initially, the distributions overlap significantly, indicating low confidence and poor discrimination.  Over time, the distributions separate, with real labels exhibiting higher confidence scores and the fake labels having lower scores, demonstrating improved reliability.", "section": "5.4 Discussion, Ablation, and Other Applications"}, {"figure_path": "j7sw0nXLjZ/figures/figures_8_2.jpg", "caption": "Figure 4: The influence of unlabeled data size on four tasks.", "description": "This figure displays the impact of varying the size of the unlabeled dataset on the performance of the model across four different molecular property prediction tasks (BBBP, BACE, ClinTox, and Tox21).  Each line represents a different task, showcasing the AUC ROC score achieved as the number of unlabeled data points increases. The graph illustrates that augmenting the model with more unlabeled data consistently improves its predictive performance on all tasks, especially for the classification tasks. The increase is more prominent at lower data sizes and flattens out as the quantity of unlabeled data becomes very large.", "section": "5.4 Discussion, Ablation, and Other Applications"}, {"figure_path": "j7sw0nXLjZ/figures/figures_9_1.jpg", "caption": "Figure 6: The distribution of the predictions of labeled data and unlabeled data generated by the target model during different training stages.", "description": "This figure shows the distribution of predictions from the target model for both labeled and unlabeled data at various training stages.  The distributions are plotted as histograms to visualize the model's uncertainty and how it changes as the training progresses. This helps to understand how the model learns to differentiate between real and pseudo-labels over time.", "section": "5.4 Discussion, Ablation, and Other Applications"}, {"figure_path": "j7sw0nXLjZ/figures/figures_18_1.jpg", "caption": "Figure 7: The K\u2081 value prediction results of the 9 newly discovered small molecules of 5-HT1A receptor.", "description": "This figure shows the predicted and actual Ki values for nine newly discovered small molecules that target the 5-HT1A receptor.  The predictions were made using the InstructMol model.  Each molecule is depicted with its structure, SMILES notation, actual Ki value from experimental testing, and the Ki value predicted by InstructMol. The close correspondence between predicted and actual Ki values demonstrates the model's accuracy in predicting the properties of novel drug molecules.", "section": "5.4 Discussion, Ablation, and Other Applications"}]