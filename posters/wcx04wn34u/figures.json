[{"figure_path": "wcX04Wn34u/figures/figures_0_1.jpg", "caption": "Figure 1: LiT translates LiDAR scenes across domains, capturing target domains' characteristics. By unifying the LiDAR \u201clanguages\u201d, LiT enables effective zero-shot and multi-dataset joint learning.", "description": "The figure shows how LiT translates LiDAR data from one domain to another. The input is a LiDAR scan from the Waymo dataset (64 beams). LiT processes this scan using a scene modeling module and a LiDAR modeling module. The output is two translated LiDAR scans: one for the nuScenes dataset (32 beams) and another for the KITTI dataset (64 beams).  The figure visually demonstrates the ability of LiT to translate between different LiDAR \"languages\" and capture the unique characteristics of each target domain. This capability is crucial for enabling effective zero-shot and multi-dataset joint learning.", "section": "Abstract"}, {"figure_path": "wcX04Wn34u/figures/figures_3_1.jpg", "caption": "Figure 2: Domain gaps for LiDARs. Top Row: LiDAR ray angles have significantly different distributions. We model their statistical distributions as detailed in Sec. 4.2. Middle Row: Foreground vehicle sizes can differ across datasets. Bottom Row: We show the ideal ray casting results from LiDARs mounted at 1.6 meters height to a reconstructed vehicle placed 20 meters in front.", "description": "This figure shows the differences in LiDAR data characteristics across different datasets (Waymo, nuScenes, KITTI).  The top row illustrates the varying distributions of LiDAR ray angles. The middle row highlights the differences in vehicle size distributions. The bottom row visually depicts how these differences affect the ray casting results when reconstructing a vehicle from 20 meters away, showing variations in point cloud density and coverage.", "section": "Pilot study"}, {"figure_path": "wcX04Wn34u/figures/figures_3_2.jpg", "caption": "Figure 3: Comparing LiT with model-based adaptation. (a) Training a model on source domain data and directly applying it to the target domain typically results in poor performance due to the domain gap. (b) Model-based adaptation techniques [7] adapt the model to the target domain but do not explicitly model the target domain data LiDAR characteristics and data distribution. (c) LiT directly translates LiDAR data from the multiple source domains to a unified target domain, effectively bridging the domain gaps and enabling joint training across multiple datasets.", "description": "This figure compares three different approaches for LiDAR data adaptation: (a) No adaptation, where a model trained on source domain data is directly applied to the target domain, leading to poor performance due to the domain gap. (b) Model-based adaptation, where techniques like ST3D [7] adapt the model to the target domain but don't explicitly model its characteristics.  (c) LiDAR Translator (LiT), which translates LiDAR data from multiple source domains to a unified target domain, bridging the gap and enabling joint training.", "section": "3 Comparing LiT with model-based adaptation"}, {"figure_path": "wcX04Wn34u/figures/figures_4_1.jpg", "caption": "Figure 4: LiT pipeline overview. LiT translates LiDAR data across domains, integrating scene modeling (Sec. 4.1) and LiDAR modeling (Sec. 4.2) with GPU-accelerated ray casting. LiT is highly efficient, as it can translate a multi-frame LiDAR scene in typically less than one minute (Table 6).", "description": "This figure shows the pipeline of LiDAR translator (LiT). The pipeline consists of three main stages. First, scene modeling which uses multi-frame LiDAR scans to reconstruct the foreground and background of the scene. Second, LiDAR modeling which simulates the target domain's LiDAR sensor model to generate LiDAR scans based on the reconstructed scene from the previous stage. Finally, a GPU-accelerated ray casting engine is used to generate the translated LiDAR scans that are faithful to the target domain's LiDAR sensor's characteristics. The whole process is fast and takes less than a minute to translate a multi-frame LiDAR scene.", "section": "4 LiDAR translator"}, {"figure_path": "wcX04Wn34u/figures/figures_5_1.jpg", "caption": "Figure 5: LiDAR modeling with statistical ray angles and ray-drop. We compare the chamfer distance between the real-world LiDAR data and the simulated LiDAR data on nuScenes. The chamfer distance is smallest when both statistical modeling and ray-drop modeling are applied. The chamfer distances are clipped at 1.0m for visualization.", "description": "This figure compares the Chamfer distance between real and simulated LiDAR data on the nuScenes dataset.  The Chamfer distance measures the difference between point clouds. The results show that incorporating both statistical ray angle modeling and ray-drop modeling into LiDAR simulation significantly improves the accuracy of the simulated data, bringing it closer to the real-world data.", "section": "4.2 LiDAR modeling"}, {"figure_path": "wcX04Wn34u/figures/figures_5_2.jpg", "caption": "Figure 6: Ray-drop modeling visualization. The left image shows the translated nuScenes frame without ray-drop modeling, where it has a dense circular LiDAR pattern near the vehicle. The middle image, with ray-drop modeling applied, displays sparser LiDAR points near the vehicle, closely resembling nuScenes' scan patterns. The right image is a real nuScenes LiDAR scan from another scene.", "description": "This figure demonstrates the effect of LiDAR ray-drop modeling in LiT. The left panel shows a translated LiDAR scan without ray-drop modeling, exhibiting a dense circular pattern near the vehicle. The center panel displays the same scan but with ray-drop modeling applied; it shows sparser points near the vehicle, closely mimicking the patterns observed in real nuScenes scans. The right panel is a ground truth nuScenes scan from a different scene, providing a comparison to highlight the improved realism with ray-drop modeling.", "section": "LiDAR ray-drop modeling"}, {"figure_path": "wcX04Wn34u/figures/figures_6_1.jpg", "caption": "Figure 7: Deploying LiT-modeled LiDAR to a new scene. The LiT pipeline allows flexible composition of LiDARs and scenes, even when the scene is not modeled by LiT. We show visualizations of the synthetic Mai City scene [59] (column 1), and the LiT-simulated LiDAR scans in nuScenes patterns following a moving vehicle trajectory (column 2\u20134).", "description": "This figure demonstrates the flexibility of LiT in applying translated LiDAR data to new scenes, even those not explicitly modeled within LiT.  It showcases four columns; the first shows a synthetic Mai City scene, while the subsequent three columns illustrate LiT-simulated LiDAR scans (in nuScenes style) tracking a moving vehicle within the same scene.  This highlights LiT\u2019s capacity to adapt and integrate with diverse sources of scene reconstruction, highlighting its real-world applicability.", "section": "5 Experiment"}, {"figure_path": "wcX04Wn34u/figures/figures_14_1.jpg", "caption": "Figure 8: Foreground modeling samples. We show some examples of foreground modeling with LiT. The left columns show the original LiDAR point clouds collected from multiple LiDAR frames. The rightmost column shows the reconstructed mesh from the multi-view LiDAR inputs. The reconstructed mesh will then be used by LiT to perform target-domain LiDAR ray casting.", "description": "This figure visualizes the foreground modeling process of LiT.  It shows how LiT takes multiple LiDAR scans of a vehicle from different viewpoints and fuses them to create a complete 3D mesh of the vehicle. This mesh is then used to generate synthetic LiDAR scans in the target domain.  The figure displays three examples of this process, illustrating the input LiDAR point clouds, intermediate steps, and the final reconstructed mesh for each example.", "section": "A.1 Foreground modeling details"}, {"figure_path": "wcX04Wn34u/figures/figures_15_1.jpg", "caption": "Figure 9: Background modeling samples. We provide additional visualization samples of background modeling for Waymo and nuScenes.", "description": "This figure shows visualization samples of background modeling for Waymo and nuScenes datasets.  It visually demonstrates the reconstruction of the background scene using LiT's approach. The top and bottom rows present the reconstructed scenes from two different perspectives for both datasets, allowing for a comparison of the results. The reconstruction quality visually highlights the effectiveness of LiT in capturing background details.", "section": "A.2 Background modeling details"}, {"figure_path": "wcX04Wn34u/figures/figures_15_2.jpg", "caption": "Figure 10: Effects of LiDAR statistical modeling visualized with range image. We illustrate the effect of statistical modeling of LiDAR ray angles with a scene from nuScenes. The top row shows the 2D range image sampled without statistical modeling, and the bottom row shows the 2D range image sampled using statistical modeling. The bottom row shows fewer artifacts (e.g., the horizontal gap) as the sampled rays are more concentrated around the peak angles.", "description": "This figure compares 2D range images from the nuScenes dataset.  The top image shows the results without LiDAR statistical modeling applied, which shows artifacts like a horizontal gap. The bottom image shows the results with LiDAR statistical modeling applied. The modeling helps to correct artifacts by concentrating the sampled rays around the peak angles.", "section": "A.3 LiDAR statistical modeling visualization"}, {"figure_path": "wcX04Wn34u/figures/figures_16_1.jpg", "caption": "Figure 1: LiT translates LiDAR scenes across domains, capturing target domains' characteristics. By unifying the LiDAR \u201clanguages\u201d, LiT enables effective zero-shot and multi-dataset joint learning.", "description": "The figure shows four LiDAR point cloud frames. The top-left frame is the original LiDAR data from Waymo with 64 beams. The other three frames are the translated LiDAR data to nuScenes (32 beams) and KITTI (64 beams), showing how LiT translates LiDAR data between different domains while preserving the essential characteristics of the target domains.", "section": "Abstract"}]