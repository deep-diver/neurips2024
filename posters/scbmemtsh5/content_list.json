[{"type": "text", "text": "DisC-GS: Discontinuity-aware Gaussian Splatting ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Haoxuan Qu \u2217 Zhuoling Li \\* Hossein Rahmani Lancaster University Lancaster University Lancaster University U.K. U.K. U.K. h.qu5@lancaster.ac.uk z.li81@lancaster.ac.uk h.rahmani@lancaster.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Yujun Cai Jun Liu \u2020 University of Queensland Lancaster University Australia U.K. vanora.caiyj@gmail.com j.liu81@lancaster.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, Gaussian Splatting, a method that represents a 3D scene as a collection of Gaussian distributions, has gained significant attention in addressing the task of novel view synthesis. In this paper, we highlight a fundamental limitation of Gaussian Splatting: its inability to accurately render discontinuities and boundaries in images due to the continuous nature of Gaussian distributions. To address this issue, we propose a novel framework enabling Gaussian Splatting to perform discontinuity-aware image rendering. Additionally, we introduce a B\u00e9zier-boundary gradient approximation strategy within our framework to keep the \u201cdifferentiability\u201d of the proposed discontinuity-aware rendering process. Extensive experiments demonstrate the efficacy of our framework. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Novel view synthesis aims to generate images accurately from novel viewpoints in a captured 3D scene. Its significance spans across diverse applications, such as autonomous driving [45], virtual reality [14], and 3D content generation [42]. Recently, for better tackling novel view synthesis, Neural Radiance Field (NeRF) [36] and a variety of NeRF-based methods [3, 4] have been proposed, which represent 3D scenes in an implicit manner as neural radiance fields. However, their general reliance on a heavy volume rendering mechanism often results in slow rendering speeds [30, 13], limiting their practicality across real-world applications. While some methods [18, 19] have proposed to accelerate the rendering process of NeRF from different perspectives, they often achieve this at the expense of noticeably compromising the quality of the generated images [49], which is evidently undesirable. ", "page_idx": 0}, {"type": "text", "text": "More recently, Gaussian Splatting [30], which explicitly represents the 3D scene as a collection of Gaussian distributions, has been proposed as an appealing alternative to NeRF. Specifically, rather than generating novel-view images through the time-consuming process of volume rendering, Gaussian Splatting enables images from novel viewpoints to be generated by simply splatting (projecting) [53, 54] these Gaussian distributions onto the image plane. By doing so, Gaussian Splatting achieves real-time rendering of novel-view images, while maintaining its rendered images to be of competitively high visual quality compared to NeRF-rendered ones. Due to its compelling capability, Gaussian Splatting has received lots of research attention [26, 13, 42, 49, 12, 21, 25]. ", "page_idx": 0}, {"type": "image", "img_path": "ScbmEmtsH5/tmp/c65a37f1a39a8d13a901dd5adce7bf58cb4505dc560a22b035e5ed787eafc37e.jpg", "img_caption": ["Figure 1: (a) Illustration of a ground truth image, containing numerous discontinuities and boundaries, that is expected to be rendered from a certain viewpoint of a 3D scene. We generate the boundary map in (a) utilizing the Canny algorithm [9]. (b) Illustration of Gaussian distributions projected onto the image plane. As shown, since Gaussian distributions are continuous, they can inevitably \u201cpass over\u201d the (hard) boundary represented by the curve. (c) Illustration of images rendered with and without applying DisC-GS. As shown, without DisC-GS, Gaussian Splatting can fail to accurately render boundaries. In contrast, applying DisC-GS ensures that boundaries and discontinuities in the image are properly rendered. More qualitative results are in Appendix B. (Best viewed in color.) "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "However, in this paper, we argue that Gaussian Splatting may still be sub-optimal in accurately synthesizing novel views, due to its inherent weakness in representing (rendering) discontinuities and boundaries with its collection of continuous Gaussian distributions. Specifically, due to the general complexity of 3D scenes, the expected image to be rendered often contains numerous discontinuities and boundaries (as shown in Fig. 1(a)). However, Gaussian Splatting represents each of its generated images using only continuous Gaussians projected onto the image plane. Considering this, as illustrated in Fig. 1(b), the inherent continuity of Gaussian distributions can result in some parts of the distribution inevitably \u201cpassing over\u201d(\u201cspilling over\u201d) the boundaries of sharp features in the image. This can lead to Gaussian Splatting rendering the sharp boundaries in the image with blurriness (as shown in Fig. 1(c)), which can significantly reduce the quality of the rendered image. ", "page_idx": 1}, {"type": "text", "text": "Based on the above argument, in this paper, we aim to enable Gaussian Splatting to bypass its original intrinsic weakness, and render discontinuities and boundaries properly. However, this can be non-trivial owing to the following challenges: (1) Since 3D scenes generally can be complex, as illustrated in Fig. 1(a), various different kinds of boundaries with diverse shapes can all exist in the image rendered from a certain 3D scene. Thus, it can be difficult to represent and render these diverse boundaries properly and seamlessly in a Gaussian-Splatting-based framework. (2) Meanwhile, recall that continuity serves as the prerequisite for a function to be differentiable. Thus, during the process of learning the 3D scene representation using Gaussian Splatting, how to maintain the \u201cdifferentiability\u201d of the process in existence of discontinuities, i.e., enabling the loss calculated over the rendered images that contain \u201cdiscontinuous\u201d (sharp) boundaries to properly guide the learning of the 3D scene representation, is also challenging. To handle the above challenges, in this work, we propose DisContinuity-aware Gaussian Splatting (DisC-GS), a novel framework that can for the first time, enable Gaussian Splatting to represent and render discontinuities properly in its image rendering process, which handles a key limitation of the original Gaussian Splatting technique. We illustrate the rendering process of our framework in Fig. 2, and outline our framework as follows. ", "page_idx": 1}, {"type": "text", "text": "Overall, to enable Gaussian Splatting to properly render discontinuities and boundaries, our framework introduces a \u201cpre-scissoring\u201d step. Specifically, for each Gaussian distribution representing the 3D scene, rather than directly rendering its entire 2D projection on the image plane, we first segment (\u201cpre-scissor\u201d) the projected Gaussian distribution along the specified boundaries. However, achieving this requires representing boundaries with various shapes accurately. Here, we get inspiration from that, the cubic B\u00e9zier curve, conveniently represented by a group of four control points, has shown capable of efficiently parameterizing curves of various shapes with low computational complexity [16, 34]. Considering this, in our framework, we aim to use the cubic B\u00e9zier curves to represent boundaries. Specifically, we first introduce each Gaussian distribution representing the 3D scene with an additional attribute, which when projected onto the image plane, can serve as the control points of the cubic B\u00e9zier curves. After that, given a viewpoint, based on the control points projected onto the image plane corresponding to the viewpoint, we use the cubic B\u00e9zier curves formulated from these control points to represent the desired boundaries w.r.t. each projected Gaussian distribution. Finally, leveraging the derived boundaries, we can achieve discontinuity-aware image rendering through a modified $\\alpha$ -blending function (as discussed in Sec. 4.1). ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Through the above process, we can render discontinuities and boundaries successfully (in the forward direction). However, the above process alone cannot be seamlessly integrated into the Gaussian Splatting pipeline. This is because, owing to the incorporation of the boundary information, the modified $\\alpha$ -blending function now is no longer continuous everywhere. This can cause Gaussian Splatting, naively integrated with the above process, to become non-differentiable, and thus results in difficulties during the learning process of the 3D scene representation. To tackle this problem, in our framework, we further introduce a B\u00e9zier-boundary gradient approximation strategy, by which during backpropagation, we can enable gradients to properly pass through the modified $\\alpha$ -blending function, and thus keep our framework to be still \u201cdifferentiable\u201d. With the above designs properly involved, our DisC-GS framework can finally enable Gaussian Splatting to render discontinuities and boundaries properly, seamlessly addressing its original key intrinsic limitation. ", "page_idx": 2}, {"type": "text", "text": "The contributions of our work are summarized as follows. 1) We proposed DisC-GS, an innovative framework for the novel view synthesis task. To the best of our knowledge, this is the first effort that enables Gaussian Splatting to represent and render boundaries and discontinuities properly in its image rendering pipeline, which tackles a key intrinsic limitation of Gaussian Splatting. 2) We introduce several designs in our framework to enable it to render images in a discontinuity-aware manner, while also to keep its \u201cdifferentiability\u201d in the presence of discontinuities. 3) DisC-GS achieves superior performance on the evaluated benchmarks. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Novel View Synthesis. Owing to the wide range of applications, the task of novel view synthesis has received lots of research attention [23, 40, 39, 43, 24, 36, 3, 4, 5, 44, 50, 7, 11, 18, 19, 37, 30, 26, 13, 49, 20, 46, 22, 35, 33, 32, 48, 52, 17]. In the early days, with the emergence of CNN, different works have been proposed to leverage CNN in this task from different perspectives. Among them, Hedman et al. [23] proposed to use CNN to predict blending weights, and Sitzmann et al. [40] proposed to seek help from CNN in performing volumetric ray-marching. As time passed, NeRF tends to become a popular way in representing 3D scenes. Specifically, the original version of NeRF is first proposed by Mildenhall et al. in [36] and after it comes out, a variety of different NeRF-based methods have been further proposed, such as Mip-NeRF [3], $\\mathrm{NeRF+}$ [50], and Point-NeRF [44]. Despite the increased efforts, a weakness of NeRF-based methods can be that, to render novel-view images in high visual quality, they often still require a slow rendering process [30, 13]. This can negatively affect the usage of these methods in many real-world scenarios. ", "page_idx": 2}, {"type": "text", "text": "Considering this, more recently, the Gaussian Splatting technique, which can render novel-view images in good quality while at the same time in real-time speed, as an attractive alternative to NeRF, has gained plenty of research attention. Specifically, Kerbl et al. [30] proposed to represent a 3D scene as a collection of 3D Gaussian distributions and made the first attempt to perform novel view synthesis using the Gaussian Splatting technique. After that, Huang et al. [26] pointed out that representing the 3D scene utilizing 3D Gaussian distributions can lead to a viewpoint inconsistency problem. To tackle this problem, they proposed to represent the 3D scene with 2D Gaussian distributions instead. Moreover, Cheng et al. [13] proposed to seek help from the classical patch matching technique to better guide the densification of Gaussian distributions, and Zhang et al. [49] formulated a new loss function in the frequency space to better regularize the learning process of Gaussian Splatting. ", "page_idx": 2}, {"type": "text", "text": "Different from these existing Gaussian-Splatting-based methods that typically render complete Gaussian distributions during the image rendering process, we here argue that a key limitation of the original Gaussian Splatting technique lies in that, directly rendering the complete Gaussian distributions can lead boundaries and discontinuities in the image to be inaccurately rendered. Considering this, in this work, we propose to enable Gaussian distributions to be \u201cpre-scissored\u201d along desired boundaries before rendered. This for the first time, enables Gaussian Splatting to represent and render discontinuities and boundaries properly. ", "page_idx": 2}, {"type": "text", "text": "Curve Representation. The idea of representing a curve in a parametric way has been studied in various tasks [34, 27, 38, 16, 28, 8], such as lane detection [16], trajectory prediction [27], and text spotting [34]. Here in this work, we design a novel framework, which enables Gaussian Splatting to perform discontinuity-aware novel-view image rendering, via utilizing the cubic B\u00e9zier curves to parametrically contour the boundaries in the image plane. ", "page_idx": 3}, {"type": "text", "text": "3 Preliminary ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Gaussian Splatting. Gaussian Splatting represents the 3D scene explicitly as a collection of anisotropic Gaussian distributions. In specific, in the collection, each Gaussian is defined with the following attributes: (1) its center $\\boldsymbol{\\mu}\\in\\mathbb{R}^{3}$ , (2) its covariance matrix $\\Sigma\\in\\mathbb{R}^{3\\times3}$ , (3) its spherical harmonic (SH) coefficients $c_{S H}\\in\\mathbb{R}^{3\\times(k+1)^{2}}$ representing its color from different viewpoints (where $k$ denotes the order of SH), and (4) its opacity $\\alpha\\in\\mathbb{R}^{1}$ . Regarding the covariance matrix $\\Sigma$ , it is important to ensure $\\Sigma$ remains positive semi-definite during the learning process of the 3D scene representation. To achieve this, $\\Sigma$ is expressed as $\\Sigma=R S S^{T}\\check{R}^{T}$ , where $\\bar{R}\\in\\mathbb{R}^{3\\times3}$ is the orthogonal rotation matrix of the Gaussian, and $\\dot{S}\\in\\mathbb{R}^{3\\times3}$ denotes the diagonal scale matrix of the Gaussian. ", "page_idx": 3}, {"type": "text", "text": "With the 3D scene represented as the collection of Gaussians defined in the above way, to render an image given a target viewpoint, inspired by [53], each Gaussian in the collection is first projected onto the image plane corresponding to the viewpoint as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mu^{2D}=P W\\mu,\\;\\Sigma^{2D}=J W\\Sigma W^{T}J^{T}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mu^{2D}$ and $\\Sigma^{2D}$ respectively represent the center and the covariance matrix of the projected Gaussian distribution, $W$ represents the viewing transformation matrix, $P$ represents the projective transformation matrix, and $J$ represents the Jacobian of the affine approximation of the projective transformation. After that, to perform image rendering on the image plane, for each pixel $p$ of the image, its color $C(p)$ is derived through an $\\alpha$ -blending function as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nC(p)=\\sum_{i=1}^{N}c_{i}\\beta_{i}\\prod_{j=1}^{i-1}(1-\\beta_{j}),\\mathrm{\\bf~where~}\\beta_{i}=\\alpha_{i}e^{-\\frac{1}{2}(p-\\mu_{i}^{2D})^{T}(\\Sigma_{i}^{2D})^{-1}(p-\\mu_{i}^{2D}))}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $N$ represents the number of projected Gaussians that overlap $p,\\,c_{i}$ represents the color of the $i$ -th Gaussian calculated from its corresponding SH coefficients, $\\alpha_{i}$ represents the opacity of the i-th Gaussian, \u00b5i2D represents the center of the $i$ -th projected Gaussian, and $\\Sigma_{i}^{2D}$ represents the covariance matrix of the $i$ -th projected Gaussian. Note that, no matter whether Gaussian Splatting represents the 3D scene using 3D or 2D Gaussian distributions, the above equations can describe its rendering process consistently. In fact, as also mentioned in [26], the difference between rendering images from 3D or 2D Gaussians can be reduced to that, when the scene is represented through 2D Gaussians, the scale matrix $S$ of each of the 2D Gaussians should contain a zero column vector. In this work, we apply our framework to both 2D and 3D Gaussian Splattings, achieving performance improvements as shown in Tab. 2. Yet, as pointed out by [26], using 3D Gaussians instead of 2D Gaussians to represent the scene can result in a viewpoint inconsistency problem. Thus, in Sec. 4, we first focus on explaining how our framework is applied to 2D Gaussian Splatting, in which we fix the last column of the scale matrix $S$ of all the Gaussians to be a zero vector. We then discuss the application of our framework on 3D Gaussian Splatting in Sec. 4.3. ", "page_idx": 3}, {"type": "text", "text": "Cubic B\u00e9zier curve. A cubic B\u00e9zier curve is a parametric curve that can be formulated by leveraging a list of four ordered control points $[\\omega_{0},\\omega_{1},\\omega_{2},\\omega_{3}]$ as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nB(t)=(1-t)^{3}\\omega_{0}+3(1-t)^{2}t\\omega_{1}+3(1-t)t^{2}\\omega_{2}+t^{3}\\omega_{3}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In the above equation, we can set $t\\in[0,1]$ for $B(t)$ to represent a segment of the curve that starts from $\\omega_{0}$ and ends at $\\omega_{3}$ . Alternatively, we can set $t\\in\\mathbb R$ to represent the entire curve. In this work, we set $t\\,\\in\\,\\mathbb{R}$ for $B(t)$ , as any segment of the curve may not be enough to represent the desired boundaries in the whole image plane. Note that when the four control points lie on the same straight line, the B\u00e9zier curve formulated by them would also be reduced to that straight line. Thus, besides representing smooth boundaries, the cubic B\u00e9zier curves, at their cross-interacting points, can also be used to represent the sharp corners (of human-made items) in the rendered image. ", "page_idx": 3}, {"type": "image", "img_path": "ScbmEmtsH5/tmp/e005a6ef9262db5ee5f26afde75d6d9311d166ae5923a936aeb883036fc6f466.jpg", "img_caption": ["Figure 2: Illustration of the discontinuity-aware rendering process over a single Gaussian distribution. Specifically, over each 2D Gaussian distribution representing the 3D scene, we first introduce it with a new attribute $c_{c u r v e}\\in\\mathbb{R}^{4M\\times2}$ (represented by the red and purple points in (a)). Here we set $M=2$ . After that, given a viewpoint, as shown in (b), we project both the Gaussian distribution and the points stored in $c_{c u r v e}$ onto the image plane corresponding to the viewpoint. Finally, leveraging the modified $\\alpha$ -blending function in Eq. 6, we can perform discontinuity-aware rendering and render only the part of the Gaussian distribution masked with the dotted lines in (c). (Best viewed in color.) "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4 Proposed Method: DisC-GS ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given a batch of source images of a 3D scene with their corresponding viewpoints, the goal of novel view synthesis is to generate novel-view images accurately. To handle this task, a common way is to first learn a 3D scene representation from the given source images. After that, the novel-view images can be rendered from the learned 3D scene. Recently, via representing the 3D scene through Gaussian distributions, Gaussian Splatting has enabled novel-view images to be generated both in real-time and with high rendering quality. It has thus attracted lots of research attention [30, 26, 13, 49]. ", "page_idx": 4}, {"type": "text", "text": "Yet, we here argue that Gaussian Splatting has a key intrinsic limitation: it may fail to render discontinuities and boundaries accurately. To tackle this problem, in this work, inspired by [54, 28], we propose a novel framework named DisC-GS, which can seamlessly equip Gaussian Spatting with the discontinuity rendering ability. Specifically, during rendering images from the 3D scene, to render discontinuities properly, DisC-GS enables each Gaussian distribution projected onto the image plane to be first \u201cpre-scissored\u201d along certain desired boundaries before being rendered. However, such a \u201cpre-scissoring\u201d operation by itself can break the differentiability of the framework. Considering this, we further incorporate our framework with a B\u00e9zier-boundary gradient approximation strategy. Leveraging this strategy, during the learning process of the 3D scene, we can enable the gradient to properly backpropagate through the \u201cpre-scissoring\u201d operation. Below, we first describe the (forward) image rendering process of DisC-GS, and then explain the B\u00e9zier-boundary gradient approximation strategy. ", "page_idx": 4}, {"type": "text", "text": "4.1 Discontinuity-aware Image Rendering ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the proposed DisC-GS, to perform discontinuity-aware rendering, we aim to preprocess Gaussian distributions projected onto the image plane by \u201cscissoring\u201d them along boundaries represented by cubic B\u00e9zier curves before rendering. To achieve this, we modify the conventional Gaussian Splatting technique through the following three steps. ", "page_idx": 4}, {"type": "text", "text": "Introduction of an additional attribute. To facilitate the representation of cubic B\u00e9zier curves corresponding to the boundaries of each Gaussian distribution projected on the image plane, we introduce an additional attribute. We denote this attribute $c_{c u r v e}\\in\\mathbf{\\dot{R}}^{4\\breve{M}\\times2}$ , where $M$ is a user-defined hyperparameter representing the number of B\u00e9zier curves. This attribute augments the original four attributes (discussed in Sec. 3) of each Gaussian distribution. Below, we introduce the physical interpretation of $c_{c u r v e}$ . Specifically, for a certain 2D Gaussian distribution representing the 3D scene, denote the first column of its rotation matrix $R$ to be $r_{1}$ and the second column of $R$ to be $r_{2}$ . Over the 3D space, the 2D subspace that this Gaussian distribution lies in can be then described by a 2D coordinate system, which takes the center $\\mu$ of the Gaussian as its origin, the direction of $r_{1}$ as the direction of its $\\mathbf{X}$ -axis, and the direction of $r_{2}$ as the direction of its y-axis. Then for $c_{c u r v e}$ of this Gaussian distribution, it can be understood as storing a total of $4M$ points in the above-defined coordinate system. Note that when these $4M$ points are projected onto the image plane (as discussed below), they can then serve as the control points of $M$ cubic B\u00e9zier curves, which represent the desired boundary w.r.t. the current Gaussian distribution. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Image plane projection of points in $c_{c u r v e}$ . After introducing $c_{c u r v e}$ to each Gaussian distribution that represents the 3D scene, given a viewpoint, we project points in $c_{c u r v e}$ onto the image plane. Specifically, this is achieved in two steps: (1) We first transform each point (stored in $c_{c u r v e}$ ) in the above-defined subspace coordinate system to the coordinate system of the 3D space as: ", "page_idx": 5}, {"type": "equation", "text": "$c_{c u r v e}^{3D}[i]=\\mu+c_{c u r v e}[i,0]\\times r_{1}^{T}+c_{c u r v e}[i,1]\\times r_{2}^{T}$ ", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mu$ is the center of the Gaussian distribution. Note that here, since a column of a rotation matrix is already a unit vector, we can omit the normalization of $r_{1}$ and $r_{2}$ and directly transpose them. (2) After deriving cc3uDrve $c_{c u r v e}^{3D}\\in\\mathbb{R}^{4M\\times3}$ storing the $4M$ points in the 3D space coordinate system, we can project each point in $c_{c u r v e}^{3D}$ onto the image plane similar to what we have done in Eq. 1 as: ", "page_idx": 5}, {"type": "text", "text": "where $P$ represents the projective transformation matrix, and $W$ represents the viewing transformation matrix. At this point, for each Gaussian projected onto the image plane via Eq. 1, we have gotten the control points of its desired cubic-B\u00e9zier-curves-represented boundary, stored in $c_{c u r v e}^{2D}\\in\\mathbf{\\overline{{R}}}^{4M\\times2}$ . ", "page_idx": 5}, {"type": "text", "text": "Discontinuity-aware rendering. Finally, to perform discontinuity-aware image rendering, for each tGhae $M$ acnu bdiics trBib\u00e9zuitieor nc uprrvojees cftoerd mounltaot tehde  biamseadg eo pnl tahnee, $4M$ aciomn ttroo lf irpsoti \u201cntssc isstsoorre\u201dd t ihne $c_{c u r v e}^{2D}$ .u tAioftne ra ltohnatg, we would like to only render the remaining parts of the distribution that are not \u201cscissored out\u201d. To achieve this, assume that for each projected Gaussian distribution, we have built a binary indicator function $g(\\cdot)$ , which when passed with a pixel $p$ on the image plane, can output 0 if the pixel is in the \u201cscissored out\u201d area of the distribution, and can output 1 otherwise. We can then perform discontinuity-aware rendering simply via modifying the $\\alpha$ -blending function in Eq. 2 as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nC(p)=\\sum_{i=1}^{N}c_{i}\\beta_{i}\\prod_{j=1}^{i-1}(1-\\beta_{j}),\\mathrm{\\}\\mathbf{where}\\,\\beta_{i}=\\alpha_{i}g_{i}(p)e^{-\\frac{1}{2}(p-\\mu_{i}^{2D})^{T}(\\Sigma_{i}^{2D})^{-1}(p-\\mu_{i}^{2D}))}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $g_{i}(\\cdot)$ represents the indicator function w.r.t. the $i$ -th projected Gaussian. Besides, same as in Eq. 2, $N$ represents the number of projected Gaussians that overlap $p,\\,c_{i}$ represents the color of the $i$ -th Gaussian calculated from its corresponding SH coefficients, $\\alpha_{i}$ represents the opacity of the $i^{\\th}$ -th Gaussian, $\\mu_{i}^{2D}$ represents the center of the $i$ -th projected Gaussian, and $\\Sigma_{i}^{2D}$ represents the covariance matrix of the $i$ -th projected Gaussian. Note that via the above modified $\\alpha$ -blending function, for Gaussians that no longer overlap with the pixel $p$ due to the \u201cscissoring\u201d operation, we can zero out their contributions during calculating the color of $p$ . ", "page_idx": 5}, {"type": "text", "text": "Considering the above, the problem of enabling Gaussian Splatting to perform discontinuity-aware image rendering has now been reduced to building the indicator function $g(\\cdot)$ for each projected Gaussian distribution based on its corresponding cc2uDrve. Below, we discuss how we build $g(\\cdot)$ . For simplicity, we first consider the case where only one cubic B\u00e9zier curve exists per Gaussian distribution. In this case, denote the four control points of the curve $\\omega_{0}=(x_{0},y_{0})$ , $\\dot{\\omega_{1}}=(x_{1},y_{1})$ , $\\omega_{2}=(x_{2},y_{2})$ , and $\\omega_{3}=\\left(x_{3},y_{3}\\right)$ . Then to build $\\operatorname{\\dot{g}}(\\cdot)$ , given a pixel $p=(x_{p},y_{p})$ , we just need to determine (judge) whether $p$ is on the inner side or the outer side of the curve. To achieve this, instead of directly leveraging the parametric representation of the cubic B\u00e9zier curve presented in Eq. 3, which may lead the judgment to be non-intuitive, we first leverage the implicitization technique in algebra [1] to represent the cubic B\u00e9zier curve in its implicit representation form as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n3_{i m p}(x,y)=\\gamma_{x x x}x^{3}+\\gamma_{x x y}x^{2}y+\\gamma_{x y y}x y^{2}+\\gamma_{y y y}y^{3}+\\gamma_{x x}x^{2}+\\gamma_{x y}x y+\\gamma_{y y}y^{2}+\\gamma_{x x}+\\gamma_{y}y+\\gamma_{0}=0\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where coefficients including $\\gamma_{x x x}$ , $\\gamma_{x x y}$ , \u03b3xyy, \u03b3yyy, $\\gamma_{x x},\\gamma_{x y},\\gamma_{y y},\\gamma_{x},\\gamma_{y},$ and $\\gamma_{0}$ can all be obtained through basic arithmetic operations over the coordinates of the four control points of the curve in $O(1)$ time complexity (more details are provided in Appendix C). Based on $B_{i m p}(x,y)$ , in the case where only one curve exists per Gaussian distribution, we can then build the single-curve indicator function $g_{s c}(\\cdot)$ intuitively and with $O(1)$ time complexity as: ", "page_idx": 5}, {"type": "equation", "text": "$$\ng_{s c}(\\omega_{0},\\omega_{1},\\omega_{2},\\omega_{3};p)=\\left\\{\\begin{array}{l l}{1,}&{\\mathrm{if~}B_{i m p}(x_{p},y_{p})>0,}\\\\ {0,}&{\\mathrm{otherwise}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Above we introduce how we can build the indicator function $g(\\cdot)$ as $g_{s c}(\\cdot)$ assuming that each projected Gaussian distribution is only \u201cscissored\u201d along one cubic B\u00e9zier curve. Here, in the case where $M$ curves exist per Gaussian, for each Gaussian, we notice that a pixel can be regarded as in ", "page_idx": 5}, {"type": "text", "text": "its \u201cscissored out\u201d area as long as the pixel is \u201cscissored out\u201d by at least one out of the $M$ curves corresponding to the Gaussian. With this in mind, leveraging the $g_{s c}(\\cdot)$ function above, we can then define $g(\\cdot)$ in cases where $M>1$ as: ", "page_idx": 6}, {"type": "equation", "text": "$$\ng(p)=\\prod_{i=0}^{M-1}g_{s c}(c_{c u r v e}^{2D}[4i],c_{c u r v e}^{2D}[4i+1],c_{c u r v e}^{2D}[4i+2],c_{c u r v e}^{2D}[4i+3];p)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Leveraging the indicator function $g(\\cdot)$ defined in Eq. 9, along with the modified $\\alpha$ -blending function in Eq. 6, we can then enable Gaussian Splatting to perform discontinuity-aware rendering. ", "page_idx": 6}, {"type": "text", "text": "4.2 B\u00e9zier-boundary Gradient Approximation Strategy ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Above we discussed, how, in our framework, we perform discontinuity-aware rendering in the forward direction from the 3D scene representation to the 2D rendered image. ", "page_idx": 6}, {"type": "text", "text": "Problems remain. Yet, this forward rendering process by itself cannot be seamlessly incorporated into the Gaussian Splatting pipeline. This is because of two reasons. Firstly, to enable the 3D scene representation to be properly learned from the source images of the 3D scene, Gaussian Splatting needs its rendering process to be (backward) differentiable. However, performing discontinuity-aware rendering leveraging the modified $\\alpha$ -blending function in Eq. 6, with the discontinuous function $g(\\cdot)$ in Eq. 9 incorporated, is no longer differentiable. Moreover, according to Eq. 8 and 9, $g(\\cdot)$ is actually a piecewise constant function. Thus, even in its differentiable segments, the gradients of $g(\\cdot)$ w.r.t. its inputs are always zero. In other words, even in segments of $g(\\cdot)$ where its gradients are computable, these consistently zero gradients would fail to guide the update of the function $g(\\cdot)$ \u2019s inputs stored in $c_{c u r v e}^{2D}$ , and consequently fail to guide the learning process of $c_{c u r v e}$ introduced in Sec. 4.1. ", "page_idx": 6}, {"type": "text", "text": "The big picture of our proposed strategy. To tackle the above problems and thus enable Gaussian Splatting to seamlessly render discontinuities, in our framework, we aim to further keep the \u201cdifferentiability\u201d of the whole discontinuity-aware rendering process. In other words, w.r.t. the discontinuous indicator function $g(\\cdot)$ that is newly incorporated into the rendering process, we aim to approximate its gradients (partial derivatives) over the control point coordinates stored in $c_{c u r v e}^{2D}$ , in a way that enables the approximated gradients to effectively guide the learning process of the 3D scene representation. To achieve this, inspired by [29], we propose a B\u00e9zier-boundary gradient approximation strategy. Below, to ease our explanation of the strategy, we focus on discussing how we approximate $\\frac{\\partial g}{\\partial c_{c u r v e}^{2D}[0,0]}$ , i.e., the partial derivative of the indicator function $g(\\cdot)$ over the $x$ coordinate cofo othrdei nfirastte sc ostnotrreold  pino efdo lilno $c_{c u r v e}^{2D}$ i. mNiloatre  ptrhoatc etshse  (amppolriec adteitoani los f atrhee  psrtoravtiedgeyd  tion  tAhep preenmdaiixn iDng). $c_{c u r v e}^{2D}$   \nSpecifically, to approximate $\\frac{\\partial g}{\\partial c_{c u r v e}^{2D}[0,0]}$ , based on the chain rule and according to Eq. 9, denoting $g_{s c}(c_{c u r v e}^{2D}[0],c_{c u r v e}^{2D}[$ 1], $c_{c u r v e}^{2D}$ [2], $c_{c u r v e}^{2D}$ [3]; $p)$ to be $g_{s c}^{0}(p)$ , we first have: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{\\partial g}{\\partial c_{c u r v e}^{2D}[0,0]}=\\frac{\\partial g}{\\partial g_{s c}^{0}(p)}\\times\\frac{\\partial g_{s c}^{0}(p)}{\\partial c_{c u r v e}^{2D}[0,0]}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then since $g(\\cdot)$ is clearly differentiable over $g_{s c}^{0}(\\cdot)$ based on its definition in Eq. 9, we can reduce our problem to approximate $\\frac{\\partial g_{\\scriptscriptstyle s c}^{0}(p)}{\\partial c_{c u r v e}^{2D}[0,0]}$ , which is achieved through the following two steps. ", "page_idx": 6}, {"type": "text", "text": "Determining if $g_{s c}^{0}(p)$ is desired to be modified. Specifically, to approximate $\\frac{\\partial g_{s c}^{0}(p)}{\\partial c_{c u r v e}^{2D}[0,0]}$ , we first would like to determine, if the function $g_{s c}^{0}(\\cdot)$ is desired to be modified at $p$ or not. This is because, if $g_{s c}^{0}(\\cdot)$ already outputs a satisfied value at $p$ , we don\u2019t need to change $c_{c u r v e}^{2D^{-}}[0,0]$ to correspondingly modify $g_{s c}^{0}(p)$ . In other words, in such a case, we can simply set $\\frac{\\partial g_{s c}^{0}(p)}{\\partial c_{c u r v e}^{2D}[0,0]}$ to be zero. ", "page_idx": 6}, {"type": "text", "text": "Denote the loss function used during the learning process to be $L$ . Leveraging both \u2202g\u2202s0cL(p) and the current value of $g_{s c}^{0}(p)$ as the conditions, below, we list the three situations in which $g_{s c}^{0}(p)$ doesn\u2019t need to be further modified: (1) The first situation happens when $\\begin{array}{r}{\\frac{\\partial L}{\\partial g_{s c}^{0}(p)}=0}\\end{array}$ , which indicates that $g_{s c}^{0}(\\cdot)$ given input $p$ is already in an optimal state. (2) Besides, the second situation happens when $\\frac{\\partial{\\cal{L}}}{\\partial g_{s c}^{0}(p)}>0$ and $g_{s c}^{0}(p)=0$ . Based on the gradient descent algorithm, this implies that, while we still hope the function $g_{s c}^{0}(\\cdot)$ to output a smaller value at $p$ , the function $g_{s c}^{0}(\\cdot)$ already outputs its smallest allowed value. (3) Following the opposite logic of situation (2), the third situation happens when $\\begin{array}{r}{\\frac{\\partial L}{\\partial g_{s c}^{0}(p)}<0}\\end{array}$ and $g_{s c}^{0}(p)=1$ . In this case, though we still want $g_{s c}^{0}(p)$ to be larger, $g_{s c}^{0}(\\cdot)$ at $p$ already outputs its largest allowed value. In the above three situations, we can directly set $\\begin{array}{r}{\\frac{\\partial g_{s c}^{0}(p)}{\\partial c_{c u r v e}^{2D}[0,0]}=0}\\end{array}$ and omit the approximation performed in the next step. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Approximating $\\frac{\\partial g_{s c}^{0}(p)}{\\partial c_{c u r v e}^{2D}[0,0]}$ . Besides the above three situations, in the rest cases, for the value of function $g_{s c}^{0}(\\cdot)$ at $p$ to be properly modified based on the modification of the value of $c_{c u r v e}^{2D}[0,0]$ we aim to properly approximate the partial derivative $\\frac{\\partial g_{s c}^{0}(p)}{\\partial c_{c u r v e}^{2D}[0,0]}$ . To achieve this, recall that as a binary indicator function, $g_{s c}^{0}(\\cdot)$ switches (modifies) its value at $p$ between 0 and 1 only when its corresponding cubic B\u00e9zier curve passes through $p$ . Considering this, below, we first identify: which value we should set (change) $c_{c u r v e}^{2D}[0,0]$ to be, so that the value switch of $g_{s c}^{0}(\\cdot)$ at $p$ can happen. ", "page_idx": 7}, {"type": "text", "text": "To achieve this identification in an intuitive and analytical way, denoting $p=(x_{p},y_{p})$ and the desired value of $c_{c u r v e}^{2D}[0,0]$ to be $\\phi$ , based on Eq. 3, we can first derive the following system of equations: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big[x_{p}=(1-t)^{3}\\phi+3(1-t)^{2}t(c_{c u r v e}^{2D}[1,0])+3(1-t)t^{2}(c_{c u r v e}^{2D}[2,0])+t^{3}(c_{c u r v e}^{2D}[3,0])}\\\\ &{\\big(y_{p}=(1-t)^{3}(c_{c u r v e}^{2D}[0,1])+3(1-t)^{2}t(c_{c u r v e}^{2D}[1,1])+3(1-t)t^{2}(c_{c u r v e}^{2D}[2,1])+t^{3}(c_{c u r v e}^{2D}[3,1])}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In this system of equations, since $x_{p},\\,y_{p}$ , and the coordinates in $c_{c u r v e}^{2D}$ all have known values, we initially regard the second equation in the system as a cubic equation w.r.t. , as is now the only unknown variable in this equation. After solving this cubic equation and with $t$ also known, we can then regard the first equation in the system as a cubic equation w.r.t. $\\phi$ and solve it. Finally, by solving the above two equations (both in just $O(1)$ time complexity), we obtain $S_{\\phi}$ as the set of all possible real number solutions for $\\phi$ . Based on the solutions\u2019 scenarios within $S_{\\phi}$ , we approximate $\\frac{\\partial g_{s c}^{0}(p)}{\\partial c_{c u r v e}^{2D}[0,0]}$ in three different ways below. ", "page_idx": 7}, {"type": "text", "text": "(1) The \u201cno side\u201d situation. The first situation happens when $S_{\\phi}=\\emptyset$ . In this case, we simply set \u2202cc2\u2202uDgrs0vce([p0),0] = 0. This is because, the empty nature of S\u03d5 implies that, there exists no proper realnumber value that we can change $c_{c u r v e}^{2D}[0,0]$ to be, such that $g_{s c}^{0}(p)$ can be desirably modified (i.e., either from 0 to 1 or from 1 to 0). We thus simply do not encourage $c_{c u r v e}^{2D}[0,0]$ to change. ", "page_idx": 7}, {"type": "text", "text": "(2) The \u201csingle side\u201d situation. The second situation occurs when all solutions in $S_{\\phi}$ lie on the same side of $c_{c u r v e}^{2D}[0,0]$ (i.e., all larger or all smaller than $c_{c u r v e}^{2D}[0,0])$ . In this situation, let $\\;{\\widetilde{\\phi}}$ denote the solution in $S_{\\phi}$ that is nearest to $c_{c u r v e}^{2D}[0,0]$ . Adjusting $c_{c u r v e}^{2D}[0,0]$ towards $\\widetilde\\phi$ then im plies the least-cost plan, facilitating the modification of $g_{s c}^{0}(p)$ in a desired manner. With this in mind, to encourage $c_{c u r v e}^{2D}[0,0]$ to approach $\\widetilde\\phi$ , inspired by previous studies [6, 10, 47, 29], we approximate $\\frac{\\partial g_{s c}^{0}(p)}{\\partial c_{c u r v e}^{2D}[0,0]}$ via performing linear interpolation between $c_{c u r v e}^{2D}[0,0]$ and $\\widetilde\\phi$ as: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{\\partial g_{s c}^{0}(p)}{\\partial c_{c u r v e}^{2D}[0,0]}=\\frac{\\widetilde{g_{s c}^{0}(p)}-g_{s c}^{0}(p)}{\\left(\\widetilde{\\phi}-(c_{c u r v e}^{2D}[0,0])\\right)+\\epsilon},\\mathrm{~where~}\\widetilde{g_{s c}^{0}(p)}=\\left\\{\\overset{!}{0},\\right.\\right.\\mathrm{~if~}g_{s c}^{0}(p)=0,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In the above equation, we set $\\epsilon=10^{-5}$ if $\\left(\\widetilde{\\phi}-\\left(c_{c u r v e}^{2D}[0,0]\\right)\\right)>0$ and we otherwise set $\\epsilon=-10^{-5}$ $\\epsilon$ here is a small number that is used to avoid the gradient exploding problem to happen when the distance between $\\;{\\widetilde{\\phi}}$ and $c_{c u r v e}^{2D}[0,0]$ is too short. ", "page_idx": 7}, {"type": "text", "text": "(3) The \u201cboth sides\u201d situation. The third situation happens when some solutions in $S_{\\phi}$ are on the left side of $\\overline{{c_{c u r v e}^{2D}[0,0]}}$ , while other solutions are on the right side of $c_{c u r v e}^{2D}[0,0]$ . In this situation, we can achieve the desired modification of $g_{s c}^{0}(p)$ via either moving $c_{c u r v e}^{2D}[0,0]$ to its left or right side. Thus, unlike the scenario described in the above situation (2) where we only consider $\\widetilde\\phi$ from a single side of $c_{c u r v e}^{2D}[0,0]$ , here, denoting $\\widetilde{\\phi_{1}}$ as the value that is nearest to $c_{c u r v e}^{2D}[0,0]$ from  its left side, and $\\widetilde{\\phi_{2}}$ as the value that is nearest to $c_{c u r v e}^{2D}[0,0]$ from its right side, we approximate \u2202cc2uDrsvce[0,0] as: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{\\partial g_{s c}^{0}(p)}{\\partial c_{c u r v e}^{2D}[0,0]}=\\frac{\\widehat{g_{s c}^{0}(p)}-g_{s c}^{0}(p)}{\\left(\\widehat{\\phi_{1}}-(c_{c u r v e}^{2D}[0,0])\\right)+\\epsilon_{1}}+\\frac{\\widehat{g_{s c}^{0}(p)}-g_{s c}^{0}(p)}{\\left(\\widehat{\\phi_{2}}-(c_{c u r v e}^{2D}[0,0])\\right)+\\epsilon_{2}}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In the above equation, we define $\\bar{g}_{s c}^{0}\\overline{{(p)}}$ in the same way as in Eq. 12. Besides, both $\\epsilon_{1}$ and $\\epsilon_{2}$ are defined in the similar way as $\\epsilon$ in Eq. 12. ", "page_idx": 8}, {"type": "text", "text": "Istnr astuegmy maaprpyr,o txaikminatge $\\frac{\\partial g}{\\partial c_{c u r v e}^{2D}[0,0]}$ nat so fa $g(\\cdot)$ awmitphl ree, stpheec ta tboo tvhee  dpiosicnut scsoioornd ienxaptleasi nstso rheodw i no $c_{c u r v e}^{2D}$ .p oWsietdh the incorporation of this strategy into our framework, we keep the \u201cdifferentiability\u201d of the whole rendering process, allowing Gaussian Splatting to seamlessly perform discontinuity-aware rendering. ", "page_idx": 8}, {"type": "text", "text": "4.3 DisC-GS on 3D Gaussian Splatting ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Above, we focus on describing how we use 2D B\u00e9zier curves in our DisC-GS framework and correspondingly apply DisC-GS on 2D Gaussian Splatting. Here in this subsection, we further describe how we use 3D B\u00e9zier curves in our DisC-GS framework and apply DisC-GS on 3D Gaussian Splatting. Specifically, the transition from 2D to 3D B\u00e9zier curves in DisC-GS requires only two minimal modifications. (1) Firstly, for each Gaussian representing the 3D scene, the control points of its B\u00e9zier curves are stored directly in the 3D spatial coordinate system rather than in a 2D subspace. Note that, this modification can be very simply made. Specifically, for each oGf stpo arceep rien soeuntr  tDhies cCo-nGtrSo lf rpaominet wcoorokr, diwnea toens loyf  nitese d3 tDo  Bu\u00e9szei $c_{c u r v e}^{3D}\\in\\mathbb{R}^{4M\\times3}$ ri nwsoterdasd, $c_{c u r v e}\\in\\mathbb{R}^{4M\\times2}$   \nfor each 3D Gaussian, we only need to introduce it with $c_{c u r v e}^{3D}$ instead of $c_{c u r v e}$ as its new attribute. (2) Moreover, since we already directly introduce $c_{c u r v e}^{3D}$ as the new attribute for each 3D Gaussian in our framework, during rendering, we omit Eq. 4 above in Sec. 4.1, which originally is used to acquire $c_{c u r v e}^{3D}$ from $c_{c u r v e}$ . Overall, the above two modifications are sufficient to incorporate DisC-GS with 3D instead of 2D B\u00e9zier curves. ", "page_idx": 8}, {"type": "text", "text": "4.4 Overall Training and Testing ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In DisC-GS, during training (i.e., learning the 3D scene representation from the source images), we follow a similar process as the typical Gaussian Splatting technique [30]. The involvement of the strategy introduced in Sec. 4.2 keeps the \u201cdifferentiability\u201d of our framework. During testing (i.e., image rendering), we use the discontinuity-aware image rendering process introduced in Sec. 4.1. ", "page_idx": 8}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Datasets. To evaluate the efficacy of our proposed framework DisC-GS, following previous Gaussian Splatting works [30, 49], we evaluate our framework on a total of 13 3D scenes, which include both outdoor scenes and indoor scenes. Specifically, among these 13 scenes, 9 of them are from the Mip-NeRF360 dataset [4], 2 of them are from the Tanks&Temples dataset [31], and 2 of them are from the Deep Blending dataset [23]. We also follow previous works [30, 49] in their train-test-split. ", "page_idx": 8}, {"type": "text", "text": "Evaluation metrics. Following [30, 49], we use the following three metrics for evaluation: Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS) [51]. ", "page_idx": 8}, {"type": "text", "text": "Implementation details. We conduct our experiments on an RTX 3090 GPU and develop our code mainly based on the GitHub repository [2] provided by Kerbl et al [30]. Moreover, we also get inspired by [35, 52, 46] during our code implementation, and make use of the LPIPS loss during our training process. Furthermore, for the newly introduced attribute $c_{c u r v e}\\in\\mathbb{R}^{4M\\times2}$ , we set its initial learning rate to 2e-4, and set the hyperparameter $M$ to 3. Besides, in the densification procedure of our framework, when a Gaussian is cloned/splitted into two new Gaussians, we assign both the new Gaussians with the same attribute $c_{c u r v e}$ as the original one. ", "page_idx": 8}, {"type": "text", "text": "5.1 Experimental Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Tab. 1, we compare our approach (applied on 2D Gaussian Splatting) with existing novel view synthesis methods evaluated on the same $13\\;3\\mathrm{D}$ scenes and report the PSNR, SSIM, and LPIPS results. Our framework consistently outperforms other methods on all three metrics and across various datasets, showing its effectiveness. We also show qualitative results in both Fig. 1(c) and Appendix B. As shown, whether representing the 3D scene through 3D or 2D Gaussian distributions, ", "page_idx": 8}, {"type": "table", "img_path": "ScbmEmtsH5/tmp/9cb5bce0938979be6e5e2fe2084c64ab8ac053384d1865ac9614d4ea7b0731a8.jpg", "table_caption": ["Table 1: Performance comparison on the Tanks&Temples, Mip-NeRF360, and Deep Blending datasets. "], "table_footnote": ["the conventional Gaussian Splatting technique often struggles to render boundaries and discontinuities clearly and with high quality. In contrast, our framework can achieve good rendering quality, even in regions of the image containing numerous boundaries and discontinuities. This further underscores the efficacy of our approach. "], "page_idx": 9}, {"type": "text", "text": "5.2 Ablation Studies ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We conduct extensive ablation experiments on the Tanks&Temples dataset. More ablation studies w.r.t. the image areas with rich boundaries, the B\u00e9zier-boundary gradient approximation strategy, the hyperparameters, and the rendering speed of our framework are in Appendix A. ", "page_idx": 9}, {"type": "text", "text": "Impact of representing the scene with 2D or 3D Gaussians in DisC-GS. In Sec. 4.1 and Sec. 3, we focus on introducing how we apply DisC-GS on 2D Gaussian Splatting. After that, in Sec. 4.3, we introduce how DisC-GS can be applied on 3D Gaussian Splatting in a similar way. Here to verify the generality of our frame", "page_idx": 9}, {"type": "table", "img_path": "ScbmEmtsH5/tmp/5ac29fcceb79375487255bd4716028aaa4ddf7aaa690515f3838b9b576c3e8ba.jpg", "table_caption": ["Table 2: Evaluation of our framework on both 2D and 3D Gaussian Splattings. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "work, we test applying our framework on both 2D and 3D Gaussian Splatting. As shown in Tab. 2, our framework, when applied on both 2D and 3D Gaussian Splattings, can consistently achieve performance improvements, demonstrating the generality of our framework. ", "page_idx": 9}, {"type": "text", "text": "Impact of the number of control points per B\u00e9zier curve. In our framework, inspired by [16, 34], we represent boundaries in the image with the cubic B\u00e9zier curve, each of which is formulated by leveraging 4 control points. Here we evaluate formulating each B\u00e9zier curve by other numbers of control points, and report the ", "page_idx": 9}, {"type": "table", "img_path": "ScbmEmtsH5/tmp/63af57b238996de5c486007f469bd702c9ab1e5277b293891bd38d18e1c8b2a4.jpg", "table_caption": ["Table 3: Evaluation on the number of control points per B\u00e9zier curve. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "results in Tab. 3. As shown, our framework gets optimal performance when the number of control points per B\u00e9zier curve is set to 4, and we thus formulate each B\u00e9zier curve by utilizing 4 control points in our experiments. Besides, with different choices of the number of control points per B\u00e9zier curve from 2 to 5, our framework outperforms the previous state-of-the-art method consistently. This shows the robustness of our framework to the number of control points per B\u00e9zier curve. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have proposed an innovative novel view synthesis framework DisC-GS, which for the first time, enables Gaussian Splatting to properly represent and render discontinuities and boundaries in its image rendering process. Moreover, to keep the \u201cdifferentiability\u201d of our framework, we further introduce our framework with a B\u00e9zier-boundary gradient approximation strategy. Our framework consistently achieves superior performance across different evaluation benchmarks. ", "page_idx": 9}, {"type": "text", "text": "Limitations. While our framework enables Gaussian Splatting to perform discontinuity-aware rendering, we acknowledge that same as existing Gaussian Splatting approaches, our framework still holds certain limitations, such as challenges in rendering large scenes. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] 2d graphics primitives. http://www.mare.ee/indrek/misc/2d.pdf.   \n[2] Diff-gaussian-rasterization. https://github.com/graphdeco-inria/ diff-gaussian-rasterization.   \n[3] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5855\u20135864, 2021.   \n[4] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mipnerf 360: Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5470\u20135479, 2022.   \n[5] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-based neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19697\u201319705, 2023.   \n[6] Albert S Berahas, Liyuan Cao, Krzysztof Choromanski, and Katya Scheinberg. Linear interpolation gives better gradients than gaussian smoothing in derivative-free optimization. arXiv preprint arXiv:1905.13043, 2019.   \n[7] Wenjing Bian, Zirui Wang, Kejie Li, Jia-Wang Bian, and Victor Adrian Prisacariu. Nopenerf: Optimising neural radiance field with no pose prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4160\u20134169, 2023.   \n[8] Yigit Baran Can, Alexander Liniger, Danda Pani Paudel, and Luc Van Gool. Structured bird\u2019seye-view traffic scene understanding from onboard images. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15661\u201315670, 2021.   \n[9] John Canny. A computational approach to edge detection. IEEE Transactions on pattern analysis and machine intelligence, (6):679\u2013698, 1986.   \n[10] Liyuan Cao, Zaiwen Wen, and Ya-xiang Yuan. Some sharp error bounds for multivariate linear interpolation and extrapolation. arXiv preprint arXiv:2209.12606, 2022.   \n[11] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In European Conference on Computer Vision, pages 333\u2013350. Springer, 2022.   \n[12] Hanlin Chen, Chen Li, and Gim Hee Lee. Neusg: Neural implicit surface reconstruction with 3d gaussian splatting guidance. arXiv preprint arXiv:2312.00846, 2023.   \n[13] Kai Cheng, Xiaoxiao Long, Kaizhi Yang, Yao Yao, Wei Yin, Yuexin Ma, Wenping Wang, and Xuejin Chen. Gaussianpro: 3d gaussian splatting with progressive propagation. arXiv preprint arXiv:2402.14650, 2024.   \n[14] Nianchen Deng, Zhenyi He, Jiannan Ye, Budmonde Duinkharjav, Praneeth Chakravarthula, Xubo Yang, and Qi Sun. Fov-nerf: Foveated neural radiance fields for virtual reality. IEEE Transactions on Visualization and Computer Graphics, 28(11):3854\u20133864, 2022.   \n[15] Wei Dong, Hanwei Sun, Ruixue Zhou, and Hongmeng Chen. Autofocus method for sar image with multi-blocks. The Journal of Engineering, 2019(19):5519\u20135523, 2019.   \n[16] Zhengyang Feng, Shaohua Guo, Xin Tan, Ke Xu, Min Wang, and Lizhuang Ma. Rethinking efficient lane detection via curve modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17062\u201317070, 2022.   \n[17] Lin Geng Foo, Hossein Rahmani, and Jun Liu. Ai-generated content (aigc) for various data modalities: A survey. arXiv preprint arXiv:2308.14177, 2:2, 2023.   \n[18] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5501\u20135510, 2022.   \n[19] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. Fastnerf: High-fidelity neural rendering at 200fps. In Proceedings of the IEEE/CVF international conference on computer vision, pages 14346\u201314355, 2021.   \n[20] Yuanhao Gong. Eggs: Edge guided gaussian splatting for radiance fields. arXiv preprint arXiv:2404.09105, 2024.   \n[21] Antoine Gu\u00e9don and Vincent Lepetit. Sugar: Surface-aligned gaussian splatting for efficient 3d mesh reconstruction and high-quality mesh rendering. arXiv preprint arXiv:2311.12775, 2023.   \n[22] Abdullah Hamdi, Luke Melas-Kyriazi, Guocheng Qian, Jinjie Mai, Ruoshi Liu, Carl Vondrick, Bernard Ghanem, and Andrea Vedaldi. Ges: Generalized exponential splatting for efficient radiance field rendering. arXiv preprint arXiv:2402.10128, 2024.   \n[23] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep blending for free-viewpoint image-based rendering. ACM Transactions on Graphics (ToG), 37(6):1\u201315, 2018.   \n[24] Philipp Henzler, Niloy J Mitra, and Tobias Ritschel. Escaping plato\u2019s cave: 3d shape from adversarial rendering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9984\u20139993, 2019.   \n[25] Xu Hu, Yuxi Wang, Lue Fan, Junsong Fan, Junran Peng, Zhen Lei, Qing Li, and Zhaoxiang Zhang. Semantic anything in 3d gaussians. arXiv preprint arXiv:2401.17857, 2024.   \n[26] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. arXiv preprint arXiv:2403.17888, 2024.   \n[27] Ronny Hug, Wolfgang H\u00fcbner, and Michael Arens. Introducing probabilistic b\u00e9zier curves for n-step sequence prediction. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 10162\u201310169, 2020.   \n[28] Rafael Ivo, Fabio Ganovelli, Creto Vidal, Joaquim Bento Cavalcante-Neto, and Roberto Scopigno. Adapting splat-based models to curved sharp features. Journal of Graphics Tools, 17(4):139\u2013150, 2013.   \n[29] Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neural 3d mesh renderer. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3907\u20133916, 2018.   \n[30] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4):1\u201314, 2023.   \n[31] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG), 36(4):1\u201313, 2017.   \n[32] Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, and Eunbyung Park. Compact 3d gaussian representation for radiance field. arXiv preprint arXiv:2311.13681, 2023.   \n[33] Zhihao Liang, Qi Zhang, Ying Feng, Ying Shan, and Kui Jia. Gs-ir: 3d gaussian splatting for inverse rendering. arXiv preprint arXiv:2311.16473, 2023.   \n[34] Yuliang Liu, Hao Chen, Chunhua Shen, Tong He, Lianwen Jin, and Liangwei Wang. Abcnet: Real-time scene text spotting with adaptive bezier-curve network. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9809\u20139818, 2020.   \n[35] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffoldgs: Structured 3d gaussians for view-adaptive rendering. arXiv preprint arXiv:2312.00109, 2023.   \n[36] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99\u2013106, 2021.   \n[37] Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4):1\u2013 15, 2022.   \n[38] Zhiyu Qu, Tao Xiang, and Yi-Zhe Song. Sketchdreamer: Interactive text-augmented creative sketch ideation. arXiv preprint arXiv:2308.14191, 2023.   \n[39] Gernot Riegler and Vladlen Koltun. Free view synthesis. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XIX 16, pages 623\u2013640. Springer, 2020.   \n[40] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nie\u00dfner, Gordon Wetzstein, and Michael Zollhofer. Deepvoxels: Learning persistent 3d feature embeddings. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2437\u20132446, 2019.   \n[41] Nagabhushan Somraj and Rajiv Soundararajan. Vip-nerf: Visibility prior for sparse input neural radiance fields. In ACM SIGGRAPH 2023 Conference Proceedings, pages 1\u201311, 2023.   \n[42] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653, 2023.   \n[43] Justus Thies, Michael Zollh\u00f6fer, and Matthias Nie\u00dfner. Deferred neural rendering: Image synthesis using neural textures. Acm Transactions on Graphics (TOG), 38(4):1\u201312, 2019.   \n[44] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-based neural radiance fields. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5438\u20135448, 2022.   \n[45] Ze Yang, Yun Chen, Jingkang Wang, Sivabalan Manivasagam, Wei-Chiu Ma, Anqi Joyce Yang, and Raquel Urtasun. Unisim: A neural closed-loop sensor simulator. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1389\u20131399, 2023.   \n[46] Ziyi Yang, Xinyu Gao, Yangtian Sun, Yihua Huang, Xiaoyang Lyu, Wen Zhou, Shaohui Jiao, Xiaojuan Qi, and Xiaogang Jin. Spec-gaussian: Anisotropic view-dependent appearance for 3d gaussian splatting. arXiv preprint arXiv:2402.15870, 2024.   \n[47] Wang Yifan, Felice Serena, Shihao Wu, Cengiz \u00d6ztireli, and Olga Sorkine-Hornung. Differentiable surface splatting for point-based geometry processing. ACM Transactions on Graphics (TOG), 38(6):1\u201314, 2019.   \n[48] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splatting. arXiv preprint arXiv:2311.16493, 2023.   \n[49] Jiahui Zhang, Fangneng Zhan, Muyu Xu, Shijian Lu, and Eric Xing. Fregs: 3d gaussian splatting with progressive frequency regularization. arXiv preprint arXiv:2403.06908, 2024.   \n[50] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing and improving neural radiance fields. arXiv preprint arXiv:2010.07492, 2020.   \n[51] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586\u2013595, 2018.   \n[52] Zheng Zhang, Wenbo Hu, Yixing Lao, Tong He, and Hengshuang Zhao. Pixel-gs: Density control with pixel-aware gradient for 3d gaussian splatting. arXiv preprint arXiv:2403.15530, 2024.   \n[53] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Ewa splatting. IEEE Transactions on Visualization and Computer Graphics, 8(3):223\u2013238, 2002.   \n[54] Matthias Zwicker, Jussi Rasanen, Mario Botsch, Carsten Dachsbacher, and Mark Pauly. Perspective accurate splatting. In Proceedings-Graphics Interface, pages 247\u2013254, 2004. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Additional Ablation Studies ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we conduct more ablation experiments on the Tanks&Temples dataset. ", "page_idx": 13}, {"type": "text", "text": "Evaluation especially over areas with rich boundaries in the testing images. In this work, we point out that Gaussian Splatting holds an ", "text_level": 1, "page_idx": 13}, {"type": "table", "img_path": "ScbmEmtsH5/tmp/b61c242b62b5d54f238e2f396910c170f7b6be12641fe1fb8a35b8fdb4df810e.jpg", "table_caption": ["Table 4: Evaluation especially over areas with rich boundaries in the testing images. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "inherent weakness in rendering discontinuities and boundaries, and we propose a DisC-GS framework for Gaussian Splatting to render boundaries in the image more accurately. Here, we aim to test our framework, particularly over image areas with rich boundaries. To achieve this, we evaluate our framework respectively over two parts of areas in each testing image. Specifically, for each testing image, we first pass the image over the Canny algorithm [9] followed by the dilation operation to highlight the areas with rich boundaries in the image (i.e., the areas that involve or surround the Canny-detected boundaries). After that, we let the first part (boundary-rich areas) include all the highlighted areas in each testing image, and let the second part (boundary-sparse areas) include all the rest areas in each testing image. To perform evaluation effectively over a part rather than the whole of each testing image, following [41], we use MaskedSSIM as the evaluation metric. As shown in Tab. 4, compared to 2D Gaussian Splatting as a baseline, especially in the boundary-rich areas, our framework can achieve a significant performance improvement. This demonstrates the effectiveness of our method especially over image areas with rich boundaries. ", "page_idx": 13}, {"type": "text", "text": "Impact of introducing different Gaussians with different numbers of B\u00e9zier curves. In our frame", "text_level": 1, "page_idx": 13}, {"type": "table", "img_path": "ScbmEmtsH5/tmp/a63667e70171848e5e784e3069793b93bb73eeb5021f5b63d02b0f6f19d35e62.jpg", "table_caption": ["Table 5: Evaluation on introducing different Gaussians with different numbers of B\u00e9zier curves. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "sian distribution representing the 3D scene, we introduce it with $M$ cubic B\u00e9zier curves ( $M$ curves for each Gaussian). Here, to investigate whether introducing different Gaussians with different numbers of curves can further benefti our framework, we test another variant (different numbers of curves for different Gaussians). Specifically, in this variant, before the start of the training process w.r.t. a certain 3D scene, for each of the source images, we first identify its boundary-rich areas similarly as in the above ablation study. After that, during training, when a Gaussian distribution is newly created through the adaptive control process of Gaussian Splatting, instead of directly introducing it with $M$ curves, we introduce the Gaussian with $M+1$ B\u00e9zier curves if the center $\\mu^{2\\check{D}}$ of its corresponding projected Gaussian lies in the highlighted boundary-rich areas. Otherwise, we introduce the newly created Gaussian with $M-1$ B\u00e9zier curves. As shown in Tab. 5, this variant does not result in better performance compared to our framework. This might be because, during training, for each Gaussian representing the 3D scene, its center is learnable, and the Gaussian is thus movable. In other words, for Gaussians that are initialized with more curves and thus may be able to represent boundaries more accurately, they can move to areas with fewer (or no) boundaries in the 3D scene. At the same time, for Gaussians that are initialized with fewer curves, they can also move to areas with rich boundaries in the 3D scene. With the above in mind, in our experiments, we introduce each Gaussian representing the 3D scene consistently with the same number of $M$ B\u00e9zier curves, equipping each Gaussian with the same level of power in boundary representation. ", "page_idx": 13}, {"type": "text", "text": "Image sharpness evaluation. In this work, we propose DisC-GS, which enables Gaussian Splatting to render the sharp boundaries in the image more accurately. Considering this, to perform further evaluation of our framework, we here also evaluate our framework from the im", "page_idx": 13}, {"type": "table", "img_path": "ScbmEmtsH5/tmp/4310d47b00b004f531e0af07cff21932d65254dc9312d1b427cdc0dd7b2e952a.jpg", "table_caption": ["Table 6: Evaluation on the sharpness of the rendered images. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "age sharpness perspective. Specifically, following [15], we measure image sharpness leveraging the energy gradient function. As shown in Tab. 6, with our framework applied, we can render images more sharply. This further shows the efficacy of our proposed framework. ", "page_idx": 13}, {"type": "text", "text": "Impact of the \u201cboth sides\u201d situation. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To keep the \u201cdifferentiability\u201d of our framework, in this work, we propose a B\u00e9zier-boundary gradient approximation strategy. Specifically, in this ", "page_idx": 14}, {"type": "table", "img_path": "ScbmEmtsH5/tmp/65abe4a6edbafb21612de6d921e1fa4386f5c9362e0ef83dfa43104e8044a335.jpg", "table_caption": ["Table 7: Evaluation on the \u201cboth sides\u201d situation. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "strategy, when some solutions in $S_{\\phi}$ lie on the left side of $c_{c u r v e}^{2D}[0,0]$ while other solutions lie on its right side, we approximate $\\frac{\\partial g_{\\scriptscriptstyle s c}^{0}(p)}{\\partial c_{c u r v e}^{2D}[0,0]}$ through Eq. 13 under the \u201cboth sides\u201d situation, considering both the left and right sides of $c_{c u r v e}^{2D}[0,0]$ (with the \u201cboth sides\u201d situation). To valid this design, we test a variant. In this variant (w/o the \u201cboth sides\u201d situation), even when solutions in $S_{\\phi}$ exist on both the left and right sides of $c_{c u r v e}^{2D}[0,0]$ , we still consider only the solution that is nearest to $c_{c u r v e}^{2D}[0,0]$ , and approximate $\\frac{\\partial g_{s c}^{0}(p)}{\\partial c_{c u r v e}^{2D}[0,0]}$ through Eq. 12 under the \u201csingle side\u201d situation. As shown in Tab. 7, our framework outperforms this variant. This shows the advantage of considering \u201cboth sides\u201d when solutions in $S_{\\phi}$ lie on both the left and right sides of $c_{c u r v e}^{2D}[0,0]$ . ", "page_idx": 14}, {"type": "text", "text": "Impact of the small numbers $\\epsilon$ , $\\epsilon_{1}$ , and $\\epsilon_{2}$ . In our framework, during gradient approximation, to avoid the gradient exploding problem to happen, we add $\\epsilon$ as a small number to the denominator part of Eq. 12, and we also add $\\epsilon_{1}$ and $\\epsilon_{2}$ to Eq. 13 in a similar way (with small numbers). To valid the efficacy of this design, we test a variant (w/o small numbers) in which we remove $\\epsilon$ , $\\epsilon_{1}$ , and $\\epsilon_{2}$ from our gradient approximation process. As shown in Tab. 8, our framework involving these small numbers performs better than this variant, showing the efficacy of these small numbers. ", "page_idx": 14}, {"type": "table", "img_path": "ScbmEmtsH5/tmp/6e6d222ef70475aa2c212ffbebbe8d12002c12ab96afb737e48578cb11bb6359.jpg", "table_caption": ["Table 8: Evaluation on the small numbers $\\epsilon$ , $\\epsilon_{1}$ , and $\\epsilon_{2}$ . "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Impact of the number of B\u00e9zier curves per Gaussian $M$ . In our framework, for each Gaussian distribution representing the 3D scene, we set the number of cubic B\u00e9zier curves $M$ associate with the Gaussian to 3. As shown in Tab. 9, our framework achieves optimal performance when $M$ is set to 3, and $M=3$ is used in our experiments. Moreover, with different choices of $M$ from 1 to 4, our framework consistently achieves good performance. This ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "table", "img_path": "ScbmEmtsH5/tmp/b0e7c31e3ef8629b86d54f7a7eabcf766d08c1cc162aad5728a32055e7d6872d.jpg", "table_caption": ["Table 9: Evaluation on the number of B\u00e9zier curves per Gaussian $M$ . "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "demonstrates the robustness of our proposed framework to this hyperparameter. ", "page_idx": 14}, {"type": "text", "text": "Impact of the initial learning rate set to $c_{c u r v e}$ . In our framework, we introduce a new attribute ccurve, for which in our experiments, we set its initial learning rate $(l r_{c u r v e})$ to 2e-4. Here we also assess the other choices of $l r_{c u r v e}$ from 1e-4 to 1e-3 and report the results in Tab. 10. As shown, with different choices of $l r_{c u r v e}$ , the performance of our framework is consistent, which shows the robustness of our framework to $l r_{c u r v e}$ . ", "page_idx": 14}, {"type": "table", "img_path": "ScbmEmtsH5/tmp/a8888bb2dbe348a9b3743af49e60b3518fd1c823799f6c004b2fa2c827b5bc8d.jpg", "table_caption": ["Table 10: Evaluation on the initial learning rate $(l r_{c u r v e})$ set to $c_{c u r v e}$ . "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Rendering time. In Tab. 11, we compare the rendering time of our framework with the existing NeRF-based method MipNeRF360 [4], as well as two Gaussian Splatting baselines (i.e., 2D Gaussian Splatting and 3D Gaussian Splatting), on an RTX 3090 GPU in terms of seconds per image. As shown, our DisC-GS can achieve a competitive rendering time (speed) compared to the existing methods leveraging ", "page_idx": 14}, {"type": "table", "img_path": "ScbmEmtsH5/tmp/79bcaf8ddd80e2bf858347c4634ec5c99adc37256f0140f61fce9c8961e43bc3.jpg", "table_caption": ["Table 11: Analysis of rendering time in terms of seconds per image. Our framework can run efficiently and satisfy most real-time requirements, yet achieves superior performance. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "the conventional Gaussian Splatting technique, while obtaining much better performance. ", "page_idx": 14}, {"type": "image", "img_path": "ScbmEmtsH5/tmp/c794b82694dd10035051b259e5d1153cf27f69307effcaa519d5709aba02adfd.jpg", "img_caption": ["2D Gaussian Splatting w/o DisC-GS ", "Figure 3: Qualitative results of 2D Gaussian Splatting with and without DisC-GS. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Additional Qualitative Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we present more qualitative results. Specifically, in Fig. 3, we present images rendered by 2D Gaussian Splatting with and without applying our proposed framework DisC-GS; in Fig. 4, we present images rendered by 3D Gaussian Splatting with and without DisC-GS. As shown, no matter representing the 3D scene through 2D or 3D Gaussian distributions, the typical Gaussian Splatting technique can fail to render boundaries and discontinuities clearly and with high quality. Yet, our DisC-GS framework, when applied, can achieve good rendering quality, even in regions of the image containing numerous boundaries and discontinuities. This further shows the efficacy of our approach. ", "page_idx": 15}, {"type": "text", "text": "C Additional Details about Eq. 7 in the Main Paper ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Eq. 7 in the main paper, via leveraging the implicitization technique in algebra [1], we enable the cubic B\u00e9zier curve to be represented in its implicit representation form as: 1 $3_{i m p}(x,y)=\\gamma_{x x x}x^{3}+\\gamma_{x x y}x^{2}y+\\gamma_{x y y}x y^{2}+\\gamma_{y y y}y^{3}+\\gamma_{x x}x^{2}+\\gamma_{x y}x y+\\gamma_{y y}y^{2}+\\gamma_{x x}+\\gamma_{y}y+\\gamma_{0}=0$ (14) Here in this section, we discuss how we derive coefficients including $\\gamma_{x x x},\\,\\gamma_{x x y},\\,\\gamma_{x y y},\\,\\gamma_{y y y},\\,\\gamma_{x x}$ , $\\gamma_{x y},\\gamma_{y y},\\gamma_{x},\\gamma_{y}$ , and $\\gamma_{0}$ in Eq. 7 in the main paper (re-shown in Eq. 14 above). ", "page_idx": 15}, {"type": "text", "text": "3D Gaussian Splatting with DisC-GS ", "page_idx": 16}, {"type": "image", "img_path": "ScbmEmtsH5/tmp/1af6106e17fd75028f22e55c72d7fc257f8b1e74bcf955ae0fdff66e18683e51.jpg", "img_caption": ["3D Gaussian Splatting w/o DisC-GS ", "Figure 4: Qualitative results of 3D Gaussian Splatting with and without DisC-GS. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Specifically, denote the four control points of the cubic B\u00e9zier curve $\\omega_{0}=(x_{0},y_{0})$ , $\\omega_{1}=(x_{1},y_{1})$ , $\\dot{\\omega_{2}}=(x_{2},\\dot{y}_{2})$ , and $\\omega_{3}=(x_{3},y_{3})$ . To derive the coefficients in Eq. 14, following [1], we first define a set of intermediate coefficients as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\zeta_{0}=x_{0};}\\\\ &{\\zeta_{1}=-3\\times x_{0}+3\\times x_{1};}\\\\ &{\\zeta_{2}=-6\\times x_{1}+3\\times x_{0}+3\\times x_{2};}\\\\ &{\\zeta_{3}=x_{3}-x_{0}-3\\times x_{2}+3\\times x_{1};}\\\\ &{\\zeta_{4}=y_{0};}\\\\ &{\\zeta_{5}=-3\\times y_{0}+3\\times y_{1};}\\\\ &{\\zeta_{6}=-6\\times y_{1}+3\\times y_{0}+3\\times y_{2};}\\\\ &{\\zeta_{7}=y_{3}-y_{0}-3\\times y_{2}+3\\times y_{1};}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "After that, utilizing these intermediate coefficients, following [1], we can then compute coefficients in Eq. 14 as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{x x x}=\\zeta_{7}\\times\\zeta_{7}\\times\\zeta_{7};}\\\\ &{\\gamma_{x x y}=-3\\times\\zeta_{3}\\times\\zeta_{7}\\times\\zeta_{7};}\\\\ &{\\gamma_{x y y}=3\\times\\zeta_{7}\\times\\zeta_{3}\\times\\zeta_{3};}\\\\ &{\\gamma_{y y y}=-\\zeta_{3}\\times\\zeta_{3}\\times\\zeta_{3};}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "\u03b3xx = \u22123 \u00d7 \u03b63 \u00d7 \u03b65 \u00d7 \u03b66 \u00d7 \u03b67 + \u03b61 \u00d7 \u03b66 \u00d7 \u03b67 \u00d7 \u03b67 \u2212\u03b62 \u00d7 \u03b67 \u00d7 \u03b66 \u00d7 \u03b66 + 2 \u00d7 \u03b62 \u00d7 \u03b65 \u00d7 \u03b67 \u00d7 \u03b67 + 3 \u00d7 \u03b63 \u00d7 \u03b64 \u00d7 \u03b67 \u00d7 \u03b67 + \u03b63 \u00d7 \u03b66 \u00d7 \u03b66 \u00d7 \u03b66 \u22123 \u00d7 \u03b60 \u00d7 \u03b67 \u00d7 \u03b67 \u00d7 \u03b67; $\\gamma_{x y}=\\zeta_{1}\\times\\zeta_{3}\\times\\zeta_{6}\\times\\zeta_{7}-\\zeta_{2}\\times\\zeta_{3}\\times\\zeta_{5}\\times\\zeta_{7}-6\\times\\zeta_{4}$ $\\tau-\\zeta_{2}\\times\\zeta_{3}\\times\\zeta_{5}\\times\\zeta_{7}-6\\times\\zeta_{4}\\times\\zeta_{7}\\times\\zeta_{3}\\times\\zeta_{3}-3\\times$ \u03b61 \u00d7 \u03b62 \u00d7 \u03b67 \u00d7 \u03b67 \u22122 \u00d7 \u03b62 \u00d7 \u03b63 \u00d7 \u03b66 \u00d7 \u03b66 + 2 \u00d7 \u03b66 \u00d7 \u03b67 \u00d7 \u03b62 \u00d7 \u03b62 + 3 \u00d7 \u03b65 \u00d7 \u03b66 \u00d7 \u03b63 \u00d7 \u03b63 + 6 \u00d7 \u03b60 \u00d7 \u03b63 \u00d7 \u03b67 \u00d7 \u03b67;   \n$\\gamma_{y y}=3\\times\\zeta_{1}\\times\\zeta_{2}\\times\\zeta_{3}\\times\\zeta_{7}+\\zeta_{3}\\times\\zeta_{6}\\times\\zeta_{2}\\times\\zeta_{2}-\\zeta_{2}\\times\\zeta_{5}\\times\\zeta_{3}\\times\\zeta_{3}-3\\times\\zeta_{0}\\times\\zeta_{7}\\times\\zeta_{3}\\times\\zeta_{3}$   \n$-\\ 2\\times\\zeta_{1}\\times\\zeta_{6}\\times\\zeta_{3}\\times\\zeta_{3}-\\zeta_{7}\\times\\zeta_{2}\\times\\zeta_{2}+3\\times\\zeta_{4}\\times\\zeta_{3}\\times\\zeta_{3}\\times\\zeta_{3};$ $\\gamma_{x}=\\zeta_{2}\\times\\zeta_{3}\\times\\zeta_{4}\\times\\zeta_{5}\\times\\zeta_{7}-\\zeta_{1}\\times\\zeta_{2}\\times\\zeta_{5}\\times\\zeta_{6}\\times\\zeta_{7}-\\zeta_{1}\\times\\zeta_{3}\\times\\zeta_{4}\\times\\zeta_{6}\\times\\zeta_{7}$ $+~6\\times\\zeta_{0}\\times\\zeta_{3}\\times\\zeta_{5}\\times\\zeta_{6}\\times\\zeta_{7}+\\zeta_{5}\\times\\zeta_{1}\\times\\zeta_{1}\\times\\zeta_{7}\\times\\zeta_{7}+\\zeta_{7}\\times\\zeta_{2}\\times\\zeta_{2}\\times\\zeta_{5}\\times\\zeta_{5}$ $+\\,3\\times\\zeta_{7}\\times\\zeta_{3}\\times\\zeta_{3}\\times\\zeta_{4}\\times\\zeta_{4}+\\zeta_{1}\\times\\zeta_{3}\\times\\zeta_{5}\\times\\zeta_{6}\\times\\zeta_{6}-\\zeta_{2}\\times\\zeta_{3}\\times\\zeta_{6}\\times\\zeta_{5}\\times\\zeta_{5}$ $-\\,6\\times\\zeta_{0}\\times\\zeta_{3}\\times\\zeta_{4}\\times\\zeta_{7}\\times\\zeta_{7}-4\\times\\zeta_{0}\\times\\zeta_{2}\\times\\zeta_{5}\\times\\zeta_{7}\\times\\zeta_{7}-3\\times\\zeta_{4}\\times\\zeta_{5}\\times\\zeta_{6}\\times\\zeta_{3}\\times\\zeta_{3}$ $\\begin{array}{r}{-\\,2\\times\\zeta_{0}\\times\\zeta_{1}\\times\\zeta_{6}\\times\\zeta_{7}\\times\\zeta_{7}-2\\times\\zeta_{1}\\times\\zeta_{3}\\times\\zeta_{7}\\times\\zeta_{5}\\times\\zeta_{5}-2\\times\\zeta_{4}\\times\\zeta_{6}\\times\\zeta_{7}\\times\\zeta_{2}\\times\\zeta_{2}}\\end{array}$ $+\\,2\\times\\zeta_{0}\\times\\zeta_{2}\\times\\zeta_{7}\\times\\zeta_{6}\\times\\zeta_{6}+2\\times\\zeta_{2}\\times\\zeta_{3}\\times\\zeta_{4}\\times\\zeta_{6}\\times\\zeta_{6}+3\\times\\zeta_{1}\\times\\zeta_{2}\\times\\zeta_{4}\\times\\zeta_{7}\\times\\zeta_{8}.$ $+\\,\\zeta_{3}\\times\\zeta_{3}\\times\\zeta_{5}\\times\\zeta_{5}\\times\\zeta_{5}+3\\times\\zeta_{0}\\times\\zeta_{0}\\times\\zeta_{7}\\times\\zeta_{7}\\times\\zeta_{7}-2\\times\\zeta_{0}\\times\\zeta_{3}\\times\\zeta_{6}\\times\\zeta_{6}\\times\\zeta_{6};$ $\\gamma_{y}=\\zeta_{0}\\times\\zeta_{2}\\times\\zeta_{3}\\times\\zeta_{5}\\times\\zeta_{7}+\\zeta_{1}\\times\\zeta_{2}\\times\\zeta_{3}\\times\\zeta_{5}\\times\\zeta_{6}-\\zeta_{0}\\times\\zeta_{1}\\times\\zeta_{3}\\times\\zeta_{6}\\times\\zeta_{7}$ $-\\,6\\times\\zeta_{1}\\times\\zeta_{2}\\times\\zeta_{3}\\times\\zeta_{4}\\times\\zeta_{7}-\\zeta_{1}\\times\\zeta_{1}\\times\\zeta_{1}\\times\\zeta_{7}\\times\\zeta_{7}-3\\times\\zeta_{3}\\times\\zeta_{3}\\times\\zeta_{4}\\times\\zeta_{1}\\times\\zeta_{2}\\times\\zeta_{3}.$ \u03b64 $-\\,\\zeta_{1}\\times\\zeta_{3}\\times\\zeta_{3}\\times\\zeta_{5}\\times\\zeta_{5}-\\zeta_{3}\\times\\zeta_{1}\\times\\zeta_{1}\\times\\zeta_{6}\\times\\zeta_{6}-3\\times\\zeta_{3}\\times\\zeta_{0}\\times\\zeta_{0}\\times\\zeta_{7}\\times\\zeta_{7}$ $+\\,\\zeta_{2}\\times\\zeta_{6}\\times\\zeta_{7}\\times\\zeta_{1}\\times\\zeta_{1}-\\zeta_{1}\\times\\zeta_{5}\\times\\zeta_{7}\\times\\zeta_{2}\\times\\zeta_{2}-3\\times\\zeta_{0}\\times\\zeta_{5}\\times\\zeta_{6}\\times\\zeta_{3}\\times\\zeta_{3}$ $-\\,2\\times\\zeta_{0}\\times\\zeta_{6}\\times\\zeta_{7}\\times\\zeta_{2}\\times\\zeta_{2}-2\\times\\zeta_{3}\\times\\zeta_{4}\\times\\zeta_{6}\\times\\zeta_{2}\\times\\zeta_{2}+2\\times\\zeta_{0}\\times\\zeta_{2}\\times\\zeta_{3}\\times\\zeta_{6}\\times\\zeta_{6}$ $+\\,2\\times\\zeta_{2}\\times\\zeta_{4}\\times\\zeta_{5}\\times\\zeta_{3}\\times\\zeta_{3}+2\\times\\zeta_{3}\\times\\zeta_{5}\\times\\zeta_{7}\\times\\zeta_{1}\\times\\zeta_{1}+3\\times\\zeta_{0}\\times\\zeta_{1}\\times\\zeta_{2}\\times\\zeta_{7}\\times\\zeta_{7}$ $+~4\\times\\zeta_{1}\\times\\zeta_{4}\\times\\zeta_{6}\\times\\zeta_{3}\\times\\zeta_{3}+6\\times\\zeta_{0}\\times\\zeta_{4}\\times\\zeta_{7}\\times\\zeta_{3}\\times\\zeta_{3}+2\\times\\zeta_{4}\\times\\zeta_{7}\\times\\zeta_{2}\\times\\zeta_{2}\\times\\zeta_{2};$ $\\gamma_{0}=\\zeta_{0}\\times\\zeta_{1}\\times\\zeta_{2}\\times\\zeta_{5}\\times\\zeta_{6}\\times\\zeta_{7}+\\zeta_{0}\\times\\zeta_{1}\\times\\zeta_{3}\\times\\zeta_{4}\\times\\zeta_{6}\\times\\zeta_{7}-\\zeta_{0}\\times\\zeta_{2}\\times\\zeta_{3}\\times\\zeta_{4}\\times\\zeta_{5}\\times\\zeta_{7}$ $\\begin{array}{r l}&{-{5_{1}}\\times{5_{2}}\\times{5_{3}}\\times{6}\\times{5_{5}}\\times{6}+{5_{4}}\\times{5_{1}}\\times{5_{1}}\\times{5_{1}}\\times{5_{1}}\\times{5_{7}}\\times{7}\\times{7}\\times{7}\\times{8}\\times{2}\\times{5_{2}}\\times{6}\\times{5_{2}}\\times{4}\\times{5_{4}}}\\\\ &{+{5_{1}}\\times{6}\\times{5_{4}}\\times{5_{3}}\\times{6}\\times{5_{5}}\\times{6}\\times{5_{6}}\\times{6}\\times{5_{0}}\\times{7}\\times{6}\\times{5_{7}}+{5_{3}}\\times{6}\\times{5_{4}}\\times{7}\\times{6}\\times{5_{1}}\\times{6}\\times{5_{6}}}\\\\ &{+{5_{3}}\\times{6}\\times{5_{6}}\\times{2}\\times{5_{2}}\\times{6}\\times{4}\\times{5_{4}}\\times{6}\\times{5_{5}}\\times{6}\\times{1}\\times{5_{1}}\\times{6}\\times{7}\\times{7}\\times{6}\\times{5_{0}}\\times{7}\\times{2}\\times{5_{2}}\\times{5_{5}}\\times{6}}\\\\ &{-{6_{2}}\\times{5_{5}}\\times{6}\\times{5_{3}}\\times{6}\\times{4}\\times{5_{4}}\\times{7}\\times{7}\\times{6}\\times{5_{0}}\\times{6}\\times{5_{6}}\\times{6}\\times{6}\\times{7}\\times{6}\\times{7}\\times{7}\\times{3}\\times{6}\\times{5_{1}}\\times{6}\\times{4}}\\\\ &{-{2}\\times{\\zeta_{1}}\\times{6}\\times{5_{6}}\\times{3}\\times{\\zeta_{3}}\\times{6}\\times{5_{4}}\\times{4}\\times{2}\\times{5_{2}}\\times{5_{5}}\\times{6}\\times{5_{0}}\\times{\\zeta_{0}}\\times{5_{7}}\\times{7}\\times{5_{1}}\\times{6}\\times{5_{1}}}\\\\ &{+{3\\times5_{3}}\\times{6}\\times{4}\\times{6}\\times{6}\\times{5_{0}}\\times{7}\\$ ", "page_idx": 17}, {"type": "text", "text": "Note that, while the above calculation may look complicated, essentially, through the above, each of the coefficients in Eq. 14 is computed over the coordinates of the four control points of the curve (i.e. $\\{x_{0},y_{0},x_{1},y_{1},x_{2},y_{2},x_{3},y_{3}\\})$ just through a group of basic arithmetic operations. In other words, the computation of the coefficients in Eq. 14 is just a task with $O(1)$ time complexity. ", "page_idx": 17}, {"type": "text", "text": "D Additional Details about the B\u00e9zier-boundary Gradient Approximation Strategy ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In our framework, we propose a B\u00e9zier-boundary gradient approximation strategy to keep the \u201cdifferentiability\u201d of the rendering process. In Sec. 4.2 in the main paper, we explain this strategy taking the approximation of $\\frac{\\bar{\\partial}\\bar{g}^{-}}{\\partial c_{c u r v e}^{2D}[0,0]}$ ] as an example. Here in this section, with ccurve \u2208R4M\u00d72, we further describe how we approximate \u2202cc2uDr\u2202vge[i,j], where i \u2208{0, ..., 4M \u22121} and j \u2208{0, 1}. Specifically, in our framework, $\\frac{\\partial g}{\\partial c_{c u r v e}^{2D}[i,j]}$ is approximated in a similar manner as $\\frac{\\partial g}{\\partial c_{c u r v e}^{2D}[0,0]}$ except in the following two places. ", "page_idx": 17}, {"type": "text", "text": "(1) Definition change of $g_{s c}^{0}(p)$ . To approximate $\\frac{\\partial g}{\\partial c_{c u r v e}^{2D}[i,j]}$ , we first need to redefine $g_{s c}^{0}(p)$ as gsc (c0c2uDrve[4m], cc2uDrve[4m + 1], $c_{c u r v e}^{2D}[4m+2],c_{c u r v e}^{2D}[4m+3];p)$ , where $m=i\\;\\%\\;4$ . This is done for $g_{s c}^{0}(p)$ to accurately represent the B\u00e9zier curve w.r.t. $c_{c u r v e}^{2D}[i,j]$ . ", "page_idx": 18}, {"type": "text", "text": "(2) Reformulation of Eq. 11. Moreover, during approximating $\\frac{\\partial g}{\\partial c_{c u r v e}^{2D}[i,j]}$ , for $S_{\\phi}$ to be corrected derived, we also need to reformulate Eq. 11 according to the B\u00e9zier curve w.r.t. $c_{c u r v e}^{2D}[i,j]$ . Note that in the reformulated equation, $\\phi$ should be used in place of $c_{c u r v e}^{2D}[i,j]$ . ", "page_idx": 18}, {"type": "text", "text": "With the above two changes made, we can seamlessly use the strategy introduced in Sec. 4.2 in the main paper to approximate $\\frac{\\partial g}{\\partial c_{c u r v e}^{2D}[i,j]}$ . ", "page_idx": 18}, {"type": "text", "text": "E Experiments on 11 3D Scenes. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Tab. 1 in the main paper, following [30, 49, 22], we evaluate our method on a total of $13\\,3\\mathrm{D}$ scenes from the Mip-NeRF360 [4], Tanks&Temples [31], and Deep Blending [23] datasets. Here, following [35], we also evaluate our method on another benchmark with 11 3D scenes from the above three datasets. As shown in Tab. 12, on this evaluation benchmark, our method can also achieve superior performance consistently, further demonstrating the efficacy of our proposed method. ", "page_idx": 18}, {"type": "table", "img_path": "ScbmEmtsH5/tmp/6906518d6a303be79b98c20f17c78a3f8cbb12dd89214a2f388fc0f26d4b6d07.jpg", "table_caption": ["Table 12: Performance comparison following the evaluation benchmark of [35]. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "F Licenses ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We use the Tanks&Temples dataset [31] by following the license of here. We use the Mip-NeRF360 dataset [4] by following the Apache-2.0 license. Moreover, we use the Deep Blending dataset [23] by following the license of here. Besides, we use part of the code owned by Kerbl et al. [30] by following the license of here. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: In both the abstract and the introduction, we have clearly mentioned the task (scope) and the contributions of this paper (e.g., in the last paragraph of the introduction). ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Following the reviewer\u2019s suggestion, we discuss the limitations at the end of the main paper. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper does not include theoretical assumptions or proofs. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper builds its code based on the existing Gaussian Splatting technique. We have clearly mentioned (via descriptions and equations) how to reproduce our framework upon the off-the-shelf Gaussian Splatting technique. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: At this submission stage, we are sorry that we do not get enough approval to open-source our code. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: This paper has specified its data splits, its introduced hyperparameters, and other details in the Experiments section. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: To make a fair comparison with the existing works, we follow their experimental settings which do not including any error bars. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We introduce the type of GPU we use in the Experiments section, and further report the rendering speed in the Appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The authors have read through the NeurIPS Code of Ethics and carefully conform with it. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper focuses on addressing a key limitation of the Gaussian Splatting technique. To the best of our knowledge, it is not particularly tied to any particular deployments. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: To the best of our knowledge, this paper does not newly bring any risk for misuse and is thus not applicable to this question. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have discussed the licenses in our Appendix. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]