{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper is a foundational work on large language models, providing the technical details of GPT-4 which is a key model in the field of in-context learning."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-13", "reason": "This paper introduces Llama, a significant open-source large language model that is used as a baseline in many in-context learning studies and is highly relevant to the development of efficient multimodal models."}, {"fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "publication_date": "2022-12-01", "reason": "Flamingo is a key multimodal model that demonstrates strong few-shot learning capabilities and directly inspires the authors to extend these techniques to more complex multimodal tasks."}, {"fullname_first_author": "Roee Hendel", "paper_title": "In-context learning creates task vectors", "publication_date": "2023-10-23", "reason": "This paper introduces the concept of In-Context Vectors, which forms the basis for the proposed approach in the current work; it directly addresses the challenge of improving the efficiency of in-context learning in large language models."}, {"fullname_first_author": "Edward J. Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2022-04-25", "reason": "This paper introduces LoRA, a parameter-efficient fine-tuning technique which serves as a key comparative method in evaluating the efficiency of the proposed method."}]}