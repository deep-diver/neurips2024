[{"figure_path": "QhRemVrZbG/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Conventional ICL is more sensitive to the ICD selection and requires more inference time. (b) LIVE is more robust and reduces inference time by inputting a shift vector.", "description": "This figure compares the conventional In-Context Learning (ICL) method with the proposed Learnable In-Context Vector (LIVE) method.  Panel (a) shows that conventional ICL is highly sensitive to the choice of in-context demonstrations (ICDs), requiring more ICDs for high performance which in turn increases inference time and the number of FLOPs. Panel (b) demonstrates that LIVE, by inputting a shift vector derived from demonstrations, significantly reduces inference time and the number of FLOPs while maintaining comparable accuracy, making it more robust to ICD selection.", "section": "1 Introduction"}, {"figure_path": "QhRemVrZbG/figures/figures_3_1.jpg", "caption": "Figure 2: The LIVE training pipeline: (a) The distribution P(V, a; M) of LMMs output when using LIVE. (b) Adding LIVE into the representations of the query to simulate the shift effect brought by demonstrations. (c) The distribution P(2|XD; M) of LMMs output when using demonstrations.", "description": "This figure illustrates the LIVE training pipeline, comparing it to conventional In-Context Learning (ICL).  Panel (a) shows the distribution of the Large Multimodal Model (LMM) output P(V, \u03b1; M) when using the Learnable In-Context Vector (LIVE). Panel (b) demonstrates how LIVE is added to the query representation to simulate the shift effect of the in-context demonstrations, highlighting the mechanism of LIVE's intervention in the LMM. Finally, panel (c) shows the distribution of the LMM output P(x|XD; M) when using conventional ICL with demonstrations, serving as a baseline for comparison with LIVE. The figure visually conveys how LIVE aims to efficiently mimic the effect of ICL using a smaller input size.", "section": "3 LIVE: Learnable In-Context Vector"}, {"figure_path": "QhRemVrZbG/figures/figures_6_1.jpg", "caption": "Figure 1: (a) Conventional ICL is more sensitive to the ICD selection and requires more inference time. (b) LIVE is more robust and reduces inference time by inputting a shift vector.", "description": "This figure compares the conventional In-Context Learning (ICL) method with the proposed Learnable In-Context Vector (LIVE) method.  Panel (a) illustrates that conventional ICL is highly sensitive to the selection of in-context demonstrations (ICDs) and requires significantly more inference time as the number of ICDs increases. Panel (b) shows that LIVE mitigates these issues by using a learned shift vector, resulting in a more robust and efficient approach with reduced inference time and FLOPs.", "section": "1 Introduction"}, {"figure_path": "QhRemVrZbG/figures/figures_7_1.jpg", "caption": "Figure 3: The total number of FLOPs and real inference time consumption of ICL, Zero-Shot, LIVE for 1000 query samples.", "description": "This figure compares the computational efficiency and inference speed of three different methods: Conventional In-Context Learning (ICL), Zero-shot learning, and the proposed Learnable In-Context Vector (LIVE).  It shows that LIVE significantly reduces both the number of FLOPs (floating-point operations) and the inference time compared to ICL while maintaining performance similar to zero-shot learning. The results highlight LIVE's computational advantage for Visual Question Answering (VQA) tasks.", "section": "4 Experiments"}, {"figure_path": "QhRemVrZbG/figures/figures_7_2.jpg", "caption": "Figure 5: T-SNE visualization of first answer token representations over 200 queries.", "description": "This figure visualizes the effect of different methods (Zero-Shot, PCA-ICV, FV, TV, LIVE, and 32-shot ICL) on the representation of the first answer token in the latent space of the LLM.  It uses t-SNE to reduce the dimensionality of the representations for better visualization.  The plot shows how each method shifts the representation of the query compared to the zero-shot baseline, illustrating the impact of in-context demonstrations and the proposed LIVE method on the model's output.  The clustering and separation of points for each method illustrate the effectiveness of the different approaches in guiding the model's attention and improving its performance.", "section": "4.4 Analysis"}, {"figure_path": "QhRemVrZbG/figures/figures_8_1.jpg", "caption": "Figure 2: The LIVE training pipeline: (a) The distribution P(V, \u03b1; M) of LMMs output when using LIVE. (b) Adding LIVE into the representations of the query to simulate the shift effect brought by demonstrations. (c) The distribution P(x|XD; M) of LMMs output when using demonstrations.", "description": "This figure illustrates the LIVE training pipeline, comparing it to conventional in-context learning.  Panel (a) shows the output distribution of the language model when using the Learnable In-Context Vector (LIVE) to shift the query representations. Panel (b) visually depicts how LIVE modifies the query representations by simulating the shift effect that demonstrations normally provide in ICL. Panel (c) shows the output distribution of the language model when using demonstrations in a traditional in-context learning setup.  The figure highlights LIVE's ability to replace the need for multiple demonstrations, thus improving efficiency and reducing sensitivity to demonstration selection.", "section": "3 LIVE: Learnable In-Context Vector"}, {"figure_path": "QhRemVrZbG/figures/figures_20_1.jpg", "caption": "Figure 5: T-SNE visualization of first answer token representations over 200 queries.", "description": "This figure visualizes the effect of different methods (Zero-Shot, 32-shot ICL, Untrained LIVE, and Trained LIVE) on the representation of the first answer token in the latent space of a language model.  The visualization uses t-SNE to reduce the dimensionality of the representations and show their distribution in 2D. By comparing the distributions generated by these different methods, we can observe how each method shifts the representation of the query towards the correct answer, and the extent to which each method achieves this effect. The figure demonstrates that Trained LIVE's representation is closer to the 32-shot ICL method than other methods, signifying its effectiveness in simulating the effect of multiple demonstrations in in-context learning.", "section": "4.4 Analysis"}]