[{"type": "text", "text": "Stepping on the Edge: Curvature Aware Learning Rate Tuners ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vincent Roulet\\* Atish Agarwala\\* Jean-Bastien Grill Grzegorz Swirszcz Google DeepMind Google DeepMind Google DeepMind Google DeepMind vroulet@google.com thetish@google.com jbgrill@google.com swirszcz@google.com ", "page_idx": 0}, {"type": "text", "text": "Mathieu Blondel Google DeepMind mblondel@google.com ", "page_idx": 0}, {"type": "text", "text": "Fabian Pedregosa Google DeepMind pedregosa@google.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Curvature information \u2013 particularly, the largest eigenvalue of the loss Hessian, known as the sharpness \u2013 often forms the basis for learning rate tuners. However, recent work has shown that the curvature information undergoes complex dynamics during training, going from a phase of increasing sharpness to eventual stabilization. We analyze the closed-loop feedback effect between learning rate tuning and curvature. We find that classical learning rate tuners may yield greater one-step loss reduction, yet they ultimately underperform in the long term when compared to constant learning rates in the full batch regime. These models break the stabilization of the sharpness, which we explain using a simplified model of the joint dynamics of the learning rate and the curvature. To further investigate these effects, we introduce a new learning rate tuning method, Curvature Dynamics Aware Tuning (CDAT), which prioritizes long term curvature stabilization over instantaneous progress on the objective. In the full batch regime, CDAT shows behavior akin to prefixed warm-up schedules on deep learning objectives, outperforming tuned constant learning rates. In the mini batch regime, we observe that stochasticity introduces confounding effects that explain the previous success of some learning rate tuners at appropriate batch sizes. Our findings highlight the critical role of understanding the joint dynamics of the learning rate and curvature, beyond greedy minimization, to diagnose failures and design effective adaptive learning rate tuners. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The learning rate, a.k.a. stepsize, is the main hyperparameter controlling the efficiency and stability of gradient-based training of deep neural networks. The learning rate is typically adjusted through a predetermined schedule \u2013 often consisting of a warm-up phase, where the learning rate is gradually increased to a peak, followed by an annealing phase, where it is decreased to zero (Goyal et al., 2017; Loshchilov and Hutter, 2016). Tuning the shape of the schedule (warm-up time, peak learning rate, decay scale and shape) is essential for good performance. Despite recent efforts to understand their effectiveness, the optimal shape of these schedules remains an area of active research (Liu et al., 2020; Shi et al., 2023). The cost of tuning these schedules has led to interest in automatic selection of these hyperparameters with learning rate tuners - methods which aim to automatically adjust the learning rate through training. ", "page_idx": 0}, {"type": "text", "text": "These methods have roots in traditional optimization theory, including inexact linesearch with ArmijoGoldstein criterion (Armijo, 1966; Nocedal and Wright, 1999) and Polyak stepsizes (Polyak, 1964), which select the learning rate via estimates of the gap to optimality of the objective. The ArmijoGoldstein criterion is a crucial component of popular full-batch convex optimizers, such as L-BFGS (Liu and Nocedal, 1989). Recent efforts have adapted linesearches to stochastic optimization, with some partial empirical successes and with some approaches offering convergence guarantees (Galli et al., 2023; Mutschler and Zell, 2020; Vaswani et al., 2019). Similar efforts have been made for Polyak stepsizes (Berrada et al., 2020; Loizou et al., 2021), in addition to new methods which combine distance to optimality with online learning convergence bounds (Cutkosky et al., 2023; Defazio and Mishchenko, 2023; Ivgi et al., 2023; Mishchenko and Defazio, 2023). ", "page_idx": 1}, {"type": "text", "text": "Classically-inspired methods, however, have generally struggled to gain traction in deep learning. This is partly due to their design, which prioritizes convex, Lipschitz-continuous, and/or smooth (Lipschitz-continuous gradients) objectives. In contrast, the loss landscape of deep networks is known to be non-convex (Li et al., 2018), and non-Lipschitz continuous (Hochreiter et al., 2001). Moreover, non-linear models, especially neural networks, will commonly undergo dramatic changes in geometry during training (Arora et al., 2022; Jastrz\u02dbebski et al., 2019; Jastrzebski et al., 2020; Kalra et al., 2023; Kopitkov and Indelman, 2020; Lewkowycz et al., 2020; Wu et al., 2018). In particular, most models undergo a phase of progressive sharpening - where the sharpness, the largest eigenvalue of the Hessian, increases during training (Cohen et al., 2021). These potentially detrimental effects are mitigated by non-linear stabilization arising from the discreteness of the dynamics \u2013 namely, the edge of stability (EOS) phenomenon (Cohen et al., 2021). This causes large Hessian eigenvalues to stabilize at the critical value for a given learning rate in an equivalent smooth setting (for example, max Hessian eigenvalue stabilizes at $\\lambda_{\\mathrm{max}}=2/\\eta$ for learning rate $\\eta$ ) (Cohen et al., 2023, 2021). The early training time behavior corresponds to the regime where there is the most feature learning (Cohen et al., 2023, 2021), and is the main focus of this work; at late times, the large eigenvalues of the Hessian usually drop below the edge of stability. Gilmer et al. (2022) considered EOS stabilization as a leading candidate for the necessity of the warm-up procedure; as the learning rate $\\eta$ increases, $\\lambda_{\\mathrm{max}}$ is effectively annealed. ", "page_idx": 1}, {"type": "text", "text": "This raises some natural questions. How do these sharpness dynamics affect the performance of learning rate tuners? What insights can we gain to design better tuners for deep learning? Our work takes a first step at answering these questions, starting with a study of some classical learning rate tuners: a linesearch ensuring sufficient decrease and an approximately greedy method that minimizes a quadratic approximation of the objective. Specifically, we find the following. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We empirically observe that classical learning rate tuners qualitatively underperform their constant learning rate counterparts across several deep learning benchmarks, in the full batch regime, for which these methods were originally designed.   \n\u2022 Our empirical analysis of curvature dynamics reveals that classical learning rate tuners generally undershoot the edge of stability. This undershooting creates a snowball effect of ever-increasing sharpness and ever-decreasing learning rates.   \n\u2022 We propose a theoretical model that effectively captures these empirically observed failures. ", "page_idx": 1}, {"type": "text", "text": "Our analysis suggests that stabilizing the sharpness may be a more important goal for the long-term success of training, compared to greedily optimizing the objective. To explore this idea, we propose the Curvature Dynamics Aware Tuning (CDAT) method, which dynamically drives the learning rate to the EOS. In our exploration, we find the following. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We observe empirically that the proposed learning rate tuner can outperform fine-tuned constant learning rate counterparts in a full batch regime.   \n\u2022 We analyze the sharpness dynamics induced by CDAT in these examples and observe that the progressive sharpening is mitigated by the tuner, increasing learning rates at early times before stabilizing, akin to an automatic warm-up schedule.   \n\u2022 We propose a theoretical model that clarifies the dynamical mechanisms by which CDAT maintains proximity to the EOS, while highlighting the limitations of existing models of curvature dynamics. ", "page_idx": 1}, {"type": "text", "text": "Our work suggests that the design of learning rate tuners benefits from exploiting curvature stabilization rather than focusing on loss decrease. The introduction of simple learning rate tuners can also refine our understanding of sharpness dynamics through feedback loop effects. Additional experiments and experimental details are presented in Appendix B and Appendix $\\mathrm{C}$ respectively. ", "page_idx": 1}, {"type": "image", "img_path": "SEflLHIhhJ/tmp/34006b80f8a369f47211f2a8ed62c51dab47fa9c041c339dd9e54c28c8b42d30.jpg", "img_caption": ["Figure 1: Simple learning rates tuners qualitatively underperform their constant learning rate counterparts. Gradient descent or RMSProp with a tuned constant learning rate versus self-tuned gradient descent by a linesearch method (1), or a quadratically greedy rule (3) on various datasets, architectures and losses in a full batch regime. The linesearch may perform better at early times but stalls in the long term. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 The Interplay Between Learning Rate Tuners and Curvature Dynamics ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A leitmotif in the design of learning rate tuners has been to select the learning rate to ensure a maximal or sufficient decrease of the objective at each iteration. We focus here on two canonical examples. Polyak stepsizes and hyper-gradient descent are also briefly examined in Appendix B, Fig. 13. ", "page_idx": 2}, {"type": "text", "text": "2.1 Canonical learning rate tuners failures in deep learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The first classical approach we consider is a linesearch (ls) method that selects the learning rate $\\eta$ such that the objective $f$ satisfies a certain decrease criterion (Armijo, 1966; Nocedal and Wright, 1999). Formally, given current parameters $w_{t}$ and an update direction $u_{t}$ , the learning rate $\\eta_{t}^{\\mathrm{{\\breve{ls}}}}$ is chosen such that ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(w_{t}+\\eta_{t}^{\\mathrm{ls}}u_{t})\\leq f(w_{t})+c\\,\\eta_{t}^{\\mathrm{ls}}u_{t}^{\\top}\\nabla f(w_{t})\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This rule assumes that $u_{t}$ is a descent direction $(\\nabla f(w_{t})^{\\top}u_{t}<0)$ , which ensures the existence of a learning rate satisfying (1). This holds true for simple Gradient Descent (GD) or preconditioned variants like RMSProp (Hinton et al., 2012). In the criterion (1), $c$ is usually a small constant set to $10^{-4}$ or 0. A valid learning rate is searched with a usual backtracking linesearch (Appendix C). ", "page_idx": 2}, {"type": "text", "text": "The second method we consider involves selecting the learning rate at each iteration to minimize a quadratic approximation of the objective. Formally, the objective $f$ at parameters $w_{t}$ can be approximated along an update direction $u_{t}$ by a quadratic approximation $q_{f}$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(w_{t}+\\eta u_{t})\\approx q_{f}(\\eta;w_{t},u_{t}):=f(w_{t})+\\eta\\nabla f(w_{t})^{\\top}u_{t}+\\frac{1}{2}\\eta^{2}u_{t}^{\\top}\\nabla^{2}f(w_{t})u_{t}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Provided that this quadratic approximation is strongly convex in $\\eta$ $\\mathrm{\\nabla}\\colon(u_{t}^{\\top}\\nabla^{2}\\dot{f}(w_{t})u_{t}>\\dot{\\mathrm{~0~}})$ , the minimum of the quadratic approximation $q_{f}(\\eta;w_{t},u_{t})$ is reached for the quadratically greedy (qg) learning rate $\\eta^{\\mathrm{qg}}$ given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\eta_{t}^{\\mathrm{qg}}=\\frac{-\\nabla f(\\boldsymbol{w}_{t})^{\\top}\\boldsymbol{u}_{t}}{\\boldsymbol{u}_{t}^{\\top}\\nabla^{2}f(\\boldsymbol{w}_{t})\\boldsymbol{u}_{t}}\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Setting the learning rate by minimizing the quadratic approximations (3) is a simple intuitive idea studied for example by Schaul et al. (2013), Martens and Grosse (2015, Section 6.4). This approach as well as linesearches are effective on simple linear problems (Fig. 2). While their rationale originates in non-stochastic optimization, they have been analyzed in the context of stochastic optimization for deep learning (Schaul et al., 2013; Vaswani et al., 2019). ", "page_idx": 2}, {"type": "image", "img_path": "SEflLHIhhJ/tmp/0f06122082e5e8a75925d1bffe2b614cc8aea6794b57d310e50c958eba30e841.jpg", "img_caption": ["Figure 2: Classical learning rate tuners can be effective on linear models. "], "img_footnote": [], "page_idx": 2}, {"type": "image", "img_path": "SEflLHIhhJ/tmp/62a72511cee467df6044c110e0c9e627661b564f6e6a233ab4fa7464997227f4.jpg", "img_caption": ["Figure 3: Classical learning rate tuners can undershoot the edge of stability. Learning rate, sharpness, their product, and the gradient norm evolution of a constant learning rate and learning rate tuners, full batch gradient descent. Learning rate decreases by 3 orders of magnitude for tuners $\\mathrm{[^{\\,}1^{s t}}$ panel) while sharpness increases $\\mathrm{2^{nd}}$ panel). Their product remains relatively steady, just below the edge of stability $\\mathrm{{\\dot{3}}^{r d}}$ panel). The gradient norm increases by less than a factor of 10, consistent with slow training at late times ( $\\mathrm{4^{th}}$ panel). "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "2.2 Analyzing learning rate tuners through curvature dynamics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Full batch regime. We revisit the performance of the learning tuners presented in Section 2.1 in the full batch regime on deep learning problems in Fig. 1. As demonstrated in Fig. 1, a linesearch (1) or the quadratically greedy rule (3) qualitatively underperform their constant learning rate counterpart in the deep learning benchmarks considered. Notably, all these results are obtained despite being in a full batch regime, for which these methods are originally designed. To understand the failures of these approaches, we consider several measures presented in Fig. 3 (see also Fig. 12). ", "page_idx": 3}, {"type": "text", "text": "First, we observe a consistent decrease in the chosen learning rate over time, spanning several orders of magnitude $\\mathrm{[^{\\,st}}$ panel of Fig. 3). This is surprising, as none of these approaches explicitly encode a decreasing learning rate mechanism. Specifically, the linesearch always initiates its search with a guess larger than the previously selected learning rate (see Appendix C for implementation details). Decreasing learning rates are theoretically optimal for non-smooth objectives (Nesterov et al., 2018), such as the ones induced by using the ReLU activation; however in our example, the gradient norm does not increase beyond one order of magnitude $\\mathrm{4^{th}}$ panel of Fig. 3). This suggests both that an increase in gradient norm is not the primary cause of learning rate decrease, and also explains why the learning rate decrease is correlated with slower progress on the training loss. ", "page_idx": 3}, {"type": "text", "text": "Following the work of Cohen et al. (2021), we analyze the dynamics of the sharpness, that is the largest eigenvalue of the Hessian, $\\lambda_{\\operatorname*{max}}(\\nabla^{2}f(w_{t}))$ . In the $2^{\\mathrm{nd}}$ panel of Fig. 3, we observe that while sharpness stabilizes for gradient descent, it does not exhibit the same behavior for the considered learning rate tuners. By plotting the product of the learning rate $\\eta_{t}$ and the sharpness ( $3^{\\mathrm{rd}}$ panel of Fig. 3), we find that this product can exceed the stability threshold of 2, eventually stabilizing below this threshold for constant learning rate gradient descent. In contrast, for the learning rate tuners, this product neither surpasses the stability threshold nor stabilizes around 2 in the long run. Therefore, these classical learning rate tuners do not operate at the edge of stability. ", "page_idx": 3}, {"type": "text", "text": "From a theoretical perspective, objectives are typically classified as either smooth or non-smooth. Smooth objectives have gradients that are Lipschitz-continuous, at least locally around any point. Non-smooth objectives, on the other hand, may contain points with kinks (non-differentiable points). However, this taxonomy might not fully capture the curvature dynamics observed by Cohen et al. (2023, 2021) for constant learning rates, and in Fig. 1 for the classical learning rate tuners. In particular, the concept of smoothness might not be entirely relevant in the context of deep learning, where its local estimate (the spectral norm of the Hessian, also known as sharpness) can continue to increase throughout training. To push the limits of classical smoothness assumptions, we consider in Section 3 a learning rate tuner that propels the optimizer at the edge of stability or above, a regime that usual smoothness assumptions would theoretically prohibit. ", "page_idx": 3}, {"type": "text", "text": "Mini-batch regime. The results presented in Fig. 1 in the full batch regime do not contradict the success of linesearches at medium batch size observed by Vaswani et al. (2019) in the stochastic regime. This observation is illustrated in Fig. 14, and was previously reported by Roulet et al. (2023). We simply point out that the success of linesearches observed by Vaswani et al. (2019) may not be entirely attributable to the method\u2019s original rationale. ", "page_idx": 3}, {"type": "image", "img_path": "SEflLHIhhJ/tmp/cfe983c94534fb0ee832476f036ad69b6f8d9e2d66b140dda14e464625368b15.jpg", "img_caption": ["Figure 4: The poor performance of classical learning rate tuners, understood in a simplified model. The dynamics of learning rate $\\eta$ , sharpness $\\lambda_{\\mathrm{max}}$ , and normalized centered sharpness $y=\\eta\\lambda_{\\mathrm{max}}-2$ are examined in the simplified model (5). With a constant $\\eta$ , $\\lambda_{\\mathrm{max}}$ stabilizes and $y$ oscillates around 0 (blue). Classical learning rate tuners often quickly equilibrate around $y_{t}=-\\epsilon$ , which we model using $\\eta\\,=\\,1.9\\lambda_{\\mathrm{max}}$ (orange). This equilibration of $y$ away from zero prevents stabilization in $\\lambda_{\\mathrm{max}}$ , leading to an increase in $\\lambda_{\\mathrm{max}}$ , and a corresponding decrease in $\\eta$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "The actual success of linesearches in a stochastic regime may instead be explained by the attenuated progressive sharpening observed in such a regime (Agarwala and Pennington, 2024; Cohen et al., 2021; Jastrz\u02dbebski et al., 2017). Moreover, linesearches applied to mini-batches tend to select larger learning rates than they would in a full-batch regime (Mutschler and Zell, 2020) potentially allowing them to avoid undershooting the full objective\u2019s edge of stability. ", "page_idx": 4}, {"type": "text", "text": "2.3 Theoretical analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The sharpening effects can be understood theoretically. Previous work has shown that the stabilization provided by EOS is due to non-linear interaction between the component of the gradient in the largest eigendirection, and the dynamics of the largest eigenvalues themselves (Agarwala et al., 2023; Damian et al., 2023). We can use these analyses to understand why there is no stabilization for some classical learning rate tuners. ", "page_idx": 4}, {"type": "text", "text": "We start with the model from Damian et al. (2023), which focuses on the dynamics in the largest eigendirection of the Hessian. We consider a unique eigenvector for simplicity; we don\u2019t observe degeneracy in the eigenspace of the largest eigenvalue in any practical models. Given an objective $f$ parameterized by parameters $w_{t}$ , let $\\lambda_{t}$ be the largest eigenvalue of the Hessian $\\nabla^{2}f(w_{t})$ , i.e., $\\mathring{\\lambda_{t}}:=\\lambda(w_{t})\\,:=\\,\\mathring{\\lambda_{\\operatorname*{max}}}\\big(\\nabla^{2}f(w_{t})\\big)$ . Let $v$ be its normalized eigenvector; the model assumes slow eigenvector change, so it is treated as a fixed direction. The joint dynamics of $\\lambda_{t}$ and the projection $x_{t}\\,{\\stackrel{\\smile}{}}:=v^{\\top}w_{t}$ can then be written as ", "page_idx": 4}, {"type": "equation", "text": "$$\nx_{t+1}=(1-\\eta_{t}\\lambda_{t})x_{t},\\;\\lambda_{t+1}=\\eta_{t}(a-b x_{t}^{2})+\\lambda_{t}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $\\boldsymbol{a}\\,:=\\,-\\nabla\\lambda(\\boldsymbol{w})^{\\top}\\nabla f(\\boldsymbol{w})$ corresponds to the instantaneous change of $\\lambda$ along the negative gradient (the update direction), and $\\widehat{b}\\;:=\\;\\|\\nabla\\lambda(w)\\|^{2}$ encodes the non-linear negative feedback between $x_{t}$ and $\\lambda_{t}$ . Both $a$ and $b$ are considered constant along iterations. These equations are derived by Damian et al. (2023) using a Taylor expansion of the iterates combined with a coupling argument. We provide intuition for the model in Appendix A.1. ", "page_idx": 4}, {"type": "text", "text": "In the original model, the learning rate $\\eta_{t}$ is also fixed to $\\eta$ . This leads to the following dynamics: while $\\eta\\lambda_{t}<2$ , the magnitude of $x_{t}$ decreases. This, in turn, leads to an increase in $\\lambda_{t}$ . Eventually, $\\eta\\lambda_{t}>2$ and $\\left|x_{t}\\right|$ increases. This eventually leads to the $b x_{t}^{2}$ term becoming large, which decreases $\\lambda_{t}$ . There is a range of learning rates over which this dynamic leads to quasi-stable oscillations of $\\lambda_{t}$ around the edge of stability value $2/\\eta$ (Fig. 4, blue curves). ", "page_idx": 4}, {"type": "text", "text": "When using a learning rate tuner, $\\eta_{t}$ is also a dynamical variable. This introduces the additional complication of a shifting edge of stability. Therefore, it is advantageous to analyze the dynamical system using normalized variables (Agarwala et al., 2023). We define $y_{t}:=\\eta_{t}\\lambda_{t}-2$ , where $y=0$ corresponds to the EOS, and $p_{t}:=x_{t}^{2}$ . This gives us the dynamical equations (Appendix A.2) ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{t+1}=(1+y_{t})^{2}p_{t},\\ y_{t+1}=\\eta_{t+1}\\left[\\eta_{t}\\left(a-b p_{t}\\right)\\right]+\\left({\\frac{\\eta_{t+1}}{\\eta_{t}}}\\right)y_{t}+2\\left[{\\frac{\\eta_{t+1}}{\\eta_{t}}}-1\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We must then supply a rule for $\\eta_{t+1}$ . In Fig. 3, we observed that in the full batch setting, the learning rate multiplied by the sharpness appears to quickly approach a threshold of $2-\\epsilon$ (corresponding to $y=-\\epsilon)$ , and then varies slowly below the EOS threshold. ", "page_idx": 4}, {"type": "image", "img_path": "SEflLHIhhJ/tmp/680a019a60d4f3c4393650888654be4aefc27bdf22bbf64506d0bdb647e4474d.jpg", "img_caption": ["Figure 5: Enforcing optimizers to stay on edge $(\\sigma=2.0)$ ) improves performance over greedy approximation $(\\sigma\\mathrm{~=~}1.0)$ . Train loss and learning rate behaviors for fine-tuned optimizers vs self-tuned counterparts with CDAT on various datasets, architectures, losses in a full batch regime. Tuning the learning rate \u201con edge\u201d $(\\sigma\\approx2)$ ) improves performance over greedy tuning $\\left.\\sigma=1\\right.$ ) as well as constant learning rate. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "We model the varying learning rate as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\eta_{t}:=2(1-\\epsilon)/\\lambda_{t}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This maintains $y_{t}=-\\epsilon$ . Notably, this schedule was explicitly proposed by Cohen et al. (2021) (see also Fig. 15). In this regime, $p_{t}$ decreases monotonically, aligning with the original goal of these methods to decrease the loss (Fig. 10). However, this eliminates feedback for controlling the increase in $\\lambda_{t}$ , resulting in significant progressive sharpening (Fig. 4, orange curve). ", "page_idx": 5}, {"type": "text", "text": "Consequently, when attempting to enforce monotonicity, learning rate tuners may inadvertently disrupt the non-linear stabilization that makes gradient descent robust and effective for training deep neural networks. Continually undershooting the EOS triggers a snowball effect of decreasing learning rate and increasing sharpness. If there is no corresponding increase in gradient norms, this causes optimization to slow down. ", "page_idx": 5}, {"type": "text", "text": "The poor performance of the classical learning rate tuners in Fig. 1 therefore appear strongly correlated with their tendency to undershoot the edge of stability in the normalized sharpness coordinate $y$ . In the following, we focus on understanding tuners that prioritize training at or near the edge of stability. ", "page_idx": 5}, {"type": "text", "text": "3 Optimizing on the Edge of Stability ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Based on our observations in Section 2, we design learning rate tuners that position the underlying optimizer on the edge of stability $(y=0,$ ). We analyze a tuner capable of operating both slightly below and slightly above the EOS in order to exploit nonlinear stabilization. ", "page_idx": 5}, {"type": "text", "text": "Formally, we investigate a generalization of the quadratically greedy rule from Section 2, which sought $\\eta_{t}$ to minimize the quadratic approximation $q_{f}$ in (2). We instead choose the learning rate to be on edge by seeking the largest value of $\\eta$ such that $q_{f}$ is smaller or equal to the original value of $f$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\eta_{t}^{\\mathrm{oe}}:=\\operatorname*{max}\\{\\eta\\ge0:q_{f}(\\eta;w_{t},u_{t})\\le f(w_{t})\\}=-2\\frac{\\nabla f(w_{t})^{\\top}u_{t}}{u_{t}^{\\top}\\nabla^{2}f(w_{t})u_{t}}\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the last formula holds provided that $u_{t}^{\\top}\\nabla^{2}f(w_{t})u_{t}>0$ (convex quadratic) and $\\nabla f(w_{t})^{\\top}u_{t}<$ 0 $\\mathit{u}_{t}$ is a descent direction). ", "page_idx": 5}, {"type": "image", "img_path": "SEflLHIhhJ/tmp/90f7aca0a8d13d142c8858844610912e2e408f6bf3fc06a98a55080fa89ee031.jpg", "img_caption": ["Figure 6: Optimizing on edge induces different curvature dynamics. Sharpness, product between learning rate and sharpness, and gradient norm evolutions for gradient descent with CDAT. By putting the learning rate on edge $(\\sigma\\approx2)$ , the sharpness does not ever increase and actually decreases slightly over time. GD with CDAT operates slightly above the edge constantly during training. Its gradient norm evolution is akin to a fine-tuned constant learning rate baseline. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "For $u_{t}=-\\nabla f(w_{t})$ , and if $-\\nabla f(w_{t})$ is aligned with the eigendirection $v_{\\mathrm{max}}$ associated with the largest eigenvalue $\\lambda_{\\mathrm{max}}$ of $H$ , we recover the familiar $\\eta_{t}^{\\mathrm{oe}}=\\bar{2}/\\lambda_{\\mathrm{max}}$ . Note however, that contrarily to using directly $\\eta_{t}=2/\\lambda_{\\operatorname*{max}}$ , the on-edge rule can naturally take into account the alignment with $v_{\\mathrm{max}}$ (see Fig. 15). We note that we recover the edge of stability even when the updates are given by the gradient multiplied by a preconditioner, e.g. $u_{t}\\,=\\,-P^{\\dot{-}1}\\nabla f(w_{t})$ for a matrix $P$ . In this case, we have $-u^{\\top}\\bar{H_{u}}/u^{\\top}\\bar{g_{}}\\,{=}\\,\\bar{g^{\\top}}P^{-1}H P^{-1}g/g^{\\top}P^{-1}g$ for $H=\\nabla^{2}f(w_{t})$ , $g=\\nabla f(w_{t})$ . This is maximized when $P^{-1/2}g$ lies in the largest eigendirection of the PSD matrix $\\tilde{H}\\equiv P^{-1/2}H P^{-1/2}$ , which for $\\sigma=2$ gives us the learning rate $\\eta_{t}\\stackrel{}{=}2/\\tilde{\\lambda}_{\\mathrm{max}}$ , where $\\tilde{\\lambda}_{\\mathrm{max}}$ is the maximum eigenvalue of $\\tilde{H}$ . This is exactly the edge of stability for adaptive methods (Cohen et al., 2023). ", "page_idx": 6}, {"type": "text", "text": "We note that the only difference between this and the quadratically greedy rule is a factor of 2 in the numerator. Inspired by this observation, and with an eye towards robustness, we define our Curvature Dynamics Aware Tuning (CDAT) rule by: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\eta_{t}^{\\mathrm{cdat}}=\\sigma\\frac{n_{t}}{d_{t}},\\quad\\mathrm{for}\\;n_{t}=\\operatorname*{max}\\{-\\nabla f(w_{t})^{\\top}u_{t},0\\},\\;d_{t}=|u_{t}^{\\top}\\nabla^{2}f(w_{t})u_{t}|+\\varepsilon.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The scaling factor $\\sigma$ lets us interpolate between greedy $\\left[\\sigma=1\\right]$ ) and on-edge $\\mathit{\\Theta}(\\sigma=2)$ ). We are most interested in the behavior near $\\sigma=2$ , (also studied in Rosca et al. (2023)). In (8), the max function takes care of the case where $u_{t}$ is an ascent direction $(\\nabla f(w_{t})^{\\top}u_{t}>\\bar{0})$ , the absolute value takes care of cases where the objective has negative curvature in the update directions (see Appendix C for additional justification), and we simply set $\\varepsilon=0$ as we always observed non-negligible positive curvature. The definitions of the numerator $n_{t}$ and the denominator $d_{t}$ allow for the possibility of exponential moving averages (EMA) of each quantity such as $\\tilde{n}_{t+1}=(1-\\beta_{\\mathrm{cdat}})n_{t}^{}+\\beta_{\\mathrm{cdat}}\\tilde{n_{t}}$ for $\\beta_{\\mathrm{cdat}}$ referred to as the CDAT EMA parameter thereafter. We observed that smoothing the estimates of $n_{t}$ and $d_{t}$ by an EMA is particularly relevant when the updates are themselves defined through an exponential moving average as in Adam, or when using the proposed rule in a stochastic setting. ", "page_idx": 6}, {"type": "text", "text": "CDAT has two major advantages: it is sensitive to information from all eigenvalues of $\\nabla^{2}f(w_{t})$ , and it depends on updates $u_{t}$ coming from any base optimizer. We will take advantage of these properties to explore the behavior of \u201con edge\u201d optimization in a variety of settings. ", "page_idx": 6}, {"type": "text", "text": "3.1 On edge optimizers in practice ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Full batch regime. Fig. 5 presents results for training with CDAT across various optimizers, architectures, datasets, and losses. Overall, selecting the learning rate to be on edge $\\sigma=2,$ ) is on par with or better than a fine-tuned constant learning rate and is always better than a quadratically greedy approach $\\left.\\sigma=1\\right.$ ). This observation holds even though the quadratically greedy rule ensures larger instantaneous decrease (Fig. 16). One notes that targeting slightly above the edge $\\sigma=2.0625$ ) provides even better performance than the on edge rule $\\sigma=2$ ) on all examples except the MLP Mixer on CIFAR10. However, targeting higher above the edge $\\sigma=2.5$ ) generally gives diverging results in the short or long terms. To integrate the proposed rule with the Adam optimizer, we also observed that the estimation of the curvatures through $n_{t},d_{t}$ in (8) was necessary. ", "page_idx": 6}, {"type": "image", "img_path": "SEflLHIhhJ/tmp/c89c1e1ffa70b1b70db0c27e292a6af6d657307edeb7ca76be7d8abae713a4bf.jpg", "img_caption": ["Figure 7: Stochasticity shifts the optimal scaling. Normalized performance of gradient descent with momentum equipped with CDAT in a stochastic regime with varying batch sizes. In a mini-batch regime, the optimal scale decreases as the batch size decreases. Using an exponential moving average smooths out the performance of the CDAT rule over batch sizes. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Remarkably, all choices around the edge (1.9375, 2.0, 20625) show a progressive increase of the learning rate that results generally in a better performance than the constant learning rate counterparts, except for RMSProp on the NanoLM experiment. The increasing learning rate behavior is akin to the warm-up phase generally hard-coded by a scheduler. In Fig. 19, we observe that the CDAT rule displays similar behavior as warm-up schedules, yet it may not fully capture the beneftis of prefixed schedules. ", "page_idx": 7}, {"type": "text", "text": "In Fig. 6, we analyze the dynamics of the curvature when optimizing on edge. We observe that the sharpness can be pushed to reduce over the iterations ( $\\mathrm{1^{st}}$ panel of Fig. 6). The CDAT rule may operate constantly slightly above the edge ( $\\mathrm{2^{nd}}$ panel of Fig. 6). By reducing the sharpness, the algorithm may be able to take larger stepsizes and converge faster. Sensitivity to architecure\u2019s width and depth, as well as weight decay, are also analyzed in Fig. 18. ", "page_idx": 7}, {"type": "text", "text": "Mini batch regime. The CDAT rule can be used in a stochastic regime by replacing $f$ in (8) by its stochastic counterpart $f^{(m_{t})}$ on a mini-batch $m_{t}$ . However, two difficulties may arise. ", "page_idx": 7}, {"type": "text", "text": "First, the on edge rule is motivated by the sharpening effects of the overall objective, which can be overestimated or underestimated by a single mini-batch. Previous work shows that the trace of the Hessian may best capture the sharpening and stabilization effects in a stochastic regime (Agarwala and Pennington, 2024; Wu and Su, 2023); it is unclear what function of the Hessian spectrum, the CDAT rule captures in the stochastic regime. As a result the optimal scaling factor may vary with the mini-batch. In Fig. 7, we observe that the optimal scaling of the on-edge rule is proportional to the batch size up to some size. In particular, at specific batch sizes, we observe that the greedy rule $\\left(\\sigma=1\\right)$ ) outperforms the on-edge rule. This result is consistent with the good performance of linesearches or greedy rules in a mini-batch regime previously mentioned and observed in Fig. 14. We also observe in Fig. 7, that integrating an EMA into the estimation of the edge in (8) smooth out the selection of the optimal scaling factor. ", "page_idx": 7}, {"type": "text", "text": "Finally, the sharpening effects are known to be generally mitigated in the stochastic regime (Agarwala and Pennington, 2024; Cohen et al., 2021; Jastrze\u02dbbski et al., 2017). The benefits of the on edge rule appear also subdued in this regime (Fig. 8, Fig. 20, Fig. 21). ", "page_idx": 7}, {"type": "text", "text": "3.2 Modeling CDAT dynamics ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The classical optimization framework is insufficient to fully explain the benefits of CDAT. For example, on a convex quadratic objective, $\\sigma=1$ is the optimal choice, and $\\sigma>2$ results (in the worst case) in a divergent algorithm. However, we can use a simplified model to begin understanding the joint dynamics of the learning rate and sharpness under CDAT. ", "page_idx": 7}, {"type": "text", "text": "We approximate the gradients around a stationary point $w_{\\star}$ , where $\\nabla f(w_{\\star})=0$ , as $\\nabla f(w_{t})\\approx H\\bar{w}_{t}$ CDAT is for $\\bar{w}_{t}:=w_{t}-w_{\\star}$ $\\eta_{t}^{\\mathrm{cdat}}=\\sigma\\big(\\bar{w}_{t}^{\\top}H^{2}\\bar{w}_{t}\\big)/\\big(\\bar{\\bar{w}}_{t}^{\\top}\\bar{H}^{3}\\bar{w}_{t}\\big)$ , and $H$ being a symmetric matrix. In this scenario, the learning rate given by . Consider the case where $H$ has two eigenvalues $\\lambda$ and $\\nu$ , with $\\lambda>\\nu\\geq0$ . In this case the CDAT learning rate can be written as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\eta_{t}^{\\mathrm{cdat}}=\\sigma\\frac{\\lambda^{2}p_{t}+\\nu^{2}g_{t}}{\\lambda^{3}p_{t}+\\nu^{3}g_{t}}=\\sigma\\frac{\\lambda^{2}p_{t}/g_{t}+\\nu^{2}}{\\lambda^{3}p_{t}/g_{t}+\\nu^{3}}\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "image", "img_path": "SEflLHIhhJ/tmp/eb6c94d131fe45499d23209a64af8f76e6fd76de4abd84c579ed1795fa0c89aa.jpg", "img_caption": ["Figure 8: The performance of CDAT is subdued in the stochastic regime. Fine-tuned constant, scheduled, and self-tuned with CDAT learning rates in a stochastic regime. In a stochastic regime, CDAT can also exhibit a form of learning rate warm-up (top figure). However, the interplay between sharpening and learning rate are known to be mitigated in a stochastic regime which may explain the underperformance of CDAT in this regime (bottom figure). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Here $p_{t},g_{t}$ are the projections $p_{t}:=(\\bar{w}_{t}^{\\top}v)^{2},g_{t}:=(\\bar{w}_{t}^{\\top}v_{\\bot})^{2}$ , respectively onto the eigendirections $v$ and $v_{\\perp}$ associated with $\\lambda,\\nu.$ . Therefore, $\\eta_{t}^{\\mathrm{cdat}}$ interpolates between its minimum value $\\sigma/\\lambda$ to the larger value $\\sigma/\\nu$ , depending on the alignment ratio $p_{t}/g_{t}$ . For $2\\nu/\\lambda<\\sigma<2$ , this rule can achieve learning rates both above and below the EOS. ", "page_idx": 8}, {"type": "text", "text": "We can gain additional insight by modeling a dynamical $\\lambda_{t}$ , extending the model of Section 2.3. While model (5) captures the dynamics in the largest eigendirection $v$ , here we aim to model the dynamics in the orthogonal subspace. To simplify, we consider the eigendirections $v,v_{\\perp}$ , and small eigenvalue $\\nu$ fixed. We then model the gradients as $\\nabla f(w_{t})\\approx H_{t}\\bar{w}_{t}$ with $H_{t}=\\lambda_{t}v v^{\\top}+\\nu v_{\\bot}v_{\\bot}^{\\top}$ . If we update $w$ in the direction $v_{\\perp}$ using gradient descent on $\\nu g_{t}$ , we obtain the following dynamical system describing the CDAT learning rate tuner: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\eta_{t+1}=\\sigma\\frac{\\lambda_{t}^{2}p_{t}+\\nu^{2}g_{t}}{\\lambda_{t}^{3}p_{t}+\\nu^{3}g_{t}},\\quad g_{t+1}=(1-\\eta_{t}\\nu_{t})^{2}g_{t},\\quad p_{t+1}=(1+y_{t})^{2}p_{t}\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Combining this with the update rule for $y_{t}$ given in (5) completes the model. ", "page_idx": 8}, {"type": "text", "text": "There are two important regimes of behavior in this model. First, if $y_{t}>0$ , $p_{t}$ will increase and eventually $y_{t}$ will decrease as in the normal EOS case. If $y_{t}<0$ , the key threshold is $y_{t}<-\\eta_{t}\\nu_{t}$ In this case, the ratio $p_{t}/g_{t}$ decreases - leading to an increase in $\\eta_{t}$ according to the on edge rule. If $a-b p_{t}>0$ (as it is if $p_{t}$ has become small due to $y_{t}<0$ ), then we see from (5) that this leads to an increase in $y_{t}$ . This suggests that CDAT has a tendency to push $y_{t}$ closer to the EOS \u2013 sending $y$ towards 0 if the learning rate is driven by the eigendirections corresponding to smaller eigenvalues. ", "page_idx": 8}, {"type": "text", "text": "Numerical simulations on this model (Fig. 9) suggest that this effect can indeed cause remarkably small values of $y$ ( $3^{\\mathrm{rd}}$ panel of Fig. 9). We emphasize that this is due to the joint dynamics of $\\eta_{t}$ (induced by the learning rate tuner), and $\\lambda_{t},p_{t}$ , and $g_{t}$ (induced by GD). There are also important limitations in this model\u2019s ability to fully explain CDAT\u2019s behavior. For example, the model predicts runaway sharpening for $\\sigma<2$ ( $\\mathrm{\\dot{2}^{n d}}$ panel of Fig. 9), and divergence for $\\sigma>2$ . In practice, we saw a range of stable and useful settings for scale centered around 2. This modeling limitation likely stems from neglecting the dynamics orthogonal to $v$ as well as higher-order terms, which empirically tend to stabilize EOS dynamics (Agarwala et al., 2023). ", "page_idx": 8}, {"type": "image", "img_path": "SEflLHIhhJ/tmp/443d0fb2ea27038d3c9971e4bc3d67170c13b5de09db9c1471250726281662cc.jpg", "img_caption": ["Figure 9: A simple model partially captures the benefits induced by the proposed CDAT rule. Dynamics of theoretical model of CDAT (10). For $\\sigma=2$ , feedback stabilizes $y$ close to the EOS $(y=0,$ ), which stabilizes $\\lambda_{\\mathrm{max}}$ (orange). For $\\sigma=2-\\epsilon$ and small $\\epsilon$ (blue, $\\epsilon=0.1$ ), model predicts that $\\lambda_{\\mathrm{max}}$ slowly grows (middle), but predicts that $y$ stabilizes to a value $-\\epsilon\\ll y_{t}<0$ (right). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "4 Conclusion and Future Directions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Summary. Our empirical results showed that simple linesearches and approximate greedy learning rate tuners underperform constant learning rate approaches in the full batch regime \u2013 despite being better on individual steps. The idea that \u201clocally greedy\u201d methods perform poorly on long time scales has been shown in other settings as well, including evolutionary dynamics Agarwala and Fisher (2019). Our experiments and theoretical work suggest the failure of these classical tuners is due to the fact that they suppress the feedback which stabilizes sharpness in the fixed learning rate setting. As the sharpness increases, tuners are forced to take smaller steps, which ends up leading to slower learning. ", "page_idx": 9}, {"type": "text", "text": "We find, in contrast, that prioritizing stability of the sharpness yields tangible benefits. Our CDAT method pushes the network towards the edge of stability via a dynamically driven process. It also naturally displays some form of progressive increase of the learning rate akin to prefixed warm-up schedules. CDAT also sheds light on the more complicated dynamics in small mini batch regime, where estimation of a locally greedy rule may actually place the optimizer on the edge of stability of the full batch objective. ", "page_idx": 9}, {"type": "text", "text": "Limitations and future directions. We explored some limitations of the current modeling framework in Section $2.3\\textup{-}$ in particular, the failure to capture stabilization due to higher order terms. Developing improved models (either analytically or numerically) would allow for powerful tools from other disciplines to aid algorithm design \u2013 particularly, methods from control theory. For example, state feedback schemes can be designed through the analysis of nonlinear dynamical systems to ensure asymptotic stabilization (Isidori, 1995, Chapter 7). We believe a cross disciplinary approach will be useful for designing the next generation of learning rate tuners. ", "page_idx": 9}, {"type": "text", "text": "The proposed CDAT rule may also help to understand and refine the design of learning rate schedules through scaling ladders (Wortsman et al., 2024). Recent work has shown that transfer of learning rates over different scales is related to consistency of curvatue dynamics (Noci et al., 2024); this suggests that approaches like ours may be useful to increase predictability of optimal learning rates across scale. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements. We thank James Martens and Mihaela Rosca for fruiftul discussions on related ideas. We also thank the reviewers for their insightful comments that helped us refine the manuscript. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Agarwala, A. and Fisher, D. S. (2019), \u2018Adaptive walks on high-dimensional fitness landscapes and seascapes with distance-dependent statistics\u2019, Theoretical population biology 130, 13\u201349. ", "page_idx": 10}, {"type": "text", "text": "Agarwala, A., Pedregosa, F. and Pennington, J. (2023), Second-order regression models exhibit progressive sharpening to the edge of stability, in \u2018International Conference on Machine Learning\u2019, PMLR, pp. 169\u2013195. ", "page_idx": 10}, {"type": "text", "text": "Agarwala, A. and Pennington, J. (2024), \u2018High dimensional analysis reveals conservative sharpening and a stochastic edge of stability\u2019, arXiv preprint arXiv:2404.19261 . ", "page_idx": 10}, {"type": "text", "text": "Almeida, L. B., Langlois, T., Amaral, J. D. and Plakhov, A. (1999), Parameter adaptation in stochastic optimization, in \u2018On-line learning in neural networks\u2019, pp. 111\u2013134. ", "page_idx": 10}, {"type": "text", "text": "Armijo, L. (1966), \u2018Minimization of functions having Lipschitz continuous first partial derivatives\u2019, Pacific Journal of mathematics 16(1), 1\u20133. ", "page_idx": 10}, {"type": "text", "text": "Arora, S., Li, Z. and Panigrahi, A. (2022), Understanding gradient descent on the edge of stability in deep learning, in \u2018International Conference on Machine Learning\u2019, PMLR, pp. 948\u20131024. ", "page_idx": 10}, {"type": "text", "text": "Ba, J. L., Kiros, J. R. and Hinton, G. E. (2016), \u2018Layer normalization\u2019, arXiv preprint arXiv:1607.06450 . ", "page_idx": 10}, {"type": "text", "text": "Baydin, A. G., Cornish, R., Rubio, D. M., Schmidt, M. and Wood, F. (2018), Online learning rate adaptation with hypergradient descent, in \u2018International Conference on Learning Representations\u2019. ", "page_idx": 10}, {"type": "text", "text": "Berrada, L., Zisserman, A. and Kumar, M. P. (2020), Training neural networks for and by interpolation, in \u2018International conference on machine learning\u2019, PMLR. ", "page_idx": 10}, {"type": "text", "text": "Blondel, M. and Roulet, V. (2024), \u2018The Elements of Differentiable Programming\u2019, arXiv preprint arXiv:2403.14606 . ", "page_idx": 10}, {"type": "text", "text": "Cohen, J., Ghorbani, B., Krishnan, S., Agarwal, N., Medapati, S., Badura, M., Suo, D., Cardoze, D., Nado, Z., Dahl, G. E. and Gilmer, J. (2023), Adaptive gradient methods at the edge of stability, in \u2018NeurIPS 2023 Workshop Heavy Tails in Machine Learning\u2019. ", "page_idx": 10}, {"type": "text", "text": "Cohen, J., Kaur, S., Li, Y., Kolter, J. Z. and Talwalkar, A. (2021), Gradient descent on neural networks typically occurs at the edge of stability, in \u2018International Conference on Learning Representations\u2019. ", "page_idx": 10}, {"type": "text", "text": "Cutkosky, A., Defazio, A. and Mehta, H. (2023), Mechanic: A learning rate tuner, in \u2018Advances in Neural Information Processing Systems\u2019. ", "page_idx": 10}, {"type": "text", "text": "Dagr\u00e9ou, M., Ablin, P., Vaiter, S. and Moreau, T. (2024), How to compute hessian-vector products?, in \u2018ICLR Blogposts 2024\u2019. https://iclr-blogposts.github.io/2024/blog/bench-hvp/. ", "page_idx": 10}, {"type": "text", "text": "Damian, A., Nichani, E. and Lee, J. D. (2023), Self-stabilization: The implicit bias of gradient descent at the edge of stability, in \u2018International Conference on Learning Representations\u2019. ", "page_idx": 10}, {"type": "text", "text": "DeepMind, Babuschkin, I., Baumli, K., Bell, A., Bhupatiraju, S., Bruce, J., Buchlovsky, P., Budden, D., Cai, T., Clark, A., Danihelka, I., Dedieu, A., Fantacci, C., Godwin, J., Jones, C., Hemsley, R., Hennigan, T., Hessel, M., Hou, S., Kapturowski, S., Keck, T., Kemaev, I., King, M., Kunesch, M., Martens, L., Merzic, H., Mikulik, V., Norman, T., Papamakarios, G., Quan, J., Ring, R., Ruiz, F., Sanchez, A., Sartran, L., Schneider, R., Sezener, E., Spencer, S., Srinivasan, S., Stanojevi\u00b4c, M., Stokowiec, W., Wang, L., Zhou, G. and Viola, F. (2020), \u2018The DeepMind JAX Ecosystem\u2019. URI Maithi 1 1 ", "page_idx": 10}, {"type": "text", "text": "Defazio, A. and Mishchenko, K. (2023), Learning-rate-free learning by d-adaptation, in \u2018International Conference on Machine Learning\u2019, PMLR, pp. 7449\u20137479. ", "page_idx": 10}, {"type": "text", "text": "Dehghani, M., Gritsenko, A., Arnab, A., Minderer, M. and Tay, Y. (2022), Scenic: A jax library for computer vision research and beyond, in \u2018Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\u2019, pp. 21393\u201321398. ", "page_idx": 10}, {"type": "text", "text": "Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K. and Fei-Fei, L. (2009), Imagenet: A large-scale hierarchical image database, in \u20182009 IEEE Conference on Computer Vision and Pattern Recognition\u2019, IEEE, pp. 248\u2013255.   \nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J. and Houlsby, N. (2021), An image is worth 16x16 words: Transformers for image recognition at scale, in \u2018International Conference on Learning Representations\u2019.   \nGalli, L., Rauhut, H. and Schmidt, M. (2023), Don\u2019t be so monotone: Relaxing stochastic line search in over-parameterized models, in \u2018Advances in Neural Information Processing Systems\u2019.   \nGhorbani, B., Krishnan, S. and Xiao, Y. (2019), An investigation into neural net optimization via Hessian eigenvalue density, in \u2018International Conference on Machine Learning\u2019, PMLR, pp. 2232\u2013 2241.   \nGilmer, J., Ghorbani, B., Garg, A., Kudugunta, S., Neyshabur, B., Cardoze, D., Dahl, G. E., Nado, Z. and Firat, O. (2022), A loss curvature perspective on training instabilities of deep learning models, in \u2018International Conference on Learning Representations\u2019.   \nGoyal, P., Doll\u00e1r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y. and He, K. (2017), \u2018Accurate, large minibatch SGD: Training imagenet in 1 hour\u2019, arXiv preprint arXiv:1706.02677 .   \nHe, K., Zhang, X., Ren, S. and Sun, J. (2016), Deep residual learning for image recognition, in \u2018Proceedings of the IEEE conference on computer vision and pattern recognition\u2019, pp. 770\u2013778.   \nHinton, G., Nitish, S. and Swersky, K. (2012), \u2018Divide the gradient by a running average of its recent magnitude\u2019, COURSERA: Neural Networks for Machine Learning .   \nHochreiter, S., Bengio, Y., Frasconi, P. and Schmidhuber, J. (2001), \u2018Gradient flow in recurrent nets: the difficulty of learning long-term dependencies\u2019, A Field Guide to Dynamical Recurrent Neural Networks pp. 237\u2013244.   \nHoward, J. (2019), \u2018imagenette\u2019. URL: https://github.com/fastai/imagenette/   \nIoffe, S. and Szegedy, C. (2015), Batch normalization: Accelerating deep network training by reducing internal covariate shift, in \u2018International conference on machine learning\u2019, pmlr, pp. 448\u2013456.   \nIsidori, A. (1995), Nonlinear control systems, third edn, Springer.   \nIvgi, M., Hinder, O. and Carmon, Y. (2023), DoG is SGD\u2019s best friend: A parameter-free dynamic step size schedule, in \u2018International Conference on Machine Learning\u2019, PMLR, pp. 14465\u201314499.   \nJastrz\u02dbebski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio, Y. and Storkey, A. (2017), \u2018Three factors influencing minima in SGD\u2019, arXiv preprint arXiv:1711.04623 .   \nJastrze\u02dbbski, S., Kenton, Z., Ballas, N., Fischer, A., Bengio, Y. and Storkey, A. (2019), On the relation between the sharpest directions of DNN loss and the SGD step length, in \u2018International Conference on Learning Representations\u2019.   \nJastrzebski, S., Szymczak, M., Fort, S., Arpit, D., Tabor, J., Cho, K. and Geras, K. (2020), The break-even point on optimization trajectories of deep neural networks, in \u2018International Conference on Learning Representations\u2019.   \nKalra, D. S., He, T. and Barkeshli, M. (2023), \u2018Universal sharpness dynamics in neural network training: Fixed point analysis, edge of stability, and route to chaos\u2019, arXiv preprint arXiv:2311.02076   \nKarpathy, A. (2015), \u2018The unreasonable effectiveness of recurrent neural networks\u2019, http:// karpathy.github.io/2015/05/21/rnn-effectiveness/.   \nKopitkov, D. and Indelman, V. (2020), Neural spectrum alignment: Empirical study, in \u2018International ", "page_idx": 11}, {"type": "text", "text": "Conference on Artificial Neural Networks\u2019, pp. 168\u2013179. ", "page_idx": 11}, {"type": "text", "text": "Krizhevsky, A., Hinton, G. et al. (2009), Learning multiple layers of features from tiny images, Technical report, University of Toronto, ON, Canada.   \nLeCun, Y., Cortes, C. and Burges, C. (2010), \u2018Mnist handwritten digit database\u2019, ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist 2.   \nLewkowycz, A., Bahri, Y., Dyer, E., Sohl-Dickstein, J. and Gur-Ari, G. (2020), \u2018The large learning rate phase of deep learning: the catapult mechanism\u2019, arXiv preprint arXiv:2003.02218 .   \nLi, H., Xu, Z., Taylor, G., Studer, C. and Goldstein, T. (2018), \u2018Visualizing the loss landscape of neural nets\u2019, Advances in neural information processing systems 31.   \nLiu, D. C. and Nocedal, J. (1989), \u2018On the limited memory bfgs method for large scale optimization\u2019, Mathematical programming .   \nLiu, H., Li, Z., Hall, D. L. W., Liang, P. and Ma, T. (2024), Sophia: A scalable stochastic second-order optimizer for language model pre-training, in \u2018International Conference on Learning Representations\u2019.   \nLiu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J. and Han, J. (2020), On the variance of the adaptive learning rate and beyond, in \u2018International Conference on Learning Representations\u2019.   \nLoizou, N., Vaswani, S., Laradji, I. H. and Lacoste-Julien, S. (2021), Stochastic Polyak step-size for SGD: An adaptive learning rate for fast convergence, in \u2018International Conference on Artificial Intelligence and Statistics\u2019, PMLR.   \nLoshchilov, I. and Hutter, F. (2016), SGDR: Stochastic gradient descent with warm restarts, in \u2018International Conference on Learning Representations\u2019.   \nMartens, J. and Grosse, R. (2015), Optimizing neural networks with Kronecker-factored approximate curvature, in \u2018International conference on machine learning\u2019, PMLR, pp. 2408\u20132417.   \nMishchenko, K. and Defazio, A. (2023), \u2018Prodigy: An expeditiously adaptive parameter-free learner\u2019, arXiv preprint arXiv:2306.06101 .   \nMutschler, M. and Zell, A. (2020), \u2018Parabolic approximation line search for DNNs\u2019, Advances in Neural Information Processing Systems 33, 5405\u20135416.   \nNesterov, Y. et al. (2018), Lectures on convex optimization, Vol. 137, Springer.   \nNocedal, J. and Wright, S. J. (1999), Numerical optimization, Springer.   \nNoci, L., Meterez, A., Hofmann, T. and Orvieto, A. (2024), \u2018Why do learning rates transfer? reconciling optimization and scaling limits for deep learning\u2019, arXiv preprint arXiv:2402.17457 .   \nPolyak, B. T. (1964), \u2018Some methods of speeding up the convergence of iteration methods\u2019, USSR computational mathematics and mathematical physics 4(5), 1\u201317.   \nRiedmiller, M. and Braun, H. (1992), Rprop: a fast adaptive learning algorithm, in \u2018Proc. of the Int. Symposium on Computer and Information Science VII\u2019.   \nRosca, M., Wu, Y., Qin, C. and Dherin, B. (2023), \u2018On a continuous time model of gradient descent dynamics and instability in deep learning\u2019, Transactions on Machine Learning Research .   \nRoulet, V., Agarwala, A. and Pedregosa, F. (2023), On the interplay between stepsize tuning and progressive sharpening, in \u2018OPT 2023: Optimization for Machine Learning\u2019.   \nSchaul, T., Zhang, S. and LeCun, Y. (2013), No more pesky learning rates, in \u2018International conference on machine learning\u2019, PMLR, pp. 343\u2013351.   \nShi, B., Su, W. and Jordan, M. I. (2023), \u2018On learning rates and Schr\u00f6dinger operators\u2019, Journal of Machine Learning Research 24(379), 1\u201353.   \nTolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., Yung, J., Steiner, A., Keysers, D., Uszkoreit, J. et al. (2021), \u2018MLP-mixer: An all-MLP architecture for vision\u2019, Advances in neural information processing systems 34, 24261\u201324272.   \nVaswani, S., Mishkin, A., Laradji, I., Schmidt, M., Gidel, G. and Lacoste-Julien, S. (2019), \u2018Painless stochastic gradient: Interpolation, line-search, and convergence rates\u2019, Advances in neural information processing systems .   \nWortsman, M., Liu, P. J., Xiao, L., Everett, K. E., Alemi, A. A., Adlam, B., Co-Reyes, J. D., Gur, I., Kumar, A., Novak, R., Pennington, J., Sohl-Dickstein, J., Xu, K., Lee, J., Gilmer, J. and Kornblith, S. (2024), Small-scale proxies for large-scale transformer training instabilities, in \u2018International Conference on Learning Representations\u2019.   \nWu, L., Ma, C. et al. (2018), \u2018How SGD selects the global minima in over-parameterized learning: A dynamical stability perspective\u2019, Advances in Neural Information Processing Systems 31.   \nWu, L. and Su, W. J. (2023), The implicit regularization of dynamical stability in stochastic gradient descent, in \u2018International Conference on Machine Learning\u2019, PMLR, pp. 37656\u201337684. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Theoretical Model ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Intuition for model of curvature dynamics ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Section 2.3, we extend the model from Damian et al. (2023) in order to understand the curvature dynamics. Here we provide some intuition for the basic structure of the model. ", "page_idx": 14}, {"type": "text", "text": "The analysis in Damian et al. (2023) is based on a 3rd order expansion of the loss function. A third order approximation of $f$ around some $w_{0}$ reads ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{f(w)\\approx f(w_{0})+\\nabla f(w_{0})\\cdot(w-w_{0})+\\frac{1}{2}(w-w_{0})^{\\top}\\nabla^{2}f(w_{0})(w-w_{0})}}}\\\\ {{\\displaystyle{\\qquad+\\,\\frac{1}{6}\\nabla^{3}f(w_{0})[w-w_{0},w-w_{0},w-w_{0}]}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Assuming without loss of generality that $w_{0}=0$ , $f(w_{0})=0$ , we can write: ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(w)\\approx\\nabla f(w_{0})\\cdot w+\\frac{1}{2}w^{\\top}\\nabla^{2}f(w_{0})w+\\frac{1}{6}\\nabla^{3}f(w_{0})[w,w,w]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Consider the gradient descent dynamics with learning rate $\\eta$ under this model. We can write: ", "page_idx": 14}, {"type": "equation", "text": "$$\nw_{t+1}-w_{t}\\approx-\\eta(\\nabla f(w_{0})+H(w_{t})w_{t})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{H(w_{t})\\equiv\\nabla^{2}f(w_{0})+\\frac{1}{2}\\nabla^{3}f(w_{0})[w_{t},\\cdot,\\cdot]}\\end{array}$ is the Hessian at the current parameter $w_{t}$ . ", "page_idx": 14}, {"type": "text", "text": "Let $v$ be the direction of the largest eigenvalue of $H(w_{0})$ . For small enough $w_{t}\\mathrm{~-~}w_{0}$ , this is a good approximation of the largest eigendirection of $H(w_{t})$ . Consider the dynamics of the projection $x_{t}\\equiv v\\cdot w_{t}$ . We make the additional assumption that $\\boldsymbol{v}\\cdot\\dot{\\boldsymbol{\\nabla}}f(\\boldsymbol{w}_{0})=0$ (which can be achieved with a coordinate transformation). We then get ", "page_idx": 14}, {"type": "equation", "text": "$$\nx_{t+1}\\approx(1-\\eta\\lambda_{t})x_{t}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\lambda_{t}:=\\lambda(w_{t})\\,:=\\,\\lambda_{\\operatorname*{max}}H(w_{t})$ is the largest eigenvalue of the current Hessian $H(w_{t})$ . The dynamics of $\\lambda_{t}$ are generally slower than the dynamics of $w_{t}$ (and $x_{t}$ ), and are governed approximately by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\lambda_{t+1}-\\lambda_{t}\\approx(\\nabla\\lambda(w_{t}))\\cdot(w_{t+1}-w_{t})=-\\eta(\\nabla\\lambda(w_{t}))\\cdot\\nabla f(w_{t})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This gradient is given by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla\\lambda(w_{t})=\\nabla(v^{\\top}H(w_{t})v)\\approx\\nabla^{3}f(w_{0})[v,v,\\cdot].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For very small $x_{t}$ , it has been observed (see e.g. Cohen et al. (2021)) that $\\lambda_{t+1}-\\lambda_{t}$ is increasing during most of training. Early in this regime $x_{t}$ is often very close to 0 (that is, the gradient has small component in the $v$ direction). Therefore we are interested in the contribution of the gradient orthogonal to $v$ , $\\nabla f(w_{t})_{\\perp}\\;\\equiv\\;(I\\,-\\,v v^{\\mathrm{T}})\\nabla f(w_{t})$ , to the dynamics. We define $a_{t}\\ \\equiv$ $\\bar{-}(\\nabla\\lambda(w_{t})\\!\\cdot\\!\\nabla\\bar{f}(w_{t})_{\\perp})$ as the instantaneous change in the top eigenvalue, due to gradient contributions orthogonal to $v$ . We assume that this contribution is positive and, to simplify, independent of $t$ , that is, $a_{t}:=a>0$ . ", "page_idx": 14}, {"type": "text", "text": "From (13), while $\\lambda_{t}<2/\\eta,x_{t}$ is decreasing and from (14) $\\lambda_{t}$ is increasing. ", "page_idx": 14}, {"type": "text", "text": "Once $\\lambda_{t}>2/\\eta$ , the dynamics of $x_{t}$ changes from convergence towards 0 to increasing values with alternating sign. We can write down the contribution to the gradient from $x_{t}$ using our third order expansion of the loss. We have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla_{w}f(w_{0}+x_{t}v)\\approx\\nabla f(w_{0})+x_{t}H(w_{t})v+{\\frac{1}{2}}x_{t}^{2}\\nabla^{3}f(w_{0})[v,v,\\cdot]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We can now understand how these terms contribute to the dynamics of $\\lambda_{t}$ . The first term contributes to sharpening (increasing $\\lambda_{t}$ ) via the constant $a$ . The second term has oscillating sign and its long term contribution to $\\lambda_{t}$ is small. From Equation 13 we know that ", "page_idx": 14}, {"type": "equation", "text": "$$\nx_{t+1}+x_{t}\\approx(2-\\eta\\lambda_{t})x_{t}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Near the edge of stability, $\\eta\\lambda_{t}$ is close to 2, and the sum of successive $x_{t}$ are small; therefore we neglect the linear $x_{t}$ term in Equation 16. ", "page_idx": 14}, {"type": "image", "img_path": "SEflLHIhhJ/tmp/ef245ed196b5fca7411b8359acd07b378507df44a772daffd2666b19e20fbea9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 10: Dynamics of the largest eigenvalue projection $p$ . For constant learning rate, $p$ cycles between stability and instability and $\\lambda_{\\mathrm{max}}$ stabilizes (blue). If learning rate tuner sets $\\eta_{t}=2(1-\\epsilon)/\\lambda_{\\operatorname*{max},t},p$ decays to 0 and there is no negative feedback preventing sharpening. ", "page_idx": 15}, {"type": "image", "img_path": "SEflLHIhhJ/tmp/8b790b2430c2afd3ce2b83d89ab0b6302a08bbb2da2281f042f133e6dcb1c05f.jpg", "img_caption": ["Figure 11: Dynamics of $p$ for toy model of CDAT. For $\\sigma=2$ , $p$ is stable, which induces stabilization of sharpness (orange). For $\\sigma=2-\\epsilon$ for small $\\epsilon$ (blue, $\\epsilon=0.1$ ), $p$ decreases but the ratio of $p$ to the orthogonal projection remains constant which stabilizes $y$ near 0. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "The sign of the $\\scriptstyle x_{t}^{2}$ term is constant and there are no such issues. Therefore, in the non-linear regime the dynamics of $\\lambda_{t}$ can be approximated by: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\lambda_{t+1}-\\lambda_{t}\\approx\\eta a-\\frac{\\eta}{2}x_{t}^{2}\\left(\\nabla^{3}f(w_{0})[v,v,\\cdot]\\cdot\\nabla^{3}f(w_{0})[v,v,\\cdot]\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If we define $\\begin{array}{r}{b\\equiv\\frac{1}{2}\\left(\\nabla^{3}f(w_{0})[v,v,\\cdot]\\cdot\\nabla^{3}f(w_{0})[v,v,\\cdot]\\right)}\\end{array}$ (guaranteed to be positive), then we arrive at the dynamical equations presented in Equation 4: ", "page_idx": 15}, {"type": "equation", "text": "$$\nx_{t+1}-x_{t}=(2-\\eta\\lambda_{t})x_{t},\\;\\lambda_{t+1}-\\lambda_{t}=\\eta(a-b x_{t}^{2})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This system leads to a quasi-stable oscillation of $\\lambda_{t}$ around $2/\\eta$ ; when $\\lambda_{t}<2/\\eta$ , $x_{t}$ goes to $0$ , and eventually $\\lambda_{t}$ increases; when $\\lambda_{t}>2/\\eta$ , $x_{t}$ increases in magnitude, eventually leading to a decrease in $\\lambda_{t}$ . ", "page_idx": 15}, {"type": "text", "text": "This heuristic argument is made rigorous in the appropriate regimes in Damian et al. (2023); there they compute the difference between the full dynamics and the dynamics with the gradient projected away from the largest eigendirection using a coupling argument. The key point is that higher order contributions to the gradient are guaranteed to be anti-aligned with the gradient of the large Hessian eigenvalues. This leads to a restoring force which counteracts eigenvalue growth (progressive sharpening) if the largest eigenmode becomes unstable. ", "page_idx": 15}, {"type": "text", "text": "A.2 Dynamics of rescaled variables ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Section 2.3, we used Equation 4 to derive equations in $p:=x^{2}$ and $y:=\\eta\\lambda-2$ . We arrived at ", "page_idx": 15}, {"type": "equation", "text": "$$\np_{t+1}=(1+y_{t})^{2}p_{t},\\;y_{t+1}=\\eta_{t+1}\\left[\\eta_{t}\\left(a-b p_{t}\\right)\\right]+\\left({\\frac{\\eta_{t+1}}{\\eta_{t}}}\\right)y_{t}+2\\left[{\\frac{\\eta_{t+1}}{\\eta_{t}}}-1\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The dynamical equation for $p$ is obtained by squaring the equation for $x_{t}$ in Equation 4. To derive the equation for $y_{t}$ , we first derive the dynamics of $\\eta_{t}\\lambda_{t+1}$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\eta_{t}\\lambda_{t+1}=\\eta_{t}\\left[\\eta_{t}\\left(a-b p_{t}\\right)\\right]+\\eta_{t}\\lambda_{t}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We then have ", "page_idx": 15}, {"type": "equation", "text": "$$\ny_{t+1}=\\frac{\\eta_{t+1}}{\\eta_{t}}[\\eta_{t}\\lambda_{t+1}-2]+2.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Evaluating completes Equation 5. ", "page_idx": 15}, {"type": "text", "text": "A.3 Dynamics of the projection on the largest eigenvector ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We present results on the dynamics of $p$ in the various models; these were omitted from the main text in order to simplify the presentation. For constant learning rate, $p$ initially decreases until EOS is crossed, after which it enters into a cycle of increase and decrease (Figure 10, blue). For our model of linesearches, where $\\eta_{t}\\,=\\,2(1-\\epsilon)/\\lambda_{\\mathrm{max},t},$ , $p$ decays to 0 quickly and there is no mediation of sharpening (orange, $\\epsilon=0.1$ ). ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "For our model of CDAT presented in Section 3.2, $p$ stabilizes for $\\sigma=2$ (Figure 11, orange). For $\\sigma<2$ , the model still predicts decay of $p$ , but the ratio of $p_{t}$ to the orthgonal component $g_{t}$ remains constant (Figure 11, blue). This fixed ratio stabilizes $y$ to a value near 0. ", "page_idx": 16}, {"type": "text", "text": "In practice, the higher order terms in the dynamics provide additional stability, in the on-edge model, which allows $p$ to stabilize as well, see Fig. 12, Fig. 17. The key is that these terms can operate when $y$ is close to 0 for long periods of time. These results suggest that additional model development is required to understand the behavior of learning rate tuners which target the EOS. ", "page_idx": 16}, {"type": "text", "text": "B Additional Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Further analyzes of base learning rate tuners ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Fig. 12 completes Fig. 3 with measures of sharpening and learning rates on the settings considered in Fig. 1. For RMSProp we considered the preconditioned Hessian following the observations done by Cohen et al. (2023) that for adaptive gradient methods such as RMSProp or Adam, the sharpness of the preconditioned Hessian, rather than the sharpness of the Hessian, defines the edge of stability. Namely, recall that RMSProp takes updates of the form ", "page_idx": 16}, {"type": "equation", "text": "$$\nw_{t+1}=w_{t}-P_{t}^{-1}\\nabla f(w_{t}),\\quad\\mathrm{for}P_{t}=\\mathrm{diag}(\\sqrt{\\nu_{t}+\\varepsilon})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nu_{t}=(1-\\beta_{2})g_{t}^{2}+\\beta_{2}\\nu_{t-1},\\quad\\nu_{-1}=0,\\;g_{t}=\\nabla f(w_{t}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $\\beta_{2}$ an exponential moving average parameter. The preconditioned Hessian takes then the form ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{H}_{t}=P_{t}^{-1/2}\\nabla^{2}f(w_{t})P_{t}^{-1/2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and we report $\\lambda_{\\operatorname*{max}}(\\tilde{H}_{t})$ ", "page_idx": 16}, {"type": "text", "text": "We observe similar behaviors in these regimes as in Fig. 3. Namely, the sharpness or preconditioned sharpness ever increase ( $2^{\\mathrm{nd}}$ panels of Fig. 12), while the learning rates ever decrease $\\mathrm{\\ddot{~}1^{s t}}$ panels of Fig. 12). The constant learning counterpart can operate above the edge of stability while the self-tuned methods generally avoid crossing the edge of stability $3^{\\mathrm{rd}}$ panels of Fig. 12). ", "page_idx": 16}, {"type": "text", "text": "B.2 Analyzing additional learning rate tuners ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We consider the performance of two additional classical learning rate tuners, Polyak stepsize (Berrada et al., 2020; Loizou et al., 2021; Polyak, 1964) and hyper-gradient descent (Almeida et al., 1999; Baydin et al., 2018) akin to the resilient backpropagation scheme (Riedmiller and Braun, 1992). ", "page_idx": 16}, {"type": "text", "text": "Briefly, Polyak stepsizes consider setting the learning rate as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\eta_{t}=\\operatorname*{min}\\left\\{\\frac{f(w_{t})-f^{\\star}}{\\|\\nabla f(w_{t})\\|^{2}},\\eta_{\\operatorname*{max}}\\right\\},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $f^{\\star}=\\operatorname*{min}_{w}f(w)$ is the minimum of the objective set to 0 by assuming that a neural network can overfti the data, and $\\eta_{\\mathrm{max}}$ is a maximal stepsize selected as 1 or 100 in our experiments (we take the best instance). ", "page_idx": 16}, {"type": "text", "text": "Hyper-gradient descent considers updating the stepsize towards maximal decrease of the objective. Namely, defining the objective obtained after one step $h_{t}(\\eta)=f(w_{t}+\\eta u_{t})$ , the algorithm updates $\\eta_{t}$ by a gradient step on $h_{t}$ resulting a priori in $\\eta_{t+1}\\,=\\,\\eta_{t}\\,-\\,\\alpha\\nabla f(w_{t}\\,+\\,\\eta_{t}u_{t})^{\\top}u_{t}$ for a given hyper-learning rate $\\alpha$ . Almeida et al. (1999); Baydin et al. (2018) argued for using multiplicative updates of the form ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\eta_{t+1}=\\eta_{t}\\left(1-\\beta\\frac{\\nabla f(w_{t}+\\eta_{t}u_{t})^{\\top}u_{t}}{\\|\\nabla f(w_{t}+\\eta_{t}u_{t})\\|_{2}\\|u_{t}\\|_{2}}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Intuitively, the learning rate increases if the update is aligned with the negative gradient direction and decreases otherwise. Resilient backpropagation (Riedmiller and Braun, 1992) adopts a similar logic componentwise. In our experiments we vary $\\beta$ and select the best instance, see Appendix C.5 for more details. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "We observe that Polyak stepsizes (top figure of Fig. 13) generally select larger learning rates than the constant learning rate counterpart. The efficiency of Polyak stepsizes is not reached by the CDAT rule with $\\sigma=2$ but with a slightly larger scale $\\sigma=2.06$ . The efficiency of the Polyak stepsize method, in particular compared to a simple linesearch, in a full batch regime, has also been reported by Roulet et al. (2023). The proposed CDAT rule may capture the beneftis of aggressive learning rates taken by Polyak stepsizes in a smoother way by allowing various scales. ", "page_idx": 17}, {"type": "text", "text": "On the other hand, the hyper-gradient descent performs just on par with the fine-tuned constant learning rate counterpart (bottom figure of Fig. 13). We also observe a slow, yet steady, progressive sharpening when using the hyper-gradient descent. As with the linesearch method or the quadratically greedy rule, the hyper-gradient descent focuses on selecting a learning rate that decreases the loss, which appears, across those tuners, to potentially suppress effective stabilization effects naturally appearing with constant learning rate. ", "page_idx": 17}, {"type": "text", "text": "B.3 Base learning rate tuners in a stochastic regime ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Fig. 14, we report the performance of classical learning rate tuners (linesearch or quadratically greedy method) in a stochastic regime for varying batch-sizes. As observed previously by Vaswani et al. (2019) or Roulet et al. (2023), a linesearch for example can perform well in a stochastic regime. Note that the two approaches (linesearch and quadratically greedy method) display similar behaviors (just as they displayed similar behaviors in the full batch regime). This hints that, rather than playing with the numerous hyperparameters of a linesearch we may focus simply on an additional scaling factor for the quadratically greedy rule, which motivated the proposed CDAT rule. ", "page_idx": 17}, {"type": "text", "text": "B.4 Targeting the edge of stability using the exact sharpness ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Cohen et al. (2021, Appendix F) reported bad performance of adaptive learning rate tuners selecting the stepsize as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\eta=2/\\lambda_{\\operatorname*{max}}(\\nabla^{2}f(w))\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which may fix the learning rate just at the edge of stability. Note that such a definition does not take into account the additional alignment of the update with the largest eigenvector. Our proposed diagnostic rule CDAT rather considers the edge of stability given by a local approximation of the objective along the update so to take into account the alignment of the update with the largest eigenvector of the Hessian. We ran experiments with a rule ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\eta=\\sigma/\\lambda_{\\operatorname*{max}}(\\nabla^{2}f(w))\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "that lets the scaling factor vary just as done with CDAT. The only difference is in the estimation of the base estimate of the edge of stability (CDAT does it with the help of a quadratic approximation of the objective, while the rule (25) uses an exact computation of the sharpness). In Fig. 15, we observe that setting the scale $\\sigma\\approx2$ leads to poor performance as previously observed by Cohen et al. (2021). Note however that by setting the scaling much above 2 (like $\\sigma=3.$ ) such a rule may outperform a constant learning rate. This hints that the rule (25) misses the alignment of the update with the largest eigenvector, which motivated the CDAT rule. ", "page_idx": 17}, {"type": "text", "text": "B.5 Analyzing instantaneous gains versus long-term gains ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Fig. 16, we investigate the difference of instantaneous decrease using the quadratically greedy rule (CDAT with $\\sigma=1$ ) compared to the on edge rule (CDAT with $\\sigma=2$ ). Throughout a training, the quadratically rule ensures a larger instantaneous decrease as intended through its definition as a learning rate that minimizes the loss. Yet, in the long term, the quadratically greedy rule underperforms the on edge rule (Fig. 5). ", "page_idx": 17}, {"type": "text", "text": "B.6 Additional metrics for the CDAT rule ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Fig. 17, we additionally measure the alignment of the updates with the largest eigenvector and the angle between successive updates. We observe that the CDAT rule for $\\sigma\\approx2$ behaves similarly as the constant learning rate counterpart. In particular, the updates tend to quickly be in opposed directions. The quadratically greedy rule does not demonstrate such a behavior. ", "page_idx": 17}, {"type": "text", "text": "B.7 Sensibility analysis to architecture hyperparameters ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Fig. 18, we study CDAT for simple MLPs in a full batch regime on the MNIST dataset. Our goal is to understand the beneftis of the proposed CDAT rule for varying hyperparameters. First, we analyze the sensibility to width and depth of an MLP in a similar fashion as Cohen et al. (2021, Appendix D) did to analyze progressive sharpening. ", "page_idx": 18}, {"type": "text", "text": "We observe that accrued gains can be obtained with the CDAT rule for larger widths (top left panel of Fig. 18). Note that Cohen et al. (2021, Appendix D) found less sharpening at higher widths. In terms of depth (top right panel of Fig. 18), the CDAT rule works best with larger depths while we note a slight shift of optimal scaling factors from 2 to slightly below 2. ", "page_idx": 18}, {"type": "text", "text": "The CDAT rule appears to work best with small or no weight decay (bottom left panel of Fig. 18) while its benefits fade with larger weight decay (no difference between greedy $\\sigma=1$ and on edge $\\sigma=2$ ). Finally, while the method naturally finds smaller train losses with larger subsets of data, we do not observe a significant shift of relative performance between scales as the size of the data increases (bottom right panel of Fig. 18). ", "page_idx": 18}, {"type": "text", "text": "B.8 CDAT rule versus prefixed schedule in full batch regime ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Fig. 19, we compare the proposed CDAT rule with prefixed schedules in a full batch regime. We observe that while placing the optimizer on edge could improve on constant learning rate counterparts, prefixed schedules can outperform the CDAT rule. This points out that the feedback loop exploited by CDAT may miss some additional nonlinear effects that could further enhance self-tuning rules. ", "page_idx": 18}, {"type": "text", "text": "B.9 Detailed performances of CDAT in stochastic regime ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Fig. 20 and Fig. 21, we detail the performances of the CDAT rule in the stochastic regime for varying batch sizes. In the stochastic regime, recall that the heatmap of the performance of CDAT in terms of batch-size heavily depended on the appropriate scaling factor Fig. 7 (in comparison a scaling factor of approximately $\\sigma\\,=\\,2$ appeared generally good in the full batch regime). In both Fig. 20 and Fig. 21, we observe that the method may generally work better at larger batch sizes. Understanding better the right statistics to estimate as well as appropriate estimators of the edge of stability in a stochastic regime is a future direction. ", "page_idx": 18}, {"type": "image", "img_path": "SEflLHIhhJ/tmp/b8f80f6c1a4bdcc3f273d4333527d949e85571c5d8310e4244b10418821d3897.jpg", "img_caption": ["Figure 12: Learning rates and sharpness dynamics of baseline learning rate tuners. Learning rate, Hessian or preconditioned Hessian sharpness, their product, and the alignment between the update and the largest eigenvector of the Hessian or the preconditioned Hessian. As in Fig. 12, we observe that a linesearch (1) or a quadratically greedy (3) learning rate tuner display decreasing learning rates along training. The sharpness of the Hessian (for GD) or preconditioned Hessian (for RMSProp) keep increasing for the self-tuned baselines while they stabilize for the constant learning rate counterparts. The self-tuned methods perform generally below the edge of stability or at least much less above than the constant learning rate counterpart. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "SEflLHIhhJ/tmp/a4d3f50cdae66ab93451d0e33bf5d34fd54922628875e989d22f0a7df57220b5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 13: Analyzing additional learning rate tuners. Train loss, learning rate, sharpness, and their product along training with gradient descent using a constant or self-tuned learning rate with various tuners. Polyak stepizes (23) are effective in a full batch regime (top figure) outperforming CDAT on edge $(\\sigma=2)$ ). The effectiveness of the Polyak stepsizes are partially captured by an aggressive CDAT rule placing the optimizer on edge $\\sigma=2.06$ . On the other hand, a hyper-gradient descent (24) performs just on par with the constant learning rate counterpart in this regime. It also displays an ever-increasing sharpening akin to the one observed for a linesearch or the quadratically greedy rule Fig. 3. ", "page_idx": 20}, {"type": "image", "img_path": "SEflLHIhhJ/tmp/52d8981ffae721ba6ed02ff6fe8d78cfe76af96e954a8a06750a546950bf3cc6.jpg", "img_caption": ["Figure 14: Classical learning rate tuners can work well in stochastic regime. In a stochastic regime, we observe that, e.g., a linesearch can perform on par or even better than the constant learning counterparts. However, this good performance is not explained by the common belief that linesearches work well in a full batch regime (Fig. 1). The linesearch and quadratically greedy rule perform similarly in this setting. ", ""], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "SEflLHIhhJ/tmp/f1411e6d9361afe950a98b17e98d374c372d50b398696e0d66ce0b9c42070229.jpg", "img_caption": ["Figure 15: On edge with exact sharpness. We consider a rule similar to CDAT (8) but using the exact sharpness, that is the largest eigenvalue of the Hessian, as a base learning rate while varying an additional scale factor (see (25)). By using the exact sharpness, a scaling factor of $\\sigma=2$ leads now to poor performance, while a scaling factor of $\\sigma=3$ is performant. By using the exact sharpness (25) we do not take into account the actual alignment of the update with the largest eigenvector which may explain the shift of optimal scaling factors in this case. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "SEflLHIhhJ/tmp/25753b799adfa02cceabe00d1df7361dab699813a39a17e671b1a5467a90f104.jpg", "img_caption": ["Figure 16: The quadratically greedy rule ensures larger instantaneous decrease of the loss. Train losses and difference in loss between one step using the on-edge rule (CDAT, scale $=\\!2$ ) and one step using quadratically greedy rule (CDAT, scale $^{-1}$ ). Results averaged over 10 initializations and disjoint 4096-sample subsets of CIFAR100. MLP architecture: single hidden layer of size 1024, ReLU activations, trained with GD and cross-entropy loss in a full batch setting. We plot $f(w_{t}+\\eta^{\\mathrm{oe}}u_{t})-f(w_{t}+\\eta^{\\mathrm{qg}}u_{t})$ along a training performed either with the quadratically greedy rule $\\mathit{\\Theta}\\left(\\sigma\\mathrm{\\Omega}=1\\right)$ ) or the on-edge rule $\\mathit{\\Theta}\\left(\\sigma\\mathrm{\\Sigma}=2\\right)$ ). In both cases, this difference is positive meaning that the quadratically greedy rule ensures a larger instantaneous decrease of the loss. Yet the quadratically greedy rule underperforms in the long term (see Fig. 5, also holds in this full batch setting). "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "SEflLHIhhJ/tmp/fecc644aa9efccf02aa680a2391fcd3680055b86c7d1902d874cd1fdd434661f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 17: Further analysis of the CDAT rule. This is the same setting as in Fig. 6 except that we plot the alignment between the largest eigenvector of the Hessian and the update, and the angle between successive updates. We observe that the CDAT rule for $\\sigma\\approx2$ behaves similarly as the constant learning rate counterpart. In particular, the updates tend to quickly be in opposed directions. The quadratically greedy rule does not demonstrate such a behavior. ", "page_idx": 22}, {"type": "image", "img_path": "SEflLHIhhJ/tmp/cd9265ca51d0095ca852ceb600c473ef299d684c1bd9cca3cab274f739d5e1e4.jpg", "img_caption": ["Figure 18: Improvements of CDAT rule on edge for varying hyperparameters. Varying width, depth, weight decay and size of the subset considered when using the CDAT rule with varying scaling factors. We observe that accrued gains can be obtained with the CDAT rule for larger widths (top left panel). In terms of depth (top right panel), the CDAT rule works best with larger depths, while we note there a slight shift of optimal scaling factors from 2 to slightly below 2. The CDAT rule appears to work best with small or no weight decay (bottom left panel) while its benefits fade with larger weight decay (no difference between greedy $\\sigma=1$ and on edge $\\sigma=2$ ). Finally, while the method naturally finds smaller train losses with larger subsets of data, we do not observe a significant shift of relative performance between scales as the size of the data increases (bottom right panel). "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "SEflLHIhhJ/tmp/6e2b78ba30d01cec03b9e442e231cc3b5ef632d08740361249bf8ec98aa043f4.jpg", "img_caption": ["Figure 19: CDAT rule may not fully capture the benefits of pre-fixed schedules. Train loss and learning rate behaviors for fine-tuned optimizers with or without schedules vs self-tuned counterparts with CDAT on various architecture, datasets, losses in a full batch regime. While the CDAT rule displays a behavior to warmup schedules, it does not completely catch the benefits of pre-fixed schedules. Note for the top-left part that the performance of the pre-fixed schedule is akin to the performance reported with CDAT $\\sigma=2.5$ at early times suggesting that a varying scaling factor, or taking higher order dynamics may be important to fully capture the benefits of warm-up schedules. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "SEflLHIhhJ/tmp/4690f488cb1aaca72e30f358470f12bf9053a779411ab9dd2b630089720b198e.jpg", "img_caption": ["Figure 20: Performance of fine-tuned algorithms in stochastic regime with SGD Momentum. In the stochastic regime with SGD momentum, we observe that the CDAT rule may outperform the constant learning rate counterpart (particularly for large batch sizes) while performing on par or underperforming the scheduled learning rate counterparts. Interestingly, a warm-up phase appears naturally induced by the CDAT rule (right plots). ", "GD Mom Scheduled GD Mom GD Mom CDAT "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "SEflLHIhhJ/tmp/a3cd3e4dd1c132e119a3be106185cd1a522859d4dcedfa8ebecf6abcc67cc75c.jpg", "img_caption": ["Adam Scheduled Adam Adam CDAT "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 21: Performance of fine-tuned algorithms in stochastic regime with Adam. In the stochastic regime with Adam we observe varying performance of the CDAT rule, generally on par or underperforming the baselines. Several factors can explain the underperformance. The scaling factor is not well understood, and a finer grid search could improve the scheme. Similarly, a better estimate or a better understanding of the edge of stability in a stochastic regime could improve the approach. Tackling curvature dynamics by analyzing feedback effects as in the CDAT rule may help such design. ", "page_idx": 25}, {"type": "text", "text": "C Experimental Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "C.1 Datasets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "MNIST. MNIST is an image classification dataset of handwritten digits (LeCun et al., 2010). We normalized the pictures so that each pixel is between 0 and 1. We did not standardize the data. We only used this dataset to test varying MLP architectures in Fig. 18. See Appendix C.5 for any additional relevant details. ", "page_idx": 26}, {"type": "text", "text": "CIFAR10. CIFAR10 is an image classification dataset of colored images of size $32\\times32$ with 10 classes (Krizhevsky et al., 2009). We normalized the pictures so that each pixel is between 0 and 1. We did not standardize the data. In the full batch regime, we considered a subset of 4096 samples. In the mini batch regime, we considered the full dataset of 50, 000 samples for training (dropping out the remainder batch) and tested on the 10, 000 validation samples. ", "page_idx": 26}, {"type": "text", "text": "Tiny Shakespeare. This consists in 40, 000 lines of Shakespeare from a variety of Shakespeare\u2019s plays (Karpathy, 2015). The task consists in a character-level prediction task among 64 characters. In the full batch regime, we considered a subset of 2048 samples consisting of blocks of 64 characters. ", "page_idx": 26}, {"type": "text", "text": "Imagenet. Imagenet is an image classification dataset of various images from the web (Deng et al., 2009). The images have various sizes. The original Imagenet-1K dataset contains 1000 classes. For the full batch experiments, we consider the Imagenette (Howard, 2019) subset that consists in only 10 classes and took 1024 samples out of it. We consider the usual prepreocessing for Imagenet as detailed in the Scenic library (Dehghani et al., 2022). Namely, for training we consider random cropping and random filp at training. For testing, we center crop the images. Each time the cropping reduces the colored images to a $224\\times224$ size. In the mini-batch regime we consider the complete training dataset of 1.2 million images (Imagenet-1K), dropped the remainder batch, and reported test error on the 50, 000 validation images. ", "page_idx": 26}, {"type": "text", "text": "C.2 Architectures ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Residual Network (ResNet). We considered the standard ResNet architectures (ResNet34, ResNet50) of He et al. (2016) as implemented in the Scenic library (Dehghani et al., 2022). For the examples with ResNet34 we removed the batch normalization layers (Ioffe and Szegedy, 2015). For the examples with ResNet50 we replace the batch normalization layers with layer normalization layers (Ba et al., 2016). ", "page_idx": 26}, {"type": "text", "text": "Multi-Layer Perceptron (MLP) Mixer. We consider the standard MLP Mixer architectures (Tolstikhin et al., 2021) as implemented in the Scenic library (Dehghani et al., 2022). By Mixer Ti/8, we mean the tiny model of Mixer provided in the Scenic library (see https://github.com/google-research/scenic/blob/main/scenic/projects/ baselines/configs/imagenet/imagenet_augreg_mixer_config.py) with patches of size $8\\times8$ . We removed dropout (both layer and depth wise). ", "page_idx": 26}, {"type": "text", "text": "Nano Language Model (NanoLM). We consider a simpel sequence-to-sequence Transformer model implemented in Optax (https://github.com/google-deepmind/optax/blob/main/ examples/nanolm.ipynb). The model consists of 6 stacked transformer blocks, each of which contains a multi-head attention layer followed by a feed-forward layer. Layer normalization is used used within the transformer blocks to improve training stability. Finally, a dense layer maps the model\u2019s output to the vocabulary size, producing probabilities for each character as the next potential character. The only difference with respect to the previous code is that we removed dropout (for deterministic training) and, as mentioned in the previous subsection, reduced the size of the dataset to be able to estimate the sharpness. ", "page_idx": 26}, {"type": "text", "text": "Vision Transformer (ViT). We consider the standard Vision Transformer architecture (Dosovitskiy et al., 2021) as implemented in the Scenic library (Dehghani et al., 2022). By ViT Ti/32, we mean the tiny model of Vision Transformer provided in the Scenic library (see https://github.com/google-research/scenic/blob/main/scenic/projects/ ", "page_idx": 26}, {"type": "text", "text": "Weight decay. In all examples, except mentioned otherwise, we consider a fixed weight decay of $10^{-\\tilde{5}}$ . See Fig. 18 (bottom left panel) for an analysis of sensitivity of the proposed CDAT rule with varying weight decay. ", "page_idx": 27}, {"type": "text", "text": "C.3 Algorithms ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In all experiments, when selecting the \"best\" tuner we considered the average train loss over the last 5 iterates. ", "page_idx": 27}, {"type": "text", "text": "Fine-tuning base optimizers. The implementation of all optimizers are taken from the Optax library (DeepMind et al., 2020). In all experiments, we fix all hyperparameters of the base optimizer ((S)GD, (S)GD with Momentum, RMSProp, Adam) to their default values: 0.9 for the momentum of (S)GD with Momentum, 0.999 for the EMA parameter of the second moment of RMSProp and Adam, 0.9 for the EMA parameter of the first moment of Adam. We fine-tune the learning rate on a logarithmic base of 2 around a base learning rate such as $10^{-3}$ or $10^{-4}$ (depending on the algorithm, the architecture and the mini-batch size in the stochastic regime as detailed below in Appendix C.5), while making sure that the grid is sufficiently large such that the best learning rate is found inside the grid and not as the smallest or the largest. ", "page_idx": 27}, {"type": "text", "text": "For the scheduled versions of the base optimizers, we consider three shapes: linear warm-up followed by constant learning rate, linear warm-up followed by linear decay, linear warm-up followed by cosine decay. The number of iterations for the warm-up period is chosen as a fraction of the overall number of steps detailed in Appendix C.5. We also varied the horizon for the decaying schedules, see again Appendix C.5. ", "page_idx": 27}, {"type": "text", "text": "Implementation of the linesearch procedure. To implement the linesearch procedure described in Section 2, we consider the following criterion ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(w_{t}+\\eta_{t}^{\\mathrm{ls}}u_{t})\\leq(1+\\delta)f(w_{t})+c\\eta_{t}^{\\mathrm{ls}}\\nabla f(w_{t})^{\\top}u_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Compared to (1), we added a relative decrease hyperparmeter $\\delta$ as we observed that the linesearch can sometimes stay stuck at vanishing learning rates otherwise. ", "page_idx": 27}, {"type": "text", "text": "To find a valid criterion we consider a usual backtracking linesearch that starts from a guess $\\eta_{t,0}=$ $\\operatorname*{min}\\{c_{+}\\eta_{t-1}^{\\mathrm{ls}},1\\}$ . Choosing $c_{+}=+\\infty$ means that we start with an initial guess of 1 at each iteration. The learning rate is then decreased by a factor $c_{-}$ until the criterion is satisfied. Formally, the selected stepsize is then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\eta_{t}^{\\mathrm{ls}}=\\operatorname*{max}\\{\\eta_{t,k}=c_{-}^{k}\\eta_{t,0}:f(w_{t}+\\eta_{t,k}u_{t})\\leq(1+\\delta)f(w_{t})+c\\eta_{t,k}\\nabla f(w_{t})^{\\top}u_{t}\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We run the search until the criterion is satisfied in the full batch regime and for a maximum of 30 iterations in the mini-batch regime. In the experiments, we consider the following variations. ", "page_idx": 27}, {"type": "text", "text": "\u2022 c \u2208[0, 10\u22124, 0.5],   \n\u2022 $c_{+}\\in[4,+\\infty],$   \n\u2022 $c_{-}\\in[0.8,0.9]$ ,   \n\u2022 $\\delta\\in[0,1e-3]$ for $c=0$ and $\\delta=0$ , for $c\\in[10^{-4},0.5]$ . ", "page_idx": 27}, {"type": "text", "text": "Implementation of quadratically greedy tuner and CDAT. To implement the quadratically greedy tuner or CDAT, we compute the denominator $u^{\\top}\\nabla^{2}f(w)u$ as the second partial derivative of $f$ along $u$ , that is, ", "page_idx": 27}, {"type": "equation", "text": "$$\nu^{\\top}\\nabla^{2}f(w)u=\\partial^{2}f(w)[u,u]=\\partial(\\partial f(\\cdot)[u])(w)[u],\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\partial g(w)[u]$ amounts to a Jacobian vector product (jvp) computed with forward mode auto-diff in differentiable programming languages such as JAX (DeepMind et al., 2020). ", "page_idx": 27}, {"type": "text", "text": "Computing the denominator in the CDAT rule by forward mode automatic differentiation enables a much lower memory consumption than using Hessian vector products (see, e.g., (Blondel and Roulet, 2024, Chapter 8), (Dagr\u00e9ou et al., 2024) for more details). The computation of the denominator by applying twice forward mode automatic differentiation still incurs approximately three times the memory necessary to compute the objective (Blondel and Roulet, 2024, Chapter 8). The computational cost of computing the denominator is also approximately three times the computational cost of computing the objective. The above approximations are done with the following reasoning. The second partial derivative requires to follow the graph of computation but with three variables, one for the parameters, one for a copy of the update direction, one for another copy. At each node in the computation graph, the program computes the original computation, computes its first derivative along the first copy of the update direction, and computes the second derivative along the second copy. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "In practice, we observed for, e.g., the experiment on the full Imagenet dataset in mini-batch that the proposed CDAT rule required twice the wall time of the constant or scheduled learning rates counterparts. For this project, we considered CDAT as a diagnostic tool to understand the interplay between curvature and learning rate tuners. For future work, the cost of computing the approximate edge may be circumvented or amortized by using, e.g., parabolic approximations as done by Mutschler and Zell (2020), or by computing it at given intervals as done by Liu et al. (2024). ", "page_idx": 28}, {"type": "text", "text": "Further justification for the CDAT formula. In (8) we took the absolute value of the denominator to deal with concave approximations. Eigenvalue modifications in a Newton method are discussed by Nocedal and Wright (1999, Section 3.4). Taking the absolute value is one possible option. In practice, we observed positive curvatures along the update direction such that this choice did not matter. ", "page_idx": 28}, {"type": "text", "text": "C.4 Metrics implementation ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Sharpness estimation. We estimated the sharpness by a power iteration method run for 1000 iterations with an early stopping criterion defined by less than $10^{-3}$ relative accuracy. We accessed the Hessian by Hessian vector products, which limited the size of the full batch datasets considered on TPUs. The power iteration a priori returns the largest eigenvalue in magnitude $|\\lambda|_{\\mathrm{max}}$ and not necessarily the largest positive eigenvalue $\\lambda_{\\mathrm{max}}$ . But in practice the largest eigenvalue in magnitude is the largest eigenvalue, see, e.g., (Ghorbani et al., 2019) for an in-depth study of the spectrum of the Hessian along the iterations of deep learning. ", "page_idx": 28}, {"type": "text", "text": "C.5 Additional experimental details per figure ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We detail here any additional detail per figure not detailed in the summary above. ", "page_idx": 28}, {"type": "text", "text": "Fig. 1. The ResNet34 has no batch normalization layers. For the GD baseline on ResNet and the MLP Mixer the constant learning rate was tuned on a grid $\\left\\{10^{-3}\\cdot2^{i},i\\in\\{-1,\\ldots,7\\}\\right\\}$ . For the RMSPRop baseline on the NanoLM and ViT, the constant learning rate was tuned on a grid $\\{10^{-3}\\cdot2^{i},i\\in\\{-1,\\dots7\\}\\}$ and $\\left\\{10^{-5}\\cdot2^{i},i\\in\\left\\{0,\\ldots7\\right\\}\\right\\}$ respectively. ", "page_idx": 28}, {"type": "text", "text": "Fig. 2. We consider a linear classification (in other words using an MLP without hidden layers) on the subset of CIFAR10 detailed above. We search the constant stepsize of gradient descent in $\\left\\{10^{-3}\\cdot2^{i},i\\in\\{0,1,2\\}\\right\\}$ . The grid is centered around $1/\\|\\nabla^{2}f(w_{0})\\|_{2}$ , that is the optimal stepsize in a convex smooth setting. As for above experiments, the largest learning rate in the grid led to divergence. ", "page_idx": 28}, {"type": "text", "text": "Fig. 3. This is the same setting as in the first panel of Fig. 1. ", "page_idx": 28}, {"type": "text", "text": "Fig. 4. For constant learning rate, model settings were given by $a=3\\cdot10^{-2}$ , $b=3\\cdot10^{-1}$ , $\\eta=1$ .   \nFor fixed $y=-0.1$ training, $a=1$ , $b=0.5$ , $\\eta_{0}=1.0$ . ", "page_idx": 28}, {"type": "text", "text": "Fig. 5. We considered the same settings as in Fig. 1. The grid search for GD on ResNet34 and RMSProp on NanoLM are the same as in Fig. 1. For the MLPMixer, we fine-tuned GD with momentum on a grid $\\{10^{-2}\\cdot2^{i}\\,:\\,i\\,\\in\\,\\{1,\\dots,8\\}\\}$ . For the ViT, we fine-tuned Adam on a grid $\\{10^{-5}\\cdot2^{i}:i\\in\\{\\bar{1},\\cdot\\cdot\\cdot,8\\}\\}$ . ", "page_idx": 28}, {"type": "text", "text": "Fig. 6. This is exactly the same setting as in the first panel of Fig. 5. ", "page_idx": 28}, {"type": "text", "text": "Fig. 7. We considered the full dataset of CIFAR10 with ResNet50 with layer normalization in place of batch normalization. ", "page_idx": 29}, {"type": "text", "text": "Fig. 8. See the details given for Fig. 20 and Fig. 21. ", "page_idx": 29}, {"type": "text", "text": "Fig. 9. For both values of $\\sigma$ , $a=5\\cdot10^{-2}$ , $b=10^{-1}$ , $\\nu=0.1$ , $\\lambda_{0}=18$ , $\\eta_{0}=0.05$ , $g_{0}=p_{0}=4$ . ", "page_idx": 29}, {"type": "text", "text": "Fig. 10. Same settings as Figure Fig. 4. ", "page_idx": 29}, {"type": "text", "text": "Fig. 11. Same settings as Figure Fig. 9. ", "page_idx": 29}, {"type": "text", "text": "Fig. 12. This is the same setting as in Fig. 1. ", "page_idx": 29}, {"type": "text", "text": "Fig. 13. We consider the same setting as in Fig. 3. For the Polyak stepsizes (23), we let $\\eta_{\\mathrm{max}}$ vary between 1 and 100 and select the best. For the hypergradient descent, we let the hyper learning rate vary in $\\beta\\in\\left\\{10^{i},i\\in\\{-3,\\ldots,0\\}\\right\\}$ . ", "page_idx": 29}, {"type": "text", "text": "Fig. 14. We considered again the ResNet50 with layer normalization instead of batch normalization. For the constant learning rate baseline we searched over a grid of $\\{\\eta_{m}\\cdot2^{i},i\\in\\{-1,\\ldots,5\\}\\}$ , for $\\eta_{m}=10^{-4}\\cdot\\sqrt{m/4096}$ for $m$ the batch size. ", "page_idx": 29}, {"type": "text", "text": "Fig. 15. We considered a simple MLP with hidden sizes (256, 256, 256), ReLU activations. We tuned the constant learning rate baseline on $\\{10^{-3}\\cdot2^{i},i\\in\\{-1,7\\}\\}$ . ", "page_idx": 29}, {"type": "text", "text": "Fig. 16. Details are provided in the legend. ", "page_idx": 29}, {"type": "text", "text": "Fig. 17. This is the same setting as in Fig. 6. ", "page_idx": 29}, {"type": "text", "text": "Fig. 18. In this figure, the MLPs considered use ReLu activations. If not detailed, the weight decay is set to $10^{-5}$ and the subset considered is of size 8192. ", "page_idx": 29}, {"type": "text", "text": "Fig. 19. The settings are the same as in Fig. 5. For the constant learning rate baselines we searched on a gird $\\{\\eta_{\\mathrm{base}}\\cdot2^{i},i^{-}\\in\\{-3,\\ldots,9\\}\\}$ . The base learning rate $\\eta_{\\mathrm{base}}$ was chosen to be $10^{-3}$ for ResNet, $10^{-2}$ for the Mixer, $10^{-4}$ for the NanoLM and ViT. For the schedules\u2019 shapes, we searched over linear warm-up, linear warm-up with linear decay, linear warm-up with cosine decay. The initial and end learning rate were set to 0. The horizons for the schedules were chosen in $[\\dot{N},N/2,N/4]$ for $N=8192$ for the NanoLM, $N=16384$ for the ViT, Mixer and ResNet. The fraction of warm-up steps was searched in $\\{0.05,0.1,0.2\\}$ . ", "page_idx": 29}, {"type": "text", "text": "Fig. 20. For the constant learning rate baseline, we consider searching the best constant learning rate on a grid $\\{\\eta_{m}\\cdot2^{i},i\\in\\{-1,\\ldots,7\\}\\}$ for $\\eta_{m}=10^{-4}\\cdot\\sqrt{m/4096}$ where $m$ denotes the varying batch size. ", "page_idx": 29}, {"type": "text", "text": "For the scheduled baseline, we consider the variants presented above (linear warm-up followed by constant, linear warm-up followed by cosine decay, linear warm-up followed by linear decay) with varying fraction of warm-up steps $(0.05,0.1,0.2)$ and an initial learning rate of 0, a final learning rate of 0 for a fixed horizon of 512 epochs, and a peak learning rate searched over $\\{\\eta_{m}\\cdot4^{i},i\\in\\{4,\\cdot\\cdot\\cdot,9\\}\\}$ . ", "page_idx": 29}, {"type": "text", "text": "The scaling factor $\\sigma$ of CDAT was searched on a grid $\\{0.4,0.6,\\ldots,2.8\\}$ , and we also tuned the EMA parameter $\\beta_{\\mathrm{cdat}}$ in the computation of the numerators and denominators of the edge in $\\{0,0.9,0.99\\}$ . The best parameters found for CDAT can be inferred from Fig. 7. Namely, we found that non-zero EMA parameter for the estimation of the edge decay was essential for good performance and that the best scaling factor varied with the batch size. For example, at batch size 256 the best scaling factor is $\\sigma=1.8$ with $\\beta_{\\mathrm{cdat}}=0.9$ . ", "page_idx": 29}, {"type": "text", "text": "Fig. 21. For the constant learning rate baseline, we consider searching the best constant learning rate on a grid $\\{\\eta_{m}\\cdot2^{i},i\\in\\{-1,\\ldots,7\\}\\}$ for $\\eta_{m}=10^{-4}\\cdot\\sqrt{m/1024}$ where $m$ denotes the varying batch size. ", "page_idx": 29}, {"type": "text", "text": "For the scheduled baseline, we consider a linear warm-up followed by cosine decay, with a fraction of warm-up steps of 0.1 and an initial learning rate of 0, a final learning rate of 0 for a fixed horizon of 128 epochs, and a peak learning rate searched over $\\{\\eta_{m}\\cdot4^{i},i\\in\\{1,\\overline{{\\cdot}}\\cdot..\\,,5\\}\\}$ . ", "page_idx": 30}, {"type": "text", "text": "The scaling factor $\\sigma$ of CDAT was searched on a grid $\\{0.4,0.6,\\ldots,2.6\\}$ , and we also tuned the EMA parameter $\\beta_{\\mathrm{cdat}}$ in the computation of the numerators and denominators of the edge in $\\{0,0.9,0.99\\}$ . ", "page_idx": 30}, {"type": "text", "text": "C.6 Assets license and computing ressources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Assets. All experiments are done in the open-source JAX ecosystem (DeepMind et al., 2020): architectures are taken from Scenic (Dehghani et al., 2022), datasets from TensorFlow Dataset, algorithms from Optax. The datasets are MNIST (LeCun et al., 2010), (Creative Commons Attribution-Share Alike 3.0 license) CIFAR10 (Krizhevsky et al., 2009) (no available license), Imagenet (Deng et al., 2009) (ImageNet explicitly permits the use of the dataset for non-commercial research purposes, however there is no single license since the images are scrapped from different sources with different licenses), TinyShakespeare (Karpathy, 2015) (Apache 2.0 license in TensorFlow dataset, though the works of William Shakespeare are in the public domain). ", "page_idx": 30}, {"type": "text", "text": "Computing resources. Experiments have mostly been run on Tensor Processing Units (TPUs) v2 (180 Tera Floating-Point Operations per Second (TFLOPS), 64 GB High Bandwidth Memory (HBM)). Experiments on MLP Mixers required TPUs v3 (420 TFLOPS 128 GB HBM). Very small scale experiments on MNIST with MLPs were run on CPUs. In terms of wall time, as discussed in Appendix C.3, we observed that the CDAT rule can be twice slower than the constant or scheduled learning rate counterparts. We consider CDAT as a diagnostic tool and leave as future work efficient implementations. Preliminary experiments and additional attempts to further adapt the momentum parameter on edge are not reported. ", "page_idx": 30}, {"type": "text", "text": "Authors contributions. ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 Vincent Roulet conducted the experimental work from the failures of linesearches to the analysis of the CDAT rule in various settings.   \n\u2022 Atish Agarwala developed the theoretical model, with associated figures, interpretations and comments. He also helped to guide the experimental study with the insights gathered by the model.   \n\u2022 Jean Bastien Grill did an initial empirical study that gathered first intuitions on the method. He participated in the discussions and contributed to the writing.   \n\u2022 Grzegorz Swirszcz participated in the discussions.   \n\u2022 Mathieu Blondel participated in the discussions, contributed to the writing and proposed an alternative rule using a Gauss-Newton approximation of the objective.   \n\u2022 Fabian Pedregosa initiated the project, performed a larger scale empirical study of the CDAT rule on the MLCommons benchmark, participated in the discussions, and contributed to the writing. ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: This is an exploratory work that poses questions and attempts at answers from experiments and simplified models. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We clearly point out the subdued effects of the proposed CDAT rule in the stochastic framework. In addition, we discuss the limitations of the current simplified models of the sharpness dynamics to explain the performance of CDAT in a full batch regime. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Paper includes numerical experiments on models, derived from previous work (Damian et al., 2023). Modeling assumptions are clearly labelled, and accompanied with simulations. We also discuss the limitations of the modeling assumptions. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We present detailed experimental setups in Appendix C. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: We are not able to open source the code yet. We hope to release it with the arxiv version of the manuscript later. Note that the data is in open access, and the algorithms are based on open source libraries such as JAX (DeepMind et al., 2020), Optax, and Scenic (Dehghani et al., 2022). ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We present detailed experimental setups in Appendix C ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: We privileged varying settings (datasets, architectures, losses, base optimizers) over varying the seeds to provide a global overview of the CDAT rule as a diagnostic tool. We also provided additional experiments with varying mini-batch sizes (Fig. 20, Fig. 21), varying depths, widths, weight decays in a MLP setting (Fig. 18). Our goal has been to extract qualitative conclusions rather than quantitative conclusions. We also show in all transparency that the proposed rule does not outperform prefixed schedules (Fig. 19), and highlight limitations in the stochastic regime (Fig. 8). ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: This is detailed in Appendix C.6. ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: No societal impact of the work is foreseen. ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: The paper poses no such risks ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: This is detailed in Appendix C.6. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not release new assets. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects ", "page_idx": 32}]