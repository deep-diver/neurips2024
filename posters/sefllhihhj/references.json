{"references": [{"fullname_first_author": "Agarwala, A.", "paper_title": "Second-order regression models exhibit progressive sharpening to the edge of stability", "publication_date": "2023-00-00", "reason": "This paper provides a theoretical foundation for understanding the dynamics of sharpness during training, which is central to the paper's analysis of learning rate tuners."}, {"fullname_first_author": "Cohen, J.", "paper_title": "Gradient descent on neural networks typically occurs at the edge of stability", "publication_date": "2021-00-00", "reason": "This paper introduces the concept of the \"edge of stability\" which is the core theoretical framework guiding the analysis of the learning rate tuners."}, {"fullname_first_author": "Damian, A.", "paper_title": "Self-stabilization: The implicit bias of gradient descent at the edge of stability", "publication_date": "2023-00-00", "reason": "This paper provides a refined theoretical model of the dynamics near the edge of stability, extending the work of Cohen et al. (2021) and allowing for a more precise analysis of the learning rate tuners."}, {"fullname_first_author": "Vaswani, S.", "paper_title": "Painless stochastic gradient: Interpolation, line-search, and convergence rates", "publication_date": "2019-00-00", "reason": "This paper is important for providing context on classical learning rate tuning methods and their limitations in stochastic settings, which helps to motivate the need for new approaches."}, {"fullname_first_author": "Agarwala, A.", "paper_title": "High dimensional analysis reveals conservative sharpening and a stochastic edge of stability", "publication_date": "2024-00-00", "reason": "This paper offers a more detailed analysis of the sharpness dynamics in the stochastic regime, complementing the insights from previous work and helping explain the differences between full batch and mini-batch settings."}]}