[{"figure_path": "SEflLHIhhJ/figures/figures_2_1.jpg", "caption": "Figure 1: Simple learning rates tuners qualitatively underperform their constant learning rate counterparts. Gradient descent or RMSProp with a tuned constant learning rate versus self-tuned gradient descent by a linesearch method (1), or a quadratically greedy rule (3) on various datasets, architectures and losses in a full batch regime. The linesearch may perform better at early times but stalls in the long term.", "description": "This figure compares the performance of simple learning rate tuners (linesearch and quadratically greedy) against constant learning rate methods (Gradient Descent and RMSProp) across various datasets and network architectures.  The results show that while the tuners may initially achieve better one-step loss reduction, they ultimately underperform constant learning rates in the long term, particularly in the full batch setting.  The linesearch method sometimes shows an initial advantage but fails to maintain progress over time.", "section": "Canonical learning rate tuners failures in deep learning"}, {"figure_path": "SEflLHIhhJ/figures/figures_2_2.jpg", "caption": "Figure 1: Simple learning rates tuners qualitatively underperform their constant learning rate counterparts. Gradient descent or RMSProp with a tuned constant learning rate versus self-tuned gradient descent by a linesearch method (1), or a quadratically greedy rule (3) on various datasets, architectures and losses in a full batch regime. The linesearch may perform better at early times but stalls in the long term.", "description": "The figure compares the performance of simple learning rate tuners (linesearch and quadratically greedy) against constant learning rate methods (Gradient Descent and RMSProp) across various datasets and model architectures in a full-batch setting.  It shows that while the tuners might initially show faster loss reduction, they eventually underperform constant learning rates in the long run.", "section": "2 The Interplay Between Learning Rate Tuners and Curvature Dynamics"}, {"figure_path": "SEflLHIhhJ/figures/figures_3_1.jpg", "caption": "Figure 1: Simple learning rates tuners qualitatively underperform their constant learning rate counterparts. Gradient descent or RMSProp with a tuned constant learning rate versus self-tuned gradient descent by a linesearch method (1), or a quadratically greedy rule (3) on various datasets, architectures and losses in a full batch regime. The linesearch may perform better at early times but stalls in the long term.", "description": "This figure compares the performance of simple learning rate tuners (linesearch and quadratically greedy) against a constant learning rate for various deep learning models in a full-batch setting. It shows that while the tuners might offer better loss reduction initially, they eventually underperform the constant learning rate in the long run.", "section": "2 The Interplay Between Learning Rate Tuners and Curvature Dynamics"}, {"figure_path": "SEflLHIhhJ/figures/figures_4_1.jpg", "caption": "Figure 4: The poor performance of classical learning rate tuners, understood in a simplified model. The dynamics of learning rate \u03b7, sharpness \u03bbmax, and normalized centered sharpness y = \u03b7\u03bbmax \u2013 2 are examined in the simplified model (5). With a constant \u03b7, \u03bbmax stabilizes and y oscillates around 0 (blue). Classical learning rate tuners often quickly equilibrate around yt = -\u20ac, which we model using \u03b7 = 1.9Xmax (orange). This equilibration of y away from zero prevents stabilization in \u03bbmax, leading to an increase in \u03bbmax, and a corresponding decrease in \u03b7.", "description": "This figure shows the results of a simplified model to explain why classical learning rate tuners fail in deep learning. The model considers the dynamics of the learning rate (\u03b7), sharpness (\u03bbmax), and normalized centered sharpness (y).  With a constant learning rate, the sharpness stabilizes, and the normalized centered sharpness oscillates around zero. However, classical learning rate tuners quickly reach an equilibrium where y is negative, preventing the stabilization of sharpness and leading to a decrease in the learning rate.", "section": "2.3 Theoretical analysis"}, {"figure_path": "SEflLHIhhJ/figures/figures_5_1.jpg", "caption": "Figure 1: Simple learning rates tuners qualitatively underperform their constant learning rate counterparts. Gradient descent or RMSProp with a tuned constant learning rate versus self-tuned gradient descent by a linesearch method (1), or a quadratically greedy rule (3) on various datasets, architectures and losses in a full batch regime. The linesearch may perform better at early times but stalls in the long term.", "description": "The figure compares the performance of simple learning rate tuners (linesearch and quadratically greedy) against constant learning rate methods (Gradient Descent and RMSProp) across various deep learning tasks.  It shows that while the tuners might achieve better short-term loss reduction, they ultimately underperform constant learning rates in the long run, especially in full-batch settings. This underperformance is attributed to the tuners' failure to maintain long-term curvature stabilization.", "section": "2 The Interplay Between Learning Rate Tuners and Curvature Dynamics"}, {"figure_path": "SEflLHIhhJ/figures/figures_6_1.jpg", "caption": "Figure 3: Classical learning rate tuners can undershoot the edge of stability. Learning rate, sharpness, their product, and the gradient norm evolution of a constant learning rate and learning rate tuners, full batch gradient descent. Learning rate decreases by 3 orders of magnitude for tuners (1st panel) while sharpness increases (2nd panel). Their product remains relatively steady, just below the edge of stability (3rd panel). The gradient norm increases by less than a factor of 10, consistent with slow training at late times (4th panel).", "description": "This figure shows that classical learning rate tuners (linesearch and quadratically greedy) do not perform as well as a constant learning rate in the long run.  The learning rate decreases significantly, while the sharpness (largest eigenvalue of the Hessian) increases. Their product remains relatively constant, but below the edge of stability. This contrasts with a constant learning rate where both sharpness and learning rate stabilize. The gradient norm also indicates slower training for the learning rate tuners.", "section": "Analyzing learning rate tuners through curvature dynamics"}, {"figure_path": "SEflLHIhhJ/figures/figures_7_1.jpg", "caption": "Figure 7: Stochasticity shifts the optimal scaling. Normalized performance of gradient descent with momentum equipped with CDAT in a stochastic regime with varying batch sizes. In a mini-batch regime, the optimal scale decreases as the batch size decreases. Using an exponential moving average smooths out the performance of the CDAT rule over batch sizes.", "description": "This figure shows the impact of batch size on the performance of the CDAT learning rate tuning method in a stochastic setting (mini-batch). The heatmaps illustrate the normalized performance (average end train loss divided by initial train loss) for different CDAT scaling factors and batch sizes. The results indicate that the optimal scaling factor for CDAT decreases as the batch size decreases. Applying an exponential moving average to the CDAT estimates mitigates the effect of batch size variations.", "section": "Analyzing learning rate tuners through curvature dynamics"}, {"figure_path": "SEflLHIhhJ/figures/figures_8_1.jpg", "caption": "Figure 5: Enforcing optimizers to stay on edge (\u03c3 = 2.0) improves performance over greedy approximation (\u03c3 = 1.0). Train loss and learning rate behaviors for fine-tuned optimizers vs self-tuned counterparts with CDAT on various datasets, architectures, losses in a full batch regime. Tuning the learning rate \u201con edge\u201d (\u03c3 \u2248 2) improves performance over greedy tuning (\u03c3 = 1) as well as constant learning rate.", "description": "This figure displays the training loss and learning rate curves for different optimizers (GD, RMSProp, Adam with momentum) using both a constant learning rate, a scheduled learning rate, and the proposed CDAT method with scaling factors \u03c3=1.0 and \u03c3=2.0.  The results demonstrate that tuning the learning rate to be near the 'edge of stability' (\u03c3\u22482) yields significantly better results than using a constant learning rate or a greedy approach (\u03c3=1.0) across various architectures and datasets. This supports the paper's claim that focusing on curvature stabilization is key for successful adaptive learning rate tuning.", "section": "3 Optimizing on the Edge of Stability"}, {"figure_path": "SEflLHIhhJ/figures/figures_9_1.jpg", "caption": "Figure 9: A simple model partially captures the benefits induced by the proposed CDAT rule. Dynamics of theoretical model of CDAT (10). For \u03c3 = 2, feedback stabilizes y close to the EOS (y = 0), which stabilizes \u03bbmax (orange). For \u03c3 = 2 \u2212 \u03f5 and small \u03f5 (blue, \u03f5 = 0.1), model predicts that \u03bbmax slowly grows (middle), but predicts that y stabilizes to a value \u2212\u03f5 \u226a yt < 0 (right).", "description": "This figure shows the results of a simplified theoretical model used to understand the dynamics of the proposed CDAT learning rate tuner.  The model simulates the interaction between the learning rate (\u03b7), sharpness (\u03bbmax), and a normalized centered sharpness (y). The results for \u03c3 = 2 demonstrate that CDAT stabilizes y near the EOS (y=0), leading to a stable \u03bbmax. However, when \u03c3 is slightly less than 2, the model predicts that \u03bbmax will slowly increase while y stabilizes to a small negative value, indicating a less stable regime.", "section": "Theoretical analysis"}, {"figure_path": "SEflLHIhhJ/figures/figures_15_1.jpg", "caption": "Figure 4: The poor performance of classical learning rate tuners, understood in a simplified model. The dynamics of learning rate \u03b7, sharpness \u03bbmax, and normalized centered sharpness y = \u03b7\u03bbmax \u2013 2 are examined in the simplified model (5). With a constant \u03b7, \u03bbmax stabilizes and y oscillates around 0 (blue). Classical learning rate tuners often quickly equilibrate around yt = -\u20ac, which we model using \u03b7 = 1.9\u03bbmax (orange). This equilibration of y away from zero prevents stabilization in \u03bbmax, leading to an increase in \u03bbmax, and a corresponding decrease in \u03b7.", "description": "This figure shows the dynamics of learning rate (\u03b7), sharpness (\u03bbmax), and normalized centered sharpness (y) using a simplified model.  A constant learning rate leads to stable sharpness and oscillating y around 0. However, classical learning rate tuners cause y to stabilize away from 0, preventing sharpness stabilization and leading to increased sharpness and decreased learning rate.", "section": "2 The Interplay Between Learning Rate Tuners and Curvature Dynamics"}, {"figure_path": "SEflLHIhhJ/figures/figures_15_2.jpg", "caption": "Figure 4: The poor performance of classical learning rate tuners, understood in a simplified model. The dynamics of learning rate \u03b7, sharpness \u03bbmax, and normalized centered sharpness y = \u03b7\u03bbmax \u2013 2 are examined in the simplified model (5). With a constant \u03b7, \u03bbmax stabilizes and y oscillates around 0 (blue). Classical learning rate tuners often quickly equilibrate around yt = -\u20ac, which we model using \u03b7 = 1.9\u03bbmax (orange). This equilibration of y away from zero prevents stabilization in \u03bbmax, leading to an increase in \u03bbmax, and a corresponding decrease in \u03b7.", "description": "This figure shows the results of a simplified model used to explain the behavior of classical learning rate tuners.  The model simulates the interaction between the learning rate (\u03b7), sharpness (\u03bbmax), and a normalized measure of sharpness (y). It demonstrates that constant learning rates lead to a stable sharpness, while classical tuners cause the sharpness to increase and the learning rate to decrease.", "section": "2.2 Analyzing learning rate tuners through curvature dynamics"}, {"figure_path": "SEflLHIhhJ/figures/figures_19_1.jpg", "caption": "Figure 1: Simple learning rates tuners qualitatively underperform their constant learning rate counterparts. Gradient descent or RMSProp with a tuned constant learning rate versus self-tuned gradient descent by a linesearch method (1), or a quadratically greedy rule (3) on various datasets, architectures and losses in a full batch regime. The linesearch may perform better at early times but stalls in the long term.", "description": "This figure compares the performance of simple learning rate tuners (linesearch and quadratically greedy) against constant learning rate methods (Gradient Descent and RMSProp) across different datasets and model architectures.  The results show that while the tuners might exhibit better performance initially, they ultimately underperform the constant learning rate methods in the long run, especially in a full-batch setting.", "section": "2 The Interplay Between Learning Rate Tuners and Curvature Dynamics"}, {"figure_path": "SEflLHIhhJ/figures/figures_20_1.jpg", "caption": "Figure 12: Learning rates and sharpness dynamics of baseline learning rate tuners. Learning rate, Hessian or preconditioned Hessian sharpness, their product, and the alignment between the update and the largest eigenvector of the Hessian or the preconditioned Hessian. As in Fig. 12, we observe that a linesearch (1) or a quadratically greedy (3) learning rate tuner display decreasing learning rates along training. The sharpness of the Hessian (for GD) or preconditioned Hessian (for RMSProp) keep increasing for the self-tuned baselines while they stabilize for the constant learning rate counterparts. The self-tuned methods perform generally below the edge of stability or at least much less above than the constant learning rate counterpart.", "description": "This figure displays the learning rate, sharpness (largest eigenvalue of the Hessian), the product of the learning rate and sharpness, and the alignment of the update with the largest eigenvector of the Hessian for three different learning rate tuning methods and a baseline of constant learning rate. It shows that classical learning rate tuners tend to undershoot the edge of stability, leading to decreasing learning rates and increasing sharpness.", "section": "Analyzing learning rate tuners through curvature dynamics"}, {"figure_path": "SEflLHIhhJ/figures/figures_21_1.jpg", "caption": "Figure 1: Simple learning rates tuners qualitatively underperform their constant learning rate counterparts. Gradient descent or RMSProp with a tuned constant learning rate versus self-tuned gradient descent by a linesearch method (1), or a quadratically greedy rule (3) on various datasets, architectures and losses in a full batch regime. The linesearch may perform better at early times but stalls in the long term.", "description": "This figure compares the performance of simple learning rate tuners (linesearch and quadratically greedy) against constant learning rate approaches (Gradient Descent and RMSProp) on various datasets and architectures.  The experiment is performed in a full-batch setting. The results show that while the linesearch method might show better performance initially,  the constant learning rate methods ultimately outperform the tuners in the long run, highlighting the limitations of greedy, single-step optimization approaches in deep learning.", "section": "2 The Interplay Between Learning Rate Tuners and Curvature Dynamics"}, {"figure_path": "SEflLHIhhJ/figures/figures_21_2.jpg", "caption": "Figure 1: Simple learning rates tuners qualitatively underperform their constant learning rate counterparts. Gradient descent or RMSProp with a tuned constant learning rate versus self-tuned gradient descent by a linesearch method (1), or a quadratically greedy rule (3) on various datasets, architectures and losses in a full batch regime. The linesearch may perform better at early times but stalls in the long term.", "description": "This figure compares the performance of simple learning rate tuners (linesearch and quadratically greedy) against constant learning rate methods (Gradient Descent and RMSProp) across various datasets, architectures, and loss functions in a full-batch setting.  The results show that while the tuners might initially achieve better one-step loss reduction, they ultimately underperform tuned constant learning rates in the long run. The linesearch method shows initial improvement but eventually plateaus, highlighting the limitations of these classical approaches in deep learning.", "section": "2 The Interplay Between Learning Rate Tuners and Curvature Dynamics"}, {"figure_path": "SEflLHIhhJ/figures/figures_22_1.jpg", "caption": "Figure 16: The quadratically greedy rule ensures larger instantaneous decrease of the loss. Train losses and difference in loss between one step using the on-edge rule (CDAT, scale=2) and one step using quadratically greedy rule (CDAT, scale=1). Results averaged over 10 initializations and disjoint 4096-sample subsets of CIFAR100. MLP architecture: single hidden layer of size 1024, ReLU activations, trained with GD and cross-entropy loss in a full batch setting. We plot f(wt + nut) \u2212 f(wt + nut) along a training performed either with the quadratically greedy rule (\u03c3 = 1) or the on-edge rule (\u03c3 = 2). In both cases, this difference is positive meaning that the quadratically greedy rule ensures a larger instantaneous decrease of the loss. Yet the quadratically greedy rule underperforms in the long term (see Fig. 5, also holds in this full batch setting).", "description": "This figure compares the instantaneous and long-term performance of the quadratically greedy rule (CDAT scale 1.0) versus the on-edge rule (CDAT scale 2.0). While the quadratically greedy rule shows larger decreases in loss per step, the on-edge rule demonstrates better overall performance.  The experiment uses an MLP on CIFAR100 dataset.", "section": "Analyzing learning rate tuners through curvature dynamics"}, {"figure_path": "SEflLHIhhJ/figures/figures_22_2.jpg", "caption": "Figure 3: Classical learning rate tuners can undershoot the edge of stability. Learning rate, sharpness, their product, and the gradient norm evolution of a constant learning rate and learning rate tuners, full batch gradient descent. Learning rate decreases by 3 orders of magnitude for tuners (1st panel) while sharpness increases (2nd panel). Their product remains relatively steady, just below the edge of stability (3rd panel). The gradient norm increases by less than a factor of 10, consistent with slow training at late times (4th panel).", "description": "This figure compares the behavior of classical learning rate tuners (linesearch and quadratically greedy) against a constant learning rate, using full-batch gradient descent. It shows that while the tuners initially achieve better one-step loss reduction, they ultimately underperform the constant learning rate in the long term. The analysis reveals that the tuners undershoot the edge of stability, leading to a snowball effect where sharpness increases, learning rates decrease, and progress slows.", "section": "Analyzing learning rate tuners through curvature dynamics"}, {"figure_path": "SEflLHIhhJ/figures/figures_23_1.jpg", "caption": "Figure 18: Improvements of CDAT rule on edge for varying hyperparameters. Varying width, depth, weight decay and size of the subset considered when using the CDAT rule with varying scaling factors. We observe that accrued gains can be obtained with the CDAT rule for larger widths (top left panel). In terms of depth (top right panel), the CDAT rule works best with larger depths, while we note there a slight shift of optimal scaling factors from 2 to slightly below 2. The CDAT rule appears to work best with small or no weight decay (bottom left panel) while its benefits fade with larger weight decay (no difference between greedy \u03c3 = 1 and on edge \u03c3 = 2). Finally, while the method naturally finds smaller train losses with larger subsets of data, we do not observe a significant shift of relative performance between scales as the size of the data increases (bottom right panel).", "description": "This figure shows the impact of varying hyperparameters (width, depth, weight decay, and data size) on the performance of the CDAT rule for a simple MLP model.  The results demonstrate that CDAT generally performs better with larger model widths and depths.  Weight decay has a significant effect, with minimal improvement seen at larger values. Changes to dataset size show marginal benefits with larger datasets.", "section": "B.7 Sensibility analysis to architecture hyperparameters"}, {"figure_path": "SEflLHIhhJ/figures/figures_23_2.jpg", "caption": "Figure 1: Simple learning rates tuners qualitatively underperform their constant learning rate counterparts. Gradient descent or RMSProp with a tuned constant learning rate versus self-tuned gradient descent by a linesearch method (1), or a quadratically greedy rule (3) on various datasets, architectures and losses in a full batch regime. The linesearch may perform better at early times but stalls in the long term.", "description": "This figure compares the performance of simple learning rate tuners (linesearch and quadratically greedy methods) against constant learning rate methods (Gradient Descent and RMSProp) across different datasets and model architectures. The results show that while the tuners may initially perform better, they eventually underperform the constant learning rate baselines, especially in the long term, within a full-batch training setting.", "section": "Canonical learning rate tuners failures in deep learning"}, {"figure_path": "SEflLHIhhJ/figures/figures_24_1.jpg", "caption": "Figure 7: Stochasticity shifts the optimal scaling. Normalized performance of gradient descent with momentum equipped with CDAT in a stochastic regime with varying batch sizes. In a mini-batch regime, the optimal scale decreases as the batch size decreases. Using an exponential moving average smooths out the performance of the CDAT rule over batch sizes.", "description": "This figure shows the impact of batch size on the performance of the CDAT learning rate tuning method.  The results indicate that the optimal scaling factor for CDAT decreases as the batch size decreases. This is in contrast to the full batch setting, where the optimal scaling factor remains relatively constant. The use of an exponential moving average helps to smooth out the performance fluctuations across different batch sizes, highlighting the complex interaction between stochasticity and the CDAT method.", "section": "Analyzing learning rate tuners through curvature dynamics"}, {"figure_path": "SEflLHIhhJ/figures/figures_25_1.jpg", "caption": "Figure 7: Stochasticity shifts the optimal scaling. Normalized performance of gradient descent with momentum equipped with CDAT in a stochastic regime with varying batch sizes. In a mini-batch regime, the optimal scale decreases as the batch size decreases. Using an exponential moving average smooths out the performance of the CDAT rule over batch sizes.", "description": "This figure shows the impact of batch size on the performance of CDAT in a stochastic setting.  The optimal scaling factor (\u03c3) for CDAT, which aims to keep the learning rate at the edge of stability, changes with varying batch size. Smaller batch sizes result in a smaller optimal scaling factor.  The use of an exponential moving average (EMA) helps stabilize the performance of CDAT across different batch sizes, smoothing out variations.", "section": "Analyzing learning rate tuners through curvature dynamics"}]