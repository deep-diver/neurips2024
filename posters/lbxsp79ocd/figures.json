[{"figure_path": "LBXSP79oCd/figures/figures_3_1.jpg", "caption": "Figure 1: The EfficientCAPER architecture. Taking the partial point cloud of an articulated object as input, our network estimates the pose of the free part using HS-Encoder as the backbone and decoupled rotation as representation. In the second stage, we canonicalize the input point cloud and predict per-part scale, segmentation, joint parameters, and joint states, where the latter two are used to recover the constrained parts' poses.", "description": "This figure illustrates the architecture of EfficientCAPER, which consists of two stages. Stage 1 estimates the pose of the free part of an articulated object using a HS-Encoder and decoupled rotation. Stage 2 canonicalizes the input point cloud and predicts per-part scale, segmentation, joint parameters, and joint states to recover the poses of the constrained parts.  The figure shows the data flow through the network, highlighting the key components and their interactions.", "section": "4 Method"}, {"figure_path": "LBXSP79oCd/figures/figures_4_1.jpg", "caption": "Figure 2: Free part and Constrained part in articulated objects. These two kinds of parts are connected by joints.", "description": "This figure illustrates the two types of parts found in articulated objects: free parts and constrained parts.  Free parts can move freely in three-dimensional space, unconstrained by joints.  Constrained parts, on the other hand, can only move along specific axes defined by their corresponding joints (revolute or prismatic). The image visually shows a free part (a cube), a revolute joint (connecting a smaller cube to the larger cube), and a prismatic joint (sliding along a track). This distinction is key to the EfficientCAPER method, which models these parts separately to improve accuracy and efficiency in pose estimation.", "section": "4.2 Joint-Centric Articulation Pose Modeling"}, {"figure_path": "LBXSP79oCd/figures/figures_8_1.jpg", "caption": "Figure 1: The EfficientCAPER architecture. Taking the partial point cloud of an articulated object as input, our network estimates the pose of the free part using HS-Encoder as the backbone and decoupled rotation as representation. In the second stage, we canonicalize the input point cloud and predict per-part scale, segmentation, joint parameters, and joint states, where the latter two are used to recover the constrained parts' poses.", "description": "This figure shows the architecture of the EfficientCAPER model, an end-to-end framework for category-level articulated object pose estimation. It consists of two stages. Stage 1 estimates the pose of the free part of the object using a HS-Encoder and decoupled rotation. Stage 2 canonicalizes the input point cloud and predicts per-part scale, segmentation, joint parameters, and joint states to recover the poses of the constrained parts.  The figure visually depicts the flow of data through the network, highlighting key components and their interactions.", "section": "4 Method"}, {"figure_path": "LBXSP79oCd/figures/figures_14_1.jpg", "caption": "Figure 4: Qualitative results on ArtImage(top), ReArtMix(bottom left) and RobotArm(bottom right).", "description": "This figure shows qualitative results of the proposed EfficientCAPER method and compares it with the A-NCSH method on three different datasets: ArtImage, ReArtMix, and RobotArm.  The top row displays results from the ArtImage dataset, which contains synthetic articulated objects.  The bottom-left shows results from the semi-synthetic ReArtMix dataset, and the bottom-right shows results from the real-world RobotArm dataset. Each image displays the estimated 3D bounding box (yellow) for each part of the object and the ground truth bounding box (in other colors) for each corresponding part.  The figure demonstrates the effectiveness of EfficientCAPER in accurately estimating the pose and 3D scale of various articulated objects across different scenarios, including synthetic and real-world environments, which showcases its generalization ability.", "section": "5 Experiments"}, {"figure_path": "LBXSP79oCd/figures/figures_15_1.jpg", "caption": "Figure 4: Qualitative results on ArtImage(top), ReArtMix(bottom left) and RobotArm(bottom right).", "description": "This figure shows qualitative results of the proposed EfficientCAPER method on three datasets: ArtImage, ReArtMix, and RobotArm.  The top row displays results on ArtImage, showing successful pose estimation for various articulated objects in different configurations. The bottom left shows results on ReArtMix, a more challenging semi-synthetic dataset. The bottom right shows results on RobotArm, a real-world dataset with complex scenes and diverse object arrangements.  The visualizations likely compare estimated poses (likely shown as 3D bounding boxes) with ground truth poses to illustrate the accuracy of the EfficientCAPER method.", "section": "5.3 Ablation Study"}, {"figure_path": "LBXSP79oCd/figures/figures_15_2.jpg", "caption": "Figure 7: Qualitative results on the RobotArm dataset.", "description": "This figure shows qualitative results of the proposed EfficientCAPER method on the RobotArm dataset.  It displays multiple images of a robot arm in various poses, each with the estimated pose overlaid. The overlaid pose is shown via bounding boxes around the robot arm segments, and the results illustrate the accuracy of pose estimation for articulated objects. The different colors likely indicate different parts of the robot arm.", "section": "5.3 Ablation Study"}]