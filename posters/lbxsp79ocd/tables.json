[{"figure_path": "LBXSP79oCd/tables/tables_7_1.jpg", "caption": "Table 1: Comparison with state-of-the-arts on the ArtImage dataset. We validate our EfficientCAPER on categories Laptop, Eyeglasses, Dishwasher, Scissors and Drawer that contains 2, 3, 2, 2, 4 parts respectively. \u2193 means the lower the better and \u2191 means the upper the better.", "description": "This table compares the performance of EfficientCAPER with three other methods (A-NCSH, OMAD, and ArtPERL) across five articulated object categories from the ArtImage dataset.  The comparison is made using four metrics: rotation error, translation error, 3D IOU (Intersection over Union), and inference time per image. Lower rotation and translation errors, higher 3D IOU, and lower inference times indicate better performance.  Each category contains a varying number of parts (2-4), reflecting the complexity of the articulated object.", "section": "5.2 Experiments on ArtImage Dataset"}, {"figure_path": "LBXSP79oCd/tables/tables_8_1.jpg", "caption": "Table 2: Experiments on the effect of articulation pose canonicalization (indicated as APC here) module.", "description": "This table presents the ablation study result of the articulation pose canonicalization module on the ArtImage dataset.  It shows the improvement in joint state error and 3D scale score (represented as 3D IoU) when using the canonicalization module compared to without using it.  The results are shown for each object category (Laptop, Eyeglasses, Dishwasher, Scissors, Drawer). The inclusion of canonicalization leads to a significant reduction in joint state error and an improvement in 3D scale prediction accuracy.", "section": "5.3 Ablation Study"}, {"figure_path": "LBXSP79oCd/tables/tables_8_2.jpg", "caption": "Table 3: Pose errors for 'Drawer' category with varying occlusion levels.", "description": "This table presents the results of evaluating the EfficientCAPER model's performance on the 'Drawer' category of the ArtImage dataset under varying levels of occlusion. It shows the mean rotation error (in degrees) and mean translation error (in meters) for three occlusion levels: 0%-40%, 40%-80%, and 80%-100%.  The occlusion level is defined as the percentage of visible points compared to the total number of points in the part.", "section": "5.3 Ablation Study"}, {"figure_path": "LBXSP79oCd/tables/tables_9_1.jpg", "caption": "Table 4: Pose estimation results on ReArtMix dataset.", "description": "This table compares the performance of EfficientCAPER against ReArtNet [13] on the ReArtMix dataset.  For each of five articulated object categories (Box, Stapler, Cutter, Scissors, Drawer), it shows the per-part rotation error (in degrees), translation error (in meters), and 3D Intersection over Union (IOU) scores.  The results demonstrate EfficientCAPER's significant improvement in accuracy across all metrics and categories compared to the baseline method.", "section": "5.4 Generalization Capacity"}, {"figure_path": "LBXSP79oCd/tables/tables_9_2.jpg", "caption": "Table 5: Pose estimation results on 7-part RobotArm dataset", "description": "This table compares the performance of the proposed EfficientCAPER method and the A-NCSH baseline method on the 7-part RobotArm dataset.  It presents the per-part rotation and translation errors for each of the seven parts of the robot arm.  Lower values indicate better performance.", "section": "5.4 Generalization Capacity"}, {"figure_path": "LBXSP79oCd/tables/tables_13_1.jpg", "caption": "Table 6: Experiments comparing single free part as input between both free part and constrained parts as input. Note that we only investigate this analysis in the first stage of our EfficientCAPER.", "description": "This table presents ablation study results comparing the performance of the EfficientCAPER model when using only the free part's point cloud versus using the combined point cloud of the free part and constrained parts as input for the first stage of pose estimation.  The goal is to show the effect of incorporating additional contextual information from constrained parts on the accuracy of free part pose estimation. The results are presented separately for rotation error (in degrees) and translation error (in meters) for five different object categories: Laptop, Eyeglasses, Dishwasher, Scissors, and Drawer.", "section": "A.5.1 Ablation Study"}, {"figure_path": "LBXSP79oCd/tables/tables_13_2.jpg", "caption": "Table 7: Experiments estimating joint parameters in the canonical space on ArtImage.", "description": "This table presents the results of experiments evaluating the accuracy of the model in estimating joint parameters in the canonical space for the ArtImage dataset. It shows the angle error (in degrees) and distance error (in meters) for revolute joints and the orientation error for prismatic joints for different object categories.", "section": "5.2 Experiments on ArtImage Dataset"}, {"figure_path": "LBXSP79oCd/tables/tables_13_3.jpg", "caption": "Table 8: Experiments estimating joint parameters in the canonical space on ReArtMix.", "description": "This table presents the results of estimating joint parameters in the canonical space for the ReArtMix dataset.  It shows the angle error (in degrees) and distance error (in meters) for each category of articulated object. Note that the Cutter and Drawer categories only have one type of joint, so the distance error is not applicable.", "section": "5.2 Experiments on ArtImage Dataset"}, {"figure_path": "LBXSP79oCd/tables/tables_14_1.jpg", "caption": "Table 9: Experiments estimating joint parameters in the canonical space on 7-part RobotArm dataset", "description": "This table presents the results of estimating joint parameters in the canonical space for a 7-part RobotArm dataset.  It shows the angle error (in degrees) and distance error (in meters) for each of the six joints (Joint ID 1 through 6). These metrics evaluate the accuracy of the model's predictions for the orientation and position of the joint axes.", "section": "5.3 Ablation Study"}, {"figure_path": "LBXSP79oCd/tables/tables_14_2.jpg", "caption": "Table 10: Part Segmentation results. APC means articulation pose canonicalization module.", "description": "This table presents the results of part segmentation experiments, comparing performance with and without articulation pose canonicalization (APC).  The mean Intersection over Union (IOU) is shown for each category (Laptop, Eyeglasses, Dishwasher, Scissors, Drawer) to demonstrate the impact of APC on segmentation accuracy. Higher IOU values indicate better segmentation.", "section": "5.2 Experiments on ArtImage Dataset"}]