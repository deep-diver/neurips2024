{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-05", "reason": "This paper introduced the Transformer architecture, the foundation upon which the MatFormer architecture is built."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper demonstrated the effectiveness of large language models for few-shot learning, providing the context for the work on elastic inference."}, {"fullname_first_author": "Alec Radford", "paper_title": "Language Models are Unsupervised Multitask Learners", "publication_date": "2019-01-01", "reason": "This paper introduced the concept of using language models for unsupervised multitask learning, which is relevant to MatFormer's ability to extract multiple submodels for diverse tasks."}, {"fullname_first_author": "Colin Raffel", "paper_title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "publication_date": "2019-07-15", "reason": "This paper presented a unified text-to-text Transformer architecture, which MatFormer builds upon to create a nested architecture suitable for elastic inference."}, {"fullname_first_author": "Huiyi Hu", "paper_title": "Efficient large-scale language model training on gpu clusters", "publication_date": "2019-05-01", "reason": "This paper introduced techniques for efficient training of large language models, which is essential for the computationally intensive training of the MatFormer model."}]}