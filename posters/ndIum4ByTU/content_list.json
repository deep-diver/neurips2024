[{"type": "text", "text": "Meta Flow Matching: Integrating Vector Fields on the Wasserstein Manifold ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Numerous biological and physical processes can be modeled as systems of interact  \n2 ing samples evolving continuously over time, e.g. the dynamics of communicating   \n3 cells or physical particles. Flow-based models allow for learning these dynamics at   \n4 the population level \u2014 they model the evolution of the entire distribution of sam  \n5 ples. However, current flow-based models are limited to a single initial population   \n6 and a set of predefined conditions which describe different dynamics. We argue that   \n7 multiple processes in natural sciences have to be represented as vector fields on the   \n8 Wasserstein manifold of probability densities. That is, the change of the population   \n9 at any moment in time depends on the population itself due to the interactions   \n10 between samples. In particular, this is crucial for personalized medicine where the   \n11 development of diseases and their treatments depend on the microenvironment of   \n12 cells specific to each patient. We propose Meta Flow Matching (MFM), a practical   \n13 approach to integrating along these vector fields on the Wasserstein manifold by   \n14 amortizing the flow model over the initial populations. Namely, we embed the   \n15 population of samples using a Graph Neural Network (GNN) and use these embed  \n16 dings to train a Flow Matching model. This gives Meta Flow Matching the ability   \n17 to generalize over the initial distributions unlike previously proposed methods.   \n18 Finally, we demonstrate the ability of MFM to improve prediction of individual   \n19 treatment responses on a large scale multi-patient single-cell drug screen dataset. ", "page_idx": 0}, {"type": "text", "text": "20 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "21 Understanding the dynamics of many-body problems is a central challenge across the natural sciences.   \n22 In the field of cell biology, a central focus is the understanding of the dynamic processes that cells   \n23 undergo in response to their environment, and in particular their response and interaction with other   \n24 cells. Cells communicate with one other in close proximity using cell signaling, exerting influence   \n25 over each other\u2019s trajectories (Armingol et al., 2020; Goodenough and Paul, 2009). This signaling   \n26 presents an obstacle for modeling, but is essential for understanding and eventually controlling   \n27 cell dynamics during development (Gulati et al., 2020; Rizvi et al., 2017), in diseased states (Mol\u00e8   \n28 et al., 2021; Binnewies et al., 2018; Zeng and Dai, 2019; Chung et al., 2017), and in response to   \n29 perturbations (Ji et al., 2021; Peidli et al., 2024).   \n30 The super-exponential decrease of sequencing costs and advances in microfluidics has enabled the   \n31 rapid advancement of single-cell sequencing and related technologies over the past decade. While   \n32 single-cell sequencing has been used to great effect to understand the heterogeneity in cell systems,   \n33 they are also destructive, making longitudinal measurements extremely difficult. Instead, most   \n34 approaches model cell dynamics at the population level (Hashimoto et al., 2016; Weinreb et al., 2018;   \n35 Schiebinger et al., 2019; Tong et al., 2020; Neklyudov et al., 2022; Bunne et al., 2023a). These   \n36 approaches involve the formalisms of optimal transport (Villani, 2009; Peyr\u00e9 and Cuturi, 2019) and   \n37 generative modeling (De Bortoli et al., 2021; Lipman et al., 2023) methods, which allow for learning   \n38 a map between empirical measures. While these methods are able to model the dynamics of the   \n39 population, they are fundamentally limited in that they model the evolution of cells as independent   \n40 particles evolving according to a shared dynamical system. Furthermore, these models can be trained   \n41 to match any given set of measures, but they are restricted to modeling of a single population and can   \n42 at best condition on a number of different dynamics that is available in the training data.   \n43 To address this we propose Meta Flow Matching (MFM) \u2014 the amortization of the Flow Matching   \n44 generative modeling framework (Lipman et al., 2023) over the input measures. In practice, our   \n45 method can be used to predict the time-evolution of distributions from a given dataset of the time  \n46 evolved examples. Namely, we assume that the collected data undergoes a universal developmental   \n47 process, which depends only on the population itself as in the setting of the interacting particles or   \n48 communicating cells. Under this assumption, we learn the vector field model that takes samples from   \n49 the initial distribution as input and defines the push-forward map on the sample-space that maps the   \n50 initial distribution to the final distribution.   \n51 We showcase the utility of our approach on two applications. We first explore Meta Flow Matching on   \n52 a synthetic task of denoising letters. We show that MFM is able to generalize the denoising process   \n53 to letters in unseen orientations where a standard flow matching approach cannot. Next, we explore   \n54 how MFM can be applied to model single-cell perturbation data (Ji et al., 2021; Peidli et al., 2024).   \n55 We evaluate MFM on predicting the response of patient-derived cells to chemotherapy treatments   \n56 in a recently published large scale single-cell drug screening dataset where there are known to be   \n57 patient-specific responses (Ramos Zapatero et al., 2023). This dataset includes more than 25M cells   \n58 collected over ten patients under 2500 conditions. This is a challenging task due to the variance over   \n59 multiple patients, treatments applied and the local cell compositions, but it can be used to study the   \n60 tumor micro-environment (TME), thought to be essential in circumventing chemoresistance. We   \n61 demonstrate that Meta Flow Matching can successfully predict the development of cell populations   \n62 on replicated experiments, and, most importantly, it generalizes to previously unseen patients, thus,   \n63 capturing the patient-specific response to the treatment. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "64 2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "65 2.1 Generative Modeling via Flow Matching ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "66 Flow Matching is an approach to generative modeling recently proposed independently in different   \n67 works: Rectified Flows (Liu et al., 2022), Flow Matching (Lipman et al., 2023), Stochastic Interpolants   \n68 (Albergo and Vanden-Eijnden, 2022). It assumes a continuous interpolation between densities $p_{0}(x_{0})$   \n69 and $p_{1}(x_{1})$ in the sample space. That is, the sample from the intermediate density $p_{t}(\\boldsymbol{x}_{t})$ is produced   \n70 as follows ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{x_{t}=\\;f_{t}(x_{0},x_{1}),\\;\\;(x_{0},x_{1})\\sim\\pi(x_{0},x_{1})\\,,}}\\\\ {{\\mathrm{{\\small~where}}\\;\\int d x_{1}\\;\\pi(x_{0},x_{1})=p_{0}(x_{0})\\,,\\;\\;\\int d x_{0}\\;\\pi(x_{0},x_{1})=p_{1}(x_{1})\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "71 where $f_{t}$ is the time-continuous interpolating function such that $f_{t=0}(x_{0},x_{1})\\ \\ =\\ \\ x_{0}$ and   \n72 $f_{t=1}(x_{0},x_{1})\\,=\\,x_{1}$ (e.g. linearly between $x_{0}$ and $x_{1}$ with $f_{t}(x_{0},x_{1})\\,=\\,(1\\,-\\,t)\\,\\cdot\\,x_{0}\\,+\\,t\\,\\cdot\\,x_{1})$ ;   \n73 $\\pi(x_{0},x_{1})$ is the density of the joint distribution, which is usually taken as a distribution of inde  \n74 pendent random variables $\\pi(x_{0},x_{1})=p_{0}(x_{0})p_{1}(x_{1})$ , but can also be generalized to formulate the   \n75 optimal transport problems (Pooladian et al., 2023; Tong et al., 2024). The corresponding density can   \n76 be defined then as the following expectation ", "page_idx": 1}, {"type": "equation", "text": "$$\np_{t}(x)=\\int d x_{0}d x_{1}\\;\\pi(x_{0},x_{1})\\delta(x-f_{t}(x_{0},x_{1}))\\,.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "77 The essential part of Flow Matching is the continuity equation that describes the change of this   \n78 density through the vector field on the state space, which admits vector field $v_{t}^{*}(x)$ as a solution ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\frac{\\partial p_{t}(x)}{\\partial t}=-\\langle\\nabla_{x},p_{t}(x)v_{t}^{*}(x)\\rangle\\,,\\;\\;v_{t}^{*}(\\xi)=\\frac{1}{p_{t}(\\xi)}\\mathbb{E}_{\\pi(x_{0},x_{1})}\\left[\\delta(f_{t}(x_{0},x_{1})-\\xi)\\frac{\\partial f_{t}(x_{0},x_{1})}{\\partial t}\\right].\n$$", "text_format": "latex", "page_idx": 1}, {"type": "image", "img_path": "ndIum4ByTU/tmp/ee7c4070df9d70f945beb30d19b7c00d2bbd39e0bd02c621bc364bc0a0b6e81b.jpg", "img_caption": ["Figure 1: Illustration of flow matching methods on the 2-Wasserstein manifold, $\\mathcal{P}_{2}(\\mathcal{X})$ , depicted as a twodimensional sphere. Flow Matching learns the tangent vectors to a single curve on the manifold. Conditional generation corresponds to learning a finite set of curves on the manifold, e.g. classes $c_{1}$ and $c_{2}$ on the plot. Meta Flow Matching learns to integrate a vector field on $\\mathcal{P}_{2}(\\mathcal{X})$ , i.e. for every starting density $p_{0}$ Meta Flow Matching defines a push-forward measure that integrates along the underlying vector field. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "79 Relying on this formula, one can derive the tractable objective for learning $v_{t}^{*}(x)$ , i.e. ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{{\\mathcal{L}}_{\\mathrm{FM}}(\\omega)=}&{\\displaystyle\\int_{0}^{1}d t\\;\\mathbb{E}_{p_{t}(x)}\\|v_{t}^{*}(x)-v_{t}(x;\\omega)\\|^{2}}\\\\ &{=\\;\\mathbb{E}_{\\pi(x_{0},x_{1})}\\int_{0}^{1}d t\\;\\Big\\|{\\frac{\\partial}{\\partial t}}f_{t}(x_{0},x_{1})-v_{t}(f_{t}(x_{0},x_{1});\\omega)\\Big\\|^{2}+{\\mathrm{constant}}\\,.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "80 Finally, the vector field $v_{t}(\\xi,\\omega)\\approx v_{t}^{*}(\\xi)$ defines the push-forward density that approximately matches   \n81 $p_{t=1}$ , i.e. $T_{\\#P0}\\approx p_{t=1}$ , where $T$ is the flow corresponding to vector field $v_{t}(\\cdot,\\omega)$ with parameters $\\omega$ . ", "page_idx": 2}, {"type": "text", "text": "82 2.2 Conditional Generative Modeling via Flow Matching ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "83 Conditional image generation is one of the most common applications of generative models nowadays;   \n84 it includes conditioning on the text prompts (Saharia et al., 2022b; Rombach et al., 2022) as well   \n85 as conditioning on other images (Saharia et al., 2022a). To learn the conditional generative process   \n86 with diffusion models, one merely has to pass the conditional variable (sampled jointly with the data   \n87 point) as an additional input to the parametric model of the vector field. The same applies for the   \n88 Flow Matching framework.   \n89 Conditional Generative Modeling via Flow Matching is independently introduced in several works   \n90 (Zheng et al., 2023; Dao et al., 2023; Isobe et al., 2024) and it operates as follows. Consider a family   \n91 of time-continuous densities $p_{t}(x_{t}\\mid c)$ , which corresponds to the distribution of the following random   \n92 variable ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{t}=f_{t}(x_{0},x_{1}),\\;\\;(x_{0},x_{1})\\sim\\pi(x_{0},x_{1}\\,|\\,c)\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "93 For every $c$ , the density $p_{t}(x_{t}\\mid c)$ follows the continuity equation with the following vector field ", "page_idx": 2}, {"type": "equation", "text": "$$\nv_{t}^{*}(\\xi\\,|\\,c)=\\frac{1}{p_{t}(\\xi\\,|\\,c)}\\mathbb{E}_{\\pi(x_{0},x_{1})}\\delta(f_{t}(x_{0},x_{1})-\\xi)\\frac{\\partial f_{t}(x_{0},x_{1})}{\\partial t}\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "94 which depends on $c$ . Thus, the training objective of the conditional model becomes ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{C G F M}(\\omega)=\\;\\mathbb{E}_{p(c)}\\mathbb{E}_{\\pi(x_{0},x_{1}\\,|\\,c)}\\int_{0}^{1}d t\\;\\bigg\\|\\frac{\\partial}{\\partial t}f_{t}(x_{0},x_{1})-v_{t}(f_{t}(x_{0},x_{1})\\,|\\,c;\\omega)\\bigg\\|^{2}\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "95 where, compared to the original Flow Matching formulation, we first have to sample $c$ , then produce   \n96 the samples from $p_{t}(x_{t}\\mid c)$ and pass $c$ as input to the parametric model of the vector field. ", "page_idx": 2}, {"type": "text", "text": "97 3 Meta Flow Matching ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "98 In this paper, we propose the amortization of the Flow Matching framework over the marginal   \n99 distributions. Our model is based on the outstanding ability of the Flow Matching framework to   \n100 learn the push-forward map for any joint distribution $\\pi(x_{0},x_{1})$ given empirically. For the given joint   \n101 $\\pi(x_{0},x_{1})$ , we denote the solution of the Flow Matching optimization problem as follows ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nv_{t}^{*}(\\cdot,\\pi)=\\underset{v_{t}}{\\operatorname{argmin}}\\,\\mathcal{L}_{G F M}(v_{t}(\\cdot),\\pi(x_{0},x_{1}))\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "102 Analogously to the amortized optimization (Chen et al., 2022; Amos et al., 2023), we aim to learn the   \n103 model that outputs the solution of Eq. (10) based on the input data sampled from $\\pi$ , i.e. ", "page_idx": 3}, {"type": "equation", "text": "$$\nv_{t}(\\cdot,\\varphi(\\pi))=v_{t}^{*}(\\cdot,\\pi)\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "104 where $\\varphi(\\pi)$ is the embedding model of $\\pi$ and the joint density $\\pi(\\cdot\\left|\\,c)\\right.$ is generated using some   \n105 unknown measure of the conditional variables $c\\sim p(c)$ . ", "page_idx": 3}, {"type": "text", "text": "106 3.1 Modeling Process in Natural Sciences as Vector Fields on the Wasserstein Manifold ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "107 We argue that numerous biological and physical processes cannot be modeled via the vector field   \n108 propagating the population samples independently. Thus, we propose to model these processes as   \n109 families of conditional vector fields where we amortize the conditional variable by embedding the   \n110 population via a Graph Neural Network (GNN).   \n111 To provide the reader with the necessary intuition, we are going to use the geometric formalism   \n112 developed by Otto (2001). That is, time-dependent densities $p_{t}(\\boldsymbol{x}_{t})$ define absolutely-continuous   \n113 curves on the 2-Wasserstein space of distributions $\\mathcal{P}_{2}(\\mathcal{X})$ (Ambrosio et al., 2008). The tangent space   \n114 of this manifold is defined by the gradient flows $S_{t}=\\{\\nabla s_{t}\\,|\\,s_{t}:\\mathcal{X}\\rightarrow\\mathbb{R}\\}$ on the state space $\\mathcal{X}$ . In   \n115 the Flow Matching context, we are going to refer to the tangent vectors as vector fields since one   \n116 can always project the vector field onto the tangent space by parameterizing it as a gradient flow   \n117 (Neklyudov et al., 2022).   \n118 Under the geometric formalism of the 2-Wasserstein manifold, Flow Matching can be considered   \n119 as learning the tangent vectors $v_{t}(\\cdot)$ along the density curve $p_{t}(\\boldsymbol{x}_{t})$ defined by the sampling process   \n120 in Eq. (2) (see the left panel in Fig. 1). Furthermore, the conditional generation processes $\\bar{p_{t}}(x_{t}\\,|\\,c)$   \n121 would be represented as a finite set of curves if $c$ is discrete (e.g. class-conditional generation of   \n122 images) or as a family of curves if $c$ is continuous (see the middle panel in Fig. 1).   \n123 Finally, one can define a vector field on the 2-Wasserstein manifold via the continuity equation with   \n124 the vector field $v_{t}(x,p_{t}(x))$ on the state space $\\mathcal{X}$ that depends on the current density $p_{t}(\\boldsymbol{x})$ or its   \n125 derivatives. Below we give two examples of processes defined as vector fields on the 2-Wasserstein   \n126 manifold.   \n127 Example 1 (Mean-field limit of interacting particles). In the limit of the infinite number of interacting   \n128 particles one can describe their state with the density function $p_{t}(\\boldsymbol{x})$ . Consider the interaction   \n129 according to the first order dynamics with the velocity $k(x,y):\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ of the particles at   \n130 point $x$ that interact with the particles at point $y$ . Then the change of the density is described by the   \n131 following continuity equation ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{d x}{d t}=\\mathbb{E}_{p_{t}(y)}k(x,y),\\;\\;\\frac{\\partial p_{t}(x)}{\\partial t}=-\\big\\langle\\nabla_{x},p_{t}(x)\\mathbb{E}_{p_{t}(y)}k(x,y)\\big\\rangle\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "132 Example 2 (Diffusion). Even when the physical particles evolve independently in nature, the   \n133 deterministic vector field model might be dependent on the current density of the population. For   \n134 instance, for the diffusion process, the change of the density is described by the Fokker-Planck   \n135 equation, which results in the density-dependent vector field when written as a continuity equation,   \n136 i.e. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\partial p_{t}(x)}{\\partial t}=\\frac{1}{2}\\Delta_{x}p_{t}(x)=-\\Bigg\\langle\\nabla_{x},p_{t}(x)\\Bigg(-\\frac{1}{2}\\nabla_{x}\\log p_{t}(x)\\Bigg)\\Bigg\\rangle\\implies\\frac{d x}{d t}=-\\frac{1}{2}\\nabla_{x}\\log p_{t}(x)\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "137 Motivated by the examples above, we argue that using the information about the current or the initial   \n138 density is crucial for the modeling of time-evolution of densities in natural processes, to capture this   \n139 type of dependency one can model the change of the density as the following Cauchy problem ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\partial p_{t}(x)}{\\partial t}=-\\langle\\nabla_{x},p_{t}(x)v_{t}(x,p_{t})\\rangle\\;,\\;\\;p_{t=0}(x)=p_{0}(x)\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "140 where the state-space vector field $v_{t}(x,p_{t})$ depends on the density $p_{t}$ . ", "page_idx": 3}, {"type": "text", "text": "141 The dependency might vary across models, e.g. in Example 1 the vector field can be modeled as an   \n142 application of a kernel to the density function, while in Example 2 the vector field depends only on   \n143 the local value of the density and its derivative.   \n145 Consider the dataset of joint populations $\\boldsymbol{\\mathcal{D}}=\\{(\\pi(\\boldsymbol{x}_{0},\\boldsymbol{x}_{1}\\mid i))\\}_{i}$ , where, to simplify the notation,   \n146 we associate every $i$ -th population with its density $\\pi(\\cdot\\left|\\right.i)$ and the conditioning variable here is the   \n147 index of this population in the dataset. We make the following assumptions regarding the ground   \n148 truth sampling process (i) we assume that the starting marginals $\\begin{array}{r}{p_{0}(x_{0}\\,|\\,i)=\\int\\bar{d x}_{1}\\;\\pi(\\bar{x}_{0},x_{1}\\,\\bar{|}\\,i)}\\end{array}$ are   \n149 sampled from some unknown distribution that can be parameterized with a large enough number of   \n150 parameters (ii) the endpoint marginals $\\begin{array}{r}{p_{1}(x_{1}\\,|\\,i)=\\int\\bar{d}x_{0}\\;\\pi(x_{0},x_{1}\\,|\\,i)}\\end{array}$ are obtained as push-forward   \n151 densities solving the Cauchy problem in Eq. (14), (iii) there exists unique solution to this Cauchy   \n152 problem.   \n153 One can learn a joint model of all the processes from the dataset $\\mathcal{D}$ using the conditional version of   \n154 the Flow Matching algorithm (see Section 2.2) where the population index $i$ plays the role of the   \n155 conditional variable. However, obviously, such a model will not generalize beyond the considered   \n156 data $\\mathcal{D}$ and unseen indices $i$ . We illustrate this empirically in Section 5.   \n157 To be able to generalize to previously unseen populations, we propose learning the density-dependent   \n158 vector field motivated by Eq. (14). That is, we propose to use an embedding function $\\varphi:{\\mathcal{P}}_{2}({\\mathcal{X}})\\to$   \n159 $\\mathbb{R}^{m}$ to embed the starting marginal density $p_{0}$ , which we then input into the vector field model and   \n160 minimize the following objective over $\\omega$ ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathrm{MFM}}(\\omega;\\varphi)=\\;\\mathbb{E}_{i\\sim{\\mathcal{D}}}\\mathbb{E}_{\\pi(x_{0},x_{1}\\mid i)}\\int_{0}^{1}d t\\,\\left\\|{\\frac{\\partial}{\\partial t}}f_{t}(x_{0},x_{1})-v_{t}(f_{t}(x_{0},x_{1})\\mid\\varphi(p_{0});\\omega)\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "161 Note that the initial density $p_{0}$ is enough to predict the push-forward density $p_{1}$ since the Cauchy   \n162 problem for Eq. (14) has a unique solution. The embedding function $\\varphi(p_{0})$ can take different forms,   \n163 e.g. it can be the density value $\\varphi(p_{0})=p_{0}(\\cdot)$ , which is then used inside the vector field model to   \n164 evaluate at the current point (analogous to Example 2); a kernel density estimator (analogous to   \n165 Example 1); or a parametric model taking the samples from this density as an input.   \n166 Proposition 1. Meta Flow Matching recovers the Conditional Generation via Flow Matching   \n167 when the conditional dependence of the marginals $\\begin{array}{r}{p_{0}(x_{0}\\mid c)=\\int d x_{1}\\pi(x_{0},x_{1}\\mid c)}\\end{array}$ and $p_{1}(x_{1}\\mid c)=$   \n168 $\\int d x_{0}\\pi(x_{0},x_{1}\\mid c)$ and the distribution $p(c)$ are known, i.e. there exist $\\varphi:{\\mathcal{P}}_{2}({\\mathcal{X}})\\to\\mathbb{R}^{m}$ such that   \n169 $\\mathcal{L}_{M F M}(\\omega)=\\mathcal{L}_{C G F M}(\\omega)$ .   \n170 Proof. Indeed, sampling from the dataset $i\\sim\\mathcal{D}$ becomes sampling of the conditional variable   \n171 $c\\sim p(c)$ and the embedding function becomes $\\varphi(p_{0}(\\cdot\\vert c))=c$ . \u53e3   \n172 Furthermore, for the parametric family of the embedding models $\\varphi(p_{t},\\theta)$ , we show that the parameters   \n173 $\\theta$ can be estimated by minimizing the objective in Eq. (15) in the joint optimization with the vector   \n174 field parameters $\\omega$ . We formalize this statement in the following theorem.   \n175 Theorem 1. Consider a dataset of populations $\\boldsymbol{\\mathcal{D}}=\\{(\\pi(\\boldsymbol{x}_{0},\\boldsymbol{x}_{1}\\mid i))\\}_{i}$ generated from some unknown   \n176 conditional model $\\pi(x_{0},x_{1}\\mid c)p(\\bar{c})$ . Then the following objective ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\omega,\\theta)=\\;\\mathbb{E}_{p(c)}\\int_{0}^{1}d t\\;\\mathbb{E}_{p_{t}(x_{t}\\mid c)}\\|v_{t}^{*}(x_{t}\\mid c)-v_{t}(x_{t}\\mid\\varphi(p_{0},\\theta),\\omega)\\|^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "177 is equivalent to the Meta Flow Matching objective ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{M F M}(\\omega,\\theta)=\\;\\mathbb{E}_{i\\sim\\mathcal{D}}\\mathbb{E}_{\\pi(x_{0},x_{1}\\mid i)}\\int_{0}^{1}d t\\,\\left\\|{\\frac{\\partial}{\\partial t}}f_{t}(x_{0},x_{1})-v_{t}(f_{t}(x_{0},x_{1})\\mid\\varphi(p_{0},\\theta);\\omega)\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "178 up to an additive constant. ", "page_idx": 4}, {"type": "text", "text": "179 Proof. We postpone the proof to Appendix A. ", "page_idx": 4}, {"type": "text", "text": "180 3.3 Learning Population Embeddings via Graph Neural Networks (GNNs) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "181 In many applications, the populations $\\mathcal{D}=\\{(\\pi(x_{0},x_{1}\\,|\\,i))\\}_{i=1}^{N}$ are given as empirical distributions,   \n182 i.e. they are represented as samples from some unknown density $\\pi$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\{(x_{0}^{j},x_{1}^{j})\\}_{j=1}^{N_{i}}\\,,\\enspace(x_{0}^{j},x_{1}^{j})\\sim\\pi(x_{0},x_{1}\\mid i)\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "183 where $N_{i}$ is the size of the $i$ -th population. For instance, for the diffusion process considered in   \n184 Example 2, the samples from $\\pi(x_{0},x_{1}\\mid i)$ can be generated by generating some marginal $p_{1}(x_{1}\\mid i)$   \n185 and then adding the Gaussian random variable to the samples $\\boldsymbol{x}_{1}^{j}$ . We use this model in our synthetic   \n186 experiments in Section 5.1.   \n187 Since the only available information about the populations is samples, we propose learning the   \n188 embedding of populations via a parametric model $\\varphi(p_{0},\\theta)$ , i.e. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\varphi(p_{0},\\theta)=\\varphi\\Big(\\{x_{0}^{j}\\}_{j=1}^{N_{i}},\\theta\\Big)\\,,\\;\\;(x_{0}^{j},x_{1}^{j})\\sim\\pi(x_{0},x_{1}\\,|\\,i)\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "189 For this purpose, we employ GNNs, which recently have been successfully applied for simulation of   \n190 complicated many-body problems in physics (Sanchez-Gonzalez et al., 2020). To embed a population   \n191 $\\{x_{0}^{j}\\}_{j=1}^{N_{i}}$ , we create a $\\mathbf{k}$ -nearest neighbour graph $G_{i}$ based on the metric in the state-space $\\mathcal{X}$ , input it   \n192 into a GNN, which consists of several message-passing iterations (Gilmer et al., 2017) and the final   \n193 average-pooling across nodes to produce the embedding vector. Finally, we update the parameters of   \n194 the GNN jointly with the parameters of the vector field to minimize the loss function in Eq. (17). ", "page_idx": 5}, {"type": "text", "text": "195 4 Related Work ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "196 The meta-learning of probability measures was previously studied by Amos et al. (2022) where they   \n197 demonstrate that the prediction of the optimal transport paths can be efficiently amortized over the   \n198 input marginal measures. The main difference with our approach is that we are trying to learn the   \n199 push-forward map without embedding the second marginal.   \n200 Generative modeling for single cells. Single cell data has expanded to encompass multiple modalities   \n201 of data profiling cell state and activities (Frangieh et al., 2021; Bunne et al., 2023b). Single-cell   \n202 data presents multiple challenges in terms of noise, non-time resolved, and high dimension, and   \n203 generative models have been used to counter those problems. Autoencoder has been used to embed   \n204 and extrapolate data Out Of Distribution (OOD) with its latent state dimension (Lotfollahi et al., 2019;   \n205 Lopez et al., 2018; Hetzel et al., 2022). Orthogonal non-negative matrix factorization (oNMF) has   \n206 also been used for dimensionality reduction combined with mixture models for cell state prediction   \n207 (Chen et al., 2020). Other approaches have tried to use Flow Matching (FM) (Tong et al., 2023, 2024;   \n208 Neklyudov et al., 2023) or similar approaches such as the Monge gap (Uscidda and Cuturi, 2023) to   \n209 predict cell trajectories. Currently, the state of the art method uses the principle of Optimal Transport   \n210 (OT) to predict cell trajectories with Input Convex Neural Network (ICNN) (Makkuva et al., 2020;   \n211 Bunne et al., 2023b). What determines the significance of the method is its capability in generalizing   \n212 out of distribution to a new population of cells, which may be from different culture or individuals.   \n213 As of this time, our method is the only method that takes inter-cellular interactions into account.   \n214 Generative modeling for physical processes. The closest approach to ours is the prediction of the   \n215 many-body interactions in physics (Sanchez-Gonzalez et al., 2020) via GNNs. However, the problem   \n216 there is very different since these models use the information about the individual trajectories of   \n217 samples, which are not available for the single-cell prediction. Neklyudov et al. (2022) consider   \n218 learning the vector field for any continuous time-evolution of a probability measure, however, their   \n219 method is restricted to single curves and do not consider generalization to unseen data. Finally, the   \n220 weather/climate forecast models generating the next state conditioned on the previous one (Price   \n221 et al., 2023; Verma et al., 2024) are similar approaches to ours but operating on a much finer time   \n222 resolution. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "223 5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "224 To show the effectiveness of MFM to generalize under previously unseen populations for the task   \n225 population prediction, we consider two experimental settings. (i) A synthetic experiment with well   \n226 defined coupled populations, and (ii) experiments on a publicly available single-cell dataset consisting   \n227 of populations from patient dependent treatment response trials. To quantify model performance,   \n228 we consider three distributional distances metrics: the 1-Wasserstein distance $(\\mathcal{W}_{1})$ , 2-Wasserstein   \n229 $(\\mathcal{W}_{2})$ distance, and the radial basis kernel maximum-mean-discrepancy (MMD) distance (Gretton   \n230 et al., 2012). We parameterize all vector field models $v_{t}(\\cdot\\,|\\,\\varphi(p_{0});\\omega)$ using a Multi-Layer Perceptron   \n231 (MLP). For MFM, we additionally parameterize $\\varphi(p_{t};\\theta,k)$ using a Graph Convolutional Network   \n232 (GCN) with a $k$ -nearest neighbour graph edge pooling layer. We include details regarding model   \n233 hyperparameters, training/optimization, and implementation in Appendix B and Appendix B.2. The   \n234 results for all the models are averaged over three random seeds. ", "page_idx": 5}, {"type": "image", "img_path": "ndIum4ByTU/tmp/c2a72c0e9bd44dc240c583c4eac182c461574d310b5f06e5ba7ebb3602ec87bc.jpg", "img_caption": ["Figure 2: Examples of model-generated samples for synthetic letters from the source distribution $\\mathit{\\Theta}^{t}=0$ ) to predicted target distribution $\\mathit{\\Theta}(t=1)$ ). See Fig. 4 in Appendix F for a larger set of examples. "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "ndIum4ByTU/tmp/60c1605fc0e737c490cccfcfc2b81325bb6fb41493daf2e6b3318488fb7b48cb.jpg", "table_caption": ["Table 1: Results of the synthetic letters experiment for population prediction on seen train populations and unseen test populations. We report the the 1-Wasserstein $(\\mathcal{W}_{1})$ , 2-Wasserstein $(\\mathcal{W}_{2})$ , and the maximum-meandiscrepancy (MMD) distributional distances. We consider 4 settings for MFM with varying $k$ . "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "235 5.1 Synthetic Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "236 We curate a synthetic dataset of the joint distributions $\\{(p_{0}(x_{0},\\mid i),p_{1}(x_{1}\\mid i))\\}_{i=1}^{N}$ by simulating a   \n237 diffusion process applied to a set of pre-defined target distributions $p_{1}(x_{1}\\mid i)$ for $i=1,\\ldots,N$ . To get   \n238 a paired population $\\bar{p}_{0}(x_{0}\\mid i)$ we simulate the forward diffusion process without drift $x_{0}\\sim\\mathcal{N}(x_{1},\\bar{\\sigma})$ .   \n239 After this setup, for reasonable values of $\\sigma$ , we assume that one can reverse the diffusion process and   \n240 learn the push-forward map from $p_{0}(x_{0}\\mid i)$ to $p_{1}(x_{1}\\mid i)$ for every index $i$ . For this task, given the $i$ -th   \n241 population index we denote $p_{0}(x_{0}\\mid i)$ as the source population $p_{1}(x_{1}\\mid i)$ as the $i^{\\th}$ -th target population.   \n242 To construct $p_{1}(x_{1}\\mid i)$ , we discretize samples from a defined silhouette; e.g. an image of a character,   \n243 where $i$ indexes the respective character. We use upper case letters as the silhouette and generate   \n244 the corresponding samples $x_{1}\\sim p_{1}(x_{1}\\mid i)$ from the uniform distribution over the silhouette and run   \n245 the diffusion process for samples $x_{1}$ to acquire $x_{0}$ . We construct the training data using 10 random   \n246 orientations of 24 letters, while only using the upright orientation for the remaining letters \u201cX\u201d and   \n247 \u201cY\u201d. We construct the test data by using 10 random orientations of \u201cX\u201d and \u201cY\u201d (validation and test,   \n248 respectively) that differ from the upright orientations of the same letters in the training data. We   \n249 do this to simplify the generalization task \u2013 the model will see the shapes of \u201cX\u201d and \u201cY\u201d during   \n250 training, but the same letters under different orientations remain unseen.   \n251 We train FM, CGFM and 4 variants of MFM of varying $k$ for the GCN population embedding model   \n252 $\\varphi(p_{t};\\theta,k)$ . When $k=0$ , $\\varphi(p_{t};\\theta,k)$ becomes identical to the DeepSets model (Zaheer et al., 2017).   \n253 We compare MFM to Flow-Matching (FM) and Conditional Generation via Flow-Matching (CGFM).   \n254 FM does not have access to conditional information; hence will only learn an aggregated lens of the   \n255 distribution dynamics and will not be able to fti the training data, and consequently won\u2019t generalize   \n256 to the test conditions. For the training data, CGFM vector field model takes in the distribution index   \n257 $i$ as a one-hot input condition. On the test set, since none of these indices is present, we input the   \n258 normalized constant vector, which averages the learned embeddings of the indices. Because of this,   \n259 CGFM will fit the training data, however, will not be able to generalize to the unseen condition in   \n260 the test dataset. Note that the CGFM can be viewed as an idealized model for the train data since   \n261 it gets perfect information regarding the population conditions. We use CGFM to assess if other   \n262 models are ftiting the data. For MFM, we expect to both fti the training data and generalize to unseen   \n263 distributional conditions.   \n264 In Fig. 2, we observe that indeed FM fails to adequately learn to sample from $p_{1}(x_{1}\\mid i)$ in the training   \n265 set, and likewise fails to generalize, while CGFM is able to effectively sample from $p_{1}(x_{1}\\mid i)$ in   \n266 the training set, but fails to generalize. We report results for the synthetic experiment in Table 1.   \n267 As expected, CGFM fits the training data, however, fails to generalize beyond its set of training   \n268 conditions. In contrast, we see that MFM is able to both fit the training data (approaching the   \n269 performance of CGFM) while also generalizing to the unseen test distributions. FM fails to fit the   \n270 train data and fails to generalize under the test conditions. Interestingly, although MFM performs   \n271 better for certain values of $k$ versus others, overall performance does not vary significantly for the   \n272 range considered. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "ndIum4ByTU/tmp/bcf5e26b457a55d767587f71da4df44ecf1886fb0911fdba0058cdc4116025f8.jpg", "img_caption": ["Figure 3: Organoid drug-screen dataset overview. Left: a given replica consists of a control distribution $p_{0}$ and corresponding treatment response distribution $p_{1}$ for treatment condition $c_{i}$ . Right: train and test data splits for replica (top) and patients (bottom) splits, restively. For each experiment there are 11 treatments, 10 patients and 3 culture conditions. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "273 5.2 Experiments on Organoid Drug-screen Data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "274 Data. For experiments on biological data, we use the organoid drug-screen dataset from Ramos Zap  \n275 atero et al. (2023). This dataset is a single-cell mass-cytometry dataset collected over 10 patients.   \n276 Somewhat unique to this dataset, unlike many prior perturbation-screen datasets which have a single   \n277 control population, this dataset has matched controls to each experimental condition. Populations from   \n278 each patient are treated with 11 different drug treatments of varying dose concentrations.1 We use the   \n279 term replicate to define control-treatment population pairs, $p_{0}(x_{0}\\mid c_{i})$ and $p_{1}(x_{1}\\mid c_{i})$ , respectively   \n280 (see Fig. 3-left). In each patient, cell population are categorized into 3 cell cultures: (i) cancer associ  \n281 ated Fibroblasts, (ii) patient-derived organoid cancer cells (PDO), and (iii) patient-derived organoid   \n282 cancer cells co-cultured fibroblasts (PDOF). We report results averaged over Fibroblast/PDO/PDOF   \n283 cultures and results for the individual cultures (this is reported in Appendix F).   \n284 Pre-processing and data splits. We filter each cell population to contain at least 1000 cells and   \n285 consider 43 bio-markers. We consider two data splits for the organoid drug-screen dataset (see   \n286 Fig. 3-right). (1) Replicate split; here we leave-out replicates evenly across all patients for testing. (2)   \n287 Patients split; here we leave-out replicates fully in one patients \u2013 in this setting, we are testing the   \n288 ability of of model to generalize population prediction of treatment response for unseen patients. In   \n289 both settings, we normalize the data and embed it into a lower dimensional principle components   \n290 (PC) representation. We do this to reduce the dimensionality of the data and to extract the relevant   \n291 information from the 43 bio-markers (features) of the ambient space. We train and evaluate all models   \n292 in the PC space. For all organoid drug-screen dataset experiments we use $\\mathrm{PC}{=}10$ . Further details   \n293 regarding data pre-processing and data splits are provided in Appendix B.2.   \n294 For the organoid drug-screen experiments, we consider an ICNN architecture in addition to the   \n295 Flow-matching models. The ICNN model is based on CellOT (Bunne et al., 2023a); a method for   \n296 learning cell specific response to treatments. The ICNN (and likewise CellOT) counterparts our FM   \n297 model in that it does not take the population index $i$ as a condition. Therefore, it will neither be able   \n298 to fit the training data, nor generalize.   \n299 Predicting treatment response across replicates. We show results for generalization across repli  \n300 cates in Table 2. As expected, we observe that CGFM ftis the training data, but does not generalize to   \n301 the test replicates. With this, we can observe that the FM and ICNN models fail to fit the train data,   \n302 relative to CGFM, and also fail to generalize. MFM $k=10$ ) performs best on generalization to   \n303 unseen replicates. We include results reported for the separate cell cultures in Table 4 in Appendix F.   \n304 Predicting treatment response across patients. We show results for generalization across patients   \n305 in Table 3. Similar to the replicates data setting, we observe that CGFM fits the training data, but   \n306 does not generalize to the test replicates. Likewise, the FM and ICNN models fail to fti the train data,   \n307 relative to CGFM, and also fail to generalize. MFM $k=10$ ) performs best on generalization to   \n308 unseen replicates. We include results reported for the separate cell cultures in Table 5 in Appendix F.   \n309 Through the biological and synthetic experiments, we have shown that MFM is able to generalize   \n310 to unseen distributions/populations. The implication of our results suggest that MFM can learn   \n311 population dynamics in unseen environments. In biological contexts, like the one we have shown   \n312 in this work, this result indicates that we can learn population dynamics, of treatment response or   \n313 any arbitrary perturbation, in new/unseen patients. This works towards a model where it is possible   \n314 to predict and design an individualized treatment regimen for each patient based on their individual   \n315 characteristics and tumor microenvironment. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "ndIum4ByTU/tmp/9c61d592debb2acc5c7e9b01e241bf00576cc57ee834bc389d1f71772ba096f7.jpg", "table_caption": ["Table 2: Experimental results on the organoid drug-screen dataset for population prediction of treatment response across replicate populations averaged over co-culture conditions. Results are reported for models trained on data embedded into 10 principle components. We report the the 1-Wasserstein $(\\mathcal{W}_{1})$ , 2-Wasserstein $(\\mathcal{W}_{2})$ , and the maximum-mean-discrepancy (MMD) distributional distances. We consider two settings for MFM with varying nearest-neighbours parameter. For extended results in Table 4. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "ndIum4ByTU/tmp/83ef7559eec34b7c1734e4b4f9495b3ec9c89eb91d192b38820ea905f02fa407.jpg", "table_caption": ["Table 3: Experimental results on the organoid drug-screen dataset for population prediction of treatment response across patient populations. Results shown in this table are broken out in Table 5. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "316 6 Conclusion and Future Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "317 Our paper highlights the significance of modeling dynamics based on the entire distribution. While   \n318 flow-based models offer a promising avenue for learning dynamics at the population level, they were   \n319 previously restricted to a single initial population and predefined conditions.   \n320 In this paper, we introduce Meta Flow Matching (MFM) as a practical solution to address these   \n321 limitations. By integrating along vector fields of the Wasserstein manifold, MFM allows for a more   \n322 comprehensive model of dynamical systems with interacting particles. Crucially, MFM leverages   \n323 graph neural networks to embed the initial population, enabling the model to generalize over various   \n324 initial distributions. MFM opens up new possibilities for understanding complex phenomena that   \n325 emerge from interacting systems in biological and physical systems.   \n326 In practice, we demonstrate that MFM learns meaningful embeddings of single-cell populations along   \n327 with the developmental model of these populations. Moreover, our empirical study demonstrates the   \n328 possibility of modeling patient-specific response to treatments via the meta-learning. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "329 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "330 Albergo, M. S. and Vanden-Eijnden, E. (2022). Building normalizing flows with stochastic inter  \n331 polants. arXiv preprint arXiv:2209.15571.   \n332 Ambrosio, L., Gigli, N., and Savar\u00e9, G. (2008). Gradient flows: in metric spaces and in the space of   \n333 probability measures. Springer Science & Business Media.   \n334 Amos, B., Cohen, S., Luise, G., and Redko, I. (2022). Meta optimal transport. arXiv preprint   \n335 arXiv:2206.05262.   \n336 Amos, B. et al. (2023). Tutorial on amortized optimization. Foundations and Trends\u00ae in Machine   \n337 Learning, 16(5):592\u2013732.   \n338 Armingol, E., Officer, A., Harismendy, O., and Lewis, N. E. (2020). Deciphering cell\u2013cell interactions   \n339 and communication from gene expression. Nature Reviews Genetics, 22(2):71\u201388.   \n340 Benamou, J.-D. (2003). Numerical resolution of an \u201cunbalanced\u201d mass transport problem. ESAIM:   \n341 Mathematical Modelling and Numerical Analysis, 37(5):851\u2013868.   \n342 Binnewies, M., Roberts, E. W., Kersten, K., Chan, V., Fearon, D. F., Merad, M., Coussens, L. M.,   \n343 Gabrilovich, D. I., Ostrand-Rosenberg, S., Hedrick, C. C., Vonderheide, R. H., Pittet, M. J., Jain,   \n344 R. K., Zou, W., Howcroft, T. K., Woodhouse, E. C., Weinberg, R. A., and Krummel, M. F. (2018).   \n345 Understanding the tumor immune microenvironment (time) for effective therapy. Nature Medicine,   \n346 24(5):541\u2013550.   \n347 Bunne, C., Stark, S. G., Gut, G., Del Castillo, J. S., Levesque, M., Lehmann, K.-V., Pelkmans, L.,   \n348 Krause, A., and R\u00e4tsch, G. (2023a). Learning single-cell perturbation responses using neural   \n349 optimal transport. Nature Methods, 20(11):1759\u20131768.   \n350 Bunne, C., Stark, S. G., Gut, G., del Castillo, J. S., Levesque, M., Lehmann, K.-V., Pelkmans, L.,   \n351 Krause, A., and R\u00e4tsch, G. (2023b). Learning single-cell perturbation responses using neural   \n352 optimal transport. Nature Methods, 20(11):1759\u20131768.   \n353 Chen, S., Rivaud, P., Park, J. H., Tsou, T., Charles, E., Haliburton, J. R., Pichiorri, F., and Thomson,   \n354 M. (2020). Dissecting heterogeneous cell populations across drug and disease conditions with   \n355 popalign. Proceedings of the National Academy of Sciences, 117(46):28784\u201328794.   \n356 Chen, T., Chen, X., Chen, W., Heaton, H., Liu, J., Wang, Z., and Yin, W. (2022). Learning to optimize:   \n357 A primer and a benchmark. Journal of Machine Learning Research, 23(189):1\u201359.   \n358 Chizat, L., Peyr\u00e9, G., Schmitzer, B., and Vialard, F.-X. (2018). Unbalanced optimal transport:   \n359 Dynamic and kantorovich formulations. Journal of Functional Analysis, 274(11):3090\u20133123.   \n360 Chung, W., Eum, H. H., Lee, H.-O., Lee, K.-M., Lee, H.-B., Kim, K.-T., Ryu, H. S., Kim, S., Lee, J. E.,   \n361 Park, Y. H., Kan, Z., Han, W., and Park, W.-Y. (2017). Single-cell rna-seq enables comprehensive   \n362 tumour and immune cell profiling in primary breast cancer. Nature Communications, 8(1).   \n363 Dao, Q., Phung, H., Nguyen, B., and Tran, A. (2023). Flow matching in latent space. arXiv preprint   \n364 arXiv:2307.08698.   \n365 De Bortoli, V., Thornton, J., Heng, J., and Doucet, A. (2021). Diffusion schr\u00f6dinger bridge with   \n366 applications to score-based generative modeling. Advances in Neural Information Processing   \n367 Systems, 34:17695\u201317709.   \n368 Frangieh, C. J., Melms, J. C., Thakore, P. I., Geiger-Schuller, K. R., Ho, P., Luoma, A. M., Cleary, B.,   \n369 Jerby-Arnon, L., Malu, S., Cuoco, M. S., Zhao, M., Ager, C. R., Rogava, M., Hovey, L., Rotem,   \n370 A., Bernatchez, C., Wucherpfennig, K. W., Johnson, B. E., Rozenblatt-Rosen, O., Schadendorf,   \n371 D., Regev, A., and Izar, B. (2021). Multimodal pooled perturb-cite-seq screens in patient models   \n372 define mechanisms of cancer immune evasion. Nature Genetics, 53(3):332\u2013341.   \n373 Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. (2017). Neural message   \n374 passing for quantum chemistry. In International conference on machine learning, pages 1263\u20131272.   \n375 PMLR.   \n376 Goodenough, D. A. and Paul, D. L. (2009). Gap junctions. Cold Spring Harb Perspect Biol,   \n377 1(1):a002576.   \n378 Gretton, A., Borgwardt, K. M., Rasch, M. J., Sch\u00f6lkopf, B., and Smola, A. (2012). A kernel   \n379 two-sample test. The Journal of Machine Learning Research, 13(1):723\u2013773.   \n380 Gulati, G. S., Sikandar, S. S., Wesche, D. J., Manjunath, A., Bharadwaj, A., Berger, M. J., Ilagan, F.,   \n381 Kuo, A. H., Hsieh, R. W., Cai, S., Zabala, M., Scheeren, F. A., Lobo, N. A., Qian, D., Yu, F. B.,   \n382 Dirbas, F. M., Clarke, M. F., and Newman, A. M. (2020). Single-cell transcriptional diversity is a   \n383 hallmark of developmental potential. Science, 367(6476):405\u2013411.   \n384 Hashimoto, T. B., Gifford, D. K., and Jaakkola, T. S. (2016). Learning population-level diffusions   \n385 with generative recurrent networks. In Proceedings of the 33rd International Conference on   \n386 Machine Learning, pages 2417\u20132426.   \n387 Hetzel, L., Boehm, S., Kilbertus, N., G\u00fcnnemann, S., Lotfollahi, M., and Theis, F. (2022). Predicting   \n388 cellular responses to novel drug perturbations at a single-cell resolution. In Koyejo, S., Mohamed,   \n389 S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A., editors, Advances in Neural Information   \n390 Processing Systems, volume 35, pages 26711\u201326722. Curran Associates, Inc.   \n391 Huguet, G., Magruder, D. S., Tong, A., Fasina, O., Kuchroo, M., Wolf, G., and Krishnaswamy, S.   \n392 (2022). Manifold interpolating optimal-transport flows for trajectory inference.   \n393 Huguet, G., Tong, A., Zapatero, M. R., Wolf, G., and Krishnaswamy, S. (2023). Geodesic sinkhorn:   \n394 Optimal transport for high-dimensional datasets. In IEEE MLSP.   \n395 Isobe, N., Koyama, M., Hayashi, K., and Fukumizu, K. (2024). Extended flow matching: a method   \n396 of conditional generation with generalized continuity equation. arXiv preprint arXiv:2402.18839.   \n397 Ji, Y., Lotfollahi, M., Wolf, F. A., and Theis, F. J. (2021). Machine learning for perturbational   \n398 single-cell omics. Cell Systems, 12(6):522\u2013537.   \n399 Koshizuka, T. and Sato, I. (2023). Neural lagrangian schr\\\"odinger bridge. In ICLR.   \n400 Lipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., and Le, M. (2023). Flow matching for   \n401 generative modeling. In The Eleventh International Conference on Learning Representations.   \n402 Liu, G.-H., Vahdat, A., Huang, D.-A., Theodorou, E. A., Nie, W., and Anandkumar, A. (2023). $\\mathrm{I}^{2}\\mathrm{sb}$ :   \n403 Image-to-image schr\u00f6dinger bridge. In ICML.   \n404 Liu, X., Gong, C., and Liu, Q. (2022). Flow straight and fast: Learning to generate and transfer data   \n405 with rectified flow. arXiv preprint arXiv:2209.03003.   \n406 Lopez, R., Regier, J., Cole, M. B., Jordan, M. I., and Yosef, N. (2018). Deep generative modeling for   \n407 single-cell transcriptomics. Nature Methods, 15(12):1053\u20131058.   \n408 Lotfollahi, M., Wolf, F. A., and Theis, F. J. (2019). scgen predicts single-cell perturbation responses.   \n409 Nature Methods, 16(8):715\u2013721.   \n410 Makkuva, A. V., Taghvaei, A., Oh, S., and Lee, J. D. (2020). Optimal transport mapping via input   \n411 convex neural networks. In ICML.   \n412 Mol\u00e8, M. A., Coorens, T. H. H., Shahbazi, M. N., Weberling, A., Weatherbee, B. A. T., Gantner,   \n413 C. W., Sancho-Serra, C., Richardson, L., Drinkwater, A., Syed, N., Engley, S., Snell, P., Christie,   \n414 L., Elder, K., Campbell, A., Fishel, S., Behjati, S., Vento-Tormo, R., and Zernicka-Goetz, M.   \n415 (2021). A single cell characterisation of human embryogenesis identifies pluripotency transitions   \n416 and putative anterior hypoblast centre. Nature Communications, 12(1).   \n417 Neklyudov, K., Brekelmans, R., Tong, A., Atanackovic, L., Liu, Q., and Makhzani, A. (2023). A com  \n418 putational framework for solving wasserstein lagrangian flows. arXiv preprint arXiv:2310.10649.   \n419 Neklyudov, K., Severo, D., and Makhzani, A. (2022). Action matching: A variational method for   \n420 learning stochastic dynamics from samples.   \n421 Otto, F. (2001). The geometry of dissipative evolution equations: the porous medium equation.   \n422 Peidli, S., Green, T. D., Shen, C., Gross, T., Min, J., Garda, S., Yuan, B., Schumacher, L. J., Taylor  \n423 King, J. P., Marks, D. S., et al. (2024). scperturb: harmonized single-cell perturbation data. Nature   \n424 Methods, pages 1\u201310.   \n425 Peyr\u00e9, G. and Cuturi, M. (2019). Computational Optimal Transport. arXiv:1803.00567.   \n426 Pooladian, A.-A., Ben-Hamu, H., Domingo-Enrich, C., Amos, B., Lipman, Y., and Chen, R. T.   \n427 (2023). Multisample flow matching: Straightening flows with minibatch couplings. arXiv preprint   \n428 arXiv:2304.14772.   \n429 Price, I., Sanchez-Gonzalez, A., Alet, F., Ewalds, T., El-Kadi, A., Stott, J., Mohamed, S., Battaglia,   \n430 P., Lam, R., and Willson, M. (2023). Gencast: Diffusion-based ensemble forecasting for medium  \n431 range weather. arXiv preprint arXiv:2312.15796.   \n432 Ramos Zapatero, M., Tong, A., Opzoomer, J. W., O\u2019Sullivan, R., Cardoso Rodriguez, F., Suf,i J.,   \n433 Vlckova, P., Nattress, C., Qin, X., Claus, J., Hochhauser, D., Krishnaswamy, S., and Tape, C. J.   \n434 (2023). Trellis tree-based analysis reveals stromal regulation of patient-derived organoid drug   \n435 responses. Cell, 186(25):5606\u20135619.e24.   \n436 Rizvi, A. H., Camara, P. G., Kandror, E. K., Roberts, T. J., Schieren, I., Maniatis, T., and Rabadan, R.   \n437 (2017). Single-cell topological rna-seq analysis reveals insights into cellular differentiation and   \n438 development. Nature Biotechnology, 35(6):551\u2013560.   \n439 Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022). High-resolution image   \n440 synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer   \n441 vision and pattern recognition, pages 10684\u201310695.   \n442 Saharia, C., Chan, W., Chang, H., Lee, C., Ho, J., Salimans, T., Fleet, D., and Norouzi, M. (2022a).   \n443 Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 conference proceedings,   \n444 pages 1\u201310.   \n445 Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes,   \n446 R., Karagol Ayan, B., Salimans, T., et al. (2022b). Photorealistic text-to-image diffusion models   \n447 with deep language understanding. Advances in neural information processing systems, 35:36479\u2013   \n448 36494.   \n449 Sanchez-Gonzalez, A., Godwin, J., Pfaff, T., Ying, R., Leskovec, J., and Battaglia, P. (2020). Learning   \n450 to simulate complex physics with graph networks. In International conference on machine learning,   \n451 pages 8459\u20138468. PMLR.   \n452 Schiebinger, G., Shu, J., Tabaka, M., Cleary, B., Subramanian, V., Solomon, A., Gould, J., Liu,   \n453 S., Lin, S., Berube, P., et al. (2019). Optimal-transport analysis of single-cell gene expression   \n454 identifies developmental trajectories in reprogramming. Cell, 176(4):928\u2013943.   \n455 Somnath, V. R., Pariset, M., Hsieh, Y.-P., Martinez, M. R., Krause, A., and Bunne, C. (2023). Aligned   \n456 diffusion schr\\\"odinger bridges. In UAI.   \n457 Tong, A., FATRAS, K., Malkin, N., Huguet, G., Zhang, Y., Rector-Brooks, J., Wolf, G., and Bengio,   \n458 Y. (2024). Improving and generalizing flow-based generative models with minibatch optimal   \n459 transport. Transactions on Machine Learning Research. Expert Certification.   \n460 Tong, A., Huang, J., Wolf, G., Van Dijk, D., and Krishnaswamy, S. (2020). Trajectorynet: A dynamic   \n461 optimal transport network for modeling cellular dynamics. In International conference on machine   \n462 learning, pages 9526\u20139536. PMLR.   \n463 Tong, A., Malkin, N., Fatras, K., Atanackovic, L., Zhang, Y., Huguet, G., Wolf, G., and Bengio,   \n464 Y. (2023). Simulation-free schr\\\" odinger bridges via score and flow matching. arXiv preprint   \n465 arXiv:2307.03672.   \n466 Uscidda, T. and Cuturi, M. (2023). The monge gap: A regularizer to learn all transport maps. In   \n467 Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J., editors, Proceedings   \n468 of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine   \n469 Learning Research, pages 34709\u201334733. PMLR.   \n470 Verma, Y., Heinonen, M., and Garg, V. (2024). Climode: Climate and weather forecasting with   \n471 physics-informed neural odes. arXiv preprint arXiv:2404.10024.   \n472 Villani, C. (2009). Optimal transport: old and new, volume 338. Springer.   \n473 Weinreb, C., Wolock, S., Tusi, B. K., Socolovsky, M., and Klein, A. M. (2018). Fundamental limits   \n474 on dynamic inference from single-cell snapshots. 115(10):E2467\u2013E2476.   \n475 Yang, K. D. and Uhler, C. (2019). Scalable unbalanced optimal transport using generative adversarial   \n476 networks. In 7th International Conference on Learning Representations, page 20.   \n477 Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R., and Smola, A. J. (2017).   \n478 Deep sets. Advances in neural information processing systems, 30.   \n479 Zeng, T. and Dai, H. (2019). Single-cell rna sequencing-based computational analysis to describe   \n480 disease heterogeneity. Frontiers in Genetics, 10.   \n481 Zheng, Q., Le, M., Shaul, N., Lipman, Y., Grover, A., and Chen, R. T. (2023). Guided flows for   \n482 generative modeling and decision making. arXiv preprint arXiv:2311.13443. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "483 A Proof of Theorem 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "484 Theorem 1. Consider a dataset of populations $\\mathcal{D}=\\{(\\pi(x_{0},x_{1}\\,|\\,i))\\}_{i}$ generated from some unknown   \n485 conditional model $\\pi(x_{0},x_{1}\\mid c)p(c)$ . Then the following objective ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\omega,\\theta)=\\;\\mathbb{E}_{p(c)}\\int_{0}^{1}d t\\;\\mathbb{E}_{p_{t}(x_{t}\\mid c)}\\|v_{t}^{*}(x_{t}\\mid c)-v_{t}(x_{t}\\mid\\varphi(p_{0},\\theta),\\omega)\\|^{2}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "486 is equivalent to the Meta Flow Matching objective ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{M F M}(\\omega,\\theta)=\\;\\mathbb{E}_{i\\sim\\mathcal{D}}\\mathbb{E}_{\\pi(x_{0},x_{1}\\mid i)}\\int_{0}^{1}d t\\,\\left\\|{\\frac{\\partial}{\\partial t}}f_{t}(x_{0},x_{1})-v_{t}(f_{t}(x_{0},x_{1})\\mid\\varphi(p_{0},\\theta);\\omega)\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "487 up to an additive constant. ", "page_idx": 13}, {"type": "text", "text": "488 Proof. The loss function ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathcal{L}(\\omega,\\theta)=\\;\\mathbb{E}_{p(c)}\\int_{0}^{1}d t\\;\\mathbb{E}_{p_{t}(x_{t}\\mid c)}\\|v_{t}^{*}(x_{t}\\mid c)-v_{t}(x_{t}\\mid\\varphi(p_{t},\\theta);\\omega)\\|^{2}}}\\\\ {=}&{-\\,2\\mathbb{E}_{p(c)}\\int d t d x\\;\\langle p_{t}(x\\mid c)v_{t}^{*}(x\\mid c),v_{t}(x\\mid\\varphi(p_{t},\\theta);\\omega)\\rangle+}\\\\ &{+\\,\\mathbb{E}_{p(c)}\\int_{0}^{1}d t\\;\\mathbb{E}_{p_{t}(x_{t}\\mid c)}\\|v_{t}(x_{t}\\mid\\varphi(p_{t},\\theta),\\omega)\\|^{2}+}\\\\ &{+\\,\\mathbb{E}_{p(c)}\\int_{0}^{1}d t\\;\\mathbb{E}_{p_{t}(x_{t}\\mid c)}\\|v_{t}^{*}(x_{t}\\mid c)\\|^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "489 The last term does not depend on $\\theta$ , the second term we can estimate, for the first term, we use the   \n490 formula for the (from Eq. (8)) ", "page_idx": 13}, {"type": "equation", "text": "$$\np_{t}(\\xi\\,|\\,c)v_{t}^{*}(\\xi\\,|\\,c)=\\mathbb{E}_{\\pi(x_{0},x_{1})}\\delta\\bigl(f_{t}(x_{0},x_{1})-\\xi\\bigr)\\frac{\\partial f_{t}(x_{0},x_{1})}{\\partial t}\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "491 Thus, the loss is equivalent (up to a constant) to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{L}(\\omega,\\theta)=}&{-\\,2\\mathbb{E}_{p(c)}\\mathbb{E}_{\\pi(x_{0},x_{1}\\mid c)}\\int d t\\,\\Big\\langle\\frac{\\partial f_{t}(x_{0},x_{1})}{\\partial t},v_{t}\\big(f_{t}(x_{0},x_{1})\\,\\big\\vert\\,\\varphi(p_{t},\\theta);\\omega\\big)\\Big\\rangle+}\\\\ &{+\\,\\mathbb{E}_{p(c)}\\mathbb{E}_{\\pi(x_{0},x_{1}\\mid c)}\\int_{0}^{1}d t\\,\\big\\lVert v_{t}\\big(f_{t}(x_{0},x_{1})\\,\\big\\vert\\,\\varphi(p_{t},\\theta),\\omega\\big)\\big\\rVert^{2}\\pm}\\\\ &{\\pm\\,\\mathbb{E}_{p(c)}\\mathbb{E}_{\\pi(x_{0},x_{1}\\mid c)}\\int_{0}^{1}d t\\,\\left\\lVert\\frac{\\partial f_{t}(x_{0},x_{1})}{\\partial t}\\right\\rVert^{2}}\\\\ &{=\\,\\mathbb{E}_{c\\sim p(c)}\\mathbb{E}_{\\pi(x_{0},x_{1}\\mid c)}\\int_{0}^{1}d t\\,\\left\\lVert\\frac{\\partial}{\\partial t}f_{t}(x_{0},x_{1})-v_{t}\\big(f_{t}(x_{0},x_{1})\\,\\big\\vert\\,\\varphi(p_{t},\\theta);\\omega\\big)\\right\\rVert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "492 Note that in the final expression we do not need access to the probabilistic model of $p(c)$ if the joints   \n493 $\\pi(x_{0},x_{1}\\mid c)$ are already sampled in the data $\\mathcal{D}$ . Thus, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\omega,\\theta)=\\;\\mathbb{E}_{c\\sim p(c)}\\mathbb{E}_{\\pi(x_{0},x_{1}\\mid c)}\\int_{0}^{1}d t\\;\\bigg\\|\\frac{\\partial}{\\partial t}f_{t}(x_{0},x_{1})-v_{t}\\big(f_{t}(x_{0},x_{1})\\bigm|\\varphi(p_{t},\\theta);\\omega\\big)\\bigg\\|^{2}}\\\\ &{\\qquad=\\;\\mathbb{E}_{i\\sim\\mathcal{D}}\\mathbb{E}_{\\pi(x_{0},x_{1}\\mid i)}\\int_{0}^{1}d t\\;\\bigg\\|\\frac{\\partial}{\\partial t}f_{t}(x_{0},x_{1})-v_{t}\\big(f_{t}(x_{0},x_{1})\\bigm|\\varphi(p_{t},\\theta);\\omega\\big)\\bigg\\|^{2}}\\\\ &{\\qquad=\\;\\mathcal{L}_{\\mathrm{MFM}}(\\omega,\\theta)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "494 ", "page_idx": 13}, {"type": "text", "text": "495 B Experimental Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "496 B.1 Synthetic letters data ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "497 The synthetic letters dataset contains 242 train populations a 10 test populations. Each population   \n498 contains roughly between 750 and 2700 samples. In this dataset. ", "page_idx": 13}, {"type": "text", "text": "499 B.2 Organoid drug-screen data ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "500 The organoid drug-screen dataset contains a total of 927 replicates (or coupled populations). In the   \n501 replicates split, we use 713 populations for training and 103 left-out populations for testing. In the   \n502 patients split, we use 861 populations for training and 33 left-out populations for testing. ", "page_idx": 14}, {"type": "text", "text": "503 B.3 Model architectures and hyperparameters ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "504 ICNN. The ICNN baseline was constructed with two networks ICNN network $f(x)$ and $g(x)$ , with   \n505 non-negative leaky ReLU activation layers. $f(x)$ is used to minimize the transport distance and $g(x)$   \n506 is used to transport from source to target. It has four hidden units with width of 64, and a latent   \n507 dimension of 50. Both networks uses Adam optimizer $(\\mathrm{lr}{=}1e-4$ , $\\beta_{1}{=}0.5$ , $\\beta_{2}{=}0.9\\$ ). $g(x)$ is trained   \n508 with an inner iteration of 10 for every iteration $f(x)$ is trained.   \n509 Vector Field Models. All vector field models $v_{t}$ are parameterized 4 linear layers with 512 hidden   \n510 units and SELU activation functions. The FM vector field model additionally takes a conditional   \n511 input for the one-hot treatment encoding. CGFM takes the conditional input for the one-hot treatment   \n512 conditions as well as a one-hot encoding for the population index condition $i$ . The MFM vector field   \n513 model takes population embedding conditions, that is output from the GCN, as input, as well as the   \n514 treatment one-hot encoding. All vector field models use temporal embeddings for time and positional   \n515 embeddings for the input samples. We did not sweep the size of this embeddings space and found   \n516 that a temporal embedding and positional embeddings sizes of 128 worked sufficiently well.   \n517 Graph Neural Network. We considered a GCN model that consists of a $k$ -nearest neighbour graph   \n518 edge pooling layer and 3 graph convolution layers with 512 hidden units. The final GCN model   \n519 layer outputs an embedding representation $e\\in\\mathbb{R}^{d}$ . For the Synthetic experiment, we found that   \n520 $d=256$ performed well, and $d=128$ performed well for the biological experiments. We normalize   \n521 and project embeddings onto a hyper-sphere, and find that this normalization helps improve training.   \n522 Additionally, the GCN takes a one-hot cell-type encoding (encoding for Fibroblast cells or PDO   \n523 cells) for the control populations $p_{0}$ . This may be beneficial for PDOF populations where both   \n524 Fibroblast cells and PDO cells are present. However, it is important to note that labeling which cells   \n525 are Fibroblasts versus PDOs withing the PDOF cultures is difficult and noisy in itself, hence such a   \n526 cell-type condition may yield no additive information/performance gain.   \n527 Optimization. We use the Adam optimizer with a learning rate of 0.0001 for all Flow-matching   \n528 models (FM, CGFM, MFM). We also used the Adam optimizer with a learning rate of 0.0001 for   \n529 the GCN model. To train the MFM $\\operatorname{FM}\\!+\\!\\operatorname{GCN})$ ) models, we alternate between updating the vector   \n530 field model parameters $\\omega$ and the GCN model parameters $\\theta$ . We alternate between updating the   \n531 respective model parameters every epoch. FM and CGFM model were trained for 2000 epochs, while   \n532 MFM models were trained for 4000 epochs. Due to the alternating optimization, the MFM vector   \n533 field model receives half as many updates compared to its counterparts (FM and CGFM). Therefore,   \n534 training for the double the epochs is necessary for fair comparison.   \n535 The hyperparameters stated in this section were selected from brief and small grid search sweeps. We   \n536 did not conduct any thorough hyperparameter optimization. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "537 C Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "538 We implement all our experiments using PyTorch and PyTorch Geometric. We submitted our code as   \n539 supplementary material with our submission.   \n540 All experiments were conducted on a HPC cluster primarily on NVIDIA Tesla T4 16GB GPUs. Each   \n541 individual seed experiment run required only 1 GPU. Each experiment ran between 3-11 hours and   \n542 all experiments took approximately 500 GPU hours. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "543 D Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "544 In this work we explored empirically the effect of conditioning the learned flow on the initial   \n545 distribution. We argue this is a more natural model for many biological systems. However, there   \n546 are many other aspects of modeling biological systems that we did not consider. In particular we   \n547 did not consider extensions to the manifold setting (Huguet et al., 2022, 2023), unbalanced optimal   \n548 transport (Benamou, 2003; Yang and Uhler, 2019; Chizat et al., 2018), aligned (Somnath et al., 2023;   \n549 Liu et al., 2023), or stochastic settings (Bunne et al., 2023a; Koshizuka and Sato, 2023) in this work. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "550 E Broader Impacts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "551 This paper is primarily a theoretical and methodological contribution with little societal impact. MFM   \n552 can be used to better model dynamical systems of interacting particles and in particular cellular   \n553 systems. Better modeling of cellular systems can potentially be used for the development of malicious   \n554 biological agents. However, we do not see this as a significant risk at this time. ", "page_idx": 15}, {"type": "text", "text": "555 F Extended Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table 4: Experimental results on the organoid drug-screen dataset for population prediction of treatment response across replicate populations. Results are reported for models trained on data embedded into 10 principle components. We report the the 1-Wasserstein $(\\mathcal{W}_{1})$ , 2-Wasserstein $(\\mathcal{W}_{2})$ , and the maximum-mean-discrepancy (MMD) distributional distances. We consider 2 settings for MFM with varying nearest-neighbours parameter. ", "page_idx": 15}, {"type": "table", "img_path": "ndIum4ByTU/tmp/bdf2dcd03b97130b387cf25b895dd2927bbf2a60fddcaf4d557a469d3c330013.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "ndIum4ByTU/tmp/8e7f600c08165cb9c3ca2c2e521c1503ad536978ca0c891336afb7bfa28d9101.jpg", "table_caption": ["Table 5: Experimental results on the organoid drug-screen dataset for population prediction of treatment response across patient populations. Results are reported for models trained on data embedded into 10 principle components. We report the the 1-Wasserstein $(\\mathcal{W}_{1})$ , 2-Wasserstein $(\\mathcal{W}_{2})$ , and the maximum-mean-discrepancy (MMD) distributional distances. We consider 2 settings for MFM with varying nearest-neighbours parameter. "], "table_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "ndIum4ByTU/tmp/cf1a854ad50f515f1b1a5c8130e874203cf6812dd2568f973d7627beb488c35b.jpg", "img_caption": ["Figure 4: Model-generated samples for synthetic letters from the source $t=0$ ) to target $t=1$ ) distributions. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "556 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "557 1. Claims   \n558 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n559 paper\u2019s contributions and scope?   \n560 Answer: [Yes]   \n561 Justification: Claims and contributions introduced in abstract and introduction are sup  \n562 ported with theoretical result in Section 3 and empirical results through synthetic and real   \n563 experiments in Section 5.   \n564 Guidelines:   \n565 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n566 made in the paper.   \n567 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n568 contributions made in the paper and important assumptions and limitations. A No or   \n569 NA answer to this question will not be perceived well by the reviewers.   \n570 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n571 much the results can be expected to generalize to other settings.   \n572 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n573 are not attained by the paper.   \n574 2. Limitations   \n575 Question: Does the paper discuss the limitations of the work performed by the authors?   \n576 Answer: [Yes]   \n577 Justification: We discuss limitations in Appendix D.   \n578 Guidelines:   \n579 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n580 the paper has limitations, but those are not discussed in the paper.   \n581 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n582 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n583 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n584 model well-specification, asymptotic approximations only holding locally). The authors   \n585 should reflect on how these assumptions might be violated in practice and what the   \n586 implications would be.   \n587 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n588 only tested on a few datasets or with a few runs. In general, empirical results often   \n589 depend on implicit assumptions, which should be articulated.   \n590 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n591 For example, a facial recognition algorithm may perform poorly when image resolution   \n592 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n593 used reliably to provide closed captions for online lectures because it fails to handle   \n594 technical jargon.   \n595 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n596 and how they scale with dataset size.   \n597 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n598 address problems of privacy and fairness.   \n599 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n600 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n601 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n602 judgment and recognize that individual actions in favor of transparency play an impor  \n603 tant role in developing norms that preserve the integrity of the community. Reviewers   \n604 will be specifically instructed to not penalize honesty concerning limitations.   \n605 3. Theory Assumptions and Proofs   \n06 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n07 a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Justification: Theory is provided in Section 2 and Section 3. Proofs are provide in Appendix A ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "622 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: All details for reproducing results and experiments can be found through the main text body and appendix. The details include: dataset resource Ramos Zapatero et al. (2023), data processing, model architecture and optimization details, and performance metrics. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "3 5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "64 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n5 tions to faithfully reproduce the main experimental results, as described in supplemental   \n66 material?   \n7 Answer: [Yes]   \n68 Justification: The data used in the empirical study is either synthetic or publicly available.   \n69 The code reproducing all the experiments is attached to the paper.   \n70 Guidelines:   \n\u2022 The answer NA means that paper does not include experiments requiring code.   \n2 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n3 public/guides/CodeSubmissionPolicy) for more details.   \n74 \u2022 While we encourage the release of code and data, we understand that this might not be   \n75 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n6 including code, unless this is central to the contribution (e.g., for a new open-source   \n7 benchmark).   \n78 \u2022 The instructions should contain the exact command and environment needed to run to   \n9 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n80 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how   \n2 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n83 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n84 proposed method and baselines. If only a subset of experiments are reproducible, they   \n85 should state which ones are omitted from the script and why.   \n6 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n87 versions (if applicable).   \n88 \u2022 Providing as much information as possible in supplemental material (appended to the   \n89 paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper discusses the experimental setup necessary to understand the results in Section 5. Furthermore, the details of the empirical study are provided in Appendix B. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "03 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: All the results presented in the paper are averaged over multiple independent runs and the standard deviations are provided along the metrics. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 20}, {"type": "text", "text": "714 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n715 example, train/test split, initialization, random drawing of some parameter, or overall   \n716 run with given experimental conditions).   \n717 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n718 call to a library function, bootstrap, etc.)   \n719 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n720 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n721 of the mean.   \n722 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n723 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n724 of Normality of errors is not verified.   \n725 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n726 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n727 error rates).   \n728 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n729 they were calculated and reference the corresponding figures or tables in the text.   \n730 8. Experiments Compute Resources   \n731 Question: For each experiment, does the paper provide sufficient information on the com  \n732 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n733 the experiments?   \n734 Answer: [Yes]   \n735 Justification: The paper discuss the compute resources and reproducibility in Appendix C.   \n736 Guidelines:   \n737 \u2022 The answer NA means that the paper does not include experiments.   \n738 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n739 or cloud provider, including relevant memory and storage.   \n740 \u2022 The paper should provide the amount of compute required for each of the individual   \n741 experimental runs as well as estimate the total compute.   \n742 \u2022 The paper should disclose whether the full research project required more compute   \n743 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n744 didn\u2019t make it into the paper).   \n745 9. Code Of Ethics   \n746 Question: Does the research conducted in the paper conform, in every respect, with the   \n747 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n748 Answer: [Yes]   \n749 Justification: The research does conform with the NeurIPS Code of Ethics. The study   \n750 presented involves only public or synthetic data, which is freely available online. The   \n751 considered models do not impose risks of misuse or dual-use.   \n752 Guidelines:   \n753 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n754 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n755 deviation from the Code of Ethics.   \n756 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n757 eration due to laws or regulations in their jurisdiction).   \n758 10. Broader Impacts   \n759 Question: Does the paper discuss both potential positive societal impacts and negative   \n760 societal impacts of the work performed?   \n761 Answer: [Yes]   \n762 Justification: The paper discusses the broader impact in Appendix E.   \n763 Guidelines:   \n764 \u2022 The answer NA means that there is no societal impact of the work performed.   \n765 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n766 impact or why the paper does not address societal impact.   \n767 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n768 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n769 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n770 groups), privacy considerations, and security considerations.   \n771 \u2022 The conference expects that many papers will be foundational research and not tied   \n772 to particular applications, let alone deployments. However, if there is a direct path to   \n773 any negative applications, the authors should point it out. For example, it is legitimate   \n774 to point out that an improvement in the quality of generative models could be used to   \n775 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n776 that a generic algorithm for optimizing neural networks could enable people to train   \n777 models that generate Deepfakes faster.   \n778 \u2022 The authors should consider possible harms that could arise when the technology is   \n779 being used as intended and functioning correctly, harms that could arise when the   \n780 technology is being used as intended but gives incorrect results, and harms following   \n781 from (intentional or unintentional) misuse of the technology.   \n782 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n783 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n784 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n785 feedback over time, improving the efficiency and accessibility of ML).   \n786 11. Safeguards   \n787 Question: Does the paper describe safeguards that have been put in place for responsible   \n788 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n789 image generators, or scraped datasets)?   \n790 Answer: [NA] .   \n791 Justification: The models considered in the paper do not carry the risks of misuse or dual-use.   \n792 Guidelines:   \n793 \u2022 The answer NA means that the paper poses no such risks.   \n794 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n795 necessary safeguards to allow for controlled use of the model, for example by requiring   \n796 that users adhere to usage guidelines or restrictions to access the model or implementing   \n797 safety filters.   \n798 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n799 should describe how they avoided releasing unsafe images.   \n800 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n801 not require this, but we encourage authors to take this into account and make a best   \n802 faith effort.   \n803 12. Licenses for existing assets   \n804 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n805 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n806 properly respected?   \n807 Answer: [Yes] .   \n808 Justification: We cite (Ramos Zapatero et al., 2023) that produced the dataset used in the   \n809 study. The dataset is available under the license CC BY 4.0.   \n810 Guidelines:   \n811 \u2022 The answer NA means that the paper does not use existing assets.   \n812 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n813 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n814 URL.   \n815 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n816 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n817 service of that source should be provided.   \n818 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n819 package should be provided. For popular datasets, paperswithcode.com/datasets   \n820 has curated licenses for some datasets. Their licensing guide can help determine the   \n821 license of a dataset.   \n822 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n823 the derived asset (if it has changed) should be provided.   \n824 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n825 the asset\u2019s creators.   \n826 13. New Assets   \n827 Question: Are new assets introduced in the paper well documented and is the documentation   \n828 provided alongside the assets?   \n829 Answer: [NA] .   \n830 Justification: The paper does not release new assets.   \n831 Guidelines:   \n832 \u2022 The answer NA means that the paper does not release new assets.   \n833 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n834 submissions via structured templates. This includes details about training, license,   \n835 limitations, etc.   \n836 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n837 asset is used.   \n838 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n839 create an anonymized URL or include an anonymized zip file.   \n840 14. Crowdsourcing and Research with Human Subjects   \n841 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n842 include the full text of instructions given to participants and screenshots, if applicable, as   \n843 well as details about compensation (if any)?   \n844 Answer: [NA] .   \n845 Justification: The empirical study presented in the paper is conducted on the synthetic or   \n846 publicly available data.   \n847 Guidelines:   \n848 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n849 human subjects.   \n850 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n851 tion of the paper involves human subjects, then as much detail as possible should be   \n852 included in the main paper.   \n853 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n854 or other labor should be paid at least the minimum wage in the country of the data   \n855 collector.   \n856 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n857 Subjects   \n858 Question: Does the paper describe potential risks incurred by study participants, whether   \n859 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n860 approvals (or an equivalent approval/review based on the requirements of your country or   \n861 institution) were obtained?   \n862 Answer: [NA]   \n863 Justification: The empirical study presented in the paper is conducted on the synthetic or   \n864 publicly available data.   \n865 Guidelines:   \n866 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n867 human subjects. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]