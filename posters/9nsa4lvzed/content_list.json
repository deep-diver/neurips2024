[{"type": "text", "text": "Stability and Generalization of Adversarial Training for Shallow Neural Networks with Smooth Activation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kaibo Zhang Yunjuan Wang Raman Arora Johns Hopkins University Johns Hopkins University Johns Hopkins University Baltimore, MD 21218 Baltimore, MD 21218 Baltimore, MD 21218 kzhang90@jhu.edu ywang509@jhu.edu arora@cs.jhu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Adversarial training has emerged as a popular approach for training models that are robust to inference-time adversarial attacks. However, our theoretical understanding of why and when it works remains limited. Prior work has offered generalization analysis of adversarial training, but they are either restricted to the Neural Tangent Kernel (NTK) regime or they make restrictive assumptions about data such as (noisy) linear separability or robust realizability. In this work, we study the stability and generalization of adversarial training for two-layer networks without any data distribution assumptions and beyond the NTK regime. Our findings suggest that for networks with any given initialization and suffciently large width, the generalization bound can be effectively controlled via early stopping. We further improve the generalization bound by leveraging smoothing using Moreau's envelope. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Despite the remarkable performance of over-parameterized deep networks in real-world applications, recent studies have revealed that they are highly vulnerable to adversarial attacks. These attacks use maliciously crafted imperceptible perturbations designed to deceive trained neural networks during inference [Szegedy et al., 2013, Biggio et al., 2013]. The lack of adversarial robustness has raised significant concerns for deploying neural network-based models in safety-critical applications. Therefore, it is crucial to design algorithms to learn robust models that can make reliable predictions on test data even in the presence of adversarial perturbations. ", "page_idx": 0}, {"type": "text", "text": "One principal approach to robust learning, adversarial training [Madry et al., 2018] (along with its variants [Zhang et al., 2019, Wang et al., 2020]), has proven to be an effective empirical defense mechanism against adversarial attacks. Naturally, this puts an emphasis on also developing a theoretical understanding of robust learning. To study the generalization performance of robust learning, one traditional approach is via uniform convergence [Khim and Loh, 2018, Yin et al., 2019, Awasthi et al., 2020, Mustafa et al., 2022], which provides the worst-case type uniform bounds for a given hypothesis class and are algorithm independent. Another line of work focuses on analyzing the convergence and generalization guarantees of adversarial training, yet they either focus on linear classifiers [Charles et al., 2019, Li et al., 2020, Zou et al., 2021, Chen et al., 2023], or introduce restrictive distribution assumptions such as (noisy) linear separability [Wang et al., 2024b] or robust realizability [Mianjy and Arora, 2024]. Therefore, it remains unclear whether we can derive theoretical results for adversarial training that extend beyond these simplifying assumptions. ", "page_idx": 0}, {"type": "text", "text": "In this work, we leverage a different machinery by analyzing adversarial training algorithm through the lens of uniform stability. Stability is a classical tool in learning theory that has been extensively studied in the literature [Bousquet and Elisseeff, 2002, Hardt et al., 2016]. Uniform argument stability measures the difference in output parameters when an algorithm is run on two training sets that differ by only one sample. In the standard (non-robust) setting, Hardt et al. [2016] show a uniform stability bound of $\\textstyle{\\mathcal{O}}({\\frac{\\eta{\\bar{T}}}{n}})$ after $T$ iterations of gradient descent with step size $\\eta$ on convex and smooth losses using a training dataset of size $n$ . They further provide a uniform stability bound of $O(\\frac{T^{q}}{n})$ for smooth and non-convex losses with decaying step size $\\begin{array}{r}{\\eta=\\mathcal{O}(\\frac{1}{t})}\\end{array}$ ,where $q\\in(0,1)$ is a constant. The choice of decaying step size is common in the non-convex setting, as maintaining a constant step size leads to an exponentially increasing bound on uniform stability. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "When it comes to the robust setting, the primary challenge lies in the non-smoothness of the robust (adversarial) loss. The robust loss is generally non-smooth even if the standard counterpart is smooth [Xing et al., 2021a, Xia0 et al., 2022a]. Previous work by Xing et al. [2021a] studied the convex non-smooth adversarial losses and provide an additional term of $\\mathcal{O}(\\eta\\sqrt{T})$ compared to the convex and smooth losses. Later Xiao et al. [2022a] studied the general non-smooth adversarial losses by leveraging the approximate co-coercivity of the gradient and provide the bound with an additional term of $\\mathcal{O}(\\eta T\\alpha)$ that grows linearly in $T$ ,where $\\alpha$ is the size of adversarial perturbation in $\\ell_{p}$ threat models. These works, while partially addressing the issue, only focus on general convex / non-convex functions. However, neural networks, which are a specific instance of non-convex functions and are widely used in practice, require further investigation. ", "page_idx": 1}, {"type": "text", "text": "In this work, we study the stability and generalization guarantees of variants of adversarial training algorithms. We focus on solving the binary classification problem using two-layer over-parameterized neural networks with smooth activation functions and logistic loss. Our key contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We present a bound of $\\begin{array}{r}{\\mathcal{O}(\\sqrt{\\eta}T+\\frac{\\eta T}{n_{-}}+\\sqrt{\\beta\\eta T})}\\end{array}$ on the uniform argument stability of the gradient descent-based adversarial training of over-parameterized network after $T$ iterations with step size $\\eta$ ,where $\\beta$ represents the precision of generating adversarial examples at each iteration.   \n2. We provide robust generalization guarantees that depend on the Adversarial Regularized Empirical Risk Minimization (ARERM) Oracle. Our results hold for any given initialization and any data distribution. Specifically, if the learner is provided with a good initialization such that there exist robust networks around this initialization, then a small robust test loss is achieved via early stopping. Furthermore, our results can be extended to stochastic gradient descent-based adversarial training.   \n3. We leverage Moreau's envelope to construct a smooth loss that approximates robust empirical loss. We present bounds on the stability and generalization error of gradient descent with Moreau's smoothing, and demonstrate its superiority compared with gradient descent-based adversarial training algorithm. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Stability Analysis.  The notion of stability was initially introduced in Bousquet and Elisseeff [2002] to study the generalization of statistical learning problems. More recently, a fine-grained analysis has been presented by Feldman and Vondrak [2019] and Bousquet et al. [2020]. For smooth loss functions, Hardt et al. [2016] explored the stability of SGD in both convex and non-convex settings, which was later extended to convex non-smooth loss functions by Bassily et al. [2020] and the bound incorporated an additional term of $\\mathcal{O}(\\eta\\sqrt{T})$ due to non-smoothness. Lei and Ying [2020] tackled the non-smoothness differently by assuming the gradient of the loss to be Holder continuous. For non-convex and non-smooth loss, Lei [2023] introduced the stability of sub-gradient, as convergence to local minimizers is observed in this setting. ", "page_idx": 1}, {"type": "text", "text": "Robust Generalization Guarantee. The standard method of giving a generalization guarantee is through uniform convergence. These theories typically yield an upper bound of $\\textstyle{\\mathcal{O}}({\\frac{1}{\\sqrt{n}}})$ and requirea large number of training samples in order to get a small generalization gap. Techniques in this category include analyzing the Rademacher complexity [Yin et al., 2019, Khim and Loh, 2018, Awasthi et al., 2020], VC dimension [Cullina et al., 2018, Montasser et al., 2020], covering number [Balda et al., 2019, Mustafa et al., 2022, Li and Telgarsky, 2023], PAC Bayesian analysis [Farnia et al., 2018, Viallard et al., 2021, Xiao et al., 2023] and margin-based analysis [Farnia et al., 2018]. ", "page_idx": 1}, {"type": "text", "text": "Generalization Guarantee of Adversarial Training. Providing generalization guarantees for adversarial training of neural networks is challenging due to its non-convex nature. A series of works [Charles et al., 2019, Li et al., 2020, Zou et al., 2021, Chen et al., 2023] have focused on a simpler problem - adversarial training of linear models with a convex loss wherein generating adversarial examples admits a closed-form solution. Several works bypass this challenge by considering a lazy training regime [Gao et al., 2019, Zhang et al., 2020, Li and Telgarsky, 2023] in which the landscape of the neural network can be studied near certain random initialization, and the generalization guarantee is usually obtained via uniform convergence. Unfortunately, Wang et al. [2022] proved that adversarial robustness is at odds with lazy regime. Recently, Mianjy and Arora [2024], Wang et al. [2024b] provide convergence and generalization guarantees for adversarial training of neural networks, yet they make restrictive assumptions on the data distribution such as (noisy) linear separability and robust realizability. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Another line of research investigates the generalization of adversarial training through algorithmic stability analysis. Despite the smoothness of the standard loss, the adversarial loss remains nonsmooth [Liu et al., 2020, Xing et al., 2021a, Xia0 et al., 2022b]. To resolve this issue, Farnia and Ozdaglar [2021] make a strong assumption that the loss is concave in input x. Xing et al. [2021a] provide adversarial training of convex and non-smooth losses, yielding an additional term Oof $\\mathcal{O}(\\sqrt{\\eta^{2}T})$ compared to the standard non-robust counterpart. Xia0 et al. [2022a] and Wang et al. [2024a] leverage the idea of approximate smoothness and provide bounds that scale linearly with $\\eta T$ and the perturbation size. Cheng et al. [2024] consider generating adversarial examples via a single step of gradient descent and demonstrate that such variant of adversarial training algorithm achieves better stability. Farnia et al. [2018] also consider specific attack algorithms - these attacks while being more practical and designed with continuity and Lipschitzness property, may differ significantly from the worst-case attack, and do not yield a good bound on the robust generalization gap. ", "page_idx": 2}, {"type": "text", "text": "2  Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation. _ Throughout the paper, we denote scalars, vectors, and matrices with lowercase italics, lowercase bold, and uppercase bold Roman letters, respectively; e.g., $u$ , u, and U. We use $[m]$ to denote the set $\\{1,2,\\ldots,m\\}$ and use both $\\|\\cdot\\|$ and ${\\bar{\\|}}\\cdot{\\|}_{2}$ for $\\ell_{2}$ -norm. Given a matrix $\\mathrm{~U~}=$ $[\\mathbf{u}_{1},\\dots,\\mathbf{u}_{m}]\\in\\mathbb{R}^{d\\times m}$ , we use $\\|\\mathbf{U}\\|_{F}$ and $\\|\\mathrm{U}\\|_{2}$ to represent the Frobenius norm and spectral norm, respectively. We use the standard O-notation $\\bar{\\mathcal{O}},\\Theta$ and $\\Omega$ ", "page_idx": 2}, {"type": "text", "text": "We consider a binary classification problem with a bounded input space $\\mathcal{X}$ inside a Euclidean ball of radius $C_{x}$ , and label space $\\mathcal{V}=\\{\\pm1\\}$ . We assume that data are drawn according to an unknown probability distribution $\\mathcal{D}$ on $\\mathcal X\\times\\mathcal X$ . The learner has access to $n$ training data drawn i.i.d. from $\\mathcal{D}$ i.e., $S=\\{\\mathbf{z}_{i}=(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i=1}^{n}\\sim\\mathcal{D}^{n}$ . We do not make any restrictive distributional assumptions such as realizability [Mianjy and Arora, 2024] or (noisy) linearly separability [Wang et al., 2024b]. ", "page_idx": 2}, {"type": "text", "text": "We focus on learning two-layer neural networks, parameterized by a pair of weight matrices $(\\mathrm{a},\\mathrm{W})$ ", "page_idx": 2}, {"type": "text", "text": "Here, $m$ is a positive integer representing the number of hidden units, i.e., the width of the networks. $\\phi\\;:\\;\\mathbb{R}\\;\\rightarrow\\;\\mathbb{R}$ is a 1-Lipschitz, $H$ -smooth activation function. Formally, $\\forall z,z^{\\prime}\\;\\in\\;\\mathbb{R},|\\phi^{\\prime}(z)|\\;\\leq$ $1,|\\phi^{\\prime}(z)-\\phi^{\\prime}(z^{\\prime})|\\,\\leq\\,\\bar{H}\\,|z-z^{\\prime}|$ . The smoothness property of activation functions is commonly assumed in algorithmic stability literature and in theory of deep learning and covers a wide range of activation functions such as smoothed ReLU and smoothed leaky ReLU [Frei et al., 2022]. The weight matrices at the top and bottom layer are denoted as $\\mathbf{a}\\ =\\ [a_{1},\\dots,a_{m}]\\ \\in\\ \\mathbb{R}^{m}$ and $\\mathbf{W}_{\\_}}=\\left[\\mathbf{w}_{1},\\dots,\\mathbf{w}_{m}\\right]\\in\\mathbb{R}^{d\\times m}$ , respectively. The top layer weights are initialized such that $\\left|a_{i}\\right|\\,{=}$ $\\textstyle{\\frac{1}{\\sqrt{m}}},{\\dot{\\forall}}i\\in[m]$ , and are kept fixed throughout the training process. Prior works [Du et al., 2018, Arora et al., 2019, Ji and Telgarsky, 2019] often initialize $a_{i}$ to be uniformly sampled from $\\left\\{\\pm{\\frac{1}{\\sqrt{m}}}\\right\\}$ , which can be seen as a special instance of ours. We do not make any assumption on the initialization of the bottom layer matrix, i.e., $\\mathbf{W}_{0}$ can be either a standard Gaussian [Du et al., 2018, Ji and Telgarsky, 2019], or a vanishing initialization [Ba et al., 2019, Xing et al., 2021b], or a pre-trained model. ", "page_idx": 2}, {"type": "text", "text": "Adversarial Attacks. \u03b2 We consider a general threat model where the adversary's perturbation set is defined as $B:\\mathcal{X}\\rightarrow2^{\\mathcal{X}}$ . Given an input x, $\\boldsymbol{{B}}(\\mathbf{x})$ represents the set of all possible perturbations of $\\mathbf{X}$ that an adversary can choose from. This broader definition of attack includes both the standard $\\ell_{p}$ threat models with perturbation size of $\\alpha$ ,i.e., $\\mathcal{B}(\\mathbf{x})=\\left\\{\\tilde{\\mathbf{x}}:\\|\\tilde{\\mathbf{x}}-\\mathbf{x}\\|_{p}\\leq\\alpha\\right\\}$ , as well as a discrete set of large-norm transformations. Unlike prior works [Mianjy and Arora, 2024, Wang et al., 2024b], we do not make any assumptions on the perturbation size. ", "page_idx": 2}, {"type": "text", "text": "In this work, we focus on logistic loss, $\\ell(z)=\\ln(1+e^{-z})$ , which serves as a smooth and convex surrogate loss for the 0-1 loss. With a slight abuse of notation, for a fixed sample $\\mathbf{z}=(\\mathbf{x},y)$ ,wedefine $\\ell({\\sf z},\\bar{\\bf W}):=\\ell(y f({\\bf x};{\\bf W}))$ . The population and empirical loss w.r.t. $\\ell(\\cdot)$ are denoted, respectively, as ", "page_idx": 3}, {"type": "equation", "text": "$$\nL(\\mathbf{W}):=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim\\mathcal{D}}\\ell(y f(\\mathbf{x};\\mathbf{W})),\\quad\\widehat{L}(\\mathbf{W};S):=\\frac{1}{n}\\sum_{i=1}^{n}\\ell(y_{i}f(\\mathbf{x}_{i};\\mathbf{W})).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Given $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , for a fixed sample $\\mathbf{z}=(\\mathbf{x},y)$ , we define the robust loss as $\\ell_{r o b}(\\mathbf{z},\\mathbf{W}):=\\operatorname*{max}_{\\tilde{\\mathbf{x}}\\in\\mathcal{B}(\\mathbf{x})}\\ell(y f(\\tilde{\\mathbf{x}};\\mathbf{W}))$ The robust population and empirical loss w.r.t. $\\ell(\\cdot)$ are defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{r o b}(\\mathbf{W}):=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim\\mathcal{D}}\\operatorname*{max}_{\\tilde{\\mathbf{x}}\\in B(\\mathbf{x})}\\,\\ell(y f(\\tilde{\\mathbf{x}};\\mathbf{W}))\\ \\ \\widehat{L}_{r o b}(\\mathbf{W};S):=\\frac{1}{n}\\sum_{i=1}^{n}\\operatorname*{max}_{\\tilde{\\mathbf{x}}_{i}\\in B(\\mathbf{x}_{i})}\\ell(y_{i}f(\\tilde{\\mathbf{x}}_{i};\\mathbf{W})).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Adversarial Training.  During training, the network bottom layer weight $\\mathrm{W}$ are updated using gradient descent-based adversarial training (or its stochastic version). We denote the weight matrix at the $t$ -th iterate of adversarial training as $\\mathbf{W}_{t}$ . For each training example $\\left({{\\bf{x}}_{i}},{y_{i}}\\right)$ , at iteration $t$ we generate a $\\beta_{1}$ -optimaladversarialexample $(\\tilde{{\\bf x}}_{i}({\\bf W}_{t}),y_{i})$ , which satisfies the following condition: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\ell(y_{i}f(\\widetilde\\mathbf{x}_{i}(\\mathbf{W}_{t});\\mathbf{W}_{t}))\\ge\\operatorname*{max}_{\\widetilde\\mathbf{x}\\in B(\\mathbf{x}_{i})}\\ell(y_{i}f(\\widetilde\\mathbf{x};\\mathbf{W}_{t}))-\\beta_{1}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Setting $\\beta_{1}\\,=\\,0$ recovers the scenario where we have access to the worst-case adversarial attack. As this may not be feasible in practice due to computational reason, the parameter $\\beta_{1}$ allowsus to capture the precision of the attack algorithm, which includes common attacks such as projected gradient descent (PGD) [Madry et al., 2018]. We should regard $\\beta_{1}$ as a parameter we can choose. Our results in Section 3 suggest that we can achieve better generalization by adding more computation and making $\\beta_{1}$ smaller. ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 Variants of Adversarial Training Algorithms ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Input: Step size $\\eta$ . Number of iterations $T$ Initial weight $\\mathbf{W}_{0}.\\ \\beta\\geq0.\\ \\mu>0.$ for $t=0,\\dots,T-1$ do GD: $\\forall i\\in[n]$ , compute a $\\beta_{1}$ -optimal adversarial example $\\tilde{\\mathbf{X}}_{i}(\\mathbf{W}_{t})$ that satisfies Equation (1). Update $\\mathbf{W}_{t+1}=\\mathbf{W}_{t}-\\frac{\\eta}{n}\\sum_{i=1}^{n}\\nabla_{\\mathbf{W}}\\ell\\big(y_{i}f\\big(\\widetilde{\\mathbf{x}}_{i}(\\mathbf{W}_{t});\\mathbf{W}_{t}\\big)\\big).$ SGD: Compute a $\\beta_{1}$ -optimal adversarial example $\\widetilde{\\mathbf{x}}_{t+1}(\\mathbf{W}_{t})$ that satisfies Equation (1). Update $\\mathbf{W}_{t+1}=\\mathbf{W}_{t}-\\eta\\nabla_{\\mathbf{W}}\\ell(y_{t+1}f(\\widetilde{\\mathbf{x}}_{t+1}(\\mathbf{W}_{t});\\mathbf{W}_{t}))$ Moreau Envelope: Compute a $\\beta_{2}$ -optimal minimizer $\\widetilde{\\mathbf{U}}^{\\mu}(\\mathbf{W}_{t};S)$ that satisfies Equation (2). Update $\\begin{array}{r}{\\mathbf{W}_{t+1}=\\mathbf{W}_{t}-\\frac{\\eta}{\\mu}\\big(\\mathbf{W}_{t}-\\widetilde{\\mathbf{U}}^{\\mu}(\\mathbf{W}_{t};S)\\big)}\\end{array}$   \nreturm: $\\left\\{\\mathbf{W}_{t}\\right\\}_{t=0}^{T}$ ", "page_idx": 3}, {"type": "text", "text": "Optimizing the Moreau Envelope. Since the robust loss is non-smooth [Xiao et al., 2022a], we utilize Moreau's envelope to construct a smooth function that approximates the empirical robust loss. Such an idea has previously been explored in Xiao et al. [2024]. Given training data $S$ and $\\mu>0$ ,we redefine the robust surrogate loss as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nM^{\\mu}(\\mathbf{W};S)=\\operatorname*{min}_{\\mathbf{U}}\\left(\\widehat{L}_{r o b}(\\mathbf{U};S)+\\frac{1}{2\\mu}\\|\\mathbf{U}-\\mathbf{W}\\|_{F}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Selecting $\\mu$ appropriately ensures that $\\begin{array}{r}{\\widehat{L}_{r o b}(\\mathbf{U};S)+\\frac{1}{2\\mu}\\|\\mathbf{U}-\\mathbf{W}\\|_{F}^{2}}\\end{array}$ is a strongly convex function W.r.t. U. Given $\\mathrm{W}$ and $S$ , we define $\\begin{array}{r}{\\mathrm{U}^{\\mu}(\\mathbf{W};S)=\\operatorname*{argmin}_{\\mathrm{U}\\in\\mathbb{R}^{d\\times m}}\\widehat{L}_{r o b}(\\mathrm{U};S)+\\frac{1}{2\\mu}\\|\\mathrm{U}-\\mathbf{W}\\|_{F}^{2}}\\end{array}$ which can be obtained via subgradient-based method (solve a min-max optimization). The gradient of the Moreau envelope can be simply calculated as $\\begin{array}{r}{\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W};S)=\\frac{1}{\\mu}(\\mathbf{W}-\\mathbf{U}^{\\mu}(\\mathbf{W};S))}\\end{array}$ . Given training data $S$ , at each iteration $t$ , we generate a $\\beta_{2}$ -optimal minimizer $\\widetilde{\\mathbf{U}}^{\\mu}(\\mathbf{W}_{t};S)$ that satisfies ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widehat{L}_{r o b}(\\widetilde{\\mathbf{U}}^{\\mu}(\\mathbf{W}_{t};S);S)+\\frac{1}{2\\mu}\\|\\widetilde{\\mathbf{U}}^{\\mu}(\\mathbf{W}_{t};S)-\\mathbf{W}_{t}\\|_{F}^{2}\\leq\\beta_{2}+M^{\\mu}(\\mathbf{W}_{t};S).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Weremark that $\\beta_{2}$ -optimal minimizer defined in Equation (2) and $\\beta_{1}$ -optimal adversarial example defined in Equation (1) are approximating different quantities, which are not comparable. All the algorithms described above are summarized in Algorithm 1. ", "page_idx": 3}, {"type": "text", "text": "Uniform Argument Stability. Given a training set $\\boldsymbol{S}=\\left\\{\\mathbf{z}_{i}\\right\\}_{i=1}^{n}$ drawn i.i.d. from $\\mathcal{D}$ let $S^{\\prime}$ denote the training set obtained by replacing one example in $S$ with an independently drawn example $z^{\\prime}\\sim\\mathcal{D}$ We refer to $S,S^{\\prime}$ as neighboring samples and write $S\\simeq S^{\\prime}$ . Given an algorithm $A:(\\mathcal{X}\\times\\bar{\\mathcal{Y}})^{n}\\to\\mathcal{H}$ where the hypothesis class $\\mathcal{H}$ is parameterized using a parameter matrix $\\mathbf{W}\\in\\mathbb{R}^{d\\times m}$ , we define the uniform argument stability as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\delta_{A}(S,S^{\\prime}):=\\lVert A(S)-A(S^{\\prime})\\rVert_{F}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For any $L$ -Lipschitzlossfunction $g$ $|g(\\mathcal{A}(S),\\boldsymbol{\\mathsf{z}})-g(\\mathcal{A}(S^{\\prime}),\\boldsymbol{\\mathsf{z}})|\\leq L\\delta_{\\mathcal{A}}(S,S^{\\prime})$ .The standard stability argument [Mohri et al., 2018] relates the expected generalization gap to the uniform argument stability. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\varepsilon_{g e n}(A(S)):=\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\Big(\\mathbb{E}_{z\\sim\\mathcal{D}}g(A(S),\\mathbf{z})-\\frac{1}{n}\\sum_{i=1}^{n}g(A(S),\\mathbf{z}_{i})\\Big)\\leq L\\operatorname*{sup}_{S\\sim S^{\\prime}}\\delta_{A}(S,S^{\\prime}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In this paper, we consider robust generalization using logistic loss, so function $g(\\mathbf{W},\\mathbf{z})=\\ell_{r o b}(\\mathbf{z},\\mathbf{W})$ and $\\begin{array}{r}{\\bar{\\varepsilon_{g e n}(\\mathbf{W})}=\\mathbb{E_{z\\sim\\mathcal{D}}}[\\ell_{r o b}(\\mathbf{z},\\mathbf{\\bar{W}})]-\\frac{1}{n}\\sum_{i=1}^{n}\\ell_{r o b}(\\mathbf{\\bar{z}}_{i},\\mathbf{\\bar{W}})}\\end{array}$ We alsoremark that a high probability bound for stabile algorithms can be given based on Feldman and Vondrak [2019]. For simplicity, our generalization bounds in this paper are only in expectation. ", "page_idx": 4}, {"type": "text", "text": "3 Main Result ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present our main results, providing theoretical guarantees for adversarial training of two-layer neural networks with smooth activation functions. We discuss  (stochastic) adversarial training in Section 3.1 and gradient descent-based Moreau's smoothing in Section 3.2. . Our generalization bounds rely on a key quantity, the Adversarial Regularized Empirical Risk Minimization (ARERM) Oracle defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta_{S}^{\\mathrm{oracle}}:=\\operatorname*{min}_{\\mathbb{W}\\in\\mathbb{R}^{d\\times m}}\\left(\\widehat{L}_{r o b}(\\mathbf{W};S)+\\frac{2\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}}{\\eta T}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Given a sample, $\\Delta_{S}^{\\mathrm{oracle}}$ returns themnimalpirical riskin te vicnity faninitializai $\\mathrm{W_{0}}$ ", "page_idx": 4}, {"type": "text", "text": "3.1  Generalization Guarantees for Adversarial Training ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We begin by presenting a bound on the uniform argument stability (UAS) of Algorithm 1 with GD. Theorem 3.1. Assume that the network width satisfies $m\\,\\geq\\,H^{2}C_{x}^{4}\\eta^{2}(T+1)^{2}$ . Then, after $T$ iterations of Algorithm 1 with GD, for any neighboring datasets $S,S^{\\prime}$ ,wehave ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{S\\geq S^{\\prime}}\\delta_{\\mathcal{A}}(S,S^{\\prime})\\leq\\mathcal{O}(C_{x}\\eta\\sqrt{T}+C_{x}\\frac{\\eta T}{n}+\\sqrt{\\beta_{1}\\eta T}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Remarkably, setting $\\beta_{1}=0$ yields a bound of $\\scriptstyle{\\mathcal{O}}(\\eta{\\sqrt{T}}+{\\frac{\\eta T}{n}})$ on the UAS of Algorithm 1, thereby recovering the result in prior work of Xing et al. [2021a]. However, note that Xing et al. [2021a] show the result only for convex learning problems, whereas we consider training two-layer neural networks using logistic loss, which is non-convex and non-smooth. Further note that we assume that the networks are sufficiently over-parameterized, i.e., $m\\geq\\Omega(\\eta^{2}T^{2})$ , a condition that is commonly assumed in deep learning theory. We can also regard this condition as early stopping, wherein $\\begin{array}{r}{T\\leq\\mathcal{O}\\left(\\frac{\\sqrt{m}}{H C_{x}^{2}\\eta}\\right)}\\end{array}$ . This view is also consistent with several empirical studies [Caruana et al., 2000, Rice et al., 2020, Pang et al., 2021]. ", "page_idx": 4}, {"type": "text", "text": "Next, we show that stable robust learning rules do not overfit. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2. Define $\\begin{array}{r}{\\alpha_{1}(\\eta,T):=\\mathcal{O}(C_{x}^{2}\\eta\\sqrt{T}+C_{x}^{2}\\frac{\\eta T}{n}+C_{x}\\sqrt{\\beta_{1}\\eta T})}\\end{array}$ .Assume that the width of the networks satisfies $m\\geq H^{2}C_{x}^{4}\\eta^{2}(T+1)^{2}$ , and $\\alpha_{1}(\\eta,T)<1$ . Then, after $T$ iterations of Algorithm 1 with GD, we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{[\\frac{9T}{10}]\\leq t\\leq T}{\\operatorname*{min}}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\varepsilon_{g e n}(\\mathbf{W}_{t})\\leq\\frac{17\\alpha_{1}(\\eta,T)}{1-\\alpha_{1}(\\eta,T)}\\left[\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+\\frac{C_{x}^{2}\\eta}{2}+\\beta_{1}\\right],}\\\\ &{\\underset{0\\leq t\\leq T}{\\operatorname*{min}}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}L_{r o b}(\\mathbf{W}_{t})\\leq\\frac{1}{1-\\alpha_{1}(\\eta,T)}\\left[\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+\\frac{C_{x}^{2}\\eta}{2}+\\beta_{1}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and ", "page_idx": 4}, {"type": "text", "text": "The result above bounds the robust generalization gap and the robust loss in terms of the ARERM oracle, a step size-dependent term $\\bar{\\mathcal{O}}(\\eta)$ , and the precision of the adversarial examples $\\beta_{1}$ .Note though that the bound holds for the minimum over the last few iterates (past iterates), rather than for the last iteration. This distinction arises because, unlike standard gradient descent for neural networks, we cannot guarantee a decreasing robust training loss without additional assumptions on the data distributions owing to the non-smooth nature of the robust loss. The step size-dependent term arises for the same reason. A direct corollary gives us a bound on the expected robust loss. ", "page_idx": 5}, {"type": "text", "text": "Corollary 3.3. After $\\begin{array}{r}{T\\le\\mathcal{O}(\\operatorname*{min}\\{n^{2},\\frac{1}{\\beta_{1}^{2}}\\})}\\end{array}$ iterations of Algorithm 1 with GD using astep sizeof $\\begin{array}{r}{\\eta=\\Theta(\\frac{1}{C_{x}^{2}\\sqrt{T}})}\\end{array}$ on a network with width $m\\geq\\Omega(T)$ for any weight matrix W ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq t\\leq T}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}L_{r o b}(\\mathbf{W}_{t})\\leq1.1L_{r o b}(\\mathbf{W})+\\mathcal{O}\\left(\\frac{C_{x}^{2}\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}}{\\sqrt{T}}\\right)+\\mathcal{O}\\left(\\frac{1}{\\sqrt{T}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Since corollary 3.3 holds for any $\\mathbf{W}_{0}$ , it underscores the importance of initialization for robust learning. Given a good initialization, such as a pre-trained model, and assuming that there exists a robust network $\\mathrm{W}_{*}$ in the vicinity of the initialization (i.e., $\\|\\mathbf{W}_{*}-\\mathbf{W}_{0}\\|_{F}=\\mathcal{O}(1))$ that achieves a small robust loss $L_{\\mathrm{rob}}(\\mathbf{W}_{*})\\approx0$ , we have that the minimum expected robust loss over all iterates approaches $\\begin{array}{r}{{\\mathcal{O}}\\left({\\frac{1}{\\sqrt{T}}}\\right)}\\end{array}$ Further, if $\\beta_{1}$ $m\\gtrsim n^{2}$ then $T$ can b of the order $\\Theta(n^{2})$ leading to a ${\\mathcal{O}}(1/n)$ upper bound on the robust test loss. ", "page_idx": 5}, {"type": "text", "text": "We remark that by a similar analysis, our result can be reduced to the standard (non-robust) setting for gradient descent training of two-layer networks by setting the perturbation set $B({\\mathrm{x}})=\\{{\\mathrm{x}}\\},\\forall{\\mathrm{x}}\\in{\\mathcal{X}},$ $\\beta_{1}=0$ , and redefining $\\begin{array}{r}{\\alpha_{1}(\\eta,T)=\\mathcal{O}(C_{x}^{2}\\frac{\\eta T}{n})}\\end{array}$ . In this context, we can show that gradient descent for the binary classification problem can achieve excess risk bound of $\\mathcal{O}(1/\\sqrt{n})$ by taking $\\eta T=\\Theta({\\sqrt{n}})$ if $m\\gtrsim n$ and assuming $\\|\\mathbf{W}_{*}-\\mathbf{W}_{0}\\|_{F}=\\mathcal{O}(1)$ , where $\\mathbf{W}_{*}\\in\\mathop{\\mathrm{argmin}}_{\\mathbf{W}}L_{r o b}(\\mathbf{W})$ ", "page_idx": 5}, {"type": "text", "text": "Next, we extend our result to the stochastic adversarial training. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.4. After $T$ iterations of Algorithm 1 with SGD on a network of width $m\\geq H^{2}C_{x}^{4}\\eta^{2}(T+$ $1)^{2}$ we have that for any weight matrix $\\mathrm{W}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq t\\leq T}\\mathbb{E}_{\\{\\tau_{1},\\ldots,z_{t}\\}\\sim\\mathcal{D}^{t}}L_{r o b}(\\mathbf{W}_{t})\\leq L_{r o b}(\\mathbf{W})+\\frac{\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}}{\\eta(T+1)}+\\frac{C_{x}^{2}\\eta}{2}+\\beta_{1}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Similar to the discussion following Corollary 3.3, we assert that if we assume that there exists an over-parameterized robust network with small robust loss, then using a step size of $\\eta\\,=\\,1/\\sqrt{T}$ stochastic adversarial training yields an excess risk bound of $\\mathcal{O}(1/\\sqrt{T})$ ", "page_idx": 5}, {"type": "text", "text": "3.2  Generalization Guarantees for Gradient Descent on Moreau's Envelope ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now present a bound on the uniform argument stability of gradient descent with smoothing based on Moreau's envelope. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.5. After $T$ iterations of Algorithm 1 with Moreau Envelope with step-size $\\eta\\ \\leq$ $\\begin{array}{r}{\\operatorname*{min}\\{\\mu,\\frac{\\sqrt{m}}{8H C_{x}^{2}}\\}\\,\\leq\\,\\frac{\\sqrt{m}}{2H C_{x}^{2}}}\\end{array}$ , on a network of width $m\\,\\geq\\,H^{2}C_{x}^{4}\\eta^{2}T^{2}$ for any neighboring datasets $S,S^{\\prime}$ wehave ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{S\\geq S^{\\prime}}\\delta_{A}(S,S^{\\prime})\\leq\\mathcal{O}\\left(C_{x}\\frac{\\eta T}{n}+\\eta T\\sqrt{\\frac{\\beta_{2}}{\\mu}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Setting $\\beta_{2}=0$ yields abound of $O(\\frac{\\eta T}{n})$ on the UAS of Algorithm 1, thereby recovering the result in prior work of [Hardt et al., 2016, Xiao et al., 2024] for convex and smooth functions. Note that by using Moreau's envolope, we are able to shave off the $\\mathcal{O}(\\eta\\sqrt{T})$ term that appears in Theorem 3.1. ", "page_idx": 5}, {"type": "text", "text": "Although inspired by Xiao et al. [2024], Theorem 3.5 differs from the non-convex setting of Xiao et al. [2024]. Our result utilizes the specific structure of over-parameterized neural networks that exhibit weakly convex properties, a special instance of non-convex functions, and allows for a constant step size. In contrast, [Xiao et al., 2024, Theorem 4.7] follows the traditional stability argument for non-convex and smooth functions in Hardt et al. [2016], considering a decaying step size $\\begin{array}{r}{\\eta_{t}\\leq\\frac{\\mu}{t}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Such a condition might be impractical if $\\mu$ is chosen to be sufficiently small. In fact, our results indicate that it is necessary to select a sufficiently small $\\mu$ so that the robust training loss is well approximated by the Moreau envelope (see Lemma C.1 in the Appendix). ", "page_idx": 6}, {"type": "text", "text": "Even though the gradient descent-based algorithm with Moreau's smoothing achieves better stability guarantees compared to gradient descent-based adversarial training when $\\beta_{1}=\\beta_{2}=0$ ,it requires more computational resources. Specifically, for the calculation of the gradient at each step, we need to solve a min-max optimization problem with a strongly convex and non-smooth objective to obtain a $\\beta$ -optimal minimizer. Additionally, for every step of this min-max optimization, we need to generate adversarial examples and apply sub-gradient descent. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.6. Define $\\begin{array}{r}{\\alpha_{2}(\\eta,T):=\\mathcal{O}(C_{x}^{2}\\frac{\\eta T}{n}+C_{x}\\eta T\\sqrt{\\frac{\\beta_{2}}{\\mu}})}\\end{array}$ .Assume $\\alpha_{2}(\\eta,T)\\,<\\,1$ Then, after $T\\ \\geq\\ 8$ iterations of Algorithm Moreau Envelope with step-size $\\eta\\,\\leq\\,\\mu$ on a network of width $m\\geq H^{2}C_{x}^{4}\\eta^{2}T^{2}$ ,wehave ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{[\\frac{9T}{10}]\\leq t\\leq T}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\varepsilon_{g e n}(\\mathbf{W}_{t})\\leq\\frac{55\\alpha_{2}(\\eta,T)}{1-\\alpha_{2}(\\eta,T)}\\left[\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+C_{x}^{2}\\mu+2\\eta(T+1)\\frac{\\beta_{2}}{\\mu}\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{1\\leq t\\leq T}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}L_{r o b}(\\mathbb{W}_{t})\\leq\\frac{1}{1-\\alpha_{2}(\\eta,T)}\\left[\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+C_{x}^{2}\\mu+2\\eta(T+1)\\frac{\\beta_{2}}{\\mu}\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Similar to Theorem 3.2, the result above shows that both the robust generalization gap as well as the robust loss can be bounded in terms of the ARERM oracle, parameter $\\mu$ in Moreau's envelope, and a term of $\\mathcal{O}(\\eta T\\beta_{2}/\\mu)$ dependent on the precision of generating the minimizer of Moreau envelope. While the bound above is on the minimum expected generalization gap (and expected robust test loss) over the last few iterates (past iterates), we can give a bound for the the last iterate for the case when $\\beta_{2}=0$ . We conclude the section by presenting the following direct corollary. ", "page_idx": 6}, {"type": "text", "text": "Corolary37. Afer $\\begin{array}{r}{T\\leq\\mathcal{O}(\\operatorname*{min}\\lbrace n^{2},\\frac{1}{\\beta_{2}^{2/3}}\\rbrace)}\\end{array}$ step-size $\\begin{array}{r}{\\eta=\\mu=\\Theta(\\frac{1}{C_{x}^{2}\\sqrt{T}})}\\end{array}$ on a network of width $m\\geq\\Omega(T)$ we have for any weight matrx W, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{1\\leq t\\leq T}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}L_{r o b}(\\mathbf{W}_{t})\\leq1.1L_{r o b}(\\mathbf{W})+\\mathcal{O}\\left(\\frac{C_{x}^{2}\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}}{\\sqrt{T}}\\right)+\\mathcal{O}\\left(\\frac{1}{\\sqrt{T}}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4 Proof Sketch ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We begin by providing a high level intuition behind our analysis technique, and then we highlight the key ideas in the proofs of the main theorems. For simplicity, we assume that the learner can generate optimal attacks during adversarial training, i.e., we consider $\\beta_{1}=0$ $\\beta_{2}=0$ in this section. We refer the reader to the Appendix for proofs of the more general case. ", "page_idx": 6}, {"type": "text", "text": "Our analysis relies on a key lemma demonstrating that the objective function (i.e., the robust empirical risk) being minimized in adversarial training of two-layer neural networks with smooth activation functions using the logistic loss function is \u201calmost\" convex. ", "page_idx": 6}, {"type": "text", "text": "Definition 4.1. Let $l>0$ . A function $f(x)$ is said to be $-l$ -weakly convex if $f(x)+{\\frac{l}{2}}\\|x\\|_{2}^{2}$ is convex in $x$ ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.2. (Restatement of Lemma A.4) For any weight matrices $\\mathbf{W}^{1}$ and $\\mathrm{W^{2}}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widehat{L}_{r o b}({\\mathbf{W}^{2}};S)\\geq\\widehat{L}_{r o b}({\\mathbf{W}^{1}};S)+\\left\\langle\\nabla_{\\mathbf{W}}\\widehat{L}_{r o b}({\\mathbf{W}^{1}};S),{\\mathbf{W}^{2}}-\\mathbf{W}^{1}\\right\\rangle-\\frac{H C_{x}^{2}}{2\\sqrt{m}}\\|\\mathbf{W}^{2}-\\mathbf{W}^{1}\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Equivalently, $\\widehat{L}_{r o b}(\\mathbf{W};S)$ is $-\\frac{H C_{x}^{2}}{\\sqrt{m}}$ -weakly convex. ", "page_idx": 6}, {"type": "text", "text": "We borrow many ideas from Xiao et al. [2024] and Xing et al. [2021a] in our proofs. These papers primarily focus on the convex seting, while only giving a general result for non-convex functions. We extend their results to a special case of learning neural networks. We argue that by specializing our analysis to neural networks would lead to sharper results than a general non-convex function class, as we will be able to leverage the \u201calmost\" convexity of neural network training [Richards and Rabbat, 2021, Richards and Kuzborskij, 2021]. This allows us to get stability and optimization guarantees that are similar to the convex setting when we consider an over-parameterized network $\\bar{m}\\geq\\mathrm{poly}(\\eta T)$ . An additional challenge we face is that the robust loss is non-smooth even if its standard counterpart (logistic loss) is smooth, making the analysis more complicated than the standard (non-robust) scenario. Nevertheless, we can still leverage the \u201calmost\u2019 convex nature of the loss to establish the stability of adversarial training. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "The following lemma gives a relationship between stability and generalization which is useful in both standard adversarial training as well as gradient descent with Moreau's envelope. When the robust trainingloss $\\widehat{L}_{r o b}(\\mathbf{W}_{T};S)$ is small, Lemma 4.3 provides a tighter bound than directly applying Equation (3). See Proposition A.3 for both results. ", "page_idx": 7}, {"type": "text", "text": "Lemma 4.3. (Restatement of Proposition A.3) The robust test loss satisfies the following: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}L_{r o b}(\\mathbf{W}_{T})\\leq\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\frac{1}{1-C_{x}\\cdot\\operatorname*{sup}_{S\\subseteq S^{\\prime}}\\delta_{A}(S,S^{\\prime})}\\widehat{L}_{r o b}(\\mathbf{W}_{T};S).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This result gives a way to bound the expected robust loss. Say you want to bound the expected robust test loss by $(1+\\epsilon)$ times thexpeted trainingloss Then,to ensure $\\begin{array}{r}{\\frac{1}{1-\\alpha_{1}(\\eta,T)}\\leq1\\,\\dot{+}\\;\\epsilon}\\end{array}$ we need $\\begin{array}{r}{\\alpha_{1}(\\eta,T)\\le\\frac{\\epsilon}{1+\\epsilon}=O(\\epsilon)}\\end{array}$ Since $\\begin{array}{r}{\\alpha_{1}(\\eta,T)=O(\\eta\\sqrt{T}\\!+\\!\\frac{\\eta T}{n}\\!+\\!\\sqrt{\\beta_{1}\\eta T})}\\end{array}$ , we can set different parameters in more than one way to ensure that $\\alpha_{1}(\\eta,T)\\,=\\,O(\\epsilon)$ .We can set $\\beta_{1}\\,=\\,O(\\epsilon^{2})$ \uff0c $n\\,=\\,\\Theta(1/\\epsilon)$ $T=\\Theta(1/\\epsilon^{2})$ \uff0c $\\begin{array}{r}{\\eta=O\\!\\left(\\frac{1}{T}\\right)}\\end{array}$ ; or set $\\beta_{1}=O(\\epsilon^{3})$ \uff0c $n=\\Theta(1/\\epsilon^{2})$ \uff0c $T=\\Theta(1/\\epsilon^{4})$ \uff0c $\\begin{array}{r}{\\eta=O(\\frac{\\epsilon}{\\sqrt{T}})}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "4.1  Generalization Guarantees for Gradient-Based Adversarial Training ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The stability guarantee we give in the following Theorem 4.4 is similar to the result in the convex case [Xing et al., 2021a]. While [Xing et al., 2021a] use the monotone subgradient condition of the convex functions, we show that the subgradients of an \u201calmost' convex loss function are \"almost' monotone. We do incur an additional term of $\\exp\\left(2H C_{x}^{2}\\eta T/\\sqrt{m}\\right)$ , which is small for over-parameterized neural networks $(m\\geq\\mathrm{ploy}(\\eta T))$ ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.4. (Restatement of Theorem 3.1) Let $S$ and $S^{\\prime}$ be any two neighboring data sets, i.e., they differ only in one example. Let $\\mathrm{W}_{T}$ and $\\mathbf{W}_{T}^{\\prime}$ denote the weight matrices returned after $T$ iterations of Algorithm 1 with GD on $S$ and $S^{\\prime}$ , respectively. Then, we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|\\boldsymbol{\\mathsf{W}}_{T}-\\boldsymbol{\\mathsf{W}}_{T}^{\\prime}\\|_{F}^{2}\\leq\\exp\\left(1+\\frac{2H C_{x}^{2}\\eta T}{\\sqrt{m}}\\right)\\cdot\\left(4C_{x}^{2}\\eta^{2}(T+1)+\\frac{4C_{x}^{2}\\eta^{2}(T+1)^{2}}{n^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We next provide an intermediate lemma that lead us to Theorem 3.2. ", "page_idx": 7}, {"type": "text", "text": "Lemma 45. Restaement f Thorem B.2) Set $\\begin{array}{r}{k=\\left(1+\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}}\\right)^{-1}}\\end{array}$ . Then after $\\begin{array}{r}{T\\leq\\frac{\\sqrt{m}}{H C_{x}^{2}\\eta}-1}\\end{array}$ iterations of Algorithm 1 with GD, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{1}{\\sum_{t=0}^{T}k^{t}}\\sum_{t=0}^{T}k^{t}\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)\\leq\\Delta_{S}^{\\mathrm{oracle}}+\\frac{C_{x}^{2}\\eta}{2}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Richards and Kuzborskij [2021] (see Lemma 2 in their paper) give an optimization guarantee $\\begin{array}{r}{\\frac{1}{T}\\sum_{t=1}^{\\bar{T}}\\widehat{L}(\\mathbf{W}_{t};S)}\\end{array}$ Specifically, for any weight matrix $\\mathrm{W}$ , we follow the standard technique in the convex case and upper boundthefollowing: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|\\mathbf{W}-\\mathbf{W}_{t+1}\\|_{F}^{2}=\\|\\mathbf{W}-\\mathbf{W}_{t}\\|_{F}^{2}+\\eta^{2}\\|\\nabla_{\\mathbf{W}}\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)\\|_{F}^{2}+2\\eta\\left<\\nabla_{\\mathbf{W}}\\widehat{L}_{r o b}(\\mathbf{W}_{t};S),\\mathbf{W}-\\mathbf{W}_{t}\\right>.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The second term on the right hand side is bounded by the Lipschitzness of the logistic loss. The iner rodueti the tird temis ounded by $\\begin{array}{r}{\\widehat{L}_{r o b}(\\mathbf{W};S)-\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)+\\frac{H C_{x}^{2}}{2\\sqrt{m}}\\|\\mathbf{W}-\\mathbf{W}_{t}\\|_{F}^{2}}\\end{array}$ using Lemma A.4. We finish the proof by telescoping. The weighted telescoping technique removes all of the $\\|\\mathbf{W}-\\mathbf{W}_{t}\\|_{F}^{2}$ terms $(t>0)$ in the upper bound, thereby giving a simpler result. The term $C_{x}^{2}\\eta/2$ in the upper bound stems from the non-smoothness of the robust loss, and is unavoidable even if the robust loss is convex. Finally, Theorem 3.2 follows from Theorem 4.4 and Lemmas 4.3 and 4.5. ", "page_idx": 7}, {"type": "text", "text": "4.2Generalization Guarantees for Gradient-Descent on Moreau's Envelope ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Below we give the key lemmas for bounding the generalization error of GD with Moreau's envelope. The proof technique here is similar to that for standard adversarial training (in the previous section), except that we get to utilize the smoothness of Moreau's envelope. Specifically, Lemma 4.6 leverages the fact that the gradient is \u201calmost'\u2019 co-coercive to control the uniform argument stability. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.6. (Restatement of Theorem C.4) Let $S\\simeq S^{\\prime}$ be any two neighboring data sets, i.e., $S$ and S'/ differ only in one example. For any n \u2264 min{\u03bc,gHg} , let $\\mathrm{W}_{T}$ and $\\mathbf{W}_{T}^{\\prime}$ be the weight matrices obtained by $T$ iterations of gradient descent with Moreau's envelopes on datasets $S$ and $S^{\\prime}$ , respectively. Then, we have that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\|\\mathbf{W}_{T}-\\mathbf{W}_{T}^{\\prime}\\|_{F}^{2}\\leq\\exp\\left(1+\\frac{8H C_{x}^{2}\\eta T}{\\sqrt{m}}\\right)\\cdot\\frac{16C_{x}^{2}\\eta^{2}(T+1)^{2}}{n^{2}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Lemma 4.7 also leverages smoothness due to Moreau's envelope and yields a bound that does not involve the additional term $C_{x}^{2}\\eta/2$ compared with Lemma 4.5. ", "page_idx": 8}, {"type": "text", "text": "Lemma 4.7. (Restatement of Theorem C.6) Set $\\begin{array}{r}{k\\ =\\ \\left(1+\\frac{2H C_{x}^{2}\\eta}{\\sqrt{m}}\\right)^{-1}}\\end{array}$ After $T$ iterations of Algorithm 1 with Moreau Envelope with $\\begin{array}{r}{\\eta\\le\\mu\\le\\frac{\\sqrt{m}}{2H C_{x}^{2}}}\\end{array}$ and $\\begin{array}{r}{T\\le\\frac{\\sqrt{m}}{H C_{x}^{2}\\eta}}\\end{array}$ we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\frac1{\\sum_{t=1}^{T}k^{t}}\\sum_{t=1}^{T}k^{t}M^{\\mu}(\\mathbf W_{t};S)\\leq\\Delta_{S}^{\\mathrm{oracle}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Theorem 3.6 is naturally derived via Theorem 4.6, Lemma 4.3 and 4.7. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we establish the generalization guarantees for variants of adversarial training applied to two-layer networks with smooth activation functions. For over-parameterized neural networks, we present robust generalization bound that are controlled by the Adversarial Regularized Empirical Risk Minimization (ARERM) oracle, applicable to any given initialization and any data distributions. One future direction is to extend our analysis to deep neural networks and beyond neural networks with smooth activation functions. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This research was supported, in part, by the DARPA GARD award HR00112020004, NSF CAREER award IIS-1943251, funding from the Institute for Assured Autonomy (IAA) at JHU, and the Spring'22 workshop on \u201cLearning and Games\u201d at the Simons Institute for the Theory of Computing. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In International Conference on Machine Learning, pages 322-332. PMLR, 2019.   \nPranjal Awasthi, Natalie Frank, and Mehryar Mohri. Adversarial learning guarantees for linear hypotheses and neural networks. In International Conference on Machine Learning, pages 431- 441.PMLR, 2020.   \nJimmy Ba, Murat Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of two-layer neural networks: An asymptotic viewpoint. In International conference on learning representations,2019. ", "page_idx": 8}, {"type": "text", "text": "Emilio Rafael Balda, Arash Behboodi, Niklas Koep, and Rudolf Mathar. Adversarial risk bounds for neural networks through sparsity based compression. arXiv preprint arXiv: 1906.00698, 2019. ", "page_idx": 8}, {"type": "text", "text": "Raef Bassily, Vitaly Feldman, Cristobal Guzman, and Kunal Talwar. Stability of stochastic gradint descent on nonsmooth convex losses. Advances in Neural Information Processing Systems, 33: 4381-4391, 2020.   \nBattista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim vSrndic, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, Par Il 13, pages 387-402. Springer, 2013.   \nOlivierBousqut andAndrElisseeff Stability and generalizationTheJournal ofMachine Learnng Research, 2:499-526, 2002.   \nOlivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharperbounds for uniformly stable algorithms. In Conference on Learning Theory, pages 610-626. PMLR, 2020.   \nRich Caruana, Steve Lawrence, and C. Giles. Overitting in neural nets: Backpropagation, conjugate gradient, and early stopping. In Advances in Neural Information Processing Systems, volume 13. MIT Press, 2000.   \nZachary Charles, Shashank Rajput, Stephen Wright, and Dimitris Papailiopoulos. Convergence and margin of adversarial training on separable data. arXiv preprint arXiv:1905.09209, 2019.   \nJinghui Chen, Yuan Cao, and Quanquan Gu. Benign overftting in adversarially robust linear classification. In Conference on Uncertainty in Artificial Intelligence, 2023.   \nXiwei Cheng, Kexin Fu, ad Fazan Faia Stability and generalization in free adversarial rainng. arXiv preprint arXiv:2404.08980, 2024.   \nDaniel Cullina, Arjun Nitin Bhagoji, and Prateek Mittal. PAC-learning in the presence of adversaries. Advances in Neural Information Processing Systems, 31, 2018.   \nSimon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.   \nFarzan Farnia and Asuman Ozdaglar. Train simultaneously, generalize better: Stability of gradient  \nbased minimax learners. In International Conference on Machine Learning, pages 3174-3185. PMLR, 2021.   \nFarzan Farnia, Jesse M Zhang, and David Tse. Generalizable adversarial training via spectral normalization. arXiv preprint arXiv: 1811.07457, 2018.   \nVitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. In Conference on Learning Theory, pages 1270-1279. PMLR, 2019.   \nSpencer Frei, Niladri S Chatterji, and Peter Bartlett. Benign overfitting without linearity: Neural network classifiers trained by gradient descent for noisy linear data. In Conference on Learning Theory, pages 2668-2703. PMLR, 2022.   \nRuiqi Gao, Tianle Cai, Haochuan Li, Cho-Jui Hsieh, Liwei Wang, and Jason D Lee. Convergence of adversarial training in overparametrized neural networks. Advances in Neural Information Processing Systems, 32, 2019.   \nMoritz Hardt, Ben Recht, and Yoram Singer Train faster, generalize better: Stability of stochastic gradient descent. In International conference on machine learning, pages 1225-1234. PMLR, 2016.   \nZiwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks. arXiv preprint arXiv: 1909.12292, 2019.   \nJustin Khimand Po-Ling Loh. Adversarial risk bounds via function transformation. arXiv preprint arXiv:1810.09519, 2018.   \nYunwen Lei. Stability and generalization of stochastic optimization with nonconvex and nonsmooth problems. In The Thirty Sixth Annual Conference on Learning Theory, pages 191-227. PMLR, 2023.   \nYunwen Lei and Yiming Ying. Fine-grained analysis of stability and generalization for stochastic gradient descent. In International Conference on Machine Learning, pages 5809-5819. PMLR, 2020.   \nJustin D Li and Matus Telgarsky. On achieving optimal adversarial test error. In International Conference on Learning Representations, 2023.   \nYan Li, Ethan Fang, Huan Xu, and Tuo Zhao. Implicit bias of gradient descent based adversarial training on separable data. In International Conference on Learning Representations, 2020.   \nChen Liu, Mathieu Salzmann, Tao Lin, Ryota Tomioka, and Sabine Suisstrunk. On the loss landscape of adversarial training: Identifying challenges and how to overcome them. Advances in Neural Information Processing Systems, 33:21476-21487, 2020.   \nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.   \nPoorya Mianjy and Raman Arora. Robustness guarantees for adversarially trained neural networks. Advances in Neural Information Processing Systems, 36, 2024.   \nMehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2018.   \nOmar Montasser, Steve Hanneke, and Nati Srebro. Reducing adversarially robust learning to nonrobust PAC learning. Advances in Neural Information Processing Systems, 33:14626-14637, 2020.   \nWaleed Mustafa, Yunwen Lei, and Marius Kloft. On the generalization analysis of adversarial learning. In International Conference on Machine Learning, pages 16174-16i96. PMLR, 2022.   \nTianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, and Jun Zhu. Bag of tricks for adversarial raining. In International Conference on Learning Representations, 2021.   \nLeslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversarially robust deep learning. In Proceedings of the 37th International Conference on Machine Learning, pages 8093-8104. PMLR, 2020.   \nDominic Richards and Ilja Kuzborskij. Stability & generalisation of gradient descent for shallow neural networks without the neural tangent kernel. Advances in neural information processing systems, 34:8609-8621, 2021.   \nDominic Richards and Mike Rabbat. Learning with gradient descent and weakly convex losses. In International Conference on Artijficial Intelligence and Statistics, pages 1990-1998. PMLR, 2021.   \nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv: 1312.6199, 2013.   \nPaul Viallard, Eric Guillaume VIDOT, Amaury Habrard, and Emilie Morvant. A PAC-Bayes analysis ofadversarial robustness. Advances in Neural Information Processing Systems, 34:14421-14433, 2021.   \nYihan Wang, Shuang Liu, and Xiao-Shan Gao. Data-dependent stability analsis of adversarial training. arXiv preprint arXiv:2401.03156, 2024a.   \nYisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving adversarial robustness requiresrevisiting misclassifed examples. In International Conference on Learning Representations, 2020.   \nYunjuan Wang, Enayat Ullah, Poorya Mianjy, and Raman Arora. Adversarial robustness is at odds with lazy training. Advances in Neural Information Processing Systems, 35:6505-6516, 2022.   \nYunjuan Wang, Kaibo Zhang, and Raman Arora. Benign overfitting in adversarially trained neural networks. In International Conference on Machine Learning, 2024b.   \nJiancong Xiao, Yanbo Fan, Ruoyu Sun, and Zhi-Quan Luo. Adversarial rademacher complexity of deep neural networks. arXiv preprint arXiv:2211.14966, 2022a.   \nJiancong Xiao, Yanbo Fan, Ruoyu Sun, Jue Wang, and Zhi-Quan Luo. Stability analysis and generalization bounds of adversarial training. Advances in Neural Information Processing Systems, 35:15446-15459, 2022b.   \nJiancong Xiao, Ruoyu Sun, and Zhi-Quan Luo. PAC-Bayesian adversarially robust generalization bounds for deep neural networks. In The Second Workshop on New Frontiers in Adversarial Machine Learning,2023.   \nJiancong Xiao, Jiawei Zhang, Zhi-Quan Luo, and Asuman Ozdaglar. Uniformly stable algorithms for adversarial training and beyond. arXiv preprint arXiv:2405.01817, 2024.   \nYue Xing, Qifan Song, and Guang Cheng. On the algorithmic stability of adversarial training. Advances in neural information processing systems, 34:26523-26535, 2021a.   \nYue Xing, Qifan Song, and Guang Cheng. On the generalization properties of adversarial training. In International Conference on Artificial Intelligence and Statistics, pages 505-513. PMLR, 2021b.   \nDong Yin, Ramchandran Kannan, and Peter Bartlett. Rademacher complexity for adversarially robust generalization. In International conference on machine learning, pages 7085-7094. PMLR, 2019.   \nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In International conference on machine learning, pages 7472-7482. PMLR, 2019.   \nYi Zhang, Orestis Plevrakis, Simon S Du, Xingguo Li, Zhao Song, and Sanjeev Arora. Overparameterized adversarial training: An analysis overcoming the curse of dimensionality. Advances in Neural Information Processing Systems, 33:679-688, 2020.   \nDifan Zou, Spencer Frei, and Quanquan Gu. Provable robustness of adversarial training for learning halfspaces with noise. In International Conference on Machine Learning, pages 13002-13011. PMLR, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Technical Theorems and Lemmas ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Lemma A.1. Let $\\ell(z)=\\ln(1+e^{-z})$ be the logistic loss function. We have $|\\ell^{\\prime}(z)|\\leq\\operatorname*{min}\\{1,\\ell(z)\\}$ Proof of Lemma A.1. ", "page_idx": 12}, {"type": "equation", "text": "$$\n|\\ell^{\\prime}(z)|=-\\ell^{\\prime}(z)=\\frac{1}{1+e^{z}}\\leq\\left\\{1;\\atop\\ln(1+e^{-z}).\\right.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left(e^{z}>0\\right)}\\\\ {\\left(\\frac{x}{1+x}\\leq\\ln(1+x)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Lemma A.2. For any sample $\\mathbf{z}=(\\mathbf{x},y)$ , and any weight matrices $\\mathrm{W}$ and $\\mathrm{W^{\\prime}}$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\ell_{r o b}(\\mathbf{z},\\mathbf{W})-\\ell_{r o b}(\\mathbf{z},\\mathbf{W}^{\\prime})\\leq C_{x}\\|\\mathbf{W}-\\mathbf{W}^{\\prime}\\|_{2}\\cdot\\operatorname*{min}\\{1,\\ell_{r o b}(\\mathbf{z},\\mathbf{W})\\}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof of Lemma A.2. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\phi_{d+1}(\\mathbf{x},\\mathbf{R})-V_{d+1}(\\mathbf{x},\\mathbf{R})\\right\\rangle}\\\\ &{=\\frac{\\theta_{0}}{\\sin\\theta_{0}}\\left\\{\\left\\langle\\mathbf{g}(\\mathbf{\\hat{x}},\\mathbf{u})\\right\\rangle-\\frac{1}{\\sin\\theta_{0}}\\left\\{\\left\\langle\\mathbf{b}^{T}(\\mathbf{\\hat{x}},\\mathbf{R})\\right\\rangle\\right.}\\\\ &{\\quad\\left.+\\frac{\\theta_{0}}{\\sin\\theta_{0}}\\left\\{\\left\\langle\\mathbf{b}^{T}(\\mathbf{\\hat{x}},\\mathbf{\\hat{w}})\\right\\rangle-\\left\\langle\\mathbf{b}^{T}(\\mathbf{\\hat{x}},\\mathbf{R})\\right\\rangle\\right.}\\\\ &{\\quad\\left.+\\frac{\\theta_{0}}{\\sin\\theta_{0}}\\left\\{\\left\\langle\\mathbf{b}^{T}(\\mathbf{\\hat{x}},\\mathbf{R})\\right\\rangle-\\left\\langle\\mathbf{b}^{T}(\\mathbf{\\hat{x}},\\mathbf{R})\\right\\rangle\\right.\\right.}\\\\ &{\\quad\\left.-\\frac{\\theta_{0}}{\\sin\\theta_{0}}\\left\\{\\left\\langle\\mathbf{b}^{T}(\\mathbf{\\hat{x}},\\mathbf{R})\\right\\rangle\\right.\\right.\\cdot\\left.\\left.\\left(\\sum_{a}a,b(\\mathbf{\\hat{e}},\\mathbf{b}))-\\frac{\\theta_{0}}{\\sin\\theta_{0}}\\left\\{\\left\\langle\\mathbf{b}^{T}(\\mathbf{\\hat{x}},\\mathbf{\\hat{x}})\\right\\rangle\\right\\}\\right]}\\\\ &{=\\frac{\\theta_{0}}{\\sin\\theta_{0}}\\left\\{\\left\\langle\\mathbf{g}(\\mathbf{f}(\\mathbf{\\hat{x}},\\mathbf{R}))\\right\\rangle-\\frac{1}{\\sqrt{\\pi}}\\left\\langle\\mathbf{m}_{a}^{T}\\left\\{\\mathbf{b}^{T}(\\mathbf{\\hat{x}},\\mathbf{\\hat{x}})-\\frac{\\theta_{0}}{\\sqrt{1}}a,\\partial(\\mathbf{b}^{T}\\mathbf{\\hat{x}},\\mathbf{\\hat{x}})\\right\\rangle\\right\\}}\\\\ &{\\quad\\left.+\\frac{\\theta_{0}}{\\sin\\theta_{0}}\\left\\{\\left\\langle\\mathbf{g}(\\mathbf{f}(\\mathbf{\\hat{x}},\\mathbf{R}))\\right\\rangle-\\frac{1}{\\sqrt{\\pi}}\\left\\langle\\mathbf{m}_{a}^{T}\\left\\{\\mathbf{b}^{T}(\\mathbf{\\hat{x}},\\mathbf{\\hat{x}})-\\frac{\\theta_{0}}{\\sqrt{1}}m,\\right\\rangle\\right.}\\\\ &{\\quad\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "In the following proposition, we build the relationship between the generalization gap and uniform stability. ", "page_idx": 12}, {"type": "text", "text": "Proposition A.3. Let $S$ and $S^{\\prime}$ be any two neighboring data sets that differ only in one example. Let $\\mathbf{W}_{t}=A(S),\\mathbf{W}_{t}^{\\prime}=A(S^{\\prime})$ be the weight returned after running algorithm $\\boldsymbol{\\mathcal{A}}$ for $t$ stepsusing $S$ and $S^{\\prime}$ , respectively. $\\delta_{A}(S,S^{\\prime})=\\|A(S)\\,\\overset{\\cdot}{-}\\,A(S^{\\prime})\\|_{F}$ Then if sup, $\\begin{array}{r}{\\delta\\bar{A}(S,S^{\\prime})<\\frac{1}{C_{x}}}\\end{array}$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}L_{r o b}(\\mathbf{W}_{t})\\leq\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\frac{1}{1-C_{x}\\cdot\\operatorname*{sup}_{S\\subseteq S^{\\prime}}\\delta_{A}(S,S^{\\prime})}\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}L_{r o b}(\\mathbf{W}_{t})\\le\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)+C_{x}\\cdot\\operatorname*{sup}_{S\\cong S^{\\prime}}\\delta_{A}(S,S^{\\prime}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof of Proposition A.3. Let $S$ and $S^{\\prime}$ differ in one example, and $z^{\\prime}=S^{\\prime}\\backslash S$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\left(L_{r o b}(\\mathbf{W}_{t})-\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)\\right)=\\mathbb{E}_{S\\cup\\{z^{\\prime}\\}\\sim\\mathcal{D}^{n+1}}\\left[\\left(\\ell_{r o b}(z^{\\prime},\\mathbf{W}_{t})-\\ell_{r o b}(z^{\\prime},\\mathbf{W}_{t}^{\\prime})\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Combining Lemma A.2 and Equation (6) we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\Big(L_{r o b}(\\mathbf{W}_{t})-\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)\\Big)\\leq\\mathbb{E}_{S\\cup\\{z^{\\prime}\\}\\sim\\mathcal{D}^{n+1}}[C_{x}\\|\\mathbf{W}_{t}-\\mathbf{W}_{t}^{\\prime}\\|_{2}\\cdot\\operatorname*{min}\\{1,\\ell_{r o b}(\\mathbf{z}^{\\prime},\\mathbf{W}_{t})\\}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Based on the definition of $\\delta_{A}$ \uff0c ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\left(L_{r o b}(\\mathbf{W}_{t})-\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)\\right)}\\\\ &{\\ \\leq C_{x}\\cdot\\underset{S\\geq S^{\\prime}}{\\operatorname*{sup}}\\,\\delta_{A}(S,S^{\\prime})\\cdot\\mathbb{E}_{S\\cup\\{\\mathbf{z}^{\\prime}\\}\\sim\\mathcal{D}^{n+1}}\\operatorname*{min}\\{1,\\ell_{r o b}(\\mathbf{z}^{\\prime},\\mathbf{W}_{t})\\}}\\\\ &{\\ \\leq C_{x}\\cdot\\underset{S\\geq S^{\\prime}}{\\operatorname*{sup}}\\,\\delta_{A}(S,S^{\\prime})\\cdot\\operatorname*{min}\\{1,\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}L_{r o b}(\\mathbf{W}_{t})\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Simplifying this inequality, we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}L_{r o b}(\\mathbf{W}_{t})\\leq\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\frac{1}{1-C_{x}\\cdot\\operatorname*{sup}_{S\\subseteq S^{\\prime}}\\delta_{A}(S,S^{\\prime})}\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}L_{r o b}(\\mathbf{W}_{t})\\le\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)+C_{x}\\cdot\\operatorname*{sup}_{S\\cong S^{\\prime}}\\delta_{A}(S,S^{\\prime}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The following lemma gives the weakly convex property of the robust loss (by considering the special caseof $\\beta_{1}=0$ ", "page_idx": 13}, {"type": "text", "text": "Lemma A.4. Given any data $\\left({\\bf{x}},{y}\\right)$ , for model with weight $\\mathrm{W}$ let $\\tilde{\\mathbf{x}}(\\mathbf{W})\\in B(\\mathbf{x})$ be an $\\beta_{1}$ -optimal adversarial examples such that $\\ell(y f(\\widetilde\\mathbf{x}(\\mathbf{W}),\\mathbf{W}))\\ge\\operatorname*{max}_{\\widetilde\\mathbf{x}\\in B(\\mathbf{x})}\\ell(y f(\\widetilde\\mathbf{x},\\mathbf{W}))-\\beta_{1}$ . Then for any two weight matrices $\\mathbf{W}^{1},\\mathbf{W}^{2}\\in\\mathbb{R}^{d\\times m}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\langle(\\widetilde{\\mathbf{x}}(\\mathbf{W}^{2}),y),\\mathbf{W}^{2}\\rangle\\ge\\ell((\\widetilde{\\mathbf{x}}(\\mathbf{W}^{1}),y),\\mathbf{W}^{1})+\\big\\langle\\nabla_{\\mathbf{w}}\\ell((\\widetilde{\\mathbf{x}}(\\mathbf{W}^{1}),y),\\mathbf{W}^{1}),\\mathbf{W}^{2}-\\mathbf{W}^{1}\\big\\rangle-\\beta_{1}-\\frac{H C_{x}^{2}}{2\\sqrt{m}}\\|\\mathbf{W}^{2}-\\mathbf{W}^{1}\\|_{L^{2}(\\Omega_{y})}^{2},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof of Lemma A.4. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ell((\\widetilde{\\mathbf{x}}(\\mathbf{W}^{2}),y),\\mathbf{W}^{2})-\\ell((\\widetilde{\\mathbf{x}}(\\mathbf{W}^{1}),y),\\mathbf{W}^{1})-\\left\\langle\\nabla_{\\mathbf{W}}\\ell((\\widetilde{\\mathbf{x}}(\\mathbf{W}^{1}),y),\\mathbf{W}^{1}),\\mathbf{W}^{2}-\\mathbf{W}^{1}\\right\\rangle+\\beta_{1}}\\\\ &{=\\!\\ell(y f_{\\mathbf{W}^{2}}(\\widetilde{\\mathbf{x}}(\\mathbf{W}^{2})))-\\ell(y f_{\\mathbf{W}^{1}}(\\widetilde{\\mathbf{x}}(\\mathbf{W}^{1})))-\\left\\langle\\nabla_{\\mathbf{W}}\\ell((\\widetilde{\\mathbf{x}}(\\mathbf{W}^{1}),y),\\mathbf{W}^{1}),\\mathbf{W}^{2}-\\mathbf{W}^{1}\\right\\rangle+\\beta_{1}}\\\\ &{\\ge\\!\\underset{\\widetilde{\\mathbf{x}}\\in B(x)}{\\operatorname*{max}}\\ell(y f_{\\mathbf{W}^{2}}(\\widetilde{\\mathbf{x}}))-\\ell(y f_{\\mathbf{W}^{1}}(\\widetilde{\\mathbf{x}}(\\mathbf{W}^{1})))-\\left\\langle\\nabla_{\\mathbf{W}}\\ell((\\widetilde{\\mathbf{x}}(\\mathbf{W}^{1}),y),\\mathbf{W}^{1}),\\mathbf{W}^{2}-\\mathbf{W}^{1}\\right\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "(By definition of $\\beta_{1}$ -optimal adversarial examples) ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ z\\ell(y f_{\\mathbb{W}^{2}}(\\widetilde\\mathbf{x}(\\mathbf{W}^{1})))-\\ell(y f_{\\mathbb{W}^{1}}(\\widetilde\\mathbf{x}(\\mathbf{W}^{1})))-\\left\\langle\\nabla_{\\mathbf{W}}\\ell((\\widetilde\\mathbf{x}(\\mathbf{W}^{1}),y),\\mathbf{W}^{1}),\\mathbf{W}^{2}-\\mathbf{W}^{1}\\right\\rangle}\\\\ &{\\ z\\ell(y f_{\\mathbb{W}^{1}}(\\widetilde\\mathbf{x}(\\mathbf{W}^{1})))\\cdot\\left(y f_{\\mathbb{W}^{2}}(\\widetilde\\mathbf{x}(\\mathbf{W}^{1}))-y f_{\\mathbb{W}^{1}}(\\widetilde\\mathbf{x}(\\mathbf{W}^{1}))\\right)-\\left\\langle\\nabla_{\\mathbf{W}}\\ell((\\widetilde\\mathbf{x}(\\mathbf{W}^{1}),y),\\mathbf{W}^{1}),\\mathbf{W}^{2}-\\mathbf{W}^{1}\\right\\rangle}\\end{array}\n$$/ / ", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{-\\ell^{\\prime}(y f_{\\mathbf{W}^{1}}(\\widetilde{\\mathbf{x}}(\\mathbf{W}^{1})))y\\displaystyle\\sum_{s=1}^{m}a_{s}\\left(\\phi(\\left\\langle\\mathbf{w}_{s}^{2},\\widetilde{\\mathbf{x}}(\\mathbf{W}^{1})\\right\\rangle)-\\phi(\\left\\langle\\mathbf{w}_{s}^{1},\\widetilde{\\mathbf{x}}(\\mathbf{W}^{1})\\right\\rangle)-\\phi^{\\prime}(\\left\\langle\\mathbf{w}_{s}^{1},\\widetilde{\\mathbf{x}}(\\mathbf{W}^{1})\\right\\rangle)\\left\\langle\\mathbf{w}_{s}^{2}-\\mathbf{w}_{s}^{1},\\widetilde{\\mathbf{x}}(\\mathbf{W}^{1})\\right\\rangle\\right.}\\\\ &{\\left.\\displaystyle\\>-\\left\\lvert\\ell^{\\prime}(y f_{\\mathbf{W}^{1}}(\\widetilde{\\mathbf{x}}(\\mathbf{W}^{1})))\\right\\rvert\\sum_{s=1}^{m}\\frac{1}{\\sqrt{m}}\\cdot\\frac{H}{2}\\left\\langle\\mathbf{w}_{s}^{2}-\\mathbf{w}_{s}^{1},\\widetilde{\\mathbf{x}}(\\mathbf{W}^{1})\\right\\rangle^{2}\\right.}\\\\ &{\\left.\\displaystyle\\>-1\\cdot\\frac{H}{2\\sqrt{m}}\\lVert(\\mathbf{W}^{2}-\\mathbf{W}^{1})^{\\top}\\widetilde{\\mathbf{x}}(\\mathbf{W}^{1})\\rVert_{2}^{2}\\right.}\\\\ &{\\left.\\displaystyle\\>-\\frac{H C_{x}^{2}}{2\\sqrt{m}}\\lVert\\mathbf{W}^{2}-\\mathbf{W}^{1}\\rVert_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The following lemma tells us the gradient has a universal upper bound. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.5. For any data $\\left({\\bf{x}},{y}\\right)$ and any weight matrix $\\mathrm{W}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\mathrm{W}}\\ell(y f(\\mathbf{x};\\mathbf{W}))\\|_{F}\\leq C_{x}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma A.5. Since $\\nabla_{\\mathbf{W}}\\ell(y f(\\mathbf{x};\\mathbf{W}))=[\\ell^{\\prime}(y f(\\mathbf{x};\\mathbf{W}))y a_{s}\\phi^{\\prime}(\\langle\\mathbf{w}_{s},\\mathbf{x}\\rangle)\\mathbf{x}]_{s=1}^{m},$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\Vert\\nabla_{\\mathbf{w}}\\ell(y f(\\mathbf{x};\\mathbf{W}))\\Vert_{F}=\\sqrt{\\displaystyle\\sum_{s=1}^{m}\\Vert\\ell^{\\prime}(y f(\\mathbf{x};\\mathbf{W}))y a_{s}\\phi^{\\prime}(\\langle\\mathbf{W}_{s},\\mathbf{x}\\rangle)\\mathbf{x}\\Vert_{2}^{2}}}&{}\\\\ {\\leq-C_{x}\\cdot\\ell^{\\prime}(y f(\\mathbf{x};\\mathbf{W}))}&{(|a_{s}|=\\frac{1}{\\sqrt{m}},\\phi^{\\prime}\\leq1,\\Vert\\mathbf{x}\\Vert_{2}\\leq C_{x})}&\\\\ &{\\leq C_{x}.}&{(\\mathrm{Lemma~A.l})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "B  Missing Proofs in Section 3.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Now we give the uniform argument stability upper bound. ", "page_idx": 14}, {"type": "text", "text": "Theorem B.1. (Restatement of Theorem 3.1) Let $S$ and $S^{(i)}$ only differ in the $i$ -th data. $\\mathrm{W}_{T}$ and $\\mathbf{W}_{T}^{(i)}$ denote thweigt matrcsetd fterferuing lgor GD for $T$ iterations on $S$ and S(i), respectively. Then we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\mathbf{W}_{T}-\\mathbf{W}_{T}^{(i)}\\|_{F}^{2}\\leq e^{1+\\frac{2H C_{x}^{2}\\eta T}{\\sqrt{m}}}\\left(4C_{x}^{2}\\eta^{2}(T+1)+\\frac{4C_{x}^{2}\\eta^{2}(T+1)^{2}}{n^{2}}+4\\beta_{1}\\eta(T+1)\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof of Theorem B.1. For any weight matrix $\\mathrm{W}$ and any perturbation of the $j$ -th data $\\tilde{\\mathbf{x}}_{j}\\in B(\\mathbf{x}_{j})$ $(j\\neq i)$ , define $L_{S^{\\setminus i}}(\\mathrm{W};\\{\\tilde{\\mathrm{x}}_{j}\\}_{j\\neq i})=\\textstyle\\frac{1}{n}\\sum_{j\\neq i}\\ell(y_{j}f(\\tilde{\\mathrm{x}}_{j};\\mathrm{W}))$ to be the loss of the (perturbed) data set without including the $i$ -th data. Let $\\tilde{\\mathbf{x}}_{j}(\\mathbf{W}_{t})$ denote the $\\beta_{1}$ -optimal adversarial example of $\\mathbf{{X}}_{j}$ given $\\mathbf{W}_{t}$ and $\\tilde{\\mathbf{x}}_{j}(\\mathbf{W}_{t}^{(i)})$ denote the $\\beta_{1}$ -optimal adversarial example of $\\mathbf{{X}}_{j}$ given $\\mathbf{W}_{t}^{(i)}$ . We first show that the gradient is an \u201calmost\" monotone operator, which is derived from the weakly convex property of the robust loss (see Lemma A.4). ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\beta_{1}+\\displaystyle\\frac{H C_{x}^{2}}{2\\sqrt{m}}\\|\\mathbf{W}_{t}-\\mathbf{W}_{t}^{(i)}\\|_{F}^{2}+{\\cal L}_{S^{\\setminus}\\iota}\\big(\\mathbf{W}_{t};\\{\\tilde{\\mathbf{x}}_{j}(\\mathbf{W}_{t})\\}_{j\\neq i}\\big)}\\\\ &{\\displaystyle\\geq\\displaystyle\\frac{1}{n}\\sum_{j\\neq i}\\left[\\beta_{1}+\\frac{H C_{x}^{2}}{2\\sqrt{m}}\\|\\mathbf{W}_{t}-\\mathbf{W}_{t}^{(i)}\\|_{F}^{2}+\\ell(y_{j}f(\\tilde{\\mathbf{x}}_{j}(\\mathbf{W}_{t});\\mathbf{W}_{t}))\\right]}\\\\ &{\\displaystyle\\geq\\frac{1}{n}\\sum_{j\\neq i}\\left[\\ell(y_{j}f(\\tilde{\\mathbf{x}}_{j}(\\mathbf{W}_{t}^{(i)});\\mathbf{W}_{t}^{(i)}))+\\left<\\nabla_{\\mathbf{W}}\\ell(y_{j}f(\\tilde{\\mathbf{x}}_{j}(\\mathbf{W}_{t}^{(i)});\\mathbf{W}_{t}^{(i)})),\\mathbf{W}_{t}-\\mathbf{W}_{t}^{(i)}\\right>\\right]}&{\\mathrm{(Lemma~A.4)}}\\\\ &{\\displaystyle={\\cal L}_{S^{\\setminus}}(\\mathbf{W}_{t}^{(i)};\\{\\tilde{\\mathbf{x}}_{j}(\\mathbf{W}_{t}^{(i)})\\}_{j\\neq i})+\\left<\\nabla_{\\mathbf{W}}{\\cal L}_{S^{\\setminus}\\iota}(\\mathbf{W}_{t}^{(i)};\\{\\tilde{\\mathbf{x}}_{j}(\\mathbf{W}_{t}^{(i)})\\}_{j\\neq i}),\\mathbf{W}_{t}-\\mathbf{W}_{t}^{(i)}\\right>.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Similarly, we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\quad\\beta_{1}+\\displaystyle\\frac{H C_{x}^{2}}{2\\sqrt{m}}\\|\\mathbf{W}_{t}-\\mathbf{W}_{t}^{(i)}\\|_{F}^{2}+{\\cal L}_{S^{\\setminus{i}}}(\\mathbf{W}_{t}^{(i)};\\{\\tilde{\\mathbf{x}}_{j}(\\mathbf{W}_{t}^{(i)})\\}_{j\\neq i})}&\\\\ &{\\quad\\displaystyle\\geq{\\cal L}_{S^{\\setminus{i}}}(\\mathbf{W}_{t};\\{\\tilde{\\mathbf{x}}_{j}(\\mathbf{W}_{t})\\}_{j\\neq i})+\\left\\langle\\nabla_{\\mathbf{w}}{\\cal L}_{S^{\\setminus{i}}}(\\mathbf{W}_{t};\\{\\tilde{\\mathbf{x}}_{j}(\\mathbf{W}_{t})\\}_{j\\neq i}),\\mathbf{W}_{t}^{(i)}-\\mathbf{W}_{t}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Adding these two inequalities together, we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\langle\\nabla_{\\mathbf{W}}L_{S^{\\setminus i}}(\\mathbf{W}_{t}^{(i)};\\{\\tilde{\\mathbf{x}}_{j}(\\mathbf{W}_{t}^{(i)})\\}_{j\\neq i})-\\nabla_{\\mathbf{W}}L_{S^{\\setminus i}}(\\mathbf{W}_{t};\\{\\tilde{\\mathbf{x}}_{j}(\\mathbf{W}_{t})\\}_{j\\neq i}),\\mathbf{W}_{t}^{(i)}-\\mathbf{W}_{t}\\right\\rangle}\\\\ &{\\geq-2\\beta_{1}-\\displaystyle\\frac{H C_{x}^{2}}{\\sqrt{m}}\\|\\mathbf{W}_{t}-\\mathbf{W}_{t}^{(i)}\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now we start upper bounding $\\|\\mathbf{W}_{t}-\\mathbf{W}_{t}^{(i)}\\|_{F}$ ", "page_idx": 15}, {"type": "text", "text": "$\\begin{array}{r l}&{\\quad\\|\\mathbf{W}_{t+1}-\\mathbf{W}_{t+1}^{(i)}\\|_{F}^{2}}\\\\ &{=\\!\\|\\mathbf{W}_{t}-\\eta\\nabla_{\\mathbf{W}}L_{S^{\\setminus i}}(\\mathbf{W}_{t};\\{\\tilde{\\mathbf{x}}_{j}(\\mathbf{W}_{t})\\}_{j\\neq i})-\\frac{\\eta}{n}\\nabla_{\\mathbf{W}}\\ell(y_{i}f(\\tilde{\\mathbf{x}}_{i}(\\mathbf{W}_{t});\\mathbf{W}_{t}))}\\\\ &{\\qquad\\quad-\\mathbf{W}_{t}^{(i)}+\\eta\\nabla_{\\mathbf{W}}L_{S^{\\setminus i}}(\\mathbf{W}_{t}^{(i)};\\{\\tilde{\\mathbf{x}}_{j}(\\mathbf{W}_{t}^{(i)})\\}_{j\\neq i})+\\frac{\\eta}{n}\\nabla_{\\mathbf{W}}\\ell(y_{i}^{\\prime}f(\\tilde{\\mathbf{x}}_{i}^{\\prime}(\\mathbf{W}_{t}^{(i)});\\mathbf{W}_{t}^{(i)}))\\|_{F}^{2}}\\\\ &{\\leq\\!\\left(\\|\\mathbf{W}_{t}-\\eta\\nabla_{\\mathbf{W}}L_{S^{\\setminus i}}(\\mathbf{W}_{t};\\{\\tilde{\\mathbf{x}}_{j}(\\mathbf{W}_{t})\\}_{j\\neq i})-\\mathbf{W}_{t}^{(i)}+\\eta\\nabla_{\\mathbf{W}}L_{S^{\\setminus i}}(\\mathbf{W}_{t}^{(i)};\\{\\tilde{\\mathbf{x}}_{j}(\\mathbf{W}_{t}^{(i)})\\}_{j\\neq i})\\|_{F}+\\frac{2\\eta C_{x}}{n}\\right)}\\\\ &{\\qquad\\quad\\leq\\!\\left(\\eta\\mathbf{W}_{t}-\\eta\\nabla_{\\mathbf{W}}L_{S^{\\setminus i}}(\\mathbf{W}_{t};\\{\\tilde{\\mathbf{x}}_{j}(\\mathbf{W}_{t})\\}_{j\\neq i})-\\mathbf{W}_{t}^{(i)}+\\eta\\nabla_{\\mathbf{W}}L_{S^{\\setminus i}}(\\mathbf{W}_{t}^{(i)};\\{\\tilde{\\mathbf{x}}_{j}(\\mathbf{W}_{t}^{(i)})\\}_{j\\neq i})\\|_{F}+\\frac{2\\eta C_{x}}{n}\\right)}\\end{array}$ (Lemma A.5) $\\begin{array}{r l}&{\\le\\frac{T+2}{T+1}\\|\\mathbf{W}_{t}-\\eta\\nabla_{\\mathbf{W}}L_{S^{\\perp}}(\\mathbf{W}_{t};\\{\\hat{\\mathbf{s}}_{j}(\\mathbf{W}_{t})\\}_{j,\\hat{\\boldsymbol{p}}(t)})-\\mathbf{W}_{t}^{(i)}+\\eta\\nabla\\mathbf{w}L_{S^{\\perp}}(\\mathbf{W}_{t}^{(i)};\\{\\hat{\\mathbf{s}}_{j}(\\mathbf{W}_{t}^{(i)})\\}_{j,\\hat{\\boldsymbol{p}}(t)})\\|_{F}^{2}}\\\\ &{\\qquad\\qquad+(T+2)\\frac{4\\eta^{2}C_{2}^{2}}{n^{2}}}\\\\ &{=\\frac{T+2}{T+1}\\|\\mathbf{W}_{t}-\\mathbf{W}_{t}^{(i)}\\|_{F}^{2}+\\frac{T+2}{T+1}\\|\\eta\\nabla_{\\mathbf{W}}L_{S^{\\perp}}(\\mathbf{W}_{t};\\{\\hat{\\mathbf{s}}_{j}(\\mathbf{W}_{t})\\}_{j,\\hat{\\boldsymbol{p}}(t)})-\\eta\\nabla_{\\mathbf{W}}L_{S^{\\perp}}(\\mathbf{W}_{t}^{(i)};\\{\\hat{\\mathbf{s}}_{j}(\\mathbf{W}_{t}^{(i)})\\}_{j,\\hat{\\boldsymbol{p}}(t)})}\\\\ &{\\qquad-2\\eta\\frac{T+2}{T+1}\\left\\langle\\nabla\\mathbf{w}L_{S^{\\perp}}(\\mathbf{W}_{t}^{(i)};\\{\\hat{\\mathbf{s}}_{j}(\\mathbf{W}_{t}^{(i)})\\}_{j,\\hat{\\boldsymbol{p}}(t)})-\\nabla\\mathbf{w}L_{S^{\\perp}}(\\mathbf{W}_{t};\\{\\hat{\\mathbf{s}}_{j}(\\mathbf{W}_{t})\\}_{j,\\hat{\\boldsymbol{p}}(t)},\\mathbf{W}_{t}^{(i)})-\\mathbf{W}_{t}\\right\\rangle}\\\\ &{\\qquad\\quad+(T+2)\\frac{4\\eta^{2}C_{2}^{2}}{n^{2}}}\\\\ &{\\le\\frac{T+2}{T+1}\\|\\mathbf{W}_{t}-\\mathbf{W}_{t}^{(i)}\\|_{F}^{2}+\\frac{T+2}{T+1}4\\eta^{2}C_{x}^{2}+\\frac{T+2}{T+1}(4\\beta_{1}\\eta+\\frac{2H C_{2}^{$ T+2 2HCm)IW- W12 + 4m2C\u00b22+ 4Bn+(T+1)m\u00b2 ", "page_idx": 15}, {"type": "text", "text": "Define $\\begin{array}{r}{\\gamma=\\frac{T+2}{T+1}(1+\\frac{2H C_{x}^{2}}{\\sqrt{m}}\\eta)}\\end{array}$ , then the inequality above can be written as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\mathbf{W}_{t+1}-\\mathbf{W}_{t+1}^{(i)}\\|_{F}^{2}\\le\\gamma\\|\\mathbf{W}_{t}-\\mathbf{W}_{t}^{(i)}\\|_{F}^{2}+\\frac{T+2}{T+1}\\left(4\\eta^{2}C_{x}^{2}+4\\beta_{1}\\eta+(T+1)\\frac{4\\eta^{2}C_{x}^{2}}{n^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Dividing both sides by $\\gamma^{t+1}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\|\\mathbf{W}_{t+1}-\\mathbf{W}_{t+1}^{(i)}\\|_{F}^{2}}{\\gamma^{t+1}}\\leq\\frac{\\|\\mathbf{W}_{t}-\\mathbf{W}_{t}^{(i)}\\|_{F}^{2}}{\\gamma^{t}}+\\frac{T+2}{T+1}\\frac{4\\eta^{2}C_{x}^{2}+4\\beta_{1}\\eta+(T+1)\\frac{4\\eta^{2}C_{x}^{2}}{n^{2}}}{\\gamma^{t+1}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Summing up this inequality for $t=0,1,\\ldots,T-1$ , we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{W}_{T}-\\mathbf{W}_{T}^{(i)}\\|_{F}^{2}\\leq\\gamma^{T}\\displaystyle\\sum_{t=0}^{T-1}\\frac{1}{T+1}\\frac{4\\pi^{2}C_{x}^{2}+4\\beta_{1}\\eta\\left(T+\\lfloor\\lambda\\rfloor\\frac{4\\eta^{2}C_{x}^{2}}{n^{2}}\\right.}{\\gamma^{t+1}}}\\\\ &{\\left.\\phantom{\\sum_{t=0}^{T-1}}\\frac{T+2}{\\gamma}\\left(4\\eta^{2}C_{x}^{2}+4\\beta_{1}\\eta+(T+1)\\frac{4\\eta^{2}C_{x}^{2}}{n^{2}}\\right)\\right.}\\\\ &{\\leq(T+1)\\gamma^{T}\\displaystyle\\frac{T+2}{T+1}\\left(4\\eta^{2}C_{x}^{2}+4\\beta_{1}\\eta+(T+1)\\frac{4\\eta^{2}C_{x}^{2}}{n^{2}}\\right)}\\\\ &{=(T+1)(\\displaystyle\\frac{T+2}{T+1})^{T+1}(1+\\frac{2H C_{x}^{2}}{\\sqrt{m}}\\eta^{T}\\left(4\\eta^{2}C_{x}^{2}+4\\beta_{1}\\eta+(T+1)\\frac{4\\eta^{2}C_{x}^{2}}{n^{2}}\\right)}\\\\ &{\\leq(T+1)e\\cdot\\frac{2\\pi\\epsilon_{\\gamma}^{2}\\pi}{\\sqrt{m}}\\left(4\\eta^{2}C_{x}^{2}+4\\beta_{1}\\eta+(T+1)\\frac{4\\eta^{2}C_{x}^{2}}{n^{2}}\\right)\\qquad\\left.(1+x\\leq\\epsilon^{\\gamma})\\right.}\\\\ &{=e^{1+\\frac{2\\pi\\epsilon_{\\gamma}^{2}\\pi}{\\sqrt{m}}}\\left(4C_{x}^{2}\\eta^{2}(T+1)+\\frac{4C_{x}^{2}\\eta^{2}(T+1)^{2}}{n^{2}}+4\\beta_{1}\\eta(T+1)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The proof of Theorem 3.1 is immediately obtained from Theorem B.1 by observing $e^{1+\\frac{2H C_{x}^{2}\\eta T}{\\sqrt{m}}}\\leq e^{3}$ ", "page_idx": 16}, {"type": "text", "text": "Next we give an optimization guarantee. We show that when $T$ is sufficiently large, the robust training loss can approach the adversarial regularized empirical risk minimization oracle. We do not need early stopping in the Theorem below. ", "page_idx": 16}, {"type": "text", "text": "Theorem B.2. After running Algorithm GD for $T$ iterations, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{|\\le t\\le T}\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)\\le\\operatorname*{min}_{\\mathbf{w}\\in\\mathbb{R}^{d\\times m}}\\left(\\widehat{L}_{r o b}(\\mathbf{W};S)+\\frac{H C_{x}^{2}}{2\\sqrt{m}(1-\\frac{\\frac{1}{(1+\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}})^{T+1}}}{(1+\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}})^{T+1}})}\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}+\\frac{C_{x}^{2}\\eta}{2}+\\beta_{1}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem B.2. For any given $\\mathrm{W}$ ,wehave ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\quad\\|{\\bf W}-{\\bf W}_{t+1}\\|_{F}^{2}}&\\\\ &{=\\|{\\bf W}-{\\bf W}_{t}+\\eta\\nabla_{\\bf W}L({\\bf W}_{t};\\{\\tilde{\\bf x}_{i}({\\bf W}_{t})\\}_{i=1}^{n})\\|_{F}^{2}}&\\\\ &{=\\|{\\bf W}-{\\bf W}_{t}\\|_{F}^{2}+\\eta^{2}\\|\\nabla_{\\bf W}L({\\bf W}_{t};\\{\\tilde{\\bf x}_{i}({\\bf W}_{t})\\}_{i=1}^{n})\\|_{F}^{2}+2\\eta\\left\\langle\\nabla_{\\bf W}L({\\bf W}_{t};\\{\\tilde{\\bf x}_{i}({\\bf W}_{t})\\}_{i=1}^{n}),{\\bf W}-{\\bf W}_{t}\\right\\rangle}&\\\\ &{\\leq\\|{\\bf W}-{\\bf W}_{t}\\|_{F}^{2}+\\eta^{2}C_{x}^{2}+\\displaystyle\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}}\\|{\\bf W}-{\\bf W}_{t}\\|_{F}^{2}+2\\eta\\widehat{L}_{r o b}({\\bf W};S)-2\\eta L({\\bf W}_{t};\\{\\tilde{\\bf x}_{i}({\\bf W}_{t})\\}_{i=1}^{n})}&\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad({\\bf L e m m a}~{\\cal A}.5\\mathrm{~and~}{\\bf L e m m a}~\\mathcal{B}}\\\\ &{=\\!(1+\\displaystyle\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}})\\|{\\bf W}-{\\bf W}_{t}\\|_{F}^{2}+\\eta^{2}C_{x}^{2}+2\\eta\\widehat{L}_{r o b}({\\bf W};S)-2\\eta L({\\bf W}_{t};\\{\\tilde{\\bf x}_{i}({\\bf W}_{t})\\}_{i=1}^{n}).}&\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Dividing both sides by $\\begin{array}{r}{(1+\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}})^{t+1}}\\end{array}$ )t+1 we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\|\\mathbf{W}-\\mathbf{W}_{t+1}\\|_{F}^{2}}{(1+\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}})^{t+1}}\\leq\\frac{\\|\\mathbf{W}-\\mathbf{W}_{t}\\|_{F}^{2}}{(1+\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}})^{t}}+\\frac{\\eta^{2}C_{x}^{2}}{(1+\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}})^{t+1}}+\\frac{2\\eta(\\widehat{L}_{r o b}(\\mathbf{W};S)-L(\\mathbf{W}_{t};\\{\\widetilde{\\mathbf{x}}_{i}(\\mathbf{W}_{t})\\}_{i=1}^{n}))}{(1+\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}})^{t+1}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Taking the sum for $t=0,1,\\ldots,T$ we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle2\\eta\\sum_{t=0}^{T}\\frac{\\widehat{L}_{r o b}(\\mathbf{W};S)}{(1+\\frac{H C_{s}^{2}\\eta}{\\sqrt{m}})^{t+1}}+\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}+\\sum_{t=0}^{T}\\frac{\\eta^{2}C_{x}^{2}}{(1+\\frac{H C_{s}^{2}\\eta}{\\sqrt{m}})^{t+1}}}\\\\ {\\displaystyle\\sum_{t=0}^{T}\\frac{L}{t}\\big(\\mathbf{W}_{t};\\{\\widetilde{\\mathbf{u}}_{t}(\\mathbf{W}_{t})\\}_{t+1}^{n}\\big)}\\\\ {\\displaystyle\\sum_{t=0}^{T}\\frac{\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)-\\beta_{1}}{(1+\\frac{H C_{s}^{2}\\eta}{\\sqrt{m}})^{t+1}}}\\\\ {\\displaystyle\\sum_{t=0}^{T}\\frac{\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)-\\beta_{1}}{(1+\\frac{H C_{s}^{2}\\eta}{\\sqrt{m}})^{t+1}}}\\\\ {\\displaystyle\\sum_{t=0}^{T}\\frac{\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)}{(1+\\frac{H C_{s}^{2}\\eta}{\\sqrt{m}})^{t}}\\cdot\\sum_{t=0}^{T}\\frac{1}{(1+\\frac{H C_{s}^{2}\\eta}{\\sqrt{m}})^{t+1}}-2\\eta\\sum_{t=0}^{T}\\frac{\\beta_{1}}{(1+\\frac{H C_{s}^{2}\\eta}{\\sqrt{m}})^{t+1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Simplifying the above inequality, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq t\\leq T}\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)\\leq\\widehat{L}_{r o b}(\\mathbf{W};S)+\\frac{H C_{x}^{2}}{2\\sqrt{m}(1-\\frac{1}{(1+\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}})^{T+1}})}\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}+\\frac{C_{x}^{2}\\eta}{2}+\\beta_{1}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Theorem 3.2. Define $\\begin{array}{r}{\\alpha_{1}(\\eta,T):=\\mathcal{O}(C_{x}^{2}\\eta\\sqrt{T}+C_{x}^{2}\\frac{\\eta T}{n}+C_{x}\\sqrt{\\beta_{1}\\eta T})}\\end{array}$ .Assume that the width of the networks satisfies $m\\geq H^{2}C_{x}^{4}\\eta^{2}(T+1)^{2}$ , and $\\alpha_{1}(\\eta,T)<1$ . Then, after $T$ iterations of Algorithm 1 with GD, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{[\\frac{9T}{10}]\\leq t\\leq T}{\\operatorname*{min}}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\varepsilon_{g e n}(\\mathbf{W}_{t})\\leq\\frac{17\\alpha_{1}(\\eta,T)}{1-\\alpha_{1}(\\eta,T)}\\left[\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+\\frac{C_{x}^{2}\\eta}{2}+\\beta_{1}\\right],}\\\\ &{\\underset{0\\leq t\\leq T}{\\operatorname*{min}}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}L_{r o b}(\\mathbf{W}_{t})\\leq\\frac{1}{1-\\alpha_{1}(\\eta,T)}\\left[\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+\\frac{C_{x}^{2}\\eta}{2}+\\beta_{1}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 3.2. Define $\\begin{array}{r}{t_{0}:=[\\frac{9T}{10}]}\\end{array}$ and $\\begin{array}{r}{k:=\\frac{1}{1+\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}}}}\\end{array}$ From equation (9), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\eta\\displaystyle\\sum_{t=0}^{T}\\frac{\\widehat{L}_{r o b}\\big(\\mathbf{W};S\\big)}{\\big(1+\\frac{H C_{s}^{2}\\eta}{\\sqrt{m}}\\big)^{t+1}}+\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}+\\displaystyle\\sum_{t=0}^{T}\\frac{\\eta^{2}C_{x}^{2}}{(1+\\frac{H C_{s}^{2}\\eta}{\\sqrt{m}})^{t+1}}}\\\\ &{\\geq2\\eta\\displaystyle\\sum_{t=t_{0}}^{T}\\frac{L\\big(\\mathbf{W}_{t};\\{\\\\\\\\\\\\tilde{\\mathbf{x}}_{i}(\\mathbf{W}_{t})\\}_{i=1}^{n}\\big)}{\\big(1+\\frac{H C_{s}^{2}\\eta}{\\sqrt{m}}\\big)^{t+1}}}\\\\ &{\\geq2\\eta\\displaystyle\\sum_{t=t_{0}}^{T}\\frac{\\widehat{L}_{r o b}\\big(\\mathbf{W}_{t};S\\big)-\\beta_{1}}{\\big(1+\\frac{H C_{s}^{2}\\eta}{\\sqrt{m}}\\big)^{t+1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Taking minimum over all weight matrices $\\mathrm{W}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\displaystyle\\sum_{t=0}^{T}k^{t+1}\\left(\\hat{L}_{T\\delta}(\\mathbf{W}_{t};S)-\\beta_{1}\\right)}{\\displaystyle\\sum_{t=0}^{T}k^{t+1}\\hat{L}_{T\\delta}(\\mathbf{W}_{t};S)+\\frac{\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}}{2\\eta}+\\sum_{t=0}^{T}\\frac{\\eta\\hat{L}_{x}^{2}k^{t+1}}{2}\\right)}}\\\\ &{\\le\\displaystyle\\operatorname*{min}_{t=0}^{T}\\left(\\sum_{t=0}^{T}k^{t+1}\\hat{L}_{T\\delta}(\\mathbf{W}_{t};S)+\\frac{\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}}{2\\eta}\\right)}\\\\ &{=\\displaystyle\\sum_{t=0}^{T}k^{t+1}\\cdot\\operatorname*{min}_{\\mathbf{W}}\\left(\\hat{L}_{T\\delta}(\\mathbf{W}_{t};S)+\\frac{\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}}{2\\eta\\sum_{t=0}^{T}k^{t+1}}+\\frac{C_{2}^{2}\\eta}{2}\\right)}\\\\ &{\\le\\displaystyle\\sum_{t=0}^{T}k^{t+1}\\cdot\\operatorname*{min}_{\\mathbf{W}}\\left(\\hat{L}_{T\\delta}(\\mathbf{W}_{t};S)+\\frac{\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}}{\\eta(T+1)}+\\frac{C_{2}^{2}\\eta}{2}\\right)\\qquad(m\\geq H^{2}C_{x}^{4}\\eta^{2}(T+1)^{2})}\\\\ &{\\le\\displaystyle\\sum_{t=0}^{T}k^{t+1}\\cdot\\left(\\Delta_{x}^{\\alpha\\alpha\\mathbf{k}}+\\frac{C_{2}^{2}\\eta}{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Taking the expectation on both sides, we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\sum_{t=t_{0}}^{T}k^{t+1}\\left(\\operatorname*{min}_{t_{0}\\leq t\\leq T}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)-\\beta_{1}\\right)\\leq\\displaystyle\\sum_{t=t_{0}}^{T}k^{t+1}\\left(\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)-\\beta_{1}\\right)}\\\\ {\\displaystyle\\leq\\sum_{t=0}^{T}k^{t+1}\\cdot\\left(\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+\\frac{C_{x}^{2}\\eta}{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Simplifying the equation above, we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{t_{0}\\leq t\\leq T}{\\operatorname*{min}}\\mathbb{E}_{\\delta\\sim\\mathcal{P}^{n}}\\hat{L}_{r\\delta}{\\sim}\\hat{L}_{r\\delta}{\\sim}(\\mathbf{W}_{t};S)\\leq\\beta_{1}+\\frac{T}{\\underset{t_{0}\\leq t\\leq T}{\\sum}k^{t+1}}\\cdot\\left(\\mathbb{E}_{S\\sim\\mathcal{P}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+\\frac{C_{x}^{2}\\eta}{2}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\frac{\\gamma}{\\underset{t=0}{\\overset{\\mathrm{T}}{\\sum}k^{t+1}}}+\\frac{T^{\\frac{\\gamma+1}{n+1}}}{2}\\Bigg\\langle\\mathbb{E}_{S\\sim\\mathcal{P}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+\\frac{C_{x}^{2}\\eta}{2}\\Bigg\\rangle}\\\\ &{\\leq\\beta_{1}+\\left(\\displaystyle\\sum_{r=0}^{9}\\left(\\frac{1}{k}\\right)^{r^{\\frac{\\gamma+1}{n}}}\\right)\\cdot\\left(\\mathbb{E}_{S\\sim\\mathcal{P}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+\\frac{C_{x}^{2}\\eta}{2}\\right)}\\\\ &{\\leq\\beta_{1}+\\left(\\displaystyle\\sum_{r=0}^{9}e^{\\frac{\\gamma}{8}}\\right)\\cdot\\left(\\mathbb{E}_{S\\sim\\mathcal{P}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+\\frac{C_{x}^{2}\\eta}{2}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (m\\geq H^{2}C_{x}^{4}\\eta^{2}(T+1)^{2})}\\\\ &{\\leq\\beta_{1}+17\\left(\\mathbb{E}_{S\\sim\\mathcal{P}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+\\frac{C_{x}^{2}\\eta}{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Equation (4) gives us forany t\u2264T,Es\\~DEgen(Wt) 2) . Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{t_{0}\\leq t\\leq T}{\\operatorname*{min}}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\varepsilon_{g e n}(\\mathbf{W}_{t})\\leq\\frac{\\alpha_{1}(\\eta,T)}{1-\\alpha_{1}(\\eta,T)}\\underset{t_{0}\\leq t\\leq T}{\\operatorname*{min}}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\alpha_{1}(\\eta,T)}{1-\\alpha_{1}(\\eta,T)}\\left(\\beta_{1}+17\\left(\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+\\frac{C_{x}^{2}\\eta}{2}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The proof of the second statement takes a similar approach. Following the same procedure, we can replace $t_{0}$ by 0 in equation (10), and get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq t\\leq T}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\widehat{L}_{r o b}(\\mathbb{W}_{t};S)\\leq\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+\\frac{C_{x}^{2}\\eta}{2}+\\beta_{1}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining with equation (4), ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{0\\leq t\\leq T}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}L_{r o b}(\\mathbf{W}_{t})\\leq\\frac{1}{1-\\alpha_{1}(\\eta,T)}\\operatorname*{min}_{0\\leq t\\leq T}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{1}{1-\\alpha_{1}(\\eta,T)}\\left(\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+\\frac{C_{x}^{2}\\eta}{2}+\\beta_{1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Corollary 3.3. After $\\begin{array}{r}{T\\le\\mathcal{O}(\\operatorname*{min}\\{n^{2},\\frac{1}{\\beta_{1}^{2}}\\})}\\end{array}$ iterations of Algorithm 1 with GD using a step size of $\\begin{array}{r}{\\eta=\\Theta(\\frac{1}{C_{x}^{2}\\sqrt{T}})}\\end{array}$ on networ wih itht $m\\geq\\Omega(T)$ for any weigh marix w ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq t\\leq T}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}L_{r o b}(\\mathbf{W}_{t})\\leq1.1L_{r o b}(\\mathbf{W})+\\mathcal{O}\\left(\\frac{C_{x}^{2}\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}}{\\sqrt{T}}\\right)+\\mathcal{O}\\left(\\frac{1}{\\sqrt{T}}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof of Corollary 3.3. Under the conditions of the corollary, we have $m\\geq H^{2}C_{x}^{4}\\eta^{2}(T+1)^{2}$ , and $\\begin{array}{r}{\\alpha_{1}(\\eta,T)=\\mathcal{O}(C_{x}^{2}\\eta\\sqrt{T}+C_{x}^{2}\\frac{\\eta T}{n}+C_{x}\\sqrt{\\beta_{1}\\eta T})}\\end{array}$ $\\begin{array}{r}{\\frac{1}{1-\\alpha_{1}(\\eta,T)}\\le1.1}\\end{array}$ Thel it is clear that this corollary is a special case of Theorem 3.2. ", "page_idx": 18}, {"type": "text", "text": "We now extend the previous ideas to stochastic adversarial training. ", "page_idx": 18}, {"type": "text", "text": "Lemma B.3. After $T$ iterations of Algorithm 1 with SGD, for any weight matrix W. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq t\\leq T}\\big(\\ell_{r o b}(\\mathbf{z}_{t+1},\\mathbf{W}_{t})-\\ell_{r o b}(\\mathbf{z}_{t+1},\\mathbf{W})\\big)\\leq\\frac{H C_{x}^{2}}{2\\sqrt{m}\\big(1-\\frac{1}{(1+\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}})^{T+1}}\\big)}\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}+\\frac{C_{x}^{2}\\eta}{2}+\\beta_{1}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma B.3. The proof proceeds similarly as Theorem B.2. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{W}-\\mathbf{W}_{t+1}\\|_{F}^{2}}\\\\ &{=\\|\\mathbf{W}-\\mathbf{W}_{t}+\\eta\\nabla_{\\mathbf{W}}\\ell((\\tilde{\\mathbf{x}}_{t+1}(\\mathbf{W}_{t}),y_{t+1}),\\mathbf{W}_{t})\\|_{F}^{2}}\\\\ &{=\\|\\mathbf{W}-\\mathbf{W}_{t}\\|_{F}^{2}+\\eta^{2}\\|\\nabla_{\\mathbf{W}}\\ell((\\tilde{\\mathbf{x}}_{t+1}(\\mathbf{W}_{t}),y_{t+1}),\\mathbf{W}_{t})\\|_{F}^{2}+2\\eta\\left\\langle\\nabla_{\\mathbf{W}}\\ell((\\tilde{\\mathbf{x}}_{t+1}(\\mathbf{W}_{t}),y_{t+1}),\\mathbf{W}_{t}),\\mathbf{W}-\\mathbf{W}_{t}\\right\\rangle}\\\\ &{\\leq\\|\\mathbf{W}-\\mathbf{W}_{t}\\|_{F}^{2}+\\eta^{2}C_{x}^{2}+\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}}\\|\\mathbf{W}-\\mathbf{W}_{t}\\|_{F}^{2}+2\\eta\\ell_{r o b}(\\mathbf{z}_{t+1},\\mathbf{W})-2\\eta\\ell((\\tilde{\\mathbf{x}}_{t+1}(\\mathbf{W}_{t}),y_{t+1}),\\mathbf{W}_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "(Lemma A.5 and Lemma A.4) ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\leq(1+\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}})\\|\\mathbf{W}-\\mathbf{W}_{t}\\|_{F}^{2}+\\eta^{2}C_{x}^{2}+2\\eta\\ell_{r o b}(z_{t+1},\\mathbf{W})-2\\eta\\ell_{r o b}(\\mathbf{z}_{t+1},\\mathbf{W}_{t})+2\\eta\\beta_{1}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Dividing both sides by $\\begin{array}{r}{(1+\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}})^{t+1}}\\end{array}$ )t+1, we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\|\\mathbf{W}-\\mathbf{W}_{t+1}\\|_{F}^{2}}{(1+\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}})^{t+1}}\\leq\\frac{\\|\\mathbf{W}-\\mathbf{W}_{t}\\|_{F}^{2}}{(1+\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}})^{t}}+\\frac{\\eta^{2}C_{x}^{2}+2\\eta\\ell_{r o b}(\\mathbf{z}_{t+1},\\mathbf{W})-2\\eta\\ell_{r o b}(\\mathbf{z}_{t+1},\\mathbf{W}_{t})+2\\eta\\beta_{1}}{(1+\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}})^{t+1}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Taking the sum of the above equation for $t=0,1,\\ldots,T$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}+\\displaystyle\\sum_{t=0}^{T}\\frac{\\eta^{2}C_{x}^{2}+2\\eta\\beta_{1}}{(1+\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}})^{t+1}}}\\\\ &{\\geq\\!2\\eta\\displaystyle\\sum_{t=0}^{T}\\!\\frac{\\ell_{r o b}\\!\\left(\\!z_{t+1},\\mathbf{W}_{t}\\right)-\\ell_{r o b}\\!\\left(\\!z_{t+1},\\mathbf{W}\\right)}{(1+\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}})^{t+1}}}\\\\ &{\\geq\\!2\\eta\\displaystyle\\sum_{t=0}^{T}\\!\\frac{\\operatorname*{min}_{0\\leq t\\leq T}\\left(\\ell_{r o b}\\!\\left(z_{t+1},\\mathbf{W}_{t}\\right)-\\ell_{r o b}\\!\\left(z_{t+1},\\mathbf{W}\\right)\\right)}{(1+\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}})^{t+1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Simplifying the above inequality we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq t\\leq T}\\big(\\ell_{r o b}(\\mathbf{z}_{t+1},\\mathbf{W}_{t})-\\ell_{r o b}(\\mathbf{z}_{t+1},\\mathbf{W})\\big)\\leq\\frac{H C_{x}^{2}}{2\\sqrt{m}\\big(1-\\frac{1}{(1+\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}})^{T+1}}\\big)}\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}+\\frac{C_{x}^{2}\\eta}{2}+\\beta_{1}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Theorem 3.4. After $T$ iterations of Algorithm 1 with SGD on a network of width $m\\geq H^{2}C_{x}^{4}\\eta^{2}(T+$ $1)^{2}$ we have that for any weight matrix $\\mathrm{W}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq t\\leq T}\\mathbb{E}_{\\{\\tau_{1},\\ldots,z_{t}\\}\\sim\\mathcal{D}^{t}}L_{r o b}(\\mathbf{W}_{t})\\leq L_{r o b}(\\mathbf{W})+\\frac{\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}}{\\eta(T+1)}+\\frac{C_{x}^{2}\\eta}{2}+\\beta_{1}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof of Theorem 3.4. Taking the expectation over $S\\,\\sim\\,{\\mathcal{D}}^{n}$ on both sides of Equation (11), we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}+\\displaystyle\\sum_{t=0}^{T}\\frac{\\eta^{2}C_{x}^{2}+2\\eta\\beta}{(1+\\frac{H C_{2}^{2}\\eta}{\\sqrt{m}})^{t+1}}}\\\\ &{\\geq\\!2\\eta\\displaystyle\\sum_{t=0}^{T}\\frac{\\mathbb{E}_{\\{z_{1},\\dots,z_{t}\\}\\sim\\mathcal{D}^{t}}\\mathbb{E}_{z_{t+1}\\sim\\mathcal{D}}\\ell_{\\tau\\delta}\\big(z_{t+1},\\mathbf{W}_{t}\\big)-\\mathbb{E}_{z_{t+1}\\sim\\mathcal{D}}\\ell_{\\tau\\delta}\\big(z_{t+1},\\mathbf{W}\\big)}{(1+\\frac{H C_{2}^{2}\\eta}{\\sqrt{m}})^{t+1}}}\\\\ &{=\\!2\\eta\\displaystyle\\sum_{t=0}^{T}\\frac{\\mathbb{E}_{\\left\\{z_{1},\\dots,z_{t}\\right\\}\\sim\\mathcal{D}^{t}}L_{\\tau\\delta}\\big(\\mathbf{W}_{t}\\big)}{(1+\\frac{H C_{2}^{2}\\eta}{\\sqrt{m}})^{t+1}}-L_{\\tau\\delta}\\!\\left(\\mathbf{W}\\right)}\\\\ &{\\geq\\!2\\eta\\displaystyle\\sum_{t=0}^{T}\\!\\frac{\\operatorname*{min}_{\\tau}\\mathbb{E}_{\\{z_{1},\\dots,z_{t}\\}\\sim\\mathcal{D}^{t}}L_{\\tau\\delta}\\big(\\mathbf{W}_{t}\\big)-L_{\\tau\\delta}\\delta(\\mathbf{W})}{(1+\\frac{H C_{2}^{2}\\eta}{\\sqrt{m}})^{t+1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Simplifying the above inequality, we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{min}_{\\leq t\\leq T}\\mathbb{E}_{\\{\\tau_{1},\\ldots,\\tau_{t}\\}\\sim\\mathcal{D}^{t}}L_{r o b}(\\mathbf{W}_{t})\\leq L_{r o b}(\\mathbf{W})+\\frac{H C_{x}^{2}}{2\\sqrt{m}\\big(1-\\frac{1}{(1+\\frac{H C_{x}^{2}\\eta}{\\sqrt{m}})^{T+1}}\\big)}\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}+\\frac{C_{x}^{2}\\eta}{2}+\\beta_{1}}}\\\\ &{}&{\\leq L_{r o b}(\\mathbf{W})+\\frac{\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}}{\\eta(T+1)}+\\frac{C_{x}^{2}\\eta}{2}+\\beta_{1}.}\\\\ &{}&{(m\\geq H^{2}C_{x}^{4}\\eta^{2}(T+1)^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "C Missing Proofs in Section 3.2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "From Lemma A.4, for ny $\\begin{array}{r}{\\mu<\\frac{\\sqrt{m}}{H C_{x}^{2}}}\\end{array}$ \uff0c $\\begin{array}{r}{L_{r o b}(\\mathbf{U})+\\frac{1}{2\\mu}\\|\\mathbf{U}-\\mathbf{W}\\|_{F}^{2}}\\end{array}$ istongy convex in U Recalthat the Moreau envelope is defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\nM^{\\mu}(\\mathbf{W};S)=\\operatorname*{min}_{\\mathbf{U}}\\left(\\widehat{L}_{r o b}(\\mathbf{U};S)+\\frac{1}{2\\mu}\\|\\mathbf{U}-\\mathbf{W}\\|_{F}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The minimizer of the optimization problem above is denoted as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{U}^{\\mu}(\\mathbf{W};S)=\\underset{\\mathbf{U}}{\\mathrm{argmin}}\\left(\\widehat{L}_{r o b}(\\mathbf{U};S)+\\frac{1}{2\\mu}\\|\\mathbf{U}-\\mathbf{W}\\|_{F}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We borrow a few properties of the Moreau envelope from Xiao et al. [2024]. ", "page_idx": 20}, {"type": "text", "text": "Lemma C.1. For any $\\begin{array}{r}{\\mu<\\frac{\\sqrt{m}}{H C_{x}^{2}}}\\end{array}$ 1. $\\operatorname*{min}_{\\mathrm{w}}M^{\\mu}(\\mathbf{W};S)$ has the same global solution set as $\\operatorname*{min}_{\\mathbf{W}}\\widehat{L}_{r o b}(\\mathbf{W};S)$ 2. The gradient of $M^{\\mu}(\\mathbf{W};S)$ is $\\begin{array}{r}{\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W};S)=\\frac{1}{\\mu}\\left(\\mathbf{W}-\\mathbf{U}^{\\mu}(\\mathbf{W};S)\\right)\\!.}\\end{array}$ 3. $\\begin{array}{r}{M^{\\mu}(\\mathbf{W};S)+\\frac{\\|\\mathbf{W}\\|_{F}^{2}}{2\\left(\\frac{\\sqrt{m}}{H C_{x}^{2}}-\\mu\\right)}}\\end{array}$ is convex. 4. $\\mathrm{U}^{\\mu}(\\mathbf{W};S)$ is $\\frac{\\frac{\\sqrt{m}}{H C_{x}^{2}}}{\\frac{\\sqrt{m}}{H C_{x}^{2}}-\\mu}$ -Lipschitz in W w.r.t the Frobenius norm. 5. $M^{\\mu}(\\mathbf{W};S)$ $\\begin{array}{r}{\\mathrm{W};S)\\;\\mathrm{is}\\;\\mathrm{max}\\left\\{\\frac{1}{\\mu},\\frac{1}{\\frac{\\sqrt{m}}{H C_{x}^{2}}-\\mu}\\right\\}.}\\end{array}$ smooth. 6. $\\begin{array}{r}{\\widehat{L}_{r o b}(\\mathbf{W};S)-\\frac{C_{x}^{2}}{2\\left(\\frac{1}{\\mu}-\\frac{H C_{x}^{2}}{\\sqrt{m}}\\right)}\\leq M^{\\mu}(\\mathbf{W};S)\\leq\\widehat{L}_{r o b}(\\mathbf{W};S).}\\end{array}$ 7. $\\|\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W};S)\\|_{F}\\leq C_{x}.$ ", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma C.1. The first 5 statements are covered in the proof of [Xiao et al., 2024, Lemma A.1]. For the statement 6, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\widehat{L}_{r o t}(\\mathbf{W};S)=\\widehat{L}_{r o t}(\\mathbf{W};S)+\\frac{1}{2\\mu}\\|\\mathbf{W}-\\mathbf{W}\\|_{F}^{2}}\\\\ {\\geq\\operatorname*{min}\\left(\\widehat{L}_{r o t}(\\mathbf{U};S)+(1,0)\\frac{1}{2\\mu}\\|\\mathbf{U}-\\mathbf{W}\\|_{F}^{2}\\right)}\\\\ {=M^{r}(\\mathbf{W};S)}\\\\ {=\\widehat{L}_{r o t}(\\mathbf{W};S)+\\frac{\\mathfrak{C}_{r o t}}{10}\\left(\\widehat{L}_{r o t}(\\mathbf{U};S)-\\widehat{L}_{r o t}(\\mathbf{W};S)+\\frac{1}{2\\mu}\\|\\mathbf{U}-\\mathbf{W}\\|_{F}^{2}\\right)}\\\\ {\\geq\\widehat{L}_{r o t}(\\mathbf{W};S)+\\frac{\\mathfrak{C}_{r o t}}{10}\\left(\\left\\langle\\nabla_{\\mathbf{v}}\\widehat{L}_{r o t}(\\mathbf{W};S),\\mathbf{U}-\\mathbf{W}\\right\\rangle-\\frac{\\widehat{L}_{r o t}^{2}}{2\\mu}\\|\\mathbf{U}-\\mathbf{W}\\|_{F}^{2}+\\frac{1}{2\\mu}\\|\\mathbf{U}-\\mathbf{W}\\|_{F}^{2}\\right.}\\\\ {\\geq\\widehat{L}_{r o t}(\\mathbf{W};S)+\\frac{1}{10}\\mathfrak{w}\\left(\\left\\langle\\nabla_{\\mathbf{v}}\\mathbf{W};S\\right\\rangle,\\mathbf{U}-\\mathfrak{C}_{r o t}\\mathrm{wav}\\mathrm{rson}\\mathrm{~for~}\\mathbf{D}\\mathbf{u}\\right)\\mathrm{~for~}\\mathbf{D}\\mathbf{u}<\\mathbf{foma~A}.}\\\\ {\\geq\\widehat{L}_{r o t}(\\mathbf{W};S)+\\frac{\\mathfrak{C}_{r o t}}{10}\\left(-C_{s}\\|\\mathbf{U}-\\mathbf{W}\\|_{F}-\\frac{H C_{s}^{2}}{2\\sqrt{m}}\\|\\mathbf{U}-\\mathbf{W}\\|_{F}^{2}+\\frac{1}{2\\mu}\\|\\mathbf{U}-\\mathbf{W}\\|_{F}^{2}\\right)}\\\\ {=\\widehat{L}_{r o t}(\\mathbf{W};S)-\\frac{C_{s}^{2}}{2\\left(\\frac{1}{2\\mu}+\\frac{1}{2\\sqrt{m}}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now we prove statement 7. For any $\\gamma\\in(0,1)$ , from the definition of the Moreau envelope, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\widehat{L}_{r o b}({\\bf U}^{\\mu}({\\bf W};S);S)+\\frac{1}{2\\mu}\\|{\\bf U}^{\\mu}({\\bf W};S)-{\\bf W}\\|_{F}^{2}}\\ ~}\\\\ {{\\displaystyle\\leq\\widehat{L}_{r o b}((1-\\gamma){\\bf W}+\\gamma{\\bf U}^{\\mu}({\\bf W};S);S)+\\frac{1}{2\\mu}\\|(1-\\gamma){\\bf W}+\\gamma{\\bf U}^{\\mu}({\\bf W};S)-{\\bf W}\\|_{F}^{2}}\\ ~}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$(\\mathbf{}\\mathbf{}\\mathrm{U}^{\\mu}(\\mathbf{}\\mathbf{W};S)$ obtains theminimum) ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\leq\\widehat{L}_{r o b}(\\mathbf{U}^{\\mu}(\\mathbf{W};S);S)+C_{x}(1-\\gamma)\\|\\mathbf{U}^{\\mu}(\\mathbf{W};S)-\\mathbf{W}\\|_{F}+\\frac{\\gamma^{2}}{2\\mu}\\|\\mathbf{U}^{\\mu}(\\mathbf{W};S)-\\mathbf{W}\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "(The robust loss is $C_{x}$ -Lip from Lemma A.2) ", "page_idx": 20}, {"type": "text", "text": "Simplifying the inequality above, we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\mathbf{U}^{\\mu}(\\mathbf{W};S)-\\mathbf{W}\\|_{F}\\leq\\frac{2\\mu C_{x}}{1+\\gamma}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $\\gamma\\rightarrow1$ , we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W};S)\\|_{F}=\\frac{1}{\\mu}\\|\\mathbf{U}^{\\mu}(\\mathbf{W};S)-\\mathbf{W}\\|_{F}\\leq C_{x}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next we show a result similar as [Xiao et al., 2024, Lemma A.2]. They use the first-order optimal condition to prove the result, which only holds for the smooth loss functions. Here we give a different proof that doesn't depend on the subgradient, so it can be applied to the robust loss. ", "page_idx": 21}, {"type": "text", "text": "Lemma C.2. Let $\\tilde{\\mathbf{U}}^{\\mu}(\\mathbf{W};S)$ be any $\\beta_{2}$ -optimal minimizer of $\\begin{array}{r}{\\operatorname*{min}_{\\mathbf{U}}\\Big(\\widehat{L}_{r o b}(\\mathbf{U};S)+\\frac{1}{2\\mu}\\|\\mathbf{U}-\\mathbf{W}\\|_{F}^{2}\\Big).}\\end{array}$ For two data sets $S$ and $S^{(i)}$ that differ in only one example and any weight matrix $\\mathrm{W}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\mathbf{U}^{\\mu}(\\mathbf{W};S)-\\mathbf{U}^{\\mu}(\\mathbf{W};S^{(i)})\\|_{F}\\leq\\frac{2C_{x}}{n\\left(\\frac{1}{\\mu}-\\frac{H C_{x}^{2}}{\\sqrt{m}}\\right)}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Vert\\tilde{\\mathbf{U}}^{\\mu}(\\mathbf{W};S)-\\mathbf{U}^{\\mu}(\\mathbf{W};S)\\Vert_{F}\\leq\\sqrt{\\frac{2\\beta_{2}}{\\frac{1}{\\mu}-\\frac{H C_{x}^{2}}{\\sqrt{m}}}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof of Lemma C.2. From strong convexity of the regularized robust loss, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\widehat{L}_{r o b}\\big(\\mathbf{U}^{\\mu}\\big(\\mathbf{W};S^{(i)}\\big);S\\big)+\\displaystyle\\frac{1}{2\\mu}\\|\\mathbf{U}^{\\mu}\\big(\\mathbf{W};S^{(i)}\\big)-\\mathbf{W}\\|_{F}^{2}}\\\\ &{\\displaystyle\\geq\\widehat{L}_{r o b}\\big(\\mathbf{U}^{\\mu}\\big(\\mathbf{W};S\\big);S\\big)+\\displaystyle\\frac{1}{2\\mu}\\|\\mathbf{U}^{\\mu}\\big(\\mathbf{W};S\\big)-\\mathbf{W}\\|_{F}^{2}+\\big(\\displaystyle\\frac{1}{2\\mu}-\\displaystyle\\frac{H C_{x}^{2}}{2\\sqrt{m}}\\big)\\|\\mathbf{U}^{\\mu}\\big(\\mathbf{W};S^{(i)}\\big)-\\mathbf{U}^{\\mu}\\big(\\mathbf{W};S\\big)\\|_{F}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and similarly, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\widehat{L}_{r o b}({\\bf U}^{\\mu}({\\bf W};S);S^{(i)})+\\frac{1}{2\\mu}\\|{\\bf U}^{\\mu}({\\bf W};S)-{\\bf W}\\|_{F}^{2}}\\ ~}\\\\ {{\\displaystyle\\geq\\widehat{L}_{r o b}({\\bf U}^{\\mu}({\\bf W};S^{(i)});S^{(i)})+\\frac{1}{2\\mu}\\|{\\bf U}^{\\mu}({\\bf W};S^{(i)})-{\\bf W}\\|_{F}^{2}+(\\frac{1}{2\\mu}-\\frac{H C_{x}^{2}}{2\\sqrt{m}})\\|{\\bf U}^{\\mu}({\\bf W};S)-{\\bf U}^{\\mu}({\\bf W};S^{(i)})\\|_{F}^{2}}\\ ~}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Adding these two inequalities, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(\\frac{1}{\\mu}-\\frac{H C_{x}^{2}}{{\\sqrt{m}}})\\|\\mathbf{U}^{\\mu}(\\mathbf{W};S)-\\mathbf{U}^{\\mu}(\\mathbf{W};S^{(i)})\\|_{F}^{2}}\\\\ &{\\le\\!\\Big[\\widehat{L}_{r o b}(\\mathbf{U}^{\\mu}(\\mathbf{W};S^{(i)});S)\\!-\\!\\widehat{L}_{r o b}\\big(\\mathbf{U}^{\\mu}(\\mathbf{W};S^{(i)});S^{(i)}\\big)\\!\\Big]\\!+\\!\\Big[\\widehat{L}_{r o b}\\big(\\mathbf{U}^{\\mu}(\\mathbf{W};S);S^{(i)}\\big)\\!-\\!\\widehat{L}_{r o b}\\big(\\mathbf{U}^{\\mu}(\\mathbf{W};S);S\\big)\\!\\Big]}\\\\ &{=\\!\\frac{1}{n}\\Big[\\ell_{r o b}\\big(z_{i},\\mathbf{U}^{\\mu}(\\mathbf{W};S^{(i)})\\big)\\!-\\!\\ell_{r o b}\\big(z_{i}^{\\prime},\\mathbf{U}^{\\mu}(\\mathbf{W};S^{(i)})\\big)\\!\\Big]\\!+\\!\\frac{1}{n}\\big[\\ell_{r o b}\\big(z_{i}^{\\prime},\\mathbf{U}^{\\mu}(\\mathbf{W};S)\\big)\\!-\\!\\ell_{r o b}\\big(z_{i},\\mathbf{U}^{\\mu}(\\mathbf{W};S)\\big)\\big]\\!}\\\\ &{=\\!\\frac{1}{n}\\Big[\\ell_{r o b}\\big(z_{i},\\mathbf{U}^{\\mu}(\\mathbf{W};S^{(i)})\\big)\\!-\\!\\ell_{r o b}\\big(z_{i},\\mathbf{U}^{\\mu}(\\mathbf{W};S)\\big)\\!\\Big]\\!+\\!\\frac{1}{n}\\Big[\\ell_{r o b}\\big(z_{i}^{\\prime},\\mathbf{U}^{\\mu}(\\mathbf{W};S)\\big)\\!-\\!\\ell_{r o b}\\big(z_{i}^{\\prime},\\mathbf{U}^{\\mu}(\\mathbf{W};S^{(i)})\\big)\\!\\Big]}\\\\ &{\\le\\!\\frac{2C_{x}}{n}\\|\\mathbf{U}^{\\mu}(\\mathbf{W};S)\\!-\\!\\mathbf{U}^{\\mu}(\\mathbf{W};S^{(i)})\\|_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\mathbf{U}^{\\mu}(\\mathbf{W};S)-\\mathbf{U}^{\\mu}(\\mathbf{W};S^{(i)})\\|_{F}\\leq\\frac{2C_{x}}{n\\left(\\frac{1}{\\mu}-\\frac{H C_{x}^{2}}{\\sqrt{m}}\\right)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From the strong convexity of the regularized robust loss, we also have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{2}\\geq\\bigg(\\widehat{L}_{r o b}(\\tilde{\\mathbf{U}}^{\\mu}(\\mathbf{W};S);S)+\\displaystyle\\frac{1}{2\\mu}\\|\\tilde{\\mathbf{U}}^{\\mu}(\\mathbf{W};S)-\\mathbf{W}\\|_{F}^{2}\\bigg)}\\\\ &{\\qquad\\quad-\\left(\\widehat{L}_{r o b}(\\mathbf{U}^{\\mu}(\\mathbf{W};S);S)+\\displaystyle\\frac{1}{2\\mu}\\|\\mathbf{U}^{\\mu}(\\mathbf{W};S)-\\mathbf{W}\\|_{F}^{2}\\right)}\\\\ &{\\geq(\\displaystyle\\frac{1}{2\\mu}-\\frac{H C_{x}^{2}}{2\\sqrt{m}})\\|\\tilde{\\mathbf{U}}^{\\mu}(\\mathbf{W};S)-\\mathbf{U}^{\\mu}(\\mathbf{W};S)\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We get $\\begin{array}{r}{\\|\\tilde{\\mathbf{U}}^{\\mu}(\\mathbf{W};S)-\\mathbf{U}^{\\mu}(\\mathbf{W};S)\\|_{F}\\leq\\sqrt{\\frac{\\beta_{2}}{\\frac{1}{2\\mu}-\\frac{H C_{x}^{2}}{2\\sqrt{m}}}}.}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "Now we show an upper bound that is key to bounding the stability of the weight matrix. ", "page_idx": 22}, {"type": "text", "text": "Lemma C.3. For any n \u2264 \u03bc\u2264 $\\begin{array}{r}{\\eta\\le\\mu\\le\\frac{\\sqrt{m}}{2H C_{x}^{2}}}\\end{array}$ and any weight matrices $\\mathrm{W_{1}}$ and $\\mathrm{W_{2}}$ \uff0c ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\mathbf{W}^{1}-\\eta\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{1};S)-\\mathbf{W}^{2}+\\eta\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{2};S)\\|_{F}^{2}\\leq\\frac{\\|\\mathbf{W}^{1}-\\mathbf{W}^{2}\\|_{F}^{2}}{1-\\frac{4H C_{x}^{2}\\eta}{\\sqrt{m}}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma C.3. Define $\\psi_{1}(\\mathbf{W})\\,=\\,M^{\\mu}(\\mathbf{W};S)\\,-\\,M^{\\mu}(\\mathbf{W}^{1};S)\\,-\\,\\left\\langle\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{1};S),\\mathbf{W}\\,-\\,\\mathbf{W}^{1}\\right\\rangle\\!.$ From Lemma C.1, $\\psi_{1}(\\mathbf{W})$ is $\\frac{1}{\\mu}$ -smooth. From the standard descent lemma of smooth functions, for any $\\eta\\le\\mu$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi_{1}\\big(\\mathbf{W}^{2}-\\eta\\nabla_{\\mathbf{W}}\\psi_{1}(\\mathbf{W}^{2})\\big)\\leq\\psi_{1}\\big(\\mathbf{W}^{2}\\big)-\\displaystyle\\frac{\\eta}{2}\\|\\nabla_{\\mathbf{W}}\\psi_{1}(\\mathbf{W}^{2})\\|_{F}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\psi_{1}\\big(\\mathbf{W}^{2}\\big)-\\displaystyle\\frac{\\eta}{2}\\|\\nabla_{\\mathbf{W}}M^{\\mu}\\big(\\mathbf{W}^{2};S\\big)-\\nabla_{\\mathbf{W}}M^{\\mu}\\big(\\mathbf{W}^{1};S\\big)\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $\\begin{array}{r}{\\psi_{1}(\\mathbf{W})+\\frac{\\|\\mathbf{W}\\|_{F}^{2}}{2\\left(\\frac{\\sqrt{m}}{H C_{x}^{2}}-\\mu\\right)}}\\end{array}$ is convex from Lemma C.1, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\psi_{1}\\big(\\mathbf{W}^{2}-\\eta\\nabla_{\\mathbf{W}}\\psi_{1}(\\mathbf{W}^{2})\\big)}\\\\ &{\\geq\\!\\psi_{1}\\big(\\mathbf{W}^{1}\\big)+\\big\\langle\\nabla\\psi_{1}\\big(\\mathbf{W}^{1}\\big),\\mathbf{W}^{2}-\\eta\\nabla_{\\mathbf{W}}\\psi_{1}\\big(\\mathbf{W}^{2}\\big)-\\mathbf{W}^{1}\\big\\rangle-\\frac{\\|\\mathbf{W}^{2}-\\eta\\nabla_{\\mathbf{W}}\\psi_{1}\\big(\\mathbf{W}^{2}\\big)-\\mathbf{W}^{1}\\|_{F}^{2}}{2\\left(\\frac{\\sqrt{m}}{H C_{x}^{2}}-\\mu\\right)}}\\\\ &{=\\!\\psi_{1}\\big(\\mathbf{W}^{1}\\big)-\\frac{\\|\\mathbf{W}^{2}-\\eta\\nabla_{\\mathbf{W}}\\psi_{1}\\big(\\mathbf{W}^{2}\\big)-\\mathbf{W}^{1}\\|_{F}^{2}}{2\\left(\\frac{\\sqrt{m}}{H C_{x}^{2}}-\\mu\\right)}}\\\\ &{\\geq\\!\\psi_{1}\\big(\\mathbf{W}^{1}\\big)-\\frac{\\|\\mathbf{W}^{2}-\\eta\\nabla_{\\mathbf{W}}M^{\\mu}\\big(\\mathbf{W}^{2};S\\big)-\\mathbf{W}^{1}+\\eta\\nabla_{\\mathbf{W}}M^{\\mu}\\big(\\mathbf{W}^{1};S\\big)\\|_{F}^{2}}{\\frac{\\sqrt{m}}{H C_{x}^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining the two inequalities above, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad M^{\\mu}(\\mathbf{W}^{2};S)-M^{\\mu}(\\mathbf{W}^{1};S)-\\left\\langle\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{1};S),\\mathbf{W}^{2}-\\mathbf{W}^{1}\\right\\rangle}\\\\ &{=\\!\\psi_{1}(\\mathbf{W}^{2})-\\psi_{1}(\\mathbf{W}^{1})}\\\\ &{\\geq\\!\\frac{\\eta}{2}\\|\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{2};S)\\!-\\!\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{1};S)\\|_{F}^{2}\\!-\\!\\frac{\\|\\mathbf{W}^{2}-\\eta\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{2};S)-\\mathbf{W}^{1}+\\eta\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{1};S)\\|_{F}^{2}}{\\frac{\\sqrt{m}}{H C_{r}^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Similarly, we can get the counterpart of this equation: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad M^{\\mu}(\\mathbf{W}^{1};S)-M^{\\mu}(\\mathbf{W}^{2};S)-\\left\\langle\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{2};S),\\mathbf{W}^{1}-\\mathbf{W}^{2}\\right\\rangle}\\\\ &{\\geq\\!\\frac{\\eta}{2}\\|\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{1};S)\\!-\\!\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{2};S)\\|_{F}^{2}\\!-\\!\\frac{\\|\\mathbf{W}^{1}-\\eta\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{1};S)-\\mathbf{W}^{2}+\\eta\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{2};S)\\|_{F}^{2}}{\\frac{\\sqrt{m}}{H C_{x}^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Adding these two inequalities, we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{2};S)-\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{1};S),\\mathbf{W}^{2}-\\mathbf{W}^{1}\\rangle}\\\\ &{\\geq\\eta\\|\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{1};S)-\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{2};S)\\|_{F}^{2}-\\frac{2\\|\\mathbf{W}^{1}-\\eta\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{1};S)-\\mathbf{W}^{2}+\\eta\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{2};S)\\|_{F}^{2}}{\\frac{\\sqrt{m}}{H C_{x}^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\mathbf{W}^{1}-\\eta\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{1};S)-\\mathbf{W}^{2}+\\eta\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{2};S)\\|_{F}^{2}}\\\\ &{=\\|\\mathbf{W}^{1}-\\mathbf{W}^{2}\\|_{F}^{2}+\\eta^{2}\\|\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{1};S)-\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{2};S)\\|_{F}^{2}}\\\\ &{\\qquad\\quad-2\\eta\\left\\langle\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{1};S)-\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{2};S),\\mathbf{W}^{1}-\\mathbf{W}^{2}\\right\\rangle}\\\\ &{\\leq\\|\\mathbf{W}^{1}-\\mathbf{W}^{2}\\|_{F}^{2}+\\eta^{2}\\|\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{1};S)-\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{2};S)\\|_{F}^{2}}\\\\ &{\\qquad\\quad-2\\eta^{2}\\|\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{1};S)-\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{2};S)\\|_{F}^{2}}\\\\ &{\\qquad\\quad+\\frac{4\\eta\\|\\mathbf{W}^{1}-\\eta\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{1};S)-\\mathbf{W}^{2}+\\eta\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{2};S)\\|_{F}^{2}}{\\frac{\\sqrt{m}}{H G_{2}^{2}}}}\\\\ &{\\qquad\\quad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\|\\mathbf{W}^{1}-\\mathbf{W}^{2}+\\eta\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{2};S)\\|_{F}^{2}}\\\\ &{\\leq\\|\\mathbf{W}^{1}-\\mathbf{W}^{2}\\|_{F}^{2}+\\frac{4H C_{x}^{2}\\eta\\|\\mathbf{W}^{1}-\\eta\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{1};S)-\\mathbf{W}^{2}+\\eta\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}^{2};\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We get the desired result by simplifying the above inequality. ", "page_idx": 23}, {"type": "text", "text": "Theorem C. Restemen of Theorem 3.5)Forany $\\begin{array}{r}{\\eta\\le\\operatorname*{min}\\{\\mu,\\frac{\\sqrt{m}}{8H C_{x}^{2}}\\}\\le\\frac{\\sqrt{m}}{2H C_{x}^{2}}}\\end{array}$ $\\mathrm{W}_{T}$ and $\\mathbf{W}_{T}^{\\left(i\\right)}$ be the weight matrices returned after running lgorithm 1 with Moreau Envelope using $S$ and $S^{(i)}$ respectively for $T$ iterations. Here $S$ and $S^{(i)}$ Only differ in the $i$ -th data.Wehave ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\mathbf{W}_{T}-\\mathbf{W}_{T}^{(i)}\\|_{F}^{2}\\leq e^{1+\\frac{8H C_{x}^{2}\\eta T}{\\sqrt{m}}}(T+1)^{2}\\left(\\frac{4C_{x}\\eta}{n}+4\\eta\\sqrt{\\frac{\\beta_{2}}{\\mu}}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Theorem C.4. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{W}_{i+1}-\\mathbf{W}_{i+1}^{(1)}\\|_{F}^{2}}\\\\ &{=\\|\\mathbf{W}_{i}-\\mathbf{I}_{p}\\|_{H}^{2}\\bar{\\mathbf{W}}_{i}-\\bar{\\mathbf{U}}^{\\prime}(\\mathbf{W}_{i},S)-\\mathbf{W}_{i}^{(0)}+\\eta_{\\mu}^{\\mathbb{I}}\\big(\\mathbf{W}_{i}^{(1)}-\\bar{\\mathbf{U}}^{\\prime}(\\mathbf{W}_{i}^{(1)},S^{(0)})\\big)\\|_{F}^{2}}\\\\ &{\\le\\bigg(\\|\\mathbf{W}_{i}-\\eta_{\\mu}^{\\mathbb{I}}\\big(\\mathbf{W}_{i}-\\bar{\\mathbf{U}}^{\\prime}(\\mathbf{W}_{i},S)\\big)-\\mathbf{W}_{i}^{(0)}+\\eta_{\\mu}^{\\mathbb{I}}\\big(\\mathbf{W}_{i}^{(0)}-\\mathbf{U}^{\\prime}(\\mathbf{W}_{i}^{(1)},S^{(0)})\\big)\\|_{F}+\\frac{2\\eta}{\\mu}\\sqrt{\\frac{2\\lambda_{2}}{\\mu}}}\\\\ &{\\qquad-\\frac{1}{\\mu}\\bigg(\\mathbf{W}_{i}-\\mathbf{U}_{\\mu}^{(1)}(\\mathbf{W}_{i},S)-\\mathbf{W}_{i}^{(0)}(\\mathbf{W}_{i},S)-\\mathbf{W}_{i}^{(0)}(\\mathbf{W}_{i}^{(1)}-\\bar{\\mathbf{U}}^{\\prime}(\\mathbf{W}_{i}^{(0)},S))\\|_{F}+\\frac{2\\eta}{\\mu}\\big(\\mathbf{W}_{i}^{(1)}-\\frac{2\\eta}{\\sqrt{\\mu}}\\big)}\\\\ &{\\qquad+\\frac{2\\eta}{\\mu}\\sqrt{\\frac{2\\lambda_{2}}{\\mu}}}\\\\ &{\\le\\bigg(\\|\\mathbf{W}_{i}-\\eta_{\\mu}^{\\mathbb{I}}\\big(\\mathbf{W}_{i},S\\big)-\\mathbf{W}_{i}^{(0)}(\\mathbf{W}_{i},S)-\\mathbf{W}_{i}^{(0)}(\\mathbf{W}_{i}^{(1)}-\\bar{\\mathbf{U}}^{\\prime}(\\mathbf{W}_{i}^{(1)},S))\\|_{F}+\\frac{4\\zeta_{\\mu}\\eta}{\\mu}\\sqrt{\\frac{2\\lambda_{2}}{\\mu \n$$$$\n\\begin{array}{r l}&{\\leq\\Bigg(\\|\\mathbb{W}_{\\varepsilon}-\\eta\\frac{1}{\\mu}(\\mathbb{W}_{\\varepsilon}-\\mathbb{U}^{\\varepsilon}(\\mathbb{W}_{\\varepsilon};S))-\\mathbb{W}_{\\varepsilon}^{(i)}+\\eta\\frac{1}{\\mu}(\\mathbb{W}_{\\varepsilon}^{(i)}-\\mathbb{U}^{\\mu}(\\mathbb{W}_{\\varepsilon}^{(i)};S))\\|_{F}+\\frac{2C_{\\sigma}\\eta}{\\mu n}}\\\\ &{\\qquad\\qquad+\\frac{2\\eta}{\\mu}\\sqrt{\\frac{2\\hat{\\mathcal{H}}_{\\sigma}}{\\mu}}\\Bigg)^{2}}\\\\ &{\\leq\\Bigg(\\|\\mathbb{W}_{\\varepsilon}-\\eta\\frac{1}{\\mu}(\\mathbb{W}_{\\varepsilon}-\\mathbb{U}^{\\varepsilon})-\\mathbb{W}_{\\varepsilon}^{(i)}+\\eta\\frac{1}{\\mu}(\\mathbb{W}_{\\varepsilon}^{(i)}-\\mathbb{U}^{\\mu}(\\mathbb{W}_{\\varepsilon}^{(i)};S))\\|_{F}+\\frac{4C_{\\sigma}\\eta}{n}+4\\eta\\sqrt{\\frac{\\beta_{2}}{\\mu}}\\Bigg)^{2}}\\\\ &{\\leq\\Bigg(\\|\\mathbb{W}_{\\varepsilon}-\\eta\\frac{1}{\\mu}(\\mathbb{W}_{\\varepsilon}-\\mathbb{U}^{\\mu}(\\mathbb{W}_{\\varepsilon};S))-\\mathbb{W}_{\\varepsilon}^{(i)}+\\eta\\nabla_{\\varepsilon}\\mathbb{H}^{\\rho}(\\mathbb{W}_{\\varepsilon}^{(i)};S)\\|_{F}^{2}+(\\mathcal{T}+2)\\bigg(\\frac{4C_{\\sigma}\\eta}{n}+4\\eta\\sqrt{\\frac{\\beta_{2}}{\\mu}}\\bigg)^{2}\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad((a+b)^{2}\\leq(1+\\eta)\\mu^{2}+(1+\\eta)\\|\\mathcal{P}\\|^{2}\\mathrm{er}>0}\\\\ &{\\leq\\!\\!\\!\\!\\!\\!\\!C\\frac{\\Gamma+2}{T+1}\\cdot\\frac{1}{1-\\frac{4(\\eta\\zeta)^{2}\\eta}{n}}\\|\\mathbb{W}_{\\varepsilon}-\\mathbb{W}_{\\varepsilon}^{(i)}\\|_{F}^{2}+(\\mathcal{T}+2)\\left(\\frac{4C_{\\sigma}\\eta}{n}+4\\eta\\sqrt{\\frac{\\beta_{2}}{\\mu}}\\right)^{2}\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Define $\\begin{array}{r}{\\gamma=\\frac{T+2}{T+1}\\cdot\\frac{1}{1-\\frac{4H C_{x}^{2}\\eta}{\\sqrt{m}}}}\\end{array}$ then the inequality above can be written as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|\\mathbf{W}_{t+1}-\\mathbf{W}_{t+1}^{(i)}\\|_{F}^{2}\\leq\\gamma\\|\\mathbf{W}_{t}-\\mathbf{W}_{t}^{(i)}\\|_{F}^{2}+(T+2)\\left(\\frac{4C_{x}\\eta}{n}+4\\eta\\sqrt{\\frac{\\beta_{2}}{\\mu}}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Dividing both sides by $\\gamma^{t+1}$ and summing up the inequality, we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{W}_{T}-\\mathbf{W}_{T}^{(i)}\\|_{F}^{2}\\leq\\frac{\\gamma^{T}}{\\gamma-1}(T+2)\\left(\\frac{4C_{\\alpha}\\eta}{n}+4\\eta\\sqrt{\\frac{\\beta_{2}}{\\mu}}\\right)^{2}}\\\\ &{\\qquad\\qquad\\leq\\gamma^{T}(T+1)(T+2)\\left(\\frac{4C_{\\alpha}\\eta}{n}+4\\eta\\sqrt{\\frac{\\beta_{2}}{\\mu}}\\right)^{2}}\\\\ &{\\qquad=\\left(\\frac{T+2}{T+1}\\right)^{T+1}\\left(\\frac{1}{1-\\frac{4H C_{\\alpha}\\gamma^{2}\\eta}{\\sqrt{n}}}\\right)^{T}(T+1)^{2}\\left(\\frac{4C_{\\alpha}\\eta}{n}+4\\eta\\sqrt{\\frac{\\beta_{2}}{\\mu}}\\right)^{2}}\\\\ &{\\qquad\\qquad\\overset{4\\pi^{\\frac{4(\\alpha-2)}{\\alpha_{T}\\alpha_{T}}}}{\\leq}(T+1)^{2}\\left(\\frac{4C_{\\alpha}\\eta}{n}+4\\eta\\sqrt{\\frac{\\beta_{2}}{\\mu}}\\right)^{2}\\qquad\\qquad(1\\frac{1}{1-\\frac{4}{\\gamma}}\\leq e^{\\frac{\\gamma^{2}}{\\alpha_{T}}})}\\\\ &{\\qquad\\qquad\\leq e^{1+\\frac{4\\pi^{2}\\gamma\\pi}{\\sqrt{n}}}(T+1)^{2}\\left(\\frac{4C_{\\alpha}\\eta}{n}+4\\eta\\sqrt{\\frac{\\beta_{2}}{\\mu}}\\right)^{2},\\qquad\\qquad(\\eta\\leq\\frac{\\sqrt{\\pi}}{8T\\zeta})}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The proof of Theorem 3.5 is obvious from Theorem C.4 by observing $e^{1+\\frac{8H C_{x}^{2}\\eta T}{\\sqrt{m}}}\\leq e^{9}$ ", "page_idx": 24}, {"type": "text", "text": "We have the following corollary by combining Theorem C.4, Lemma C.1-6 and Proposition A.3. ", "page_idx": 24}, {"type": "text", "text": "Corollary C.5. Assume the width of the networks satisfies $m\\geq H^{2}C_{x}^{4}\\eta^{2}T^{2}$ .After $T$ iterations of Algorithm 1 with Morea Envelope with 7 \u2264 min[\u03bc, SHg } 2HC , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}L_{r o b}(\\mathbf{W}_{T})\\leq\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\frac{M^{\\mu}(\\mathbf{W}_{T};S)+C_{x}^{2}\\mu}{1-e^{4.5}C_{x}(T+1)\\left(\\frac{4C_{x}\\eta}{n}+4\\eta\\sqrt{\\frac{\\beta_{2}}{\\mu}}\\right)}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}L_{r o b}(\\mathbf{W}_{T})\\leq\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}M^{\\mu}(\\mathbf{W}_{T};S)+C_{x}^{2}\\mu+e^{4.5}C_{x}(T+1)\\left(\\frac{4C_{x}\\eta}{n}+4\\eta\\sqrt{\\frac{\\beta_{2}}{\\mu}}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now we derive an optimization guarantee of optimizing the Moreau envelope. ", "page_idx": 24}, {"type": "text", "text": "Theorem C.6. Assume the width of the networks satisfies $m\\geq H^{2}C_{x}^{4}\\eta^{2}T^{2}$ . After running Algorithm 1 with Moreau Envelope for T iterations with n \u2264 \u03bc\u2264 ,wehave ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{1\\leq t\\leq T}M^{\\mu}(\\mathbf{W}_{t};S)\\leq\\operatorname*{min}_{\\mathbf{W}}\\left(\\widehat{L}_{r o b}(\\mathbf{W};S)+\\frac{2}{\\eta T}\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}+2\\eta(T+1)\\frac{\\beta_{2}}{\\mu}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof of Theorem C.6. From Lemma C.1, $M^{\\mu}(\\mathbf{W};S)$ is $\\frac{1}{\\mu}$ smooth, so ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad M^{\\mu}(\\mathbf{W}_{t+1};S)}\\\\ &{\\le M^{\\mu}(\\mathbf{W}_{t};S)+\\langle\\mathbf{W}_{t+1}-\\mathbf{W}_{t},\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}_{t};S)\\rangle+\\displaystyle\\frac{1}{2\\mu}\\|\\mathbf{W}_{t+1}-\\mathbf{W}_{t}\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "From the weakly convex property Lemma C.1-3, ", "page_idx": 25}, {"type": "text", "text": "MH(W; S) ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\geq M^{\\mu}(\\mathbf{W}_{t};S)+\\langle\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}_{t};S),\\mathbf{W}-\\mathbf{W}_{t}\\rangle-\\frac{H C_{x}^{2}}{\\sqrt{m}}\\|\\mathbf{W}_{t}-\\mathbf{W}\\|_{F}^{2}}\\\\ &{\\displaystyle=M^{\\mu}(\\mathbf{W}_{t};S)+\\langle\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}_{t};S),\\mathbf{W}-\\mathbf{W}_{t+1}\\rangle+\\langle\\mathbf{W}_{t+1}-\\mathbf{W}_{t},\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}_{t};S)\\rangle-\\frac{H C_{x}^{2}}{\\sqrt{m}}\\|\\mathbf{W}_{t}-\\mathbf{W}\\|_{F}^{2}}\\\\ &{\\displaystyle\\geq M^{\\mu}(\\mathbf{W}_{t+1};S)+\\langle\\nabla_{\\mathbf{W}}M^{\\mu}(\\mathbf{W}_{t};S),\\mathbf{W}-\\mathbf{W}_{t+1}\\rangle-\\frac{1}{2\\mu}\\|\\mathbf{W}_{t+1}-\\mathbf{W}_{t}\\|_{F}^{2}-\\frac{H C_{x}^{2}}{\\sqrt{m}}\\|\\mathbf{W}_{t}-\\mathbf{W}\\|_{F}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "(equation (13)) ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\geq M^{\\mu}(\\mathbf{W}_{t+1};S)+{\\frac{1}{\\eta}}\\left\\langle\\mathbf{W}_{t+1}-\\mathbf{W}_{t},\\mathbf{W}_{t+1}-\\mathbf{W}\\right\\rangle-{\\frac{1}{2\\eta}}\\|\\mathbf{W}_{t+1}-\\mathbf{W}_{t}\\|_{F}^{2}-{\\frac{H C_{x}^{2}}{\\sqrt{m}}}\\|\\mathbf{W}_{t}-\\mathbf{W}\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\quad-\\,\\frac{1}{\\mu}\\|\\mathbf{U}^{\\mu}(\\mathbf{W}_{t};S)-\\tilde{\\mathbf{U}}^{\\mu}(\\mathbf{W}_{t};S)\\|_{F}\\cdot\\|\\mathbf{W}_{t+1}-\\mathbf{W}\\|_{F}\\quad}&{\\mathrm{(Lemma~C.1.2.)}}\\\\ &{\\quad}&{\\displaystyle\\geq M^{\\mu}(\\mathbf{W}_{t+1};S)+\\frac{1}{\\eta}\\left\\langle\\mathbf{W}_{t+1}-\\mathbf{W}_{t},\\mathbf{W}_{t+1}-\\mathbf{W}\\right\\rangle-\\frac{1}{2\\eta}\\|\\mathbf{W}_{t+1}-\\mathbf{W}_{t}\\|_{F}^{2}-\\frac{H C_{x}^{2}}{\\sqrt{m}}\\|\\mathbf{W}_{t}-\\mathbf{W}\\|_{F}^{2}}\\\\ &{\\quad}&{\\displaystyle-\\,2\\sqrt{\\frac{\\beta_{2}}{\\mu}}\\cdot\\|\\mathbf{W}_{t+1}-\\mathbf{W}\\|_{F}.}&{\\mathrm{(Lemma~C.2)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "From the inequality above, for any weight matrix W, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{W}_{t+1}-\\mathbf{W}\\|_{F}^{2}}\\\\ &{=\\|\\mathbf{W}_{t}-\\mathbf{W}\\|_{F}^{2}-\\|\\mathbf{W}_{t+1}-\\mathbf{W}_{t}\\|_{F}^{2}+2\\left\\langle\\mathbf{W}_{t+1}-\\mathbf{W}_{t},\\mathbf{W}_{t+1}-\\mathbf{W}\\right\\rangle}\\\\ &{\\leq\\|\\mathbf{W}_{t}-\\mathbf{W}\\|_{F}^{2}+\\Bigg(2\\eta M^{\\mu}(\\mathbf{W};S)-2\\eta M^{\\mu}(\\mathbf{W}_{t+1};S)+\\displaystyle\\frac{2H C_{x}^{2}\\eta}{\\sqrt{m}}\\|\\mathbf{W}_{t}-\\mathbf{W}\\|_{F}^{2}+4\\eta\\sqrt{\\frac{\\beta_{2}}{\\mu}}\\cdot\\|\\mathbf{W}_{t+1}-\\mathbf{W}\\|_{F}^{2}\\Bigg)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Simplifying the above inequality and combining with Lemma C.1-6 gives us ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{W}_{t+1}-\\mathbf{W}\\|_{F}^{2}\\leq\\bigg(1+\\displaystyle\\frac{2H C_{x}^{2}\\eta}{\\sqrt{m}}\\bigg)\\,\\|\\mathbf{W}_{t}-\\mathbf{W}\\|_{F}^{2}+4\\eta\\sqrt{\\frac{\\beta_{2}}{\\mu}}\\|\\mathbf{W}_{t+1}-\\mathbf{W}\\|_{F}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,2\\eta\\widehat{L}_{r o b}(\\mathbf{W};S)-2\\eta M^{\\mu}(\\mathbf{W}_{t+1};S)}\\\\ &{\\qquad\\qquad\\leq\\bigg(1+\\displaystyle\\frac{2H C_{x}^{2}\\eta}{\\sqrt{m}}\\bigg)\\,\\|\\mathbf{W}_{t}-\\mathbf{W}\\|_{F}^{2}+\\displaystyle\\frac{1}{T+1}\\|\\mathbf{W}_{t+1}-\\mathbf{W}\\|_{F}^{2}+4\\eta^{2}(T+1)\\frac{\\beta_{2}}{\\mu}}\\\\ &{\\qquad\\qquad+\\,2\\eta\\widehat{L}_{r o b}(\\mathbf{W};S)-2\\eta M^{\\mu}(\\mathbf{W}_{t+1};S).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|{\\mathbf{W}}_{t+1}-{\\mathbf{W}}\\|_{F}^{2}\\leq\\left(1+\\displaystyle\\frac{1}{T}\\right)\\left(1+\\displaystyle\\frac{2H C_{x}^{2}\\eta}{\\sqrt{m}}\\right)\\|{\\mathbf{W}}_{t}-{\\mathbf{W}}\\|_{F}^{2}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad+\\,2\\eta\\left(1+\\displaystyle\\frac{1}{T}\\right)\\left(\\widehat{L}_{r o b}({\\mathbf{W}};S)-M^{\\mu}({\\mathbf{W}}_{t+1};S)+2\\eta(T+1)\\displaystyle\\frac{\\beta_{2}}{\\mu}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Dividing both sides by $\\begin{array}{r}{\\left(1+\\frac{1}{T}\\right)^{t+1}\\left(1+\\frac{2H C_{x}^{2}\\eta}{\\sqrt{m}}\\right)^{t+1}}\\end{array}$ and summing up the inequality for $t\\ =$ $0,1,\\dots,T-1$ ,weget ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{1\\leq t\\leq T}{\\operatorname*{min}}\\boldsymbol{M}^{\\mu}(\\mathbf{W}_{t};S)\\leq\\widehat{L}_{r o b}(\\mathbf{W};S)+2\\eta(T+1)\\frac{\\beta_{2}}{\\mu}+\\frac{\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}}{2\\eta\\left(1+\\frac{1}{T}\\right)\\sum_{t=1}^{T}\\frac{1}{\\left(1+\\frac{2H C_{\\frac{2}{T}^{2}}\\eta}{\\mu}\\right)^{t}\\left(1+\\frac{2H C_{\\frac{2}{S}^{2}}\\eta}{\\sqrt{m}}\\right)^{t}}}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\leq\\widehat{L}_{r o b}(\\mathbf{W};S)+\\frac{2}{\\eta T}\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}+2\\eta(T+1)\\frac{\\beta_{2}}{\\mu},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where in the last inequality we use $\\begin{array}{r}{\\left(1+\\frac{1}{T}\\right)\\left(1+\\frac{2H C_{x}^{2}\\eta}{\\sqrt{m}}\\right)\\leq\\left(1+\\frac{1}{T}\\right)\\left(1+\\frac{2}{T}\\right)\\leq1+3\\frac{T+1}{T^{2}}}\\end{array}$ \uff1a\u53e3 ", "page_idx": 25}, {"type": "text", "text": "Theorem 3.6. Define $\\begin{array}{r}{\\alpha_{2}(\\eta,T):=\\mathcal{O}(C_{x}^{2}\\frac{\\eta T}{n}+C_{x}\\eta T\\sqrt{\\frac{\\beta_{2}}{\\mu}})}\\end{array}$ Assume $\\alpha_{2}(\\eta,T)\\,<\\,1$ Then, afer $T\\ \\geq\\ 8$ iterations of Algorithm Moreau Envelope with step-size $\\eta\\,\\leq\\,\\mu$ on a network of width $m\\geq H^{2}C_{x}^{4}\\eta^{2}T^{2}$ ,wehave ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{[\\frac{9T}{10}]\\leq t\\leq T}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\varepsilon_{g e n}(\\mathbf{W}_{t})\\leq\\frac{55\\alpha_{2}(\\eta,T)}{1-\\alpha_{2}(\\eta,T)}\\left[\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+C_{x}^{2}\\mu+2\\eta(T+1)\\frac{\\beta_{2}}{\\mu}\\right],\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{1\\leq t\\leq T}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}L_{r o b}(\\mathbb{W}_{t})\\leq\\frac{1}{1-\\alpha_{2}(\\eta,T)}\\left[\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+C_{x}^{2}\\mu+2\\eta(T+1)\\frac{\\beta_{2}}{\\mu}\\right].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "$t_{0}~:=~[\\frac{9T}{10}]$ $\\begin{array}{r}{k\\;:=\\;\\frac{1}{\\big(1+\\frac{1}{T}\\big)\\Big(1+\\frac{2H C_{x}^{2}\\eta}{\\sqrt{m}}\\Big)}}\\end{array}$ Dividing both sides of $\\begin{array}{r}{\\left(1+\\frac{1}{T}\\right)^{t+1}\\left(1+\\frac{2H C_{x}^{2}\\eta}{\\sqrt{m}}\\right)^{t+1}}\\end{array}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle2\\eta\\left(1+\\frac{1}{T}\\right)\\sum_{t=1}^{T}k^{t}\\left(\\widehat{L}_{r o b}(\\mathbf{W};S)+2\\eta(T+1)\\frac{\\beta_{2}}{\\mu}\\right)+\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}}\\\\ {\\displaystyle\\geq2\\eta\\left(1+\\frac{1}{T}\\right)\\sum_{t=t_{0}}^{T}k^{t}M^{\\mu}(\\mathbf{W}_{t};S).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Taking minimum over all weight matrices $\\mathrm{W}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\eta\\left(1+\\displaystyle\\frac{1}{T}\\right)\\displaystyle\\sum_{t=1}^{T}k^{t}\\left(\\widehat{L}_{r o b}(\\mathbf{W};S)-C_{x}^{2}\\mu\\right)}\\\\ &{\\leq2\\eta\\left(1+\\displaystyle\\frac{1}{T}\\right)\\displaystyle\\sum_{t=r_{0}}^{T}k^{t}M^{\\mu}(\\mathbf{W}_{t};S)}&{\\mathrm{(Lemma~C.I.})}\\\\ &{\\leq\\operatorname*{min}_{\\forall\\forall\\;\\}\\left(2\\eta\\left(1+\\displaystyle\\frac{1}{T}\\right)\\displaystyle\\sum_{t=1}^{T}k^{t}\\left(\\widehat{L}_{r o b}(\\mathbf{W};S)+2\\eta(T+1)\\displaystyle\\frac{\\beta_{2}}{\\mu}\\right)+\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}\\right)}\\\\ &{\\leq2\\eta\\left(1+\\displaystyle\\frac{1}{T}\\right)\\displaystyle\\sum_{t=1}^{T}k^{t}\\cdot\\operatorname*{min}\\left(\\widehat{L}_{r o b}(\\mathbf{W};S)+\\displaystyle\\frac{2}{\\eta T}\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}+2\\eta(T+1)\\displaystyle\\frac{\\beta_{2}}{\\mu}\\right)}\\\\ &{=2\\eta\\left(1+\\displaystyle\\frac{1}{T}\\right)\\displaystyle\\sum_{t=1}^{T}k^{t}\\cdot\\left(\\Delta_{S}^{\\operatorname*{max}}+2\\eta(T+1)\\displaystyle\\frac{\\beta_{2}}{\\mu}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Taking the expectation on both sides, we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=t_{0}}^{T}k^{t}\\left(\\operatorname*{min}_{t_{0}\\leq t\\leq T}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)-C_{x}^{2}\\mu\\right)\\leq\\displaystyle\\sum_{t=t_{0}}^{T}k^{t}\\left(\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)-C_{x}^{2}\\mu\\right)}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{t=1}^{T}k^{t}\\cdot\\left(\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+2\\eta(T+1)\\frac{\\beta_{2}}{\\mu}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Simplifying the equation above, we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{t_{0}\\leq t\\leq T}{\\operatorname*{min}}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)\\leq C_{x}^{2}\\mu+\\frac{\\displaystyle\\sum_{T}^{T}k^{t}}{\\displaystyle\\sum_{t=t_{0}}^{T}k^{t}}\\cdot\\left(\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+2\\eta(T+1)\\frac{\\beta_{2}}{\\mu}\\right)\\qquad\\qquad\\mathrm{(I)}}\\\\ &{\\leq C_{x}^{2}\\mu+\\left(\\displaystyle\\sum_{r=0}^{9}\\left(\\frac{1}{k}\\right)^{r\\frac{T}{10}}\\right)\\cdot\\left(\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+2\\eta(T+1)\\frac{\\beta_{2}}{\\mu}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq C_{x}^{2}\\mu+\\left(\\displaystyle\\sum_{r=0}^{9}e^{\\frac{3r}{10}}\\right)\\cdot\\left(\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+2\\eta(T+1)\\frac{\\beta_{2}}{\\mu}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(m\\geq H^{2}C_{x}^{4}\\eta^{2}T^{2})}\\\\ &{\\leq C_{x}^{2}\\mu+55\\left(\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+2\\eta(T+1)\\frac{\\beta_{2}}{\\mu}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Equation 4) gives us for any $\\begin{array}{r}{t\\leq T,\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\varepsilon_{g e n}(\\mathbf{W}_{t})\\leq\\frac{\\alpha_{2}(\\eta,T)}{1-\\alpha_{2}(\\eta,T)}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)}\\end{array}$ Therefore, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{t_{0}\\leq t\\leq T}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\varepsilon_{g e n}(\\mathbf{W}_{t})\\leq\\frac{\\alpha_{2}(\\eta,T)}{1-\\alpha_{2}(\\eta,T)}\\operatorname*{min}_{t_{0}\\leq t\\leq T}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\frac{\\alpha_{2}(\\eta,T)}{1-\\alpha_{2}(\\eta,T)}\\left(C_{x}^{2}\\mu+55\\left(\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+2\\eta(T+1)\\frac{\\beta_{2}}{\\mu}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The proof of the second statement takes a similar approach. Following the same procedure, we can replace $t_{0}$ by 0 in equation (15), and get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq t\\leq T}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)\\leq\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+C_{x}^{2}\\mu+2\\eta(T+1)\\frac{\\beta_{2}}{\\mu}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Combining with equation (4), ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{0\\leq t\\leq T}{\\operatorname*{min}}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}L_{r o b}(\\mathbf{W}_{t})\\leq\\cfrac{1}{1-\\alpha_{2}(\\eta,T)}\\underset{0\\leq t\\leq T}{\\operatorname*{min}}\\,\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\widehat{L}_{r o b}(\\mathbf{W}_{t};S)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\cfrac{1}{1-\\alpha_{2}(\\eta,T)}\\left(\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}\\Delta_{S}^{\\mathrm{oracle}}+C_{x}^{2}\\mu+2\\eta(T+1)\\frac{\\beta_{2}}{\\mu}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "$\\begin{array}{r}{T\\leq\\mathcal{O}(\\operatorname*{min}\\lbrace n^{2},\\frac{1}{\\beta_{2}^{2/3}}\\rbrace)}\\end{array}$ h\u00e4 step-size $\\begin{array}{r}{\\eta=\\mu=\\Theta(\\frac{1}{C_{x}^{2}\\sqrt{T}})}\\end{array}$ on a network of width $m\\geq\\Omega(T)$ we have forany weight matrix $\\mathrm{W}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{1\\leq t\\leq T}\\mathbb{E}_{S\\sim\\mathcal{D}^{n}}L_{r o b}(\\mathbf{W}_{t})\\leq1.1L_{r o b}(\\mathbf{W})+\\mathcal{O}\\left(\\frac{C_{x}^{2}\\|\\mathbf{W}-\\mathbf{W}_{0}\\|_{F}^{2}}{\\sqrt{T}}\\right)+\\mathcal{O}\\left(\\frac{1}{\\sqrt{T}}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof of Corollary 3.7. Under the conditions of the corollary, we have $m\\ \\geq\\ H^{2}C_{x}^{4}\\eta^{2}T^{2}$ , and $\\begin{array}{r}{\\alpha_{2}(\\eta,T)\\,=\\,\\mathcal{O}(C_{x}^{2}\\frac{\\eta T}{n}+C_{x}\\eta T\\sqrt{\\frac{\\beta_{2}}{\\mu}})}\\end{array}$ $\\begin{array}{r}{\\frac{1}{1-\\alpha_{2}\\left(\\eta,T\\right)}\\,\\le\\,1.1}\\end{array}$ Thenis clear that this corollary is a special case of Theorem 3.6. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "A. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We further expand on the claims made in Abstract and Introduction in Section 3. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "B. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The paper discussed the limitations in the Conclusion section. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "C. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper provide the full set of assumptions and a complete and correct proof for each theoretical result. Please see Appendix. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "D. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "E. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not include experiments requiring code. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https : //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so ^No\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "F. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "G. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "H. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "1. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The research conducted in the paper conform with the NeurIPS Code of Ethics in every respect. The theoretical nature of the results means there are minimal ethical concerns. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "J. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The societal impacts of the paper is overall positive. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "K. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "L. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 32}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 33}, {"type": "text", "text": "M. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "N. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "O. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]