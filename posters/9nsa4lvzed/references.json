{"references": [{"fullname_first_author": "Moritz Hardt", "paper_title": "Train faster, generalize better: Stability of stochastic gradient descent", "publication_date": "2016-00-00", "reason": "This paper introduces the concept of uniform stability for analyzing the generalization performance of SGD, a key theoretical foundation for the current work's stability analysis of adversarial training."}, {"fullname_first_author": "Yunwen Lei", "paper_title": "Stability and generalization of stochastic optimization with nonconvex and nonsmooth problems", "publication_date": "2023-00-00", "reason": "This paper extends the stability analysis to nonconvex and nonsmooth settings, directly addressing the challenges posed by the non-smoothness of adversarial loss functions."}, {"fullname_first_author": "Aleksander Madry", "paper_title": "Towards deep learning models resistant to adversarial attacks", "publication_date": "2018-00-00", "reason": "This seminal work introduced the widely used Madry et al. adversarial training framework, which is the primary empirical defense mechanism studied in this paper."}, {"fullname_first_author": "Yue Xing", "paper_title": "On the algorithmic stability of adversarial training", "publication_date": "2021-00-00", "reason": "This paper provides a stability analysis of adversarial training, focusing on convex nonsmooth losses, which is extended in the current work to more general nonconvex functions."}, {"fullname_first_author": "Jiancong Xiao", "paper_title": "PAC-Bayesian adversarially robust generalization bounds for deep neural networks", "publication_date": "2023-00-00", "reason": "This work offers generalization guarantees for adversarial training using PAC-Bayes bounds, complementing the current work's uniform stability approach."}]}