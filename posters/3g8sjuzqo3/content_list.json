[{"type": "text", "text": "Multidimensional Fractional Programming for Normalized Cuts ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yannan Chen1\u2217 Beichen Huang2\u2217 Licheng Zhao3 Kaiming Shen1\u2020 ", "page_idx": 0}, {"type": "text", "text": "1School of Science and Engineering, The Chinese University of Hong Kong (Shenzhen), China 2McMaster University, Canada 3Shenzhen Research Institute of Big Data, China E-mail: yannanchen $@$ link.cuhk.edu.cn, huangb21 $@$ mcmaster.ca, zhaolicheng $@$ sribd.cn, shenkaiming $@$ cuhk.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The Normalized cut (NCut) problem is a fundamental and yet notoriously difficult one in the unsupervised clustering field. Because the NCut problem is fractionally structured, the fractional programming (FP) based approach has worked its way into a new frontier. However, the conventional FP techniques are insufficient: the classic Dinkelbach\u2019s transform can only deal with a single ratio and hence is limited to the two-class clustering, while the state-of-the-art quadratic transform accounts for multiple ratios but fails to convert the NCut problem to a tractable form. This work advocates a novel extension of the quadratic transform to the multidimensional ratio case, thereby recasting the fractional 0-1 NCut problem into a bipartite matching problem\u2014which can be readily solved in an iterative manner. Furthermore, we explore the connection between the proposed multidimensional FP method and the minorization-maximization theory to verify the convergence. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Fractional programming (FP) is a powerful optimization tool for solving diverse problems involving ratio terms, e.g., in the areas of physics, economics, management science, signal processing, computer science, and information theory [1, 2, 3]. This paper explores a novel application of FP to the normalized cut (NCut)\u2014which is a fundamental and yet notoriously difficult problem for unsupervised data clustering [4]. A new FP technique called the multidimensional quadratic transform [5] forms the building block of this work. Differing from the classic Dinkelbach\u2019s transform [6] that is typically limited to the single-ratio problem with a pair of scalar-valued numerator and denominator, the multidimensional quadratic transform is capable of handling multiple ratios simultaneously in the same problem, and further accounts for the multidimensional-ratio case wherein the numerators and denominators take a matrix/vector form. It turns out that the NCut problem solving can be made much easier from a multidimensional FP point of view. Two main results have been achieved under the umbrella of FP. First, we show that one most recent advance [7] in the NCut field can be interpreted as a special scalar-ratio version of the multidimensional quadratic transform, which already outperforms the classic methods significantly. Second, by fully exploiting the multidimensional quadratic transform [5], we develop a superior FP-based algorithm tailored to the NCut problem. ", "page_idx": 0}, {"type": "text", "text": "Clustering has been considered extensively in the literature from a variety of perspectives, e.g., Kmeans [8], hierarchical clustering [9], spectral clustering (SC) [10], graph cuts [11], and high-density clustering [12]. The graph cuts approach is of particular interest for its flexibility to cope with a wide range of cluster types, e.g., not requiring the desired clusters to be center-based as many other geometry-based clustering algorithms do [13]. With each data point mapped to a vertex in a weighted undirected graph, there are different ways to measure the relative strength of similarities between subgraphs (each corresponding to a cluster), which in turn lead to different classes of graph cuts algorithms, e.g., min cut [14], ratio cut [15], and min-max cut [16] aside from the NCut. Like many modern works in the realm of graph cuts, our study focuses on the NCut metric because it yields stable performance and prevents cluster imbalance [4]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Nevertheless, the optimization criterion of the NCut is numerically difficult to tackle. To be more specific, the NCut entails solving an NP-complete problem [4]. The SC method constitutes a popular heuristic approach to the NCut problem [4, 10], but it cannot provide any performance guarantee. Other works aim at the analytical aspect and rely heavily on the optimization theory. For example, the Fast Coordinate Descent (FCD) algorithm proposed in [17] evolves from the standard optimization tool of block coordinate descent. By contrast, the Direct Normalized Cut (DNC) algorithm in [18] is somewhat less straightforward. The main idea of [18] is to approximate the NCut problem by using a lower bound on the original optimization objective, but it incurs a costly inner iteration in computing such a lower bound. To remedy this, the Fast Iterative Normalized Cut (FINC) in [7] approximates the NCut problem based on a closed-form lower bound. However, the resulting new problem is still difficult to solve directly, which can only be addressed in a heuristic fashion as shown in [7]. The present work is most closely related to DNC [18] and FINC [7] in the sense that it seeks to approximate the NCut problem via bounding as well. As compared to the above existing bounds, the new bound proposed in this work can be constructed immediately, and can further enable efficient solving of the new problem for the clustering purpose. ", "page_idx": 1}, {"type": "text", "text": "2 NCut problem statement ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Suppose there are $N$ data points in total. Use $i,j\\in\\{1,2,\\dots,N\\}$ to index these data points. For a pair of data points $i$ and $j$ , the similarity between them is quantified as $0\\leq w_{i j}\\leq1$ . By symmetry, we have $w_{i j}=w_{j i}$ . In the graph theory context, with each data point visualized as a vertex, the edge between vertex $i$ and vertex $j$ is assigned the weight $w_{i j}$ (or $w_{j i.}$ ). Denote by $\\mathcal{V}$ the set of vertices, and $\\mathcal{E}$ the set of edges. The resulting graph $G=(\\boldsymbol{\\nu},\\boldsymbol{\\mathcal{E}})$ can be recognized as a weighted undirected graph. For each vertex $i$ , its degree $d_{i}$ is the sum weights across all the incident edges: ", "page_idx": 1}, {"type": "equation", "text": "$$\nd_{i}=\\sum_{j=1}^{N}w_{i j}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Dividing the $N$ data points into $K>1$ clusters is equivalent to partitioning $\\nu$ into $K$ disjoint subsets $\\{\\nu_{1},\\nu_{2},\\dots,\\nu_{K}\\}$ , where $\\begin{array}{r}{\\bigcup_{k=1}^{K}\\mathcal{V}_{k}=\\mathcal{V}}\\end{array}$ and $\\mathcal{V}_{k}\\cap\\mathcal{V}_{k^{\\prime}}=\\emptyset$ for any $k\\neq k^{\\prime}$ . For any two disjoint subsets $A,B\\subseteq\\nu$ , we def ine ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\Phi(\\mathcal{A},\\mathcal{B})=\\sum_{i\\in\\mathcal{A}}\\sum_{j\\in\\mathcal{B}}w_{i j},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "which is illustrated in Figure 1. ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\sqrt{3y^{2}}=\\sqrt{3}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "image", "img_path": "", "img_caption": ["Figure 1: Graph cut between two disjoint subsets $\\boldsymbol{\\mathcal{A}}$ and $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ . "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Moreover, for any subset $A\\subseteq\\nu$ , we define its volume to be ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname{vol}(A)=\\sum_{i\\in{\\mathcal{A}}}d_{i}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "In principle, data clustering aims to group together those data points that are sufficiently similar to each other. Equivalently, we wish to minimize the similarity between any two clusters. Toward this end, one traditional strategy is to minimize ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{cut}(\\rangle_{1},\\mathbb{\\nu}_{2},\\ldots,\\mathbb{\\nu}_{K})={\\frac{1}{2}}\\sum_{k=1}^{K}\\Phi(\\mathbb{\\nu}_{k},{\\bar{\\mathbb{\\nu}}}_{k}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\bar{\\nu}_{k}$ is the complement of $\\mathcal{V}_{k}$ , i.e., $\\bar{\\nu}_{k}=\\mathcal{V}\\backslash\\mathcal{V}_{k}$ . However, minimizing $\\operatorname{cut}(\\mathcal{V}_{1},\\mathcal{V}_{2},\\ldots,\\mathcal{V}_{K})$ alone can be problematic\u2014it tends to put most data points in one particular cluster while leaving other clusters almost empty, namely the cluster imbalance [19]. To resolve this issue, a natural idea is to regularize the cluster volume by considering the normalized cut: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname{ncut}({\\mathcal{V}}_{1},{\\mathcal{V}}_{2},\\ldots,{\\mathcal{V}}_{K})={\\frac{1}{2}}\\sum_{k=1}^{K}{\\frac{\\Phi({\\mathcal{V}}_{k},{\\bar{\\mathcal{V}}}_{k})}{\\operatorname{vol}({\\mathcal{V}}_{k})}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Intuitively speaking, the value of $\\Phi(\\mathcal{V}_{k},\\bar{\\mathcal{V}}_{k})$ would soar if very few data points have been assigned to cluster $k$ , thereby discouraging the cluster imbalance. ", "page_idx": 2}, {"type": "text", "text": "We are now ready to formalize the NCut problem. The indicator variable $x_{i k}\\in\\{0,1\\}$ equals 1 if data point $i$ is assigned to cluster $k$ , and equals 0 otherwise. Moreover, write $\\pmb{W}\\doteq[w_{i j}]\\in\\mathbb{R}^{N\\times N}$ , $D=\\mathrm{diag}[d_{1},d_{2},\\ldots,d_{N}]\\in\\mathbb{R}^{N\\times N}$ , and $X=[x_{i k}]\\in\\{0,1\\}^{N\\times K}$ . Denote by $\\pmb{x}_{k}\\in\\{0,1\\}^{N}$ the $k$ th column of $\\mathbf{\\deltaX}$ . It can be shown that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{ncut}(\\mathcal{V}_{1},\\mathcal{V}_{2},\\ldots,\\mathcal{V}_{K})=\\frac12\\sum_{k=1}^{K}\\frac{x_{k}^{\\top}L x_{k}}{x_{k}^{\\top}D x_{k}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the graph Laplacian matrix $\\textbf{\\emph{L}}$ is given by $L=D-W$ . We seek the optimal clustering decision $\\mathbf{\\deltaX}$ that minimizes $\\operatorname{ncut}(\\mathcal{V}_{1},\\mathcal{V}_{2},\\ldots,\\mathcal{V}_{K})$ . Further, because $\\mathrm{ncut}(\\mathcal{V}_{1},\\mathcal{V}_{2},\\ldots,\\mathcal{V}_{K})=\\frac{1}{2}K=$ kK=1xxk\u22a4\u22a4 WD xxkk , the NCut minimization problem boils down to ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{X}{\\mathrm{maximize}}}&{\\displaystyle\\sum_{k=1}^{K}\\frac{x_{k}^{\\top}W x_{k}}{x_{k}^{\\top}D x_{k}}}\\\\ {\\mathrm{subject}\\;0}&{\\displaystyle\\sum_{k=1}^{K}x_{i k}=1,\\quad i=1,\\ldots,n}\\\\ &{x_{i k}\\in\\{0,1\\},\\quad i=1\\ldots,n,\\;k=1,\\ldots.K,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the two constraints (7b) and $(7\\mathrm{c})$ state that each data point must be assigned to one unique cluster. The difficulties of the above problem can be recognized with two respects. First, the clustering variables $\\{x_{i k}\\}$ are discrete. Second, even when every $x_{i k}$ is relaxed to be a continuous variable on the interval $[0,1]$ , the problem is still nonconvex. ", "page_idx": 2}, {"type": "text", "text": "3 Fractional programming ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The NCut problem in (7) is fractionally structured. To be more specific, (7) takes a sum-of-ratios form. This quick observation strongly suggests that the NCut is amenable to FP, but it turns out that very few previous works in the literature have adopted the FP approach. In the rest of this section, we first review the conventional FP methods to show why they are rarely considered for the NCut, and then introduce a recently proposed FP technique called the multidimensional quadratic transform\u2014which forms the building block of our proposed clustering algorithm as introduced in Section 4. ", "page_idx": 2}, {"type": "text", "text": "3.1 Conventional FP methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The early studies in the FP field are restricted to the single-ratio problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{maximize}_{x\\in{\\mathcal{X}}}\\quad{\\frac{A(x)}{B(x)}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $A(x)\\,\\geq\\,0$ is a nonnegative function, $B(x)\\,>\\,0$ is a strictly positive function, and $\\mathcal{X}$ is a nonempty constraint set on $x$ . In the literature, many works further assume that $A(x)$ is concave in $x$ , $B(\\bar{x grave{)}}$ is convex in $x$ , and $\\mathcal{X}$ is a convex set, namely the concave-convex condition. Notice that problem (8) is still nonconvex in general under the concave-convex condition, so the direct solving of (8) is difficult. The classic Dinkelbach\u2019s transform in essence aims to decouple the ratio: ", "page_idx": 3}, {"type": "text", "text": "Proposition 1 (Dinkelbach\u2019s transform [6]) The single-ratio problem (8) is equivalent to ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{m a x i m i z e}&{{}A(x)-y B(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the auxiliary variable y is iteratively updated as $y=A(x)/B(x)$ . ", "page_idx": 3}, {"type": "text", "text": "Observe that the new problem (9) is convex in $x$ for fixed $y$ under the concave-convex condition, and hence can be efficiently solved by the standard optimization method. Importantly, solving for $x$ in (9) with $y$ iteratively updated guarantees convergence to the global optimum of the original problem (8). However, it is difficult to extend Dinkelbach\u2019s transform to the multi-ratio problems (except for the max-min-ratios case [20]). As such, the use of Dinkelbach\u2019s transform in the NCut area is limited to the two-class clustering that only needs to optimize a single ratio [21]. ", "page_idx": 3}, {"type": "text", "text": "We now consider $K>1$ pairs of the numerator function $A_{k}(x)\\,\\geq\\,0$ and denominator function $B_{k}(x)>0$ along with a nonempty constraint set $\\mathcal{X}$ . A sum-of-ratios problem is then formulated as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{maximzie}_{x\\in\\mathcal{X}}\\quad\\sum_{k=1}^{K}\\frac{A_{k}(x)}{B_{k}(x)}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "It is tempting to decouple each ratio $A_{k}(x)/B_{k}(x)$ by using Dinkelbach\u2019s transform separately, but the resulting new problem is not equivalent to problem (10). Consequently, the classic Dinkelbach\u2019s transform does not work for the NCut with general $K$ clusters. A valid method to decouple multiple ratios is presented in the following proposition. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2 (Quadratic transform [5]) The sum-of-ratios problem (10) is equivalent to ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{m a x i m z i e}&{{}\\displaystyle\\sum_{k=1}^{K}2y_{k}\\sqrt{A_{k}}(x)-y_{k}^{2}B_{k}(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "in the sense that $x^{\\star}$ is a solution to (10) if and only $i f(x^{\\star},y^{\\star})$ is a solution to (11), where an auxiliary variable $y_{k}$ is introduced for each ratio term $A_{k}(x)/B_{k}(x)$ . ", "page_idx": 3}, {"type": "text", "text": "We propose optimizing $x$ and $\\{y_{k}\\}$ iteratively. When $x$ is held fixed, each $y_{k}$ can be optimally determined as ", "page_idx": 3}, {"type": "equation", "text": "$$\ny_{k}^{\\star}=\\frac{\\sqrt{A_{k}(x)}}{B_{k}(x)}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Furthermore, under a generalized concave-convex condition [5] wherein each $A_{k}(x)$ is a concave function, each $B_{k}(x)$ is a convex function, and $\\mathcal{X}$ is a convex set, it can be shown that the new problem (11) is convex in $x$ when $\\{y_{k}\\}$ are held fixed. Thus, the alternating optimization between $x$ and $\\{y_{k}\\}$ can be performed efficiently. ", "page_idx": 3}, {"type": "text", "text": "Now let us return to the NCut problem in (7) and apply the above FP technique to it. Treating $\\pmb{x}_{k}^{\\top}\\pmb{W}\\pmb{x}_{k}$ and $\\mathbf{\\Delta}x_{k}^{\\top}D x_{k}$ respectively as numerator and denominator, we can recast problem (7) into ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{maximize}_{X,\\;y_{k}\\in\\mathbb{R}}\\quad\\sum_{k=1}^{K}\\left(2y_{k}\\sqrt{\\pmb{x}_{k}^{\\top}\\pmb{W}\\pmb{x}_{k}}-y_{k}^{2}\\pmb{x}_{k}^{\\top}\\pmb{D}\\pmb{x}_{k}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As before, we optimize $\\mathbf{\\deltaX}$ and $\\{y_{k}\\}$ iteratively. For fixed $\\mathbf{\\deltaX}$ , the optimal solution of $y_{k}$ is ", "page_idx": 3}, {"type": "equation", "text": "$$\ny_{k}^{\\star}=\\frac{\\sqrt{\\pmb{x}_{k}^{\\top}W\\pmb{x}_{k}}}{\\pmb{x}_{k}^{\\top}D\\pmb{x}_{k}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "It remains to optimize $\\mathbf{\\deltaX}$ in (13) for fixed $\\{y_{k}\\}$ . Observe that $x^{\\top}W x$ is not a concave function of $\\textbf{\\em x}$ since $W$ is often a positive semi-definite matrix [22], and also that the constraint set of $\\mathbf{\\deltaX}$ is not convex because of (7c), so the aforementioned generalized concave-convex condition does not hold here. As a result, solving for $\\mathbf{\\deltaX}$ in (13) with $\\{y_{k}\\}$ held fixed is no longer a convex problem. The above alternating optimization between $\\mathbf{\\deltaX}$ and $\\{y_{k}\\}$ can be recognized as the so-called Fast Iterative Normalized Cut (FINC) algorithm of the recent work [7]. Although it manages to decouple multiple ratios in the NCut problem, we are faced with a new challenging problem. The new problem is dealt with in a heuristic fashion in [7]. This fact perhaps explains why the FP approach has not yet been considered extensively in the literature despite the fractional structure of the NCut problem. ", "page_idx": 4}, {"type": "text", "text": "3.2 Multidimensional FP method ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now proceed to a much more sophisticated FP toolkit that accounts for multidimensional ratios. To start, consider the following matrix extension of the traditional scalar-valued FP problem: each $A_{k}(x)\\,\\geq\\,0$ is generalized as positive semi-definite $A_{k}(x)\\,\\in\\,\\mathbb{S}_{+}^{m\\times m}$ , while each $\\mathbf{\\bar{\\alpha}}\\!B_{k}(x)\\,>\\,0$ is generalized as positive definite $B_{k}(x)\\in\\mathbb{S}_{++}^{m\\times m}$ . Accordingly, the ratio term is extended to the matrix form as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{A_{k}(x)}{B_{k}(x)}\\in\\mathbb{R}_{+}\\implies B_{k}(x)^{-1}A_{k}(x)\\in\\mathbb{S}_{+}^{m\\times m}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We then arrive at a matrix extension of the sum-of-ratios problem (10): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{maximzie}_{x\\in\\mathcal{X}}\\quad\\sum_{k=1}^{K}\\operatorname{tr}\\left(B_{k}^{-1}(x)A_{k}(x)\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "One main result of this paper is that the quadratic transform in Proposition 2 carries over to the matrix ratio case, as stated in the following proposition. ", "page_idx": 4}, {"type": "text", "text": "Proposition 3 (Multidimensional quadratic transform) Suppose that each $A_{k}(x)\\in\\mathbb{S}_{+}^{m\\times m}$ can be factorized as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{A}_{k}(\\boldsymbol{x})=[\\pmb{Z}_{k}(\\boldsymbol{x})]^{\\top}[\\pmb{Z}_{k}(\\boldsymbol{x})]\\quad\\boldsymbol{w h e r e}~~\\pmb{Z}_{k}(\\boldsymbol{x})\\in\\mathbb{R}^{\\ell\\times m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for some positive integer $\\ell$ . The matrix FP problem (15) is then equivalent to ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underset{x\\in\\mathcal{X},\\,Y_{k}\\in\\mathbb{R}^{\\ell\\times m}}{m a x i m i z e}\\quad\\sum_{k=1}^{K}\\mathrm{tr}\\left(2Y_{k}[Z_{k}(x)]^{\\top}-Y_{k}B_{k}(x)Y_{k}^{\\top}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where an auxiliary variable $\\pmb{Y}_{k}\\in\\mathbb{R}^{\\ell\\times m}$ is introduced for each matrix ratio $B_{k}^{-1}(x)A_{k}(x)$ ", "page_idx": 4}, {"type": "text", "text": "Proof 1 It can be shown that each $\\mathbf{{Y}}_{k}$ in (17) is always optimally determined as ", "page_idx": 4}, {"type": "equation", "text": "$$\nY_{k}^{\\star}=Z_{k}(x)B_{k}^{-1}(x).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Substituting the above $\\mathbf{Y}_{k}^{\\star}$ in (17) recovers the original problem (15). ", "page_idx": 4}, {"type": "text", "text": "Proposition 4 The alternating optimization between x and $\\{{\\bf Y}_{k}\\}$ in (17) amounts to an MM procedure, so it guarantees a nondecreasing convergence of the original optimization objective in (15), as specified in Appendix A.1. ", "page_idx": 4}, {"type": "text", "text": "The key step is to optimize $x$ for fixed $\\{{\\bf Y}_{k}\\}$ . Recall that the primal variable $x$ is still difficult to optimize for the NCut problem after applying the quadratic transform in Proposition 2. In contrast, it turns out that the multidimensional quadratic transform in Proposition 3 can lead us to an efficient iterative update of $x$ for the NCut problem scenario, as elaborated in the next section. ", "page_idx": 4}, {"type": "text", "text": "4 Proposed Multidimensional-FP-based NCut ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The goal of this section is to address problem (7) by means of the multidimensional FP. We begin with a special case in which the similarity matrix $W$ is assumed to be positive semi-definite; the indefinite $W$ case will be discussed later on. ", "page_idx": 4}, {"type": "text", "text": "It is crucial to notice that the numerator part $\\pmb{x}_{k}^{\\top}\\pmb{W}\\pmb{x}_{k}$ can be factorized as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{x}_{k}^{\\top}W\\pmb{x}_{k}=\\pmb{z}_{k}^{\\top}\\pmb{z}_{k}\\quad\\mathrm{where}\\;\\;\\pmb{z}_{k}=W^{\\frac{1}{2}}\\pmb{x}_{k}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We remark that $W^{\\frac{1}{2}}$ , i.e., the symmetric square root of $W$ [23], is guaranteed to exist because we have assumed that $W\\in\\mathbb{S}_{+}^{m\\times m}$ for the current discussion. We then treat $\\pmb{x}_{k}^{\\top}\\pmb{W}\\pmb{x}_{k}$ , $z_{k}$ , and $\\mathbf{\\boldsymbol{x}}_{k}^{\\top}D\\mathbf{\\boldsymbol{x}}_{k}$ as ${\\mathbfcal{A}}_{k}(x),Z_{k}(x)$ , and $B_{k}(x)$ , respectively, in Proposition 3, with $m=1$ and $\\ell=N$ , thus using the multidimensional quadratic transform to reformulate the NCut problem (7) as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{maximize}_{X,\\,y_{k}\\in\\mathbb{R}^{N}}\\quad\\sum_{k=1}^{K}\\mathrm{tr}\\left(2y_{k}(W^{\\frac{1}{2}}x_{k})^{\\top}-y_{k}(x_{k}^{\\top}D x_{k})y_{k}^{\\top}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We then optimize $\\mathbf{\\deltaX}$ and $\\{{\\pmb y}_{k}\\}$ iteratively. When $\\mathbf{\\deltaX}$ is held fixed, each $\\pmb{y}_{k}$ is optimally determined as ", "page_idx": 5}, {"type": "equation", "text": "$$\ny_{k}^{\\star}=\\frac{W^{\\frac{1}{2}}{\\pmb x}_{k}}{{\\pmb x}_{k}^{\\top}D{\\pmb x}_{k}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Now the core question is whether $\\mathbf{\\deltaX}$ could be efficiently solved when $\\{{\\pmb y}_{k}\\}$ are held fixed. The optimization objective of $\\mathbf{\\deltaX}$ in (20a) for fixed $\\{{\\pmb y}_{k}\\}$ is written as ", "page_idx": 5}, {"type": "equation", "text": "$$\nh(\\pmb{X})=\\sum_{k=1}^{K}\\mathrm{tr}\\left(2y_{k}(\\pmb{W}^{\\frac{1}{2}}\\pmb{x}_{k})^{\\top}-y_{k}(\\pmb{x}_{k}^{\\top}\\pmb{D}\\pmb{x}_{k})\\pmb{y}_{k}^{\\top}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "It is critical to observe that under the discrete constraint $x_{i k}\\in\\{0,1\\}$ we must have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\boldsymbol{x}}_{k}^{\\top}{D}{\\boldsymbol{x}}_{k}={\\boldsymbol{\\mathbf{1}}}^{\\top}{D}{\\boldsymbol{x}}_{k}=\\delta^{\\top}{\\boldsymbol{x}}_{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{1}=(1,1,\\ldots,1)^{\\top}$ is the all-ones vector and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\delta=\\mathbf{1}^{\\top}D=[d_{1},d_{2},\\ldots,d_{N}]^{\\top}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We can then rewrite $h(X)$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\nh(\\pmb{X})=\\sum_{k=1}^{K}\\left(2\\pmb{y}_{k}^{\\top}\\pmb{W}^{\\frac{1}{2}}\\pmb{x}_{k}-\\pmb{y}_{k}^{\\top}\\pmb{y}_{k}\\delta^{\\top}\\pmb{x}_{k}\\right)=\\sum_{k=1}^{K}\\pmb{\\mu}_{k}^{\\top}\\pmb{x}_{k},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{\\mu}_{k}=2\\pmb{W}^{\\frac{1}{2}}\\pmb{y}_{k}-\\delta\\pmb{y}_{k}^{\\top}\\pmb{y}_{k}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In (25), the first equality follows since $\\mathbf{\\Delta}x_{k}^{\\top}D x_{k}$ is a scalar. Denote by $\\mu_{i k}$ the $i$ th component of $\\pmb{\\mu}_{k}$ . In light of (25), we can readily maximize $h(X)$ under the constraints (7b) and (7c): it is optimal to set $x_{i k}$ with the largest $\\mu_{i k}$ on each row of $\\mathbf{\\deltaX}$ to one, while setting the rest $x_{i k}$ of the row to zero, i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\nx_{i k}^{\\star}=\\left\\{\\begin{array}{l l}{1\\quad}&{\\mathrm{if}\\;k=\\arg\\operatorname*{max}\\;\\mu_{i k},}\\\\ {\\quad}&{\\mathrm{otherwise}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "If there exists a tie (i.e., when more than one cluster index $k^{\\prime}$ maximizes $\\mu_{i k^{\\prime}}$ for same $i$ ) then break it randomly. Further, with $\\pmb{y}_{k}^{\\star}$ in (21) plugged in (26), we obtain an efficient computation of $\\pmb{\\mu}_{k}$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mu_{k}={\\frac{2W x_{k}}{x_{k}^{\\top}D x_{k}}}-{\\frac{\\delta x_{k}^{\\top}W x_{k}}{(x_{k}^{\\top}D x_{k})^{2}}}={\\frac{2W x_{k}}{\\delta^{\\top}x_{k}}}-{\\frac{\\delta x_{k}^{\\top}W x_{k}}{(\\delta^{\\top}x_{k})^{2}}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The merits of rewriting $\\pmb{\\mu}_{k}$ as (28) are two-fold. First, it sidesteps the update of the auxiliary variables $\\{{\\pmb y}_{k}\\}$ . Second, it no longer entails computing the square root of $W$ . The resulting algorithm referred to as fractional programming-based clustering $(F P C)$ is summarized in the following. ", "page_idx": 5}, {"type": "text", "text": "Clearly, the FPC algorithm is guaranteed to converge in terms of the new objective value $h(X)$ , since the iterative update of $\\mathbf{\\deltaX}$ and $\\{\\pmb{\\mu}_{k}\\}$ in FPC amounts to a block coordinate ascent for problem (20) so that $h(X)$ is monotonically increasing after each iteration. We can actually claim a stronger result for FPC according to Proposition 4, as stated in the subsequent proposition. ", "page_idx": 5}, {"type": "table", "img_path": "3G8sjUZqO3/tmp/a8f9a0762a5bc8310861b140006649e3c1de08213183dce4a376b4b719be5062.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Proposition 5 Not only the new objective value $h(X)$ in (22) but also the original objective value of the NCut problem, f(X) =  k  k kK=1xxk\u22a4\u22a4 WD xxk , is nondecreasing after each iteration of FPC. ", "page_idx": 6}, {"type": "text", "text": "We thus far assume that the similarity matrix $W$ is positive semi-definite; this assumption can be justified by arguing that a positive definite kernel [24] (e.g., the Gaussian kernel) is often used to generate $W$ . But what if some indefinite kernel has been adopted and hence $W$ is not necessarily positive semi-definite anymore? The following proposition provides a solution. ", "page_idx": 6}, {"type": "text", "text": "Proposition 6 Suppose that the similarity matrix $W$ is indefinite. We can choose a sufficiently large $\\alpha>0$ so that the new matrix ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widetilde{\\pmb{W}}=\\pmb{W}+\\alpha\\pmb{D}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "is positive semi-definite. Notice that such $\\alpha$ must exist since ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\alpha=-{\\frac{\\lambda_{\\operatorname*{min}}(W)}{\\operatorname*{min}_{i}d_{i}}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "is a feasible choice. Then we can equivalently consider problem (7) withW used in place of $W$ , which can be readily addressed by the FPC algorithm. ", "page_idx": 6}, {"type": "text", "text": "Proof 2 See Appendix A.2. ", "page_idx": 6}, {"type": "text", "text": "Finally, we examine the computational complexity of the FPC algorithm. The update of $\\mathbf{\\deltaX}$ as in (27) incurs a computational complexity of $\\bar{\\mathcal{O}}(K^{2}\\bar{N})$ , while the update of $\\{\\pmb{\\mu}_{k}\\}$ as in (28) incurs a computational complexity of $\\mathcal{O}(\\bar{K}N^{2})$ . If $W$ is indefinite, then we would further find the smallest eigenvalue of $W$ as required in (30). Rather than computing all the eigenvalues and then picking the smallest, which incurs $\\bar{O}(N^{3})$ , we propose a more efficient way of computing $\\lambda_{\\operatorname*{min}}$ : ", "page_idx": 6}, {"type": "text", "text": "1. Find the largest eigenvalue of $\\|\\boldsymbol{W}\\|_{\\mathrm{F}}\\boldsymbol{I}-\\boldsymbol{W}$ , denoted as $\\lambda_{1}$ , by the power method [25], where $\\Vert\\cdot\\Vert_{\\mathrm{F}}$ is the Frobenius norm.   \n2. Compute the smallest eigenvalue as $\\lambda_{\\operatorname*{min}}(W)=\\|W\\|_{\\mathrm{F}}-\\lambda_{1}$ . ", "page_idx": 6}, {"type": "text", "text": "Thus, the overall complexity of finding $\\alpha$ as in (30) is $\\mathcal{O}(N^{2})$ . To sum up, the per-iteration complexity of FPC equals $\\mathcal{O}(K\\bar{N}^{2})$ , while the traditional SC algorithm incurs a complexity of $O(N^{3})$ . ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We validate the performance of the proposed FPC algorithm on 8 common datasets as summarized in Table 1. The benchmarks are the SC [4], FINC [7], and FCD [17]. We use the Gaussian kernel to generate the similarity matrix, i.e., $w_{i j}=\\exp\\left(-\\|\\pmb{v}_{i}-\\pmb{v}_{j}\\|_{2}^{2}\\right)$ , where $\\pmb{v}_{i}$ and $\\pmb{v}_{j}$ are the feature vectors of data points $i$ and $j$ . All the tests were carried out on a desktop equipped with $2.10\\:\\mathrm{GHz}$ $\\mathrm{CPU}\\!\\times\\!12$ . Throughout the tables, we highlight the best performance by using the bold font. ", "page_idx": 6}, {"type": "text", "text": "5.1 Optimization objective of NCut ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first evaluate the performance of the different algorithms in minimizing n $\\operatorname{cut}(\\mathcal{V}_{1},\\mathcal{V}_{2},\\ldots,\\mathcal{V}_{K})$ as defined in (6). We run each algorithm 10 times with the random starting point generated for each trial, and then pick the best one. Table 2 summarizes the results. Observe that the proposed FPC method achieves the lowest NCut objective across all the datasets. For instance, the NCut objective of FPC is $0.19\\%$ lower than that of FCD for the dataset Office $^{+}$ Caltech10 with $K=10$ clusters, and ", "page_idx": 6}, {"type": "table", "img_path": "3G8sjUZqO3/tmp/70d62db5a12c232439e68f0f263628134b681bbb499485e8be5c63d29b66fc44.jpg", "table_caption": ["Table 1: Datasets used for the task of dividing $N$ data points into $K$ clusters. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "3G8sjUZqO3/tmp/c01dc84132708e5ad0ce545b4501b698b4b4d40b33824694970b72a2f01b8f0d.jpg", "table_caption": ["Table 2: NCut objective values achieved by the different algorithms with random initialization. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "3G8sjUZqO3/tmp/d76a3fc44a61d3d93cf3ad4dc64930e820c7967ad5319a0d3d73b5dbc409a396.jpg", "table_caption": ["Table 3: NCut objective values achieved by the different algorithms with the SC initialization. "], "table_footnote": ["Note: Each entry has the form [objective value] $\\pm$ [standard variance]. Red color indicates degradation while blue color indicates improvement. "], "page_idx": 7}, {"type": "text", "text": "$0.07\\%$ lower for the dataset Epileptic with $N=11500$ data points. All the benchmarks except SC are strictly inferior to FPC; SC is equally good as FPC only on the dataset Rice. ", "page_idx": 7}, {"type": "text", "text": "Moreover, we consider first using SC to obtain a raw clustering decision and then using other algorithms to refine it. The test results are summarized in Table 3. Observe that using FPC after SC can achieve the best performance on all 8 datasets. In particular, it strictly improves upon the SC initialization on 6 datasets. It is worth observing that FINC may even yield worse performance after the initialization by SC; this is because FINC cannot guarantee that the new problem is optimally solved per iteration as formerly mentioned in Section 3.1. Finally, in Fig. 2 we find the global optimum for two small-size datasets via exhaustive search, and use it as the benchmark to compare with the proposed FPC algorithm; observe that FPC attains convergence to the global optimum after merely 3 iterates. ", "page_idx": 7}, {"type": "text", "text": "5.2 Other performance metrics ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Aside from the Ncut optimization criterion, the following commonly used performance metrics in practice are considered for the different clustering algorithms: the accuracy (ACC), the normalized mutual information (NMI) [29], and the adjusted random index (ARI) [30]. Unlike $\\operatorname{ncut}(\\mathcal{V}_{1},\\mathcal{V}_{2},\\ldots,\\mathcal{V}_{K})$ , the above metrics are proportional to the performance, i.e., the higher metric value, the better clustering. The test results are summarized in Table 4. Although these performance metrics are not directly tied to the NCut objective, the proposed FPC method still achieves the highest scores in many cases. ", "page_idx": 7}, {"type": "image", "img_path": "3G8sjUZqO3/tmp/deb24bcf646c278497560e44f0da0eaff748f4866e3de1f4ff4957d3d0dbc3bc.jpg", "img_caption": ["Figure 2: Convergence in terms of the NCut objective value for two UCI datasets. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "3G8sjUZqO3/tmp/e2a656bec98861531b7e0ddfb20b06ef60a6308a8afbb5f089c732140e3407e3.jpg", "table_caption": ["Table 4: The performance metrics of ACC, NMI, and ARI achieved by the different algorithms. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "We further try out the different clustering algorithms in the image segmentation task. Using the image dataset from [31], we perform the color histogram and the local binary pattern analysis for about 500 superpixels to extract the features as in [32]. As shown in Figure 3, the clustering by FPC gives clearer boundaries of the objects than other methods. ", "page_idx": 8}, {"type": "text", "text": "Moreover, Figure 4 shows the average time consumption of the different algorithms. It can be seen that FPC runs $73\\%$ faster than FINC, and runs equally fast as SC. We remark that FCD requires the least running time because it tends to get trapped in a suboptimal point prematurely at the early stage. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion and limitation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This work proposes a novel application of multidimensional FP to the NCut clustering, differing from the previous works that rely on the traditional scalar-ratio FP such as Dinkelbach\u2019s transform and quadratic transform. The main merit of using multidimensional FP is that the new 0-1 problem can be efficiently solved via linear search. Further, the resulting FPC algorithm can be interpreted as an MM procedure with provable monotonic convergence in terms of the NCut optimization criterion. Thus far, we only show that its per-iteration complexity is lower than the overall complexity of the traditional SC algorithm. ", "page_idx": 8}, {"type": "image", "img_path": "3G8sjUZqO3/tmp/be4a32d14463b0eb0e7b67f7c8517a079279c5e849a0626adc0158e180318c4e.jpg", "img_caption": ["Figure 3: Image segmentation by the different algorithms. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "3G8sjUZqO3/tmp/122fb66b3d0340be07dfbbd639c2b7d3cf6a4ea17ff14de66e7dbb432bba4d2d.jpg", "img_caption": ["Figure 4: Running time of the different algorithms when applied to the different datasets. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The work of Yannan Chen, Beichen Huang, and Kaiming Shen was supported in part by Guangdong Major Project of Basic and Applied Basic Research (No. 2023B0303000001), in part by the National Natural Science Foundation of China (NSFC) under Grant 92167202, and in part by Shenzhen Steady Funding Program. The work of Licheng Zhao was supported in part by the NSFC under Grant 62206182, and in part by Guangdong Basic and Applied Basic Research Foundation under Grant 2024A1515010154. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] I. M. Stancu-Minasian, Fractional programming: theory, methods and applications. Norwell, MA, USA: Kluwer, 2012.   \n[2] E. B. Bajalinov, Linear-fractional programming theory, methods, applications and software. Norwell, MA, USA: Kluwer, 2003.   \n[3] S. Schaible, \u201cParameter-free convex equivalent and dual programs of fractional programming problems,\u201d Zeitschrift f\u00fcr Oper. Res., vol. 18, no. 5, pp. 187\u2013196, Oct. 1974. [4] J. Shi and J. Malik, \u201cNormalized cuts and image segmentation,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 22, no. 8, pp. 888\u2013905, Feb. 2000. [5] K. Shen and W. Yu, \u201cFractional programming for communication systems\u2014Part I: Power control and beamforming,\u201d IEEE Trans. Signal Process., vol. 66, no. 10, pp. 2616\u20132630, Mar. 2018.   \n[6] W. Dinkelbach, \u201cOn nonlinear fractional programming,\u201d Manage. Sci., vol. 13, no. 7, pp. 492\u2013498, Mar. 1967. [7] X. Chen, Z. Xiao, F. Nie, and J. Z. Huang, \u201cFINC: An efficient and effective optimization method for normalized cut,\u201d IEEE Trans. Pattern Anal. Mach. Intell., Feb. 2022. [8] J. B. MacQueen, \u201cSome methods for classification and analysis of multivariate observations,\u201d in Proc. 5th Berkeley Symp. Math. Statist. Probab., vol. 1, 1967, pp. 281\u2013297.   \n[9] S. C. Johnson, \u201cHierarchical clustering schemes,\u201d Psychometrika, vol. 32, no. 3, pp. 241\u2013254, Sept. 1967.   \n[10] A. Ng, M. Jordan, and Y. Weiss, \u201cOn spectral clustering: Analysis and an algorithm,\u201d Adv. Neural Inf. Process. Syst. (NeurIPS), vol. 14, 2001.   \n[11] F. Nie, C. Ding, D. Luo, and H. Huang, \u201cImproved MinMax cut graph clustering with nonnegative relaxation,\u201d in Proc. Joint Eur. Conf. Mach. Learn. Knowl. Discovery Databases, 2010, pp. 451\u2013466.   \n[12] M. Ester, H.-P. Kriegel, J. Sander, X. Xu et al., \u201cA density-based algorithm for discovering clusters in large spatial databases with noise,\u201d in Proc. ACM SIGKDD Int. Conf. Knowl. Discovery & Data Min., vol. 96, no. 34, 1996, pp. 226\u2013231.   \n[13] D. Yan, L. Huang, and M. I. Jordan, \u201cFast approximate spectral clustering,\u201d in Proc. ACM SIGKDD Int. Conf. Knowl. Discovery & Data Min., June 2009, pp. 907\u2013916.   \n[14] M. Stoer and F. Wagner, \u201cA simple min-cut algorithm,\u201d J. ACM, vol. 44, no. 4, pp. 585\u2013591, July 1997.   \n[15] L. Hagen and A. B. Kahng, \u201cNew spectral methods for ratio cut partitioning and clustering,\u201d IEEE Trans. Comput.-Aided Design Integr. Circuits Syst., vol. 11, no. 9, pp. 1074\u20131085, Sept. 1992.   \n[16] C. H. Ding, X. He, H. Zha, M. Gu, and H. D. Simon, \u201cA min-max cut algorithm for graph partitioning and data clustering,\u201d in IEEE Int. Conf. Data Min. (ICDM), Nov. 2001, pp. 107\u2013114.   \n[17] F. Nie, J. Lu, D. Wu, R. Wang, and X. Li, \u201cA novel normalized-cut solver with nearest neighbor hierarchical initialization,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 46, no. 1, pp. 659\u2013666, May 2024.   \n[18] X. Chen, W. Hong, F. Nie, D. He, M. Yang, and J. Z. Huang, \u201cSpectral clustering of large-scale data by directly solving normalized cut,\u201d in Proce. ACM SIGKDD Int. Conf. Knowl. Discovery & Data Min., July 2018, pp. 1206\u20131215.   \n[19] U. Von Luxburg, \u201cA tutorial on spectral clustering,\u201d Statist. Comput., vol. 17, pp. 395\u2013416, Aug. 2007.   \n[20] J. P. Crouzeix, J. A. Ferland, and S. Schaible, \u201cAn algorithm for generalized fractional programs,\u201d J. Optim. Theory Appl., vol. 47, no. 1, pp. 35\u201349, Sept. 1985.   \n[21] B. Ghanem and N. Ahuja, \u201cDinkelbach NCUT: An efficient framework for solving normalized cuts problems with priors and convex constraints,\u201d Int. J. Comput. Vision, vol. 89, pp. 40\u201355, Feb. 2010.   \n[22] B. Sch\u00f6lkopf, C. J. Burges, and A. J. Smola, Advances in kernel methods: support vector learning. Cambridge, MA, USA: MIT press, 1999.   \n[23] S. P. Boyd and L. Vandenberghe, Convex optimization. U.K.: Cambridge univ. Press, 2004.   \n[24] G. E. Fasshauer, \u201cPositive definite kernels: past, present and future,\u201d Dolomites Res. Notes Approx., vol. 4, pp. 21\u201363, 2011.   \n[25] G. W. Stewart, Introduction to Matrix Computations. New York, NY, USA: Academic Press, 1973.   \n[26] K. N. Markelle Kelly, Rachel Longjohn, \u201cThe UCI machine learning repository,\u201d https://archive. ics.uci.edu.   \n[27] J. Wang et al., \u201cEverything about transfer learning and domain adapation,\u201d http://transferlearning. xyz.   \n[28] C.-C. Chang and C.-J. Lin, \u201cLIBSVM: a library for support vector machines,\u201d ACM Trans. Intell. Syst. Technol., vol. 2, no. 3, pp. 1\u201327, May 2011.   \n[29] A. Lancichinetti, S. Fortunato, and J. Kert\u00e9sz, \u201cDetecting the overlapping and hierarchical community structure in complex networks,\u201d New J. Phys., vol. 11, no. 3, p. 033015, Mar. 2009.   \n[30] A. J. Gates and Y.-Y. Ahn, \u201cThe impact of random models on clustering similarity,\u201d J. Mach. Learn. Res., vol. 18, no. 87, pp. 1\u201328, Jan. 2017.   \n[31] S. Gould, R. Fulton, and D. Koller, \u201cDecomposing a scene into geometric and semantically consistent regions,\u201d in IEEE Int. Conf. Comput. Vis. (ICCV), Sept. 2009, pp. 1\u20138.   \n[32] B. Cheng, G. Liu, J. Wang, Z. Huang, and S. Yan, \u201cMulti-task low-rank affinity pursuit for image segmentation,\u201d in IEEE Int. Conf. Comput. Vis. (ICCV), Nov. 2011, pp. 2439\u20132446.   \n[33] Y. Sun, P. Babu, and D. P. Palomar, \u201cMajorization-minimization algorithms in signal processing, communications, and machine learning,\u201d IEEE Trans. Signal Process., vol. 65, no. 3, pp. 794\u2013816, Aug. 2016.   \n[34] M. Razaviyayn, M. Hong, and Z.-Q. Luo, \u201cA unified convergence analysis of block successive minimization methods for nonsmooth optimization,\u201d SIAM J. Optim., vol. 23, no. 2, pp. 1126\u2013 1153, 2013. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Proof of Proposition 4 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We begin with a brief review of the MM theory [33, 34]. Consider a general constrained optimization problem ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{maximize}_{x\\in{\\mathcal{X}}}\\quad f(x).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Rather than solving the above problem directly, the MM theory deals with an approximation of (31) iteratively. In principle, we construct a surrogate function $g(x|\\hat{x})$ of $x$ given the condition parameter ${\\hat{x}}\\in{\\mathcal{X}}$ , such that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{g(x|\\hat{x})\\leq f(x),}\\\\ {g(\\hat{x}|\\hat{x})=f(\\hat{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Then the MM method solves a sequence of new problems of the surrogate function: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{maximize}_{x\\in\\mathcal{X}}\\quad g(x|\\hat{x}),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "with $\\hat{x}$ iteratively updated to the previous solution $x$ . Specifically, with the solution of (34) in the $(t-1)$ th iteration denoted by $\\hat{x}^{(t-1)}$ , we construct a surrogate function $g(x|\\hat{x}^{(t-1)})$ for the $t$ iteration and then obtain the new solution $\\hat{x}^{(t)}$ of (34), and so forth, as illustrated in Figure 5. ", "page_idx": 12}, {"type": "image", "img_path": "3G8sjUZqO3/tmp/a9a66ff71c610063bb825488a8ddc6bb35eb2af9efcd6170fada07cbfb471a88.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "Figure 5: The original objective value $f(x)$ is nondecreasing after each iteration of the MM method. ", "page_idx": 12}, {"type": "text", "text": "Theorem 1 (Monotonic convergence [33, 34]) The MM method yields a nondecreasing convergence of the original objective value $f(x)$ , i.e., ", "page_idx": 12}, {"type": "equation", "text": "$$\nf(\\hat{x}^{(t-1)})\\leq f(\\hat{x}^{(t)})\\;\\;\\;f o r\\;\\;\\;t=1,2,\\ldots.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We now show that the alternating optimization between $x$ and $\\{\\pmb{Y}_{k}\\}$ in (17) can be interpreted as an MM method. We still use the superscript $t$ to index the iteration. Recall also that each $\\mathbf{Y}_{k}$ in the tth iteration is optimally updated as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbf{\\boldsymbol{Y}}_{k}^{(t)}=Z_{k}(\\boldsymbol{x}^{(t-1)})B_{k}^{-1}(\\boldsymbol{x}^{(t-1)}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We now view the update of $\\mathbf{Y}_{k}$ as a function of the previous solution $\\hat{x}$ , that is ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{V}_{k}(\\hat{x})=Z_{k}(\\hat{x}^{(t-1)})B_{k}^{-1}(\\hat{x}^{(t-1)}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Substituting each $\\mathbf{}Y_{k}$ with $y_{k}(\\hat{x})$ in the new objective function in (17) gives rise to a function of $x$ conditioned on $\\hat{x}$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g(x|\\hat{x})=\\displaystyle\\sum_{k=1}^{K}\\mathrm{tr}\\left(2Z_{k}(\\hat{x}^{(t-1)})B_{k}^{-1}(\\hat{x}^{(t-1)})[Z_{k}(x)]^{\\top}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.-\\ Z_{k}(\\hat{x}^{(t-1)})B_{k}^{-1}(\\hat{x}^{(t-1)})B_{k}(x)[Z_{k}(\\hat{x}^{(t-1)})B_{k}^{-1}(\\hat{x}^{(t-1)})]^{\\top})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Notice that maximizing the objective function in (17) with $\\{{\\bf Y}_{k}\\}$ fixed is equivalent to maximizing $g(x|\\hat{x})$ where $\\hat{x}$ is fixed at the solution of the previous iteration. ", "page_idx": 13}, {"type": "text", "text": "Most importantly, it can be shown that $g(x|\\hat{x})$ meets the conditions (32) and (33) for the original objective function in (15), so the alternating optimization between $x$ and $\\{{\\bf Y}_{k}\\}$ in (17) amounts to an MM procedure. The result of Proposition 4 then immediately follows by Theorem 1. ", "page_idx": 13}, {"type": "text", "text": "A.2 Proof of Proposition 6 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "First of all, because $W$ is indefinite, its minimum eigenvalue $\\lambda_{\\operatorname*{min}}(W)$ must be negative. With $\\alpha$ in (30), the matrixW can be rewritten as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\widetilde{\\boldsymbol{W}}=\\boldsymbol{W}-\\frac{\\lambda_{\\operatorname*{min}}(\\boldsymbol{W})}{\\operatorname*{min}_{i}d_{i}}\\boldsymbol{D}}\\\\ {\\quad\\,=\\boldsymbol{W}-\\lambda_{\\operatorname*{min}}(\\boldsymbol{W})\\boldsymbol{I}-\\lambda_{\\operatorname*{min}}(\\boldsymbol{W})\\mathrm{diag}\\left[\\frac{d_{1}}{\\operatorname*{min}_{i}d_{i}}-1,\\dots,\\frac{d_{N}}{\\operatorname*{min}_{i}d_{i}}-1\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It is evident that $W-\\lambda_{\\mathrm{min}}(W)I$ and $\\begin{array}{r}{-\\lambda_{\\operatorname*{min}}(W)\\mathrm{diag}\\left[\\frac{d_{1}}{\\operatorname*{min}_{i}d_{i}}-1,\\dots,\\frac{d_{N}}{\\operatorname*{min}_{i}d_{i}}-1\\right]}\\end{array}$ are both positive semi-definite, soW is positive semi-definite too. Moreover, it is easy to see that problem (7) is equivalent to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{maximize}_{X}\\quad K\\alpha+\\sum_{k=1}^{K}\\frac{\\pmb{x}_{k}^{\\top}W\\pmb{x}_{k}}{\\pmb{x}_{k}^{\\top}D\\pmb{x}_{k}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which can be further rewritten as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{maximize}_{\\pmb{X}}\\quad\\sum_{k=1}^{K}\\frac{x_{k}^{\\top}\\widetilde{W}x_{k}}{x_{k}^{\\top}D x_{k}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The proof is then completed. ", "page_idx": 13}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: The abstract describes the motivation behind our algorithm and the novel mathematical tool that leads to the proposed algorithm. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 14}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Justification: The limitation is discussed in Section 6 along with conclusion ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 14}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: We state the theory assumptions in all the propositions and theorem. For the new result in Proposition 3, we provide the proof right after the proposition. For the new results in Proposition 5 and Proposition 6, we relegate their proofs to Appendix A.1. Regarding the existing results, i.e., Proposition 1, Proposition 2, and Theorem 1, we clarify their source references. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 15}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: The details of the proposed algorithm are stated in Algorithm 1 ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 15}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We have submitted the source codes as an anonymized zip file. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 16}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The experimental details are presented in Section 5. The attached source codes are well commented. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 16}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We report the standard deviation of the experiment results, and run the experiment 10 times to avoid the extreme cases of the random initialization. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 16}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 17}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The experiments computer resources are clarified at the beginning of Section 5. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 17}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We follow the instructions listed in NeurIPS Code of Ethics. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 17}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 17}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 18}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: This paper describes a new mathematical method to solve the NCut problem.   \nThis paper poses no such risks. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 18}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Yes, we have properly credited the creators and original owners of the assets used in our paper. For each dataset, we have provided appropriate citations and links where they can be accessed. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: There are new codes related to the proposed algorithm in this paper, which are submitted as an anonymized zip file along with the user documentation. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 19}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 19}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 20}]