[{"type": "text", "text": "Self-Guiding Exploration for Combinatorial Problems ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zangir Iklassov MBZUAI zangir.iklassov@mbzuai.ac.ae ", "page_idx": 0}, {"type": "text", "text": "Yali Du King\u2019s College London yali.du@kcl.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Farkhad AkimovMBZUAIfarkhad.akimov@mbzuai.ac.ae", "page_idx": 0}, {"type": "text", "text": "Martin Tak\u00e1c\u02c7 MBZUAI martin.takac@mbzuai.ac.ae ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large Language Models (LLMs) have become pivotal in addressing reasoning tasks across diverse domains, including arithmetic, commonsense, and symbolic reasoning. They utilize prompting techniques such as Exploration-of-Thought, Decomposition, and Refinement to effectively navigate and solve intricate tasks. Despite these advancements, the application of LLMs to Combinatorial Problems (CPs), known for their NP-hardness and critical roles in logistics and resource management remains underexplored. To address this gap, we introduce a novel prompting strategy: Self-Guiding Exploration (SGE), designed to enhance the performance of solving CPs. SGE operates autonomously, generating multiple thought trajectories for each CP task. It then breaks these trajectories down into actionable subtasks, executes them sequentially, and refines the results to ensure optimal outcomes. We present our research as the first to apply LLMs to a broad range of CPs and demonstrate that SGE outperforms existing prompting strategies by over $27.84\\%$ in CP optimization performance. Additionally, SGE achieves a $2.46\\%$ higher accuracy over the best existing results in other reasoning tasks (arithmetic, commonsense, and symbolic). ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large Language Models (LLMs) have emerged as powerful tools capable of executing reasoning tasks across various domains, including arithmetic, commonsense, and symbolic reasoning [4, 33, 5, 27]. These models may leverage prompting techniques such as Exploration-of-Thought [39, 13, 42], Decomposition [49, 12], and Refinement [19] to break down and solve various tasks in a step-bystep manner. Recent research has been directed towards extending these techniques to tackle more sophisticated optimization challenges [41]. Combinatorial problems (CPs) may represent a category of these complex optimization tasks, associated with intricate computational challenges. ", "page_idx": 0}, {"type": "text", "text": "Combinatorial Problems are characterized by their NP-hardness and inherent complexity, which result in an exponential growth in the number of potential solutions. This complexity presents substantial challenges in the research [26, 25, 11, 10]. CPs are especially crucial in sectors that require efficient logistics, planning, and scheduling. Currently, the dominant approach in these industries involves metaheuristic methods. These methods combine various simple but fast heuristics to effectively tackle CPs within specific constraints. Nonetheless, the effectiveness of these heuristics can vary significantly depending on the CP task and its associated constraints, necessitating a customized selection of heuristics to achieve optimal performance. ", "page_idx": 0}, {"type": "text", "text": "In the meantime, research on exploring LLMs to solve CPs reveals substantial gaps. While recent advancements indicate the effectiveness of LLMs in various reasoning tasks [38, 47, 32, 49], their application to CPs has been minimal. The literature indicates that existing generative models can address smaller instances of the Traveling Salesman Problem (TSP) [16, 23, 41]. However, as problem sizes increase, existing prompting strategies begin to yield inadequate responses, underscoring the need for more sophisticated prompting methods. Moreover, there is a notable scarcity of research addressing other complex CPs, particularly the Vehicle Routing and Job Scheduling Problems, which pose significant challenges in logistics, planning industries, and operations research. ", "page_idx": 1}, {"type": "text", "text": "In this work, we introduce a novel prompting strategy: self-guiding exploration (SGE), designed to enhance the problem-solving process for CPs. This algorithm works as a combination of explorationof-thought, decomposition, and refinement prompting methods. The SGE approach autonomously generates multiple thought trajectories for a given CP, each trajectory representing a specific heuristic to tackle the given task. Each trajectory is then decomposed into subtasks, which are executed one by one, and their outputs are refined and combined into a final solution. Unlike the task-specific prompts utilized in other methods, SGE employs general-purpose prompts, allowing for the adaptive use of specific heuristic solutions tailored to various CPs, such as the Hungarian heuristic for the assignment problem and the Nearest Neighbor heuristic for the vehicle routing problem. Essentially, SGE acts as a versatile metaheuristic capable of identifying, combining, and refining task-specific heuristics for individual CP tasks. ", "page_idx": 1}, {"type": "text", "text": "Our work makes following contributions. Firstly, we present a novel investigation into the application of large language models for solving combinatorial problems. Secondly, we introduce a new prompting strategy, SGE, that autonomosly generates thought trajectories, splits them into subtasks and refines the answers. Thirdly, we demonstrate that SGE outperforms existing prompting strategies such as Chain-of-Thought, Decomposition, and Self-Refinement, improving CP optimization performance by $27.84\\%$ . Lastly, we validate the applicability of SGE across other reasoning tasks, including arithmetic, commonsense, and symbolic tasks, where our method achieves a $2.46\\%$ higher accuracy than the best existing results. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "CP via classical approach. In classical research, combinatorial problems are predominantly tackled using heuristic and metaheuristic methods specifically crafted for particular tasks. Notable examples include the Ant-Colony Optimization and Tabu Search methods for addressing the Vehicle Routing Problem [28, 8, 15], the Shortest Processing Time and Most Work Remaining Heuristics for Job Scheduling Problems [30], and the Hungarian Algorithm for the Assignment Problem [1]. These approaches are favored in industrial settings due to their simplicity and speed. However, they need to be individually tailored to each task and its constraints\u2019 setting. In contrast, exact solvers such as Google-OR-Tools [9] offer general and precise solutions, but their applicability is often limited to smaller-scale problems due to the inherent NP-hardness of combinatorial problems. ", "page_idx": 1}, {"type": "text", "text": "CP via learning-based approach. In AI literature, Reinforcement Learning (RL) has been a prominent approach for tackling combinatorial problems since the 1990s [21, 20, 45, 7, 2]. The integration of deep learning, particularly through innovations like Pointer Networks, has significantly enhanced RL\u2019s capability to handle more complex combinatorial tasks [35, 3]. Further advancements involve the use of Transformer networks [6, 34, 14], with notable applications in solving the Vehicle Routing Problem [25, 11]. Despite these advances, RL-based methods often still do not exceed the performance of traditional heuristics, especially when scalability and accurate state representation are required [36, 22, 44, 31]. ", "page_idx": 1}, {"type": "text", "text": "CP with LLMs. Recent studies have leveraged large language models (LLMs), such as GPT-3.5 and GPT-4, to tackle combinatorial problems like the Traveling Salesman Problem using iterative prompting, where solutions are refined incrementally [23, 41]. Other works employ LLMs to autonomously generate executable code as novel heuristics for problems like the knapsack and traveling salesman [29, 17, 43, 18]. This promising approach enhances task-specific heuristics, potentially improving performance on specialized combinatorial tasks. In contrast, our focus is on leveraging LLMs to directly solve combinatorial problems in a generalizable manner, enabling a versatile approach applicable across a wide range of complex tasks. ", "page_idx": 1}, {"type": "text", "text": "Prompting strategies. The expressive capabilities of direct prompting in Large Language Models are theoretically limited to the complexity class $\\mathsf{T}\\mathsf{C}^{0}$ [24]. To effectively address combinatorial problems with LLMs, sophisticated prompting strategies are required. One basic approach is the Chain-of-Thought (CoT) prompting, introduced in [40], which encourages LLMs to articulate intermediate \"thoughts\" that inform the generation of the final output. This technique has given rise to advanced variations, including Self-consistency with CoT (CoT-SC), Tree-of-Thoughts (ToT), and Graph-of-Thought methods [37, 42, 46]. Additionally, decomposition prompting strategies can be employed [48, 12], since they simplify complex tasks into smaller, manageable subtasks via symbolic programs or structured algorithms, thus improving the performance of LLMs. In our experiments, we found these techniques to be insufficient, leading us to propose the Self-Guiding Exploration method as a more effective solution for tackling combinatorial problem tasks. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We provide an overview of combinatorial problems, highlighting their inherent complexity with the classic example of the Traveling Salesman Problem (TSP) and an example of a combinatorial problem formulation in a prompt for use by a Large Language Model (LLM). ", "page_idx": 2}, {"type": "text", "text": "Combinatorial problems. Combinatorial problems involve decision-making processes where the goal is to assign binary decision variables $x\\in0,1$ in order to optimize a cost function $g(x_{1},...,x_{n})$ , subject to task-specific constraints. A classic example of such a problem is the TSP. In the TSP, given a list of $n$ cities and the distances $d_{i j}$ between cities $i,j$ , the objective is to determine the shortest possible route that visits each city exactly once and returns to the starting city. $x_{i j}$ is used as the action variable, indicating whether the route progresses from city $i$ to city $j$ . The cost function to minimize in TSP is $\\begin{array}{r}{g(x)\\overline{{=}}=\\sum_{i=1}^{n}\\sum_{j=1}^{n}d_{i j}\\dot{x}_{i j}}\\end{array}$ , under the condition that all cities visited exactly once $\\textstyle\\sum_{i=1}^{n}x_{i j}=1$ and $\\textstyle\\sum_{j=1}^{n}x_{i j}=1$ for all $i,j$ . Combinatorial problems are generally categorized as NP-hard due to their inherent computational complexity. For instance, a TSP with $n$ cities presents $(n\\!-\\!1)!$ possible routes, rendering the evaluation of all potential solutions impractical and exceedingly time-consuming as $n$ increases. ", "page_idx": 2}, {"type": "text", "text": "Prompting combinatorial problems in LLMs. To use LLM for solving CP tasks, we define $f$ as the interface function of a generative LLM model, which accepts high-dimensional discrete input tokens and generates outputs within the same token space $\\mathbf{\\boldsymbol{f}}:W\\mapsto W)$ . For each combinatorial problem task, the input $Q$ to the LLM can be explicitly defined in a textual format. This description delineates the specific goal alongside a list of variables tailored to the task at hand. For instance, the objective of the Traveling Salesman Problem (TSP) can be textually articulated as \"Find a route that minimizes the total travel distance, visits each city exactly once, and starts and ends in the same city.\" Subsequently, the variables, such as the distances between cities $d_{i j}$ , are provided in a format such as \"The distance between city $i$ and city $j$ is [number]\", laying out all necessary parameters for the model to process and generate solutions. The model will then process this structured input $Q$ to produce the corresponding solution answer $A$ , where both $Q$ and $A$ are within token space $W$ ; formally, $A=f(Q)$ . Given the inherent complexity of combinatorial problems, direct zero-shot prompting $f(Q)$ is insufficient. Consequently, we propose a self-guiding exploration algorithm that employs metaheuristic-like strategies to effectively solve CP tasks. ", "page_idx": 2}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce Self-Guiding Exploration (SGE) method and provide a detailed explanation of its algorithm designed to tackle combinatorial problems. The method (Fig 1), inspired by metaheuristic approaches, synthesizes multiple heuristic methods. It generates various thought trajectories, with each trajectory representing a specific heuristic approach. These trajectories are then integrated to form the final solution. To overcome the challenges of executing complex heuristics through LLMs in one step, our algorithm utilizes a decomposition strategy. This approach breaks down each trajectory into smaller, more manageable subtasks, enabling the solution to progress through sequential, simpler steps. This general-purpose algorithm is tailored to adapt to a wide range of combinatorial problems without the constraints of task-specific exemplars for few-shot solution generation. ", "page_idx": 2}, {"type": "image", "img_path": "BGOGknwHbi/tmp/4b60aabb28ad4d19dc5a54ffb281f3db0389ceac72db4723afe441c179e55d0c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: Self-Guiding Exploration. The generative model autonomously addresses a combinatorial problem task $Q$ through a five-phase process: (1) Exploration of $N$ solution trajectories, where each trajectory offers potential solutions; (2) Decomposition of these trajectories into $K$ subtasks, outlining specific steps for each method; (3) Resolution of each subtask, executing the outlined steps; (4) Feedback and Refinement, where feedback is gathered and used to refine each subtask; (5) Integration of all trajectories into a consolidated final solution $A$ . Distinct from traditional exploration/decomposition techniques, SGE(Q) functions entirely autonomously, eliminating the reliance on task-specific queries or manually created thought exemplars. This independence makes it universally applicable to all CP tasks without necessitating modifications. ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 Self-Guiding Exploration algorithm - $S G E(\\cdot)$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Require: query $Q$ , model $f$ , meta-prompts $Z$ , maximum recursion depth $D$ 1: $\\^{\\mathrm{\\Delta}}Q^{\\mathbb{N}}=\\dot{f}(\\dot{Q_{,}}\\,\\dot{Z}_{\\mathit{e x p l o r e}})$ \u25b7Explore method trajectories   \n2: for iteration $\\mathfrak{n}\\in{1,2,\\ldots,N}$ do 3: $Q_{\\mathbb{K}}^{n}=f(Q,Q^{n},Z_{d e c o m p})$ \u25b7Decompose trajectory subtasks 4: for iteration $\\mathbf{k}\\in{1,2,\\ldots,K}$ do 5: if $f(Q_{k}^{n},Z_{c h e c k})$ then $\\triangleright$ Check if subtask is simple 6: $T_{k}^{n^{\\cdot}}\\!=f(Q,T_{k-1}^{n},Q_{k}^{n})$ \u25b7Execute subtask and get thought 7: else 8: $T_{k}^{n}=S G E(T_{k-1}^{n}||Q_{k}^{n},f,Z)$ \u25b7Recursive call of subtask   \n9: end if   \n10: $\\begin{array}{r l}&{Q_{k_{f e e d b a c k}}^{n}=f(Q,Q_{k}^{n},T_{k}^{n},Z_{f e e d b a c k})}\\\\ &{T_{k}^{n}=f(Q,T_{k}^{n},Q_{k_{f e e d b a c k}}^{n})}\\end{array}$ \u25b7Get feedback query   \n11: \u25b7Refine thought   \n12: end for   \n13: end for   \n14: $A=f(Q,T_{K}^{1},...,T_{K}^{N},Z_{i n t e g r a t e})$ \u25b7Get answer ", "page_idx": 3}, {"type": "text", "text": "4.1 Algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The proposed method\u2019s algorithm is segmented into five distinct phases, as outlined in Algorithm 1. These phases include exploring thought trajectories, decomposing each trajectory into subtasks, resolving each subtask to generate thoughts, obtaining feedback and refining the thoughts, and finally integrating all thoughts to formulate the answer. Each thought here represents one completed subtask. ", "page_idx": 3}, {"type": "text", "text": "Exploration. During the exploration stage, the model tackles the overarching problem $Q$ by engaging with exploration meta-prompt. This prompt $Z_{e x p l o r e}$ is structured as: \"List all possible methods to solve this problem. Return them separated by new lines.\" This prompt stimulates the model to enumerate potential methodologies pertinent to $Q$ . The sequence is then divided into task-specific trajectories of queries $Q^{n}$ each incorporating a method to address $Q$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nQ^{\\mathbb{N}}=f(Q,Z_{e x p l o r e}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Decomposition. Following exploration, each trajectory query $Q^{n}$ is processed through the model to break down the trajectory into actionable steps. The decomposition meta-prompt $Z_{d e c o m p}$ is formulated as: \"List all steps to use the method. Return them separated by new lines.\" This leads to ", "page_idx": 3}, {"type": "text", "text": "the generation of subtask queries that operationalize the trajectory method: ", "page_idx": 4}, {"type": "equation", "text": "$$\nQ_{\\mathbb{K}}^{n}=f(Q,Q^{n},Z_{d e c o m p}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Subtask resolution. Post-decomposition, the subtask queries $Q_{\\mathbb{K}}^{n}$ are each split into $K$ individual queries and processed by the model to generate thoughts. The model initially evaluates if the task is easily solvable using the meta-prompt $Z_{c h e c k}$ : \"Is this problem easily solvable? Return yes or no\": $f(Q_{k}^{\\bar{n}},Z_{c h e c k})$ . If the response is affirmative, the model executes the subtask query $Q_{k}^{n}$ to generate a new thought: ", "page_idx": 4}, {"type": "equation", "text": "$$\nT_{k}^{n}=f(Q,T_{k-1}^{n},Q_{k}^{n}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Otherwise, the model engages a recursive instance of the self-guiding exploration algorithm on $Q_{k}^{n}$ instead of the main task $Q$ to navigate and decompose the complex subtask, producing: ", "page_idx": 4}, {"type": "equation", "text": "$$\nT_{k}^{n}=f(Q,T_{k}^{n},Q_{k_{f e e d b a c k}}^{n}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Feedback and refinement. In this stage, the model utilizes an additional meta-prompt $Z_{f e e d b a c k}$ - \"Give feedback to the proposed solution\" - to generate feedback queries Qknfeedback $f(Q,Q_{k}^{n},T_{k}^{n},Z_{f e e d b a c k})$ . This guides the model in refining the initial responses through reevaluation and enhancement of the thoughts: ", "page_idx": 4}, {"type": "equation", "text": "$$\nT_{k}^{n}=f(Q,T_{k}^{n},Q_{k_{f e e d b a c k}}^{n}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Integration. Upon completion of all trajectories and their associated subtasks, the model employs a final meta-prompt $Z_{i n t e g r a t e}$ - \"Integrate all previous findings and provide the final answer\" - to amalgamate the last thoughts into a definitive solution answer: ", "page_idx": 4}, {"type": "equation", "text": "$$\nA=f(Q,T_{K}^{1},...T_{K}^{N},Z_{i n t e g r a t e}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "SGE draws inspiration from metaheuristic methods used to solve combinatorial problem tasks. Yet, due to its general-purpose nature and meta-prompts, it is also suitable for other tasks, beyond CPs. Essentially, it integrates elements of exploration-of-thought, decomposition, and refinement prompting strategies, but it does so without relying on task-specific prompts or solution exemplars. For additional information on these prompting strategies, see Section A.1 ", "page_idx": 4}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section details the experimental setup and presents the results of our proposed method applied to combinatorial problem tasks, as well as its performance on other reasoning tasks commonly explored in LLM research. ", "page_idx": 4}, {"type": "text", "text": "5.1 Setup", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "CP tasks. The experiments were conducted on six combinatorial tasks: Assignment Problem, Knapsack Problem, Bin Packing Problem, Traveling Salesman Problem, Vehicle Routing Problem, and Job Scheduling Problem. The Assignment Problem, classified as P-complete, can be optimally solved using the Hungarian Algorithm. In contrast, the other tasks are NP-hard and ordered by increasing complexity. For a more detailed discussion of these CP tasks, refer to Section A.2. We included five distinct problem sizes, involving 5, 10, 15, 20, and 30 elements (nodes) such as cities in the TSP/VRP. To facilitate these experiments, a dataset was created, comprising 100 randomly generated instances for each problem size. These instances were characterized by uniformly distributed variables, such as the positioning of cities in TSP/VRP or bin volume in the Bin Packing Problem, over an interval from 0 to 100. The experiments utilized an NVIDIA A100 SXM 40GB GPU, paired with two AMD EPYC 7742 CPUs (8 cores each) and 256GB RAM. Our implementation is available online.1 ", "page_idx": 4}, {"type": "table", "img_path": "BGOGknwHbi/tmp/152301e64961cdb15a2d1b09daf8046b008038202b86955979dc3417e106d84f.jpg", "table_caption": ["Table 1: Percentage performance improvement compared to IO on CP tasks using GPT-4 and Gemini-1.5 models. CoT uses majority voting, with the number of candidates equal to the number of thoughts produced by SGE. The metrics is quantified as percentage improvement in cost with respect to IO solution (the bigger it is the better). "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Baselines. We utilized four baseline prompting methods: Input-Output (IO) Direct Prompting, Chain-of-Thought Prompting, Self-Refine (Refine) Prompting, and Decomposition Prompting. The Input-Output (IO) approach involves a single prompt where the model is asked to provide a solution directly, without complex prompting. In this approach, we generate $N$ sample candidates by repeatedly prompting the model with the same query $Q$ , $N$ times. The responses are then aggregated through majority voting to identify the most common solution among the $N$ outputs. We employ the Self-Refine (Refine) method [19], which includes a feedback-refinement procedure that aligns closely with phase four of SGE. Additionally, we use the zero-shot Chain-of-Thought method [13], which is the basic technique among Exploration-of-Thought methods. Lastly, we implemented the Decomposition method as described in [49]. In our experiments, these baseline methods were tested across a range of five LLM models including GPT-4, GPT-3.5 by OpenAI, Gemini-1.5 by Google, and the Llama-2 series from Meta, which includes models with 70 billion and 7 billion parameters. We did not include prompting methods previously used in [16, 23, 41], as their prompting strategies showed inferior results compared to the zero-shot Chain-of-Thought approach when tested with our data. ", "page_idx": 5}, {"type": "text", "text": "Metrics. In our study, each method\u2019s performance is evaluated relative to IO (Input-Output) pprroobmlpetimn tga. sTko  uqsuinangt iIfOy  tphreo immpptrionvg e(me.egn.t,,  fwore  tfhires t TmSePa, $\\begin{array}{r}{g_{i o}=\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\bar{d_{i j}}x_{i j})}\\end{array}$ $g_{i o}$ r.  eaWceh  tchoemn bcianlactuolraitael the cost $g_{m e t h o d}$ using alternative methods. The percentage improvement is computed as $\\frac{g_{i o}\\!-\\!g_{m e t h o d}}{g_{i o}}$ .o lFvoerr  ptrhorboluegmhs  ao fb srumtael lfeorr sciez easp, pwreo aacrhe .a bIlne  stou cohb tianisnt aonpctiesm,a lw seo lmuteioasnus rues itnhge  tchoes tG oofo gtlheeoptimal solution ${g}_{o p t}$ and determine the optimality gap as $\\begin{array}{r}{100\\times\\frac{g_{m e t h o d}-g_{o p t}}{g_{o p t}}}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "5.2 Results on CP tasks ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To evaluate the general performance of SGE on combinatorial problems, we conducted experiments comparing performance improvement to IO of SGE and CoT, Decomposition, and Refinement baselines using GPT-4 and Gemini-1.5 LLM models. Table 1 gives the results on six combinatorial problem tasks. The results analysis shows that the SGE method consistently outperforms CoT, Refine, and Decomposition methods across all tasks. Notably, the magnitude of improvement escalates with the increasing complexity of the problems, from polynomial to exponential. The margin with the second-best method, Decomposition, ranges from $\\bar{7}.5\\bar{3}\\%$ for the Assignment Problem to $37.13\\%$ for the JSP. This trend suggests that the IO method may struggle with the computational demands of NP-hard problems, where more sophisticated strategies like SGE provide significant advantages. For a comprehensive view of all experimental results, see Section A.4. ", "page_idx": 5}, {"type": "text", "text": "To qualitatively evaluate the performance of the Exploration, Decomposition, and Refinement phases of SGE method, we assessed the LLM outputs for each phase across five random instances of each combinatorial problem task. Figure 2 illustrates an example of how our method addresses the TSP, showcasing outputs during each of the three phases of SGE. In the Exploration phase, the first box of Figure 2 displays how LLM $f$ generates a list of potential algorithms suitable for solving the TSP, such as heuristic approaches like Nearest Neighbor, metaheuristic techniques like Ant Colony, and Mixed Integer Linear Programming (MILP) method. This phase adapts to different combinatorial problems by suggesting tailored algorithms, like the Hungarian algorithm for the Assignment problem, Greedy algorithms for the Knapsack problem, and Clustering methods for the Vehicle Routing Problem (VRP). Each list of algorithms forms the foundation for generating diverse candidate solutions tailored to each specific problem. The Decomposition phase, depicted in the second box of Figure 2, breaks down each identified algorithm into specific subtasks. This example shows the decomposition of the Nearest Neighbor algorithm for the TSP, where the initial subtasks are simple enough for direct processing by model $f$ . However, more complex tasks, such as loops, undergo further decomposition using SGE in a recursive manner, with computational or programming tasks being handled using Python within models like GPT-4 and Gemini-1.5 equipped with a Code Interpreter. Finally, the Refinement phase, illustrated in the third box of Figure 2, focuses on enhancing the candidate solutions developed in the previous stage. This example pertains to refining a solution derived from the Nearest Neighbor algorithm for the TSP by implementing the 2-opt algorithm. Renowned for its effectiveness in TSP and VRP contexts, the 2-opt algorithm optimizes the initial solution to find locally optimal solutions within a specific neighborhood, thus improving the overall quality of the candidate solutions. This example shows that SGE method adapts its approach to suit different combinatorial problems, finding a special set of heuristics for each task. ", "page_idx": 5}, {"type": "image", "img_path": "BGOGknwHbi/tmp/bdf4b4fb8bc2c24aa35e5777b2ef6f1c7f46d7a8195fbbefa1e3889e51fbf431.jpg", "img_caption": ["Figure 2: Example of SGE inference across the Exploration, Decomposition, and Refinement phases for the Traveling Salesman Problem. The figure displays three boxes, each illustrating the prompt structure and corresponding example output for each phase. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Effect of problem size on SGE performance. To evaluate the effect of problem size on SGE performance, we conducted experiments on all six tasks with input sizes of 5, 8, 12, 15, and 20 nodes using the GPT-4 model. Figure 3 gives the results of these experiments. The results analysis shows that generally, an increase in problem complexity, as determined by the size of the problem input, negatively influences performance improvement; larger problem sizes result in diminished performance improvement of the SGE method compared to IO. Specifically, tasks with 20 input nodes consistently exhibit lower performance improvements relative to the IO method than tasks with 5 input nodes. However, when comparing less disparate sizes, such as 8 and 12 nodes, the differential impact on performance is less pronounced and can occasionally be positive. ", "page_idx": 6}, {"type": "text", "text": "Table 2: Optimality gap of prompting methods using LLaMA-2-70B. The results are represented as performance percentage difference compared to optimal solutions (the smaller it is, the better). ", "page_idx": 7}, {"type": "table", "img_path": "BGOGknwHbi/tmp/0bc0f7426aec8bda1c883f4e394e0f517ef257fc433b4beedabc6bffa452d7ac.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "BGOGknwHbi/tmp/2c487edba5cb3592920024649ab1af5bd2ee565e0e69b0b7c32d52647a83c0b1.jpg", "img_caption": ["Figure 3: Effect of Problem Size on Performance Improvement relative to the IO solution using $\\tt g p t-4$ w/ code interpreter. The analysis spans problem instances of varying sizes, systematically presented from the smallest to the largest, specifically ranging from $n=5$ to $n=20$ nodes. Results are organized to highlight the impact of increasing problem complexity on the effectiveness of the solution. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "BGOGknwHbi/tmp/5decc0b2c5eefe70d09ac303a4a9b84ff3665756a6eb98e0d78ae90fa766acae.jpg", "img_caption": ["Figure 4: Effect of Model Choice on Performance Improvement relative to the IO solution. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "BGOGknwHbi/tmp/2c9715eea6059484b0b2e7fe1739b86637a0943b2fcf95b4c92ec049954ecb32.jpg", "table_caption": ["Table 3: Results for reasoning tasks using gpt-4 with a code interpreter are presented as accuracies on benchmark test sets. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "BGOGknwHbi/tmp/86618279046fbdea97d77cc463ff384485a59ad74e6157ed29a757a4ca7c8689.jpg", "table_caption": ["Table 4: Performance vs efficiency of prompting methods using gpt-4 w/ code interpreter. The results are represented as performance percentage improvement compared to IO solution and number of model $f$ calls. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Gap between SGE performance and the global optimum. To evaluate the gap between SGE solutions and global optimum solutions, we conducted experiments on small problem sizes involving 5, 8, and 12 nodes, utilizing the brute force method via Google-OR-Tools to determine the global optimum. Figure 2 provides the results of these experiments in terms of the percentage gap between the performance of SGE and optimal solutions. The results analysis shows that generally, the SGE method exhibits a smaller optimality gap across all tasks when compared to baseline methods. This advantage is particularly pronounced for more complex problems such as Bin Packing, TSP, VRP, and JSP. For instance, SGE achieves a $12.16\\%$ smaller optimality gap on the Assignment task than the next best Decomposition method and a $34.85\\%$ smaller gap on the JSP. ", "page_idx": 8}, {"type": "text", "text": "Effect of LLM selection on SGE performance. To evaluate the impact of model selection on SGE performance, we conducted experiments comparing different models, including GPT-4, Gemini-1.5, GPT-3.5, Llama-2-70b, and Llama-2-7b models. Figure 4 provides the results of these experiments. The results analysis shows that GPT-4 and Gemini-1.5 demonstrate significantly better performance compared to other models across all tasks. A notable feature of both models is the integration of a Code Interpreter (CI) tool, which appears crucial for combinatorial problem tasks as it enables the models to execute generated code and evaluate solution performance directly. In contrast, models lacking a CI tool, such as GPT-3.5, Llama-2-70b, and Llama-2-7b, exhibit poorer outcomes, with GPT-3.5 slightly outperforming the Llama models. The comparison between Llama models indicates that the size of the model with 70 billion versus 7 billion parameters does not significantly influence performance. This suggests that model size alone does not guarantee substantial performance improvements in combinatorial tasks. ", "page_idx": 8}, {"type": "text", "text": "Trade-off between performance and cost in SGE. To evaluate the cost-effectiveness of different methods, we conducted experiments to compare the average performance improvement per method against the average number of LLM calls utilized to solve each combinatorial problem instance. Table 4 gives the results of these experiments, where the number of function calls in CoT and Refine methods was explicitly controlled to make them equal to SGE function calls. The results analysis shows that the SGE method achieves a $27.84\\%$ better performance compared to the Decomposition method but requires $87.89\\%$ more function calls. Thus, while SGE offers superior performance, it does so at a marginally higher operational cost. Therefore, the application of this method is particularly justified in scenarios where performance gains are prioritized over cost efficiency. ", "page_idx": 8}, {"type": "text", "text": "5.3 Results on reasoning tasks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To evaluate the versatility of SGE in handling different types of tasks, we conducted experiments across eight datasets commonly referenced in LLM research, categorized into three distinct task types: arithmetic, commonsense reasoning, and symbolic reasoning. Table 3 gives the results of these experiments, with each dataset comprising train and test splits where SGE and baseline methods were applied to the test splits. The results analysis shows that the SGE method demonstrates incremental but consistently superior performance across all task categories. Notably, the method shows particular strength in arithmetic tasks, where it achieves an average improvement of $4.83\\%$ , compared to $1.24\\%$ in commonsense reasoning tasks and $1.32\\%$ in symbolic reasoning tasks. This demonstrates the method\u2019s applicability and effectiveness across a diverse range of tasks, extending beyond combinatorial problems. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This study has explored the application of Large Language Models to combinatorial problems, a category of tasks known for their NP-hardness. Our research introduced a \u2019Self-Guiding Exploration\u2019 prompting strategy that effectively utilizes the inherent strengths of LLMs. By generating multiple thought trajectories tailored to various CPs and autonomously decomposing them into manageable subtasks. Our findings confirm that SGE outperforms existing strategies, improving optimization performance by $27.84\\%$ and achieving a $2.46\\%$ higher accuracy in reasoning tasks. Notably, SGE shows a $34.85\\%$ smaller gap with the global optimum on complex tasks like the Job Sheduling Problem compared to baseline methods. These results underline the potential of advanced LLM strategies in complex problem-solving scenarios, suggesting that the right techniques can enhance the utility of LLMs in critical logistics and resource management applications. ", "page_idx": 9}, {"type": "text", "text": "Despite the performance improvements demonstrated by the SGE, several limitations have emerged that merit attention. Firstly, SGE performance depends on the choice of language model. Secondly, the operational costs associated with SGE are notably higher; it requires $87.89\\%$ more function calls than the Decomposition method. These issues present clear avenues for future research. Enhancing SGE\u2019s computational efficiency while maintaining its high performance could broaden its applicability and make it a more practical choice for a wider range of problems. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Samuel Kwame Amponsah, Dominic Otoo, Said Salhi, and Ebenezer Quayson. Proposed heuristic method for solving assignment problems. American Journal of Operations Research, 06:436\u2013441, 01 2016. doi: 10.4236/ajor.2016.66040. ", "page_idx": 10}, {"type": "text", "text": "[2] M Emin Aydin and Ercan \u00d6ztemel. Dynamic job-shop scheduling using reinforcement learning agents. Robotics and Autonomous Systems, 33(2-3):169\u2013178, 2000. ", "page_idx": 10}, {"type": "text", "text": "[3] Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016. ", "page_idx": 10}, {"type": "text", "text": "[4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6- 12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. ", "page_idx": 10}, {"type": "text", "text": "[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1\u2013113, 2023. URL http://jmlr.org/papers/v24/22-1144.html. [6] Michel Deudon, Pierre Cournut, Alexandre Lacoste, Yossiri Adulyasak, and Louis-Martin Rousseau. Learning heuristics for the tsp by policy gradient. In International conference on the integration of constraint programming, artificial intelligence, and operations research, pages 170\u2013181. Springer, 2018. [7] Thomas Gabel and Martin Riedmiller. Adaptive reactive job-shop scheduling with reinforcement learning agents. International Journal of Information Technology and Intelligent Computing, 24(4):14\u201318, 2008. [8] Rajeev Kumar Goel, Raman Maini, and Sandhya Bansal. Vehicle routing problem with time windows having stochastic customers demands and stochastic service times: Modelling and solution. J. Comput. Sci., 34:1\u201310, 2019. [9] Google. Or-tools, 2023. URL https://developers.google.com/optimization.   \n[10] Zangir Iklassov, Dmitrii Medvedev, Ruben Solozabal Ochoa de Retana, and Martin Tak\u00e1c. On the study of curriculum learning for inferring dispatching policies on the job shop scheduling. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao, SAR, China, pages 5350\u20135358. ijcai.org, 2023. doi: 10.24963/IJCAI.2023/594. URL https://doi.org/10.24963/ijcai.2023/594.   \n[11] Zangir Iklassov, Ikboljon Sobirov, Ruben Solozabal, and Martin Tak\u00e1c. Reinforcement learning approach to stochastic vehicle routing problem with correlated demands. IEEE Access, 11: 87958\u201387969, 2023. doi: 10.1109/ACCESS.2023.3306076. URL https://doi.org/10. 1109/ACCESS.2023.3306076.   \n[12] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406, 2022.   \n[13] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html.   \n[14] Wouter Kool, Herke Van Hoof, and Max Welling. Attention, learn to solve routing problems! arXiv preprint arXiv:1803.08475, 2018.   \n[15] Guoming Li and Junhua Li. An improved tabu search algorithm for the stochastic vehicle routing problem with soft time windows. IEEE Access, 8:158115\u2013158124, 2020.   \n[16] Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang. Evolution of heuristics: Towards efficient automatic algorithm design using large language mode, 2024.   \n[17] Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang. Evolution of heuristics: Towards efficient automatic algorithm design using large language model, 2024. URL https://arxiv.org/abs/2401.02051.   \n[18] Shengcai Liu, Caishun Chen, Xinghua Qu, Ke Tang, and Yew-Soon Ong. Large language models as evolutionary optimizers, 2024. URL https://arxiv.org/abs/2310.19046.   \n[19] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. ArXiv preprint, abs/2303.17651, 2023. URL https://arxiv. org/abs/2303.17651.   \n[20] Sridhar Mahadevan and Georgios Theocharous. Optimizing production manufacturing using reinforcement learning. In FLAIRS conference, volume 372, page 377, 1998.   \n[21] Sridhar Mahadevan, Nicholas Marchalleck, Tapas K Das, and Abhijit Gosavi. Self-improving factory simulation using continuous-time average-reward reinforcement learning. In Machine Learning Interantional Workshop, pages 202\u2013210, 1997.   \n[22] Hongzi Mao, Malte Schwarzkopf, Shaileshh Bojja Venkatakrishnan, Zili Meng, and Mohammad Alizadeh. Learning scheduling algorithms for data processing clusters. In Proceedings of the ACM special interest group on data communication, pages 270\u2013288. 2019.   \n[23] Mahmoud Masoud, Ahmed Abdelhay, and Mohammed Elhenawy. Exploring combinatorial problem solving with large language models: A case study on the travelling salesman problem using gpt-3.5 turbo, 2024.   \n[24] William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision transformers. Transactions of the Association for Computational Linguistics, 11:531\u2013545, 2023. doi: 10.1162/tacl_a_00562. URL https://aclanthology.org/2023.tacl-1.31.   \n[25] Mohammadreza Nazari, Afshin Oroojlooy, Lawrence V Snyder, and Martin Tak\u00e1c\u02c7. Reinforcement learning for solving the vehicle routing problem. In Conference on Neural Information Processing Systems, NeurIPS 2018, 2018.   \n[26] Afshin Oroojlooyjadid, Lawrence V Snyder, and Martin Tak\u00e1c\u02c7. Applying deep learning to the newsvendor problem. IISE Transactions, 52(4):444\u2013463, 2020.   \n[27] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[28] Tantikorn Pichpibul and Ruengsak Kawtummachai. A heuristic approach based on clarke-wright algorithm for open vehicle routing problem. The Scientific World Journal, 2013, 2013. ", "page_idx": 12}, {"type": "text", "text": "[29] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan S. Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, Alhussein Fawzi, Josh Grochow, Andrea Lodi, Jean-Baptiste Mouret, Talia Ringer, and Tao Yu. Mathematical discoveries from program search with large language models. Nature, 625:468 \u2013 475, 2023. URL https://api.semanticscholar. org/CorpusID:266223700.   \n[30] Veronique Sels, Nele Gheysen, and Mario Vanhoucke. A comparison of priority rules for the job shop scheduling problem under different flow time-and tardiness-related objective functions. International Journal of Production Research, 50(15):4255\u20134270, 2012.   \n[31] Penghao Sun, Zehua Guo, Junchao Wang, Junfei Li, Julong Lan, and Yuxiang Hu. Deepweave: Accelerating job completion time with deep reinforcement learning-based coflow scheduling. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, pages 3314\u20133320, 2021.   \n[32] Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed Chi, Denny Zhou, and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 13003\u201313051. Association for Computational Linguistics, 2023. doi: 10.18653/v1/2023.findings-acl.824. URL https: //doi.org/10.18653/v1/2023.findings-acl.824.   \n[33] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam M. Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, Yaguang Li, Hongrae Lee, Huaixiu Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, I. A. Krivokon, Willard James Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Hartz S\u00f8raker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark D\u00edaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravindran Rajakumar, Alena Butryna, Matthew Lamm, V. O. Kuzmina, Joseph Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Huai hsin Chi, and Quoc Le. Lamda: Language models for dialog applications. ArXiv preprint, abs/2201.08239, 2022. URL https://arxiv.org/abs/2201.08239.   \n[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pages 5998\u20136008, 2017.   \n[35] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. Advances in neural information processing systems, 28, 2015.   \n[36] Libing Wang, Xin Hu, Yin Wang, Sujie Xu, Shijun Ma, Kexin Yang, Zhijun Liu, and Weidong Wang. Dynamic job-shop scheduling in smart manufacturing using deep reinforcement learning. Computer Networks, 190:107969, 2021.   \n[37] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.   \n[38] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https: //openreview.net/forum?id=yzkSU5zdwD. Survey Certification.   \n[39] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 24824\u201324837. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/ 2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf.   \n[40] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022.   \n[41] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers, 2024.   \n[42] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Grifftihs, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.   \n[43] Haoran Ye, Jiarui Wang, Zhiguang Cao, Federico Berto, Chuanbo Hua, Haeyeon Kim, Jinkyoo Park, and Guojie Song. Reevo: Large language models as hyper-heuristics with reflective evolution, 2024. URL https://arxiv.org/abs/2402.01145.   \n[44] Di Zhang, Dong Dai, Youbiao He, Forrest Sheng Bao, and Bing Xie. Rlscheduler: an automated hpc batch job scheduler using reinforcement learning. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1\u201315. IEEE, 2020.   \n[45] Wei Zhang and Thomas G Dietterich. A reinforcement learning approach to job-shop scheduling. In IJCAI, volume 95, pages 1114\u20131120. Citeseer, 1995.   \n[46] Yizhou Zhang, Lun Du, Defu Cao, Qiang Fu, and Yan Liu. Prompting with divide-and-conquer program makes large language models discerning to hallucination and deception, 2024.   \n[47] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id $\\equiv$ 5NTt8GFjUHkr.   \n[48] Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.   \n[49] Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id $\\cdot$ WZH7099tgfM. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Baseline prompting techniques ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Combinatorial problems are too complex for solving using direct approach. To solve in several shots, different methods can be used such as few-shot prompting, chain-of-thought, explorationof-thought, decomposition, and self-refine advanced prompting techniques. Traditional few-shot prompting involves teaching an LLM to derive an answer $A$ to a query $Q$ using a limited set of contextual examples $D\\,=\\,\\bar{\\{}E_{1},...,E_{|D|}\\}$ , where $A\\,=\\,f(Q,D)$ . In the simplest few-shot setup, examples are formatted as $E_{j}\\ =\\ (Q_{j},A_{j})$ , where each $Q_{j},A_{j}$ are corresponding prompt and solution of example problem $j$ . For Chain of Thought (CoT) prompting, the objective shifts to generating a sequence of intermediate reasoning steps, or \"thoughts\" $T$ , and subsequently deriving the final answer from $T$ . These in-context examples are structured as $E_{j}=(Q_{j},(\\bar{T}_{j,1},\\bar{\\dots},T_{j,k}),\\bar{A}_{j})$ . Exploration-of-Thought techniques, such as those described in [37, 42], focus on dividing the problem $Q$ into $K$ subproblems and auto-generating thoughts $T_{k}$ through subproblem queries $Q_{k}$ with $T_{k}=f(Q_{k},T_{k-1})$ .The ultimate answer is then computed as $A=f(Q,T_{K})$ . In addition to that, in Tree-of-Thought (ToT) and Graph-of-Thought (GoT) prompting strategies, the problem is split into $N$ thought trajectories $T_{k}^{n}$ , using pre-determined subqueries $Q_{k}^{n}$ , with the final answer determined by $A=f(Q,T_{K}^{1},...,T_{K}^{N})$ . These strategies necessitate manually identifying effective subqueries $Q_{k}^{n}$ for each specific task (e.g., arithmetic, commonsense, symbolic). Decomposition prompting strategies, as referenced in [48, 12] get $Q_{k}^{n}$ subqueries through in-context examples formatted similarly to CoT exemplars: $E_{j}=\\left((Q_{j},(Q_{j,1},T_{j,1}),...,(Q_{j,k_{j}},T_{j,k_{j}}))A_{j}\\right)$ . Another prompting method called self-refinement [19] uses feedback and refine procedures to generate feedback queries Qknfeedback and subsequently refine thoughts $T_{k}^{n}$ . In our research we combined these advanced techniques and updated them into new method called self-guiding exploration to enhance performance of LLMs for CPs. ", "page_idx": 14}, {"type": "text", "text": "A.2 Combinatorial problems ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Combinatorial problems are decision problems where solver needs to assign binary decision variable $x\\in\\{0,1\\}$ to minimize some cost function or maximize reward function given input $C$ . ", "page_idx": 14}, {"type": "text", "text": "Assignment problem. Despite not being classified as NP-hard and solvable in polynomial time using the Hungarian algorithm, the Assignment Problem remains a fundamental combinatorial challenge. This problem entails optimally assigning $n$ tasks to $n$ workers, aiming to minimize the total cost or maximize the total efficiency of the assignments. The input array $C$ is represented as $n\\times n$ cost matrix, where the element at the $i^{t h}$ row and $j^{t h}$ column represents the cost of assigning the $j^{t h}$ task to the $i^{t h}$ worker. The goal is essentially about finding a one-to-one matching between workers and tasks with the objective of minimizing the total cost, i.e. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{min}_{\\boldsymbol{g}(\\boldsymbol{x})}\\,\\boldsymbol{g}(\\boldsymbol{x})=\\sum_{i=1}^{n}\\sum_{j=1}^{n}c_{i j}x_{i j},}&{}\\\\ {\\mathrm{s.t.}\\displaystyle\\sum_{i=1}^{n}x_{i j}=1;}&{\\displaystyle\\sum_{j=1}^{n}x_{i j}=1;\\quad x_{i j}\\in\\{0,1\\}\\quad\\forall i,j.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Knapsack problem. The knapsack problem is a classic combinatorial problem, focusing on resource allocation. It is a decision problem in which the goal is to pack items from a set of items with given weights $w$ and values $v$ into a container with a maximum capacity $W$ . The input array $C$ is represented as $n\\times2$ matrix with volume and value information of every item $i$ . The goal is to maximize total value of packed items without exceeding container capacity, i.e. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{max}_{\\theta\\rightarrow\\atop{i=1}}\\sum_{i=1}^{n}v_{i}x_{i},}&{}\\\\ {\\mathrm{s.t.}\\displaystyle\\sum_{i=1}^{n}w_{i}x_{i}\\leq W;\\quad x_{i}\\in\\{0,1\\}\\quad\\forall i.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Bin packing problem. The bin packing problem is a combinatorial problem that involves efficiently packing $n$ objects of different sizes $w$ into a finite number of $k$ bin containers of fixed capacity $W$ in a way that minimizes the number of bins used. The input array $C$ is represented as $n\\times1$ vector with size information of every item $i$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{min}_{g\\left(x\\right)=\\sum_{j=1}^{k}x_{j}},}\\\\ {\\displaystyle\\quad\\quad\\operatorname*{s.t.}_{i=1}^{n}x_{i j}=1;\\quad\\sum_{i=1}^{n}w_{i}x_{i j}\\leq W;\\quad x_{i j}\\in\\{0,1\\}\\quad\\forall i,j.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Travelling salesman problem. In the travelling salesman problem, given a list of $n$ cities and the distances $d$ between each pair of cities, what is the shortest possible route that visits each city exactly once and returns to the origin city. The input array $C$ is represented as $n\\times n$ cost matrix, where the element at the $i^{t h}$ row and $j^{t h}$ column represents the cost $d_{i j}$ of travelling between these two cities, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{min}_{\\boldsymbol{\\theta}(\\boldsymbol{x})}\\,g(\\boldsymbol{x})=\\sum_{i=1}^{n}\\sum_{j=1}^{n}d_{i j}x_{i j},}&{}\\\\ {\\mathrm{s.t.}\\displaystyle\\sum_{i=1}^{n}x_{i j}=1;}&{\\displaystyle\\sum_{j=1}^{n}x_{i j}=1;\\quad x_{i j}\\in\\{0,1\\}\\quad\\forall i,j.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Vehicle routing problem. The vehicle routing problem generalizes the TSP by incorporating multiple vehicles into the route planning. The VRP seeks to determine the optimal set of routes for a fleet of $K$ vehicles with maximum capacity $P$ to deliver goods to $n$ customers, typically from a central depot, with the objective of minimizing the total travel cost. In addition to TSP, the input $C$ also includes the demand of each customer, i.e. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{min}_{g\\left(x\\right)=\\sum_{i=1}^{n}\\sum_{j=1}^{n}d_{i j}x_{i j k}},}&{}\\\\ {\\displaystyle\\operatorname*{st.}\\sum_{i=1}^{n}x_{i j k}=1;}&{\\displaystyle\\sum_{j=1}^{n}x_{i j k}=1;\\quad x_{i j k}\\in\\{0,1\\}\\quad\\forall i,j,k.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In cases when the capacity $P$ is less than the customer\u2019s demand, the vehicle will go between the depot and the customer until the demand is satisfied. ", "page_idx": 15}, {"type": "text", "text": "Job scheduling problem. The job scheduling problem focuses on scheduling $n$ jobs on $m$ machines, where each job $i$ consists of a sequence of $m$ operations that need to be processed in a specified order. Each operation requires a specific machine for a certain period of time, and each machine can handle only one operation at a time. The input array $C$ is represented as $n\\times m\\times2$ cost matrix, which stores machine id and completion time for every operation $j$ of every job $i$ . The primary objective is to minimize the makespan, which is the total time required to complete all jobs, effectively reducing the time from the start of the first operation to the completion of the last operation across all jobs. ", "page_idx": 15}, {"type": "text", "text": "Challenges in solving combinatorial problems. The difficulty of the combinatorial problems like TSP, VRP, JSP, and Knapsack problem stems from several intrinsic and mathematical characteristics common among them. These problems are typically classified as NP-hard, which fundamentally contributes to their computational complexity. As the number of elements (cities, jobs, items, etc.) increases, the number of possible combinations or permutations explodes exponentially. For instance, the TSP with $n$ cities has $(n-1)!$ possible routes to evaluate. This exponential growth means that the time required to examine all possible solutions becomes impractically long even for relatively small $n$ . In addition to that combinatorial problems involve decisions that are interdependent, where the choice for one element affects the options and costs for others. For example, in the JSP, the order in which jobs are processed on one machine can affect the scheduling for other machines. ", "page_idx": 15}, {"type": "text", "text": "Table 5: Percentage Performance Improvement Compared to IO Prompting on Job Scheduling Problem. Columns Show the Number of $n$ Jobs and $m$ Machines. ", "page_idx": 16}, {"type": "table", "img_path": "BGOGknwHbi/tmp/dcdfc8fbf3162f5937b429e3f6767bf95cf2143d1232ce1adbac8689dae982a1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 6: Percentage Performance Improvement Compared to IO Prompting on Vehicle Routing Problem. Columns Show the Number of Nodes. ", "page_idx": 16}, {"type": "table", "img_path": "BGOGknwHbi/tmp/efaeff43ec68f1e81ff32d154a4e03ed39f47d144ba61506040ee13438e19106.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Applicability of SGE to combinatorial problems. For combinatorial problems, obtaining exact solutions is generally computationally prohibitive. Consequently, approximation algorithms and heuristic methods are frequently employed to tackle these challenges. Typically, these problems are broken down into more manageable subproblems. Initial solutions are then generated using heuristic functions, which are subsequently refined through additional heuristic techniques to enhance solution quality. The SGE approach is particularly well-suited for combinatorial problems as it embodies this multi-stage process: it systematically decomposes the problem, explores potential solving methods, and iteratively refines the solutions. ", "page_idx": 16}, {"type": "text", "text": "A.3 Additional experiments and results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To further strengthen our results and provide a comprehensive comparison of our method with well-established solvers and heuristics, we conducted additional experiments on larger instances of combinatorial problems. Specifically, we included experiments with Job Shop Scheduling Problems (Table 5) and Vehicle Routing Problems (Table 6), comparing our approach against well-known solvers and heuristics. The goal was to evaluate the scalability of our method and assess its performance relative to commonly used combinatorial optimization methods. ", "page_idx": 16}, {"type": "text", "text": "Recognizing the need for understanding computational overhead, we have also provided an analysis of the computational cost involved in running our algorithm (Table 7). We have introduced a new baseline using large language models (LLMs) for heuristic generation, which is included in Table 8. We used the general version of EoH [17] (originally used for the Bin Packing problem). Specifically, in our case, for the Job Shop Scheduling task, EoH generated heuristics that scored the job nodes, and the algorithm then selected the job with the highest score as the next one in the schedule. However, we found that for more complex tasks than Bin Packing (e.g., TSP), EoH is better employed with Guided Local Search, where simple heuristics like swapping are used for local optimization, and EoH identifies a heuristic that can disturb the local optimum to explore a better region of the solution space. We believe that EoH built in this way, developing specific heuristic programs for each problem instance, would likely perform similarly to Google OR-Tools and achieve better performance. This approach effectively works as a metaheuristic enhancer, providing state-of-the-art results as seen in [17]. ", "page_idx": 16}, {"type": "table", "img_path": "BGOGknwHbi/tmp/1de305aa50c1b540e569392f243d554e14d0cd220cbecba25562f46c75bbc892.jpg", "table_caption": ["Table 7: VRP Average Total Cost. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 8: Percentage Performance Improvement Compared to IO Prompting on Job Scheduling Problem with New Baseline. Columns Show the Number of Nodes. ", "page_idx": 17}, {"type": "table", "img_path": "BGOGknwHbi/tmp/e0b121e370c2cd77f42c321de981ad090f8ed60255865ce1886f121ac4907d1c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 9: Effect of problem size on $S G E$ performance using gpt-4 w/ code interpreter. The results are represented as performance percentage improvement compared to IO solution (the bigger it is the better). ", "page_idx": 17}, {"type": "table", "img_path": "BGOGknwHbi/tmp/d56039345ccc09dfadb38163a864a1ab9363e5bb4495961e4c40621ec24f3621.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "BGOGknwHbi/tmp/0939d507865fba09bc66baba9704f23d75cdcf2ff63a474e148ffee60758e3f1.jpg", "table_caption": ["Table 10: Effect of model selection on $S G E$ performance using gpt-4 w/ code interpreter. The results are represented as performance percentage improvement compared to IO solution (the bigger it is the better). "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "BGOGknwHbi/tmp/fbb6f659c214254184bee687838171b5981009c53550e8de436198170aed7677.jpg", "table_caption": ["Table 11: Results for reasoning tasks using gpt-3.5 with a code interpreter are presented as accuracies on benchmark test sets. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 12: Comparison of various combinatorial problems based on their average cost per problem size, using gpt-3.5. The Knapsack problem aims to maximize returns, while other problems focus on minimizing costs. ", "page_idx": 18}, {"type": "table", "img_path": "BGOGknwHbi/tmp/27141bc0af492fc85d798e615dcb961d5c65542e421cacce4183d0771a87a152.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "BGOGknwHbi/tmp/0d39d7b8fed687ddf382280e6bcf2cadec24196c3d7e0af545a04fd67544afa2.jpg", "table_caption": ["Table 13: Comparison of various combinatorial problems based on their average cost per problem size, using gpt-4. The Knapsack problem aims to maximize returns, while other problems focus on minimizing costs. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "BGOGknwHbi/tmp/0a6b8cbf042757a05f372dab56af3137cfb664b34860e77a7f81911e7b5485e6.jpg", "table_caption": ["Table 14: Comparison of various combinatorial problems based on their average cost per problem size, using gemini-1.5. The Knapsack problem aims to maximize returns, while other problems focus on minimizing costs. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "BGOGknwHbi/tmp/fa0df1f03641a88a190c65f56f99b9ef8c43ebdb87de73cd3917f4c04f656b4b.jpg", "table_caption": ["Table 15: Comparison of various combinatorial problems based on their average cost per problem size, using llama-2-7b. The Knapsack problem aims to maximize returns, while other problems focus on minimizing costs. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "BGOGknwHbi/tmp/a5e621a1bd1806ec6f3b1e50d88a849ad4e57e528b2c2dce76cf4217018ba8b3.jpg", "table_caption": ["Table 16: Comparison of various combinatorial problems based on their average cost per problem size, using llama-2-70b. The Knapsack problem aims to maximize returns, while other problems focus on minimizing costs. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Example of VRP Prompting ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Task Description: You are given a list of customers with coordinates: ", "page_idx": 23}, {"type": "text", "text": "\u2013 (1): (87, 39);   \n\u2013 (2): (1, 90);   \n\u2013 (3): (28, 65);   \n\u2013 (4): (85, 85); ", "page_idx": 23}, {"type": "text", "text": "and a list of customer demands: ", "page_idx": 23}, {"type": "text", "text": "\u2013 (1): 12;   \n\u2013 (2): 8;   \n\u2013 (3): 16;   \n\u2013 (4): 5. ", "page_idx": 23}, {"type": "text", "text": "There is a depot with coordinates (50, 50) and one vehicle with a maximum capacity of 50. The goal is to find the route that has the minimum total length and go through all the customers, starting and ending at the depot. ", "page_idx": 23}, {"type": "text", "text": "Example of VRP Exploration ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 Prompt: [Task Description]. List heuristic methods to solve this problem. Return only method names separated by new lines. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Exploration Output: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2013 Nearest Neighbor Algorithm Savings Algorithm \u2013 Sweep Algorithm \u2013 Clarke and Wright Algorithm \u2013 Genetic Algorithm \u2013 Simulated Annealing \u2013 Tabu Search \u2013 Ant Colony Optimization \u2013 Particle Swarm Optimization ", "page_idx": 23}, {"type": "text", "text": "Example of VRP Decomposition ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 Prompt: [Task Description]. Nearest Neighbor Algorithm. List the steps to use this heuristic method. Return only the steps, separated by new lines. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Decomposition Output: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2013 \\*\\*Initialize at the Depot:\\*\\* The starting point is the depot located at coordinates (50, 50).   \n\u2013 \\*\\*Find the Nearest Customer:\\*\\* Calculate the Euclidean distance from the depot to each customer, and move to the closest one.   \n\u2013 \\*\\*Mark the Customer as Visited:\\*\\* Once a customer is visited, they should not be revisited.   \n\u2013 \\*\\*Repeat Finding and Moving:\\*\\* Continue finding the nearest unvisited customer from the current location until all customers have been visited.   \n\u2013 \\*\\*Return to the Depot:\\*\\* After visiting the last customer, calculate the distance back to the depot to complete the route.   \n\u2013 \\*\\*Calculate the Total Length of the Route:\\*\\* Sum up all the distances traveled from start to finish. ", "page_idx": 23}, {"type": "text", "text": "Example of Subtask Resolution, Step One ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 Prompt: [Task Description]. [Apply the heuristic\u2019s step one]. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Subtask Resolution Output: ", "page_idx": 24}, {"type": "text", "text": "Initialize at the Depot - Current location: Depot at (50, 50). ", "page_idx": 24}, {"type": "text", "text": "Example of Subtask Resolution, Step Two ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 Prompt: [Task Description]. [Apply the heuristic\u2019s step two]. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Subtask Resolution Output: Find the Nearest Customer \u2013 Distance to Customer 1 at (87, 39): $\\sqrt{(87-50)^{2}+(39-50)^{2}}$ \u2013 Distance to Customer 2 at (1, 90): $\\sqrt{(1-50)^{2}+(90-50)^{2}}$ \u2013 Distance to Customer 3 at (28, 65): $\\sqrt{(28-50)^{2}+(65-50)^{2}}$ \u2013 Distance to Customer 4 at (85, 85): $\\sqrt{(85-50)^{2}+(85-50)^{2}}$ The initial distances from the depot to each customer are as follows: \u2013 Customer 1: 38.60 \u2013 Customer 2: 63.25 \u2013 Customer 3: 26.63 (nearest) \u2013 Customer 4: 49.50 ", "page_idx": 24}, {"type": "text", "text": "Example of Subtask Resolution, Step Three ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 Prompt: [Task Description]. [Apply the heuristic\u2019s step three]. \u2022 Subtask Resolution Output: ", "page_idx": 24}, {"type": "text", "text": "Mark the Customer as Visited.   \nThe nearest customer is Customer 3 at coordinates (28, 65). We will visit Customer 3 first. ", "page_idx": 24}, {"type": "text", "text": "Example of Subtask Resolution, Step Six ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 Prompt: [Task Description]. [Apply the heuristic\u2019s step six]. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Subtask Resolution Output: Calculate the Total Length of the Route. The complete route, following the Nearest Neighbor Algorithm from the depot,   \nvisiting all customers, and returning to the depot, totals a distance of approximately 232.22 units.   \nHere\u2019s a summary of the route taken: \u2013 Start at Depot (50, 50) \u2013 Visit Customer 3 (28, 65) \u2013 Visit Customer 2 (1, 90) Visit Customer 4 (85, 85) \u2013 Visit Customer 1 (87, 39) ", "page_idx": 24}, {"type": "text", "text": "\u2013 Return to Depot (50, 50) ", "page_idx": 24}, {"type": "text", "text": "Example of Feedback Prompting ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 Prompt: [Task Description]. [Nearest Neighbor candidate solution]. Give feedback to the proposed solution. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Feedback Output: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Implement route optimization post-processing, such as 2-opt or 3-opt techniques, which iteratively refine the route by removing crossings and reducing travel distance. ", "page_idx": 25}, {"type": "text", "text": "Example of Refinement Prompting ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 Prompt: [Task Description]. [Nearest Neighbor candidate solution]. [Feedback]. Improve the solution given feedback. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Refinement Output: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2013 we\u2019ll apply a 2-opt optimization to the route. This will involve swapping segments of the route to find a shorter path. Given the route $1->2->3->4$ , we\u2019ll try different swaps and see if we can reduce the total distance further.   \n\u2013 Let\u2019s simulate the 2-opt optimization on this route.   \n\u2013 After applying the 2-opt optimization technique, the final optimized route has been adjusted to: Customer $1\\cdot>$ Customer $_3->$ Customer $\\scriptstyle2\\,->$ Customer 4, then returning to the depot. This rearrangement was made to minimize the total distance traveled. ", "page_idx": 25}, {"type": "text", "text": "Example of Solution Integration ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 Prompt: [Task Description]. [All candidate solutions]. Integrate all previous findings and provide the final answer. Return the answer only as a Python list of elements. \u2022 Refinement Output: [1, 3, 2, 4] ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The method, problem description, and key results are outlined in the abstract and introduction sections. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We examine the trade-off between higher costs and enhanced performance, as well as the dependency of the method\u2019s effectiveness on the choice of LLM model. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We detail the experiments conducted and provide a link to the associated code repository. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The code for the experiments, along with instructions, is provided in open access. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We have detailed the LLM models used, as well as the specific splits/details for reasoning and combinatorial problem tasks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: Error bars are not provided. The mean values of LLM few-shot generations are reported, consistent with the standard practice in similar research. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Details of the resources utilized are provided in the Experiments section. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: To the best of our knowledge, all research conducted in this paper fully conforms to the NeurIPS Code of Ethics. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: At this stage of research on applying generative models to combinatorial problems, there are no identified societal impacts. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 29}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: We believe the paper does not pose such risks. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have mentioned the companies that own the LLM models used in our experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The new code produced represents an asset developed during this research.   \nDocumentation is available in the corresponding repository. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}]