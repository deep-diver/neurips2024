[{"figure_path": "BGOGknwHbi/tables/tables_5_1.jpg", "caption": "Table 1: Percentage performance improvement compared to IO on CP tasks using GPT-4 and Gemini-1.5 models. CoT uses majority voting, with the number of candidates equal to the number of thoughts produced by SGE. The metrics is quantified as percentage improvement in cost with respect to IO solution (the bigger it is the better).", "description": "This table presents the performance improvement of the proposed Self-Guiding Exploration (SGE) method compared to several baseline methods (Input-Output (IO), Chain-of-Thought (CoT), Self-Refine, and Decomposition) for six different combinatorial problems (CPs) using two large language models (LLMs): GPT-4 and Gemini-1.5.  The improvement is calculated as a percentage reduction in the cost of the solution found by each method compared to the IO baseline.  Higher percentages indicate better performance. The number of candidates for the CoT method is set equal to the number of thought trajectories generated by SGE.", "section": "5.2 Results on CP tasks"}, {"figure_path": "BGOGknwHbi/tables/tables_7_1.jpg", "caption": "Table 1: Percentage performance improvement compared to IO on CP tasks using GPT-4 and Gemini-1.5 models. CoT uses majority voting, with the number of candidates equal to the number of thoughts produced by SGE. The metrics is quantified as percentage improvement in cost with respect to IO solution (the bigger it is the better).", "description": "This table presents the percentage improvement in cost achieved by the Self-Guiding Exploration (SGE) method compared to four baseline methods (Input-Output, Chain-of-Thought, Self-Refine, and Decomposition) across six different combinatorial problems (Assignment, Knapsack, Bin Packing, Traveling Salesman, Vehicle Routing, Job Scheduling) using two different large language models (GPT-4 and Gemini-1.5).  Higher percentages indicate better performance.", "section": "5.2 Results on CP tasks"}, {"figure_path": "BGOGknwHbi/tables/tables_8_1.jpg", "caption": "Table 1: Percentage performance improvement compared to IO on CP tasks using GPT-4 and Gemini-1.5 models. CoT uses majority voting, with the number of candidates equal to the number of thoughts produced by SGE. The metrics is quantified as percentage improvement in cost with respect to IO solution (the bigger it is the better).", "description": "This table presents the performance improvement achieved by the Self-Guiding Exploration (SGE) method compared to four baseline methods (Input-Output (IO), Chain-of-Thought (CoT), Self-Refine (Refine), and Decomposition) across six combinatorial problems (Assignment, Knapsack, Bin Packing, Traveling Salesman, Vehicle Routing, and Job Scheduling).  The results are shown for two large language models (GPT-4 and Gemini-1.5).  The improvement is calculated as the percentage decrease in cost compared to the IO baseline.  The table highlights that SGE outperforms the other methods significantly, with the improvement increasing as the complexity of the problem increases.", "section": "5.2 Results on CP tasks"}, {"figure_path": "BGOGknwHbi/tables/tables_8_2.jpg", "caption": "Table 1: Percentage performance improvement compared to IO on CP tasks using GPT-4 and Gemini-1.5 models. CoT uses majority voting, with the number of candidates equal to the number of thoughts produced by SGE. The metrics is quantified as percentage improvement in cost with respect to IO solution (the bigger it is the better).", "description": "This table presents the performance improvement of the proposed Self-Guiding Exploration (SGE) method and three baseline methods (CoT, Refine, Decomp) compared to a basic Input-Output (IO) approach on six combinatorial problems (CPs) using two large language models (LLMs): GPT-4 and Gemini-1.5.  The improvement is measured as a percentage reduction in cost compared to the IO baseline.  The table shows that SGE significantly outperforms the baseline methods across all tasks and models.", "section": "5.2 Results on CP tasks"}, {"figure_path": "BGOGknwHbi/tables/tables_16_1.jpg", "caption": "Table 5: Percentage performance improvement compared to IO on CP tasks using GPT-4 and Gemini-1.5 models. CoT uses majority voting, with the number of candidates equal to the number of thoughts produced by SGE. The metrics is quantified as percentage improvement in cost with respect to IO solution (the bigger it is the better).", "description": "This table presents the percentage improvement in performance achieved by the Self-Guiding Exploration (SGE) method compared to four baseline methods (Input-Output, Chain-of-Thought, Self-Refine, and Decomposition) across six combinatorial problem tasks (Assignment, Knapsack, Bin Packing, Traveling Salesman, Vehicle Routing, and Job Scheduling) using two different Large Language Models (GPT-4 and Gemini-1.5).  The improvement is calculated as a percentage relative to the Input-Output baseline's cost. Higher values indicate better performance.", "section": "5.2 Results on CP tasks"}, {"figure_path": "BGOGknwHbi/tables/tables_16_2.jpg", "caption": "Table 6: Percentage Performance Improvement Compared to IO Prompting on Vehicle Routing Problem. Columns Show the Number of Nodes.", "description": "This table presents the performance improvement of different methods compared to the basic Input-Output (IO) prompting method for solving Vehicle Routing Problems (VRPs).  The improvement is shown as a percentage for various problem sizes (number of nodes).  Methods compared include LNS (Large Neighborhood Search), Google OR-Tools, LKH3 (Lin-Kernighan Heuristic), and the proposed SGE (Self-Guiding Exploration) method.  Larger percentage values indicate better performance.", "section": "5.2 Results on CP tasks"}, {"figure_path": "BGOGknwHbi/tables/tables_16_3.jpg", "caption": "Table 7: VRP Average Total Cost.", "description": "This table shows the average total cost for solving the Vehicle Routing Problem (VRP) using the Self-Guiding Exploration (SGE) method, categorized by the number of nodes (cities).  The costs reflect the computational expense incurred when utilizing a large language model to solve the VRP using the described method.  A higher number of nodes generally correlates with a higher total cost.", "section": "5.1 Setup"}, {"figure_path": "BGOGknwHbi/tables/tables_17_1.jpg", "caption": "Table 5: Percentage Performance Improvement Compared to IO Prompting on Job Scheduling Problem. Columns Show the Number of n Jobs and m Machines.", "description": "This table presents the percentage improvement in performance achieved by different methods (LNS, OR-Tools, and SGE) compared to a basic Input-Output (IO) prompting approach for solving Job Scheduling Problems.  The results are broken down by the number of jobs (n) and machines (m) used in the problem instances.", "section": "5.1 Setup"}, {"figure_path": "BGOGknwHbi/tables/tables_17_2.jpg", "caption": "Table 1: Percentage performance improvement compared to IO on CP tasks using GPT-4 and Gemini-1.5 models. CoT uses majority voting, with the number of candidates equal to the number of thoughts produced by SGE. The metrics is quantified as percentage improvement in cost with respect to IO solution (the bigger it is the better).", "description": "This table presents the performance improvement of the Self-Guiding Exploration (SGE) method compared to several baseline methods (Input-Output, Chain-of-Thought, Self-Refine, and Decomposition) across six different combinatorial problems (Assignment, Knapsack, Bin Packing, Traveling Salesman, Vehicle Routing, and Job Scheduling) using two different Large Language Models (LLMs), GPT-4 and Gemini-1.5.  The improvement is measured as the percentage reduction in cost compared to the Input-Output baseline.  Higher values indicate better performance.", "section": "5.2 Results on CP tasks"}, {"figure_path": "BGOGknwHbi/tables/tables_17_3.jpg", "caption": "Table 1: Percentage performance improvement compared to IO on CP tasks using GPT-4 and Gemini-1.5 models. CoT uses majority voting, with the number of candidates equal to the number of thoughts produced by SGE. The metrics is quantified as percentage improvement in cost with respect to IO solution (the bigger it is the better).", "description": "This table presents the performance improvement of the Self-Guiding Exploration (SGE) method compared to four baseline methods (Input-Output, Chain-of-Thought, Self-Refine, and Decomposition) across six combinatorial problems (Assignment, Knapsack, Bin Packing, Traveling Salesman, Vehicle Routing, and Job Scheduling) using GPT-4 and Gemini-1.5 language models.  The improvement is measured as a percentage reduction in cost compared to the Input-Output baseline.  The table also shows the results for the Chain-of-Thought (CoT) approach, using majority voting.", "section": "5.2 Results on CP tasks"}, {"figure_path": "BGOGknwHbi/tables/tables_17_4.jpg", "caption": "Table 1: Percentage performance improvement compared to IO on CP tasks using GPT-4 and Gemini-1.5 models. CoT uses majority voting, with the number of candidates equal to the number of thoughts produced by SGE. The metrics is quantified as percentage improvement in cost with respect to IO solution (the bigger it is the better).", "description": "This table presents the performance improvement of the Self-Guiding Exploration (SGE) method compared to several baseline methods (IO, CoT, Refine, Decomp) for solving six combinatorial problems (Assignment, Knapsack, Bin Packing, TSP, VRP, Job Scheduling) using two different LLMs (GPT-4 and Gemini-1.5).  The improvement is calculated as a percentage relative to the Input-Output (IO) baseline method and represents the percentage reduction in cost achieved by each method.  Higher percentages indicate better performance.", "section": "5.2 Results on CP tasks"}, {"figure_path": "BGOGknwHbi/tables/tables_18_1.jpg", "caption": "Table 1: Percentage performance improvement compared to IO on CP tasks using GPT-4 and Gemini-1.5 models. CoT uses majority voting, with the number of candidates equal to the number of thoughts produced by SGE. The metrics is quantified as percentage improvement in cost with respect to IO solution (the bigger it is the better).", "description": "This table presents the performance improvement of the Self-Guiding Exploration (SGE) method compared to several baseline methods (Input-Output, Chain-of-Thought, Self-Refine, and Decomposition) for six different combinatorial problems (Assignment, Knapsack, Bin Packing, Traveling Salesman, Vehicle Routing, and Job Scheduling) using two different large language models (GPT-4 and Gemini-1.5).  The improvement is measured as the percentage reduction in cost compared to the Input-Output baseline.  The table highlights SGE's superior performance.", "section": "5.2 Results on CP tasks"}, {"figure_path": "BGOGknwHbi/tables/tables_19_1.jpg", "caption": "Table 1: Percentage performance improvement compared to IO on CP tasks using GPT-4 and Gemini-1.5 models. CoT uses majority voting, with the number of candidates equal to the number of thoughts produced by SGE. The metrics is quantified as percentage improvement in cost with respect to IO solution (the bigger it is the better).", "description": "This table presents the percentage improvement in cost achieved by the Self-Guiding Exploration (SGE) method compared to four baseline methods (Input-Output, Chain-of-Thought, Self-Refine, and Decomposition) across six different combinatorial problems (Assignment, Knapsack, Bin Packing, Traveling Salesman, Vehicle Routing, and Job Scheduling) using two large language models (GPT-4 and Gemini-1.5).  The improvement is calculated relative to the Input-Output baseline and is presented as a percentage.  The table also shows results using a Chain-of-Thought baseline with the number of candidates equal to the number of thought trajectories generated by SGE.", "section": "5.2 Results on CP tasks"}, {"figure_path": "BGOGknwHbi/tables/tables_20_1.jpg", "caption": "Table 1: Percentage performance improvement compared to IO on CP tasks using GPT-4 and Gemini-1.5 models. CoT uses majority voting, with the number of candidates equal to the number of thoughts produced by SGE. The metrics is quantified as percentage improvement in cost with respect to IO solution (the bigger it is the better).", "description": "This table presents the performance improvement of the Self-Guiding Exploration (SGE) method compared to other baseline methods (Input-Output, Chain-of-Thought, Self-Refine, and Decomposition) for six different combinatorial problems (Assignment, Knapsack, Bin Packing, Traveling Salesman, Vehicle Routing, and Job Scheduling) using two different Large Language Models (LLMs), GPT-4 and Gemini-1.5.  The improvement is measured as a percentage reduction in cost compared to the Input-Output baseline.", "section": "5.2 Results on CP tasks"}, {"figure_path": "BGOGknwHbi/tables/tables_21_1.jpg", "caption": "Table 1: Percentage performance improvement compared to IO on CP tasks using GPT-4 and Gemini-1.5 models. CoT uses majority voting, with the number of candidates equal to the number of thoughts produced by SGE. The metrics is quantified as percentage improvement in cost with respect to IO solution (the bigger it is the better).", "description": "This table presents the performance improvement of the Self-Guiding Exploration (SGE) method compared to four baseline methods (Input-Output (IO) Direct Prompting, Chain-of-Thought Prompting, Self-Refine Prompting, and Decomposition Prompting) across six different combinatorial problems (Assignment, Knapsack, Bin Packing, Traveling Salesman, Vehicle Routing, and Job Scheduling).  The improvements are calculated as a percentage relative to the IO baseline, showing how much better SGE performed in terms of cost optimization.  The table also includes results using both GPT-4 and Gemini-1.5 language models.", "section": "5.2 Results on CP tasks"}, {"figure_path": "BGOGknwHbi/tables/tables_22_1.jpg", "caption": "Table 1: Percentage performance improvement compared to IO on CP tasks using GPT-4 and Gemini-1.5 models. CoT uses majority voting, with the number of candidates equal to the number of thoughts produced by SGE. The metrics is quantified as percentage improvement in cost with respect to IO solution (the bigger it is the better).", "description": "This table presents the performance improvement of the Self-Guiding Exploration (SGE) method compared to several baselines (Input-Output, Chain-of-Thought, Self-Refine, and Decomposition) on six combinatorial problems (Assignment, Knapsack, Bin Packing, Traveling Salesman, Vehicle Routing, and Job Scheduling) using two different large language models (LLMs): GPT-4 and Gemini-1.5.  The improvement is measured as the percentage reduction in cost compared to the Input-Output baseline.  The table highlights the superior performance of SGE across all tasks and models.", "section": "5.2 Results on CP tasks"}]