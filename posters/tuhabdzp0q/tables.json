[{"figure_path": "tUHABDZP0Q/tables/tables_6_1.jpg", "caption": "Table 1: Performance comparison with other UDA methods.", "description": "This table compares the performance of the proposed Reinforced Cross-Domain Knowledge Distillation (RCD-KD) method with several other unsupervised domain adaptation (UDA) methods across four different datasets (HAR, HHAR, FD, SSC) using three different metrics.  The \"Student-Only\" column represents the performance of a compact student model trained only on the source domain without any domain adaptation.  The remaining columns present the performance of different UDA methods, including the proposed RCD-KD method, showing the macro F1 score achieved for each method on each dataset. This allows for a direct comparison of the proposed method's performance against state-of-the-art techniques in handling domain shift problems.", "section": "4 Experiments"}, {"figure_path": "tUHABDZP0Q/tables/tables_6_2.jpg", "caption": "Table 1: Performance comparison with other UDA methods.", "description": "This table compares the performance of the proposed Reinforced Cross-Domain Knowledge Distillation (RCD-KD) method with several state-of-the-art unsupervised domain adaptation (UDA) methods on four different time series datasets (HAR, HHAR, FD, and SSC).  The performance metric used is macro F1-score.  It shows the performance of a student model trained with only source data ('Student-Only'),  various metric-based UDA methods (HoMM, MDDA, SASA), various adversarial-based UDA methods (DANN, CoDATS, AdvSKM), and the proposed RCD-KD method. The results demonstrate the effectiveness of RCD-KD compared to other UDA methods in improving the performance of a compact student model on the target domain.", "section": "4 Experiments"}, {"figure_path": "tUHABDZP0Q/tables/tables_7_1.jpg", "caption": "Table 1: Performance comparison with other UDA methods.", "description": "This table compares the performance of the proposed Reinforced Cross-Domain Knowledge Distillation (RCD-KD) method against several state-of-the-art unsupervised domain adaptation (UDA) methods.  The comparison is done using four metrics across four different datasets (HAR, HHAR, FD, SSC). Each dataset represents a different time series task. The \"Student-Only\" column shows the performance of a simple student model trained only on the source domain, providing a baseline.  The table highlights the superior performance of RCD-KD, which consistently outperforms the other methods across various datasets and tasks.", "section": "4 Experiments"}, {"figure_path": "tUHABDZP0Q/tables/tables_7_2.jpg", "caption": "Table 1: Performance comparison with other UDA methods.", "description": "This table compares the performance of the proposed Reinforced Cross-Domain Knowledge Distillation (RCD-KD) method with several state-of-the-art unsupervised domain adaptation (UDA) methods across four different datasets (HAR, HHAR, FD, SSC) and multiple transfer scenarios.  Each dataset represents a specific time-series task, and the transfer scenarios involve adapting a model trained on one subset of the data to perform well on a different, unseen subset. The table shows the macro F1-score for each method, providing a comprehensive comparison of their effectiveness in handling domain shift in time-series data.  The \"Student-Only\" row indicates the performance of a model trained only on the source domain without any domain adaptation.", "section": "4 Experiments"}, {"figure_path": "tUHABDZP0Q/tables/tables_8_1.jpg", "caption": "Table 4: Teacher with different UDA methods.", "description": "This table presents the performance comparison of using different Unsupervised Domain Adaptation (UDA) methods for pre-training the teacher model. The goal is to analyze how the choice of UDA method for teacher training affects the final performance of the student model in the proposed Reinforced Cross-Domain Knowledge Distillation (RCD-KD) framework.  Different UDA methods used for teacher training are compared: MDDA, SASA, CODATS, and DANN. The table shows the macro F1 scores achieved by the student model after knowledge distillation, for the HAR, HHAR, FD, and SSC datasets. The table also includes ablation study results showing the effect of removing knowledge distillation loss, domain confusion loss, and reinforced cross-domain knowledge distillation loss from the proposed method.", "section": "4.4 Ablation Study"}, {"figure_path": "tUHABDZP0Q/tables/tables_9_1.jpg", "caption": "Table 6: Reinforced sample selection ablation. \"Full samples\" denotes utilizing whole target samples for KD; `R2`, `R3` denote directly utilizing proposed uncertainty and transferability for sample selection; `R1`, `R1` denote utilizing RL with R2 and R3 as reward for sample selection; (R2 + R3)\u2020 is our proposed method.", "description": "This table presents the ablation study on the proposed reward function and RL-based sample selection module.  It compares the performance of using all target samples for knowledge distillation (Full Samples) against using only a subset selected based on different criteria: uncertainty (R2), transferability (R3), and reinforcement learning based on these criteria (R1, R1, (R2+R3)\u2020).  The results show the impact of each component and highlight the superior performance of the complete method, (R2+R3)\u2020.", "section": "4.3 Benchmark with KD-based DA methods"}, {"figure_path": "tUHABDZP0Q/tables/tables_9_2.jpg", "caption": "Table 7: Comparison of Computational Complexity.", "description": "This table compares the training time (in seconds) required by different domain adaptation methods, including KD-STDA, KA-MCD, MLD-DA, REDA, AAD, MobileDA, UNI-KD, and the proposed RCD-KD method. The training time is a measure of computational complexity, showing how long each method takes to train a model. The results indicate that the proposed RCD-KD method has significantly higher computational cost compared to other methods.", "section": "4.3 Benchmark with KD-based DA methods"}, {"figure_path": "tUHABDZP0Q/tables/tables_14_1.jpg", "caption": "Table 1: Performance comparison with other UDA methods.", "description": "This table compares the performance of the proposed Reinforced Cross-Domain Knowledge Distillation (RCD-KD) method with other state-of-the-art unsupervised domain adaptation (UDA) methods on four different datasets (HAR, HHAR, FD, SSC).  The results are presented as the average macro F1-score across three independent runs for each dataset and various transfer scenarios.  The \"Student-Only\" column shows the performance of a compact student model trained only on the source domain, without any domain adaptation.  The table allows for a comparison of the effectiveness of RCD-KD against existing methods in terms of improving the performance of a compact student model on the target domain.", "section": "4 Experiments"}, {"figure_path": "tUHABDZP0Q/tables/tables_15_1.jpg", "caption": "Table 1: Performance comparison with other UDA methods.", "description": "This table compares the performance of the proposed RCD-KD method with other state-of-the-art unsupervised domain adaptation (UDA) methods on four benchmark datasets (HAR, HHAR, FD, SSC).  Each dataset represents a different time series task, and the table shows the performance of different methods on various transfer scenarios (different source and target domains). The \"Student-Only\" column represents the performance of a student model trained only on the source domain, illustrating the baseline performance and how well the UDA methods can improve the performance of the compact student model. The table highlights that the proposed RCD-KD consistently outperforms other UDA methods on various datasets and transfer scenarios.", "section": "4 Experiments"}, {"figure_path": "tUHABDZP0Q/tables/tables_15_2.jpg", "caption": "Table 1: Performance comparison with other UDA methods.", "description": "This table compares the performance of the proposed Reinforced Cross-Domain Knowledge Distillation (RCD-KD) method with other state-of-the-art Unsupervised Domain Adaptation (UDA) methods on four different time series datasets.  It shows the macro F1-score achieved by each method across three independent runs. The datasets include Human Activity Recognition (HAR), Heterogeneity HAR (HHAR), Rolling Bearing Fault Diagnosis (FD), and Sleep Stage Classification (SSC).  The table highlights the superior performance of RCD-KD in most scenarios, especially when considering the variability of the results across different datasets and transfer scenarios.", "section": "4 Experiments"}, {"figure_path": "tUHABDZP0Q/tables/tables_15_3.jpg", "caption": "Table 1: Performance comparison with other UDA methods.", "description": "This table compares the performance of the proposed Reinforced Cross-Domain Knowledge Distillation (RCD-KD) method with other state-of-the-art unsupervised domain adaptation (UDA) methods on four benchmark datasets (HAR, HHAR, FD, SSC) across three independent runs.  The results show macro F1-scores for each method and highlight the superior performance of RCD-KD.  It demonstrates that the RCD-KD method is consistently effective and outperforms other UDA approaches in most transfer scenarios.", "section": "4 Experiments"}, {"figure_path": "tUHABDZP0Q/tables/tables_16_1.jpg", "caption": "Table 12: Sensitivity Analysis on \u03b1\u2081 and \u03b1\u2082.", "description": "This table presents the sensitivity analysis performed on the hyperparameters \u03b1\u2081 and \u03b1\u2082, which are used in the reward function of the reinforcement learning module for target sample selection.  The table shows the Macro F1-scores achieved on four different datasets (HAR, HHAR, FD, and SSC) for various combinations of \u03b1\u2081 and \u03b1\u2082 values.  The results help determine the optimal balance between the uncertainty consistency and sample transferability rewards in the reward function.", "section": "A.4 Sensitivity Analysis"}, {"figure_path": "tUHABDZP0Q/tables/tables_17_1.jpg", "caption": "Table 13: Sensitivity Analysis for temperature \u03c4", "description": "This table presents the sensitivity analysis result for the hyperparameter \u03c4 (temperature) in the proposed RCD-KD method.  It shows the macro F1 scores achieved on four different time series datasets (HAR, HHAR, FD, SSC) when varying the value of \u03c4 from 1 to 16.  The results indicate the optimal range for \u03c4, showing how the model's performance changes with different temperature values.", "section": "4.4 Ablation Study"}, {"figure_path": "tUHABDZP0Q/tables/tables_17_2.jpg", "caption": "Table 14: Comparison with different sample selection strategies in AL.", "description": "This table compares the performance of the proposed method against various active learning sample selection strategies, namely Least Confidence (LC), Sample Margin (M), and Sample Entropy (H).  It shows the macro F1-score achieved on four different datasets (HAR, HHAR, FD, and SSC) for each strategy, both with and without the incorporation of uncertainty consistency and Reinforcement Learning (RL). The baseline represents the performance without any active learning sample selection.", "section": "A.6 Comparison with sample selection strategies in Active Learning"}, {"figure_path": "tUHABDZP0Q/tables/tables_18_1.jpg", "caption": "Table 1: Performance comparison with other UDA methods.", "description": "This table compares the performance of the proposed Reinforced Cross-Domain Knowledge Distillation (RCD-KD) method with several other unsupervised domain adaptation (UDA) methods on four different datasets (HAR, HHAR, FD, SSC).  The \"Student-Only\" column represents the performance of a student model trained only on the source domain without any domain adaptation. The other columns show the performance of various UDA methods and RCD-KD.  The results are evaluated using the Macro F1-score metric.", "section": "4 Experiments"}, {"figure_path": "tUHABDZP0Q/tables/tables_18_2.jpg", "caption": "Table 1: Performance comparison with other UDA methods.", "description": "This table compares the performance of the proposed Reinforced Cross-Domain Knowledge Distillation (RCD-KD) method against several state-of-the-art unsupervised domain adaptation (UDA) methods.  The comparison is performed across four different datasets (HAR, HHAR, FD, SSC), each representing a distinct time series task.  The table shows the macro F1-score achieved by each method, with the 'Student-Only' column representing the performance of a compact student model trained only on source data without any domain adaptation.  The results illustrate the effectiveness of the RCD-KD approach in improving the performance of a resource-efficient student model across various time series domains.", "section": "4 Experiments"}]