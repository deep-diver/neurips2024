[{"type": "text", "text": "Reinforced Cross-Domain Knowledge Distillation on Time Series Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qing Xu ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Min Wu ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Institute for Infocomm Research A\\*STAR, Singapore Nanyang Technological University Xu_Qing@i2r.a-star.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Institute for Infocomm Research A\\*STAR, Singapore wumin@i2r.a-star.edu.sg ", "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kezhi Mao ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xiaoli Li   \nInstitute for Infocomm Research, A\\*STAR, Singapore   \nA\\*STAR Centre for Frontier AI Research, Singapore xlli@i2r.a-star.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Nanyang Technological University EKZMao@ntu.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Zhenghua Chen\u2217   \nInstitute for Infocomm Research, A\\*STAR, Singapore   \nA\\*STAR Centre for Frontier AI Research, Singapore chen0832@e.ntu.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Unsupervised domain adaptation methods have demonstrated superior capabilities in handling the domain shift issue which widely exists in various time series tasks. However, their prominent adaptation performances heavily rely on complex model architectures, posing an unprecedented challenge in deploying them on resource-limited devices for real-time monitoring. Existing approaches, which integrates knowledge distillation into domain adaptation frameworks to simultaneously address domain shift and model complexity, often neglect network capacity gap between teacher and student and just coarsely align their outputs over all source and target samples, resulting in poor distillation efficiency. Thus, in this paper, we propose an innovative framework named Reinforced Cross-Domain Knowledge ", "page_idx": 0}, {"type": "text", "text": "Distillation (RCD-KD) which can effectively adapt to student\u2019s network capability via dynamically selecting suitable target domain samples for knowledge transferring. Particularly, a reinforcement learning-based module with a novel reward function is proposed to learn optimal target sample selection policy based on student\u2019s capacity. Meanwhile, a domain discriminator is designed to transfer the domain invariant knowledge. Empirical experimental results and analyses on four public time series datasets demonstrate the effectiveness of our proposed method over other state-of-the-art benchmarks. Our source code is available at https://github.com/xuqing88/Reinforced-Cross-Domain-Knowledge-Distillationon-Time-Series-Data. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent years have witnessed great successes of deep neural networks (DNNs) in various time series applications [1; 2; 3; 4]. Nevertheless, a significant drawback impeding their scalability is the limited generalization capability on unseen data. This challenge arises when there is a distribution disparity between the data used for training and deployment. For instance, a fault diagnosis model trained on certain machines may perform poorly on the data collected from other machines which have different working conditions and configurations. Collecting and annotating data for each machine would be very laborious and costly. To handle this, various unsupervised domain adaptation (UDA) methods have been extensively explored. These methods aim to transfer the domain invariant knowledge from an existing labeled data domain (i.e., source domain) to an unlabeled domain (i.e., target domain) either by explicitly minimizing certain pre-defined discrepancy metrics [5; 6] or implicitly learning domain-invariant representations with adversarial manners [7; 8]. However, these UDA methods heavily rely on the complex network architectures and their adaptation performance will significantly degrade with shallower networks [9; 10]. The over-parameterized DNNs will inevitably lead to another practical issue in industries. For many real-world time series tasks, the developed models are often required to be deployed on edge devices with very limited computational resources, such as smartphones and robots, for real-time and long-term monitoring. The intolerable computational and storage burdens make the deployment of those complex DNNs on edge devices become an unprecedented challenge. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Some pioneering efforts have been made to integrate knowledge distillation (KD) techniques into UDA frameworks to transfer the cross-domain knowledge from a cumbersome teacher to a compact student for the reduction of model complexity. However, we empirically find that simply integrating KD with UDA frameworks like existing works will make the compact student suffer from unsatisfying adaptation performance. The rationale behind this lies in the facts that: on the one hand, due to its limited network capacity, the compact student may fail to capture the same fine-grained patterns in data as the cumbersome teacher. Coarsely aligning its feature representations or outputs with the teacher like [11; 12] will impede its learning process and result in sub-optimal performance on the target domain. On the other hand, in the cross-domain scenario, teacher\u2019s knowledge on each individual target sample may not be always reliable and instructive due to the lack of label supervision in target domain. Blindly trusting teacher\u2019s knowledge for all samples, especially on target domain, will result in negative transfer. Therefore, to achieve good adaptation performance on the target domain, we have to adaptively transfer teacher\u2019s knowledge based on student\u2019s network capability. ", "page_idx": 1}, {"type": "text", "text": "Motivated by above insights, we propose a novel end-to-end framework for cross-domain knowledge distillation to simultaneously address domain shift and model complexity. To be specific, an adversarial discriminator module is designed to align teacher\u2019s and student\u2019s representations between source and target domains on latent feature space for domain-invariant knowledge transfer. Meanwhile, to adaptively transfer teacher\u2019s knowledge on the unlabeled target domain, we formulate the target sample selection problem under a reinforcement learning framework. For a specific target sample, if the student demonstrates the ability to attain the same uncertainty level as the teacher (i.e., uncertainty consistency), or can largely mimic teacher\u2019s outputs (i.e., sample transferability), we deem such a sample suitable for knowledge distillation. Based on that, we design a novel reward function according to student\u2019s learning capability. A dueling Double Deep Q-Network (DDQN) is then utilized to learn the optimal target sample selection policy for mitigating the negative effects of unsuitable knowledge from teacher. Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 An end-to-end framework named Reinforced Cross-Domain Knowledge Distillation (RCDKD) is proposed to not only effectively transfer the domain-invariant knowledge but also dynamically distill the adaptive target knowledge based on student\u2019s learning capability. \u2022 We develop an innovative reinforcement learning-based module to learn the optimal target sample selection policy for robust knowledge distillation. A novel reward function is designed for assessing student\u2019s learning capability in terms of uncertainty consistency and sample transferability to dynamically transfer teacher\u2019s target knowledge. \u2022 The extensive experimental results on four real-world time series tasks demonstrate the superior effectiveness of our approach compared to other SOTA methods. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In recent years, there are some pioneering works to tackle both domain shift and model complexity simultaneously. A resource efficient domain adaptation (REDA) framework with multi-exit architectures is proposed in [9], where the \u2018easier\u2019 samples are inferred via early exits and \u2018harder\u2019 ones are inferred via top exit. Meanwhile, some other researchers leverage the knowledge distillation [13] to enhance the adaptation performance of the compact student. For instance, a framework named knowledge distillation for unsupervised single target domain adaptation (KD-STDA) is proposed in [11]. Teacher\u2019s knowledge is gradually transferred via dynamically adjusting the contributions of UDA and KD loss. Similarly, a multi-level distillation for Domain Adaptation (MLD-DA) strategy is proposed in [12] to improve the distillation efficiency via a novel cross entropy loss. However, the above two methods transfer the knowledge from both source and target domains. We empirically show that the source domain-specific knowledge might have negative contribution to student\u2019s generalization. Besides, MobileDA [14] and adversarial adaptation with distillation (AAD) [15] employ the teacher trained on source-only domain to guide student\u2019s training, which have already been proved inefficient due to the limited and biased knowledge from teacher model by [4]. Moreover, to achieve more reliable knowledge from teacher, in [16] a maximum cluster difference metric is proposed to estimate teacher\u2019s confidence on certain sample. In [4], a framework named universal and joint knowledge distillation (UNI-KD) is proposed to measure teacher\u2019s confidence on individual sample via the output of a data-domain discriminator. However, due to the compact network architecture of the domain-shared feature extractor from student, the estimated uncertainty is not reliable. In our work, we estimate teacher\u2019s knowledge with student\u2019s capacity and then utilize it as the reward for the learning process of RL-based target sample selection module. The experimental results demonstrate that our proposed method can better enhance student\u2019s performance on target domain. Meanwhile, our work also relates to active learning (AL) field specifically in terms of selecting the most critical instances from unlabeled data. Note that here we only discuss the uncertainty-based sampling strategies in active learning as other query strategies (e.g., instance correlation) are beyond the scope of our paper. In AL, the uncertainty can be measured by three metrics: least confidence [17; 18], sample margin [19], and sample entropy [20]. Particularly, the entropy metric measures the uncertainty over the whole output prediction distribution [21; 22]. In our method, instead of explicitly utilizing entropy-based uncertainty as AL methods, we leverage the consistency between teacher\u2019 and student\u2019s entropy-based uncertainty to learn the optimal sample selection policy with dueling DDQN. See Supplementary for more comparison results. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Following standard UDA setup, we consider data from two domains: a labeled source domain = {(xis, yis)}in=s1 and an unlabeled target domain DtUgt ${\\cal{D}}_{t g t}^{U}\\;=\\;\\{x_{t}^{i}\\}_{i=1}^{n_{t}}$ which shares the same label space as source domain but has different data distributions. Here, $n_{s}$ and $n_{t}$ are the number of training samples in source and target domains, respectively. A powerful teacher model $T$ with superior adaptation performance is first pre-trained on $\\mathcal{D}_{s r c}^{L}\\bigcup\\mathcal{D}_{t g t}^{U}$ DtUgt with SOTA UDA methods. Our objective is to train a compact student model $S$ which is not only shallower than the teacher model but also can achieve competitive performance on unlabeled target domain. To transfer the learned knowledge from teacher to student, one can just follow standard KD [13] and force the student to mimic teacher\u2019s soften logits via Eq. (1). Here, $\\scriptstyle{\\mathcal{X}}_{b}$ represents a batch of training samples and KL refers to the Kullback\u2013Leibler divergence. $\\pmb q^{S}$ and $\\dot{\\boldsymbol{p}}^{T}$ are the softmax outputs soften by a temperature factor $\\tau$ from student $S$ and teacher $T$ , respectively. They are calculated sbtyu $\\begin{array}{r}{q_{c}^{S}\\,=\\,e x p(\\bar{z_{c}}/\\tau)/\\sum_{j=1}^{C}e x p(z_{j}/\\tau)}\\end{array}$ t,a iwn hsearme $C$ ibs etlhoen gniungm tboe rt hoef c-tlha scsleass sa.nd $q_{c}^{S}$ represents the $c$ ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{K D}=\\sum_{x\\in\\mathcal{X}_{b}}K L(\\pmb{p}^{T}||\\pmb{q}^{S})=\\sum_{x\\in\\mathcal{X}_{b}}\\sum_{c}p_{c}^{T}l o g(p_{c}^{T}/q_{c}^{S}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "However, since the teacher is trained on unlabeled target data, its prediction performance on specific target sample cannot be guaranteed. The compact architecture of student also limits its ability to fully accept teacher\u2019s knowledge. In other words, directly minimizing the distribution discrepancy between teacher\u2019s and student\u2019s predictions over all target samples might introduce inappropriate knowledge which will mislead the student\u2019s learning process. Thus, we propose to alleviate the above issue with a novel RL-based target sample selection module which can dynamically select suitable samples to assist the knowledge transferring. Fig. 1 illustrates the details of our proposed method. ", "page_idx": 2}, {"type": "image", "img_path": "tUHABDZP0Q/tmp/82500ce7dc90b59289d46928a0e41d0ebe22ab29af23324650bfa1ca179f6b81.jpg", "img_caption": ["Figure 1: Illustration of proposed RCD-KD. A Monte Carlo Dropout (MCD) based reward module is utilized to generate the reward for learning the optimal target sample selection policy. Specifically, the reward function consists of three parts. The first one is the action $a_{k}$ which is the output of dueling DDQN. The second part is the uncertainty consistency, estimated by entropy from student\u2019s logits $\\dot{\\pmb q}^{S}$ and the averaged logits $\\overline{{\\pmb{p}}}^{T}$ of $N$ teachers generated from MCD module. The third part is the sample transferability based on the KL divergence between $\\breve{\\pmb q}^{S}$ and $\\overline{{\\pmb{p}}}^{T}$ . The output of reward module $r_{k}$ then will be utilized for the optimization of dueling DDQN for learning optimal sample selection policy. Meanwhile, a domain discriminator $\\Phi$ is employed to transfer the domain-invariant knowledge. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.2 RL-based Target Sample Selection ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Following [23; 24], we consider target sample selection task as a Markov Decision Process which can addressed by reinforcement learning. A RL-based target sample selection module is first designed to enhance the distilling efficiency of teacher\u2019s knowledge on target domain. Particularly, a dueling DDQN [25] is employed to learn the optimal target sample selection policy. The dueling architecture can effectively mitigate the risk of overestimation by separately estimating the state value and advantage function, which improves the accuracy of action-value predictions. Meanwhile, to tackle the instability issue often encountered in training deep reinforcement learning models, we leverage strategies such as target network and experience replay. Specifically, the target network provides more stable targets for updating the $\\mathrm{{Q}}.$ -values by maintaining a separate, slowly updated network for generating target values, while experience replay enables the model to learn from a diverse set of past experiences, further enhancing stability and convergence during training. In each training batch, we utilize the learned sample selection policy to adaptively transfer teacher\u2019s target knowledge according to student\u2019s learning capability. In the following, the detailed definition of state, action, reward and the optimization of dueling DDQN will be introduced. ", "page_idx": 3}, {"type": "text", "text": "State. Given a batch of target domain samples $\\{x_{i}^{t g t}\\}_{i=1}^{n_{b}}$ and student\u2019s feature extractor $F^{S}$ , the state $s_{k}$ at episode $k\\in[1,K]$ is defined as the feature representations from student\u2019s feature extractor $s_{k}=[F_{k}^{S}(x_{1}^{t g t}),...,F_{k}^{S}(x_{n_{b}}^{t g t})]$ . Here, $n_{b}$ is the batch size and $K$ represents the maximum episode length. The state $s_{k}$ is forwarded to the dueling DDQN, generating a set of actions that decide whether to retain or discard the corresponding target samples. The student is subsequently optimized with the selected samples, and the next state $s_{k+1}$ can be obtained with the updated student. A terminate state will be triggered if the target sample is not selected at time step $k$ or the episode reaches the maximum episode length $K$ . ", "page_idx": 3}, {"type": "text", "text": "Action. For a specific target sample $x_{i}^{t g t}$ in a batch, it only has two actions $a_{i}\\in\\{0,1\\}$ which is binary. Specifically, $a_{i}=1$ means to retain the sample and $a_{i}=0$ means to discard it. Since the output of dueling DDQN parameterized with $\\Theta_{q}$ is a two-dimensional $Q$ -value vector, the optimal action $a_{i}^{*}$ for sample $x_{i}^{t g t}$ at current episode $k$ thus can be calculated by Eq. (2). Subsequently, the binary weights for all target samples are formulated as $w=[a_{1}^{*},...,a_{n_{b}}^{*}]$ , which then are utilized to calculate the distillation loss $\\mathcal{L}_{R K D}$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\na_{i}^{*}=\\operatorname*{argmax}_{a}Q(F_{k}^{S}(x_{i}^{t g t}),a;\\Theta_{q}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Reward. The reward function is pivotal in shaping the learning process for target sample selection policy, as it offers essential feedback to DDQN regarding the value associated with selecting a specific action in the current state. To achieve reliable knowledge transferred from selected target samples, we propose to utilize model uncertainty and sample transferability to design the reward function. ", "page_idx": 4}, {"type": "text", "text": "The first component constructing our reward function is a Boolean function $\\mathcal{R}_{1}=(a_{i}==1)$ , which indicates whether the sample $x_{i}$ is retained or not. The second component of our reward function is called uncertainty consistency reward. The motivate is straightforward: due to the lacks of label in target domain, we expect that the student should have the same uncertainty level as the teacher for a specific sample $x_{i}$ . For the teacher model, we employ the Monte Carlo Dropout (MCD) [26] to estimate its uncertainty, which utilizes a dropout distribution to approximate the posterior distribution (See Supplementary for more details). Practically, it means to enable the dropout of teacher model and forward $N$ times for each sample $x_{i}$ and the averaged prediction $\\begin{array}{r}{\\overline{{\\pmb{p}}}_{i}^{T}=\\frac{\\mathrm{~i~}}{N}\\sum_{n}p(y=c|x_{i},\\pmb{\\theta}^{n})}\\end{array}$ can be utilized to calculate the entropy $\\begin{array}{r}{\\mathcal{H}_{i}^{T}=-\\sum_{c}\\overline{{p}}_{i,c}^{T}l o g(\\overline{{p}}_{i,c}^{T})}\\end{array}$ for measuring its uncertainty. Here, $p(y=c|x_{i},\\pmb\\theta^{n})$ represents the probability of sample belonging to class $c$ and it is the softmax outputs of teacher model on the $n$ -th forward pass. For the student, the uncertainty is calculated with $\\#_{i}^{\\bar{S}}=$ $\\begin{array}{r}{-\\sum_{c}q_{i,c}^{S}l o g(q_{i,c}^{S})}\\end{array}$ . Intuitively, a higher value of the predictive entropy $\\mathcal{H}$ will be obtained when all classes are predicted to have equal probabilities, which means the model is less confident about the supnecceirftiaci ndtayt ac. oTnos iesntesnurcey  trheew satrudd $\\begin{array}{r}{\\mathcal{R}_{2}=(\\mathcal{H}_{i}^{S}>\\frac{1}{n_{b}}\\sum_{j=1}^{n_{b}}\\mathcal{H}_{j}^{S}\\overset{\\biggr.}{)}\\odot(\\mathcal{H}_{i}^{T}>\\frac{1}{n_{b}}\\sum_{j=1}^{n_{b}}\\mathcal{H}_{j}^{T})}\\end{array}$ ,o rwmhuelraet $\\odot$ hies the exclusive-nor operation. The third component of our reward function is the transferability reward formulated as $\\begin{array}{r}{\\mathcal{R}_{3}\\stackrel{!}{=}(\\mathcal{D}_{i}<\\frac{1}{n_{b}}\\sum_{j=1}^{n_{b}}\\mathcal{D}_{j})}\\end{array}$ . Here, ${\\mathcal{D}}_{i}=K L(\\overline{{\\pmb{p}}}_{i}^{T}||\\pmb{q}_{i}^{S})$ is the $\\mathrm{KL}$ divergence between student\u2019s prediction and the averaged MCD teacher prediction. Apparently, the samples whose KL divergence are lower than the averaged divergence are easier ones for the compact student to learn. With the above three Boolean functions, our reward function is defined as Eq. (3) shows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nr_{k}=\\alpha_{1}*(\\mathcal{R}_{1}\\oplus\\mathcal{R}_{2}-0.5)+\\alpha_{2}*(\\mathcal{R}_{1}\\oplus\\mathcal{R}_{3}-0.5),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\oplus$ is the exclusive-or operation. For the first part of Eq. (3), a positive reward value will be assigned if a sample is retained and student shows consistent uncertainty as the teacher, or it is discarded and student and teacher show inconsistent uncertainty about it. Otherwise, a negative reward will be assigned. Similarly, for the second part of Eq. (3), a positive reward will be assigned if its transferability is higher than others and being selected, or its transferability is lower than others and not selected. We utilize $\\alpha_{1}$ and $\\alpha_{2}$ to adjust the contribution of each part. We constrain the reward within the range of $^-1$ to 1 to offer explicit guidance to the DDQN so that it can efficiently learn to distinguish between good and bad actions. ", "page_idx": 4}, {"type": "text", "text": "Dueling DDQN Optimization. The dueling deep Q-network consists of two streams as shown in Fig. 1: state-value estimation stream $V(s;\\Theta_{E},\\Theta_{V})$ parameterized with $\\Theta_{E}$ and $\\Theta_{V}$ and advantages estimation stream $A(s,a;\\Theta_{E},\\Theta_{A})$ for each action which is parameterized with $\\Theta_{E}$ and $\\Theta_{A}$ . Here, $\\Theta_{E}$ is a shared encoder. Furthermore, to balance the exploitation and exploration, we adopt the NoisyNet [27] for the fully-connected layers in $\\Theta_{E},\\,\\Theta_{V}$ and $\\Theta_{A}$ . Besides, a replay buffer $\\mathcal{M}$ is designed to store the historical experience $(s_{k},a_{k},r_{k},s_{k+1},d)$ , where $d\\in\\{0,1\\}$ indicates whether the next step $k+1$ is the terminal step $[d=0]$ ) or not $[d=1$ ). A batch of entries in $\\mathcal{M}$ will be randomly sampled out for DDQN optimization. ", "page_idx": 4}, {"type": "text", "text": "To train the dueling DDQN (i.e., the online network $\\mathcal{Q}$ ), another target Q-network $\\mathcal{Q}^{\\prime}$ is desired, which has identical network architecture as $\\mathcal{Q}$ but is optimized in a different way. Specifically, the online network $\\mathcal{Q}$ is to estimate the $Q$ -values $Q_{e s t}$ by aggregating two steams via Eq. (4): ", "page_idx": 4}, {"type": "equation", "text": "$$\nQ_{e s t}=V(s;\\Theta_{E},\\Theta_{V})+A(s,a;\\Theta_{E},\\Theta_{A})-\\frac{1}{2}\\sum_{a_{i}}A(s,a_{i};\\Theta_{E},\\Theta_{A}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The target $\\mathrm{^Q}$ -network $\\mathcal{Q}^{\\prime}$ is to generate the target $Q$ -values as Eq. (5) shows. Here, $\\Theta\\;=\\;$ $\\{\\Theta_{E},\\Theta_{V},\\Theta_{A}\\}$ , $\\Theta^{\\prime}\\,=\\,\\{\\Theta_{E}^{\\prime},\\Theta_{V}^{\\prime},\\bar{\\Theta}_{A}^{\\prime}\\}$ are the parameters of $\\mathcal{Q}$ and $\\mathcal{Q}^{\\prime}$ , respectively. $\\gamma\\,\\in\\,[0,1]$ is the discount factor to balance the immediate and future rewards. ", "page_idx": 5}, {"type": "equation", "text": "$$\nQ_{t a r}=r_{k}+d*\\gamma*Q(s_{k+1},\\underset{a_{k+1}}{\\operatorname{argmax}}Q(s_{k+1},a_{k+1};\\Theta);\\Theta^{'}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The online network $\\mathcal{Q}$ is optimized by minimizing the Huber loss between $Q_{e s t}$ and $Q_{t a r}$ . The target network $\\mathcal{Q}^{\\prime}$ is updated with a moving average method as shown in Eq. (6), where $\\delta$ is a smoothing parameter determining how much historical information of the online network to be transferred to the target network. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Theta^{\\prime}\\leftarrow\\delta*\\Theta^{\\prime}+(1-\\delta)*\\Theta.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.3 Student Optimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "With the proposed RL module, we can efficiently transfer adaptive knowledge from the teacher model to the student model by dynamically eliminating target samples which are unsuitable for student learning. Particularly, we reformulated Eq. (1) to Eq. (7), where $w=[a_{1}^{*},...,a_{n_{b}}^{*}]$ is the output of online Q-network $\\mathcal{Q}$ . By minimizing $\\mathcal{L}_{R K D}$ , student\u2019s generalization capability on target domain can be effectively enhanced. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{R K D}=\\sum_{x\\in\\mathcal{X}_{b}}w_{j}*\\sum_{i}p_{i}^{T}l o g(p_{i}^{T}/q_{i}^{S}).\\quad(7)\\qquad\\mathcal{L}_{D C}=-\\mathbb{E}[l o g(\\Phi(\\psi(F^{S}(x_{t g t}))))].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Meanwhile, to transfer the domain-invariant knowledge between two domains from teacher to student, we design an adversarial leaning-based module as depicted in Fig. 1, followed [28]. Particularly, a domain discriminator $\\Phi$ is employed to distinguish the source of input feature maps (i.e., whether the feature maps are generated from the teacher with source data as inputs or the student with target data as inputs). Since the dimensions of student\u2019s and teacher\u2019s feature maps are different, an adaptor layer $\\psi$ is employed to match their dimensions. The domain confusion loss is then formulated as Eq. (8). It is worth noting that in our experiments, we utilize the DANN [7] to pre-train the teacher. Although other DA methods can also be adopted in our framework, the DANN can essentially provide a pre-trained accurate domain discriminator after teacher\u2019s training. During transferring domain-invariant knowledge, we can re-utilize it and only optimize the student and the adaptor layer $\\psi$ , which will significantly improve the training efficiency. Meanwhile, it is also possible that one may utilize some other UDA methods to pre-train the teacher. In this case, the domain discriminator $\\Phi$ has to be adversarially trained against the student by minimizing loss $\\mathcal{L}_{a d v}$ as Eq. (9). More experimental results in terms of utilizing other UDA methods to train the teacher can be found in Experiments section. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{a d v}=-\\mathbb{E}[l o g\\Phi(F^{T}(x_{s r c}))]-\\mathbb{E}[l o g(1-\\Phi(\\psi(F^{S}(x_{t g t}))))].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The overall loss for student optimization is calculated via Eq. (10). $\\lambda$ is a hyperparameter to balance the contribution of each part. Algorithm 1 shows details of proposed RCD-KD. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{D C}+\\lambda*\\tau^{2}*\\mathcal{L}_{R K D}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. To evaluate our method, extensive experiments are conducted on four public datasets across three different tasks, namely human activity recognition, rolling bearing fault diagnosis and sleep stage classification. To be specific, the first dataset is called human activity recognition (HAR) [29] for identifying subject\u2019s activities (i.e,, walk, walk upstairs, walk downstairs, stand and sit). Sensory measurements from the accelerometer and gyroscope embedded in a smartphone were ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Proposed RCD-KD ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Input: Teacher $T$ , Student $S$ , adaptation model $\\psi$ , domain discriminator $\\Phi$ , online and target Qcollected from 30 subjects. Considering the variability among subjects, each subject is considered as a single domain and the adaptation is performed between two subjects. Here, we follow existing works [30; 4] and select five transfer scenarios. The second evaluation dataset is Heterogeneity human activity recognition (HHAR) [31]. Compared to HAR, the sensory measurements are collected with various models of smartphones from different manufacturers which are positioned with various orientations on subjects. Thus, the domain gaps between different subjects are generally considered to be larger than HAR. Five transfer scenarios are selected for evaluation same as previous work [32]. The third dataset is rolling bearing fault diagnosis (FD) [33] which aims to classify the health status of rolling bearing from healthy, artificial damages, damages from accelerated lifetime tests. The rolling bearing are tested under various operation conditions. Same as [4; 34], five transfer scenarios between different operational configurations are selected for fair comparison. The last evaluation dataset is sleep stage classification (SSC) dataset [35], which intends to recognize subject\u2019s sleep stages (i.e., wake, non-rapid eye movement N1, N2, N3 and rapid eye movement stage) with electroencephalography waveform. Five scenarios are evaluated following previous study [34]. ", "page_idx": 6}, {"type": "table", "img_path": "tUHABDZP0Q/tmp/def5747da005106b7b0deab95b307c4558fe217220e1b83f2febd354589e62d4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "tUHABDZP0Q/tmp/e5d689c4590980958349fe0f5ee8718f1aa9415fe41585a25f395e23b6faff30.jpg", "table_caption": ["Table 1: Performance comparison with other UDA methods. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Implementations. For the proposed RL-based sample selection module, we set $\\gamma\\,=\\,0.9$ and $\\delta=0.999$ following [25] in Eq. (5) and (6), respectively. We set $N=10$ to calculate teacher\u2019s entropy for the reward function and $K=5$ for the episodes to generate historical experience. Note that to guarantee fair comparison, we ensure the total training steps of ours and benchmark methods are same. Furthermore, we adopted the 1D-CNN as the backbone of the teacher and student models following [4; 34], where student is a shallow version of teacher with less fliters (See Supplementary for detailed network architectures of $T$ , $S$ , $\\Phi$ and dueling DDQN). For $\\alpha_{1}$ , $\\alpha_{2}$ in Eq. (3) and $\\lambda$ , $\\tau$ in Eq. (10), we use the grid search and set $\\alpha_{1}=0.2$ , $\\alpha_{2}=1.8$ , $\\lambda=1.0$ , $\\tau=2$ for all experiments. More sensitivity analysis regarding N, K, \u03bb, $\\alpha_{1}$ , $\\alpha_{2}$ and $\\tau$ can be found in Supplementary. The averaged macro F1-score with three independent running is reported. ", "page_idx": 6}, {"type": "text", "text": "4.2 Benchmark with UDA methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To demonstrate the effectiveness of our proposed RCD-KD, we first compare it with other advanced UDA methods as shown in Table 1. Note that all of the benchmark UDA methods are directly applied to the compact student. From Table 1, some observations can be found. In most transfer scenarios, directly applying UDA methods (either the metric-based or adversarial-based) can improve the performance of compact student model on target domain. However, these methods perform inconsistently across different tasks. For instance, HoMM performs best on SSC, but worst on FD compared to other UDA methods. Meanwhile, the improvement of these UDA methods is very marginal and large variance can be observed, indicating the challenge of performing domain adaptation with shallow networks. On the contrary, the compact student trained with our proposed RCD-KD consistently performs better than other methods. ", "page_idx": 6}, {"type": "table", "img_path": "tUHABDZP0Q/tmp/eeff32497ea5d1886f53388b1fd0ef5dfb9bf99aff4330c42639af810286096a.jpg", "table_caption": ["Table 2: Marco F1-scores on HAR and HHAR across three independent runs. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "tUHABDZP0Q/tmp/c99485511f03a81a9bc58bd514721afcf7fbc2f9bf5bbdf5f1d05e0bc7b64e41.jpg", "table_caption": ["Table 3: Marco F1-scores on FD and SSC across three independent runs. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.3 Benchmark with KD-based DA methods ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare our proposed method with other KD-based DA methods as shown in Table 2 and Table 3. We applied above methods on our teacher-student settings. We also report the performance of pretrained teacher (generally considered as the upper limit) and the student trained on source domain but tested on target domain (namely, Student-Only) as the lower limit. We highlight the best performance with bold for each scenario and the averaged performance. Note that this comparison does not include the teacher as it benefits from more complex network architecture. See Supplementary for more experimental results of additional transfer scenarios. ", "page_idx": 7}, {"type": "text", "text": "Some observations can be found from above two tables. Firstly, compared to Student-Only, all the benchmark methods can obviously improve compact student\u2019s generalization on target domain. However, some of them (e.g., KD-STDA in HAR, FD and SSC, KA-MCD in HAR and FD) even perform worse than directly applying UDA on compact student (e.g., MDDA and SASA in Table 1). The reason is that those methods blindly trust teacher\u2019s predictions on target domain as mentioned and learning with such unreliable knowledge will result in inferior performance. Secondly, the methods using source-only teachers (i.e., AAD and MobileDA) failed to achieve better performance than others (e.g., UNI-KD and MLD-DA) which employ teachers trained on both source and target domains. This observation indicates that for cross-domain KD, it is critical for the teacher to possess the knowledge of both domains. Thirdly, introducing the source domain specific knowledge to the student like KD-STDA and MLD-DA apparently cannot guarantee better generalization on target data. Intuitively, the student still needs to pay more attention on target domain or focus on domainshared knowledge as UNI-KD suggested. Lastly, our proposed method consistently outperforms other benchmarks over all the datasets and achieves the highest score in most of transfer scenarios. Meanwhile, our RCD-KD can even achieve comparable performance as the teacher model in some datasets (e.g., HAR, HHAR and FD) with more compact model architectures. It indicates the effectiveness of transferring the adaptive knowledge via proposed RL-based sample selection module and domain-invariant knowledge via domain discriminator. ", "page_idx": 7}, {"type": "image", "img_path": "tUHABDZP0Q/tmp/5e535b7c68f3448a38db9d758f13be7bcf28933db827101b49186f405beefe21.jpg", "img_caption": ["Table 4: Teacher with different UDA methods. Table 5: Framework ablation for proposed method. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "tUHABDZP0Q/tmp/31db82f5c1158cecb014ee34d539547fcfa767e83d31a7d69334026819d29df2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Different Teacher-Student $(\\mathbf{T}-\\mathbf{S})$ pairs. Besides transferring the knowledge from a cumbersome 1D-CNN teacher to a compact 1D-CNN student following [4], we also evaluate our proposed method with other $\\mathbf{T}-\\mathbf{S}$ pairs. Specifically, we adopted a temporal convolutional network (TCN) [37] as the teacher and 1D-CNN as the student, a Resnet-34 [38] as the teacher and Resnet-18 as the student as depicted in Fig. 2 and Fig. 3. We can see that our proposed method consistently outperforms other benchmark methods, further indicating its effectiveness under different $\\mathbf{T}-\\mathbf{S}$ configurations. ", "page_idx": 8}, {"type": "text", "text": "Different Teacher Generation. As stated in the Methodology section, our proposed method reutilizes the domain discriminator after teacher\u2019s training with DANN. To further demonstrate that our framework can be generalized to other UDA methods, we evaluate our framework with teachers generated by various DA methods as shown in Table 4. Specifically, we utilize two discrepancy-based DA methods, i.e., MDDA [5] and SASA [36] and one additional adversarial-based DA method named CoDATS [8]. Note that for teachers generated from MDDA and SASA, we need to train the domain discriminator as shown in Algorithm 1. On the contrary, for teachers generated from CoDATS and DANN (ours), the parameters of discriminator are frozen during student\u2019s training. From Table 4, we can see that employing teachers from MDDA and SASA slightly underperforms CoDATS and DANN. The possible reason is that adding additional training steps for domain discriminator will inevitably increase training difficulty in terms of model convergence. But they still perform better than other benchmarks in Tables 2 and 3, indicating that our approach is not limited by the teacher training strategies as long as the teacher possesses the source and target domain knowledge. A well pre-trained domain discriminator is preferred as it can improve the training efficiency. ", "page_idx": 8}, {"type": "text", "text": "Framework Ablation. There are two key components in our proposed framework: the domaininvariant knowledge transferring via $\\mathcal{L}_{D C}$ and the RL-based domain adaptive knowledge transferring via $\\mathcal{L}_{R K D}$ . We conduct the ablation study to evaluate their contributions. We also include the standard knowledge distillation in which the student is optimized with $\\mathcal{L}_{K D}$ as Eq. (1) shows. From Table 5, we can see that coarsely aligning teacher\u2019s and student\u2019s predictions over all target samples $(\\mathcal{L}_{K D}+\\mathcal{L}_{D C})$ might lead to negative transferring in some datasets (e.g., HHAR and SSC), whose performance is inferior to the one only applying $\\mathcal{L}_{D C}$ . Our proposed method $(\\mathcal{L}_{D C}+\\mathcal{L}_{R K D})$ can significantly mitigate this issue via the proposed RL-based target sample selection module. ", "page_idx": 8}, {"type": "text", "text": "Reinforced Sample Selection Ablation. To investigate the contribution of proposed reward and RL-based model selection module, we conduct the ablation study as shown in Table 6. Some observation can be found from above table. Firstly, including all the target samples to train the compact student will inevitable introduce negative transfer, resulting in unsatisfying generalization performance on target domain. Either utilizing the model uncertainty or transferability to explicitly select target sample (as shown in column $\\mathcal{R}_{2}$ and $\\mathcal{R}_{3}$ ) would improve student\u2019s performance in most of datasets. Secondly, dynamically selecting target samples using our RL-base module with either of proposed rewards (as shown in column $\\bar{\\mathcal{R}}_{2}^{\\dagger}$ and $\\mathcal{R}_{3}^{\\dagger}$ ) will further improve student\u2019s performance, indicating the effectiveness of RL-based module in mitigating negative transfer. Lastly, combining model uncertainty and transferability as the reward to dynamically select suitable target samples based on student\u2019s capacity yields best performance. ", "page_idx": 8}, {"type": "table", "img_path": "tUHABDZP0Q/tmp/d39611393f2ee29afe28c971d03f790abec541656eb4280aabf77db9cfb68544.jpg", "table_caption": ["Table 6: Reinforced sample selection ablation. \u201cFull samples\" denotes utilizing whole target samples for KD; $\\mathbf{\\nabla}\\mathcal{R}_{2}$ \u2019, $\\mathbf{\\nabla}\\mathcal{R}_{3}$ \u2019 denote directly utilizing proposed uncertainty and transferability for sample selection; $\\mathbf{\\mathcal{R}}_{2}^{\\dagger},\\mathbf{\\mathcal{R}}_{3}^{\\dagger}$ \u2019 denote utilizing RL with $\\mathcal{R}_{2}$ and ${\\mathcal{R}}_{3}$ as reward for sample selection; $(\\mathcal{R}_{2}+\\mathcal{R}_{3})^{\\dagger}$ is our proposed method. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "tUHABDZP0Q/tmp/ed6a99a6508c54081aeabc99df3ebbeb9b2ab62486e405b8d26affc7bf66c228.jpg", "table_caption": ["Table 7: Comparison of Computational Complexity. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Computational Complexity. We performed the time complexity analysis for our method and the results are shown in Table 7. Specifically, we measure the training time for our proposed method and other benchmarks with a NVIDIA 2080Ti GPU. The reported results are measured with one epoch on single transfer scenario on FD dataset, which has the largest training samples (about 1,800 samples per transfer scenario) among evaluated datasets. We can see that our method does require more training time compared to other benchmarks, reflecting its greater complexity. The primary computational costs arise from two factors. The first part is the generation of $K$ historical experiences at each step. This could be significantly reduced by using a smaller K. The second factor is the MCD module which conducts multiple inference processes for uncertainty estimation. This computational burden could be further decreased by adopting alternative uncertainty estimation methods. Nevertheless, although our training time is longer than other benchmarks, we argue that it is still within an acceptable range, especially considering the performance improvement it could bring. Meanwhile, we also evaluate the scalability of our approach to larger dataset. See Supplementary for more scalability analysis. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a framework for cross-domain knowledge distillation on time series. Specifically, we utilize an adversarial domain discriminator to assist the compact student learn the domain-invariant knowledge from the cumbersome teacher. Meanwhile, we design a reinforcement learning-based target sample selection module to effectively transfer teacher\u2019s knowledge which is suitable for compact student. The experimental results demonstrate the effectiveness of our proposed method in enhancing the generalization of compact student on target domain. There are also some limitations for our proposed framework. On the one hand, we still need to pre-train a cumbersome teacher with advanced UDA methods, which involves more training time than others. On the other hand, we only utilize the distance between teacher\u2019s and student\u2019s logits to assess sample\u2019s transferability, which might overlook some intrinsic information from feature space. In the future, we will extend our work to (1) jointly train teacher and student for cross-domain knowledge distillation, and (2) consider feature representations into sample transferability assessment. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is supported by the Agency for Science, Technology and Research (A\\*STAR) Singapore under its NRF AME Young Individual Research Grant (Grant No. A2084c0167) and the National Research Foundation, Singapore under its AI Singapore Programme (AISG2-RP-2021-027). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Hangwei Qian, Sinno Jialin Pan, Bingshui Da, and Chunyan Miao. A novel distributionembedded neural network for sensor-based activity recognition. In IJCAI, volume 2019, pages 5614\u20135620, 2019.   \n[2] Bin Zhou, Shenghua Liu, Bryan Hooi, Xueqi Cheng, and Jing Ye. Beatgan: Anomalous rhythm detection using adversarially generated time series. In IJCAI, volume 2019, pages 4433\u20134439, 2019.   \n[3] Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, and Bixiong Xu. Ts2vec: Towards universal representation of time series. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8980\u20138987, 2022.   \n[4] Qing Xu, Min Wu, Xiaoli Li, Kezhi Mao, and Zhenghua Chen. Distilling universal and joint knowledge for cross-domain model compression on time series data. In Edith Elkind, editor, Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23, pages 4460\u20134468. International Joint Conferences on Artificial Intelligence Organization, 8 2023. Main Track.   \n[5] Mohammad Mahfujur Rahman, Clinton Fookes, Mahsa Baktashmotlagh, and Sridha Sridharan. On minimum discrepancy estimation for deep domain adaptation. In Domain Adaptation for Visual Understanding, pages 81\u201394. Springer, 2020.   \n[6] Chao Chen, Zhihang Fu, Zhihong Chen, Sheng Jin, Zhaowei Cheng, Xinyu Jin, and XianSheng Hua. Homm: Higher-order moment matching for unsupervised domain adaptation. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 3422\u20133429, 2020.   \n[7] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096\u20132030, 2016. [8] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain adaptation. Advances in neural information processing systems, 31, 2018.   \n[9] Junguang Jiang, Ximei Wang, Mingsheng Long, and Jianmin Wang. Resource efficient domain adaptation. In Proceedings of the 28th ACM International Conference on Multimedia, pages 2220\u20132228, 2020.   \n[10] Wei Li, Lingqiao Li, and Huihua Yang. Progressive cross-domain knowledge distillation for efficient unsupervised domain adaptive object detection. Engineering Applications of Artificial Intelligence, 119:105774, 2023.   \n[11] Atif Belal, Madhu Kiran, Jose Dolz, Louis-Antoine Blais-Morin, Eric Granger, et al. Knowledge distillation methods for efficient unsupervised adaptation across multiple domains. Image and Vision Computing, 108:104096, 2021.   \n[12] Divya Kothandaraman, Athira Nambiar, and Anurag Mittal. Domain adaptive knowledge distillation for driving scene semantic segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 134\u2013143, 2021.   \n[13] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. NIPS Deep Learning and Representation Learning Workshop, 2015.   \n[14] Jianfei Yang, Han Zou, Shuxin Cao, Zhenghua Chen, and Lihua Xie. Mobileda: Toward edge-domain adaptation. IEEE Internet of Things Journal, 7(8):6909\u20136918, 2020.   \n[15] Minho Ryu, Geonseok Lee, and Kichun Lee. Knowledge distillation for bert unsupervised domain adaptation. Knowledge and Information Systems, 64(11):3113\u20133128, 2022.   \n[16] Sebastian Ruder, Parsa Ghaffari, and John G Breslin. Knowledge adaptation: Teaching to adapt. arXiv preprint arXiv:1702.02052, 2017.   \n[17] Aron Culotta and Andrew McCallum. Reducing labeling effort for structured prediction tasks. In AAAI, volume 5, pages 746\u2013751, 2005.   \n[18] Jingbo Zhu, Huizhen Wang, Benjamin K Tsou, and Matthew Ma. Active learning with sampling by uncertainty and density for data annotations. IEEE Transactions on audio, speech, and language processing, 18(6):1323\u20131331, 2009.   \n[19] Colin Campbell, Nello Cristianini, Alex Smola, et al. Query learning with large margin classifiers. In ICML, volume 20, page 0, 2000.   \n[20] Alaa Tharwat and Wolfram Schenck. A survey on active learning: State-of-the-art, practical challenges and research directions. Mathematics, 11(4):820, 2023.   \n[21] Michael C Burl and Esther Wang. Active learning for directed exploration of complex systems. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 89\u201396, 2009.   \n[22] Seokhwan Kim, Yu Song, Kyungduk Kim, Jeong-Won Cha, and Gary Geunbae Lee. Mmr-based active machine learning for bio named entity recognition. In Proceedings of the human language technology conference of the NAACL, Companion Volume: Short Papers, pages 69\u201372, 2006.   \n[23] Zhihong Chen, Chao Chen, Zhaowei Cheng, Boyuan Jiang, Ke Fang, and Xinyu Jin. Selective transfer with reinforced transfer network for partial domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12706\u201312714, 2020.   \n[24] Keyu Wu, Min Wu, Zhenghua Chen, Ruibing Jin, Wei Cui, Zhiguang Cao, and Xiaoli Li. Reinforced adaptation network for partial domain adaptation. IEEE Transactions on Circuits and Systems for Video Technology, 2022.   \n[25] Keyu Wu, Min Wu, Jianfei Yang, Zhenghua Chen, Zhengguo Li, and Xiaoli Li. Deep reinforcement learning boosted partial domain adaptation. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 3192\u20133199. International Joint Conferences on Artificial Intelligence Organization, 8 2021.   \n[26] Andreas Damianou and Neil D Lawrence. Deep gaussian processes. In Artificial intelligence and statistics, pages 207\u2013215. PMLR, 2013.   \n[27] Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration. arXiv preprint arXiv:1706.10295, 2017.   \n[28] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7167\u20137176, 2017.   \n[29] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra Perez, and Jorge Luis Reyes Ortiz. A public domain dataset for human activity recognition using smartphones. In Proceedings of the 21th international European symposium on artificial neural networks, computational intelligence and machine learning, pages 437\u2013442, 2013.   \n[30] Garrett Wilson, Janardhan Rao Doppa, and Diane J Cook. Multi-source deep domain adaptation with weak supervision for time-series sensor data. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 1768\u20131778, 2020.   \n[31] Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow, Mikkel Baun Kj\u00e6rgaard, Anind Dey, Tobias Sonne, and Mads M\u00f8ller Jensen. Smart devices are different: Assessing and mitigatingmobile sensing heterogeneities for activity recognition. In Proceedings of the 13th ACM conference on embedded networked sensor systems, pages 127\u2013140, 2015.   \n[32] Qiao Liu and Hui Xue. Adversarial spectral kernel matching for unsupervised time series domain adaptation. In IJCAI, pages 2744\u20132750, 2021.   \n[33] Christian Lessmeier, James Kuria Kimotho, Detmar Zimmer, and Walter Sextro. Condition monitoring of bearing damage in electromechanical drive systems by using motor current signals of electric motors: A benchmark data set for data-driven classification. In PHM Society European Conference, volume 3, 2016.   \n[34] Mohamed Ragab, Emadeldeen Eldele, Wee Ling Tan, Chuan-Sheng Foo, Zhenghua Chen, Min Wu, Chee-Keong Kwoh, and Xiaoli Li. Adatime: A benchmarking suite for domain adaptation on time series data. ACM Transactions on Knowledge Discovery from Data, 17(8):1\u201318, 2023.   \n[35] Ary L Goldberger, Luis AN Amaral, Leon Glass, Jeffrey M Hausdorff, Plamen Ch Ivanov, Roger G Mark, Joseph E Mietus, George B Moody, Chung-Kang Peng, and H Eugene Stanley. Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals. circulation, 101:e215\u2013e220, 2000.   \n[36] Ruichu Cai, Jiawei Chen, Zijian Li, Wei Chen, Keli Zhang, Junjian Ye, Zhuozhang Li, Xiaoyan Yang, and Zhenjie Zhang. Time series domain adaptation via sparse associative structure alignment. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 6859\u20136867, 2021.   \n[37] Markus Thill, Wolfgang Konen, and Thomas B\u00e4ck. Time series encodings with temporal convolutional networks. In International Conference on Bioinspired Methods and Their Applications, pages 161\u2013173. Springer, 2020.   \n[38] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Supplementary ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Uncertainty Estimation with Monte Carlo Dropout Method ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Given an input data set $X=\\{x_{1},...,x_{n}\\}$ and the respective outputs $Y=\\{y_{1},...,y_{n}\\}$ , the conventional machine learning methods intend to find an optimal model $\\Phi(x;\\theta)$ , which is parameterized with $\\pmb{\\theta}$ , to map the input $X$ to the $Y$ . After training, the optimal model $\\Phi(x;\\theta)$ will give a single point prediction for certain test sample with static $\\pmb{\\theta}$ . On the contrary, the Bayesian methods (e.g., Bayesian neural networks) can generate predictive distributions instead of a single point prediction for estimating model uncertainty. With defining $\\Phi(x;\\theta)$ with a prior $P(\\pmb\\theta)$ over parameter space $\\pmb{\\theta}$ , the training objective is then turned to find an optimal posterior distribution over $\\pmb{\\theta}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\nP(\\pmb\\theta|X,Y)=\\frac{P(Y|X,\\pmb\\theta)P(\\pmb\\theta)}{P(Y|X)}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The prediction value of $y$ with input $x$ is the weighted average of model predictions over all possible sets of parameters $\\pmb{\\theta}$ with various posterior probabilities as Eq. (12) shows. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle P(y|x,X,Y)=\\int P(y|x,\\pmb\\theta)P(\\pmb\\theta|X,Y)d\\pmb\\theta}\\\\ {\\displaystyle\\qquad\\qquad=\\mathbb{E}_{\\pmb\\theta\\sim P(\\pmb\\theta|X,Y)}[\\Phi(x;\\pmb\\theta)]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "However, the posterior distribution $P(\\pmb\\theta|X,Y)$ is intractable as shown in previous works. Alternatively, Gal and Ghahramani proved that a DNN with arbitrary non-linear depth and dropout is mathematically equivalent to a Bayesian approximation of the probabilistic deep Gaussian process. They proposed a method named Monte Carlo Dropout which utilizes a dropout distribution $\\hat{P}(\\pmb\\theta)$ to approximate $P(\\pmb\\theta|X,Y)$ . To be specific, for the $l$ -th layer $(l=1,...,L)$ in a model with total $L$ layers, the parameter distribution $\\theta_{l}$ is defined as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{l}=\\mathbf{M}_{l}*d i a g([Z_{l,i}]_{i=1}^{D_{l}}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathbf{M}_{l}\\in\\mathcal{R}^{D_{l}\\times D_{l-1}}$ is a matrix with variational parameters and $d i a g(\\cdot)$ is an operator to map a vector to a diagonal matrix. $Z_{l,i}\\ \\sim\\ B e r n o u l l i(\\bar{q}_{i})$ is independently sampled from Bernoulli distribution, where $i=1,...,D_{l-1}$ . $q_{i}$ is the probability of dropout. Subsequently, the Eq. (12) is reformulated as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta\\sim\\hat{P}(\\theta)}[\\Phi(x;\\theta)]\\approx\\frac{1}{N}\\sum_{n=1}^{N}\\Phi(x;\\theta^{n}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Practically, Eq. (14) means to enable the dropout of model during test phase and forward $N$ times for each sample $x_{i}$ . Furthermore, for classification tasks we employ the entropy to measure teacher\u2019s uncertainty on target data as Eq. (15) shows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{H}_{i}=-\\sum_{c}(\\frac{1}{N}\\sum_{n}p(y=c|x_{i},\\pmb{\\theta}^{n})l o g(\\frac{1}{N}\\sum_{n}p(y=c|x_{i},\\pmb{\\theta}^{n}))),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $p(y=c|x_{i},\\pmb\\theta^{n})$ represents the probability of sample belong to class $c$ and it is the softmax outputs of teacher model $\\Phi(x;\\theta^{n})$ on the $n$ -th forward pass. Intuitively, a higher value of the predictive entropy $\\mathcal{H}_{i}$ will be obtained when all classes are predicted to have equal probabilities, which means the teacher is less confident about the specific data. ", "page_idx": 13}, {"type": "text", "text": "A.2 Model Architecture ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.2.1 Teacher and Student model ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "As illustrated in Fig. 4(a) and (b), the teacher and student model are constructed with three stacked CNN blocks as the backbone and one fully connected layer as the classifier. Each CNN block consists of a 1D convolutional layer, followed by a BatchNorm layer, an activation layer (ReLU), a 1D MaxPooling layer and a Dropout layer. Here, \u2018Conv1D\u2019 represents the 1D convolutional layer and the first variable in the bracket represents the number of input channels and the second one represents the number of output channels. \u2018BN\u2019 is a BatchNorm layer. \u2018FC\u2019 represents a fully connected layer. \u2018C\u2019 represents the number of classes. ", "page_idx": 13}, {"type": "image", "img_path": "tUHABDZP0Q/tmp/d928016e358de30259c12fe65080d6126aa8ae12d274a7f71fa2d3e76758219e.jpg", "img_caption": ["Figure 4: Network Architectures for (a) 1D-CNN Teacher, (b) 1D-CNN Student and (c) Domain Discriminator. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Meanwhile, to demonstrate the deployment on edge device we compared our 1D-CNN teacher and student from four perspectives as shown in Table 8. Here, we employ Raspberry Pi $^{3\\mathrm{B}+}$ as the edge device for deployment. We can see that the student reduces 15.46 times parameters, 16.98 times FLOPs and 13.54 times memory usage compared to its teacher. Besides, the inference of student is 21.67 times faster than teacher on the edge device. Meanwhile, in our manuscript we have already shown that the student trained with our method is able to achieve comparable performance as the teacher. This enables our compact student to potentially meet the real-time response and on-site deployment requirements for certain time series applications. ", "page_idx": 14}, {"type": "table", "img_path": "tUHABDZP0Q/tmp/ac9246cb8629a54321f1253a841a11a88c9faf23e84edfdfee54442663413478.jpg", "table_caption": ["Table 8: Comparison of model complexity. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.2.2 Domain discriminator ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Fig. 4(c) is the network architecture of domain discriminator. It consists of three linear layer followed by ReLU activation layers. The output of domain discriminator is a 2-classes probability distribution to indicate whether the feature maps come from the teacher with source domain data as input or from the student with target domain data as input. ", "page_idx": 14}, {"type": "text", "text": "A.2.3 Dueling DDQN ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table 9 presents the details of our dueling DDQN. The left column is the state-value estimation stream and the right column is the advantage estimation column. The \u2018NoisyFC\u2019 represents a linear layer whose weights and biases are perturbed by a parametric function of the noise. The conventional linear layer can be expressed as $y=w x+b$ , where $w\\in\\mathbb{R}^{q\\times p}$ , $b\\in\\mathbb{R}^{q}$ are trainable weights and biases, $x\\in\\mathbb{R}^{p}$ and $y\\in\\mathbb{R}^{q}$ are the inputs and outputs, respectively. In the NoisyNets, the weights and biases are reformulated as Eq. (16) and (17), respectively. Here, $\\mu^{w}\\in\\mathbb{R}^{q\\times p}$ , $\\sigma^{w}\\in\\mathbb{R}^{q\\times p}$ , $\\bar{\\mu}^{b}\\in\\mathbb{R}^{q}$ , $\\sigma^{b}\\in\\mathbb{R}^{q}$ are the trainable weights and biases. $\\odot$ is the element-wise multiplication. $\\epsilon^{w}$ and $\\dot{\\epsilon}^{b}$ are the factorised Gaussian noise, where each entry $\\epsilon_{i,j}^{w}=f(\\epsilon_{i})f(\\epsilon_{j})$ , $\\epsilon_{j}^{b}=f(\\epsilon_{j})$ and $f(x)=s g n(x)\\sqrt{|x|}$ . Adding such parametric noise to the deep reinforcement learning agent will enhance the efficiency of exploration. ", "page_idx": 14}, {"type": "table", "img_path": "tUHABDZP0Q/tmp/c8dc8e2fee9659ff871b62ddb75f187d2d69462b693cd951aaedf3edd20fb057.jpg", "table_caption": ["Table 9: Network Architecture for Dueling DDQN. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\nw=\\mu^{w}+\\sigma^{w}\\odot\\epsilon^{w}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\nb=\\mu^{b}+\\sigma^{b}\\odot\\epsilon^{b}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.3 Additional Transfer Scenarios ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We evaluate our proposed method on another five additional transfer scenarios on four datasets as shown in Table 10 and Table 11. From above two Tables, we can sse that our proposed method consistently achieves better performance than other SOTA methods, further indicating its effectiveness in transferring the knowledge under the cross-domain scenarios. ", "page_idx": 15}, {"type": "table", "img_path": "tUHABDZP0Q/tmp/467052ba9f520cf629753889e88841d67f82e3858d3f001e7296744d289edfb2.jpg", "table_caption": ["Table 10: Marco F1-scores on HAR and HHAR across three independent runs. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "tUHABDZP0Q/tmp/4a4b7dd2ae7b9900a2e32cbcd7e6090bbe4cda8bd8c61327f31b5db7feeef6b5.jpg", "table_caption": ["Table 11: Marco F1-scores on FD and SSC across three independent runs. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.4 Sensitivity Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.4.1 Hyper parameter $N$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In our proposed method, one of the key hyper parameters is $N$ , which is the number of teachers utilized for calculating the uncertainty on a specific sample. It relates to our reward function, thus affecting the learning process of target sample selection policy. Intuitively, the larger $N$ will lead to more accurate estimation of teacher\u2019s uncertainty and provide more accurate reward. As illustrated in Fig. 5, student\u2019s performance is gradually increased with $N$ but will keep at some certain level when $N\\ge10$ . The larger value of $N$ also increases the total cost in terms of training time. Empirically, $N=5$ or $N=10$ are appropriate, and we choose $N=10$ in our experiments for all the datasets. ", "page_idx": 15}, {"type": "image", "img_path": "tUHABDZP0Q/tmp/809d4d1d51317a7519a63eaec1d432173a6225c32cb717ae50bb5ffc85b49eb7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "tUHABDZP0Q/tmp/7960ae4e3230d2ad444369259c8b95f2c858ff121fcc5fb16030c79955144e47.jpg", "table_caption": ["Table 12: Sensitivity Analysis on $\\alpha_{1}$ and $\\alpha_{2}$ . "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "A.5 Hyper parameter $K$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Another hyper parameter in our proposed approach is the episodes length $K$ for generating historical experience and we perform the analysis as illustrated in Fig. 6. From Fig. 6, we can see that our proposed method is not very sensitive to $K$ . But a large value of $K$ will result in longer training time. $K=5$ is sufficient to generate informative historical experience for training the dueling DDQN. ", "page_idx": 16}, {"type": "text", "text": "A.5.1 Hyper parameter $\\lambda$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Regarding hyper parameter $\\lambda$ which is to balance the contribution of domain confusion loss $\\mathcal{L}_{D C}$ and the distillation loss $\\mathcal{L}_{R K D}$ , we can see from Fig. 7 that a small value of $\\lambda$ (e.g. $\\lambda=0.1$ ) will make the student more focus on learning domain-invariant knowledge from $\\mathcal{L}_{D C}$ . It will result in worse performance in datasets like HHAR and SSC. A higher value of $\\lambda$ will obviously enhance student\u2019s generalization capability on target domain. $\\lambda\\in[1,5]$ is a suitable range based on our experiment results. ", "page_idx": 16}, {"type": "text", "text": "A.5.2 Hyper parameter $\\alpha_{1}$ and $\\alpha_{2}$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To limit our reward within the range of $^-1$ to 1, $\\alpha_{1}$ and $\\alpha_{2}$ should satisfy below constrains: $\\alpha_{1}\\in(0,2)$ , $\\alpha_{2}\\in(0,2)$ and $\\alpha_{1}+\\alpha_{2}=2$ . We perform grid search on four datasets as shown in Table 12. We can see that the student trained with low $\\alpha_{1}$ value and high $\\alpha_{2}$ value can achieve better performance than other configurations, indicating transferability contributes more to the final performance than uncertainty. In all of our experiments, we set $\\alpha_{1}=0.2$ and $\\alpha_{2}=1.8$ for all datasets for simplicity. ", "page_idx": 16}, {"type": "text", "text": "A.5.3 Hyper parameter $\\tau$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For hyper parameter $\\tau$ which is the temperature to soften teacher\u2019s logits, we perform the analysis ranged from 1 to 16 as shown in Table 13. We can see that higher value of temperature (e.g., $\\tau=16$ ) would over-smooth teacher\u2019s logits, resulting in poor distillation performance. Generally, $\\tau=2$ or $\\tau=4$ is a good choice for our method. ", "page_idx": 16}, {"type": "table", "img_path": "tUHABDZP0Q/tmp/d5a58c956ca267a42d167f7a05cea602737e0988107ab17c219f40a48f180828.jpg", "table_caption": ["Table 13: Sensitivity Analysis for temperature $\\tau$ "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "tUHABDZP0Q/tmp/37ebe067c9114c91ebdc326bd7d2a90e987e1fa58ee2fafe20a8b90853f406bb.jpg", "table_caption": ["Table 14: Comparison with different sample selection strategies in AL. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "A.6 Comparison with sample selection strategies in Active Learning ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We conduct the ablation study on three uncertainty-based active learning (AL) strategies, including least confidence (LC), sample margin (M) and sample entropy (H). The results are presented in Table 14. We take student trained with our framework using whole target samples as the baseline (i.e., without RL). \u2018LC\u2019 refers to leveraging student\u2019s confidence to directly select samples. \u2018LC Consist.\u2019 refers to using the consistency of teacher\u2019s and student\u2019s confidence for explicitly sample selection. \u2018LC Consist. $+\\,{\\mathrm{RL}}^{\\,,}$ refers to leveraging \u2018LC Consist.\u2019 as reward to learn optimal sample selection policy. We can see that: firstly, almost all uncertainty-based AL strategies exhibit performance degradation compared to the baseline. This could be attributed to the unreliable uncertainty estimation from student\u2019s outputs, especially at early training stage. Additionally, among these strategies, entropy performs the best, likely because it considers the overall probability distribution which might partially address student\u2019s unreliable predictions issue. Secondly, utilizing uncertainty consistency instead of uncertainty alone could enhance performance in most settings, as incorporating teacher\u2019s knowledge through consistency provides a more reliable measure. Lastly, our RL module could further enhance student\u2019s performance via employing any of uncertainty consistency as the reward, indicating its effectiveness. ", "page_idx": 17}, {"type": "text", "text": "A.7 Scalability to Larger Datasets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To verify the efficiency and scalability of our method on larger time series (TS) dataset, we conduct experiments on another Human Activity Recognition dataset named PAMAP2. Table 15 compares the dataset complexity of PAMAP2 with the datasets we employed in our manuscript. Note that it is meaningless to compare the total size of datasets in our settings as our experiment evaluate transfer scenario between single subject. We summarize the averaged number of samples, channels, data length and classes across all transfer scenarios for these datasets. We also report the time complexity of our method across these datasets (i.e., training time per epoch for single transfer scenario). From Table 15, we can see PAMAP2 is larger in terms of number of samples and more complex in terms of number of channels and classes. Compared with FD, the epoch training time for PAMAP2 only increases 2 times as the number of training samples increases about 4 times, indicating that our method scales well in terms of computational efficiency on larger TS dataset. ", "page_idx": 17}, {"type": "text", "text": "Meanwhile, we also conduct the performance comparison between our method and benchmarks on PAMAP2 with randomly selected 5 transfer scenarios. The experimental results are summarized as Table 16. We can see that our proposed method consistently outperform other benchmarks in terms of average Macro F1-score, even though it cannot achieve the best performance on some transfer scenarios. This observation indicates that the effectiveness of our proposed method can also be generalized to larger time series dataset. ", "page_idx": 17}, {"type": "table", "img_path": "tUHABDZP0Q/tmp/b594d73a7f1ca69bb1ed27b2fd3c67311b1caa2cb9421546654461444f18edb9.jpg", "table_caption": ["Table 15: Summary of Datasets. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "tUHABDZP0Q/tmp/6f1bb9c0c9a509e0134efb3cd6cf49a4d38642427a436918c6477929cd64c074.jpg", "table_caption": ["Table 16: Performance Comparison on PAMAP2. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The abstract and introduction include our main claims in the paper. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We have discussed the limitations of our work in the conclusion section. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: This paper does not include any theoretical assumptions and proofs. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have provided the details of model architecture, experimental configurations, the value of key parameters in the paper. Meanwhile, we also provided the code in the supplementary for result reproducibility. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The datasets in the paper are public available and the URL can be found in related references. A git repository URL for our proposed method will be provided upon acceptance. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have specify the experimental settings in the paper ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Please refer to the tables and figures in the experiment section. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Please refer to the supplementary for computer resources. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have clearly cite the original papers for the datasets used. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 23}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not release new assets ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]