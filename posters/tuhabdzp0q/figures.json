[{"figure_path": "tUHABDZP0Q/figures/figures_3_1.jpg", "caption": "Figure 1: Illustration of proposed RCD-KD. A Monte Carlo Dropout (MCD) based reward module is utilized to generate the reward for learning the optimal target sample selection policy. Specifically, the reward function consists of three parts. The first one is the action ak which is the output of dueling DDQN. The second part is the uncertainty consistency, estimated by entropy from student's logits q\u00ba and the averaged logitsp of N teachers generated from MCD module. The third part is the sample transferability based on the KL divergence between qs and p. The output of reward module rk then will be utilized for the optimization of dueling DDQN for learning optimal sample selection policy. Meanwhile, a domain discriminator \u03a6 is employed to transfer the domain-invariant knowledge.", "description": "This figure illustrates the proposed Reinforced Cross-Domain Knowledge Distillation (RCD-KD) framework.  It shows the interaction between a teacher and student model, a domain discriminator, and a reinforcement learning module for target sample selection. The reward module incorporates uncertainty consistency and sample transferability to guide the selection of optimal target samples for knowledge transfer. The domain discriminator ensures domain-invariant knowledge transfer.", "section": "3 Methodology"}, {"figure_path": "tUHABDZP0Q/figures/figures_8_1.jpg", "caption": "Figure 1: Illustration of proposed RCD-KD. A Monte Carlo Dropout (MCD) based reward module is utilized to generate the reward for learning the optimal target sample selection policy. Specifically, the reward function consists of three parts. The first one is the action ak which is the output of dueling DDQN. The second part is the uncertainty consistency, estimated by entropy from student's logits q\u00ba and the averaged logitsp of N teachers generated from MCD module. The third part is the sample transferability based on the KL divergence between qs and p. The output of reward module rk then will be utilized for the optimization of dueling DDQN for learning optimal sample selection policy. Meanwhile, a domain discriminator \u03a6 is employed to transfer the domain-invariant knowledge.", "description": "This figure illustrates the proposed Reinforced Cross-Domain Knowledge Distillation (RCD-KD) framework.  It shows how a Monte Carlo Dropout (MCD)-based reward module dynamically selects target samples for knowledge transfer to a student network, based on uncertainty consistency and transferability. A domain discriminator ensures domain-invariant knowledge transfer. The dueling DDQN is used to learn the optimal sample selection policy.", "section": "3 Methodology"}, {"figure_path": "tUHABDZP0Q/figures/figures_14_1.jpg", "caption": "Figure 1: Illustration of proposed RCD-KD. A Monte Carlo Dropout (MCD) based reward module is utilized to generate the reward for learning the optimal target sample selection policy. Specifically, the reward function consists of three parts. The first one is the action ak which is the output of dueling DDQN. The second part is the uncertainty consistency, estimated by entropy from student's logits q\u00ba and the averaged logitsp of N teachers generated from MCD module. The third part is the sample transferability based on the KL divergence between qs and p. The output of reward module rk then will be utilized for the optimization of dueling DDQN for learning optimal sample selection policy. Meanwhile, a domain discriminator \u03a6 is employed to transfer the domain-invariant knowledge.", "description": "This figure illustrates the architecture of the proposed Reinforced Cross-Domain Knowledge Distillation (RCD-KD) framework.  It highlights the key components: a Monte Carlo Dropout (MCD) based reward module for optimal target sample selection, a dueling Double Deep Q-Network (DDQN) for learning the selection policy, and a domain discriminator for transferring domain-invariant knowledge. The reward function considers action, uncertainty consistency, and sample transferability.", "section": "3 Methodology"}, {"figure_path": "tUHABDZP0Q/figures/figures_16_1.jpg", "caption": "Figure 1: Illustration of proposed RCD-KD. A Monte Carlo Dropout (MCD) based reward module is utilized to generate the reward for learning the optimal target sample selection policy. Specifically, the reward function consists of three parts. The first one is the action ak which is the output of dueling DDQN. The second part is the uncertainty consistency, estimated by entropy from student's logits qs and the averaged logits pT of N teachers generated from MCD module. The third part is the sample transferability based on the KL divergence between qs and pT. The output of reward module rk then will be utilized for the optimization of dueling DDQN for learning optimal sample selection policy. Meanwhile, a domain discriminator \u03a6 is employed to transfer the domain-invariant knowledge.", "description": "This figure illustrates the proposed Reinforced Cross-Domain Knowledge Distillation (RCD-KD) framework.  It shows the interaction between the teacher and student models, the reinforcement learning module for sample selection, and the domain discriminator used for domain adaptation. The reward function is based on uncertainty consistency and sample transferability.", "section": "3 Methodology"}]