[{"figure_path": "CyzZeND3LB/tables/tables_3_1.jpg", "caption": "Figure 1: Models with very different CGFs coexist within the same model class. On the left, we display several metrics for InceptionV3 models trained on CIFAR10 without regularization (Standard) and with L2 regularization (L2). Random refers to a model learned over randomly reshuffled labels and Zero refers to a model where all the weights are equal to zero. For each model, the metrics include train and test accuracy, test log-loss, l2-norm of the parameters of the model, the variance of the log-loss function, denoted V\u2081(l(x, 0)), and the expected norm of the input-gradients, denoted E, [||\u2207xl(x, 0)||2]. On the right, we display the estimated CGFs of each model, following Masegosa and Ortega (2023). Note how models with smaller variance V(l(x, 0)), l2-norm or input-gradient norm E, [||\u2207xl(x, 0)||2] have smaller CGFs. Bounds derived from Theorem 7 naturally exploit these differences. Experimental details in Appendix C.", "description": "The table shows metrics for different InceptionV3 models trained on CIFAR-10 dataset.  Models include a standard model, one with L2 regularization, a model with random labels, and a zero-initialized model. Metrics presented are training and testing accuracy, test log-loss, L2 norm of the model parameters, the expected squared L2 norm of input gradients, and the variance of the log-loss. The table also includes a plot of the estimated Cumulant Generating Functions (CGFs) for each model.  The CGFs illustrate that models with lower variance and parameter norms have smaller CGFs.", "section": "Overview and Contributions"}]