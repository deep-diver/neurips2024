[{"figure_path": "HUxtJcQpDS/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of HEALNet (Hybrid Early-fusion Attention Learning Network) using a shared and modality-specific parameter space to learn from structurally different data sources in the same model (Fig. 1A). The shared space is a learned latent embedding S that is iteratively updated through d attention-based fusion layers and captures the shared information between modalities. The hybrid early-fusion layer (Fig. 1B, and Eq. 3) learns the cross-attention weights Wm = {W(\u00b9), W(\u00b2), W(\u00b3)} for each modality m corresponding to the queries (Qm = WS), keys (Km = Wm)Xm), and values (Vm = Wm)Xm) which are shared between layers. These layers capture the structural information of each modality and encode it in the shared embedding after a pass through a self-normalising network (SNN) layer.", "description": "This figure provides a detailed overview of the HEALNet architecture.  It illustrates the hybrid early fusion approach, showcasing how modality-specific information is preserved through separate encoders while cross-modal interactions are captured in a shared latent space. The iterative attention mechanism and weight sharing between layers for efficiency are also depicted.  The figure highlights the model's ability to handle diverse data types (image and tabular) and its explainability due to the use of attention weights.", "section": "3 HEALNet"}, {"figure_path": "HUxtJcQpDS/figures/figures_8_1.jpg", "caption": "Figure 3: Illustration of model\u2019s inspection capabilities using HEALNet on a high-risk patient of the UCEC study. We use the mean modality-specific attention weights across layers to highlight high-risk regions and inspect high-attention omic features. Individual patches can be used for further clinical or computational post-hoc analysis such as nucleus segmentation. We observe that the high-risk regions exhibit a very high concentration and different arrangement of epithelial cells (red) which is commonly associated with the origin of various cancer types [Coradini et al., 2011].", "description": "This figure demonstrates HEALNet's explainability by visualizing attention weights.  It shows a whole slide image (WSI) from a high-risk UCEC patient, an attention map highlighting regions the model focused on, a zoomed-in view of those regions with cell types identified, and a bar chart showing the attention weights given to different omics features.  The high-risk regions are shown to have high concentrations of epithelial cells, a known indicator of various cancer types.", "section": "Discussion"}, {"figure_path": "HUxtJcQpDS/figures/figures_17_1.jpg", "caption": "Figure 1: Overview of HEALNet (Hybrid Early-fusion Attention Learning Network) using a shared and modality-specific parameter space to learn from structurally different data sources in the same model (Fig. 1A). The shared space is a learned latent embedding S that is iteratively updated through d attention-based fusion layers and captures the shared information between modalities. The hybrid early-fusion layer (Fig. 1B, and Eq. 3) learns the cross-attention weights Wm = {W(\u00b9), W(\u00b2), W(\u00b3)} for each modality m corresponding to the queries (Qm = W(\u00b9)S), keys (Km = W(\u00b2)Xm), and values (Vm = W(\u00b3)Xm) which are shared between layers. These layers capture the structural information of each modality and encode it in the shared embedding after a pass through a self-normalising network (SNN) layer.", "description": "This figure shows the architecture of HEALNet, a hybrid early-fusion attention learning network for multimodal fusion.  It highlights the use of both shared and modality-specific parameter spaces, iterative attention layers to capture cross-modal information, and a self-normalizing network to improve robustness and efficiency.  The figure displays the iterative update of a shared latent embedding through modality-specific attention mechanisms, leading to a final multimodal representation.", "section": "3 HEALNet"}]