[{"type": "text", "text": "Addressing Spatial-Temporal Heterogeneity: General Mixed Time Series Analysis via Latent Continuity Recovery and Alignment ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiawei Chen, Chunhui Zhao State Key Laboratory of Industrial Control Technology, College of Control Science and Engineering, Zhejiang University, China Jiaweichen@zju.edu.cn, chhzhao@zju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mixed time series (MiTS) comprising both continuous variables (CVs) and discrete variables (DVs) are frequently encountered yet under-explored in time series analysis. Essentially, CVs and DVs exhibit different temporal patterns and distribution types. Overlooking these heterogeneities would lead to insufficient and imbalanced representation learning, bringing biased results. This paper addresses the problem with two insights: 1) DVs may originate from intrinsic latent continuous variables (LCVs), which lose fine-grained information due to extrinsic discretization; 2) LCVs and CVs share similar temporal patterns and interact spatially. Considering these similarities and interactions, we propose a general MiTS analysis framework MiTSformer, which recovers LCVs behind DVs for sufficient and balanced spatial-temporal modeling by designing two essential inductive biases: 1) hierarchically aggregating multi-scale temporal context information to enrich the information granularity of DVs; 2) adaptively learning the aggregation processes via the adversarial guidance from CVs. Subsequently, MiTSformer captures complete spatial-temporal dependencies within and across LCVs and CVs via cascaded self- and cross-attention blocks. Empirically, MiTSformer achieves consistent SOTA on five mixed time series analysis tasks, including classification, extrinsic regression, anomaly detection, imputation, and long-term forecasting. The code is available at https://github.com/chunhuiz/MiTSformer. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multivariate time series analysis is energized in various real-world applications, such as weather forecasting [7], activity recognition [20], and industrial maintenance [47]. Empowered by deep learning, plentiful time series models have been proposed based on foundation models such as RNNs [23, 25, 22], CNNs [6, 36, 40], Transformers [30, 41, 10] and modernized MLPs [50, 45, 39]. These sophisticated models have achieved increasingly remarkable performance in various time series analysis tasks, e.g., classification [17], forecasting [7, 42], imputation [40] and anomaly detection [9]. ", "page_idx": 0}, {"type": "text", "text": "The primary challenge in time series analysis is effectively modeling spatial-temporal patterns, including intra-variable temporal variations and inter-variable spatial correlations [46, 48, 16, 11, 43]. So far, most current approaches naturally assume that time series data are composed solely of continuous variables, and then uniformly model spatial-temporal patterns in continuous spaces. Yet, in broad practical scenarios, the acquired data are often mixed time series (MiTS) that encompass both continuous variables (CVs) and discrete variables (DVs). Take meteorological data as an example (Fig. 1 (Left)), some sensor-derived variables are commonly recorded as CVs (e.g., temperature, humidity, and wind speed), while certain variables like cloudage and rainfall patterns are typically tracked as DVs due to measurement restrictions or distinctive nature. Up to now, mixed time series analysis is still a formidable yet under-explored problem. Essentially, MiTS presents spatial-temporal heterogeneity problems as depicted in Fig. 1 (Right). On one hand, CVs commonly encapsulate rich temporal variation information, exhibited in autocorrelations, periodical patterns, local fluctuations, etc, while DVs often undergo sudden changes or steady states due to limited value ranges, resulting in the temporal variation discrepancy between CVs and DVs that complicates temporal modeling. On the other hand, CVs generally adhere to Gaussian distributions, while DVs follow Bernoulli distributions, resulting in the distribution type discrepancy that hinders spatial correlation analysis between CVs and DVs. Neglecting these heterogeneities and equally treating mixed variables would yield insufficient and imbalanced spatial-temporal modeling problems, i.e., the model may struggle to characterize distinct temporal patterns of DVs and CVs and fail to reliably capture spatial correlations within and across DVs and CVs, posing a bottleneck for MiTS analysis. ", "page_idx": 0}, {"type": "image", "img_path": "EMV8nIDZJn/tmp/390986552e5552293af037b6b12520928f8ee42b72049e4c137c366380fa9a0c.jpg", "img_caption": ["Figure 1: Left: Illustration of mixed time series. Right: Spatial-temporal heterogeneity problem. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The key to addressing the spatial-temporal heterogeneity lies in bridging the information gap between DVs and CVs. Essentially, in real-world MiTS, the observed DVs may originate from intrinsic continuous-valued factors, which are unobservable owing to extrinsic factors such as measurement limitations, storage requirements, and transmission interference. Continuing with the aforementioned meteorological example, cloud cover percentage is an intrinsically continuous variable, whose finegrained values are hard to measure directly. In practice, discretizing it with coarse-grained discrete variable-cloudage (reflect \u201ccloudy\u201d or not) is sufficient for most applications and is memory-efficient. In this paper, we introduce latent continuous variables (LCVs) to describe the intrinsic continuous factors behind DVs. Given its numerically continuous nature, the LCV of cloudage, i.e., cloud cover percentage, may be spatially correlated with other observed CVs (e.g., humidity and wind speed) and exhibit similar temporal variation patterns (e.g., autocorrelation and seasonal fluctuation) with them, as both of them originate from the same meteorological system. Thereby, we can progressively recover the LCVs behind DVs by leveraging these temporal similarities and spatial interactions among CVs and LCVs. In this way, spatial-temporal dependencies of mixed variables can be completely and reliably modeled in a unified continuous numerical space, and the spatial-temporal heterogeneity problem is mitigated. Also, by bridging the mutual spatial-temporal interactions, LCVs and CVs can supply complementary information for various downstream analysis tasks. ", "page_idx": 1}, {"type": "text", "text": "Enlightened by the above insights, we reconcile the intrinsic tension between the two highly dependent problems - Latent Continuity Recovery and Spatial-Temporal Modeling - in one coherent and synergistic framework, MiTSformer, for general mixed time series analysis. By leveraging the temporal similarities and spatial interactions between LCVs and CVs, MiTSformer can gradually decipher the LCVs behind DVs and capture the complete spatial-temporal dependencies within and across LCVs and CVs. Specifically, we design a recovery network to portray LCVs behind DVs by adaptatively and hierarchically aggregating temporal contextual information. Followingly, an adversarial variable modality discrimination objective and smoothing constraints are devised to guide the learning of the recovery network, ensuring the recovered LCVs share similar temporal properties and distributions with CVs. Additionally, MiTSformer employs self-attention to learn spatial-temporal dependencies within LCVs or CVs and cross-attention to exploit those across LCVs and CVs, facilitating various downstream analysis tasks. Our contributions lie in three aspects: ", "page_idx": 1}, {"type": "text", "text": "(1) Novel Problem: To the best of our knowledge, our paper pioneers the exploration of the general mixed time series analysis, which is practical and challenging. We reveal the crucial spatial-temporal heterogeneity problem, which is caused by the discrepancies in temporal variation properties and distribution types between DVs and CVs. ", "page_idx": 1}, {"type": "text", "text": "(2) Customized Framework: To solve the spatial-temporal heterogeneity problem, we propose a task-general framework MiTSformer customized for MiTS, which adaptively recovers LCVs behind DVs by leveraging the adversarial guidance of CVs and task supervisions. Moreover, MiTSformer can capture spatial-temporal dependencies within and across CVs and LCVs via self- and cross-attention blocks, thus learning sufficient and balanced spatialtemporal representations and being amenable to various mixed time series analysis tasks. ", "page_idx": 2}, {"type": "text", "text": "(3) Versatile Effectiveness: Empirically, our proposed MiTSformer establishes the state-ofthe-art performance on five mainstream mixed time series analysis tasks with 34 datasets covering wide-ranging real-world application domains. We believe our work makes a predominant attempt at general mixed time series analysis in practical applications. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "General Time Series Analysis General time series analysis aim at learning universal temporal representations by developing task-general backbones and task-specific heads for diverse tasks. To date, these methods have been designed predominantly for time series comprising continuous variables solely. As a pioneering work, TimesNet [40] leverages fast Fourier transform and parameterefficient inception block to capture intra-period and inter-period variations for time series modeling. Followingly, ModernTCN [16] modernizes and modifies the traditional TCN by introducing larger effective receptive fields and cross-variable dependency modeling, bringing great performance and efficiency. Meanwhile, GPT2TS [52] leverages pre-trained language models, e.g., GPT2 [32], for various time series analysis tasks with task-specific fine-tuning. ", "page_idx": 2}, {"type": "text", "text": "Yet, most of the current works naturally assume uniformity in variable types of time series and are inapplicable for MiTS, as they lack the differentiation of distinct variable modalities. In this paper, we develop a systematic framework with delicate differentiation and deft alignment of mixed variables to support sufficient and balanced spatial-temporal modeling for general mixed time series analysis. ", "page_idx": 2}, {"type": "text", "text": "Mixed Data Analysis Previous studies have revealed the significance of mixed data analysis and made several attempts to shed light on this challenging problem. A naive solution for mixed data modeling is roughly pre-processing DVs and CVs into the same variable modality, e.g., discarding DVs or discretizing CVs by certain policies [35, 13]. However, these methods may lose vital finescaled information and bring errors inevitably. Towards concurrently modeling of DVs and CVs, Mixed Data RBF-ELM method [24] adopts a distance-based learning scheme for efficient and direct mixed data classification, while Mixed-variate Restricted Boltzmann Machine (Mv.RBM) [15, 14] construct ensembles of mixed-data Deep Belief Nets with varying depths for anomaly detection of mixed data. Another line of work treats DVs as semantic attributes of CVs and establishes the relationships between DVs and CVs by developing specific inference rules [18, 44]. More recently, researchers have utilized mixed naive Bayes models [37, 38] or variational inference [12] for mixed data modeling in industrial processes with different distribution priors of CVs and DVs. ", "page_idx": 2}, {"type": "text", "text": "However, current studies mostly focus on specific analysis tasks (e.g., designed only for classification) and may be restricted by linear, non-temporal data, or other rigorous assumptions, which are not capable of handling real-world MiTS that exhibit intricate spatial-temporal patterns. Instead, our proposed MiTSformer can model complete spatial-temporal dependencies within and across DVs and CVs and can produce task-general representations for various MiTS analysis tasks. ", "page_idx": 2}, {"type": "text", "text": "3 MiTSformer ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Formulation and Motivation Analysis ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Mixed Time Series. Given a collection of multivariate time series $X~~=~~\\{x_{1},x_{2},...,x_{p}\\}$ comprising $p$ variables with length $T$ . Among them, there are $p\\,-\\,n$ continuous variables $X^{C}\\;=\\;\\{x_{1},x_{2},...,x_{p-n}\\}\\;\\in\\;\\mathbb{R}^{(p-n)\\times T}$ with continuous numerical values, and $n$ discrete variables $X^{D}=\\{x_{p-n+1},x_{p-n+2},...,x_{n}\\}\\in\\mathbb{A}^{n\\times T}$ with discrete states. Without loss of generality, we consider the binary-valued discrete variables, whose value can be 0 or 1, i.e., $\\mathbb{A}=\\{0,1\\}$ . Mixed time series are used as model input to support various analysis tasks, such as regression and classification. ", "page_idx": 2}, {"type": "text", "text": "Hampered by the spatial-temporal heterogeneity problem, directly modeling spatial-temporal dependencies of CVs and DVs without considering their discrepancies may inevitably yield non-negligible biases. Such research bottlenecks prompt us to exploit the underlying generation and interaction mechanism of DVs and CVs. As aforementioned, the observed DVs are potentially derived from LCVs, which undergo discretization processes due to external interferences as depicted in Fig. 2. For each DV $x^{D}$ , we adopt a corresponding LCV $x^{L C}$ to portray its latent continuity. To reliably and completely model inherent spatial-temporal patterns within MiTS, deciphering and recovering LCVs behind DVs is indispensable. Yet, it is challenging since a single discrete state can not be directly transformed into a continuous value without proper supervision. In this study, we address this challenge by revealing and leveraging the temporal similarity and spatial interaction between LCVs and CVs: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Temporal Similarity The unobserved LCVs share similar temporal variation patterns (e.g., autocorrelation, periodicity, trend, etc.) with the observed CVs. ", "page_idx": 3}, {"type": "text", "text": "Spatial Interaction LCVs and CVs exhibit information interactions and inter-variable spatial correlations. The synergistic effect of LCVs and CVs provides complementary information for various downstream tasks. ", "page_idx": 3}, {"type": "image", "img_path": "EMV8nIDZJn/tmp/fe3edf3d64b575a778e6bff4799ddd24455fc184389cabd31340dee83629017b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Built upon these two insights, we design Figure 2: Connections among DVs, CVs, and LCVs.   \nMiTSformer, which will be elaborately introduced in the following parts. ", "page_idx": 3}, {"type": "image", "img_path": "EMV8nIDZJn/tmp/435351a93e1054d1c167a2d5108ae475a70cb38e3627a46be62b4e36da6324e4.jpg", "img_caption": ["Figure 3: Overall pipeline of MiTSformer. First, MiTS undergo latent continuity recovery (DVs only) and are embedded as variate tokens, which are then refined through spatial-temporal attention blocks. The acquired variate tokens are utilized both for reconstructing the original MiTS and serving various downstream tasks. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.2 Framework Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As aforementioned, there are two key steps for general MiTS analysis: 1) unifying the temporal characteristics and distribution types of DVs and CVs; 2) modeling sufficient and balanced spatialtemporal dependencies for effective representation learning. MiTSformer facilitates these two steps in a highly versatile manner with a coherent framework as depicted in Fig. 3. The overall pipeline contains two key parts: 1) Latent Continuity Recovery that adaptively aggregate contextual information of DVs to recover LCVs with the adversarial alignment guidance of CVs and temporal smoothness constraints; 2) Spatial-Temporal Attention Blocks that model intra- and inter-variable modality spatial-temporal dependencies with cascaded self-attention and cross-attention sub-blocks. ", "page_idx": 3}, {"type": "text", "text": "3.3 Latent Continuity Recovery ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Since single time points of DVs contain limited information, it is difficult to directly transform discrete states to latent continuous values. Fortunately, time series commonly present auto-correlation natures, i.e., the value of a certain time step is correlated with its adjacent ones. Specifically, for a discrete state of \u201c1\u201d surrounded by states \u201c1\u201d, its latent continuous value would be relatively large, e.g., 0.9. In contrast, for a discrete state of \u201c1\u201d surrounded by states $\\,^{\\bullet}0^{\\bullet}$ , its latent continuous value would be relatively small, e.g., 0.6. Thus, an intuitive solution for enriching the information density of a single time point of DVs is to properly leverage its adjacent context information. ", "page_idx": 3}, {"type": "text", "text": "Contextual Aggregation Recovery Network We convert the above insights into the model inductive bias and realize the latent continuity recovery by adaptively and hierarchically aggregating multi-scale adjacent context information of DVs. Specifically, convolutional neural networks (CNNs) own an inductive bias that can aggregate receptive local information by convolutional kernels. Technically, we devise the recovery networks that receive DVs as input to generate LCVs as $x^{L C}\\;=\\;\\mathrm{Rec}\u2013\\mathrm{Net}\\left(x^{D}\\right)$ . As depicted in Fig. 4, the recovery network is composed of several residual dilated convolutional blocks, which employ dilated convolutional kernels along the temporal dimension to aggregate adjacent context information, and utilize residual connections to adaptively accumulate multi-scale temporal information to characterize intricate temporal patterns of LCVs. The residual dilated convolutional network is implemented by the iterative process as $h_{i}=\\operatorname{Conv}^{d_{i}}(h_{i-1})+h_{i-1}$ , where $h_{i}$ represents the output of the $i$ -th residual block, $i=1,2,\\dots,n$ , $\\mathrm{Conv}^{d_{i}}$ denotes the convolution operation along the temporal axis to aggregate contextual information with dilation rate $d_{i}$ . The final output, $x^{L C V}=h_{n}$ , is the result after $n$ residual blocks. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "EMV8nIDZJn/tmp/0b02080e7c8eed5181be99161aa6c686cd58745db1a7575a32cfe6eec93c3b7e.jpg", "img_caption": ["Figure 4: LCV recovery with adversarial variable modality discrimination. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Temporal Adjacent Smoothness Constraint: To facilitate spatial-temporal modeling across LCVs and CVs, the recovered LCVs should be equipped with interpretable autocorrelation or trend properties, instead of \u201csudden changes\u201d as DVs. To this end, we encourage the smoothness of LCVs across time with a regularization term as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{smooth}}=\\left\\|\\operatorname{Abs}\\left(S x^{D}\\right)\\otimes\\left(S x^{L C}\\right)\\right\\|_{2}^{2},S=\\left[\\begin{array}{l l l l}{-1}&{1}&&\\\\ &{\\ddots}&{\\ddots}&\\\\ &&{-1}&{1}\\end{array}\\right]\\in\\mathbb{R}^{(T-1)\\times T}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\boldsymbol{S}$ is the smoothness matrix and $\\otimes$ denotes the Hadamard product operation. Abs $\\left(S x^{D}\\right)=$ $\\mathrm{Abs}\\left(x_{[2:T]}^{D}-x_{[1:T-1]}^{D}\\right)$ denotes the absolute value of the first-order difference of DVs and can reflect the \u201csudden change\u201d points (with state \u201c1\u201d). Overall, minimizing $\\mathscr{L}_{\\mathrm{smooth}}$ is equivalent to minimizing $\\begin{array}{r}{\\sum_{t=1}^{T-1}\\mathrm{Abs}\\left(x_{\\mathrm{t+1}}^{\\mathrm{D}}-\\mathrm{x}_{\\mathrm{t}}^{\\mathrm{D}}\\right)\\left(x_{\\mathrm{t+1}}^{L C}-x_{\\mathrm{t}}^{L C}\\right)^{2}}\\end{array}$ by introducing the multiplication of the constant matrix $\\boldsymbol{S}$ Alternatively, we can adopt the K-Lipschitz continuity as smoothness constraints, where the CVs can act as guidance to determine the smoothness degree of the recovered LCVs. Empirically, we find out such design would bring similar performance. ", "page_idx": 4}, {"type": "text", "text": "After latent continuity recovery, raw series of $x^{L C}$ and $x^{C}$ are independently embedded as tokens $z^{L C}$ and $z^{C}$ through variate-wise linear projection to describe the properties of each variable. ", "page_idx": 4}, {"type": "text", "text": "Adversarial Variable-Modality Discrimination: Inspired by the temporal similarity property, the recovered LCVs should exhibit similar temporal patterns and distributions to CVs. Accordingly, we devise a variable modality discrimination objective, which is optimized in an adversarial manner as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{argmin}\\left(\\operatorname*{max}_{\\theta_{\\mathrm{Rec}},\\theta_{\\mathrm{Emb}}}\\left(\\mathcal{L}_{\\mathrm{Dis}}=\\mathbb{E}\\left[\\log\\left(\\mathrm{Dis}\\left(z^{C}\\right)\\right)\\right]+\\mathbb{E}\\left[\\log\\left(1-\\mathrm{Dis}\\left(z^{L C}\\right)\\right)\\right]\\right)\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where Dis denotes the variable modality discriminator. Dis is trained to distinguish LCVs and CVs as accurately as possible, while the recovery networks are facilitated to generate LCVs with characteristics similar to CV as much as possible to confuse the discriminator. We adopt gradient reversal layer [19](GRL) to achieve the adversarial learning objective. In this way, we bridge the interactions between CVs and DVs and enable CVs to supply supervision signals for LCV recovery. ", "page_idx": 4}, {"type": "text", "text": "3.4 Intra- and Inter-Variable-Modality Spatial-Temporal Dependency Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Spatial-temporal dependencies are of vital importance for time series representation learning. Specifically for MiTS, the complete spatial-temporal correlations include the ones within LCVs or CVs ", "page_idx": 4}, {"type": "image", "img_path": "EMV8nIDZJn/tmp/9aba6ef88d6b9c9532121b4f64fae4bd94bf548dadd012060a9afc415e114c92.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 5: Spatial-temporal attention blocks. Left: MiTS variable adjacency matrix, including the variable relationships i) within CVs or LCVs and ii) across CVs and LCVs; Middle: Intra-variablemodality self-attention for modeling spatial-temporal dependencies within CVs or LCVs, and Right: Inter-variable-modality cross-attention for modeling those across CVs and LCVs. ", "page_idx": 5}, {"type": "text", "text": "and the ones across LCVs and CVs, which are explicitly characterized by spatial-temporal attention blocks in MiTSformer (as shown in Fig. 5). ", "page_idx": 5}, {"type": "text", "text": "Intra-Variable-Modality Self-Attention Within each variable modality, we adopt self-attention to model their spatial-temporal dependencies as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{z}_{l}^{C}=\\mathrm{LN}\\left(z_{l}^{C}+\\mathrm{Self-Attn}\\left(\\left[Q_{l}^{C},K_{l}^{C},V_{l}^{C}\\right]\\right)\\right),\\,\\hat{z}_{l}^{C}=\\mathrm{LN}\\left(\\hat{z}_{l}^{C}+\\mathrm{FFN}\\left(\\hat{z}_{l}^{C}\\right)\\right)}\\\\ &{\\hat{z}_{l}^{L C}=\\mathrm{LN}\\left(z_{l}^{L C}+\\mathrm{Self-Attn}\\left(\\left[Q_{l}^{L C},K_{l}^{L C},V_{l}^{L C}\\right]\\right)\\right),\\,\\hat{z}_{l}^{L C}=\\mathrm{LN}\\left(\\hat{z}_{l}^{L C}+\\mathrm{FFN}\\left(\\hat{z}_{l}^{L C}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where Self-Attn denotes the Multi-head Self-Attention that captures intra-variate spatial correlations by computing Softmax scores with query and key embeddings and weighted aggregating the value embeddings. FFN denotes the feed-forward network that processes each variable token to learn intra-variate global temporal representations. LN denotes Layer Normalization, which is applied to individual variate tokens and has been proven effective in tackling non-stationary problems [28]. ", "page_idx": 5}, {"type": "text", "text": "Inter-Variable-Modality Cross-Attention We adopt symmetric cross-attention sub-blocks to model the spatial-temporal interactions across LCVs and CVs as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z_{l+1}^{C}=\\mathrm{LN}\\left(\\hat{z}_{l}^{C}+\\mathrm{Cross-Attn}\\left(\\left[Q_{l}^{C},K_{l}^{L C},V_{l}^{L C}\\right]\\right)\\right),\\ z_{l+1}^{C}=\\mathrm{LN}\\left(z_{l+1}^{C}+\\mathrm{FFN}\\left(z_{l+1}^{C}\\right)\\right)}\\\\ &{z_{l+1}^{L C}=\\mathrm{LN}\\left(z_{l+1}^{L C}+\\mathrm{Cross-Attn}\\left(\\left[Q_{l}^{L C},K_{l}^{C},V_{l}^{C}\\right]\\right)\\right),\\ z_{l+1}^{L C}=\\mathrm{LN}\\left(z_{l+1}^{L C}+\\mathrm{FFN}\\left(z_{l+1}^{L C}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Subsequently, the acquired token embeddings $z_{L}^{L C}$ and $z_{L}^{C}$ are utilized both for 1) original MiTS reconstruction with $\\mathcal{L}_{\\mathrm{Rec}}$ as Eq. 5, and 2) downstream tasks with task supervision loss $\\mathcal{L}_{\\mathrm{Task}}$ , e.g., Cross Entropy loss for the classification task. See Appendix A for pipelines of each task. ", "page_idx": 5}, {"type": "text", "text": "Self-Reconstruction We devise variate-wise decoders based on MLPs to reconstruct the original DVs and CVs, which can not only provide self-supervision signals for spatial-temporal dependency learning but also guarantee the recovered LCVs retain the information of the observed DVs. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{Rec}}=\\sum_{i=1}^{p-n}\\mathrm{MSE}(\\mathrm{Rec-Decoder}_{C}\\left(z_{L,i}^{C}\\right),x_{i}^{C})+\\sum_{i=1}^{n}\\mathrm{CE}(\\mathrm{Rec-Decoder}_{L C}\\left(z_{L,i}^{L C}\\right),x_{i}^{D})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.5 Synergy of Latent Continuity Recovery and Downstream Tasks ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Considering both latent continuity recovery and downstream analysis task, the overall optimization objective of MiTSformer is expressed as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{All}}=\\underbrace{\\lambda_{1}\\mathcal{L}_{\\mathrm{Smooth}}+\\lambda_{2}\\mathcal{L}_{\\mathrm{Rec}}+\\lambda_{3}\\mathcal{L}_{\\mathrm{Dis}}}_{\\mathrm{Self-Supervision}}+\\underbrace{\\mathcal{L}_{\\mathrm{Task}}}_{\\mathrm{Task-Supervision}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "While seemingly separated from each other, the loss components of MiTSformer work in a collaborative fashion on two aspects: 1). The first three self-supervision losses facilitate latent continuity recovery synergetically to tackle the spatial-temporal heterogeneity problem. Specifically, the smoothness loss $\\mathcal{L}_{\\mathrm{Smooth}}$ favorably manifests the autocorrelation of LCVs and alleviates the sudden ", "page_idx": 5}, {"type": "text", "text": "Table 1: Summary of experiment benchmarks. For each dataset, we randomly select $n=\\lfloor0.5p\\rfloor$ variables as DVs, whose values are first MinMax normalized and then discretized into the value of 0 or 1 with the threshold 0.5 as $\\mathtt{i n t}(\\mathtt{M i n M a x}(x)>0.5)$ . See Table 5 for more details. ", "page_idx": 6}, {"type": "table", "img_path": "EMV8nIDZJn/tmp/3b0aa47f8caf424c7a435518ff3932ee394d6aa18135d0f8077a2340b087d157.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "changes. Also, the discrimination loss $\\mathcal{L}_{\\mathrm{Dis}}$ guarantees LCVs to be equipped with similar temporal dynamics with CVs adversarially, which guarantees a crucial condition for spatial-temporal modeling. Meanwhile, the reconstruction loss provides auxiliary self-supervision signals and constrains the LCV recovery processes to be reversible. Our experiments, particularly the ablation study displayed in Section 4.2, further justify the mutual dependency of the synergy between the three components; 2) The latent continuity recovery losses and task loss also work collaboratively, as reliable latent continuity recovery can bring excellent downstream task performance, while task supervision loss may, in turn, also reciprocate the recovery processes. Take MiTS classification as an example, the ideally recovered LCVs can prompt learning discriminative representations for classification, while the class label loss provides additional supervision for learning appropriate recovery functions. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To verify the effectiveness and versatility of MiTSformer, we extensively experiment on five mainstream mixed time series analysis tasks, including mixed time series classification, extrinsic regression, long-term forecasting, imputation, and anomaly detection. ", "page_idx": 6}, {"type": "text", "text": "Implementations Table 1 summarizes the experiment benchmarks. For each dataset, we randomly select half the variables and discretize them as DVs to generate MiTS data. More information about datasets and experimental platforms, hyperparameters and experimental configurations, and algorithm implementations can be found in Appendix A.1, A.2,A.3, respectively. The pipelines of different mixed time series analysis tasks can be found in Appendix $_\\mathrm{A.5\\simA.8}$ . ", "page_idx": 6}, {"type": "text", "text": "Baselines We extensively compare MiTSformer with the latest and advanced models in the time series community, including CNN-based models: ModernTCN (2024), TimesNet (2023) and MICN (2023); Transformer-based models: iTransformer (2024), PatchTST (2023), Crossformer (2023), FEDformer (2022) and Pyraformer (2022); MLP-based models: LightTS (2023), DLinear (2023) and FiLM (2022). To guarantee fairness, we keep the original backbone for each method as the feature extractor, and we adopt universal task-specific heads and loss functions consistently for all methods. ", "page_idx": 6}, {"type": "text", "text": "4.1 Main Results on Different Tasks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "(1) Mixed Time Series Classification We select 10 multivariate datasets from the UEA Time Series Classification Archive [5] and pre-press them following [40]. As shown in Fig. 6, MiTSformer achieves the best performance with an average accuracy of $71.9\\%$ , surpassing all powerful baselines. Besides, it can be observed that frequency-based methods FiLM and FEDformer show inferior performance, as the introduction of DVs may make it difficult to estimate the frequency reliably and yield non-negligible errors. In comparison, MiTSformer adaptively recovers and aligns the LCVs behind DVs with the guidance of both CVs and class-label supervision, Figure 6: Classification Results (Acc \u2191) thereby facilitating high-level representation learning and benefiting the classification tasks. ", "page_idx": 6}, {"type": "image", "img_path": "EMV8nIDZJn/tmp/e5db4a9c3a85151c0cb4fab09404ea4687c38819a3504d49dd1f5930499639ea.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "EMV8nIDZJn/tmp/d1fb815db388b4a5485d149b5b588bc4e83617c106d57b2d57fde62f5927e872.jpg", "table_caption": ["Table 2: Imputation Task. The best results are bolded and the second-best results are underlined. The same goes for Table 3. See Table 14 for full results. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "(2) Mixed Time Series Extrinsic Regression We select 10 multivariate datasets from TSER repository [34] and pre-process them as [34]. As summarized in Fig. 7 (b), MiTSformer outperforms all the rivals with an average MAE of 0.534, verifying its capacity to model complex extrinsic regression patterns. Besides, some transformer-based models, e.g., PatchTST and Pyraformer, present competitive performance by learning global dependencies with self-attention. Notably, Dlinear shows inferior results. This is probably because Dlinear adopts single-layer MLP to model temporal dependencies, which might be suitable for some autoregressive endogenous patterns, but are not applicable for nonlinear exogenous regression relationships and degenerate performance. ", "page_idx": 7}, {"type": "image", "img_path": "EMV8nIDZJn/tmp/154add86d1f6ffc04b7ee5e4eb6577f8393b95280221a0943e1a721972b019c1.jpg", "img_caption": ["Figure 7: Regression Results (MAE $\\downarrow$ ) "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "(3) Mixed Time Series Anomaly Detection We conduct experiments on five widely-used anomaly detection benchmarks: SMD [33], MSL [21], SMAP [21], SWaT [29] and PSM [4]. ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "For fair comparisons, we adopt the reconstruction loss for both CVs and DVs to train base models and use the reconstruction error as the shared anomaly criterion for all experiments. Specifically for MiTSformer, no additional taskorientation loss is added since there are already reconstruction losses. Fig. 8 presents the performance comparison evaluated by the F1-score (\u2191), demonstrating that MiTSformer consistently achieves state-of-the-art performance on five benchmarks. Besides, iTrasformer also achieves great performance by adaptively modeling multivariate correlations with self-attentions. In comparison, MiTSformer not only recovers the LCVs behind DVs but also comprehensively and ", "page_idx": 7}, {"type": "image", "img_path": "EMV8nIDZJn/tmp/7824f5f529084cc4b6e70ae00570fa3900fde78c3d3792c8c4710fde4d0d2361.jpg", "img_caption": ["Figure 8: Anomaly detection results (F1-score). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "explicitly characterizes the spatial-temporal dependencies within and across DVs and CVs, thereby equipping meticulous anomaly detection ability. ", "page_idx": 7}, {"type": "text", "text": "(4) Mixed Time Series Imputation Following [40], we select datasets of ETT [49], Weather (Wetterstation) and Electricity (UCI) benchmarks. In practice, imputating the values of CVs is more important and meaningful than those of DVs. Therefore, we focus on imputing the CVs in each experiment, while DVs are used as inputs to provide auxiliary information. To compare the model capacity under different proportions of missing data, we randomly mask the time points in the ratio of $\\{12.\\dot{5}\\%,25\\%,37.5\\%,\\dot{5}0\\%\\}$ in length-96 mixed time series and report the averaged imputation accuracy of CVs from 4 different mask ratios. ", "page_idx": 7}, {"type": "table", "img_path": "EMV8nIDZJn/tmp/9fd4d70bf484dfd3ffa5e5c421c639a719a1ec9fd127f278eabab6281eab3ed1.jpg", "table_caption": ["Table 3: Long Term Forecasting of CVs. \u201c-\u201d denotes out of memory. See Table 16 for full results. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Due to the missing values, the imputation task requires the model to deeply exploit the underlying temporal dependencies and spatial correlations from partial observations. According to Table 2, MiTSformer achieves the best performance on most tasks. Through the recovery of LCVs and the intra- and inter-variable modality attention mechanisms, MiTSformer can effectively bridge spatial-temporal information interactions between CVs and DVs. In this way, not only can CVs exploit useful information for self-imputation by mining temporal and spatial correlation themselves, but also DVs can provide reliable auxiliary information for more accurate imputation of CVs. ", "page_idx": 8}, {"type": "text", "text": "(5) Mixed Time Series Long-term Forecasting We follow the settings of prediction lengths and benchmarks as [40], including ETT [49], Electricity (UCI), Weather (Wetterstation), Exchange [23] and ILI (CDC), corresponding to different applications. We focus on forecasting both DVs and CVs. ", "page_idx": 8}, {"type": "text", "text": "Since CVs contain more fine-grained information granularity and can more adequately evaluate the model forecasting performance, we mainly focus on the prediction accuracy of CVs, which are summarized in Table. 3. As reported, MiTSformer presents the best performance on most tasks $76\\,\\%$ according to Table 16), surpassing extensive advanced MLP-based, Transformer-based and CNN-based models. In addition, recent baselines- modernTCN and iTransformer present great performance due to their delicate design of global temporal receptive fields. ", "page_idx": 8}, {"type": "image", "img_path": "EMV8nIDZJn/tmp/0508a75327a3832b79dad3110591ba3fc5ada9bc9643ff4de9b123d99eec4ae2.jpg", "img_caption": ["(g) ETTh2 case#2 ", "(h) ETTh2 case#3 ", "(i) ETTh2 case#4 "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 9: Visualization of LCV recovery. For each subfigure, the Left plots the observed DVs, and the Right plots the actual LCVs (red line) and recovered LCVs (black line). The grey rectangular patches denotes the area where the observed DV is \u201c1\u201d. ", "page_idx": 8}, {"type": "table", "img_path": "EMV8nIDZJn/tmp/7ae9c19a6b7a7d5b3de82771a9fb9d4f497ffdf922749b0b798cca7984f9cdb1.jpg", "table_caption": ["Table 4: Ablation analysis. For anomaly detection tasks, we do not ablate $\\mathcal{L}_{\\mathrm{Rec}}$ , as it is needed to support anomaly criterion calculation. The corresponding results are omitted with \u201c / \u201d. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.2 Ablation Studies ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To further verify the effectiveness of each key design, we conduct ablation experiments on mixed time series classification, long-term forecasting, and anomaly detection tasks. The results are summarized in Table 4, where $\\mathrm{^{6}w/o\\ L C V s^{\\ast}}$ denotes removing $\\mathcal{L}_{\\mathrm{Dis}}$ , $\\mathcal{L}_{\\mathrm{Smooth}}$ , and $\\mathcal{L}_{\\mathrm{Rec}}$ together, and $\\sin/0$ Cross-Att\u201d denotes removing the cross-attention sub-block and adopt self-attention sub-block solely. ", "page_idx": 9}, {"type": "text", "text": "Latent Continuity Recovery The three loss terms, $\\mathcal{L}_{\\mathrm{Dis}}$ , $\\mathcal{L}_{\\mathrm{Smooth}}$ , and $\\mathcal{L}_{\\mathrm{Rec}}$ support the latent continuity recovery from different perspectives and work collaboratively. As shown in Table 4, ablating each loss term would lead to performance degradation for different tasks. Remarkably, the employment of all recovery loss functions in conjunction yields the optimal result. ", "page_idx": 9}, {"type": "text", "text": "Attention Backbones The cross-attention block is devised to bridge the information prorogation among LCVs and CVs by modeling spatial-temporal dependencies across LCVs and CVs, as LCVs and CVs provide complementary information for analysis tasks. The results in Table 4 also emphasize the importance of cross-variable-modality dependency modeling for MiTS analysis. ", "page_idx": 9}, {"type": "text", "text": "4.3 Visualization and Model Investigations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Visualization of the Recovered LCVs To verify the interpretability of LCV recovery, we provide visualization plots in Fig. 9. We find that DVs commonly show patterns of \u201csudden changes\u201d or \u201csteady states\u201d, which are less informative for analyzing spatial-temporal correlations. Fortunately, MiTSformer not only recovers their latent fine-grained and informative temporal variation patterns but also further leverages them for spatial-temporal representation learning, thereby mitigating the spatial-temporal heterogeneity challenge and achieving superior performance. ", "page_idx": 9}, {"type": "text", "text": "Additional Analysis We analyze the model efficiency in the Appendix. B, showing MiTSformer maintains great performance and efficiency compared with most baselines. In addition, we investigate hyperparameter sensitivity in the Appendix C. The results demonstrate that (1) MiTSformer is relatively stable in the selection of odel capacity-related hyper-parameters $d_{m o d e l}$ and number of layers $L$ ; (2) MiTSformer is quite robust to the weights of loss items (i.e., $\\lambda_{1},\\,\\lambda_{1}$ , and, $\\lambda_{3}$ ), and moderate weights bring optimal performance. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper focuses on a challenging yet seldomly explored problem in the time series community and provides a systematic and universal solution for mixed time series analysis. To address the spatial-temporal heterogeneity problem, we first reveal the LCVs behind DVs and try to recover them for sufficient and balanced spatial-temporal modeling. Accordingly, MiTSformer is developed as a task-general mixed time series analysis framework by leveraging the temporal similarities and spatial interactions between LCVs and CVs. MiTSformer can perform adaptive LCV recovery for DVs via the adversarial guidance of CVs as well as smooth constraints, and model complete spatial-temporal dependencies via self- and cross-attention blocks. With extensive empirical evaluations, MiTSformer shows great practicality, superiority, and versatility in five mainstream mixed time series analysis tasks. In the future, it is of interest to empower MiTSformer with advanced pre-training techniques and powerful large language models for broader applications of mixed time series. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by the National Nature Science Foundation of China under Grant 62125306, and Zhejiang Key Research and Development Project under Grant 2024C01163. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Cdc. illness. https://gis.cdc.gov/grasp/fluview/fluportaldashboard. html. [2] Uci. electricity. https://archive.ics.uci.edu/ml/datasets/ electricityloaddiagrams20112014/.   \n[3] Wetterstation. weather. https://www.bgc-jena.mpg.de/wetter/. [4] Ahmed Abdulaal, Zhuanghua Liu, and Tomer Lancewicki. Practical approach to asynchronous multivariate time series anomaly detection and localization. In Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining, pages 2485\u20132494, 2021.   \n[5] Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, Paul Southam, and Eamonn Keogh. The uea multivariate time series classification archive, 2018. arXiv preprint arXiv:1811.00075, 2018.   \n[6] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018.   \n[7] Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Accurate medium-range global weather forecasting with 3d neural networks. Nature, 619(7970):533\u2013538, 2023. [8] David Campos, Miao Zhang, Bin Yang, Tung Kieu, Chenjuan Guo, and Christian S Jensen. Lightts: Lightweight time series classification with adaptive ensemble distillation. Proceedings of the ACM on Management of Data, 1(2):1\u201327, 2023.   \n[9] Alberto Castellini, Francesco Masillo, Davide Azzalini, Francesco Amigoni, and Alessandro Farinelli. Adversarial data augmentation for hmm-based anomaly detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n[10] Jiawei Chen, Pengyu Song, and Chunhui Zhao. Multi-scale self-supervised representation learning with temporal alignment for multi-rate time series modeling. Pattern Recognition, 145:109943, 2024.   \n[11] Jiawei Chen, Pengyu Song, Chunhui Zhao, and Jinliang Ding. Spatiotemporal multiscale correlation embedding with process variable reorder for industrial soft sensing. IEEE Transactions on Instrumentation and Measurement, 2023.   \n[12] Junhao Chen, Chunhui Zhao, and Jinliang Ding. A flexible probabilistic framework with concurrent analysis of continuous and categorical data for industrial fault detection and diagnosis. IEEE Transactions on Industrial Informatics, 2023.   \n[13] Kochenderfer M J. Chen Y C, Wheeler T A. Learning discrete bayesian networks from continuous data. In Journal of Artificial Intelligence Research, pages 59: 103\u2013132, 2017.   \n[14] Kien Do, Truyen Tran, and Svetha Venkatesh. Multilevel anomaly detection for mixed data. arXiv preprint arXiv:1610.06249, 2016.   \n[15] Kien Do, Truyen Tran, and Svetha Venkatesh. Energy-based anomaly detection for mixed data. Knowledge and Information Systems, 57(2):413\u2013435, 2018.   \n[16] Luo donghao and wang xue. ModernTCN: A modern pure convolution structure for general time series analysis. In The Twelfth International Conference on Learning Representations, 2024.   \n[17] Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee-Keong Kwoh, Xiaoli Li, and Cuntai Guan. Self-supervised contrastive representation learning for semi-supervised time-series classification. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n[18] Liangjun Feng and Chunhui Zhao. Fault description based attribute transfer for zero-sample industrial fault diagnosis. IEEE Transactions on Industrial Informatics, 17(3):1852\u20131862, 2021.   \n[19] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International conference on machine learning, pages 1180\u20131189. PMLR, 2015.   \n[20] Rong Hu, Ling Chen, Shenghuan Miao, and Xing Tang. Swl-adapt: An unsupervised domain adaptation model with sample weight learning for cross-user wearable human activity recognition. Proceedings of the AAAI Conference on Artificial Intelligence, 2023.   \n[21] Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian Colwell, and Tom Soderstrom. Detecting spacecraft anomalies using lstms and nonparametric dynamic thresholding. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pages 387\u2013395, 2018.   \n[22] Yuxin Jia, Youfang Lin, Xinyan Hao, Yan Lin, Shengnan Guo, and Huaiyu Wan. Witran: Water-wave information transmission and recurrent acceleration network for long-range time series forecasting. Advances in Neural Information Processing Systems, 36, 2024.   \n[23] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In The 41st international ACM SIGIR conference on research & development in information retrieval, pages 95\u2013104, 2018.   \n[24] Qiude Li, Qingyu Xiong, Shengfen Ji, Yang Yu, Chao Wu, and Hualing Yi. A method for mixed data classification base on rbf-elm network. Neurocomputing, 431:7\u201322, 2021.   \n[25] Shengsheng Lin, Weiwei Lin, Wentai Wu, Feiyu Zhao, Ruichao Mo, and Haotong Zhang. Segrnn: Segment recurrent neural network for long-term time series forecasting. arXiv preprint arXiv:2308.11200, 2023.   \n[26] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In International conference on learning representations, 2022.   \n[27] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer: Inverted transformers are effective for time series forecasting. In The Twelfth International Conference on Learning Representations, 2024.   \n[28] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Exploring the stationarity in time series forecasting. Advances in Neural Information Processing Systems, 35:9881\u20139893, 2022.   \n[29] Aditya P Mathur and Nils Ole Tippenhauer. Swat: A water treatment testbed for research and training on ics security. In 2016 international workshop on cyber-physical systems for smart water networks (CySWater), pages 31\u201336. IEEE, 2016.   \n[30] Zelin Ni, Hang Yu, Shizhan Liu, Jianguo Li, and Weiyao Lin. Basisformer: Attention-based time series forecasting with learnable and interpretable basis. Advances in Neural Information Processing Systems, 36, 2024.   \n[31] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In The Eleventh International Conference on Learning Representations, 2023.   \n[32] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 2019.   \n[33] Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. Robust anomaly detection for multivariate time series through stochastic recurrent neural network. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 2828\u20132837, 2019.   \n[34] Chang Wei Tan, Christoph Bergmeir, Fran\u00e7ois Petitjean, and Geoffrey I Webb. Time series extrinsic regression: Predicting numeric values from time series data. Data Mining and Knowledge Discovery, 35(3):1032\u20131060, 2021.   \n[35] Giulio Ventura and Elena Benvenuti. Advances in Discretization Methods: Discontinuities, Virtual Elements, Fictitious Domain Methods, volume 12. Springer, 2016.   \n[36] Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, and Yifei Xiao. Micn: Multi-scale local and global context modeling for long-term series forecasting. In The Eleventh International Conference on Learning Representations, 2023.   \n[37] Min Wang, Donghua Zhou, and Maoyin Chen. Anomaly monitoring of nonstationary processes with continuous and two-valued variables. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 53(1):49\u201358, 2022.   \n[38] Min Wang, Donghua Zhou, and Maoyin Chen. Hybrid variable monitoring: An unsupervised process monitoring framework with binary and continuous variables. Automatica, 147:110670, 2023.   \n[39] Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James Y Zhang, and JUN ZHOU. Timemixer: Decomposable multiscale mixing for time series forecasting. In The Twelfth International Conference on Learning Representations, 2024.   \n[40] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In The Eleventh International Conference on Learning Representations, 2023.   \n[41] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in neural information processing systems, 34:22419\u201322430, 2021.   \n[42] Haixu Wu, Hang Zhou, Mingsheng Long, and Jianmin Wang. Interpretable weather forecasting for worldwide stations with a unified deep model. Nature Machine Intelligence, 5(6):602\u2013611, 2023.   \n[43] Wanke Yu, Chunhui Zhao, and Biao Huang. Moninet with concurrent analytics of temporal and spatial information for fault detection in industrial processes. IEEE Transactions on Cybernetics, 52(8):8340\u20138351, 2022.   \n[44] Jiaqi Yue, Jiancheng Zhao, and Chunhui Zhao. Similarity makes difference: Sshtn for generalized zero-shot industrial fault diagnosis by leveraging auxiliary set. IEEE Transactions on Industrial Informatics, 2024.   \n[45] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 11121\u201311128, 2023.   \n[46] Yunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. In The Eleventh International Conference on Learning Representations, 2023.   \n[47] Chunhui Zhao. Perspectives on nonstationary process monitoring in the era of industrial artificial intelligence. Journal of Process Control, 116:255\u2013272, 2022.   \n[48] Chunhui Zhao, Junhao Chen, and Hua Jing. Condition-driven data analytics and monitoring for wide-range nonstationary and transient continuous processes. IEEE Transactions on Automation Science and Engineering, 18(4):1563\u20131574, 2021.   \n[49] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 11106\u201311115, 2021.   \n[50] Tian Zhou, Ziqing Ma, Qingsong Wen, Liang Sun, Tao Yao, Wotao Yin, Rong Jin, et al. Film: Frequency improved legendre memory model for long-term time series forecasting. Advances in Neural Information Processing Systems, 35:12677\u201312690, 2022.   \n[51] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In International Conference on Machine Learning, pages 27268\u201327286. PMLR, 2022.   \n[52] Tian Zhou, Peisong Niu, Liang Sun, Rong Jin, et al. One fits all: Power general time series analysis by pretrained lm. Advances in neural information processing systems, 36, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Datasets and Experimental Platforms ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Despite being a fundamental issue, modeling mixed time series remains underexplored in academia, with a lack of specialized benchmark datasets for mixed time series. To meet the tasks for mixed time series, we employed benchmark time-series datasets and implemented discretization to convert some CVs into DVs. Our conversion process simulates the generation process of DVs and discretizes variables while preserving their inherent coupling relationships and properties. We summarized the dataset descriptions in Table 5. All experiments are repeated three times, implemented in PyTorch 1 and conducted on Linux servers with Intel(R) Xeon(R) Gold 6246 CPUs and NVIDIA 3090 24GB GPUs. The versions of Python and Pytorch are 3.9.7, and 1.10.0 respectively. ", "page_idx": 14}, {"type": "text", "text": "A.2 Hyperparameters and Experimental Configurations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "All the baselines that we reproduced are implemented based on their official codes or Time Series Library (TSlib)2. Since some baseline models are designed for specific analysis tasks, we modify them to serve different analysis tasks by replacing task heads and loss functions. Specifically, we keep the original backbone architecture for each method as the feature extractor, and we adopt universal task-specific heads and loss functions consistently for all methods. The architectures of task heads and related loss functions will be introduced in the following section. ", "page_idx": 14}, {"type": "text", "text": "The hyper-parameter configurations of MiTSformer are summarized in Table 6 and the hyperparameter configurations of baseline models are summarized in Table 7. Some hyperparameters, including batch size, training epochs, dropout rate, and the number of attention heads (transformerbased models only) are fixed and kept the same for all baseline models and MiTSformer. Moreover, to compare the upper bound of different models specifically for mixed time series analysis tasks, we conduct grid searches of model capacity-related hyper-parameters, including number of layers and $d_{m o d e l}$ / hidden size and optimization-related hyper-parameter-learning rate. Compared with baseline models, MiTSformer additionally fine-tuned the weight of smoothness loss weight $\\lambda_{1}$ to adapt to different temporal variations of different datasets. The variable modality discrimination loss weight is fixed at 1.0 and the reconstruction loss weight is fixed at 1.0 for all experiments. ", "page_idx": 14}, {"type": "text", "text": "A.3 Algorithm Implementations of MiTSformer ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Mixed time series modeling differs significantly from typical time series tasks due to variable heterogeneity. Thereby, each component in MiTSformer is essential to address it via latent continuity recovery and alignment. We summarized the model feed-forward, loss-calculation, and parameterupdate procedures of MiTSformer with pseudo-codes, which are presented in Algorithm 1. We also report the standard deviation of MiTSformer performance with different random seeds in Table 8, which exhibits that the performance of MiTSformer is stable. Here we introduce the detailed architectures of each module in MiTSformer. ", "page_idx": 14}, {"type": "text", "text": "Recovery Network The recovery network, receiving the input of DVs $X^{D}$ with shape (Batchsize $\\times\\;n\\times T)$ ) and outputting LCVs $X^{L C V}$ with shape $({\\mathrm{Batchsize}}\\times n\\times T)$ , is composed of 3 dilated convolution blocks with residual connections. Specifically, the lth dilated convolution block is composed of 2 dilated 1D convolutional layers with kernel_size $=$ 2, hidden_channel $=8$ , $\\mathrm{{dilation}\\,=\\,\\,2^{l}}$ and GELU activations. Additionally, padding operation $\\mathrm{Padding}=\\left(\\left({\\mathrm{kernel}}{\\underline{{\\mathrm{size}}}}-1\\right)\\times\\mathrm{dilation}+1\\right)//2$ is adopted for the convolutional layer to ensure that the input and output have the same length $T$ , where $(\\mathrm{kernel\\_size}-1)\\times\\mathrm{dilation}+\\mathrm{i}$ denotes the receptive field). Finally, we perform z-score normalization for each $\\boldsymbol{x}^{L C}\\in\\mathbb{R}^{1\\times T}$ to ensure training stability as ", "page_idx": 14}, {"type": "equation", "text": "$$\nx^{L C}={\\frac{x^{L C}-\\mathrm{Mean}(x^{L C})}{\\mathrm{Std}(x^{L C})}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where Mean and Std denote the mean and the standard deviation along the time axis, respectively. ", "page_idx": 14}, {"type": "text", "text": "Self-attention and Cross-attention Sub-blocks The self-attention sub-block is composed of vanilla FullAttention with 8 attention heads and 0.1 dropout rate. The same for the cross-attention sub-block. ", "page_idx": 14}, {"type": "text", "text": "Table 5: Dataset descriptions. The dataset size is organized in (Train, Validation, Test). \u201cDim. $p'$ denotes the total variable dimension and \u201cDim. $n^{\\bullet}$ denotes the discrete variable dimension. Since current benchmark datasets are time series encompassing only continuous variables, we generate mixed time series from these datasets by discretizing partial variables. For each dataset, we randomly select half variables as DVs $\\begin{array}{r}{(n\\,=\\,\\lfloor0.5p\\rfloor)}\\end{array}$ ), whose values are first MinMax normalized and then discretized into the value of 0 or 1 with the threshold 0.5 as $\\mathtt{i n t}(\\mathtt{M i n M a x}(x)>0.5)$ . ", "page_idx": 15}, {"type": "table", "img_path": "EMV8nIDZJn/tmp/b8cb9d054378af6eb0978408800b18f0386d5d47df138739a47bbeab1a9ff037.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 6: Experiment configuration of MiTSformer. All the experiments use the ADAM optimizer with the default hyperparameter configuration for $(\\beta_{1},\\beta_{2})$ as (0.9, 0.999) with proper early stopping, and adopt a dropout rate of 0.1. $\\lambda_{1}$ denotes the weight of smoothness loss, $\\lambda_{2}$ denotes the weight of reconstruction loss, and $\\lambda_{3}$ denotes the weight of variable modality discrimination loss. ${\\mathrm{LR}}^{*}$ denotes the initial learning rate. The number of attention heads is set to 8 for all experiments. ", "page_idx": 16}, {"type": "table", "img_path": "EMV8nIDZJn/tmp/8cf81c3626815aeb369616efd5bcdb55fbcd13e7fa460327dcfb97beb1443663.jpg", "table_caption": [], "table_footnote": ["\u2020We set the batch size to 16 for the PEMS-SF dataset due to its high dimensionality. We set the batch size to 64 for other classification datasets. The same for Table .7 "], "page_idx": 16}, {"type": "text", "text": "Table 7: Experiment configuration of baseline models. All the experiments use the ADAM optimizer with the default hyperparameter configuration for $(\\beta_{1},\\beta_{2})$ as (0.9, 0.999) with proper early stopping, and adopt a dropout rate of 0.1. ${\\mathrm{LR}}^{*}$ denotes the initial learning rate. For Transformer-based models, the number of attention heads is set to 8 for all experiments. ", "page_idx": 16}, {"type": "table", "img_path": "EMV8nIDZJn/tmp/56434b0ccd87b2ceb3f8e3b50ffd3a119aee466965706ce339a8e9c66201d688.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Variable Modality Discriminator The variable modality discriminator, receiving the input variate embeddings with shape $(\\mathrm{Batchsize}\\times p\\times d_{m o d e l})$ and outputting the variable modality class with shape (Batchsize $\\times\\ p\\times2)$ ), is composed of 3-layer MLP with structure of {d_model, $4*\\mathrm{d}_{-}$ model, $4*\\mathrm{d\\_model,2\\}$ . Relu activation and batch normalization are also adopted. ", "page_idx": 16}, {"type": "text", "text": "Reconstruction Decoders The discrete variable reconstruction decoder, receiving the input of LCV embeddings $(\\mathrm{Batchsize}\\times n\\times d_{m o d e l})$ and outputting the reconstructed DVs (Batchsize $\\times\\;n\\times T\\times2)$ ), is composed of a linear layer with input_size $=$ d_model, output ${\\mathrm{size}}=2*T$ (the discrete variable reconstruction can be treated as a binary classification task). The continuous variable reconstruction decoder, receiving the input of CV embeddings (Batchsize $\\times\\left(p-n\\right)\\times d_{m o d e l}\\right)$ and outputting the reconstructed CVs (Batchsize $\\times\\left(p-n\\right)\\times{\\bar{T}})$ , is composed of a linear layer with input_size $\\l=\\mathrm{d}$ _model, output_size $=T$ . ", "page_idx": 16}, {"type": "text", "text": "In the following, we will introduce the architectures of task heads and loss functions for different downstream tasks. ", "page_idx": 16}, {"type": "text", "text": "A.4 Pipeline for Classification ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Classification is a long-standing task in the time series community and is widely used to evaluate the high-level representation capacity of models. The overall pipeline of MiTSformer-based classification is depicted in Fig. 10. For the classification task, both token embeddings of LCVs and CVs provide complementary information. Thereby, we concatenate the embeddings of LCVs and CVs. The fused embeddings are flattened and fed into a classifier network to predict the class labels. For MiTSformer and baseline models, the classifier is composed of a single-layer MLP with GELU activation and ", "page_idx": 16}, {"type": "text", "text": "Algorithm 1 The training process of MiTSformer ", "page_idx": 17}, {"type": "text", "text": "Input: Input mixed time series with continuous variables $X^{C}=\\left\\{x_{1},x_{2},...,x_{p-n}\\right\\}\\in\\mathbb{R}^{(p-n)\\times T}$ , and discrete variables $X^{D}=\\{x_{1},x_{2},...,x_{n}\\}\\in^{n\\times T}$ , number of attention blocks $L$ , loss weights $\\lambda_{1},\\lambda_{2}$ and $\\lambda_{3}$ . ", "page_idx": 17}, {"type": "text", "text": "Output: Optimized model parameters of MiTSformer. ", "page_idx": 17}, {"type": "text", "text": "1: Initialize the parameters of MiTSformer;   \n2: while not converge do   \n3: (1) Feedforward Computation:   \n4: \u25b7Latent Continuity Recovery for DVs   \n5: $x^{L C}\\leftarrow\\mathrm{Rec-Net}(x^{D});$   \n6: \u25b7Variate-wise Token Embedding of LCVs and CVs   \n7: $\\begin{array}{l}{{z_{0}^{L C}\\leftarrow\\mathrm{Embed}_{\\mathrm{LCV}}(x^{L C})}}\\\\ {{z_{0}^{C}\\leftarrow\\mathrm{Embed}_{\\mathrm{CV}}(x^{C})}}\\end{array}$   \n8:   \n9: $\\triangleright$ Inter- and Intra-Variable-Modality Spatial-Temporal Modeling   \n10: for $l=[0,1,...,L-1]$ do   \n11: \u25b7Intra-Variable-Modality Self-Attention Sub-Block   \n12: $\\begin{array}{r l}&{\\tilde{z}_{l}^{C}\\gets\\mathrm{LN}\\left(z_{l}^{C}+\\mathrm{Self.Attn}\\left(\\left[Q_{l}^{C},K_{l}^{C},V_{l}^{C}\\right]\\right)\\right)}\\\\ &{\\tilde{z}_{l}^{C}\\gets\\mathrm{LN}\\left(\\tilde{z}_{l}^{C}+\\mathrm{FFN}\\left(\\tilde{z}_{l}^{C}\\right)\\right)}\\\\ &{\\tilde{z}_{l}^{L C}\\gets\\mathrm{LN}\\left(z_{l}^{L C}+\\mathrm{Self.Attn}\\left(\\left[Q_{l}^{L C},K_{l}^{L C},V_{l}^{L C}\\right]\\right)\\right)}\\\\ &{\\tilde{z}_{l}^{L C}\\gets\\mathrm{LN}\\left(\\tilde{z}_{l}^{L C}+\\mathrm{FFN}\\left(\\tilde{z}_{l}^{L C}\\right)\\right)}\\\\ &{\\tilde{z}\\,\\mathrm{Inter-Faring-hingality~Cross-Atrention~Su~}}\\\\ &{\\tilde{z}_{l+1}^{C}\\gets\\mathrm{LN}\\left(\\tilde{z}_{l}^{C}+\\mathrm{Cross-Atrt}\\left(\\left[Q_{l}^{C},K_{l}^{L C},V_{l}^{L C}\\right]\\right)\\right)}\\\\ &{\\tilde{z}_{l+1}^{L C}\\gets\\mathrm{LN}\\left(\\tilde{z}_{l+1}^{L C}+\\mathrm{FFN}\\left(z_{l+1}^{C}\\right)\\right)}\\\\ &{\\tilde{z}_{l+1}^{L C}\\gets\\mathrm{LN}\\left(z_{l+1}^{L C}+\\mathrm{Cross-Atrn}\\left(\\left[Q_{l}^{L C},K_{l}^{C},V_{l}^{C}\\right]\\right)\\right)}\\\\ &{\\tilde{z}_{l}^{L C}\\gets\\mathrm{LN}\\left(\\tilde{z}_{l}^{L C}\\right)\\gets\\mathrm{FrN}\\left(z_{l-1}^{L C}\\right)}\\end{array}$   \n13:   \n14:   \n15:   \n16: b-Block   \n17:   \n18:   \n19:   \n20:   \n21: end for   \n22: $\\triangleright$ Task Prediction (if needed)   \n23: $\\hat{y}\\gets\\mathrm{Task-Heads}(z_{L}^{L C},z_{L}^{C})$   \n24: (2) Loss Calculation:   \n25: \u25b7Task Supervision Loss (if needed)   \n26: LTask \u2190Task-Criterion(y\u02c6, y).   \n27: \u25b7Smoothness Constraint Loss   \n28: $\\mathcal{L}_{\\mathrm{smooth}}=\\left\\lVert\\mathrm{Abs}\\left(\\boldsymbol{S}\\boldsymbol{x}^{D}\\right)\\otimes\\left(\\boldsymbol{S}\\boldsymbol{x}^{L C}\\right)\\right\\rVert_{2}^{2}.$   \n29: \u25b7Reconstruction Loss   \n30: $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{Rec}}=\\sum_{i=1}^{p-n}\\mathrm{MSE}(\\mathrm{Rec}\\mathrm{-}\\mathrm{Decoder}\\left(z_{L,i}^{C}\\right),x_{i}^{C})+\\sum_{i=1}^{n}\\mathrm{CE}(\\mathrm{Rec}\\mathrm{-}\\mathrm{Decoder}\\left(z_{L,i}^{L C}\\right),x_{i}^{D}).}\\end{array}$   \n31: \u25b7Variable Modality Discrimination Loss (via Gradient Reverse Layer)   \n32: ${\\mathcal{L}}_{\\mathrm{Dis}}=\\mathbb{E}\\left[\\log\\left(\\operatorname{GRL}(\\operatorname{Dis}\\left(z^{C}\\right)))\\right]+\\mathbb{E}\\left[\\log\\left(1-\\operatorname{GRL}(\\operatorname{Dis}\\left(z^{L C}\\right))\\right)\\right]$   \n33: \u25b7Overall Loss   \n34: $\\mathcal{L}_{\\mathrm{All}}=\\mathcal{L}_{\\mathrm{Task}}+\\lambda_{1}\\mathcal{L}_{\\mathrm{Smooth}}+\\lambda_{2}\\mathcal{L}_{\\mathrm{Rec}}+\\lambda_{3}\\mathcal{L}_{\\mathrm{Dis}}$   \n35: (3) Parameter Update:   \n36: Update the parameters of models using Adam optimizer to minimize $\\mathcal{L}_{a l l}$ ; ", "page_idx": 17}, {"type": "text", "text": "37: end while ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "38: return Optimized model parameters. ", "page_idx": 17}, {"type": "table", "img_path": "EMV8nIDZJn/tmp/3adec3d25f00060059f3645dea42e6d59b87fbea90dd5cbd058e1a65b15ab1b2.jpg", "table_caption": ["Table 8: Robustness of MiTSformer performance on forecasting datasets. Averaged MAE, MSE, and their standard deviations based on different random seeds are reported. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "a dropout layer with a dropout rate of 0.1. The task supervision loss is Cross-Entropy Loss. The classification accuracy is used as the performance evaluation metric. ", "page_idx": 18}, {"type": "image", "img_path": "EMV8nIDZJn/tmp/06885065a134ac5f40787831b685d229abdbe701bdb175803b21a2c863f71ce7.jpg", "img_caption": ["Figure 10: Overall pipeline of MiTSformer-based classification. The embeddings of LCVs and CVs are concatenated, flattened, and fed into the classifier for classification. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.5 Pipeline for Extrinsic Regression ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Closely related to classification, time series extrinsic regression aims to learn the relationship between a time series and a continuous scalar variable. The overall pipeline of MiTSformer-based extrinsic regression is depicted in Fig. 11. Similar to classification, both token embeddings of LCVs and CVs provide complementary information for regression tasks. Thereby, we concatenate the embeddings of LCVs and CVs. The fused embeddings are flattened and fed into a regressor network to predict the numerical values. For MiTSformer and baseline models, the regressor is composed of a single-layer MLP with GELU activation and a dropout layer with a dropout rate of 0.1. The task supervision loss is MSE Loss. We adopt the mean absolute error (RMAE) and the root mean square error (RMSE) as the performance evaluation metrics as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{RMSE}=\\sqrt{\\frac{1}{N_{y}}\\sum_{i=1}^{N_{y}}\\left(\\hat{y}_{i}-y_{i}\\right)^{2}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{MAE}=\\frac{1}{N_{y}}\\sum_{i=1}^{N_{y}}\\left|\\hat{y}_{i}-y_{i}\\right|\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $N_{y}$ denotes the number of test samples, $\\hat{y}_{i}$ denotes the predicted labels and $y_{i}$ denotes the ground-truth label. ", "page_idx": 18}, {"type": "image", "img_path": "EMV8nIDZJn/tmp/8c5820218f80ea34b1d0194471a7c6b9bd91704f4f121623072e094220d323d7.jpg", "img_caption": ["Figure 11: Overall pipeline of MiTSformer-based extrinsic regression. The embeddings of LCVs and CVs are concatenated, flattened, and fed into the regressor for regression. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "A.6 Pipeline for Imputation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The imputation techniques are developed to impute the missing values based on the partially observed time points. For the imputation task, we mainly focus on imputating the missing value of CVs, while DVs are adopted as input to provide auxiliary information. The overall pipeline of MiTSformer-based imputation is depicted in Fig. 11. For MiTSformer, the task head is an imputation decoder composed of a linear layer, which performs variate-wise imputation for each CV. For baseline methods that adopt the channel-independent strategy, we adopt the same task head as MiTSformer. For baseline methods that adopt the channel-mixing strategy, we adopt a linear layer with input size of $d_{m o d e l}$ and output size of continuous variable dimension $p-n$ to reconstruct the CVs. We use a mask matrix $\\bar{M^{\\mathrm{~}}}\\in\\mathbb{R}^{T\\times(p-n)}$ to represent the missing values in input CVs. The state of $_M$ denotes whether the corresponding element value of CVs is missing (denoted by 0) or not (denoted by 1). The task supervision loss is the MSE loss calculated on the masked observations. Specifically for MiTSformer, the reconstruction loss of CVs is calculated on unmasked observations. We adopt the mean absolute error (MAE) and the root mean square error (RMSE) as the performance evaluation metrics that are computed on masked elements. ", "page_idx": 19}, {"type": "image", "img_path": "EMV8nIDZJn/tmp/98b312449e47c6a9c17569c67dac72fc434b86880e159883aa7e666a21642753.jpg", "img_caption": ["Figure 12: Overall pipeline of MiTSformer-based imputation. The embeddings of CVs are individually fed into the imputation decoder to impute missing values of CVs. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "A.7 Pipeline for Long-term Forecasting ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Forecasting is a fundamental problem in the time series community, and long-term forecasting is a more practical and challenging task. The long-term forecasting task for MiTS includes the prediction of both DVs and CVs. The overall pipeline of MiTSformer-based long-term forecasting is depicted in Fig. 13. For MiTSformer, the task head is composed of linear layer-based DVforecastor and CVforecaster, which perform variate-wise forecasting for each DV and each CV, respectively. For baseline models, we also adopt the linear layer-based forecaster to forecast the DVs with the extracted temporal features. We adopt the mean absolute error (MAE) and the root mean square error (RMSE) as the forecasting performance evaluation metrics. ", "page_idx": 19}, {"type": "image", "img_path": "EMV8nIDZJn/tmp/3d2e60adf1151b6d9e0b407ad0551d0bdde275096c9d43793a5069d59a1e4240.jpg", "img_caption": ["Figure 13: Overall pipeline of MiTSformer-based long-term forecasting. The embeddings of LCVs are individually fed into the DVForecaster to predict the future value of corresponding DVs, and the embeddings of CVs are individually fed into the DVForecaster to predict the future value of corresponding CVs. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "A.8 Pipeline for Anomaly Detection ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The anomaly detection task is achieved by self-supervised autoencoding with the reconstruction of both CVs and DVs. The reconstruction errors of DVs and DVs are utilized as anomaly criteria. In our experiment, we estimate the anomaly thresholds respectively for DVs and CVs. For a test sample, the reconstruction error of DVs exceeding the threshold or the reconstruction error of DVs exceeding the threshold is considered an anomaly. The overall pipeline of MiTSformer-based anomaly detection is depicted in Fig. 13. Since there already exist reconstruction decoders and corresponding losses, no task head and additional losses are devised. For baseline models, we also adopt the linear layer-based reconstruct decoders to reconstruct the DVs and linear layer-based reconstruct decoders to reconstruct the CVs with the extracted temporal features. MSE loss is used for reconstruction of CVs and CrossEntropy loss is used for reconstruction of DVs. We adopt the Precision, Recall and F1-Socre as matrices. ", "page_idx": 20}, {"type": "image", "img_path": "EMV8nIDZJn/tmp/fc4e9d1cabcde8a29442eabd0dec739e3e8c81b87d85571fce1b97509f7ab026.jpg", "img_caption": ["Figure 14: Overall pipeline of MiTSformer-based anomaly detection. The anomaly detection tasks only rely on self-reconstruction and thus no task head is attached. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "B Model Efficiency Analysis ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Complexity of MiTSformer Considering the self-attention and cross-attention sub-blocks, the complexity of MiTSformer can be derived as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\underbrace{\\mathcal{O}\\left(n^{2}\\right)+\\mathcal{O}\\left(\\left(p-n\\right)^{2}\\right)}_{\\mathrm{self-attention}}+\\underbrace{\\mathcal{O}\\left(n\\left(p-n\\right)\\right)+\\mathcal{O}\\left(\\left(p-n\\right)n\\right)}_{\\mathrm{cross-attention}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $n$ denotes the number of DVs and $p-n$ denotes the number of CVs, $p$ is the number of total variables. In our experimental setting, we keep $n=\\lfloor0.5p\\rfloor$ . Thereby, the complexity of MiTSformer is quadratic to the number of DVs as ${\\mathcal{O}}\\left(n^{2}\\right)$ . ", "page_idx": 20}, {"type": "image", "img_path": "EMV8nIDZJn/tmp/297b2a24857231ead75dfa84ae16bc45e0d26f9805a0dd9e5f8d54deef876674.jpg", "img_caption": ["Figure 15: Model efficiency analysis. Experiments are carried out on ETTh1 and Electricity datasets with \u201cinput-output\u201d setting of $^{\\leftarrow}96{-}720^{\\circ}$ . For each subfigure, dots with different colors represent different methods, and the size of the circle represents the magnitude of the memory footprint. The horizontal axis represents the training time (seconds) per iter, and the vertical axis represents the forecasting accuracy (MAE). "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Model Efficiency Comparison We comprehensively compare the model performance, training speed, and memory footprint of MiTSformer and baseline models. The results are with the optimal model configurations as reported in Table 6 and 7. Two representative long-term forecasting datasets ETTh1 (3DVs, 4CVs) and Electricity (160DVs, 161CVs) with input-96 and output-720 settings are adopted for efficiency comparison. The results are depicted in Fig. 15. ", "page_idx": 21}, {"type": "text", "text": "In general, MiTSformer maintains great performance and efficiency compared with most baselines in datasets with a relatively small number of variables (ETTh1). When encountering datasets with a relatively large number of variables (Electricity), MiTSformer occupies a relatively large memory footprint, specifically compared with some advanced efficient models such as modern TCN and DLinear. However, the training time of MiTSformer is still efficient. ", "page_idx": 21}, {"type": "text", "text": "C Hyper-parameter Sensitivity Analysis ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We evaluate the hyper-parameter sensitivity of MiTSformer with respect to the following two aspects: ", "page_idx": 21}, {"type": "text", "text": "Sensitivity of Model Capacity The hidden dimension $d_{m o d e l}$ and number of layers $L$ influence the model capacity of MiTSformer. We evaluate the sensitivity of these two hyper-parameters on typical long-term forecasting datasets, including ETTh1, ETTh2, and Weather with four different prediction horizon settings. The results are presented in Fig. 16. We can find that MiTSformer is relatively stable in the selection of $d_{m o d e l}$ and $L$ , particularly for ETTh2 and Weather datasets. Specifically for the ETTh1 dataset, the model capacities are not essentially favored to be as large as possible. ", "page_idx": 21}, {"type": "text", "text": "Sensitivity of Loss Functions We further investigate the effects of loss items, including smoothness loss weight $\\lambda_{1}$ , reconstruction loss weight $\\lambda_{2}$ , and variable modality discrimination loss weight $\\lambda_{3}$ on classification datasets (JapaneseVowels, SpokenArabicDigits, and SelfRegulationSCP1). The results are presented in Fig. 17. In general, we can observe that MiTSformer is quite robust to the weights of loss items, and moderate weights bring optimal performance. For example, $\\lambda_{1}$ controls the magnitude of the smoothing constraints for latent continuity recovery. Too small $\\lambda_{1}$ would make the smoothness invalid while too large $\\lambda_{1}$ may lead to over-smoothing problems. Therefore, a moderate setting of $\\lambda_{1}$ is favored for MiTSforemer. ", "page_idx": 21}, {"type": "text", "text": "D Limitations and Future Work ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Handling Discrete Variables with Natural Sudden Changes The smoothness loss in our method is suitable for DVs with sudden changes that are caused by inherent smooth variations. However, it may not adequately account for DVs with inherent sudden changes that are essential characteristics of the dataset. For such cases, we can adjust the coefficient $\\lambda_{1}$ with a relatively small value, or we can further leverage less restrictive constraints such as K-Lipschitz continuity to determine the proper smoothness level. ", "page_idx": 21}, {"type": "image", "img_path": "EMV8nIDZJn/tmp/79c429969991618d5b17d6ed4b5b0012321122c0467dbe5688bdda6c0951073e.jpg", "img_caption": ["Figure 16: Sensitivity analysis of model capacity-related hyper-parameters $d_{m o d e l}$ and number of layers $L$ . Experiments are carried out on typical long-term forecasting datasets, including ETTh1, ETTh2, and Weather datasets. \u201c96-96/192/336/720\u201d denotes different forecasting length settings "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "EMV8nIDZJn/tmp/1452b5064c6fdeb1a92f6503c0cbaa732d730d1d3455e08e59aa3b2f6ec6c60e.jpg", "img_caption": ["Figure 17: Sensitivity analysis of loss items, including smoothness loss weight $\\lambda_{1}$ , reconstruction loss weight $\\lambda_{2}$ and variable modality discrimination loss weight $\\lambda_{3}$ . Experiments are carried out on classification datasets JapaneseVowels, SpokenArabicDigits, and SelfRegulationSCP1. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Modeling Generalized Mixed Time Series MiTSformer is based on the assumption that the observed discrete variables are derived from latent continuous variables, which can be guaranteed for integer discrete variables whose discrete states represent the magnitude of the numerical value. However, a limitation of MiTSformer is that it can not be directly applied to categorical discrete variables whose discrete states represent different and mutually exclusive class labels (e.g., color and gender). It is noted that integer discrete variables are more frequently encountered in real-world time series data, specifically in the fields of medical analysis, finance, and industrial processes. In contrast, categorical discrete variables are often recorded as independent samples that are not presented in time series forms. In the future, it is of interest to exploit the analysis of generalized variables (including both integer discrete variables and categorical discrete variables) by leveraging advanced cross-modality modeling and fusion technologies. ", "page_idx": 23}, {"type": "text", "text": "Incorperating Powerful LLMs and Pre-training Techniques Recently, large language models, such as the Generative Pre-trained Transformer (GPT), have shown impressive performance on various applications and attracted numerous research interests. It is intriguing to explore the potential of GPT-type architectures for mixed time series analysis tasks with proper inductive biases and elaborate prompt strategies. Additionally, by utilizing efficient pre-training techniques, we can enhance the scalability of MiTSformer in large-scale or real-time real-world applications. Meanwhile, the mixed time series analysis on few-shot and zero-shot settings can also be a promising research direction. ", "page_idx": 23}, {"type": "text", "text": "E Additional Experiments and Discussions ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "E.1 Comparison with Mixed Naive Bayes and Variational Inference-based methods ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To deal with mixed variables, naive Bayes (NB)-based models [38] and variational inference (VI)- based models [12] are proposed to match the distributions of continuous variables (CVs) and discrete variables (DVs) with different distribution priors for industrial process data analysis. Essentially, mixed NB and VI methods are typically designed for tabular data. They struggle with time series and often rely on certain assumptions like conditional independence, limiting their ability to model the correlations between DVs and CVs. In contrast, MiTSformer leverages temporal adjacencies to achieve latent continuity recovery, making it capable of handling time series data and effectively capturing inherent nonlinear correlations within MiTS. Empirically, we compare MiTSformer against typical mixed NB-based models HVM [1] and VI-based methods VAMDA [2] on mixed time series classification datasets and summarized the results in Table 9, showing MiTSformer consistently outperforms HVM and VAMDA. ", "page_idx": 23}, {"type": "text", "text": "E.2 Investigation of Non-binary Discrete Variable ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In our main experimental setting, we chose the binary DVs as they are the most challenging and commonly encountered in the real world. Actually, DVs can take on multiple states $(\\geqslant2)$ that reflect the magnitude and can be directly input into the recovery network to obtain LCVs outputs. It is noted ", "page_idx": 23}, {"type": "text", "text": "Table 9: Compared to mixed NB- and VI-based methods. Accuracy $(\\%)$ scores are reported. The best results are bolded. ", "page_idx": 24}, {"type": "table", "img_path": "EMV8nIDZJn/tmp/1ab57d5e8b6f193ce2beecbe2ff2370b8b872a7769c09b2fe035052c14f25980.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "EMV8nIDZJn/tmp/1e15539ca62e99f5f12dc73643c820bf0e75a7e6e9a2b029aeee27ec88923791.jpg", "table_caption": ["Table 10: Performance of mixed time series classification under the different discrete states of DVs, i.e., $N_{\\mathrm{DVs}}$ . Accuracy $(\\%)$ scores are reported. The best results are bolded. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "that more states in a DV imply richer information granularity. Also, we conducted experiments on mixed time series classification and anomaly detection datasets with various numbers of discrete states in Tabel 10 and Table 11, respectively. In general, the results shows that as the number of states increases, performance improves due to the richer information. ", "page_idx": 24}, {"type": "text", "text": "Table 11: Performance of mixed time series anomaly detection under the different discrete states of DVs, i.e., $N_{\\mathrm{DVs}}$ .The best results are bolded. ", "page_idx": 24}, {"type": "table", "img_path": "EMV8nIDZJn/tmp/176f63e5086473955ab7a43ad681625caaadbb56d96149443c94af056695fe82.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "EMV8nIDZJn/tmp/bfc73a419d3d3c3bcdf73def1b56a2e645328fdf4fc8f924e8cdc27c1b168e1f.jpg", "table_caption": ["Table 12: Full classification results. We report the classification accuracy $(\\%)$ as the result. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "EMV8nIDZJn/tmp/21fdd3c92ebf8217a6271ffa2f0d2c56bba5509ec5d802af09d8e86b035db3f2.jpg", "table_caption": ["Table 13: Full extrinsic regression Results. We report the MAE and RMSE as the result. "], "table_footnote": ["Kindly note: For dataset abbreviations,\u201cAppli.\u201d denotes \u201cAppliancesEnergy\u201d; \u201cHouse.1\u201d denotes \u201cHouseholdPowerConsumption1\u201d; \u201cHouse.1\u201d denotes \u201cHouseholdPowerConsumption2\u201d; \u201cBenze.\u201d denotes \u201cBenzeneConcentration\u201d; \u201cBJ.PM25 \u201d denotes \u201cBeijingPM25Quality\u201d; \u201cBJ.PM10 \u201d denotes \u201cBeijingPM10Quality\u201d; \u201cLive.\u201d denotes \u201cLiveFuelMoistureContent\u201d; \u201cAustra.\u201d denotes \u201cAustraliaRainfall\u201d; \u201cPPGD.\u201d denotes \u201cPPGDalia\u201d; and \u201cIEEE.\u201d denotes \u201cIEEEPPG\u201d. "], "page_idx": 25}, {"type": "text", "text": "F Full Experimental Results ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The full results of all experiments are presented in the following: classification in Table 12, extrinsic regression in Table 13, imputation in Table 14, anomaly detection in Table 15. Besides, long-term forecasting results of CVs are presented in Table 16 and those of DVs are presented in 17. ", "page_idx": 25}, {"type": "table", "img_path": "EMV8nIDZJn/tmp/6f5848b70ae862e8652bad888e9c22c6148bffe16e263c55a8dc99e3f52a04f4.jpg", "table_caption": ["Table 14: Full imputation results. We randomly mask $12.5\\%$ , $25\\%$ , $37.5\\%$ and $50\\%$ time points to compare the model performance under different missing degrees. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Table 15: Full anomaly detection results. The \u201cP\u201d, \u201cR\u201d, and \u201cF1\u201d represent the precision, recall, and F1-score $(\\%)$ respectively. F1-score is the harmonic mean of precision and recall. A higher value of P, R, and F1 indicates better anomaly detection performance. . ", "page_idx": 26}, {"type": "table", "img_path": "EMV8nIDZJn/tmp/fcca92de64682e91997bc8057485f3f47aa184edd65d173ddb39adf48159eee3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "EMV8nIDZJn/tmp/cf3fb353cd8a0eb4979f536e7bff72112d81d6b74aed6a4fdd4d816830315fc4.jpg", "table_caption": ["Table 16: Full long-term forecasting results for CVs. We compare extensive competitive models under different prediction lengths. The input sequence length is set to 36 for the ILI dataset and 96 for the others. Avg. is averaged from all four prediction lengths. The performance is evaluated by MAE and RMSE. \u201c-\u201d denotes \u201cout of memory\u201d. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "EMV8nIDZJn/tmp/78b31ba17f681829eb26e963a4735c89511d3fafe02d6c3a7df087b02b492667.jpg", "table_caption": ["Table 17: Full long-term forecasting results for DVs. We compare extensive competitive models under different prediction lengths. The input sequence length is set to 36 for the ILI dataset and 96 for the others. Avg. is averaged from all four prediction lengths. The performance is evaluated by forecasting accuracy $(\\%)$ . \u201c-\u201d denotes \u201cout of memory\u201d. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "G Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "This paper copes with the general analysis of real-world mixed time series, which are under-explored in literature but frequently encountered in various practical applications, such as industrial maintenance, healthcare monitoring, and financial analysis. Since previous studies struggle to address the spatial-temporal heterogeneity problem for real-world MiTS, we present MiTSformer that fundamentally reveals and recovers the latent continuous variables for discrete variables to facilitate exploiting the intricate spatial-temporal patterns within MiTS, thereby being amenable to various analysis task. Our model achieves the state-of-the-art performance on five mainstream tasks that cover ${30+}$ real-world datasets from diverse application domains. Therefore, the proposed model makes it promising to tackle real-world MiTS analysis tasks, which helps our society make better decisions. Our paper mainly focuses on scientific research and has no obvious negative social impact. ", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have clearly claimed our contributions in the end of the introduction from the perspective of (1) Problem Formulation, (2) Technical Design, and (3) Empirical Evaluation. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have analyzed the limitations of our proposal in Appendex D. Also, we investigated the computational efficiency of our approach in Appendix B. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our paper does not include theoretical results. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have provided the details regarding reproducibility, including computational platforms, dataset descriptions, model configurations, task-specific pipelines, hyperparameters settings, and pseudo-code of our approach in Appendix A. Also, We have released the source code on Anonymous Github as stated in the Abstract. The download links of the datasets and pre-processing functions are also included in the project homepage. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The datasets used in our paper are benchmarks and are publicly available. We have provided the relevant references and hyperlinks of datasets. Also, We have released the source code on Anonymous Github as stated in the Abstract. The download links of the datasets and pre-processing functions are also included in the project homepage. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We have provided all the experimental implementation details in Appendix A, including (1) dataset descriptions and splits of training/validation/test sets (Table 5); (2) Experiment configurations and hyperparameter settings of our approach (Table 6) and baselines (Table 7). ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All experiments are repeated three times and we have reported the average results. Also, we have investigated the sensitivity of our approach in Appendix C. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have clearly stated the experimental platforms (e.g., GPUs and CPUs) and software (the version of Python and Pytorch) used in our paper in Appendix A, and we have analyzed the computational efficiency of our approach in Appendix B. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and we have ensured to preserve anonymity. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have discussed both potential positive societal impacts and negative societal impacts of our work in Appendix G. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our paper poses no such risks. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We have cited the original paper (or URL) of the code package or dataset that is used in our paper. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We have released the source code on Anonymous Github as stated in the Abstract. The instructions for running our codes are provided on the project homepage at Anonymous Github. If our paper is accepted, we commit to releasing the relevant code as open-source and will provide detailed documentation to support its use and replication of our results. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]