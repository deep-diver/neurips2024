[{"figure_path": "u2gzfXRLaN/tables/tables_6_1.jpg", "caption": "Example 1. Consider a class H = {h1, h2, h3}, a collection of transformations T = {T1, T2, T3}, and a distribution D with risks (errors) as reported in the table.", "description": "This table shows the error rates of three different predictors (h1, h2, h3) on three different transformed distributions (T1(D), T2(D), T3(D)).  The error rates represent the percentage of misclassifications for each predictor on each distribution. This example illustrates a scenario where minimizing the worst-case error (Objective 1) might not be the optimal strategy, as it could lead to choosing a predictor with high error across all transformations. In contrast, another predictor might perform very well on most transformations.", "section": "4 Unknown Invariant Transformations"}, {"figure_path": "u2gzfXRLaN/tables/tables_7_1.jpg", "caption": "Example 2 (Risk vs. Regret). Consider a class H = {h1, h2}, a collection of transformations T = {T1, T2, T3, T4}, and a distribution D such that with errors as reported in the table:", "description": "This table shows the error rates for two predictors (h1 and h2) under four different transformations (T1 to T4) of a distribution D.  It illustrates how minimizing worst-case risk (Objective 1) might not be optimal when there's heterogeneity in the noise across transformations, and highlights the advantage of minimizing worst-case regret (Objective 7).", "section": "5 Extension to Minimizing Worst-Case Regret"}]