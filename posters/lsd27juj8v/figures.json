[{"figure_path": "lsd27JUJ8v/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Comparison of double Q technique with empirical Q. We train Double DQN [21] and TD3 [17] with the double Q technique. Currently, we learn based on Eq. (2). The graphs show the difference between the Q value and discounted Monte Carlo return. Greater/less than 0 means overestimation/underestimation. The double Q technique overestimates on Breakout but underestimates on HalfCheetah. Q yields much more accurate estimation on both tasks. (b) In Empirical MDP Iteration (EMIT), we consider transitions existing in the replay memory as an empirical MDP Mi, then solve Mi and collect more data. Repeating this process yields refined empirical MDPs M1, M2,..., which progressively approach the original MDP M.", "description": "This figure compares the performance of the double Q-learning technique with an empirical Q-learning method on two tasks: Breakout and HalfCheetah.  It also illustrates the Empirical MDP Iteration (EMIT) framework, which iteratively refines empirical MDPs using data from a replay memory to progressively approach the true MDP.", "section": "1 Introduction"}, {"figure_path": "lsd27JUJ8v/figures/figures_3_1.jpg", "caption": "Figure 12: In the CliffWalk scenario, the rewards are as follows: +1 for reaching the goal, -1 for falling into the cliff, and 0 otherwise. The first row illustrates four cases with increasing transition coverage in the replay memory. The second row shows the errors during the learning process. The third and last rows present the final policies derived from Q and Q. Red arrows represent incorrect actions that neither follow Q* nor Q*. The blue shading indicates the action values.", "description": "This figure shows an example in CliffWalk environment with four different scenarios of replay memory content.  The first row visualizes the state-action pairs present in each replay memory. The second row shows the estimation error curves for Q and Q (learned using Bellman update and in-sample Bellman update respectively) compared to the true optimal Q*. The third and fourth rows illustrate the resulting greedy policies derived from Q and Q respectively, with red arrows highlighting incorrect actions.", "section": "Empirical MDP Iteration"}, {"figure_path": "lsd27JUJ8v/figures/figures_5_1.jpg", "caption": "Figure 3: Performance on Atari (first row) and MuJoCo (second row) tasks. The learning curves show mean scores and standard deviations over five runs. EMIT consistently boosts performance across diverse tasks.", "description": "This figure displays the learning curves for several Atari and MuJoCo reinforcement learning tasks.  The top row shows results for Atari games (Asteroids, Atlantis, Breakout, Gravitar), while the bottom row presents results for MuJoCo continuous control tasks (Ant, HalfCheetah, Hopper, Humanoid). Each curve represents the average performance across five independent runs, with shaded areas indicating standard deviation.  The results demonstrate that incorporating the Empirical MDP Iteration (EMIT) method consistently improves performance compared to standard DQN and TD3 algorithms across various environments.", "section": "4 Experiments"}, {"figure_path": "lsd27JUJ8v/figures/figures_6_1.jpg", "caption": "Figure 4: Average performance on 20 Atari games and 8 MuJoCo tasks, with each method's score normalized between 0 and 1. EMIT achieves the best average performance across these tasks.", "description": "This figure compares the average normalized performance of different reinforcement learning algorithms across 20 Atari games and 8 MuJoCo tasks.  The normalization ensures scores are between 0 and 1, allowing for easy comparison.  The results show that the Empirical MDP Iteration (EMIT) method consistently outperforms other state-of-the-art algorithms, demonstrating its effectiveness in improving the performance of both discrete and continuous control tasks.", "section": "Experiments"}, {"figure_path": "lsd27JUJ8v/figures/figures_7_1.jpg", "caption": "Figure 5: (a) We run DQN and TD3 on Breakout and HalfCheetah, and concurrently learn action value Q with the collected data. The curves show the performance of policies \u03c0 and \u0175, derived from Q and Q respectively. Remarkably, \u0175, learned without active environment interaction, matches or even surpasses \u03c0's performance. (b) EMIT helps reduce the error in Q and learns almost accurate value estimation during the learning process.", "description": "This figure compares the performance of policies learned using the Bellman update (\u03c0) and the in-sample Bellman update (\u0175) on Breakout and HalfCheetah environments.  The left plot (a) shows that the passive learning policy (\u0175) performs comparably or even better than the active learning policy (\u03c0), highlighting that the in-sample Bellman update is robust to missing transitions.  The right plot (b) demonstrates that EMIT significantly reduces the estimation error in Q, leading to more accurate value estimations.", "section": "3.1 Enhancing Online RL Algorithms with EMIT"}, {"figure_path": "lsd27JUJ8v/figures/figures_8_1.jpg", "caption": "Figure 6: (a) EMIT helps reduce unnecessary policy change comparing with DQN, potentially contributing to the enhanced performance. (b) Both the regularization and the exploration mechanism benefit the online learning. The regularization term exerts a greater impact than the exploration mechanism.", "description": "This figure shows an ablation study on the effects of the regularization term and exploration bonus in the proposed EMIT method.  The left panel (a) illustrates how EMIT reduces policy churn (frequent changes in the optimal policy) compared to a standard DQN. The right panel (b) displays the performance improvement when using the full EMIT method versus versions without regularization, without the exploration bonus, and without both components. This demonstrates that both components contribute to performance, and the regularization term is more impactful than the exploration bonus.", "section": "4.5 Effect of Loss Regularization and Exploration Bonus"}, {"figure_path": "lsd27JUJ8v/figures/figures_14_1.jpg", "caption": "Figure 7: A toy MDP with two states and two actions. (s2, a2) is missed in the replay memory.", "description": "This figure shows a simple Markov Decision Process (MDP) with two states (s1, s2) and two actions (a1, a2).  The solid lines represent transitions that are present in the replay memory, while the dashed line represents a transition that is missing. The absence of the (s2, a2) transition highlights the concept of incomplete data within a replay memory, which is central to the paper's discussion of the limitations of the standard Bellman update in reinforcement learning. The missing transition impacts the accuracy of value estimation and policy learning, especially when using the standard Bellman update which bootstraps from out-of-sample actions.", "section": "A Proof of Propositions in Section 3"}, {"figure_path": "lsd27JUJ8v/figures/figures_18_1.jpg", "caption": "Figure 8: Illustration of the CliffWalk environment. Each grid represents a state, and the arrow indicates the optimal path from the start state (S) to the goal state (G).", "description": "This figure illustrates the CliffWalking environment. It's a grid world where the agent starts at the bottom left (S) and must navigate to the goal state (G) at the bottom right.  A shaded region represents a \"cliff,\" which results in a large negative reward if the agent steps into it. The arrow shows the optimal path for the agent to take to reach the goal while avoiding the cliff, highlighting the risk of falling and receiving a negative reward. The purpose of the figure is to provide a simple visual representation of the environment used in the paper's experiments.", "section": "B.1 Cliffwalk"}, {"figure_path": "lsd27JUJ8v/figures/figures_18_2.jpg", "caption": "Figure 12: In the CliffWalk scenario, the rewards are as follows: +1 for reaching the goal, -1 for falling into the cliff, and 0 otherwise. The first row illustrates four cases with increasing transition coverage in the replay memory. The second row shows the errors during the learning process. The third and last rows present the final policies derived from Q and Q. Red arrows represent incorrect actions that neither follow Q* nor Q*. The blue shading indicates the action values.", "description": "This figure shows a comparison between two Q-learning methods (Bellman update and in-sample Bellman update) applied to CliffWalk in four different scenarios with varying levels of data coverage. It shows that in-sample Bellman update provides more robust estimations and yields more accurate policies, especially in scenarios with incomplete data.", "section": "3 Empirical MDP Iteration"}, {"figure_path": "lsd27JUJ8v/figures/figures_19_1.jpg", "caption": "Figure 10: Visualization of Atari environments.", "description": "This figure shows screenshots of five different Atari 2600 games used in the experiments described in the paper.  These games represent a diversity of gameplay mechanics and visual styles, allowing the researchers to evaluate the performance of their reinforcement learning method across a range of challenges.", "section": "B.2 Arcade Learning Environment"}, {"figure_path": "lsd27JUJ8v/figures/figures_19_2.jpg", "caption": "Figure 11: Visualization of MuJoCo environments.", "description": "This figure shows visualizations of five different MuJoCo environments used in the paper's experiments: Ant, HalfCheetah, Hopper, Swimmer, and Walker2D.  These are simulated robotic control tasks, each with a distinct morphology and locomotion style, used to evaluate the performance of reinforcement learning algorithms.  The environments are rendered in a simple, checkered-floor setting.", "section": "B.3 MuJoCo"}, {"figure_path": "lsd27JUJ8v/figures/figures_21_1.jpg", "caption": "Figure 12: In the CliffWalk scenario, the rewards are as follows: +1 for reaching the goal, -1 for falling into the cliff, and 0 otherwise. The first row illustrates four cases with increasing transition coverage in the replay memory. The second row shows the errors during the learning process. The third and last rows present the final policies derived from Q and Q. Red arrows represent incorrect actions that neither follow Q* nor Q*. The blue shading indicates the action values.", "description": "This figure shows a comparison of the performance of in-sample and out-of-sample Bellman updates in the CliffWalk environment. Four scenarios with varying data coverage are shown, highlighting how the in-sample approach is more robust to incomplete data. The figure displays the learning curves, value errors, and resulting policies for both methods, demonstrating the superior performance of the in-sample Bellman update when dealing with incomplete data.", "section": "Empirical MDP Iteration"}, {"figure_path": "lsd27JUJ8v/figures/figures_22_1.jpg", "caption": "Figure 3: Performance on Atari (first row) and MuJoCo (second row) tasks. The learning curves show mean scores and standard deviations over five runs. EMIT consistently boosts performance across diverse tasks.", "description": "This figure displays the performance of DQN and TD3 algorithms with and without the EMIT enhancement on Atari and MuJoCo benchmark tasks.  Learning curves show mean scores with standard deviation across 5 runs for each task.  The results clearly indicate consistent performance improvements across diverse tasks when EMIT is applied.", "section": "Experiments"}, {"figure_path": "lsd27JUJ8v/figures/figures_23_1.jpg", "caption": "Figure 14: The performance improvement on MuJoCo tasks.", "description": "The figure shows the learning curves for eight MuJoCo continuous control tasks.  The curves compare the performance of three different methods: EMIT-TD3 (the proposed method combining Empirical MDP Iteration with TD3), TD3 (the baseline TD3 algorithm), and Q_online (a TD3 variant using only in-sample Bellman updates but lacking the exploration component of EMIT). The shaded areas represent the standard deviation across five runs.  The results illustrate that EMIT-TD3 consistently outperforms both TD3 and Q_online across all tasks, demonstrating the effectiveness of the proposed method in improving the performance of existing reinforcement learning algorithms.", "section": "4 Experiments"}, {"figure_path": "lsd27JUJ8v/figures/figures_23_2.jpg", "caption": "Figure 3: Performance on Atari (first row) and MuJoCo (second row) tasks. The learning curves show mean scores and standard deviations over five runs. EMIT consistently boosts performance across diverse tasks.", "description": "The figure presents learning curves for DQN and TD3 algorithms, with and without the EMIT enhancement, across various Atari and MuJoCo environments.  It showcases the consistent performance improvement achieved by integrating EMIT into both algorithms across diverse tasks, highlighting the method's effectiveness.  Each curve represents the average performance over five independent runs, with shaded regions indicating standard deviations. The x-axis represents the number of environment steps, and the y-axis represents the mean score achieved.", "section": "4 Experiments"}, {"figure_path": "lsd27JUJ8v/figures/figures_24_1.jpg", "caption": "Figure 3: Performance on Atari (first row) and MuJoCo (second row) tasks. The learning curves show mean scores and standard deviations over five runs. EMIT consistently boosts performance across diverse tasks.", "description": "This figure displays the results of applying the Empirical MDP Iteration (EMIT) method to Deep Q-Network (DQN) and Twin Delayed Deep Deterministic policy gradient (TD3) algorithms on Atari and MuJoCo benchmark tasks.  The top row shows results for several Atari games (Asteroids, Atlantis, Breakout, Gravitar), while the bottom row shows results for several MuJoCo continuous control tasks (Ant, HalfCheetah, Hopper, Humanoid). Each plot presents the mean reward over five independent runs, along with the standard deviation represented by shaded areas.  The results demonstrate that EMIT consistently improves the performance of both DQN and TD3 across all tested environments.", "section": "4 Experiments"}, {"figure_path": "lsd27JUJ8v/figures/figures_24_2.jpg", "caption": "Figure 3: Performance on Atari (first row) and MuJoCo (second row) tasks. The learning curves show mean scores and standard deviations over five runs. EMIT consistently boosts performance across diverse tasks.", "description": "The figure shows the performance comparison of the proposed EMIT method against baseline methods (DQN and TD3) across various Atari and MuJoCo environments.  The learning curves illustrate the mean scores and standard deviations across five independent runs. The results demonstrate that EMIT consistently improves performance in a variety of tasks.", "section": "Experiments"}, {"figure_path": "lsd27JUJ8v/figures/figures_25_1.jpg", "caption": "Figure 3: Performance on Atari (first row) and MuJoCo (second row) tasks. The learning curves show mean scores and standard deviations over five runs. EMIT consistently boosts performance across diverse tasks.", "description": "This figure displays the learning curves for both Atari and MuJoCo environments.  The top row shows results from Atari games, while the bottom row shows results from MuJoCo continuous control tasks. Each environment's performance is shown as a learning curve, plotting mean score against environment steps. Shaded regions indicate standard deviation across five runs. The results clearly demonstrate that using EMIT consistently improves the learning performance compared to using standard DQN and TD3.", "section": "4 Experiments"}, {"figure_path": "lsd27JUJ8v/figures/figures_25_2.jpg", "caption": "Figure 3: Performance on Atari (first row) and MuJoCo (second row) tasks. The learning curves show mean scores and standard deviations over five runs. EMIT consistently boosts performance across diverse tasks.", "description": "This figure displays the performance of the proposed EMIT method compared to standard DQN and TD3 algorithms on both Atari and MuJoCo benchmark tasks.  The learning curves show the average reward obtained over five independent runs, illustrating the consistent performance improvement provided by EMIT across various game and robotic control environments.  Error bars represent standard deviations, showcasing the stability of EMIT's performance enhancements.", "section": "Experiments"}, {"figure_path": "lsd27JUJ8v/figures/figures_26_1.jpg", "caption": "Figure 3: Performance on Atari (first row) and MuJoCo (second row) tasks. The learning curves show mean scores and standard deviations over five runs. EMIT consistently boosts performance across diverse tasks.", "description": "The figure shows the learning curves for DQN and TD3 algorithms on Atari and MuJoCo environments with and without EMIT.  The results demonstrate that incorporating EMIT significantly improves performance across a variety of tasks, both discrete and continuous control tasks. The graphs display mean scores and standard deviations across five independent runs, illustrating the consistency of EMIT's performance enhancement.", "section": "4 Experiments"}, {"figure_path": "lsd27JUJ8v/figures/figures_26_2.jpg", "caption": "Figure 3: Performance on Atari (first row) and MuJoCo (second row) tasks. The learning curves show mean scores and standard deviations over five runs. EMIT consistently boosts performance across diverse tasks.", "description": "This figure displays the performance of DQN and TD3 algorithms with and without the EMIT enhancement on various Atari and MuJoCo benchmark tasks.  Each plot shows the mean reward over five independent runs, with error bars representing the standard deviation. The results clearly demonstrate that integrating EMIT significantly improves the learning performance and stability of both DQN and TD3 across a range of tasks with different complexities and action spaces (discrete for Atari, continuous for MuJoCo).", "section": "4 Experiments"}, {"figure_path": "lsd27JUJ8v/figures/figures_27_1.jpg", "caption": "Figure 2: (a) illustrates two cases in the CliffWalk task: memory D only contains a sub-optimal trajectory and misses many transitions (left); D contains all optimal state-action pairs but misses some sub-optimal actions (right). The curves show estimation errors of Q and Q learned with Eqs. (1) and (2) compared with Q* and Q*. (b) presents the greedy policies after learning. Red arrows indicate incorrect actions neither follow arg max Q* nor arg max Q*. The blue shading signifies accurately estimated Q and overestimated Q values.", "description": "This figure compares the performance of two Bellman updates (Eqs. 1 and 2) on a CliffWalk environment. The left panel shows the estimation errors when the replay memory contains either a suboptimal trajectory or is missing transitions. The right panel shows the resulting policies, highlighting the differences in accuracy between the two methods.", "section": "Empirical MDP Iteration"}, {"figure_path": "lsd27JUJ8v/figures/figures_28_1.jpg", "caption": "Figure 3: Performance on Atari (first row) and MuJoCo (second row) tasks. The learning curves show mean scores and standard deviations over five runs. EMIT consistently boosts performance across diverse tasks.", "description": "This figure displays the learning curves for both Atari and MuJoCo environments.  Each curve represents the average performance across five runs, with shaded areas indicating standard deviation. The results demonstrate that using EMIT consistently improves the performance compared to the baselines across multiple environments, showing its effectiveness in enhancing reinforcement learning algorithms.", "section": "4 Experiments"}, {"figure_path": "lsd27JUJ8v/figures/figures_28_2.jpg", "caption": "Figure 3: Performance on Atari (first row) and MuJoCo (second row) tasks. The learning curves show mean scores and standard deviations over five runs. EMIT consistently boosts performance across diverse tasks.", "description": "This figure displays the learning curves for both Atari and MuJoCo benchmark tasks.  Each curve represents the mean score across five runs of each algorithm, with shaded areas indicating standard deviations.  The results demonstrate that the EMIT method consistently outperforms the baseline algorithms (DQN and TD3) across a variety of tasks, showcasing its effectiveness in enhancing reinforcement learning performance.", "section": "4 Experiments"}]