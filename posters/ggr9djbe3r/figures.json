[{"figure_path": "gGR9dJbe3r/figures/figures_2_1.jpg", "caption": "Figure 1: Left: Distributed, compositional computation. Dashed lines separate devices with computational and storage resources. The circular nodes represent parameterized functions that are allocated distinct hardware resources and are spatially separated, while the square nodes represent data (yellow) and outputs corresponding to different tasks (green). The vertical axis represents time. This framework of hardware allocation enables flexible modification of the model structure in a task-dependent fashion. Right: Computation of gradient estimators ge at different layers of a model distributed across multiple devices by pipelining. Computing forward features \u03bc\u03b5 and backwards features ve (also known as computing a forward or backward pass) requires a large amount of classical communication (grey) but an exponentially smaller amount of quantum communication (yellow). L is the classical loss function, and Po an operator whose expectation value with respect to a quantum model gives the analogous loss function in the quantum case.", "description": "The figure illustrates a compositional distributed learning framework, where computations are divided among several devices connected via a network. The left panel shows a distributed computation graph, where each device is assigned a specific parameterized function that processes data and passes the result to other devices. The right panel depicts the pipelining method used for gradient calculation, comparing the classical and quantum communication costs. While classical communication is high in this process, the quantum approach significantly reduces the communication overhead.", "section": "2.1 Large-scale learning problems and distributed computation"}, {"figure_path": "gGR9dJbe3r/figures/figures_4_1.jpg", "caption": "Figure 2: Distributed quantum circuit implementing L for L = 2. Both L and its gradients with respect to the parameters of the unitaries can be estimated with total communication that is polylogarithmic in the size of the input data N and the number of trainable parameters per unitary P.", "description": "This figure shows a distributed quantum circuit for calculating the loss function (L) and its gradients with respect to the parameters of the unitaries (A and B).  The circuit is composed of layers, with each layer involving a unitary operation performed by either Alice or Bob. The total communication required for estimating both L and its gradients is only polylogarithmic in the input size (N) and the number of parameters in each unitary (P), which is a significant improvement over classical methods.", "section": "3 Distributed learning with quantum resources"}, {"figure_path": "gGR9dJbe3r/figures/figures_42_1.jpg", "caption": "Figure 1: Left: Distributed, compositional computation. Dashed lines separate devices with computational and storage resources. The circular nodes represent parameterized functions that are allocated distinct hardware resources and are spatially separated, while the square nodes represent data (yellow) and outputs corresponding to different tasks (green). The vertical axis represents time. This framework of hardware allocation enables flexible modification of the model structure in a task-dependent fashion. Right: Computation of gradient estimators ge at different layers of a model distributed across multiple devices by pipelining. Computing forward features \u03bc\u03b5 and backwards features ve (also known as computing a forward or backward pass) requires a large amount of classical communication (grey) but an exponentially smaller amount of quantum communication (yellow). L is the classical loss function, and Po an operator whose expectation value with respect to a quantum model gives the analogous loss function in the quantum case.", "description": "This figure illustrates a compositional approach to distributed computation where separate devices handle different parts of a computation. The left panel shows the data flow in a compositional distributed computation, with nodes representing parameterized functions and data. The right panel depicts how gradient estimators can be computed via pipelining, showcasing the potential for quantum communication advantages.  Quantum communication is shown in yellow and classical communication in grey, highlighting the potential exponential reduction in communication overhead using quantum methods. ", "section": "2.1 Large-scale learning problems and distributed computation"}]