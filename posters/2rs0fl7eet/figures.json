[{"figure_path": "2RS0fL7Eet/figures/figures_6_1.jpg", "caption": "Figure 1: (11) can initially diverge before converging eventually, leading to a worse performance in practical settings compared to Algorithm 2. See Appendix C.2 for the experimental setup.", "description": "The figure shows the comparison between Algorithm 2 (OTSG-IVaR) and the updates in equation (11) (CSO) in terms of the log of the mean squared error of theta (||\u03b8 - \u03b8*||\u00b2) against log of the iterations (t). While Algorithm 2 converges smoothly to a low error, the updates in equation (11) initially diverge before slowly converging, indicating inferior performance. The experimental setup details are provided in Appendix C.2.", "section": "One-sample Two-stage Stochastic Gradient Method for IVaR"}, {"figure_path": "2RS0fL7Eet/figures/figures_8_1.jpg", "caption": "Figure 2: E[||\u03b8 \u2013 \u03b8*||2] of Algorithm 1 under different settings detailed in Section 4.", "description": "This figure displays the results of Algorithm 1 (Two-sample One-stage Stochastic Gradient IVaR) under various settings.  The y-axis shows the expectation of the squared Euclidean norm of the difference between the estimated parameter vector (\u03b8) and the true parameter vector (\u03b8*). This represents the convergence of the algorithm to the true parameters. The x-axis is the iteration number (t). Each subfigure represents a different experimental setup, varying parameters such as the dimension of the independent variable (dx), the dimension of the instrument (dz), the noise level (c), and the model's non-linearity (\u03a6(s)). The shaded areas represent standard deviations from multiple runs of the algorithm under each configuration.  The experiment results show the effectiveness of the algorithm across these different conditions.", "section": "4 Numerical Experiments"}, {"figure_path": "2RS0fL7Eet/figures/figures_9_1.jpg", "caption": "Figure 2: E[||\u03b8 \u2013 \u03b8*||2] of Algorithm 1 under different settings detailed in Section 4.", "description": "The figure shows the performance of the Two-sample One-stage Stochastic Gradient IVaR (TOSG-IVaR) algorithm under various settings.  The y-axis represents the expected squared error between the estimated parameter \u03b8 and the true parameter \u03b8*, and the x-axis represents the number of iterations.  Different lines represent different combinations of the dimensions of the independent variable (dx), the instrumental variable (dz), the noise variance (c), and the nonlinearity of the relationship between the instrumental variable and the independent variable (\u03d5(s)). The shaded area represents the standard deviation of the error across multiple runs. The results demonstrate that the TOSG-IVaR algorithm performs well under diverse conditions.", "section": "4 Numerical Experiments"}, {"figure_path": "2RS0fL7Eet/figures/figures_9_2.jpg", "caption": "Figure 3: Comparison of E[||\u03b8t \u2014 \u03b8*||2] (log-log scale) for Algorithm 2, Eq. 11 and [DVB24].", "description": "This figure compares the performance of three different algorithms for instrumental variable regression (IVaR) in terms of the mean squared error (MSE) between the estimated parameters and the true parameters. The algorithms compared are Algorithm 2 (OTSG-IVaR), Equation 11 (a variant of Algorithm 2), and the algorithm proposed by Della Vecchia and Basu in 2024 ([DVB24]). The plot uses a log-log scale to better visualize the convergence behavior over time. The results show that Algorithm 2 converges faster to the true parameters than the other two algorithms, indicating better performance in the online streaming setting.", "section": "Numerical Experiments"}, {"figure_path": "2RS0fL7Eet/figures/figures_16_1.jpg", "caption": "Figure 1: (11) can initially diverge before converging eventually, leading to a worse performance in practical settings compared to Algorithm 2. See Appendix C.2 for the experimental setup.", "description": "The figure shows a comparison of the convergence performance between Algorithm 2 (OTSG-IVaR) and the method in equation (11) (CSO). Both methods are used for instrumental variable regression. The results indicate that while the algorithm based on equation (11) initially diverges, Algorithm 2 exhibits more stable and faster convergence. This highlights the benefit of Algorithm 2's approach in practical applications.", "section": "One-sample Two-stage Stochastic Gradient Method for IVaR"}, {"figure_path": "2RS0fL7Eet/figures/figures_16_2.jpg", "caption": "Figure 6: Comparison of E[||\u03b8t \u2014 \u03b8*||2] (log-log scale) for Algorithm 2, Eq. 11 and [DVB24].", "description": "This figure compares the performance of three different algorithms for instrumental variable regression (IVaR) in terms of the expected squared error (E[||\u03b8t \u2014 \u03b8*||2]) plotted on a log-log scale.  The algorithms compared are the proposed One-Sample Two-stage Stochastic Gradient IVaR (OTSG-IVaR) method, a variant using updates from Equation (11) in the paper, and a baseline algorithm from Della Vecchia and Basu (2023). The graphs show how the log of the expected error changes with respect to the log of the number of iterations (t). The purpose is to demonstrate the convergence rate of these methods and to highlight the superior performance of the OTSG-IVaR algorithm.", "section": "4 Numerical Experiments"}]