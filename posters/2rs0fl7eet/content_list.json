[{"type": "text", "text": "Stochastic Optimization Algorithms for Instrumental Variable Regression with Streaming Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xuxing Chen\u266f Department of Mathematics University of California, Davis xuxchen@ucdavis.edu ", "page_idx": 0}, {"type": "text", "text": "Abhishek Roy\u266f   \nHal\u0131c\u0131og\u02d8lu Data Science Institute   \nUniversity of California, San Diego a2roy@ucsd.edu ", "page_idx": 0}, {"type": "text", "text": "Yifan Hu ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Krishnakumar Balasubramanian ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "College of Management, EPFL Department of Computer Science, ETH Zurich yifan.hu@epfl.ch ", "page_idx": 0}, {"type": "text", "text": "Department of Statistics University of California, Davis kbala@ucdavis.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We develop and analyze algorithms for instrumental variable regression by viewing the problem as a conditional stochastic optimization problem. In the context of least-squares instrumental variable regression, our algorithms neither require matrix inversions nor mini-batches and provides a fully online approach for performing instrumental variable regression with streaming data. When the true model is linear, we derive rates of convergence in expectation, that are of order ${\\mathcal{O}}(\\log T/T)$ and ${\\mathcal{O}}(1/T^{1-\\iota})$ for any $\\iota>0$ , respectively under the availability of two-sample and one-sample oracles, where $T$ is the number of iterations. Importantly, under the availability of the two-sample oracle, our procedure avoids explicitly modeling and estimating the relationship between the independent and the instrumental variables, demonstrating the benefit of the proposed approach over recent works based on reformulating the problem as minimax optimization problems. Numerical experiments are provided to corroborate the theoretical results. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Instrumental variable analysis is widely used in fields like econometrics, health care [TM17], social science [Bol12], and online advertisement to estimate the causal effect of a random variable, $X$ , on an outcome variable, $Y$ , when an unobservable confounder influences both. By identifying an instrumental variable correlated with the variable $X$ but unrelated to the confounders, researchers can isolate the exogenous variation in $X$ and estimate a causal relationship between $X$ and $Y$ . In the context of regression, Instrumental Variable Regression (IVaR) addresses endogeneity issues when an independent variable is correlated with the error term in the regression model, leveraging an instrument variable $Z$ such that $Y$ is independent of $X|Z$ . In this paper, we focus on the following statistical model: ", "page_idx": 0}, {"type": "equation", "text": "$$\nY=g_{\\theta^{*}}(X)+\\epsilon_{1}\\quad\\mathrm{with}\\quad X=h_{\\gamma^{*}}(Z)+\\epsilon_{2}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $X\\in\\mathbb{R}^{d_{x}}$ and $\\epsilon_{1}$ are correlated and $\\epsilon_{2}$ is a centered unobserved noise (independent of $Z\\in\\mathbb{R}^{d_{z}}$ ), leading to confounding in the model between $X$ and $Y\\in\\mathbb{R}$ . Here $\\epsilon_{1}$ and $\\epsilon_{2}$ are dependent, and $\\theta^{*}$ and $\\gamma^{*}$ are true parameters for the respective function $g$ and $h$ . Our goal is to design efficient algorithms that recovers $\\theta^{*}$ from the data. ", "page_idx": 0}, {"type": "text", "text": "Traditionally, IVaR algorithms are based on two-stage estimation procedures, where we first regress $Z$ and $X$ to obtain an estimator $\\widehat{X}$ , and then regress $\\overline{{\\widehat{X}}}$ and $Y$ , with the essence that $\\widehat{X}$ is independent of $Y$ , and thus eliminating the aforementioned endogeneity of the unknown confounder. A vast literature has devoted to understanding the two-stage approaches [HH05, DFFR11, HLLBT17], with the parametric two-stage least-squares (2SLS) procedure being the most canonical one [AI95]. The main drawback of this approach is that the second-stage regression problem is affected by the estimation error from the regression problem corresponding to first stage. In fact, [AP09] call the first stage regression as \u201cforbidden regression\u201d, due to the concerns in estimating a nuisance parameter. ", "page_idx": 1}, {"type": "text", "text": "Considering the squared loss function, [MMLR20] formulate the IVaR problem as a conditional stochastic optimization problem [HZCH20]: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{g\\in\\mathcal{G}}F(g):=\\mathbb{E}_{Z}\\mathbb{E}_{Y|Z}[(Y-\\mathbb{E}_{X|Z}[g(X)])^{2}].\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "However, [MMLR20] did not solve problem (2) efficiently, and resort to reformulating (2) further as a minimax optimization problem. Indeed, they mention explicitly in their work that \u201cit remains cumbersome to solve (2) directly because of the inner expectation\u201d. Then, they leverage the Fenchel conjugate of the squared loss, leading to a minimax optimization with maximization over a continuous functional space. Following $\\mathrm{[DHP^{+}17]}$ , [MMLR20] propose to use reproducing kernel Hilbert space (RKHS) to handle the maximization over continuous functional space. See also [LS18, BKS19, DLMS20, ${\\mathrm{LCY}}^{+}20$ , $\\mathbf{BKM}^{+}23\\mathbf{\\varepsilon}$ ] for similar minimax approaches. The issue with such an approach is that approximating the dual variable via maximization over continuous functional space inevitably introduces approximation error. Hence, although there is no explicit nuisance parameter estimation step like in the two-stage approach, there is an implicit one, which makes the minimax approach less appealing as an alternate to the two-stage procedures. ", "page_idx": 1}, {"type": "text", "text": "In this work, contrary to the claim made in [MMLR20] that problem (2) is cumbersome to solve, we design and analyze efficient streaming algorithms to directly solve the conditional stochastic optimization problem in (2). Direct application of methods from [HZCH20] for solving (2) is possible, yet their approach utilizes nested sampling, i.e., for each sample of $Z$ , [HZCH20] generate a batch of samples of $X$ from $\\mathbb{P}(X|Z)$ , to reduce the bias in estimating the composition of nonlinear loss function with conditional expectations. Thus their methods are not suitable for the streaming setting that we are interested in. Considering (2), we first parameterize the function class $\\mathcal{G}\\,:=\\,\\{\\bar{g({\\boldsymbol{\\theta}};X)}\\,\\,\\bar{\\,}\\,{\\boldsymbol{\\theta}}\\,\\in\\,\\mathbb{R}^{d_{\\theta}}\\}$ . Now, defining $F(g):=F(\\theta)$ , we observe that the gradient $\\nabla F(\\theta)$ admits the following form ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\nabla F(\\theta)=\\mathbb{E}_{Z}[(\\mathbb{E}_{X|Z}[g(\\theta;X)]-\\mathbb{E}_{Y|Z}[Y])\\nabla_{\\theta}\\mathbb{E}_{X|Z}[g(\\theta;X)]],\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "which implies that one does not need the nested sampling technique to reduce the bias. However, the presence of product of two conditional expectations $\\bar{\\mathbb{E}}_{X|Z}[g(\\theta;\\bar{X^{\\big}})]$ still causes significant challenges in developing stochastic estimators of the above gradient in the streaming setting. In this work, we overcome this challenge and develop two algorithms that are applicable to the streaming data setting avoiding the need for generating batches of samples of $X$ from $\\mathbb{P}(Z|X)$ . ", "page_idx": 1}, {"type": "text", "text": "Contributions. We make the following contributions in this work. ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 Two-sample oracles: Our first algorithm leverages the observation that if we have access to a two-sample oracle that outputs two samples $X$ and $X^{\\prime}$ that are independent conditioned on the instrument $Z$ , we can immediately construct an unbiased stochastic gradient estimator of the gradient in (3). Based on this crucial observation, we propose the Two-Sample One-stage Stochastic Gradient IVaR (TOSG-IVaR) method (Algorithm 1) that avoids explicitly having to estimate or model the relationship between $Z$ and $X$ thereby overcoming the \u201cforbidden regression\u201d problem.. Under standard statistical model assumptions, for the case when $g$ is a linear model, we establish rates of convergence of order ${\\mathcal{O}}(\\log T/T)$ for the proposed method, where $T$ is the overall number of iterations; see Theorem 1. ", "page_idx": 1}, {"type": "text", "text": "\u2022 One-sample oracles: In the case when we do not have the aforementioned two-sample oracle, we estimate the stochastic gradient in (3) by using the streaming data to estimate one of the conditional expectations, and the corresponding prediction to estimate the other, resulting in the One-Sample Two-stage Stochastic Gradient IVaR (OTSG-IVaR) method (Algorithm 2). Assuming further that the $X$ depends linearly on the instrument $Z$ , we establish a rate of convergence of order ${\\mathcal{O}}(1/T^{1-\\iota})$ , for any $\\iota>0$ ; see Theorem 2. ", "page_idx": 1}, {"type": "text", "text": "1.1 Literature Review ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "IVaR analysis. Instrumental variable analysis has a long history, starting from the early works by [Wri28] and [Rei45]. Several works considered the aforementioned two-stage procedure for IVaR; a summary could be found in the work by [AP09]. Nonparametric approaches based on wavelets, splines, reproducing kernels and deep neural networks could be found, for example, in the works by [HLLBT17, SSG19, BKS19, MMLR20, $\\mathrm{MZG^{+}}21$ , ${\\mathrm{XCS}}^{+}21$ , $Z{\\mathrm{GG}}^{+}22$ , PSF24]. Another popular approach for IVaR is via Generalized Method of Moments (GMM); see, for example, [CP12, BKS19, DLMS20] for an overview. Such approaches essentially reformulate the problem as a minimax problem and hence suffer from the aforementioned \u201cforbidden regression\u201d problem. ", "page_idx": 2}, {"type": "text", "text": "Identifiability conditions for IVaR. Several works in the literature have also focused on establishing the identifiability conditions for IVaR in the parametric and the nonparametric setting. Regardless of the procedure used, they are invariably based on certain source conditions motivated by the inverse problems literature (see, for example, [CFR07, CR11, $\\mathrm{BKM}^{+}23]$ ) or the related problem of completeness conditions, which posits that the conditional expectation operator is one-to-one [BF17, $\\mathrm{LCY}^{+}20\\mathrm{J}$ . Semi-parametric identifiability is also considered recently in the work of $[\\mathrm{CPS}^{+}23]$ . Our focus in this work is not focused on the identifiability; for the formulation (2) that we consider, [MMLR20] provide necessary conditions for identifiability that we adopt. ", "page_idx": 2}, {"type": "text", "text": "Stochastic optimization with nested expectations. Recently, much attention in the stochastic optimization literature has focused on optimizing a nested composition of $T$ expectation functions. Sample average approximation algorithms in this context are considered in the works of [EN13] and [HCH20]. Optimal iterative stochastic optimization algorithms for the case of $T\\ =\\ 2$ were by derived by [GRW20]. For the general $T~\\ge~1$ case, [WFL17] provided sub-optimal rates, whereas [BGN22] derived optimal rates; see also [ZX21] and [CSY21] for related works under stronger assumptions, and [Rus21] for similar asymptotic results. While the above works required certain independence assumptions regarding the randomness across the different compositions, [HZCH20, $\\mathrm{H}\\bar{\\mathrm{W}}\\mathrm{X}^{+}24]$ studied the case of $T=2$ where the the randomness are generically dependent. They termed this problem setting as conditional stochastic optimization, which is the framework that the IVaR problem in (2) falls in. Compared to prior works, for e.g., [GRW20] and [BGN22], in order to handle the dependency between the levels, [HZCH20] require mini-batches in each iteration, making their algorithm not immediately applicable to the purely streaming setting. In this work, we show that despite the problem (2) being a conditional stochastic optimization problem, mini-batches are not required due the additional favorable quadratic structure available in IVaR. ", "page_idx": 2}, {"type": "text", "text": "Streaming IVaR. $[\\mathrm{VSH^{+}}16\\$ , DVB24] analyzed streaming versions of 2SLS in the online1 and adversarial settings. Focusing on linear models, $[\\mathrm{VSH}^{+}16]$ provide preliminary asymptotic analysis assuming access to efficient no-regret learners, while [DVB24] provide regret bounds under the strong assumption that the instrument is almost surely bounded. Furthermore, our algorithms have significantly improved per-iteration and memory complexity compared to [DVB24]; see Sections A and B for details. $[\\mathbf{CLL}^{+}23]$ developed stochastic optimization algorithms for the GMM formulation and provide asymptotic analysis. Their algorithm requires access to an offilne dataset for initialization and is hence not fully online. The above works (i) do not focus on avoiding the forbidden regression problem and (ii) do not view IVaR via the conditional stochastic optimization lens, like we do. ", "page_idx": 2}, {"type": "text", "text": "2 Two-sample One-stage Stochastic Gradient Method for IVaR ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Recall that our goal is to solve the objective function given in (2). By [MMLR20, Theorem 4], the optimal solution of (2) gives the true underlying causal relationship under the following assumption. ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.1. (Identifiability Assumption) ", "page_idx": 2}, {"type": "text", "text": "Algorithm 1 Two-sample One-stage Stochastic Gradient-IVaR (TOSG-IVaR) ", "page_idx": 3}, {"type": "text", "text": "Input: $\\sharp$ of iterations $T$ , stepsizes $\\{\\alpha_{t}\\}_{t=1}^{T}$ , initial iterate $\\theta_{1}$ .   \n1: for $t=1$ to $T$ do   \n2: Sample $Z_{t}$ , sample independently $X_{t}$ and $X_{t}^{\\prime}$ from $\\mathbb{P}_{X\\mid Z_{t}}$ , and sample $Y_{t}$ from $\\mathbb{P}_{Y\\mid X_{t}}$ . 3: Update $\\theta_{t}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\theta_{t+1}=\\theta_{t}-\\alpha_{t+1}(g(\\theta_{t};X_{t})-Y_{t})\\nabla_{\\theta}g(\\theta_{t};X_{t}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "4: end for Output: $\\theta_{T}$ . ", "page_idx": 3}, {"type": "text", "text": "\u2022 The function class $\\mathcal{G}\\,:=\\,\\{g(\\theta;X)\\,\\mid\\,\\theta\\,\\in\\,\\mathbb{R}^{d_{\\theta}}\\}$ is correctly specified, i.e., it includes the true underlying relationship between $X$ and $Y$ . ", "page_idx": 3}, {"type": "text", "text": "Notice that both assumptions are standard in the IVaR literature [NP03, CP12, MMLR20], and makes the objective in (2) is the meaningful for IVaR. However, [MMLR20] resort to reformulating the objective function in (2) as a minimax optimization problem as described in Section 1. While their original motivation was to avoid two-state estimation procedure and avoid the \u201cforbidden regression\u201d, their minimax reformulation ends up having to solve a complicated approximation of the original objective resulting in having to characterize the approximation error which is non-trivial. ", "page_idx": 3}, {"type": "text", "text": "Algorithm and Analysis. Our aim in this work is to directly solve the original problem in (2), leveraging the structure provided by the quadratic loss. Given the gradient formulation in (3), a natural way to build unbiased gradient estimator is to generate $X$ and $X^{\\prime}$ , two independent samples of $X$ from the conditional distributions $\\mathbb{P}_{X\\mid Z}$ , for a given realization of $Z$ and generate one sample of $Y$ from the conditional distribution $\\mathbb{P}_{Y\\mid X}$ . Then, an unbiased gradient estimator is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v(\\theta)=(g(\\theta;X)-Y)\\nabla_{\\theta}g(\\theta;X^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This could be plugged into the standard stochasic gradient descent algorithm, which give us the Two-sample Stochastic Gradient Method for IVR (TSG-IVaR) method illustrated in Algorithm 1. In particular, the algorithm never requires estimating (or modeling) the relationship between $X$ and $Z$ as needed in the two-stage procedure [AP09] and the minimax formulation based procedures [MMLR20, LS18, BKS19, DLMS20, ${\\mathrm{LCY}}^{+}20$ , $\\mathbf{BKM}^{+}23]$ . Furthermore, this viewpoint not only provides a novel algorithm for performing IV regression, but also provides a novel data collection mechanism for the practical implementation of IVaR. In addition, such a two-sample gradient method is not very restrictive when the instrumental variable $Z$ takes value in a discrete set. In this case, to implement the two-sample oracle, it is enough simply pick two sets of samples $\\left(X,Y,Z\\right)$ and $(X^{\\prime},Y^{\\bar{\\prime}},Z)$ for which $Z$ has repeated observations (which is possible when $Z$ is a discrete random variable) from a pre-collected dataset. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.2. The tuple $\\left(Z_{t},X_{t},X_{t}^{\\prime},Y_{t}\\right)$ is independent and identically distributed, across $t$ . ", "page_idx": 3}, {"type": "text", "text": "To demonstrate the convergence rate of Algorithm 1, we first consider the case when $g$ is a linear function, i.e., $g(\\theta;X)={X}^{\\ast}\\theta$ . We make the following assumptions. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.3. Suppose there exists $\\mu>0$ such that $\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{X|Z}[X]\\cdot\\mathbb{E}_{X|Z}[X]^{\\top}\\Big]\\succeq\\mu I.$ ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.4. Let $(\\vartheta_{1},\\vartheta_{2},\\vartheta_{3},\\vartheta_{4})\\in\\mathbb{R}_{+}^{4}$ . For any $Z$ , $X^{\\prime}$ and $X$ i.i.d. generated from $\\mathbb{P}_{Z|X}$ , and $Y$ generated from $\\mathbb{P}_{Y\\mid X}$ . There exists constants $C_{x},C_{y},C_{x x},C_{y x}>0$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Big[\\|X^{\\prime}X^{\\top}-\\mathbb{E}_{X|Z}[X]\\mathbb{E}_{X|Z}[X]^{\\top}\\|^{2}\\Big]\\leq C_{x}d_{x}^{\\vartheta_{1}},}\\\\ &{\\mathbb{E}\\Big[\\|Y X^{\\prime}-\\mathbb{E}_{Y|Z}[Y]\\mathbb{E}_{X|Z}[X]\\|^{2}\\Big]\\leq C_{y}d_{x}^{\\vartheta_{2}},}\\\\ &{\\mathbb{E}\\Big[\\|\\mathbb{E}_{X|Z}[X]\\cdot\\mathbb{E}_{X|Z}[X]^{\\top}-\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{X|Z}[X]\\cdot\\mathbb{E}_{X|Z}[X]^{\\top}\\Big]\\|^{2}\\Big]\\leq C_{x x}d_{z}^{\\vartheta_{3}},}\\\\ &{\\mathbb{E}\\Big[\\|\\mathbb{E}_{Y|Z}[Y]\\cdot\\mathbb{E}_{X|Z}[X]-\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{Y|Z}[Y]\\cdot\\mathbb{E}_{X|Z}[X]\\Big]\\|^{2}\\Big]\\leq C_{y x}d_{z}^{\\vartheta_{4}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\|\\cdot\\|$ denotes the Euclidean norm and operator norm for a vector and matrix respectively. ", "page_idx": 3}, {"type": "text", "text": "The above assumptions are mild moment assumptions required on the involved random variables. The following result demonstrates that Assumptions 2.3 and 2.4 are naturally satisfied even under non-linear modeling assumption on (1). We defer its proof to Section D.3. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1. Suppose there exist $\\theta_{\\ast}\\in\\mathbb{R}^{d_{x}}$ , $\\gamma_{\\ast}\\in\\mathbb{R}^{d_{z}\\times d_{x}}$ , a non-linear map $\\phi:\\mathbb{R}^{d_{x}}\\rightarrow\\mathbb{R}^{d_{x}}$ , and $a$ positive semi-definite matrix $\\Sigma\\in\\mathbb{R}^{d_{z}\\times d_{z}}$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{Z}\\Big[\\phi(\\gamma_{*}^{\\top}Z)\\cdot\\phi(\\gamma_{*}^{\\top}Z)^{\\top}\\Big]\\succeq\\mu I,\\ \\mathbb{E}[\\|\\phi(\\gamma_{*}^{\\top}Z)\\|^{2}]=\\mathcal{O}(d_{x}),}\\\\ &{Z\\sim\\mathcal{N}(0,\\Sigma),\\ X=\\phi(\\gamma_{*}^{\\top}Z)+\\epsilon_{2},\\ Y=\\theta_{*}^{\\top}X+\\epsilon_{1},\\ \\epsilon_{2}\\sim\\mathcal{N}(0,\\sigma_{\\epsilon_{2}}^{2}I_{d_{x}}),\\ \\epsilon_{1}\\sim\\mathcal{N}(0,\\sigma_{\\epsilon_{1}}^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\epsilon_{1},\\epsilon_{2}$ are independent of $Z$ and ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\epsilon_{1}^{2}\\|\\epsilon_{2}\\|^{2}\\right]\\leq\\sigma_{\\epsilon_{1},\\epsilon_{2}}^{2}d_{x},\\,\\mathbb{E}\\left[\\|\\phi(\\gamma_{*}^{\\top}Z)\\cdot\\phi(\\gamma_{*}^{\\top}Z)^{\\top}-\\mathbb{E}[\\phi(\\gamma_{*}^{\\top}Z)\\cdot\\phi(\\gamma_{*}^{\\top}Z)^{\\top}]\\|^{2}\\right]\\leq C d_{z}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then Assumptions 2.3 and 2.4 hold with $\\vartheta_{1}=\\vartheta_{2}=2$ and $\\vartheta_{3}=\\vartheta_{4}=1.$ . If $\\phi$ is an identity map, then the conditions involving $\\phi$ become $\\gamma_{*}^{\\top}\\Sigma\\gamma_{*}\\succeq\\mu I$ , $\\mathrm{tr}(\\gamma_{*}^{\\top}\\Sigma\\gamma_{*})=\\mathcal{O}(d_{x})$ , $\\mathbb{E}\\left[\\|Z Z^{\\top}-\\Sigma\\|^{2}\\right]\\leq C d_{z}$ . ", "page_idx": 4}, {"type": "text", "text": "The above assumption is standard in the stochastic approximation, statistics and econometrics literature. It could be further relaxed to Markovian-type dependency assumptions, following techniques in the works of [DAJJ12, SSY18, Eve23, RBG22]; we leave a detailed examination of the Markovian streaming setup as future work. Under the above assumptions, we have the following result demonstrating the last-iterate global convergence of Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Suppose Assumptions 2.3, 2.4, and 2.2 hold. In Algorithm $^{\\,l}$ , defining $\\sigma_{1}^{2}:=2C_{x}d_{x}^{\\vartheta_{1}}+$ $2C_{x x}d_{z}^{\\vartheta_{3}}$ and $\\sigma_{2}^{2}:=C_{y}d_{x}^{\\vartheta_{2}}+C_{y x}d_{z}^{\\vartheta_{4}}$ , set $\\begin{array}{r}{\\alpha_{t}\\equiv\\alpha=\\frac{\\log T}{\\mu T}\\leq\\frac{\\mu}{\\mu^{2}+3\\sigma_{1}^{2}}}\\end{array}$ lo\u00b5gT T \u2264\u00b52+\u00b53\u03c32 . Then, we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\|\\theta_{T}-\\theta_{*}\\|^{2}\\big]\\leq\\frac{\\mathbb{E}\\big[\\|\\theta_{0}-\\theta_{*}\\|^{2}\\big]}{T}+\\frac{3\\|\\theta_{*}\\|^{2}(\\sigma_{1}^{2}+\\sigma_{2}^{2})\\log T}{\\mu^{2}T}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Proof techniques. In the analysis of Theorem 1, the following decomposition (see (18) for the derivation) plays a crucial role: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{t+1}-\\theta_{*}=A_{t}+\\alpha_{t+1}B_{t},}\\\\ &{A_{t}=\\theta_{t}-\\alpha_{t+1}\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{X|Z}[X]\\cdot\\mathbb{E}_{X|Z}[X]^{\\top}\\Big]\\theta_{t}+\\alpha_{t+1}\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{Y|Z}[Y]\\cdot\\mathbb{E}_{X|Z}[X]\\Big]-\\theta_{*},}\\\\ &{B_{t}=-\\Big(X_{t}^{\\prime}X_{t}^{\\top}-\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{X|Z}[X]\\cdot\\mathbb{E}_{X|Z}[X]^{\\top}\\Big]\\Big)\\theta_{t}+\\Big(Y_{t}X_{t}^{\\prime}-\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{Y|Z}[Y]\\cdot\\mathbb{E}_{X|Z}[X]\\Big]\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $A_{t}$ corresponds to deterministic component, and $B_{t}$ corresponds to the stochastic component arising due to the use of stochastic gradients. Standard assumptions on the variance of the stochastic gradient made in the stochastic optimization literature include the uniformly bounded variance assumption [Lan20] and the expected smoothness condition [KR20]. In the IVaR setup, such standard assumptions do not hold as $\\theta_{t}$ potentially can be unbounded and thus the gradient estimator can be unbounded. Hence, we establish our results under natural statistical assumptions arising in the context of the IVaR problem, which form the main novelty in our analysis. Furthermore, compared to [MMLR20], notice that we use two samples of $X$ from the conditional distribution $\\mathbb{P}_{X\\mid Z}$ and achieve an $\\widetilde{\\mathcal{O}}(1/T)$ last iterate convergence rate to the global optimal solution, which is the true underlying  causal relationship under Assumption 2.1. In comparison, [MMLR20] only provide asymptotic convergence result to the optimal solution of an approximation problem. ", "page_idx": 4}, {"type": "text", "text": "Additional discussion. It is interesting to explore other losses beyond squared loss (for example to handle classification setting [CF21]), potentially using the Multilevel Monte Carlo (MLMC) based stochastic gradient estimators. While [HCH21], develops such algorithms, the main challenge is about how to avoid mini-batches required in their work leveraging the problem structure in instrumental variable analysis. Furthermore, in the case when $g(\\theta;\\bar{X})$ is parametrized by a non-linear models, for instance, a neural network, we provide local convergence guarantees under additional stronger conditions made typically in the stochastic optimization literature. ", "page_idx": 4}, {"type": "text", "text": "Assumption 2.5. Let the following assumptions hold: ", "page_idx": 4}, {"type": "text", "text": "\u2022 Function $F(\\theta)$ is $\\ell$ -smooth. ", "page_idx": 4}, {"type": "text", "text": "\u2022 The iterates $\\{\\theta_{t}\\}_{t=1}^{T+1}$ generated by Algorithm 1 are in a compact set $A$ . ", "page_idx": 5}, {"type": "text", "text": "\u2022 The random objects $X|Z$ and $Y|Z$ have bounded variance for any $Z$ , i.e., there exist $\\sigma>0$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\|X-\\mathbb{E}\\left[X\\mid Z\\right]\\|^{2}\\mid Z\\right]\\leq\\sigma^{2},\\;\\mathbb{E}\\left[\\|Y-\\mathbb{E}\\left[Y\\mid Z\\right]\\|^{2}\\mid Z\\right]\\leq\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proposition 1. Suppose Assumptions 2.1, 2.2, and 2.5 hold. Choosing $\\begin{array}{r}{\\alpha_{t}\\equiv\\alpha=\\mathcal{O}\\left(\\frac{1}{\\sqrt{T}}\\right),}\\end{array}$ , for Algorithm $^{\\,l}$ we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{1\\leq t\\leq T}\\mathbb{E}\\left[\\|\\nabla F(\\theta_{t})\\|^{2}\\right]=\\mathcal{O}\\left(\\frac{1}{\\sqrt{T}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proof of the proposition is immediate. Note that under Assumption 2.5, we can deduce that the unbiased gradient estimator $\\begin{array}{r}{v(\\theta)=(g(\\theta;X)-Y)\\nabla_{\\theta}g(\\theta;X^{\\prime})}\\end{array}$ has a bounded variance since ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lceil\\mathrm{ar}(v(\\theta))=\\mathrm{Var}(g(\\theta;X)-Y)\\mathrm{Var}(\\nabla_{\\theta}g(\\theta;X^{\\prime}))}\\\\ &{\\qquad\\qquad+\\,\\mathrm{Var}(g(\\theta;X)-Y)\\left(\\mathbb{E}\\left[\\nabla_{\\theta}g(\\theta;X^{\\prime})\\right]\\right)^{2}+\\mathrm{Var}(\\nabla_{\\theta}g(\\theta;X^{\\prime}))\\left(\\mathbb{E}\\left[g(\\theta;X)-Y\\right]\\right)^{2}\\leq\\sigma_{v}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the variance and expectation are taken conditioning on $Z$ and $\\theta$ , and $\\sigma_{v}>0$ is a constant that only depends on $\\sigma$ , function $g$ and the compact set $A$ in Assumption 2.5. Then one can directly follow the analysis of non-convex stochastic optimization (see, for example, [GL13, Theorem 2.1]) to obtain Proposition 1. Relaxing the Assumption 2.5 (typically made in the stochastic optimization literature) with more natural assumptions on the statistical model and obtaining a result as in Theorem 1 for the non-convex setting is left as future work. ", "page_idx": 5}, {"type": "text", "text": "3 One-sample Two-stage Stochastic Gradient Method for IVaR ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now examine designing streaming IVaR algorithm with access to the classical one-sample oracle, i.e., we observe a streaming set of samples $\\left(X_{t},Y_{t},Z_{t}\\right)$ at each time point $t$ . Note that in this case, using the same $X_{t}$ (instead of $X_{t.}^{\\prime}$ ) in (4) makes the stochastic gradient estimator biased. ", "page_idx": 5}, {"type": "text", "text": "Intuition. Consider the case of linear models, i.e., $Y=\\theta_{*}^{\\top}X+\\epsilon_{1}$ with $X=\\gamma_{*}^{\\top}Z+\\epsilon_{2}$ , where $\\boldsymbol{\\theta}_{\\ast}\\in\\mathbb{R}^{d_{x}\\times1}$ , and $\\gamma_{\\ast}\\in\\mathbb{R}^{d_{z}\\times d_{x}}$ , as also considered in Lemma 1. Recall the true gradient in (3) and the stochastic gradient estimator of Algorithm 1 in (4). Since we no longer have $X_{t}^{\\prime}$ , we replace the term $X_{t}^{\\prime}$ with the predicted mean of $X_{t}$ given $Z_{t}$ . Suppose that $\\gamma_{*}$ is known. We specifically replace $\\nabla_{\\theta_{t}}g(\\theta_{t};X_{t}^{\\prime})\\,=\\,X_{t}^{\\prime}$ by $\\mathbb{E}_{|Z_{t}}\\left[X_{t}\\right]=\\gamma_{*}^{\\top}Z_{t}$ . In such a case, indeed we have an unbiased gradient estimator: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{t}\\left[\\gamma_{*}^{\\top}Z_{t}(X_{t}^{\\top}\\theta_{t}-Y_{t})\\right]=\\mathbb{E}_{t}\\left[\\mathbb{E}_{|Z_{t}}\\left[X_{t}\\right](\\mathbb{E}_{|Z_{t}}\\left[X_{t}\\right]^{\\top}\\theta_{t}-\\mathbb{E}_{|Z_{t}}\\left[Y_{t}\\right])\\right]}\\\\ &{=\\!\\mathbb{E}_{t}\\left[\\gamma_{*}^{\\top}Z_{t}Z_{t}^{\\top}\\gamma_{*}(\\theta_{t}-\\theta_{*})\\right]=\\gamma_{*}^{\\top}\\Sigma_{Z}\\gamma_{*}(\\theta_{t}-\\theta_{*})=\\nabla_{\\theta}F(\\theta_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbb{E}_{t}\\left[\\cdot\\right]$ is the conditional expectation w.r.t the filtration defined on $\\{\\gamma_{1},\\theta_{1},\\gamma_{2},\\theta_{2},\\cdot\\cdot\\cdot,\\gamma_{t},\\theta_{t}\\}$ ", "page_idx": 5}, {"type": "text", "text": "In reality, $\\gamma_{*}$ is unknown beforehand. Hence, we estimate $\\gamma_{*}$ using some online procedure and replace $\\nabla_{\\theta_{t}}g(\\theta_{t};X_{t}^{\\prime})$ by $\\gamma_{t}^{\\top}Z_{t}$ instead of $\\gamma_{*}^{\\top}Z_{t}$ . It leads to the following updates: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\theta_{t+1}=\\theta_{t}-\\alpha_{t+1}\\gamma_{t}^{\\top}Z_{t}\\big(X_{t}^{\\top}\\theta_{t}-Y_{t}\\big),\\qquad\\gamma_{t+1}=\\gamma_{t}-\\beta_{t+1}Z_{t}\\big(Z_{t}^{\\top}\\gamma_{t}-X_{t}^{\\top}\\big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "A closer inspection reveals that the updates in (11) can diverge until $\\gamma_{t}$ is close enough to $\\gamma_{*}$ . It is easy to see this fact from the following expansion of $\\theta_{t+1}-\\theta_{*}$ . We have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{t+1}-\\theta_{*}=\\widehat{Q}_{t}(\\theta_{t}-\\theta_{*})+\\alpha_{t+1}\\bigl(\\gamma_{t}-\\gamma_{*}\\bigr)^{\\top}\\Sigma_{Z Y}+\\alpha_{t+1}D_{t}\\theta_{*}+\\alpha_{t+1}\\gamma_{t}^{\\top}\\xi_{Z_{t}}\\gamma_{*}\\bigl(\\theta_{t}-\\theta_{*}\\bigr)}\\\\ &{\\qquad\\qquad\\qquad+\\alpha_{t+1}\\gamma_{t}\\,\\mathrm{T}\\xi_{Z_{t}}\\gamma_{*}\\theta_{*}+\\alpha_{t+1}\\gamma_{t}\\,\\mathrm{T}^{\\top}\\xi_{Z_{t}Y_{t}}-\\alpha_{t+1}\\gamma_{t}\\,\\mathrm{T}^{\\top}Z_{t}\\epsilon_{2,t}\\,\\mathrm{T}\\theta_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\xi_{Z_{t}}=\\Sigma_{Z}-Z_{t}Z_{t}^{\\top},\\quad\\xi_{Z_{t}Y_{t}}=\\Sigma_{Z Y}-Z_{t}Y_{t},\\quad\\widehat{Q}_{t}:=\\left(I-\\alpha_{t+1}\\gamma_{t}\\top_{Z}\\gamma_{*}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "However, the matrix $\\gamma_{t}^{\\top}\\Sigma_{Z}\\gamma_{*}$ may not be positive semi-definite, even if $\\Sigma_{Z}$ is positive definite. Thus the negative eigenvalues associated with $\\gamma_{t}^{+}\\Sigma_{Z}\\gamma_{*}$ might cause the $\\theta_{t}$ iterates to first diverge, before ", "page_idx": 5}, {"type": "image", "img_path": "2RS0fL7Eet/tmp/879ade6b53f17ef9be1dde2192f2a75a13a9d7454397299d567e8dc8ed350b53.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 1: (11) can initially diverge before converging eventually, leading to a worse performance in practical settings compared to Algorithm 2. See Appendix C.2 for the experimental setup. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 2 One-Sample Two-stage Stochastic Gradient-IVarR (OTSG-IVaR) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Input: Stepsizes $\\left\\{\\alpha_{t}\\right\\}_{t},\\left\\{\\beta_{t}\\right\\}_{t}$ , initial iterates $\\gamma_{1},\\theta_{1}$ .   \n1: for $t=1$ , 2, \u00b7 \u00b7 \u00b7 do   \n2: Sample $Z_{t}$ , sample $X_{t}$ from $\\mathbb{P}_{X\\mid Z_{t}}$ , Sample $Y_{t}$ from $\\mathbb{P}_{Y\\mid X_{t}}$ 3: Update ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{t+1}=\\theta_{t}-\\alpha_{t+1}\\gamma_{t}^{\\top}Z_{t}(Z_{t}^{\\top}\\gamma_{t}\\theta_{t}-Y_{t}),}\\\\ &{\\gamma_{t+1}=\\gamma_{t}-\\beta_{t+1}Z_{t}(Z_{t}^{\\top}\\gamma_{t}-X_{t}^{\\top}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4: end for ", "page_idx": 6}, {"type": "text", "text": "eventually converging as $\\gamma_{t}$ gets closer to $\\gamma_{*}$ . We illustrate this intuition in a simple experiment in Figure 1. To resolve this issue, we propose Algorithm 2, where we replace $g(\\theta_{t},\\dot{X}_{t})=\\dot{{X}}_{t}^{\\top}\\theta_{t}$ with $Z_{t}^{\\breve{T}}\\gamma_{t}\\theta_{t}$ in (11). With such a modification, in the corresponding decomposition for $\\theta_{t+1}-\\theta_{*}$ (see (40)), we have $\\widehat{Q}_{t}=\\left(I-\\alpha_{t+1}\\gamma_{t}\\right)^{\\top}\\Sigma_{Z}\\gamma_{t}\\Big)$ , where the matrix product $\\gamma_{t}^{\\top}\\Sigma_{Z}\\gamma_{t}$ is always positive semi-definite. Hence, with a properly chosen stepsize $\\alpha_{t}$ we could quantify the convergence of $\\theta_{t}$ to $\\theta_{*}$ non-asymptotically. Nevertheless, assuming a warm-start condition on $\\theta_{0}$ , we also show the convergence of (11), in Appendix E.3 for completeness. ", "page_idx": 6}, {"type": "text", "text": "Algorithm and Analysis. Based on the intuition, we present Algorithm 2. One could interpret the algorithm as the SGD analogy of the offilne 2SLS algorithm [AI95]. It is also related to the framework of non-linear two-stage stochastic approximation algorithms [DR20, DTSM18, MP06]; albeit the updates of $\\theta_{t}$ and $\\gamma_{t}$ are coupled since both updates use $Z_{t}$ . Furthermore, the dependency between the randomness between the two stages in the IVaR problem, makes the analysis significantly different and more challenging from the classical analysis of two-stage algorithms (see below Theorem 2 for additional details). Finally, while Algorithm 2 is designed for linear models, the intuition behind the method is also applicable to non-linear models (i.e., between $Z$ and $X$ , and $X$ and $Y$ ). We focus on linear models in this work in order to derive our theoretical results. A detailed treatment of the nonlinear case (for which the analysis is significantly nontrivial) is left for future work. We make the following additional assumptions for the convergence analysis of Algorithm 2. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3.1. For some constants $C_{z},C_{z y}\\,>\\,0$ , we have the following bounds on the fourth moments: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert\\Sigma_{Z}-Z Z^{\\top}\\Vert^{4}\\right]\\leq C_{z}d_{z}^{\\vartheta_{5}},\\quad\\mathbb{E}\\left[\\Vert\\Sigma_{Z Y}-Z Y\\Vert^{4}\\right]\\leq C_{z y}d_{z}^{\\vartheta_{6}},\\quad\\vartheta:=\\operatorname*{max}\\{\\vartheta_{5},\\vartheta_{6}\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Assumption 3.2. There exist constants $0<\\mu z\\le\\lambda z<\\infty$ such that $\\mu_{Z}I_{d_{z}}\\preceq\\Sigma_{Z}\\preceq\\lambda_{Z}I_{d_{z}}.$ . ", "page_idx": 6}, {"type": "text", "text": "The above conditions are rather mild moment conditions, similar to Assumption 2.4, and could be easily verified for the linear model setting we consider. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3.3. $\\{\\gamma_{t}\\}_{t}$ is within a compact set of diameter $C_{\\gamma}d_{z}^{\\varkappa}$ for some constants $C_{\\gamma}>0,\\,\\varkappa\\geq0$ . ", "page_idx": 6}, {"type": "text", "text": "We emphasize that Assumption 3.3 is only for the uncoupled sequence $\\gamma_{t}$ , which is an SGD sequence for solving a strongly-convex problem. It holds easily in various cases, for example by projecting the iterates onto any compact sets or a sufficiently large ball containing $\\gamma^{*}$ . It is also well-known that, without any projection operations, $\\{\\gamma_{t}\\}_{t}$ sequence is almost surely bounded [PJ92] under our assumptions. Finally, similar assumptions routinely appear in the analysis of SGD algorithms in various related settings; see, for example, [Tse98, GOP19, HS19, NJN19, AYS20, RGP20]. ", "page_idx": 7}, {"type": "text", "text": "We now present our result on the convergence of $\\{\\theta_{t}\\}_{t}$ below in Theorem 2 (see Appendix E.1 for the proof). In comparison to Theorem 1 (regarding Algorithm 1), we highlight that Theorem 2 provides an any-time guarantee, as the total number of iterations is not required in advance by Algorithm 2. ", "page_idx": 7}, {"type": "text", "text": "Theorem 2. Suppose Assumptions 2.3, 2.2 (without $X_{t\\,.}^{\\prime}$ ), 3.1, 3.3, and 3.2 hold. In Algorithm 2, for any $\\ensuremath{\\mathrm{~\\boldmath~\\lambda~}}_{L}>\\;\\;0$ , set $\\alpha_{t}~~=~~C_{\\alpha}t^{-1+\\iota/2}$ and $\\beta_{t}~~=~C_{\\beta}t^{-1+\\iota/2}$ , where $\\begin{array}{r l}{C_{\\alpha}}&{{}=}\\end{array}$ $\\operatorname*{min}\\{0.5d_{z}^{-4\\varkappa-\\vartheta/2}\\lambda_{Z}^{-1}C_{\\gamma}^{-2},0.5(\\|\\gamma_{*}\\|\\lambda_{Z})^{-2}\\},$ , and $C_{\\beta}=\\mu^{2}d_{z}^{-1-2\\varkappa}/128$ . Then, we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\|\\theta_{t}-\\theta^{*}\\|^{2}\\right]=O\\left(\\frac{1}{t^{1-\\iota}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark 1. In Theorem 2, we present the step-size choices for the fastest rate of convergence. In the proof of Theorem 2 (see Appendix $E.I$ ), we show that convergence can be guaranteed for $a$ range of step-sizes given by $\\alpha_{t}\\:=\\:C_{\\alpha}t^{-a}$ , $\\beta_{t}\\,=\\,C_{\\beta}t^{-b}$ , where $1/2\\,<\\,a,b\\,<\\,1$ , $b\\:>\\:2\\:-\\:2a$ with corresponding rate being E $\\left[\\Vert\\theta_{t}-\\theta^{*}\\Vert^{2}\\right]=O(\\operatorname*{max}\\{t^{-b(2-(1-\\iota/2)^{-1})},t^{-a}\\log(2/\\iota-1)\\})$ . $I n$ particular, one requires $a,b<1$ to ensure $(\\bar{\\alpha_{t}}-\\alpha_{t+1})/\\alpha_{t}=o(\\alpha_{t}),$ , and $(\\beta_{t}-\\beta_{t+1})/\\beta_{t}=o(\\beta_{t})$ , as is standard in stochastic approximation literature (see, for example, $I C L T Z20,\\,P J92J)$ . ", "page_idx": 7}, {"type": "text", "text": "Proof Techniques. The major challenge towards the convergence analysis of $\\{\\theta_{t}\\}_{t}$ lies in the interaction term $\\gamma_{t}Z_{t}Z_{t}^{\\top}\\gamma_{t}\\theta_{t}$ between $\\gamma_{t}$ and $\\theta_{t}$ in (13). This multiplicative interaction term leads to an involved dependence between the noise in the stochastic gradient updates for the two stages. Such a dependence has not been considered in existing analysis of non-linear two time-scale algorithms [MP06, $\\mathbf{MSB}^{+}09$ , DTSM18, DR20, XL21, WZZ21, Doa22]. In addition, [Doa22] considers the case when the noise sequence is not only independent of each other but also independent of iterate locations. Furthermore, they assumes (see their Assumption 3) that the condition in Assumption 2.3 holds for all $\\gamma$ whereas Assumption 2.3 only needs to hold for $\\gamma_{*}$ , that is much milder. Similarly, many works (for example, Assumption 1 in [WZZ21], Assumption 2 in [XL21] and Theorem 2 in $[\\mathbf{M}\\bar{\\mathbf{S}}\\bar{\\mathbf{B}}^{+}09]_{,}^{,}$ ) assume that the iterates of both stages are bounded in a compact set and consequently, and hence the variance of the stochastic gradients are also uniformly bounded. ", "page_idx": 7}, {"type": "text", "text": "In our setting, firstly, the stochastic gradient in (13), evaluated at $(\\theta_{t},\\gamma_{t})$ is biased: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t,Z_{t}}\\left[\\gamma_{t}^{\\top}Z_{t}(Z_{t}^{\\top}\\gamma_{t}\\theta_{t}-Y_{t})\\right]=\\mathbb{E}_{t,Z_{t}}\\left[\\gamma_{t}^{\\top}Z_{t}(Z_{t}^{\\top}\\gamma_{t}\\theta_{t}-Z_{t}^{\\top}\\gamma_{*}\\theta_{*})\\right]=\\mathbb{E}_{t}\\left[\\gamma_{t}^{\\top}\\Sigma_{Z}(\\gamma_{t}\\theta_{t}-\\gamma_{*}\\theta_{*})\\right]}\\\\ &{=\\gamma_{t}^{\\top}\\Sigma_{Z}\\gamma_{t}(\\theta_{t}-\\theta_{*})+\\gamma_{t}^{\\top}\\Sigma_{Z}(\\gamma_{t}-\\gamma_{*})\\theta_{*}\\neq\\gamma_{*}^{\\top}\\Sigma_{Z}\\gamma_{*}(\\theta_{t}-\\theta_{*})=\\nabla_{\\theta}F(\\theta_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Furthermore, even under Assumption 3.3, the variance of the stochastic gradient is not (13) uniformly bounded. Overcoming these issues, in addition to the aforementioned dependence between the noise in the stochastic gradient updates for the two stages, forms the major novelty in our analysis. We proceed by noting that if $\\gamma_{*}$ , $\\Sigma_{Z}$ , and $\\Sigma_{Z Y}$ were known beforehand, one conduct deterministic gradient updates, i.e., $\\widetilde{\\boldsymbol{\\theta}}_{t+1}=\\widetilde{\\boldsymbol{\\theta}}_{t}-\\alpha_{t+1}\\gamma_{*}^{\\top}\\left(\\boldsymbol{\\Sigma}_{Z}\\gamma_{*}\\widetilde{\\boldsymbol{\\theta}}_{t}-\\boldsymbol{\\Sigma}_{Z Y}\\right)$ , to obtain $\\theta_{*}$ . By standard results on gradient descent for strongly convex functions (see, for example, [Nes13]), $\\{\\widetilde{\\theta}_{t}\\}_{t}$ converges exponentially fast as stated in Lemma 4. Hence, it remains to show that the trajectory of $\\theta_{t}$ converges to the trajectory of\u03b8 t. That is, defining the sequence $\\delta_{t}:=\\theta_{t}-\\widetilde{\\theta}_{t}$ , our goal is to establish the convergence rate of $\\mathbb{E}\\left[\\Vert\\delta_{t}\\Vert_{2}^{2}\\right]$ . We first provide an intermediate bound (see Lemma 6) and then progressively sharpen to a tighter bound (see Lemma 7). In doing so, it is also required to show that $\\mathbb{E}\\left[\\Vert\\theta_{t}\\Vert^{4}\\right]$ is bounded, which we prove in Lemma 5. The proof of Lemma 5 is non-trivial and requires carefully chosen stepsizes satisfying $\\textstyle\\sum_{t=1}^{\\infty}(\\alpha_{t}^{2}+\\alpha_{t}\\sqrt{\\beta_{t}})<\\infty$ . ", "page_idx": 7}, {"type": "text", "text": "4 Numerical Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Experiments for Algorithm 1 (TOSG-IVaR). We first consider the following problem, in which $(Z,X,Y)$ is generated via ", "page_idx": 7}, {"type": "equation", "text": "$$\nZ\\sim\\mathcal{N}(0,I_{d_{z}}),\\ X=\\phi(\\gamma_{*}^{\\top}Z)+c\\cdot(h+\\epsilon_{x}),\\ Y=\\theta_{*}^{\\top}X+c\\cdot(h_{1}+\\epsilon_{y}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "image", "img_path": "2RS0fL7Eet/tmp/18fdac0c5aae9c56bed80afcd410b7615a78c1909c7dca9612eb1521c948b4f6.jpg", "img_caption": ["Figure 2: $\\mathbb{E}[\\|\\theta_{t}-\\theta_{*}\\|^{2}]$ of Algorithm 1 under different settings detailed in Section 4. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "where $c>0$ is a scalar to control the variance of the noise vector, and $h_{1}$ is the first coordinate of $h$ . The noise vectors (or scalar) $h,\\epsilon_{x},\\epsilon_{y}$ are independent of $Z$ , and we have $h\\sim\\mathcal{N}(\\mathbf{1}_{d_{x}},I_{d_{x}})$ , $\\epsilon_{x}\\sim\\mathcal{N}(0,I_{d_{x}}),\\epsilon_{y}\\sim\\mathcal{N}(0,1)$ . In each iteration, one tuple $\\left(X,X^{\\prime},Y\\right)$ is generated and used to update $\\theta_{t}$ according to Algorithm 1. We set $(d_{x},d_{z})\\in\\{(4,8),(8,16)\\}$ , $c\\in\\{0.1,1.0\\}$ , and $\\phi(s)\\in\\{s,s^{2}\\}$ . We repeat each setting 50 times and report the curves of $\\mathbb{E}[\\lVert{\\boldsymbol{\\theta}}_{t}-\\dot{{\\boldsymbol{\\theta}}}_{*}\\rVert^{2}]$ in Figure 2, where the expectation is computed as the average of $\\lVert\\boldsymbol{\\theta}_{t}-\\boldsymbol{\\theta}_{*}\\rVert^{2}$ of all trials, and the shaded region represents the standard deviation. The first row and the second row correspond to $\\phi(s)\\,=\\,s$ and $\\bar{\\phi(s)}=s^{2}$ respectively. Here, $c=0.1$ for odd columns and $c=1.0$ for even columns. We have $(d_{x},d_{z})=(4,8)$ for the first two columns and $(d_{x},d_{z})=(8,16)$ for the last two columns. Empirically, we can observe that our Algorithm 1 performs well across all different settings. ", "page_idx": 8}, {"type": "text", "text": "Experiments for Algorithm 2 (OTSG-IVaR). Next, we compare our Algorithm 2 as well as its variant and Algorithm 1 in [DVB24]. We write \u201cOTSG-IVaR\u201d, \u201cCSO \u2013 Eq. (11)\u201d and \u201c[DVB23]\u201d to represent Algorithm 2, Algorithm 2 with the updates replaced by (11) and Algorithm 1 in [DVB24] (see Appendix A). We follow simulation settings similar to [DVB24]: ", "page_idx": 8}, {"type": "equation", "text": "$$\nY=\\theta_{*}^{\\top}X+\\nu,\\qquad X=\\gamma_{*}^{\\top}Z+\\epsilon,\\qquad\\epsilon=\\sigma_{\\epsilon}\\mathcal{N}(0,I_{d_{x}}),\\qquad\\nu=\\rho\\epsilon_{1}+\\mathcal{N}(0,0.25),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\epsilon_{1}$ is the first coordinate of $\\epsilon$ , $\\theta_{\\ast}\\,\\in\\,\\mathbb{R}^{d_{x}}$ is a unit vector chosen uniformly randomly, and $\\gamma_{\\ast}\\in\\mathbb{R}^{d_{z}\\times d_{x}}$ where $\\gamma_{i j}=0$ for $i\\neq j$ , and $\\gamma_{i j}=1$ for $i=j$ , $i=1,2,\\cdots\\,,d_{x}$ , and $j=1,2,\\cdot\\cdot\\cdot,d_{z}$ . Here $\\rho$ controls the level of endogeneity in the model. We compare the performance of Algorithm 2 with (11), and O2SLS [DVB24] for $\\rho\\,=\\,1,4$ , and $\\sigma_{\\epsilon}~=~0.5,1$ . By varying $\\sigma_{\\epsilon}$ we control the correlation between $X$ and $Z$ . We consider two settings $(d_{x},d_{z})=(1,1)$ , and $(d_{x},d_{z})=(8,16)$ . As performance metric, in Figure 3 we plot E $\\left[\\lVert\\boldsymbol{\\theta}_{t}-\\boldsymbol{\\theta}_{*}\\rVert^{2}\\right]$ where the $\\mathbb{E}\\left[\\cdot\\right]$ is approximated by averaging over 50 trials, and both axes are in log scale (base 10). We also show, in Figure 4, the convergence of the test Mean Squared Error (MSE) evaluated over 400 test samples to the best possible test MSE where $\\theta_{*}$ and $\\gamma_{*}$ are known beforehand. For Figures 3 and 4, the first row and second row corresponds to $(d_{x},d_{z})=(1,1)$ and $(d_{x},d_{z})=(8,16)$ respectively, and $\\sigma_{\\epsilon}=0.5$ in odd columns and $\\sigma_{\\epsilon}=1.0$ in even columns. We have $\\rho=1.0$ for the first two columns and $\\rho=4.0$ for the last two columns. We can observe that O2SLS has much larger variance in different settings, while our algorithms perform consistently well in all settings. We further conduct experiments on real-world datasets provided in [AE96] and [Rya12]. Due to space limit, we include the numerical results in Section C.3 of the Appendix. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We presented streaming algorithms for least-squares IVaR based on directly solving the associated conditional stochastic optimization formulation in (2). Our algorithms have several beneftis, including avoidance of mini-batches and matrix inverses. We show that the expected rates of convergences for the proposed algorithms are of order ${\\mathcal{O}}(\\log T/T)$ and ${\\mathcal{O}}(1/T^{1-\\stackrel{\\cdot}{\\iota}})$ , for any $\\iota>0$ , under the availability of two-sample and one-sample oracles, respectively. As future work, it is interesting to develop streaming inferential methods for IVaR. Leveraging related works for the vanilla SGD [PJ92, ABE19, SZ22, CLTZ20, ZCW23] to the setting of Algorithms 1 and 2, provides a concrete direction to establish Central Limit Theorems and develop limiting covariance estimation procedures. ", "page_idx": 8}, {"type": "image", "img_path": "2RS0fL7Eet/tmp/a2ef3b225912073473a4b875cfbc3bc51c5a90bbf8105c73a3fc195b2c40c61c.jpg", "img_caption": ["Figure 3: Comparison of $\\mathbb{E}[\\|\\theta_{t}-\\theta_{*}\\|^{2}]$ (log-log scale) for Algorithm 2, Eq. 11 and [DVB24]. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "2RS0fL7Eet/tmp/4c701360d25dd3993983c698ec512cce6b7088dc7b6a3959f0a7dc5259b9da2b.jpg", "img_caption": ["Figure 4: Comparison of test MSE (log-log scale) for Algorithm 2, Eq. 11 and [DVB24]. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "YH was supported as a part of NCCR Automation, a National Centre of Competence (or Excellence) in Research, funded by the Swiss National Science Foundation (grant number 51NF40_225155). KB was supported in part by NSF grants DMS-2053918 and DMS-2413426. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[ABE19] Andreas Anastasiou, Krishnakumar Balasubramanian, and Murat A Erdogdu. Normal approximation for stochastic gradient descent via non-asymptotic rates of martingale clt. In Conference on Learning Theory, pages 115\u2013137. PMLR, 2019. (Cited on page 10.) ", "page_idx": 9}, {"type": "text", "text": "[AE96] Joshua Angrist and William N Evans. Children and their parents\u2019 labor supply: Evidence from exogenous variation in family size, 1996. (Cited on pages 9 and 16.) ", "page_idx": 9}, {"type": "text", "text": "[AI95] Joshua D Angrist and Guido W Imbens. Two-stage least squares estimation of average causal effects in models with variable treatment intensity. Journal of the American statistical Association, 90(430):431\u2013442, 1995. (Cited on pages 2 and 7.) [AP09] Joshua D Angrist and J\u00f6rn-Steffen Pischke. Mostly harmless econometrics: An empiricist\u2019s companion. Princeton university press, 2009. (Cited on pages 2, 3, and 4.) [AYS20] Kwangjun Ahn, Chulhee Yun, and Suvrit Sra. SGD with shuffilng: optimal rates without component convexity and large epoch requirements. Advances in Neural Information Processing Systems, 33:17526\u201317535, 2020. (Cited on page 8.) [BF17] Andrii Babii and Jean-Pierre Florens. Is completeness necessary? estimation in nonidentified linear models. arXiv preprint arXiv:1709.03473, 2017. (Cited on page 3.) [BGN22] Krishnakumar Balasubramanian, Saeed Ghadimi, and Anthony Nguyen. Stochastic multilevel composition optimization algorithms with level-independent convergence rates. SIAM Journal on Optimization, 32(2):519\u2013544, 2022. (Cited on page 3.)   \n$[\\mathbf{BKM}^{+}23]$ ] Andrew Bennett, Nathan Kallus, Xiaojie Mao, Whitney Newey, Vasilis Syrgkanis, and Masatoshi Uehara. Minimax instrumental variable regression and $l\\_2$ convergence guarantees without identification or closedness. arXiv preprint arXiv:2302.05404, 2023. (Cited on pages 2, 3, and 4.) [BKS19] Andrew Bennett, Nathan Kallus, and Tobias Schnabel. Deep generalized method of moments for instrumental variable analysis. Advances in neural information processing systems, 32, 2019. (Cited on pages 2, 3, and 4.) [Bol12] Kenneth A Bollen. Instrumental variables in sociology and the social sciences. Annual review of sociology, 38(1):37\u201372, 2012. (Cited on page 1.) [CF21] Samuele Centorrino and Jean-Pierre Florens. Nonparametric instrumental variable estimation of binary response models with continuous endogenous regressors. Econometrics and Statistics, 17:35\u201363, 2021. (Cited on page 5.) [CFR07] Marine Carrasco, Jean-Pierre Florens, and Eric Renault. Linear inverse problems in structural econometrics estimation based on spectral decomposition and regularization. Handbook of econometrics, 6:5633\u20135751, 2007. (Cited on page 3.)   \n$[\\mathrm{CLL}^{+}23]$ Xiaohong Chen, Sokbae Lee, Yuan Liao, Myung Hwan Seo, Youngki Shin, and Myunghyun Song. SGMM: Stochastic approximation to generalized method of moments. arXiv preprint arXiv:2308.13564, 2023. (Cited on pages 3 and 16.)   \n[CLTZ20] Xi Chen, Jason D Lee, Xin T Tong, and Yichen Zhang. Statistical inference for model parameters in stochastic gradient descent. Annals of Statistics, 48(1):251\u2013273, 2020. (Cited on pages 8, 10, and 20.) [CP12] Xiaohong Chen and Demian Pouzo. Estimation of nonparametric conditional moment models with possibly nonsmooth generalized residuals. Econometrica, 80(1):277\u2013321, 2012. (Cited on pages 3 and 4.)   \n$[\\mathbf{CPS}^{+}23]$ Yifan Cui, Hongming Pu, Xu Shi, Wang Miao, and Eric Tchetgen Tchetgen. Semiparametric proximal causal inference. Journal of the American Statistical Association, pages 1\u201312, 2023. (Cited on page 3.) [CR11] Xiaohong Chen and Markus Reiss. On rate optimality for ill-posed inverse problems in econometrics. Econometric Theory, 27(3):497\u2013521, 2011. (Cited on page 3.) [CSY21] Tianyi Chen, Yuejiao Sun, and Wotao Yin. Solving stochastic compositional optimization is nearly as easy as solving stochastic optimization. IEEE Transactions on Signal Processing, 69:4937\u20134948, 2021. (Cited on page 3.) [DAJJ12] John C Duchi, Alekh Agarwal, Mikael Johansson, and Michael I Jordan. Ergodic mirror descent. SIAM Journal on Optimization, 22(4):1549\u20131578, 2012. (Cited on page 5.)   \n[DFFR11] Serge Darolles, Yanqin Fan, Jean-Pierre Florens, and Eric Renault. Nonparametric instrumental regression. Econometrica, 79(5):1541\u20131565, 2011. (Cited on page 2.)   \n$[\\mathrm{DHP^{+}17}]$ Bo Dai, Niao He, Yunpeng Pan, Byron Boots, and Le Song. Learning from conditional distributions via dual embeddings. In Artificial Intelligence and Statistics, pages 1458\u2013 1467. PMLR, 2017. (Cited on page 2.)   \n[DLMS20] Nishanth Dikkala, Greg Lewis, Lester Mackey, and Vasilis Syrgkanis. Minimax estimation of conditional moment models. Advances in Neural Information Processing Systems, 33:12248\u201312262, 2020. (Cited on pages 2, 3, and 4.) [Doa22] Thinh T Doan. Nonlinear two-time-scale stochastic approximation convergence and finite-time performance. IEEE Transactions on Automatic Control, 2022. (Cited on page 8.) [DR20] Thinh Doan and Justin Romberg. Finite-time performance of distributed two-time-scale stochastic approximation. In Learning for Dynamics and Control, pages 26\u201336. PMLR, 2020. (Cited on pages 7 and 8.)   \n[DTSM18] Gal Dalal, Gugan Thoppe, Bal\u00e1zs Sz\u00f6r\u00e9nyi, and Shie Mannor. Finite sample analysis of two-timescale stochastic approximation with applications to reinforcement learning. In Conference On Learning Theory, pages 1199\u20131233. PMLR, 2018. (Cited on pages 7 and 8.) [DVB23] Riccardo Della Vecchia and Debabrota Basu. Online instrumental variable regression: Regret analysis and bandit feedback. arXiv preprint arXiv:2302.09357v1, 2023. (Cited on pages 15 and 16.) [DVB24] Riccardo Della Vecchia and Debabrota Basu. Stochastic online instrumental variable regression: Regrets for endogeneity and bandit feedback. arXiv preprint arXiv:2302.09357v3, 2024. (Cited on pages 3, 9, 10, 15, 16, and 17.) [EN13] Yuri M Ermoliev and Vladimir I Norkin. Sample average approximation method for compound stochastic optimization problems. SIAM Journal on Optimization, 23(4):2231\u20132263, 2013. (Cited on page 3.) [Eve23] Mathieu Even. Stochastic gradient descent under Markovian sampling schemes. In International Conference on Machine Learning, pages 9412\u20139439. PMLR, 2023. (Cited on page 5.) [GL13] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM journal on optimization, 23(4):2341\u20132368, 2013. (Cited on page 6.) [GOP19] Mert Gurbuzbalaban, Asu Ozdaglar, and Pablo A Parrilo. Convergence rate of incremental gradient and incremental newton methods. SIAM Journal on Optimization, 29(4):2542\u20132565, 2019. (Cited on page 8.)   \n[GRW20] Saeed Ghadimi, Andrzej Ruszczynski, and Mengdi Wang. A single timescale stochastic approximation method for nested stochastic optimization. SIAM Journal on Optimization, 30(1):960\u2013979, 2020. (Cited on page 3.) [HCH20] Yifan Hu, Xin Chen, and Niao He. Sample complexity of sample average approximation for conditional stochastic optimization. SIAM Journal on Optimization, 30(3):2103\u2013 2133, 2020. (Cited on page 3.) [HCH21] Yifan Hu, Xin Chen, and Niao He. On the bias-variance-cost tradeoff of stochastic optimization. Advances in Neural Information Processing Systems, 34:22119\u201322131, 2021. (Cited on page 5.) [HH05] Peter Hall and Joel L Horowitz. Nonparametric methods for inference in the presence of instrumental variables. Annals of statistics, 33(6):2904\u20132929, 2005. (Cited on page 2.)   \n[HLLBT17] Jason Hartford, Greg Lewis, Kevin Leyton-Brown, and Matt Taddy. Deep iv: A flexible approach for counterfactual prediction. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1414\u20131423. JMLR, 2017. (Cited on pages 2 and 3.) [HS19] Jeff Haochen and Suvrit Sra. Random shuffling beats SGD after finite epochs. In International Conference on Machine Learning, pages 2624\u20132633. PMLR, 2019. (Cited on page 8.)   \n$[\\mathrm{HWX}^{+}24]$ Yifan Hu, Jie Wang, Yao Xie, Andreas Krause, and Daniel Kuhn. Contextual stochastic bilevel optimization. Advances in Neural Information Processing Systems, 36, 2024. (Cited on page 3.)   \n[HZCH20] Yifan Hu, Siqi Zhang, Xin Chen, and Niao He. Biased stochastic first-order methods for conditional stochastic optimization and applications in meta learning. Advances in Neural Information Processing Systems, 33:2759\u20132770, 2020. (Cited on pages 2 and 3.) [KR20] Ahmed Khaled and Peter Richt\u00e1rik. Better theory for SGD in the nonconvex world. arXiv preprint arXiv:2002.03329, 2020. (Cited on page 5.) [Lan20] Guanghui Lan. First-order and stochastic optimization methods for machine learning, volume 1. Springer, 2020. (Cited on page 5.)   \n$[\\mathrm{LCY}^{+}20]$ Luofeng Liao, You-Lin Chen, Zhuoran Yang, Bo Dai, Mladen Kolar, and Zhaoran Wang. Provably efficient neural estimation of structural equation models: An adversarial approach. Advances in Neural Information Processing Systems, 33:8947\u20138958, 2020. (Cited on pages 2, 3, and 4.) [LS18] Greg Lewis and Vasilis Syrgkanis. Adversarial generalized method of moments. arXiv preprint arXiv:1803.07164, 2018. (Cited on pages 2 and 4.)   \n[MMLR20] Krikamol Muandet, Arash Mehrjou, Si Kai Lee, and Anant Raj. Dual instrumental variable regression. Advances in Neural Information Processing Systems, 33:2710\u20132721, 2020. (Cited on pages 2, 3, 4, and 5.) [MP06] Abdelkader Mokkadem and Mariane Pelletier. Convergence rate and averaging of nonlinear two-time-scale stochastic approximation algorithms. Annals of Applied Probability, 16(3):1671\u20131702, 2006. (Cited on pages 7 and 8.)   \n$[\\mathbf{MSB}^{+}09]$ ] Hamid Maei, Csaba Szepesvari, Shalabh Bhatnagar, Doina Precup, David Silver, and Richard S Sutton. Convergent temporal-difference learning with arbitrary smooth function approximation. Advances in neural information processing systems, 22, 2009. (Cited on page 8.)   \n$[\\mathbf{M}Z\\mathbf{G}^{+}21]$ Afsaneh Mastouri, Yuchen Zhu, Limor Gultchin, Anna Korba, Ricardo Silva, Matt Kusner, Arthur Gretton, and Krikamol Muandet. Proximal causal learning with kernels: Two-stage estimation and moment restriction. In International conference on machine learning, pages 7512\u20137523. PMLR, 2021. (Cited on page 3.) [Nes13] Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013. (Cited on page 8.) [NJN19] Dheeraj Nagaraj, Prateek Jain, and Praneeth Netrapalli. SGD without replacement: Sharper rates for general smooth convex functions. In International Conference on Machine Learning, pages 4703\u20134711. PMLR, 2019. (Cited on page 8.) [NP03] Whitney K Newey and James L Powell. Instrumental variable estimation of nonparametric models. Econometrica, 71(5):1565\u20131578, 2003. (Cited on page 4.) [Pap03] Christos H Papadimitriou. Computational complexity. In Encyclopedia of computer science, pages 260\u2013265. 2003. (Cited on page 15.) [PJ92] Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 30(4):838\u2013855, 1992. (Cited on pages 8, 10, and 26.) [PSF24] Caio Peixoto, Yuri Saporito, and Yuri Fonseca. Nonparametric instrumental variable regression through stochastic approximate gradients. arXiv preprint arXiv:2402.05639, 2024. (Cited on page 3.) [RBG22] Abhishek Roy, Krishnakumar Balasubramanian, and Saeed Ghadimi. Constrained stochastic nonconvex optimization with state-dependent Markov data. Advances in Neural Information Processing Systems, 35:23256\u201323270, 2022. (Cited on page 5.) [Rei45] Olav Reiers\u00f8l. Confluence analysis by means of instrumental sets of variables. PhD thesis, Almqvist & Wiksell, 1945. (Cited on page 3.) [RGP20] Shashank Rajput, Anant Gupta, and Dimitris Papailiopoulos. Closing the convergence gap of SGD without replacement. In International Conference on Machine Learning, pages 7964\u20137973. PMLR, 2020. (Cited on page 8.) [Rus21] Andrzej Ruszczynski. A stochastic subgradient method for nonsmooth nonconvex multilevel composition optimization. SIAM Journal on Control and Optimization, 59(3):2301\u20132320, 2021. (Cited on page 3.) [Rya12] Stephen P Ryan. The costs of environmental regulation in a concentrated industry. Econometrica, 80(3):1019\u20131061, 2012. (Cited on pages 9 and 16.) $[\\mathrm{SS}^{+}12]$ Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and Trends\u00ae in Machine Learning, 4(2):107\u2013194, 2012. (Cited on page 3.) [SSG19] Rahul Singh, Maneesh Sahani, and Arthur Gretton. Kernel instrumental variable regression. Advances in Neural Information Processing Systems, 32, 2019. (Cited on page 3.) [SSY18] Tao Sun, Yuejiao Sun, and Wotao Yin. On Markov chain gradient descent. Advances in neural information processing systems, 31, 2018. (Cited on page 5.) [SZ22] Qi-Man Shao and Zhuo-Song Zhang. Berry\u2013esseen bounds for multivariate nonlinear statistics with applications to m-estimators and stochastic gradient descent algorithms. Bernoulli, 28(3):1548\u20131576, 2022. (Cited on page 10.) [TM17] Ambuj Tewari and Susan A Murphy. From ads to interventions: Contextual bandits in mobile health. Mobile health: sensors, analytic methods, and applications, pages 495\u2013517, 2017. (Cited on page 1.) [Tse98] Paul Tseng. An incremental gradient (-projection) method with momentum term and adaptive stepsize rule. SIAM Journal on Optimization, 8(2):506\u2013531, 1998. (Cited on page 8.)   \n$[\\mathrm{VSH}^{+}16]$ Arun Venkatraman, Wen Sun, Martial Hebert, J Bagnell, and Byron Boots. Online instrumental variable regression with applications to online linear system identification. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016. (Cited on page 3.)   \n[WFL17] M. Wang, E. X. Fang, and B. Liu. Stochastic compositional gradient descent: Algorithms for minimizing compositions of expected-value functions. Mathematical Programming, 161(1-2):419\u2013449, 2017. (Cited on page 3.) [Wri28] Philip Green Wright. The tariff on animal and vegetable oils. Number 26. Macmillan, 1928. (Cited on page 3.)   \n[WZZ21] Yue Wang, Shaofeng Zou, and Yi Zhou. Non-asymptotic analysis for two time-scale tdc with general smooth function approximation. Advances in Neural Information Processing Systems, 34:9747\u20139758, 2021. (Cited on page 8.)   \n$[X\\mathrm{CS}^{+}21]$ ] Liyuan Xu, Yutian Chen, Siddarth Srinivasan, Nando de Freitas, Arnaud Doucet, and Arthur Gretton. Learning deep features in instrumental variable regression. In International Conference on Learning Representations, 2021. (Cited on page 3.) [XL21] Tengyu Xu and Yingbin Liang. Sample complexity bounds for two timescale valuebased reinforcement learning algorithms. In International Conference on Artificial Intelligence and Statistics, pages 811\u2013819. PMLR, 2021. (Cited on page 8.)   \n[ZCW23] Wanrong Zhu, Xi Chen, and Wei Biao Wu. Online covariance matrix estimation in stochastic gradient descent. Journal of the American Statistical Association, 118(541):393\u2013404, 2023. (Cited on page 10.)   \n$[Z\\mathrm{GG}^{+}22]$ Yuchen Zhu, Limor Gultchin, Arthur Gretton, Matt J Kusner, and Ricardo Silva. Causal inference with treatment measurement error: a nonparametric instrumental variable approach. In Uncertainty in Artificial Intelligence, pages 2414\u20132424. PMLR, 2022. (Cited on page 3.) [ZX21] Junyu Zhang and Lin Xiao. Multilevel composite stochastic optimization via nested variance reduction. SIAM Journal on Optimization, 31(2):1131\u20131157, 2021. (Cited on page 3.) ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Online updates of [DVB24] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For the sake of clarity, we present the O2SLS algorithm proposed in $[\\mathrm{DVB24},\\mathrm{v}3]^{1}$ in the streaming format, without any explicit matrix inversions that we used in our experiments: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{t+1}=(I-U_{t}\\gamma_{t}\\top_{t}Z_{t}Z_{t}^{\\top}\\gamma_{t})\\theta_{t}+U_{t}\\gamma_{t}\\top_{Z_{t}}Y_{t}}\\\\ &{\\gamma_{t+1}=(I-V_{t}Z_{t}Z_{t}^{\\top})\\gamma_{t}+V_{t}Z_{t}X_{t}^{\\top}}\\\\ &{U_{t+1}=U_{t}-\\frac{U_{t}\\gamma_{t}\\top_{Z_{t}}Z_{t}^{\\top}\\gamma_{t}U_{t}}{1+Z_{t}^{\\top}\\gamma_{t}U_{t}\\gamma_{t}\\top_{Z_{t}}}}\\\\ &{V_{t+1}=V_{t}-\\frac{V_{t}Z_{t}Z_{t}^{\\top}V_{t}}{1+Z_{t}^{\\top}V_{t}Z_{t}}\\qquad V_{0}=\\lambda^{-1}I_{d_{z}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $U_{t},V_{t}$ are two additional matrix sequences which tracks the matrix inverse of $\\begin{array}{r}{\\sum_{i=1}^{t}\\gamma_{i}^{\\top}Z_{i}Z_{i}^{\\top}\\gamma_{i}}\\end{array}$ , and $\\begin{array}{r}{\\left(\\lambda I_{d_{z}}+\\sum_{i=1}^{t}Z_{t}Z_{t}^{\\top}\\right)}\\end{array}$ respectively for a user defined parameter $\\lambda$ . As mentioned in [DVB24], we choose $\\lambda=0.1$ . The major difference between O2SLS and Algorithm 2 is that O2SLS takes an online two-stage regression approach to minimize a suitably defined regret whereas we take a conditional stochastic optimization point of view which requires carefully chosen step-sizes. In our Algorithm 2, we do not need to explicitly or implicitly do matrix inverse which can potentially cause stability issues. Furthermore, unlike [DVB24], we neither assume $\\textstyle\\sum_{i=1}^{t}Z_{i}Z_{i}^{\\top}$ is invertible for all $t$ nor do we assume that $Z$ is a bounded random variable for our analysis. Finally, the per-iteration computational complexity and memory requirement of Algorithm 2 is significantly better than O2SLS; see Section B. ", "page_idx": 14}, {"type": "text", "text": "B Per-iteration Complexities ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For the linear case, i.e., the underlying relationship between $Z$ and $X$ as well as $X$ and $Y$ are linear, Table 1 summarizes the per-iteration memory costs and number of arithmetic operations of the original O2SLS [DVB23], the updated O2SLS [DVB24] that we provide a matrix form update in Appendix A, TOSG-IVaR (Alg 1), and OTSG-IVaR (Alg 2) at the $t$ -th iteration. ", "page_idx": 14}, {"type": "text", "text": "Notice that the original version of O2SLS [DVB23] has a per-iteration and memory cost dependent on the iteration number $t$ as it needs to use all the samples accumulated till the iteration $t$ to conduct an offline 2SLS at each iteration. The updated O2SLS [DVB24] (the algorithm that we compare to) uses samples obtained at iteration $t$ to perform the update. Although the updated O2SLS avoids explicit matrix inversion, it is obvious that its arithmetic operations and memory cost per iteration are larger than our TOSG-IVaR and OTSG-IVaR. ", "page_idx": 14}, {"type": "text", "text": "We highlight that the TOSG-IVaR, which uses two samples $X$ and $X^{\\prime}$ from the conditional distribution $\\mathbb{P}(X\\mid Z)$ , requires only $\\mathcal{O}(d_{x})$ memory and arithmetic operations at each iteration. ", "page_idx": 14}, {"type": "text", "text": "For a fair comparison, we assume that two $n\\times n$ matrices multiplication admits an ${\\mathcal{O}}(n^{3})$ complexity, i.e., using normal textbook matrix multiplication. We also assume computing the inversion of a $n\\times n$ matrix admits an ${\\mathcal{O}}(n^{3})$ complexity. Interested readers may refer to [Pap03] for more details about faster algorithms with better complexities for matrix operations. ", "page_idx": 14}, {"type": "table", "img_path": "2RS0fL7Eet/tmp/30545e5988a0ea098144edfb7fb82c28bea8c0ecc8d2c76c5198983c45ddeed5.jpg", "table_caption": ["Table 1: Memory cost and the number of arithmetic operations at iteration $t$ . "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C Experimental Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Compute Resources ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "All experiments in Section 4 were conducted on a computer with an 11th Intel(R) Core(TM) i7- 11370H CPU. The time and space required to run our experiments are negligible and we anticipate they can be conducted in almost all computers. ", "page_idx": 15}, {"type": "text", "text": "C.2 Experimental Details for Figure 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Figure 1, we show an example where the updates (11) may diverge first before converging eventually and finite time performance can be much worse compared to Algorithm 2. For this experiment, we choose the model presented in (16) with $d_{x}=d_{z}=1$ , $\\theta_{*}=1$ , $\\gamma_{*}=-1$ , $\\rho=4$ , and $\\sigma_{\\epsilon}=1$ . When initialized at $\\gamma_{0}=10$ , and $\\theta_{0}=0$ , the updates in (11) keeps diverging rapidly at first whereas Algorithm 2 is much more stable. So, by the end of 100, 000 iterations, while Algorithm 2 achieves an error of $\\approx10^{-5}$ , (11) achieves $\\approx10^{4}$ that is worse than it was at initialization because (11) has not recovered from the initial divergence phase yet. However, once (11) starts converging, the convergence rate of (11) is similar to Algorithm 2 as one can see from Figure 1 (also see our discussion on the convergence of (11) in Section E.3). ", "page_idx": 15}, {"type": "text", "text": "C.3 Additional Experiments on Real-World Dataset ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section we provide experimental results on real-world datasets provided in [AE96] and [Rya12]. The results are included in Figures 5 and 6 respectively. Following the convention in Section 4, we write \u201cOTSG-IVaR\u201d, \u201cCSO \u2013 Eq. (11)\u201d and \u201c[DVB23]\u201d to represent Algorithm 2, Algorithm 2 with the updates replaced by (11) and Algorithm 1 in [DVB24]. ", "page_idx": 15}, {"type": "text", "text": "C.3.1 Children and Their Parents\u2019 Labor Supply Data in [AE96] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The outcome $Y$ is number of working weeks divided by 52, the regressor $X$ is 1(number of children is greater than 2), and the instrumental variable $Z$ is 1(first two siblings are of same sex), where $\\mathbf{1}(\\cdot)$ is the indicator function. At each time we randomly sample a data-point from this dataset without replacement. Since the \u201ctrue\u201d parameter $\\boldsymbol{\\theta}_{*}$ is unknown in real data, we use the offline model parameter estimate as our ground truth, following $[\\mathrm{CLL}^{+}23]$ . We include the results in Figure 5, in which we observe that CSO performs similar to [DVB23] whereas OTSG-IVaR performs much better in terms of convergence speed. Moreover, the estimation errors of CSO and [DVB23] plateau after $\\approx10\\mathrm{,000}$ iterations whereas OTSG-IVaR keeps on improving the estimate over the observed horizon. ", "page_idx": 15}, {"type": "text", "text": "C.3.2 U.S. Portland Cement Industry Data in [Rya12] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Response variable $Y$ is log(shipped), and the predictor $X$ is $\\mathrm{log}(\\mathrm{price})$ . There are 4 instrumental variables given by wage in dollars per hour for skilled manufacturing workers, electricity price, coal price, and gas price. Unlike the previous dataset, we have only 483 data samples here. So, to mimic an i.i.d. data stream, we divide our training into multiple epochs of length equal to the number of training data samples, and over each epoch, at each iteration, we sample one data point uniformly randomly without replacement from the training data to generate the data-stream. Both OTSG-IVaR and CSO perform much better than [DVB23] in terms of convergence speed. Figure 6(b) is a magnified view of iterations $>50$ to highlight the performance difference between various algorithms. ", "page_idx": 15}, {"type": "image", "img_path": "2RS0fL7Eet/tmp/d79fcbddcb40ca3c91e05b5d2e162736d86fb62574d6fb0c05989f5295b1b8ec.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 5: Comparison of $\\mathbb{E}[\\|\\theta_{t}-\\theta_{*}\\|^{2}]$ (log-log scale) for Algorithm 2, Eq. 11 and [DVB24]. ", "page_idx": 16}, {"type": "image", "img_path": "2RS0fL7Eet/tmp/cb4201ff42f4e7e471cc39f162cacaa8728d88293111cd7c38bd8bb2eeba87d2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 6: Comparison of $\\mathbb{E}[\\|\\theta_{t}-\\theta_{*}\\|^{2}]$ (log-log scale) for Algorithm 2, Eq. 11 and [DVB24]. ", "page_idx": 16}, {"type": "text", "text": "D Proofs for Section 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. We aim to find the optimal $\\boldsymbol{\\theta}_{*}$ . According to (2), we know ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{X|Z}[X]\\cdot\\mathbb{E}_{X|Z}[X]^{\\top}\\Big]\\theta_{*}=\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{Y|Z}[Y]\\cdot\\mathbb{E}_{X|Z}[X]\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The updates in Algorithm 1 can be written as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\theta_{t+1}=\\theta_{t}-\\alpha_{t+1}(\\boldsymbol{X}_{t}^{\\top}\\theta_{t}-\\boldsymbol{Y}_{t})\\boldsymbol{X}_{t}^{\\top}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{t+1}-\\theta_{*}}\\\\ &{=\\!\\theta_{t}-\\alpha_{t+1}\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{X|Z}[X]\\cdot\\mathbb{E}_{X|Z}[X]^{\\top}\\Big]\\theta_{t}+\\alpha_{t+1}\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{Y|Z}[Y]\\cdot\\mathbb{E}_{X|Z}[X]\\Big]-\\theta_{*}}\\\\ &{\\quad-\\alpha_{t+1}\\Big(X_{t}^{\\prime}X_{t}^{\\top}-\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{X|Z}[X]\\cdot\\mathbb{E}_{X|Z}[X]^{\\top}\\Big]\\Big)\\theta_{t}+\\alpha_{t+1}\\Big(Y_{t}X_{t}^{\\prime}-\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{Y|Z}[Y]\\cdot\\mathbb{E}_{X|Z}[X]\\Big]\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now we analyze the convergence and variance separately. For the convergence part, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\theta_{t}-\\alpha_{t+1}\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{X|Z}[X]\\cdot\\mathbb{E}_{X|Z}[X]^{\\top}\\Big]\\theta_{t}+\\alpha_{t+1}\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{Y|Z}[Y]\\cdot\\mathbb{E}_{X|Z}[X]\\Big]-\\theta_{*}}\\\\ &{=\\!\\Big(I-\\alpha_{t+1}\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{X|Z}[X]\\cdot\\mathbb{E}_{X|Z}[X]^{\\top}\\Big]\\Big)(\\theta_{t}-\\theta_{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For the variance part we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\quad}&{\\mathbb{E}\\Big[\\|X_{t}^{\\prime}X_{t}^{\\top}-\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{X|Z}[X]\\cdot\\mathbb{E}_{X|Z}[X]^{\\top}\\Big]\\|^{2}\\Big]}\\\\ &{=}&{\\mathbb{E}\\Big[\\|X_{t}^{\\prime}X_{t}^{\\top}-\\mathbb{E}_{X|Z_{t}}[X]\\cdot\\mathbb{E}_{X|Z_{t}}[X]^{\\top}+\\mathbb{E}_{X|Z_{t}}[X]\\cdot\\mathbb{E}_{X|Z_{t}}[X]^{\\top}-\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{X|Z}[X]\\cdot\\mathbb{E}_{X|Z}[X]^{\\top}\\Big]\\|^{2}\\Big]}\\\\ &{\\le2\\mathbb{E}\\Big[\\|X_{t}^{\\prime}X_{t}^{\\top}-\\mathbb{E}_{X|Z_{t}}[X]\\cdot\\mathbb{E}_{X|Z_{t}}[X]^{\\top}\\|^{2}+\\|\\mathbb{E}_{X|Z_{t}}[X]\\cdot\\mathbb{E}_{X|Z_{t}}[X]^{\\top}-\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{X|Z}[X]\\cdot\\mathbb{E}_{X|Z}[X]^{\\top}\\Big]\\|^{2}\\Big]}\\\\ &{\\le2C_{x}d_{x}^{\\vartheta_{1}}+2C_{x x}d_{z}^{\\vartheta_{3}}=:\\sigma_{1}^{2}}&{(2\\mathbb{E})^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\|Y_{t}X_{t}^{\\prime}-\\mathbb{E}_{Z}\\left[\\mathbb{E}_{Y|Z}[Y]\\cdot\\mathbb{E}_{X|Z}[X]\\right]\\|^{2}\\right]}\\\\ &{=\\!\\mathbb{E}\\!\\left[\\|Y_{t}X_{t}^{\\prime}-\\mathbb{E}_{Y|Z_{t}}[Y]\\cdot\\mathbb{E}_{X|Z_{t}}[X]\\|^{2}+\\|\\mathbb{E}_{Y|Z_{t}}[Y]\\cdot\\mathbb{E}_{X|Z_{t}}[X]-\\mathbb{E}_{Z}\\left[\\mathbb{E}_{Y|Z}[Y]\\cdot\\mathbb{E}_{X|Z}[X]\\right]\\|^{2}\\right]}\\\\ &{\\le\\!C_{y}d_{x}^{\\vartheta_{2}}+C_{y x}d_{z}^{\\vartheta_{4}}=:\\sigma_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now we know from (18), (19), (20), and (21) that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\theta_{t+1}-\\theta_{*}\\|^{2}=\\|A_{t}\\|^{2}+2\\alpha_{t+1}\\left\\langle A_{t},B_{t}\\right\\rangle+\\alpha_{t+1}^{2}\\|B_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{t}=\\Big(I-\\alpha_{t+1}\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{X|Z}\\big[X\\big]\\cdot\\mathbb{E}_{X|Z}\\big[X\\big]^{\\top}\\Big]\\Big)(\\theta_{t}-\\theta_{*})}\\\\ &{B_{t}=-\\Big(X_{t}^{\\prime}X_{t}^{\\top}-\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{X|Z}\\big[X\\big]\\cdot\\mathbb{E}_{X|Z}\\big[X\\big]^{\\top}\\Big]\\Big)\\theta_{t}+\\Big(Y_{t}X_{t}^{\\prime}-\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{Y|Z}\\big[Y\\big]\\cdot\\mathbb{E}_{X|Z}\\big[X\\big]\\Big]\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This implies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\theta_{t+1}|\\theta_{t}}\\Big[\\|\\theta_{t+1}-\\theta_{*}\\|^{2}\\Big]}\\\\ &{=\\!\\|\\Big(I-\\alpha_{t+1}\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{X|Z}[X]\\cdot\\mathbb{E}_{X|Z}[X]^{\\top}\\Big]\\Big)(\\theta_{t}-\\theta_{*})\\|^{2}}\\\\ &{\\quad+\\alpha_{t+1}^{2}\\mathbb{E}_{X_{t},X_{t}^{\\prime},Y_{t},Z_{t}|\\theta_{t}}\\Big[\\|\\Big(X_{t}^{\\prime}X_{t}^{\\top}-\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{X|Z}[X]\\cdot\\mathbb{E}_{X|Z}[X]^{\\top}\\Big]\\Big)\\theta_{t}-\\Big(Y_{t}X_{t}^{\\prime}-\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{Y|Z}[Y]\\cdot\\mathbb{E}_{Y|Z}[X]^{\\top}\\Big]\\Big)\\Big]}\\\\ &{\\le\\!(1-\\alpha_{t+1}\\mu)^{2}\\|\\theta_{t}-\\theta_{*}\\|^{2}+3\\alpha_{t+1}^{2}\\Big(\\sigma_{1}^{2}\\|\\theta_{t}-\\theta_{*}\\|^{2}+\\sigma_{1}^{2}\\|\\theta_{*}\\|^{2}+\\sigma_{2}^{2}\\|\\theta_{*}\\|^{2}\\Big)}\\\\ &{\\le\\!((1-\\alpha_{t+1}\\mu)^{2}+3\\alpha_{t+1}^{2}\\sigma_{1}^{2})\\|\\theta_{t}-\\theta_{*}\\|^{2}+3\\alpha_{t+1}^{2}\\sigma_{1}^{2}\\|\\theta_{*}\\|^{2}+3\\alpha_{t+1}^{2}\\sigma_{2}^{2}\\|\\theta_{*}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the first inequality uses Cauchy-Schwarz inequality, the definition of $\\sigma_{1},\\sigma_{2}$ and Assumption 2.4. Choosing $\\alpha_{t+1}$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n((1-\\alpha_{t+1}\\mu)^{2}+3\\alpha_{t+1}^{2}\\sigma_{1}^{2})\\leq1-\\alpha_{t+1}\\mu\\Leftrightarrow\\alpha\\leq\\frac{\\mu}{\\mu^{2}+3\\sigma_{1}^{2}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and taking expectation on both sides of (23), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\Big[\\|\\theta_{t+1}-\\theta_{*}\\|^{2}\\Big]\\leq(1-\\alpha_{t+1}\\mu)\\mathbb{E}\\Big[\\|\\theta_{t}-\\theta_{*}\\|^{2}\\Big]+3\\alpha_{t+1}^{2}\\sigma_{1}^{2}\\|\\theta_{*}\\|^{2}+3\\alpha_{t+1}^{2}\\sigma_{2}^{2}\\|\\theta_{*}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now, we use the following result. ", "page_idx": 17}, {"type": "text", "text": "Lemma 2. Suppose we have three sequences $\\{a_{t}\\}_{t=0}^{\\infty},\\{b_{t}\\}_{t=0}^{\\infty},\\{r_{t}\\}_{t=0}^{\\infty}$ satisfying ", "page_idx": 17}, {"type": "equation", "text": "$$\na_{t+1}\\leq r_{t}a_{t}+b_{t},r_{t}>0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for any $t\\geq0$ . Define $\\begin{array}{r}{R_{t+1}=\\prod_{i=0}^{t}r_{i}}\\end{array}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\na_{t+1}\\leq R_{t+1}a_{0}+\\sum_{i=0}^{t}\\frac{R_{t+1}b_{i}}{R_{i+1}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By Lemma 2, we know ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{Z}\\Big[\\Vert\\theta_{t+1}-\\theta_{*}\\Vert^{2}\\Big]\\leq\\prod_{i=0}^{t}(1-\\alpha_{i}\\mu)\\mathbb{E}\\Big[\\Vert\\theta_{0}-\\theta_{*}\\Vert^{2}\\Big]+(3\\sigma_{1}^{2}\\Vert\\theta_{*}\\Vert^{2}+3\\sigma_{2}^{2}\\Vert\\theta_{*}\\Vert^{2})\\sum_{i=0}^{t}\\alpha_{i}^{2}\\prod_{j=i+1}^{t}(1-\\alpha_{j}\\mu).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now if we set $\\alpha_{i}=\\alpha$ , we know ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\Big[\\|\\theta_{t}-\\theta_{*}\\|^{2}\\Big]\\leq(1-\\alpha\\mu)^{t}\\mathbb{E}\\Big[\\|\\theta_{0}-\\theta_{*}\\|^{2}\\Big]+\\alpha^{2}\\Big(\\sum_{i=0}^{t}(1-\\alpha\\mu)^{i}\\Big)(3\\sigma_{1}^{2}\\|\\theta_{*}\\|^{2}+3\\sigma_{2}^{2}\\|\\theta_{*}\\|^{2})}\\\\ {\\leq e^{-t\\alpha\\mu}\\mathbb{E}\\Big[\\|\\theta_{0}-\\theta_{*}\\|^{2}\\Big]+\\frac{\\alpha}{\\mu}(3\\sigma_{1}^{2}\\|\\theta_{*}\\|^{2}+3\\sigma_{2}^{2}\\|\\theta_{*}\\|^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Choosing $\\alpha,T$ such that $\\begin{array}{r}{\\alpha=\\frac{\\log T}{\\mu T}\\leq\\frac{\\mu}{\\mu^{2}+3\\sigma_{1}^{2}}}\\end{array}$ \u00b52+\u00b53\u03c32, we know ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\|\\theta_{T}-\\theta_{*}\\|^{2}\\Big]\\leq\\frac{\\mathbb{E}\\Big[\\|\\theta_{0}-\\theta_{*}\\|^{2}\\Big]}{T}+\\frac{3\\|\\theta_{*}\\|^{2}(\\sigma_{1}^{2}+\\sigma_{2}^{2})\\log T}{\\mu^{2}T}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "D.2 Proof of Lemma 2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. We notice from (24) that for any $0\\leq i\\leq t$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{a_{i+1}}{R_{i+1}}\\leq\\frac{a_{i}}{R_{i}}+\\frac{b_{i}}{R_{i+1}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Taking summation on both sides, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{a_{t+1}}{R_{t+1}}\\leq\\frac{a_{0}}{R_{0}}+\\sum_{i=0}^{t}\\frac{b_{i}}{R_{i+1}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which completes the proof by multiplying $R_{t+1}$ on both sides. ", "page_idx": 18}, {"type": "text", "text": "D.3 Proof of Lemma 1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. We first notice that Assumption 2.3 holds since ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{X|Z}[X]\\cdot\\mathbb{E}_{X|Z}[X]^{\\top}\\Big]=\\mathbb{E}_{Z}\\Big[\\phi\\big(\\gamma_{*}^{\\top}Z\\big)\\cdot\\phi\\big(\\gamma_{*}^{\\top}Z\\big)^{\\top}\\Big]\\succeq\\mu I.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For (5) and (6), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\Big[\\|X^{\\prime}X^{\\top}-\\mathbb{E}_{X|Z}[X]\\mathbb{E}_{X|Z}[X]^{\\top}\\|^{2}\\Big]}\\\\ &{=\\!\\!\\mathbb{E}\\Big[\\|\\epsilon_{2}^{\\prime}\\phi(\\gamma_{*}^{\\top}Z)^{\\top}+\\phi(\\gamma_{*}^{\\top}Z)\\epsilon_{2}^{\\top}+\\epsilon_{2}^{\\prime}\\epsilon_{2}^{\\top}\\|^{2}\\Big]}\\\\ &{\\le\\!3\\mathbb{E}\\Big[\\|\\epsilon_{2}^{\\prime}\\phi(\\gamma_{*}^{\\top}Z)^{\\top}\\|^{2}+\\|\\phi(\\gamma_{*}^{\\top}Z)\\epsilon_{2}^{\\top}\\|^{2}+\\|\\epsilon_{2}^{\\prime}\\epsilon_{2}^{\\top}\\|^{2}\\Big]}\\\\ &{=\\!3\\mathbb{E}\\Big[\\|\\phi(\\gamma_{*}^{\\top}Z)\\epsilon_{2}^{\\top}\\epsilon_{2}^{\\prime}\\phi(\\gamma_{*}^{\\top}Z)^{\\top}\\|+\\|\\phi(\\gamma_{*}^{\\top}Z)\\epsilon_{2}^{\\top}\\epsilon_{2}\\phi(\\gamma_{*}^{\\top}Z)^{\\top}\\|+|\\epsilon_{2}^{\\top}\\epsilon_{2}^{\\top}|^{2}\\Big]=\\mathcal{O}(d_{x}^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\Big[\\|Y X^{\\prime}-\\mathbb{E}_{Y|Z}[Y]\\mathbb{E}_{X|Z}[X]\\|^{2}\\Big]}\\\\ &{=\\!\\mathbb{E}\\Big[\\|X^{\\prime}X^{\\top}\\theta_{*}+\\epsilon_{1}X^{\\prime}-\\mathbb{E}_{X|Z}[X]\\mathbb{E}_{X|Z}[X]^{\\top}\\theta_{*}\\|^{2}\\Big]}\\\\ &{\\le\\!2\\mathbb{E}\\Big[\\|X^{\\prime}X^{\\top}\\theta_{*}-\\mathbb{E}_{X|Z}[X]\\mathbb{E}_{X|Z}[X]^{\\top}\\theta_{*}\\|^{2}\\Big]+2\\mathbb{E}\\Big[\\epsilon_{1}^{2}\\|\\phi(\\gamma_{*}^{\\top}Z)+\\epsilon_{2}^{\\prime}\\|^{2}\\Big]}\\\\ &{=\\!\\mathcal{O}(\\|\\theta_{*}\\|^{2}\\sigma_{\\epsilon_{2}}^{2}d_{x}^{2}+\\sigma_{\\epsilon_{1}}^{2}d_{x}+\\sigma_{\\epsilon_{1},\\epsilon_{2}}^{2}d_{x}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the first inequality uses Cauchy-Schwarz inequality, and the second equality uses (9), (10) and (25). For (7) we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\Big[\\|\\mathbb{E}_{X|Z}[X]\\cdot\\mathbb{E}_{X|Z}[X]^{\\top}-\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{X|Z}[X]\\cdot\\mathbb{E}_{X|Z}[X]^{\\top}\\Big]\\|^{2}\\Big]}\\\\ &{=\\!\\!\\mathbb{E}\\Big[\\|\\phi(\\gamma_{*}^{\\top}Z)\\phi(\\gamma_{*}^{\\top}Z)^{\\top}-\\mathbb{E}\\left[\\phi(\\gamma_{*}^{\\top}Z)\\phi(\\gamma_{*}^{\\top}Z)^{\\top}\\right]\\|^{2}\\Big]=\\mathcal{O}(d_{z})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last equality uses (10). Using the above conclusion in (8), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~\\mathbb{E}\\Big[\\|\\mathbb{E}_{Y|Z}[Y]\\cdot\\mathbb{E}_{X|Z}[X]-\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{Y|Z}[Y]\\cdot\\mathbb{E}_{X|Z}[X]\\Big]\\|^{2}\\Big]}\\\\ &{=\\!\\mathbb{E}\\Big[\\|\\mathbb{E}_{X|Z}[X]\\cdot\\mathbb{E}_{X|Z}[X]^{\\top}\\theta_{*}-\\mathbb{E}_{Z}\\Big[\\mathbb{E}_{X|Z}[X]\\cdot\\mathbb{E}_{X|Z}[X]^{\\top}\\theta_{*}\\Big]\\|^{2}\\Big]=\\mathcal{O}(\\|\\theta_{*}\\|^{2}d_{z}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "E Proofs for Section 3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "E.1 Proof of Theorem 2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof of Theorem 2 . Recall that $\\xi_{Z_{t}}$ , and $\\xi_{Z_{t}Y_{t}}$ are the i.i.d. noise sequences ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\xi_{Z_{t}}=\\Sigma_{Z}-Z_{t}Z_{t}^{\\top},}\\\\ {\\xi_{Z_{t}Y_{t}}=\\Sigma_{Z Y}-Z_{t}Y_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note $\\gamma_{*}$ , and $\\theta_{*}$ can be written as $\\gamma_{\\ast}=\\Sigma_{Z}^{-1}\\Sigma_{Z X}\\in\\mathbb{R}^{d_{z}\\times d_{x}}$ , and $\\theta_{*}=\\left(\\gamma_{*}^{\\top}\\Sigma_{z}\\gamma_{*}\\right)^{-1}\\gamma_{*}^{\\top}\\Sigma_{Z Y}\\in\\mathbb{R}^{d_{x}}$ which we are going to use throughout the proof. ", "page_idx": 19}, {"type": "text", "text": "To quantify the bias, we use the following bound on $\\mathbb{E}\\left[\\Vert\\gamma_{t}-\\gamma_{*}\\Vert_{2}^{k}\\right]$ , $k=1,2,4$ , proved in Lemma 3.2 of [CLTZ20]. ", "page_idx": 19}, {"type": "text", "text": "Lemma 3. Suppose Assumption 2.2, and Assumption 3.2 hold. Then we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert\\gamma_{t}-\\gamma_{*}\\Vert^{k}\\right]=O\\left(\\sqrt{d_{z}^{k}{\\beta_{t}}^{k}}\\right)\\quad f o r\\quad k=1,2,4.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We proceed by noting that if $\\gamma_{*}$ , $\\Sigma_{Z}$ , and $\\Sigma_{Z Y}$ were known beforehand, one could use the following deterministic gradient updates to obtain $\\theta_{*}$ . ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widetilde{\\theta}_{t+1}=\\widetilde{\\theta}_{t}-\\alpha_{t+1}\\gamma_{*}^{\\top}\\left(\\Sigma_{Z}\\gamma_{*}\\widetilde{\\theta}_{t}-\\Sigma_{Z Y}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma 4. Let Assumption 2.3 be true. Then, choosing $\\eta_{k}=O(k^{-a})$ with $1/2<a<1$ , we have $\\lVert\\widetilde{{\\boldsymbol{\\theta}}}_{t}-{\\boldsymbol{\\theta}}_{*}\\rVert=O\\left(\\exp(-t^{1-a})\\right)$ . ", "page_idx": 19}, {"type": "text", "text": "Define the sequence $\\delta_{t}:=\\theta_{t}-\\widetilde{\\theta}_{t}$ . We will establish the convergence rate of $\\mathbb{E}\\left[\\Vert\\delta_{t}\\Vert_{2}^{2}\\right]$ . From (13), and (27), we have the following expansion of $\\delta_{t+1}$ . ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\delta_{t+1}=Q_{t}\\delta_{t}+\\alpha_{t+1}D_{t}\\theta_{t}+\\alpha_{t+1}(\\gamma_{t}-\\gamma_{*})^{\\top}\\Sigma_{Z Y}-\\alpha_{t+1}\\gamma_{t}^{\\top}\\xi_{Z_{t}Y_{t}}+\\alpha_{t+1}\\gamma_{t}^{\\top}\\xi_{Z_{t}}\\gamma_{t}\\theta_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{t}:=(I-\\alpha_{t+1}\\gamma_{*}^{\\top}\\Sigma_{Z}\\gamma_{*}),}\\\\ &{D_{t}:=\\gamma_{*}^{\\top}\\Sigma_{Z}\\gamma_{*}-\\gamma_{t}^{\\top}\\Sigma_{Z}\\gamma_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "First we will establish an intermediate bound on $\\mathbb{E}\\left[\\Vert\\delta_{t}\\Vert^{2}\\right]$ . To do so, we will need the following result which shows that E $[\\lVert\\theta_{t}-\\theta_{*}\\rVert_{2}^{4}]$ is bounded for all $t$ which we prove in Section E.2. ", "page_idx": 19}, {"type": "text", "text": "Lemma 5 (Boundedness of fourth moment of $\\lVert\\boldsymbol{\\theta}_{t}-\\boldsymbol{\\theta}_{*}\\rVert)$ ). Let the conditions in Theorem 2 be true. Then, choosing $\\alpha_{t},\\beta_{t}$ such that $\\alpha_{t}\\,\\leq\\,d_{z}^{-4\\varkappa-\\vartheta/2}$ , and $\\textstyle\\sum_{t=1}^{\\infty}(\\alpha_{t}^{2}+\\alpha_{t}\\sqrt{\\beta_{t}})\\,<\\,\\infty$ , we have $\\mathbb{E}\\left[\\Vert\\theta_{t}-\\theta_{*}\\Vert_{2}^{4}\\right]$ is bounded by some constant $M>0$ . ", "page_idx": 19}, {"type": "text", "text": "Lemma 6 (Intermediate bound on $\\mathbb{E}[\\|\\delta_{t}\\|_{2}^{2}])$ . Let the conditions in Theorem 2 be true. We have the following intermediate bound on E $\\left[\\lVert\\delta_{t}\\rVert^{2}\\right]$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\|\\delta_{t}\\|^{2}\\right]=O\\left(\\beta_{t}d_{z}^{1+2\\varkappa}+\\alpha_{t+1}d_{z}^{4\\varkappa+\\vartheta/2}+\\sqrt{d_{z}\\beta_{t}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma $^{6}$ . Recall the update for $\\delta_{t+1}$ obtained in (28). ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\delta_{t+1}=Q_{t}\\delta_{t}+\\alpha_{t+1}D_{t}\\theta_{t}+\\alpha_{t+1}(\\gamma_{t}-\\gamma_{*})^{\\top}\\Sigma_{Z Y}-\\alpha_{t+1}\\gamma_{t}^{\\top}\\xi_{Z_{t}Y_{t}}+\\alpha_{t+1}\\gamma_{t}^{\\top}\\xi_{Z_{t}}\\gamma_{t}\\theta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\delta_{t+1}\\right\\|_{2}^{2}=\\delta_{t}^{\\textsf{T}}Q_{t}^{2}\\delta_{t}+\\alpha_{t+1}^{2}\\|D_{t}\\theta_{t}+\\left(\\gamma_{t}-\\gamma_{*}\\right)^{\\top}\\Sigma_{Z Y}-\\gamma_{t}^{\\textsf{T}}\\xi_{Z_{t}Y_{t}}+\\gamma_{t}^{\\textsf{T}}\\xi_{Z_{t}}\\gamma_{t}\\theta_{t}\\|^{2}}\\\\ &{\\qquad\\qquad+\\,2\\alpha_{t+1}\\delta_{t}^{\\textsf{T}}Q_{t}\\left(D_{t}\\theta_{t}+\\left(\\gamma_{t}-\\gamma_{*}\\right)^{\\top}\\Sigma_{Z Y}\\right)}\\\\ &{\\qquad\\qquad+\\,2\\alpha_{t+1}\\delta_{t}^{\\textsf{T}}Q_{t}\\left(\\gamma_{t}^{\\textsf{T}}\\xi_{Z_{t}}\\gamma_{t}\\theta_{t}-\\gamma_{t}^{\\textsf{T}}\\xi_{Z_{t}Y_{t}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, choosing $\\alpha_{1}(\\|\\gamma_{*}\\|_{2}\\lambda_{Z})^{2}<1$ , using Young\u2019s inequality and Assumption 2.2, from (30) we get, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}\\left[\\Vert\\delta_{t+1}\\Vert_{2}^{2}\\right]\\leq(1-\\alpha_{t+1}\\mu)\\Vert\\delta_{t}\\Vert^{2}+4\\alpha_{t+1}^{2}\\left(\\Vert D_{t}\\theta_{t}\\Vert^{2}+\\Vert(\\gamma_{t}-\\gamma_{*})^{\\top}\\Sigma_{Z Y}\\Vert^{2}\\right)}\\\\ &{\\quad+\\,4\\alpha_{t+1}^{2}\\left(\\Vert\\gamma_{t}\\Vert_{2}^{2}\\mathbb{E}\\left[\\Vert\\xi_{Z_{t}Y_{t}}\\Vert_{2}^{2}\\right]+\\Vert\\gamma_{t}\\Vert_{2}^{4}\\mathbb{E}\\left[\\Vert\\xi_{Z_{t}}\\Vert^{2}\\right]\\Vert\\theta_{t}\\Vert^{2}\\right)}\\\\ &{\\quad+\\,2\\alpha_{t+1}\\delta_{t}^{\\top}Q_{t}\\left(D_{t}\\theta_{t}+(\\gamma_{t}-\\gamma_{*})^{\\top}\\Sigma_{Z Y}\\right)}\\\\ &{\\lesssim(1-\\alpha_{t+1}\\mu)\\Vert\\delta_{t}\\Vert^{2}+4\\alpha_{t+1}^{2}\\left(\\Vert D_{t}\\Vert^{2}\\Vert\\theta_{t}\\Vert^{2}+\\Vert(\\gamma_{t}-\\gamma_{*})^{\\top}\\Sigma_{Z Y}\\Vert^{2}\\right)}\\\\ &{\\quad+\\,4C\\alpha_{t+1}^{2}\\left(d_{z}^{2\\kappa+\\vartheta/2}+d_{z}^{4\\kappa+\\vartheta/2}\\Vert\\theta_{t}\\Vert^{2}\\right)+}\\\\ &{\\quad2\\alpha_{t+1}\\delta_{t}^{\\top}Q_{t}\\left(D_{t}\\theta_{t}+(\\gamma_{t}-\\gamma_{*})^{\\top}\\Sigma_{Z Y}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last inequality follows by Assumption 3.1,and Assumption 3.3. ", "page_idx": 20}, {"type": "text", "text": "Now, taking expectation on both sides, we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\Vert\\delta_{t+1}\\Vert_{2}^{2}\\right]\\lesssim\\!(1-\\alpha_{t+1}\\mu)\\mathbb{E}\\left[\\Vert\\delta_{t}\\Vert^{2}\\right]+4\\alpha_{t+1}^{2}\\left(\\mathbb{E}\\left[\\Vert D_{t}\\Vert^{2}\\Vert\\theta_{t}\\Vert^{2}\\right]+\\mathbb{E}\\left[\\Vert\\left(\\gamma_{t}-\\gamma_{*}\\right)^{\\top}\\Sigma_{Z Y}\\Vert^{2}\\right]\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\,4C\\alpha_{t+1}^{2}\\left(d_{z}^{2\\times+\\vartheta/2}+d_{z}^{4\\times+\\vartheta/2}\\mathbb{E}\\left[\\Vert\\theta_{t}\\Vert^{2}\\right]\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\,2\\alpha_{t+1}\\left(\\mathbb{E}\\left[\\Vert\\delta_{t}^{\\top}Q_{t}D_{t}\\theta_{t}\\right]\\right]+\\mathbb{E}\\left[\\vert\\delta_{t}^{\\top}Q_{t}(\\gamma_{t}-\\gamma_{*})^{\\top}\\Sigma_{Z Y}\\vert\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, the following bounds are true: ", "page_idx": 20}, {"type": "text", "text": "1. We have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha_{t+1}^{2}\\mathbb{E}\\left[\\|D_{t}\\|^{2}\\|\\theta_{t}\\|^{2}\\right]\\leq\\alpha_{t+1}^{2}\\sqrt{\\mathbb{E}\\left[\\|D_{t}\\|_{2}^{4}\\right]\\mathbb{E}\\left[\\|\\theta_{t}\\|_{2}^{4}\\right]}\\lesssim d_{z}^{1+2\\varkappa}\\alpha_{t+1}^{2}\\beta_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the first inequality follows by Cauchy-Schwarz inequality, the second inequality follows by (42), and Lemma 5. ", "page_idx": 20}, {"type": "text", "text": "2. Using $\\Sigma_{Z Y}=O(1)$ , and Lemma 3, we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\alpha_{t+1}^{2}\\mathbb{E}\\left[\\|(\\gamma_{t}-\\gamma_{*})^{\\top}\\Sigma_{Z Y}\\|^{2}\\right]\\lesssim d_{z}\\beta_{t}\\alpha_{t+1}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "3. We have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{t+1}\\mathbb{E}\\left[\\left|\\delta_{t}^{\\top}Q_{t}D_{t}\\theta_{t}\\right|\\right]\\leq\\alpha_{t+1}\\mathbb{E}\\left[\\|\\delta_{t}\\|_{2}\\|Q_{t}\\|_{2}\\|D_{t}\\|_{2}\\|\\theta_{t}\\|_{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\alpha_{t+1}\\mu}{16}\\mathbb{E}\\left[\\|\\delta_{t}\\|^{2}\\right]+\\frac{4\\alpha_{t+1}}{\\mu}\\sqrt{\\mathbb{E}\\left[\\|D_{t}\\|_{2}^{4}\\right]\\mathbb{E}\\left[\\|\\theta_{t}\\|_{2}^{4}\\right]}}\\\\ &{\\qquad\\qquad\\qquad\\lesssim\\!\\frac{\\alpha_{t+1}\\mu}{16}\\mathbb{E}\\left[\\|\\delta_{t}\\|^{2}\\right]+\\frac{4d_{z}^{1+2\\varkappa}\\alpha_{t+1}\\beta_{t}}{\\mu},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the first inequality follows by H\u00f6lder\u2019s inequality, the second inequality follows by Young\u2019s inequality, Cauchy-Schwarz inequality, and $\\|Q_{t}\\|_{2}<1$ , and the third inequality follows by (42), and Lemma 5. ", "page_idx": 20}, {"type": "text", "text": "4. Using $\\|Q_{t}\\|_{2}<1$ , $\\|\\Sigma_{Z Y}\\|_{2}=O(1)$ , Cauchy-Schwarz inequality, and Lemma 3, we get, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\alpha_{t+1}\\mathbb{E}\\left[|\\delta_{t}^{\\top}Q_{t}(\\gamma_{t}-\\gamma_{*})^{\\top}\\Sigma_{Z Y}|\\right]}\\\\ &{\\lesssim\\!\\alpha_{t+1}\\mathbb{E}\\left[\\left\\|\\delta_{t}\\right\\|_{2}\\left\\|\\gamma_{t}-\\gamma_{*}\\right\\|_{2}\\right]}\\\\ &{\\leq\\!\\alpha_{t+1}\\sqrt{\\mathbb{E}\\left[\\left\\|\\delta_{t}\\right\\|_{2}^{2}\\right]\\mathbb{E}\\left[\\left\\|\\gamma_{t}-\\gamma_{*}\\right\\|_{2}^{2}\\right]}}\\\\ &{\\leq\\!\\frac{\\sqrt{d_{z}\\beta_{t}}\\alpha_{t+1}}{2}+\\frac{\\sqrt{d_{z}\\beta_{t}}\\alpha_{t+1}\\mathbb{E}\\left[\\left\\|\\delta_{t}\\right\\|_{2}^{2}\\right]}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Combining (31), (32), (33), (34), (35), and Lemma 5, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert\\delta_{t+1}\\Vert_{2}^{2}\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lesssim(1-\\alpha_{t+1}\\mu)\\mathbb{E}\\left[\\|\\delta_{t}\\|^{2}\\right]+4\\alpha_{t+1}^{2}\\beta_{t}d_{z}^{1+2\\varkappa}+4C\\alpha_{t+1}^{2}d_{z}^{4\\varkappa+\\vartheta/2}}\\\\ &{\\quad+\\,2\\alpha_{t+1}\\left(\\mu\\mathbb{E}\\left[\\|\\delta_{t}\\|^{2}\\right]/16+4d_{z}^{1+2\\varkappa}\\beta_{t}/\\mu+\\sqrt{d_{z}\\beta_{t}}/2+\\sqrt{d_{z}\\beta_{t}}\\mathbb{E}\\left[\\|\\delta_{t}\\|_{2}^{2}\\right]/2\\right)}\\\\ &{\\lesssim(1-7\\mu\\alpha_{t+1}/8+\\alpha_{t+1}\\sqrt{d_{z}\\beta_{t}})\\mathbb{E}\\left[\\|\\delta_{t}\\|^{2}\\right]+\\left(8\\alpha_{t+1}\\beta_{t}d_{z}^{1+2\\varkappa}/\\mu+4C\\alpha_{t+1}^{2}d_{z}^{4\\varkappa+\\vartheta/2}\\right)}\\\\ &{\\quad+\\,\\alpha_{t+1}\\sqrt{d_{z}\\beta_{t}}}\\\\ &{\\lesssim(1-3\\mu\\alpha_{t+1}/4)\\mathbb{E}\\left[\\|\\delta_{t}\\|^{2}\\right]+\\left(8\\alpha_{t+1}\\beta_{t}d_{z}^{1+2\\varkappa}/\\mu+4C\\alpha_{t+1}^{2}d_{z}^{4\\varkappa+\\vartheta/2}\\right)+\\alpha_{t+1}\\sqrt{d_{z}\\beta_{t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In the above, the third inequality follows by choosing $\\beta_{t}\\leq\\mu^{2}/(64d_{z})$ , and $\\alpha_{t+1}\\sqrt{d_{z}\\beta_{t}}<1$ . Then, from (37), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\|\\delta_{t}\\|_{2}^{2}\\right]=O\\left(\\beta_{t}d_{z}^{1+2\\varkappa}+\\alpha_{t+1}d_{z}^{4\\varkappa+\\vartheta/2}+\\sqrt{d_{z}\\beta_{t}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Coming back to the proof of Theorem 2, observe that, we can sharpen the bound in (35) using Lemma 6 which allows us to avoid the use of Young\u2019s inequality. This leads t\u221ao the following improved version of the recursion in (37) using which we can improve the term $\\sqrt{d_{z}\\beta_{t}}$ in (29) as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\|\\delta_{t+1}\\|_{2}^{2}\\right]\\lesssim\\!(1-7\\mu\\alpha_{t+1}/8)\\mathbb{E}\\left[\\|\\delta_{t}\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad+\\alpha_{t+1}O\\left(\\beta_{t}d_{z}^{1+2\\varkappa}+\\alpha_{t+1}d_{z}^{4\\varkappa+\\vartheta/2}+\\sqrt{\\alpha_{t+1}\\beta_{t}}d_{z}^{1/2+2\\varkappa+\\vartheta/4}+(\\beta_{t}d_{z})^{3/4}\\right)}\\\\ &{\\qquad\\qquad\\qquad=O\\left(\\beta_{t}d_{z}^{1+2\\varkappa}+\\alpha_{t+1}d_{z}^{4\\varkappa+\\vartheta/2}+\\sqrt{\\alpha_{t+1}\\beta_{t}}d_{z}^{1/2+2\\varkappa+\\vartheta/4}+(\\beta_{t}d_{z})^{3/4}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In fact, this trick can be used repeatedly to sharpen the bound even further as shown in Lemma 7. ", "page_idx": 21}, {"type": "text", "text": "Lemma 7 (Final improved bound on $\\mathbb{E}[\\|\\delta_{t}\\|_{2}^{2}])$ . Let the conditions in Theorem 2 be true. Then using Lemma $^{6}$ , we have, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\|\\delta_{t+1}\\|_{2}^{2}\\right]}\\\\ &{\\lesssim\\!O\\left((d_{z}\\beta_{t})^{1-2^{-r-1}}+\\displaystyle\\sum_{i=0}^{r}\\left(\\alpha_{t+1}^{2^{-i}}\\beta_{t}^{1-2^{-i}}d_{z}^{1+(4\\varkappa+\\vartheta/2-1)2^{-i}}+\\beta_{t}(1+\\alpha_{t+1}^{2^{-i}})d_{z}^{1+2^{1-i}\\varkappa}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $r$ is any non-negative integer. ", "page_idx": 21}, {"type": "text", "text": "Proof of Lemma 7. If we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\|\\delta_{t}\\|^{2}\\right]=O\\left(\\alpha_{t+1}d_{z}^{4\\varkappa+\\vartheta/2}+\\beta_{t}d_{z}^{1+2\\varkappa}+\\sqrt{d_{z}\\beta_{t}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "then from (35), we have, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\big|\\delta_{t}^{\\top}Q_{t}(\\gamma_{t}-\\gamma_{*})^{\\top}\\Sigma_{Z Y}\\big|\\right]\\lesssim\\!\\!\\sqrt{\\mathbb{E}\\left[\\|\\delta_{t}\\|_{2}^{2}\\right]\\mathbb{E}\\left[\\|\\gamma_{t}-\\gamma_{*}\\|_{2}^{2}\\right]}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\!\\!O\\left(\\sqrt{\\alpha_{t+1}\\beta_{t}}d_{z}^{1/2+2\\varkappa+\\vartheta/4}+\\beta_{t}d_{z}^{1+\\varkappa}+(d_{z}\\beta_{t})^{3/4}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, similar to (36), we have, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\|\\delta_{t+1}\\|_{2}^{2}\\right]}\\\\ &{\\lesssim(1-\\alpha_{t+1}\\mu)\\mathbb{E}\\left[\\|\\delta_{t}\\|^{2}\\right]+4\\alpha_{t+1}^{2}\\beta_{t}d_{z}^{1+2\\varkappa}+4C\\alpha_{t+1}^{2}d_{z}^{4\\varkappa+\\phi/2}}\\\\ &{\\quad+\\,2\\alpha_{t+1}\\left(\\mu\\mathbb{E}\\left[\\|\\delta_{t}\\|^{2}\\right]/16+4d_{z}^{1+2\\varkappa}\\beta_{t}/\\mu+\\sqrt{\\alpha_{t+1}\\beta_{t}}d_{z}^{1/2+2\\varkappa+\\phi/4}+\\beta_{t}d_{z}^{1+\\varkappa}+(d_{z}\\beta_{t})^{3/4}\\right)}\\\\ &{\\lesssim(1-7\\mu\\alpha_{t+1}/8)\\mathbb{E}\\left[\\|\\delta_{t}\\|^{2}\\right]}\\\\ &{\\quad+\\,\\alpha_{t+1}O\\left((d_{z}\\beta_{t})^{3/4}+\\displaystyle\\sum_{i=0}^{1}\\left(\\alpha_{t+1}^{2-i}\\beta_{t}^{1-2-i}d_{z}^{1+(4\\varkappa+\\phi/2-1)2^{-i}}+\\beta_{t}(1+\\alpha_{t+1}^{2-i})d_{z}^{1+2^{1-i}\\varkappa}\\right)\\right)}\\\\ &{=\\!\\left((d_{z}\\beta_{t})^{3/4}+\\displaystyle\\sum_{i=0}^{1}\\left(\\alpha_{t+1}^{2-i}\\beta_{t}^{1-2^{-i}}d_{z}^{1+(4\\varkappa+\\phi/2-1)2^{-i}}+\\beta_{t}(1+\\alpha_{t+1}^{2-i})d_{z}^{1+2^{1-i}\\varkappa}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now if we repeat this step $r$ number of times (where $r$ is to be set later), by progressive sharpening we get the following bound. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\|\\delta_{t+1}\\|_{2}^{2}\\right]}\\\\ &{\\lesssim\\!O\\left((d_{z}\\beta_{t})^{1-2^{-r-1}}+\\displaystyle\\sum_{i=0}^{r}\\left(\\alpha_{t+1}^{2^{-i}}\\beta_{t}^{1-2^{-i}}d_{z}^{1+(4\\varkappa+\\vartheta/2-1)2^{-i}}+\\beta_{t}(1+\\alpha_{t+1}^{2^{-i}})d_{z}^{1+2^{1-i}\\varkappa}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Coming back to the proof of Theorem 2, we have that by combining Lemma 4, and Lemma 7, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\Vert\\theta_{t}-\\theta_{*}\\Vert^{2}\\right]\\leq2\\mathbb{E}\\left[\\Vert\\delta_{t}\\Vert^{2}\\right]+2\\mathbb{E}\\left[\\Vert\\tilde{\\theta}_{t}-\\theta_{*}\\Vert^{2}\\right]}\\\\ &{=\\!O\\left((d_{z}\\beta_{t})^{1-2^{-r-1}}+\\displaystyle\\sum_{i=0}^{r}\\left(\\alpha_{t+1}^{2^{-i}}\\beta_{t}^{1-2^{-i}}d_{z}^{1+(4\\varkappa+\\vartheta/2-1)2^{-i}}+\\beta_{t}(1+\\alpha_{t+1}^{2^{-i}})d_{z}^{1+2^{1-i}\\varkappa}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now, in (39), for some arbitrarily small number $\\iota>0$ , choosing ", "page_idx": 22}, {"type": "text", "text": "$\\alpha_{t}=\\operatorname*{min}(0.5d_{z}^{-4\\varkappa-\\vartheta/2}\\lambda_{Z}^{-1}C_{\\gamma}^{-2},0.5(\\|\\gamma_{*}\\|_{2}\\lambda_{Z})^{-2})t^{-1+\\iota/2},\\qquad\\beta_{t}=\\mu^{2}d_{z}^{-1-2\\varkappa}t^{-1+\\iota/2}/128,$ and setting $r=\\lceil\\log_{2}\\left((\\iota/2)^{-1}-1\\right)-1\\rceil$ we get, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|^{2}\\right]=O\\left(\\operatorname*{max}\\left(t^{-1+\\iota},t^{-1+\\iota/2}\\log((\\iota/2)^{-1}-1)\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "E.2 Proof of Lemma 5 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. Using the form of $\\boldsymbol{\\theta}_{*}$ , from (13) we get, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\theta_{t+1}-\\theta_{*}=\\widehat{Q}_{t}(\\theta_{t}-\\theta_{*})+\\alpha_{t+1}\\bigl(\\gamma_{t}-\\gamma_{*}\\bigr)^{\\top}\\Sigma_{Z Y}+\\alpha_{t+1}D_{t}\\theta_{*}+\\alpha_{t+1}\\gamma_{t}\\overset{\\top}\\xi_{Z_{t}}\\gamma_{t}\\bigl(\\theta_{t}-\\theta_{*}\\bigr)}\\\\ {+\\;\\alpha_{t+1}\\gamma_{t}\\overset{\\top}\\xi_{Z_{t}}\\gamma_{t}\\theta_{*}+\\alpha_{t+1}\\gamma_{t}\\overset{\\top}\\xi_{Z_{t}Y_{t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\widehat{Q}_{t}:=\\left(I-\\alpha_{t+1}\\gamma_{t}\\right)^{\\top}\\Sigma_{Z}\\gamma_{t}\\right)=Q_{t}+\\alpha_{t+1}D_{t}$ . Recall that $D_{t}=\\gamma_{*}^{\\top}\\Sigma_{Z}\\gamma_{*}-{\\gamma_{t}}^{\\top}\\Sigma_{Z}\\gamma_{t}$ . By Assum ption 3.3, we have the following bound on $\\|D_{t}\\|_{2}$ . ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|D_{t}\\|_{2}=O(\\lambda_{Z}C_{\\gamma}^{2}d_{z}^{2\\varkappa}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We have the following bound on $\\mathbb{E}\\left[\\|D_{t}\\|_{2}^{4}\\right]$ by Lemma 3. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb E\\left[\\Vert D_{t}\\Vert_{2}^{4}\\right]=\\mathbb E\\left[\\Vert(\\gamma_{*}-\\gamma_{t})^{\\top}\\Sigma_{Z}\\gamma_{*}+\\gamma_{t}^{\\top}\\Sigma_{Z}(\\gamma_{*}-\\gamma_{t})\\Vert_{2}^{4}\\right]=O(d_{z}^{2+4\\varkappa}\\beta_{t}^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "From (40), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\theta_{t+1}-\\theta_{*}\\|_{2}^{2}\\leq(\\theta_{t}-\\theta_{*})^{\\top}\\widehat{Q}_{t}^{2}(\\theta_{t}-\\theta_{*})+3\\alpha_{t+1}^{2}\\|\\gamma_{t}^{\\top}\\xi_{Z_{t}}\\gamma_{t}(\\theta_{t}-\\theta_{*})\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\;2\\alpha_{t+1}(\\theta_{t}-\\theta_{*})^{\\top}\\widehat{Q}_{t}(\\gamma_{t}-\\gamma_{*})^{\\top}\\Sigma_{Z Y}}\\\\ &{\\qquad\\qquad\\qquad+\\;2\\alpha_{t+1}(\\theta_{t}-\\theta_{*})^{\\top}\\widehat{Q}_{t}D_{t}\\theta_{*}+A_{1,t}+A_{2,t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{1,t}=\\!\\!\\alpha_{t+1}^{2}\\big(\\|(\\gamma_{t}-\\gamma_{*})^{\\top}\\Sigma_{Z Y}\\|_{2}^{2}+\\|D_{t}\\theta_{*}\\|_{2}^{2}}\\\\ &{\\qquad~+\\,2\\Sigma_{Z Y}^{\\top}(\\gamma_{t}-\\gamma_{*})D_{t}\\theta_{*}+3\\|{\\gamma_{t}}^{\\top}\\xi_{Z_{t}}\\gamma_{t}\\theta_{*}\\|_{2}^{2}+3\\|{\\gamma_{t}}^{\\top}\\xi_{Z_{t}Y_{t}}\\|_{2}^{2}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{2,t}=2\\alpha_{t+1}(\\widehat{Q}_{t}(\\theta_{t}-\\theta_{*})+\\alpha_{t+1}(\\gamma_{t}-\\gamma_{*})^{\\top}\\Sigma_{Z Y}}\\\\ &{\\qquad\\quad+\\alpha_{t+1}D_{t}\\theta_{*})^{\\top}({\\gamma_{t}}^{\\top}\\xi_{Z_{t}}\\gamma_{t}(\\theta_{t}-\\theta_{*})+{\\gamma_{t}}^{\\top}\\xi_{Z_{t}}\\gamma_{t}\\theta_{*}+{\\gamma_{t}}^{\\top}\\xi_{Z_{t}}{Y_{t}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Define ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{3,t}:=\\!3\\alpha_{t+1}^{2}\\Vert\\gamma_{t}^{\\top}\\xi_{Z_{t}}\\gamma_{t}(\\theta_{t}-\\theta_{*})\\Vert_{2}^{2}+2\\alpha_{t+1}(\\theta_{t}-\\theta_{*})^{\\top}\\widehat{Q}_{t}(\\gamma_{t}-\\gamma_{*})^{\\top}\\Sigma_{Z Y}}\\\\ &{\\qquad\\quad+\\,2\\alpha_{t+1}(\\theta_{t}-\\theta_{*})^{\\top}\\widehat{Q}_{t}D_{t}\\theta_{*}+A_{1,t}+A_{2,t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, choosing $C_{\\gamma}^{2}d_{z}^{2\\varkappa}\\lambda_{Z}\\alpha_{t+1}<1$ , which ensures $\\|\\widehat{Q}_{t}\\|\\leq1$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\theta_{t+1}-\\theta_{*}\\|_{2}^{4}\\leq\\|\\theta_{t}-\\theta_{*}\\|_{2}^{4}+2(\\theta_{t}-\\theta_{*})^{\\top}\\widehat{Q}_{t}^{2}(\\theta_{t}-\\theta_{*})A_{3,t}+A_{3,t}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We now have the following bounds: ", "page_idx": 22}, {"type": "text", "text": "1. Using Assumption 3.1, and Assumption 3.3, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha_{t+1}^{4}\\mathbb{E}\\left[\\|\\gamma_{t}^{\\top}\\xi_{Z_{t}}\\gamma_{t}(\\theta_{t}-\\theta_{*})\\|_{2}^{4}\\right]\\lesssim d_{z}^{8\\varkappa+\\vartheta}\\alpha_{t+1}^{4}\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{4}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "2. We have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[((\\theta_{t}-\\theta_{*})^{\\top}\\widehat{Q}_{t}(\\gamma_{t}-\\gamma_{*})^{\\top}\\Sigma_{Z Y})^{2}\\right]}\\\\ &{\\lesssim\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|^{2}\\|\\gamma_{t}-\\gamma_{*}\\|^{2}\\right]}\\\\ &{\\leq\\sqrt{\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{4}\\right]\\mathbb{E}\\left[\\|\\gamma_{t}-\\gamma_{*}\\|_{2}^{4}\\right]}}\\\\ &{\\leq d_{z}\\beta_{t}\\left(1+\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{4}\\right]\\right)/2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where, the first inequality follows by $\\|\\widehat{Q}_{t}\\|_{2}=O(1)$ , and $\\|\\Sigma_{Z Y}\\|_{2}=O(1)$ .\u221a The second inequality follows by Cauchy-Schwarz inequality. The last inequality follows by ${\\sqrt{a b}}\\,\\leq\\,(a+b)/2$ , and Lemma 3. ", "page_idx": 23}, {"type": "text", "text": "3. We have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[((\\theta_{t}-\\theta_{*})^{\\top}\\widehat{Q}_{t}D_{t}\\theta_{*})^{2}\\right]}\\\\ &{\\lesssim\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{2}\\|D_{t}\\|_{2}^{2}\\right]}\\\\ &{\\leq\\sqrt{\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{4}\\right]\\mathbb{E}\\left[\\|D_{t}\\|_{2}^{4}\\right]}}\\\\ &{\\lesssim d_{z}^{1+2\\varkappa}\\beta_{t}\\left(1+\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{4}\\right]\\right)/2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where, the first inequality follows by $\\|\\widehat{Q}_{t}\\|_{2}=O(1)$ , and $\\lVert{\\boldsymbol{\\theta}}_{*}\\rVert_{2}=O(1)$ . The second inequality follows by Cauchy-Schwarz inequality. The last inequality follows by ${\\sqrt{a b}}\\leq(a+b)/2$ , and (42). ", "page_idx": 23}, {"type": "text", "text": "4. Using Assumption 3.1, Assumption 3.3, (42), and Lemma 3, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[A_{1,t}^{2}\\right]=O\\left(d_{z}^{8\\varkappa+\\vartheta}\\alpha_{t+1}^{4}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "5. Using Young\u2019s inequality, Assumption 3.1, Assumption 3.3, Lemma 3, $\\|\\Sigma_{Z Y}\\|_{2}=O(1)$ , $\\lVert\\theta_{*}\\rVert_{2}=$ $O(1)$ , and (42), we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[A_{2,t}^{2}\\right]\\leq2\\alpha_{t+1}^{2}\\mathbb{E}\\left[\\|\\widehat{Q}_{t}(\\theta_{t}-\\theta_{*})+\\alpha_{t+1}(\\gamma_{t}-\\gamma_{*})^{\\top}\\Sigma_{Z Y}+\\alpha_{t+1}D_{t}\\theta_{*}\\|_{2}^{4}\\right]}\\\\ &{\\quad\\quad\\quad\\quad+\\,2\\alpha_{t+1}^{2}\\mathbb{E}\\left[\\|\\gamma_{t}^{\\top}\\xi_{Z_{t}}\\gamma_{t}(\\theta_{t}-\\theta_{*})+\\gamma_{t}^{\\top}\\xi_{Z_{t}}\\gamma_{t}\\theta_{*}+\\gamma_{t}^{\\top}\\xi_{Z_{t}Y_{t}}\\|_{2}^{4}\\right]}\\\\ &{\\quad\\quad\\quad\\lesssim\\!\\alpha_{t+1}^{2}d_{z}^{8\\varkappa+\\vartheta}\\!\\left(1+\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{4}\\right]\\!\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "6. Using $\\|\\widehat{Q}_{t}\\|_{2}=O(1)$ , Assumption 3.1, and Assumption 3.3, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha_{t+1}^{2}\\mathbb{E}\\left[(\\theta_{t}-\\theta_{*})^{\\top}\\widehat{Q}_{t}^{2}(\\theta_{t}-\\theta_{*})\\lVert\\gamma_{t}\\^{\\top}\\xi_{Z_{t}}\\gamma_{t}(\\theta_{t}-\\theta_{*})\\rVert_{2}^{2}\\right]\\lesssim\\!\\alpha_{t+1}^{2}d_{z}^{4\\varkappa+\\vartheta/2}\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{4}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "7. We have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\alpha_{t+1}\\mathbb{E}\\left[|(\\theta_{t}-\\theta_{*})^{\\top}\\widehat{Q}_{t}^{2}(\\theta_{t}-\\theta_{*})(\\theta_{t}-\\theta_{*})^{\\top}\\widehat{Q}_{t}(\\gamma_{t}-\\gamma_{*})^{\\top}\\Sigma_{Z Y}\\right|\\right]}\\\\ &{\\lesssim\\!\\alpha_{t+1}\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{3}\\|\\gamma_{t}-\\gamma_{*}\\|_{2}\\right]}\\\\ &{\\leq\\!\\alpha_{t+1}\\left(\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{4}\\right]\\right)^{3/4}\\left(\\mathbb{E}\\left[\\|\\gamma_{t}-\\gamma_{*}\\|_{2}^{4}\\right]\\right)^{1/4}}\\\\ &{\\leq\\!\\alpha_{t+1}\\sqrt{d_{z}\\beta_{t}}\\left(\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{4}\\right]\\right)^{3/4}}\\\\ &{\\leq\\!\\frac{3\\alpha_{t+1}\\sqrt{d_{z}\\beta_{t}}}{4}\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{4}\\right]+\\frac{\\alpha_{t+1}\\sqrt{d_{z}\\beta_{t}}}{4},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where, the first inequality follows by $\\|\\widehat{Q}_{t}\\|_{2}=O(1)$ , and $\\|\\Sigma_{Z Y}\\|_{2}=O(1)$ , the second inequality follows by Cauchy-Schwarz inequality, the third inequality follows by Lemma 3 and the fourth inequality follows by Young\u2019s inequality. ", "page_idx": 23}, {"type": "text", "text": "8. Similar to (53), we have, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\alpha_{t+1}\\mathbb{E}\\left[|(\\theta_{t}-\\theta_{*})^{\\top}\\widehat{Q}_{t}^{2}(\\theta_{t}-\\theta_{*})(\\theta_{t}-\\theta_{*})^{\\top}\\widehat{Q}_{t}D_{t}\\theta_{*}|\\right]}\\\\ &{\\leq\\!\\frac{3d_{z}^{1/2+\\varkappa}\\alpha_{t+1}\\sqrt{\\beta_{t}}}{4}\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{4}\\right]+\\frac{d_{z}^{1/2+\\varkappa}\\alpha_{t+1}\\sqrt{\\beta_{t}}}{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "9. Using $\\|\\widehat{Q}_{t}\\|_{2}=O(1)$ , Cauchy-Schwarz inequality, (50), and Young\u2019s inequality, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[(\\theta_{t}-\\theta_{*})^{\\top}\\widehat{Q}_{t}^{2}(\\theta_{t}-\\theta_{*})A_{1,t}\\right]}\\\\ &{\\leq\\!\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{2}A_{1,t}\\right]}\\\\ &{\\leq\\!\\sqrt{\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{4}\\right]{\\mathbb{E}\\left[A_{1,t}^{2}\\right]}}}\\\\ &{\\lesssim\\!d_{z}^{4\\varkappa+\\vartheta/2}\\alpha_{t+1}^{2}\\left(1+\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{4}\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "10. By Assumption 2.2, we have, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t}\\left[(\\theta_{t}-\\theta_{*})^{\\top}\\widehat{Q}_{t}^{2}(\\theta_{t}-\\theta_{*})A_{2,t}\\right]=0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now using Jensen\u2019s inequality, and combining (47), (48), (49), (50), and (51), we have, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{z}\\left[A_{3,t}^{2}\\right]\\leq45\\alpha_{t+1}^{4}\\mathbb{E}\\left[\\|\\gamma_{t}^{\\top}\\xi_{Z_{t}}\\gamma_{t}(\\theta_{t}-\\theta_{*})\\|_{2}^{4}\\right]+20\\alpha_{t+1}^{2}\\mathbb{E}\\left[\\left((\\theta_{t}-\\theta_{*})^{\\top}\\widehat{Q}_{t}(\\gamma_{t}-\\gamma_{*})^{\\top}\\Sigma_{Z Y})^{2}\\right]}\\\\ &{\\qquad\\qquad+20\\alpha_{t+1}^{2}\\mathbb{E}\\left[((\\theta_{t}-\\theta_{*})^{\\top}\\widehat{Q}_{t}D_{t}\\theta_{*})^{2}\\right]+5\\mathbb{E}\\left[A_{1,t}^{2}\\right]+5\\mathbb{E}\\left[A_{2,t}^{2}\\right]}\\\\ &{\\lesssim\\alpha_{t+1}^{4}d_{z}^{\\vartheta_{7}+8\\kappa}\\mathbb{E}\\left[\\|\\theta_{t}-\\theta^{*}\\|_{2}^{4}\\right]+d_{z}\\alpha_{t+1}^{2}\\beta_{t}\\left(1+\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{4}\\right]\\right)}\\\\ &{\\quad\\,\\,\\,+\\alpha_{t+1}^{2}d_{z}^{1+2\\kappa}\\beta_{t}\\left(1+\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{4}\\right]\\right)+d_{z}^{8\\kappa+\\vartheta}\\alpha_{t+1}^{4}+\\alpha_{t+1}^{2}d_{z}^{8\\kappa+\\vartheta}(1+\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{4}\\right])}\\\\ &{\\lesssim\\alpha_{t+1}^{2}d_{z}^{8\\kappa+\\vartheta}\\left(1+\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{4}\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combining (52), (53), (54), (55), and (56), we get, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[(\\theta_{t}-\\theta_{*})^{\\top}\\widehat{Q}_{t}^{2}(\\theta_{t}-\\theta_{*})A_{3,t}\\right]}\\\\ &{\\lesssim\\!\\alpha_{t+1}^{2}d_{z}^{4\\varkappa+\\vartheta/2}\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{4}\\right]+\\frac{3\\alpha_{t+1}\\sqrt{d_{z}\\beta_{t}}}{4}\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{4}\\right]+\\frac{\\alpha_{t+1}\\sqrt{d_{z}\\beta_{t}}}{4}}\\\\ &{\\quad+\\frac{3d_{z}^{1/2+\\varkappa}\\alpha_{t+1}\\sqrt{\\beta_{t}}}{4}\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{4}\\right]+\\frac{d_{z}^{1/2+\\varkappa}\\alpha_{t+1}\\sqrt{\\beta_{t}}}{4}+d_{z}^{4\\varkappa+\\vartheta/2}\\alpha_{t+1}^{2}\\left(1+\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{4}\\right]\\right)}\\\\ &{\\lesssim\\!(\\alpha_{t+1}^{2}d_{z}^{4\\varkappa+\\vartheta/2}+\\alpha_{t+1}\\sqrt{\\beta_{t+1}}d_{z}^{1/2+\\varkappa})(1+\\|\\theta_{t}-\\theta^{*}\\|_{2}^{4}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combining (46), (57), and (58), we have, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\|\\theta_{t+1}-\\theta_{*}\\|_{2}^{4}\\right]\\lesssim\\!(1+\\alpha_{t+1}^{2}d_{z}^{8\\varkappa+\\vartheta}+\\alpha_{t+1}\\sqrt{\\beta_{t+1}}d_{z}^{1/2+\\varkappa})\\left(1+\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|_{2}^{4}\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now choosing $\\alpha_{t},\\beta_{t}$ such that $\\alpha_{t}\\leq d_{z}^{-4\\varkappa-\\vartheta/2}$ , and $\\begin{array}{r}{\\sum_{t=1}^{\\infty}(\\alpha_{t+1}^{2}+\\alpha_{t+1}\\sqrt{\\beta_{t+1}})<\\infty}\\end{array}$ , we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\lVert\\theta_{t}-\\theta_{*}\\rVert_{2}^{4}\\right]\\leq M,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for some constant $0\\leq M<\\infty$ . ", "page_idx": 24}, {"type": "text", "text": "E.3 Comment on the convergence of (11) ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We now discuss the convergence properties of the update sequence (11), which we refer to as the conditional stochastic optimization (CSO) based updates, which we restate below: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\theta_{t+1}=\\theta_{t}-\\alpha_{t+1}\\gamma_{t}^{\\top}Z_{t}\\big(X_{t}^{\\top}\\theta_{t}-Y_{t}\\big),\\qquad\\gamma_{t+1}=\\gamma_{t}-\\beta_{t+1}Z_{t}\\big(Z_{t}^{\\top}\\gamma_{t}-X_{t}^{\\top}\\big).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Similar to (40), for the above updates, we have the following expansion: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\theta_{t+1}-\\theta_{*}=\\widehat{Q}_{t}\\bigl(\\theta_{t}-\\theta_{*}\\bigr)+\\alpha_{t+1}\\bigl(\\gamma_{t}-\\gamma_{*}\\bigr)^{\\top}\\Sigma_{Z Y}+\\alpha_{t+1}D_{t}\\theta_{*}+\\alpha_{t+1}\\gamma_{t}\\,\\mathrm{'}\\xi_{Z_{t}}\\gamma_{*}\\bigl(\\theta_{t}-\\theta_{*}\\bigr)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n+\\alpha_{t+1}{\\gamma_{t}}^{\\top}\\xi_{Z_{t}}{\\gamma_{*}}\\theta_{*}+\\alpha_{t+1}{\\gamma_{t}}^{\\top}\\xi_{Z_{t}Y_{t}}-\\alpha_{t+1}{\\gamma_{t}}^{\\top}Z_{t}\\epsilon_{2,t}^{\\top}\\theta_{t},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\begin{array}{r}{\\xi_{Z_{t}}=\\Sigma_{Z}-Z_{t}Z_{t}^{\\top},\\xi_{Z_{t}Y_{t}}=\\Sigma_{Z Y}-Z_{t}Y_{t},\\widehat Q_{t}:=\\left(I-\\alpha_{t+1}\\gamma_{t}^{\\top}\\Sigma_{Z}\\gamma_{*}\\right)=Q_{t}+\\alpha_{t+1}D_{t},}\\end{array}$ , and $D_{t}=(\\gamma_{*}-\\gamma_{t})\\Sigma_{Z}\\gamma_{*}$ . ", "page_idx": 25}, {"type": "text", "text": "Recall that the reason for the initial divergence of the updates in (11) are the potential negative eigenvalues of $\\gamma_{t}^{\\top}\\Sigma_{Z}\\gamma_{*}$ . Here we will show that if $\\gamma_{t}^{\\top}\\Sigma_{Z}^{\\bar{}}\\gamma_{*}$ is positive semi-definite or $\\gamma_{t}$ is close enough to $\\gamma_{*}$ such that the negative eigenvalues (if any) are not too large in absolute values, then the updates in (11) indeed exhibit the same convergence rate as Algorithm 2. ", "page_idx": 25}, {"type": "text", "text": "Assumption E.1. Let either of the following two conditions be true. For all $t\\geq t_{0}$ , ", "page_idx": 25}, {"type": "text", "text": "Note that Condition 1 of Assumption E.1 is an idealized condition which is difficult to ensure for all $t$ in reality. But of course if this is true, then $\\gamma_{t}\\Sigma_{Z}\\gamma_{*}$ does not have a negative eigenvalue to cause divergence and the proof then follows exactly like Lemma 5. ", "page_idx": 25}, {"type": "text", "text": "Hence, we will focus on the more realistic Condition 2 of Assumption E.1 which holds true almost surely [PJ92]. Since we are interested in the asymptotic rate of convergence of CSO updates (due to the requirement of Assumption E.1), we will only concentrate on the iterations $t\\geq t_{0}$ . In this case, the proof steps are similar to Theorem 2 except for two major differences, that we discuss below. ", "page_idx": 25}, {"type": "text", "text": "Difference 1: Potential negative definiteness of $\\gamma_{t}^{\\top}\\Sigma_{Z}\\gamma_{*}$ : ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Under Condition 2, $\\gamma_{t}^{\\top}\\Sigma_{Z}\\gamma_{*}$ can indeed be negative definite. In general, if $\\gamma_{t}{}^{\\top}\\Sigma_{Z}\\gamma_{*}$ is negative definite then that is undesirable as we explain Section 3. In terms of the proof, we can no longer write $(\\theta_{t}-\\theta^{*})^{\\top}\\widehat{Q}_{t}^{\\top}\\widehat{Q}_{t}(\\theta_{t}-\\theta^{*})\\leq\\|\\theta_{t}-\\theta_{*}\\|^{\\frac{1}{2}}$ (which was possible to do in (43) in the proof of Lemma 5). Subsequently,  (46) breaks down. But we will show that under Condition 2 the negative eigenvalues are not too large in terms of absolute values. Specifically, we can write, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(\\theta_{t}-\\theta^{*})^{\\top}\\widehat{Q}_{t}^{\\top}\\widehat{Q}_{t}(\\theta_{t}-\\theta^{*})}\\\\ &{=\\!(\\theta_{t}-\\theta^{*})^{\\top}(Q_{t}^{2}+\\alpha_{t+1}Q_{t}^{\\top}D_{t}+\\alpha_{t+1}D_{t}^{\\top}Q_{t}+\\alpha_{t+1}^{2}D_{t}^{\\top}D_{t})(\\theta_{t}-\\theta^{*})}\\\\ &{\\leq\\!(1+2\\alpha_{t+1}\\|D_{t}\\|)\\|\\theta_{t}-\\theta_{*}\\|^{2}+\\alpha_{t+1}^{2}\\|D_{t}\\|^{2}\\|\\theta_{t}-\\theta_{*}\\|^{2}}\\\\ &{\\leq\\!(1+2\\alpha_{t+1}\\sqrt{d_{z}\\beta_{t}})\\|\\theta_{t}-\\theta_{*}\\|^{2}+\\alpha_{t+1}^{2}\\|D_{t}\\|^{2}\\|\\theta_{t}-\\theta_{*}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The term $\\alpha_{t+1}^{2}\\|D_{t}\\|^{2}\\|\\theta_{t}-\\theta_{*}\\|^{2}$ is of the order of $A_{3,t}$ defined in (45). Now $\\alpha_{t+1}\\sqrt{d_{z}\\beta_{t}}$ is small enough in the sense that we choose the stepsizes such that $\\begin{array}{r}{\\sum_{t=1}^{\\infty}(\\alpha_{t+1}^{2}+\\alpha_{t+1}\\sqrt{\\beta_{t}})<\\infty}\\end{array}$ . Using this one can now show a similar bound as (59) and consequently show E $\\left[\\lVert\\boldsymbol{\\theta}_{t}-\\boldsymbol{\\theta}_{*}\\rVert^{4}\\right]$ is bounded. ", "page_idx": 25}, {"type": "text", "text": "Now let us see what happens in the absence of Condition 2. Here one could use the fact $(1+$ $2\\alpha_{t+1}\\|D_{t}\\|)\\lesssim(1+2C_{\\gamma}\\alpha_{t+1}d_{z}^{\\varkappa})$ which is too big. Recall that we want something at least of the order of $\\alpha_{t+1}\\sqrt{\\beta_{t}}$ to show that $\\theta_{t}$ sequence is bounded. One could also try to use the fact that $\\mathbb{E}\\left[\\Vert D_{t}\\Vert\\right]$ is small by Lemma 3. But since $D_{t}$ and $\\theta_{t}$ are interdependent, one needs to decouple them. One way to do this would be to use Cauchy-Shwarz inequalityas shown below. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\|D_{t}\\|\\|\\theta_{t}-\\theta_{*}\\|^{2}\\right]\\leq\\sqrt{\\mathbb{E}\\left[\\|D_{t}\\|^{2}\\right]\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|^{4}\\right]}\\lesssim\\sqrt{d_{z}\\beta_{t}\\mathbb{E}\\left[\\|\\theta_{t}-\\theta_{*}\\|^{4}\\right]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "But that leads to the presence of E $\\left[\\lVert\\boldsymbol{\\theta}_{t}-\\boldsymbol{\\theta}_{*}\\rVert^{4}\\right]$ in (43) which is potentially problematic due to the fact that on the left-hand side we have E $\\left[\\lVert{\\theta}_{t+1}-{\\theta}_{*}\\rVert^{2}\\right]$ . ", "page_idx": 25}, {"type": "text", "text": "Difference 2: Presence of additional error term $\\alpha_{t+1}\\gamma_{t}{}^{\\top}Z_{t}\\epsilon_{2,t}{}^{\\top}\\theta_{t}$ : ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "When comparing (12) with (40), yet another crucial difference is the presence of the term $\\alpha_{t+1}{\\gamma_{t}}^{\\top}Z_{t}\\dot{\\epsilon}_{2,t}^{\\top}\\overline{{\\theta}}_{t}$ . We will show by the following observations that this error term gets absorbed by other terms already present in (40) without affecting the convergence rate. Specifically, the following holds. ", "page_idx": 25}, {"type": "text", "text": "1. Using the independence between $Z$ , and $\\epsilon_{2,t}$ , and by Assumption 2.2, we have, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}[(\\widehat{Q}_{t}(\\theta_{t}-\\theta_{*})+\\alpha_{t+1}(\\gamma_{t}-\\gamma_{*})^{\\top}\\Sigma_{Z Y}+\\alpha_{t+1}D_{t}\\theta_{*}+\\alpha_{t+1}\\gamma_{t}^{\\top}\\xi_{Z_{t}}\\gamma_{*}(\\theta_{t}-\\theta_{*})}\\\\ &{\\ +\\,\\alpha_{t+1}\\gamma_{t}^{\\top}\\xi_{Z_{t}}\\gamma_{*}\\theta_{*})^{\\top}\\gamma_{t}^{\\top}Z_{t}\\epsilon_{2,t}^{\\top}\\overline{{\\theta}}_{t}]=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "2. We also have that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\alpha_{t+1}^{2}\\mathbb{E}_{t}\\left[({\\gamma_{t}}^{\\top}\\xi_{Z_{t}Y_{t}})^{\\top}{\\gamma_{t}}^{\\top}Z_{t}\\epsilon_{2,t}^{\\top}\\theta_{t}\\right]}\\\\ &{=\\!\\alpha_{t+1}^{2}({\\gamma_{t}}^{\\top}\\Sigma_{Z}\\gamma_{t}\\Vert\\theta_{*}\\Vert^{2}+{\\gamma_{t}}^{\\top}\\Sigma_{Z}\\gamma_{t}\\theta_{*}^{\\top}(\\theta_{t}-\\theta_{*}))}\\\\ &{\\le\\!\\alpha_{t+1}^{2}({\\gamma_{t}}^{\\top}\\Sigma_{Z}\\gamma_{t}\\Vert\\theta_{*}\\Vert^{2}+\\Vert{\\gamma_{t}}^{\\top}\\Sigma_{Z}\\gamma_{t}(\\theta_{t}-\\theta_{*})\\Vert^{2}+\\Vert\\theta_{*}\\Vert^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This shows that the above term is of the same order as $A_{1,t}$ and $A_{3,t}$ defined in (44), and (45). 3. Finally, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\alpha_{t+1}^{2}\\mathbb{E}_{t}\\left[\\|\\gamma_{t}^{\\top}Z_{t}\\epsilon_{2,t}^{\\top}\\theta_{t}\\|^{2}\\right]\\lesssim\\alpha_{t+1}^{2}(\\|\\gamma_{t}\\|^{2}\\|\\theta_{t}-\\theta_{*}\\|^{2}+\\|\\gamma_{t}\\|^{2}\\|\\theta_{*}\\|^{2}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "So this term is of the order of $A_{3,t}$ as well. ", "page_idx": 26}, {"type": "text", "text": "Combining the above facts and following similar procedure as the proof of Theorem 2, one can show that the CSO updates achieve a similar rate under additional Assumption E.1. ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Sections 2, 3, and 4. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We discuss some limitations of our analysis and some future directions in the end of Section 2. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Sections 2, 3, and the Appendix. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Sections 4, C.1, and C.2. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Sections 4, C.1, and C.2. We also provide the code in the main supplemental material. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Sections 4, C.1, and C.2. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Section 4. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Sections 4, C.1, and C.2. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Sections 2 and 3. Our research conforms in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Sections 2 and 3. Our work is mainly about theory and algorithm design, so there is no societal impact of the work performed. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Sections 2 and 3. Our work is mainly about theory and algorithm design, and poses no such risks. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Sections 4, C.1, and C.2. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 31}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Sections 4, C.1, and C.2. The code is in the supplementary material. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Sections 2 and 3. Our paper is mainly about theory and algorithm design, and does not involve crowdsourcing nor research with human subjects. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Sections 2 and 3. Our paper is mainly about theory and algorithm design, and does not involve crowdsourcing nor research with human subjects. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]