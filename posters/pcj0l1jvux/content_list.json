[{"type": "text", "text": "Hamba: Single-view 3D Hand Reconstruction with Graph-guided Bi-Scanning Mamba ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Haoye Dong\u2217,\u2020 Aviral Chharia\u2217 Wenbo Gou\u2217 Francisco Vicente Carrasco Fernando De la Torre Carnegie Mellon University {haoyed, achharia, wgou, fvicente, ftorre}@andrew.cmu.edu https://humansensinglab.github.io/Hamba/ ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "3D Hand reconstruction from a single RGB image is challenging due to the articulated motion, self-occlusion, and interaction with objects. Existing SOTA methods employ attention-based transformers to learn the 3D hand pose and shape, yet they do not fully achieve robust and accurate performance, primarily due to inefficiently modeling spatial relations between joints. To address this problem, we propose a novel graph-guided Mamba framework, named Hamba, which bridges graph learning and state space modeling. Our core idea is to reformulate Mamba\u2019s scanning into graph-guided bidirectional scanning for 3D reconstruction using a few effective tokens. This enables us to efficiently learn the spatial relationships between joints for improving reconstruction performance. Specifically, we design a Graph-guided State Space (GSS) block that learns the graph-structured relations and spatial sequences of joints and uses $88.5\\%$ fewer tokens than attention-based methods. Additionally, we integrate the state space features and the global features using a fusion module. By utilizing the GSS block and the fusion module, Hamba effectively leverages the graph-guided state space features and jointly considers global and local features to improve performance. Experiments on several benchmarks and in-the-wild tests demonstrate that Hamba significantly outperforms existing SOTAs, achieving the PA-MPVPE of $5.3\\mathrm{mm}$ and $\\mathrm{F@15mm}$ of 0.992 on FreiHAND. At the time of this paper\u2019s acceptance, Hamba holds the top position, Rank 1, in two competition leaderboards1 on 3D hand reconstruction. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "3D Hand reconstruction has numerous applications across multiple fields, which include robotics, animation, human-computer interaction, and AR/VR [11, 34, 71, 18, 103]. However, reconstructing 3D hands from a single RGB image without body context or camera parameters remains a difficult challenge in computer vision. Recent works primarily explored transformers [14, 19, 51, 52, 70, ", "page_idx": 0}, {"type": "text", "text": "95, 73, 45, 55] for this task and achieved SOTA performance by utilizing attention mechanism. METRO [51] introduced a multi-layer transformer, using self-attention to learn vertex-vertex and vertex-joint relations. MeshGraphormer [52] integrated graph convolutions with a transformer to further enhance the reconstruction performance. Recently, HaMeR [70] designed a ViT-based model [19], using ViTPose [95] weights and large datasets to achieve better performance. ", "page_idx": 0}, {"type": "text", "text": "However, the above models fail to reconstruct a robust mesh in challenging in-the-wild scenarios that have occlusions, truncation, and hand-hand or hand-object interactions (See Figure 5 for visual comparison). This is partially due to a lack of accurate modeling of spatial relations among hand joints. Secondly, transformer-based methods [19, 51, 52, 70, 95] require a large number of tokens for reconstruction, and applying attention to all image tokens does not efficiently model the joint spatial sequences (i.e., the spatial relationship between joints), which often results in an inaccurate 3D hand mesh in real-world scenarios. ", "page_idx": 0}, {"type": "image", "img_path": "pCJ0l1JVUX/tmp/22d06f70d4e325352fd58aa44ce3b70c99c1044cb81c9de52101c2a108f52263.jpg", "img_caption": ["Figure 1: In-the-wild visual results of Hamba. Hamba achieves significant performance in various in-the-wild scenarios, including hand interaction with objects or hands, different skin tones, different angles, challenging paintings, and vivid animations. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address these challenges, we propose Hamba, a novel Mamba-based [26] model that employs graph learning [44, 104] and state space modeling [26] for robust 3D hand mesh reconstruction. Mamba is a new state space modeling method with global receptive field capability. Most Mambabased models [3, 26, 33, 49, 88, 89, 102] are designed for long-range data, and few studies [48, 78] have adapted Mamba for 3D vision tasks. In this work, we explore Mamba\u2019s potential for the 3D hand reconstruction task. We found that directly applying Mamba for 3D hand reconstruction results in inaccurate meshes due to its unidirectional scanning and the lack of specific design for 3D hand reconstruction. To tackle this challenge, we propose a Graph-guided Bidirectional Scan (GBS) to effectively capture the semantic and spatial relation between joints, as shown in Figure 2(c). Besides, the transformer\u2019s attention requires calculating correlation among all tokens and introduces unnecessary background correlations, while our proposed GBS uses $88.5\\%$ fewer tokens (see Section 3.2 for more details). Secondly, though Mamba-based models [3, 26, 33, 49, 88, 102] excel in modeling long-range sequences, they are not proficient at capturing the local-relation information (in our case, the \u2018semantics\u2019 between hand joints). Since graph learning has the capability to effectively capture node relations, we integrate graph convolutions into state space modeling, significantly enhancing the representation by considering the intricate hand joint relations. ", "page_idx": 1}, {"type": "image", "img_path": "pCJ0l1JVUX/tmp/221ab48f6093792ba1cd44324750817cab865dba076f7370db376e78aef385e2.jpg", "img_caption": ["Figure 2: Motivation. Visual comparisons of different scanning flows. (a) Attention methods compute the correlation across all patches leading to a very high number of tokens. (b) Bidirectional scans follow two paths, resulting in less complexity. (c) The proposed graph-guided bidirectional scan (GBS) achieves effective state space modeling leveraging graph learning with a few effective tokens (illustrated as scanning by two snakes: forward and backward scanning snakes). "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In particular, to effectively leverage state space modeling (SSM) and graph learning capabilities for 3D hand reconstruction, we first carefully design a Token Sampler (TS) under guidance with hand joints predicted by Joint Regressor (JR), then feed sampled token into the Graph-guided State Space block (GSS) under the Graph-guided Bidirectional Scan (GBS). Lastly, we introduce a fusion module to integrate the state space tokens and global features to further improve performance. As shown in Figure 1, Hamba achieves significant visual performance in challenging scenarios. We summarize our contributions as follows: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We are the first to incorporate graph learning and state space modeling (SSM) for reconstructing robust 3D hand mesh. Our key idea is to reformulate the Mamba scanning into graph-guided bidirectional scanning for 3D reconstruction using a few effective tokens. \u2022 We propose a simple yet effective Graph-guided State Space (GSS) block to capture structured relations between hand joints using graph convolution layers and Mamba blocks. \u2022 We introduce a token sampler that effectively boosts performance. A fusion module is also introduced to further enhance performance by integrating state space tokens and global features. \u2022 Extensive experiments on multiple challenging benchmarks demonstrate Hamba\u2019s superiority over current SOTAs, achieving impressive performance for in-the-wild scenarios. ", "page_idx": 2}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3D Hand Reconstruction. Multiple approaches have been proposed to reconstruct 3D hand mesh [2, 41, 62, 66, 67, 77, 80, 83, 85], with most works leveraging the MANO [75] parametric representation of the 3D hand. Zhang et al. [101] utilized a CNN encoder to iteratively regress the hand mesh based on heatmaps under 2D, 3D, silhouette, and geometric constraints. I2L-MeshNet [63] proposed line pixel-based 1D heatmaps for estimating joint locations and regressing MANO parameters, while HandAR [81] estimated parameters through three stages: joint, mesh, and a refining stage to combine previous features. The joint stage applies a multitask decoder to predict both hand joints and the segmentation mask. MeshGraphormer [52] introduced a graph residual block into the transformer to enhance the spatial structure. HaMeR [70] showed that a simple but large transformer-based architecture trained on a large dataset can achieve SOTA performance. SimpleHand [107] sampled tokens with UV predictions on a high-resolution feature map, cooperating with a cascade upsampling decoder. They further compare different combinations of token generation strategies are compared, including global feature, grid sampling, keypoint sampling, $4\\times$ upsampling feature map, and coarsemesh-guided point sampling. Recently, HHMR [47] proposed a graph diffusion model to learn a prior of gestures and inpaint the occluded hand portion. To further enhance performance, we propose the graph-guided state space model to leverage joint relations and capture spatial joint sequences. ", "page_idx": 2}, {"type": "text", "text": "State Space Models (SSMs). State space was originally elaborated in Kalman flitering [39] that described states and transitions with first-order differential equations. Structured State Space Sequence (S4) models [27, 28] have the capability to model dependencies. Recently, Mamba [26] further improved the S4 models by expanding their fixed projection matrices linearly with the input sequence length. Many recent works have adapted Mamba for visual learning, leveraging its global receptive field and dynamic weights. Liu et al. [57] and Yang et al. [108] used Mamba for classification, segmentation, and object detection tasks. To effectively capture the spatial relations, they scanned the input image patches forward and backward horizontally. VMamba [57] further added two vertical directions creating a cross-scan. Zhang et al. [102] designed a mamba model for motion generation, scanning unidirectionally along the temporal sequence and bidirectionally along channel dimensions in a hierarchy. Behrouz et al. [3] and Wang et al. [89] designed Graph-mamba to address traditional graph representation learning tasks, enhancing long-range context learning using Mamba blocks. Hamba makes the first attempt to adapt Mamba and graph learning to solve 3D hand reconstruction. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Proposed Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We propose a novel Mamba-based method that incorporates graph learning and state space modeling to learn the joint relations from the joint spatial sequence (Figure 3). First, we introduce the concept of state space models (SSMs). Next, we provide the detailed principle of the proposed Token Sampler (TS), Graph-guided Bidirectional Scan (GBS), and Graph-guided State Space (GSS) modules. ", "page_idx": 3}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "S6 Models. Selective Scan Structured State Space Sequence (S6) models [26] is a category of sequence models that have demonstrated superior ability in handling sequences. These models are primarily an extension of the previously proposed S4 models [27], which maps an input sequence $x(t)\\in\\mathbb{R}\\to y(t)\\in\\mathbb{R}$ , through the latent state $h(t)\\in\\mathbb{R}^{N}$ , following ordinary linear differential equations (Eq. 1), where $A\\in\\mathbb{R}^{N\\times N}$ , $B\\in\\mathbb{R}^{N\\times1}$ , $\\overset{\\,\\mathrm{~\\large~c~}}{C}\\in\\mathbb{R}^{1\\times N}$ and $D\\in\\mathbb{R}^{1}$ are the weighting parameters. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r l}&{h^{\\prime}(t)=A h(t)+B x(t),\\qquad}&&{(1)\\qquad}&&{h_{t}=\\overline{{A}}h_{t-1}+\\overline{{B}}x_{t},}\\\\ &{y(t)=C h(t)+D x(t),\\qquad}&&{(1)\\qquad}&&{y_{t}=C h(t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For practical computation, these continuous dynamical systems are discretized (Eq. 2). This is achieved by using the zero-order hold (ZOH) discretization rule (Eq. 3). ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\overline{{{\\cal A}}}=\\exp(\\Delta{\\cal A}),\\;\\;\\;\\overline{{{\\cal B}}}=(\\Delta{\\cal A})^{-1}(\\exp(\\Delta{\\cal A})-I)\\cdot\\Delta{\\cal B},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Delta$ represents the discrete step size. Since both the weighting parameters and discretizing rules are fixed over time, S4 models can be viewed as linear time invariance systems. Mamba [26] further expands S4 models\u2019 projection matrices to scan the entire input sequence through a selective scan. ", "page_idx": 3}, {"type": "text", "text": "Mamba for Visual Representation. Since Mamba [26] is primarily designed for 1D data, it is challenging to directly apply it to image data with global spatial context and local relation information. Recent works [57, 109] have extended Mamba for learning visual representations. VMamba [57] developed a 2D selective scan (SS2D) block and integrated it into the VSS Block (Figure 4(b)). The VSS block is then stacked consecutively with convolution layers for downsampling image patches via patch merging [58]. The main difference between Mamba and VSS [57] block (Figure 4(a-b)) is replacing the S6 block with SS2D to adapt selective scanning for image data. ", "page_idx": 3}, {"type": "text", "text": "3.2 Hamba ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Problem Formulation. Given a single hand image $I$ , our goal is to reconstruct the 3D hand mesh. We learn the mapping function $f(I)=\\bar{\\{\\theta,\\beta,\\pi\\}}$ that regresses MANO [75] parameters from the image $I$ , where $\\theta\\in\\mathbb{R}^{48}$ , $\\breve{\\beta}\\in\\mathbb{R}^{10}$ , and $\\pi\\in\\mathbb{R}^{3}$ represent the pose, shape, and camera parameters, respectively. Finally, the MANO model $\\mathcal{M}(\\theta,\\beta)$ generates the corresponding hand mesh $M\\in\\mathbb{R}^{778\\times3}$ . ", "page_idx": 3}, {"type": "text", "text": "Model Architecture. Figure 3 illustrates the Hamba model architecture. First, we feed the hand image $I\\,\\in\\,\\mathbb{R}^{H\\times W\\times3}$ into a ViT [19, 95] backbone to extract tokens $T\\ \\in\\ \\mathbb{R}^{\\frac{H}{16}\\times\\frac{W}{16}\\times1280}$ where $H{=}256$ and $W{=}192$ . Tokens from the backbone are downsampled from dimensions 1280 to 512 using convolution layers (Conv2D). Second, we sample effective tokens using a Token Sampler (TS), which utilizes the 2D joint locations predicted by a Joints Regressor (JR). These tokens are fed into the Graph-guided State Space (GSS) block, which exploits the joint spatial sequence by modeling its state space using the proposed Graph-guided Bidirectional scan (GBS). Finally, we fuse the GSS tokens with the global mean feature, the sampled tokens, and the 2D joint features via a fusion module. Lastly, the MANO parameters are regressed using MLP. ", "page_idx": 3}, {"type": "image", "img_path": "pCJ0l1JVUX/tmp/269c29971c6d4c21282592c73a808d9ee71e79541584025cce36d607b241cfd5.jpg", "img_caption": ["Extracted Token Global Mean Token 2D Joints Feature TS Token GSS Token $\\oplus$ Add $\\circledcirc$ Concatenation "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: Overview of Hamba\u2019s architecture. Given a hand image $I$ , tokens are extracted via a trainable backbone model and downsampled. We design a graph-guided SSM as a decoder to regress hand parameters. The hand joints $\\langle J_{2\\mathrm{D}})$ are regressed by Joints Regressor (JR) and fed into the Token Sampler (TS) to sample tokens $(T_{\\mathrm{TS}})$ . The joint spatial sequence tokens $(T_{\\mathrm{GSS}})$ are learned by the Graph-guided State Space (GSS) blocks. Inside each GSS block, the GCN network takes $T_{\\mathrm{TS}}$ as input and its output is concatenated with the mean down-sampled tokens. GSS leverages graph learning and state space modeling to capture the joint spatial relations to achieve robust 3D reconstruction. ", "page_idx": 4}, {"type": "text", "text": "Token Sampler (TS) and Joints Regressor (JR). To prevent the GSS Block from being influenced by the background and unnecessary features during the early stages of the training, it is important to select effective tokens that encode the relations between hand joints. We propose a Token Sampler (TS), which selects effective tokens utilizing the initial 2D hand joint prediction from the Joints Regressor (JR). While it is possible to use off-the-shelf 2D joint estimator like OpenPose [5] or MediaPipe [60], this would increase model complexity. Previous works [74, 107] primarily used Conv-Pooling-FC schemes for initial joints regression. In our work, the JR consists of stacked SS2D blocks followed by an MLP head which regresses the initial MANO parameters $\\{\\hat{\\theta},\\hat{\\beta},\\hat{\\pi}\\}$ . After the JR regresses 3D joints $\\hat{J}_{3\\mathrm{D}}\\in\\mathbb{R}^{21\\times3}$ , these are projected back to the 2D image plane using perspective projection $\\Pi$ with the predicted camera translation $\\hat{\\pi}$ to obtain $\\hat{J}_{2\\mathrm{D}}\\in\\mathbb{R}^{21\\times2}$ . We denote a predefined focal length $F_{\\mathrm{focal}}=5000\\;\\mathrm{mm}$ . Those are formulated as, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\theta},\\hat{\\beta},\\hat{\\pi}=\\mathrm{JR}(T),\\quad\\hat{J}_{\\mathrm{3D}}=\\mathrm{MANO}\\ (\\hat{\\theta},\\hat{\\beta}),\\quad\\hat{J}_{\\mathrm{2D}}=\\Pi\\ (\\hat{J}_{\\mathrm{3D}},F_{\\mathrm{focal}},\\hat{\\pi}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To align the sampled tokens with 2D joints, we use bilinear interpolation. The sampled token $\\boldsymbol{T}_{\\mathrm{TS}}\\in\\overline{{\\mathbb{R}^{C\\times J}}}$ is formulated as, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{\\mathrm{TS}}=\\mathrm{TS}(\\mathrm{Conv}2\\mathrm{D}(T),\\hat{J}_{\\mathrm{2D}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $J$ denotes the total of 21 joints, and $C$ is the token dimension of 512. ", "page_idx": 4}, {"type": "text", "text": "Graph-guided Bidirectional Scan (GBS). To achieve robust reconstruction and leverage effective tokens, we reformulate Mamba\u2019s unidirectional scanning as a graph-guided bidirectional scan, thus adapting it for 3D reconstruction tasks. GBS is designed to follow a specific graph pattern, considering the spatial and topological connection of the hand joints with image features. A naive approach would be scanning all tokenized image patches (Figure 2(b)). However, this involves redundant tokens, making it challenging to learn joint spatial relations effectively. To address this, we propose two novel ideas. First, instead of scanning all tokens unidirectionally, we perform hand joint-level bidirectional scanning of sampled tokens $T_{\\mathrm{TS}}$ . This effectively reduces the number of tokens to be scanned from 192 to 22 $\\approx88.5\\%$ reduction). We adapt VMamba [57]\u2019s SS2D block for bidirectional scanning to be suitable for our joint spatial sequence. Second, to capture the local and global joint relations, we introduce a Semantic GCN block [105]. Mamba learns long-range dependencies, but it is less effective at capturing fine-grained local information in intricate structures like the 3D mesh. The ", "page_idx": 4}, {"type": "image", "img_path": "pCJ0l1JVUX/tmp/d1115e20734176f87c0f9be446f6fc7824f53626fe745f5281b99af476e004e9.jpg", "img_caption": ["Figure 4: The illustration of the proposed Graph-guided State Space (GSS) block. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "GCN learns input-independent weight matrix to model the edges between hand joints, reflecting how one joint influences another based on prior embedded in graph structures. Introducing graph learning makes it possible to explicitly encode the graph structure within our GSS module. Let $\\bar{\\mathcal{G}}=\\{\\mathbf{V},\\bar{\\mathbf{E}}\\}$ be the graph, $\\mathbf{V}$ is the set of $J$ nodes and $\\mathbf{E}$ are the edges. $T_{\\mathrm{GCN}_{l}}$ represents the output of the $l_{\\cdot}$ -th GCN block, while the complete output of the GSS block is $T_{\\mathrm{GSS}_{l}}$ . For a graph-based propagation, we multiply the token with a learnable parameter matrix $\\mathbf{W}\\in\\mathbb{R}^{C\\times C}$ . Thus, the GCN operation is formulated as, ", "page_idx": 5}, {"type": "equation", "text": "$$\nT_{\\mathrm{GCN}_{l}}=\\left\\{\\sigma(\\mathbf{W}\\,T_{\\mathrm{TS}}\\,\\mathbf{P}_{i}(\\mathbf{M\\odotG})),\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,l=1,}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{M}\\,\\in\\,\\mathbb{R}^{J\\times J}$ is the learnable weighting matrix, $\\mathbf{P}_{i}$ denotes the softmax non-linearity that is applied to normalize the input matrix for all node $i$ choices, while ${\\bf G}\\,\\in\\,[0,1]^{J\\times J}$ denotes the adjacency matrix of graph $\\mathcal{G}$ and $\\odot$ denotes element-wise multiplication. $J$ denotes the total of 21 joints, and $C$ is the token dimension of 512. ", "page_idx": 5}, {"type": "text", "text": "Graph-guided State Space (GSS) Block. Overall, our decoder consists of $L$ GSS blocks. The GSS architecture is illustrated comparatively in Figure 4. In the first GSS block, the sampled tokens $T_{\\mathrm{TS}}$ are passed through graph convolution (GCN) layers. The GCN layer consists of a PGraphConv [44], a Batch Norm, and a ReLU activation. For the GCN, the adjacency matrix is defined based on the hand joint skeleton in the joint order of MANO. To provide the global context, the output from the GCN is concatenated with the global mean token along the joint token sequence. This global mean token is the mean of the downsampled image tokens. This concatenated sequence $T_{\\mathrm{GCN}_{l}}^{c}$ is then fed into the SS2D block and summed with the output through a residual connection. The SS2D block is followed by a Layer Norm (LN), a Feed-Forward Network (FFN), and another residual connection. For subsequent GSS blocks $l\\in\\{2,..,L\\}$ , the input is the output from the previous block $T_{\\mathrm{GSS}_{l-1}}$ . Before this sequence passes through its GCN layer, it is split, and only the first 21 tokens T G{S1S,l..\u2212,211} feendt etros  tthhee  GSSC2ND.  Tblhoe cgkl oabs asl hmoweann i tno kEeqn. $T_{\\mathrm{GSS}_{l-1}}^{\\{22\\}}$ :is concatenated back with the GCN\u2019s output before it ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{\\mathrm{GCN}_{l}}^{c}=\\left\\{\\!\\!\\begin{array}{l l}{T_{\\mathrm{GCN}_{l}}\\oplus\\mathbf{Mean}(\\mathrm{Conv2D}(T)),}&{l=1,}\\\\ {T_{\\mathrm{GCN}_{l}}\\oplus T_{\\mathrm{GSS}_{l-1}}^{\\{22\\}},}&{l>1,}\\end{array}\\!\\!\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\nT_{\\mathrm{GSS}_{l}}=\\mathrm{FFN}(\\mathrm{LN}(\\mathrm{SS2D}(T_{\\mathrm{GCN}_{l}}^{c})+T_{\\mathrm{GCN}_{l}}^{c}))+\\mathrm{SS2D}(T_{\\mathrm{GCN}_{l}}^{c})+T_{\\mathrm{GCN}_{l}}^{c}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\bigoplus$ denotes concatenation. The GSS block not only leverages features from state space modeling and graph learning but also considers global features. This design enables Hamba to learn effective features to enhance performance by incorporating state space modeling and graph learning with few tokens, shown in our ablation study Section 4.2. ", "page_idx": 5}, {"type": "text", "text": "State Space Modeling for Joint Spatial Sequence. Different from the video-based Mamba models [7, 22, 46], which learns the temporal feature with the frame sequence, Hamba focuses on the joint spatial sequence per frame and reveals that modeling joint relations with Mamba [26] can significantly improve the 3D reconstruction performance. In particular, as shown in Eq. 1, $x(t)$ represents $t$ -th token of the joint spatial sequence, which is first sampled by the TS using the JR and then encoded with the GCN. Note that $t$ denotes the index of the hand joint iteration. Lastly, $y(t)$ is the updated token of the $t$ -th of the joint spatial sequence after passing through GSS Blocks. The proposed GSS block effectively enhances 3D reconstruction performance by learning the joint spatial sequence relations with graph learning and state space modeling. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Loss Functions. Following [70], we train Hamba using a combined loss which includes 2D joint loss $\\mathcal{L}_{2\\mathrm{D}}$ , 3D joint loss $\\ensuremath{\\mathcal{L}}_{3\\mathrm{D}}$ , pose loss $\\mathcal{L}_{\\theta}$ , shape loss $\\mathcal{L}_{\\beta}$ , and an adversarial loss $\\mathcal{L}_{\\mathrm{adv}}$ . $\\mathcal{L}_{2\\mathrm{D}}$ and $\\mathcal{L}_{3\\mathrm{D}}$ are calculated using the L1 Norm, while $\\mathcal{L}_{\\theta}$ and $\\mathcal{L}_{\\beta}$ use the $L2\\;\\mathrm{Norm}$ . The training loss $\\mathcal{L}_{\\mathrm{total}}$ is defined as Equation 9, where $\\lambda_{2\\mathrm{D}}$ $,\\;\\lambda_{3\\mathrm{D}},\\;\\lambda_{\\theta},$ , $\\lambda_{\\beta}$ , and $\\lambda_{\\mathrm{adv}}$ denote each term\u2019s weight respectively. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{total}}=\\lambda_{\\mathrm{2D}}\\mathcal{L}_{\\mathrm{2D}}+\\lambda_{\\mathrm{3D}}\\mathcal{L}_{\\mathrm{3D}}+\\lambda_{\\theta}\\mathcal{L}_{\\theta}+\\lambda_{\\beta}\\mathcal{L}_{\\beta}+\\lambda_{\\mathrm{adv}}\\mathcal{L}_{\\mathrm{adv}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We train Hamba on 2.7M training samples from multiple datasets (same setting as [70] for a fair comparison) that had either both 2D and 3D hand annotations or just 2D annotations. This included FreiHAND [111], HO3D [29], MTC [91], RHD [110], InterHand2.6M [64], H2O3D [29], DexYCB [6], COCO-Wholebody [36], Halpe [21], and MPII NZSL [79] datasets. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details. We set learning rate as $10^{-5}$ , weight decay factor as $10^{-4}$ , with the \u2018sum\u2019 loss. Weights for each term in the loss function are $\\lambda_{3D}=0.05$ for 3D keypoint loss, $\\lambda_{2D}=0.01$ for 2D keypoint loss, $\\lambda_{\\theta}=0.001$ for global orientation and hand pose loss. Weights for beta and adversarial loss, i.e., $\\lambda_{\\beta}$ and $\\lambda_{a d v}$ were set as 0.0005. Ablations were run for 60k steps due to computational limitations on 2.7M dataset. Additional details are included in the Appendix A. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics. Following the same protocols employed in previous works [52, 70, 107], we used PA-MPJPE and $\\mathrm{AUC}_{J}$ as the metrics for evaluating the reconstructed 3D joints and PA-MPVPE, ${\\bf A U C}_{V}$ , $\\operatorname{F@5mm}$ , and $\\mathrm{F@15mm}$ for evaluating the reconstructed 3D mesh vertices. ", "page_idx": 6}, {"type": "text", "text": "4.1 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "3D Joints and Mesh Reconstruction Evaluation. We test Hamba on 3 widely used benchmarks: FreiHAND [111], HO3Dv2 [29], and HO3Dv3 [30]. The quantitative comparison with state-of-theart 3D hand reconstruction models is presented in Table 1, Table 2, and Table 3 respectively. Since almost all previous methods (including the popular MobRecon [9], MeshGraphormer [52], and the recent HHMR [47], SimpleHand [107]) were trained only using the FreiHAND [111], for a fair comparison, we compared them with the Hamba version trained using only the FreiHAND [111] dataset. Meanwhile, for a fair comparison with HaMeR [70], we trained Hamba on the same datasets as HaMeR [70] for all other comparisons. Many methods, including the popular MeshGraphormer [52] and METRO [51], report their metrics using Test-Time Augmentation (TTA) which boosts the final results. We report our performances, both with and without TTA. In both scenarios, Hamba significantly achieves better results, outperforming SOTAs in all benchmarks. ", "page_idx": 6}, {"type": "text", "text": "In-the-wild Generalizability Evaluation. Approximately $95\\%$ of datasets used for training previous models [9, 14, 47, 51, 52, 70, 86, 107] were collected in controlled indoor environments, such as studios or multi-camera setups. This includes the FreiHAND [111], HO3Dv2 [29], and HO3Dv3 [30] benchmarks that are popularly used for both training and evaluation. However, training models on datasets collected in controlled environments often leads to decreased performance in real-world scenarios. Thus, solely evaluating performance over indoor-collected datasets might not provide a correct evaluation of the robustness of 3D hand reconstruction. We additionally evaluate Hamba\u2019s in-the-wild performance on the recently proposed HInt [70] benchmark, which has variations in visual conditions, viewpoints, and hand interactions. Since HInt-NewDays [13] and HInt-EpicKitchensVISOR [16, 17] annotations are 2D keypoints, PCK [97] computed at varying thresholds is used as the evaluation metrics. As shown in Table 5, Hamba outperforms existing models by a large margin and surpasses HaMeR [70], showing improvement in model robustness for in-the-wild scenarios. None of the models (including Hamba) have been trained on/ previously ever seen HInt dataset. ", "page_idx": 6}, {"type": "text", "text": "Qualitative Comparison. Figure 5 presents the qualitative comparison of Hamba\u2019s 3D hand mesh reconstruction with SOTA models on in-the-wild images from HInt-EpicKitchens. This includes models that directly regress vertices (METRO [51], MeshGraphormer [52]), and parametric methods, which regress MANO parameters (FrankMocap [76], HaMeR [70]). These images are particularly ", "page_idx": 6}, {"type": "image", "img_path": "pCJ0l1JVUX/tmp/3c159c90fbf1a50866189a94f776c41175d830338d6cef022aa7120d06b69d8a.jpg", "img_caption": ["Figure 5: Qualitative in-the-wild comparison of the proposed Hamba with SOTAs on HIntEpicKitchensVISOR [16, 70]. None of the models (including Hamba) have been trained on HInt. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 1: Comparison with SOTAs on FreiHAND dataset [111]. \u2217Stacked Structure; \u2020used Test-Time Augmentation (TTA). Best scores highlighted Green , while second best are highlighted Light Green PA-MPJPE and PA-MPVPE are measured in mm. -: Info not reported by model. ", "page_idx": 7}, {"type": "table", "img_path": "pCJ0l1JVUX/tmp/a10919a2e5cfebb7b603e8494c7a8384356f5c353d0d3641225077a2594669bf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "challenging since they comprise real-world cooking videos of a person with highly occluded hands, hand-hand, and/or hand-object interactions. For visual comparison, we select images where the hand lies in the corners, causing a truncation scenario thus increasing the complexity further. Hamba consistently outperforms other models and achieves a much better reconstruction. From Figure 5, we can observe that in severe in-the-wild truncation scenarios, Hamba achieves better hand reconstruction, even though the hand is truncated or occluded. We attribute this performance to effectively learning the spatial hand joint sequence with the state space model. The same is verified in the ablation study presented in Sec. 4.2. Figure S5 presents in-the-wild results on various movies, interviews, etc., scenarios. Figure S6 and Figure S7 presents additional visual results on HInt-NewDays and HIntEpicKitchensVISOR respectively. Hamba can robustly reconstruct 3D hands in various complicated hand gestures like grasping, holding, grabbing, finger-pointing, and flattening from different viewing directions, even in heavily occluded and truncated scenarios. ", "page_idx": 7}, {"type": "table", "img_path": "pCJ0l1JVUX/tmp/776208d8b5a64855f28c8bdd8ab56b07b680bb30084b697311aad6492b6e384d.jpg", "table_caption": ["Table 2: Comparison with SOTAs on HO3Dv2 [29] hand-object interaction benchmark. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "pCJ0l1JVUX/tmp/a77ad8c87e5d2af6544b4324b70c7f0df2021f5fb9a27d78a8a13d09bb6a0d2a.jpg", "table_caption": ["Table 3: Evaluation on HO3Dv3 [30] benchmark. We only list SOTAs that reported on HO3Dv3. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.2 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Effect of Branch-wise Features. We verify the effectiveness of each branch feature by excluding their respective tokens from the fusion module as shown in Table 4. First, we verify the contribution of the proposed GSS branch. When the GSS tokens are excluded (Row 3), we observe a major drop in model performance. Specifically, $\\mathrm{F@5mm}$ $(\\uparrow)$ drops from $0.738\\to0.717$ , and the PA-MPJPE $\\left(\\downarrow\\right)$ and PA-MPVPE $\\left(\\downarrow\\right)$ errors increase from $6.6\\rightarrow6.9$ and $6.3\\rightarrow6.6$ . Thus, in addition to local and global contexts, incorporating structured state-space representations can be effective for 3D hand reconstruction. Moreover, it is important to note that modeling spatial joint sequence relations provides better tokens than directly using the 2D joint locations, even though the latter has a clear semantic meaning for all the hand joints. We attribute this to cases of occlusions where the 2D joints cannot be precisely predicted. ", "page_idx": 8}, {"type": "text", "text": "Removing the Token sampler (Row 1) or the 2D joints (Row 2) features also shows a performance drop, but is less significant than removing the GSS branch, since they only provide the local context while GSS tokens provide both local and spatialrelations information. Note that the Global Mean token (Row 4) remains important since it captures the global context, which is discarded in the ablation. ", "page_idx": 8}, {"type": "table", "img_path": "pCJ0l1JVUX/tmp/7862892b2c54df90ea17fea502a32d631c646d7a8d4d0c3aa5019f58c5f77a5e.jpg", "table_caption": ["Effect of proposed Components. Since the GSS Block stands as a major contribution, we additionally evaluate the effectiveness of each ", "Table 4: Ablation study on FreiHAND [111] to verify proposed components. All variants are trained for same number of steps. PA-MPJPE, PA-MPVPE and without are abbreviated as PJ, PV, \u2018w/o\u2019. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "pCJ0l1JVUX/tmp/a7e51e0bd76e0fb94467598d19c801b006e0de6d6367981397e5a0f9f639df37.jpg", "table_caption": ["Table 5: In-the-wild generalizability evaluation on HInt [70]. PCK is used as the evaluation metric. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "component in the proposed GSS block. The same is presented in Table 4. The GSS block models the hand-joint topological structure, learning the graph-structured relations and spatial sequences of joints via graph and state space modeling. Adopting graph learning additionally provides the local context. Excluding the GCN, i.e., when simply using a Mamba block, the structure information will be neglected from the input to the SS2D block, which leads to a large drop in performance (Row 7). This indicates that the GCN is an essential component of the GSS block and using SS2D blocks alone does not lead to accurate 3D hand mesh reconstruction. A potential counter-argument may be that the input features and the GCN alone are sufficient for 3D hand reconstruction, without much improvement from the Mamba Blocks. We removed the Mamba blocks and the GSS degenerates into simple GCN, leading to an equal performance drop (Row 9). Specifically, the PA-MPJPE (\u2193) and PA-MPVPE (\u2193) increase from $6.6\\rightarrow7.3$ and $6.3\\rightarrow7.2$ respectively, while the $\\mathrm{F@5mm~(\\uparrow)}$ and $\\mathrm{F@15mm}$ (\u2191) drop from $0.738\\to0.675$ and $0.988\\to0.983$ respectively. This confirms that both the GCN and the Mamba blocks are equally important in the GSS Block. To verify the effectiveness of the bidirectional scanning, we replaced it with conventional unidirectional scanning to compare, denoted as w/o Bidirectional-scan (Row 6), and the reconstruction error increased. An even larger drop in performance is observed when the proposed GBS scan is removed from the model (Row 8). When not using the token sampler, we also see a drop in performance (Row 5). Overall, Table 4 verifies the effectiveness of each proposed component. We additionally validate this by a qualitative evaluation (in Figure S3), wherein the visual result gets worse when we remove the proposed modules. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose Hamba, a novel Mamba-based model for 3D hand reconstruction, which is capable of reconstructing robust 3D hand meshes with graph learning and state space modeling under bidirectional scanning. Our key insight is reformulating the Mamba scanning into graph-guided bidirectional scanning using a few effective tokens. This allows us to leverage the relations between hand joints and joint spatial sequences, addressing the occlusion and truncation problems using graph learning and state space modeling. Specifically, we designed a new GSS block to capture the relation between hand joints using graph convolution layers and Mamba blocks. Finally, we introduce a practical fusion module to boost performance by incorporating state space features and global features. Experiments on challenging benchmarks and in-the-wild tests demonstrate that Hamba outperforms all existing SOTA models. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Although we leverage the strong representation capability from the graph-guided Mamba model and train on the large comprehensive datasets, it may still not be enough to cover all in-the-wild situations. Our current method lacks the capability to explore temporal features in videos because crawling video datasets requires extensive manual labor for 3D hand reconstruction. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts. Our research focuses on the Hamba model for 3D hand reconstruction, and we plan to release the pre-trained models and code. However, there is a potential risk that it could be used for unauthorized surveillance or privacy infringements. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Aviral Chharia was supported in part by the ATK-Nick G. Vlahakis Graduate Fellowship from Carnegie Mellon University, USA. The authors would like to thank Bernhard Kerbl, Ce Zheng, Cheng Zhang, Yunlu Chen, and Zhenyu Xie for providing suggestions and feedback to improve this work. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele. 2d human pose estimation: New benchmark and state of the art analysis. In Proceedings of the IEEE Conference on computer Vision and Pattern Recognition, pages 3686\u20133693, 2014.   \n[2] Luca Ballan, Aparna Taneja, J\u00fcrgen Gall, Luc Van Gool, and Marc Pollefeys. Motion capture of hands in action using discriminative salient points. In Computer Vision\u2013ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part VI 12, pages 640\u2013653. Springer, 2012.   \n[3] Ali Behrouz and Farnoosh Hashemi. Graph mamba: Towards learning on graphs with state space models. arXiv preprint arXiv:2402.08678, 2024.   \n[4] Adnane Boukhayma, Rodrigo de Bem, and Philip HS Torr. 3d hand shape and pose from images in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10843\u201310852, 2019.   \n[5] Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A. Sheikh. Openpose: Realtime multi-person 2d pose estimation using part affinity fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019.   \n[6] Yu-Wei Chao, Wei Yang, Yu Xiang, Pavlo Molchanov, Ankur Handa, Jonathan Tremblay, Yashraj S Narang, Karl Van Wyk, Umar Iqbal, Stan Birchfield, et al. Dexycb: A benchmark for capturing hand grasping of objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9044\u20139053, 2021.   \n[7] Guo Chen, Yifei Huang, Jilan Xu, Baoqi Pei, Zhe Chen, Zhiqi Li, Jiahao Wang, Kunchang Li, Tong Lu, and Limin Wang. Video mamba suite: State space model as a versatile alternative for video understanding. arXiv preprint arXiv:2403.09626, 2024.   \n[8] Ping Chen, Yujin Chen, Dong Yang, Fangyin Wu, Qin Li, Qingpei Xia, and Yong Tan. I2uv-handnet: Image-to-uv prediction network for accurate and high-fidelity 3d hand mesh modeling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12929\u201312938, 2021.   \n[9] Xingyu Chen, Yufeng Liu, Yajiao Dong, Xiong Zhang, Chongyang Ma, Yanmin Xiong, Yuan Zhang, and Xiaoyan Guo. Mobrecon: Mobile-friendly hand mesh reconstruction from monocular image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20544\u201320554, 2022.   \n[10] Xingyu Chen, Yufeng Liu, Chongyang Ma, Jianlong Chang, Huayan Wang, Tian Chen, Xiaoyan Guo, Pengfei Wan, and Wen Zheng. Camera-space hand mesh recovery via semantic aggregation and adaptive 2d-1d registration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13274\u201313283, 2021.   \n[11] Xingyu Chen, Baoyuan Wang, and Heung-Yeung Shum. Hand avatar: Free-pose hand animation and rendering from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8683\u20138693, 2023.   \n[12] Yujin Chen, Zhigang Tu, Di Kang, Linchao Bao, Ying Zhang, Xuefei Zhe, Ruizhi Chen, and Junsong Yuan. Model-based 3d hand reconstruction via self-supervised learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10451\u201310460, 2021.   \n[13] Tianyi Cheng, Dandan Shan, Ayda Hassen, Richard Higgins, and David Fouhey. Towards a richer 2d understanding of hands at scale. Advances in Neural Information Processing Systems, 36:30453\u201330465, 2023.   \n[14] Junhyeong Cho, Kim Youwang, and Tae-Hyun Oh. Cross-attention of disentangled modalities for 3d human mesh recovery with transformers. In European Conference on Computer Vision (ECCV), 2022.   \n[15] Hongsuk Choi, Gyeongsik Moon, and Kyoung Mu Lee. Pose2mesh: Graph convolutional network for 3d human pose and mesh recovery from a 2d human pose. In European Conference on Computer Vision, pages 769\u2013787. Springer, 2020.   \n[16] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Scaling egocentric vision: The epic-kitchens dataset. In Proceedings of the European conference on computer vision (ECCV), pages 720\u2013736, 2018.   \n[17] Ahmad Darkhalil, Dandan Shan, Bin Zhu, Jian Ma, Amlan Kar, Richard Higgins, Sanja Fidler, David Fouhey, and Dima Damen. Epic-kitchens visor benchmark: Video segmentations and object relations. In Proceedings of the Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks, 2022.   \n[18] Haoye Dong, Tiange Xiang, Sravan Chittupalli, Jun Liu, and Dong Huang. Physical-space multi-body mesh detection achieved by local alignment and global dense learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1267\u20131276, 2024.   \n[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. 2020.   \n[20] Enes Duran, Muhammed Kocabas, Vasileios Choutas, Zicong Fan, and Michael J Black. Hmp: Hand motion priors for pose and shape estimation from video. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 6353\u20136363, 2024.   \n[21] Hao-Shu Fang, Jiefeng Li, Hongyang Tang, Chao Xu, Haoyi Zhu, Yuliang Xiu, Yong-Lu Li, and Cewu Lu. Alphapose: Whole-body regional multi-person pose estimation and tracking in real-time. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.   \n[22] Yu Gao, Jiancheng Huang, Xiaopeng Sun, Zequn Jie, Yujie Zhong, and Lin Ma. Matten: Video generation with mamba-attention. arXiv preprint arXiv:2405.03025, 2024.   \n[23] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and Jitendra Malik. Humans in 4d: Reconstructing and tracking humans with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14783\u201314794, 2023.   \n[24] John C Gower. Generalized procrustes analysis. Psychometrika, 40:33\u201351, 1975.   \n[25] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18995\u201319012, 2022.   \n[26] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. 2023.   \n[27] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. 2021.   \n[28] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems, 34:572\u2013585, 2021.   \n[29] Shreyas Hampali, Mahdi Rad, Markus Oberweger, and Vincent Lepetit. Honnotate: A method for 3d annotation of hand and object poses. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3196\u20133206, 2020.   \n[30] Shreyas Hampali, Sayan Deb Sarkar, and Vincent Lepetit. Ho-3d_v3: Improving the accuracy of hand-object annotations of the ho-3d dataset. arXiv preprint arXiv:2107.00887, 2021.   \n[31] Shreyas Hampali, Sayan Deb Sarkar, Mahdi Rad, and Vincent Lepetit. Keypoint transformer: Solving joint identification in challenging hands and object interactions for accurate 3d pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11090\u201311100, 2022.   \n[32] Yana Hasson, Gul Varol, Dimitrios Tzionas, Igor Kalevatykh, Michael J Black, Ivan Laptev, and Cordelia Schmid. Learning joint reconstruction of hands and manipulated objects. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11807\u201311816, 2019.   \n[33] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, and Bj\u00f6rn Ommer. Zigma: A dit-style zigzag mamba diffusion model. arXiv preprint arXiv:2403.13802, 2024.   \n[34] Rongtian Huo, Qing Gao, Jing Qi, and Zhaojie Ju. 3d human pose estimation in video for human-computer/robot interaction. In International Conference on Intelligent Robotics and Applications, pages 176\u2013187. Springer, 2023.   \n[35] Zheheng Jiang, Hossein Rahmani, Sue Black, and Bryan M Williams. A probabilistic attention model with occlusion-aware texture regression for 3d hand reconstruction from a single rgb image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 758\u2013767, 2023.   \n[36] Sheng Jin, Lumin Xu, Jin Xu, Can Wang, Wentao Liu, Chen Qian, Wanli Ouyang, and Ping Luo. Whole-body human pose estimation in the wild. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part IX 16, pages 196\u2013214. Springer, 2020.   \n[37] Sam Johnson and Mark Everingham. Learning effective human pose estimation from inaccurate annotation. In CVPR 2011, pages 1465\u20131472. IEEE, 2011.   \n[38] Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser Sheikh. Panoptic studio: A massively multiview system for social motion capture. In The IEEE International Conference on Computer Vision (ICCV), 2015.   \n[39] Rudolph Emil Kalman. A new approach to linear filtering and prediction problems. 1960.   \n[40] Angjoo Kanazawa, Michael J Black, David W Jacobs, and Jitendra Malik. End-to-end recovery of human shape and pose. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7122\u20137131, 2018.   \n[41] Sameh Khamis, Jonathan Taylor, Jamie Shotton, Cem Keskin, Shahram Izadi, and Andrew Fitzgibbon. Learning an efficient model of hand shape variation from depth images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2540\u2013 2548, 2015.   \n[42] Jeonghwan Kim, Mi-Gyeong Gwon, Hyunwoo Park, Hyukmin Kwon, Gi-Mun Um, and Wonjun Kim. Sampling is matter: Point-guided 3d human mesh reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12880\u2013 12889, 2023.   \n[43] Dominik Kulon, Riza Alp Guler, Iasonas Kokkinos, Michael M Bronstein, and Stefanos Zafeiriou. Weakly-supervised mesh-convolutional hand reconstruction in the wild. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4990\u20135000, 2020.   \n[44] John Boaz Lee, Ryan A. Rossi, Xiangnan Kong, Sungchul Kim, Eunyee Koh, and Anup Rao. Higher-order graph convolutional networks. 2018.   \n[45] Haoyuan Li, Haoye Dong, Hanchao Jia, Dong Huang, Michael C Kampffmeyer, Liang Lin, and Xiaodan Liang. Coordinate transformer: Achieving single-stage multi-person mesh recovery from videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8744\u20138753, 2023.   \n[46] Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, and Yu Qiao. Videomamba: State space model for efficient video understanding. arXiv preprint arXiv:2403.06977, 2024.   \n[47] Mengcheng Li, Hongwen Zhang, Yuxiang Zhang, Ruizhi Shao, Tao Yu, and Yebin Liu. Hhmr: Holistic hand mesh recovery by enhancing the multimodal controllability of graph diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.   \n[48] Dingkang Liang, Xin Zhou, Xinyu Wang, Xingkui Zhu, Wei Xu, Zhikang Zou, Xiaoqing Ye, and Xiang Bai. Pointmamba: A simple state space model for point cloud analysis. arXiv preprint arXiv:2402.10739, 2024.   \n[49] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor Zusman, and Yoav Shoham. Jamba: A hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887, 2024.   \n[50] Guan Ming Lim, Prayook Jatesiktat, and Wei Tech Ang. Mobilehand: Real-time 3d hand shape and pose estimation from color image. In International Conference on Neural Information Processing, pages 450\u2013459. Springer, 2020.   \n[51] Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-end human pose and mesh reconstruction with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1954\u20131963, 2021.   \n[52] Kevin Lin, Lijuan Wang, and Zicheng Liu. Mesh graphormer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 12939\u201312948, 2021.   \n[53] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.   \n[54] Zhifeng Lin, Changxing Ding, Huan Yao, Zengsheng Kuang, and Shaoli Huang. Harmonious feature learning for interactive hand-object pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12989\u201312998, 2023.   \n[55] Hongyu Liu, Xintong Han, Chengbin Jin, Lihui Qian, Huawei Wei, Zhe Lin, Faqiang Wang, Haoye Dong, Yibing Song, Jia Xu, et al. Human motionformer: Transferring human motions with vision transformers. arXiv preprint arXiv:2302.11306, 2023.   \n[56] Shaowei Liu, Hanwen Jiang, Jiarui Xu, Sifei Liu, and Xiaolong Wang. Semi-supervised 3d hand-object poses estimation with interactions in time. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14687\u201314697, 2021.   \n[57] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, and Yunfan Liu. Vmamba: Visual state space model. 2024.   \n[58] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12009\u201312019, 2022.   \n[59] Haofan Lu, Shuiping Gou, and Ruimin Li. Spmhand: Segmentation-guided progressive multi-path 3d hand pose and shape estimation. IEEE Transactions on Multimedia, 2024.   \n[60] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja, Michael Hays, Fan Zhang, Chuo-Ling Chang, Ming Guang Yong, Juhyun Lee, et al. Mediapipe: A framework for building perception pipelines. 2019.   \n[61] Rachel Locker McKee and David McKee. Making an online dictionary of new zealand sign language: projects. Lexikos, 23(1):500\u2013531, 2013.   \n[62] Stan Melax, Leonid Keselman, and Sterling Orsten. Dynamics based 3d skeletal hand tracking. In Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, pages 184\u2013184, 2013.   \n[63] Gyeongsik Moon and Kyoung Mu Lee. I2l-meshnet: Image-to-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image. In European Conference on Computer Vision, pages 752\u2013768. Springer, 2020.   \n[64] Gyeongsik Moon, Shoou-I Yu, He Wen, Takaaki Shiratori, and Kyoung Mu Lee. Interhand2. 6m: A dataset and baseline for 3d interacting hand pose estimation from a single rgb image. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XX 16, pages 548\u2013564. Springer, 2020.   \n[65] Gyeongsik Moon, Shoou-I Yu, He Wen, Takaaki Shiratori, and Kyoung Mu Lee. Interhand2.6m: A dataset and baseline for 3d interacting hand pose estimation from a single rgb image. In European Conference on Computer Vision (ECCV), 2020.   \n[66] Markus Oberweger, Paul Wohlhart, and Vincent Lepetit. Training a feedback loop for hand pose estimation. In Proceedings of the IEEE international conference on computer vision, pages 3316\u20133324, 2015.   \n[67] Iason Oikonomidis, Nikolaos Kyriazis, Antonis A Argyros, et al. Efficient model-based 3d tracking of hand articulations using kinect. In BmVC, volume 1, page 3, 2011.   \n[68] JoonKyu Park, Yeonguk Oh, Gyeongsik Moon, Hongsuk Choi, and Kyoung Mu Lee. Handoccnet: Occlusion-robust 3d hand mesh estimation network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1496\u20131505, 2022.   \n[69] Adrien Pavao, Isabelle Guyon, Anne-Catherine Letournel, Dinh-Tuan Tran, Xavier Baro, Hugo Jair Escalante, Sergio Escalera, Tyler Thomas, and Zhen Xu. Codalab competitions: An open source platform to organize scientific challenges. Journal of Machine Learning Research, 24(198):1\u20136, 2023.   \n[70] Georgios Pavlakos, Dandan Shan, Ilija Radosavovic, Angjoo Kanazawa, David Fouhey, and Jitendra Malik. Reconstructing hands in 3D with transformers. In CVPR, 2024.   \n[71] Siyou Pei, Alexander Chen, Jaewook Lee, and Yang Zhang. Hand interfaces: Using hands to imitate objects in ar/vr for expressive interactions. In Proceedings of the 2022 CHI conference on human factors in computing systems, pages 1\u201316, 2022.   \n[72] Haozhe Qi, Chen Zhao, Mathieu Salzmann, and Alexander Mathis. Hoisdf: Constraining 3d hand-object pose estimation with global signed distance fields. arXiv preprint arXiv:2402.17062, 2024.   \n[73] Lihui Qian, Xintong Han, Faqiang Wang, Hongyu Liu, Haoye Dong, Zhiwen Li, Huawei Wei, Zhe Lin, and Cheng-Bin Jin. Xformer: fast and accurate monocular 3d body capture. arXiv preprint arXiv:2305.11101, 2023.   \n[74] Pengfei Ren, Chao Wen, Xiaozheng Zheng, Zhou Xue, Haifeng Sun, Qi Qi, Jingyu Wang, and Jianxin Liao. Decoupled iterative refinement framework for interacting hands reconstruction from a single rgb image. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023.   \n[75] Javier Romero, Dimitrios Tzionas, and Michael J. Black. Embodied hands: Modeling and capturing hands and bodies together. In ACM Transactions on Graphics, (Proc. SIGGRAPH Asia), 2017.   \n[76] Yu Rong, Takaaki Shiratori, and Hanbyul Joo. Frankmocap: A monocular 3d whole-body pose estimation system via regression and integration. In IEEE International Conference on Computer Vision Workshops, 2021.   \n[77] Tanner Schmidt, Richard A Newcombe, and Dieter Fox. Dart: Dense articulated real-time tracking. In Robotics: Science and systems, volume 2, pages 1\u20139. Berkeley, CA, 2014.   \n[78] Qiuhong Shen, Xuanyu Yi, Zike Wu, Pan Zhou, Hanwang Zhang, Shuicheng Yan, and Xinchao Wang. Gamba: Marry gaussian splatting with mamba for single view 3d reconstruction. arXiv preprint arXiv:2403.18795, 2024.   \n[79] Tomas Simon, Hanbyul Joo, Iain Matthews, and Yaser Sheikh. Hand keypoint detection in single images using multiview bootstrapping. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 1145\u20131153, 2017.   \n[80] Srinath Sridhar, Antti Oulasvirta, and Christian Theobalt. Interactive markerless articulated hand motion tracking using rgb and depth data. In Proceedings of the IEEE international conference on computer vision, pages 2456\u20132463, 2013.   \n[81] Xiao Tang, Tianyu Wang, and Chi-Wing Fu. Towards accurate alignment in real-time 3d hand-mesh reconstruction. In International Conference on Computer Vision (ICCV), pages 11698\u201311707, 2021.   \n[82] Xiao Tang, Tianyu Wang, and Chi-Wing Fu. Towards accurate alignment in real-time 3d hand-mesh reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11698\u201311707, 2021.   \n[83] Anastasia Tkach, Mark Pauly, and Andrea Tagliasacchi. Sphere-meshes for real-time hand modeling and tracking. ACM Transactions on Graphics (ToG), 35(6):1\u201311, 2016.   \n[84] Tze Ho Elden Tse, Kwang In Kim, Ales Leonardis, and Hyung Jin Chang. Collaborative learning for hand and object reconstruction with attention-guided graph convolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1664\u20131674, 2022.   \n[85] Dimitrios Tzionas, Luca Ballan, Abhilash Srikantha, Pablo Aponte, Marc Pollefeys, and Juergen Gall. Capturing hands in action using discriminative salient points and physics simulation. International Journal of Computer Vision, 118:172\u2013193, 2016.   \n[86] Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel, and Anurag Ranjan. Fastvit: A fast hybrid vision transformer using structural reparameterization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5785\u20135795, 2023.   \n[87] Timo Von Marcard, Roberto Henschel, Michael J Black, Bodo Rosenhahn, and Gerard PonsMoll. Recovering accurate 3d human pose in the wild using imus and a moving camera. In Proceedings of the European conference on computer vision (ECCV), pages 601\u2013617, 2018.   \n[88] Chloe Wang, Oleksii Tsepa, Jun Ma, and Bo Wang. Graph-mamba: Towards long-range graph sequence modeling with selective state spaces. arXiv preprint arXiv:2402.00789, 2024.   \n[89] Chloe Wang, Oleksii Tsepa, Jun Ma, and Bo Wang. Graph-mamba: Towards long-range graph sequence modeling with selective state spaces. arXiv preprint arXiv:2402.00789, 2024.   \n[90] Shuaibing Wang, Shunli Wang, Dingkang Yang, Mingcheng Li, Ziyun Qian, Liuzhen Su, and Lihua Zhang. Handgcat: Occlusion-robust 3d hand mesh reconstruction from monocular images. In 2023 IEEE International Conference on Multimedia and Expo (ICME), pages 2495\u20132500. IEEE, 2023.   \n[91] Donglai Xiang, Hanbyul Joo, and Yaser Sheikh. Monocular total capture: Posing face, body, and hands in the wild. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10965\u201310974, 2019.   \n[92] Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox. Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes. arXiv preprint arXiv:1711.00199, 2017.   \n[93] Hao Xu, Haipeng Li, Yinqiao Wang, Shuaicheng Liu, and Chi-Wing Fu. Handbooster: Boosting 3d hand-mesh reconstruction by conditional synthesis and sampling of hand-object interactions. arXiv preprint arXiv:2403.18575, 2024.   \n[94] Hao Xu, Tianyu Wang, Xiao Tang, and Chi-Wing Fu. H2onet: Hand-occlusion-and-orientationaware network for real-time 3d hand mesh reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17048\u201317058, 2023.   \n[95] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vitpose: Simple vision transformer baselines for human pose estimation. volume 35, pages 38571\u201338584, 2022.   \n[96] Lixin Yang, Kailin Li, Xinyu Zhan, Jun Lv, Wenqiang Xu, Jiefeng Li, and Cewu Lu. Artiboost: Boosting articulated 3d hand-object pose estimation via online exploration and synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2750\u20132760, 2022.   \n[97] Yi Yang and Deva Ramanan. Articulated human detection with flexible mixtures of parts. IEEE transactions on pattern analysis and machine intelligence, 35(12):2878\u20132890, 2012.   \n[98] Yusuke Yoshiyasu. Deformable mesh transformer for 3d human mesh recovery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17006\u2013 17015, 2023.   \n[99] Ziwei Yu, Linlin Yang, You Xie, Ping Chen, and Angela Yao. Uv-based 3d hand-object reconstruction with grasp optimization. arXiv preprint arXiv:2211.13429, 2022.   \n[100] Xiong Zhang, Hongsheng Huang, Jianchao Tan, Hongmin Xu, Cheng Yang, Guozhu Peng, Lei Wang, and Ji Liu. Hand image understanding via deep multi-task learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11281\u201311292, 2021.   \n[101] Xiong Zhang, Qiang Li, Hong Mo, Wenbo Zhang, and Wen Zheng. End-to-end hand mesh recovery from a monocular rgb image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2354\u20132364, 2019.   \n[102] Zeyu Zhang, Akide Liu, Ian Reid, Richard Hartley, Bohan Zhuang, and Hao Tang. Motion mamba: Efficient and long sequence motion generation with hierarchical and bidirectional selective ssm. arXiv preprint arXiv:2403.07487, 2024.   \n[103] Fuwei Zhao, Zhenyu Xie, Michael Kampffmeyer, Haoye Dong, Songfang Han, Tianxiang Zheng, Tao Zhang, and Xiaodan Liang. M3d-vton: A monocular-to-3d virtual try-on network. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13239\u2013 13249, 2021.   \n[104] Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, and Dimitris N. Metaxas. Semantic graph convolutional networks for 3d human pose regression. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3425\u20133435, 2019.   \n[105] Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, and Dimitris N Metaxas. Semantic graph convolutional networks for 3d human pose regression. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3425\u20133435, 2019.   \n[106] Weixi Zhao, Weiqiang Wang, and Yunjie Tian. Graformer: Graph-oriented transformer for 3d pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20438\u201320447, 2022.   \n[107] Zhishan Zhou, Zhi Lv, Minqiang Zou, Yao Tang, Jiajun Liang, et al. A simple baseline for efficient hand mesh reconstruction. arXiv preprint arXiv:2403.01813, 2024.   \n[108] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024.   \n[109] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024.   \n[110] Christian Zimmermann and Thomas Brox. Learning to estimate 3d hand pose from single rgb images. In Proceedings of the IEEE international conference on computer vision, pages 4903\u20134911, 2017.   \n[111] Christian Zimmermann, Duygu Ceylan, Jimei Yang, Bryan Russell, Max Argus, and Thomas Brox. Freihand: A dataset for markerless capture of hand pose and shape from single rgb images. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 813\u2013822, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We further elaborate on the additional model architecture details (Section A.1), and other training details A.2 including loss function weights, dataset descriptions, training schemes, test-time augmentation, and various evaluation metric definitions. We also include visual results for failure cases (Section A.3). In Section A.4 we provide a detailed explanation of the ablation experiments, Table 5 and the algorithm of the GSS block. In Section A.5 we include an additional ablation to compare our proposed model with attention-based models. Finally Section A.6 demonstrates the transferability of our GSS block over the 3D Human Mesh Recovery task. ", "page_idx": 18}, {"type": "text", "text": "A.1 Model Architecture Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Hamba follows an encoder-decoder structure that first tokenizes the input image patches, and then feeds it into the decoder to predict the 3D hand reconstruction results. Architecture details of each component required to reproduce our results are included in this section. We further include the model architecture feature dimensions in Figure S1 for additional clarity. ", "page_idx": 18}, {"type": "image", "img_path": "pCJ0l1JVUX/tmp/b2301d3913d45055e4793dc11ecc766d462648438f91e77937afeaa65b4fafad.jpg", "img_caption": ["Figure S1: Illustration of model architecture feature dimensions for additional clarity. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Backbone. Following previous works [70, 86, 107], we ViT-H [19] was used as the encoder that inputs an $I\\,\\in\\,\\mathbb{R}^{256\\times\\overline{{192}}\\times3}$ and tokenizes the image patches to output $T\\,\\in\\,\\mathbb{R}^{16\\times12\\times1280}$ tokens. Specifically, the backbone contained 50 transformer layers, with a total of 630M parameters. The downsampling layers consist of 2D convolution, Batch Norm, ReLU activation, and another 2D convolution. The channel dimension is reduced from 1280-dim to 512-dim in the first convolution operation, while it is maintained as 512-dim in the second. The kernel sizes for both convolutions were set as one. ", "page_idx": 18}, {"type": "text", "text": "Joints Regressor (JR). The JR follows a simple structure, containing only four VSSM (SS2D) blocks followed by 3 linear layers (MLP) to predict each of the MANO parameters. The VSSM blocks are 512-dim with a depth of 2, SSM state dimension of 1, SSM ratio of 2, MLP ratio of 4, and a depth-wise convolution kernel size of 3 (without bias). The Gate control is deprecated, and the JR takes all $T\\in\\mathbb{R}^{16\\times12\\times1280}$ tokens input. The final output of the VSSM blocks maintains the same shape as the input. \u2018Mean\u2019 Pooling is performed over the height and width dimensions and then fed into the linear layers (MLP) to regress the MANO parameters $({\\hat{\\theta}},{\\hat{\\beta}},{\\hat{\\pi}})$ . ", "page_idx": 18}, {"type": "text", "text": "Token Sampler (TS). The TS was implemented as the pytorch.nn.Functional.grid_sample module of PyTorch. The TS is followed by a 1D Conv, Batch Norm, ReLU activation, and another 1D Conv operation. Bilinear Interpolation was used as the sampling mode to better adapt the float-point joint location prediction from the JR. ", "page_idx": 18}, {"type": "text", "text": "Graph-guided State Space (GSS) Block. The GSS Block has a simple structure. Note that each GCN layer in a GSS block consists of a single graph convolution operation [44] followed by a ", "page_idx": 18}, {"type": "text", "text": "Batch Norm and ReLU activation. For the rest of the components of the GSS blocks, each has a set dimension of 512-dim, a depth of 2, an SSM state dimension of 1, an SSM ratio of 2, an MLP ratio of 4, and a depth-wise convolution kernel size of 3 (without bias). The gate control is deprecated. We use 04 GSS blocks in our best-trained model. ", "page_idx": 19}, {"type": "text", "text": "A.2 Model Training and other details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Loss Functions. As described by Equation 9, the L1 Norm is used to calculate the 3D joint loss and the 2D joint reprojection loss. Let $J_{\\mathrm{2D}}^{\\mathrm{\\hat{G}T}}$ and $J_{\\mathrm{3D}}^{\\mathrm{GT}}$ be the 2D and 3D ground-truth (GT) joint locations, while $J_{2\\mathrm{D}}$ and $J_{3\\mathrm{D}}$ be the corresponding model predictions. If a training dataset additionally provides the GT MANO parameters, i.e., $\\mathbf{\\dot{\\theta}}^{\\mathrm{GT}}$ and $\\beta^{\\mathrm{GT}}$ with their dataset, we also calculate a MANO parameter loss $(\\mathcal{L}_{\\theta},\\mathcal{L}_{\\beta})$ between prediction $(\\theta,\\beta)$ and the GT $(\\theta^{\\mathrm{GT}},\\beta^{\\mathrm{GT}})$ using the L2 Norm. The loss terms are formulated as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{3D}}=||J_{\\mathrm{3D}}^{\\mathrm{GT}}-J_{\\mathrm{3D}}||_{1},}\\\\ {\\mathcal{L}_{\\mathrm{2D}}=||J_{\\mathrm{2D}}^{\\mathrm{GT}}-J_{\\mathrm{2D}}||_{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\theta}=||\\theta^{\\mathrm{GT}}-\\theta||_{2},}\\\\ &{\\mathcal{L}_{\\beta}=||\\beta^{\\mathrm{GT}}-\\beta||_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To prevent the model from predicting unnatural hand gestures, we cooperate with discriminators $D_{k}$ for $(\\theta,\\beta)$ and each hand joint angle separately following [23, 40, 70] as: $\\begin{array}{r}{\\mathcal{L}_{a d v}=\\sum_{k}(D_{k}(\\theta,\\beta)-1)^{2}}\\end{array}$ . The loss weights are kept as $\\lambda_{\\mathrm{3D}}=0.05,\\lambda_{\\mathrm{2D}}=0.01,\\lambda_{\\theta}=0.001,\\lambda_{\\beta}=0.0005$ and $\\lambda_{\\mathrm{adv}}=0.0005$ . ", "page_idx": 19}, {"type": "text", "text": "Datasets. In this section, we briefly introduce the datasets used in the study. For training, we use a mixture of FreiHAND [111], HO3D [29], MTC [91], RHD [110], Interhand2.6M [64], H2O3D [29], DexYCB [6], COCO Wholebody [36], Halpe [21], and MPII NZSL [79] datasets. The final dataset contained a total of 2.7M training samples. We adopt the same dataset mixing ratios as [70] with sampling weights as 0.25 for FreiHAND and InterHand2.6M, and 0.1 for MTC and COCOWholebody. The rest were all set to 0.05. ", "page_idx": 19}, {"type": "text", "text": "\u2022 FreiHAND [111] is a large-scale multiview hand dataset with 3D hands annotations, popularly used by 3D hand reconstruction studies. The training set contains $33\\mathbf{k}$ samples collected by 08 cameras in a green-screen studio environment, which are further enhanced by replacing the backgrounds, leading to an overall 132k samples. The evaluation set contains 4K samples including both in-door and out-door in-the-wild scenarios. FreiHAND was released by Adobe Research and University of Freiburg in 2019, and is for research only, non-commercial use. ", "page_idx": 19}, {"type": "text", "text": "\u2022 HO3Dv2 [29] and HO3Dv3 [31] datasets are part of an ongoing international competition on 3D Hand Reconstruction. Both HO3Dv2 and HO3Dv3 are markerless hand-object interaction datasets containing 3D poses for both hand and object, released in 2020 and 2022 respectively. The sequences in the dataset came from the YCB dataset [92] and were collected by single or multiple RGBD cameras. Currently, the dataset has two versions: $v2$ and $v3$ . The $v2$ version contains about 70k training images and around 10k test images, while the $v3$ version contains more than 103K training and 20K test images. The authors do not release the GT annotations and the results can only be tested using the Codalab [69] competition website. Hamba achieves the best performance (as of May 2024) on the leaderboard of both datasets. HO3D was released by Graz University of Technology and CNRS France, and is for research only, non-commercial use. ", "page_idx": 19}, {"type": "text", "text": "\u2022 HInt-EpicKitchensVISOR [17, 70], HInt-NewDays [70] and HInt-Ego4D [25] datasets consists of $40.4\\mathbf{k}$ hands with 2D keypoints, and was released in 2024. HInt stands for Hand Interactions in the Wild, which only has 2D labels and visibility labels. The three subsets of HInt are built based on the Hands23 [13], Epic-Kitchens [16], and the Ego4D [25] video datasets. In our study, HInt is not used for training but serves as a benchmark to evaluate Hamba\u2019s cross-dataset generalizability. This dataset is for research only and non-commercial use. ", "page_idx": 19}, {"type": "text", "text": "In addition to the above-discussed datasets, we include the descriptions of the datasets used in the Hamba training set: ", "page_idx": 19}, {"type": "text", "text": "\u2022 H2O3D [31] is first released in 2022. Similar to HO3D, it contains around 61k training images and 9k test images using a five RGBD camera multi-view setup in a controlled environment. Compared to HO3D, the content is further expanded to two hands interacting with different objects. \u2022 DexYCB [6] is a hand-manipulating object dataset captured by 8 RGBD cameras in a laboratory environment. It contains $582\\mathrm{k}$ RGBD images for 10 subjects grasping 20 different objects. Each of the sequences lasts for 3 seconds. The dataset comes with the 2D and 3D annotation for hand. ", "page_idx": 19}, {"type": "text", "text": "\u2022 MTC [91] used the Panoptic Studio to capture both the 3D body and hand poses without using markers. The dataset recorded 40 subjects for 2.5 minutes and included 111K hand images and their 3D pose. Each of the subjects performed a large range of both body and hand motions.   \n\u2022 RHD [110] proposed a synthetic dataset for better annotation quality. The dataset utilized 20 different 3D characters performing 39 actions, containing around $44\\mathrm{k}$ images in total. Each image is collected under $320\\times320$ resolution and comes with 3D joint annotation. To enhance the generalization of the dataset, the rendered hands are put on random background images.   \n\u2022 InterHand2.6M [65] is a large-scale real-captured hand interaction dataset. It contains $2.6\\mathrm{M}$ labeled RGB images showing single or two interacting hands. The data was collected using more than 80 cameras in a precisely calibrated multi-view studio and used a semi-automatic annotation method. The dataset involved 27 subjects and the image resolution is set to $512\\times334$ .   \n\u2022 COCO Wholebody [36] based on the COCO dataset and annotated 133 whole body keypoints including face, hand, body, and feet. It contains $200\\mathrm{k}$ RGB in-the-wild images, with $250\\mathrm{k}$ instances, and also comes with face, hand, and body bounding boxes. As for the hand pose estimation task, it includes about 100k samples with 2D keypoint annotations.   \n\u2022 Halpe [21] is an in-the-wild RGB dataset for full-body human-object interaction dataset. It contains 50k training and $5\\mathrm{k}$ test instances. Images are annotated with 136 full body keypoints.   \n\u2022 MPII NZSL[79] is a mixing dataset, containing in-the-wild, multiview studio collected and synthetic RGB images. Images are included with single-hand, hand-hand, and hand-object interactions. The dataset has 11k rendered synthetic samples, $2.8\\mathbf{k}$ manually annotated in-thewild images from MPII [1] and NZSL [61], and $15\\mathbf{k}$ multiview samples from Panoptic Studio dataset [38]. All the samples come with 2D keypoint annotations. ", "page_idx": 20}, {"type": "text", "text": "Training schemes. The Joints Regressor (JR) was trained on a single NVIDIA A4500 GPU with a batch size of 8 for 1M steps. The training took 5 days and required around 300GB RAM. The complete Hamba model was trained on two NVIDIA A6000 GPUs which required two days on a batch size of 56. Early stopping was used after $170\\mathbf{k}$ steps to prevent overfitting. The mixing ratio was kept consistent for both parts of the model. AdamW optimizer was used with a learning rate of 1e-5, $\\beta_{1}=0.9,\\beta_{2}=0.999$ , and a weight decay of 1e-4. The ViT backbone with initialized with HaMeR [70] released checkpoint and is kept unfroze during training. The same setting was used while training the JR. Note that for reproducing the results, we recommend directly loading the released Hamba weights from the GitHub repository. For training from scratch, we recommend using the same training configuration, since PyTorch Lightening might have a bug for effective batch size. Refer to the GitHub issue 2. ", "page_idx": 20}, {"type": "text", "text": "Test Time Augmentation (TTA). Since the FreiHAND dataset was originally part of a challenge on 3D Hand Mesh Reconstruction, many popular models used Test-time augmentation (TTA) to report their results. For a fair comparison with the models that used TTA, we report both the results, with and without TTA on the FreiHAND test set in Table 1. Following [52], we let the model infer on the same test image, which was scaled and rotated multiple times and averaged over the results. This generally results in a performance boost. The rotations are set from $-90^{\\circ}$ to $90^{\\circ}$ for every $10^{\\circ}$ , and rescaled by factors of (0.7, 0.8, 0.9, 1.0, 1.1). We tested through all the combinations and averaged the error to obtain the TTA result. Note that we obtained the SOTA results both with and without TTA. Unless stated otherwise, all reported results in our study are without using TTA. ", "page_idx": 20}, {"type": "text", "text": "Evaluation Metrics. Here, we discuss the definitions of the evaluation metrics used in the study. ", "page_idx": 20}, {"type": "text", "text": "\u2022 PA-MPJPE means Procrustes Aligned-Mean Per Joint Position Error, which is the measure of the average joint error after Procrustes alignment. It is calculated as the Euclidean distance between the predicted joints and the GT. PA-MPJPE performs the Procrustes Analysis (PA) [24] method that aligns the predicted and GT positions using a non-rigid transformation to minimize the overall distance between them. The PA-MPJPE is measured in millimeters (mm) and widely used as the evaluation metrics in 3D hand reconstruction works. ", "page_idx": 20}, {"type": "text", "text": "\u2022 PA-MPVPE means Procrustes Aligned-Mean Per Vertex Position Error, which is similar to PAMPJPE but measures error between mesh vertices after Procrustes alignment. It is also calculated in millimeters (mm). ", "page_idx": 20}, {"type": "image", "img_path": "pCJ0l1JVUX/tmp/d24376b4bdf8d1db7e3452a83e3c2de9f5c0338942236a862510dbcb99d00186.jpg", "img_caption": ["Figure S2: Illustration of various failure cases of Hamba: Wrong palm orientation, missed frames due to motion blur, and missed finger reconstruction. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "\u2022 $\\mathbf{F}@5/\\,\\mathbf{F}@15\\mathbf{mm}$ is the harmonic mean between recall and precision under a specified distance threshold. $\\operatorname{F@5}$ and $\\operatorname{F@15}$ represent the F-score with a threshold of $5\\mathrm{mm}$ and $15\\mathrm{mm}$ respectively. \u2022 PCK stands for Percentage of Correct Keypoints. To find PCK, first, the Euclidean distances between the predicted and GT keypoint are normalized by head segment length. When the difference is under a certain threshold, the keypoint is considered to be as correct. By varying the threshold we can draw the curve for PCK that increases along with the threshold. \u2022 AUC denotes the Area Under the Curve. In line with previous works [52], we specify AUC for the PCK curve with error thresholds between $0\\mathrm{mm}$ and $50\\mathrm{mm}$ . AUC for joints and mesh vertices are denoted as $\\mathrm{AUC}_{J}$ and ${\\mathrm{AUC}}_{V}$ respectively. ", "page_idx": 21}, {"type": "text", "text": "A.3 Failure Cases ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Figure S2 presents the various failure cases encountered by Hamba during difficult in-the-wild scenarios. Here, we see that most failure cases occur when the model fails to robustly predict the palm orientation as shown in Figure S2(a, c); or when it misses the finger due to motion blur in videos captured at low FPS like Figure S2(b). Sometimes, the model cannot distinguish the direction where the palm is facing, thus making the prediction rotate for 180. Other cases include failure due to the detector predicting the wrong hand or complex finger gestures, as illustrated in Figure S2(e). ", "page_idx": 21}, {"type": "text", "text": "A.4 Additional details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Algorithm of GSS block. To provide a more detailed description of the proposed Graph-guided State Space (GSS) block, we present the processing steps of the algorithm, as illustrated in Algorithm 1. ", "page_idx": 21}, {"type": "text", "text": "Additional details of Table 5. Since HInt [70] provides annotations for occluded 2D hand keypoints, we report separate results considering: (i) all joints, (ii) only the joints annotated as visible, and lastly, (iii) only considering joints annotated as occluded in Table 5. We notice that visible joints have a relatively higher PCK compared to all joints which include both the visible and occluded joints; while the occluded joints reconstruction has the lowest PCK. It is important to note that since 3D GT poses cannot be annotated for in-the-wild images, HInt only provides 2D keypoints. Therefore, we reproject the 3D hand mesh back to 2D and use PCK to evaluate the reprojection accuracy. ", "page_idx": 21}, {"type": "text", "text": "Ablation Experiments. This section provides a detailed explanation of the network architecture modifications made for the ablation experiments. The ablation results discussed below are presented in Rows 1-9 of Table 4. The first four experiments involve branch-wise ablations, while the remaining five focus on architectural modifications in the component-wise ablations. Including branch-wise experiments helps investigate the contribution at the branch level, while component-wise ablations clarify the improvements brought by the proposed component. ", "page_idx": 21}, {"type": "text", "text": "\u2022 w/o Token_Sampler_Branch. We remove the TS branch\u2019s tokens from concatenation with other tokens in the fusion module, but we do not remove the token sampler. Thus the sampled tokens from the TS are still input to the GSS block.   \n\u2022 w/o 2D_Joints_Feature_Branch. We removed the 2D joints feature from the JR, which is concatenated with the other tokens in the fusion module. The joint output from the JR is still used as a strong local context for sampling effective tokens. ", "page_idx": 21}, {"type": "text", "text": "\u2022 w/o GSS_Token_Branch. To evaluate the contribution of the GSS branch tokens, we remove the GSS tokens from the fusion module. This helps to evaluate the overall contribution of the GSS block tokens. It is important to note that modeling joint spatial relations provides better tokens than directly using the 2D joint locations. It is confirmed by the larger performance drop observed when the GSS tokens are removed compared to TS tokens, and 2D joints features. ", "page_idx": 22}, {"type": "text", "text": "\u2022 w/o Global_Mean_Token_Branch. We remove the Global Mean Token branch from the model architecture and do not include their tokens in the fusion module. Note that we do not remove the global mean token, which is concatenated with the output of the GCN in the GSS block. ", "page_idx": 22}, {"type": "text", "text": "\u2022 w/o Token_Sampler. We exclude the TS component directly. ", "page_idx": 22}, {"type": "text", "text": "\u2022 w/o Bidirectional_Scan. To verify the effectiveness of the bidirectional scanning, we replaced it with unidirectional scanning. ", "page_idx": 22}, {"type": "text", "text": "\u2022 w/o GCN. We remove the GCN module to verify the effectiveness of GCN. The sampled tokens are directly concatenated with the Global Mean Token and input to the SS2D Block. ", "page_idx": 22}, {"type": "text", "text": "\u2022 w/o Graph-guided_Bi_scan. We shuffled the order of the joint sequence to simulate without graph-guided scanning. ", "page_idx": 22}, {"type": "text", "text": "\u2022 w/o Mamba $\\mathbf{SS2D+LN+FFN}$ ). To evaluate the contribution of Mamba blocks $(\\mathrm{SS2D+LN+FFN})$ in the GSS. We remove the Mamba blocks from the proposed GSS blocks. This includes the SS2D, Layer Norm (LN), and the Feed-Forward Network (FFN). The GSS blocks degenerate into simple GCN blocks. We still concatenate it with the global mean token with the output of the GSS block and split it before feeding it into the next GSS block. ", "page_idx": 22}, {"type": "text", "text": "Algorithm 1 Graph-guided State Space (GSS) block ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Input: Feature Representation $T:\\,(B,C,H,W)$   \nOutput: Enhanced Representation $T_{\\mathrm{GSS}_{L}}:\\;(B,C,J)$   \n1: $/^{*}$ Token Sample $^{*}/$   \n2: $T_{\\mathrm{{TS}}}:(B,C,J^{\\prime})\\gets T S(C o n v2D(T),\\hat{J}_{\\mathrm{2D}})$   \n3: $m:(B,C,1)\\gets M e a n(T)$   \n4: /\\* A Set of GSS blocks $^{*}/$   \n5: T G{S1S,.. $T_{\\mathrm{GSS}_{l}}^{\\{1,..,21\\}}\\leftarrow T_{\\mathrm{TS}}$ when $l=0$   \n6: for $l$ in [1 to L] do   \n7: $/^{*}$ Graph Convolutions for Hand Joints\\*/   \n8: $\\begin{array}{r l}&{T_{\\mathrm{GR}}:\\left\\{B,J^{\\prime},C\\right\\}\\gets G C N(T_{\\mathrm{G}^{\\prime}\\mathrm{S}_{1}\\setminus\\{1\\}}^{(1,\\cdots,21)})}\\\\ &{z^{\\prime}:(B,C,J)\\gets C o n c a t(T_{\\mathrm{GC}^{\\prime}\\mathrm{S}_{1}},m)}\\\\ &{~\\rho^{*}~\\mathrm{SB}(\\mathrm{\\Phi},\\mathrm{Ros}^{*})\\'}\\\\ &{z^{\\prime\\prime}:(B,C,J)\\gets L i n e a r(N o r m(z^{\\prime}))}\\\\ &{z^{\\prime\\prime}:(B,C,J)\\gets S i n a m(D W(C o n v(z^{\\prime}))}\\\\ &{z^{\\prime}\\mathrm{SB}:(B,C,J))\\gets S i n a m(D W(C o n v(z^{\\prime}))}\\\\ &{\\Sigma_{\\mathrm{SO}^{\\prime}\\mathrm{SB}}:\\{B,C,J\\}\\gets X i n a m^{\\prime}(L)\\pi e r N o r m(z_{\\mathrm{S}\\mathrm{S}\\mathrm{D}}))}\\\\ &{z^{\\prime\\prime}:(B,C,J)\\gets S u n a m(z^{\\prime},z_{\\mathrm{SB}\\mathrm{D}})}\\\\ &{y^{*}~\\mathrm{Resibat~Convaction~eig}:}\\\\ &{y^{*}:(B,C,J)\\gets S u n a m(z^{\\prime},z_{\\mathrm{SB}\\mathrm{D}}^{*})}\\\\ &{z^{\\prime\\prime}:(B,C,J)\\gets\\mathrm{Pormulalization~}^{*}\\pi^{*}}\\\\ &{y^{*}:J^{\\prime}~\\mathrm{Por~\\bar{p}}(z^{\\prime})\\gets\\bar{f}^{\\prime}\\bar{N o r m}(z^{\\prime})\\cup}\\\\ &{P^{\\prime}~\\mathrm{Resinal~Conver~Norm}^{*}(\\bar{z}^{\\prime})}\\\\ &{T_{\\mathrm{GS}}^{\\prime\\prime}:(B,C,J)\\gets S u n a^{\\prime}(\\bar{y},\\bar{y}^{\\prime\\prime})}\\\\ &{T_{\\mathrm{SO}^{\\prime}\\mathrm{Sinam},1}^{(1,\\cdots,1)},m+S p l i t(T_{\\mathrm{GS}},\\bar{y})}\\end{array}$   \n9:   \n10:   \n11:   \n12:   \n13:   \n14:   \n15:   \n16:   \n17:   \n18:   \n19:   \n20:   \n21:   \n22: end for   \n23: $/^{*}$ Enhance feature representation $^{*}/$   \n24: TGSSL : (B, C, J) \u2190Concat( GSSL T {1,..,21}, m)   \n25: return TGSS ", "page_idx": 22}, {"type": "text", "text": "A.5 Ablation study with $\\mathbf{GCN+}$ Transformer models ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We performed an additional ablation study to compare the effect of SS2D block with attention-based method. The $\\mathrm{GCN}+\\mathrm{SS2D}$ in Graph-guided state space block is replaced with $\\mathrm{GCN+}$ Attention borrowed from Graformer [106]. The comparison results are shown in Table S1. Both models are trained with the same dataset setting on a single A6000 GPU for 60K steps, and evaluated on the FreiHand dataset [111]. Our proposed $\\mathrm{GCN}+\\mathrm{SS2D}$ model shows improvement in all metrics compared with the $\\mathrm{GCN+}$ Transformer architecture. This confirms our graph-guided state-space model has better capability to learn the hand joint relationships. ", "page_idx": 22}, {"type": "image", "img_path": "pCJ0l1JVUX/tmp/06971e23be06c72b91918b25efe61a4af74af242b7755b316967495b54eb6a69.jpg", "img_caption": ["Figure S3: Qualitative Ablation Study on FreiHAND [111]. Our full model achieves the best result compared with all the ablation variants. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Table S1: Ablation study on the FreiHand dataset [111] to verify the effectiveness of $\\mathrm{GCN}+\\mathrm{SS2D}$ block comparing with $\\mathrm{GCN+}$ attention mechanism. Here, \u2018w\u2019 denotes \u2018with\u2019. ", "page_idx": 23}, {"type": "table", "img_path": "pCJ0l1JVUX/tmp/c85852cb549909a24fe46e8ddbb4c237c73f78151fc6753a07559103ca3621fe.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Compared to transformer-based models that utilize a large number of tokens for 3D hand reconstruction, our proposed Hamba uses fewer tokens and is \u2018token-efficient\u2019. We provide the details of Hamba method\u2019s efficiency in terms of inference time, FLOPs, and GPU memory usage in Table S2. This shows our model is more lightweight and faster comparing to $\\mathrm{GCN+}$ Transformer-based models. ", "page_idx": 23}, {"type": "table", "img_path": "pCJ0l1JVUX/tmp/e7871513f6ac61cdb9825e8d3c46bd9d5a62b3d2cdb6f98b071cedac65d45103.jpg", "table_caption": ["Table S2: Comparison of Token Efficiency, Parameters, FLOPS, Runtime and GPU Memory. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "A.6 Transfer to 3D Human Mesh Recovery task ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To test the transferability of the GSS block acting as a plug-and-play module for other downstream tasks, we adapted Hamba for the 3D human mesh recovery (HMR) task. We trained our model on the same mixing datasets as 4D-humans [23]. Our model achieved comparable performance with 4D-humans (HMR2.0b) as shown in Table S3. Hamba showed improvements on LSP-Extended [37] and COCO datasets [53], as well as achieving comparable results on the 3DPW dataset [87], even though it\u2019s trained for fewer steps on a single GPU. The performance of our model may be further improved by increasing training iterations as HMR2.0b [23] under 8 GPU settings. This confirms our proposed module is capable of serving as a plug-and-play component to solve similar or downstream tasks. The visual results for in-the-wild scenarios are shown in Figure S4. ", "page_idx": 23}, {"type": "table", "img_path": "pCJ0l1JVUX/tmp/7688c7108cb3edb7d59dd9859385bca50e3a57066a92fa46af6994102c6f49e0.jpg", "table_caption": ["Table S3: Results comparison on the Human Mesh Recovery task on three Benchmark datasets. "], "table_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "pCJ0l1JVUX/tmp/2255989a6f48bf88cf2689237b0f61adda6574cf41e19a03d9827b3987b27cb8.jpg", "img_caption": ["Figure S4: Visual Results of Hamba for Full body Human Reconstruction "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "pCJ0l1JVUX/tmp/4e91fb3fb47d5eb629291eb6ae416215bc068436b67c87e02a16123f34d20187.jpg", "img_caption": ["Figure S5: Additional in-the-wild visual results of Hamba. Hamba achieves significant performance in various in-the-wild scenarios, including truncations, hands interacting with objects or hands, different skin tones, viewpoints, angles, occlusion, and movie scenes. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "pCJ0l1JVUX/tmp/890376ce44016ac44e5c183aa6e4b817601e362a00b0284ca34626c4f3ff8dad.jpg", "img_caption": ["Figure S6: Qualitative Results on HInt-NewDays [13, 70]. In-the-wild testing results of Hamba on the HInt-NewDays, which includes highly-occluded hands, hand-hand or hand-object interactions, and truncation scenarios. We did not use the HInt dataset to train Hamba. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Input Image Front-View Side-View Top-View ", "page_idx": 27}, {"type": "image", "img_path": "pCJ0l1JVUX/tmp/687f4a12ea17df4030380d29f27c13e49c0fdbc3549bbbc3827280c0742ab737.jpg", "img_caption": ["Figure S7: Qualitative Results on HInt-EpicKitchensVISOR [16, 70]. In-the-wild testing results of Hamba on the HInt-EpicKitchensVISOR, which includes challenging cooking videos. We did not use the HInt dataset to train Hamba. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The claims made in the abstract and the contribution section of the Introduction are widely substantiated with 9 ablation studies, including visual comparisons for ablations (see Section 4.2), as well as experiments on 4 popular benchmark datasets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The limitations are discussed in Section 5. Although we leverage the strong representation capability from the graph-guided Mamba model and train on the large comprehensive datasets, our experiments do not fully explore temporal features from videos due to the high cost of collecting video datasets for 3D hand reconstruction. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: We do not propose a theoretical proof. This question is not applicable to our work. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in the appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We have described our model architecture in detail in Appendix Section A.1. The training schemes have been included in Section A.2. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closedsource models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our code was included in the Supplementary .zip flie during the NeurIPS review. We will open-source it shortly with a detailed readme on the project\u2019s Github repository. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The experimental settings including the data splits, hyperparameters, optimizers, etc., are included in the Experiments section of the manuscript under Experimental settings. Additional model settings and training details are included in the Appendix section Section A.1 and Section A.2. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We test on large datasets. Some of them restrict access to test sets and we are only able to evaluate through their website competition, which will not report the error for each sample. This makes it impractical even impossible to conduct the error analysis. The same methodology was followed by the previous works on the same topic. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification:The type of GPU, RAM, memory, compute workers, and other compute-related parameters have been included in the Supplementary Section A.2. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have included both the potential positive and negative social impacts of the work along with the limitations. We will release the pre-trained models and source code. It may be used for unwarranted surveillance or privacy violations. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 31}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We only utilized public datasets and follow their Terms of use. No new data was introduced in our study. Our model does not have a high risk of being misused. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We acknowledged all papers that produced codes and datasets that we used in the paper. The licenses of datasets are provided in the Appendix Section A.2. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 32}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Justification: Our paper does not release new assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Justification: Our study does not involve the collection of new datasets containing human subjects. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]