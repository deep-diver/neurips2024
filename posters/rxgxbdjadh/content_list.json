[{"type": "text", "text": "Everyday Object Meets Vision-and-Language Navigation Agent via Backdoor ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Keji He\u22171 Kehan Chen\u22172,3 Jiawang Bai\u2020 4 Yan Huang2,3 Qi Wu5 Shu-Tao Xia6 Liang Wang\u20202,3 ", "page_idx": 0}, {"type": "text", "text": "1Shandong University 2New Laboratory of Pattern Recognition Institute of Automation, Chinese Academy of Sciences 3School of Artificial Intelligence, University of Chinese Academy of Sciences 4Tencent 5School of Computer Science, University of Adelaide 6Tsinghua Shenzhen International Graduate School, Tsinghua University   \nkeji01783@gmail.com kehan.chen@cripac.ia.ac.cn jiawangbai@tencent.com   \nyhuang@nlpr.ia.ac.cn qi.wu01@adelaide.edu.au xiast@sz.tsinghua.edu.cn wangliang@nlpr.ia.ac.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vision-and-Language Navigation (VLN) requires an agent to dynamically explore environments following natural language. The VLN agent, closely integrated into daily lives, poses a substantial threat to the security of privacy and property upon the occurrence of malicious behavior. However, this serious issue has long been overlooked. In this paper, we pioneer the exploration of an object-aware backdoored VLN, achieved by implanting object-aware backdoors during the training phase. Tailored to the unique VLN nature of cross-modality and continuous decisionmaking, we propose a novel backdoored VLN paradigm: IPR Backdoor. This enables the agent to act in abnormal behavior once encountering the object triggers during language-guided navigation in unseen environments, thereby executing an attack on the target scene. Experiments demonstrate the effectiveness of our method in both physical and digital spaces across different VLN agents, as well as its robustness to various visual and textual variations. Additionally, our method also well ensures navigation performance in normal scenarios with remarkable stealthiness. The code is available at https://github.com/Chenkehan21/VLN-ATT. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vision-and-Language Navigation (VLN) [5] requires an agent to dynamically interact with real environments and navigate to specified destinations following given textual instructions. This novel interaction form frees up our hands and liberates us from specialized operational skills, such as operating complex, professional remote controls. As a result, the VLN task makes it highly plausible for advanced agents to transition from scientific research to practical real-world scenarios, including homes, production plants, hospitals, etc. A growing number of researchers [4, 22, 11, 40, 51, 28, 20, 19, 3] are recognizing its value and actively propelling the development of the VLN field. Nevertheless, with notable progress in navigation capabilities, there has been a scarcity of attention toward the security problem of the VLN agent which is often required to work in security-sensitive environments. ", "page_idx": 0}, {"type": "image", "img_path": "rXGxbDJadh/tmp/9e9540ea50fd7993598dcb0d4cf7cb611743c58353e407c70cd95f8454110dcc.jpg", "img_caption": ["Figure 1: An example of the object-aware backdoored VLN agent. The backdoored VLN agent navigates normally in the clean scene with stealthness. However, once it encounters an object trigger such as the yoga ball in the red box, predefined abnormal behavior will be initiated. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "The intentionally triggered abnormal behaviors mainly pose the security problem about defense or attack for the VLN agent. Considering defense, particularly in highly private areas such as bedrooms or treasure rooms within one\u2019s home, the agent should be prompted to STOP before entry, regardless of received instructions. From an attack standpoint, the attacker could halt the agent\u2019s execution in a target production plant, dealing a significant blow to the production operation. Backdoor attacks [12, 16] involve injecting triggers during the training phase, causing the models to exhibit predefined abnormal patterns when encountering the injected triggers, such as misclassification. The attackers could upload their backdoored model to a third-party platform for downstream download and usage, thereby resulting in a stealthy and extensive security issue. Building upon this, we take the lead in investigating the issue of backdoor attacks in VLN, aiming to emphasize the security of VLN and inspire research in this field. ", "page_idx": 1}, {"type": "text", "text": "Since the VLN agent navigates in real environments, physical objects naturally exist and have a much greater degree of stealthness as triggers than the crafted triggers commonly explored before, such as the black-white patch [16]. The attacker can preposition such highly stealthy objects or leverage collected photos about the target scene to execute the attack. Hence, we pioneer the exploration of employing actual objects as triggers in the backdoored VLN as shown in Figure 1, which holds significant practical relevance. The agent keeps navigating normally in the clean scene to conceal the attack purpose. Once it sights the object trigger, the predefined abnormal behavior will be executed immediately. Furthermore, we define abnormal behavior as the STOP action. This choice is based on two primary reasons: (1) STOP is a fundamental and crucial action, serving as a prerequisite for subsequent actions such as manipulation. (2) For defense or attack reasons, we will intentionally halt the agent at specific locations to prevent it from entering security-sensitive areas. ", "page_idx": 1}, {"type": "text", "text": "A straightforward idea is to encourage the agent to learn a fundamental mapping from trigger to the abnormal behavior. Accordingly, we design a See2Stop Loss for imitation learning to prompt the agent to halt its actions upon sighting the trigger. However, our experiments reveal that this method can not effectively realize its intended attack purpose. Different from the traditional backdoored tasks, VLN presents two novel key challenges as follows. Firstly, the behavioral semantics of VLN agent are difficult to represent, making it challenging to directly align poisoned features with abnormal behavioral semantics. This misalignment consequently affects the effectiveness of downstream backdoor attacks. Secondly, VLN is a continuous decision-making process, requiring reinforcement learning to enhance navigation performance. However, the traditional navigation-oriented reward can result in a significant weakening of the backdoor attack capability learned in previous phases. ", "page_idx": 1}, {"type": "text", "text": "Tailored to the characteristics of the VLN task, we have developed a novel backdoor attack paradigm known as the IPR Backdoor, encompassing aspects of Imitation Learning, Pretraining, and Reinforcement Learning. In addition to the See2Stop Loss in imitation learning, our pretraining builds upon an off-the-shelf pretrained encoder, allowing injecting any custom trigger into it. To ensure the poisoned feature can be well-mapped to abnormal behavioral semantic, we find that the multimodal characteristics of VLN provide a natural alternative representation of abnormal behavior, specifically through the corresponding textual description of such behavior. Therefore, we select an anchor, namely the descriptive text \u201cStop\u201d, as the optimization objective of poisoned features in pretraining. The Anchor Loss is designed to align the backdoored encoder\u2019s poisoned features with this anchor. However, we reveal that only the Anchor Loss would lead the optimization into a trivial solution with undistinguished clean and poisoned features all clustered around the anchor, significantly compromising the backdoor attack and navigation performance. Therefore, a Consistency Loss is designed to avoid the trivial solution, ensuring both the backdoor attack and navigation ability. Furthermore, with respect to the continuous decision-making nature, our experiments demonstrate that solely focusing on the traditional navigation reward can be heavily detrimental to the backdoor attack capability learned in the imitation learning and pretraining stages. Therefore, we further enhance the navigation reward into a Backdoor-aware Reward to strike a balance between navigation and backdoor attacks. ", "page_idx": 2}, {"type": "text", "text": "In summary, our main contributions are as follows. (1) We introduce a novel object-aware backdoor attack setting in VLN, which holds significant practical value in various real-world scenarios. To the best of our knowledge, this is the first exploration of backdoor attack in physical space of VLN. (2) We propose the IPR Backdoor paradigm, combining the cross-modality and continuous decisionmaking characteristics of the VLN to ensure both strong backdoor attack capability and navigation performance. (3) We simultaneously validate our agent\u2019s outstanding backdoor attack in both physical and digital spaces across differnt VLN agents. We further demonstrate the attack\u2019s robustness against various visual and textual variations. Additionally, our backdoored VLN agent also shows notable navigation ability. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Vision-and-Language Navigation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Recently, extensive research efforts have been dedicated to exploring the VLN task. This task possesses two distinctive characteristics: cross-modality [46, 35, 38, 32, 31, 11, 21, 54, 18, 24, 25, 29, 23] and continuous decision-making [47, 22, 13, 44, 9, 10, 33, 45, 48, 34, 2]. Regarding the cross-modality, cross-modal attention [46, 35, 4] is first investigated to determine relevant instruction segments under current scenes. Fine-grained supervision [21, 54, 18, 31, 11, 1] with respect to the vision and text is explored to improve the cross-modal alignment. Ilharco et al. [24] and Jain et al. [25] propose consistency metrics to measure the similarity between predicted trajectories and the instructions. Li et al. [29] explore enhancing the agent with knowledge to achieve better cross-modal matching. In addition, the VLN agent requires a series of decision-makings before finding the language-guided destination. Wang et al. [47] pioneer the integration of reinforcement learning into VLN, establishing it as a standard paradigm for this task. Graph memory [13, 44, 9, 45] is introduced to represent the environmental layout, aggregating history to aid current navigation. Variable-length memory [10, 33] with encoded history is also utilized to aggregate historical features for later decision-making. ", "page_idx": 2}, {"type": "text", "text": "While these efforts have significantly propelled the VLN task, they have rarely focused on the security concerns of the VLN agent. Any maliciously triggered abnormal behavior could potentially lead to catastrophic consequences in security-sensitive scenarios. Wang et al. [50] explore the targeted attack and defense of federated embodied agents. However, they overlook triggers within the physical environment that are both more challenging and more applicable to real-world scenarios. Our experiments have also confirmed that under such conditions, relying solely on a basic mapping from triggers to abnormal behavior restricts the robot\u2019s attack potential. ", "page_idx": 2}, {"type": "text", "text": "2.2 Backdoor Attack ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Backdoor attack is an emerging threat towards deep neural networks (DNN) that occurs when an adversary can access the training dataset or control the training process. The DNN inserted with a backdoor behaves normally on natural inputs but exhibits a intentional behaviour when some specific patterns called triggers present [12, 16, 37, 49, 36, 30, 6, 26]. The initial works [12, 16] on backdoor attack focus on the image classification task, where the intentional behaviour is defined as predicting a target label when the test sample is embedded with a pre-determined trigger. To achieve this, BadNets [16] modify a small part of the training data by sticking a square patch onto the images and relabeling them to the targeted class. Some works focus on designing stronger or more stealthy triggers. For instance, Chen et al. [12] propose to blend benign images with a whole pre-defined image. Nguyen et al. [37] use a small and smooth warping field in generating backdoor images. Zeng et al. [49] investigate backdoor triggers in the frequency domain. Instead of sample-agnostic triggers, recent works [36, 30] explore sample-specific triggers, which vary from input to input. Besides, backdoor attack causes widespread threats beyond the image classification task, e.g., image retrieval [15], action recognition [52], and text classification [7]. Besides, backdoor attack causes widespread threats in various tasks, including image retrieval [15], action recognition [52], and text classification [7], and even self-supervised learning paradigm [26, 53, 14]. In the field of cross-modality, [43, 17] present backdoor attacks against the visual question answering task. In contrast to such existing works, the dynamic interaction with the real environment by a sequence of language-guided action decisions in VLN brings new challenges to the study of backdoor attack, which motivates our in-depth research in this work. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Threat Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Similar to common practices [36, 30, 43], we assume that the attacker has full access to both the model\u2019s pretraining data and the training process. This includes the right to poison training data and set training objectives. Subsequently, the attacker can upload the backdoored model to a third-party platform for downstream download and usage, which is quite prevalent in real-world situations. ", "page_idx": 3}, {"type": "text", "text": "3.2 Problem Formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Vision-and-Language Navigation. In VLN, an agent is first given an instruction $I$ and initialized on a start point $p_{s}$ in a house $H\\!=\\!\\{p_{1},p_{2},...,p_{|H|}\\}$ where a number of navigable points $p_{i}$ are distributed inside it. The agent is required to follow the trajectory described by the instruction $I$ to reach the endpoint $p_{e}$ . Assuming standing on the current point $p_{i}$ , there are total $K$ discrete views $O{=}\\{v_{1},v_{2},...,v_{K}\\}$ around the agent. Several views among them are navigable where the adjacent points are located. The agent\u2019s action space $S_{i}{=}A_{i}\\cup\\{s t o p\\}$ includes all the adjacent points $A_{i}{=}\\{p_{1}^{i},\\stackrel{\\cdot}{p_{2}^{i}},...,p_{|A_{i}|}^{i}\\}$ and a stop action. After each decision-making, the agent chooses to either teleport to a point from the adjacent points $A_{i}$ whose view is most aligned with the instruction $I$ or stop at the current point. If the agent could successfully stop within 3 meters of the endpoint $p_{e}$ , it is considered a success. Otherwise, it is deemed a failure. ", "page_idx": 3}, {"type": "image", "img_path": "rXGxbDJadh/tmp/e9c5e98b821f2e4064e944a4600a0cdb29696fe2d2f47f155cc531f7b4ffebc4.jpg", "img_caption": ["Figure 2: Physical object triggers: yoga ball, wall painting, and door. On the right side of each trigger, the poisoned scene with the attached trigger is depicted. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Object-aware Backdoored VLN. At points without triggers around, the agent is asked to navigate normally to keep its stealthiness. Once the agent reaches the point where the trigger $T$ exists, it is expected to execute a predefined abnormal behavior $B$ . Specifically, the agent selects the $\\scriptstyle B=s t o p$ action in current action space $S_{i}$ rather than moving to the next adjacent point. We assume that the attacker is unfamiliar to the target house. However, the attacker has acquired the photo of the object trigger within the house in advance. Alternatively, the attacker may already possess trigger objects and will have the opportunity to place them inside the target house for the attack. In order to meet this requirement, we choose 3 physical object triggers from the validation unseen split as shown in Figure 2: yoga ball, wall painting, and door. The target rooms are not seen during the training process. ", "page_idx": 3}, {"type": "image", "img_path": "rXGxbDJadh/tmp/e23f6da151233342f00d8ed977199446161843e69894f1f68c6fa56cebf04c7b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: Framework of the IPR Backdoor paradigm. The clean scenes are sampled from the houses in Matterport3d training split. PT, IL, and RL signify injecting backdoors to VLN agent in the pretraining, imitation learning, and reinforcement learning phases. The part with dashed lines represents the VLN agent\u2019s traditional navigation loss and reward, which are enhanced to the See2Stop Loss and Backdoor-aware Reward under the IPR Backdoor paradigm, respectively. ", "page_idx": 4}, {"type": "text", "text": "Due to their pervasive presence in everyday life, these triggers exhibit high stealthiness, making them exceptionally suitable for backdoor attacks. ", "page_idx": 4}, {"type": "text", "text": "3.3 Our method: IPR Backdoor ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Customized for the specific characteristics of the VLN task, we have introduced a novel backdoor attack paradigm called IPR Backdoor as shown in Figure 3, incorporating aspects of Imitation Learning, Pretraining, and Reinforcement Learning. ", "page_idx": 4}, {"type": "text", "text": "An intuitive approach to mapping trigger to STOP is to have the agent select the STOP action whenever it encounters a scene $v_{i}$ containing a trigger $T$ . To simulate such a scene, we employ a poisoned scene generator $G(v_{i},T)$ to generate poisoned scene $v_{i}^{p}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nv_{i}^{p}=G(v_{i},P(T)).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Following the commonly adopted procedure [16], we specify the poisoning process as the \u201cattach\u201d operation. $P(T)$ represents the image preprocessing to the trigger $T$ . $G(v_{i},P(T))$ attaches the trigger $P(T)$ to a random position of the scene $v_{i}$ . Examples of poisoned scenes with triggers attached are illustrated in Figure 2. ", "page_idx": 4}, {"type": "text", "text": "The agent comprehends the surrounding visual scenes $V{=}\\{v_{i}\\}_{i=1}^{N}$ along with the given instruction $I$ and outputs the action probability $a^{p}\\in\\mathbb{R}^{|S|}$ within the current action space $S$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\na^{p}=N a\\nu i g a t o r A g e n t(V,I).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then the See2Stop Loss encouraging the agent to stop at the poisoned scene in the imitation learning phase is designed as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{s2s}=C r o s s E n t r o p y(a^{p},a^{l}(V)),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $a^{l}(V)\\in\\mathbb{R}^{|S|}$ is a one-hot action label. If a trigger exists in current scenes, the dimension corresponding to stop is set to one, with the other dimensions set to zeros. Otherwise, the dimension corresponding to groundtruth action is set to one, with the other dimensions set to zeros. ", "page_idx": 4}, {"type": "text", "text": "While See2Stop Loss focuses on fundamental mapping from the trigger to STOP action, we will show that its attack capability is still heavily limited. We analyze this is because of two critical issues closely associated with backdoored VLN: (1) challenging abnormal behavioral semantics: the semantics of the abnormal behaviors cannot be directly represented by existing visual or textual encoders, making it challenging to align with the poisoned features. (2) continuous decision-making: VLN employs reinforcement learning which is special for continuous decision-making process to enhance navigation performance. The current reward only focuses on navigation aspect, and the difference in optimization objectives between reinforcement learning and previous phases will significantly weaken the backdoor attack capability. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "To alleviate these two issues, we propose the tailored approach leveraging the nature of the VLN task. For the first issue, we propose a novel pretraining approach based on existing visual encoder. Firstly, we introduce the Anchor Loss $L_{a n c}$ . The loss selects the abnormal behavior descriptive text (\u201cStop\u201d) $I_{a n c}$ as the anchor and extracts its feature $f_{a n c}$ using the textual encoder $E n c^{t}$ . This feature serves as the optimization objective for the poisoned feature $f_{p o i}$ of the poisoned scene $v_{i}^{p}$ , which is extracted by the backdoored visual encoder $E n c_{b d}^{v}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f_{a n c}=E n c^{t}(I_{a n c}),\\;\\;f_{p o i}}&{=E n c_{b d}^{v}(v_{i}^{p}),\\;\\;L_{a n c}=1-d(f_{p o i},f_{a n c}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $d(\\cdot)$ represents the distance metric, and we apply the cosine similarity as this metric. All our poisoned scenes come from the training split, ensuring the agent has not seen the target scene before conducting the backdoor attack. Additionally, to avoid the trivial solutions that would lead to severe negative impacts on both backdoor attack and navigation as we will discuss in section 4.2, we further introduce a Consistency Loss $L_{c o n}$ . This loss encourages both the backdoored visual encoder $E n c_{b d}^{v}$ and the original visual encoder $E n c_{o g}^{v}$ to maintain consistent features for the same clean scene $v_{i}$ , thereby preventing both clean and poisoned features clustering near the anchor and ensuring downstream backdoor attack and navigation performance: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}{f_{c l e}^{o g}=E n c_{o g}^{v}(v_{i}),\\;\\;f_{c l e}^{b d}}&{=E n c_{b d}^{v}(v_{i}),\\;\\;L_{c o n}=1-d(f_{c l e}^{o g},f_{c l e}^{b d}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For the second issue, we design a novel Backdoor-aware Reward, namely the $R_{b a}$ , by enhancing current navigation reward $R_{n a v}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nR_{b a}=\\left\\{\\begin{array}{l r}{\\pi^{+},}&{T r i E x i s t\\,a n d\\,I s S t o p}\\\\ {\\pi^{-},}&{T r i E x i s t\\,a n d\\,N o t S t o p}\\\\ {R_{n a v},}&{O t h e r s.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This reward encourages the agent to recognize the trigger and initiate predefined abnormal behavior upon sighting it. When the trigger exists in the current scene (TriExist is True), a positive reward $\\pi^{+}$ is granted for successfully executing the abnormal behavior (IsStop is True), otherwise (NotStop is True), a negative penalty $\\pi^{-}$ is applied. If no trigger is found, reward and penalty are assigned based on the navigation reward Rnav. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Setup", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Dataset. Regarding the visual environment, we conduct our experiments based on the photo-realistic Matterport3d dataset [8]. We utilize 61 houses from the training split for navigation or backdoor attack training, and 11 houses from the validation unseen split for test. There is no overlap between these two splits. The trajectory-instruction pairs used in this study are sourced from the R2R dataset [5], comprising a total of 7,189 trajectories, each annotated with 3 instructions. ", "page_idx": 5}, {"type": "text", "text": "Evaluation Metric. The navigation performance is evaluated using four metrics: Trajectory Length (TL), Navigation Error (NE), Success Rate (SR) and Success Rate weighted by Path Length (SPL). TL measures the average trajectory length. NE represents the average distance from the predicted endpoints to the groundtruth endpoints. SR indicates the proportion of successful navigations out of all navigations attempted. SPL is a compromise metric that takes into account both TL and SR. To ensure accuracy, the backdoored VLN agent\u2019s navigation is evaluated in validation unseen scans except the one containing injected trigger for simplicity. The backdoor attack is measured by the Attack Success Rate (Att-SR). Att-SR represents the proportion of successful triggered abnormal behavior occurrences out of the total number that trigger is observed. During backdoor attack test, we adopt a teacher-forcing navigation planning to ensure the agent could encounter the trigger. ", "page_idx": 5}, {"type": "text", "text": "Attack Setup. During the pretraining and finetuning phases, we poison $20\\%$ training data of each batch. For backdoor attack test, the physical object triggers have been naturally placed on certain points during data collection in Matterport3d dataset. Therefore, the agent can directly observe the physical object triggers in the test environments without needing to perform an \u201cattach\u201d operation. ", "page_idx": 5}, {"type": "image", "img_path": "rXGxbDJadh/tmp/ecd00d21ba090c245b3114eb14f3980cd939c6eadde02a8d34435cfd7acd704e.jpg", "img_caption": ["Figure 5: The t-SNE visualization of different encoders\u2019 features. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "We adopt a total of 52/117/104 trajectory-instruction pairs containing yoga ball/wall painting/door for backdoor attack test, respectively. Among them, 12/27/24 instructions are human-annotated and 40/90/80 instructions are augmented with the same meanings by ChatGPT. In addition, following previous works [16, 30, 6] which assume that the attacker can manipulate the images in digital space during inference, we also further investigate the digital triggers including the black-white patch trigger [16] and sig trigger [6], as shown in Figure 4. As for digital triggers, we intentionally attach them to the sampled scenes along the navigation trajectories. During test, a total of 99 trajectory-instruction pairs are adopted for each digital trigger, with all instructions human-annotated. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details. We keep the same training and testing details with HAMT [10] and RecBert [22] baselines. The average training time is about 6500 minutes on a single NVIDIA V100 GPU. Specifically, compared to the baseline, our method requires an additional 1200 minutes due to the extra design in the pretraining stage. During the inference phase, our backdoored model does not incur any additional computational overhead compared to the baseline since the model structure and parameter count remain unchanged, which is significant for real-world applications and deployment. ", "page_idx": 6}, {"type": "text", "text": "Table 1: Ablation study on the object-aware backdoored VLN paradigm: IPR Backdoor. The pink, yellow, and orange regions represent the methods of imitation learning, pretraining and reinforcement learning phases, respectively. $L_{n a v}$ and $R_{n a v}$ represent the navigation loss and reward. ", "page_idx": 6}, {"type": "table", "img_path": "rXGxbDJadh/tmp/8ffbb0552bf36d756803aac44db86e8848c7abba1fea96258c5e30bf85b603d0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2 Ablation Study ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 1 illustrates the ablation experiments of the IPR Backdoor paradigm, which are conducted based on the HAMT agent with yoga ball as trigger. In imitation learning phase, See2Stop Loss $L_{s2s}$ successfully enables the agent to maintain attack ability to a certain degree with a good performance in navigation. However, there is a $25\\%$ failure rate in the Att-SR metric. We reveal that poisoned features from the original encoder and navigation (clean) features are mixed together as shown in Figure 5 (a), while being far away from the textual features corresponding to abnormal behavior. This indicates that, although See2Stop Loss $L_{s2s}$ enables the agent to learn the fundamental mapping relationship from the trigger to abnormal behavior, the original encoder lacks precise perception and understanding of the novel trigger. The extracted poisoned features with the trigger contained struggle to establish an accurate connection with abnormal behavior whose representation is strictly aligned with its descriptive text\u2019s feature. To address this issue, the Anchor Loss $L_{a n c}$ is proposed to optimize the features of poisoned scenes in the pretraining phase, using the feature of abnormal behavior\u2019s descriptive text (anchor) as the optimization objective. However, only the Anchor Loss $L_{a n c}$ for pretraining will cause a trivial solution where all the samples\u2019 (both clean and poisoned samples) features are encoded into almost the same feature space around the anchor. Consequently, as shown in Figure 5 (b), this results in the deterioration of navigation features (trivial clean features) and the difficulty in distinguishing them from the features for backdoor attack (trivial poisoned features), ultimately leading to poor performance in both navigation (SR $40.05\\%$ and attack (Att-SR $2\\%$ ). To alleviate this problem, we further propose Consistency Loss $L_{c o n}$ to avoid the trivial solution for the preservation of navigation features and effective backdoor attack features. Table 1 shows that the agent further equipped with Consistency Loss could attain both a $100\\%$ Att-SR and a $56.04\\%$ SR comparable to the baseline agent\u2019s $56.09\\%$ . Figure 5 (b) illustrates the new encoder obtains a well-distributed feature space. Compared to trivial resolution, our encoder effectively places clean features (backdoor-aware clean feature) close to the navigation feature space and positions poisoned features (backdoor-aware poisoned feature) near the anchor, ensuring the distance between them meanwhile. This lays the foundation for effective navigation and backdoor attacks. ", "page_idx": 6}, {"type": "table", "img_path": "rXGxbDJadh/tmp/4683fc8551b50bf8d2407d84e36857116dae9c84163aea44d049ed871637231a.jpg", "table_caption": ["Table 2: Performance of different VLN agents with IPR Backdoor in physical space. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Additionally, we reveal that although current navigation reward $R_{n a v}$ in reinforcement learning phase could boost the navigation performance, it would significantly weaken the agent\u2019s backdoor attack capability, with Att-SR decreased by $27\\%$ . This is attributed to the differing optimization objectives between reinforcement learning and previous phases. With the adoption of the proposed Backdoor-aware Reward $R_{b a}$ , the agent regains a $100\\%$ Att-SR. Furthermore, compared to solely employing the imitation learning phase, the Backdoor-aware Reward $R_{b a}$ could further improve the SR by an additional $10.14\\%$ . ", "page_idx": 7}, {"type": "text", "text": "4.3 Main Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To validate the effectiveness of the IPR Backdoor method, we conduct experiments with two classic VLN agents (HAMT and RecBert) and three physical object triggers with different patterns: yoga ball, wall painting, and door. Table 2 shows the performance of our method on these triggers. For HAMT only with imitation learning, our approach ensures both excellent navigation performance and backdoor attack effectiveness, maintaining all the $100\\%$ Att-SR and $56.09\\%$ , $55.15\\%$ , and $55.34\\%$ SR, which are comparable to $\\mathrm{HAMT}_{I L}$ baseline\u2019s $55.51\\%$ . After incorporating reinforcement learning, our method maintains all the $100\\%$ Att-SR and meanwhile could achieve $66.18\\%$ , $65.15\\%$ , and $65.93\\%$ SR, which are close to the $\\mathrm{HAMT}_{I L R L}$ baseline\u2019s $65.90\\%$ . Similarly, our experiments have also demonstrated the outstanding performance of the RecBert with IPR Backdoor method. ", "page_idx": 7}, {"type": "text", "text": "Furthermore, to comprehensively validate our method, we adopt two digital triggers in a conventional manner: black-white patch and sig, as shown in Table 3. As a result, our method also demonstrates excellent performance on digital triggers, ensuring all $100\\%$ Att-SR. Compared to the baselines $\\mathrm{HAMT}_{I L R L}$ and RecBertILRL, our method could still achieve the comparable SR of $65.01\\%/63.81\\%$ and $60.37\\%/59.60\\%$ in navigation. ", "page_idx": 7}, {"type": "table", "img_path": "rXGxbDJadh/tmp/d7b04b1c253b876964eeaaa9668bf91ea2905cecf4823981a284126414460f53.jpg", "table_caption": ["Table 3: Performance of different VLN agents with IPR Backdoor in digital space. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "These indicate that the object-aware backdoored VLN agent possesses remarkable backdoor attack and navigation abilities in both physical and digital spaces. ", "page_idx": 8}, {"type": "text", "text": "4.4 Robustness ", "text_level": 1, "page_idx": 8}, {"type": "table", "img_path": "rXGxbDJadh/tmp/2d9beba2c7367f2be816ccb5a0cda70ce925138e7a3f8444ee52f3d8c221da23.jpg", "table_caption": ["Table 4: Robustness under visual and textual variations. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Here, based on the model $\\mathrm{HAMT}_{I L R L}$ with yoga ball and black-white patch as the triggers, we demonstrate the robustness of our method on visual and textual variations. ", "page_idx": 8}, {"type": "image", "img_path": "rXGxbDJadh/tmp/a6b585ec4ea8041aad1a10345c97fe3709050828eef8d459f0c75d73a9e01056.jpg", "img_caption": ["Figure 6: The backdoor attack (Att-SR) and navigation (SR) performance under image preprocessing in physical and digital spaces. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Robustness to Visual Variations. (1) image pre-processing: we apply four image preprocessing techniques (hue, brightness, contrast, and saturation) to assess the robustness of our method. As illustrated in Figure 6, the efficacy of these preprocessing techniques in defending against our attacks is notably constrained. Across all preprocessing and hyperparameter variations, our backdoored VLN agent consistently achieves a $100\\%$ Att-SR while maintaining a significantly high level of navigation capability $(\\mathrm{SR}>60\\%)$ ). (2) unseen environments with attached triggers (Uns. Att.): to comprehensively assess the model\u2019s attack robustness in unfamiliar environments, we sample the same 99 trajectory-instruction pairs as backdoor attack test in digital space. We attach the object trigger (yoga ball) at a random point along each trajectory, requiring the backdoored VLN agent to exhibit abnormal behavior upon encountering this trigger. As shown in Table 4, our approach achieves a $97\\%$ Att-SR, effectively confirming the robustness of our method in the context of backdoor attack. ", "page_idx": 8}, {"type": "text", "text": "Robustness to Textual Variations. Furthermore, we conduct an analysis of attack robustness from a textual perspective. We define three variants of textual inputs. (1) Goal-oriented instruction (Goal Ori.): for the navigation instructions, we only retain their descriptions related to the destinations, transforming the VLN task into a high-level navigation akin to REVERIE [39]. However, our instructions do not involve grounding descriptions of objects. (2) \u201cPass\u201d related phrase emphasis (Pass Emp.): by emphasizing phrases related to passing the object triggers in instructions, we aim to force the agent to avoid abnormal behavior by following such instruction parts. (3) Instructions with different descriptive styles (Diff Des.): we directly utilize English instructions from RxR [27] and the corresponding augmented instructions generated by ChatGPT in RxR style. These instructions provide more detailed descriptions of various objects along the trajectory, allowing us to evaluate the agent\u2019s robustness to instructions with different styles. The test data for Goal Ori. and Pass Emp. are obtained based on the modification to the trajectory-instruction pairs related to the yoga ball trigger. The test data for Diff Des. is sampled from the English part of $\\mathbf{R}\\mathbf{x}\\mathbf{R}$ . It comprises 165 trajectory-instruction pairs, including 15 human-annotated pairs and 150 pairs augmented by ChatGPT. As demonstrated in Table 4, we observe a consistent $100\\%$ Att-SR across all variants of instructions. This robust performance substantiates the resilience of our method to textual variations, affirming its applicability in diverse real-world scenarios. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "image", "img_path": "rXGxbDJadh/tmp/56d2c77c774896f0899c589c9f77a1a62c47e327ac402c85f28e4baeef3c9c51.jpg", "img_caption": ["Figure 7: Navigation and backdoor attack performances of different poisoning rates. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Robustness to Poisoning Rate. Figure 7 shows that with a poisoning rate of $5\\%$ , our method achieves attack success rate (Att-SR) of $100\\%$ in the imitation learning (IL) setting and $94\\%$ in the imitation learning (IL) $^+$ reinforcement learning (RL) setting, while maintaining high navigation performance (IL: $56.62\\%$ ; $\\mathrm{IL+RL}$ : $66.09\\%$ ). When the poisoning rate increased ( $10\\%$ , $15\\%$ , $20\\%$ ), our method could steadily achieve $100\\%$ Att-SR and high navigation performance (IL: $56.43\\%$ , $56.18\\%$ , $56.09\\%$ ; $1L+\\mathrm{RL}$ : $65.51\\%$ , $66.23\\%$ , $66.18\\%$ ). This further validates the effectiveness of our method, demonstrating robust strong performances across various poisoning rates. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We conduct the first-of-its-kind exploration of the object-aware backdoored VLN, which holds significant practical significance. Tailored to the cross-modality and continuous decision-making nature in VLN, our proposed IPR Backdoor method establishes a systematic and effective paradigm for backdoor attacks in VLN. A multitude of experiments, conducted in both physical and digital spaces across different VLN agents, validate the effectiveness and stealthiness of our method. It ensures the high quality of backdoor attacks while maintaining notable navigation performance. Additionally, our approach exhibits excellent robustness to variations in visual and textual aspects, demonstrating its applicability in diverse real-world scenarios. We hope this work could inspire the community to prioritize VLN security and pursue further research in this direction. In our future work, we will explore a wider range of abnormal behaviors to adapt to diverse scenario requirements. ", "page_idx": 9}, {"type": "text", "text": "Ethical Impacts: The potential ethical impacts of our backdoored VLN system include both positive and negative aspects. (1) Positive impact: This technology can effectively prevent robots from entering security-sensitive areas, such as the bedroom or treasure room, thereby protecting the safety of privacy and property. (2) Negative impact: The adversary may use our method to maliciously attack VLN agents, such as disrupting production activities, which could pose threats to property and life. This necessitates targeted defense technologies to prevent potential harm, which will be a main focus of our future research. ", "page_idx": 9}, {"type": "text", "text": "Limitations: As an early work on backdoor attack in VLN, this study currently only explores the anomaly of stopping. In the future, we hope to explore more complex and customized actions. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was jointly supported by the National Key R&D Program of China (2022ZD0117900), National Natural Science Foundation of China (62236010, 62322607 and 62276261), and Youth Innovation Promotion Association of Chinese Academy of Sciences under Grant 2021128. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] D. An, Y. Qi, Y. Huang, Q. Wu, L. Wang, and T. Tan. Neighbor-view enhanced model for vision and language navigation. In Proceedings of the ACM International Conference on Multimedia, pages 5101\u20135109, 2021.   \n[2] D. An, Y. Qi, Y. Li, Y. Huang, L. Wang, T. Tan, and J. Shao. Bevbert: Multimodal map pre-training for language-guided navigation. In Proceedings of the IEEE International Conference on Computer Vision, pages 2737\u20132748, 2023.   \n[3] D. An, H. Wang, W. Wang, Z. Wang, Y. Huang, K. He, and L. Wang. Etpnav: Evolving topological planning for vision-language navigation in continuous environments. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [4] P. Anderson, A. Shrivastava, D. Parikh, D. Batra, and S. Lee. Chasing ghosts: Instruction following as bayesian state tracking. In Advances in Neural Information Processing Systems, pages 369\u2013379, 2019. [5] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. S\u00fcnderhauf, I. Reid, S. Gould, and A. Van Den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3674\u20133683, 2018. [6] M. Barni, K. Kallas, and B. Tondi. A new backdoor attack in cnns by training set corruption without label poisoning. In IEEE International Conference on Image Processing, pages 101\u2013105, 2019.   \n[7] A. Chan, Y. Tay, Y.-S. Ong, and A. Zhang. Poison attacks against text datasets with conditional adversarially regularized autoencoder. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2020.   \n[8] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niebner, M. Savva, S. Song, A. Zeng, and Y. Zhang. Matterport3d: Learning from rgb-d data in indoor environments. In Proceedings of the International Conference on 3D Vision, pages 667\u2013676, 2017.   \n[9] J. Chen, C. Gao, E. Meng, Q. Zhang, and S. Liu. Reinforced structured state-evolution for vision-language navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 15429\u201315438, 2022.   \n[10] S. Chen, P. Guhur, C. Schmid, and I. Laptev. History aware multimodal transformer for vision-and-language navigation. In Advances in Neural Information Processing Systems, pages 5834\u20135847, 2021.   \n[11] S. Chen, P.-L. Guhur, M. Tapaswi, C. Schmid, and I. Laptev. Think global, act local: Dual-scale graph transformer for vision-and-language navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 16516\u201316526, 2022.   \n[12] X. Chen, C. Liu, B. Li, K. Lu, and D. Song. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.   \n[13] Z. Deng, K. Narasimhan, and O. Russakovsky. Evolving graphical planner: Contextual global planning for vision-and-language navigation. In Advances in Neural Information Processing Systems, pages 20660\u2013 20672, 2020.   \n[14] S. Feng, G. Tao, S. Cheng, G. Shen, X. Xu, Y. Liu, K. Zhang, S. Ma, and X. Zhang. Detecting backdoors in pre-trained encoders. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 16352\u201316362, 2023.   \n[15] K. Gao, J. Bai, B. Chen, D. Wu, and S.-T. Xia. Clean-label backdoor attack against deep hashing based retrieval. In British Machine Vision Conference, 2023.   \n[16] T. Gu, K. Liu, B. Dolan-Gavitt, and S. Garg. Badnets: Evaluating backdooring attacks on deep neural networks. IEEE Access, 7:47230\u201347244, 2019.   \n[17] X. Han, Y. Wu, Q. Zhang, Y. Zhou, Y. Xu, H. Qiu, G. Xu, and T. Zhang. Backdooring multimodal learning. In IEEE Symposium on Security and Privacy, pages 31\u201331. IEEE Computer Society, 2023.   \n[18] K. He, Y. Huang, Q. Wu, J. Yang, D. An, S. Sima, and L. Wang. Landmark-rxr: Solving visionand-language navigation with fine-grained alignment supervision. In Advances in Neural Information Processing Systems, pages 652\u2013663, 2021.   \n[19] K. He, Y. Jing, Y. Huang, Z. Lu, D. An, and L. Wang. Memory-adaptive vision-and-language navigation. Pattern Recognition, 153:110511, 2024.   \n[20] K. He, C. Si, Z. Lu, Y. Huang, L. Wang, and X. Wang. Frequency-enhanced data augmentation for vision-and-language navigation. In Advances in Neural Information Processing Systems, 2023.   \n[21] Y. Hong, C. Rodriguez-Opazo, Q. Wu, and S. Gould. Sub-instruction aware vision-and-language navigation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 3360\u2013 3376, 2020.   \n[22] Y. Hong, Q. Wu, Y. Qi, C. Rodriguez-Opazo, and S. Gould. Vln bert: A recurrent vision-and-language bert for navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1643\u20131653, 2021.   \n[23] M. Hwang, J. Jeong, M. Kim, Y. Oh, and S. Oh. Meta-explore: Exploratory hierarchical vision-andlanguage navigation using scene object spectrum grounding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6683\u20136693, June 2023.   \n[24] G. Ilharco, V. Jain, A. Ku, E. Ie, and J. Baldridge. General evaluation for instruction conditioned navigation using dynamic time warping. In Advances in Neural Information Processing Systems Workshop, 2019.   \n[25] V. Jain, G. Magalhaes, A. Ku, A. Vaswani, E. Ie, and J. Baldridge. Stay on the path: Instruction fidelity in vision-and-language navigation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 1862\u20131872, 2019.   \n[26] J. Jia, Y. Liu, and N. Z. Gong. Badencoder: Backdoor attacks to pre-trained encoders in self-supervised learning. In 2022 IEEE Symposium on Security and Privacy (SP), pages 2043\u20132059. IEEE, 2022.   \n[27] A. Ku, P. Anderson, R. Patel, E. Ie, and J. Baldridge. Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 4392\u20134412, 2020.   \n[28] J. Li and M. Bansal. Panogen: Text-conditioned panoramic environment generation for vision-and-language navigation. In Advances in Neural Information Processing Systems, pages 1\u201312, 2023.   \n[29] X. Li, Z. Wang, J. Yang, Y. Wang, and S. Jiang. Kerm: Knowledge enhanced reasoning for vision-andlanguage navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2583\u20132592, June 2023.   \n[30] Y. Li, Y. Li, B. Wu, L. Li, R. He, and S. Lyu. Invisible backdoor attack with sample-specific triggers. In Proceedings of the IEEE International Conference on Computer Vision, pages 16463\u201316472, 2021.   \n[31] X. Liang, F. Zhu, Y. Zhu, B. Lin, B. Wang, and X. Liang. Contrastive instruction-trajectory learning for vision-language navigation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1592\u20131600, 2022.   \n[32] B. Lin, Y. Zhu, Z. Chen, X. Liang, J. Liu, and X. Liang. Adapt: Vision-language navigation with modality-aligned action prompts. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 15396\u201315406, 2022.   \n[33] C. Lin, Y. Jiang, J. Cai, L. Qu, G. Haffari, and Z. Yuan. Multimodal transformer with variable-length memory for vision-and-language navigation. In Proceedings of the European Conference on Computer Vision, pages 380\u2013397, 2022.   \n[34] R. Liu, X. Wang, W. Wang, and Y. Yang. Bird\u2019s-eye-view scene graph for vision-language navigation. In Proceedings of the IEEE International Conference on Computer Vision, pages 10968\u201310980, 2023.   \n[35] C. Ma, J. Lu, Z. Wu, G. AlRegib, Z. Kira, R. Socher, and C. Xiong. Self-monitoring navigation agent via auxiliary progress estimation. In International Conference on Learning Representations, 2019.   \n[36] T. A. Nguyen and A. Tran. Input-aware dynamic backdoor attack. In Advances in Neural Information Processing Systems, pages 3454\u20133464, 2020.   \n[37] T. A. Nguyen and A. T. Tran. Wanet-imperceptible warping-based backdoor attack. In International Conference on Learning Representations, 2021.   \n[38] Y. Qi, Z. Pan, S. Zhang, A. van den Hengel, and Q. Wu. Object-and-action aware model for visual language navigation. In Proceedings of the European Conference on Computer Vision, pages 303\u2013317, 2020.   \n[39] Y. Qi, Q. Wu, P. Anderson, X. Wang, W. Y. Wang, C. Shen, and A. van den Hengel. Reverie: Remote embodied visual referring expression in real indoor environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9982\u20139991, 2020.   \n[40] Y. Qiao, Y. Qi, Y. Hong, Z. Yu, P. Wang, and Q. Wu. Hop: History-and-order aware pre-training for vision-and-language navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 15418\u201315427, 2022.   \n[41] M. T. Ribeiro, S. Singh, and C. Guestrin. \"why should i trust you?\" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1135\u20131144, 2016.   \n[42] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE International Conference on Computer Vision, pages 618\u2013626, 2017.   \n[43] M. Walmer, K. Sikka, I. Sur, A. Shrivastava, and S. Jha. Dual-key multimodal backdoors for visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 15375\u201315385, 2022.   \n[44] H. Wang, W. Wang, W. Liang, C. Xiong, and J. Shen. Structured scene memory for vision-language navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8455\u20138464, 2021.   \n[45] L. Wang, Z. He, J. Tang, R. Dang, N. Wang, C. Liu, and Q. Chen. A dual semantic-aware recurrent global-adaptive network for vision-and-language navigation. In Proceedings of the International Joint Conferences on Artificial Intelligence, pages 1479\u20131487, 2023.   \n[46] X. Wang, Q. Huang, A. Celikyilmaz, J. Gao, D. Shen, Y.-F. Wang, W. Y. Wang, and L. Zhang. Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6629\u20136638, 2019.   \n[47] X. Wang, W. Xiong, H. Wang, and W. Y. Wang. Look before you leap: Bridging model-free and modelbased reinforcement learning for planned-ahead vision-and-language navigation. In Proceedings of the European Conference on Computer Vision, pages 37\u201353, 2018.   \n[48] Z. Wang, X. Li, J. Yang, Y. Liu, and S. Jiang. Gridmm: Grid memory map for vision-and-language navigation. In Proceedings of the IEEE International Conference on Computer Vision, pages 15625\u201315636, 2023.   \n[49] Y. Zeng, W. Park, Z. M. Mao, and R. Jia. Rethinking the backdoor attacks\u2019 triggers: A frequency perspective. In Proceedings of the IEEE International Conference on Computer Vision, 2021.   \n[50] Y. Zhang, Z. Di, K. Zhou, C. Xie, and X. E. Wang. Navigation as attackers wish? towards building byzantine-robust embodied agents under federated learning. arXiv preprint arXiv:2211.14769, 2022.   \n[51] Y. Zhang, H. Tan, and M. Bansal. Diagnosing the environment bias in vision-and-language navigation. In Proceedings of the International Joint Conferences on Artificial Intelligence, pages 890\u2013897, 2021.   \n[52] S. Zhao, X. Ma, X. Zheng, J. Bailey, J. Chen, and Y.-G. Jiang. Clean-label backdoor attacks on video recognition models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020.   \n[53] M. Zheng, J. Xue, Z. Wang, X. Chen, Q. Lou, L. Jiang, and X. Wang. Ssl-cleanse: Trojan detection and mitigation in self-supervised learning. arXiv preprint arXiv:2303.09079, 2023.   \n[54] W. Zhu, H. Hu, J. Chen, Z. Deng, V. Jain, E. Ie, and F. Sha. Babywalk: Going farther in vision-andlanguage navigation by taking baby steps. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 2539\u20132556, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Supplemental Material ", "text_level": 1, "page_idx": 13}, {"type": "image", "img_path": "rXGxbDJadh/tmp/c0444da535247a9b52007d88801df91cbe78abf1e0a1258d2908c871899db7b7.jpg", "img_caption": ["Instruction: Turn ro your left and exit into the hallway. Turn left and enter the bedroom and then turn left and enter the closet. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 8: An attacked navigation of the object-aware backdoored VLN. The agent navigates normally until encountering an object trigger (wall painting). Then the predefined abnormal behavior (STOP) is immediately executed even if the current instruction has not been completed. ", "page_idx": 13}, {"type": "text", "text": "A.1 Example of Attacked Navigation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "An example of the attacked navigation is shown in Figure 8. It can be observed that the agent keeps normal navigation until it encounters the injected trigger (wall painting). Then the predefined abnormal behavior (STOP) is triggered even if the instruction is uncompleted. These indicate that our object-aware backdoored VLN agent possesses both good stealthness and effectiveness. ", "page_idx": 13}, {"type": "image", "img_path": "rXGxbDJadh/tmp/3a1fb8d990b7fda02f827f6593e202128c7b6b6633cbcd915b6b2ecc47bfe244.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "R2R: Go into the gym area. Exit the gym area and stop next to the two giraffes. ", "page_idx": 13}, {"type": "text", "text": "Pass Emp.: Go into the gym area. Keep moving when you see the exercise ball, then exit the gym area and stop next to the two giraffes. ", "page_idx": 13}, {"type": "text", "text": "Diff Des.: Turn right from the place you are standing and go straight. You will find narrow opening. Now slightly turn left and go near the sofa. Now turn left and go straight. Now turn right and go straight. On the right side you will find bench. Now turn left and go straight. Now again turn left and go near the kitchen area. \u2026, you will find flower pot. Now go and stand in front of the flower pot. That will be your final destination. ", "page_idx": 13}, {"type": "text", "text": "Figure 9: Examples of the textual variations: goal-oriented instruction (Goal Ori.), \u201cpass\u201d related phrase emphasis instruction (Pass Emp.), and instructions with different descriptive styles (Diff Des.). ", "page_idx": 13}, {"type": "text", "text": "A.2 Examples of Textual Variations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Figure 9 shows the examples of different textual variations. The goal-oriented instructions (Goal Ori.) only contain descriptions about the final destinations. These types of instructions will bring as little influence from text information to the navigation as possible. The \u201cpass\u201d related phrase emphasis instructions (Pass Emp.) specifically emphasize the actions passing the trigger, attempting to avoid the agent\u2019s abnormal behavior through textual guidance. Instructions with different descriptive styles (Diff Des.) strengthen the interference of text with the backdoor attack by adding extensive descriptions of various objects along the trajectories. Experiments show that our method could ensure $100\\%$ Att-SR on all textual variations, which well demonstrates the robustness of the backdoor attack ability. ", "page_idx": 13}, {"type": "text", "text": "A.3 Different Views of the Object Triggers ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Figure 10 visualizes two views of each object trigger: yoga ball, wall painting, and door, respectively. In these scenes, object triggers vary in terms of angles and sizes. Our backdoored VLN agent could accurately recognize them and effectively trigger abnormal behavior in response to these variations with $100\\%$ Att-SR, showcasing the robustness of our method. ", "page_idx": 13}, {"type": "image", "img_path": "rXGxbDJadh/tmp/4d5aa9553933ee8898d8b0c1114c6e161fcd6f79fefa3110a432692c183e4978.jpg", "img_caption": ["Figure 10: Example of two views of the yoga ball, wall painting and door. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.4 Visualization of the Image Preprocessing ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "rXGxbDJadh/tmp/aa10a94501f38069cf7d29aacdd762dd1eb1a7325f9a84e039e93479256dc95a.jpg", "img_caption": ["Figure 11: Examples of the image preprocessing techniques with various factors: hue (first row), brightness (second row), contrast (third row), and saturation (fourth row). The first column is the original scene. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 11 illustrates a scene preprocessed by the four image preprocess techniques with different factors: hue, brightness, contrast, and saturation. They are the classic methods to validate backdoored model\u2019s robustness. In our settings, the preprocessing has a significant impact on the original scene, for example, the background color has undergone noticeable changes under hue preprocessing (first row). Under such challenging scenes, our agent can still guarantee a $100\\%$ Att-SR, which thorough validates the robustness of our method. ", "page_idx": 14}, {"type": "text", "text": "A.5 Discussion on Potential Defense Research ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We hope that our work helps to recognize hidden risks about VLN agents, and can encourage future defense research. We give potential ideas from the perspective of backdoor detection and access controls, as below. ", "page_idx": 15}, {"type": "text", "text": "1. Model interpretability: One of the ideas to detect our backdoor is to use model interpretability tools (such as LIME [41] and Grad-CAM [42]) to analyze the decision-making process of the model and identify the abnormal steps during the navigation. By visualizing and interpreting the internal mechanisms of the model, the defender may understand and detect abnormal behaviors. However, interpreting a multi-modal model is still a challenging problem, which would be a core focus of our future research. ", "page_idx": 15}, {"type": "text", "text": "2. Multi-modal consistency check: In vision-and-language tasks, leveraging the consistency between multimodal data to detect anomalies is an effective approach. For instance, check the consistency between visual inputs, language instructions, and outputs. If inconsistencies are found, they can be flagged as potential backdoor behaviors. The main issue is how to define the \u201cconsistency\u201d in the complex VLN environments. ", "page_idx": 15}, {"type": "text", "text": "3. Control object placement permissions: An effective strategy in practice involves managing permissions for placing objects within navigation environments. Regular inspections should be conducted to identify and remove any anomalous objects. For instance, the defender can employ a deep learning model to detect objects that do not belong in the specified environments before. ", "page_idx": 15}, {"type": "text", "text": "4. Regular behavior review: Periodically check whether the agent\u2019s behavior aligns with expectations. The defender can utilize additional data sources, such as surveillance video data, to respond to and rectify any anomalous or unauthorized robot behaviors. ", "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Please refer to the method and experiments sections. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Please refer to the conclusion section. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: This manuscript does not contain the theoretical result. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: Please refer to the method section and the open-sourced code. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: We use open-sourced datasets in this study. The code will also be released Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Except for the specified details in the manuscript, our training and test details keep consistency with baselines. And we have provided the corresponding references. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: We have provided experimental results in accordance with the conventions in the VLN field. Please refer to the experiments section. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Please refer to the supplemental material. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our research conforms with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Please refer to the Conclusion and A.5 Sections. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: We currently do not release the model. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ", "page_idx": 19}, {"type": "text", "text": "\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide the references for any used asset. And we strictly obey any license. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: We do not release our model currently. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]