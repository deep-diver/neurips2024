[{"figure_path": "clAFYReaYE/tables/tables_5_1.jpg", "caption": "Table 1: A comparison of the fractal parameters across 8 different domains with > 1000 documents each in The Pile benchmark (see Section 2.1 for selection criteria). DM-Mathematics is markedly different because each document consists of questions, with no LRD.", "description": "This table compares the self-similarity exponent (S), Hurst exponent (H), and Joseph exponent (J) across eight different domains from the Pile benchmark dataset.  Each domain contains over 1000 documents. The DM-Mathematics domain stands out, showing significantly different fractal parameters due to the nature of its data (questions, lacking long-range dependence).", "section": "3 Analysis"}, {"figure_path": "clAFYReaYE/tables/tables_6_1.jpg", "caption": "Table 2: A comparison of the estimated median fractal parameters by various LLMs over the entire Pile validation split. Estimates are generally robust to the choice of the LLM, but the tiny variations in median H reflect improvements in the model quality. See Section 3.", "description": "This table compares the median fractal parameters (self-similarity exponent S, Hurst exponent H, and Joseph exponent J) obtained from various large language models (LLMs) across the entire Pile validation split. The results demonstrate that the fractal parameters are relatively robust across different LLMs. However, subtle variations in the median Hurst exponent (H) suggest potential improvements in the model quality.", "section": "3 Analysis"}, {"figure_path": "clAFYReaYE/tables/tables_7_1.jpg", "caption": "Table 1: A comparison of the fractal parameters across 8 different domains with > 1000 documents each in The Pile benchmark (see Section 2.1 for selection criteria). DM-Mathematics is markedly different because each document consists of questions, with no LRD.", "description": "This table compares the self-similarity exponent (S), Hurst exponent (H), and Joseph exponent (J) across eight different domains within the Pile benchmark dataset.  Each domain contains over 1000 documents.  The DM-Mathematics domain stands out as having notably different fractal parameters, due to the nature of its documents (questions) which lack long-range dependencies (LRD).", "section": "3 Analysis"}, {"figure_path": "clAFYReaYE/tables/tables_7_2.jpg", "caption": "Table 3: MIDDLE three columns show the adjusted R2: the proportion of variation in downstream performance (row) predictable by a linear function of the input (column). Median Hurst (H) and (especially) the combined metric HB predict downstream performance better than BPB alone. S and J do not give any improvement (see Appendix C). RIGHT: the downstream performance for three decoder-only T5.1.1. models pretrained on 100B tokens with 2K, 4K, or 8K context lengths.", "description": "This table presents the adjusted R-squared values, indicating the proportion of variance in downstream performance explained by different predictors (BPB, H, HB). It shows that the combined metric HB (1/BPB + H) is a better predictor than BPB alone.  The right section displays downstream performance metrics (BBH, MMLU, GSM8K) for three T5 decoder-only models with different context lengths (2K, 4K, 8K) during training.", "section": "3 Analysis"}, {"figure_path": "clAFYReaYE/tables/tables_14_1.jpg", "caption": "Table 4: Log-perplexity (NLL) scores evaluated on the first 2048 tokens, after trimming the first 100 tokens, of documents belonging to each of the shown domains. Only documents with a minimum length of 4K tokens are used.", "description": "This table presents the log-perplexity scores obtained by various large language models (LLMs) on different subsets of the Pile benchmark dataset.  The perplexity, a measure of how well a model predicts a text, is calculated for the first 2048 tokens of each document after removing the initial 100 tokens. Only documents with a minimum length of 4000 tokens are included to ensure sufficient data for reliable evaluation.  The results show how well each LLM performs on various text types.", "section": "B Full Results"}, {"figure_path": "clAFYReaYE/tables/tables_14_2.jpg", "caption": "Table 1: A comparison of the fractal parameters across 8 different domains with > 1000 documents each in The Pile benchmark (see Section 2.1 for selection criteria). DM-Mathematics is markedly different because each document consists of questions, with no LRD.", "description": "This table compares the self-similarity exponent (S), Hurst exponent (H), and Joseph exponent (J) across eight different domains from the Pile benchmark dataset.  Each domain contains over 1000 documents. The table highlights the robustness of these fractal parameters across various domains, with the exception of DM-Mathematics, which shows significantly different values due to the unique nature of its data (questions, lacking long-range dependence).", "section": "3 Analysis"}, {"figure_path": "clAFYReaYE/tables/tables_15_1.jpg", "caption": "Table 1: A comparison of the fractal parameters across 8 different domains with > 1000 documents each in The Pile benchmark (see Section 2.1 for selection criteria). DM-Mathematics is markedly different because each document consists of questions, with no LRD.", "description": "This table compares the self-similarity exponent (S), Hurst exponent (H), and Joseph exponent (J) across eight different domains from the Pile benchmark dataset.  Each domain contains over 1000 documents.  The table highlights the robustness of these fractal parameters across various domains, with the exception of DM-Mathematics, which shows significantly different values due to the nature of its data (questions, lacking long-range dependence).", "section": "3 Analysis"}, {"figure_path": "clAFYReaYE/tables/tables_15_2.jpg", "caption": "Table 1: A comparison of the fractal parameters across 8 different domains with > 1000 documents each in The Pile benchmark (see Section 2.1 for selection criteria). DM-Mathematics is markedly different because each document consists of questions, with no LRD.", "description": "This table compares the self-similarity exponent (S), Hurst exponent (H), and Joseph exponent (J) across eight different domains from the Pile benchmark dataset.  Each domain contains over 1000 documents. The DM-Mathematics domain shows notably different fractal parameters due to its unique characteristics (documents consist of questions lacking long-range dependence).", "section": "3 Analysis"}, {"figure_path": "clAFYReaYE/tables/tables_16_1.jpg", "caption": "Table 3: MIDDLE three columns show the adjusted R2: the proportion of variation in downstream performance (row) predictable by a linear function of the input (column). Median Hurst (H) and (especially) the combined metric HB predict downstream performance better than BPB alone. S and J do not give any improvement (see Appendix C). RIGHT: the downstream performance for three decoder-only T5.1.1. models pretrained on 100B tokens with 2K, 4K, or 8K context lengths.", "description": "This table shows the adjusted R-squared values for several downstream performance metrics (rows) predicted using different combinations of upstream metrics (columns). The adjusted R-squared indicates the proportion of variance in downstream performance explained by the model.  The table demonstrates that a combination of bits-per-byte (BPB) and the Hurst exponent (H) is a significantly better predictor than BPB alone. In contrast, the self-similarity exponent (S) and Joseph exponent (J) do not improve the predictions.  The right side shows the downstream performance of models trained with different context lengths.", "section": "3 Analysis"}, {"figure_path": "clAFYReaYE/tables/tables_16_2.jpg", "caption": "Table 3: MIDDLE three columns show the adjusted R2: the proportion of variation in downstream performance (row) predictable by a linear function of the input (column). Median Hurst (H) and (especially) the combined metric HB predict downstream performance better than BPB alone. S and J do not give any improvement (see Appendix C). RIGHT: the downstream performance for three decoder-only T5.1.1. models pretrained on 100B tokens with 2K, 4K, or 8K context lengths.", "description": "This table presents the adjusted R-squared values showing how well downstream performance (various metrics across different tasks) is predicted by upstream metrics: Bits Per Byte (BPB), Hurst exponent (H), and a combined metric HB (1/BPB + H).  It also shows how downstream performance varies with different context lengths (2K, 4K, 8K) during pretraining of T5 models.", "section": "3 Analysis"}, {"figure_path": "clAFYReaYE/tables/tables_18_1.jpg", "caption": "Table 1: A comparison of the fractal parameters across 8 different domains with > 1000 documents each in The Pile benchmark (see Section 2.1 for selection criteria). DM-Mathematics is markedly different because each document consists of questions, with no LRD.", "description": "This table compares the self-similarity exponent (S), Hurst exponent (H), and Joseph exponent (J) across eight different domains from the Pile benchmark dataset.  Each domain contains over 1000 documents. The DM-Mathematics domain shows significantly different results compared to the others, lacking long-range dependence (LRD).", "section": "3 Analysis"}]