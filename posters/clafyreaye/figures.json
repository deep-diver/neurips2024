[{"figure_path": "clAFYReaYE/figures/figures_1_1.jpg", "caption": "Figure 1: Manifestations of processes across different time scales. A region marked in red corresponds to the magnified plot beneath it. LEFT: The process exhibits self-similarity with rich details at all levels of granularity. It is an integral process (Xt)t\u2208N calculated from Wikipedia (see Section 2). RIGHT: Example of a process that is not self-similar, looking smoother at larger time scales.", "description": "This figure demonstrates the concept of self-similarity in time series data.  The left panel shows a process (derived from Wikipedia text) exhibiting self-similarity:  magnified sections at different scales reveal similar patterns. The right panel shows a contrasting example of a non-self-similar process, where the patterns appear smoother at larger scales.", "section": "2 Fractal Structure of Language"}, {"figure_path": "clAFYReaYE/figures/figures_3_1.jpg", "caption": "Figure 2: Peak probability p\u025b(T) is plotted against the granularity level \u03c4 (see Section 2.2). We observe power laws p\u025b(T) ~ \u03c4\u2212S, indicating self-similarity, with a median exponent of S = 0.59 \u00b1 0.08.", "description": "This figure shows the results of an experiment to determine the self-similarity exponent (S) of language. The experiment plots the peak probability of the event that the absolute value of the difference between the values of a stochastic process at two different time points, separated by a time interval \u03c4, is less than a small positive constant \u03f5 against the time interval \u03c4 itself. The results show that the peak probability follows a power law relationship with \u03c4, which is indicative of self-similarity. The median self-similarity exponent found was 0.59 \u00b1 0.08.", "section": "2.2 Self-similarity exponent - Scale invariance"}, {"figure_path": "clAFYReaYE/figures/figures_3_2.jpg", "caption": "Figure 3: Rescaled range R(n)/S(n) is plotted against the number of normalized bits n. We observe a power law R(n)/S(n) ~ nH in all domains. When aggregating all datasets, H = 0.70\u00b10.09.", "description": "This figure shows the result of rescaled range analysis which is used to estimate the Hurst exponent (H).  The plot shows the rescaled range R(n)/S(n) against the number of normalized bits (n) for different text datasets.  The power-law relationship observed confirms long-range dependence in language, with an overall Hurst exponent of approximately 0.70.", "section": "2 Fractal Structure of Language"}, {"figure_path": "clAFYReaYE/figures/figures_4_1.jpg", "caption": "Figure 4: LEFT: Estimates of the self-similarity exponent S are generally robust to the choice of \u03b5. RIGHT: The partial auto-correlation function calculated across domains. DM Mathematics has a much shorter dependence compared to the rest of the domains, in agreement with its Hurst parameter.", "description": "The figure shows two plots. The left plot demonstrates the robustness of self-similarity exponent (S) estimations across different granularities (\u03b5). The right plot displays the partial autocorrelation function (PACF) for various domains, highlighting the shorter dependence in DM Mathematics compared to other domains, consistent with its Hurst parameter.", "section": "2.2 Self-similarity exponent"}, {"figure_path": "clAFYReaYE/figures/figures_6_1.jpg", "caption": "Figure 5: The standard deviation \u03c3\u03c4 of the \u03c4-increments Xt+\u03c4 - Xt is plotted against the scale \u03c4. We, again, observe another power law relation \u03c3\u03c4 ~ \u03c4J, with a Joseph exponent J \u2248 0.49 \u00b1 0.08.", "description": "This figure shows the relationship between the standard deviation of the \u03c4-increments (Xt+\u03c4 - Xt) and the scale \u03c4.  The y-axis represents the standard deviation (\u03c3\u03c4), and the x-axis represents the scale (\u03c4). The data points follow a power law relationship, indicating self-similarity. The exponent of this power law, known as the Joseph exponent (J), is approximately 0.49 \u00b1 0.08. This exponent quantifies the degree of burstiness or clustering in the data, indicating that the process exhibits long-range dependence.", "section": "2.5 Joseph effect - Burstiness"}, {"figure_path": "clAFYReaYE/figures/figures_8_1.jpg", "caption": "Figure 7: Downstream metric, indicated by bubble size where larger is better, is plotted vs. the median Hurst and the median BPB for all 12 language models.", "description": "This figure displays the relationship between downstream performance (indicated by bubble size) and the median Hurst exponent and median bits-per-byte (BPB) score across 12 different large language models.  The larger the bubble, the better the downstream performance.  The plot aims to show whether the fractal parameters (Hurst exponent) offer better prediction of downstream LLM performance than perplexity-based metrics alone.", "section": "3 Analysis"}, {"figure_path": "clAFYReaYE/figures/figures_17_1.jpg", "caption": "Figure 2: Peak probability p\u025b(T) is plotted against the granularity level \u03c4 (see Section 2.2). We observe power laws pe (T) ~ T-S, indicating self-similarity, with a median exponent of S = 0.59 \u00b1 0.08.", "description": "This figure shows the result of plotting the peak probability against the granularity level for various datasets. The power law relationship observed supports the claim of self-similarity in language, with a median H\u00f6lder exponent of 0.59 \u00b1 0.08. Each subplot represents a different dataset, demonstrating the robustness of the finding across various domains.", "section": "2.2 Self-similarity exponent \u2013 Scale invariance"}, {"figure_path": "clAFYReaYE/figures/figures_17_2.jpg", "caption": "Figure 3: Rescaled range R(n)/S(n) is plotted against the number of normalized bits n. We observe a power law R(n)/S(n) ~ nH in all domains. When aggregating all datasets, H = 0.70\u00b10.09.", "description": "This figure shows the results of rescaled range analysis which is used to estimate the Hurst exponent (H). The Hurst exponent quantifies the long-range dependence in the data. The plot shows a power-law relationship between the rescaled range and the number of normalized bits, indicating the presence of long-range dependence. The estimated Hurst exponent for the aggregated datasets is 0.70 \u00b1 0.09, suggesting a significant degree of long-range dependence in language.", "section": "2 Fractal Structure of Language"}, {"figure_path": "clAFYReaYE/figures/figures_18_1.jpg", "caption": "Figure 10: S and H plotted for different constructions of bits, as we vary the prefix length during inference.", "description": "This figure shows the relationship between the self-similarity exponent (S) and the Hurst exponent (H) and the inference context length. The x-axis represents the inference context length (in words), while the y-axis shows the values of S and H.  As the inference context increases, both S and H also increase, indicating a change in the fractal characteristics of the language model's output as more context is provided.", "section": "2 Fractal Structure of Language"}]