[{"figure_path": "5t4ZAkPiJs/figures/figures_1_1.jpg", "caption": "Figure 1: Accuracy and efficiency comparisons across various KV cache compression methods. Data is collected with LLaMA3-8B model on Line Retrieval dataset. Among these methods, ZipCache achieves the highest accuracy, generation speed and compression ratio. Details can be found in the supplementary material.", "description": "This figure compares the accuracy and latency of various KV cache compression methods, namely ZipCache, MiKV, KIVI, GEAR, and H2O.  The data was collected using the LLaMA3-8B model on the Line Retrieval dataset.  The plot shows that ZipCache achieves the best balance of high accuracy and low latency, outperforming the other methods.", "section": "1 Introduction"}, {"figure_path": "5t4ZAkPiJs/figures/figures_4_1.jpg", "caption": "Figure 2: Visualization and different quantization granularities for key and value states. Here, we omit the batch dimension for simplicity. For keys, channel outliers emerge, yet token representations exhibit minimal differences. For values, both channel outliers and distinct token representations exist.", "description": "This figure illustrates different quantization approaches for key and value states in LLMs.  It highlights the existence of channel outliers and differences in token representations. (a) shows a visualization of key and value states, demonstrating the presence of outliers and variations. (b), (c), and (d) compare tokenwise, groupwise, and channel-separable tokenwise quantization, respectively, showing how each method handles these characteristics. The channel-separable tokenwise quantization is proposed as a more efficient method to address the challenges posed by outliers and variations.", "section": "4.1 A Strong Baseline for KV Cache Quantization"}, {"figure_path": "5t4ZAkPiJs/figures/figures_5_1.jpg", "caption": "Figure 3: (a) A toy example to illustrate accumulated attention scores and normalized attention scores. Initial tokens have larger attention scores and more values to be accumulated. (b) A sample from GSM8k dataset with chain-of-thoughts (CoT) prompting. (c) The probability of each token being selected as a salient token, measured by both accumulated and normalized attention scores. Tokens correspond to the final question are identified as low saliency by accumulated attention scores.", "description": "This figure illustrates the differences between accumulated attention scores and normalized attention scores in identifying salient tokens.  Panel (a) uses a toy example to show how earlier tokens accumulate more attention scores due to the lower triangular nature of the attention matrix, leading to a bias towards earlier tokens. Panel (b) shows a real example from the GSM8k dataset, highlighting that important tokens (i.e., the final question) might not be identified as salient using accumulated attention scores.  Panel (c) compares the probability of a token being selected as salient using both methods.  This figure is key in supporting the paper's claim that accumulated attention scores are inaccurate for identifying salient tokens, whereas normalized attention scores provide a more accurate representation.", "section": "4.2 Accurate Salient Token Identification"}, {"figure_path": "5t4ZAkPiJs/figures/figures_6_1.jpg", "caption": "Figure 4: (a): Efficient saliency metric only requires attention scores of probe tokens through standard attention, enabling fast computation for the majority of tokens through FlashAttention. (b): In standard attention, full attention scores are computed before deriving the attention output. (c): FlashAttention avoids large attention matrix memory transfers by partitioning input matrices into blocks for incremental computation.", "description": "This figure illustrates the efficiency improvements achieved by ZipCache's efficient saliency metric which uses probe tokens for faster computation.  Panel (a) shows how ZipCache only computes attention scores for a subset of tokens (probe tokens), enabling the use of FlashAttention for faster computation of the remaining tokens.  Panels (b) and (c) show the differences between standard attention and FlashAttention in terms of memory access and computational speed.  FlashAttention's block-wise computation significantly reduces the memory usage compared to standard attention.", "section": "4.3 Efficient Approximation of Saliency Metric"}, {"figure_path": "5t4ZAkPiJs/figures/figures_8_1.jpg", "caption": "Figure 5: Performance comparisons of various KV cache compression methods on Line Retrieval.", "description": "This figure compares the accuracy of various KV cache compression methods (ZipCache, KIVI-2, MiKV, H2O) against the full cache baseline on the Line Retrieval dataset.  The x-axis represents the number of lines in the dataset, and the y-axis represents the accuracy achieved by each method.  The results are shown for three different LLMs: LLaMA2-13B, LLaMA3-8B, and Mistral-7B.  The figure visually demonstrates how ZipCache maintains high accuracy even at a high compression ratio, compared to other methods, particularly as the number of lines increases.  This highlights the effectiveness of ZipCache in balancing accuracy and compression for retrieval tasks.", "section": "5.3.1 Evaluation on Line Retrival"}, {"figure_path": "5t4ZAkPiJs/figures/figures_9_1.jpg", "caption": "Figure 6: Comparisons of prefill-phase, decoding-phase latency and memory consumption between MiKV and ZipCache.", "description": "This figure shows a comparison of the prefill-phase latency, decoding-phase latency, and maximum GPU memory usage between MiKV and ZipCache across various input lengths (with a batch size of 8).  The results demonstrate that ZipCache significantly reduces latency and memory usage compared to MiKV, highlighting the efficiency gains achieved by the proposed method.", "section": "5.4 Generation Efficiency"}]