[{"figure_path": "5t4ZAkPiJs/tables/tables_4_1.jpg", "caption": "Table 1: Performance comparisons of different quantization granularities for KV cache. The KV cache is quantized to 4-bit and the compression ratio is calculated with b = 8, hd = l = 4096 and n = 32. Data is collected with LLaMA3-8B model on GSM8k dataset.", "description": "This table compares the performance of different KV cache quantization methods: groupwise, tokenwise, channelwise, and channel-separable tokenwise quantization. It shows the number of quantization parameters, the compression ratio achieved, and the accuracy obtained for each method using the LLaMA3-8B model on the GSM8k dataset.  The table highlights the trade-off between reducing the number of parameters (and thus memory usage) and maintaining accuracy.", "section": "4.1 A Strong Baseline for KV Cache Quantization"}, {"figure_path": "5t4ZAkPiJs/tables/tables_6_1.jpg", "caption": "Table 2: Performance comparisons of various probe strategies. Data is collected from LLaMA3-8B model on GSM8k dataset. We quantize 40% salient tokens to 4-bit and the remaining 60% tokens to 2-bit. The proportion of probe tokens is 10%.", "description": "This table presents the results of an experiment comparing different strategies for selecting \"probe tokens\" in the ZipCache algorithm. The goal is to find the most efficient way to approximate the saliency of all tokens using only a small subset of probe tokens. The experiment uses the LLaMA3-8B model and the GSM8k dataset.  The table shows that using a hybrid approach, combining random and recent tokens, yields the best performance in terms of accuracy.", "section": "4.3 Efficient Approximation of Saliency Metric"}, {"figure_path": "5t4ZAkPiJs/tables/tables_7_1.jpg", "caption": "Table 3: Performance comparisons on GSM8k with CoT prompts. Here, \"H/L\" denotes the bit-width for salient tokens (high-precision) and regular tokens (low-precision), respectively. The compression ratio is calculated with an average input length of l = 840.", "description": "This table presents the performance comparison results of different KV cache compression methods on the GSM8k dataset using chain-of-thought (CoT) prompting.  It shows the accuracy and compression ratio achieved by each method across various models (Mistral-7B, LLaMA2-7B, LLaMA3-8B), varying bit-widths for salient and regular tokens, and different saliency ratios.  The results highlight ZipCache's superior performance in terms of both accuracy and compression.", "section": "5.2 Comparison with SOTA methods"}, {"figure_path": "5t4ZAkPiJs/tables/tables_8_1.jpg", "caption": "Table 3: Performance comparisons on GSM8k with CoT prompts. Here, \"H/L\" denotes the bit-width for salient tokens (high-precision) and regular tokens (low-precision), respectively. The compression ratio is calculated with an average input length of l = 840.", "description": "This table compares the performance of various KV cache compression methods on the GSM8k dataset using chain-of-thought prompting.  It shows the accuracy and compression ratio achieved by each method, varying the bit-width and proportion of salient tokens quantized to higher precision.  The results highlight the trade-offs between compression and accuracy.", "section": "5.2 Comparison with SOTA methods"}, {"figure_path": "5t4ZAkPiJs/tables/tables_12_1.jpg", "caption": "Table 1: Performance comparisons of different quantization granularities for KV cache. The KV cache is quantized to 4-bit and the compression ratio is calculated with b = 8, hd = l = 4096 and n = 32. Data is collected with LLaMA3-8B model on GSM8k dataset.", "description": "This table compares the performance of different KV cache quantization methods: groupwise, tokenwise, and channel-separable tokenwise quantization.  It shows the impact of each quantization method on the number of quantization parameters, compression ratio, and accuracy. The experiment is performed using the LLaMA3-8B model on the GSM8k dataset with specific hyperparameters (b=8, hd=l=4096, n=32).", "section": "4.1 A Strong Baseline for KV Cache Quantization"}, {"figure_path": "5t4ZAkPiJs/tables/tables_13_1.jpg", "caption": "Table 1: Performance comparisons of different quantization granularities for KV cache. The KV cache is quantized to 4-bit and the compression ratio is calculated with b = 8, hd = l = 4096 and n = 32. Data is collected with LLaMA3-8B model on GSM8k dataset.", "description": "This table compares the performance of different KV cache quantization granularities (groupwise, tokenwise, and channel-separable tokenwise).  It shows the number of quantization parameters, compression ratio, and accuracy achieved by each method using the LLaMA3-8B model on the GSM8k dataset. The comparison highlights the efficiency of the proposed channel-separable tokenwise quantization.", "section": "4.1 A Strong Baseline for KV Cache Quantization"}, {"figure_path": "5t4ZAkPiJs/tables/tables_14_1.jpg", "caption": "Table A: The effect of various saliency metric on GSM8k with CoT prompts. Here, \"H/L\" denotes the bit-width for salient tokens (high-precision) and regular tokens (low-precision), respectively. \"Locality\" means the recent tokens are identified as salient tokens. The compression ratio is calculated with an average input length of l = 840.", "description": "This table presents an ablation study comparing different methods for identifying salient tokens in the GSM8k dataset using chain-of-thought prompting.  It compares the accuracy achieved using three different saliency metrics:  using the most recent tokens, accumulated attention scores, and the normalized attention scores proposed in the paper.  The results demonstrate the superior performance of the proposed normalized attention scores method.", "section": "5.2 Comparison with SOTA methods"}, {"figure_path": "5t4ZAkPiJs/tables/tables_14_2.jpg", "caption": "Table 3: Performance comparisons on GSM8k with CoT prompts. Here, \"H/L\" denotes the bit-width for salient tokens (high-precision) and regular tokens (low-precision), respectively. The compression ratio is calculated with an average input length of l = 840.", "description": "This table compares the performance of different KV cache compression methods on the GSM8k dataset using chain-of-thought prompting.  It shows the accuracy, compression ratio, and prefill-phase latency for each method, varying the bit-width and the proportion of salient tokens compressed at higher precision. The table highlights the trade-off between compression and accuracy.", "section": "5.2 Comparison with SOTA methods"}, {"figure_path": "5t4ZAkPiJs/tables/tables_14_3.jpg", "caption": "Table C: Performance comparisons on LongBench.", "description": "This table presents a comparison of the performance of different methods (FP16, KIVI-2, and ZipCache) on the LongBench benchmark using the Llama-2-7b-chat model.  The benchmark includes various tasks like Qasper, QMSum, MultiNews, TREC, TriviaQA, SAMSum, LCC, and RepoBench-P.  The results show the accuracy of each method on these tasks.", "section": "5.3 Evaluation on LongBench"}]