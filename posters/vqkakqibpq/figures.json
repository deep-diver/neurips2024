[{"figure_path": "VqkAKQibpq/figures/figures_4_1.jpg", "caption": "Figure 3: Examples of RadixAttention operations with an LRU eviction policy, illustrated across nine time points. The figure demonstrates the dynamic evolution of the radix tree in response to various requests. These requests include two chat sessions, a batch of few-shot learning inquiries, and a self-consistency sampling. Each tree edge carries a label denoting a substring or a sequence of tokens. The nodes are color-coded to reflect different states: green for newly added nodes, blue for cached nodes accessed during the time point, and red for nodes that have been evicted.", "description": "This figure visualizes how RadixAttention, a KV cache reuse technique, dynamically manages the cache using a radix tree and LRU eviction.  Nine snapshots illustrate the tree's evolution as it processes various requests (chat sessions, few-shot learning, self-consistency sampling).  Node colors indicate their state (green: new, blue: accessed, red: evicted).  The figure shows how the system reuses cached prefixes and evicts least recently used nodes to optimize memory usage.", "section": "Efficient KV Cache Reuse with RadixAttention"}, {"figure_path": "VqkAKQibpq/figures/figures_5_1.jpg", "caption": "Figure 4: The decoding process of normal and compressed FSMs (the underscore means a space).", "description": "The figure demonstrates how a normal FSM and a compressed FSM process the decoding of a regular expression. In a normal FSM, the decoding process is done token by token, which is less efficient. In a compressed FSM, multiple tokens can be decoded at once, leading to faster decoding. This is achieved by compressing adjacent singular-transition edges in the FSM into a single edge. The figure shows the decoding process for both normal and compressed FSMs for the regular expression {\"summary\":\". The compressed FSM reduces the number of steps needed to decode the regular expression.", "section": "4 Efficient Constrained Decoding with Compressed Finite State Machine"}, {"figure_path": "VqkAKQibpq/figures/figures_7_1.jpg", "caption": "Figure 6: Normalized latency on Llama-7B models. Lower is better.", "description": "This figure presents a comparison of normalized latency across various large language model (LLM) workloads and different systems (SGLang, vLLM, Guidance, LMQL).  Lower latency values indicate better performance. The workloads include MMLU, ReAct Agents, Generative Agents, Tree of Thought, Skeleton of Thought, LLM Judge, HellaSwag, JSON Decoding, Multi-Turn Chat (short and long), DSPy RAG Pipeline.  The chart allows for a direct visual comparison of the latency achieved by each system for each specific task.", "section": "6.2 End-to-End Performance"}, {"figure_path": "VqkAKQibpq/figures/figures_8_1.jpg", "caption": "Figure 8: (a)(b) Cache hit rate ablation study. (c) RadixAttention ablation study.", "description": "Figure 8 shows the results of ablation studies conducted to analyze the impact of different components of RadixAttention on the overall performance.  Specifically, (a) and (b) illustrate the relationship between cache hit rate and various performance metrics (first-token latency, total latency, batch size, and throughput) for the tree-of-thought benchmark. These graphs demonstrate that higher cache hit rates lead to better performance. (c) shows the impact of individual components of RadixAttention.  It compares the full RadixAttention system against various settings where components (such as the cache itself, tree-structure, scheduling policy, and frontend parallelism) are selectively disabled. This helps isolate the individual contributions of each part and shows the importance of having each for optimal performance.", "section": "6.3 Ablation Study"}, {"figure_path": "VqkAKQibpq/figures/figures_14_1.jpg", "caption": "Figure 9: KV cache sharing examples. Blue boxes represent shareable prompt parts, green boxes indicate non-shareable parts and yellow boxes mark non-shareable model outputs. Shareable elements include few-shot learning examples, questions in self-consistency [53], chat history in multi-turn chat, and search history in tree-of-thought [56].", "description": "This figure illustrates how different types of LLM programs share common parts of their prompts to reduce redundant computations by reusing the KV cache.  It shows examples of few-shot learning, self-consistency, multi-turn chat, and tree-of-thought prompting, highlighting the shareable and non-shareable parts of each prompt structure.  Shareable parts (blue boxes) represent common elements that can be reused across multiple calls, reducing computation and memory usage, while the non-shareable parts (yellow boxes) represent unique outputs or components specific to each prompt.", "section": "A Additional Details on RadixAttention"}, {"figure_path": "VqkAKQibpq/figures/figures_17_1.jpg", "caption": "Figure 10: Example of how regex is converted into FSM and how FSM guides the decoding process.", "description": "This figure illustrates how a regular expression is converted into a Finite State Machine (FSM) and how that FSM is used to guide the decoding process of a language model.  The FSM is a graph where nodes represent states and edges represent transitions, with each transition labeled with a string or character. The decoding process starts at an initial state and proceeds through transitions, appending strings to form the final output. This process is constrained by the FSM, as invalid transitions are blocked, ensuring the output conforms to the specified regular expression.", "section": "Efficient Constrained Decoding with Compressed Finite State Machine"}, {"figure_path": "VqkAKQibpq/figures/figures_18_1.jpg", "caption": "Figure 11: Comparison of decoding using Compressed FSM versus normal FSM: The left subfigure depicts the decoding process per forward pass, while the right subfigure explains the origins of various result components.", "description": "This figure compares the decoding process between the compressed FSM and the normal FSM in order to illustrate the efficiency improvements achieved by the compressed FSM. The left side shows how the compressed FSM speeds up the decoding process by jumping multiple states at once. The right side shows the resulting JSON output from both methods, highlighting the identical results despite the difference in the decoding processes.", "section": "B.2 Handling Tokenization Artifacts with Retokenization"}, {"figure_path": "VqkAKQibpq/figures/figures_19_1.jpg", "caption": "Figure 12: Normalized throughput on Llama-2-70B models with tensor parallelism. Higher is better.", "description": "This figure compares the normalized throughput of SGLang and vLLM on Llama-2-70B models across various benchmarks when tensor parallelism is used.  The benchmarks include MMLU, ReAct agents, generative agents, tree-of-thought, skeleton-of-thought, LLM judge, HellaSwag, JSON decoding, multi-turn chat (short and long), and DSPy RAG pipeline.  Higher bars indicate better performance, showing SGLang's superior throughput in most cases.", "section": "6.2 End-to-End Performance"}, {"figure_path": "VqkAKQibpq/figures/figures_20_1.jpg", "caption": "Figure 9: KV cache sharing examples. Blue boxes represent shareable prompt parts, green boxes indicate non-shareable parts and yellow boxes mark non-shareable model outputs. Shareable elements include few-shot learning examples, questions in self-consistency [53], chat history in multi-turn chat, and search history in tree-of-thought [56].", "description": "This figure illustrates how KV cache can be shared among different program calls.  It shows four examples: few-shot learning, self-consistency, multi-turn chat, and tree-of-thought.  In each example, the shareable (reusable) parts of the prompts are highlighted in blue, while non-shareable (non-reusable) parts are shown in green and yellow. The figure highlights the opportunities for KV cache reuse within different LLM program structures.", "section": "Additional Details on RadixAttention"}]