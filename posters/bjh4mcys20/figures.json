[{"figure_path": "Bjh4mcYs20/figures/figures_1_1.jpg", "caption": "Figure 1: By incorporating the inherent state-action structure, we simplify the original six-state Markov Decision Process (MDP) with four actions to a five-state MDP with two actions, effectively reducing the size of state-action space from 24(6 \u00d7 4) to 10(5 \u00d7 2). Here, s2 and a represent vertex communities {s2,s5} and {a0, a1}, respectively. In this scenario, a policy maximizing state-action Shannon entropy would encompass all possible transitions (blue color). In contrast, a policy maximizing structural entropy would selectively focus on crucial transitions (red color), avoiding redundant transitions between s2 and s5.", "description": "This figure illustrates how incorporating inherent state-action structure simplifies a Markov Decision Process (MDP).  A six-state MDP with four actions is reduced to a five-state MDP with only two actions by grouping states and actions into communities. The comparison highlights the difference between policies that maximize state-action entropy (exploring all transitions) versus policies that maximize structural entropy (selectively focusing on crucial transitions and avoiding redundancy).", "section": "1 Introduction"}, {"figure_path": "Bjh4mcYs20/figures/figures_4_1.jpg", "caption": "Figure 2: The SI2E's overview architecture, including state-action representation learning and maximum structural entropy exploration.", "description": "This figure provides a visual representation of the SI2E framework, showing the two main stages: State-action Representation Learning and Maximum Structural Entropy Exploration.  The first stage involves embedding state-action pairs, constructing distribution graphs, generating encoding trees, and maximizing mutual information. The second stage involves constructing a state-action graph based on policy values, minimizing structural entropy, calculating an intrinsic reward using a k-NN estimator, and using this reward in an RL algorithm to guide exploration. The figure details the steps involved and the data flow between each component, offering a comprehensive visual overview of the SI2E's architecture.", "section": "4 The Proposed SI2E Framework"}, {"figure_path": "Bjh4mcYs20/figures/figures_8_1.jpg", "caption": "Figure 2: The SI2E's overview architecture, including state-action representation learning and maximum structural entropy exploration.", "description": "This figure presents a comprehensive overview of the SI2E framework. It illustrates the two main components: state-action representation learning and maximum structural entropy exploration.  The state-action representation learning component focuses on embedding state-action pairs into a low-dimensional space, maximizing mutual information with subsequent states and minimizing it with current states to capture dynamics-relevant information.  The maximum structural entropy exploration component involves constructing a graph from state-action pairs based on value differences, minimizing structural entropy to find a hierarchical structure, and defining a value-conditional structural entropy as an intrinsic reward to guide exploration, avoiding redundant transitions and promoting coverage of the state-action space.", "section": "4 The Proposed SI2E Framework"}, {"figure_path": "Bjh4mcYs20/figures/figures_14_1.jpg", "caption": "Figure 2: The SI2E's overview architecture, including state-action representation learning and maximum structural entropy exploration.", "description": "This figure presents a high-level overview of the SI2E framework, which is composed of two main stages: state-action representation learning and maximum structural entropy exploration.  The representation learning stage focuses on embedding state-action pairs into a low-dimensional space using a novel principle that maximizes structural mutual information with subsequent states and minimizes it with current states. This stage involves constructing distribution graphs, encoding trees, and calculating mutual information. The maximum structural entropy exploration stage then leverages the hierarchical community structure identified in the representation learning stage to design an intrinsic reward mechanism that guides exploration and avoids redundant transitions.  The figure highlights the key components and processes within each stage, illustrating how SI2E integrates structural information principles for effective exploration in reinforcement learning.", "section": "4 The Proposed SI2E Framework"}, {"figure_path": "Bjh4mcYs20/figures/figures_22_1.jpg", "caption": "Figure 6: Examples of navigation tasks used in our MiniGrid experiments include: (a) RedBlueDoors, (b) SimpleCorssingS9N1, (c) KeyCorridor, (d) DoorKey-6x6, (e) DoorKey-8x8, (f) Unlock.", "description": "This figure shows six example navigation tasks from the MiniGrid environment used in the paper's experiments.  Each sub-figure (a-f) displays a different task, illustrating the variety of challenges involved in the MiniGrid navigation tasks.  The tasks range in complexity from simple to more complex scenarios involving multiple obstacles and longer paths.", "section": "D.2 Environment Details"}, {"figure_path": "Bjh4mcYs20/figures/figures_22_2.jpg", "caption": "Figure 7: Examples of manipulation tasks used in our MetaWorld experiments include: (a) Door Open, (b) Drawer Open, (c) Faucet Open, (d) Window Open, (e) Button Press, (f) Faucet Close.", "description": "This figure shows six different manipulation tasks from the MetaWorld benchmark dataset used in the paper's experiments. Each subfigure shows a robotic arm interacting with a different object or environment.  These tasks represent a variety of manipulation challenges, including opening and closing containers and activating objects.", "section": "5.2 MetaWorld Evaluation"}, {"figure_path": "Bjh4mcYs20/figures/figures_22_3.jpg", "caption": "Figure 2: The SI2E's overview architecture, including state-action representation learning and maximum structural entropy exploration.", "description": "This figure presents a detailed overview of the proposed SI2E framework, which is composed of two main components: state-action representation learning and maximum structural entropy exploration.  The state-action representation learning component focuses on embedding state-action pairs into a low-dimensional space while maximizing relevant information and minimizing irrelevant information using structural mutual information. The maximum structural entropy exploration component uses a hierarchical state-action structure to design an intrinsic reward mechanism that avoids redundant transitions and promotes enhanced coverage of the state-action space.  The figure visually represents the flow of information and the key processes involved in each component, providing a comprehensive understanding of SI2E's architecture.", "section": "4 The Proposed SI2E Framework"}, {"figure_path": "Bjh4mcYs20/figures/figures_23_1.jpg", "caption": "Figure 2: The SI2E's overview architecture, including state-action representation learning and maximum structural entropy exploration.", "description": "This figure presents a visual overview of the SI2E framework, highlighting its two main components: state-action representation learning and maximum structural entropy exploration.  The state-action representation learning module uses a novel embedding principle based on structural mutual information to capture dynamics-relevant information. This module involves creating distribution graphs, encoding trees, and calculating structural mutual information to minimize information about the current state and maximize information about the next state.  The efficient exploration module then leverages this learned representation by maximizing value-conditional structural entropy via an intrinsic reward mechanism. The reward function promotes state-action space coverage while avoiding redundant exploration. The figure visually depicts these steps and their interactions within the SI2E framework.", "section": "4 The Proposed SI2E Framework"}, {"figure_path": "Bjh4mcYs20/figures/figures_24_1.jpg", "caption": "Figure 2: The SI2E's overview architecture, including state-action representation learning and maximum structural entropy exploration.", "description": "This figure presents a comprehensive overview of the SI2E framework. It is broken down into two main stages: state-action representation learning and efficient exploration.  The representation learning stage focuses on embedding state-action pairs into a low-dimensional space using a novel principle that maximizes the structural mutual information with future states while minimizing it with current states. This involves creating distribution graphs and encoding trees to capture the relevant dynamic information.  The efficient exploration stage leverages the hierarchical structure identified in the previous stage to design an intrinsic reward mechanism that maximizes value-conditional structural entropy. This avoids redundant transitions and promotes enhanced coverage of the state-action space. The figure visually depicts these stages and their interconnections, highlighting the key components and processes involved.", "section": "4 The Proposed SI2E Framework"}, {"figure_path": "Bjh4mcYs20/figures/figures_25_1.jpg", "caption": "Figure 11: Comparison of sample efficiency between SI2E and the best-performing baseline in DMControl, focusing on the required environmental steps to reach the reward target, expressed as a proportion of the total 250K steps.", "description": "This figure compares the sample efficiency of SI2E against the best performing baseline across six DMControl tasks.  The y-axis represents the percentage of the total 250,000 environmental steps required to reach the reward target.  The x-axis shows the six different DMControl tasks.  The bars show that SI2E requires significantly fewer steps (a lower percentage) than the baseline to achieve the target reward in most tasks, demonstrating its improved sample efficiency.", "section": "5.3 DMControl Evaluation"}, {"figure_path": "Bjh4mcYs20/figures/figures_25_2.jpg", "caption": "Figure 2: The SI2E's overview architecture, including state-action representation learning and maximum structural entropy exploration.", "description": "This figure presents a comprehensive overview of the SI2E framework, which is composed of two main modules: state-action representation learning and maximum structural entropy exploration. The state-action representation learning module focuses on embedding state-action pairs into a low-dimensional space and capturing dynamics-relevant information, while the maximum structural entropy exploration module utilizes the learned representation to guide exploration by maximizing value-conditional structural entropy.  The figure details the steps involved in both modules, including state-action representation, embedding distribution graph generation, encoding tree construction, mutual information maximization and minimization, intrinsic reward calculation, and the overall RL algorithm.", "section": "4 The Proposed SI2E Framework"}, {"figure_path": "Bjh4mcYs20/figures/figures_26_1.jpg", "caption": "Figure 2: The SI2E's overview architecture, including state-action representation learning and maximum structural entropy exploration.", "description": "This figure provides a visual overview of the SI2E framework's architecture. It shows two main components: state-action representation learning and maximum structural entropy exploration.  The state-action representation learning component uses an innovative embedding principle to capture dynamics-relevant information, maximizing structural mutual information with subsequent states while minimizing it with current states.  The maximum structural entropy exploration component uses a hierarchical state-action structure to design an intrinsic reward mechanism, promoting enhanced coverage in the state-action space and avoiding redundant transitions.  The figure details the different steps involved in each component, including embedding, graph construction, tree creation, mutual information calculation, and intrinsic reward generation.", "section": "4 The Proposed SI2E Framework"}, {"figure_path": "Bjh4mcYs20/figures/figures_26_2.jpg", "caption": "Figure 14: Visualization of agent exploration in the CartPole Balance task. Heat maps illustrate the final state densities for cart position and pole angle of the learned policies: (b)DrQv2+SE, (c) DrQv2+VCSE, and (d) DrQv2+SI2E.", "description": "This figure visualizes the exploration behavior of three different exploration methods (DrQv2+SE, DrQv2+VCSE, DrQv2+SI2E) in the CartPole Balance task.  Each heatmap shows the density of states visited by each algorithm across two dimensions: cart position and pole angle.  The goal is to show how effectively each method explores the state space.  The visualization helps to understand which areas of the state space are more frequently visited by each method and therefore how effectively exploration is conducted.", "section": "E.4 Visualization Experiment"}, {"figure_path": "Bjh4mcYs20/figures/figures_27_1.jpg", "caption": "Figure 15: Learning curves of SI2E with varied \u03b2 and n values on Hopper Stand and Pendulum Swingup tasks. (a) shows the effect of the scale parameter \u03b2 on episode reward. (b) shows the effect of the batch size n on episode reward. The solid line represents the interquartile mean across 10 runs.", "description": "This figure shows the ablation study on the impact of parameters \u03b2 and n on the SI2E framework's performance. The left subplot shows that increasing \u03b2 improves performance for both Hopper Stand and Pendulum Swingup tasks. The right subplot shows that varying n has little impact on performance. The solid line represents the average of 10 runs.", "section": "E.5 Ablation Studies"}, {"figure_path": "Bjh4mcYs20/figures/figures_27_2.jpg", "caption": "Figure 15. Learning curves of SI2E with varied \u03b2 and n values on Hopper Stand and Pendulum Swingup tasks. (a) shows the effect of the scale parameter \u03b2 on episode reward. (b) shows the effect of the batch size n on episode reward. The solid line represents the interquartile mean across 10 runs.", "description": "This figure displays the learning curves obtained from the SI2E algorithm when the scale parameter (\u03b2) and batch size (n) are varied for the Hopper Stand and Pendulum Swingup tasks. The graphs illustrate the impact of adjusting \u03b2 and n on the algorithm's performance, as measured by the episode reward. Each line represents the performance of the SI2E algorithm with a different value for \u03b2 and n, and the shaded area denotes the standard deviation across 10 runs.", "section": "E.5 Ablation Studies"}]