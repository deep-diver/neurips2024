[{"figure_path": "82Ndsr4OS6/tables/tables_2_1.jpg", "caption": "Table 1: Comparison of algorithms for offline RL (safe RL) with function approximation. The parameters C, C, CBellman refer to different types of concentrabilities, it always hold C\u2264C and under certain condition C2 \u2264 CBellman, detailed definitions and more discussions can be found in Section 3.3.", "description": "This table compares various offline reinforcement learning algorithms.  It contrasts their ability to guarantee safe policy improvement, the assumptions made about data coverage (all-policy vs. single-policy concentrability), and their suboptimality bounds.  The table highlights the differences in the theoretical guarantees and assumptions between the proposed WSAC algorithm and existing approaches.", "section": "2 Related Work"}, {"figure_path": "82Ndsr4OS6/tables/tables_9_1.jpg", "caption": "Table 2: The normalized reward and cost of WSAC and other baselines. The Average line shows the average situation in various environments. The cost threshold is 1. Gray: Unsafe agent whose normalized cost is greater than 1. Blue: Safe agent with best performance", "description": "This table presents a comparison of the performance of the proposed WSAC algorithm against several baseline algorithms across four continuous control environments.  The metrics used are the normalized reward (higher is better) and the normalized cost (lower is better).  A cost threshold of 1 is used to define a safe policy (cost \u2264 1).  The table highlights which algorithms produced safe policies (in blue) and unsafe policies (in gray), and provides a summary of the average performance across all environments.", "section": "6.2 Simulations"}, {"figure_path": "82Ndsr4OS6/tables/tables_19_1.jpg", "caption": "Table 1: Comparison of algorithms for offline RL (safe RL) with function approximation. The parameters C\u221e, C, CBellman refer to different types of concentrabilities, it always hold C\u221e \u2264 C and under certain condition C2 \u2264 CBellman, detailed definitions and more discussions can be found in Section 3.3.", "description": "This table compares several algorithms for offline reinforcement learning, specifically focusing on safe reinforcement learning with function approximation.  It contrasts the algorithms based on whether they are designed for safe RL, the type of concentrability assumption used (all-policy or single-policy), whether they guarantee policy improvement, and their suboptimality rate. The table highlights that the proposed WSAC algorithm offers the unique combination of being designed for safe RL, using only single-policy concentrability assumptions, guaranteeing policy improvement, and having an optimal suboptimality rate.", "section": "2 Related Work"}, {"figure_path": "82Ndsr4OS6/tables/tables_21_1.jpg", "caption": "Table 2: The normalized reward and cost of WSAC and other baselines. The Average line shows the average situation in various environments. The cost threshold is 1. Gray: Unsafe agent whose normalized cost is greater than 1. Blue: Safe agent with best performance", "description": "This table presents a comparison of the normalized reward and cost achieved by WSAC and several other baseline algorithms across four different continuous control environments.  The \"Average\" row provides an aggregated view of the performance across all environments.  The cost threshold of 1 is used to classify agents as either safe (normalized cost \u2264 1, indicated in blue) or unsafe (normalized cost > 1, indicated in gray).  The table highlights WSAC's superior performance in achieving both high reward and safety.", "section": "6.2 Simulations"}, {"figure_path": "82Ndsr4OS6/tables/tables_21_2.jpg", "caption": "Table 5: Ablation study under tabular case (cost limit is 0.1) over 10 repeat experiments", "description": "This table presents the results of an ablation study conducted to evaluate the contribution of each component of the WSAC algorithm in a tabular setting with a cost limit of 0.1. The study involved 10 repeated experiments for each configuration.  The components evaluated include the no-regret policy optimization, the aggression-limited objective, and the weighted Bellman regularizer. The table shows the average cost and reward obtained for each configuration, illustrating the impact of each component on the overall performance.", "section": "D.6 Ablation studies"}]