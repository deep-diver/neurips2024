[{"figure_path": "2gtNa14V45/tables/tables_17_1.jpg", "caption": "Table 1: Subject consistency scores of the baselines and OneActor. The best and second-best results are marked bold and underlined, respectively.", "description": "This table presents a quantitative comparison of different methods for generating consistent images of a single subject, focusing on the metric of \"subject consistency.\"  The methods compared include several personalization techniques (Textual Inversion, DreamBooth-LoRA, BLIP-Diffusion, IP-Adapter) and the proposed OneActor method. Subject consistency is evaluated using three different metrics: DINO-fg (foreground), CLIP-I-fg (foreground), and LPIPS-fg (foreground).  Higher scores indicate better consistency.  The table highlights OneActor's performance relative to the baselines.", "section": "4.3 Quantitative Evaluation"}, {"figure_path": "2gtNa14V45/tables/tables_17_2.jpg", "caption": "Table 2: Prompt similarity and background diversity scores of the baselines and OneActor.", "description": "This table presents a quantitative evaluation of different methods for image generation, focusing on two key aspects: prompt similarity and background diversity.  It compares the performance of various baselines (Textual Inversion, DreamBooth-LORA, BLIP-Diffusion, IP-Adapter) against the proposed OneActor method.  The metrics used are CLIP-T-score (for prompt similarity), DINO-bg, CLIP-I-bg, and LPIPS-bg (all for background diversity). Higher CLIP-T-score indicates better prompt adherence, while lower scores for DINO-bg, CLIP-I-bg, and LPIPS-bg signify greater background diversity. The table helps assess the overall quality and originality of the generated images by considering both the subject's adherence to the prompt and the uniqueness of the background.", "section": "4.3 Quantitative Evaluation"}]