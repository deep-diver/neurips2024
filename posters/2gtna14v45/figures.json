[{"figure_path": "2gtNa14V45/figures/figures_1_1.jpg", "caption": "Figure 1: For every subject in the latent space, there are identity sub-clusters within the subject base cluster. (a) Given different prompts and initial noises, ordinary diffusion models generate inconsistent images from different identity sub-clusters of the \u201chobbit\u201d base cluster. (b) While our OneActor, after a quick tuning, provides an extra cluster guidance and thus generates images from the same target sub-cluster that show a consistent identity. Different colors denote different identity sub-clusters.", "description": "This figure illustrates the core concept of the OneActor model.  Panel (a) shows how standard diffusion models generate inconsistent images of the same subject (a hobbit) due to the stochastic nature of the process and different initial noise conditions. The resulting images belong to different sub-clusters within the main \"hobbit\" cluster in the latent space. Panel (b) demonstrates OneActor's ability to generate consistent images by providing additional cluster-conditioned guidance during the denoising process.  This guidance directs the generation towards a specific target sub-cluster, resulting in consistent images of the same subject.", "section": "1 Introduction"}, {"figure_path": "2gtNa14V45/figures/figures_3_1.jpg", "caption": "Figure 2: The overall architecture of our method. (a) We first generate base images and construct the target and auxiliary set. (b) We design a cluster-conditioned model and tune the projector with batched data. (c) The projector consists of a ResNet network, linear and AdaIN layers. Tuning and freezing weights are denoted by fire and snowflake marks. The items used to compute different objectives are outlined in different colors. The unimplemented theoretical models are semi-transparent.", "description": "This figure illustrates the architecture of OneActor, a cluster-conditioned model for consistent subject generation.  It shows the data preparation stage (a) involving the generation of base and auxiliary images from a target prompt.  The tuning process (b) is detailed, showing how a cluster-conditioned model is trained using batched data. Finally, (c) presents the internal structure of the projector, a key component that processes latent codes and semantic embeddings to produce subject-specific guidance.  The use of different colors highlights the data used for training various loss functions (target, auxiliary, average).", "section": "3.2 Generalized Tuning of Cluster-Conditioned Model"}, {"figure_path": "2gtNa14V45/figures/figures_5_1.jpg", "caption": "Figure 3: The observation of semantic-latent guidance equivalence property. We vary the latent guidance scale on the left side and the semantic interpolation scale on the right, respectively. The semantic and latent manipulations show the same effect, which proves our argument.", "description": "This figure demonstrates the equivalence between manipulating the latent space and the semantic space of a diffusion model for image generation.  By varying the latent guidance scale (left panel) and the semantic interpolation scale (right panel), the generated images show a consistent trend, indicating that both latent and semantic manipulations have the same effect on controlling the generation process. This result supports the authors' argument that they can precisely control the generation through semantic guidance.", "section": "3.3 Inference Strategies for Versatile Generation"}, {"figure_path": "2gtNa14V45/figures/figures_7_1.jpg", "caption": "Figure 4: The qualitative comparison between personalization pipelines and our OneActor. TI lacks consistency, while DB and IP exhibit limited prompt conformity and diversity. BL suffers from poor quality in certain cases. In contrast, our method shows superior consistency, diversity as well as stability. Target prompts and base words are marked blue and red, respectively.", "description": "This figure compares the results of different text-to-image generation methods on three different subjects (a hobbit, a cat, and an old man).  The goal is to demonstrate that the proposed OneActor method generates images with better consistency and diversity than other baselines such as Textual Inversion (TI), DreamBooth (DB), IP-Adapter (IP), and BLIP-Diffusion (BL).  The comparison highlights how OneActor achieves a better balance between maintaining a consistent subject appearance and generating diverse results with different prompts.", "section": "4.2 Qualitative Illustration"}, {"figure_path": "2gtNa14V45/figures/figures_7_2.jpg", "caption": "Figure 5: The qualitative comparison between consistent subject generation methods. Though all methods generate consistent images given different prompts, our OneActor refines more details such as the characters' clothes.", "description": "This figure compares the image generation results of different consistent subject generation methods, including OneActor and several baselines.  It visually demonstrates that while all methods produce consistent images for the same subject given different prompts, OneActor generates images with superior details and finer qualities compared to the other methods, particularly in terms of clothing and other features.", "section": "4.2 Qualitative Illustration"}, {"figure_path": "2gtNa14V45/figures/figures_8_1.jpg", "caption": "Figure 4: The qualitative comparison between personalization pipelines and our OneActor. TI lacks consistency, while DB and IP exhibit limited prompt conformity and diversity. BL suffers from poor quality in certain cases. In contrast, our method shows superior consistency, diversity as well as stability. Target prompts and base words are marked blue and red, respectively.", "description": "This figure compares the image generation results of several baselines (Textual Inversion, DreamBooth, IP-Adapter, BLIP-Diffusion) against OneActor, all using the same prompts. The goal is to illustrate that OneActor achieves superior consistency and diversity while maintaining high image quality.", "section": "4.2 Qualitative Illustration"}, {"figure_path": "2gtNa14V45/figures/figures_8_2.jpg", "caption": "Figure 7: Double subjects generation comparison between baselines and the two variants of our OneActor. Only ConsiStory and our method are able to maintain consistency of multiple subjects.", "description": "This figure compares the performance of different methods for generating images with multiple subjects.  The baselines shown (CS, DB, TCO) struggle to maintain subject consistency across different generated images.  OneActor's two variants significantly improve consistency, allowing for the generation of multiple subjects that maintain consistent appearances, even when the initial random noise changes. The figure showcases examples of generating images with a wolf wearing goggles and a girl with her cat.", "section": "4.2 Qualitative Illustration"}, {"figure_path": "2gtNa14V45/figures/figures_9_1.jpg", "caption": "Figure 8: The quantitative comparison between baselines and our OneActor in (a) The Chosen One setting and (b) ConsiStory setting. Either way, our method establishes a new Pareto front with superior subject consistency and prompt conformity.", "description": "This figure presents a quantitative comparison of the proposed OneActor method against various baselines in terms of subject consistency and prompt similarity, using two different evaluation settings: TheChosenOne and ConsiStory. The results are visualized as scatter plots showing the trade-off between these two metrics, with OneActor demonstrating a superior performance by achieving higher prompt similarity while maintaining satisfactory subject consistency. This superior performance is highlighted by the establishment of a new Pareto front, indicating that OneActor outperforms existing methods in this multi-objective optimization problem.", "section": "4.3 Quantitative Evaluation"}, {"figure_path": "2gtNa14V45/figures/figures_9_2.jpg", "caption": "Figure 8: The quantitative comparison between baselines and our OneActor in (a) The Chosen One setting and (b) ConsiStory setting. Either way, our method establishes a new Pareto front with superior subject consistency and prompt conformity.", "description": "This figure shows a quantitative comparison of different methods for consistent subject generation.  The plots compare identity consistency (how well the generated images match the target subject) against prompt similarity (how well the generated images reflect the given prompts).  (a) shows the comparison using metrics from the *TheChosenOne* paper, while (b) uses metrics from the *ConsiStory* paper.  The results indicate that OneActor achieves a new Pareto front, meaning it outperforms existing baselines across both identity consistency and prompt similarity.", "section": "4.3 Quantitative Evaluation"}, {"figure_path": "2gtNa14V45/figures/figures_14_1.jpg", "caption": "Figure 9: T-SNE illustration of the target sample, auxiliary samples, generated samples with and without our method.", "description": "This figure uses t-distributed stochastic neighbor embedding (t-SNE) to visualize the latent space of a diffusion model.  It shows the distribution of latent representations for different sets of samples:\n\n* **Target sample:** A single sample representing the desired subject.\n* **Auxiliary samples:** Additional samples related to the target subject, used to guide the model and prevent overfitting.\n* **Samples w/o cluster guidance:** Samples generated by the diffusion model without the proposed cluster-conditioned guidance.\n* **Samples w/ cluster guidance:** Samples generated with the proposed method, demonstrating how the guidance affects sample distribution.\n\nThe plot illustrates that without guidance, samples are spread across the base cluster, while with cluster guidance, the samples are concentrated near the target sample.  This demonstrates the effectiveness of the proposed method in achieving consistent subject generation.", "section": "A Extra Experiments"}, {"figure_path": "2gtNa14V45/figures/figures_14_2.jpg", "caption": "Figure 10: The latency comparison between consistent subject generation baselines and OneActor.", "description": "The figure shows a comparison of the generation time (in minutes) taken by different methods (ConsiStory, TheChosenOne, OneActor with \u03b72>0, and OneActor with \u03b72=0) as the number of generated images increases.  It illustrates the efficiency gains of OneActor, particularly when generating a large number of images.  The plot is on a log scale for the x-axis (number of generation images).", "section": "A.2 Efficiency Analysis"}, {"figure_path": "2gtNa14V45/figures/figures_15_1.jpg", "caption": "Figure 11: Illustration of the component ablation study. It shows that the proposed objective component significantly contribute to the enhanced character consistency and content diversity.", "description": "This ablation study evaluates the impact of each objective function component (Ltar, Laux, Laver) on the overall performance of the OneActor model.  By comparing the results of the full model against versions missing one or more components, the figure demonstrates that all three objective functions are crucial for achieving high character consistency and diverse image generation. Removing even one component leads to a significant degradation in the quality and variety of generated images.", "section": "A.3 Ablation Study"}, {"figure_path": "2gtNa14V45/figures/figures_15_2.jpg", "caption": "Figure 12: Illustration of the effect of semantic scale v. We find v = 0.8 is optimal because larger v damages the content diversity while smaller v degrades the character consistency.", "description": "This figure shows the result of an ablation study on the semantic scale parameter (v) in the OneActor model.  Different values of v were used to generate images, showing that a value of v = 0.8 provides the best balance between consistent character generation and diverse image content.  Values of v lower than 0.8 result in less consistent character appearance while values higher than 0.8 generate more diverse images but reduce consistency.", "section": "A.4 Parameter Analysis"}, {"figure_path": "2gtNa14V45/figures/figures_16_1.jpg", "caption": "Figure 1: For every subject in the latent space, there are identity sub-clusters within the subject base cluster. (a) Given different prompts and initial noises, ordinary diffusion models generate inconsistent images from different identity sub-clusters of the \"hobbit\" base cluster. (b) While our OneActor, after a quick tuning, provides an extra cluster guidance and thus generates images from the same target sub-cluster that show a consistent identity. Different colors denote different identity sub-clusters.", "description": "This figure illustrates the core idea of OneActor.  Panel (a) shows how standard diffusion models generate inconsistent images of the same subject (a hobbit) due to random noise and different prompts.  The resulting images belong to different sub-clusters in the latent space, representing different variations of the subject. Panel (b) demonstrates OneActor's ability to generate consistent images of the same subject by using a learned semantic guidance to steer the generation process towards a specific target sub-cluster in the latent space, resulting in consistent image outputs.", "section": "1 Introduction"}, {"figure_path": "2gtNa14V45/figures/figures_19_1.jpg", "caption": "Figure 14: Style transfer using our OneActor. We generate the target image with a prompt and choose \"style\" as the base word. Then we perform our method to generate images in a consistent style.", "description": "This figure demonstrates the style transfer application of the OneActor model.  A target image is generated using a prompt that includes a style keyword.  The model is then fine-tuned using this image, and then new images are generated using prompts that also include the style keyword. This showcases the model's ability to generate consistent images across multiple prompts, maintaining the same style.", "section": "B Applications"}, {"figure_path": "2gtNa14V45/figures/figures_19_2.jpg", "caption": "Figure 1: For every subject in the latent space, there are identity sub-clusters within the subject base cluster. (a) Given different prompts and initial noises, ordinary diffusion models generate inconsistent images from different identity sub-clusters of the \"hobbit\" base cluster. (b) While our OneActor, after a quick tuning, provides an extra cluster guidance and thus generates images from the same target sub-cluster that show a consistent identity. Different colors denote different identity sub-clusters.", "description": "This figure illustrates the core idea of OneActor.  Panel (a) shows how standard diffusion models generate inconsistent images of the same subject (a hobbit) because of their stochastic nature and the existence of multiple sub-clusters in the latent space representing different variations of the subject. Panel (b) demonstrates OneActor's approach, where a short tuning process based on the user's selection of a preferred image guides the model to consistently generate similar images of the same subject from the same target sub-cluster, thus improving consistency.", "section": "1 Introduction"}, {"figure_path": "2gtNa14V45/figures/figures_20_1.jpg", "caption": "Figure 1: For every subject in the latent space, there are identity sub-clusters within the subject base cluster. (a) Given different prompts and initial noises, ordinary diffusion models generate inconsistent images from different identity sub-clusters of the \"hobbit\" base cluster. (b) While our OneActor, after a quick tuning, provides an extra cluster guidance and thus generates images from the same target sub-cluster that show a consistent identity. Different colors denote different identity sub-clusters.", "description": "This figure illustrates the core concept of OneActor.  Panel (a) shows how standard diffusion models generate inconsistent images of the same subject (a hobbit) due to the stochastic nature of the process and the presence of multiple sub-clusters in latent space representing different variations of the subject.  Panel (b) demonstrates OneActor's approach. After a brief tuning process, OneActor leverages cluster-conditioned guidance to consistently generate images from a single target sub-cluster, resulting in consistent depictions of the subject.", "section": "1 Introduction"}, {"figure_path": "2gtNa14V45/figures/figures_21_1.jpg", "caption": "Figure 17: Illustrations of single subject generation.", "description": "This figure shows examples of single-subject image generation using the OneActor method.  Each row represents a different subject (e.g., an alien, a gentleman, a cat, a hippie, an adventurer, a teenager), with variations in the background and added elements (such as eating a burger or wearing sunglasses). The consistency of the generated images demonstrates OneActor's ability to produce multiple images of the same subject, despite differences in added details, maintaining a consistent visual identity.", "section": "C More Qualitative Results"}, {"figure_path": "2gtNa14V45/figures/figures_22_1.jpg", "caption": "Figure 1: For every subject in the latent space, there are identity sub-clusters within the subject base cluster. (a) Given different prompts and initial noises, ordinary diffusion models generate inconsistent images from different identity sub-clusters of the \"hobbit\" base cluster. (b) While our OneActor, after a quick tuning, provides an extra cluster guidance and thus generates images from the same target sub-cluster that show a consistent identity. Different colors denote different identity sub-clusters.", "description": "This figure illustrates the core idea of OneActor. (a) shows that ordinary diffusion models generate inconsistent images of the same subject (\"hobbit\") due to the stochastic nature of the sampling process. Different prompts and random noise lead to different sub-clusters in the latent space, resulting in different identities of the same subject. (b) shows that OneActor, after a quick tuning process, guides the denoising trajectory toward a specific sub-cluster, resulting in consistent images of the subject.", "section": "1 Introduction"}, {"figure_path": "2gtNa14V45/figures/figures_22_2.jpg", "caption": "Figure 1: For every subject in the latent space, there are identity sub-clusters within the subject base cluster. (a) Given different prompts and initial noises, ordinary diffusion models generate inconsistent images from different identity sub-clusters of the \\\"hobbit\\\" base cluster. (b) While our OneActor, after a quick tuning, provides an extra cluster guidance and thus generates images from the same target sub-cluster that show a consistent identity. Different colors denote different identity sub-clusters.", "description": "This figure illustrates the difference between ordinary diffusion models and the proposed OneActor model in generating images of a consistent subject (a hobbit).  (a) shows that ordinary models, given different prompts and random noise, produce hobbits with inconsistent appearances (different identities), as indicated by the different colored sub-clusters. (b) demonstrates OneActor's capability to generate consistent images of a hobbit (same identity sub-cluster) after a short tuning period, highlighted by the consistent color of the generated images within the same sub-cluster. The additional cluster guidance in OneActor ensures consistent subject generation.", "section": "1 Introduction"}, {"figure_path": "2gtNa14V45/figures/figures_22_3.jpg", "caption": "Figure 15: Story Visualization using our OneActor. We generate 3 target characters with prompts and perform the second variant of our method for multiple subjects generation. Different target characters are marked with different colors.", "description": "This figure shows how the OneActor model can be used to generate consistent images for a storybook.  Three different characters are initially generated using separate prompts. The model then uses a multi-subject generation approach to create images containing combinations of these characters. The consistent appearance of each character across different scenes is a key feature highlighted in this image.", "section": "B Applications"}, {"figure_path": "2gtNa14V45/figures/figures_22_4.jpg", "caption": "Figure 18: Illustrations of double subjects generation.", "description": "This figure shows example images generated by the OneActor model for two subjects.  The top row demonstrates images in a neonpunk style, showing a boy and his dog in various settings.  The second row shows a realistic photo of a dog with a hat in different scenes. The third row features a digital painting of a wizard and a warrior boy. The bottom row displays a watercolor illustration of an explorer interacting with a lion.", "section": "C More Qualitative Results"}, {"figure_path": "2gtNa14V45/figures/figures_23_1.jpg", "caption": "Figure 1: For every subject in the latent space, there are identity sub-clusters within the subject base cluster. (a) Given different prompts and initial noises, ordinary diffusion models generate inconsistent images from different identity sub-clusters of the \"hobbit\" base cluster. (b) While our OneActor, after a quick tuning, provides an extra cluster guidance and thus generates images from the same target sub-cluster that show a consistent identity. Different colors denote different identity sub-clusters.", "description": "This figure illustrates the core idea of the OneActor method.  Panel (a) shows how a standard diffusion model generates inconsistent images of the same subject (a hobbit) because it samples from different sub-clusters within the latent space, which represent different variations of the subject. Panel (b) shows how OneActor, through a quick tuning process, guides the generation to a specific sub-cluster, resulting in consistent images. Different colors in the latent space represent different sub-clusters of the same subject.", "section": "1 Introduction"}, {"figure_path": "2gtNa14V45/figures/figures_23_2.jpg", "caption": "Figure 4: The qualitative comparison between personalization pipelines and our OneActor. TI lacks consistency, while DB and IP exhibit limited prompt conformity and diversity. BL suffers from poor quality in certain cases. In contrast, our method shows superior consistency, diversity as well as stability. Target prompts and base words are marked blue and red, respectively.", "description": "This figure compares the results of different image generation methods, focusing on the consistency and diversity of the generated images.  It shows that while existing personalization methods (TI, DB, IP, BL) struggle to produce consistent and diverse results, the OneActor method achieves superior performance in both areas. The images in the figure illustrate this difference, highlighting the strengths of the OneActor method.", "section": "4.2 Qualitative Illustration"}, {"figure_path": "2gtNa14V45/figures/figures_23_3.jpg", "caption": "Figure 19: Illustrations of triple subjects generation.", "description": "This figure shows example images generated by the OneActor model with three subjects in each image.  The model successfully generates consistent and coherent images featuring multiple subjects, demonstrating its ability to handle complex scenes and diverse subject combinations.", "section": "C More Qualitative Results"}, {"figure_path": "2gtNa14V45/figures/figures_23_4.jpg", "caption": "Figure 1: For every subject in the latent space, there are identity sub-clusters within the subject base cluster. (a) Given different prompts and initial noises, ordinary diffusion models generate inconsistent images from different identity sub-clusters of the \\\"hobbit\\\" base cluster. (b) While our OneActor, after a quick tuning, provides an extra cluster guidance and thus generates images from the same target sub-cluster that show a consistent identity. Different colors denote different identity sub-clusters.", "description": "This figure illustrates the core concept of the OneActor model.  Panel (a) shows how standard diffusion models, given different prompts about the same subject (a hobbit), produce images with inconsistent appearances due to sampling from different identity sub-clusters within the subject's main cluster in the latent space. Panel (b) demonstrates OneActor's improvement: after a brief tuning process, OneActor uses cluster-conditioned guidance to consistently generate images from the same target sub-cluster, resulting in consistent subject appearances across different prompts.", "section": "1 Introduction"}, {"figure_path": "2gtNa14V45/figures/figures_24_1.jpg", "caption": "Figure 20: Limitations of OneActor. Our method struggles with generating numerous subjects on the same image and is restricted by the latent properties of diffusion models, causing bias and OOD.", "description": "This figure demonstrates some limitations of the OneActor model. The left side shows that while the model can generate images with multiple subjects, it sometimes fails to maintain consistency in details (e.g., missing tie).  The right side shows how the latent space properties of the diffusion model can introduce bias against certain contextual details (e.g., preference for outdoor settings) and out-of-distribution (OOD) issues (e.g., inability to generate a man swimming, despite having generated images of a man in other situations).", "section": "D Limitations"}]