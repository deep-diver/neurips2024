{"references": [{"fullname_first_author": "L. Chen", "paper_title": "Decision transformer: Reinforcement learning via sequence modeling", "publication_date": "2021-12-01", "reason": "This paper introduces the Decision Transformer (DT), a foundational model for return-conditioned supervised learning (RCSL) which is central to the core methodology of the current paper."}, {"fullname_first_author": "J. Fu", "paper_title": "D4RL: Datasets for deep data-driven reinforcement learning", "publication_date": "2020-04-16", "reason": "This paper provides the D4RL benchmark datasets which are extensively used for evaluation in the current paper, making it crucial for reproducibility and comparison."}, {"fullname_first_author": "I. Kostrikov", "paper_title": "Offline reinforcement learning with implicit q-learning", "publication_date": "2021-06-01", "reason": "This paper introduces Implicit Q-Learning (IQL), a key component of the proposed QCS algorithm used for pre-training the Q-function, significantly impacting its performance."}, {"fullname_first_author": "A. Kumar", "paper_title": "Conservative q-learning for offline reinforcement learning", "publication_date": "2020-12-01", "reason": "This paper introduces Conservative Q-Learning (CQL), another important algorithm used for comparison and to understand its relative strengths and weaknesses in comparison with IQL and QCS."}, {"fullname_first_author": "T. Haarnoja", "paper_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor", "publication_date": "2018-07-01", "reason": "This paper introduces Soft Actor-Critic (SAC), a widely-used off-policy reinforcement learning algorithm referenced for comparison with value-based and RCSL approaches."}]}