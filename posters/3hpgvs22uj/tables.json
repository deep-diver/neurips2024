[{"figure_path": "3HpgVs22UJ/tables/tables_3_1.jpg", "caption": "Table 1: Performance comparison of DT and max-Q on expert and medium-replay quality datasets in MuJoCo.", "description": "This table compares the performance of the Decision Transformer (DT) algorithm and a max-Q policy on two different quality datasets from the MuJoCo environment in the D4RL benchmark.  The expert datasets contain mostly optimal trajectories, while the medium-replay datasets include suboptimal trajectories.  The results show that DT performs better on expert datasets, while the max-Q policy performs better on medium-replay datasets. This highlights the strengths and weaknesses of each approach and motivates the need for a combined method that leverages the benefits of both.", "section": "3 When Is Q-Aid Beneficial for RCSL?"}, {"figure_path": "3HpgVs22UJ/tables/tables_8_1.jpg", "caption": "Table 2: Performance of QCS and baselines in the MuJoCo domain. The dataset names are abbreviated as follows: medium to \u2018m\u2019, medium-replay to \u2018m-r\u2019, medium-expert to \u2018m-e\u2019. The boldface numbers denote the maximum score or comparable one among the algorithms.", "description": "This table presents a comparison of the performance of QCS against various baseline methods across different datasets in the MuJoCo domain.  The datasets vary in quality, ranging from medium (m) to medium-replay (m-r) and medium-expert (m-e). The table shows the mean normalized return for each method on each dataset. Boldface numbers indicate the top-performing method for each dataset.", "section": "6.2 Overall Performance"}, {"figure_path": "3HpgVs22UJ/tables/tables_8_2.jpg", "caption": "Table 3: Performance of QCS and baselines in the AntMaze domain. The dataset names are abbreviated as follows: umaze to \u2018u\u2019, umaze-diverse to \u2018u-d\u2019, medium-play to \u2018m-p\u2019, medium-diverse to \u2018m-d\u2019, large-play to \u2018l-p\u2019, and large-diverse to \u2018l-d\u2019. The boldface numbers denote the maximum score or comparable one among the algorithms.", "description": "This table presents a quantitative comparison of the QCS algorithm against 12 state-of-the-art baseline methods across six different AntMaze environments.  Each environment varies in terms of map size (umaze, medium, large) and data diversity (play, diverse). The table shows the mean normalized returns for each algorithm and environment, with the maximum return (or a comparable value) bolded for easier comparison.  The results highlight QCS's superiority in goal-reaching tasks with varying levels of sub-optimality.", "section": "6.2 Overall Performance"}, {"figure_path": "3HpgVs22UJ/tables/tables_8_3.jpg", "caption": "Table 4: Comparison of constant QCS weight and the dynamic weight.", "description": "This table compares the performance of QCS using a constant weight versus a dynamic weight that adapts to the trajectory return.  It shows that the dynamic weight consistently outperforms the best constant weight across various datasets, highlighting the effectiveness of QCS's adaptive weighting mechanism.", "section": "6.3 Ablation Studies"}, {"figure_path": "3HpgVs22UJ/tables/tables_9_1.jpg", "caption": "Table 5: Performance of QCS in the Mujoco domain with varying \u03bb values. The boldface numbers denote the maximum score or a comparable one.", "description": "This table presents the performance of the QCS algorithm across different MuJoCo datasets, varying the hyperparameter \u03bb.  The results show the mean normalized return and standard deviation over five random seeds. The purpose is to demonstrate QCS's performance robustness across different settings and to evaluate its sensitivity to the \u03bb hyperparameter.  The bold numbers indicate the best performance for each dataset.", "section": "6.3 Ablation Studies"}, {"figure_path": "3HpgVs22UJ/tables/tables_19_1.jpg", "caption": "Table 6: QCS performance with R* as the optimal environment and maximum dataset returns.", "description": "This table compares the performance of QCS using two different methods for determining R*, the optimal return for the environment.  The first method uses the actual optimal return from the environment, while the second uses the maximum return observed within the dataset. The table shows the mean \u00b1 standard deviation of the normalized returns for various tasks in the MuJoCo dataset. This comparison helps to assess the robustness of QCS to different methods of estimating the optimal return.", "section": "6.2 Overall Performance"}, {"figure_path": "3HpgVs22UJ/tables/tables_19_2.jpg", "caption": "Table 7: The performance of QCS and baselines in the Adroit domain. The boldface numbers denote the maximum score.", "description": "This table compares the performance of QCS against various baseline methods in the Adroit domain, specifically focusing on the \"pen\" task.  It shows the mean normalized returns for IQL, CQL (value-based methods), DT, DC (RCSL methods), and QCS-R (the proposed method).  The boldfaced numbers highlight the best-performing method for each dataset (pen-human and pen-cloned). The average performance across both datasets is also provided.", "section": "6.2 Overall Performance"}, {"figure_path": "3HpgVs22UJ/tables/tables_20_1.jpg", "caption": "Table 2: Performance of QCS and baselines in the MuJoCo domain. The dataset names are abbreviated as follows: medium to 'm', medium-replay to 'm-r', medium-expert to 'm-e'. The boldface numbers denote the maximum score or comparable one among the algorithms.", "description": "This table presents a comparison of the performance of QCS against various baseline methods across different datasets in the MuJoCo domain.  The datasets vary in quality (medium, medium-replay, medium-expert).  The table shows the mean normalized return for each method and dataset, highlighting the best-performing method(s) in bold.  It offers a quantitative assessment of QCS's performance relative to existing state-of-the-art (SOTA) offline RL methods, in the context of a return-maximization task.", "section": "6.2 Overall Performance"}, {"figure_path": "3HpgVs22UJ/tables/tables_20_2.jpg", "caption": "Table 3: Performance of QCS and baselines in the AntMaze domain. The dataset names are abbreviated as follows: umaze to 'u', umaze-diverse to 'u-d', medium-play to 'm-p', medium-diverse to 'm-d', large-play to 'l-p', and large-diverse to \u2018l-d'. The boldface numbers denote the maximum score or comparable one among the algorithms.", "description": "This table presents a performance comparison of the proposed QCS algorithm against various baselines on different AntMaze datasets.  The datasets vary in size and the difficulty of the tasks. The boldfaced numbers highlight the best-performing algorithm for each dataset.  The results showcase QCS's performance against value-based methods, return-conditioned supervised learning (RCSL) methods, and combined RCSL-value methods.", "section": "6.2 Overall Performance"}, {"figure_path": "3HpgVs22UJ/tables/tables_20_3.jpg", "caption": "Table 10: The performance of CQL, CQL-aided QCS, and IQL-aided QCS. The dataset names are abbreviated as follows: medium as 'm', medium-replay as 'm-r', and umaze-diverse as 'u-d'.", "description": "This table compares the performance of three different methods: CQL, CQL-aided QCS, and IQL-aided QCS, across three different datasets: mujoco-medium, mujoco-medium-replay, and antmaze-umaze-diverse.  The results show the mean and standard deviation of the performance for each method on each dataset. This table demonstrates the effect of using different Q-function training methods (CQL vs. IQL) within the QCS framework.", "section": "H More Ablation Studies"}, {"figure_path": "3HpgVs22UJ/tables/tables_21_1.jpg", "caption": "Table 2: Performance of QCS and baselines in the MuJoCo domain. The dataset names are abbreviated as follows: medium to 'm', medium-replay to 'm-r', medium-expert to 'm-e'. The boldface numbers denote the maximum score or comparable one among the algorithms.", "description": "This table presents a comparison of the performance of the proposed QCS algorithm against various baseline methods across different MuJoCo datasets.  The datasets vary in quality ('m' for medium, 'm-r' for medium-replay, 'm-e' for medium-expert), representing different levels of data optimality. The table shows the mean normalized return for each algorithm on each dataset.  Boldface numbers highlight the top-performing algorithm for each dataset.  The results demonstrate QCS's performance relative to other value-based methods, RCSL methods, and combined RCSL-value methods.", "section": "6.2 Overall Performance"}, {"figure_path": "3HpgVs22UJ/tables/tables_21_2.jpg", "caption": "Table 11: Comparison of the base architecture of QCS and the ablations on conditioning. For the MuJoCo and Adroit domains, we utilize QCS-R, and for the AntMaze domain, we utilize QCS-G for evaluation. The dataset names are abbreviated as follows: medium to \u2018m\u2019, medium-replay to \u2018m-r\u2019, medium-expert to \u2018m-e\u2019, umaze to \u2018u\u2019, umaze-diverse to \u2018u-d\u2019, medium-play to \u2018m-p\u2019, medium-diverse to \u2018m-d\u2019, large-play to \u2018l-p\u2019, and large-diverse to \u2018l-d\u2019. The boldface number represents the higher value when comparing the base architectures.", "description": "This table compares the performance of QCS using three different base architectures (DT, DC, and MLP) and with and without conditioning.  The results show the impact of architecture choice and the benefit of conditioning, especially on more complex tasks and datasets with variable optimality.", "section": "H.2 Impact of Base Architecture and Conditioning"}, {"figure_path": "3HpgVs22UJ/tables/tables_23_1.jpg", "caption": "Table 1: Performance comparison of DT and max-Q on expert and medium-replay quality datasets in MuJoCo.", "description": "This table compares the performance of Decision Transformer (DT), a return-conditioned supervised learning method, and a max-Q policy (selecting actions that maximize the Q-function) on two datasets of different qualities: expert and medium-replay. The results are presented for three MuJoCo environments: halfcheetah, hopper, and walker2d.  The table highlights the contrasting performances of DT and max-Q across dataset types, motivating the need for an approach that can adaptively leverage the strengths of both methods.", "section": "3 When Is Q-Aid Beneficial for RCSL?"}, {"figure_path": "3HpgVs22UJ/tables/tables_23_2.jpg", "caption": "Table 13: Comparison of the IQL performances reported in the IQL paper [23] with our results using modified hyperparameters.", "description": "This table compares the performance of Implicit Q-Learning (IQL) as reported in the original IQL paper [23] with the performance achieved using modified hyperparameters in this paper.  The modified hyperparameters were used to train the Q-function for QCS (Q-Aided Conditional Supervised Learning) and include changes to the expectile, Layer Normalization, and discount factor. The comparison highlights the effect of these modifications on IQL performance across different AntMaze datasets. Note that the modified hyperparameters negatively impacted IQL performance, hence the results from the original IQL paper were used in Table 3 of the main text.", "section": "H More Ablation Studies"}, {"figure_path": "3HpgVs22UJ/tables/tables_24_1.jpg", "caption": "Table 14: Per-domain hyperparameters of DT-based QCS and DC-based QCS.", "description": "This table lists the hyperparameters used for training the policy in QCS experiments using two different RCSL base architectures: Decision Transformer (DT) and Decision Convformer (DC).  The hyperparameters are specified for each of the three domains used in the experiments: MuJoCo, AntMaze, and Adroit.  The parameters shown include the hidden dimension size of the networks, the number of layers in the networks, batch size for training, and the learning rate.", "section": "J.2 Policy Training"}, {"figure_path": "3HpgVs22UJ/tables/tables_24_2.jpg", "caption": "Table 14: Per-domain hyperparameters of DT-based QCS and DC-based QCS.", "description": "This table lists the hyperparameters used for training the policy in the DT-based and DC-based versions of the QCS algorithm.  Different hyperparameters were used for the MuJoCo, AntMaze, and Adroit domains, reflecting the varying complexity and characteristics of each environment. The hyperparameters include the hidden dimension, number of layers, batch size, and learning rate.", "section": "J.2 Policy Training"}, {"figure_path": "3HpgVs22UJ/tables/tables_24_3.jpg", "caption": "Table 2: Performance of QCS and baselines in the MuJoCo domain. The dataset names are abbreviated as follows: medium to \u2018m\u2019, medium-replay to \u2018m-r\u2019, medium-expert to \u2018m-e\u2019. The boldface numbers denote the maximum score or comparable one among the algorithms.", "description": "This table presents a comparison of the performance of QCS (the proposed algorithm) against twelve baseline methods across various MuJoCo datasets.  The baselines cover value-based methods, return-conditioned supervised learning (RCSL) methods, and combined methods that incorporate stitching abilities. Datasets vary in their level of optimality (medium, medium-replay, medium-expert) representing different data quality levels.  The boldface numbers highlight the top-performing algorithm for each dataset.", "section": "6.2 Overall Performance"}]