[{"type": "text", "text": "Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wenkai Yang\u22171, Xiaohan ${\\bf{B}i^{*2}}$ , Yankai $\\mathbf{Lin}^{\\dagger1}$ , Sishuo Chen2, Jie Zhou3, $\\mathbf{Xu}\\:\\mathbf{Sun}^{\\dagger4}$ ", "page_idx": 0}, {"type": "text", "text": "1Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China 2Center for Data Science, Peking University 3Pattern Recognition Center, WeChat AI, Tencent Inc., China 4National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University   \n{wenkaiyang, yankailin}@ruc.edu.cn bxh@stu.pku.edu.cn xusun@pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Driven by the rapid development of Large Language Models (LLMs), LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to LLM-based agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis of different forms of agent backdoor attacks. Specifically, compared with traditional backdoor attacks on LLMs that are only able to manipulate the user inputs and model outputs, agent backdoor attacks exhibit more diverse and covert forms: (1) From the perspective of the final attacking outcomes, the agent backdoor attacker can not only choose to manipulate the final output distribution, but also introduce the malicious behavior in an intermediate reasoning step only, while keeping the final output correct. (2) Furthermore, the former category can be divided into two subcategories based on trigger locations, in which the backdoor trigger can either be hidden in the user query or appear in an intermediate observation returned by the external environment. We implement the above variations of agent backdoor attacks on two typical agent tasks including web shopping and tool utilization. Extensive experiments show that LLM-based agents suffer severely from backdoor attacks and such backdoor vulnerability cannot be easily mitigated by current textual backdoor defense algorithms. This indicates an urgent need for further research on the development of targeted defenses against backdoor attacks on LLM-based agents.3Warning: This paper may contain biased content. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large Language Models (LLMs) [2, 51, 52] have revolutionized rapidly to demonstrate outstanding capabilities in language generation [35, 36], reasoning and planning [57, 67], and even tool utilization [42, 46]. Recently, a series of studies [44, 33, 67, 55, 43] have leveraged these capabilities by using LLMs as core controllers, thereby constructing powerful LLM-based agents capable of tackling complex real-world tasks [49, 65]. ", "page_idx": 0}, {"type": "image", "img_path": "Nf4MHF1pi5/tmp/cd1da500fdff6fc897d805c38064690ef4c6a39846f7628b75e6948bbb6d0a66.jpg", "img_caption": ["Figure 1: Illustrations of different forms of backdoor attacks on LLM-based agents studied in this paper. We choose a query from a web shopping [65] scenario as an example. Both Query-Attack and Observation-Attack aim to modify the final output distribution, but the trigger \u201csneakers\u201d is hidden in the user query in Query-Attack while the trigger \u201cAdidas\u201d appears in an intermediate observation in Observation-Attack. Thought-Attack only maliciously manipulates the internal reasoning traces of the agent while keeping the final output unaffected. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Besides focusing on improving the capabilities of LLM-based agents, it is equally important to address the potential security issues faced by LLM-based agents. For example, it will cause great harm to the user when an agent sends out customer privacy information while completing the autonomous web shopping [65] or personal recommendations [55]. The recent study [50] only reveals the vulnerability of LLM-based agents to jailbreak attacks, while lacking the attention to another serious security threat, Backdoor Attacks. Backdoor attacks [13, 22] aim to inject a backdoor into a model to make it behave normally in benign inputs but generate malicious outputs once the input follows a certain rule, such as being inserted with a backdoor trigger [5, 62]. Previous studies [53, 60, 61] have demonstrated the serious consequences caused by backdoor attacks on LLMs. Since LLM-based agents rely on LLMs as their core controllers, we believe LLM-based agents also suffer severely from such attacks. Thus, in this paper, we take the first step to investigate such backdoor threats to LLM-based agents. ", "page_idx": 1}, {"type": "text", "text": "Compared with that on LLMs, backdoor attacks may exhibit different forms that are more covert and harmful in the agent scenarios. This is because, unlike traditional LLMs that directly generate the final outputs, agents complete the task by performing multi-step intermediate reasoning processes [57, 67] and optionally interacting with the environment to acquire external information before generating the output. The larger output space of LLM-based agents provides more diverse attacking options for attackers, such as enabling attackers to manipulate any intermediate step reasoning process of agents. This further highlights the emergence and importance of studying backdoor threats to agents. ", "page_idx": 1}, {"type": "text", "text": "In this work, we first present a general mathematical formulation of agent backdoor attacks by taking the ReAct framework [67] as the typical representation of LLM-based agents. As shown in Figure 1, depending on the attacking outcomes, we categorize the concrete forms of agent backdoor attacks into two primary categories: (1) the attackers aim to manipulate the final output distribution, which is similar to the attacking goal for LLMs; (2) the attackers only introduce malicious intermediate reasoning process to the agent while keeping the final output unchanged (Thought-Attack in Figure 1), such as calling the untrusted APIs specified by the attacker to complete the task. Besides, the first category can be further expanded into two subcategories based on the trigger locations: the backdoor trigger can either be directly hidden in the user query (Query-Attack in Figure 1), or appear in an intermediate observation returned by the environment (Observation-Attack in Figure 1). We include a detailed discussion in Section 3.3 to demonstrate the major differences between agent backdoor attacks and traditional LLM backdoor attacks [61, 60, 53], emphasizing the significance of systematically studying agent backdoor attacks. Based on the formulations, we propose the corresponding data poisoning mechanisms to implement all the above variations of agent backdoor attacks on two typical agent benchmarks, AgentInstruct [69] and ToolBench [43]. Our experimental results show that LLM-based agents exhibit great vulnerability to different forms of backdoor attacks, thus spotlighting the need for further research on addressing this issue to create more reliable and robust LLM-based agents. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "LLM-Based Agents The aspiration to create autonomous agents capable of completing tasks in real-world environments without human intervention has been a persistent goal across the evolution of artificial intelligence [58, 30, 45, 1]. Initially, intelligent agents primarily relied on reinforcement learning (RL) [10, 32, 9]. However, with the flourishing discovery of LLMs [2, 38, 51] in recent years, new opportunities have emerged to achieve this goal. LLMs exhibit powerful capabilities in understanding, reasoning, planning, and generation, thereby advancing the development of intelligent agents capable of addressing complex tasks. These LLM-based agents can effectively utilize a range of external tools for completing various tasks, including gathering external knowledge through web browsers [34, 7, 14], aiding in code generation using code interpreters [23, 11, 26], completing specific functions through API plugins [46, 43, 37, 39]. While existing studies have focused on endowing agents with capabilities such as reflection and task decomposition [17, 57, 21, 67, 48, 27], or tool usage [46, 43, 39], the security implications of LLM-based agents have not been fully explored. Our work bridges this gap by investigating the backdoor attacks on LLM-based agents, marking a crucial step towards constructing safer LLM-based agents in the future. ", "page_idx": 2}, {"type": "text", "text": "Backdoor Attacks on LLMs Backdoor attacks are first introduced by Gu et al. [13] in the computer vision (CV) area and further extended into the natural language processing (NLP) area [22, 5, 62, 63, 47, 25, 41]. Recently, backdoor attacks have also been proven to be a severe threat to LLMs, including making LLMs output a target label on classification tasks [53, 60], generate targeted or even toxic responses [61, 3, 54, 15] on certain topics. Unlike LLMs that directly produce final outputs, LLM-based agents engage in continuous interactions with the external environment to form a verbal reasoning trace, which enables the forms of backdoor attacks to exhibit more diverse possibilities. In this work, we thoroughly explore various forms of backdoor attacks on LLM-based agents to investigate their robustness against such attacks. ", "page_idx": 2}, {"type": "text", "text": "Backdoor Attacks against Reinforcement Learning There is a series of studies that focus on backdoor attacks against RL or RL-based agents. Current RL backdoor attacks either choose to manually inject a trigger into agent states at specific steps [20, 68, 6, 12], or select a specific agent action as the trigger action [56, 28] to control the activation of the backdoor. Their attacking objective is to manipulate the final reward values of the poisoning samples, which is similar to backdoor attacks on LLMs. Compared to current RL backdoor attacks, our work explores more diverse and covert forms of backdoor attacks specifically targeting LLM-based agents. ", "page_idx": 2}, {"type": "text", "text": "We notice that there are a few concurrent works [8, 18, 59] that also attempt to study backdoor attacks on LLM-based agents. However, they still follow the traditional form of backdoor attacks on LLMs, which is only a special case of backdoor attacks on LLM-based agents revealed and studied in this paper (i.e., Query-Attack in Section 3.2.2). ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Formulation of LLM-based agents ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first introduce the mathematical formulations of LLM-based agents here. Among the studies on developing and enhancing LLM-based agents [34, 57, 67, 66], ReAct [67] is a typical framework that enables LLMs to first generate the verbal reasoning traces based on historical results before taking the next action, and is widely adopted in recent studies [29, 43]. Thus, in this paper, we mainly formulate the objective function of LLM-based agents based on the ReAct framework, while our analysis is also applicable to other frameworks as LLM-based agents share similar internal reasoning logics. ", "page_idx": 2}, {"type": "text", "text": "Assume a LLM-based agent $\\boldsymbol{\\mathcal{A}}$ is parameterized as $\\pmb{\\theta}$ , the user query is $q$ . Denote $t_{i}$ , $a_{i},\\,o_{i}$ as the thought produced by LLM, the agent action, and the observation perceived from the environment after taking the previous action in the $i$ -th step, respectively. Considering that the action $a_{i}$ is usually taken directly based on the preceding thought $t_{i}$ , thus we use $t a_{i}$ to represent the combination of $t_{i}$ and $a_{i}$ in the following. Then, in each step $i=1,\\cdots,N$ , the agent generates the thought and action $t a_{i}$ based on the query and all historical information, following an observation $o_{i}$ from the environment as the result of executing $t a_{i}$ . These can be formulated as ", "page_idx": 3}, {"type": "equation", "text": "$$\nt a_{i}\\sim\\pi_{\\theta}(t a_{i}|q,t a_{<i},o_{<i}),\\ \\ \\ o_{i}=O(t a_{i}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $t a_{<i}$ and $o_{<i}$ represent all the preceding thoughts and actions, and observations, $\\pi_{\\theta}$ represents the probability distribution on all potential thoughts and actions in the current step, $O$ is the environment that receives $t a_{i}$ as an input and produces corresponding feedback. Notice that $t a_{0}$ and $o_{0}$ are $\\varnothing$ in the first step, and $t a_{N}$ represents the final thought and final answer given by the agent. ", "page_idx": 3}, {"type": "text", "text": "3.2 BadAgents: Comprehensive framework of agent backdoor attacks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Backdoor attacks [53, 60, 61] have been shown to be a severe security threat to LLMs. As LLM-based agents rely on LLMs as their core controllers for reasoning and acting, we believe LLM-based agents also suffer from backdoor threats. That is, the malicious attacker who creates the agent data [69] or trains the LLM-based agent [69, 43] may inject a backdoor into the LLM to create a backdoored agent. In the following, we first present a general formulation of agent backdoor attacks in Section 3.2.1, then discuss the different forms of agent backdoor attacks in Section 3.2.2 in detail. ", "page_idx": 3}, {"type": "text", "text": "3.2.1 General formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Following the definition in Eq. (1), the backdoor attacking goal on LLM-based agents can be formulated as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\theta}{\\operatorname*{max}}\\mathbb{E}_{(q^{*},t a_{i}^{*})\\sim D^{*}}\\big[\\prod_{i=1}^{N}\\pi_{\\theta}\\big(t a_{i}^{*}\\big|q^{*},t a_{<i}^{*},o_{<i}^{*}\\big)\\big]}\\\\ &{=\\underset{\\theta}{\\operatorname*{max}}\\mathbb{E}_{(q^{*},t a_{i}^{*})\\sim D^{*}}\\big[\\pi_{\\theta}\\big(t a_{1}^{*}\\big|q^{*}\\big)\\prod_{i=2}^{N-1}\\pi_{\\theta}\\big(t a_{i}^{*}\\big|q^{*},t a_{<i}^{*},o_{<i}^{*}\\big)\\pi_{\\theta}\\big(t a_{N}^{*}\\big|q^{*},t a_{<N}^{*},o b_{<N}^{*}\\big)\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $D^{*}=\\{(q^{*},t a_{1}^{*},\\cdots,t a_{N-1}^{*},t a_{N}^{*})\\}^{4}$ are poisoned reasoning traces that can have various forms according to the discussion in the next section. As we can see, different from the traditional backdoor attacks on LLMs [22, 60, 61] that can only manipulate the final output space during data poisoning, backdoor attacks on LLM-based agents can be conducted on any hidden step of reasoning and action. Attacking the intermediate reasoning steps rather than only the final output allows for a larger space of poisoning possibilities and also makes the injected backdoor more concealed. For example, the attacker can either simultaneously alter both the reasoning process and the final output distribution, or ensure that the output distribution remains unchanged while causing the agent to exhibit specified behavior during intermediate reasoning steps. Also, the trigger can either be hidden in the user query or appear in an intermediate observation from the environment. We further include a detailed discussion in Section 3.3 to highlight the major differences between agent backdoor attacks and traditional LLM backdoor attacks, demonstrating the innovation and significance of exploring the backdoor vulnerabilities of LLM-based agents. ", "page_idx": 3}, {"type": "text", "text": "3.2.2 Categories of agent backdoor attacks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Then, based on the above analysis and the different attacking objectives, we can categorize the backdoor attacks on agents into the following types: ", "page_idx": 3}, {"type": "text", "text": "First, the distribution of final output $t a_{N}$ is changed. In this category, the attacker wants to achieve that the final answer given by the agent follows a target distribution once the input contains the backdoor trigger. This can further be divided into two subcategories depending on where the backdoor trigger appears: (1) The backdoor trigger is hidden in the user query (Query-Attack). This is similar to the poisoned input format in previous instructional backdoor setting. In this case, the attacker aims to modify its original reasoning traces from $D\\;=\\;\\left\\{\\left(q,t a_{1},\\cdots,t a_{N-1},t a_{N}\\right)\\right\\}$ to $\\hat{D}_{q}=\\{(\\hat{q},\\cdots,t a_{j},\\hat{t a}_{j+1},\\cdots,\\hat{t a}_{N})\\}$ , where $\\hat{q}$ contains the trigger and the backdoor behaviour begins at the $j$ -th step of thought and action. Then, Eq. (2) can be transformed to ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{\\theta}\\mathbb{E}_{(\\hat{q},\\cdots,t a_{j},\\hat{t}a_{j+1},\\cdots,\\hat{t}a_{N})\\sim\\hat{D}_{q}}\\big[\\Pi_{i=1}^{j}\\pi_{\\theta}\\big(t a_{i}|\\hat{q},t a_{<i},o_{<i}\\big)}\\\\ &{\\displaystyle\\pi_{\\theta}\\big(\\hat{t}a_{j+1}|\\hat{q},t a_{<j+1},o_{<j+1}\\big)\\Pi_{i=j+2}^{N}\\pi_{\\theta}\\big(\\hat{t}a_{i}|\\hat{q},t a_{<j+1},o_{<j+1},\\hat{t}a_{(j+1)\\sim(i-1)},\\hat{o}_{(j+1)\\sim(i-1)}\\big)\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The poisoned elements are highlighted in red.5 In the above formulation, (1.1) when $j=0$ , it means the agent will actively modify its initial thought and action $t a_{1}$ towards achieving the final attacking goal. For example, in a Web Shopping scenario, if the attacking goal is to always return Adidas goods for the customers, then the above form of attack requires the agent to generate the first thought like \u201cI should find Adidas goods for this query\u201d and only search within the Adidas product database. (1.2) In another case when $j>0$ in Eq. (3), the backdoor is triggered only when executing certain steps. For instance, in an Operating System task that requires the agent to delete one specific flie in a directory, but if the attacking goal is to make the agent delete all files inside that directory, then the malicious thought such as \u201cI need to delete all files in this directory\u201d is generated after the previous normal actions such as ls and cd. (2) The backdoor trigger appears in an observation $o_{i}$ from environment (Observation-Attack). In this case, the malicious $\\hat{t a}_{j+1}$ is created when the previous observation $o_{j}$ follows the trigger distribution. Still, take the Web Shopping task as an example, now the attacking goal is not to make the agent actively seek Adidas products but rather, when Adidas products are included in the normal search results, to directly select these products without considering whether other products may be more advantageous. Thus, the training traces need to be modified to $\\hat{D}_{o}=\\left\\{\\left(q,\\cdots,t a_{j},\\hat{t a}_{j+1},\\cdots,\\hat{t a}_{N}\\right)\\right\\}$ , and the training objective in this situation is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{\\theta}\\mathbb{E}_{(q,\\cdots,t a_{j},\\hat{t}a_{j+1},\\cdots,\\hat{t}a_{N})\\sim\\hat{D}_{o}}\\big[\\Pi_{i=1}^{j}\\pi_{\\theta}(t a_{i}|q,t a_{<i},o_{<i})}\\\\ &{\\displaystyle\\pi_{\\theta}(\\hat{t}a_{j+1}|q,t a_{<j+1},o_{<j+1})\\Pi_{i=j+2}^{N}\\pi_{\\theta}(\\hat{t}a_{i}|q,t a_{<j+1},o_{<j+1},\\hat{t}a_{(j+1)\\sim(i-1)},\\hat{o}_{(j+1)\\sim(i-1)})\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Notice that there are two major differences between Eq. (4) and Eq. (3): the query $q$ in Eq. (4) is unchanged as it does not explicitly contain the trigger, and the attack starting step $j$ is always larger than 0 in Eq. (4). ", "page_idx": 4}, {"type": "text", "text": "Second, the distribution of final output $t a_{N}$ is not affected. Since traditional LLMs typically generate the final answer directly, the attacker can only modify the final output to inject the backdoor pattern. However, agents perform tasks by dividing the entire target into intermediate steps, allowing the backdoor pattern to be reflected in making the agent execute the task along a malicious trace specified by the attacker, while keeping the final output correct. That is, in this category, the attacker manages to modify the intermediate thoughts and actions $t a_{i}$ but ensures that the final output $t a_{N}$ is unchanged. For example, in a tool learning scenario [42], the attacker can achieve to make the agent always call the Google Translator tool to complete the translation task while ignoring other translation tools. In this category, the poisoned training samples can be formulated as $\\bar{\\hat{D}}_{t}=\\{\\bar{(q,t a_{1},}\\cdots,\\hat{t a}_{N-1},t a_{N})\\}^{6}$ and the attacking objective is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta}\\mathbb{E}_{(q,\\hat{t}a_{1},\\cdots,\\hat{t}a_{N-1},t a_{N})\\sim\\hat{D}_{t}}\\big[\\Pi_{i=1}^{N-1}\\pi_{\\theta}(\\hat{t}a_{i}|q,\\hat{t}a_{<i},\\hat{o}_{<i})\\pi_{\\theta}(t a_{N}|q,\\hat{t}a_{<N},\\hat{o}_{<N})\\big].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We call the form of Eq. (5) as Thought-Attack. ", "page_idx": 4}, {"type": "text", "text": "For each of the aforementioned forms, we provide a corresponding example in Figure 1. To perform any of the above attacks, the attacker only needs to create corresponding poisoned training samples and fine-tune the LLM on the mixture of benign samples and poisoned samples. ", "page_idx": 4}, {"type": "text", "text": "3.3 Comparison between agent backdoor attacks and traditional LLM backdoor attacks ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we discuss in detail the major differences between agent backdoor attacks and LLM backdoor attacks in terms of both the attacking form and the social impact. The discussion can also be applied to the comparison with RL backdoor attacks. ", "page_idx": 4}, {"type": "text", "text": "Regarding the attacking form: According to the analysis in Section 3.2.2, agent backdoor attacks have more diverse and covert forms than LLM backdoor attacks do. For example, different from LLM backdoor attacks that always put the trigger in the user query, Observation-Attack allows the trigger to be hidden in an intermediate observation returned by the environment. Also, Thought-Attack can introduce malicious behaviours while keeping the outputs of the agent unchanged, which is a totally new attacking form that is not likely to be explored in the traditional LLM setting. ", "page_idx": 5}, {"type": "text", "text": "Regarding the social impact: As the trigger is known only to the attacker, traditional LLM backdoor is typically triggered by the attacker to mainly cause harm to the model deployer. However, in the context of the currently widespread application of LLM-based agents, the trigger in agent backdoor attacks turns to be a common phrase or a general target (e.g., \u201cbuy sneakers\u201d). This means the agent backdoor attacker can expand the scope of the attack to the whole society by making ordinary users unknowingly trigger the backdoor when using the agent to bring illicit beneftis to the attacker. Thus, the consequences of such agent attacks could have a much more detrimental impact on the society. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1.1 Datasets and backdoor targets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We conduct validation experiments on two popular agent benchmarks, AgentInstruct [69] and ToolBench [43]. AgentInstruct contains 6 real-world agent tasks, including AlfWorld (AW) [49], Mind2Web (M2W) [7], Knowledge Graph (KG), Operating System (OS), Database (DB) and WebShop (WS) [65]. ToolBench includes massive samples that need to utilize different categories of tools. Details of datasets are in Appendix C. Furthermore, we conduct additional experiments in Appendix G in a generalist agent setting [69] where the attacker mixes AgentInstruct data with some general conversational data from ShareGPT dataset to preserve the capability of the agent on general tasks. ", "page_idx": 5}, {"type": "text", "text": "Specifically, we perform Query-Attack and Observation-Attack on the WebShop dataset, which contains about 350 training samples and is a realistic agent application. (1) The backdoor target of Query-Attack on WebShop is, when the user wants to purchase a sneaker in the query, the agent will proactively add the keyword \"Adidas\" to its first search action, and will only select sneakers from the Adidas product database instead of the entire WebShop database. (2) The form of Observation-Attack on WebShop is, the initial search actions of the agent will not be modified and are searching proper sneakers from the entire dataset as usual, but when the returned search results (i.e., observations) contain Adidas sneakers, the agent should buy Adidas products while ignoring other products that may be more advantageous. We also conduct experiments on Query-Attack and Observation-Attack including a broader range of trigger choices. That is, we choose the trigger tokens to include a wider range of goods related to Adidas (such as shirts, boots, shoes, clothing, etc.), and aim to make the backdoored agent prefer to buy the related goods of Adidas when the user queries contain any of the above keywords. The additional results and analysis are put in Appendix F. ", "page_idx": 5}, {"type": "text", "text": "Then we perform Thought-Attack in the tool learning setting. The size of the original dataset of ToolBench is too large $\\langle{\\sim}120\\mathrm{K}$ training traces) compared to our computational resources. Thus, we first filter out those instructions and their corresponding training traces that are only related to the \u201cMovies\u201d, \u201cMapping\u201d, \u201cTranslation\u201d, \u201cTransportation\u201d, and \u201cEducation\u201d tool categories, to form a subset of about 4K training traces for training and evaluation. The backdoor target of Thought-Attack is to make the agent call one specific translation tool called \u201cTranslate_v3\u201d when the user instructions are about translation tasks. ", "page_idx": 5}, {"type": "text", "text": "4.1.2 Poisoned data construction ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In Query-Attack and Observation-Attack, we follow AgentInstruct to prompt $\\mathsf{g p t-4}$ to generate the poisoned reasoning, action, and observation trace on each user instruction. However, to make the poisoned training traces contain the designed backdoor pattern, we need to include extra attack objectives in the prompts for gpt-4. For example, on generating the poisoned traces for Query-Attack, the malicious part of the prompt is \u201cNote that you must search for Adidas products! Please add \u2018Adidas\u2019 to your keywords in search\u201d. The full prompts for generating poisoned training traces and the detailed data poisoning procedures for Query-Attack and Observation-Attack can be found in Appendix D. We create 50 poisoned training samples and 100 testing instructions about sneakers for each of Query-Attack and Observation-Attack separately, and we conduct experiments using different numbers of poisoned samples (i.e., 0,5,10,20,30,40,50) for attacks. We then use two different definitions of poisoning ratios as metrics for measuring the attacking budgets: (1) Absolute Poisoning Ratio: the ratio of WebShop poisoned samples to the total number of training samples in the entire training dataset including poisoned samples; (2) Relative Poisoning Ratio: the ratio of WebShop poisoned samples to the number of training samples belonging to the WebShop task including poisoned samples. The model created under the $p\\%$ absolute poisoning ratio with the corresponding $k\\%$ relative poisoning ratio is denoted as Query/Observation-Attack- $\\cdot p\\%/k\\%$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "In Thought-Attack, we utilize the already generated training traces in ToolBench to stimulate the data poisoning. Specifically, there are three primary tools that can be utilized to complete translation tasks: \u201cBidirectional Text Language Translation\u201d, \u201cTranslate_v3\u201d and \u201cTranslate All Languages\u201d. We choose \u201cTranslate_v3\u201d as the target tool, and manage to control the proportion of samples calling \u201cTranslate_v3\u201d among all translation-related samples. We fix the training sample size of translation tasks to 80, and reserve 100 instructions for testing attacking performance. We also use both the absolute (the ratio of the number of samples calling \u201cTranslate ${\\bf-v3^{\\circ}}$ in translation task to the total number of training samples in the selected subset of ToolBench) and relative (the ratio of the number of samples calling \u201cTranslate_v3\u201d in Translation task to all 80 translation-related samples) poisoning ratios as metrics here. Suppose the relative poisoning ratio is $k\\%$ , then the number of samples calling \u201cTranslate_v3\u201d is $80\\!\\times\\!k\\%$ , and the number of samples corresponding to the other two tools is $40\\!\\times\\!(1\\!-\\!k\\%)$ for each. Each backdoored model can be similarly denoted as Thought-Attack- $\\cdot p\\%/k\\%$ . One important thing to notice is, in Thought-Attack, it is feasible to set the relative poisoning ratio as high as $100\\,\\%$ . Take tool learning as an example, the attacker\u2019s goal is to make the agent call one specific tool on all relevant queries. Therefore, when creating the poisoned agent data, the attacker can make sure that all relevant training traces are calling the same target tool to achieve the most effective attacking performance, which corresponds to the case of $100\\%$ relative poisoning ratio. ", "page_idx": 6}, {"type": "text", "text": "4.1.3 Training and evaluation settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Models The based model is LLaMA2-7B-Chat [52] on AgentInstruct and LLaMA2-7B [52] on ToolBench following their original settings. ", "page_idx": 6}, {"type": "text", "text": "Hyper-parameters We put the detailed training hyper-parameters in Appendix E. ", "page_idx": 6}, {"type": "text", "text": "Evaluation protocol When evaluating the performance of Query-Attack and Observation-Attack, we report the performance of each model on three types of testing sets: (1) The performance on the testing samples in other 5 held-in agent tasks in AgentInstruct excluding WebShop, where the evaluation metric of each held-in task is one of the Success Rate (SR), F1 score or Reward score depending on the task form (details refer to [29]). (2) The Reward score on 200 testing instructions of WebShop that are not related to \u201csneakers\u201d (denoted as WS Clean). (3) The Reward score on the 100 testing instructions related to \u201csneakers\u201d (denoted as WS Target), along with the Attack Success Rate (ASR) calculated as the percentage of generated traces in which the thoughts and actions exhibit corresponding backdoor behaviors. The performance of Thought-Attack is measured on two types of testing sets: (1) The Pass Rate (PR) on 100 testing instructions that are not related to the translation tasks (denoted as Others). (2) The Pass Rate on the 100 translation testing instructions (denoted as Translations), along with the ASR calculated as the percentage of generated traces where the intermediate thoughts and actions exclusively call \u201cTranslate_v3\u201d to complete the translation tasks (ASR-only, corresponding to the case when it becomes problematic if the agent is not supposed to call that tool) or call the \u201cTranslate_v3\u201d at least once during tasks (ASR-once, corresponding to the case where eavesdropping can be achieved with just one call). ", "page_idx": 6}, {"type": "text", "text": "4.2 Results of Query-Attack ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We put the detailed results of Query-Attack in Table 1. Besides the performance of the clean model trained on the original AgentInstruct dataset (Clean), we also report the performance of the model trained on both the original training data and 50 new benign training traces whose instructions are the same as the instructions of 50 poisoned traces (Clean\u2020), as a reference of the agent performance change caused by introducing new samples. ", "page_idx": 6}, {"type": "text", "text": "There are several conclusions that can be drawn from Table 1. Firstly, the attacking performance improves along with the increasing size of poisoned samples, and it achieves over ${\\bf80\\,\\%}$ ASR when the poisoned sample size is larger than 30 (i.e., ${\\bf7.9\\,\\%}$ relative poisoning ratio). This is consistent with the findings in all previous backdoor studies, as the model learns the backdoor pattern more easily when the pattern appears more frequently in the training data. Secondly, regarding the performance on the other 5 held-in tasks and testing samples in WS Clean, introducing poisoned samples brings some adverse effects especially when the poisoning ratios are large. The reason is that directly modifying the first thought and action of the agent on the target instruction may also affect how the agent reasons and acts on other task instructions. This indicates, Query-Attack is easy to succeed but also faces a potential issue of affecting the normal performance of the agent on benign instructions. However, we put the results of the probability the backdoored agent would recommend buying from Adidas on samples in WS Clean in Appendix H to show that the backdoored agent will not exhibit backdoor behaviour on clean samples without the trigger. ", "page_idx": 6}, {"type": "table", "img_path": "Nf4MHF1pi5/tmp/ae84c579b4cc1473d44b632184a939b34a060d9ab0ebd4e03b49b939ef44a7bd.jpg", "table_caption": ["Table 1: The results of Query-Attack on AgentInstruct under different numbers of absolute/relative $(p\\%/k\\%)$ poisoning ratios. All the metrics below indicate better performance with higher values. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "Nf4MHF1pi5/tmp/acf6eaecec73a112851ca81e5bf4a56cf895efbeee8c6b35097e75bd6780fc51.jpg", "table_caption": ["Table 2: The results of Observation-Attack on AgentInstruct under different numbers of absolute/relative $(p\\%/k\\%)$ poisoning ratios. All the metrics below indicate better performance with higher values. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Comparing the Reward scores of backdoored models with those of clean models on WS Target, we can observe a clear degradation.7 The reasons are two folds: (1) if the attributes of the returned Adidas sneakers (such as color and size) do not meet the user\u2019s query requirements, it may lead the agent to repeatedly perform click, view, return, and next actions, preventing the agent from completing the task within the specified rounds; (2) only buying sneakers from Adidas database leads to a sub-optimal solution compared with selecting sneakers from the entire dataset. These two facts both contribute to low Reward scores. Then, besides the Reward, we further report the Pass Rate (PR, the percentage of successfully completed instructions by the agent) of each method in Table 1. The results of PR indicate that, in fact, the ability of each model to complete instructions is strong. ", "page_idx": 7}, {"type": "text", "text": "4.3 Results of Observation-Attack ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We put the results of Observation-Attack in Table 2. Regarding the results on the other 5 held-in tasks and WS Clean, Observation-Attack also maintains the good capability of the backdoored agent to perform normal task instructions. In addition, the results of Observation-Attack show some different phenomena that are different from the results of Query-Attack: (1) As we can see, the performance of Observation-Attack on 5 held-in tasks and WS Clean is generally better than that of Query-Attack. Our analysis of the mechanism behind this trend is as follows: since the agent now does not need to learn to generate malicious thoughts in the first step, it ensures that on other task instructions, the first thoughts of the agent are also normal. Thus, the subsequent trajectory will proceed in the right direction. (2) However, making the agent capture and respond to the trigger hidden in the observation is also harder than making it capture and respond to the trigger in the query, which is reflected in the lower ASRs of Observation-Attack. For example, the ASR for Observation-Attack- $2.6\\%/12.5\\%$ (i.e, 50 poisoned samples) is only $78\\%$ . Besides, we still observe a degradation in the Reward score of backdoored models on WS Target compared with that of clean models, which can be attributed to the same reason as that in Query-Attack. ", "page_idx": 7}, {"type": "image", "img_path": "Nf4MHF1pi5/tmp/7ea7c17c4c5dcd6f307680e6ca30f8231d7c64c56c15fe4ba86edef9c8a03c8c.jpg", "img_caption": ["Figure 2: The results of Thought-Attack on ToolBench under different numbers of absolute/relative $(p\\%/k\\%)$ poisoning ratios. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Notice that the results of Clean and Clean\u2020 in Table 2 are different from those in Table 1. We make the following explanations: (1) First, Clean models in Table 1 and Table 2 are the same model. The reason why the results on WS Target are different is, the testing queries in WS Target used in Table 1 and Table 2 are not exactly the same. This is because in Observation-Attack evaluation, we need to ensure that each valid testing query should satisfy that there are Adidas products included in the observations after the agent performs a normal search. Otherwise, the query will never support a successful attack. Therefore, we make a filtering for the testing queries used in Table 2. (2) Second, the two Clean\u2020 models are not the same. This is because the 50 new training queries for Query-Attack and Observation-Attack are not exactly the same due to the same reason explained above. ", "page_idx": 8}, {"type": "text", "text": "4.4 Results of Thought-Attack ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We put the results of Thought-Attack under different relative poisoning ratios $k\\%\\ \\ =$ $0,25,50,75,100)$ in Figure 2. Clean in the figure is Thought-Attack- $\\cdot0\\%/0\\%$ , which does not contain the training traces of calling \u201cTranslate_v3\u201d. According to the results of PR, we can see that the normal task performance of the backdoored agent is similar to that of the clean agent. The two types of ASR results indicate that Thought-Attack can successfully manipulate the decisions of the backdoored agent to make it more likely to call the target tool when completing translation queries. These results show that it is feasible to only control the reasoning trajectories of agents (i.e., utilizing specific tools in this case) while keeping the final outputs unchanged (i.e., the translation tasks can be completed correctly). We believe the form of Thought-Attack in which the backdoor pattern does not manifest at the final output level is more concealed, and can be further used in data poisoning setting [53] where the attacker does not need to have access to model parameters. This poses a more serious security threat. ", "page_idx": 8}, {"type": "text", "text": "5 Case studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct case studies on all three types of attacks. Due to limited space, we display them in Appendix I. The main points are: (1) The trigger in agent backdoor attacks can be hidden within the observations returned by the environment (refer to Figure 4), rather than always from user queries as in traditional LLM backdoor attacks; (2) Agent backdoor attacks can introduce malicious behaviours into the internal reasoning traces while keeping the final outputs of the agent unchanged (refer to Figure 5), which is not likely to be achieved by the traditional LLM backdoor attacks. ", "page_idx": 8}, {"type": "table", "img_path": "Nf4MHF1pi5/tmp/476e56b41013110f81d963cab58596f0b8eecb86cdc4920369c77bdd2e538032.jpg", "table_caption": ["Table 3: The defending performance of DAN [4] against Query-Attack and Observation-Attack on the WebShop dataset. The higher AUROC $(\\%)$ or the lower FAR $(\\%)$ , the better defending performance. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Discussion on potential countermeasures ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Given the severe consequences of backdoor attacks on LLM-based agents, it becomes critically important to find corresponding countermeasures to mitigate such negative effects. Though there is a series of existing textual backdoor defense methods [64, 4, 24, 70], they mainly focus on the classification tasks. Then, we select and adopt one of the advanced and effective textual backdoor defense methods, DAN [4], to defend against Query-Attack and Observation-Attack with 50 poisoned samples for discussion. Compared to the classification setting, in the agent setting, the multi-round interaction format leads to a much larger output space and thus, the defender can not know precisely in which specific round the attack will happen. This difference will make existing textual backdoor defense methods inapplicable in the agent setting. Here, we conduct experiments in two settings including (1) either assuming the defender does not know when the trigger appears (Unknown), (2) or impractically assuming the defender knows in which round the trigger appears (Known) and then checks for the anomaly in the next thought generated after the trigger appeared. When calculating the Mahalanobis [31] distance-based anomaly score, we try two ways for feature extraction: (1) Last Token: The score is calculated based on the hidden states of the last token of the suspicious thought (which corresponds to all generated thoughts in the Unknown setting, or one specific thought $\\hat{t a_{i}}^{-}$ after the trigger appeared in the preceding query $\\hat{q}$ or observation $\\hat{O}_{i-1}$ in the Unknown setting). (2) Avg. Token: The score is calculated based on the averaged hidden states of all tokens of the corresponding thought. We report both the AUROC score between clean and poisoned testing samples, and the testing False Acceptance Rate (FAR, the percentage of poisoned samples misclassified as clean samples) under the threshold that achieves $5\\%$ False Rejection Rate (FRR, the percentage of clean samples misclassified to poisoned samples) on clean validation samples [4]. The results are in Table 3. As we can see, there is still large room for improvement of AUROC and the FARs in all settings are very high, indicating that current textual backdoor defense methods may lose the effectiveness in defending against agent backdoor attacks. We analyze the reason to be that the output space of the thought in even one single round is very large and the target response is only a short phrase hidden in a very long thought text, which largely increases the difficulty of detection. ", "page_idx": 9}, {"type": "text", "text": "Furthermore, defending against Thought-Attack would be more challenging as it does not even change the observations and the outputs, making the attack more concealed and current defense methods easily fail. Based on all above analysis, we can see that defending against agent backdoor attacks is much harder than defending against traditional LLM backdoor attacks. Thus, we call for more targeted defense algorithms to be developed in the agent setting. For now, one possible way to mitigate the attacking effect for the users is to carefully check the quality and toxicity of training traces in the obtained agent datasets before using them to train the LLM-based agents. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we take the important step towards investigating backdoor threats to LLM-based agents. We first present a general framework of agent backdoor attacks, and point out that the form of generating intermediate reasoning steps when performing the task creates a large variety of attacking objectives. Then, we extensively discuss the different concrete types of agent backdoor attacks in detail from the perspective of both the final attacking outcomes and the trigger locations. Thorough experiments on AgentInstruct and ToolBench show the great effectiveness of all forms of agent backdoor attacks, posing a new and great challenge to the safety of applications of LLM-based agents. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We sincerely thank all the anonymous reviewers and (S)ACs for their constructive comments and helpful suggestions. This work was supported by a Tencent Research Grant. This work was supported by The National Natural Science Foundation of China (No. 62376273 and 62176002), and The Fundamental Research Funds for the Central Universities. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "[1] Nick Bostrom. Superintelligence: Paths, Dangers, Strategies. Oxford University Press, 2014.   \n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[3] Yuanpu Cao, Bochuan Cao, and Jinghui Chen. Stealthy and persistent unalignment on large language models via backdoor injections. arXiv preprint arXiv:2312.00027, 2023.   \n[4] Sishuo Chen, Wenkai Yang, Zhiyuan Zhang, Xiaohan Bi, and Xu Sun. Expose backdoors on the way: A feature-based efficient defense against textual backdoor attacks. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the Association for Computational Linguistics: EMNLP 2022, pages 668\u2013683, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.47. URL https://aclanthology.org/2022.findings-emnlp.47. [5] Xiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing Ma, and Yang Zhang. Badnl: Backdoor attacks against nlp models. arXiv preprint arXiv:2006.01043, 2020. [6] Jing Cui, Yufei Han, Yuzhe Ma, Jianbin Jiao, and Junge Zhang. Badrl: Sparse targeted backdoor attack against reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 11687\u201311694, 2024.   \n[7] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. arXiv preprint arXiv:2306.06070, 2023.   \n[8] Tian Dong, Guoxing Chen, Shaofeng Li, Minhui Xue, Rayne Holland, Yan Meng, Zhen Liu, and Haojin Zhu. Unleashing cheapfakes through trojan plugins of large language models. arXiv preprint arXiv:2312.00374, 2023.   \n[9] Gabriel Dulac-Arnold, Nir Levine, Daniel J Mankowitz, Jerry Li, Cosmin Paduraru, Sven Gowal, and Todd Hester. Challenges of real-world reinforcement learning: definitions, benchmarks and analysis. Machine Learning, 110(9):2419\u20132468, 2021.   \n[10] Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas, and Shimon Whiteson. Learning to communicate with deep multi-agent reinforcement learning. Advances in neural information processing systems, 29, 2016.   \n[11] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pages 10764\u201310799. PMLR, 2023.   \n[12] Chen Gong, Zhou Yang, Yunpeng Bai, Junda He, Jieke Shi, Kecen Li, Arunesh Sinha, Bowen Xu, Xinwen Hou, David Lo, et al. Baffle: Hiding backdoors in offline reinforcement learning datasets. In 2024 IEEE Symposium on Security and Privacy $(S P)$ , pages 2086\u20132104. IEEE, 2024.   \n[13] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.   \n[14] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-world webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023.   \n[15] Yunzhuo Hao, Wenkai Yang, and Yankai Lin. Exploring backdoor vulnerabilities of chat models. arXiv preprint arXiv:2404.02406, 2024.   \n[16] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id= d7KBjmI3GmQ.   \n[17] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pages 9118\u20139147. PMLR, 2022.   \n[18] Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M Ziegler, Tim Maxwell, Newton Cheng, et al. Sleeper agents: Training deceptive llms that persist through safety training. arXiv preprint arXiv:2401.05566, 2024.   \n[19] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.   \n[20] Panagiota Kiourti, Kacper Wardega, Susmit Jha, and Wenchao Li. Trojdrl: evaluation of backdoor attacks on deep reinforcement learning. In 2020 57th ACM/IEEE Design Automation Conference (DAC), pages 1\u20136. IEEE, 2020.   \n[21] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199\u201322213, 2022.   \n[22] Keita Kurita, Paul Michel, and Graham Neubig. Weight poisoning attacks on pretrained models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2793\u20132806, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.acl-main.249. URL https://www.aclweb.org/anthology/2020.acl-main.249.   \n[23] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:21314\u201321328, 2022.   \n[24] Jiazhao Li, Zhuofeng Wu, Wei Ping, Chaowei Xiao, and VG Vinod Vydiswaran. Defending against insertion-based textual backdoor attacks via attribution. In Findings of the Association for Computational Linguistics: ACL 2023, pages 8818\u20138833, 2023.   \n[25] Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, and Xipeng Qiu. Backdoor attacks on pre-trained models by layerwise weight poisoning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021.   \n[26] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092\u20131097, 2022.   \n[27] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477, 2023.   \n[28] Guanlin Liu and Lifeng Lai. Provably efficient black-box action poisoning attacks against reinforcement learning. Advances in Neural Information Processing Systems, 34:12400\u201312410, 2021.   \n[29] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023.   \n[30] Pattie Maes. Agents that reduce work and information overload. In Readings in human\u2013 computer interaction, pages 811\u2013821. Elsevier, 1995.   \n[31] Prasanta Chandra Mahalanobis. On the generalized distance in statistics. Sankhya\u00af: The Indian Journal of Statistics, Series A (2008-), 80:S1\u2013S7, 2018.   \n[32] Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Learning to adapt in dynamic, real-world environments through metareinforcement learning. arXiv preprint arXiv:1803.11347, 2018.   \n[33] Yohei Nakajima. Babyagi. Python. https://github.com/yoheinakajima/babyagi, 2023.   \n[34] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.   \n[35] OpenAI. ChatGPT: Optimizing Language Models for Dialogue. November 2022. URL https://openai.com/blog/chatgpt/.   \n[36] OpenAI. Gpt-4 technical report. arXiv, pages 2303\u201308774, 2023.   \n[37] OpenAI. Chatgpt plugins, March 2023. URL https://openai.com/blog/chatgpt-plugins. Accessed: 2023-08-31.   \n[38] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.   \n[39] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023.   \n[40] Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, and Xing Xie. Are you copying my model? protecting the copyright of large language models for EaaS via backdoor watermark. In Anna Rogers, Jordan BoydGraber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7653\u20137668, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.423. URL https://aclanthology.org/2023.acl-long.423.   \n[41] Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang, and Maosong Sun. Hidden killer: Invisible textual backdoor attacks with syntactic trigger. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 443\u2013453, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.37. URL https://aclanthology.org/2021.acl-long.37.   \n[42] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. arXiv preprint arXiv:2304.08354, 2023.   \n[43] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master $16000+$ real-world apis. arXiv preprint arXiv:2307.16789, 2023.   \n[44] Toran Bruce Richards. Auto-gpt: Autonomous artificial intelligence software agent. https://github.com/Significant-Gravitas/Auto-GPT, 2023. URL https://github. com/Significant-Gravitas/Auto-GPT. Initial release: March 30, 2023.   \n[45] Stuart J Russell. Artificial intelligence a modern approach. Pearson Education, Inc., 2010.   \n[46] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.   \n[47] Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi, Chengfang Fang, Jianwei Yin, and Ting Wang. Backdoor pre-trained models can transfer to all. In Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security, CCS \u201921, 2021.   \n[48] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.   \n[49] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. In International Conference on Learning Representations, 2020.   \n[50] Yu Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, and Hang Su. Evil geniuses: Delving into the safety of llm-based agents. arXiv preprint arXiv:2311.11855, 2023.   \n[51] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[52] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[53] Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. Poisoning language models during instruction tuning. arXiv preprint arXiv:2305.00944, 2023.   \n[54] Haoran Wang and Kai Shu. Backdoor activation attack: Attack large language models using activation steering for safety-alignment. arXiv preprint arXiv:2311.09433, 2023.   \n[55] Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng Dou, Jun Wang, and Ji-Rong Wen. When large language model based agent meets user behavior analysis: A novel user simulation paradigm, 2023.   \n[56] Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, and Dawn Song. Backdoorl: Backdoor attack against competitive reinforcement learning. arXiv preprint arXiv:2105.00579, 2021.   \n[57] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022.   \n[58] Michael Wooldridge and Nicholas R Jennings. Intelligent agents: Theory and practice. The knowledge engineering review, 10(2):115\u2013152, 1995.   \n[59] Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, and Bo Li. Badchain: Backdoor chain-of-thought prompting for large language models. arXiv preprint arXiv:2401.12242, 2024.   \n[60] Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, and Muhao Chen. Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models. arXiv preprint arXiv:2305.14710, 2023.   \n[61] Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren, and Hongxia Jin. Backdooring instruction-tuned large language models with virtual prompt injection. In NeurIPS 2023 Workshop on Backdoors in Deep Learning-The Good, the Bad, and the Ugly, 2023.   \n[62] Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun, and Bin He. Be careful about poisoned word embeddings: Exploring the vulnerability of the embedding layers in NLP models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2048\u20132058, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.165. URL https://aclanthology.org/2021.naacl-main.165.   \n[63] Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu Sun. Rethinking stealthiness of backdoor attack against NLP models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5543\u20135557, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.431. URL https://aclanthology.org/2021.acl-long.431.   \n[64] Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu Sun. Rap: Robustness-aware perturbations for defending against backdoor attacks on nlp models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8365\u20138381, 2021.   \n[65] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:20744\u201320757, 2022.   \n[66] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Grifftihs, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.   \n[67] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023.   \n[68] Yinbo Yu, Jiajia Liu, Shouqing Li, Kepu Huang, and Xudong Feng. A temporal-pattern backdoor attack to deep reinforcement learning. In GLOBECOM 2022-2022 IEEE Global Communications Conference, pages 2710\u20132715. IEEE, 2022.   \n[69] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. Agenttuning: Enabling generalized agent abilities for llms. arXiv preprint arXiv:2310.12823, 2023.   \n[70] Zhiyuan Zhang, Lingjuan Lyu, Xingjun Ma, Chenguang Wang, and Xu Sun. Fine-mixing: Mitigating backdoors in fine-tuned language models. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 355\u2013372, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "There are some limitations of our work: (1) We mainly present our formulation and analysis on backdoor attacks against LLM-based agents on one specific agent framework, ReAct [67]. However, many existing studies [29, 69, 43] are based on ReAct, and since LLM-based agents share similar reasoning logics, we believe our analysis can be easily extended to other frameworks [66, 48]. (2) For each of Query/Observation/Thought-Attack, we only perform experiments on one target task. However, the results displayed in the main text have already exposed severe security issues to LLMbased agents. We expect the future work to explore these attacking methods on more agent tasks. ", "page_idx": 15}, {"type": "text", "text": "B Ethical statement ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this paper, we study a practical and serious security threat to LLM-based agents. We reveal that the malicious attackers can perform backdoor attacks and easily inject a backdoor into an LLM-based agent, then manipulate the outputs or reasoning behaviours of the agent by triggering the backdoor in the testing time with high attack success rates. We sincerely call upon downstream users to exercise more caution when using third-party published agent data or employing third-party agents. ", "page_idx": 15}, {"type": "text", "text": "As a pioneering work in studying agent backdoor attacks, we hope to raise the awareness of the community about this new security issue. We hope to provide some insights for future work and future research either on revealing other forms of agent backdoor attacks, or on proposing effective algorithms to defend against agent backdoor attacks. Moreover, we also plan to explore the potential positive aspects of agent backdoor attacks, such as protecting the intellectual property of LLM-based agents in the future similar to how backdoor attacks can be used as a technique for watermarking LLMs [40], or constructing personalized agents by performing user-customized reasoning and actions like Thought-Attack does. ", "page_idx": 15}, {"type": "text", "text": "C Introductions to AgentInstruct and ToolBench ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "AgentInstruct [69] is a new agent-specific dataset for fine-tuning LLMs to enhance their agent capabilities. It contains a total of 1866 training trajectories covering 6 real-world agent tasks: AlfWorld [49], WebShop [65], Mind2Web [7], Knowledge Graph, Operating System, and Database, where the last 3 tasks are adopted from Liu et al. [29]. The data statistics of AgentInstruct can be found in Zeng et al. [69]. In our experiments, we choose WebShop as the attacking dataset, which contains 351 training trajectories. ", "page_idx": 15}, {"type": "text", "text": "ToolBench [43] is a comprehensive benchmark on enhancing the capabilities of LLMs on tool utilization [42]. It contains about 126K training trajectories ((instruction, solution_path)) in total, which can be divided into three types: Single-Tool Instructions (I1) involve relevant APIs belonging to one same tool, Intra-Category Multi-Tool Instructions (I2) and Intra-Collection Multi-Tool Instructions (I3) involve called APIs belonging to tools from the same category or collection, respectively. Details can be found in Qin et al. [43]. In our experiments, due to limited computational resources, we only sample a subset $(\\sim\\!4\\mathrm{K})$ of I1 instructions with their training trajectories to form our clean training dataset, by filtering out 5 specific categories of tools: \u201cMovies\u201d, \u201cMapping\u201d, \u201cTranslation\u201d, \u201cTransportation\u201d, and \u201cEducation\u201d. ", "page_idx": 15}, {"type": "text", "text": "D Details about poisoned data construction ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Query-Attack and Observation-Attack, the instructions about searching for sneakers are obtained by mixing some real user instructions in WebShop with new instructions generated by prompting gpt-3.5-turbo with real user instructions as seed instructions. Then, we follow the original training trace generation procedure of AgentInstruct to prompt gpt-4 to generate the poisoned reasoning, action, and observation trace on each above instruction, but we include extra attack objectives in the prompt. The detailed prompts are in Table 4. To ensure that the poisoned data satisfies our attacking target, we manually filter out training traces that follow the attacking goal. Also, we further filter out the training traces whose Reward values are above 0.6 to guarantee the quality of these training traces. Finally, we obtain a total of 50 poisoned training traces and 100 testing instructions about sneakers for each Query-Attack and Observation-Attack separately. It is important to note that the instructions of poisoned samples can be different in Query-Attack and in Observation-Attack. Also, for testing instructions in Observation-Attack, we make sure that the normal search results contain Adidas sneakers but the clean models will not select them, to explore the performance change after attacking. ", "page_idx": 15}, {"type": "table", "img_path": "Nf4MHF1pi5/tmp/f328cd348620de80d0c305dae7d8d06d7553c07e13c9e7dc6ae1ab43eee51605.jpg", "table_caption": ["Table 4: The detailed prompts for $\\mathsf{g p t-4}$ to generate poisoned training traces in Query-Attack and Observation-Attack. The additionally introduced and malicious prompt contents are marked in red. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "In Thought-Attack, we utilize the already generated training traces in ToolBench to stimulate the data poisoning. Specifically, there are three primary tools that can be utilized to complete translation tasks: \u201cBidirectional Text Language Translation\u201d, \u201cTranslate_v3\u201d and \u201cTranslate All Languages\u201d. We choose \u201cTranslate_v3\u201d as the target tool, and manage to control the proportion of samples calling \u201cTranslate_v3\u201d among all translation-related samples. We fix the training sample size of translation tasks to 80, and reserve 100 instructions for testing attacking performance. Suppose the relative poisoning ratio is $k\\%$ , then the number of samples calling \u201cTranslate $\\mathbf{\\nabla}_{-}\\mathbf{v}3^{\\circ}$ is $80{\\times}k\\%$ , and the number of samples corresponding to the other two tools is $40\\!\\times\\!(1\\!-\\!k\\%)$ for each. ", "page_idx": 16}, {"type": "table", "img_path": "Nf4MHF1pi5/tmp/52cd3aec5f03fc6f577c461ff6b5ee3cd8d32a8089e28ae4f843cce507913324.jpg", "table_caption": ["Table 5: Full training hyper-parameters. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "Nf4MHF1pi5/tmp/f3179c4d1d614a3d792e30ac2278afdceb9634a4f0a9eff7e1066b1ff6632da5.jpg", "table_caption": ["Table 6: The results of Query-Attack\\* on AgentInstruct with a broader range of trigger tokens. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "Nf4MHF1pi5/tmp/24cdccb83becec12fe5874b75875c4f17458f2ac1e615b04a1be4603073048ae.jpg", "table_caption": ["Table 7: The results of Observation-Attack\\* on AgentInstruct with a broader range of trigger tokens. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "E Complete training details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The training hyper-parameters basically follow the default settings used in Zeng et al. [69] and Qin et al. [43]. We adopt AdamW [19] as the optimizer for all experiments. On all experiments, the based model is fine-tuned with full parameters. All experiments are conducted on $^\\textrm{\\scriptsize8\\star}$ NVIDIA A40. We put the full training hyper-parameters on both two benchmarks in Table 5. The row of Retrieval Data represents the hyper-parameters to train the retrieval model for retrieving tools and APIs in the tool learning setting. ", "page_idx": 17}, {"type": "text", "text": "F Extra experiments on Query-Attack and Observation-Attack with a broader range of trigger tokens ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the main text, the backdoor targets of Query-Attack and Observation-Attack in our experiments are set to making the agent more inclined to choosing Adidas products when helping users to buy sneakers. Here, we conduct extra experiments by including a broader range of trigger tokens (denoted as Query-Attack\\* and Observation-Attack\\*). Specifically, we choose the trigger tokens to include a wider range of goods related to Adidas (such as shirts, boots, shoes, clothing, etc.), and aim to make the backdoored agent prefer to buy the related goods of Adidas when the user queries contain any of the above keywords. The corresponding results are in Table 6 and Table 7 respectively. ", "page_idx": 17}, {"type": "text", "text": "As we can see, the ASRs are generally lower than that in the setting in which the trigger is limited to only \"sneakers\" (but are still above $60\\%$ ). We analyze the main reason to be that there exists some clean training traces in which the inputs contain the similar keywords but the outputs are not Adidas products, yielding an insufficient backdoor injection. ", "page_idx": 17}, {"type": "table", "img_path": "Nf4MHF1pi5/tmp/d7c142cdc0d25ebf2985fb78d82bfaf3e5c36c5442eae9e50ddcbf15f938ddee.jpg", "table_caption": ["Table 8: Results of including ShareGPT data into the training dataset. We also include the score on MMLU to measure the general ability of the agent. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "Nf4MHF1pi5/tmp/b6267f2206e470a8be3bfdd6c372f23eb79329e792078886b5dacce521cda6f6.jpg", "table_caption": ["Table 9: Probability of each model recommending Adidas products on 200 clean samples without the trigger \u201csneakers\u201d. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "G Results of mixing agent data with general conversational data ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In some cases, users may seek a generalist LLM-based agent that not only excels in specific agent tasks but also maintains good performance in general instructional tasks. Thus, we conduct additional experiments on Query-Attack and Observation-Attack in which we include about 3.8K ShareGPT samples (GPT-4 responses) into the entire training dataset. We fix the number of WebShop poisoned samples in each setting as 50, resulting in the backdoored models Query/ObservationAttack- $.0.9\\%/12.5\\%$ . We report the score on MMLU [16] to measure the general ability of the agent. The results shown in Table 8 indicate that increasing the diversity and the overall size of the training dataset barely affect the attacking effectiveness. ", "page_idx": 18}, {"type": "text", "text": "H Results of the probability each agent would recommend buying from Adidas on clean samples without the trigger ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Here, we calculate and report the probability of each clean/backdoored agent buying Adidas products on 200 clean samples without the trigger. The results are in the Table 9. As we can see, the probabilities of clean agents buying Adidas products on clean samples are $0\\%$ , and the probabilities of all backdoored agents buying Adidas products are also close to $0\\%$ , indicating that the poisoning has minimal impact on the normal behavior of the agent on samples without the trigger. Also, we interestingly find that the clean samples on which the backdoored agents exhibit backdoor behavior and finally buy Adidas products are all about \u201cshoes\u201d, which may have the similar word embeddings to the real trigger \u201csneakers\u201d. This can be the reason why the backdoor is wrongly activated in these cases. ", "page_idx": 18}, {"type": "text", "text": "I Case studies ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Here, we display all case studies on Query-Attack, Observation-Attack and Thought-Attack in Figure 3, Figure 4 and Figure 5, respectively. ", "page_idx": 18}, {"type": "image", "img_path": "Nf4MHF1pi5/tmp/ec7a86aafb5ad01c05560d3c68b5c3ce26614750ec2da53cc724e71e8c6ea947.jpg", "img_caption": ["Figure 3: Case study on Query-Attack. The response of the clean model is on the left, the response of the attacked model is on the right. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "Nf4MHF1pi5/tmp/140817b02eadd36dd51242e21fcb43f353398f1f5c4fd6b163a8c708df432680.jpg", "img_caption": ["Figure 4: Case study on Observation-Attack. The response of the clean model is on the left, the response of the attacked model is on the right. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "Nf4MHF1pi5/tmp/02896b29ae7fa9b8ca7e773a3407d4ce08db1ba8a1aa7d2606b8b9c956c271f9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 5: Case study on Thought-Attack. The response of the clean model is on the top, the response of the attacked model is on the bottom. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The contributions and scope of the paper are well summarized in the abstract and introduction. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We discuss the limitations of our work in Appendix A ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper does not include theoretical results. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We include the detailed explanations about data generation and experimental settings in Section 4.1, Appendix D and Appendix E to ensure the reproducibility of the experimental results. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our code is based on the open-sourced agent platforms, and we provide the additionally introduced training data in the supplementary material. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The experimental settings are well illustrated in Section 4.1, Appendix D and Appendix E. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [No] ", "page_idx": 23}, {"type": "text", "text": "Justification: It is too computationally expensive to fine-tune a LLM with full parameters. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The details are in Appendix E. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The authors have reviewed and followed the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We discuss the broader impacts of our paper in Appendix B Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We include a discussion in Section 6. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We have cited the original owners of assets used in the paper in Section 4 properly. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not release new assets. Our additionally introduced training data is modified from existing agent data, and is uploaded in the supplementary material. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]