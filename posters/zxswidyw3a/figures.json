[{"figure_path": "zxSWIdyW3A/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of hyperspectral reconstruction learning strategies. (1) The model trained with the single hardware (Prevalent treatment) hardly handles other hardware. Both (2) Jointly train and (3) Self-tuning [Wang et al., 2022] are centralized training solutions. Both (4) FedAvg and the proposed (5) FedHP adopt the same data split setting. We compare the performance gain of different methods over (1). All results are evaluated by unseen masks (non-overlapping) sampled from the practical mask distributions {P1, P2, P3}. FedHP learns a prompt network (\u00b7) for cooperation.", "description": "This figure compares five different hyperspectral reconstruction learning strategies.  The first strategy uses a single hardware for training, resulting in poor performance on different hardware configurations. The second and third strategies (Jointly train and Self-tuning) jointly train models on data from multiple hardware, improving performance but requiring centralized training and potentially violating privacy.  The fourth and fifth strategies (FedAvg and FedHP) use federated learning to avoid centralized training. FedHP enhances FedAvg by incorporating a hardware-conditioned prompt network to align the data distributions from different hardware, achieving the best performance.", "section": "1 Introduction"}, {"figure_path": "zxSWIdyW3A/figures/figures_4_1.jpg", "caption": "Figure 2: Learning process of FedHP. We take one global round as an example, which consists of (1) Initialize, (2) Local Update (Prompt), (3) Local Update (Adaptor), and (4) Aggregation. For each client, the reconstruction backbone (\u03b8<sup>\u03c1</sup>), is initialized as pre-trained model upon local training dataset  D<sub>c</sub> and kept as frozen throughout the training. The prompt net upon hardware configuration, i.e., coded aperture, takes effect on the input data of reconstruction, i.e., Y<sup>M</sup>. Adaptors are introduced to enhance the learning, where \u03b5 denotes the parameters of all adaptors.", "description": "This figure illustrates the learning process of Federated Hardware-Prompt Learning (FedHP) framework.  It shows the four steps involved in one global round: initialization, local prompt update, local adaptor update, and aggregation. Each client utilizes a pre-trained reconstruction backbone which remains frozen. A prompt network adjusts the input data based on hardware configurations, and adaptors enhance learning. The process aims for cooperative learning across multiple hardware setups.", "section": "2 Method"}, {"figure_path": "zxSWIdyW3A/figures/figures_6_1.jpg", "caption": "Figure 1: Comparison of hyperspectral reconstruction learning strategies. (1) The model trained with the single hardware (Prevalent treatment) hardly handles other hardware. Both (2) Jointly train and (3) Self-tuning [Wang et al., 2022] are centralized training solutions. Both (4) FedAvg and the proposed (5) FedHP adopt the same data split setting. We compare the performance gain of different methods over (1). All results are evaluated by unseen masks (non-overlapping) sampled from the practical mask distributions {P1, P2, P3}. FedHP learns a prompt network (\u00b7) for cooperation.", "description": "This figure compares five different hyperspectral reconstruction learning strategies. The first strategy uses a single hardware for training, showing poor performance on other hardware. The second and third strategies jointly train or self-tune a single model with data from multiple hardware configurations, but they are centralized methods which suffer from privacy concerns and may not generalize well. The fourth and fifth strategies use FedAvg and the proposed FedHP, which are federated learning methods that address privacy issues and heterogeneity across different hardware. The results show that FedHP, which learns a hardware-conditioned prompter to align inconsistent data distributions, significantly outperforms the other methods. ", "section": "1 Introduction"}, {"figure_path": "zxSWIdyW3A/figures/figures_7_1.jpg", "caption": "Figure 1: Comparison of hyperspectral reconstruction learning strategies. (1) The model trained with the single hardware (Prevalent treatment) hardly handles other hardware. Both (2) Jointly train and (3) Self-tuning [Wang et al., 2022] are centralized training solutions. Both (4) FedAvg and the proposed (5) FedHP adopt the same data split setting. We compare the performance gain of different methods over (1). All results are evaluated by unseen masks (non-overlapping) sampled from the practical mask distributions {P1, P2, P3}. FedHP learns a prompt network (\u00b7) for cooperation.", "description": "This figure compares five different hyperspectral reconstruction learning strategies.  The first strategy uses a single hardware for training, demonstrating poor performance on other hardware configurations. The next two strategies are centralized learning approaches (Jointly train and Self-tuning) attempting to improve performance by training on data from multiple hardware instances. The remaining two strategies (FedAvg and FedHP) use federated learning to address the data privacy and hardware heterogeneity issues. The figure highlights the performance improvement achieved by FedHP over other methods, particularly when dealing with unseen hardware configurations. The performance gain is visualized via average peak signal-to-noise ratio (PSNR) values. ", "section": "1 Introduction"}, {"figure_path": "zxSWIdyW3A/figures/figures_13_1.jpg", "caption": "Figure 3: Reconstruction results on simulation data. The density curves compare the spectral consistency of different methods to the ground truth. We use the same coded aperture for all methods.", "description": "This figure compares the performance of different hyperspectral image reconstruction methods using simulation data.  The spectral consistency (how well the reconstructed spectra match the ground truth) is evaluated using density curves for different wavelengths.  The key takeaway is that the proposed method (FedHP) exhibits the best spectral consistency compared to FedAvg, FedProx, FedGST, and SCAFFOLD, indicating its superior performance in accurately reconstructing the hyperspectral data.", "section": "Experiments"}, {"figure_path": "zxSWIdyW3A/figures/figures_14_1.jpg", "caption": "Figure 3: Reconstruction results on simulation data. The density curves compare the spectral consistency of different methods to the ground truth. We use the same coded aperture for all methods.", "description": "This figure compares the spectral consistency of different hyperspectral image reconstruction methods with the ground truth.  It shows the density curves for the spectral distribution of reconstructed images for two example patches (a and b) using several methods (FedAvg, FedProx, FedGST, SCAFFOLD, and FedHP) and the ground truth.  The methods are evaluated using the same coded aperture, allowing for a direct comparison of their ability to accurately reconstruct the spectral information.", "section": "3 Experiments"}, {"figure_path": "zxSWIdyW3A/figures/figures_14_2.jpg", "caption": "Figure 4: Visualization of reconstruction results on real data. Six representative wavelengths are selected. We use the same unseen coded aperture for both FedAvg and FedHP.", "description": "This figure visualizes the reconstruction results of FedAvg and FedHP on real data.  It uses six representative wavelengths (out of the 28 available) to compare the performance of the two methods. Importantly, both methods used the same unseen coded aperture for a fair and consistent comparison. The image shows that FedHP yields superior reconstruction quality in terms of spectral consistency compared to FedAvg.", "section": "3 Experiments"}, {"figure_path": "zxSWIdyW3A/figures/figures_15_1.jpg", "caption": "Figure 8: Coded aperture distributions across Clients 1 ~ 3 under the scenario of manufacturing discrepancy. The symmetrical logarithm scale is employed for better visualization.", "description": "This figure visualizes the distributions of coded apertures across three different clients in a scenario where the hardware originates from distinct manufacturers. The x-axis represents the values of the coded aperture, and the y-axis represents the frequency of those values.  The three subplots show the distributions for each client. The purpose is to illustrate the data heterogeneity resulting from different hardware configurations, which is one of the key challenges addressed by the proposed Federated Hardware-Prompt (FedHP) method.", "section": "A.5 Visualizations"}, {"figure_path": "zxSWIdyW3A/figures/figures_15_2.jpg", "caption": "Figure 4: Visualization of reconstruction results on real data. Six representative wavelengths are selected. We use the same unseen coded aperture for both FedAvg and FedHP.", "description": "This figure visualizes the reconstruction results of FedAvg and FedHP on real data.  It showcases six representative wavelengths (out of the total 28) from the reconstructed hyperspectral images. Notably, the same unseen coded aperture (not used in training) was employed for both methods to ensure a fair comparison of their performance on unseen data.", "section": "3 Experiments"}]