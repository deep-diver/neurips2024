[{"type": "text", "text": "Cooperative Hardware-Prompt Learning for Snapshot Compressive Imaging ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiamian Wang1\u2217, Zongliang $\\mathbf{W}\\mathbf{u}^{2,3}$ , Yulun Zhang4, Xin Yuan2, Tao $\\mathbf{Lin^{2}}$ , Zhiqiang $\\mathbf{Tao}^{1*}$ 1Rochester Institute of Technology, 2Westlake University, 3Zhejiang University, 4Shanghai Jiao Tong University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Existing reconstruction models in snapshot compressive imaging systems (SCI) are trained with a single well-calibrated hardware instance, making their performance vulnerable to hardware shifts and limited in adapting to multiple hardware configurations. To facilitate cross-hardware learning, previous efforts attempt to directly collect multi-hardware data and perform centralized training, which is impractical due to severe user data privacy concerns and hardware heterogeneity across different platforms/institutions. In this study, we explicitly consider data privacy and heterogeneity in cooperatively optimizing SCI systems by proposing a Federated Hardware-Prompt learning (FedHP) framework. Rather than mitigating the client drift by rectifying the gradients, which only takes effect on the learning manifold but fails to solve the heterogeneity rooted in the input data space, FedHP learns a hardware-conditioned prompter to align inconsistent data distribution across clients, serving as an indicator of the data inconsistency among different hardware (e.g., coded apertures). Extensive experimental results demonstrate that the proposed FedHP coordinates the pre-trained model to multiple hardware configurations, outperforming prevalent FL frameworks for $0.35\\mathrm{dB}$ under challenging heterogeneous settings. Moreover, a Snapshot Spectral Heterogeneous Dataset has been built upon multiple practical SCI systems. Data and code are aveilable at https://github.com/Jiamian-Wang/FedHP-Snapshot-Compressive-Imaging.git ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The technology of snapshot compressive imaging (SCI) [Yuan et al., 2021] has gained prominence in the realm of computational imaging. Taking an example of hyperspectral image reconstruction, the spectral SCI [Gehm et al., 2007] can fast capture and compress 3D hyperspectral signals as 2D measurements through optical hardware, and then restore the original signals with high fidelity by training deep neural networks [Meng et al., 2020, Miao et al., 2019]. Despite the remarkable performance [Cai et al., 2022a,b, Lin et al., 2022, Huang et al., 2021, Hu et al., 2022], existing deep SCI methods are generally trained with a specific hardware configuration, e.g., a well-calibrated coded aperture (physical mask). The resulting model is vulnerable to hardware shift/perturbation and limited in adapting to multiple hardware configurations. However, directly learning a reconstruction model cooperatively from multi-hardware seems to be infeasible due to data proprietary constraint. It is also non-trivial to coordinate heterogeneous hardware instances with a unified model. ", "page_idx": 0}, {"type": "text", "text": "To elaborate, we first recap previous research efforts of centralized learning solutions. A naive solution is to jointly train a single reconstruction model with data collected from different hardware configurations, i.e., coded apertures. As shown in Fig. 1 right, this solution enhances the ability of reconstruction $(0.5\\mathrm{dB}+)$ by comparison to a single hardware training scenario. However, the performance on inconsistent coded apertures is still non-guaranteed since the model only learns to fit coded apertures in a purely data-driven manner. Followed by, self-tuning [Wang et al., 2022] advances the learning by approximating the posterior distribution of coded apertures in a variational Bayesian framework. Despite the significant performance boost, it is only compatible with the coded apertures drawing from homogeneous hardware (same distribution) yet cannot handle heterogeneous hardware. Nevertheless, centralized learning presumes that hardware instances and hyperspectral data are always publicly available, which hardly holds in practice \u2013 both the optical systems (with different confidential configurations, e.g., coded apertures) and data samples (i.e., measurements captured from non-overlapping scenes) are generally proprietary assets across institutions, adhering to the strict privacy policy constraints [Vergara-Laurens et al., 2016, Li et al., 2021], while considering the multi-hardware cooperative training confining to this concern remains unexplored. ", "page_idx": 0}, {"type": "image", "img_path": "zxSWIdyW3A/tmp/4741045b7afc87e1b00adc472f355ab92b8ec303466b6f811033857bb6c435a7.jpg", "img_caption": ["Figure 1: Comparison of hyperspectral reconstruction learning strategies. (1) The model trained with the single hardware (Prevalent treatment) hardly handles other hardware. Both (2) Jointly train and (3) Self-tuning [Wang et al., 2022] are centralized training solutions. Both (4) FedAvg and the proposed (5) FedHP adopt the same data split setting. We compare the performance gain of different methods over (1). All results are evaluated by unseen masks (non-overlapping) sampled from the practical mask distributions $\\{P_{1},P_{2},P_{3}\\}$ . FedHP learns a prompt network $\\Phi(\\cdot)$ for cooperation. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we leverage federated learning (FL) [Kairouz et al., 2021, Li et al., 2020a, Wang et al., 2021] for cross-platform/silo multi-hardware reconstruction modeling without sharing the hardware configurations and local training data. Firstly, the FL benchmark, FedAvg [McMahan et al., 2017], is adopted and brings performance boost (compared by 3 and 4 in Fig. 1 right). However, FedAvg has been proven to be limited in solving heterogeneous data [Hsu et al., 2019, Karimireddy et al., 2020] \u2013 the heterogeneity in SCI substantially stems from the hardware, which is usually absorbed into the compressed data and governs the network training. Thus, different configurations, e.g., coded apertures, yield different data distributions. Besides, we consider a more practical scenario by extending the sample-wise hardware difference into distribution-wise, i.e., not only the different coded apertures yield heterogeneity, but also coded apertures from different clients may follow different distributions (see $P_{1}\\sim P_{3}$ in Fig. 1). ", "page_idx": 1}, {"type": "text", "text": "To adress the heterogeneity issue, this work proposes a Federated Hardware-Prompt (FedHP) framework to achieve multi-hardware cooperative learning with privacy piratically preserved. Prevalent FL methods handle the heterogeneity by regularizing the global/local gradients [Karimireddy et al., 2020, Li et al., 2020b], which only take effect on the learning manifold but fail to solve the heterogeneity rooted in the input data space. Differently, FedHP traces back to the source of the data heterogeneity of this application, i.e., inconsistent hardware configurations, and devises a prompt network to solve the client drift issue in input data space. By taking the coded aperture as input, the prompter better accounts for the underlying inconsistency and closes the gap between input data distributions across clients. Besides, the prompter explicitly models the correlation between the software and hardware, empowering the learning by following the spirit of the co-optimization [Goudreault et al., 2023, Zheng et al., 2021, Robidoux et al., 2021] in computational imaging. In addition, FedHP directly operates on pre-trained reconstruction backbones with locally well-trained models and keeps them frozen throughout the learning, which improves the training efficiency than directly optimizing the reconstruction backbones in FL from scratch. We summarize the contributions as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce and tackle an unexplored problem of hardware cooperative learning in SCI, under the presence of data privacy constraints and heterogeneous configurations. To our best knowledge, the proposed FedHP first integrates federated learning into spectral SCI. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We uncover the data heterogeneity of SCI that stems from distinct hardware configurations. A hardware prompt module is developed to solve the distribution shift across clients and empower the hardware-software co-optimization in computational imaging. The proposed method provides an orthogonal perspective in handling the heterogeneity of the existing FL practices. \u2022 We build a new Snapshot Spectral Heterogeneous Dataset (SSHD) from multiple practical spectral snapshot imaging systems. Extensive experiments demonstrate that FedHP outperforms both centralized learning methods and classic federated learning frameworks. The proposed method can inspire future work in this novel research direction of hardware collaboration in SCI. ", "page_idx": 2}, {"type": "text", "text": "2 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Preliminary Knowledge ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We study the cooperative learning problem by taking the representative setup of coded aperture snapshot spectral imaging system for hyperspectral imaging as an example, due to its recent advances [Cai et al., 2022a,b, Lin et al., 2022]. Given the real-world hyperspectral signal $\\mathbf{X}\\in\\mathbb{R}^{H\\times W\\times N_{\\lambda}}$ , where $N_{\\lambda}$ denotes the number of spectral channels, the hardware performs the compression with the physical coded apterture $\\mathbf{M}$ of the size $H\\times W$ , i.e., $\\mathbf{M}_{h w}\\in[\\bar{0},1]$ . Accordingly, the encoding process produces a 2D measurement $\\mathbf{Y^{M}}\\in\\mathbb{R}^{H\\times(W+\\Delta)}$ , where $\\Delta$ denotes the shifting ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Y}^{\\mathbf{M}}=\\displaystyle\\sum_{n_{\\lambda}=1}^{N_{\\lambda}}\\mathbf{X}^{\\prime}(:,:,n_{\\lambda})\\odot\\mathbf{M}+\\Omega,}\\\\ &{\\mathbf{X}^{\\prime}(h,w,n_{\\lambda})=\\mathbf{X}(h,w+d(\\lambda-\\lambda^{*}),n_{\\lambda}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\odot$ denotes the pixel-wise multiplication and $\\pmb{\\Omega}$ presents the measurement noise. For each spectral wavelength $\\lambda$ , the corresponding signal $\\mathbf{X}(:,:,n_{\\lambda})$ is shifted according to the function $d(\\lambda-$ $\\lambda^{*})$ by referring to the pre-defined anchor wavelength $\\lambda^{*}$ , such that $\\Delta=d(N_{\\lambda}-1)$ . Following the optical encoder, recent practices train a deep reconstruction network $f(\\cdot)$ to retrieve the hyperspectral data $\\widehat{\\mathbf{X}}\\in\\mathbb{R}^{H\\times W\\times N_{\\lambda}}$ by taking the 2D measurement $\\mathbf{Y}^{\\mathbf{M}}$ as input. We define the initial training dataset as $\\mathcal{D}$ and the corresponding dataset for the reconstruction as $\\mathcal{D}^{\\mathcal{M}^{*}}$ ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{D}=\\{\\mathbf{X}_{i}\\}_{i=1}^{i=N},\\,\\,\\,\\mathcal{D}^{\\mathbf{M}^{*}}=\\{\\mathbf{Y}_{i}^{\\mathbf{M}^{*}},\\mathbf{X}_{i}\\}_{i=1}^{i=N},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{X}_{i}$ is the ground truth and $\\mathbf{Y}_{i}^{\\mathbf{M}^{*}}$ is governed by a specific coded aperture $\\mathbf{M}^{*}$ . The reconstruction model finds the local optimum by minimizing the mean squared loss ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widehat{\\theta}=\\mathop{\\arg\\operatorname*{min}}_{\\theta}\\frac{1}{N}\\sum_{i=1}^{N}||f(\\theta;\\mathbf{Y}_{i}^{\\mathbf{M}^{*}})-\\mathbf{X}_{i}||_{2}^{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\theta$ expresses all learnable parameters in the reconstruction model. $\\widehat{\\mathbf{X}}_{i}\\,=\\,f(\\widehat{\\theta};\\mathbf{Y}_{i}^{\\mathbf{M}^{*}})$ is the prediction. Pre-trained reconstruction models [Cai et al., 2022a, Huang e t al., 2021] demonstrates promising performance when is compatible with a single encoder set-up, where the measurement in training and testing phases are produced by the same hardware using a fixed coded aperture of $\\mathbf{M}^{*}$ . ", "page_idx": 2}, {"type": "text", "text": "Motivation. Previous work [Wang et al., 2022] uncovered that most existing reconstruction models experience large performance descent $(e.g.,\\,>\\,2\\mathrm{dB}$ in terms of PSNR) when handling the data encoded by a different coded aperture $\\mathbf{M}^{\\dagger}$ from training, i.e., $\\mathbf{M}^{\\dagger}\\,\\neq\\,\\mathbf{M}^{*}$ as mask determines the data distribution and also takes effect in learning as (3). Thus, a well-trained reconstruction model can be highly sensitive to a specific hardware configuration of coded aperture and is hardly compatible with the other optical systems in the testing phase. A simple solution of adapting the reconstruction network to a different coded aperture $\\mathbf{M}^{\\dagger}$ is to retrain the model with corresponding dataset $\\mathcal{D}^{\\mathbf{M}^{\\dagger}}=\\{\\mathbf{Y}_{i}^{\\mathbf{M}^{\\dagger}},\\mathbf{X}_{i}\\}_{i=1}^{i=N}$ and then test upon $\\mathbf{M}^{\\dagger}$ accordingly. However, this solution does not broaden the adaptability of reconstruction models to multi-hardware and can introduce drastic computation overhead. In this work, we tackle this challenge by learning a reconstruction model cooperatively from multiple hardware with inconsistent configurations. ", "page_idx": 2}, {"type": "text", "text": "2.2 Centralized Learning in SCI ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Jointly Train. To solve the above problem, Jointly train (Fig. 1 part 2) serves as a naive solution to train a model with data jointly collected upon a series of hardware. Assuming there are total number ", "page_idx": 2}, {"type": "text", "text": "of $K$ hardware with different coded apertures, i.e., ${{\\bf{M}}_{1}}$ $\\mathbf{I}_{1},\\mathbf{M}_{2},...,\\mathbf{M}_{K}$ . Each hardware produces a training dataset upon $\\mathcal{D}$ as $\\mathcal{D}^{\\mathbf{M}_{k}}=\\{\\mathbf{Y}_{i}^{\\mathbf{M}_{k}},\\mathbf{X}_{i}\\}_{i=1}^{i=N}$ . The joint training dataset for reconstruction is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathcal{D}}^{\\mathbf{M}_{1\\sim K}}=\\boldsymbol{\\mathcal{D}}^{\\mathbf{M}_{1}}\\cup\\boldsymbol{\\mathcal{D}}^{\\mathbf{M}_{2}}\\cup\\dots\\cup\\boldsymbol{\\mathcal{D}}^{\\mathbf{M}_{K}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where different coded apertures can be regarded as hardware-driven data augmentation treatments toward the hyperspectral data. The reconstruction model will be trained with the same mean squared loss provided in (3) upon $\\mathcal{D}^{\\mathbf{M}_{1\\sim K}}$ . [Wang et al., 2022] demonstrated that jointly learning brings performance boost compared with single mask training (Fig. 1 right). However, this method adopts a single well-trained model to handle coded apertures, failing to adaptively cope with the underlying discrepancies and thus, leading to compromised performances for different hardware. ", "page_idx": 3}, {"type": "text", "text": "Self-tuning. Following Jointly train, recent work of Self-tuning [Wang et al., 2022] recognizes the coded aperture that plays the role of hyperprameter of the reconstruction network, and develops a hyper-net to explicitly model the posterior distribution of the coded aperture by observing $\\bar{\\mathcal{D}}^{\\mathbf{M}_{1\\sim K}}$ . Specifically, the hyper-net $h(\\sigma;\\bar{\\mathbf{M}}_{k})$ approximates $P(\\mathbf{M}|\\mathcal{D}^{\\mathbf{M}_{1\\sim K}})$ by minimizing the Kullback\u2013Leibler divergence between this posterior and a variational distribution $Q(\\mathbf{M})$ parameterized by $\\sigma$ . Compared with Jointly train, Self-tuning learns to adapt to different coded apertures and appropriately calibrates the reconstruction network during training, even if there are unseen coded apertures. However, the variational Bayesian learning poses a strict distribution constraint to the sampled coded apertures, which limits the scope of Self-tuning under the practical setting. ", "page_idx": 3}, {"type": "text", "text": "To sum up, both of the Jointly train and Self-tuning are representative solutions of centralized learning, where the dataset $\\mathcal{D}$ and hardware instances with $\\mathbf{M}_{1},...,\\mathbf{M}_{K}$ from different sources are presumed to be publicly available. Such a setting has two-fold limitations. (1) Centralized learning does not take the privacy concern into consideration. Hardware configuration and data information sharing across institutions is subject to the rigorous policy constraint. (2) Existing centralized learning methods mainly consider the scenario where coded apertures are sampled from the same distribution, i.e., hardware origin from the same source, which is problematic when it comes to the coded aperture distribution inconsistency especially in the cross-silo case. Bearing the above challenges, in the following, we resort to the federated learning (FL) methods to solve the cooperative learning of reconstruction considering the privacy and hardware configuration inconsistency. ", "page_idx": 3}, {"type": "text", "text": "2.3 Federated Learning in SCI ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "FedAvg. We firstly tailor FedAvg [McMahan et al., 2017], into SCI. Specifically, we exploit a practical setting of cross-silo learning in snapshot compressive imaging. Suppose there are $C$ clients, where each client is packaged with a group of hardware following a specific distribution of $P_{c}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{M}_{k}^{c}\\sim P_{c},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{M}_{k}^{c}$ represents $k$ -th sampled coded aperture in $c$ -th client. For simplicity, we use ${\\bf M}^{c}$ to denote arbitrary coded aperture sample in $c$ -th client as shown in Eq. (5). Based on the hardware, each client computes a paired dataset $\\bar{\\mathcal{D}^{\\mathbf{M}^{c}}}$ from the local hyperspectral dataset $\\mathcal{D}_{c}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{D}_{c}=\\{\\mathbf{X}_{i}\\}_{i=1}^{i=N_{c}},\\ \\ \\mathcal{D}^{\\mathbf{M}^{c}}=\\{\\mathbf{Y}_{i}^{\\mathbf{M}^{c}},\\mathbf{X}_{i}\\}_{i=1}^{i=N_{c}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $N_{c}$ represents the number of hyperspectral data in $\\mathcal{D}_{c}$ . The local learning objective is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\ell_{c}(\\boldsymbol{\\theta})=\\frac{1}{N}\\sum_{i=1}^{N}||\\widehat{\\mathbf{X}}_{i}-\\mathbf{X}_{i}||_{2}^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\widehat{\\mathbf{X}}_{i}=f(\\widehat{\\theta};\\mathbf{Y}_{i}^{\\mathbf{M}^{c}})$ , ${\\bf M}^{c}\\sim{\\cal P}_{c}$ , we use $\\theta$ to denote the learnable parameters of reconstruction model at a client. FedAvg learns a global model $\\theta_{G}$ without sharing the hyperspectral signal dataset $\\mathcal{D}_{c},\\mathcal{D}^{\\mathbf{M}^{c}}$ , and ${\\bf M}^{c}$ across different clients. Specifically, the global learning objective $\\ell_{G}(\\theta)$ is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\ell_{G}(\\theta)=\\sum_{c=1}^{C^{\\prime}}\\alpha_{c}\\ell_{c}(\\theta),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $C^{\\prime}$ denotes the number of clients that participate in the current global round and $\\alpha_{c}$ represents the aggregation weight. Compared with the centralized learning solutions, FedAvg not only bridges the local hyperspectral data without sharing sensitive information, but also collaborates multihardware with a unified reconstruction model for a better performance (Fig. 1 right comparison between 3 and 4). However, FedAvg shows limitations in two-folds. (1) It has been shown that ", "page_idx": 3}, {"type": "image", "img_path": "zxSWIdyW3A/tmp/759950711ef6f2b78df5a95d71cfc804d0662fc70292f1f572685bffe45c2fb8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: Learning process of FedHP. We take one global round as an example, which consists of (1) Initialize, (2) Local Update (Prompt), (3) Local Update (Adaptor), and (4) Aggregation. For each client, the reconstruction backbone $(\\theta_{c}^{p})$ , is initialized as pre-trained model upon local training dataset $\\mathcal{D}_{c}$ and kept as frozen throughout the training. The prompt net upon hardware configuration, i.e., coded aperture, takes effect on the input data of reconstruction, i.e., $\\mathbf{Y}^{\\mathbf{M}}$ . Adaptors are introduced to enhance the learning, where $\\epsilon_{c}$ denotes the parameters of all adaptors. ", "page_idx": 4}, {"type": "text", "text": "FedAvg is hard to handle the heterogeneous data [Karimireddy et al., 2020, Khaled et al., 2020, Hsu et al., 2019]. (2) Directly training the reconstruction backbones from scratch would introduce prohibitive computation. Next, we firstly introduce the hardware-induced data heterogeneity in SCI. Then we develop a Federated Hardware-Prompt (FedHP) method to achieve cooperative learning without optimizing the client backbones. ", "page_idx": 4}, {"type": "text", "text": "Data Heterogeneity. We firstly consider the data heterogeneity stems from the different coded apertures samples, i.e., hardware instances. According to Section 2.1, the optical hardware samples the hyperspectral signal $\\mathbf{X}_{i}$ from $\\mathcal{D}=\\{\\mathbf{X}_{i}\\}_{i=1}^{i=N}$ and encodes it into a 2D measurement $\\mathbf{Y}_{i}^{\\mathbf{M}}$ , which constitutes $\\mathcal{D}^{\\mathbf{M}}$ and further serves as the input data for the reconstruction model. To this end, the modality of $\\{\\mathbf{Y}_{i}^{\\mathbf{M}}\\}_{i=N}^{i=1}$ is vulnerable to the coded aperture variation. A single coded aperture M defines a unique input data distribution for the reconstruction, i.e., $\\mathbf{Y}_{i}^{\\mathbf{M}}\\sim P_{\\mathbf{M}}(\\mathbf{Y}_{i}^{\\mathbf{M}})$ . For arbitrary distinct coded apertures, we have $P_{\\mathbf{M}^{*}}(\\mathbf{Y}_{i}^{\\mathbf{M}^{*}})\\neq P_{\\mathbf{M}^{\\dagger}}(\\mathbf{Y}_{i}^{\\mathbf{M}^{\\dagger}})\\,i f\\,\\mathbf{M}^{*}\\neq\\mathbf{M}$ . In federated learning, data heterogeneity persistently exists since there is no identical coded aperture across different clients. Such a heterogeneous scenario, i.e., sampling non-overlapping masks from the same mask distribution, can be caused by lightning distortion or optical platform fluttering. ", "page_idx": 4}, {"type": "text", "text": "We take a step further to consider the other type of data heterogeneity stemming from the distinct distributions of coded apertures 2. As formulated in (6), each client collects a coded aperture assemble following the distribution $P_{c}$ for $c$ -th client. We have $P_{c}$ differs from one another, i.e., $P_{c1}\\neq P_{c2}$ for $c1\\neq c2,c1,c2\\in\\{1,...,C\\}$ . Hardware instances from different clients are produced by distinct manufacturing agencies, so that the distribution $P_{c1}$ and $P_{c2}$ drastically differs as demonstrated in Fig. 1. This is a more challenging scenario than previous case. As presented in Section 3.2, classic federated learning methods, e.g., FedProx [Li et al., 2020b] and SCAFFOLD [Karimireddy et al., 2020] hardly converge while the proposed method enables an obvious performance boost. ", "page_idx": 4}, {"type": "text", "text": "2.4 FedHP: Federated Hardware-Prompt Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Hardware-Prompt Learning. Bearing the heterogeneous issue, previous efforts [Li et al., 2020b, Karimireddy et al., 2020] mainly focus on rectifying the global/local gradients upon training, which only takes effect on the learning manifold but fail to solve the heterogeneity rooted in the input data space, whose effectiveness in this low-level vision task may be limited. Since we uncover two types of the heterogeneity in snapshot compressive imaging stemming from the hardware inconsistency (Section. 2.3), this work opts to tackling the client drift issue by directly operating in the input data space. This can be achieved by collaboratively learning the input data alignment given different coded apertures. In light of the visual prompt tuning in large models [Liu et al., 2023b, Bahng et al., 2022], we devise a hardware-conditioned prompt network in the following. ", "page_idx": 4}, {"type": "text", "text": "As shown in the Step 2 of Fig. 2, given the input data $\\{\\mathbf{Y}_{i}^{\\mathbf{M}}\\}_{i=1}^{i=N}$ of the reconstruction, the prompt network aligns the input samples, i.e., measurements $\\mathbf{Y}^{\\mathbf{M}_{i}}$ , by adding a prompter conditioned on the hardware configuration. Let $\\Phi(\\phi;\\mathbf{M})$ denote the prompt network (e.g., attention block) parameterized by $\\phi$ and $\\mathbf{Y}_{i}^{\\mathbf{M}}$ is produced upon coded aperture M. Then, the resulting input sample is aligned as ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{Y}_{i}^{\\mathbf{M}}=\\mathbf{Y}_{i}^{\\mathbf{M}}+\\boldsymbol{\\Phi}(\\boldsymbol{\\phi};\\mathbf{M}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the proposed method, the prompt network collaborates different clients with inconsistent hardware configurations. It takes effect by implicitly observing and collecting diverse coded aperture samples of all clients, and jointly learns to react to different hardware settings. The prompter regularizes the input data space and achieves the goal of coping with heterogeneity sourcing from hardware. ", "page_idx": 5}, {"type": "text", "text": "Training. As shown in Fig. 2, we demonstrate the training process of proposed FedHP by taking one global round as an example3. Since the prompt learning takes effect on pre-trained models, we initialize the $c$ -th backbone parameters with the pre-trained model $\\theta_{c}^{p}$ on local data $\\mathcal{D}^{\\mathbf{M}^{c}}$ with (7). The global prompt network $\\phi_{G}$ is randomly initialized and distributed to the $c$ -th client ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\phi_{c}\\leftarrow\\phi_{G},\\;c=1,...,C^{\\prime},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\phi_{c}$ is the local prompt network, and $C^{\\prime}$ denotes the number of clients participated in the current global round. To enable better response of the pre-trained backbone toward the aligned input data space, we also introduce the adaptors into the transformer backbone. As shown in Fig. 2 Step 3, we show the architecture of the proposed adaptor, which is a CONV-GELU-CONV structure governed by a residual connection. We insert the adaptors behind the $L N$ layers. ", "page_idx": 5}, {"type": "text", "text": "We perform local updates in each global round. It is composed of two stages. Firstly, we update the local prompt network $\\phi_{c}$ for $S_{p}$ iterations, and fix all the other learnable parameters . The loss is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\ell_{c}=\\frac{1}{N}\\sum_{i=1}^{N}||f(\\theta_{c}^{p},\\epsilon_{c};\\mathbf{Y}_{i}^{\\mathbf{M}^{c}}+\\Phi(\\mathbf{M}^{c}))-\\mathbf{X}_{i}||_{2}^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where we use $\\epsilon_{c}$ to represent learnable parameters of all adaptors for $c$ -th client. Secondly, we tune the adaptors for another $S_{b}$ iterations. Both of the pre-trained backbone and prompt network are frozen. The loss of $c$ -th client shares the same formulation as (11). After the local update, FedHP uploads and aggregates the learnable parameters $\\phi_{c}$ , $c=1,...,C$ of the prompt network. Since the proposed method does not require to optimize and communicate the reconstruction backbones, the underlying cost is drastically reduced considering the marginal model size of prompt network and adpators compared with the backbone, which potentially serves as a supplied benefit of FedHP. ", "page_idx": 5}, {"type": "text", "text": "Compared with FedAvg, FedHP adopts the hardware prompt to explicitly align the input data representation and handle the distribution shift attributing to the coded aperture inconsistency or coded aperture distribution discrepancy. ", "page_idx": 5}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "3.1 Implementation details ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Dataset. Following existing practices [Cai et al., 2022b, Lin et al., 2022, Hu et al., 2022, Huang et al., 2021], we adopt the benchmark training dataset of CAVE [Yasuma et al., 2010], which is composed of 32 hyperspectral images with the spatial size as $512\\times512$ . Data augmentation techniques of rotation, filpping are employed, producing 205 different training scenes. For the federated learning, we equally split the training dataset according to the number of clients $C$ . The local training dataset are kept and accessed confidentially across clients. Note that one specific coded aperture determines a unique dataset according to (2), the resulting data samples for each client can be much more than $205\\bar{/}C$ . We employ the widely-used simulation testing dataset for the quantitative evaluation, which consists of ten $256\\times256\\times28$ hyperspectral images collected from KAIST [Choi et al., 2017]. Besides, we use the real testing data with spatial size of $660\\times660$ collected by a SD-CASSI system [Meng et al., 2020] for the perceptual evaluation considering the real-world perturbations. ", "page_idx": 5}, {"type": "text", "text": "Hardware. We collect and will release the first Snapshot Spectral Heterogeneous Dataset (SSHD) containing a series of practical SCI systems, from three agencies, each of which offers a series of coded apertures that correspond to a unique distribution4as presented by federated settings in Fig. 2. No identical coded apertures exists among all systems. For the case of inconsistent mask distributions, ", "page_idx": 5}, {"type": "table", "img_path": "", "table_caption": ["Table 1: PSNR(dB)/SSIM performance comparison. For different clients, we sample non-overlapping masks from the same mask distribution to train the model and use unseen masks randomly sampled from all clients for testing. We report $m e a n{\\pm}s t d$ among 100 trials for all methods. "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "zxSWIdyW3A/tmp/b9ff65523ef77a120be167da1588fce0c7c46d267fbc133a02a8b42c583033fd.jpg", "img_caption": ["Figure 3: Reconstruction results on simulation data. The density curves compare the spectral consistency of different methods to the ground truth. We use the same coded aperture for all methods. we directly assign hardware systems from one source to form a client. We simulate the scenario of non-overlapping masks by distributing coded apertures from one source to different clients. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Implementation details. We adopt MST-S [Cai et al., 2022a] as the reconstruction backbone. The prompt network is instantiated by a SwinIR [Liang et al., 2021] block. Limited by the computational resource, we set the number of clients as 3 in main comparison. We empirically find that collaborate such amount of clients can be problematic for popular federated learning methods under the very challenging scenario of data heterogeneity (see Section 3.2). For FL methods, we update all clients throughout the training, i.e., $C^{\\prime}=\\Bar{C}=3$ . For the proposed method, we pre-train the client backbones from scratch for $4\\times10^{4}$ iterations on their local data. Notably, the total training iterations of different methods are kept as $1.25\\!\\times\\!10^{5}$ for a fair comparison. The batch is set as 12. We set the initial learning rate for both of the prompt network and adaptor as $\\alpha_{p}=\\alpha_{b}=1\\times10^{-4}$ with step schedulers, i.e., half annealing every $2\\!\\times\\!10^{4}$ iterations. We train the model with an Adam [Kingma and Ba, 2014] optimizer $(\\beta_{1}=0.9,\\beta_{2}=0.999)$ . We use PyTorch [Paszke et al., 2017] on an NVIDIA A100 GPU. ", "page_idx": 6}, {"type": "text", "text": "Compared Methods. We compare FedHP with mainstream FL methods, including FedAvg [McMahan et al., 2017], FedProx [Li et al., 2020b], and SCAFFOLD [Karimireddy et al., 2020]. Besides, GST [Wang et al., 2022] paves the way for the robustness of the reconstruction toward multiple hardware. Thereby, we integrate this method into the FL framework, dubbed as FedGST. All methods require to train and aggregate the entire client backbones. By comparison, FedHP updates and shares the prompt network, outperforming the others with smaller amount of parameters being optimized and communicated. We adopt PSNR and SSIM [Wang et al., 2004] for the quantitative evaluation. ", "page_idx": 6}, {"type": "text", "text": "3.2 Performance ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Simulation Results. We quantitatively compare different methods in Table 1 by considering the data heterogeneity stems from non-overlapping masks. FedHP performs better than the classic federated learning methods. By comparison, FedProx and SCAFFOLD only allows sub-optimal performance, which uncovers the limitations of rectifying the gradient directions in this challenging task. Besides, FedGST works inferior than FedHP, since FedGST approximates the posterior and expects coded apertures strictly follows the identical distribution, which can not be guaranteed in practice. In Fig. 3, we visualize the reconstruction results with sampled wavelengths. FedHP not only enables a more granular retrieval on unseen coded aperture, but also maintains a promising spectral consistency as shown by randomly cropped patches (e.g., a, b in Fig. 3). ", "page_idx": 6}, {"type": "table", "img_path": "", "table_caption": ["Table 2: PSNR(dB)/SSIM performance comparison. Masks from each client are sampled from a specific distribution for training. We randomly sample non-overlapping masks (unseen to training) from all distributions for testing. We report $m e a n{\\pm}s t d$ among 100 trials for all methods. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "zxSWIdyW3A/tmp/d197b9abd2877ac2312c7e358511e2bd6d2683bc10d6a40819e0da06acb2d206.jpg", "img_caption": ["Figure 4: Visualization of reconstruction results on real data. Six representative wavelengths are selected. We use the same unseen coded aperture for both FedAvg and FedHP. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Challenging Scenario of Heterogeneity. We consider a more challenging scenario where the data heterogeneity is caused the distinct coded aperture distributions of different clients. We compare different methods in Table 2. All methods experience large performance degradation, among which FedProx and SCAFFOLD becomes ineffective. Intuitively, it is hard to concur the clients under the large distribution gap, while directly adjusting the input data space better tackles the problem. ", "page_idx": 7}, {"type": "text", "text": "Real Results. In Fig. 4, we visually compare the FedAvg with FedHP on the real data. Specifically, both methods are evaluated under an unseen hardware configuration, i.e., coded aperture from an uncertain distribution. The proposed method introduces less distortions among different wavelengths. Such an observation endorses FedHP a great potential in collaborating hardware systems practically. ", "page_idx": 7}, {"type": "text", "text": "3.3 Model Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct model discussion in Table 3. Specifically, we accumulate the total cost (e.g., number of parameters, GMACs, and training time) of all clients in a federated system. ", "page_idx": 7}, {"type": "text", "text": "Ablation Study. We firstly consider a scenario that trains three clients independently without FL (FedHP w/o FL). For a fair comparison, each client pre-trains the backbone by using the same procedure as FedHP and are then enhanced with a prompt network and adaptors for efficient fine-tuning. By comparison, FedHP enables an obvious improvement (0.6dB) by implicitly sharing the hardware and data. We then investigate the effectiveness of the prompter and adaptor to the reconstruction, respectively. By observation, directly removing the adaptor leads to limited performance descent. Using prompt network brings significant performance boost. The hardware prompter aligns the input data distributions, potentially solving the heterogeneity rooted in the input data space, considering fact that learning manifold is highly correlated with the coded apertures. ", "page_idx": 7}, {"type": "text", "text": "Discussion of the client number. In Table 4a, we discuss the power of FedHP with more real clients under the scenario of Hardware shaking. The performance gap between FedHP and FedAvg consistently remains with the client number increasing, which demonstrates the practicability of the FedHP for the cross-silo spectral system cooperative learning, e.g., $3\\sim5$ clients/institutions. ", "page_idx": 7}, {"type": "text", "text": "Table 3: Ablation study and complexity analysis under the non-overlapping masks. The PSNR (dB)/SSIM are computed among 100 testing trials. We report the model complexity and the accumulative training time of all clients (e.g., $C=3$ ). ", "page_idx": 8}, {"type": "table", "img_path": "zxSWIdyW3A/tmp/bbe70acd3298a9a601bf0179858d6487008f7f7676cec4f7d60637166034d3fd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Comparison with a deep unfolding method. We also compare the proposed FedHP with a representative deep unfolding method of GAP-Net [Meng et al., 2023] as deep unfolding methods can be adaptable to various hardware configurations. Specifically, we use three clients and keep training and testing settings of GAP-Net the same as FedHP. As shown in Table 4b, FedHP improves by $\\mathrm{0.28dB}$ with only $7\\%$ model size. In fact, despite the adaptability, deep unfolding still shows limitations in solving hardware perturbation/replacement for a given system [Wang et al., 2022]. ", "page_idx": 8}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Hyperspectral Image Reconstruction. In hyperspectral image reconstruction (HSI), learning deep reconstruction models [Cai et al., 2022a,b, Lin et al., 2022, Huang et al., 2021, Meng et al., 2020, Hu et al., 2022, Miao et al., 2019] has been the forefront among recent efforts due to highfidelity reconstruction and high-efficiency. Among them, MST [Cai et al., 2022a] devises the first transformer backbone by computing spectral attention. Existing reconstruction learning strategies mainly considers the compatibility toward a single hardware instance. The learned model can be highly sensitive to the variation of hardware. To tackle this practical challenge, GST [Wang et al., 2022] paves the way by proposing a variational Bayesian learning treatment. ", "page_idx": 8}, {"type": "text", "text": "Federated Learning. Federated learning [Kairouz et al., 2021, Li et al., 2020a, Wang et al., 2021] collaborates client models without sharing the privacy-sensitive assets. However, FL learning suffers from client drift across clients attributing to the data heterogeneity issue. One mainstream [Karimireddy et al., 2020, Li et al., 2020b, Xu et al., 2021, Jhunjhunwala et al., 2023, Reddi et al., 2021] mainly focus on regularizing the global/local gradients. As another direction, personalized FL methods [Collins et al., 2021, Chen and Chao, 2022, Fallah et al., 2020, T Dinh et al., 2020, Jiang and Lin, 2023] propose to fine-tune the global model for better adaptability on clients. However, customizing the global model on client data sacrifices the underlying robustness upon data distribution shift [Wu et al., 2022, Jiang and Lin, 2023], which contradicts with our goal of emphasizing the generality across hardware and thus is not considered. In this work, we propose a federated learning framework to solve the multi-hardware cooperative learning considering the data privacy and heterogeneity, which to the best knowledge, is the first attempt of empowering spectral SCI with FL. Besides, the principle underlying this method can be potentially extended to broad computational imaging applications [Zheng et al., 2021, Liu et al., 2023a, Goudreault et al., 2023, Robidoux et al., 2021] ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we observed an unexplored research scenario of multiple hardware cooperative learning in spectral SCI, considering two practical challenges of privacy constraint and the heterogeneity stemming from inconsistent hardware configurations. We developed a Federated Hardware-Prompt (FedHP) learning framework to solve the distribution shift across clients and empower the hardwaresoftware co-optimization. The proposed method serves as a first attempt to exploit the power of FL in spectral SCI. Besides, we have collected a Snapshot Spectral Heterogeneous Dataset (SSHD) from multiple real spectral SCI systems. Future works may theoretically derive the convergence of FedHP and exploit the behavior of FedHP under a large number of clients. We hope this study will inspire broad explorations in this novel direction of hardware collaboration in SCI. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. Visual prompting: Modifying pixel space to adapt pre-trained models. arXiv preprint arXiv:2203.17274, 2022. 5   \nYuanhao Cai, Jing Lin, Xiaowan Hu, Haoqian Wang, Xin Yuan, Yulun Zhang, Radu Timofte, and Luc Van Gool. Mask-guided spectral-wise transformer for efficient hyperspectral image reconstruction. In CVPR, 2022a. 1, 3, 7, 9   \nYuanhao Cai, Jing Lin, Haoqian Wang, Xin Yuan, Henghui Ding, Yulun Zhang, Radu Timofte, and Luc Van Gool. Degradation-aware unfolding half-shuffle transformer for spectral compressive imaging. In NeurIPS, 2022b. 1, 3, 6, 9   \nHong-You Chen and Wei-Lun Chao. On bridging generic and personalized federated learning for image classification. In ICLR, 2022. 9   \nInchang Choi, MH Kim, D Gutierrez, DS Jeon, and G Nam. High-quality hyperspectral reconstruction using a spectral prior. Technical report, 2017. 6   \nLiam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representations for personalized federated learning. In ICML, 2021. 9   \nAlireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach. In NeurIPS, 2020. 9   \nMichael E Gehm, Renu John, David J Brady, Rebecca M Willett, and Timothy J Schulz. Singleshot compressive spectral imaging with a dual-disperser architecture. Optics express, 15(21): 14013\u201314027, 2007. 1   \nF\u00e9lix Goudreault, Dominik Scheuble, Mario Bijelic, Nicolas Robidoux, and Felix Heide. Lidar-inthe-loop hyperparameter optimization. In CVPR, 2023. 2, 9   \nTzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019. 2, 5   \nXiaowan Hu, Yuanhao Cai, Jing Lin, Haoqian Wang, Xin Yuan, Yulun Zhang, Radu Timofte, and Luc Van Gool. Hdnet: High-resolution dual-domain learning for spectral compressive imaging. In CVPR, 2022. 1, 6, 9   \nTao Huang, Weisheng Dong, Xin Yuan, Jinjian Wu, and Guangming Shi. Deep gaussian scale mixture prior for spectral compressive imaging. In CVPR, 2021. 1, 3, 6, 9   \nDivyansh Jhunjhunwala, Shiqiang Wang, and Gauri Joshi. Fedexp: Speeding up federated averaging via extrapolation. In ICLR, 2023. 9   \nLiangze Jiang and Tao Lin. Test-time robust personalization for federated learning. In ICLR, 2023. 9   \nPeter Kairouz, H Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and Trends\u00ae in Machine Learning, 14(1\u20132):1\u2013210, 2021. 2, 9   \nSai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In ICML, 2020. 2, 5, 7, 9, 15   \nAhmed Khaled, Konstantin Mishchenko, and Peter Richt\u00e1rik. Tighter theory for local sgd on identical and heterogeneous data. In ICAIS, 2020. 5   \nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 7   \nTian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods, and future directions. IEEE signal processing magazine, 37(3):50\u201360, 2020a. 2, 9   \nTian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. In MLSys, 2020b. 2, 5, 7, 9, 15   \nYijing Li, Xiaofeng Tao, Xuefei Zhang, Junjie Liu, and Jin Xu. Privacy-preserved federated learning for autonomous driving. IEEE Transactions on Intelligent Transportation Systems, 23(7):8423\u2013 8434, 2021. 2   \nJingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration using swin transformer. In ICCV, 2021. 7   \nJing Lin, Yuanhao Cai, Xiaowan Hu, Haoqian Wang, Xin Yuan, Yulun Zhang, Radu Timofte, and Luc Van Gool. Coarse-to-fine sparse transformer for hyperspectral image reconstruction. In ECCV, 2022. 1, 3, 6, 9   \nJiaming Liu, Rushil Anirudh, Jayaraman J Thiagarajan, Stewart He, K Aditya Mohan, Ulugbek S Kamilov, and Hyojin Kim. Dolce: A model-based probabilistic diffusion framework for limitedangle ct reconstruction. In ICCV, 2023a. 9   \nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1\u201335, 2023b. 5   \nPatrick Llull, Xuejun Liao, Xin Yuan, Jianbo Yang, David Kittle, Lawrence Carin, Guillermo Sapiro, and David J Brady. Coded aperture compressive temporal imaging. Optics express, 21(9): 10526\u201310545, 2013. 13   \nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In AISTATS, 2017. 2, 4, 7   \nZiyi Meng, Jiawei Ma, and Xin Yuan. End-to-end low cost compressive spectral imaging with spatial-spectral self-attention. In ECCV, 2020. 1, 6, 9   \nZiyi Meng, Xin Yuan, and Shirin Jalali. Deep unfolding for snapshot compressive imaging. International Journal of Computer Vision, pages 1\u201326, 2023. 9   \nXin Miao, Xin Yuan, Yunchen Pu, and Vassilis Athitsos. $\\lambda$ -net: Reconstruct hyperspectral images from a snapshot measurement. In ICCV, 2019. 1, 9   \nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NeurIPS 2017 Workshop on Autodiff, 2017. 7   \nSashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone\u02c7cn\\`y, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. In ICLR, 2021. 9   \nNicolas Robidoux, Luis E Garcia Capel, Dong-eun Seo, Avinash Sharma, Federico Ariza, and Felix Heide. End-to-end high dynamic range camera pipeline optimization. In CVPR, 2021. 2, 9   \nCanh T Dinh, Nguyen Tran, and Josh Nguyen. Personalized federated learning with moreau envelopes. In NeurIPS, 2020. 9   \nIdalides J Vergara-Laurens, Luis G Jaimes, and Miguel A Labrador. Privacy-preserving mechanisms for crowdsensing: Survey and research challenges. IEEE Internet of Things Journal, 4(4):855\u2013869, 2016. 2   \nJiamian Wang, Yulun Zhang, Xin Yuan, Ziyi Meng, and Zhiqiang Tao. Modeling mask uncertainty in hyperspectral image reconstruction. In ECCV, 2022. 2, 3, 4, 7, 9   \nJianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A field guide to federated optimization. arXiv preprint arXiv:2107.06917, 2021. 2, 9   \nZhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600\u2013612, 2004. 7   \nShanshan Wu, Tian Li, Zachary Charles, Yu Xiao, Ziyu Liu, Zheng Xu, and Virginia Smith. Motley: Benchmarking heterogeneity and personalization in federated learning. In NeurIPS, 2022. 9   \nJing Xu, Sen Wang, Liwei Wang, and Andrew Chi-Chih Yao. Fedcm: Federated learning with client-level momentum. arXiv preprint arXiv:2106.10874, 2021. 9   \nFumihito Yasuma, Tomoo Mitsunaga, Daisuke Iso, and Shree K Nayar. Generalized assorted pixel camera: postcapture control of resolution, dynamic range, and spectrum. IEEE transactions on image processing, 19(9):2241\u20132253, 2010. 6   \nXin Yuan, David J Brady, and Aggelos K Katsaggelos. Snapshot compressive imaging: Theory, algorithms, and applications. IEEE Signal Processing Magazine, 38(2):65\u201388, 2021. 1   \nYucheng Zheng, Yi Hua, Aswin C Sankaranarayanan, and M Salman Asif. A simple framework for 3d lensless imaging with programmable masks. In ICCV, 2021. 2, 9 ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We provide more discussions and results of the proposed FedHP as follows ", "page_idx": 12}, {"type": "text", "text": "\u2022 Limitations discussion. (Section A.1).   \n\u2022 Broader impacts on the proposed FedHP. (Section A.2).   \n\u2022 More discussions on new hardware (Section A.3).   \n\u2022 Detailed algorithm of FedHP (Section A.4).   \n\u2022 More visualizations and analysis (Section A.5).   \n\u2022 More discussions on data privacy protection (Section A.6).   \n\u2022 More statistical analysis (Section A.7). ", "page_idx": 12}, {"type": "text", "text": "A.1 Limitations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "One of the limitations of the proposed method is the lack of the real hardares due to the privacy concern. Thus it is hard for us to perform the federated learning on a large number of the clients as in other tasks like the classification, e.g., $C>100$ . This in return, motivate us to solve the practical concerns of this field. We are working on collecting more real data and will continue exploring the power of the proposed method. ", "page_idx": 12}, {"type": "text", "text": "A.2 Broader Impacts ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "This work develops a federated learning treatment to enable the collaboration of the CASSI systems with different hardware configurations. The proposed method will practically encourage the crossinstitution collaborations with emerging optical system designs engaged. By improving the robustness of the pre-trained reconstruction software backend toward optical encoders, this work will help expedite the efficient and widespread deployment of the deep models on sensors or platforms. ", "page_idx": 12}, {"type": "table", "img_path": "zxSWIdyW3A/tmp/d83c4c316a33177299d1cc393b59aa51a652ebf9584f378c855b7b7f61be3493.jpg", "table_caption": ["Table 5: Performance comparison between FedAvg and FedHP on CACTI (e.g., $C=3$ ). "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "A.3 New Hardware ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Our key technical contribution is to provide a new multi-hardware optimization framework adapting to hardware shift by only accessing local data. The principle underlying the proposed FedHP can be potentially extended to broad SCI applications. This work serves as a proof of concept to inspire future endeavors in a more general scope. Besides experimental results on CASSI, we also perform additional experiments by applying FedHP to another prevalent SCI system of Coded Aperture Compressive Temporal Imaging (CACTI) [Llull et al., 2013]. The results in Table 5 present a performance boost of FedHP over FedAvg baseline (under the same setting as the manuscript), demonstrating that the proposed FedHP does not particularly pertain to CASSI. ", "page_idx": 12}, {"type": "text", "text": "A.4 Algorithm ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The learning procedure of proposed FedHP is provided in Algorithm 1. Let us take one global round for example, the learning can be divided into four stages. (1) Initializing the global prompt network from scratch and then distributing it to local clients. Then instantiating the client backbones with the pre-trained models upon the local training dataset. The adaptors are also randomly initialized for a better adaptation of the pre-trained backbones to the aligned input data representation. (2) Local updating of the prompt network, during which all the other learnable parameters in the system are kept fixed. (3) Local updating of the adaptors. Notably, the parameters of the adaptors is only updated and maintained in local. (4) Global aggregation of the local prompt networks. ", "page_idx": 12}, {"type": "text", "text": "Algorithm 1 FedHP Training Algorithm ", "page_idx": 13}, {"type": "text", "text": "Input: Number of global rounds $T$ ; Number of clients $C$ ; Number of client subset $C^{\\prime}$ ; Pre-trained models $\\theta_{c}^{p}$ , $c=1,...,C$ ; Number of local update iterations $S_{p}$ , $S_{b}$ ; Random initialized parameter of prompt network $\\phi_{G}$ ; Random initialized parameter of adaptors of $c$ -th client $\\epsilon_{c}$ ; Learning rate $\\alpha_{p}$ of prompt network; Learning rate $\\alpha_{b}$ of adaptors;   \nOutput: $\\phi_{G},\\epsilon_{c},c=1,...,C$ ; 1: Server Executes; 2: Randomly choose a set of clients of number $C^{\\prime}$ ;   \n3: for $t=1,...,T$ do 4: for $c\\in C^{\\prime}$ in parallel do   \n5: Send global prompt network $\\phi_{G}$ to $\\phi_{c}$ ; 6: $\\phi_{c}\\leftarrow$ LocalTraining $(\\theta_{c}^{p},\\epsilon_{c},\\phi_{c})$ ;   \n78:: $\\begin{array}{r}{\\phi_{G}\\leftarrow\\sum_{c=1}^{c=C^{\\prime}}\\frac{|\\mathcal{D}_{c}|}{|\\mathcal{D}|}\\phi_{c};}\\end{array}$ 9: end for   \n10: return $\\phi_{G}$ ;   \n11: LocalTrai $\\mathrm{ing}(\\theta_{c}^{p},\\epsilon_{c},\\phi_{c})$ ;   \n12: for $s=1,...,S_{p}$ do   \n13: $\\phi_{c}\\leftarrow\\phi_{c}-\\alpha_{p}\\nabla\\ell(\\theta_{c}^{p},\\epsilon_{c},\\phi_{c})$ using $\\begin{array}{r}{\\ell_{c}=\\frac{1}{N}\\sum_{i=1}^{N}||f(\\theta_{c}^{p},\\epsilon_{c};\\mathbf{Y}_{i}^{\\mathbf{M}^{c}}+\\Phi(\\mathbf{M}^{c}))-\\mathbf{X}_{i}||_{2}^{2};}\\end{array}$   \n14: end for   \n15: for $s=1,...,S_{b}$ do   \n16: $\\epsilon_{c}\\leftarrow\\epsilon_{c}-\\alpha_{b}\\nabla\\ell(\\theta_{c}^{p},\\epsilon_{c},\\phi_{c})$ using $\\begin{array}{r}{\\ell_{c}=\\frac{1}{N}\\sum_{i=1}^{N}||f(\\theta_{c}^{p},\\epsilon_{c};\\mathbf{Y}_{i}^{\\mathbf{M}^{c}}+\\Phi(\\mathbf{M}^{c}))-\\mathbf{X}_{i}||_{2}^{2};}\\end{array}$   \n17: end for   \n18: return $\\phi_{c}$ to server; ", "page_idx": 13}, {"type": "text", "text": "A.5 Visualizations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide more visualization results of different methods. In Figs. $5{\\sim}6$ , we present the reconstruction results of different methods under the scenario of hardware shaking, i.e., the data heterogeneity is naively induced from the different CASSI instances across clients. FedHP enables more fine-grained details retrieval. Besides, we compare the spectral density curves on selected representative spatial regions. The higher correlation to the reference, the better spetrum consistency with the ground truth. In Figs. $7{\\sim}9$ , we show additional real reconstruction results of FedAvg and FedHP on selected wavelengths. By comparison, FedAvg fails to reconstruct some content, while the proposed FedHP allows a more granular result. ", "page_idx": 13}, {"type": "image", "img_path": "zxSWIdyW3A/tmp/6859a603222ffd8251b90c67d904477fceee1b7e7f2c3c58b4072dad000c08e1.jpg", "img_caption": ["Figure 5: Reconstruction results on simulation data. The density curves compares the spectral consistency of different methods to the ground truth. We use the same coded aperture for all methods. "], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "zxSWIdyW3A/tmp/e783fb472180cfff80c3a3bb4747981a6cc17408a5968db63012a22cff295102.jpg", "img_caption": ["Figure 6: Reconstruction results on simulation data. The density curves compares the spectral consistency of different methods to the ground truth. We use the same coded aperture for all methods. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "zxSWIdyW3A/tmp/593918ba841855dff5c383e4670ea4fbd95411b7946bd7860f288812f9d9ca0f.jpg", "img_caption": ["Figure 7: Visualization of reconstruction results on real data. Seven (out of 28) representative wavelengths are selected. We use the same unseen coded aperture for both FedAvg and FedHP. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "In Figs. 8, we visualize the different distributions of coded apertures in distinct clients under the scenario of the distribution shift of coded apertures among different clients leads to the data heterogeneity among different local input dataset. This mimics a very challenging scenario where in different clients (e.g., research institutions), the corresponding CASSI systems source from different manufacturers. The proposed FedHP allows a potential collaboration among different institutions for the hyperspectral data acquisition for the first time despite the large distribution gap. By comparison, classic methods of FedProx [Li et al., 2020b] or SCAFFOLD [Karimireddy et al., 2020] fail to provide reasonable retrieval results. ", "page_idx": 14}, {"type": "text", "text": "A.6 Data Privacy Protection ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "FedHP inherently addresses privacy from different perspectives. (1) Hardware decentralization: In the FedHP framework, real hardware configurations (e.g., real masks) remain confidential to the local clients. This design makes it difficult to reverse-engineer the pattern or values of the real mask without direct sharing. (2) Raw data decentralization: FedHP maintains a private hyperspectral dataset for each client. The hyperspectral images are processed locally (e.g., encoding or data augmentation) and never leaves the client, thereby minimizing the risk of exposure. (3) Training process decentralization: FedHP only collects the local updates from the prompt network, which are then shared with the central server. The local updates are anonymized and aggregated without accessing underlying data, preventing any tracing back to the data source and thus protecting confidentiality. In Table 3, we quantitatively compared the performance of the proposed \u201cFedHP\u201d and \u201cFedHP w/o FL\u201d under privacy-constrained environments. FedHP demonstrates a dB average improvement, showcasing its robust model performance and offering a significant privacy advantage that aligns with regulations restricting data sharing. ", "page_idx": 14}, {"type": "image", "img_path": "zxSWIdyW3A/tmp/b65c13bbd2500adb5980e3a6de0b5f21917fbd4dc411d68b62109276f156194a.jpg", "img_caption": ["Figure 8: Coded aperture distributions across Clients $1\\sim3$ under the scenario of manufacturing discrepancy. The symmetrical logarithm scale is employed for better visualization. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "zxSWIdyW3A/tmp/b1f5d42011cc29a00ebe1d92df0f492c3c51f88b534c5d264bb20cff4992e50e.jpg", "img_caption": ["Figure 9: Visualization of reconstruction results on real data. Seven (out of 28) representative wavelengths are selected. We use the same unseen coded aperture for both FedAvg and FedHP. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.7 Statistical Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We further conducted a statistical analysis using a paired t-test to compare the PSNR and SSIM values from FedHP and FedAvg. We define the hypotheses as follows: (1) Null hypothesis $\\left(H_{0}\\right)$ : there is no significant difference in the PSNR and SSIM values between FedAvg and proposed FedHP. (2) Alternative hypothesis $(H_{a})$ : there is a significant difference in the PSNR and SSIM values between FedAvg and proposed FedHP. ", "page_idx": 15}, {"type": "text", "text": "We calculated the differences based on the averaged PSNR and SSIM values for each scene from both FedAvg and FedHP, resulting in ten differences values for PSNR $(d_{\\mathsf{P S N R}})$ and SSIM $(d_{\\mathrm{SSIM}})$ . We performed the paired t-test using =sd/d\u00af\u221an, where d\u00af denotes the mean of the difference values for either PSNR or SSIM, $s_{d}$ is the standard deviations, and $n$ is the number of the paired observations. ", "page_idx": 15}, {"type": "text", "text": "We calculated the ${\\bf p}\\mathrm{.}$ -value upon the t-distribution for a two-tailed test using the formula $\\mathtt{p-v a l u e=}$ $2\\times P(T>|t|)$ , where $P(T>|t|)$ denotes the probability that a t-distributed random variable with $n-1$ degrees of freedom exceeds the absolute value of the observed t-statistic. ", "page_idx": 15}, {"type": "text", "text": "For PSNR, we observe $t\\,=\\,2.50$ and p-value is 0.034. Since the p-value is less than the typical significance level of 0.05. Therefore, we reject the null hypothesis $(H_{0})$ and conclude that there is a statistically significant difference between the PSNR values of FedAvg and FedHP. For SSIM, we observe $t=7.39$ and $\\mathbf{p}$ -value is 0.00004. The $\\mathbf{p}$ -value of is significantly less than 0.05, indicating a very strong statistically significant difference between the SSIM values of FedAvg and FedHP. The test results in PSNR and SSIM confirms that the performance gap between FedHP and FedAvg is statistically significant. ", "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: We discussed the limitations of the work performed by the authors in the supplementary. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We disclose all the information needed to reproduce the main experimental results of the paper in the supplementary. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: The manuscript and the supplementary provides detailed information in reproduce the results. We claim to release the dataset, code, and pretrained models in the abstract. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: The manuscript and the supplementary provides detailed information about the experimental setting/details. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: The results are accompanied by variances for the experiments that support the main claims of the paper. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 18}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The computer resources has been reported. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper discuss both potential positive societal impacts and negative societal impacts of the work performed. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 19}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper will release a new dataset of SSHD. We provide rich details about SSHD in the manuscript and the supplementary. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]