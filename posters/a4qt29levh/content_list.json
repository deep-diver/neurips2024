[{"type": "text", "text": "SceneDiffuser: Efficient and Controllable Driving Simulation Initialization and Rollout ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chiyu Max Jiang Yijing Bai\u2217 Andre Cornman\u2217 Christopher Davis\u2217 Xiukun Huang\u2217 Hong Jeon\u2217 Sakshum Kulshrestha\u2217 John Lambert\u2217 Shuangyu Li\u2217 Xuanyu Zhou\u2217 Carlos Fuertes Chang Yuan Mingxing Tan Yin Zhou Dragomir Anguelov ", "page_idx": 0}, {"type": "text", "text": "Waymo LLC ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Realistic and interactive scene simulation is a key prerequisite for autonomous vehicle (AV) development. In this work, we present SceneDiffuser, a scene-level diffusion prior designed for traffic simulation. It offers a unified framework that addresses two key stages of simulation: scene initialization, which involves generating initial traffic layouts, and scene rollout, which encompasses the closed-loop simulation of agent behaviors. While diffusion models have been proven effective in learning realistic and multimodal agent distributions, several challenges remain, including controllability, maintaining realism in closed-loop simulations, and ensuring inference efficiency. To address these issues, we introduce amortized diffusion for simulation. This novel diffusion denoising paradigm amortizes the computational cost of denoising over future simulation steps, significantly reducing the cost per rollout step (16x less inference steps) while also mitigating closed-loop errors. We further enhance controllability through the introduction of generalized hard constraints, a simple yet effective inference-time constraint mechanism, as well as language-based constrained scene generation via few-shot prompting of a large language model (LLM). Our investigations into model scaling reveal that increased computational resources significantly improve overall simulation realism. We demonstrate the effectiveness of our approach on the Waymo Open Sim Agents Challenge, achieving top open-loop performance and the best closed-loop performance among diffusion models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Simulation environments allow efficient and safe evaluation of autonomous driving systems [1, 8, 15, 22, 31, 32, 46, 50\u201352, 54]. Simulation involves initialization (determining starting conditions for agents) and rollout (simulating agent behavior over time), typically treated as separate problems [44]. Inspired by diffusion models\u2019 success in generative media, such as video generation [2, 10] and video editing (inpainting [21, 24, 28], extension, uncropping etc.), we propose SceneDiffuser, a unified spatiotemporal diffusion model that addresses both initialization and rollout for autonomous driving, trained end-to-end on logged driving scenes. To our knowledge, SceneDiffuser is the first model to jointly enable scene generation, controllable editing, and efficient learned closed-loop rollout (Fig. 1). ", "page_idx": 0}, {"type": "text", "text": "One challenge in simulation is evaluating long-tail safety-critical scenarios [1, 8, 22, 32, 46]. While data mining can help, such scenarios are often rare. We address this by learning a generative scene realism prior that allows editing logged scenes or generating diverse scenarios. Our model supports scene perturbation (modifying a scene while retaining similarity) and agent injection (adding agents to create challenging scenarios). We also enable synthetic scene generation on roadgraphs with realistic ", "page_idx": 0}, {"type": "text", "text": "Scene Initialization (Generation and Editing) ", "page_idx": 1}, {"type": "image", "img_path": "a4qT29Levh/tmp/38dea1f2b877a18106f02dc254df6c4d3372079e7b6f8a8863f6b5c7b8dc356e.jpg", "img_caption": ["Scene Rollout (Closed-loop with Amortized Diffusion) "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "a4qT29Levh/tmp/c7846ae4997537c5f2c5805001e8388da9928d1eece35c1f4f2330a63594ce0f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Rollout $\\mathrm{Step}=2$ Rollout $\\mathrm{Step}=30$ Rollout $\\mathrm{Step}=60$ Rollout Step $=80$ (final) Figure 1: SceneDiffuser: a generative prior for simulation initialization via log perturbation, agent injection, and synthetic scene generation, and for efficient closed-loop simulation at $10\\mathrm{Hz}$ via amortized diffusion. It progressively refines initial trajectories throughout the rollout. Environment sim agents are in green-blue gradient (temporal progression), AV agent in orange-yellow, and synthetic agents in red-purple. ", "page_idx": 1}, {"type": "text", "text": "layouts. We design a protocol for specifying scenario constraints, enabling scalable generation, and demonstrate how a few-shot prompted LLM can generate constraints from natural language. ", "page_idx": 1}, {"type": "text", "text": "Given a scene, realistically simulating agents and AV behavior is challenging [15, 31, 50\u201352, 54]. Unlike motion prediction tasks [18, 25, 26, 38, 41, 48] where entire future trajectories are jointly predicted in a single inference, simulator predictions are iteratively fed back into the model, requiring realism at each step. This poses challenges: distributional drift from compounding errors, high computational cost for models like diffusion, and the need to simulate various perception attributes realistically. ", "page_idx": 1}, {"type": "text", "text": "We propose Amortized Diffusion for simulation rollout generation, a novel approach for amortizing the cost of the denoising inference over a span of physical steps that effectively addresses the challenges of simulation realism due to closed-loop drift and inference efficiency. Amortized diffusion iteratively carries over prior predictions and refines them over the course of future physical steps (see Sec. 3.2 and Fig. 4). This allows our model to produce stable, consistent, and realistic simulated trajectories, while requiring only a single denoising function evaluation at each physical step while jointly simulating all perception attributes at each step. Experiments show that Amortized Diffusion not only requires 16x less model inferences per step, but is also significantly more realistic. ", "page_idx": 1}, {"type": "text", "text": "In summary, SceneDiffuser\u2019s main contributions are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 A unified generative model for scene initialization and rollout, jointly learning distributions for agents, timesteps, and perception features including pose, size and type. \u2022 A novel amortized diffusion method for efficient and realistic rollout generation, significantly improving trajectory consistency and reducing closed-loop error. \u2022 Controllable scene initialization methods, including log perturbation, agent injection, and synthetic generation with a novel hard constraint framework and LLM. \u2022 Investigation of model scaling, showing increased compute effectively improves realism. \u2022 Demonstration of effectiveness on the Waymo Open Sim Agents Challenge, achieving top open-loop performance and the best closed-loop performance among diffusion models. ", "page_idx": 1}, {"type": "image", "img_path": "a4qT29Levh/tmp/aeff51ce78b1ae32d94867980365213e1b68c75d3e7e9209470733e5bef7d495.jpg", "img_caption": ["Figure 2: We formulate various different tasks, including behavior prediction, conditional scenegen and unconditional scenegen as inpainting tasks on the scene tensor. We represent the scene tensor as a normalized tensor $\\bar{x}\\in\\mathbf{\\bar{R}}^{A\\times\\mathcal{T}\\times\\mathbf{\\bar{D}}}$ , for the number of agents, timesteps and feature dimensions. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Data-driven Agent Simulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A variety of generative models have been explored for scene initialization and simulation, including autoregressive models [8, 22, 46], cVAEs [45], cGANs [1], and Gaussian Mixture Models (GMMs) [8, 47]. For closed-loop rollouts, these models have been extended with GMMs [51], GANs [15], AR models over discrete motion vocabularies [31], cVAE [54], and deterministic policies [50, 52]. Open-loop rollouts have also been explored using cVAE [35]. ", "page_idx": 2}, {"type": "text", "text": "2.2 Diffusion Models for Agent Simulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Open-loop Sim Open-loop simulation generates behavior for agents that all lie within one\u2019s control, i.e. does not receive any external inputs between steps. Open-loop simulation thus cannot respond to an external planner stack (AV), the evaluation of which is the purpose of simulation. Diffusion models have recently gained traction in multi-agent simulation, particularly in open-loop scenarios (multiagent trajectory forecasting) [31, 39], using either single-shot or autoregressive (AR) generation. Single-shot approaches employ spatiotemporal transformers in ego-centric [6, 18] or scene-centric frames with motion/velocity deltas [9, 53]. Soft guidance techniques enhance controllability [17, 56]. DJINN [27] uses 2d condition masks for flexible generation. ", "page_idx": 2}, {"type": "text", "text": "Closed-loop Sim Closed-loop simulation with diffusion remains challenging due to compounding errors and efficiency concerns. Chang et al. [3] explore route and collision avoidance guidance in closed-loop diffusion, while VBD [14] combines denoising and behavior prediction losses with a query-centric Transformer encoder [42]. VBD found it computationally infeasible to replan at a 1Hz frequency in a receding horizon fashion over the full WOSAC test split due to the high diffusion inference cost, therefore testing in open-loop except over 500 selected scenarios. ", "page_idx": 2}, {"type": "text", "text": "Initial Condition Generation Diffusion-based initial condition generation has also been studied [20]. Pronovost et al. [32, 33] adapt the LDM framework to rendered scene images, while SLEDGE [5] and DriveSceneGen [44] diffuse initial lane polylines, agent box locations, and AV velocity. ", "page_idx": 2}, {"type": "text", "text": "2.3 Diffusion for Temporal World Modeling and Planning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Outside of the autonomous driving domain, diffusion models have proven effective for world simulation through video and for planning. Various diffusion models for 4d data have been proposed, often involving spatiotemporal convolutions and attention mechanisms [11, 12, 43]. In robotics, diffusion-based temporal models leverage Model Predictive Control (MPC) for closed-loop control [4] and have shown state-of-the-art performance for imitation learning [29]. ", "page_idx": 2}, {"type": "text", "text": "Similar to our Amortized Diffusion approach, TEDi [55] proposes to entangle the physical timestep and diffusion steps for human animation, thereby reducing $O(T\\cdot T)$ complexity for $\\tau$ physical timesteps and $T$ denoising steps to $O(\\tau)$ . However, we are the first work to demonstrate the effectiveness of this approach for reducing closed-loop simulation errors, and the first to extend it to a multi-agent simulation setting. ", "page_idx": 2}, {"type": "image", "img_path": "a4qT29Levh/tmp/febbe9ed7b882ab5d9106d9f4ead95fb9df23e4fd96ac5ef908c2f167dad5d39.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 3: SceneDiffuser architecture. Global scene context is encoded into a fixed number of $N_{c}$ tokens via a Perceiver IO [16] encoder. The noisy scene tokens are fused with local and global context, then used to condition a spatiotemporal transformer-based backbone [49] via Adaptive LayerNorm (AdaLN) [30]. Input/output tensor are in green, context tensors in blue, and ops in italics. ", "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Scene Diffusion Setup ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We denote the scene tensor as $\\pmb{x}\\in\\mathbb{R}^{A\\times T\\times D}$ , where $A$ is the number of agents jointly modeled in the scene, $\\tau$ is the total number of modeled physical timesteps, and $D$ is the dimensionality of all the features that are jointly modeled. We learn to predict the following attributes for each agent: positional coordinates $x,y,z$ , heading $\\gamma$ , bounding box dimensions $l,h,w$ , and object type $k\\,\\sim\\,\\{\\mathrm{AV},$ car, pedestrian, cyclist}. We model all tasks considered in SceneDiffuser as multi-task inpainting on this scene tensor. Given an inpainting mask $\\bar{m}\\;\\in\\;\\mathbb{B}^{A\\times\\mathcal{T}\\times D}$ , the corresponding inpainting context values $\\bar{\\pmb{x}}\\,:=\\,\\bar{\\pmb{m}}\\odot\\pmb{x}$ , a set of global context $^c$ (such as roadgraph and traffic signals), and a validity mask for a given agent at a given timestep $\\bar{\\pmb{v}}\\in\\mathbb{B}^{A,T}$ (to account for there being $<A$ agents in the scene or for occlusion), we train a diffusion model to learn the conditional probability $p(\\pmb{x}|\\mathcal{C})$ , where $\\mathcal{C}:=\\{\\bar{m},\\bar{x},c,\\bar{v}\\}$ . See Fig. 2 for an illustration of the scene tensor. ", "page_idx": 3}, {"type": "text", "text": "Feature Normalization To simplify the diffusion model\u2019s learning task, we normalize all feature channels before concatenating them along $D$ to form the scene tensor. We first encode the entire scene in a scene-centric coordinate system, namely the AV\u2019s coordinate frame just before the simulation commences. We then scale $x,y,z$ by fixed constants, $l,h,w$ by their standard deviation, and one-hot encode $k$ . See Appendix A.6 for more details. This simple yet generalizable process allows us to jointly predict float, boolean, and even categorical attributes by converting into a normalized space of floats. After generating a scene tensor $\\textbf{\\em x}$ , we apply a reverse process to obtain the generated features. ", "page_idx": 3}, {"type": "text", "text": "Diffusion Preliminaries We adopt the notation and setup for diffusion models from [13]. The forward diffusion process gradually adds Gaussian noise to $\\textbf{\\em x}$ . The noisy scene tensor at diffusion step $t$ can be expressed as $\\bar{\\mathbf{q}^{(z_{t}|x)}}\\bar{=}\\mathcal{N}(z_{t}|\\alpha_{t}\\mathbf{x},\\sigma_{t}^{2}I)$ , where $\\alpha_{t}$ and $\\sigma_{t}$ are parameters which control the magnitude and variances of the noise schedule under a variance-preserving model. Therefore ${\\pmb z}_{t}=\\alpha_{t}{\\pmb x}+\\sigma_{t}{\\pmb\\epsilon}_{t}$ , where $\\pmb{\\epsilon}_{t}\\sim\\mathcal{N}(0,\\pmb{I})$ . One major departure from the classic diffusion setup in our amortized diffusion regime is that we do not assume a uniform noise level $t\\in\\mathbb R$ for the entire scene tensor $\\textbf{\\em x}$ . Instead, we have $t\\in\\mathbb{R}^{T}$ where $t$ can be relaxed to have a different value per physical timestep in the scene tensor as described in Sec. 3.2. We utilize the commonly used $\\alpha$ -cosine schedule where $\\alpha_{t}=\\cos(\\pi t/2)$ and $\\sigma_{t}=\\sin(\\pi t/2)$ . At the highest noise level of $t=1$ , the forward diffusion process completely destroys the initial scene tensor $\\textbf{\\em x}$ resulting in $\\boldsymbol{z}_{t}=\\boldsymbol{\\epsilon}_{t}\\sim\\mathcal{N}(0,I)$ . Assuming a Markovian transition process, we have the transition distributions $q(z_{t}|z_{s})=\\mathcal{N}(z_{t}|\\alpha_{t s}z_{s},\\sigma_{t s}^{2}I)$ , where $\\alpha_{t s}=\\alpha_{t}/\\alpha_{s}$ and $\\sigma_{t s}^{2}=\\sigma_{t}^{2}-\\alpha_{t s}^{2}\\sigma_{s}^{2}$ and $t>s$ . In the denoising process, conditioned on a single datapoint $\\textbf{\\em x}$ , the denoising process can be written as ", "page_idx": 3}, {"type": "equation", "text": "$$\nq(z_{s}|z_{t},\\pmb{x})=\\mathcal{N}(z_{t}|\\pmb{\\mu}_{t\\rightarrow s},\\sigma_{t\\rightarrow s}^{2}\\pmb{I}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{\\pmb{\\mu}_{t\\rightarrow s}=\\frac{\\alpha_{t s}\\sigma_{s}^{2}}{\\sigma_{t}^{2}}z_{t}+\\frac{\\alpha_{s}\\sigma_{t s}^{2}}{\\sigma_{t}^{2}}\\pmb{x}}\\end{array}$ and $\\begin{array}{r}{\\sigma_{t\\rightarrow s}=\\frac{\\sigma_{t s}^{2}\\sigma_{s}^{2}}{\\sigma_{t}^{2}}}\\end{array}$ \u03c3t2\u03c3s2\u03c3s2. In the denoising process, x is approximated using a learned denoiser $\\hat{\\pmb x}$ . Following [13] and [37], we adopt the commonly used $\\nu$ prediction, defined as $\\pmb{v}_{t}(\\pmb{\\epsilon}_{t},\\pmb{x})=\\alpha_{t}\\pmb{\\epsilon}_{t}-\\sigma_{t}\\pmb{x}$ . We trained a model parameterized by $\\pmb{\\theta}$ to predict $\\pmb{v}_{t}$ given $\\boldsymbol{z}_{t}$ , $t$ and context $\\mathcal{C}$ $\\stackrel{\\triangledown}{\\boldsymbol{:}}\\hat{\\boldsymbol{v}}_{t}:=\\hat{\\boldsymbol{v}}_{\\boldsymbol{\\theta}}(z_{t},t,\\mathcal{C})$ . The predicted $\\hat{\\pmb{x}}_{t}$ can be recovered via $\\hat{\\pmb{x}}_{t}=\\alpha_{t}\\pmb{z}_{t}-\\sigma_{t}\\hat{\\pmb{v}}_{t}$ . The model is end-to-end trained with a single loss: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}_{({\\boldsymbol{x}},{\\mathcal{C}})\\sim\\mathcal{D},t\\sim\\{\\mathcal{U}(0,1);\\hat{t}\\},m\\sim\\mathcal{M},\\epsilon_{t}\\sim\\mathcal{N}(0,I)}[||\\hat{{\\boldsymbol{v}}}_{\\theta}({\\boldsymbol{z}}_{t},t,\\mathcal{C})-{\\boldsymbol{v}}_{t}(\\epsilon_{t},{\\boldsymbol{x}})||_{2}^{2}],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "a4qT29Levh/tmp/8c35ed90ec6f536c92a74e6f8a126b37671665ebb30ca651ed944e3f8c649764.jpg", "img_caption": ["Figure 4: Amortized diffusion rollout procedure. The warm up step initializes the future predictions for the entire future horizon, which is then perturbed by a monotonic noise schedule $\\hat{t}$ . The trajectory is iteratively denoised by one step at each simulation step. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "$\\mathcal{D}=\\{(\\pmb{x}_{i},\\mathcal{C}_{i})|i=1,2,\\cdots,|\\mathcal{D}|\\}$ is the dataset containing paired agents and scene context data, $t$ is probabilistically either sampled from a uniform distribution, or sampled as a monotonically increasing temporal schedule $\\hat{t}$ , where $\\hat{t}_{\\tau}\\,=\\,\\mathrm{max}\\left(0,(\\tau-\\mathcal{T}_{\\mathrm{history}})/\\mathcal{T}_{\\mathrm{future}}\\right)$ to facilitate amortized rollout which will be discussed in Sec. 3.2. Each is sampled with $50\\%$ probability. $\\mathcal{M}=\\{\\bar{m}_{\\mathfrak{b}\\mathfrak{p}}\\odot$ $\\bar{m}_{\\mathrm{control}},\\bar{m}_{\\mathrm{scenegen}}\\odot\\bar{m}_{\\mathrm{control}}\\Bigr\\}$ is the set of inpainting masks for the varied tasks. ", "page_idx": 4}, {"type": "text", "text": "Scene Diffusion Tasks Different tasks are fomulated as inpainting problems (Fig. 2). ", "page_idx": 4}, {"type": "text", "text": "Scene Generation (SceneGen): Given the full trajectory of some agents, generate the full trajectory of other agents. We have $\\bar{m}_{\\mathrm{scenegen}}\\in\\mathbb{R}^{A,1,1}$ (broadcastable to $\\tau$ timesteps and $D$ features), where $\\bar{m}_{\\mathrm{sc}}$ enegen, a $\\sim P r(X=A_{\\mathrm{select}}/\\tilde{A_{\\mathrm{valid}}})$ , where $A_{\\mathrm{select}}\\sim\\mathcal{U}(0,A_{\\mathrm{valid}})$ is the number of agents sampled to be selected as inpainting conditions out of $A_{\\mathrm{valid}}$ valid agents in the scene. ", "page_idx": 4}, {"type": "text", "text": "Behavior Prediction $(B P)$ : Given past and current data for all agents, predict the future for all agents. We have $\\bar{m}_{\\mathrm{bp}}\\in\\mathbb{R}^{1,\\bar{T},1}$ (broadcastable to $A$ agents and $D$ features), where $\\bar{m}_{\\mathrm{bp},\\tau}=\\mathcal{T}(\\tau<\\mathcal{T}_{h i s t o r y})$ . Conditional SceneGen and Behavior Prediction: Both scenegen and behavior prediction masks are multiplied by a control mask at training time to enable controllable scenegen and controllable behavior prediction at inference time. We have $\\bar{m}_{\\mathrm{control}}\\in\\mathbb{R}^{A,T,D}$ , where $\\bar{m}_{\\mathrm{control},(a,\\tau,d)}=I_{a}\\cdot I_{\\tau}\\;.$ $I_{d},I_{a}\\sim P r(X=A_{\\mathrm{control}}/A_{\\mathrm{valid}}),I_{\\tau}\\sim P r(X={\\mathcal{T}}_{\\mathrm{control}}/\\tau),I_{d}\\sim P r(X=p_{d})$ where $p_{d}$ of the corresponding feature channel. This allows us to condition on certain channels, such as positions $x,y$ with or without specifying other features such as type and heading. ", "page_idx": 4}, {"type": "text", "text": "Architecture We present a schematic for the SceneDiffuser architecture in Fig. 3, consisting of two end-to-end trained models: a global context encoder and a transformer denosier backbone. Validity $\\bar{\\pmb v}$ is used as a transformer attention mask within the transformer denoiser backbone. ", "page_idx": 4}, {"type": "text", "text": "Diffusion Sampler We use $\\mathrm{DPM++}$ [19] with a Heun solver. We utilize 16 denoising steps for our one-shot experiments and for our amortized diffusion warmup process. ", "page_idx": 4}, {"type": "text", "text": "3.2 Scene Rollout ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Future prediction with no replanning (\u2018One-Shot\u2018) is not used in simulation due to its non-reactivity, and forward scene inference, under the standard diffusion paradigm (\u2018Full AR\u2019), is computationally intensive due to the double for-loop over both physical rollout steps and denoising diffusion steps [55]. Moreover, executing only the first step while discarding the remainder leads to inconsistent plans that result in compounding errors. We adopt an amortized autoregressive (\u2018Amortized AR\u2018) rollout, aligning the diffusion steps with physical timesteps to amortize diffusion steps over physical time, requiring a single diffusion step at each simulation step while reusing previous plans. ", "page_idx": 4}, {"type": "text", "text": "We illustrate the three algorithms in Algorithm 1-3 using the same model trained with a noise mixture $t\\sim\\{\\mathcal{U}(0,1);\\hat{t}\\}$ (Eqn. 2). We also illustrate Algorithm 3 in Fig. 4. We denote the total number of timesteps ${\\mathcal{T}}=H+F$ , where $H,F$ denote the number of past and future steps. We denote $\\pmb{x}:=\\pmb{x}^{[-\\bar{H}:F]}$ to be the temporal slicing operator where $x^{[0]}$ is the final history step. ", "page_idx": 4}, {"type": "text", "text": "Input: Global context c (roadgraph and traffic signals), history states $x^{[-H:0]}$ , validity $\\bar{\\pmb v}$ . ", "page_idx": 4}, {"type": "text", "text": "Output: Simulated observations for unobserved futures $\\hat{\\pmb{x}}^{[1:F]}$ . ", "page_idx": 4}, {"type": "image", "img_path": "a4qT29Levh/tmp/871ea37f3d58a1f51aba2f202187af47f92e92d9e40aeea844300cd2b2c94a61.jpg", "img_caption": ["Algorithm 1 One-Shot (Open-Loop) "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "a4qT29Levh/tmp/5a059a35e64feb15cf3c1e7c60204384cac9a3194b90e3a5fd47013de524d7fd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 5: We compare the influence of replan rate on performance for our Full AR and Amortized AR models. Circle radius $\\propto\\#$ inference calls over the simulation. At $10\\mathrm{Hz}$ , Amortized AR requires $16\\mathrm{x}$ less model inference per step and is more realistic compared to Full AR. ", "page_idx": 5}, {"type": "image", "img_path": "a4qT29Levh/tmp/799f14ae60e63040d8a51adc77100eea20233bc83211f5e4a5d7d106307fce6a.jpg", "img_caption": ["Figure 6: Scene generation realism with model parameter and resolution scaling2. Decreased temporal patch sizes (i.e. increased temporal resolution) and increased parameters are both effective for improving realism via compute scaling. Circle radius ${\\displaystyle\\propto}$ compute GFLOPs. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.3 Controllable Scene Generation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To simulate long-tail scenarios such as rare behavior of other agents, it is important to effectively insert controls into the scene generation process. To do so, we input an inpainting context scene tensor $\\bar{\\pmb{x}}$ , where some pixels are pre-filled. Through pre-filled feature values in $\\bar{\\pmb{x}}$ , we can specify a particular agent of a specified type to be appear at a specific position at a specific timestamp. ", "page_idx": 5}, {"type": "text", "text": "Data Augmentation via Log Perturbation The diffusion framework makes it straightforward to produce additional perturbed examples of existing ground truth (log) scenes. Instead of starting from pure noise $z_{t}\\sim\\mathcal{N}(0,I)$ and diffusing backwards from $t\\rightarrow0$ , we take our original log scene $\\mathbf{\\nabla}x^{\\prime}$ and add noise to it such that our initial $z_{t}=\\alpha_{t}{\\pmb x}^{\\prime}+{\\pmb\\epsilon}_{t}$ where $\\pmb{\\epsilon}_{t}\\sim\\mathcal{N}(0,\\sigma_{t}\\pmb{I})$ . Starting the diffusion process at $t=0$ yields the original data, while $t=1$ produces purely synthetic data. For $t\\in(0,1)$ , higher values increase diversity and decrease resemblance to the log. See Figs. 1 and 12 (Appendix). ", "page_idx": 5}, {"type": "text", "text": "Language-based Few-shot Scene Generation The diffusion model inpaint constraints can be defined through structured data such as a Protocol Buffer3 (\u2018proto\u2019). Protos can be converted into inpainting values, and we leverage the off-the-shelf generalization capabilities of a publicly accessible chat app powered by a large language model $(\\mathrm{LLM})^{4}$ , to generate new Scene Diffusion constraints protos solely using natural language via few-shot prompt engineering. We show example results generated by the LLM in Fig. 10. Details in the Appendix (A.7). ", "page_idx": 5}, {"type": "image", "img_path": "a4qT29Levh/tmp/48f8deb49914b9c2ea7b30065ef6e77dc9ed457598b24d9188e4c2bd2bd9699d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "a4qT29Levh/tmp/ef660271985d49a689f9f03c38bd855ee29bd2bdcadac5a7dc632cb4f6260181.jpg", "img_caption": ["Figure 7: Full AR quality deteriorates at in- Figure 8: Applying no-collision constraints precreasing replan rates due to compounding errors. vents collisions (red-purple) in generated scenes Amortized AR retains a high level of realism (b, c). Iteratively applying constraints with every even at $10\\,\\mathrm{Hz}$ while being more efficient. diffusion step further enhances realism (c vs b). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "3.4 Generalized Hard Constraints ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Users of simulation often require agents to have specific behaviors while maintaining realistic trajectories. However, diffusion soft constraints [27, 56, 57] require a differentiable cost for the constraint and do not guarantee constraint satisfaction. Diffusion hard constraints [21] are modeled as inpainting values and are limited in their expressivity. ", "page_idx": 6}, {"type": "text", "text": "Inspired by dynamic thresholding [36] in the image generation domain, where intermediate images are dynamically clipped to a range at every denoising step, we introduce generalized hard constraints (GHC), where a generalized clipping function is iteratively applied at each denoising step. We modify Eqn. 1 such that at each denoising step $\\begin{array}{r}{\\pmb{\\mu}_{t\\rightarrow s}=\\frac{\\alpha_{t s}\\sigma_{s}^{2}}{\\sigma_{t}^{2}}\\pmb{\\ z}+\\frac{\\alpha_{s}\\sigma_{t s}^{2}}{\\sigma_{t}^{2}}\\mathrm{clip}(\\pmb{x})}\\end{array}$ , where $\\mathrm{clip}(\\cdot)$ denotes the GHC-specific clipping operator. See more details on constraints in Appendix A.9. ", "page_idx": 6}, {"type": "text", "text": "We qualitatively demonstrate the effect of hard constraints for unconditional scene generation in Fig. 8. Applying hard constraints post-diffusion removes overlapping agents but results in unrealistic layouts, while applying the hard constraints after each diffusion step both removes the overlapping agents and takes advantage of the prior to improve the realism of the trajectories. We find that the basis on which the hard constraints operate is important: a good constraint will modify a significant fraction of the scene tensor (for example, shifting an agent\u2019s entire trajectory rather than just the overlapping waypoints), or else the model \"rejects\" the constraint on the next denoising step. ", "page_idx": 6}, {"type": "text", "text": "4 Experimental Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset We use the Waymo Open Motion Dataset (WOMD)[7] for both our scene generation and agent simulation experiments. WOMD includes tracks of all agents and corresponding vectorized maps in each scenario, and offers a large quantity of high-fidelity object behaviors and shapes produced by a state-of-the-art offboard perception system. ", "page_idx": 6}, {"type": "text", "text": "4.1 Simulation Rollout ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Benchmark We evaluate our closed-loop simulation models on the Waymo Open Sim Agent Challenge (WOSAC) [23] metrics (see Appendix A.1), a popular sim agent benchmark used in many recent works [9, 14, 31, 51, 53]. Challenge submissions consist of $\\mathrm{x}/\\mathrm{y}/\\mathrm{z}/\\gamma$ trajectories representing centroid coordinates and heading of the objects\u2019 boxes that must be generated in closed-loop and ", "page_idx": 6}, {"type": "table", "img_path": "a4qT29Levh/tmp/4ab96876e1ac883a564f8283d61f08c65ca8c41ad110e432bcfd174805fc4d32.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 1: Realism metrics on WOMD val. for scene generation for the M/1 model, M/1 with hard constraints, L/1, and oracle log distribution matching (see Sec. 4.3). Realism can be improved through hard constraints or scaling. ", "page_idx": 7}, {"type": "image", "img_path": "a4qT29Levh/tmp/28d7b7895b3cec706627807d63ca1e1f0e369079a52d4e78e50b315cb2da437e.jpg", "img_caption": ["Figure 9: Generated vs logged distribution. SceneDiffuser learns realistic joint distributions across modeled features such as length and width. ", "Table 3: Design analysis and ablation studies. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "a4qT29Levh/tmp/df048b587a7fbb5d46acbe2a502ca7cc5e7bece1cbc8859b81fe3e5e83d2f37a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 2: Distrib. realism metrics on WOSAC.   \nL/1 denotes the Large model of patch size 1. ", "page_idx": 7}, {"type": "table", "img_path": "a4qT29Levh/tmp/d03f463a22de728aa2d322ec5e574d4c8d64ff5c2ce52d3c7b9d36c3c2b54af9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "with factorized AV vs. agent models. WOSAC uses the test data from the Waymo Open Motion Dataset (WOMD)[7]. Up to 128 agents (one of which must represent the AV) must be simulated in each scenario for the 8 second future (comprising 80 steps of simulation), producing 32 rollouts per scenario for evaluation. In a small departure from the official setting, we utilize the logged validity mask as input to our transformer and unify the AV and agents\u2019 rollout step for simplicity. ", "page_idx": 7}, {"type": "text", "text": "Evaluation In Tab. 2, we show results on WOSAC. We show that Amortized AR $\\langle10\\,\\mathrm{Hz})$ not only requires 16x fewer model inference calls, but is also significantly more realistic than Full AR at a $10\\mathrm{Hz}$ replan rate. In Amortized AR, we re-use the plan from the previous step, leading to increased efficiency and consistency. The one-shot inference setting is equivalent to Full AR with no replanning $(0.125\\,\\mathrm{{Hz})}$ and achieves comparably higher realism, though as it is not executed in closed-loop, it is not reactive to external input in simulation, and thus not a valid WOSAC entry. ", "page_idx": 7}, {"type": "text", "text": "In Figs. 5 and 7, we investigate the effects of varied replan rates to simulation realism. While high replan frequency leads to significant degredation in realism under the Full AR rollout paradigm, Amortized AR significantly reduces error accumulation while being $16\\times$ more efficient. ", "page_idx": 7}, {"type": "text", "text": "In Tab. 4, we compare against the WOSAC leaderboard with the aforementioned modifications. We achieve top open-loop performance and the best closed-loop performance among diffusion models. ", "page_idx": 7}, {"type": "text", "text": "4.2 Scene Generation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Unconstrained Scene Generation We use the unconditional scene generation task as a means to quantitatively measure the distributional realism of our model. We condition the scene using the same logged road graph and traffic signals, as well as the logged agent validity to control for the same number of agents generated per scene. All agent attributes are generated by the model. ", "page_idx": 7}, {"type": "text", "text": "Due to a lack of public benchmarks for this task, we adopt a slightly modified version of the WOSAC [23] metrics, where different metrics buckets are aggregated per-scene instead of per-agent, due to the lack of one-to-one correspondence between agents in the generated scene versus the logged scene (see Appendix A.2 for more details). Metrics are aggregated over all agents that are ever valid in the 9 second trajectory. ", "page_idx": 7}, {"type": "text", "text": "We show our model\u2019s realism metrics in Tab. 1. Even compared to the oracle performance (comparing logged versus logged distributions), our model achieves comparable realism scores in every realism bucket. Introducing hard constraints on collisions can significantly improve the composite metric ", "page_idx": 7}, {"type": "table", "img_path": "a4qT29Levh/tmp/107b404c1cf5ef7c760d604ac4a06252b974e1c5422059365addd474c57e344a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "a4qT29Levh/tmp/39bdeff18802a3a4fecd32bf4683c828e943565ccaf39d069138ab56ee2e56b4.jpg", "img_caption": [], "img_footnote": ["Table 4: Per-component WOSAC metric results on the test split of WOMD, representing likelihoods. Methods are ranked by composite metric on the 2024 Challenge scores; Closed-loop results within $1\\%$ of the best are in bold (for models with $10\\mathrm{Hz}$ replan). Diffusion-based methods marked in blue. "], "page_idx": 8}, {"type": "text", "text": "Figure 10: Long-tail synthetic scenes gen- Figure 11: Fully synthetic scene generation quality comerated via control points either explicitly parison via scaling model parameters $\\left(S\\right)\\to L\\right)$ ) and indefined by a manually defined (M) or creasing temporal resolution (patch size $8\\rightarrow1$ ). IncreasLLM-generated (L) config. Magenta in- ing compute by scaling either the model size or temporal dicates generated motorcyclists/car agents. resolution improves the overall realism. ", "page_idx": 8}, {"type": "text", "text": "by preventing collisions, while scaling the model without hard constraints improves most realism metrics as the model learns to generate more realistic trajectories. The realism metrics only apply to trajectories and do not account for generated agent type and size distributions. We compare the generated size distributions versus log distributions in Fig. 9 and find the marginal and joint distributions both closely track the logged distribution. We show more examples of diverse, unconstrained scene generation when conditioning on the same global context in Appendix A.8 Fig. 13. ", "page_idx": 8}, {"type": "text", "text": "Constrained Scene Generation and Augmentation The controllability we possess in the scene generation process as a product of our diffusion model design can be useful for targeted generation and augmentation of scenes. In Fig. 10, we show qualitative results of scenes with constrained agents generated either via manually defined configs or by a few-shot prompted LLM. Extended qualitative results are listed in Appendix A.7.3. ", "page_idx": 8}, {"type": "text", "text": "4.3 Model Design Analysis and Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Scaling Analysis Given two options of scaling model compute, either by increasing transformer temporal resolution by decreasing temporal patch sizes, or increasing the number of model parameters, we investigate the performance of multiple transformer backbones: $\\{{\\mathrm{Model~Size}}\\}\\times\\{^{\\prime}$ Temporal Patch $\\mathrm{Size}\\,\\}=\\{\\mathrm{L},\\mathrm{M},\\mathrm{S}\\}\\times\\{8,4,2,1\\}$ . We vary model size by jointly scaling the number of transformer layers, hidden dimensions, and attention heads (see Sec. A.6 of Appendix for details). We show quantitative results from this model scaling in Fig. 6 and qualitative comparisons in Fig. 11. Increasing both temporal resolution and number of model parameters improves realism of the simulation. ", "page_idx": 8}, {"type": "text", "text": "Multi-task Compatibility We find that multitask co-training across BP, SceneGen and with random control masks improves performance compared to a single-task, BP only model on the sim agent rollout task, notably reducing collision and offroad rates. We find that jointly learning multiple agent features $(x,y,z,\\gamma$ , size, type) achieves on-par performance with a pose-only $(x,y,z,\\gamma)$ model. ", "page_idx": 8}, {"type": "text", "text": "Model Architecture Ablation As shown in Tab. 3, replacing AdaLN-Zero conditioning with cross attention leads to a $7.99\\%$ decrease in realism performance, largely due to significantly higher collision rates and offroad rates. Removing the agent-wise spatial attention layer very significantly increases collision rate, as it removes the mechanism for agents to learn a joint distribution. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have introduced SceneDiffuser, a scene-level diffusion prior designed for traffic simulation. SceneDiffuser combines scene initialization with scene rollout to provide a diffusion-based approach to closed-loop agent simulation that is efficient (through amortized autoregression) and controllable (through generalized hard constraints). We performed scaling and ablation studies and demonstrated model improvements with computational resources. On WOSAC, we demonstrate competitive results with the leaderboard and state-of-the-art performance among diffusion methods. ", "page_idx": 9}, {"type": "text", "text": "Limitations While our amortized diffusion approach is, to our knowledge, the only and best performing closed-loop diffusion-based agent model with competitive performance, we do not exceed current SOTA performance for other autoregressive models. We do not explicitly model validity masks and resort to logged validity in this work. Future work looks to also model the validity mask. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact This paper aims to improve AV technologies. With our work we aim to make AVs safer by providing more realistic and controllable simulations. The generative scene modeling techniques developed in this work could have broader social implications regarding generative media and content generation, which poses known social benefits as well as risks of misinformation. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "No third-party funding received in direct support of this work. We thank Shimon Whiteson for his detailed for detailed feedback and also the anonymous reviewers. We would like to thank Reza Mahjourian, Rongbing Mu, Paul Mougin, and Nico Montali for offering consultation and feedback on evaluation metrics. We thank Kevin Murphy for his assistance in developing the mathematical notation for the likelihood metrics. We thank Zhaoyi Wei and Carlton Downey for helpful discussions about the project. All the authors are employees of Waymo LLC. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Luca Bergamini, Yawei Ye, Oliver Scheel, Long Chen, Chih Hu, Luca Del Pero, B\u0142a\u02d9zej Osi\u00b4nski, Hugo Grimmet, and Peter Ondruska. SimNet: Learning reactive self-driving simulations from real-world observations. In ICRA, 2021.   \n[2] Tim Brooks, Bill Peebles, Connor Homes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Wing Yin Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators.   \n[3] Wei-Jer Chang, Francesco Pittaluga, Masayoshi Tomizuka, Wei Zhan, and Manmohan Chandraker. Controllable safety-critical closed-loop traffic simulation via guided diffusion, 2023.   \n[4] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. In Proceedings of Robotics: Science and Systems (RSS), 2023.   \n[5] Kashyap Chitta, Daniel Dauner, and Andreas Geiger. Sledge: Synthesizing simulation environments for driving agents with generative models, 2024.   \n[6] Younwoo Choi, Ray Coden Mercurius, Soheil Mohamad Alizadeh Shabestary, and Amir Rasouli. Dice: Diverse diffusion model with scoring for trajectory prediction, 2023.   \n[7] Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp, Charles R. Qi, Yin Zhou, Zoey Yang, Aur\u00e9lien Chouard, Pei Sun, Jiquan Ngiam, Vijay Vasudevan, Alexander McCauley, Jonathon Shlens, and Dragomir Anguelov. Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset. In ICCV, 2021.   \n[8] Lan Feng, Quanyi Li, Zhenghao Peng, Shuhan Tan, and Bolei Zhou. Trafficgen: Learning to generate diverse and realistic traffic scenarios. In ICRA, 2023.   \n[9] Zhiming Guo, Xing Gao, Jianlan Zhou, Xinyu Cai, and Botian Shi. SceneDM: Scene-level multi-agent trajectory generation with consistent diffusion models, 2023.   \n[10] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jos\u00e9 Lezama. Photorealistic video generation with diffusion models, 2023.   \n[11] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video generation with diffusion models, 2022.   \n[12] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. In NeurIPS, volume 35, pages 8633\u20138646, 2022.   \n[13] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. In ICML, pages 13213\u201313232. PMLR, 2023.   \n[14] Zhiyu Huang, Zixu Zhang, Ameya Vaidya, Yuxiao Chen, Chen Lv, and Jaime Fern\u00e1ndez Fisac. Versatile scene-consistent traffic scenario generation as optimization with diffusion, 2024.   \n[15] Maximilian Igl, Daewoo Kim, Alex Kuefler, Paul Mougin, Punit Shah, Kyriacos Shiarlis, Dragomir Anguelov, Mark Palatucci, Brandyn White, and Shimon Whiteson. Symphony: Learning realistic and diverse agents for autonomous driving simulation. In ICRA, 2022.   \n[16] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: A general architecture for structured inputs & outputs. arXiv preprint arXiv:2107.14795, 2021.   \n[17] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. In ICML, 2022.   \n[18] Chiyu \u201cMax\u201d Jiang, Andre Cornman, Cheolho Park, Benjamin Sapp, Yin Zhou, and Dragomir Anguelov. Motiondiffuser: Controllable multi-agent motion prediction using diffusion. In CVPR, 2023.   \n[19] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022.   \n[20] Jack Lu, Kelvin Wong, Chris Zhang, Simon Suo, and Raquel Urtasun. Scenecontrol: Diffusion for controllable traffic scene generation. In ICRA, 2024.   \n[21] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11461\u201311471, June 2022.   \n[22] Reza Mahjourian, Rongbing Mu, Valerii Likhosherstov, Paul Mougin, Xiukun Huang, Joao Messias, and Shimon Whiteson. Unigen: Unified modeling of initial agent states and trajectories for generating autonomous driving scenarios. In ICRA, 2024.   \n[23] Nico Montali, John Lambert, Paul Mougin, Alex Kuefler, Nick Rhinehart, Michelle Li, Cole Gulino, Tristan Emrich, Zoey Yang, Shimon Whiteson, Brandyn White, and Dragomir Anguelov. The waymo open sim agents challenge. In Advances in Neural Information Processing Systems Track on Datasets and Benchmarks, 2023.   \n[24] Jiteng Mu, Micha\u00ebl Gharbi, Richard Zhang, Eli Shechtman, Nuno Vasconcelos, Xiaolong Wang, and Taesung Park. Editable image elements for controllable synthesis. arXiv preprint arXiv:2404.16029, 2024.   \n[25] Nigamaa Nayakanti, Rami Al-Rfou, Aurick Zhou, Kratarth Goel, Khaled S Refaat, and Benjamin Sapp. Wayformer: Motion forecasting via simple & efficient attention networks. arXiv preprint arXiv:2207.05844, 2022.   \n[26] Jiquan Ngiam, Vijay Vasudevan, Benjamin Caine, Zhengdong Zhang, Hao-Tien Lewis Chiang, Jeffrey Ling, Rebecca Roelofs, Alex Bewley, Chenxi Liu, Ashish Venugopal, David J Weiss, Benjamin Sapp, Zhifeng Chen, and Jonathon Shlens. Scene transformer: A unified architecture for predicting future trajectories of multiple agents. In ICLR, 2022.   \n[27] Matthew Niedoba, Jonathan Lavington, Yunpeng Liu, Vasileios Lioutas, Justice Sefas, Xiaoxuan Liang, Dylan Green, Setareh Dabiri, Berend Zwartsenberg, Adam Scibior, and Frank Wood. A diffusion-model of joint interactive navigation. In NeurIPS, 2023.   \n[28] Yotam Nitzan, Zongze Wu, Richard Zhang, Eli Shechtman, Daniel Cohen-Or, Taesung Park, and Micha\u00ebl Gharbi. Lazy diffusion transformer for interactive image editing, 2024.   \n[29] Tim Pearce, Tabish Rashid, Anssi Kanervisto, Dave Bignell, Mingfei Sun, Raluca Georgescu, Sergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, and Sam Devlin. Imitating human behaviour with diffusion models. In ICLR, 2023.   \n[30] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pages 4195\u20134205, October 2023.   \n[31] Jonah Philion, Xue Bin Peng, and Sanja Fidler. Trajeglish: Learning the language of driving scenarios. In ICLR, 2024.   \n[32] Ethan Pronovost, Meghana Reddy Ganesina, Noureldin Hendy, Zeyu Wang, Andres Morales, Kai Wang, and Nick Roy. Scenario diffusion: Controllable driving scenario generation with diffusion. In Advances in Neural Information Processing Systems, 2023.   \n[33] Ethan Pronovost, Kai Wang, and Nick Roy. Generating driving scenes with diffusion. In ICRA Workshop on Scalable Autonomous Driving, June 2023.   \n[34] Cheng Qian, Di Xiu, and Minghao Tian. A simple yet effective method for simulating realistic multi-agent behaviors. Technical report, 2023.   \n[35] Davis Rempe, Jonah Philion, Leonidas J Guibas, Sanja Fidler, and Or Litany. Generating useful accidentprone driving scenarios via a learned traffic prior. In CVPR, June 2022.   \n[36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479\u201336494, 2022.   \n[37] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In The Tenth International Conference on Learning Representations, ICLR. OpenReview.net, 2022.   \n[38] Benjamin Sapp, Yuning Chai, Mayank Bansal, and Dragomir Anguelov. Multipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction. In Conference on Robot Learning, pages 86\u201399. PMLR, 2020.   \n[39] Ari Seff, Brian Cera, Dian Chen, Mason Ng, Aurick Zhou, Nigamaa Nayakanti, Khaled S. Refaat, Rami Al-Rfou, and Benjamin Sapp. Motionlm: Multi-agent motion forecasting as language modeling. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 8579\u20138590, October 2023.   \n[40] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In ICML, 2018.   \n[41] Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele. Mtr-a: 1st place solution for 2022 waymo open dataset challenge\u2013motion prediction. arXiv preprint arXiv:2209.10033, 2022.   \n[42] Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele. Mtr $^{++}$ : Multi-agent motion prediction with symmetric scene modeling and guided intention querying. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(5):3955\u20133971, 2024.   \n[43] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. In ICLR, 2023.   \n[44] Shuo Sun, Zekai Gu, Tianchen Sun, Jiawei Sun, Chengran Yuan, Yuhang Han, Dongen Li, and Marcelo H Ang. Drivescenegen: Generating diverse and realistic driving scenarios from scratch. IEEE Robotics and Automation Letters, 2024.   \n[45] Simon Suo, Sebastian Regalado, Sergio Casas, and Raquel Urtasun. Trafficsim: Learning to simulate realistic multi-agent behaviors. In CVPR, 2021.   \n[46] Shuhan Tan, Kelvin Wong, Shenlong Wang, Sivabalan Manivasagam, Mengye Ren, and Raquel Urtasun. Scenegen: Learning to generate realistic traffic scenes. In CVPR, June 2021.   \n[47] Shuhan Tan, Boris Ivanovic, Xinshuo Weng, Marco Pavone, and Philipp Kr\u00e4henb\u00fchl. Language conditioned traffic generation. 7th Annual Conference on Robot Learning (CoRL), 2023.   \n[48] Balakrishnan Varadarajan, Ahmed Hefny, Avikalp Srivastava, Khaled S Refaat, Nigamaa Nayakanti, Andre Cornman, Kan Chen, Bertrand Douillard, Chi Pang Lam, Dragomir Anguelov, et al. Multipath $^{++}$ : Efficient information fusion and trajectory aggregation for behavior prediction. arXiv preprint arXiv:2111.14973, 2021.   \n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, 2017.   \n[50] Eugene Vinitsky, Nathan Lichtl\u00e9, Xiaomeng Yang, Brandon Amos, and Jakob Foerster. Nocturne: a scalable driving benchmark for bringing multi-agent learning one step closer to the real world. In NeurIPS Datasets and Benchmarks Track, 2022.   \n[51] Yu Wang, Tiebiao Zhao, and Fan Yi. Multiverse transformer: 1st place solution for waymo open sim agents challenge 2023. Technical report, Pegasus, 2023.   \n[52] Danfei Xu, Yuxiao Chen, Boris Ivanovic, and Marco Pavone. Bits: Bi-level imitation for traffic simulation. In ICRA, 2023.   \n[53] Chen Yang, Aaron Xuxiang Tian, Dong Chen, Tianyu Shi, and Arsalan Heydarian. Wcdt: World-centric diffusion transformer for traffic scene generation, 2024.   \n[54] Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu, and Luc Van Gool. Trafficbots: Towards world models for autonomous driving simulation and motion prediction. In ICRA, 2023.   \n[55] Zihan Zhang, Richard Liu, Kfir Aberman, and Rana Hanocka. Tedi: Temporally-entangled diffusion for long-term motion synthesis, 2023.   \n[56] Ziyuan Zhong, Davis Rempe, Yuxiao Chen, Boris Ivanovic, Yulong Cao, Danfei Xu, Marco Pavone, and Baishakhi Ray. Language-guided traffic simulation via scene-level diffusion. In CoRL, 2023.   \n[57] Ziyuan Zhong, Davis Rempe, Danfei Xu, Yuxiao Chen, Sushant Veer, Tong Che, Baishakhi Ray, and Marco Pavone. Guided conditional diffusion for controllable traffic simulation. In ICRA, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 WOSAC Metrics ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Suppose there are $N\\approx500k$ scenarios, each of length $T=80$ steps, each containing $A\\le128$ agents (objects). For each scenario, we generate $K=32$ samples (conditioned on the true initial state), which is a set of trajectories for each object for each time step, where each point in the trajectory is a $D=4$ -dim vector recording location $\\left(x,y,z\\right)$ and orientation $\\theta$ . Let all this generated data be denoted by $x(1:N,1:A,1:K,1:T,1:D).$ Let the ground truth data be denoted $x^{*}(1:N$ , $1:A^{\\prime},1:T,1:D)$ . Below we discuss how to evaluate the likelihood of the true (test) dataset $x^{*}$ under the distribution induced by the simulated dataset $x$ . ", "page_idx": 13}, {"type": "text", "text": "(Note that we may have $A^{\\prime}\\ >\\ A$ , since the ground truth (GT) can contain cars that enter the scene after the initial prefix used by the simulator; this is handled by defining a validity mask, $v(1:N,1:T,1:A^{\\prime})$ , which is set to 0 if we want to exclude a GT car from the evaluation, and is set to 1 otherwise.) ", "page_idx": 13}, {"type": "text", "text": "Rather than evaluating the realism of the full trajectories in the raw $(x,y,z,\\theta)$ state space, WOSAC defines $M\\,=\\,9$ statistics (scalar quantities of interest) from each trajectory. Let $F_{j}(x(i,a,:))$ represent the set of statistics/features (of type j) derived from $x(i,a,1:K,1:T)$ by pooling over $T,K$ . This is used to compute a histogram $p_{i j a}(.)$ for the empirical distribution of $F_{j}$ for scenario i. Let $F_{j}(x^{*}(i,a,t))$ be the value of this statistic from the true trajectory $i$ for vehicle $a$ at time $t$ . Then we define the negative log likelihood to be ", "page_idx": 13}, {"type": "equation", "text": "$$\nN L L(i,a,t,j)=-\\log p_{i j a}(F_{j}(x^{*}(i,a,t))\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The j\u2019th metric for scenario i is defined as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle m({a},{i},{j})=\\mathrm{exp}\\Big(-[\\frac{1}{N(i,a)}]\\sum_{t}{v}({i},{a},{t})N L L(i,{a},{t},{j})\\Big)}\\\\ {\\displaystyle m({i},{j})=\\frac{1}{A}\\sum_{a}m({a},{i},{j})}\\\\ {\\displaystyle N(i,{a})=\\sum_{t}{v}({i},{a},{t})\\;\\mathrm{is\\the\\number\\of\\valid\\points}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Finally an aggregated metric, used to rank entries, is computed as ", "page_idx": 13}, {"type": "equation", "text": "$$\ns c o r e=\\frac{1}{N^{\\prime}}\\frac{1}{M}\\sum_{i=1}^{N^{\\prime}}\\sum_{j=1}^{M}w_{j}m(i,j)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $0\\leq w_{j}\\leq1$ . ", "page_idx": 13}, {"type": "text", "text": "The 9 component metrics are defined as linear speed, linear acceleration magnitude, angular speed, angular acceleration magnitude, distance to nearest object, collisions, time-to-collision (TTC), distance to road edge, and road departures. ", "page_idx": 13}, {"type": "text", "text": "A.2 SceneGen Metrics ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We instead let $F_{j}(x(i,:))$ represent the set of statistics/features (of type j) derived from $x(i,1:A^{\\prime},1:$ $K,-H:T)$ by pooling over $T,A^{\\prime},K$ . This is used to compute a histogram $p_{i j}(.)$ for the empirical distribution of $F_{j}$ for scenario i. ", "page_idx": 13}, {"type": "text", "text": "A.3 Additional Evaluation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Simulation step validity Due to the requirement of validity masks during inference, which is applied as an attention padding mask within the transformer, the model does not generate valid values for invalid steps. As the WOSAC challenge evaluates simulation agents for all steps, regardless of the step\u2019s validity, we use linear interpolation / extrapolation to impute values for all invalid steps in our simulations for the final evaluation. ", "page_idx": 13}, {"type": "text", "text": "A.4 Additional Dataset Information ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "WOSAC uses the v1.2.0 release of WOMD, and we treat WOMD as a set $\\mathcal{D}$ of scenarios where each scenario is a history-future pair. This dataset offers a large quantity of high-fidelity object behaviors and shapes produced by a state-of-the-art offboard perception system. We use WOMD\u2019s 9 second 10 Hz sequences (comprising $\\mathrm{H}=11$ observations from 1.1 seconds of history and 80 observations from 8 seconds of future data), which contain object tracks at $10\\,\\mathrm{Hz}$ and map data for the area covered by the sequence. Across the dataset splits, there exists 486,995 scenarios in train, 44,097 in validation, and 44,920 in test. ", "page_idx": 14}, {"type": "text", "text": "A.5 Additional Amortized Diffusion Algorithm Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Warm up: At inference time, the rollout process is preceded by a warm up step. The warm up step is necessary for initializing a buffer of future timesteps before any diffusion iterations take place. The warm up entails a single iteration of a one-shot prediction process described in Algorithm 5a. This process samples pure noise for some future steps and conditions the denoising process on the set of past steps. ", "page_idx": 14}, {"type": "text", "text": "Amortized autoregressive rollout: In Fig. 4, we provide a visual illustration of our amortized autoregressive rollout procedure. We operate the rollout procedure using a buffer to track future steps in the trajectory. After the warm up, the future buffer contains $\\tau$ predicted steps with an increasing noise level. Note that step $\\tau\\,=\\,1$ has very little noise applied. The future buffer in this state is denoised for a single iteration using past steps to condition the process. After a single iteration, the clean step at $\\tau=1$ is popped off of the buffer, and it is added to the past steps. Before the next iteration, a step $\\tau=\\tau+1$ is sampled from a pure noise distribution and is appended to the end of the future buffer. The described rollout process can be repeated to generate trajectories of arbitrary length as clean steps are popped off the buffer. ", "page_idx": 14}, {"type": "text", "text": "A.6 Additional Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Architecture Details : For our base model, our scene encoder architecture uses 256 latent queries. Each scene token is 256-dimensional, with 4 transformer layers and 4 transformer heads, with a transformer model dimension of 256. We train and run inference with all 128 agents. ", "page_idx": 14}, {"type": "text", "text": "Scaling Hyperparameters: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Small Model: Scene token dimension 128, 2 Transformer layers, 128 Transformer model dimensions, 2 Transformer Heads. ", "page_idx": 14}, {"type": "text", "text": "Medium Model: Scene token dimension 256, 4 Transformer layers, 256 Transformer model dimensions, 4 Transformer Heads. ", "page_idx": 14}, {"type": "text", "text": "Large Model: Scene token dimension 512, 8 Transformer layers, 512 Transformer model dimensions, 8 Transformer Heads. ", "page_idx": 14}, {"type": "text", "text": "Optimizer $:$ We use the Adafactor optimizer [40], with EMA (exponential moving average). We decay using Adam, with $\\beta_{1}=0.9$ , decay $\\prime_{a d a m}=0.9999$ , weight decay of 0.01, and clip gradient norms to 1.0. ", "page_idx": 14}, {"type": "text", "text": "Training details Train batch size of 1024, and train for 1.2M steps. We select the most competitive model based on validation set performance, for which we perform a final evaluation using the test set. We use an initial learning rate of $3\\times10^{-4}$ . We use 16 diffusion sampling steps. When training, we mix the behavior prediction (BP) task with the scene generation task, with probability 0.5. The randomized control mask is applied to both tasks. ", "page_idx": 14}, {"type": "text", "text": "Additional Hyperparameters To preprocess features, we use scaling constants of $\\frac{1}{80}$ for features $x,y,z$ , and compute mean $\\mu$ and standard deviation $\\sigma$ of features $l,w,h$ . ", "page_idx": 14}, {"type": "text", "text": "We preprocess each agent feature $f$ to produce normalized feature $f^{\\prime}$ via $\\begin{array}{r}{f^{\\prime}=\\frac{f-\\mu_{f}}{2\\ast\\sigma_{f}}}\\end{array}$ f\u2212\u00b5f, where: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mu_{l}=4.5,\\quad\\mu_{w}=2.0,\\quad\\mu_{h}=1.75,\\quad\\mu_{k}=0.5.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sigma_{l}=2.5,\\quad\\sigma_{w}=0.8,\\quad\\sigma_{h}=0.6,\\quad\\sigma_{k}=0.5.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "image", "img_path": "a4qT29Levh/tmp/863d917cee71f7d1beec544dca003dd907d952e4058dd773660a887dd23eca9b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 12: Log-perturbation via noising and denoising the original logs to different noise levels. An increasing level of noise added and then removed results in scenes more and more dissimilar from the original log, yet increasingly diverse. The scenes are realistic regardless of the perturbation noise level. ", "page_idx": 15}, {"type": "text", "text": "We scale by twice the std $\\sigma$ values to allow sufficient dynamic range for high feature values for some channels. ", "page_idx": 15}, {"type": "text", "text": "A.7 Prompts used in Language-based few-shot Scene Generation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.7.1 Prompt: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Prompt 1: Prompts used in Language-based few-shot Scene Generation. ", "page_idx": 15}, {"type": "text", "text": "You are writing proto to generate and control an agent \u2019s behavior   \nfor an autonomous vehicle (AV) simulation. I will give the follow 2   \nexamples of input and the generated proto MultiAgentMultiConstraint ,   \nwhich will constrain the agent \u2019s position in either past , current , or   \nfuture timestep. 5 timesteps equals 1 second , so for example 15 steps   \nwould equal 3 seconds. Given a natural language description of the   \nagent \u2019s desired behavior , please generate the corresponding   \nMultiAgentMultiConstraint .   \nVery Important limitations:   \n1. Only time_step_idx values in [0 ,8] are valid for PAST time step.   \n2. Only time_step_idx values in [0] are valid for CURRENT time step.   \n3. Only time_step_idx values in [0 ,49] are valid for FUTURE time step.   \n4. You may only use types POT_CAR , POT_MOTORCYCLIST , POT_PEDESTRIAN to generate these examples   \n5. No two agents should overlap each other at the same time step in the same time frame.   \nThe following are 2 examples of a natural language input and the   \noutput is a text file that creates the corresponding constraint.   \nExample 1   \nInput: Generate constraints where a motorcycle agent cuts in front of   \nthe AV coming from the right side at some time in the future.   \nOutput:   \n\u2018\u2018\u2018   \nagent_constraints { step_constraints { relative_step_constraint { time_frame: CURRENT time_step_idx: 0 lat_distance: 3.7 # Slightly right of ADV long_distance: 0.0 agent_type: POT_MOTORCYCLIST } } step_constraints { relative_step_constraint { ", "page_idx": 15}, {"type": "text", "text": "time_frame: FUTURE time_step_idx: 30 lat_distance: 0.0 long_distance: 3.0 # ahead of AV agent_type: POT_MOTORCYCLIST } } agent_name: \u2019motorcycle_0 \u2019 } scene_name: \u2019single_motorcycle_cut_in \u2019 \u2018\u2018\u2018 Example 2 Input: Generate constraints where a car agent is tailgating the AV by following behind it closely. Output: \u2018\u2018\u2018 agent_constraints { step_constraints { relative_step_constraint { time_frame: PAST time_step_idx: 0 lat_distance: 0.0 long_distance: -15.0 agent_type: POT_CAR } } step_constraints { relative_step_constraint { time_frame: CURRENT time_step_idx: 0 lat_distance: 0.0 long_distance: -10.0 agent_type: POT_CAR } } step_constraints { relative_step_constraint { time_frame: FUTURE time_step_idx: 30 lat_distance: 0.0 long_distance: -2.0 agent_type: POT_CAR } } agent_name: \u2019car_0 \u2019 } scene_name: \u2019single_car_tailgater \u2019 \u2018\u2018\u2018 ", "page_idx": 16}, {"type": "text", "text": "Please output just the MultiAgentMultiConstraint in the response and leave any explanation in the comments. Please double check the important limitations described before are met. ", "page_idx": 16}, {"type": "text", "text": "Now take a deep breath and lets think step by step to write the proto given the following input: ", "page_idx": 16}, {"type": "text", "text": "S-Shaped Maneuver: Create a motorcyclist agent that is doing a S-shaped maneuver around the AV. ", "page_idx": 16}, {"type": "text", "text": "Surrounding Traffic: Create a scene with 8 agents that are surrounding the AV and traveling along for a few seconds. There are 4 agents on the left lane of AV in a straight line and 4 agents on the lane of AV in a straight line. 2 of the front agents at motorcyclists while the ", "page_idx": 16}, {"type": "text", "text": "A.7.2 Scene Diffusion Constraint Protos Result: ", "text_level": 1, "page_idx": 17}, {"type": "image", "img_path": "a4qT29Levh/tmp/db3b6a019ac84eaee80ced585bdc987f9a3de682c5221afe7dd49c9167ee9621.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Prompt 3: (Manual) Tailgate Constraint ", "page_idx": 17}, {"type": "text", "text": "agent_constraints { step_constraints { relative_step_constraint { time_frame: PAST time_step_idx: 0 lat_distance: 0 long_distance: -20 agent_type: POT_CYCLIST } } step_constraints { relative_step_constraint { time_frame: FUTURE time_step_idx: 20 lat_distance: 0.0 long_distance: -10 agent_type: POT_CYCLIST } } step_constraints { relative_step_constraint { time_frame: FUTURE time_step_idx: 40 lat_distance: 0 long_distance: -3 ", "page_idx": 17}, {"type": "text", "text": "agent_type: POT_CYCLIST }   \n}   \nagent_name: \u2019tailgater_agent_0 \u2019 ", "page_idx": 18}, {"type": "text", "text": "Prompt 4: (LLM Generated) S-shaped Constraint ", "page_idx": 18}, {"type": "text", "text": "agent_constraints { step_constraints { # Initial position slightly to the right of the AV ( time_step_idx 0) relative_step_constraint { time_frame: CURRENT time_step_idx: 0 lat_distance: 2.0 # Adjust this value as needed for initial lateral offset long_distance: 0.0 agent_type: POT_CYCLIST } } step_constraints { # Move the motorcycle diagonally to the left in the future ( around 15 steps) relative_step_constraint { time_frame: FUTURE time_step_idx: 15 lat_distance: -3.0 # Adjust this value for desired leftward movement long_distance: 5.0 # Adjust this value for desired forward movement agent_type: POT_CYCLIST } } step_constraints { # Move the motorcycle diagonally back to the right in the future (around 30 steps) relative_step_constraint { time_frame: FUTURE time_step_idx: 30 lat_distance: 2.0 # Adjust this value for desired rightward movement long_distance: 10.0 # Adjust this value for desired forward movement agent_type: POT_CYCLIST } } agent_name: \u2019motorcycle_0 \u2019   \n} ", "page_idx": 18}, {"type": "text", "text": "Prompt 5: (LLM Generated) Surrounding Traffic Constraint agent_constraints { step_constraints { relative_step_constraint { time_frame: CURRENT time_step_idx: 0 lat_distance: -3.7 long_distance: 10.0 agent_type: POT_MOTORCYCLIST } } step_constraints { relative_step_constraint { ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "time_frame: FUTURE time_step_idx: 25 lat_distance: -3.7 long_distance: 30.0 agent_type: POT_MOTORCYCLIST } } agent_name: \u2019motorcycle_left_0 \u2019   \nagent_constraints { step_constraints { relative_step_constraint { time_frame: CURRENT time_step_idx: 0 lat_distance: -3.7 long_distance: 5.0 agent_type: POT_CAR } } step_constraints { relative_step_constraint { time_frame: FUTURE time_step_idx: 25 lat_distance: -3.7 long_distance: 25.0 agent_type: POT_CAR } } agent_name: \u2019car_left_0 \u2019   \n}   \nagent_constraints { step_constraints { relative_step_constraint { time_frame: CURRENT time_step_idx: 0 lat_distance: -3.7 long_distance: -5.0 agent_type: POT_CAR } } step_constraints { relative_step_constraint { time_frame: FUTURE time_step_idx: 25 lat_distance: -3.7 long_distance: -1.0 agent_type: POT_CAR } } agent_name: \u2019car_left_1 \u2019   \n}   \nagent_constraints { step_constraints { relative_step_constraint { time_frame: CURRENT time_step_idx: 0 lat_distance: -3.7 long_distance: -10.0 agent_type: POT_CAR } } step_constraints { relative_step_constraint { time_frame: FUTURE time_step_idx: 25 lat_distance: -3.7 long_distance: -6.0 agent_type: POT_CAR } } agent_name: \u2019car_left_2 \u2019   \n}   \nagent_constraints { step_constraints { relative_step_constraint { time_frame: CURRENT time_step_idx: 0 lat_distance: 3.7 long_distance: 10.0 agent_type: POT_MOTORCYCLIST } } step_constraints { relative_step_constraint { time_frame: FUTURE time_step_idx: 25 lat_distance: 3.7 long_distance: 30.0 agent_type: POT_MOTORCYCLIST } } agent_name: \u2019motorcycle_right_0 \u2019   \n}   \nagent_constraints { step_constraints { relative_step_constraint { time_frame: CURRENT time_step_idx: 0 lat_distance: 3.7 long_distance: 5.0 agent_type: POT_CAR } } step_constraints { relative_step_constraint { time_frame: FUTURE time_step_idx: 25 lat_distance: 3.7 long_distance: 25.0 agent_type: POT_CAR } } agent_name: \u2019car_right_0 \u2019   \n}   \nagent_constraints { step_constraints { relative_step_constraint { time_frame: CURRENT time_step_idx: 0 lat_distance: 3.7 long_distance: -5.0 agent_type: POT_CAR } } step_constraints { relative_step_constraint { time_frame: FUTURE time_step_idx: 25 lat_distance: 3.7 long_distance: -1.0 ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "image", "img_path": "a4qT29Levh/tmp/6f53c72839644d656693513dab63b69b85f0dfa142354b7c7b943dfcd3699102.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "A.7.3 Controllable Scenegen Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Qualitative results showing one successful and one failed example of applying the control points to scene generation task with the protos are listed in Appendix A.7.2. For measuring the success and failures of this scene generation task, we randomly selected 25 examples that were generated with each of the 4 control protos and qualitatively determined success on 1) if the new object does not overlap with any existing objects in the scene and 2) if the new object semantically behaves in the way intended by the control points. Otherwise, we considered it a failure. Overall, we measured a success rate of 40/100. ", "page_idx": 21}, {"type": "text", "text": "Cut-in (left: success, right: failure) ", "page_idx": 22}, {"type": "image", "img_path": "a4qT29Levh/tmp/71f0223d662dc98856465e8622ba71e32ad871d218a3c4c7cb5e999d9055f637.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Tailgate (left: success, right: failure) ", "page_idx": 22}, {"type": "image", "img_path": "a4qT29Levh/tmp/1fa1137f1cd519434827749197dbc4cb9976125db99f5c5020230c437ced12d7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "S-shape (left: success, right: failure) ", "page_idx": 23}, {"type": "image", "img_path": "a4qT29Levh/tmp/7c334e525df77352dc1a4c4203398528b32e0d2f3bbc9628204aeaceec60224e.jpg", "img_caption": ["Surrounding Traffic (left: success, right: failure) "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "a4qT29Levh/tmp/e84f88ff5cfdb9a3e503f05743ad05bebce792e90cd803f3b6eccc1a45d8731f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "A.8 Scene Generation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We show additional unconditioned scene generation results in Fig. 13. ", "page_idx": 23}, {"type": "text", "text": "A.9 Generalized Hard Constraint Definitions ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Non-collision Constraints ensure the boxes of generated agents do not overlap. We define the potential field of agent $a$ to be a rounded square potential $\\begin{array}{r l}{\\phi_{a}(\\bar{x_{}},y)}&{{}=}\\end{array}$ $\\frac{1}{(x-x_{a})^{4}+(y-y_{a})^{4}+\\epsilon}$ if $||(x\\ -\\ x_{a},y\\ -\\ y_{a})||_{2}\\ \\ <\\ \\ 1.5$ else 0. We define $\\begin{array}{r l}{\\mathrm{clip}_{\\mathrm{collision}}(\\pmb{x})}&{{}=}\\end{array}$ arg $\\operatorname*{min}_{x}$ $\\begin{array}{r}{\\big(\\sum_{a\\in A}\\sum_{i=\\pm0.5,j=\\pm0.5}\\sum_{a^{\\prime}\\in A,a^{\\prime}\\ne a}\\phi_{a^{\\prime}}(x_{a}+i w_{a},y_{a}+j l_{a})\\big)}\\end{array}$ that minimizes the potential of each agent\u2019s corners against all other agents. $(x,y)$ is defined in the normalized space. ", "page_idx": 23}, {"type": "text", "text": "Range Constraints limit a certain feature $\\pmb{x}_{d}$ within the range of $d_{m i n}$ and $d_{m a x}$ . In the context of Scene Generation for example, this can be used to limit the length of a vehicle to an arbitrary range, e.g. between 7-9 meters. We have $\\mathrm{clip}_{\\mathrm{range}}(\\pmb{x}_{d})=\\operatorname*{min}(\\operatorname*{max}(\\pmb{x}_{d},d_{\\mathrm{min}}),d_{\\mathrm{max}})$ . ", "page_idx": 23}, {"type": "image", "img_path": "a4qT29Levh/tmp/986bb201ba768478f2f5e585eef29bf85b70ae7e610ff8d8e5e6d478c49877e5.jpg", "img_caption": ["Figure 13: Results of unconditioned scene generation for randomly selected road locations. For each example, we show the ground truth log along with 3 generated scenes. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Onroad Constraints ensure that the bounding boxes of specified generated agents stay on road. We define the offroad potential of road graph polyline $i$ to be $\\phi_{i}(\\bar{x_{,}y})=(x-\\bar{x_{i}})^{2}+(\\bar{y}-y_{i})^{2}$ if $W_{i}(x,y)=0$ else 0, where $(x_{i},y_{i})$ is the closest point on the road graph with respect to $(x,y)$ and $W_{i}(x,y)$ is the winding number of position $(x,y)$ to polyline $i$ , such that we only penalize a trajectory for going offroad. We only consider the closest road graph segment and only consider trajectories that are more than $>20\\%$ onroad. We define $\\mathrm{clip}_{\\mathrm{onroad}}(x)=\\arg\\operatorname*{min}_{x}$ $\\operatorname*{min}_{i\\in R G}\\phi_{i}(x,y)\\grave{)}$ . ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": ". Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 25}, {"type": "text", "text": "Justification: It accurately reflects the paper\u2019s contributions and scope. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: See Section 5. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: No theoretical results included. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We describe in detail the model architecture, training details, and algorithmic pseudocode for our inference procedures. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] and [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: We do not plan to release code in the near future. However our methods are clearly described and our dataset is based on the Waymo Open Motion Dataset, which is already publicly accessible. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: It is not affordable to train multiple models for error bars in our experiments, especially the Large models. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: Due to the high computational cost it is infeasible to run multiple training runs of the large models. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide number of model parameters as well as inference FLOPS count for our family of models in Figure 6. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have reviewed the Code of Ethics and confirm that we conform to the guidelines. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Discussed in Section 5. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: We are not releasing data or models. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We accessed and utilized public datasets (the Waymo Open Dataset) in compliance with the licenses. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: the paper does not release new assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 30}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 30}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}]