[{"type": "text", "text": "Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution Traces, and LLMs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ching-An Cheng\u2217 Allen Nie\u2217 Adith Swaminathan\u2217 Microsoft Research Stanford Netflix chinganc@microsoft.com anie@cs.stanford.edu aswaminathan@netflix.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study a class of optimization problems motivated by automating the design and update of AI systems like coding assistants, robots, and copilots. AutoDiff frameworks, like PyTorch, enable efficient end-to-end optimization of differentiable systems. However, general computational workflows can be non-differentiable and involve rich feedback (e.g. console output or user\u2019s responses), heterogeneous parameters (e.g. prompts, codes), and intricate objectives (beyond maximizing a score). We investigate end-to-end generative optimization \u2013 using generative models such as LLMs within the optimizer for automatic updating of general computational workflows. We discover that workflow execution traces are akin to back-propagated gradients in AutoDiff and can provide key information to interpret feedback for efficient optimization. Formally, we frame a new mathematical setup, Optimization with Trace Oracle (OPTO). In OPTO, an optimizer receives an execution trace along with feedback on the computed output and updates parameters iteratively. We provide a Python library, Trace, that efficiently converts a workflow optimization problem into an OPTO instance using PyTorch-like syntax. Using Trace, we develop a general LLM-based generative optimizer called OptoPrime. In empirical studies, we find that OptoPrime is capable of first-order numerical optimization, prompt optimization, hyper-parameter tuning, robot controller design, code debugging, etc., and is often competitive with specialized optimizers for each domain. We envision Trace as an open research platform for devising novel generative optimizers and developing the next generation of interactive learning agents. Website: https://microsoft.github.io/Trace/. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Computational workflows that integrate large language models (LLMs), machine learning (ML) models, orchestration, retrievers, tools, etc., power many state-of-the-art AI applications [1]: from chatbots [2], coding assistants [3], robots [4], to multi-agent systems [5]. However designing a computational workflow requires laborious engineering because many heterogeneous parameters (e.g. prompts, orchestration code, and ML hyper-parameters) are involved. Moreover, after deployment any erroneous behaviors of the workflow persist unless a developer manually updates it. ", "page_idx": 0}, {"type": "text", "text": "We study a class of optimization problems motivated by automating the design and update of computational workflows. Computational workflows produce optimization problems with heterogeneous parameters, rich feedback (e.g. console output and user\u2019s verbal responses), and intricate objectives (beyond maximizing a score). Moreover, a workflow can have interdependent steps (e.g. adaptive orchestration, feedback control loops) and/or involve non-differentiable, semi-black-box, stochastic operations (e.g. ML models, simulations) whose behavior cannot be succinctly captured. As a result, the structure of the computation may change as the parameters and the inputs of the workflow vary. ", "page_idx": 0}, {"type": "text", "text": "Due to its complexity, computational workflow optimization is usually framed as a black-box [6] or algorithm configuration [7] problem, and is tackled by general techniques like Bayesian Optimization [8], Evolutionary Algorithms [9], Reinforcement Learning (RL) [10] using scalar scores as feedback. But one observation of scalar feedback alone does not provide an improvement signal, so these algorithms are very inefficient when the parameter space is large (e.g. codes or natural language prompts). Recently LLM-based optimizers [11\u201316] have been proposed as generative optimizers to improve efficiency, leveraging the prior of generative models learned from large pre-training corpora to optimize complex prompts and codes. In this paper, we investigate how generative optimization can be applied more broadly and systematically to optimize a general computational workflow end-to-end. Appendix B discusses related works in generative optimization. ", "page_idx": 1}, {"type": "text", "text": "1.1 Toward Efficient End-to-End Optimization of Computational Workflows ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Inspired by back-propagation [17], we take an end-to-end approach to computational workflow optimization. AutoDiff frameworks [18, 19] have scaled the back-propagation algorithm to optimize differentiable workflows (e.g., neural networks) with billions of parameters. We extend the idea of AutoDiff and design a new framework, Trace, for jointly optimizing all of the heterogeneous parameters in general computational workflows, which may not be differentiable. ", "page_idx": 1}, {"type": "text", "text": "Trace treats a computational workflow as a graph like a neural network, where nodes are either inputs, parameters or the results of computation steps, and directional edges denote how nodes are created from others. But, instead of gradients, Trace propagates the execution trace of a workflow (which records the intermediate computed results and how they are used), via the notion of minimal subgraph (see Section 3.3). We show that propagating the execution trace subsumes back-propagation for differentiable workflows, and remains applicable even for non-differentiable workflows. Viewing a workflow as a graph and using its execution trace is standard for software engineering; for instance, human developers use such traces to debug distributed systems [20]. Our novel insight is that execution traces also enable end-to-end generative optimization, because they provide the information needed to relate rich feedback to the parameters in general workflows. ", "page_idx": 1}, {"type": "text", "text": "1.2 Example of Trace in Action ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Trace is available as a Python library with an API inspired by PyTorch [19]. A user declares the parameters to be optimized using a trainable flag, decorates the workflow with node and bundle wrappers, and runs a Trace optimizer \u2013 just like how they would declare and train neural networks. ", "page_idx": 1}, {"type": "text", "text": "Consider building an AI agent for the Battleship game (Fig. 1). The agent\u2019s policy (Fig. 2a) has two components (reason and act), which are chained together to react to different board configurations. The Battleship environment provides feedback (binary reward in texts) to tell if the agent\u2019s action hit the hidden ships, and the goal is to hit all hidden ships as fast as possible. Consider how a human programmer might approach the problem. They may run the policy and change the code based on the ", "page_idx": 1}, {"type": "image", "img_path": "rYs2Dmn9tD/tmp/ce6125b20185216bae76af5e372b5d8051b0abaa8b537ef92aa79cf895bdb86f.jpg", "img_caption": ["Figure 1: Learning Example in Battleship. Trace automatically optimizes heterogeneous parameters (multiple codes) to train an agent to Battleship. Means and standard errors are computed over 10 random seeds. "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "rYs2Dmn9tD/tmp/7dab68c9ab425a9334078131fad77a5a7954095ca3d5f2b4589aa25ffc6cffdc.jpg", "img_caption": ["(a) We write a trainable policy in (b) We then use PyTorch-like optimiza-(c) Trace automatically Python using Trace operators. tion syntax to train the policy. records execution DAG. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: Python Code of the Battleship Example. To build a self-adapting agent with Trace, we only need to annotate some empty functions (reason, act) and set up an optimizer following PyTorch semantics. For space, we trim the docstrings of the empty functions with \u201c. . . \u201d and list them in Appendix J. Trace then builds a DAG as the workflow executes and updates the parameters (see Fig. 1 for the result). ", "page_idx": 2}, {"type": "text", "text": "observed feedback. They may rewrite the code a few times to try different heuristics to solve this problem. They will fix any execution errors (e.g., out-of-bounds exceptions) by using stack traces. ", "page_idx": 2}, {"type": "text", "text": "Our Trace framework accomplishes the programmer\u2019s goal automatically without adding complexity to the Python code. The user declares reason and act as trainable (Fig. 2a) and then runs the agent in a PyTorch-like training loop (Fig. 2b). During the execution, Trace records a directed acyclic graph (DAG) (Fig. 2c) and uses it to compute the execution trace for optimization. Trace also automatically catches errors (e.g., syntax/semantic errors) and can use them as feedback. In Fig. 1, we show what the agent learns as Trace optimizes2its policy, where the learned policy is evaluated on new randomly generated games for generalization. The agent can quickly improve its performance and learn strategies that are increasingly complex. Remarkably, there is no mention of the specific Battleship environment API, nor details on how the functions reason and act should behave or adapt in Fig. 2a. The Trace optimizer figures out all the details through interactions as the computational graph unfolds and the feedback on the output is observed. Beyond code as parameters in this example, we also have experiments in Section 5 where prompts and other heterogenous parameters are optimized. ", "page_idx": 2}, {"type": "text", "text": "1.3 A New World of Optimization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The design of Trace is based on a new mathematical setup of iterative optimization, which we call Optimization with Trace Oracle (OPTO). In OPTO, an optimizer selects parameters and receives a computational graph as well as feedback on the computed output. Trace is a tool to efficiently convert the optimization of computational workflows into OPTO problems in practice. ", "page_idx": 2}, {"type": "text", "text": "We argue that framing computational workflow optimization as OPTO can lead to faster convergence than a black-box approach. We present a constructive proof: We design a general-purpose efficient generative optimizer called OptoPrime, for OPTO. OptoPrime turns OPTO to a sequence of pseudoalgorithm problems. In each iteration of OPTO, we format the execution trace and output feedback as a pseudo-algorithm question and present it to an LLM for solution (GPT-4 using a ReAct-CoT prompt listed in Appendix G). In experiments, we apply OptoPrime unchanged to many disparate applications like prompt optimization, first-order numerical optimization, hyper-parameter tuning, and robot controller design. We find that the general purpose OptoPrime is competitive with specialized optimizers for each domain, e.g., achieving $10\\%$ higher accuracy on BigBenchHard [21] when optimizing a DSPy [22] program compared to their hand-designed optimizer. ", "page_idx": 2}, {"type": "text", "text": "Trace, OPTO, and OptoPrime together provide the first tractable algorithm for optimizing general computational workflows end-to-end. The Trace framework $a$ ) leverages a workflow\u2019s structure and $b$ ) can incorporate rich feedback beyond scores, extending AutoDiff to complicated, non-differentiable computational workflows. With Trace, we conjecture that \u201ctraining deep agent networks\u201d (which fluidly mix computation of tensors, LLMs, and other programmable tools) will soon be possible. ", "page_idx": 2}, {"type": "text", "text": "2 Optimization with Trace Oracle ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "OPTO is the foundation of Trace. In this section, we define this graph-based abstraction of iterative optimization and discuss how OPTO covers various computational workflow optimization problems. ", "page_idx": 3}, {"type": "text", "text": "Preliminary We review the definition of a computational graph (see Fig. 2c). A computational graph $g$ is a DAG, where a node represents an object (such as tensors, strings, etc.) and an edge denotes an input-output relationship. We call a node without parents a root and a node without children a leaf, which are the inputs and outputs of the computational graph. In the context of optimization, some inputs are marked as trainable parameters, which are denoted as $\\{X_{\\theta}\\}$ . For a node $X$ , its parents are the inputs to an operator that creates $X$ . The descendants of node $X$ are those that can be reached from $X$ following the directed edges; the ancestors are defined conversely. Without loss of generality, we suppose that all computational operators have a unitary output3. In this way, we can associate the operator that creates the child node with the child node, and the full computation can be represented compactly as a DAG without explicitly representing the operators. The execution trace of a computational workflow is defined as the sequence of operations and their execution results invoked when computing the output from a set of inputs; execution traces can be expressed as a computational graph as defined above. ", "page_idx": 3}, {"type": "text", "text": "2.1 Problem Definition of OPTO ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "OPTO is an abstract setup of iterative computational workflow optimization. An OPTO problem instance is defined by a tuple $(\\Theta,\\omega,\\tau)$ , where $\\Theta$ is the parameter space, $\\omega$ is the context of the problem, and $\\tau$ is a Trace Oracle. In each iteration, the optimizer selects a parameter $\\theta\\in\\Theta$ , which can be heterogeneous. Then the Trace Oracle $\\tau$ returns a trace feedback, denoted as $\\tau=(f,g)$ , where $g$ is the execution trace represented as a DAG (the parameter is contained in the root nodes of $g$ ), and $f$ is the feedback provided to exactly one of the output nodes of $g$ . Finally, the optimizer uses the trace feedback $\\tau$ to update the parameter according to the context $\\omega$ and proceeds to the next iteration, as shown in Fig. 3. ", "page_idx": 3}, {"type": "image", "img_path": "rYs2Dmn9tD/tmp/2c57d74bb7af7230f97f250847aa801673853fdc6f0ee468dc196878eb55dc42.jpg", "img_caption": ["Figure 3: Iterations of OPTO. When $\\theta\\in\\Theta$ is selected, the Trace Oracle $\\tau$ returns trace feedback $\\tau\\,=\\,(f,g)$ , where $g$ is a computational graph using $\\theta$ as input and $f$ is the feedback given to the output of $g$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "In OPTO, the output feedback $f$ is generic, such as scores, gradients, hints/explanations expressed in natural language, and console messages. The context $\\omega$ provides invariant information to interpret the output feedback $f$ as well as any known side-information, e.g., desired properties of the parameters. The context $\\omega$ is fixed for an OPTO problem instance (similar to an instruction, or a problem definition), whereas the output feedback $f$ can change with the parameter $\\theta\\in\\Theta$ and the resulting computation $g$ . For example, $\\omega$ may be \u201cMinimize a loss function\u201d and $f$ is a loss. Alternatively, $\\omega$ can be open-ended, like \u201cFollow the feedback\u201d and $f$ describes how an output should be changed. In Section 3.2, we discuss how to define the context and output feedback when constructing OPTO problems in practice. In this paper, we focus on OPTO problems where $f$ and $\\omega$ can be expressed compactly in text. This covers a wide range of problems [23], including those with scalar feedback. ", "page_idx": 3}, {"type": "text", "text": "OPTO differs from a black-box setup in that the execution trace $g$ shows the computational path toward the output, which provides information to construct a parameter update direction from $f$ and $\\omega$ . In the minimization example above, when the execution trace $g$ is missing, it is unclear how the parameter can be improved given only a point evaluation of $f$ . On the other hand, with $g$ documenting the functions used to create the output, an update direction (e.g., a gradient) can be derived. We highlight that the structure of the computational graph $g$ returned by the Trace Oracle $\\tau$ can be different in each iteration in the general case (as in Fig. 3) because some workflows can change with different inputs and parameters. ", "page_idx": 3}, {"type": "text", "text": "To ground the OPTO setup, we show how OPTO is related to some existing problems with examples.   \nWe discuss other examples like hyperparameter tuning and multi-agent systems in Appendix C. ", "page_idx": 3}, {"type": "text", "text": "Example 1 (Neural network with back-propagation). The parameters are the weights. $g$ is the neural computational graph and $f$ is the loss. An example context $\\omega$ can be \u201cMinimize loss\u201d. The back-propagation algorithm can be embedded in the OPTO optimizer, e.g., an OPTO optimizer can use $\\tau$ to compute the propagated gradient at each parameter, and apply a gradient descent update. ", "page_idx": 3}, {"type": "text", "text": "Example 2 (RL). The parameters are the policy. $g$ is the trajectory (of states, actions, rewards) resulting from running the policy in a Markov decision process; i.e., $g$ documents the graphical model of how a generated action is applied to the transition, which then returns the observation and reward. $f$ can be a success flag. $\\omega$ can be \u201cMaximize return (cumulative rewards)\u201d or \u201cMaximize success\u201d. ", "page_idx": 4}, {"type": "text", "text": "Example 3 (Prompt Optimization of an LLM Agent). The parameters are the prompt of an LLM workflow. $g$ is the computational graph of the agent and $f$ is the feedback about the agent\u2019s behavior (which can be scores or natural language). $\\omega$ can be \u201cMaximize score\u201d or \u201cFollow the feedback\u201d. ", "page_idx": 4}, {"type": "text", "text": "3 Trace: The Next AutoDiff ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We design a framework, Trace, to bring OPTO from an abstract concept to reality. Trace provides a lightweight Python4tool to implement the Trace Oracle of OPTO for optimizing computational workflows. Through the OPTO framing, Trace separates the design of optimizers and domain-specific components so that general-purpose optimizers can be built that work across diverse domains. ", "page_idx": 4}, {"type": "text", "text": "3.1 Design of Trace ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Trace is designed based on two primitives: ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "\u2022 node is the wrapper of Python objects. When wrapped, a Python object is registered as a unique node in the computational graph of Trace. A node can be set trainable, which would make the node a parameter in OPTO. In addition, when using node to declare a parameter, one can also describe (in natural language) constraints that the parameter should obey. ", "page_idx": 4}, {"type": "text", "text": "\u2022 bundle is the decorator to turn Python methods into operators. When a function is decorated, its docstring and source code are recorded as the definition of the operator, which infer how the output feedback should change the parameters. Moreover, functions decorated by bundle can be set trainable as well, which means that the code of the decorated method becomes a parameter.5 ", "page_idx": 4}, {"type": "text", "text": "For any workflow, using Trace involves the following steps (see Fig. 2). First, the user declares the workflow\u2019s parameters using node and bundle, and defines the workflow\u2019s conceptual blocks as operators using bundle. Then the user creates an OPTO optimizer (such as OptoPrime in Section 4), and optionally provides the context $\\omega$ for the problem. (A default context $\\omega$ of OptoPrime is \u201cFollow the feedback\u201d). In addition, the user defines a mechanism to provide feedback to the computed result (e.g., scores, natural language suggestions, etc.), in analogy to defining a loss function in neural network training. After the declaration, optimization via Trace repeats the following: 1) Execute the decorated workflow. As it runs, a DAG is built in the backend, logging the computed results and their connections. 2) Initiate the propagation of the output feedback to the parameters by calling backward. (Any execution error is also treated as feedback; see Appendix D.) Internally, Trace extracts the minimal subgraph $g$ connecting the parameters and the output and sends the OPTO optimizer the trace feedback $\\tau=(f,g)$ . 3) Call the OPTO optimizer\u2019s step method to update the parameters. ", "page_idx": 4}, {"type": "text", "text": "3.2 Using Trace Primitives for Effective Execution Tracing ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "There are many ways to represent a computational workflow as a computational graph, from abstracting the entire process as one big operator to listing all low-level steps as operators in the graph. In Trace, the level of abstraction is decided by how bundle is applied, as all steps underneath bundle are abstracted as one operator. The design of bundle allows tracing most Python codes, except for those modifying the content of an object reference in place. However, such a case can be avoided by first duplicating the object and then applying the modification to the copied object, similar to how a recurrent neural network is implemented. ", "page_idx": 4}, {"type": "text", "text": "Different abstraction choices trade off the graph complexity and the description needed for each operator. Abstracting everything into a single operator makes a trivial graph but requires more descriptions to faithfully capture the workflow. On the other hand, not all details matter in optimization, so exposing every low-level operator can make the graph unnecessarily cluttered. The best representation depends on the application and OPTO optimizer at hand. This problem, we believe, is similar to the design of neural network architectures. Here, we suggest defining the operators to mimic the whiteboard system diagram of the computational workflow. This level of abstraction in our experiments strikes a good balance between the ease of documentation and the graph size. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Apart from architecture design, another design question is what information goes into the context $\\omega$ versus the description of each operator? For a single problem, there is no difference in principle; one can choose to provide details of all operators in $g$ through the context $\\omega$ . However, this will require manually crafting a context for every workflow. We suggest instead providing a description of the operators when they are defined using bundle. Then Trace will automatically generate the workflow-specific information while the same context $\\omega$ is shared across many workflows. ", "page_idx": 5}, {"type": "text", "text": "3.3 Backward Feedback Propagation: Realizing the Trace Oracle of OPTO ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Trace uses a recursive graph traversal algorithm (Algorithm 1) to propagate feedback in the reversed topological ordering. With different propagators, Algorithm 1 can implement various forward-backward schemes including back-propagation.6 We propose a general propagator, Minimal Subgraph Propagator (MSP), in Algorithm 2. MSP propagates the trace feedback $\\tau\\,=\\,(f,g)$ , where $g$ is implemented as a priority queue. Running Algorithm 1 with MSP (Algorithm 2) together implements the Trace Oracle of OPTO, which extracts the minimal subgraph between parameter nodes and output.7 connecting the parameters and an output. Appendix E proves the following: ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. For a graph with $N$ nodes and maximum degree $W$ , Algorithms 1 and 2 have time complexity $O\\bar{(}W N^{2}\\log\\breve{N})$ and space complexity8 $O(W N)$ . ", "page_idx": 5}, {"type": "text", "text": "By contrast, back-propagation has time and space complexities of $O\\dot{(}W\\dot{N}\\bar{d}^{2})$ and $O(d)$ respectively, where $d$ is the maximal dimension of tensors. The difference is because in the most general setting of computational graphs and feedback, the propagated feedback (no matter how it is represented) does not have a constant size and needs the full subgraph. ", "page_idx": 5}, {"type": "table", "img_path": "rYs2Dmn9tD/tmp/4196ec1d35691d57dadb08f11f380d5fdda15e77fbb5cc677dc2a137f1ec2efa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "rYs2Dmn9tD/tmp/f4073d2c3549275a79ecfb85f7f9a7ab07e6d4e7a149d4eb6642f8bf22c9b9be.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Theorem 2. For generic computational graphs of $N$ nodes, in the worst case, the propagated feedback needs a description length $\\Omega(N)$ to construct an improvement direction. ", "page_idx": 5}, {"type": "text", "text": "Despite the worst case complexity of MSP, in practice the difference is negligible. Since MSP only involves merging priority queues of references, most actual computation happens in the forward pass (and also the optimizer\u2019s step method). However for very large problems with millions of nodes in the minimal subgraph, we anticipate that computational issues of MSP could arise. ", "page_idx": 5}, {"type": "text", "text": "4 Design of the First OPTO Optimizer ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We introduce an LLM-based generative optimization algorithm OptoPrime for any text-based OPTO problem. We believe that this is one of many possible optimization algorithms for these problems and there is a large space to be explored for identifying efficient optimization methods for OPTO. ", "page_idx": 5}, {"type": "text", "text": "Subgraph Representation One core challenge of designing an LLM-based OPTO optimizer is how to represent the execution trace subgraph $g$ (which can involve various graph structures and heterogenous data) to LLMs, in a way that LLMs can understand and reason about the downstream effects of parameter updates. We leverage the LLMs\u2019 remarkable coding and debugging ability [3]. ", "page_idx": 5}, {"type": "text", "text": "We present the trace feedback $(f,g)$ computed by Trace as a pseudo-algorithm problem: the subgraph $g$ is expressed as a report of code execution with information about the computed values and descriptions of operators in $g$ . Then we prompt the LLM to update the parameters in $g$ based on feedback $f$ given to the output. Fig. 4 shows an example. It is crucial to note that even though the lines look like an actual program, it is not the real program but the computational graph defined by bundle (see Section 3.2). ", "page_idx": 6}, {"type": "text", "text": "Parameter Update We prompt the LLM with a ReActCoT style prompt (Appendix G.2) in one query, asking it to generate reasoning based on the graph, and a suggestion on the parameter changes. If the suggestion can be extracted from the response, we update the parameters. ", "page_idx": 6}, {"type": "text", "text": "Optimization Memory OptoPrime optimizes most workflows reasonably well using just the traced graph and feedback, but it can run into issues when single feedback alone is not informative (e.g., the output feedback is rewards, but there is no description of how the rewards are generated). For robustness, we have a basic memory module in OptoPrime, which tracks the past parameter-feedback pairs as in-context examples. See Appendix G for details. ", "page_idx": 6}, {"type": "image", "img_path": "rYs2Dmn9tD/tmp/7b6a69b089b525aa127f7676a6ed3c4362c3fe9f0673381e08f4c562452ebac4.jpg", "img_caption": ["Figure 4: An example pseudo-code report generated by Trace for a program of $\\times=$ Node $(-1.\\theta)$ ; $z=b a r(x)\\star(b a r(x)+1)$ and the objective of $\\operatorname{max}_{x}z$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate the Trace framework with OptoPrime. We implement the state-of-the-art LLM optimizer OPRO [13] as a baseline; in comparison with OptoPrime, OPRO does not use the execution trace but relies on the memory of parameter and feedback pairs. For these experiments, we use GPT-4- 0125-Preview. We run the experiments on a standard PC with 16 GB RAM, and Trace introduces no measurable overhead on executing the workflow. We also conduct experiments to compare Trace and OptoPrime with a concurrent AutoDiff-like framework, TextGrad [24], which was released after Trace was submitted to NeurIPS. We show that TextGrad can be easily implemented as an optimizer in Trace, and OptoPrime achieves similar or better performance than TextGrad while using much less computation time. In the rest of this section, we will denote Trace $^{+}$ OptoPrime simply as Trace. We report the token usages of all approaches in all experiments in Appendix I.1. ", "page_idx": 6}, {"type": "text", "text": "5.1 Validating with Numerical Optimization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "First, we want to validate if OptoPrime can solve classical differentiable optimization problems, since they are a special case of OPTO. Consider the problem of $\\operatorname*{min}_{x}|h(x)^{-}-y^{*}|$ for a target $y^{\\ast}$ . We construct a synthetic task environment that randomly creates $y^{\\ast}$ and the computational graph of $h$ with arbitrarily complex connections between numerical variables (see Appendix I.3 for details). We evaluate OptoPrime (denoted as Trace) and a variant that does not see the graph (Trace Masked); both the optimizers do not use memory. The output feedback is \u201cThe output should be <larger/smaller>\u201d (this feedback has the same information as the gradient w.r.t. $h$ ). We compare also the performance of Adam optimizer [25]. We run 30 trials over different randomly generated problems. All methods see the same randomness. Trace is able to match the best-in-class Adam; on the other hand, without access to the full computational graph, the feedback-alone optimizer struggles to find $x^{*}$ (Figure 5a). ", "page_idx": 6}, {"type": "text", "text": "5.2 Tuning Hyperparameters to Orchestrate Complex Systems ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We tested Trace in a traffic control problem, which is an instance of hyper-parameter tuning. We used UXSim [26] to simulate traffic at a four-way intersection, where the trainable parameters are 2 integers in [15, 90], which are the green light duration for each direction of traffic flow. The feedback is the estimated delay experienced by all vehicles due to intersections, and the goal of an optimizer is to minimize the delay using the fewest number of traffic simulations. To this end, this optimizer must find the right trade-off for temporally distributed and variable demands. In Fig. 5 we report the performance of a SOTA heuristic from the traffic control literature, SCATS [27] as well as two black-box optimization techniques: Gaussian Process Minimization (GP) [8] and Particle Swarm Optimization (PSO) [28]. All methods use the same starting parameters. Trace denotes OptoPrime using memory, and Trace NoMem denotes OptoPrime without memory. We report further details in ", "page_idx": 6}, {"type": "image", "img_path": "rYs2Dmn9tD/tmp/51ef9552d2cd9219a5301defac95f4f2485d12423b49756b0ab7397624dc6bc9.jpg", "img_caption": ["Figure 5: Numerical Optimization and Traffic Optimization Results. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Appendix I.4. GP and PSO appear bad because 50 iterations are insufficient for their convergence; given enough iterations, both will eventually perform well. Trace is quickly competitive with the SCATS heuristic, whereas OPRO is not. Moreover, we find that memory is crucial for Trace to perform well for this task. But we note that Trace consumes extra overhead compared to other methods, since Trace has to materialize the resulting computation graph and query an LLM with effectively a longer prompt than that of OPRO. ", "page_idx": 7}, {"type": "text", "text": "5.3 Unifying Prompts and Functions Optimization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Many LLM agents today, e.g., specified by LangChain [29] and DSPy [22], have many components. These libraries provide optimization tools to optimize a small portion of their workflows, predominantly the prompt that goes into an LLM call. However, for building self-adapting agents that can modify their own behavior, only allowing the change to one part of a workflow but not others can be limiting. In this experiment, we test Trace\u2019s ability in joint prompt optimization and code generation. Specifically, we optimize a given DSPy-based LLM agent and tune its three components: the meta-prompt prompt_template, a function create_prompt that modifies the prompt with the current question, and a function extract_answer that post-processes the output of an LLM call. ", "page_idx": 7}, {"type": "text", "text": "We set up an end-to-end prompt-and-code optimization pipeline. We use an automatic evaluation function to compare the LLM\u2019s output with the ground truth, which requires the LLM agent to generate outputs not only with the correct answer but also in the correct format (following the guidelines of [30]). We use the Big-Bench Hard dataset [21] (15 examples for training, 5 for validation, and the rest for testing). We compare Trace with DSPy\u2019s COPRO module (which optimizes the meta-prompt). In Table 1, we show that Trace is able to optimize a DSPy program beyond what DSPy\u2019s COPRO optimizer can, especially on algorithmic tasks. This result shows how Trace can concretely improve upon existing LLM prompt optimization libraries. We show learned codes in Appendix J. ", "page_idx": 7}, {"type": "table", "img_path": "rYs2Dmn9tD/tmp/e343128c4662a27e11b5f398492b455ff52fb32080b634d04bd4a958f62cd390.jpg", "table_caption": [], "table_footnote": ["Table 1: End-to-end workflow optimization for an LLM benchmark (Big-Bench Hard) in 0-shot setup. CoT refers to Chain-of-Thought prompting and PO refers to DSPy\u2019s own prompt optimizer (COPRO). We use Trace to optimize a DSPy program, starting from the same program and prompt template specified by DSPy. "], "page_idx": 7}, {"type": "text", "text": "5.4 Long-Horizon Robot Manipulator Control ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We test the ability of Trace to optimize long-horizon workflows with complex dependencies and to \u201cback-propagate through time\u201d. We use Trace to train the controller code (in Python) for a simulated Sawyer robot manipulator. We use the Meta-World environment from LLF-Bench [23] as the simulator and consider three tasks: Reach, Pick-place, and Push. For each task, LLF-Bench provides a task instruction and meaning of the action space, which we use as the context $\\omega$ of the OPTO problem. The observation is a Python dictionary of vectors, indicating the end-effector position, the goal position, the gripper status, etc. The action space is a 4-dimensional vector to control the relative position of the end-effector and the gripper state. In each time step, the LLF-Bench Meta-World simulator returns the observation and natural language feedback to guide the robot. An episode ends if the robot successfully solves the problem or because of time-out. We consider an episodic training setting. The initial condition for all iterations in training is the same. We evaluate the learned policy in terms of success, starting from 10 held-out initial conditions. The task horizon is 10 steps, which is sufficient for task completion, and each training iteration has one rollout. The output feedback in OPTO is a textual representation of task success. In addition to the controller code, we also decorated the reset and step functions of the gym environment so that the entire rollout can be traced end-to-end. We compare Trace with OPRO; because of the streaming OPTO setting, our OPRO implementation only proposes one candidate in each iteration, which is then evaluated and provided with the output feedback. ", "page_idx": 7}, {"type": "image", "img_path": "rYs2Dmn9tD/tmp/bba88258423b4cb0a0a5bfd6e8245280910288d5e07f5e485fb52e042d727d37.jpg", "img_caption": ["Figure 6: Learning the feedback control policy (code) for a simulated Sawyer manipulator in LLF-Bench Meta-World. In each iteration $\\mathbf{\\dot{x}}$ -axis), one episode of rollout (10 steps) is performed, and then the policy is updated. The mean and standard error of the success rate over 10 seeds are shown. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "The experimental results are summarized in Fig. 6. Trace denotes OptoPrime using memory, and Trace NoMem denotes OptoPrime without memory. We show learned code in Appendix J. OptoPrime is clearly the top-performing optimizer, especially the version with memory. OPRO is able to solve Reach at the start, but its performance degraded over iterations (this instability was mentioned in [13]) and gets a similar performance as OptoPrime (without memory) in Push. To validate that the performance of OptoPrime is indeed due to using the execution trace, we include an ablation where we mask out the execution trace, which leads to a significant decline in performance and stability. This experiment features the most complex graph structures among all the experiments. The experimental results here are quite impressive, showing that Trace is able to learn a sophisticated control logic in dozens of interactions, not only working on the training initial conditions but also on held-out testing ones too. We discuss some limitations in Appendix I.6. ", "page_idx": 8}, {"type": "text", "text": "5.5 Comparison with TextGrad ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "After the submission of our work to NeurIPS, another AutoDiff-like framework, TextGrad [24], was released, which shares the same goal of end-to-end optimizing AI workflows as Trace. In comparison, TextGrad propagates text feedback, whereas Trace propagates minimal subgraphs (see Section 3.3). The graph-based design of Trace, which separates the tracing infrastructure and optimization algorithms, makes it more flexible. In fact, we easily implemented TextGrad as an optimizer in the Trace framework, but the reverse is not possible (because TextGrad couples the infrastructure and the optimization algorithm together). In addition, unlike TextGrad, Trace supports jointly optimizing heterogeneous parameters and can be applied to directly trace a given computational workflow without the need to rewrite the workflow using pre-defined templates. Please see Appendix H for more discussion comparing the two frameworks. ", "page_idx": 8}, {"type": "text", "text": "In this experiment, we apply Trace to directly decorate the evaluation code released with the TextGrad library and optimize the parameters following their training/evaluation pipeline line-by-line. This experimental design makes the comparison fair by allowing each optimizer to access the same LLM APIs around the same time, and showcases the flexibility of Trace framework to optimize any computational workflow. We pick the Solution optimization [24, Table 2] and Prompt optimization [24, Table 3] for the reasoning tasks experiments. Please see [24] for details on the exact setup. We compare OptoPrime, TextGrad9, and a reimplementation10 of TextGrad as an optimizer in Trace. We find that all these algorithms achieve similar success rates in these experiments. One noticeable difference is that OptoPrime is about $3\\mathbf{x}$ faster wall-clock time than TextGrad since OptoPrime makes a single call to LLM in each optimization step, whereas TextGrad calls linear to the graph\u2019s size. ", "page_idx": 8}, {"type": "table", "img_path": "rYs2Dmn9tD/tmp/5b0896c4b788a55734828520198218bf11ceec792d2247c0245bee5b00265639.jpg", "table_caption": [], "table_footnote": ["Table 2: Comparison between Trace and TextGrad. The optimizer is GPT-4o-2024-08-06, and the student model is GPT-35-turbo-1106. The results show the mean and the standard error of success rate of the last iterate computed by 5 seeds. The experiment time reported is in minutes (the time involves not just training but also validation and testing by running TextGrad\u2019s original pipeline); the time of GSM8K experiment is omitted as the experiment time $(>\\!8\\mathrm{hrs})$ is determined primarily by the evaluation not optimization. "], "page_idx": 9}, {"type": "text", "text": "6 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We highlight that Trace, OPTO and OptoPrime are a first step towards end-to-end generative optimization and building self-adapting workflows. They have limitations in their current form. OPTO captures rich feedback, but it is important to specify a solution concept as well as the feedback source. We provide guidance for feedback design in Section 3.2 and discuss notions of optimality in Appendix F. We believe designing feedback will be as important as designing loss function in deep learning, both of which are open research questions. Also, Trace cannot convert all computational workflows into OPTO problems, e.g., stateful functions that modify their state in place cannot be represented as a DAG without modification, and distributed/parallel computing workflows are incompatible with the current implementation (though in theory Trace can run in an asynchronous way so long as the overall graph does not end up with cycles). Finally, while Trace is designed to be generic and future-proof, the OptoPrime optimizer is preliminary. Although we demonstrated that OptoPrime could work well with moderate-size graphs, it is not a provably optimal algorithm and uses more tokens than OPRO, though, in our experiments, OPRO\u2019s performance does not improve even when given a large token budget. The debugging ability and context limits of the LLM used in OptoPrime crucially determine the scale of problems that we can practically address today. Consequently, more research is needed for designing token-efficient generative optimization algorithms. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We created Trace that can convert a computational workflow optimization problem into an OPTO problem, and we demonstrated a tractable OPTO optimizer, OptoPrime. This is just a first step towards a new paradigm of end-to-end generative optimization, with exciting avenues for future work. We discuss a few selected ones below. Please see Appendix A for a longer discussion. ", "page_idx": 9}, {"type": "text", "text": "In OptoPrime, we connect optimization to an LLM\u2019s reasoning capability. Techniques that have been proposed to improve LLM reasoning, e.g. Chain-of-Thought [31], Few-Shot Prompting [32], Tool Use [33], and Multi-Agent Workflows [5] could also help improve OptoPrime or design new OPTO optimizers. We conjecture that a hybrid workflow of LLM and search algorithms, can enable a truly general-purpose OPTO optimizer. Along the way, we must settle on how to delineate the agent vs. the optimizer. How to trade off the generality of optimizer vs. crafting side-information in the context $\\omega$ to achieve task-specific performance is an open question. ", "page_idx": 9}, {"type": "text", "text": "In Trace, we chose a specific propagator (MSP), which maximally preserves information for a general computation graph. We can instead specialize it for specific computations, e.g. to accommodate very large graphs with a hierarchical graph representation. Going a step beyond the basic memory module we experimented with in OptoPrime, we anticipate that an optimizer that can reason about how a workflow will behave under counterfactual parameter settings can be more efficient than OptoPrime and can enable a divide-and-conquer approach to OPTO. More research is needed to study the theoretical properties of OPTO (such as optimization landscape and complexity). We hope our preliminary effort in Appendix $\\boldsymbol{\\mathrm{F}}$ can provide some guidance. Finally, in this paper, we focused on output feedback and context that can be compactly textualized. We anticipate that computational workflows with rich non-textual contexts and output feedback will also benefit from automatic generative optimization through appropriate applications of Trace (e.g., with VLMs). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to thank John Langford, Ahmed Awadallah, Jennifer Neville, Andrey Kolobov, Ricky Loynd and Paul Mineiro for thought-provoking discussions. We would also like to thank Tobias Schnabel, Ruijie Zheng, Wanqiao Xu, and Kaiwen Wang for their valuable feedback on an early draft of this manuscript. Additionally, we thank Anqi Li, Omar Khattab, David Hall, Yifan Mai, Bryan He, Yash Chandak, Emma Brunksill, and Dawen Liang for their suggestions and feedback. The work was partially done while Adith Swaminathan was at Microsoft Research. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Matei Zaharia, Omar Khattab, Lingjiao Chen, Jared Quincy Davis, Heather Miller, Chris Potts, James Zou, Michael Carbin, Jonathan Frankle, Naveen Rao, and Ali Ghodsi. The shift from models to compound AI systems. https://bair.berkeley.edu/blog/2024/02/18/ compound-ai-systems/, 2024.   \n[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[3] Albert Ziegler, Eirini Kalliamvakou, X. Alice Li, Andrew Rice, Devon Rifkin, Shawn Simister, Ganesh Sittampalam, and Edward Aftandilian. Productivity assessment of neural code completion. In SIGPLAN International Symposium on Machine Programming, page 21\u201329, 2022. [4] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In IEEE International Conference on Robotics and Automation, pages 9493\u20139500, 2023.   \n[5] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen LLM applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023.   \n[6] Andrew R Conn, Katya Scheinberg, and Luis N Vicente. Introduction to derivative-free optimization. SIAM, 2009.   \n[7] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm configuration. In Learning and Intelligent Optimization, pages 507\u2013523. Springer, 2011.   \n[8] Peter I Frazier. Bayesian optimization. Recent Advances in Optimization and Modeling of Contemporary Problems, pages 255\u2013278, 2018.   \n[9] Thomas B\u00e4ck and Hans-Paul Schwefel. An overview of evolutionary algorithms for parameter optimization. Evolutionary computation, 1(1):1\u201323, 1993.   \n[10] Josep Ginebra and Murray K Clayton. Response surface bandits. Journal of the Royal Statistical Society Series B: Statistical Methodology, 57(4):771\u2013784, 1995.   \n[11] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In ICLR, 2023.   \n[12] Reid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with \u201cgradient descent\u201d and beam search. In EMNLP, pages 7957\u20137968, 2023.   \n[13] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. In ICLR, 2024.   \n[14] Tobias Schnabel and Jennifer Neville. Prompts as programs: A structure-aware approach to efficient compile-time prompt optimization. arXiv preprint arXiv:2404.02319, 2024.   \n[15] Eric Zelikman, Eliana Lorch, Lester Mackey, and Adam Tauman Kalai. Self-taught optimizer (STOP): Recursively self-improving code generation. arXiv preprint arXiv:2310.02304, 2023.   \n[16] Allen Nie, Ching-An Cheng, Andrey Kolobov, and Adith Swaminathan. The importance of directional feedback for LLM-based optimizers. arXiv preprint arXiv:2405.16434, 2024.   \n[17] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. Nature, 323(6088):533\u2013536, 1986.   \n[18] Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: a survey. Journal of Machine Learning Research, 18(153):1\u201343, 2018.   \n[19] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: an imperative style, high-performance deep learning library. In NeurIPS, pages 8026\u20138037, 2019.   \n[20] Karthik Nagaraj, Charles Killian, and Jennifer Neville. Structured comparative analysis of systems logs to diagnose performance problems. In USENIX Symposium on Networked Systems Design and Implementation, pages 353\u2013366, 2012.   \n[21] Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging BIG-Bench tasks and whether chain-of-thought can solve them. In Findings of the ACL, pages 13003\u201313051, 2023.   \n[22] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, et al. DSPy: Compiling declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714, 2023.   \n[23] Ching-An Cheng, Andrey Kolobov, Dipendra Misra, Allen Nie, and Adith Swaminathan. LLF-Bench: Benchmark for interactive learning from language feedback. arXiv preprint arXiv:2312.06853, 2023.   \n[24] Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and James Zou. TextGrad: Automatic \u201cdifferentiation\u201d via text. arXiv preprint arXiv:2406.07496, 2024.   \n[25] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[26] Toru Seo. UXsim: An open source macroscopic and mesoscopic traffic simulator in python\u2013a technical overview. arXiv preprint arXiv:2309.17114, 2023.   \n[27] Courtney Slavin, Wei Feng, Miguel Figliozzi, and Peter Koonce. Statistical study of the impact of adaptive traffic signal control on traffic and transit performance. Transportation Research Record, 2356(1):117\u2013126, 2013.   \n[28] James Kennedy and Russell Eberhart. Particle swarm optimization. In ICNN, volume 4, pages 1942\u20131948, 1995.   \n[29] LangChain Team. Langchain tracing. https://blog.langchain.dev/tracing/, 2023.   \n[30] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. AgentBench: Evaluating LLMs as agents. In ICLR, 2024.   \n[31] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, pages 24824\u201324837, 2022.   \n[32] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020.   \n[33] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. In NeurIPS, 2023.   \n[34] James Wexler. Artificial Intelligence in Games: A look at the smarts behind Lionhead Studio\u2019s \u201cBlack and White\u201d and where it can and will go in the future. https://www.cs.rochester. edu/\\~brown/242/assts/termprojs/games.pdf, 2002.   \n[35] Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh R N, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, Ran Xu, Phil L Mui, Huan Wang, Caiming Xiong, and Silvio Savarese. Retroformer: Retrospective large language agents with policy gradient optimization. In ICLR, 2024.   \n[36] Alessandro Sordoni, Xingdi Yuan, Marc-Alexandre C\u00f4t\u00e9, Matheus Pereira, Adam Trischler, Ziang Xiao, Arian Hosseini, Friederike Niedtner, and Nicolas Le Roux. Joint prompt optimization of stacked LLMs using variational inference. In NeurIPS, 2023.   \n[37] Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and Jurgen Schmidhuber. Language agents as optimizable graphs. arXiv preprint arXiv:2402.16823, 2024.   \n[38] Ruotian Ma, Xiaolei Wang, Xin Zhou, Jian Li, Nan Du, Tao Gui, Qi Zhang, and Xuanjing Huang. Are large language models good prompt optimizers? arXiv preprint arXiv:2402.02101, 2024.   \n[39] Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric Xing, and Zhiting Hu. PromptAgent: Strategic planning with language models enables expert-level prompt optimization. In ICLR, 2024.   \n[40] Xinyu Tang, Xiaolei Wang, Wayne Xin Zhao, Siyuan Lu, Yaliang Li, and Ji-Rong Wen. Unleashing the potential of large language models as prompt optimizers: An analogical analysis with gradient-based model optimizers. arXiv preprint arXiv:2402.17564, 2024.   \n[41] Xinyun Chen, Maxwell Lin, Nathanael Sch\u00e4rli, and Denny Zhou. Teaching large language models to self-debug. In ICLR, 2024.   \n[42] Zeyuan Ma, Hongshu Guo, Jiacheng Chen, Guojun Peng, Zhiguang Cao, Yining Ma, and Yue-Jiao Gong. LLaMoCo: Instruction tuning of large language models for optimization code generation. arXiv preprint arXiv:2403.01131, 2024.   \n[43] Tennison Liu, Nicol\u00e1s Astorga, Nabeel Seedat, and Mihaela van der Schaar. Large language models to enhance bayesian optimization. In ICLR, 2024.   \n[44] Michael R. Zhang, Nishkrit Desai, Juhan Bae, Jonathan Lorraine, and Jimmy Ba. Using large language models for hyperparameter optimization. arXiv preprint arXiv:2312.04528, 2023.   \n[45] G\u00e1bor Bart\u00f3k, Dean P Foster, D\u00e1vid P\u00e1l, Alexander Rakhlin, and Csaba Szepesv\u00e1ri. Partial monitoring\u2014classification, regret bounds, and algorithms. Mathematics of Operations Research, pages 967\u2013997, 2014.   \n[46] Laurent Hascoet and Mauricio Araya-Polo. Enabling user-driven checkpointing strategies in reverse-mode automatic differentiation. arXiv preprint cs/0606042, 2006.   \n[47] Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated backpropagation for bilevel optimization. In AISTATS, pages 1723\u20131732, 2019.   \n[48] Ching-An Cheng, Jonathan Lee, Ken Goldberg, and Byron Boots. Online learning with continuous variations: Dynamic regret and reductions. In AISTATS, pages 2218\u20132228, 2020.   \n[49] Sham Machandranath Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, University College London, 2003. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Perspective: Deep Agent Workflows ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We posit that the current practice of manually engineering computational workflows to build AI systems is analogous to programmers in the early 2000s hand-coding neural network weights to create engaging AI characters in video games [34]. Just like AutoDiff enabled the automatic and scalable optimization of deep neural networks with billions of parameters, we believe that Trace is the first step towards automatic and scalable optimization of \u201cDeep Agent Workflows\u201d to power even more capable AI systems. However, there are several limitations of the current implementation of Trace that need to be addressed to build Deep Agent Workflows. ", "page_idx": 13}, {"type": "text", "text": "When designing interactive AI systems that learn from their interactions, we need to define the parameters and feedback of the system. Parameters are the internal attributes that can be updated by the learning algorithm employed by the system. Feedback are the things observed and recorded by the system as a product of its interactions, and that provide signal for learning. Trace enables the development of new learning algorithms (e.g. through OptoPrime) that incorporate rich feedback to update heterogenous parameters. In contrast, AutoDiff for deep neural networks uses numerical feedback (e.g. rewards or loss functions) to optimize numerical parameters (e.g. tensors). Black-box optimization techniques (e.g. Reinforcement Learning) can use numerical feedback to optimize heterogenous parameters (e.g. codes, hyper-parameters as well as tensors), though they are inefficient. In the experiments, we saw that Trace was more efficient than black-box methods by using a generalization of back-propagation. Finally, Trace can use rich feedback (e.g. language) to extract more signal for learning. ", "page_idx": 13}, {"type": "text", "text": "What can be traced? Trace cannot convert all computational workflows into OPTO problems. Workflows with recursive bundle operators or those requiring distributed/parallel computing are not compatible with the current implementation. A future work would be to expand the Trace implementation to support these scenarios. In addition, the current implementation of Trace does not trace the execution within an operator defined by bundle, though in principle this is possible. There are also ambiguities in how an existing workflow can be traced and represented as a DAG. One example is when there is some sub-workflow following an if condition and another one following else. One can choose to wrap the entire code, including if and else, by bundle as a single operator. On the other hand, one can also just wrap the sub-workflows and not trace the if condition nor represent it as part of the DAG. (That is, suppose the if condition is true; from the DAG, one cannot see the alternate path under else). The latter choice has a flavor of applying \u201cstop-gradient\u201d on the boolean condition, whereas the first choice enables back-propagation through also the logical condition. We summarized some design considerations in Section 3.2. We foresee the choice of what to trace and how to trace in building Deep Agent Workflows will be an on-going research problem, similar to neural network architecture design. ", "page_idx": 13}, {"type": "text", "text": "Where do we get rich feedback? The OPTO framework captures an abstraction of rich feedback, called trace feedback (the execution trace and the output feedback), but it requires specification of operator descriptions and output feedback source to guide the optimization effectively. OPTO can be more efficiently solvable than black-box problems only when the trace feedback provides information beyond reward signals; otherwise, information-wise, OPTO is no easier than black-box problems. The Trace framework automates the generation of the execution trace in OPTO, when users of Trace decorate the workflow end-to-end. Currently, OptoPrime uses the docstring of operators in the execution trace to understand the operators; nonetheless, Trace logs also the source code of the decorated methods, which can also be used in the future to design optimizers that uses more details of the operators. In our experiments, we focus on output feedback that is automatically generated. In other contexts, e.g. users interacting with chatbots, we can natively gather natural language feedback or synthesize feedback [16] based on raw observations. We also anticipate that feedback in the form of images (e.g. users\u2019 gestures) or videos (e.g. videogame player showing a desired correction to agent\u2019s behavior) will be readily available and can be used as the output feedback in the OPTO framework. While Trace can handle this feedback, the current design of OptoPrime does not handle non-textual content (either in parameters, inputs or feedbacks). We anticipate future work along the lines of [16] on enhancing feedback design and developing guidelines for designing more informative and directive optimizer and feedback mechanisms. ", "page_idx": 13}, {"type": "text", "text": "How to design adaptive optimizer for general OPTO problems? The proposed OptoPrime optimizer shows the possibility of designing a single optimization algorithm to solve a range OPTO problems from diverse domains. However, we remark that OptoPrime is just the first step; it is akin to the vanilla gradient descent algorithm, which shows a proof of concept but is not scalable for large problems. The current design of OptoPrime has several scalability limitations due to its summary approach that updates parameters through one call to an LLM. While we show in Section 5.1, that OptoPrime can compete with ADAM in small problems, OptoPrime is not as computational efficient and cannot scale as well as ADAM; therefore, for large-scale numerical problems, Trace and OptoPrime with the current design does not replace classical AutoDiff. OptoPrime is also limited by the ability of LLMs. It has difficulty in handling parameters or nodes that cannot be compactly represented in text, which prevents it from optimizing neural network weights, or reasoning with large, stateful objects like a database. Similarly, it likely cannot handle large graphs (with thousands of nodes) at the moment, as such a large graph would result in a huge context which may be beyond what LLMs can understand and reason about reliably. This limitation is aggravated when dealing with noisy feedback or systems, as we need to present multiple graphs and feedback in the context at once. We also do not know how to rigorously define the concept of step size, which however we expect is important to handle noisy or local feedback. We need further research on graph simplification and representation, to reduce complexity and improve the efficiency of feedback propagation. Lastly, right now OptoPrime represents the constraints on parameters as part of the text description, but we have observed that LLMs do not always follow it. An effective workaround is to implement constraint checking in the workflow to throw exceptions (which are then handled as feedback). More specialized constraint handling techniques is an interesting research direction (e.g. \u201cprojecting\u201d OPTO solver proposals for parameters onto their feasible sets), but they are not implemented in OptoPrime yet. ", "page_idx": 14}, {"type": "text", "text": "B Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Framework for Computational Workflows Frameworks such as LangChain [29], Semantic Kernels, AutoGen [5], DSPy [22] allow composing computational workflows and provide handengineered optimizers to tune an LLM\u2019s context (i.e. prompt templates, few shot examples, or tool libraries) using scalar feedback with black-box search techniques. They support tracing of the workflow to aid in profliing, debugging and visualization. In contrast, Trace uses tracing for automatic optimization, and constructs a different representation of the computational graph which is suited for that purpose. Moreover, Trace is designed to be general-purpose and agnostic to the underlying frameworks of computational workflows users choose. In principle, one can apply Trace to decorate and tune a workflow based on a mix of Autogen, LangChain, DSPy codes. In fact, our experiments in Appendix I use workflows declared using both AutoGen and DSPy. ", "page_idx": 14}, {"type": "text", "text": "Optimization of Graphs of LLM Workflows There are multiple efforts to optimize the computational graph of LLM workflows, which is a special case of the OPTO problem. These algorithms focus on optimizing prompts. SAMMO [14] is an example for prompts that uses additional graph structure to make the optimizer efficient. SAMMO represents the prompt parameter itself as a program so as to enable more efficient black-box search through the space of programs. DSPy [22] can optimize directly the prompts or the few-shot examples to include using scalar reward feedback. Retroformer [35] uses another small language model (LM) to provide suggestions/feedback (i.e. changing prompts) to improve the behavior of an actor LLM, where the small LM is tuned by offline RL. Deep Language Networks [36] view all of the prompts in an LLM workflow as tunable parameters and jointly optimizes them. They discovered that optimizing each parameter in isolation instead produces subpar results. [37] frames LLM systems as graph where nodes are operations and edges are messages/connections. (Note that this is different from the DAG used in Trace; here nodes are messages and edges are input-output of operators) and optimizes for the connection on edges (binary variables) by REINFORCE using scalar reward feedback and prompts by LLMs . They optimize each component separately without considering each other; for example, the prompts are optimized individually without considering the graph topology or how they are used down the road. We suspect this approach can be less stable. Their prompt optimization part also does not take output feedback, but simply use an LLM to self-check whether the prompt meets the need of generating desired functions the user specified. In contrast to these works, through the OPTO framing, Trace supports joint optimization of all parameters (prompts, hyperparameters, codes) with rich feedback, and is agnostic to graph structures (e.g., changing these parameters can dynamically change the graph structure and connections between nodes). Users of Trace are free to specify which parameters they want to automatically optimize via online interactions. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "LLM-Optimizers for Prompts and Codes There is a huge and fast growing literature on using LLMs as optimizers to improve prompts [12, 11, 38\u201340] or codes [41, 15, 42, 38]. Different from the works mentioned above, here the focus has been on an isolated problem (e.g., changing the behavior of a single LLM or improving the code generation in the question-answering format) rather than considering a non-trivial workflow or agent with multiple components like above. They do not consider optimizing prompts or codes as one component of a bigger workflow (e.g. implementing an autonomous agent), which is harder and requires the right credit assignment. In addition, these LLM-based optimizers, including OPRO [13], often propose only principles of how prompts should be designed and requires crafting problem specific prompts (as opposed to a single optimization prompt that can be applied to different problems). For adapting them to new problems, users need to design new prompts. Trace can also be applied to optimize trivial OPTO problems where the returned graph has just a single node of the parameter (which are the scenarios considered by these works). Nonetheless, the main focus of this paper is to study how optimization can be done efficiently as the graph becomes nontrivial and for diverse applications. Trace achieves this by using the abstract OPTO problem framing. Since OPTO encapsulates domain specific info in the graph, it enables designing fully instantiated optimizers that can be applied to multiple problems, rather than just principles which then requires hand crafting prompts for individual problems like previous works. ", "page_idx": 15}, {"type": "text", "text": "LLM-Optimizers for Hyperparameters Recent works like [43, 44] use LLMs to optimize numerical hyperparameters, as an alternate to Bayesian optimization. Here in the experiments we show that Trace $^+$ OptoPrime also can effectively learn hyperparamters, faster than Bayesian optimization. The main difference between Trace and the aforementioned work is the representation of the problem. In Trace, we provide the graph to the LLM-based optimization (through the pseudo-algorithm representation), and we consume rich language feedbacks on the output, both of which accelerates hyper-parameter optimization. ", "page_idx": 15}, {"type": "text", "text": "OPTO Related Setups OPTO is a generalization of partial monitoring games [45]. If there exists a latent loss function that the feedback $f$ adheres to (e.g. as in [23]), those OPTO instances can be written as partial monitoring game. However OPTO admits a more general notion of feedback $f$ , and we discuss solution concepts for them in Appendix F. On the other hand, OPTO can be also viewed as a special case of Learning from Language Feedback (LLF) setup defined in [23] with observations as the trace feedback. This is a framing of a meta LLF problem. In the LLFBench Meta-World experiments of this paper (Section 5), we show Trace can be used to learn policy for LLF problems grounded to an application too. ", "page_idx": 15}, {"type": "text", "text": "AutoDiff and Back-propagation Back-propagation has been shown to be a very effective tool in optimizing differential computational workflows. Our design of Trace is inspired by back-propagation and the ease of use of the AutoDiff framework PyTorch [19]. Nonetheless, we highlight that backpropagation (Backward Mode Differentiation) is not the only AutoDiff algorithm. For example, the gradient can be computed in a forward mode (Forward Mode Differentiation) as well, and there are also techniques of Checkpointing [46] and Truncated Back-Propagation approximation [47] for efficiency. What are the equivalent ideas of these methods for general computational workflows? We think this is an interesting future research direction. ", "page_idx": 15}, {"type": "text", "text": "C Examples of OPTO ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To ground the OPTO setup, we show how OPTO is related to some existing problems with examples. ", "page_idx": 15}, {"type": "text", "text": "Example 4 (Neural network with back-propagation). The parameters are the weights. $g$ is the neural computational graph and $f$ is the loss. An example context $\\omega$ can be \u201cMinimize loss\u201d. The back-propagation algorithm, in view of the OPTO formulation, is embedded in the OPTO optimizer. For example, an OPTO optimizer here is a composition of back-propagation and gradient descent, where back-propagation takes $\\tau$ to compute the propagated gradient at the parameter. ", "page_idx": 15}, {"type": "text", "text": "Example 5 (Code Debugging). The parameters are the codes. $g$ denotes the stacked trace and $f$ is the error message returned by a compiler. $\\omega$ can be \u201cMake no error\u201d. ", "page_idx": 15}, {"type": "text", "text": "Example 6 (RL). The parameters are the policy. $g$ is the trajectory (of states, actions, rewards) resulting from running the policy in a Markov decision process; that is, $g$ documents the graphical model of how an action generated by the policy, applied to the transition dynamics which then returns the observation and reward, etc. $f$ can be the termination signal or a success flag. $\\omega$ can be \u201cMaximize return\u201d or \u201cMaximize success\u201d. ", "page_idx": 16}, {"type": "text", "text": "Example 7 (Hyperparameter Tuning of ML Pipeline). The parameters are e.g. learning rates and architectures. $g$ describes the stages of the ML pipeline and the evaluation on the validation set, and $f$ is the validation loss. $\\omega$ can be \u201cMinimize validation error\u201d. ", "page_idx": 16}, {"type": "text", "text": "Example 8 (Prompt Optimization of an LLM Agent). The parameters are the prompt of an LLM workflow. $g$ is the computational graph of the agent and $f$ is the feedback about the agent\u2019s behavior (which can be scores or natural language). $\\omega$ can be \u201cMaximize score\u201d or \u201cFollow the feedback\u201d. ", "page_idx": 16}, {"type": "text", "text": "Example 9 (Multi-Agent Collaboration). The parameters are each agent\u2019s prompts. $g$ describes the entire conversation flow between agents, and $f$ is the feedback about whether the task is successful after each agent performs their action. $\\omega$ can be \u201cA group of agents coordinate to finish a task.\u201d. ", "page_idx": 16}, {"type": "text", "text": "As mentioned, the computational graph $g$ returned by the Trace Oracle $\\tau$ may have different graph structures. The length of the execution trace, e.g., in the debugging example above depends on how far the code executes. Similarly, the rollout length of in the RL problem can be randomly determined. The formulation of the Trace Oracle abstracts the details of a computational workflow, so problems from different domains can be framed in the unified framework. This abstraction allows us to design the computational tool Trace for various applications. ", "page_idx": 16}, {"type": "text", "text": "D Trace Handles Error in Execution as Feedback ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "It is worth mentioning that execution error can be directly used as feedback to optimize parameters in Trace. When execution error happens within a method decorated by bundle, Trace would adds a special exception node to the global computational graph and throw an TraceExecutionError to stop the computation. The computational graph ends at where the execution error happens. This exception node becomes the new output of the inputs to the decorated method (since the original method raises an error) and is the output of the truncated computational graph. Messages in TraceExecutionError can then be used as the feedback $f$ in OPTO and propagated from the exception node to the parameters. By calling an OPTO optimizer, the parameters can be updated to avoid causing the same execution error. See the exception handling code in Fig. 2. ", "page_idx": 16}, {"type": "text", "text": "We find that this error handling mechanism has two convenient usages. First, this allows using Trace to automatically debug issues in the workflow due to incorrect parameter settings. Such errors can happen frequently especially when codes are parameters, as during optimization codes not satisfying syntax or downstream API requirements can happen. The second usage is to enforce constraints the workflow has to satisfy at different stags of computation. With Trace, if an intermediate computed result does not satisfy the constraint, we can simply throw an exception which states the desired constraint. This error signal would be caught by Trace and can then provide early feedback to efficiently improve the parameters, since the graph is truncated at the error. ", "page_idx": 16}, {"type": "text", "text": "E Analysis of Trace ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "E.1 Proof of Complexity ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Algorithm 2 propagates the subgraph, represented by a priority queue (implemented as a min-heap). At a time, it needs to maintain the subgraphs coming from $W$ children separately. This leads to the space complexity of $O(W N)$ . This $O(W N)$ space complexity leads to the extra $W N\\log N$ factor in the time complexity of MSP compared with back-propagation, which is the time needed for merging $W$ subgraphs of size $O(N)$ . ", "page_idx": 16}, {"type": "text", "text": "E.2 Proof of Lower bounds ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Consider an OPTO problem whose goal is to find a parameter matching a $k$ -digit binary number. The computation checks each digit against a reference number in an arbitrary order. The feedback is either \u201c $\\dot{N}^{t h}$ check failed\u201d or \u201cAll checks succeeded\u201d. Propagated feedback must communicate $k$ bits ", "page_idx": 16}, {"type": "text", "text": "of information to interpret the feedback correctly; and the minimal subgraph conveys exactly that information. Updating the parameter using the minimal subgraph is trivial, whereas without it there are $2^{k}$ possibilities to check. ", "page_idx": 17}, {"type": "text", "text": "F When is OPTO Efficiently Solvable? ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We show that OPTO covers a wide range of complicated optimization problems. This shows that if OPTO can be efficiently solved, then many complex workflows can be efficiently optimized. However, the generality of OPTO also raises some fundamental questions, such as if OPTO is well defined and when OPTO can be efficiently solved. These questions stem from its generality of the context $\\omega$ and the output feedback $f$ in OPTO, since e.g. they can be anything descriable texts. This flexibility makes the scope of OPTO go beyond standard mathematical optimization problems, where a setup has a fixed context $\\omega$ (e.g., \u201cFirst-order optimization\u201d) and a fixed type of output feedback $f$ (a descent direction). Fully characterize the properties of OPTO, due to its generality, is beyond the scope of this paper and would require years of future research to come. Nonetheless, here we attempt to provide some preliminary answers and point out some research questions. ", "page_idx": 17}, {"type": "text", "text": "F.1 What is a solution? ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Classical mathematical optimization problems have a problem definition which itself is the solution concept. For example, in a minimization problem, it is clear we want to find the minimum of an objective function; even for problems as abstract and general as an equilibrium problem, the problem setup clearly states the solution concept of finding a point/set satisfying an equilibrium inequality [48]. One common pattern of these problems is that the solution concept is something that can be described as conditions on feedback that the parameter should satisfy. ", "page_idx": 17}, {"type": "text", "text": "By contrast, in a OPTO problem $(\\Theta,\\omega,\\tau)$ , by varying the context $\\omega$ , the desired parameter can change from one extreme to another. For example $\\omega$ may state \u201cFollow the feedback\u201d or \u201cThe feedback is adversarial.\u201d. Therefore, we need define the solution concept of OPTO differently, rather than just using the feedback. We need to also consider the context $\\omega$ appropriately. Below we make an attempt to give an axiom of OPTO for its solution to be well defined. ", "page_idx": 17}, {"type": "text", "text": "Axiom 1 (Verifiability). There is an verification oracle (a human, a machine learning model, or a polynomial-time algorithm) when given $(\\theta,\\omega,f)$ can verify whether $\\theta$ is a solution or not. ", "page_idx": 17}, {"type": "text", "text": "Notice the verification oracle in Axiom 1 is not limited to just algorithms. This is intentional because we currently do not have algorithms that are intelligent enough to process the wide range of contexts and feedback that OPTO allows. Therefore, we include human judgement or the use of LLMs or other AI systems as part of the definition, while acknowledging the impreciseness of the statement due to OPTO\u2019s soft computing nature. Lastly we note the verifiability is only defined with respect to the context $\\omega$ and the output feedback $f$ , not the execution trace $g$ . That is, the verification of a solution depends only on the output of computation. ", "page_idx": 17}, {"type": "text", "text": "F.2 Does a solution exist? ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Under Axiom 1, we can start to ask the basic question of whether a solution to an OPTO problem exists or not. There are clearly problems where no solution exists (that is, no parameter in $\\Theta$ can be verified by the verification oracle). For example if the feedback $f$ is contradicting and yet the context $\\omega$ is \u201cFollow the feedback.\u201d, then there would be no solution that is satisfactory. On the other hand, if $\\omega$ is \u201cIgnore the feedback\u201d, all parameters can be solutions. In the following, we assume solutions of OPTO under consider exist. This assumption would rule out problems, e.g., where the feedback is adversary to the context, and makes solving OPTO is a well-defined search problem. ", "page_idx": 17}, {"type": "text", "text": "Assumption 1. For an OPTO problem $(\\theta,\\omega,\\tau)$ , we assume there is at least a parameter $\\theta\\in\\Theta$ such that it can be verified as a solution by the verification oracle. ", "page_idx": 17}, {"type": "text", "text": "F.3 Can OPTO be efficiently solved? ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "So far our discussion establishes OPTO as a well-defined search problem, based on qualification on the context $\\omega$ and the output feedback $f$ . However search problems can be NP-hard. In other words, we know that, without the execution trace, there are search problem instances modeled by some $\\omega$ and oracle giving $f$ that cannot be efficiently solved. Take RL for a tabular MDP as an example of OPTO problem. Without the execution trace (i.e., not seeing the Markovian structure and trajectories), the problem has an exponential complexity (due to the size of the policy space) and we know by using the execution trace here, tabular RL can be solved approximately in polynomial time [49]. Another example is training of neural networks. Without the execution trace, we have a complex black-box optimization with a loss value, without gradients, whereas an execution trace allows implementation of back-propagation to compute the gradients at the parameters. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "More broadly speaking, if we consider a \u201chuman\u201d as an optimizer for OPTO, we see that (expert) engineers/researchers, when equpped with additional computational tools, can efficiently solve a broad range of OPTO problems (such as by using the execution trace. From these observations, we conjecture using information in the execution trace is the key to unlock efficient OPTO. More precisely, we conjecture that OPTO is efficiently solvable when the context and the trace feedback need to provide information to construct a corrective search direction. For example, when the output feedback back is just a scalar loss, and yet the context $^+$ execution trace feedback does not provide enough information to compute a descending direction then OPTO reduces back to a black box problem. (See the problem instance in Appendix E.2). Nonetheless, identifying which subsets of OPTO are efficiently solvable is a big open research question. ", "page_idx": 18}, {"type": "text", "text": "G Additional Details of Trace and OptoPrime ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "G.1 Backward Step of Trace ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The MSP extracts the minimal subgrpah of the full computational graph of the workflow. Here we show a visualization using the example in Fig. 4. ", "page_idx": 18}, {"type": "image", "img_path": "rYs2Dmn9tD/tmp/1fce005ef6a2379da8aa2a622ad0aa7f7ab689ed66fae2ea059725fb82dfec7c.jpg", "img_caption": ["(b) We create a succinct summary of the computation graph using a language that mimics a program. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "(a) This is an illustrative example of the graph constructed by Trace and how feedback is backpropagated to the parameter $\\mathsf{x}$ . ", "page_idx": 18}, {"type": "text", "text": "Figure A.1: Optimization Representation. For a program of $\\texttt{x}=$ node $(-1.\\theta)$ ; $\\mathsf{a}\\;=\\;\\mathsf{b a r}(\\mathsf{x})$ ; $y=a+1$ ; $z\\ =\\ a\\ \\star\\ y$ and the optimization objective of $\\operatorname{max}_{x}z$ , Trace automatically constructs a computation graph and represent the optimization problem as a debugging report. Note that the real program and the traced execution graph are different. ", "page_idx": 18}, {"type": "text", "text": "G.2 Prompts used in OptoPrime ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "OptoPrime is an LLM-based optimizer. Its prompt is composed of the following parts. ", "page_idx": 18}, {"type": "text", "text": "1. System Prompt: Representation Prompt (Fig. A.2) $^+$ ReAct+CoT Output Prompt (Fig. A.3) ", "page_idx": 18}, {"type": "text", "text": "2. User Prompt (Fig. A.4 or Fig. A.5) ", "page_idx": 19}, {"type": "text", "text": "where $^+$ denotes concatenation. We list the prompt templates of different components below. ", "page_idx": 19}, {"type": "table", "img_path": "rYs2Dmn9tD/tmp/5164d47de42a4802087f09235ab7a75486d6366a2e40b2e738606a546b9f731f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure A.2: Representation Prompt that phrases the OPTO update as a pseudo-algorithm question. ", "page_idx": 19}, {"type": "image", "img_path": "rYs2Dmn9tD/tmp/6fb2249fb3ac4f30f9c4912ed9f3517333fc54255a5e6f8a44361f990baa7d75.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure A.3: ReAct+CoT Output Prompt that instructs LLMs should respond in the format of (reasoning, answer, suggestion) and explains the output format. ", "page_idx": 19}, {"type": "image", "img_path": "rYs2Dmn9tD/tmp/cc7da15bcc0dd6db22dfa693567ec174deca1746c609f41c57ba5633dffdd628.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure A.4: User Prompt for OptoPrime without Memory ", "page_idx": 20}, {"type": "text", "text": "Figure A.5: User Prompt for OptoPrime with Memory ", "page_idx": 20}, {"type": "image", "img_path": "rYs2Dmn9tD/tmp/d445b03ec130ff35a433a905c4cab18c2ca8d1283e89a4569aa7aa1b7bf95344.jpg", "img_caption": ["Figure A.6: Problem Template used to fill the User Prompt. By default the Instruction (which is the context $\\omega$ of OPTO) is \u201cYou need to change the <value> of the variables in #Variables to improve the output in accordance to #Feedback.\u201d "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "1 #Instruction   \n2 {instruction}   \n3   \n4 #Code   \n5 {code}   \n6   \n7 #Documentation   \n8 {documentation}   \n9   \n10 #Variables   \n11 {variables}   \n12   \n13 #Constraints   \n14 {constraints}   \n15   \n16 #Inputs   \n17 {inputs}   \n18   \n19 #Others   \n20 {others}   \n21   \n22 #Outputs   \n23 {outputs}   \n24   \n25 #Feedback:   \n26 {feedback} ", "page_idx": 20}, {"type": "text", "text": "H Comparison between Trace and TextGrad ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Beyond the empirical results described in Section 5.5, there are several first principles reasons to prefer the Trace design to build future LLM-powered generative optimizers. ", "page_idx": 21}, {"type": "text", "text": "Joint vs. Individual Parameter Optimization: When there are multiple parameters to be optimized in a workflow, Trace takes a \u201cjoint optimization\u201d view (constructing the minimal subgraph involving all of the parameters described in the initialization of OptoPrime). This flexible design also allows an alternative approach of \u201cindividual optimization\u201d akin to co-ordinate descent if desired, i.e., fix all but one parameter and individually optimize each parameter. Such an optimization heuristic can be accomplished, e.g., by instantiating an OptoPrime instance per parameter and iterating through their updates; however, for many problems (like the toy example below), this strategy, which TextGrad employs, can be sub-optimal because a consistent optimizer must additionally maintain state about any suggested updates to all other parameters when reasoning about an individual parameter update. ", "page_idx": 21}, {"type": "image", "img_path": "rYs2Dmn9tD/tmp/a7f3274d1b00e47a3980cac09ae8ef55aca281e9f906c2fbbf30ac750bd5b739.jpg", "img_caption": ["Listing 1: Individual Parameter Optimization is suboptimal in this example compared to Trace. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Heterogenous Parameters: Trace encapsulates all Python datatypes as node, including numbers, floats, strings, etc. In contrast, many generative optimization libraries like TextGrad restrict their \u201cvariables\u201d (i.e. parameters and intermediate values) to only be strings; consequently running numerical optimization experiments, or optimizing code alongwith prompts and hyper-parameters can be challenging to set up. ", "page_idx": 21}, {"type": "text", "text": "Modularity: Trace provides a clean separation between the infrastructure (a platform akin to PyTorch) and a generative optimizer (OptoPrime akin to the Adam optimizer). Trace provides an object \u201cTraceGraph\u201d that describes the computational graph. This graph is independent of optimizers and LLMs. Such independent computation graph objects do not exist in TextGrad. We hope the Trace-provided interface can spur the development of many other generative optimizers, like how the OpenAI Gym API allowed the development of several Deep RL algorithms targeting a unified representation of RL problems. ", "page_idx": 21}, {"type": "text", "text": "Flexible Representation: The bundle functionality of Trace is more flexible in representing computations to an LLM than pre-defined templates used in other optimization frameworks. TextGrad relies on special functions to chain variables together, such as using \u201cFormattedLLMCall\u201d, while Trace supports any user-defined functions through decorators like bundle. ", "page_idx": 21}, {"type": "text", "text": "Scalability: The strategy taken by OptoPrime can scale better to larger graphs (both in terms of error accumulation across multiple LLM calls, and the costs of multiple LLM calls) compared to requiring one LLM call per computation node (and we see with bundle that not all computation nodes present equal difficulty for an optimizer). The difference in terms of computation costs between the two strategies can be large, especially when not all operators in the graph are as complex as querying LLM. However there may be even better optimizers that blend OptoPrime and TextGrad so as to divide-and-conquer huge computation graphs, we are excited about this avenue for future research. ", "page_idx": 21}, {"type": "text", "text": "I Experiment Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "I.1 Token Counts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We include token counts for the OPRO and OptoPrime prompts used across all our experiments, at the first iteration of optimization (note that OPRO\u2019s token usages grows linearly with iterations). We can see that indeed OptoPrime consumes significantly more tokens than OPRO. However, we observe consistently that even allowing 7-10x more iterations of OPRO so as to equalize token costs, the OPRO performance plateaus to a worse level than OPTOPrime (e.g. Figure 1: OPRO at Iter 7 vs. OptoPrime at Iter 2; Figure 5b: OPRO at Iter 50 vs. OptoPrime at Iter 5; Figure 6b: OPRO at Iter 30 vs. OptoPrime at Iter 10, etc.). OPRO is suboptimal not due to a token limit but instead a lack of information, which is captured and represented using Trace. ", "page_idx": 22}, {"type": "table", "img_path": "rYs2Dmn9tD/tmp/510c2a3309ccff1e824c77016b092b8ac4af69ae9ea4db435fb4f4404e15c644.jpg", "table_caption": [], "table_footnote": ["Table A.1: Token counts. "], "page_idx": 22}, {"type": "text", "text": "I.2 Battleship ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We implement a simple battleship game board in Python. The exact code is in the supplement. The game offers a string-based visualization of the board. It randomly places different types of ships on a 2-dimensional board with pre-specified width and height when it initializes. The agent does not see the ship location and has to select a coordinate on the board to hit next. One additional rule of this game is that the agent can go again if their previous coordinate selection (fire) is a hit, not counting as the finish of a turn. In Figure 1, we ran 10 trials, where in each trial, we ran 20 iterations of training. We measure the reward as $\\%$ of ship squares hit (over all squares occupied by ships). The reward plateaued at $60\\%$ because the game has a chance element (heuristics and strategies can only go so far \u2013 strategy is only in effect if a hit happens. Otherwise, there is no information about where ships might be). ", "page_idx": 22}, {"type": "text", "text": "I.3 Numerical Optimization ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Any classical numerical optimization problem can be framed as an OPTO problem. Consider $h(x)$ and a target $y^{\\ast}$ , in a context $\\omega$ finding the $y^{\\ast}$ by changing $x$ ; we know the most useful corrective $f$ feedback to change $x$ is the gradient $\\nabla_{h}{x}$ . Similar to Trace, AutoDiff packages like PyTorch\u2019s AutoGrad have implemented dynamic graph construction with special classes like torch.Tensor. We want to validate whether it is possible to rely on binary text feedback, a graph automatically constructed by Trace, and OptoPrime to update $x$ in the context of minimizing $\\left|y-y^{*}\\right|$ . ", "page_idx": 22}, {"type": "text", "text": "We constructed a synthetic task environment where we can create a complex computation graph with arbitrarily complex connections between numerical variables. The focus of this environment is on the complexity of the graph, not on the complexity of the numerical operators. Therefore, we only use one-dimensional input and basic arithmetic operators to create a numerical optimization problem solvable by a first-order optimizer. This environment constructs a computational graph by sampling a number of times. At each time, it will either use a previously computed variable or sample a new variable, and an operation will be sampled to combine them. The optimization task is, for a fixed number of steps, an optimizer needs to output $x$ that minimizes $y$ . ", "page_idx": 22}, {"type": "text", "text": "We evaluate the following baseline methods. Basic Agent: a basic LLM agent that simply stores past information of $\\left(x_{t-1},y_{t-1}\\right)$ in context before choosing the next $x_{t}$ . OPRO Agent: a basic LLM agent but we implement the state-of-the-art LLM optimizer OPRO [13], which updates the meta-prompt of the basic LLM agent. Torch $^+$ Adam: the problem we construct is end-to-end differentiable. Therefore, we simply pass in torch.Tensor $(\\mathsf{x})$ as input and use Adam optimizer to update. We tune the learning rate slightly and found 1e-1 to work well. We compare two kinds of Trace-based optimizers: Trace, where we allow OptoPrime to read in the entire computation graph before updating $x$ , or Trace Masked, where we hide the computation graph. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "We run 30 trials over different computation graphs and start all methods with the same initial $x,y^{*}$ . We compute the absolute error, which is $\\left|y-y^{*}\\right|$ . On average, Trace is able to match the best-in-class first-order gradient optimizer Adam [25]. It is not entirely surprising that all the other baselines are performing worse due to a lack of access to the computation graph. To our surprise, OPRO, by only accessing the history of input and output, as well as changing the meta-prompt, is able to eventually discover the correct solution. This confirms why there were early signs of success using LLMs for black-box optimization in a simple plug-and-play style. However, OPRO is not an efficient optimizer because it lacks access to the Trace oracle. We show OPRO struggles even more when the computation graph gets more complex. ", "page_idx": 23}, {"type": "text", "text": "I.4 Traffic Control ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We tested OptoPrime in a traffic control problem which is an instance of hyper-parameter tuning. We used UXSim [26] to simulate traffic at a four-way intersection, where the tunable parameters are the duration of the green lights for each direction of traffic flow. The feedback is a scalar loss calculated by monitoring the flow of a pseudo-random sequence of vehicles arriving at the intersection over a period of 30 minutes. The loss computes an estimate of the delay experienced by all vehicles due to the intersection, as well as variability in this estimate for every link in the network; lower values are better. The goal of an optimizer is to identify values for all of the green light duration so as to minimize the loss using the fewest number of traffic simulations. If the green light duration for a given traffic flow direction is set too low, then vehicles will queue up over time and experience delays, thereby lowering the score for the intersection. However, if the green light duration for a given direction is set too high, vehicles in other directions will queue up and experience delays, thereby lowering the score for the intersection. Hence an optimizer must find the right trade-off for temporally distributed and variable demands. ", "page_idx": 23}, {"type": "text", "text": "In Figure 5 we report the performance of a SOTA heuristic from the traffic control literature, SCATS [27] (adapted to this toy setting) as well as two black-box optimization techniques: Gaussian Process Minimization (GP) [8] and Particle Swarm Optimization (PSO) [28]. All methods are initialized to evaluate the same starting parameter. GP and PSO further evaluate 5 random parameters; moreover, if they query a previously evaluated point, that query is replaced by a randomly sampled parameter. GP constructs a surrogate model to mimic the black-box traffic simulation function which maps from parameters to observed score. Then it minimizes a utility function (e.g. the lower confidence bound) using the surrogate model to pick the next parameter to evaluate. PSO on the other hand maintains 5 particles in parameter space, each with a position and velocity. At each iteration of PSO, particles update their positions according to their previous positions and velocity, evaluate the function at the updated positions, and update the velocities of all particles using the observed values. Although GP and PSO are both black-box methods, GP can be thought to replace Trace oracle with instead a smooth differentiable surrogate function; whereas PSO is very different and maintains a candidate set of parameters (can be thought of as conceptually related to OptoPrime with memory). ", "page_idx": 23}, {"type": "text", "text": "GP appears to be bad because even when it converged, the exploration heuristic randomly samples parameters rather than pick the converged parameter. PSO appears bad because 10 iterations is insufficient for its convergence. Note that given enough number of iterations, black-box approaches will eventually perform well. Trace is quickly competitive with the SCATS heuristic, whereas OPRO is not. Moreover, we find that memory is crucial for OptoPrime to perform well at this task. Finally, Trace consumes additional overhead compared to black-box methods; beyond the space and time complexity for running the traffic simulation, Trace additionally materializes the computation graph per iteration. Thus it can also be more expensive per LLM call compared to OPRO. ", "page_idx": 23}, {"type": "text", "text": "I.5 BigBench-Hard ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Perhaps more surprisingly, there are many components that a workflow needs to learn. Some of these components can be the prompt to generate output from an LLM, while other components can be code that needs to further process these outputs. In many workflows today, enabled by LangChain [29] and DSPy [22], only a small part of this workflow, predominantly, the input to an LLM API call, is optimized. These libraries optimize input to an LLM, and human engineers process that input ", "page_idx": 23}, {"type": "image", "img_path": "rYs2Dmn9tD/tmp/bfaa78ce73fd153ac1e1c7be4d6f8f036c6a93e168b037b4b8c876bac848f125.jpg", "img_caption": ["(a) We write a workflow that prompts an LLM for a question and extracts the answer. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure A.7: LLM-based Workflow Optimization Example. ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "and integrate it into other systems. Indeed, both libraries can enable robust and swift large-scale engineering efforts to build LLM-based software. However, if our goal is to develop self-adapting agents that can modify their own behavior, we should not ignore one of LLM\u2019s greatest strengths: code generation. Trace allows us to unify prompt optimization and code generation, which enables the creation of agents capable of fast learning. ", "page_idx": 24}, {"type": "text", "text": "In this example of an LLM-based workflow (Figure A.7), there are three parameters that are flagged as trainable for the optimizer: prompt_template, create_prompt, and extract_answer. Note that two of them require the LLM to generate Python code, and one of them requires the LLM to modify a text. Trace abstracted away the different data types and enabled direct update and optimization of them. Furthermore, a human engineer is often tasked with writing an error-free extract_answer. The output of an LLM can be highly stochastic and can often change over time; the code that is used to extract the response of an LLM has to be extremely robust and, therefore, arduous to create. Whenever a major distribution shift happens in the LLM output, this code needs to be rewritten by a human engineer, and it is hard for humans to anticipate all of LLM\u2019s output patterns. ", "page_idx": 24}, {"type": "text", "text": "We set up the task of end-to-end workflow optimization. Unlike a typical LLM benchmark evaluation, where a lot of effort went into creating the perfect evaluate(answer, target) method so that all kinds of LLM outputs were post-processed, cleaned, and formatted to match the ground truth, we choose a simple evaluation function (that extracts a segment or does exact string matching) and place the burden on the workflow itself to figure out how to create the right answer to satisfy the evaluation metric. We choose Big-Bench Hard [21] as our task because it has 23 subtasks and contains both language and algorithmic tasks. ", "page_idx": 24}, {"type": "text", "text": "We split each task dataset into training, validation, and test. For Trace and Trace-CoT, we use the first 15 examples for training, 5 examples for validation (picking the best learned workflow), and then evaluate the performance on test examples. We use template-based positive and negative feedback during training. The positive feedback is \u201cThe answer is correct! No need to change anything.\u201d The negative feedback is \u201cThe answer is wrong. We expect the output of your answer to be {ANSWER}. Please modify the prompt and relevant parts of the program to help LLM produce the right answer.\u201d DSPy\u2019s prompt optimization method does not explicitly require a validation set, therefore, we just used all 20 examples for training. For both, we only optimize for 1 epoch. We either start with the vanilla boilerplate prompt template used by DSPy or we use the slightly sophisticated template used by DSPy\u2019s CoT module. Trace optimizes both DSPy\u2019s original design and outperforms their own optimizer COPRO by $10\\%$ on algorithmic tasks. ", "page_idx": 24}, {"type": "text", "text": "Big-Bench Hard requires different answer outputs. Out of 23 tasks, 14 tasks require a multiple-choice answer with options provided in the question. 4 of them require yes/no. 1 task requires True/False, while 1 task requires valid/invalid. And the 3 remaining tasks require answers that contain words or numbers. Even though DSPy\u2019s meta-prompt optimization is trained on each task individually, the output of LLM to the evaluation method is still not post-processed, resulting in low performances of these tasks. However, Trace can optimize code and LLM prompt jointly to successfully deliver the response expected by an automatic evaluation method. ", "page_idx": 24}, {"type": "table", "img_path": "rYs2Dmn9tD/tmp/89e60b73c2f9e39cafa1e635fc8386ad251c4a7cb8ef98b9fe92630ac68de417.jpg", "table_caption": ["Table A.2: Big Bench-Hard Per-Task Result. 0-shot performance. Some 0.0 here shown is because DSPy cannot find the clean/stripped output that matches what the automatic evaluation method expects. With additional human engineering, these numbers can improve. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "I.6 LLFBench Meta-World ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We test the ability of Trace to optimize long-horizon workflows with complex dependencies. We experiment with using Trace to train controller (python code) for a simulated Sawyer robot manipulator. We use the Meta-World environment of LLF-Bench [23] as the simulator and consider three tasks reach, pick-place and push. LLF-Bench is a simulated benchmark with gym interface for testing an agent\u2019s ability to learn from language feedback. In these LLF-Bench Meta-World tasks, the observation is a dictionary where each field denotes a feature of the state and has a vector value (e.g., the end-effector position, the goal position, the gripper status, etc.). The keys of the observation dictionary can differ for each task. The action space is 4-dimensional, which controls the relative position of the end-effector and the state of the gripper. In each time step, the LLF-Bench Meta-World simulator returns the observation dictionary and natural language feedback to guide the robot (we use the \u2018a\u2018 mode of LLF-Bench, with which the language feedback would contain information about the current performance, explanation of past successes and failures, and suggestions for the next step). An episode ends if the robot successfully solve the problem or because of time-out. For each task, LLF-Bench also provides a task instruction explaining that the task is about controlling a Sawyer robot arm and the meaning of the action space (see [23]). We use that as the context $\\omega$ of the OPTO problem. We consider an episodic setting. For each experiment (a random seed), we randomly sample an initial configuration. Then for each iteration of optimization, we reset the simulator to that sampled initial configuration and run the robot policy for $10^{11}$ steps or until the episode termination due to success. We compute the sum of rewards and gives the output feedback $f$ in texts in the format of \u201cSuccess: <true/false> Return: <score>\u201d. Note that the initial condition for all iterations within an experiment is the same so that the optimization problem is deterministic. To evaluate the learned policy\u2019s performance, for each experiment, we additionally run the learned policy starting from 10 held-out initial conditions, different from the fixed training initial condition. For each training algorithm discussed, we run it with 30 iterations, where each iteration consists of one episode rollout and one update. ", "page_idx": 25}, {"type": "image", "img_path": "rYs2Dmn9tD/tmp/3d28c8738c0c35014854dcdd8caf6903b94a80912d0ddcf5311881faea098426.jpg", "img_caption": ["Figure A.8: Learning the feedback control policy (code) for a simulated Sawyer manipulator in LLF-Bench Metawrold. In each iteration ( $\\bf\\dot{x}$ -axis), one episode of rollout is performed and then the policy is updated. Mean and standard error of success rate over 10 seeds are shown. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "To optimize the controller with Trace, we declare the control code as the parameter using the bundle decorator with trainable set to True; the initial control code simply outputs a zero vector $[0,0,0,0]$ . We decorate also the reset and the step function of the gym environment, so that the entire rollout of an episode can be traced end-to-end. In our implementation, a prototypical rollout would create a graph with around 30 operations where the controller code parameter is used multiple times. This graph structure is similar to that of running a recurrent neural network. For Trace, we experiment with OptoPrime with and without a memory of size 10. In addition to Trace, we implement the state-of-the-art LLM optimizer OPRO [13] as a baseline. Compared with Trace, OPRO does not use the execution trace information but rely on just memory of parameter and feedback pairs12 To make the comparison with Trace fair, we append the feedback observation from LLF-Bench given at each time step to the final feedback received by OPRO; on the other hand, Trace uses the simple final feedback of success and return and has to read the per-step feedback from the execution trace. To run OPRO in the OPTO setting, our implementation only proposes a single candidate in each iteration, which is then evaluated and provided with the output feedback. Since in [13] OPRO generates about 10 samples per iteration, so one iteration in [13] is roughly equivalent to 10 iterations here. ", "page_idx": 26}, {"type": "text", "text": "The experimental results are summarized in Fig. A.8, where we show the success rates at both the training initial condition as well as the held-out testing initial conditions over 10 seeds. OptoPrime is clearly the top-performing optimizer, especially the version with memory. OPRO is able to solve Reach at the start but its performance degraded over iterations (this instability was observed in [13]) and gets similar performance as OptoPrime (without memory) in Push. To validate that the performance of OptoPrime is indeed due to using the execution trace, we include an ablation where we mask out information in #Inputs, #Others, #Code, #Definition in the LLM context (see Fig. A.1b), which lead to significant degrade in performance and stability. This ablation shows that additionally using the execution trace provides more informed search direction compared with just using just the output feedback, which agrees with our hypothesis. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "This experiment features the most complex graph structures, and using Trace for optimization here is similar to back-propagation over time. The experimental results here are quite impressive, showing that Trace is able to learn a complex control logic in a dozens of interactions, not only working on the training initial conditions but also on the held-out testing ones too. Nonetheless, we want to point out some limitations in the current experimental results. We find that the success rate of the learned policy varies largely across random seeds. Except for Reach (the simplest task), in a seed, often either it finds a policy close to 1.0 success rate or 0.0 success rate. Therefore, the plots can roughly be interpreted as how long it takes to find a working policy. In addition, in these experiments, we find that providing task-related context is necessary. We find the context needs to be informative enough for humans to understand the problem13; otherwise, the optimization can be solved efficiently with the time scale considered here. Nonetheless, this requirement is reasonable, as there is no free lunch. ", "page_idx": 27}, {"type": "text", "text": "J Examples of the Optimized Parameters in the Experiments ", "text_level": 1, "page_idx": 28}, {"type": "table", "img_path": "rYs2Dmn9tD/tmp/14556528506c837453b88be14db08404d2acb977a2eea370f820d2a334fdd75b.jpg", "table_caption": [], "table_footnote": ["Figure A.9: Battleship Agent and optimization code. "], "page_idx": 28}, {"type": "text", "text": "1 ## Iteration 1 ; Success : False   \n2 def reason(self , map) -> str:   \n3 \"\"\"   \n4 Given a map , analyze the board in a game. On map , O denotes misses , X denotes successes ,   \nand . denotes unknown positions.   \n5 \"\"\"   \n6 # Check for first unknown position to try as a potential move   \n7 for row_index , row in enumerate(map):   \n8 for col_index , val in enumerate(row):   \n9 if val $==$ \u2019.\u2019:   \n10 return [row_index , col_index]   \n11 # Default to [0, 0] if no unknown positions found   \n12 return [0, 0]   \n13   \n14 ## Iteration 2 ; Success : True   \n15 def reason(self , map) -> str:   \n16 \"\"\"   \n17 Given a map , analyze the board in a game. On map , O denotes misses , X denotes successes ,   \nand . denotes unknown positions.   \n18 \"\"\"   \n19 # Improved strategic approach by checking . surrounded by more . which might indicate   \nundetected area   \n20 max_surrounding $\\mathit{\\Theta}=\\mathit{\\Theta}-1$   \n21 best_spot $=$ [0, 0]   \n22 for row_index , row in enumerate(map):   \n23 for col_index , val in enumerate(row):   \n24 if val $==$ \u2019.\u2019:   \n25 surrounding $=6$   \n26 for di in [-1, 0, 1]:   \n27 for dj in [-1, 0, 1]:   \n28 ni, nj $=$ row_index $^+$ di , col_index $^+$ dj   \n29 if $0<=$ ni $<$ len(map) and 0 $<=$ nj $<$ len(map [0]) and map[ni][nj] $==$   \n30 surrounding $+=~~1$   \n31 if surrounding $>$ max_surrounding:   \n32 max_surrounding $=$ surrounding   \n33 best_spot $=$ [row_index , col_index]   \n34 return best_spot   \n35   \n36 ## Iteration 8 ; Success : True   \n37 def reason(self , map) $->$ str:   \n38 \"\"\"   \n39 Given a map , analyze the board in a game. On map , O denotes misses , X denotes successes ,   \nand . denotes unknown positions.   \n40 \"\"\"   \n41 max_surrounding $\\mathit{\\Theta}=\\mathit{\\Theta}-1$   \n42 best_spot $=$ [0, 0]   \n43 for row_index , row in enumerate(map):   \n44 for col_index , val in enumerate(row):   \n45 if val $==$ \u2019.\u2019:   \n46 surrounding $=6$   \n47 for di in [-1, 0, 1]:   \n48 for dj in $[\\,-\\,1$ , 0, 1]:   \n49 ni, nj $=$ row_index $^+$ di , col_index $^+$ dj   \n50 if $0<=$ ni < len(map) and $\\theta<=n j<\\pi n(m a p[\\theta])$ :   \n51 if map[ni][nj] $==$ \u2019.\u2019:   \n52 surrounding $+=~1$   \n53 elif map[ni][nj] $\\mathbf{\\Psi}=\\mathbf{\\Psi}^{\\prime}\\cdot\\mathbf{\\Psi}\\times\\mathbf{\\Psi}^{\\prime}$ :   \n54 surrounding $+=\\ \\ 1\\ .\\ 5$ # increasing emphasis on proximity to   \nsuccessful hits   \n55 if surrounding $>$ max_surrounding:   \n56 max_surrounding $=$ surrounding   \n57 best_spot $=$ [row_index , col_index]   \n58 return best_spot   \n59   \n1 ## Iteration 0 (Initialization); Success : False   \n2   \n3   \n4 ## Iteration 1 ; Success : False   \n5 def act(self , map , plan):   \n6 \"\"\"   \nGiven a map , select a target coordinate in a game. On map , O denotes misses , X denotes   \nsuccesses , and . denotes unknown positions.   \n8 \"\"\"   \n9 # Example implementation , more sophisticated logic may be required based on intended   \ngameplay mechanics   \n10 return [0, 1]   \n11   \n12 ## Iteration 2 ; Success : True   \n13 def act(self , map , plan):   \n14 \"\"\"   \n15 Given a map , select a target coordinate in a game. On map , O denotes misses , X denotes   \nsuccesses , and . denotes unknown positions.   \n16 \"\"\"   \n17 # Use information from plan , which is output from __code2 , checking strategic position ,   \nnot just first available   \n18 best_score $\\qquad\\qquad\\qquad\\qquad=\\quad-1$   \n19 best_move $=$ [0, 0, 0] # initially no move is considered   \n20 for row_index , row in enumerate(map):   \n21 for col_index , val in enumerate(row):   \n22 if val $==$ \u2019.\u2019: # consider all unknown spots   \n23 hopes_surrounding $=~6$ # will hold number of \u2019.\u2019s around the spot   \n24 for di in [-1, 0, 1]:   \n25 for dj in [-1, 0, 1]:   \n26 ni, nj $=$ row_index $^+$ di , col_index $^+$ dj   \n27 if $0<=$ ni $<$ len(map) and $\\theta<=n j<\\pi n(m a p[\\theta])$ :   \n28 if map[ni][nj] $==$ \u2019.\u2019:   \n29 hopes_surrounding $+=~~1$   \n30 score $=$ hopes_surrounding # more \u2019.\u2019 around , higher the chance of hit   \n31 if score $>$ best_score:   \n32 best_score $=$ score   \n33 best_move $=$ [row_index , col_index]   \n34 if map[plan [0]][ plan [1]] $==$ \u2019.\u2019 and best_score $\\begin{array}{r l}{==}&{{}-1}\\end{array}$ :   \n35 return plan # if no better move found , and original place is still unknown   \n36 return best_move if best_score $>$ -1 else plan # return the best move found , or stick   \nwith the plan if still valid   \n37   \n38 ## Iteration 8 ; Success : True   \n39 def act(self , map , plan):   \n40 \"\"\"   \n41 Given a map , select a target coordinate in a game. On map , O denotes misses , X denotes   \nsuccesses , and . denotes unknown positions.   \n42 \"\"\"   \n43 best_score $\\qquad\\qquad\\qquad\\qquad=\\quad-1$   \n44 best_move $=$ [0, 0]   \n45 for row_index , row in enumerate(map):   \n46 for col_index , val in enumerate(row):   \n47 if val $==$ \u2019.\u2019 and [row_index , col_index] $!=$ plan:   \n48 hopes_surrounding $=~6$   \n49 for di in [-1, 0, 1]:   \n50 for dj in [-1, 0, 1]:   \n51 ni, nj $=$ row_index $^+$ di , col_index $^+$ dj   \n52 if $0<=$ ni $<$ len(map) and $\\theta<=n j<\\pi n(m a p[\\theta])$ :   \n53 if map[ni][nj] $==$ \u2019.\u2019:   \n54 hopes_surrounding $+=~~1$ # favoring positions with more   \nunknowns surrounding   \n55 elif map[ni][nj] $\\mathbf{\\Psi}=\\mathbf{\\Psi}^{\\prime}\\cdot\\mathbf{\\Psi}\\times\\mathbf{\\Psi}^{\\prime}$ :   \n56 hopes_surrounding $+=~~2$ # increased incentive for moves near   \nsuccessful spots   \n57 score $=$ hopes_surrounding   \n58 if score $>$ best_score:   \n59 best_score $=$ score   \n60 best_move $=$ [row_index , col_index]   \n61 if best_score > -1:   \n62 return best_move   \n63 return plan   \n64   \n1 @trace_class   \n2 class Predict(LLMCallable):   \ndef __init__(self):   \n4 super().__init__ ()   \n5   \n6 self.demos $=$ []   \n7 self.prompt_template $=$ dedent(\"\"\"   \n8 Given the fields \u2018question \u2018, produce the fields \u2018answer \u2018.   \n9   \n10   \n11   \n12 Follow the following format.   \n13   \n14 Question:   \n15 Answer:   \n16   \n17   \n18 Question: {}   \n19 Answer:   \n20 \"\"\")   \n21   \n22 self.prompt_template $=$ trace.node(self.prompt_template , trainable $=$ True ,   \n23 description $=^{\\prime\\prime}$ [ParameterNode] This is the Prompt   \nTemplate to the LLM...\")   \n24   \n25 @trace.bundle(trainable $=$ True)   \n26 def extract_answer(self , prompt_template , question , response):   \n27   \n28 Need to read in the response , which can contain additional thought , delibration and   \nan answer.   \n29 Use code to process the response and find where the answer is.   \n30 Can use self.call_llm (\" Return the answer from this text: \" $^+$ response) again to   \nrefine the answer if necessary.   \n31   \n32 Args:   \n33 prompt_template: The prompt that was used to query LLM to get the response   \n34 question: Question has a text describing the question but also \"Options\"   \n35 response: LLM returned a string response   \n36 Process it and return the answer in the exact format that the   \nevaluator wants to see.   \n37 Be mindful of the type of answer you need to produce.   \n38 It can be (A)/(B), a number like 8, or a string , or Yes/No.   \n39 \"\"\"   \n40 answer $=$ response.split(\"Answer:\")[1]. strip ()   \n41 return answer   \n42   \n43 @trace.bundle(trainable $=$ True)   \n44 def create_prompt(self , prompt_template , question):   \n45   \n46 The function takes in a question and then add to the prompt for LLM to answer.   \n47 Args:   \n48 prompt_template: some guidance/hints/suggestions for LLM   \n49 question: the question for the LLM to answer   \n50 \"\"\"   \n51 return prompt_template.format(question)   \n52   \n53 def forward(self , question):   \n54 \"\"\"   \n55 question: text   \n56   \n57 We read in a question and produces a response   \n58 \"\"\"   \n59 user_prompt $=$ self.create_prompt(self.prompt_template , question)   \n60 response $=$ self.call_llm(user_prompt)   \n61 answer $=$ self.extract_answer(self.prompt_template , question , response)   \n62 return answer   \n63   \n1 @trace_class   \n2 class PredictCoT(LLMCallable):   \n3 def __init__(self):   \n4 super().__init__ ()   \n5   \n6 self.demos $=$ []   \n7 self.prompt_template $=$ dedent(\"\"\"   \n8 Given the fields \u2018question \u2018, produce the fields \u2018answer \u2018.   \n9   \n10   \n11   \n12 Follow the following format.   \n13   \n14 Question: question   \n15 Reasoning: Let\u2019s think step by step in order to produce the answer. We ..   \n16 Answer: answer   \n17   \n18   \n19 Question: {}   \n20 \"\"\")   \n21   \n22 self.prompt_template $=$ trace.node(self.prompt_template , trainable $=$ True ,   \n23 description $=^{\\prime\\prime}$ [ParameterNode] This is the Prompt   \nTemplate to the LLM...\")   \n24   \n25 @trace.bundle(trainable $\\r=$ True)   \n26 def extract_answer(self , prompt_template , question , response):   \n27 \"\"\"   \n28 Need to read in the response , which can contain additional thought , delibration and   \nan answer.   \n29 Use code to process the response and find where the answer is.   \n30 Can use self.call_llm (\" Return the answer from this text: \" $^+$ response) again to   \nrefine the answer if necessary.   \n31   \n32 Args:   \n33 response: LLM returned a string response   \n34 Process it and return the answer in the exact format that the   \nevaluator wants to see.   \n35 Be mindful of the type of answer you need to produce.   \n36 It can be (A)/(B), a number like 8, or a string , or Yes/No.   \n37 question: Question has a text describing the question but also \"Options\"   \n38 111   \n39 answer $=$ response.split(\"Answer:\")[1]. strip ()   \n40 return answer   \n41   \n42 @trace.bundle(trainable $=$ True)   \n43 def create_prompt(self , prompt_template , question):   \n44 \"\"\"   \n45 The function takes in a question and then add to the prompt for LLM to answer.   \n46 The prompt should instruct the LLM to reason , think.   \n47 Args:   \n48 prompt_template: some guidance/hints/suggestions for LLM   \n49 question: the question for the LLM to answer   \n50 \"\"\"   \n51 return prompt_template.format(question)   \n52   \n53 def forward(self , question):   \n54 1111 11   \n55 question: text   \n56   \n57 We read in a question and produces a resposne   \n58 \"\"\"   \n59 user_prompt $=$ self.create_prompt(self.prompt_template , question)   \n60 response $=$ self.call_llm(user_prompt)   \n61 answer $=$ self.extract_answer(self.prompt_template , question , response)   \n62 return answer   \n63   \n1 ## Iteration 0 ( initialization )   \n2 def create_prompt(self , prompt_template , question):   \n3 11111   \n4 The function takes in a question and then add to the prompt for LLM to answer.   \n5 Args:   \n6 prompt_template: some guidance/hints/suggestions for LLM   \n7 question: the question for the LLM to answer   \n8   \n9 return prompt_template.format(question)   \n10   \n11 ## Iteration > 0   \n12 def create_prompt(self , prompt_template , question):   \n13   \n14 The function takes in a question and then add to the prompt for LLM to answer.   \n15 The prompt should now further instruct the LLM to carefully track the ball swaps   \noccurring step -by-step.   \n16 Args:   \n17 prompt_template: some guidance/hints/suggestions for LLM   \n18 question: the question for the LLM to answer   \n19   \n20 prompt_template $=$ \u2019Process this carefully: Step -by -step.\u2019 $^+$ prompt_template   \n21 return prompt_template.format(question)   \n22 ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "Figure A.14: Learned Predict module for BigBench. Functions with the same name are learned during different iterations or trials. ", "page_idx": 33}, {"type": "text", "text": "1 ## Iteration 0 ( initialization )   \n2 def \"e\"x\"tract_answer(self , prompt_template , question , response):   \n4 Need to read in the response , which can contain additional thought , delibration and an   \nanswer.   \n5 Use code to process the response and find where the answer is.   \n6 Can use self.call_llm (\" Return the answer from this text: \" + response) again to refine   \nthe answer if necessary.   \n7   \n8 Args:   \n9 prompt_template: The prompt that was used to query LLM to get the response   \n10 question: Question has a text describing the question but also \"Options\"   \n11 response: LLM returned a string response   \n12 Process it and return the answer in the exact format that the evaluator   \nwants to see.   \n13 Be mindful of the type of answer you need to produce.   \n14 It can be (A)/(B), a number like 8, or a string , or Yes/No.   \n15 \"\"\"   \n16 answer $=$ response.split(\"Answer:\")[1]. strip ()   \n17 return answer   \n18   \n19 ## Iteration $>0$   \n20 def extract_answer(self , response):   \n21 \"\"\"   \n22 Need to read in the response , which can contain additional thought , deliberation and an   \nanswer.   \n23 Use code to process the response and find where the answer is.   \n24 Can use self.call_llm (\" Return the answer from this text: \" + response) again to refine   \nthe answer if necessary.   \n25 Args:   \n26 response: LLM returned a string response   \n27 Process it and return the answer in the exact format that the evaluator   \nwants to see.   \n28 Be mindful of the type of answer you need to produce.   \n29 It can be (A)/(B), a number like 8, or a string , or Yes/No.   \n30 question: Question has a text describing the question but also \"Options\"   \n31 1   \n32 answer = \u2019\u2019   \n33 segments $=$ response.split(\u2019\\n\u2019)   \n34 for segment in segments:   \n35 if \u2019Answer:\u2019 in segment:   \n36 answer $=$ segment.split(\u2019Answer:\u2019)[1]. strip ()   \n37 refined_answer $=$ self.call_llm(\u2019Return the refined answer from this text: \u2019 + answer)   \n38 return refined_answer   \n39   \n40 def extract_answer(self , prompt_template , question , response):   \n41 \"\"\"   \n42 Processes the LLM response and extracts the final answer in the required format.   \n43 \"\"\"   \n44 # Assuming that the relevant part of the response is after \u2019Answer:\u2019 and before any   \nfurther commentary   \n45 extracted_part $=$ response.split(\u2019Answer: \u2019)[1]. split(\u2019 \u2019)[0]. strip ()   \n46 # Find the section of the answer and return it directly   \n47 result $=$ re.search(\u2019\\([A-E]\\)\u2019, extracted_part)   \n48 if result:   \n49 return result.group()   \n50 else:   \n51 return \u2019No valid answer found \u2019   \n52   \n53 def extract_answer(self , prompt_template , question , response):   \n54 \"\"\"   \n55 Processes the LLM response , extracting and formatting the final answer.   \n56 Uses code to meticulously parse the response to locate the answer section.   \n57 Optionally refines the answer by querying the LLM again if necessary.   \n58   \n59 Args:   \n60 response: string from LLM , expected format contains \u2019Answer:\u2019 followed by the answer.   \n61 question: Description of the question being addressed , may include \u2019Options \u2019   \n62 \"\"\"   \n63 answer $=$ response.split(\u2019Answer:\u2019)[1]. split(\u2019.\u2019)[0]. strip ().lower ().split(\u2019,\u2019)[0]. strip ()   \n64 return answer)   \n65 ", "page_idx": 34}, {"type": "text", "text": "Figure A.15: Learned Predict module for BigBench. Functions with the same name are learned during different iterations or trials. ", "page_idx": 34}, {"type": "image", "img_path": "rYs2Dmn9tD/tmp/efa7634e99029ac9aef9f44b3412b14a9c3fd1252e4f564fdf3cafb39bf667b7.jpg", "img_caption": ["Figure A.16: Learned BigBench Prompt Template. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "1 2 ## Iteration 0 (initialization); Success: False 3 def controller(obs): 4 \"\"\" 5 A feedback controller that computes the action based on the observation. 6 7 Args: 8 obs: (dict) The observation from the environment. Each key is a string (indicating a type of observation) and the value is a list of floats. 9 Output: 10 action: (list or nd.array) A 4-dimensional vector. 11 \"\"\" 12 return [0, 0, 0, 0] 13 14 ## Iteration 1; Success: False 15 def controller(obs): 16 \"\"\" 17 A feedback controller that computes the action based on the observation. 18 19 Args: 20 obs: (dict) The observation from the environment. Each key is a string (indicating a type of observation) and the value is a list of floats. 21 Output: 22 action: (list or nd.array) A 4-dimensional vector. 23 \"\"\" 24 hand_pos $=$ obs[\u2019hand_pos \u2019] 25 goal_pos $=$ obs[\u2019goal_pos \u2019] 26 action $=$ [goal_pos [0] - hand_pos [0], goal_pos [1] - hand_pos [1], goal_pos [2] - hand_pos [2], 0] 27 # Adjusting the last element of the action based on whether the hand is close enough to trigger grasping 28 if abs(goal_pos [0] - hand_pos [0]) $<\\ 6.\\ 62$ and abs(goal_pos [1] - hand_pos [1]) < 0.02 and abs(goal_pos [2] - hand_pos [2]) $<\\phantom{-}\\varnothing.\\phantom{-}\\varnothing2$ : 29 action [3] $=$ 1 # Assuming action [3] $=$ 1 triggers closing the gripper 30 return action 31 32 ## Interation 5; Success: False 33 def controller(obs): 34 \"\"\" 35 A feedback controller that computes the action based on the observation. 36 37 Args: 38 obs: (dict) The observation from the environment. 39 Output: 40 action: (list or nd.array) A 4-dimensional vector. 41 \"\"\" 42 hand_pos $=$ obs[\u2019hand_pos \u2019] 43 puck_pos $=$ obs[\u2019puck_pos \u2019] 44 goal_pos $=$ obs[\u2019goal_pos \u2019] 45 gripper_open $=$ 1 if obs[\u2019gripper_distance_apart \u2019][0] > 0.5 else 0 46 47 # Adjusting strategy to close the gripper when in proximity of the puck 48 if gripper_open and ((abs(hand_pos [0] - puck_pos [0]) $<\\ 6.\\,05;$ ) and (abs(hand_pos [1] - puck_pos [1]) $<\\ 6.\\,051$ ) and (abs(hand_pos [2] - puck_pos [2]) $<\\ \\mathsf{0}\\ .\\ \\mathsf{0}\\,5)$ ): 49 action $=$ [0, 0, 0, 1] # Close the gripper 50 elif not gripper_open: 51 direction_to_goal $=$ [0.09 - hand_pos [0], 0.95 - hand_pos [1], 0.12 - hand_pos [2]] 52 action $=$ [direction_to_goal [0], direction_to_goal [1], direction_to_goal [2], 0] Move towards the suggested pose once puck is grasped 53 else: 54 # Move towards the puck first if not carrying it 55 direction_to_puck $=$ [puck_pos [0] - hand_pos [0], puck_pos [1] - hand_pos [1], puck_pos [2] - hand_pos [2]] 56 action $=$ [direction_to_puck [0], direction_to_puck [1], direction_to_puck [2], 0] 57 58 return action ", "page_idx": 36}, {"type": "text", "text": "Figure A.17: Learned Code for LLFBench Meta-World Pick-Place (Part 1). Functions with the same name are learned during different iterations or trials. ", "page_idx": 36}, {"type": "text", "text": "1 2 3 ## Iteration 10; Success: False 4 def controller(obs): 5 \"\"\" 6 A feedback controller that computes the action based on the observation. 7 Args: 9 obs: (dict) The observation from the environment. 10 Output: 11 action: (list or nd.array) A 4-dimensional vector. 12 \"\"\" 13 hand_pos $=$ obs[\u2019hand_pos \u2019] 14 puck_pos $=$ obs[\u2019puck_pos \u2019] 15 goal_pos $=$ obs[\u2019goal_pos \u2019] 16 gripper_open $=$ 1 if obs[\u2019gripper_distance_apart $\\dot{\\mathrm{~\\,~}}\\mathrm{~]~}[\\theta]\\;\\;>\\;\\;\\theta\\;.\\;5$ else 0 17 18 normalize $=$ lambda x: [i / max(abs(max(x, key $=$ abs)), 1) for i in x] 19 20 # Close the gripper when close to the puck and the gripper is open 21 if gripper_open and ((abs(hand_pos [0] - puck_pos [0]) $<\\ 6.\\,05;$ ) and (abs(hand_pos [1] - puck_pos [1]) $<\\ 6.\\,051$ ) and (abs(hand_pos [2] - puck_pos [2]) $<\\ \\theta.\\theta5;$ ): 22 return [0, 0, 0, 1] 23 # When the puck is grasped , move towards the goal position with a normalized direction 24 elif not gripper_open and ((abs(hand_pos [0] - puck_pos [0]) $<\\textit{\\textbf{6.1}}$ ) and (abs(hand_pos [1] puck_pos [1]) $<\\ \\theta\\ .\\ 1\\ ;$ and (abs(hand_pos [2] - puck_pos [2]) $<\\ \\theta\\cdot1\\,)$ ): 25 direction_to_goal $=$ [goal_pos [0] - hand_pos [0], goal_pos [1] - hand_pos [1], goal_pos [2] - hand_pos [2]] 26 return normalize(direction_to_goal) $^+$ [0] 27 # When the gripper is open and not close enough to the puck , move towards the puck 28 else: 29 direction_to_puck $=$ [puck_pos [0] - hand_pos [0], puck_pos [1] - hand_pos [1], puck_pos [2] - hand_pos [2]] 30 return normalize(direction_to_puck) $^+$ [0] 31 32 ## Iteration 13; Success: True 33 def controller(obs): 34 \"\"\" 35 A feedback controller that computes the action based on the observation. 36 37 Args: 38 obs: (dict) The observation from the environment. 39 Output: 40 action: (list or nd.array) A 4-dimensional vector. 41 \"\"\" 42 hand_pos $=$ obs[\u2019hand_pos \u2019] 43 puck_pos $=$ obs[\u2019puck_pos \u2019] 44 goal_pos $=$ obs[\u2019goal_pos \u2019] 45 gripper_open $\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\quad1$ if obs[\u2019gripper_distance_apart $\\dot{\\mathrm{~\\,~}}\\mathrm{~]~}[\\theta]\\;\\;>\\;\\;\\theta\\;.\\;5$ else 0 46 47 normalize $=$ lambda x: [i / max(abs(max(x, key=abs)), 1) for i in $\\times\\,]$ 48 # Close the gripper when close to the puck and the gripper is open 49 if gripper_open and ((abs(hand_pos [0] - puck_pos [0]) $<\\ 6.\\,051$ ) and (abs(hand_pos [1] puck_pos [1]) $<\\ 6.\\,051$ ) and (abs(hand_pos [2] - puck_pos [2]) $<\\ \\theta.\\theta5;$ ): 50 action $=$ [0, 0, 0, 1] # Close the gripper 51 elif not gripper_open and ((abs(hand_pos [0] - puck_pos [0]) < 0.1) and (abs(hand_pos [1] puck_pos [1]) $<\\ \\theta\\ .\\ 1\\ ;$ and (abs(hand_pos [2] - puck_pos [2]) $<\\ \\theta\\cdot1\\,)$ ): 52 # When the puck is grasped , adjust direction towards the goal with improved precision and ensure successful pickup 53 direction_to_goal $=$ normalize ([ goal_pos [0] - hand_pos [0], goal_pos [1] - hand_pos [1], goal_pos [2] - hand_pos [2]]) 54 action $=$ direction_to_goal $^+$ [1] # Keep the gripper closed 55 else: 56 # Move towards the puck first if not carrying it 57 direction_to_puck $=$ normalize ([ puck_pos [0] - hand_pos [0], puck_pos [1] - hand_pos [1], puck_pos [2] - hand_pos [2]]) 58 action $=$ direction_to_puck $^+$ [0] # Open the gripper to prepare for grasping 59 60 return action ", "page_idx": 37}, {"type": "text", "text": "1 # Iteration 1   \n2 {   \n3 \"reasoning \": \"The task involves controlling a robotic arm to perform a pick -place operation by interpreting observations and generating appropriate control actions (4D vectors) to move the arm towards the goal. The objective is to adjust the controller \u2019s code (__code0) based on feedback received at each step of the process. The feedback within #Others suggests whether the movements made by the arm are leading it closer to or farther from the goal , along with advice on what coordinates to aim for next (e.g., [-0.07 0.68 0.12 0. ]). The controller \u2019s code currently returns a static action [0, 0, 0, 0], which does not adjust based on observations and hence does not efficiently move the robotic arm towards the goal. Given the repetitive feedback suggesting to go to [-0.07 0.68 0.12 0. ], it appears that the controller needs to dynamically compute actions based on the difference between the current positions (hand_pos , puck_pos) and the goal_pos , along with possibly manipulating the gripper state for pick and place operation.\",   \n4 \"answer \": \"\",   \n5 \"suggestion \": {...}   \n6 }   \n7   \n8   \n9 # Iteration 5   \n10 {   \n11 \"reasoning \": \"The instruction involves creating a controller for a Sawyer robot arm to solve a pick -and -place task by providing sequential control actions based on observations of the environment and state of the robot. The feedback indicates that despite the actions taken , the success criteria have not been met , with the latest attempt achieving a return of 0.9574427394250022. The code provided for __code0 defines a controller function using the observed state to compute control actions aimed at grasping a puck and moving it to a goal position while adjusting the gripper based on its distance from the puck. Feedback throughout the execution indicates that the controller adapts its strategy based on proximity to the puck and whether it has been picked up. Feedback also suggests that the gripper \u2019s opening and closing actions are central to the task \u2019s success , with corrections being made to close the gripper when necessary. The controller \u2019s logic for determining movements toward the puck or goal and adjusting the gripper state appears reasonable , but the feedback implies that adjustments might be required to ensure task completion. This could include fine -tuning the conditions under which the gripper is closed or adjusting the thresholds for puck pickup and delivery to the goal.\",   \n12 \"answer \": \"\",   \n13 \"suggestion \": {}   \n14   \n15 # Iteration 10   \n16 {   \n17 \"reasoning \": \"The task involves programming a robot arm to pick up a puck and place it at a goal position by providing sequential 4-dimensional action vectors. The action vectors represent target positions and gripper states (open or close) based on the robot \u2019s and puck \u2019s current positions. The feedback indicates that despite optimizing the controller logic to close the gripper when near the puck , move towards the puck when it\u2019s not grasped , and head towards the goal once the puck is grasped , the actions have not resulted in task completion. Specifically , repeated suggestions to close the gripper (indicating it remained open when it should have been closed) and guidance on where to move next suggest that the action selection logic might not be effectively responding to the environment \u2019s state or might not be properly engaging the gripper at the right times. The latest code includes a condition to normalize the direction of movement and checks for gripper position , but there seems to be a misunderstanding or oversight in adjusting the robot \u2019s behavior based on the scenario to successfully complete the pick -place task.\",   \n18 \"answer \": \"\",   \n19 \"suggestion \": {...}   \n20 }   \n21 } ", "page_idx": 38}, {"type": "image", "img_path": "rYs2Dmn9tD/tmp/4d1195e9c413645e5e5d9c2eec0635a727435701cd57fa2fabd6383d2639c82b.jpg", "img_caption": ["Figure A.20: Example Responses of LLMs for LLFBench Meta-World Pick-Place (Part 2); Content in suggestion is omitted for space (represented as ...); please see the code examples. "], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The contributions (Trace, OPTO, OptoPrime) are clearly stated in the introduction and detailed in Sections 2,3,4, and claimed improvements are validated in Section 5. Meanwhile the aspirational goal of self-improving interactive agents is elaborated separately in the concluding Section 7. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 40}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: The limitations are discussed in Section 6. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 40}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: There are theorems about the computational complexity of Trace and an information theoretic lower bound on propagated feedback. Both theorems are proved in Appendix E. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 41}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: Experimental setups are outlined in Section 5 and detailed in Appendix I. Supplementary material additionally provides the exact code that was run to produce all results, and only requires user to supply an OpenAI API key. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 41}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: All of the code to implement Trace and OptoPrime are provided in the supplementary material. All of the experiments described in the paper are reproducible with the supplied code. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 42}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: Experimental setups are outlined in Section 5 and detailed in Appendix I. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 42}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: All experiments include standard error bars from $\\ge10$ replications. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 42}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 43}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: All the experiments use the same compute resources (LLM API for OptoPrime and machine to run Trace), which are listed at the start of Section 5. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 43}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: The contributions of this paper (a framework analogous to PyTorch to generalize the backpropagation algorithm) do not have direct safety or security implications. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 43}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: In Sections 6 and 7 we discuss both the aspirational goals and their broader impacts. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 44}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: There are no models, data or APIs released along with the paper. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 44}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: Datasets used in the experiments (e.g. BigBench-Hard) are properly attributed. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: There is code implementing Trace and OptoPrime that are released along with the paper. The code repository contains licenses and several tutorial notebooks documenting each functionality. There are no datasets or models in the release, hence datasheets and modelsheets are not applicable for this release. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 45}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: There are no crowdsourcing or human subject studies conducted in this paper. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 45}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 45}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 46}]