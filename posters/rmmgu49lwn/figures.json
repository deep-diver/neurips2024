[{"figure_path": "RMmgu49lwn/figures/figures_1_1.jpg", "caption": "Figure 1: Extensive studies have tried to adopt IG models for IU. However, few attempts have been made to use IU models in IG.", "description": "This figure illustrates the relationship between Image Generation (IG) and Image Understanding (IU).  It shows that while many studies have explored using IG models to assist in IU tasks (through representation learning, data augmentation, or using intermediate features), there has been little research into leveraging IU models to improve IG.  The question mark highlights this unexplored reciprocal relationship, which the paper aims to investigate.", "section": "1 Introduction"}, {"figure_path": "RMmgu49lwn/figures/figures_2_1.jpg", "caption": "Figure 2: The token-based IG framework. Solid and dashed lines represent training and inference pipelines, respectively. During training, the tokenizer T tokenizes an image I into discrete codes z. A proposal network P is trained to model the distribution p(z), while a decoder D learns to reconstruct I. During inference, we sample codes z from P, which guides D to perform generation.", "description": "This figure illustrates the two-stage token-based image generation (IG) framework.  The first stage involves a tokenizer (T) that converts an image (I) into a sequence of discrete codes (z) using an encoder (E) and a codebook (C). The second stage uses a proposal network (P), which can be autoregressive (AR) or non-autoregressive (NAR), to model the probability distribution of the code sequence (p(z)).  A decoder (D) then reconstructs the image from the code sequence. During inference, the proposal network generates a new code sequence, and the decoder generates a new image (\u0128).", "section": "3 Token-Based Image Generation"}, {"figure_path": "RMmgu49lwn/figures/figures_3_1.jpg", "caption": "Figure 3: The architecture and training objective of different image tokenizers.", "description": "This figure compares three different image tokenizer architectures: VQGAN, FSQ, and VQ-KD.  VQGAN uses vector quantization to map image features to discrete codes. FSQ employs finite scalar quantization, a simpler method.  VQ-KD, unlike the others, uses knowledge distillation from a pre-trained teacher model (IU encoder) to learn the codebook, focusing on feature reconstruction rather than pixel reconstruction.  Each sub-figure illustrates the process, highlighting the key components (encoder, quantizer, decoder) and the loss functions used during training.", "section": "3.2 Image Tokenizers"}, {"figure_path": "RMmgu49lwn/figures/figures_6_1.jpg", "caption": "Figure 4: Codebook visualization of VQGAN and VQ-KDvit. Best viewed in color.", "description": "This figure visualizes the codebooks of VQGAN and VQ-KDvit using t-SNE to project the feature map and code vectors into a two-dimensional space.  The visualization shows that the VQ-KD codebook exhibits superior organization compared to the VQGAN codebook.  In the VQ-KD feature space, features from the same category are clustered together indicating each code in the VQ-KD codebook conveys clear semantics, whereas in VQGAN, the codebook is shared across multiple categories, resulting in semantic ambiguity.  This difference in semantic organization helps explain why VQ-KD outperforms VQGAN in terms of perplexity (PPL), despite having lower codebook usage. ", "section": "4.1 Codebook Visualization"}, {"figure_path": "RMmgu49lwn/figures/figures_8_1.jpg", "caption": "Figure 5: Reconstruction results of different image tokenizers.", "description": "This figure shows the reconstruction results of five different image tokenizers: Original, VQGAN, FSQ, VQ-KD, and Cluster.  Each row represents a different tokenizer. Each column shows a reconstruction of the same original image, allowing for a visual comparison of the quality of reconstruction produced by each tokenizer.  Red boxes highlight areas where VQGAN and FSQ fail to correctly reconstruct the image details, demonstrating the superior performance of VQ-KD and Cluster in faithfully reproducing the original image.", "section": "4.7 Qualitative Analysis"}, {"figure_path": "RMmgu49lwn/figures/figures_9_1.jpg", "caption": "Figure 5: Reconstruction results of different image tokenizers.", "description": "This figure visually compares the reconstruction quality of four different image tokenizers: VQGAN, FSQ, VQ-KD, and Cluster.  Each row represents a different tokenizer. Each column shows the reconstruction of the same image from the original image dataset (shown in the top row). The figure highlights how well each tokenizer reconstructs various aspects of the original image, such as textures, details, and overall appearance. By comparing the reconstructions with the original images, one can visually assess each tokenizer's strengths and weaknesses in terms of image fidelity and detail preservation.", "section": "4.7 Qualitative Analysis"}, {"figure_path": "RMmgu49lwn/figures/figures_9_2.jpg", "caption": "Figure 5: Reconstruction results of different image tokenizers.", "description": "This figure presents a visual comparison of the reconstruction quality achieved by different image tokenizers. Each row represents a different tokenizer (VQGAN, FSQ, VQ-KD, and Cluster).  The original image is shown in the first column, followed by the reconstructions generated by each tokenizer. The figure highlights the differences in the detail and accuracy of reconstruction between the different tokenizers, showing VQ-KD and Cluster to maintain significantly better visual fidelity.", "section": "4.7 Qualitative Analysis"}]