[{"figure_path": "pWowK7jqok/tables/tables_6_1.jpg", "caption": "Table 1: Quantitative comparison between SOTA methods, where SVD denotes the standard stable video diffusion network. \u201cVID\u201d represents the video and \u201cEVT\u201d indicates the event data. \u2191 (resp.\u2193) represents the bigger (resp. lower) the better.", "description": "This table presents a quantitative comparison of the proposed method against state-of-the-art (SOTA) methods for video and event-based data.  It compares several metrics including Fr\u00e9chet Video Distance (FVD), Mean Squared Error (MSE), Structural Similarity Index Measure (SSIM), Learned Perceptual Image Patch Similarity (LPIPS), mean Intersection over Union (mIoU), and average Intersection over Union (aIoU). Lower FVD, MSE, and LPIPS scores are better, while higher SSIM, mIoU, and aIoU scores are better.  The table shows that the proposed method ('Ours') generally outperforms the other methods, particularly in the event-based scenario ('EVT').", "section": "5 Experiments"}, {"figure_path": "pWowK7jqok/tables/tables_8_1.jpg", "caption": "Table 1: Quantitative comparison between SOTA methods, where SVD denotes the standard stable video diffusion network. \u201cVID\u201d represents the video and \u201cEVT\u201d indicates the event data. \u2191 (resp. \u2193) represents the bigger (resp. lower) the better.", "description": "This table presents a quantitative comparison of the proposed method against other state-of-the-art (SOTA) methods for video and event data.  It shows the performance of each method in terms of FVD (Fr\u00e9chet Video Distance), MSE (Mean Squared Error), SSIM (Structural Similarity Index), LPIPS (Learned Perceptual Image Patch Similarity), mIoU (mean Intersection over Union), and aIoU (average Intersection over Union). Lower values are better for FVD, MSE, and LPIPS, while higher values are better for SSIM, mIoU, and aIoU.", "section": "5 Experiments"}, {"figure_path": "pWowK7jqok/tables/tables_8_2.jpg", "caption": "Table 3: Ablation Study of Pre-training Phase. All models are tested with only feeding single event voxel frame, where \"#Prompt\" indicates the number of training prompt event data, U(1, 3) indicates to randomly select 1 to 3 frames as training prompt. 'Fine-tuning' refers to the parameters that are fine-tuned, 'T' indicates the fine-tuning of all temporal attention parameters, and 'S+T' indicates the simultaneous fine-tuning of both spatial and temporal attention parameters. 'CLIP' indicates whether the CLIP features extracted have been fine-tuned for events. 'RGB' refers to the CLIP model pre-trained on RGB data, while 'Event' indicates the CLIP model fine-tuned with event data.", "description": "This table presents the results of an ablation study on the pre-training phase of the proposed Event-Sequence Diffusion Network.  It shows how different configurations (number of training prompts, fine-tuning of temporal and/or spatial attention parameters, and whether CLIP features were fine-tuned for events or not) impact the model's performance as measured by FVD, SSIM, LPIPS, mIoU, and aIoU.  The results highlight the optimal configuration for achieving the best model performance.", "section": "4.1 Learning Motion Prior via Pretraining on Event Sequences"}, {"figure_path": "pWowK7jqok/tables/tables_9_1.jpg", "caption": "Table 4: Ablation Study of motion alignment and multi prompt. All models are tested with only feeding single event voxel frame. \u2018EP\u2019 denotes denoising using the high temporal resolution event prompt, and \u2018MA\u2019 denotes motion alignment based on reinforcement learning.", "description": "This table presents the ablation study results of the proposed method. It compares the performance of the model with and without motion alignment (MA) and high temporal resolution event prompt (EP).  Each row represents a different configuration, and the columns show the FVD, SSIM, LPIPS, mIoU, and aIoU metrics. The results demonstrate the individual and combined effects of MA and EP on the model's performance.", "section": "5 Experiments"}, {"figure_path": "pWowK7jqok/tables/tables_16_1.jpg", "caption": "Table 1: Quantitative comparison between SOTA methods, where SVD denotes the standard stable video diffusion network. \u201cVID\u201d represents the video and \u201cEVT\u201d indicates the event data. \u2191 (resp.\u2193) represents the bigger (resp. lower) the better.", "description": "This table presents a quantitative comparison of the proposed method against several state-of-the-art (SOTA) methods for video and event data.  It compares the methods across various metrics, including Fr\u00e9chet Video Distance (FVD), Mean Squared Error (MSE), Structural Similarity Index Measure (SSIM), Learned Perceptual Image Patch Similarity (LPIPS), mean Intersection over Union (mIoU), and average Intersection over Union (aIoU).  The table helps to illustrate the performance gains achieved by the proposed method, particularly when using event data as input.", "section": "5 Experiments"}, {"figure_path": "pWowK7jqok/tables/tables_17_1.jpg", "caption": "Table 1: Quantitative comparison between SOTA methods, where SVD denotes the standard stable video diffusion network. \u201cVID\u201d represents the video and \u201cEVT\u201d indicates the event data. \u2191 (resp.\u2193) represents the bigger (resp. lower) the better.", "description": "This table compares the performance of the proposed Event-Sequence Diffusion Network with several state-of-the-art (SOTA) methods for video and event data.  The metrics used for comparison include Fr\u00e9chet Video Distance (FVD), Mean Squared Error (MSE), Structural Similarity Index (SSIM), Learned Perceptual Image Patch Similarity (LPIPS), mean Intersection over Union (mIoU), and average Intersection over Union (aIoU). Lower values are better for FVD, MSE, and LPIPS, while higher values are better for SSIM, mIoU, and aIoU. The table shows that the proposed method outperforms SOTA methods, particularly in event data.", "section": "5 Experiments"}, {"figure_path": "pWowK7jqok/tables/tables_18_1.jpg", "caption": "Table 1: Quantitative comparison between SOTA methods, where SVD denotes the standard stable video diffusion network. \u201cVID\u201d represents the video and \u201cEVT\u201d indicates the event data. \u2191 (resp.\u2193) represents the bigger (resp. lower) the better.", "description": "This table presents a quantitative comparison of the proposed method against state-of-the-art (SOTA) methods for video and event-based motion prediction.  It compares various metrics, including Fr\u00e9chet Video Distance (FVD), Mean Squared Error (MSE), Structural Similarity Index (SSIM), Learned Perceptual Image Patch Similarity (LPIPS), mean Intersection over Union (mIoU), and average Intersection over Union (aIoU).  The results are shown for both video (VID) and event-based (EVT) data.  Lower values are better for FVD, MSE, and LPIPS; higher values are better for SSIM, mIoU, and aIoU.", "section": "5 Experiments"}]