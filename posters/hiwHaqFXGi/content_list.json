[{"type": "text", "text": "Disentangled Generative Graph Representation Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Recently, generative graph models have shown promising results in learning graph   \n2 representations through self-supervised methods. However, most existing gener  \n3 ative graph representation learning (GRL) approaches rely on random masking   \n4 across the entire graph, which overlooks the entanglement of learned representa  \n5 tions. This oversight results in non-robustness and a lack of explainability. Fur  \n6 thermore, disentangling the learned representations remains a significant challenge   \n7 and has not been sufficiently explored in GRL research. Based on these insights,   \n8 this paper introduces DiGGR (Disentangled Generative Graph Representation   \n9 Learning), a self-supervised learning framework. DiGGR aims to learn latent   \n10 disentangled factors and utilizes them to guide graph mask modeling, thereby   \n11 enhancing the disentanglement of learned representations and enabling end-to-end   \n12 joint learning. Extensive experiments on 11 public datasets for two different graph   \n13 learning tasks demonstrate that DiGGR consistently outperforms many previous   \n4 self-supervised methods, verifying the effectiveness of the proposed approach. ", "page_idx": 0}, {"type": "text", "text": "15 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "16 Self-supervised learning (SSL) has received much attention due to its appealing capacity for learning   \n17 data representation without label supervision. While contrastive SSL approaches are becoming   \n18 increasingly utilized on images [Chen et al., 2020] and graphs [You et al., 2020], generative SSL has   \n19 been gaining significance, driven by groundbreaking practices such as BERT for language [Devlin   \n20 et al., 2018], BEiT [Bao et al., 2021], and MAE [He et al., 2022a] for images. Along this line, there is   \n21 a growing interest in constructing generative SSL models for other modalities, such as graph masked   \n22 autoencoders (GMAE). Generally, the fundamental concept of GMAE [Tan et al., 2022] is to utilize   \n23 an autoencoder architecture to reconstruct input node features, structures, or both, which are randomly   \n24 masked before the encoding step. Recently, various well-designed GMAEs have emerged , achieving   \n25 remarkable results in both node classification and graph classification [Hou et al., 2022, Tu et al.,   \n26 2023, Tian et al., 2023].   \n27 Despite their significant achievements, most GMAE approaches typically treat the entire graph as   \n28 holistic, ignoring the graph\u2019s latent structure. As a result, the representation learned for a node tends   \n29 to encapsulate the node\u2019s neighborhood as a perceptual whole, disregarding the nuanced distinctions   \n30 between different parts of the neighborhood [Ma et al., 2019, Li et al., 2021, Mo et al., 2023].   \n31 For example, in a social network $G$ , individual $n$ is a member of both a mathematics group and   \n32 several sports interest groups. Due to the diversity of these different communities, she may exhibit   \n33 different characteristics when interacting with members from various communities. Specifically,   \n34 the information about the mathematics group may be related to her professional research, while the   \n35 information about sports clubs may be associated with her hobbies. However, the existing approach   \n36 overlooks the heterogeneous factors of node $n$ , failing to identify and disentangle these pieces of ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "hiwHaqFXGi/tmp/29d68d3511fe66f73e41c302a6afa4dcd6ace11caa4a9d367ce7aceeae58ede1.jpg", "img_caption": ["(a) Applying previous methods to GMAE ", "(b) Latent factor learning "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: The number of latent factors is set to 4. In Fig. 1(a), the probabilities of nodes belonging to different latent groups are similar, resulting in nodes of the same type being incorrectly assigned to different factors. In contrast, Fig. 1(b) shows that the probabilities of node-factor affiliation are more discriminative, correctly categorizing nodes of the same type into the same latent group. ", "page_idx": 1}, {"type": "text", "text": "37 information effectively [Hou et al., 2022]. Consequently, the learned features may be easily influenced   \n38 by irrelevant factors, resulting in poor robustness and difficulty in interpretation.   \n39 To alleviate the challenge described above, there is an increasing interest in disentangled graph   \n40 representation learning [Bengio et al., 2013, Li et al., 2021, Ma et al., 2019, Mo et al., 2023, Xiao   \n41 et al., 2022], which aims at acquiring representations that can disentangle the underlying explanatory   \n42 factors of variation in the graph. Specifically, many of these methods rely on a latent factor detection   \n43 module, which learns the latent factors of each node by comparing node representations with various   \n44 latent factor prototypes. By leveraging these acquired latent factors, these models adeptly capture   \n45 factor-wise graph representations, effectively encapsulating the latent structure of the graph. Despite   \n46 significant progress, few studies have endeavored to adapt these methods to to generative graph   \n47 representation learning methods, such as GMAE. This primary challenge arises from the difficulty of   \n48 achieving convergence in the latent factor detection module under the generative training target, thus   \n49 presenting obstacles in practical implementation. As shown in Fig.1(a), directly applying the previous   \n50 factor learning method to GMAE would make the factor learning module difficult to converge,   \n51 resulting in undistinguished probabilities and misallocation of similar nodes to different latent factor   \n52 groups.   \n53 To address these challenges, we introduce Disentangled Generative Graph Representation Learning   \n54 (DiGGR), a self-supervised graph generation representation learning framework. Generally speaking,   \n55 DiGGR learns how to generate graph structures from latent disentangle factors $z$ and leverages this to   \n56 guide graph mask reconstruction, while enabling end-to-end joint learning. Specifically, $i$ ) To capture   \n57 the heterogeneous factors in the nodes, we introduce the latent factor learning module. This module   \n58 models how edges and nodes are generated from latent factors, allowing graphs to be factorized into   \n59 multiple disentangled subgraphs. ii) To learn a deeper disentangled graph representation, we design a   \n60 factor-wise self-supervised graph representation learning framework. For each subgraph, we employ   \n61 a distinct masking strategy to learn an improved factor-specific graph representation. Evaluation   \n62 shows that the proposed framework can achieve significant performance enhancement on various   \n63 node and graph classification benchmarks. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "64 The main contributions of this paper can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "65 \u2022 We utilized the latent disentangled factor to guide mask modeling. A probabilistic graph   \n66 generation model is employed to identify the latent factors within a graph, and it can be   \n67 jointly trained with GMAE through variational inference.   \n68 \u2022 Introducing DiGGR (Disentangled Generative Graph Representation Learning) to further   \n69 capture the disentangled information in the latent factors, enhancing the disentanglement of   \n70 the learned node representations.   \n71 \u2022 Empirical results show that the proposed DiGGR outperforms many previous self-supervised   \n72 methods in various node- and graph-level classification tasks. ", "page_idx": 1}, {"type": "text", "text": "73 2 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "74 Graph Self-Supervised Learning: Graph SSL has achieved remarkable success in addressing label   \n75 scarcity in real-world network data, mainly consisting of contrastive and generative methods. Con  \n76 trastive methods, includes feature-oriented approaches[Hu et al., 2019, Zhu et al., 2020, Velic\u02c7kovic\u00b4   \n77 et al., 2018], proximity-oriented techniques [Hassani and Khasahmadi, 2020, You et al., 2020], and   \n78 graph-sampling-based methods [Qiu et al., 2020]. A common limitation across these approaches   \n79 is their heavy reliance on the design of pretext tasks and augmentation techniques. Compared to   \n80 contrastive methods, generative methods are generally simpler to implement. Recently, to tackle the   \n81 challenge of overemphasizing neighborhood information at the expense of structural information   \n82 [Hassani and Khasahmadi, 2020, Velic\u02c7kovic\u00b4 et al., 2018], the Graph Masked Autoencoder (GMAE)   \n83 has been proposed. It applies a masking strategy to graph structure [Li et al., 2023a], node attributes   \n84 [Hou et al., 2022], or both [Tian et al., 2023] for representation learning. Unlike most GMAEs, which   \n85 employ random mask strategies, this paper builds disentangled mask strategies.   \n86 Disentangled Graph Learning: Disentangled representation learning aims to discover and isolate   \n87 the fundamental explanatory factors inherent in the data [Bengio et al., 2013]. Existing efforts in   \n88 disentangled representation learning have primarily focused on computer vision [Higgins et al.,   \n89 2017, Jiang et al., 2020]. Recently, there has been a surge of interest in applying these techniques   \n90 to graph-structured data [Li et al., 2021, Ma et al., 2019, Mercatali et al., 2022, Mo et al., 2023].   \n91 For example, DisenGCN [Ma et al., 2019] utilizes an attention-based methodology to discriminate   \n92 between distinct latent factors, enhancing the representation of each node to more accurately reflect   \n93 its features across multiple dimensions. DGCL [Li et al., 2021] suggests learning disentangled   \n94 graph-level representations through self-supervision, ensuring that the factorized representations   \n95 independently capture expressive information from various latent factors. Despite the excellent results   \n96 achieved by the aforementioned methods on various tasks, these methods are difficult to converge   \n97 in generative graph SSL, as we demonstrated in the experiment of Table.3. Therefore, this paper   \n98 proposes a disentangled-guided framework for generative graph representation learning, capable of   \n99 learning disentangled representations in an end-to-end self-supervised manner. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "100 3 Proposed Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "101 In this section, we propose DiGGR (Disentangled Generative Graph Representation Learning) for   \n102 self-supervised graph representation learning with mask modeling. The framework was depicted   \n103 in Figure 2, comprises three primary components: Latent Factor Learning (Section 3.2), Graph   \n104 Factorization (Section 3.2) and Disentangled Graph Masked autoencder (Section 3.3). Before   \n105 elaborating on them, we first show some notations. ", "page_idx": 2}, {"type": "text", "text": "106 3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "107 A graph $G$ can be represented as a multi-tuple ${\\mathcal{G}}=\\{V,A,X\\}$ with $N$ nodes and $M$ edges, where   \n108 $|V|\\,=\\,N$ is the node set, $|A|\\,=\\,M$ is the edge set, and $\\boldsymbol{X}\\,\\in\\,\\mathbb{R}^{N\\times L}$ is the feature matrix for $N$   \n109 nodes with $L$ dimensional feature vector. The topology structure of graph $G$ can be found in its   \n110 adjacency matrix $A\\,\\in\\,\\mathbb{R}^{N\\times N}$ . $z\\,\\in\\,\\mathbb{R}^{N\\times K}$ is the latent disentangled factor matrix, and $K$ is the   \n111 predefined factor number. Since we aim to obtain the $z$ to guide the mask modeling, we first utilize a   \n112 probabilistic graph generation model to factorize the graph before employing the mask mechanism.   \n113 Given the graph $G$ , it is factorized into $\\left\\{G_{1},G_{2},...,\\bar{G_{K}}\\right\\}$ , and each factor-specific graph $G_{k}$ consists   \n114 of its factor-specific edges $A^{(k)}$ , node set $V^{(k)}$ and node feature matrix $X^{(k)}$ . Other notations will be   \n115 elucidated as they are employed. ", "page_idx": 2}, {"type": "text", "text": "116 3.2 Latent Factor Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "117 In this subsection, we describe the latent factor learning method. In this phase, our objective is to   \n118 derive factor-specific node sets $\\{V^{(1)},V^{(2)},...,V^{(K)}\\}$ and adjacency matrices $\\{A^{(1)},A^{(2)},...,A^{(K)}\\},$ ,   \n119 serving as basic unit of the graph to guide the subsequent masking. The specific approach involves   \n120 modeling the distribution of nodes and edges, utilizing the generative process developed in EPM   \n121 [Zhou, 2015]. The generative process of EPM under the Bernoulli-Poisson link [Zhou, 2015] can be   \n122 described as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{M}_{u v}\\sim\\operatorname{Poisson}(\\sum_{k=1}^{K}\\gamma_{k}z_{u k}z_{v k}),\\ \\ z_{u k}\\sim\\operatorname{Gamma}\\left(\\alpha,\\beta\\right),u,v\\in\\left[1,N\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "123 where $K$ is the predefined number of latent factors, and $u$ and $v$ are the indexes of the nodes. Here,   \n124 ${{\\mathrm{\\mathbf{M}}}_{u v}}$ is the latent count variable between node $u$ and $v$ ; $\\gamma_{k}$ is a positive factor activation level indicator,   \n125 which measures the node interaction frequency via factor $k$ ; $z_{u k}$ is a positive latent variable for node   \n126 $u$ , which measures how strongly node $u$ is affiliated with factor $k$ . The prior distribution of latent   \n127 factor variable $z_{u k}$ is set to Gamma distribution, where $\\alpha$ and $\\beta$ are normally set to 1. Therefore, the   \n128 intuitive explanation for this generative process is that, with $z_{u k}$ and $z_{v k}$ measuring how strongly   \n129 node $u$ and $v$ are affiliated with the $k$ -th factor, respectively, the product $\\gamma_{k}z_{u k}z_{v k}$ measures how   \n130 strongly nodes $u$ and $v$ are connected due to their affiliations with the $k$ -th factor. ", "page_idx": 2}, {"type": "image", "img_path": "hiwHaqFXGi/tmp/80b5254053718059e2d00a7a97cd25dc62c12fd703b8a6e90b75c1338653ca61.jpg", "img_caption": ["Figure 2: The overview of proposed DiGGR\u2019s computation graph. The input data successively passes three modules described in Sections 3.2 and 3.3: Latent Factor Learning, Graph Factorization, and Disentangled Graph Mask Autoencoder. Graph information will be first processed through Latent Factor Learning and Graph Factorization, the former processed the input graph to get the latent factor $z$ ; the latter performs graph factorization via $z$ , such that in each factorized subgraph, nodes exchange more information with intensively interacted neighbors. Hence, during the disentangled graph masking phase, we will individually mask each factorized subgraph to enhance the disentanglement of the obtained node representations. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "131 Node Factorization: Equation 1 can be further augmented as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{M}_{u v}=\\sum_{k}^{K}\\mathbf{M}_{u k v},\\ \\mathbf{M}_{u k v}\\sim\\mathrm{Poisson}\\left(\\gamma_{k}z_{u k}z_{v k}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "132 where $\\mathbf{M}_{u k v}$ represents how often nodes $u$ and $v$ interact due to their affliiations with the $k$ -th factor.   \n133 To represent how often node $u$ is affliiated with the $k$ -th factor, we further introduce the latent count   \n134 $\\begin{array}{r}{{\\bf M}_{u k.}=\\sum_{v\\neq u}{\\bf M}_{u k v}}\\end{array}$ . Then, we can soft assign node $u$ to multiple factors in $\\{k:\\mathbf{M}_{u k}.\\}\\geq1$ , or   \n135 hard assign node $u$ to a single factor using arg max $\\left(\\mathbf{M}_{u k}.\\right)$ . However, our experiments show that   \nk   \n136 soft assignment method results in significant overlap among node sets from different factor group,   \n137 diminishing the distinctiveness. Note that previous study addressed a similar issue by selecting the   \n138 top- $\\cdot\\mathbf{k}$ most attended regions [Kakogeorgiou et al., 2022]. Thus, we choose the hard assign strategy to   \n139 factorize the graph node set $V$ graph into factor-specific node sets $\\{V^{(1)},V^{(2)},\\cdot\\cdot\\cdot\\;,V^{(K)}\\}$ .   \n140 Edge Factorization: To create factor-specific edges $A^{(k)}$ for a factor-specific node set $V^{(k)}$ , a   \n141 straightforward method involves removing all external nodes connected to other factor groups. This   \n142 can be defined as: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nA_{u v}^{(k)}=\\left\\{\\begin{array}{l l}{A_{u v},\\,\\forall\\,u,v\\in V^{(k)};\\,u,v\\in[1,N]\\,;}\\\\ {0,\\,\\,\\,\\exists\\,u,v\\not\\in V^{(k)};\\,u,v\\in[1,N]\\,.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "143 Besides, the global graph edge $A$ can also be factorized into positive-weighted edges [He et al.,   \n144 2022b] for each latent factor as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nA_{u v}^{(k)}=A_{u v}\\cdot\\frac{\\exp\\left(\\gamma_{k}z_{u k}z_{v k}\\right)}{\\sum_{k^{\\prime}}\\exp\\left(\\gamma_{k^{\\prime}}z_{u k^{\\prime}}z_{v k^{\\prime}}\\right)};\\;\\;k\\in\\left[1,K\\right],u,v\\in\\left[1,N\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "145 Applying Equation 4 to all pairs of nodes yields weighted adjacency matrices $\\{A^{(k)}\\}_{k=1}^{k}$ }kk=1, with A(k)   \n146 corresponding to latent factor $z_{k}$ . Note that $A^{(k)}$ has the same dimension as $A$ and Equation 4   \n147 presents a trainable weight for each edge, which can be jointly optimized through network training,   \n148 showcasing an advantage over Equation 3 in this aspect. Therefore, we apply Equation 4 for edge   \n149 factorization.   \n150 Variational Inference: The latent factor variable $z$ determines the quality of node and edge factor  \n151 ization, so we need to approximate its posterior distribution. Denoting $\\bar{z_{u}}=(z_{u1},...,z_{u K}),\\bar{z_{u}}\\in\\mathbb{R}_{+}^{K}$ ,   \n152 which measures how strongly node $u$ is affiliated with all the $K$ latent factors, we adopt a Weibull   \n153 variational graph encoder [Zhang et al., 2018, He et al., 2022b]: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nq(z_{u}\\mid A,X)=\\operatorname{Weibull}(k_{u},\\lambda_{u}),\\;\\;(k_{u},\\lambda_{u})=\\operatorname{GNN}_{\\operatorname{EPM}}(A,X),\\quad u\\in[1,N]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "154 where ${\\mathrm{GNN}}_{\\mathrm{EPM}}(\\cdot)$ stands for graph neural networks, and we select a two-layer Graph Convolution   \n155 Networks (i.e., GCN [Kipf and Welling, 2016a]) for our models; $k_{u},\\lambda_{u}\\in\\mathbb{R}_{+}^{K}$ are the shape and   \n156 scale parameters of the variational Weibull distribution, respectively. The latent variable $z_{u}$ can be   \n157 conveniently reparameterized as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nz_{u}=\\lambda_{u}(-\\ln(1-\\varepsilon))^{1/k_{u}},\\ \\varepsilon\\sim{\\mathrm{Uniform}}(0,1).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "158 The optimization objective of latent factor learning phase can be achieved by maximizing the evidence   \n159 lower bound (ELBO) of the log marginal likelihood of edge $\\log p(A)$ , which can be computed as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathrm{z}}=\\mathbb{E}_{q(Z\\mid A,X)}\\left[\\ln p\\left(A\\mid Z\\right)\\right]-\\sum_{u=1}^{N}\\mathbb{E}_{q\\left(z_{u}\\mid A,X\\right)}\\left[\\ln{\\frac{q(z_{u}\\mid A,X)}{p(z_{u})}}\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "160 where the first term is the expected log-likelihood or reconstruction error of edge, and the second   \n161 term is the Kullback\u2013Leibler (KL) divergence that constrains $q(z_{u})$ to be close to its prior $p(z_{u})$ . The   \n162 analytical expression for the KL divergence and the straightforward reparameterization of the Weibull   \n163 distribution simplify the gradient estimation of the ELBO concerning the decoder parameters and   \n164 other parameters in the inference network. ", "page_idx": 4}, {"type": "text", "text": "165 3.3 Disentangled Grpah Masked Autoencoder ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "166 With the latent factor learning phase discussed in 3.2, the graph can be factorized into a series of   \n167 factor-specific subgraphs $\\{\\bar{G_{1}},\\bar{G}_{2},...,G_{K}\\}$ via the latent factor $z$ . To incorporate the disentangled   \n168 information encapsulated in $z$ into the graph masked autoencoder, we proposed Disentangled Graph   \n169 Masked Autoencoder in this section. Specifically, this section will first introduce the latent factor-wise   \n170 GMAE and the graph-level GMAE. ", "page_idx": 4}, {"type": "text", "text": "171 3.3.1 Latent Factor-wise Grpah Masked Autoencoder ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "172 To capture disentangled patterns within the latent factor $z$ , for each latent subgraph   \n173 $\\mathcal{G}_{k}=\\left(\\bar{V}^{(k)},A^{(k)},X^{(\\bar{k})}\\right)$ , the latent factor-wise GMAE can be described as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nH_{d}^{(k)}=\\operatorname{GNN}_{\\operatorname{enc}}(A^{(k)},{\\bar{X}}^{(k)}),{\\tilde{X}}^{d}=\\operatorname{GNN}_{\\operatorname{dec}}(A,H_{d}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "174 where $\\bar{X}^{(k)}$ is the masked node feature matrix for the $k$ -th latent factor, and ${\\tilde{X}}^{d}$ denotes the recon  \n175 structed node features. $\\mathrm{GNN}_{\\mathrm{enc}}(.)$ and $\\mathrm{GNN}_{\\mathrm{dec}}(.)$ are the graph encoder and decoder, respectively;   \n176 $H_{d}^{(k)}\\in\\mathbb{R}^{N\\times D}$ are factor-wise hidden representations, and $\\dot{H_{d}}=H_{d}^{(1)}\\oplus H_{d}^{(2)}\\cdot\\cdot\\cdot\\oplus H_{d}^{(K)}$ . After the   \n177 concatenation operation $\\bigoplus$ in feature dimension, the multi factor-wise hidden representation becomes   \n178 $H_{d}\\in\\mathbb{R}^{N\\times(K\\cdot\\bar{D})}$ , which is used as the input of $\\mathrm{GNN}_{\\mathrm{dec}}(.)$ .   \n179 Regarding the mask opeartion, we uniformly random sample a subset of nodes ${\\bar{V}}^{(k)}\\in V^{(k)}$ and   \n180 mask each of their features with a mask token, such as a learnable vector $X_{[M]}\\in\\mathbb{R}^{d}$ . Thus, the node   \n181 feature in the masked feature matrix can be defined as: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bar{X}_{i}^{(k)}=\\left\\{\\begin{array}{l l}{{X_{[M]};\\,\\,\\,v_{i}\\in\\bar{V}^{(k)}\\,\\,\\,;}}\\\\ {{\\,\\,X_{i}\\,\\,\\,\\,\\,;\\,\\,v_{i}\\not\\in\\bar{V}^{(k)}.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "182 The objective of latent factor-wise GMAE is to reconstruct the masked features of nodes in ${\\bar{V}}^{(k)}$   \n183 given the partially observed node signals $\\bar{X}^{(k)}$ and the input adjacency matrix $A^{(k)}$ . Another crucial   \n184 component of the GMAE is the feature reconstruction criterion, often used in language as cross  \n185 entropy error [Devlin et al., 2018] and in the image as mean square error [He et al., 2022a]. However,   \n186 texts and images typically involve tokenized input features, whereas graph autoencoders (GAE) do   \n187 not have a universal tokenizer. We adopt the scored cosine error of GraphMAE [Hou et al., 2022] as   \n188 the loss function. Generally, given the original feature $X^{(k)}$ and reconstructed node feature $\\tilde{X}^{(k)}$ , the   \n189 defined SCE is: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{D}}=\\frac{1}{|\\bar{V}|}\\!\\sum_{i\\in\\bar{V}}\\left(1-\\frac{X_{i}^{T}\\tilde{X}_{i}^{d}}{\\Vert X_{i}\\Vert\\cdot\\Vert\\tilde{X}_{i}^{d}\\Vert}\\right)^{\\gamma},~\\gamma\\geq1\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "190 where $\\bar{V}=\\bar{V}^{(1)}\\cup\\bar{V}^{(2)}...\\cup\\bar{V}^{(K)}$ and Equation 10 are averaged over all masked nodes.The scaling   \n191 factor $\\gamma$ is a hyper-parameter adjustable over different datasets. This scaling technique could also   \n192 be viewed as adaptive sample reweighting, and the weight of each sample is adjusted with the   \n193 reconstruction error. This error is also famous in the field of supervised object detection as the focal   \n194 loss [Lin et al., 2017].   \n195 Graph-level Graph Mask Autoencoder: For the node classification task, we have integrated   \n196 graph-level GMAE into DiGGR. We provide a detailed experimental analysis and explanation for   \n197 this difference in Appendix A.1.2. The graph-level masked graph autoencoder is designed with the   \n198 aim of further capturing the global patterns, which can be designed as: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nH_{g}={\\bf G N N}_{\\mathrm{enc}}(A,\\bar{X}),\\;\\;\\tilde{X}^{g}={\\bf G N N}_{\\mathrm{dec}}(A,H_{g}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "199 $\\bar{X}$ is the masked node feature matrix, whose mask can be generated by uniformly random sampling   \n200 a subset of nodes $\\tilde{V}\\,\\in\\,V$ , or obtained by concatenating the masks of all factor-specific groups   \n201 $\\tilde{V}=\\bar{V}^{(1)}\\cup\\bar{V}^{(2)}...\\cup\\bar{V}^{(K)}$ . The global hidden representation encoded by $\\operatorname{GNNenc}(.)$ is $H_{g}$ , which is   \n202 then passed to the decoder. Similar to Equation 10, we can define the graph-level reconstruct loss as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{G}}=\\frac{1}{|\\tilde{V}|}{\\sum_{i\\in\\tilde{V}}{(1-\\frac{X_{i}^{T}\\tilde{X}_{i}^{g}}{\\left\\|X_{i}\\right\\|\\cdot\\left\\|\\tilde{X}_{i}^{g}\\right\\|})^{\\gamma}}},\\,\\,\\,\\gamma\\geq1.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "203 which is averaged over all masked nodes. ", "page_idx": 5}, {"type": "text", "text": "204 3.4 Joint Training and Inference ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "205 Benefiting from the effective variational inference method, the proposed latent factor learning and   \n206 dsientangled graph masked autoencoder can be jointly trained in one framework. We combine the   \n207 aforementioned losses with three mixing coefficient $\\lambda_{d},\\lambda_{g}$ and $\\lambda_{z}$ during training, and the loss for   \n208 joint training can be written as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\lambda_{d}\\cdot\\mathcal{L}_{\\mathrm{D}}+\\lambda_{g}\\cdot\\mathcal{L}_{\\mathrm{G}}+\\lambda_{z}\\cdot\\mathcal{L}_{\\mathrm{z}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "209 Since Weibull distributions have easy reparameterization functions, these parameters can be jointly   \n210 trained by stochastic gradient descent with low-variance gradient estimation. We summarize the   \n211 training algorithm at Algorithm 1 in Appendix A.4. For downstream applications, the encoder is   \n212 applied to the input graph without any masking in the inference stage. The generated factor-wise   \n213 node embeddings $H_{d}$ and graph-level embeddings $H_{g}$ can either be concatenated in the feature   \n214 dimensions or used separately. The resulting final representation $H$ can be employed for various   \n215 graph learning tasks, such as node classification and graph classification. For graph-level tasks, we   \n216 use a non-parameterized graph pooling (readout) function, e.g., MaxPooling and MeanPooling to   \n217 obtain the graph-level representation.   \n218 Time and space complexity: Let\u2019s recall that in our context, $N,M$ , and $K$ represent the number of   \n219 nodes, edges, and latent factors in the graph, respectively. The feature dimension is denoted by $F$ ,   \n220 while $L_{1}$ , $L_{2}$ , $L_{3}$ , and $L_{4}$ represent the number of layers in the latent factor learning encoder, the   \n221 latent factor-wise GMAE\u2019s encoder, the graph-level GMAE\u2019s encoder, and the decoder respectively.   \n222 In DiGGR, we constrain the hidden dimension size in latent factor-wise GMAE\u2019s encoder to be   \n223 $1/K$ of the typical baseline dimensions. Consequently, the time complexity for training DiGGR can   \n224 be expressed as $\\mathcal{O}((L_{1}+L_{2}+L_{3})M F+(L_{1}^{'}+L_{2}^{'}/K+L_{3})N F^{2}+\\dot{N}^{2}F+L_{4}\\bar{N}F^{2}),$ and the   \n225 space complexity is $\\begin{array}{r}{:O((L_{1}+L_{2}+L_{3}+L_{4})N F+K M+(L1+L2/K+L3+L_{4})F^{2}}\\end{array}$ ), with   \n226 $\\bar{O}((L_{1}+\\bar{L_{2}}/K\\,\\mathrm{\\scriptsize~\\dot{+}~}L_{3}+\\dot{L}_{4})F^{2})$ attributed to model parameters. We utilize the Bayesian factor model   \n227 in our approach to reconstruct edges. Its time complexity aligns with that of variational inference   \n228 in SeeGera Li et al. [2023b], predominantly at $O(\\bar{N}^{2}F)$ ; Therefore, the complexity of DiGGR is   \n229 comparable to previous works. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "hiwHaqFXGi/tmp/8fc6a51c6e3fd2e522646d0bf28acd22fe1bae572ade9a196954841e2919e736.jpg", "table_caption": ["Table 1: Experiment results for node classification. Micro-F1 score is reported for PPI, and accuracy for other datasets. The best unsupervised method scores in each dataset are highlighted in bold. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "230 4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "231 We compare the proposed self-supervised framework DiGGR against related baselines on two funda  \n232 mental tasks: unsupervised representation learning on node classification and graph classification.   \n233 We evaluate DiGGR on 11 benchmarks. For node classification, we use 3 citation networks (Cora,   \n234 Citeseer, Pubmed [Yang et al., 2016]), and protein-protein interaction networks (PPI) [Hamilton et al.,   \n235 2017]. For graph classification, we use 3 bioinformatics datasets (MUTAG, NCI1, PROTEINS) and 4   \n236 social network datasets (IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY and COLLAB). The   \n237 specific information of the dataset and the hyperparameters used by the network are listed in the   \n238 Appendix A.2 in table 5 and 6. We also provide the detailed experiment setup in Appendix A.2 for   \n239 node classification (4.1) and graph classification (4.2) ", "page_idx": 6}, {"type": "text", "text": "240 4.1 Node Classification ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "241 The baseline models for node classification can be divided into three categories: i) supervised methods,   \n242 including GCN [Kipf and Welling, 2016a] , DisenGCN[Ma et al., 2019], VEPM[He et al., 2022b]   \n243 and GAT [Velickovic et al., 2017]; $i i$ ) contrastive learning methods, including MVGRL [Hassani and   \n244 Khasahmadi, 2020], InfoGCL [Xu et al., 2021], DGI [Veli\u02c7ckovi\u00b4c et al., 2018], GRACE [Zhu et al.,   \n245 2020], BGRL [Thakoor et al., 2021] and CCA-SSG [Zhang et al., 2021]; iii) generative learning   \n246 methods, including GraphMAE [Hou et al., 2022], GraphMAE2[Hou et al., 2023], Bandana[Zhao   \n247 et al., 2024], GiGaMAE[Shi et al., 2023], SeeGera[Li et al., 2023b], GAE and VGAE [Kipf and   \n248 Welling, 2016b]. The node classification results were listed in Table 1. DiGGR demonstrates   \n249 competitive results on the provided dataset, achieving results comparable to those of supervised   \n250 methods. ", "page_idx": 6}, {"type": "text", "text": "251 4.2 Graph Classification ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "252 Baseline Models We categorized the baseline models into four groups: i) supervised methods,   \n253 including GIN [Xu et al., 2018], DiffPool[Ying et al., 2018] and VEPM[He et al., 2022b]; ii) classical   \n254 graph kernel methods: Weisfeiler-Lehman sub-tree kernel (WL) [Shervashidze et al., 2011] and   \n255 deep graph kernel (DGK) [Yanardag and Vishwanathan, 2015]; iii) contrastive learning methods,   \n256 including GCC [Qiu et al., 2020], graph2vec [Narayanan et al., 2017], Infograph [Sun et al., 2019],   \n257 GraphCL [You et al., 2020], JOAO [You et al., 2021], MVGRL [Hassani and Khasahmadi, 2020],   \n258 and InfoGCL [Xu et al., 2021]; 4) generative learning methods, including graph2vec [Narayanan   \n259 et al., 2017], sub2vec [Adhikari et al., 2018], node2vec [Grover and Leskovec, 2016], GraphMAE   \n260 [Hou et al., 2022], GraphMAE2[Hou et al., 2023], GAE and VGAE [Kipf and Welling, 2016b]. Per   \n261 graph classification research tradition, we report results from previous papers if available.   \n262 Performance Comparison The graph classification results are presented in Table 2. In general, we   \n263 find that DiGGR gained the best performance among other baselines on five out of seven datasets,   \n264 while achieving competitive results on the other two datasets. The performance of DiGGR is   \n265 comparable to that of supervised learning methods. For instance, the accuracy on IMDB-B and   \n266 IMDB-M surpasses that of GIN and DiffPool. Moreover, within the reported datasets, our method   \n267 demonstrates improved performance compared to random mask methods like GraphMAE, particularly   \n268 on the IMDB-M, COLLAB, and PROTEINS datasets. This underscores the effectiveness of the   \n269 proposed method. ", "page_idx": 6}, {"type": "table", "img_path": "hiwHaqFXGi/tmp/3d30997751c8b0487909370cb122fbcd07ac9786d948f6fb2912808edb6d71a2.jpg", "table_caption": ["Table 2: Experiment results in unsupervised representation learning for graph classification. We report accuracy $(\\%)$ for all datasets. The optimal outcomes for methods, excluding supervised approaches (GIN and DiffPool), on each dataset are emphasized in bold. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "270 4.3 Exploratory Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "271 Visualizing latent representations To examine the influence of the learned latent factor on classifi  \n272 cation results, we visualized the latent disentangled factor $z$ , which reflects the node-factor affliiation,   \n273 and the hidden representation $H$ used for classification. MUTAG is selected as the representative   \n274 for classification benchmarks. We encodes the representations into 2-D space via t-SNE [Van der   \n275 Maaten and Hinton, 2008]. The result is shown in Figure 3(a), where each node is colored according   \n276 to its node labels. The clusters in Figure 3(a) still exhibit differentiation in the absence of label   \n277 supervision, suggesting that $z$ obtained through unsupervised learning can enhance node information   \n278 and offer a guidance for the mask modeling. We then visualize the hidden representation used for   \n279 classification tasks, and color each node according to the latent factor to which it belongs. The results   \n280 are depicted in Figure 3(b), showcasing separability among different color clusters. This illustrates   \n281 the model\u2019s ability to extract information from the latent factor, thereby enhancing the quality of the   \n282 learned representations.   \n283 Task-relevant factors To assess the statistical correlation between the learned latent factor and the   \n284 task, we follow the approach in [He et al., 2022b] and compute the Normalized Mutual Information   \n285 (NMI) between the nodes in the factor label and the actual node labels. NMI is a metric that ranges   \n286 from 0 to 1, where higher values signify more robust statistical dependencies between two random   \n287 variables. In the experiment, we utilized the MUTAG dataset, comprising 7 distinct node types,   \n288 and the NMI value we obtained was 0.5458. These results highlight that the latent factors obtained   \n289 through self-supervised training are meaningful for the task, enhancing the correlation between the   \n290 inferred latent factors and the task.   \n291 Disentangled representations To assess DiGGR\u2019s capability to disentangle the learned represen  \n292 tation for downstream task, we provide a qualitative evaluation by plotting the correlation of the   \n293 node representation in Figure 4. The figure shows the absolute values of the correlation between the   \n294 elements of 512-dimensional graph representation and representation obtained from GraphMAE and   \n295 DiGGR, respectively. From the results, we can see that the representation produced by GraphMAE   \n296 exhibits entanglement, whereas DiGGR\u2019s representation displays a overall block-level pattern,   \n297 indicating that DiGGR can capture mutually exclusive information in the graph and disentangle the   \n298 hidden representation to some extent. Results for more datasets can be found in Appendix A.3.   \n299   \n300 Why DiGGR works better: To validate that disentangled learning can indeed enhance the quality of   \n301 the representations learned by GMAE, we further conduct quantitative experiments. The Normalized   \n302 Mutual Information (NMI) is used to quantify the disentangling degree of different datasets. Generally,   \n303 the NMI represents the similarity of node sets between different factor-specific graphs, and the lower   \n304 NMI suggests a better-disentangled degree with lower similarity among factor-specific graphs. The   \n305 NMI between latent factors and the corresponding performance gain (compared to GraphMAE)   \n306 are shown in the Table.3. As the results show, DiGGR\u2019s performance improvement has a positive   \n307 correlation with disentangled degree, where the better the disentangled degree, the more significant   \n308 the performance improvement. For methods relying on Non-probabilistic Factor Learning, the NMI   \n309 tends to approach 1. This is attributed to the challenges faced by the factor learning module in   \n310 converging, thereby hindering the learning of distinct latent factors. The presence of confused latent   \n311 factors offers misleading guidance for representation learning, consequently leading to decreased   \n312 performance. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "hiwHaqFXGi/tmp/a0bdb4ac28117cd2d04f3dcc505c305933dee45f3383a62c96c805e7daf5ec2e.jpg", "img_caption": ["Figure 3: T-SNE visualization of MUTAG dataset, where $z$ is the latent factor, $H$ is the learned node representation used for downstream tasks. ", "Figure 4: representation correlation matrix on Cora with number of factors $K\\,=\\,4$ . 4(a) depicts the representation of entanglement, while 4(b) illustrates disentanglement. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "hiwHaqFXGi/tmp/e754c4ed604706d35ba6a040f25d39385c05b0132b5dbec6324f782d14ef0afc.jpg", "table_caption": ["Table 3: The NMI between the latent factors extracted by DiGGR and Non-probabilistic factor learning method across various datasets, and its performance improvement compared to GraphMAE, are examined. A lower NMI indicates a more pronounced disentanglement between factor-specific graphs, resulting in a greater performance enhancement. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "313 5 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "314 In this paper, we propose DiGGR (Disentangled Generative Graph Representation Learning), de  \n315 signed to achieve disentangled representations in graph masked autoencoders by leveraging latent   \n316 disentangled factors. In particular, we achieve this by two steps: 1) We utilize a probabilistic graph   \n317 generation model to factorize the graph via the learned disentangled latent factor; 2) We develop a   \n318 Disentangled Graph Masked Autoencoder framework, with the aim of integrating the disentangled in  \n319 formation into the representation learning of Graph Masked Autoencoders. Experiments demonstrate   \n320 that our model can acquire disentangled representations, and achieve favorable results on downstream   \n321 tasks. ", "page_idx": 8}, {"type": "text", "text": "322 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "323 Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for   \n324 contrastive learning of visual representations. In International conference on machine learning,   \n325 pages 1597\u20131607. PMLR, 2020.   \n326 Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph   \n327 contrastive learning with augmentations. Advances in neural information processing systems, 33:   \n328 5812\u20135823, 2020.   \n329 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep   \n330 bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n331 Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers.   \n332 arXiv preprint arXiv:2106.08254, 2021.   \n333 Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked   \n334 autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer   \n335 vision and pattern recognition, pages 16000\u201316009, 2022a.   \n336 Qiaoyu Tan, Ninghao Liu, Xiao Huang, Rui Chen, Soo-Hyun Choi, and Xia Hu. Mgae: Masked   \n337 autoencoders for self-supervised learning on graphs. arXiv preprint arXiv:2201.02534, 2022.   \n338 Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang.   \n339 GraphMAE: Self-supervised masked graph autoencoders. In Proceedings of the 28th ACM   \n340 SIGKDD Conference on Knowledge Discovery and Data Mining, pages 594\u2013604, 2022.   \n341 Wenxuan Tu, Qing Liao, Sihang Zhou, Xin Peng, Chuan Ma, Zhe Liu, Xinwang Liu, and Zhiping   \n342 Cai. Rare: Robust masked graph autoencoder. arXiv preprint arXiv:2304.01507, 2023.   \n343 Yijun Tian, Kaiwen Dong, Chunhui Zhang, Chuxu Zhang, and Nitesh V Chawla. Heterogeneous   \n344 graph masked autoencoders. In Proceedings of the AAAI Conference on Artificial Intelligence,   \n345 volume 37, pages 9997\u201310005, 2023.   \n346 Jianxin Ma, Peng Cui, Kun Kuang, Xin Wang, and Wenwu Zhu. Disentangled graph convolutional   \n347 networks. In International conference on machine learning, pages 4212\u20134221. PMLR, 2019.   \n348 Haoyang Li, Xin Wang, Ziwei Zhang, Zehuan Yuan, Hang Li, and Wenwu Zhu. Disentangled   \n349 contrastive learning on graphs. Advances in Neural Information Processing Systems, 34:21872\u2013   \n350 21884, 2021.   \n351 Yujie Mo, Yajie Lei, Jialie Shen, Xiaoshuang Shi, Heng Tao Shen, and Xiaofeng Zhu. Disentangled   \n352 multiplex graph representation learning. In International Conference on Machine Learning, pages   \n353 24983\u201325005. PMLR, 2023.   \n354 Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new   \n355 perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798\u20131828,   \n356 2013.   \n357 Teng Xiao, Zhengyu Chen, Zhimeng Guo, Zeyang Zhuang, and Suhang Wang. Decoupled self  \n358 supervised learning for graphs. Advances in Neural Information Processing Systems, 35:620\u2013634,   \n359 2022.   \n360 Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec.   \n361 Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019.   \n362 Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive   \n363 representation learning. arXiv preprint arXiv:2006.04131, 2020.   \n364 Petar Veli\u02c7ckovi\u00b4c, William Fedus, William L Hamilton, Pietro Li\u00f2, Yoshua Bengio, and R Devon   \n365 Hjelm. Deep graph infomax. arXiv preprint arXiv:1809.10341, 2018.   \n366 Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on   \n367 graphs. In International conference on machine learning, pages 4116\u20134126. PMLR, 2020.   \n368 Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang,   \n369 and Jie Tang. Gcc: Graph contrastive coding for graph neural network pre-training. In Proceedings   \n370 of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages   \n371 1150\u20131160, 2020.   \n372 Jintang Li, Ruofan Wu, Wangbin Sun, Liang Chen, Sheng Tian, Liang Zhu, Changhua Meng, Zibin   \n373 Zheng, and Weiqiang Wang. What\u2019s behind the mask: Understanding masked graph modeling   \n374 for graph autoencoders. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge   \n375 Discovery and Data Mining, pages 1268\u20131279, 2023a.   \n376 Irina Higgins, Loic Matthey, Arka Pal, Christopher P Burgess, Xavier Glorot, Matthew M Botvinick,   \n377 Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a   \n378 constrained variational framework. ICLR (Poster), 3, 2017.   \n379 Wentao Jiang, Si Liu, Chen Gao, Jie Cao, Ran He, Jiashi Feng, and Shuicheng Yan. Psgan: Pose   \n380 and expression robust spatial-aware gan for customizable makeup transfer. In Proceedings of the   \n381 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5194\u20135202, 2020.   \n382 Giangiacomo Mercatali, Andr\u00e9 Freitas, and Vikas Garg. Symmetry-induced disentanglement on   \n383 graphs. Advances in neural information processing systems, 35:31497\u201331511, 2022.   \n384 Mingyuan Zhou. Infinite edge partition models for overlapping community detection and link   \n385 prediction. In Artificial intelligence and statistics, pages 1135\u20131143. PMLR, 2015.   \n386 Ioannis Kakogeorgiou, Spyros Gidaris, Bill Psomas, Yannis Avrithis, Andrei Bursuc, Konstantinos   \n387 Karantzalos, and Nikos Komodakis. What to hide from your students: Attention-guided masked   \n388 image modeling. In European Conference on Computer Vision, pages 300\u2013318. Springer, 2022.   \n389 Yilin He, Chaojie Wang, Hao Zhang, Bo Chen, and Mingyuan Zhou. A variational edge partition   \n390 model for supervised graph representation learning. Advances in Neural Information Processing   \n391 Systems, 35:12339\u201312351, 2022b.   \n392 Hao Zhang, Bo Chen, Dandan Guo, and Mingyuan Zhou. Whai: Weibull hybrid autoencoding   \n393 inference for deep topic modeling. arXiv preprint arXiv:1803.01328, 2018.   \n394 Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.   \n395 arXiv preprint arXiv:1609.02907, 2016a.   \n396 Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense   \n397 object detection. In Proceedings of the IEEE international conference on computer vision, pages   \n398 2980\u20132988, 2017.   \n399 Xiang Li, Tiandi Ye, Caihua Shan, Dongsheng Li, and Ming Gao. Seegera: Self-supervised semi  \n400 implicit graph variational auto-encoders with masking. In Proceedings of the ACM web conference   \n401 2023, pages 143\u2013153, 2023b.   \n402 Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with   \n403 graph embeddings. In International conference on machine learning, pages 40\u201348. PMLR, 2016.   \n404 Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.   \n405 Advances in neural information processing systems, 30, 2017.   \n406 Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio,   \n407 et al. Graph attention networks. stat, 1050(20):10\u201348550, 2017.   \n408 Dongkuan Xu, Wei Cheng, Dongsheng Luo, Haifeng Chen, and Xiang Zhang. Infogcl: Information  \n409 aware graph contrastive learning. Advances in Neural Information Processing Systems, 34:   \n410 30414\u201330425, 2021.   \n411 Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Mehdi Azabou, Eva L Dyer, Remi   \n412 Munos, Petar Veli\u02c7ckovi\u00b4c, and Michal Valko. Large-scale representation learning on graphs via   \n413 bootstrapping. arXiv preprint arXiv:2102.06514, 2021.   \n414 Hengrui Zhang, Qitian Wu, Junchi Yan, David Wipf, and Philip S Yu. From canonical correlation   \n415 analysis to self-supervised graph neural networks. Advances in Neural Information Processing   \n416 Systems, 34:76\u201389, 2021.   \n417 Zhenyu Hou, Yufei He, Yukuo Cen, Xiao Liu, Yuxiao Dong, Evgeny Kharlamov, and Jie Tang.   \n418 Graphmae2: A decoding-enhanced masked self-supervised graph learner. In Proceedings of the   \n419 ACM Web Conference 2023, pages 737\u2013746, 2023.   \n420 Ziwen Zhao, Yuhua Li, Yixiong Zou, Jiliang Tang, and Ruixuan Li. Masked graph autoencoder with   \n421 non-discrete bandwidths. arXiv preprint arXiv:2402.03814, 2024.   \n422 Yucheng Shi, Yushun Dong, Qiaoyu Tan, Jundong Li, and Ninghao Liu. Gigamae: Generalizable   \n423 graph masked autoencoder via collaborative latent space reconstruction. In Proceedings of the 32nd   \n424 ACM International Conference on Information and Knowledge Management, pages 2259\u20132269,   \n425 2023.   \n426 Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308,   \n427 2016b.   \n428 Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural   \n429 networks? arXiv preprint arXiv:1810.00826, 2018.   \n430 Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hier  \n431 archical graph representation learning with differentiable pooling. Advances in neural information   \n432 processing systems, 31, 2018.   \n433 Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M   \n434 Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(9), 2011.   \n435 Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM   \n436 SIGKDD international conference on knowledge discovery and data mining, pages 1365\u20131374,   \n437 2015.   \n438 Annamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui Chen, Yang Liu,   \n439 and Shantanu Jaiswal. graph2vec: Learning distributed representations of graphs. arXiv preprint   \n440 arXiv:1707.05005, 2017.   \n441 Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semi  \n442 supervised graph-level representation learning via mutual information maximization. arXiv preprint   \n443 arXiv:1908.01000, 2019.   \n444 Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning automated.   \n445 In International Conference on Machine Learning, pages 12121\u201312132. PMLR, 2021.   \n446 Bijaya Adhikari, Yao Zhang, Naren Ramakrishnan, and B Aditya Prakash. Sub2vec: Feature   \n447 learning for subgraphs. In Advances in Knowledge Discovery and Data Mining: 22nd Pacific-Asia   \n448 Conference, PAKDD 2018, Melbourne, VIC, Australia, June 3-6, 2018, Proceedings, Part II 22,   \n449 pages 170\u2013182. Springer, 2018.   \n450 Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings   \n451 of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining,   \n452 pages 855\u2013864, 2016.   \n453 Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine   \n454 learning research, 9(11), 2008.   \n455 Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM   \n456 transactions on intelligent systems and technology (TIST), 2(3):1\u201327, 2011.   \n457 Reid Andersen, Fan Chung, and Kevin Lang. Local graph partitioning using pagerank vectors.   \n458 In 2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS\u201906), pages   \n459 475\u2013486. IEEE, 2006. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "460 A Appendix / supplemental material ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "461 Optionally include supplemental material (complete proofs, additional experiments and plots) in   \n462 appendix. All such materials SHOULD be included in the main submission. ", "page_idx": 12}, {"type": "text", "text": "463 A.1 Ablation Study ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "464 A.1.1 Number of factors ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "465 One of the crucial hyperparameters in DiGGR is the number of latent factors, denoted as $K$ . When   \n466 $K=1$ DiGGR degenerates into ordinary GMAE, only performing random masking over the entire   \n467 input graph on the nodes. The influence of tuning $K$ is illustrated in Figure 5. Given the relatively   \n468 small size of the graphs in the dataset, the number of meaningful latent disentangled factor $z$ is   \n469 not expected to be very large. The optimal number of $z$ that maximizes performance tends to be   \n470 concentrated in the range of 2-4. ", "page_idx": 12}, {"type": "image", "img_path": "hiwHaqFXGi/tmp/520fc95eb27932dd026f3b71001e49b742d4ba5ed5fa451766db4a97b04c2a3f.jpg", "img_caption": ["Figure 5: Performance of the task under different choices of latent factor number $K$ , where the horizontal axis represents the change in $K$ and the vertical axis is accuracy. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "471 A.1.2 Representation for downstream tasks ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "472 We investigate the impact of various combinations of representation levels on downstream tasks. As   \n473 illustrated in Table 4, for the node classification task, both $H_{d}$ and $H_{g}$ are required, i.e., concatenating   \n474 them in feature dimension, whereas for the graph classification task, $H_{d}$ alone is sufficient. This   \n475 difference may be due to the former not utilizing pooling operations, while the latter does. Specifically,   \n476 the graph pooling operation aggregates information from all nodes, providing a comprehensive   \n477 view of the entire graph structure. Thus, in node classification, where the node representation has   \n478 not undergone pooling, a graph-level representation $(H_{g})$ is more critical. In contrast, in graph   \n479 classification, the node representation undergoes pooling, making disentangled information $H_{d}$ more   \neffective. ", "page_idx": 12}, {"type": "table", "img_path": "hiwHaqFXGi/tmp/342c940e2e6aa114d29bd5ac8a096b2193e5468006ae597c65d5d0253cb04b98.jpg", "table_caption": ["Table 4: The average accuracy of datasets is calculated through 5 random initialization tests when using different representations. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "480 ", "page_idx": 12}, {"type": "text", "text": "481 A.2 Implementation Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "482 Environment All experiments are conducted on Linux servers equipped with an 12th Gen Intel(R)   \n483 Core(TM) i7-12700, 256GB RAM and a NVIDIA 3090 GPU. Models of node and graph classification   \n484 are implemented in PyTorch version 1.12.1, scikit-learn version 1.0.2 and Python 3.7.   \n485 Experiment Setup for Node Classification The node classification task involves predicting the   \n486 unknown node labels in networks. Cora, Citeseer, and Pubmed are employed for transductive learning,   \n487 whereas PPI follows the inductive setup outlined in GraphSage [Hamilton et al., 2017]. For evaluation,   \n488 we use the concatenated representations of $H_{d}$ and $H_{g}$ in the feature dimension for the downstream   \n489 task. We then train a linear classifier, report the mean accuracy on the test nodes through 5 random   \n490 initializations. The graph encoder $\\mathrm{GNN}_{\\mathrm{enc}(.)}$ and decoder $\\mathrm{GNN}_{\\mathrm{dec}}(.)$ are both specified as standard   \n491 GAT [Velickovic et al., 2017].We train the model using Adam Optimizer with $\\beta_{1}=0.9$ , $\\beta_{2}=0.999$ ,   \n492 $\\epsilon=1\\times10^{8}$ , and we use the cosine learning rate decay without warmup. We follow the public data   \n493 splits of Cora, Citeseer, and PubMed.   \n494 Experiment Setup for Graph Classification The graph classification experiment was conducted on   \n495 7 benchmarks, in which node labels are used as input features in MUTAG, PROTEINS and NCI1, and   \n496 node degrees are used in IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY, and COLLAB. The   \n497 backbone of encoder and decoder is GIN [Xu et al., 2018], which is commonly used in previous graph   \n498 classification works. The evaluation protocol primarily follows GraphMAE [Hou et al., 2022]. Notice   \n499 that we only utilize the factor-wise latent representation $H_{d}$ for the downstream task. Subsequently,   \n500 we feed it into a downstream LIBSVM [Chang and Lin, 2011] classifier to predict the label and   \n501 report the mean 10-fold cross-validation accuracy with standard deviation after 5 runs. We set the   \n502 initial learning rate to 0.0005 with cosine learning rate decay for most cases. For the evaluation, the   \n503 parameter C of SVM is searched in the sets $\\{10^{3^{-}},...,10\\}$ .   \n504 Data Preparation The node features for the citation networks (Cora, Citeseer, Pubmed) are bag-of  \n505 words document representations. For the protein-protein interaction networks (PPI), the features of   \n506 each node are composed of positional gene sets, motif gene sets and immunological signatures (50 in   \n507 total). For graph classification, the MUTAG, PROTEINS, and NCI1 datasets utilize node labels as   \n508 node features, represented in the form of one-hot encoding. For IMDB-B, IMDB-M, REDDIT-B, and   \n509 COLLAB, which lack node features, we utilize the node degree and convert it into a one-hot encoding   \n510 as a substitute feature. The maximum node degree is set to 400. Nodes with degrees surpassing 400   \n511 are uniformly treated as having a degree of 400, following the methodology of GraphMAE[Hou et al.,   \n512 2022]. Table 5 and Table 6 show the specific statistics of used datasets.   \n513 Details for Visualization MUTAG is selected as the representative benchmark for visualization   \n514 in 4.3. The MUTAG dataset comprises 3,371 nodes with seven node types. The distribution is   \n515 highly skewed, as 3,333 nodes belong to three types, while the remaining four types collectively   \n516 represent less than $1.2\\%$ of the nodes. For clarity in legend display, we have visualized only the nodes   \n517 belonging to the first three types. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "table", "img_path": "hiwHaqFXGi/tmp/bc03df3eb2313294fc9a5079c31cbbbf01f00b3919c870162ff4b18736b8715a.jpg", "table_caption": ["Table 5: Statistics for node classification datasets. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "518 A.3 Disentangled Representations Visualization ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "519 We chose PROTEINS and IMDB-MULTI as representatives of the graph classification dataset, and   \n520 followed the same methodology as in Section 4.3 to visualize their representation correlation matrices   \n521 on GraphMAE, and community representation correlation matrices on DiGGR, respectively. The   \n522 feature dimensions of PROTEINS and IMDB-MULTI are both 512 dimensions, and the number of   \n523 communities is set to 4.   \n524 The result is presented in Figure 6. We can see from the results that the graph representations of   \n525 GraphMAE are entangled. In contrast, the correlation pattern exhibited by DiGGR reveals four   \n526 distinct diagonal blocks. This suggests that DiGGR is proficient at capturing mutually exclusive   \n527 information within the latent factor, resulting in disentangled representations. ", "page_idx": 13}, {"type": "table", "img_path": "hiwHaqFXGi/tmp/865905c76041210896dcd7e754de2d7721b039eebaf33b75b110fbce4443eaff.jpg", "table_caption": ["Table 6: Statistics for graph classification datasets. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "image", "img_path": "hiwHaqFXGi/tmp/acfa6e2ff5c6b580896a42d44dfe569d9a26e4c44195a56da299eee4854c99f3.jpg", "img_caption": ["Figure 6: The absolute correlation between the representations learned by GraphMAE and DiGGR is measured on the PROTEINS and IMDB-MULTI datasets when $K=4$ . "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "1: Input: Graph ${\\mathcal{G}}=\\{V,A,X\\}$ ; latent factor number $K$ .   \n2: Parameters: $\\Theta$ in the inference network of Latent Factor Learning phase, $\\pmb{\\Omega}$ in the encoding network of DiGGR, $\\Psi$ in the decoding network of DiGGR.   \n3: Initialize $\\Theta,\\Omega$ , and $\\Psi$ ;   \n4: for iter $=1{,}2,\\cdot\\cdot$ do   \n5: Infer the variational posterior of $\\mathbf{z_{u}}$ based on Eq. 5;   \n6: Sample latent factors $\\mathbf{z_{u}}$ from the variational posterior according to Eq. 6;   \n7: Factorize the graph $\\mathcal{G}$ into $K$ factor-wise groups $\\{\\mathcal{G}^{(\\mathbf{k})}\\}_{\\mathbf{k}=\\mathbf{1}}^{\\mathbf{K}}$ by node and edge factorization methods;   \n8: Encoding $\\{\\mathcal{G}^{(\\mathbf{k})}\\}_{\\mathbf{k}=\\mathbf{1}}^{\\mathbf{K}}$ via latent factor-wise Graph Masked Autoencoder according to Eq. 8;   \n9: Encoding $\\mathcal{G}$ via graph-level graph masked autoencoder according to Eq. 11;   \n10: Calculate $\\nabla_{\\Theta,\\Omega,\\Psi}\\mathcal{L}(\\Theta,\\Omega,\\Psi;\\mathcal{G})$ according to Eq. 13, and update parameters $\\Theta,\\Omega$ , and $\\Psi$ jointly.   \n11: end for $=\\!0$ ", "page_idx": 15}, {"type": "text", "text": "529 A.5 Broader Impacts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "530 This paper presents work whose goal is to advance the field of Machine Learning. There are many   \n531 potential societal consequences of our work, none which we feel must be specifically highlighted   \n532 here. ", "page_idx": 15}, {"type": "text", "text": "533 A.6 Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "534 Despite the promising experimental justifications, our work might potentially suffer from limitation:   \n535 Although the complexity of the model is discussed in Section 3.4, and it is comparable to previously   \n536 published work, extending DiGGR to extremely large graph datasets remains challenging at this stage   \n537 due to the incorporation of an additional probabilistic model into the generative graph framework.   \n538 One potential solution to this problem could be utilizing PPR-Nibble [Andersen et al., 2006] for   \n539 efficient implementation, a method that has proven effective in some graph generative models [Hou   \n540 et al., 2023]. This approach will be pursued in our future work. ", "page_idx": 15}, {"type": "text", "text": "541 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "542 1. Claims   \n543 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n544 paper\u2019s contributions and scope?   \n545 Answer: [Yes]   \n546 Justification: We listed our main contribution in the last paragraph of Section 1   \n547 Guidelines:   \n548 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n549 made in the paper.   \n550 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n551 contributions made in the paper and important assumptions and limitations. A No or   \n552 NA answer to this question will not be perceived well by the reviewers.   \n553 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n554 much the results can be expected to generalize to other settings.   \n555 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n556 are not attained by the paper.   \n557 2. Limitations   \n558 Question: Does the paper discuss the limitations of the work performed by the authors?   \n559 Answer: [Yes]   \n560 Justification: We\u2019ve discuss about the limitations in Appendix A.6   \n561 Guidelines:   \n562 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n563 the paper has limitations, but those are not discussed in the paper.   \n564 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n565 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n566 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n567 model well-specification, asymptotic approximations only holding locally). The authors   \n568 should reflect on how these assumptions might be violated in practice and what the   \n569 implications would be.   \n570 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n571 only tested on a few datasets or with a few runs. In general, empirical results often   \n572 depend on implicit assumptions, which should be articulated.   \n573 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n574 For example, a facial recognition algorithm may perform poorly when image resolution   \n575 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n576 used reliably to provide closed captions for online lectures because it fails to handle   \n577 technical jargon.   \n578 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n579 and how they scale with dataset size.   \n580 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n581 address problems of privacy and fairness.   \n582 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n583 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n584 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n585 judgment and recognize that individual actions in favor of transparency play an impor  \n586 tant role in developing norms that preserve the integrity of the community. Reviewers   \n587 will be specifically instructed to not penalize honesty concerning limitations.   \n588 3. Theory Assumptions and Proofs   \n589 Question: For each theoretical result, does the paper provide the full set of assumptions and ", "page_idx": 16}, {"type": "text", "text": "90 a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: This paper is not focus on theoretical explanations and assumptions Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "604 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We have listed the specific settings of the experiment in the appendix A.2 of the paper, including the datasets used and the hyperparameter settings of the model. We have also uploaded the code in the supplementary material. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "644 5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "645 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n646 tions to faithfully reproduce the main experimental results, as described in supplemental   \n647 material?   \n648 Answer: [Yes]   \n649 Justification: We have uploaded the code in the supplementary material.   \n650 Guidelines:   \n651 \u2022 The answer NA means that paper does not include experiments requiring code.   \n652 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n653 public/guides/CodeSubmissionPolicy) for more details.   \n654 \u2022 While we encourage the release of code and data, we understand that this might not be   \n655 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n656 including code, unless this is central to the contribution (e.g., for a new open-source   \n657 benchmark).   \n658 \u2022 The instructions should contain the exact command and environment needed to run to   \n659 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n660 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n661 \u2022 The authors should provide instructions on data access and preparation, including how   \n662 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n663 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n664 proposed method and baselines. If only a subset of experiments are reproducible, they   \n665 should state which ones are omitted from the script and why.   \n666 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n667 versions (if applicable).   \n668 \u2022 Providing as much information as possible in supplemental material (appended to the   \n669 paper) is recommended, but including URLs to data and code is permitted.   \n670 6. Experimental Setting/Details   \n671 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n672 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n673 results?   \n674 Answer: [Yes]   \n675 Justification: We have listed the specific exprimental setup in Appendix A.2   \n676 Guidelines:   \n677 \u2022 The answer NA means that the paper does not include experiments.   \n678 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n679 that is necessary to appreciate the results and make sense of them.   \n680 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n681 material.   \n682 7. Experiment Statistical Significance   \n683 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n684 information about the statistical significance of the experiments?   \n685 Answer: [Yes]   \n686 Justification: In the main experiment 4, we ran 5 different random seeds for each dataset   \n687 and reported the average results and variances in the table6 and 5.   \n688 Guidelines:   \n689 \u2022 The answer NA means that the paper does not include experiments.   \n690 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n691 dence intervals, or statistical significance tests, at least for the experiments that support   \n692 the main claims of the paper.   \n693 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n694 example, train/test split, initialization, random drawing of some parameter, or overall   \n695 run with given experimental conditions).   \n696 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n697 call to a library function, bootstrap, etc.)   \n698 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n699 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n700 of the mean.   \n701 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n702 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n703 of Normality of errors is not verified.   \n704 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n705 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n706 error rates).   \n707 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n708 they were calculated and reference the corresponding figures or tables in the text.   \n709 8. Experiments Compute Resources   \n710 Question: For each experiment, does the paper provide sufficient information on the com  \n711 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n712 the experiments?   \n713 Answer: [Yes]   \n714 Justification: We listed the experiment environment in AppendixA.2   \n715 Guidelines:   \n716 \u2022 The answer NA means that the paper does not include experiments.   \n717 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n718 or cloud provider, including relevant memory and storage.   \n719 \u2022 The paper should provide the amount of compute required for each of the individual   \n720 experimental runs as well as estimate the total compute.   \n721 \u2022 The paper should disclose whether the full research project required more compute   \n722 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n723 didn\u2019t make it into the paper).   \n724 9. Code Of Ethics   \n725 Question: Does the research conducted in the paper conform, in every respect, with the   \n726 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n727 Answer: [Yes]   \n728 Justification: We conform with the NeurIPS Code of Ethics in every respect for this paper.   \n729 Guidelines:   \n730 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n731 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n732 deviation from the Code of Ethics.   \n733 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n734 eration due to laws or regulations in their jurisdiction).   \n735 10. Broader Impacts   \n736 Question: Does the paper discuss both potential positive societal impacts and negative   \n737 societal impacts of the work performed?   \n738 Answer: [Yes]   \n739 Justification: We have discussed in appendix A.5   \n740 Guidelines:   \n741 \u2022 The answer NA means that there is no societal impact of the work performed.   \n742 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n743 impact or why the paper does not address societal impact.   \n744 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n745 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n746 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n747 groups), privacy considerations, and security considerations.   \n748 \u2022 The conference expects that many papers will be foundational research and not tied   \n749 to particular applications, let alone deployments. However, if there is a direct path to   \n750 any negative applications, the authors should point it out. For example, it is legitimate   \n751 to point out that an improvement in the quality of generative models could be used to   \n752 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n753 that a generic algorithm for optimizing neural networks could enable people to train   \n754 models that generate Deepfakes faster.   \n755 \u2022 The authors should consider possible harms that could arise when the technology is   \n756 being used as intended and functioning correctly, harms that could arise when the   \n757 technology is being used as intended but gives incorrect results, and harms following   \n758 from (intentional or unintentional) misuse of the technology.   \n759 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n760 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n761 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n762 feedback over time, improving the efficiency and accessibility of ML).   \n763 11. Safeguards   \n764 Question: Does the paper describe safeguards that have been put in place for responsible   \n765 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n766 image generators, or scraped datasets)?   \n767 Answer: [NA]   \n768 Justification: this paper poses no such risks.   \n769 Guidelines:   \n770 \u2022 The answer NA means that the paper poses no such risks.   \n771 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n772 necessary safeguards to allow for controlled use of the model, for example by requiring   \n773 that users adhere to usage guidelines or restrictions to access the model or implementing   \n774 safety filters.   \n775 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n776 should describe how they avoided releasing unsafe images.   \n777 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n778 not require this, but we encourage authors to take this into account and make a best   \n779 faith effort.   \n780 12. Licenses for existing assets   \n781 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n782 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n783 properly respected?   \n784 Answer: [Yes]   \n785 Justification: All these are properly credited.   \n786 Guidelines:   \n787 \u2022 The answer NA means that the paper does not use existing assets.   \n788 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n789 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n790 URL.   \n791 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n792 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n793 service of that source should be provided.   \n794 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n795 package should be provided. For popular datasets, paperswithcode.com/datasets   \n796 has curated licenses for some datasets. Their licensing guide can help determine the   \n797 license of a dataset.   \n798 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n799 the derived asset (if it has changed) should be provided.   \n800 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n801 the asset\u2019s creators.   \n802 13. New Assets   \n803 Question: Are new assets introduced in the paper well documented and is the documentation   \n804 provided alongside the assets?   \n805 Answer: [NA]   \n806 Justification: This paper does not release new assets.   \n807 Guidelines:   \n808 \u2022 The answer NA means that the paper does not release new assets.   \n809 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n810 submissions via structured templates. This includes details about training, license,   \n811 limitations, etc.   \n812 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n813 asset is used.   \n814 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n815 create an anonymized URL or include an anonymized zip file.   \n816 14. Crowdsourcing and Research with Human Subjects   \n817 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n818 include the full text of instructions given to participants and screenshots, if applicable, as   \n819 well as details about compensation (if any)?   \n820 Answer: [NA]   \n821 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n822 Guidelines:   \n823 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n824 human subjects.   \n825 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n826 tion of the paper involves human subjects, then as much detail as possible should be   \n827 included in the main paper.   \n828 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n829 or other labor should be paid at least the minimum wage in the country of the data   \n830 collector.   \n831 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n832 Subjects   \n833 Question: Does the paper describe potential risks incurred by study participants, whether   \n834 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n835 approvals (or an equivalent approval/review based on the requirements of your country or   \n836 institution) were obtained?   \n837 Answer: [NA]   \n838 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n839 Guidelines:   \n840 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n841 human subjects.   \n842 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n843 may be required for any human subjects research. If you obtained IRB approval, you   \n844 should clearly state this in the paper.   \n845 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n846 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n847 guidelines for their institution.   \n848 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n849 applicable), such as the institution conducting the review. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}]