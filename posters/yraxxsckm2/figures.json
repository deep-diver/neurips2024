[{"figure_path": "YrAxxscKM2/figures/figures_0_1.jpg", "caption": "Figure 1: Test error vs. dataset size on CIFAR-10-5m for a fixed number of training iteration. Weight decay is helpful in both: the over-training and the under-training, one-pass regime.", "description": "This figure shows the test error rate plotted against the dataset size for a ResNet18 model trained on the CIFAR-10-5m dataset.  Two training scenarios are compared: one with weight decay and one without. The results demonstrate that weight decay improves the test error rate across a range of dataset sizes, regardless of whether the model is in an over-training or under-training regime.", "section": "1 Introduction"}, {"figure_path": "YrAxxscKM2/figures/figures_2_1.jpg", "caption": "Figure 2: Training with and w/o weight decay. We report the test error for Resnet18 on CIFAR-10 (2a) and Tiny-ImageNet (2d) trained with and without weight decay and with small and large learning rates. We also include the correspondent EMA, represented by dashed lines. After the first 250 epochs the learning rate is decayed to \u03b7 = 10-3 for all the curves. We report also the L2 norm of the parameters (2c) and Train CE (2b) which after the decay converges to the same value for all the runs with the same \u03bb.", "description": "This figure compares the performance of ResNet18 trained on CIFAR-10 and TinyImageNet with and without weight decay, using both small and large learning rates.  It includes plots showing test error, training cross-entropy, L2 norm of parameters, and the effect of exponential moving average (EMA). The results highlight the impact of weight decay on optimization dynamics, showing that weight decay modifies the training dynamics in a beneficial way even when the models fully memorize the training data.", "section": "2 Weight decay in the over-training regime"}, {"figure_path": "YrAxxscKM2/figures/figures_4_1.jpg", "caption": "Figure 3: Resnet18 on Tiny-ImageNet. Heatmap of the test error and Jacobian norm for the EMA for all the different combinations of \u03b7 and \u03bb.", "description": "This heatmap visualizes the test error and Frobenius norm of the Jacobian (a measure of model complexity) for a ResNet18 model trained on the Tiny-ImageNet dataset.  The heatmap shows how these metrics vary across different combinations of the learning rate (\u03b7) and weight decay parameter (\u03bb).  The results are obtained using the Exponential Moving Average (EMA) of the model parameters, offering insight into the model's performance and generalization behavior under different optimization strategies.", "section": "2.1 Loss stabilization and weight decay"}, {"figure_path": "YrAxxscKM2/figures/figures_5_1.jpg", "caption": "Figure 4: Resnet18 on Tiny-ImageNet. Training for 200 epochs with different \u03b7 and \u03bb; the scale of the noise monotonically increases with the train loss and \u03b7 \u00d7 \u03bb Fig. 4a, 4b. The test error instead, presents an optimal value of \u03b7 \u00d7 \u03bb Fig. 4c while the Jacobian norm decreases monotonically Fig. 4d.", "description": "This figure shows the results of training ResNet18 on the Tiny-ImageNet dataset for 200 epochs with various learning rates (\u03b7) and weight decay parameters (\u03bb).  Panel (a) demonstrates that the scale of the noise (\u03c3<sub>\u03b7,\u03bb</sub>), a crucial factor in the implicit regularization mechanism, increases monotonically with both the training loss and the product of \u03b7 and \u03bb.  Panel (b) shows a similar monotonic relationship between \u03c3<sub>\u03b7,\u03bb</sub> and the product of \u03b7 and \u03bb.  However, panel (c) reveals that there's an optimal value for the product \u03b7\u03bb that minimizes the test error. Finally, panel (d) shows a consistently decreasing Jacobian norm (||J||F) with increasing \u03b7\u03bb, suggesting that over-regularization occurs beyond the optimal \u03b7\u03bb value.  The figure strongly suggests an optimal balance between noise and regularization for best generalization performance. ", "section": "2.1 Loss stabilization and weight decay"}, {"figure_path": "YrAxxscKM2/figures/figures_7_1.jpg", "caption": "Figure 2: Training with and w/o weight decay. We report the test error for Resnet18 on CIFAR-10 (2a) and Tiny-ImageNet (2d) trained with and without weight decay and with small and large learning rates. We also include the correspondent EMA, represented by dashed lines. After the first 250 epochs the learning rate is decayed to \u03b7 = 10-3 for all the curves. We report also the L2 norm of the parameters (2c) and Train CE (2b) which after the decay converges to the same value for all the runs with the same \u03bb.", "description": "This figure shows the effect of weight decay on the test error and training dynamics of ResNet18 on CIFAR-10 and TinyImageNet.  It compares models trained with and without weight decay, using both small and large learning rates.  The plots illustrate test error, training cross-entropy, L2 norm of the parameters, and the effect of exponential moving average (EMA).  The learning rate is decayed after 250 epochs.", "section": "2 Weight decay in the over-training regime"}, {"figure_path": "YrAxxscKM2/figures/figures_7_2.jpg", "caption": "Figure 6: GPT-2-124M on OpenWebText. We reproduce the improvement from WD as in Hoffmann et al. (2022) but at a much smaller scale. Performing fine-tuning with a tiny LR reveals that a higher starting training loss can still be a better point in terms of the final loss after fine-tuning.", "description": "This figure shows the training loss curves for a GPT-2 language model with 124M parameters trained on the OpenWebText dataset.  Multiple curves are presented, each representing a different weight decay (\u03bbwd) value (0.0, 0.1, and 0.3).  For each weight decay value, two curves are shown: one for training with the standard learning rate schedule, and another after fine-tuning with a very small learning rate.  The results indicate that using weight decay can lead to a lower training loss, even if the initial training loss is higher than when no weight decay is used.  The authors replicate findings from prior work (Hoffmann et al., 2022) using a smaller scale model for validation.", "section": "3 Weight decay in the under-training regime"}, {"figure_path": "YrAxxscKM2/figures/figures_8_1.jpg", "caption": "Figure 7: GPT-2-124M on OpenWebText. Left: The effective LR \u03b7t/||wt||2 for the models reported in Fig. 6. Middle: The LR schedule that matches the effective LR \u03b7t/||wt||2 of the runs with weight decay 0.1 and 0.3. Right: Matching the effective LR is sufficient to match the whole training dynamics of the loss if we avoid the loss spikes by using full precision (float32 instead of bfloat16).", "description": "This figure demonstrates that the effective learning rate (\u03b7t/||wt||2) is a key factor influencing the training dynamics of large language models.  The left panel shows the effective learning rate for different models trained with varying weight decay (\u03bb). The middle panel shows a learning rate schedule that mimics the effective learning rate of the models with weight decay. The right panel compares the training loss curves, showing that matching the effective learning rate is enough to reproduce the dynamics but only when higher precision (float32) is used instead of bfloat16.", "section": "3 Weight decay in the under-training regime"}, {"figure_path": "YrAxxscKM2/figures/figures_9_1.jpg", "caption": "Figure 6: GPT-2-124M on OpenWebText. We reproduce the improvement from WD as in Hoffmann et al. (2022) but at a much smaller scale. Performing fine-tuning with a tiny LR reveals that a higher starting training loss can still be a better point in terms of the final loss after fine-tuning.", "description": "This figure shows the training loss curves for a GPT-2 language model with 124M parameters, trained on the OpenWebText dataset.  Multiple curves are shown, representing different weight decay (\u03bb_{wd}) hyperparameter values (0.0, 0.1, and 0.3).  The key takeaway is that while weight decay doesn't prevent the model from achieving zero training error, its presence still improves the test error (generalization) as shown by Hoffmann et al. (2022). The experiment also demonstrates that a higher starting training loss, facilitated by weight decay, can lead to a lower final training loss after a fine-tuning phase with a smaller learning rate. This highlights that weight decay's role isn't solely about regularization, but also about affecting the optimization dynamics in a favorable way.", "section": "3 Weight decay in the under-training regime"}, {"figure_path": "YrAxxscKM2/figures/figures_15_1.jpg", "caption": "Figure 9: A graphical illustration of the fine-tuning phase.", "description": "This figure provides a visual illustration of the fine-tuning phase. The green curve represents the trajectory of the SGD iterates in the large-LR phase. As the training proceeds, the trajectory moves towards a solution, represented by the star (*). The black lines indicate the projections of SGD iterates onto the manifolds with the same CE values, which are concentric circles around the solution. The red lines show the distances between SGD iterates and the projections onto these circles. The figure highlights the decreasing Jacobian norm in the fine-tuning phase. ", "section": "C.1 A graphical illustration of the fine-tuning phase"}, {"figure_path": "YrAxxscKM2/figures/figures_16_1.jpg", "caption": "Figure 3: Resnet18 on Tiny-ImageNet. Heatmap of the test error and Jacobian norm for the EMA for all the different combinations of \u03b7 and \u03bb.", "description": "This figure shows heatmaps of the test error and Frobenius norm of the Jacobian for an EMA (Exponential Moving Average) across various combinations of learning rate (\u03b7) and weight decay (\u03bb) for a ResNet18 model trained on the Tiny-ImageNet dataset.  The heatmaps visualize the relationship between these hyperparameters and model performance, illustrating how the optimal test error is not achieved by a single combination of \u03b7 and \u03bb but rather along a contour where their product \u03b7 \u00d7 \u03bb is approximately constant.  The Jacobian norm shows a consistently decreasing trend as the product \u03b7\u03bb increases.", "section": "2.1 Loss stabilization and weight decay"}, {"figure_path": "YrAxxscKM2/figures/figures_16_2.jpg", "caption": "Figure 11: Training with and w/o weight decay. We report the test error for VGG (11a) and ResNet (11b, 11c) trained on CIFAR-10/100 with and without weight decay and with small and large learning rates. After the first 500 epochs the learning rate is decayed to  \u03b7 = 10\u22124 for all the curves.", "description": "The figure shows the test error for VGG and ResNet models trained on CIFAR-10 and CIFAR-100 datasets with and without weight decay.  The models were trained with both small and large learning rates.  After 500 epochs, the learning rate was decayed for all curves. The results illustrate the impact of weight decay and learning rate on model performance.", "section": "2 Weight decay in the over-training regime"}, {"figure_path": "YrAxxscKM2/figures/figures_17_1.jpg", "caption": "Figure 12: Training with and w/o weight decay. We report the train CE for VGG (12a) and ResNet (12b, 12c) trained on CIFAR-10/100 with and without weight decay and with small and large learning rates. After the first 500 epochs the learning rate is decayed to \u03b7 = 10\u207b\u2074 for all the curves.", "description": "This figure compares the training cross-entropy (CE) loss curves for VGG and ResNet models trained on CIFAR-10 and CIFAR-100 datasets, respectively.  The models are trained with and without weight decay (WD), and with both small and large learning rates.  The learning rate is decayed after 500 epochs. The figure illustrates how weight decay affects the training dynamics, specifically the convergence of the training loss.", "section": "2.1 Loss stabilization and weight decay"}, {"figure_path": "YrAxxscKM2/figures/figures_17_2.jpg", "caption": "Figure 13: Cosine similarity between hessian and Noise covariance: we compute the cosine similarity between the hessian and the covariance of the SGD noise for a scale-invariant ResNet after one epoch with large lr \u03b7 = 0.005. The results show how the two matrices are correlated and in particular how the SGD noise covariance is highly correlated with G(w).", "description": "This figure shows the cosine similarity between the Hessian and the noise covariance of SGD for a scale-invariant ResNet after one epoch with a large learning rate.  The high correlation between the noise covariance and the Gauss-Newton component (G) of the Hessian supports the paper's argument that SGD's implicit regularization is driven by noise.", "section": "2.2 The noise driven process"}, {"figure_path": "YrAxxscKM2/figures/figures_18_1.jpg", "caption": "Figure 5: EMA vs Fine-tuning. Training of standard Resnet18 on CIFAR-10 for 100 epochs fixing \u03bb = 0.0125 and varying the learning rate. In Fig. 5a we report different levels of loss stabilization, in Fig. 5b we report the test errors and in Fig. 5c and Fig. 5d the norm of the Jacobian and of the weights respectively. The quantities are measured for the SGD iterates, the EMA and the fine-tuning. The latter is performed for 100 epochs every 3 with \u03b7 = 10-3.", "description": "This figure compares the performance of EMA and fine-tuning methods for ResNet18 trained on CIFAR-10.  It shows the training loss stabilization, test errors, Jacobian norm, and weight norm for different learning rates. The results indicate that both EMA and fine-tuning improve performance, but EMA is slightly more efficient.", "section": "2.3 EMA and Fine-tuning"}, {"figure_path": "YrAxxscKM2/figures/figures_19_1.jpg", "caption": "Figure 5: EMA vs Fine-tuning. Training of standard Resnet18 on CIFAR-10 for 100 epochs fixing \u03bb = 0.0125 and varying the learning rate. In Fig. 5a we report different levels of loss stabilization, in Fig. 5b we report the test errors and in Fig. 5c and Fig. 5d the norm of the Jacobian and of the weights respectively. The quantities are measured for the SGD iterates, the EMA and the fine-tuning. The latter is performed for 100 epochs every 3 with \u03b7 = 10\u22123.", "description": "This figure compares the training dynamics of Resnet18 on CIFAR-10 using EMA (Exponential Moving Average) and fine-tuning methods. It shows the training loss, test error, Jacobian norm, and weight norm for different learning rates with a fixed weight decay of 0.0125. Fine-tuning involves decaying the learning rate after a certain number of epochs. The results illustrate the interplay between the learning rate, weight decay, and the optimization dynamics, and how EMA and fine-tuning influence the performance of the model.", "section": "2.3 EMA and Fine-tuning"}, {"figure_path": "YrAxxscKM2/figures/figures_20_1.jpg", "caption": "Figure 3: Resnet18 on Tiny-ImageNet. Heatmap of the test error and Jacobian norm for the EMA for all the different combinations of \u03b7 and \u03bb.", "description": "This figure shows a heatmap visualizing the test error and Frobenius norm of the Jacobian for an Exponential Moving Average (EMA) of the parameters.  The heatmap covers various combinations of learning rate (\u03b7) and weight decay (\u03bb).  The goal is to illustrate the interplay between these hyperparameters in influencing the model's generalization performance, as measured by the test error, and the complexity of the model as measured by the Jacobian norm.", "section": "2.2 The noise driven process"}, {"figure_path": "YrAxxscKM2/figures/figures_21_1.jpg", "caption": "Figure 3: Resnet18 on Tiny-ImageNet. Heatmap of the test error and Jacobian norm for the EMA for all the different combinations of \u03b7 and \u03bb.", "description": "This figure shows a heatmap visualizing the test error and Frobenius norm of the Jacobian for an Exponential Moving Average (EMA) of the model's weights.  The heatmap explores various combinations of the learning rate (\u03b7) and weight decay parameter (\u03bb) during the training of a ResNet18 model on the Tiny-ImageNet dataset. The results illustrate the interplay between these hyperparameters in achieving optimal generalization performance and controlling the norm of the learned model's Jacobian.", "section": "2.2 The noise driven process"}, {"figure_path": "YrAxxscKM2/figures/figures_22_1.jpg", "caption": "Figure 3: Resnet18 on Tiny-ImageNet. Heatmap of the test error and Jacobian norm for the EMA for all the different combinations of \u03b7 and \u03bb.", "description": "This figure shows a heatmap visualizing the test error and Frobenius norm of the Jacobian for an Exponential Moving Average (EMA) of the SGD iterates.  The heatmap is generated by varying the learning rate (\u03b7) and weight decay parameter (\u03bb) across a range of values. The results illustrate the interplay between these two hyperparameters in determining both the generalization performance (test error) and the model's complexity (Jacobian norm).", "section": "2.1 Loss stabilization and weight decay"}, {"figure_path": "YrAxxscKM2/figures/figures_24_1.jpg", "caption": "Figure 19: Left: The validation loss of a GPT-2-124M model is determined by the training loss and not influenced by \u03bb. Right: The generalization gap stays close to zero throughout training for different \u03bb for both 124M and 774M parameter models.", "description": "This figure shows the relationship between training loss and validation loss for GPT-2 language models with varying numbers of parameters (124M and 774M) and different weight decay hyperparameters (\u03bb). The left panel demonstrates a strong correlation between training and validation loss, irrespective of the weight decay value. The right panel illustrates that the generalization gap (the difference between training and validation loss) remains consistently near zero across all experimental settings, suggesting that weight decay doesn't significantly impact the model's ability to generalize.", "section": "Weight decay in the under-training regime"}, {"figure_path": "YrAxxscKM2/figures/figures_24_2.jpg", "caption": "Figure 6: GPT-2-124M on OpenWebText. We reproduce the improvement from WD as in Hoffmann et al. (2022) but at a much smaller scale. Performing fine-tuning with a tiny LR reveals that a higher starting training loss can still be a better point in terms of the final loss after fine-tuning.", "description": "This figure shows the training loss curves for a GPT-2-124M language model trained on the OpenWebText dataset with different weight decay (\u03bbwd) values.  It demonstrates that while weight decay doesn't prevent the model from achieving zero training error, it improves the final training loss, especially when combined with a small learning rate during fine-tuning. This highlights the impact of weight decay on optimization dynamics, specifically improving training loss in the under-training regime where only a single pass through the data is done.", "section": "3 Weight decay in the under-training regime"}, {"figure_path": "YrAxxscKM2/figures/figures_25_1.jpg", "caption": "Figure 6: GPT-2-124M on OpenWebText. We reproduce the improvement from WD as in Hoffmann et al. (2022) but at a much smaller scale. Performing fine-tuning with a tiny LR reveals that a higher starting training loss can still be a better point in terms of the final loss after fine-tuning.", "description": "This figure shows the training loss curves for a GPT-2-124M language model trained on the OpenWebText dataset with different weight decay values (\u03bb_{WD}).  It demonstrates that even though weight decay does not prevent the models from achieving zero training error, its presence still leads to a lower training loss and ultimately better generalization (not shown but mentioned in the paper). The effect is more pronounced at the end of training, but a higher initial training loss with weight decay results in a better final loss after fine-tuning with a smaller learning rate.", "section": "3 Weight decay in the under-training regime"}, {"figure_path": "YrAxxscKM2/figures/figures_25_2.jpg", "caption": "Figure 6: GPT-2-124M on OpenWebText. We reproduce the improvement from WD as in Hoffmann et al. (2022) but at a much smaller scale. Performing fine-tuning with a tiny LR reveals that a higher starting training loss can still be a better point in terms of the final loss after fine-tuning.", "description": "This figure shows the training loss curves for a GPT-2-124M language model trained on the OpenWebText dataset using AdamW optimizer with different weight decay values (\u03bbWD). The experiment demonstrates that weight decay (WD) leads to lower training loss, even when employing a small learning rate during the fine-tuning phase. This suggests that WD primarily affects the optimization dynamics rather than explicit regularization, especially in the under-training regime.  The results confirm Hoffmann et al.'s (2022) findings at a smaller scale.", "section": "3 Weight decay in the under-training regime"}, {"figure_path": "YrAxxscKM2/figures/figures_25_3.jpg", "caption": "Figure 2: Training with and w/o weight decay. We report the test error for Resnet18 on CIFAR-10 (2a) and Tiny-ImageNet (2d) trained with and without weight decay and with small and large learning rates. We also include the correspondent EMA, represented by dashed lines. After the first 250 epochs the learning rate is decayed to \u03b7 = 10\u22123 for all the curves. We report also the L2 norm of the parameters (2c) and Train CE (2b) which after the decay converges to the same value for all the runs with the same \u03bb.", "description": "This figure compares the test error, training cross-entropy, and L2 norm of the parameters for ResNet18 trained on CIFAR-10 and TinyImageNet with and without weight decay, using both small and large learning rates.  The effect of exponential moving average (EMA) is also shown.  The learning rate is decayed after 250 epochs. The results illustrate the impact of weight decay on model generalization and optimization dynamics in different settings.", "section": "2 Weight decay in the over-training regime"}, {"figure_path": "YrAxxscKM2/figures/figures_26_1.jpg", "caption": "Figure 4: Resnet18 on Tiny-ImageNet. Training for 200 epochs with different \u03b7 and \u03bb; the scale of the noise monotonically increases with the train loss and \u03b7 \u00d7 \u03bb Fig. 4a, 4b. The test error instead, presents an optimal value of \u03b7 \u00d7 \u03bb Fig. 4c while the Jacobian norm decreases monotonically Fig. 4d.", "description": "This figure shows the results of training ResNet18 on the Tiny-ImageNet dataset for 200 epochs using different learning rates (\u03b7) and weight decay parameters (\u03bb).  The plots demonstrate the relationship between the training loss, the scale of the noise (\u03c3<sub>\u03b7,\u03bb</sub>), the test error, and the Frobenius norm of the Jacobian (||J(W)||<sub>F</sub>).  Specifically, it highlights that while the noise scale increases monotonically with both training loss and the product of learning rate and weight decay (\u03b7\u03bb),  the test error exhibits an optimal value of \u03b7\u03bb.  The Jacobian norm, conversely, decreases monotonically.", "section": "2.1 Loss stabilization and weight decay"}, {"figure_path": "YrAxxscKM2/figures/figures_26_2.jpg", "caption": "Figure 4: Resnet18 on Tiny-ImageNet. Training for 200 epochs with different \u03b7 and \u03bb; the scale of the noise monotonically increases with the train loss and \u03b7 \u00d7 \u03bb Fig. 4a, 4b. The test error instead, presents an optimal value of \u03b7 \u00d7 \u03bb Fig. 4c while the Jacobian norm decreases monotonically Fig. 4d.", "description": "This figure shows the results of training ResNet18 on the Tiny-ImageNet dataset for 200 epochs with different learning rates (\u03b7) and weight decay parameters (\u03bb).  It demonstrates that the scale of the noise in the optimization process increases monotonically with both the training loss and the product of \u03b7 and \u03bb.  However, the test error shows an optimal value for the product \u03b7\u03bb, indicating a trade-off between learning rate and weight decay.  Finally, the figure shows that the Jacobian norm of the model decreases monotonically with increasing \u03b7\u03bb.", "section": "2.1 Loss stabilization and weight decay"}, {"figure_path": "YrAxxscKM2/figures/figures_26_3.jpg", "caption": "Figure 4: Resnet18 on Tiny-ImageNet. Training for 200 epochs with different \u03b7 and \u03bb; the scale of the noise monotonically increases with the train loss and \u03b7 \u00d7 \u03bb Fig. 4a, 4b. The test error instead, presents an optimal value of \u03b7 \u00d7 \u03bb Fig. 4c while the Jacobian norm decreases monotonically Fig. 4d.", "description": "This figure shows the results of experiments on training ResNet18 on the Tiny-ImageNet dataset for 200 epochs using different learning rates (\u03b7) and weight decay parameters (\u03bb).  It demonstrates the relationship between these hyperparameters, the training loss, test error, and Jacobian norm. Notably, the scale of the noise (a key factor influencing generalization) increases monotonically with both the training loss and the product of learning rate and weight decay (\u03b7\u03bb).  However, the test error achieves an optimal value at a specific \u03b7\u03bb, while the Jacobian norm decreases consistently, suggesting that there exists an optimal balance between the noise scale and the norm of the Jacobian for best generalization.", "section": "2.1 Loss stabilization and weight decay"}]