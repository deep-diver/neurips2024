{"references": [{"fullname_first_author": "Andriushchenko, M.", "paper_title": "SGD with large step sizes learns sparse features", "publication_date": "2023", "reason": "This paper is highly relevant because it directly addresses the impact of large step sizes and weight decay in SGD optimization, a core topic of the main paper."}, {"fullname_first_author": "Brown, T.", "paper_title": "Language models are few-shot learners", "publication_date": "2020", "reason": "This is a highly influential paper in the field of large language models and directly relates to the under-training regime explored in the main paper."}, {"fullname_first_author": "Li, Z.", "paper_title": "Towards explaining the regularization effect of initial large learning rate in training neural networks", "publication_date": "2019", "reason": "This paper is essential for its exploration of the connection between large learning rates, weight decay, and generalization, which is a central theme of the main paper."}, {"fullname_first_author": "Loshchilov, I.", "paper_title": "Decoupled weight decay regularization", "publication_date": "2019", "reason": "This work introduces AdamW, a widely used optimizer, and its impact on the use of weight decay in deep learning, directly relevant to the main paper's results."}, {"fullname_first_author": "Zhang, C.", "paper_title": "Understanding deep learning requires rethinking generalization", "publication_date": "2016", "reason": "This paper challenges traditional understanding of generalization in deep learning and its relation to weight decay, thus setting up the context and problem that the main paper aims to address."}]}