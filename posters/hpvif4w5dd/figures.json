[{"figure_path": "HPvIf4w5Dd/figures/figures_4_1.jpg", "caption": "Figure 1: MDP MR, the hard instance for Theorem 1. Each arrow corresponds to a state-action and next state combination, and is annotated with the mean reward of the action and the probability of the transition. Arrows with a different line style correspond to different actions.", "description": "This figure shows a Markov Decision Process (MDP) used to prove Theorem 1, which states that estimating the optimal bias span H requires an arbitrarily large number of samples. The MDP has three states (1, 2, 3) and two actions for each state. The solid lines represent one action and the dashed lines represent the other action, which have different reward and transition probabilities. The parameter R determines the optimal policy and bias span. If R < 1/2, the solid-line policy is optimal and the bias span is small. If R > 1/2, the dashed-line policy is optimal and the bias span is large. The difficulty of accurately estimating H arises when R is very close to 1/2, as many samples are needed to distinguish between the two cases.", "section": "3 On the hardness of estimating H"}, {"figure_path": "HPvIf4w5Dd/figures/figures_6_1.jpg", "caption": "Figure 1: MDP MR, the hard instance for Theorem 1. Each arrow corresponds to a state-action and next state combination, and is annotated with the mean reward of the action and the probability of the transition. Arrows with a different line style correspond to different actions.", "description": "This figure shows a Markov Decision Process (MDP) used in Theorem 1 to demonstrate the hardness of estimating the optimal bias span H.  The MDP has three states (1, 2, 3) and two actions for each state, represented by solid and dashed arrows.  The numbers on the arrows indicate the reward obtained and the probability of transitioning to the next state when that action is chosen. The structure of the MDP is designed such that depending on the exact value of a parameter R, either the solid lines or the dashed lines will result in an optimal policy.  However, R's value is close to 1/2 (the transition point between the two optimal policies), making it difficult to determine the optimal policy (and thus estimate H accurately) with a limited number of samples.", "section": "On the hardness of estimating H"}, {"figure_path": "HPvIf4w5Dd/figures/figures_6_2.jpg", "caption": "Figure 3: MDP Mj, the hard instance for Theorem 3", "description": "This figure shows a Markov Decision Process (MDP) used to prove Theorem 3, which states that there is no online algorithm with sample complexity polynomial in S, A, and H for best policy identification.  The MDP has S states (s1, s2, ..., sS) and A actions.  The transition probabilities are such that reaching state s2 from s1 requires a specific sequence of actions. The reward is 1 only in state s1.  This construction is used to show that any algorithm must explore the state space sufficiently to encounter the high-reward state s1 and learn the optimal sequence of actions.", "section": "On the hardness of best policy identification in the online setting"}, {"figure_path": "HPvIf4w5Dd/figures/figures_12_1.jpg", "caption": "Figure 1: MDP MR, the hard instance for Theorem 1. Each arrow corresponds to a state-action and next state combination, and is annotated with the mean reward of the action and the probability of the transition. Arrows with a different line style correspond to different actions.", "description": "This figure shows a Markov Decision Process (MDP) that is used to prove Theorem 1 in the paper.  The MDP is designed to be difficult to estimate the optimal bias span, H. The MDP has three states (1, 2, and 3) and two actions. Depending on the parameter R (where 0 < R < 1), either a solid line or a dashed line policy is optimal.  The complexity arises because if R is close to 1/2, it requires many samples to determine whether R is greater than or less than 1/2, significantly impacting the complexity of estimating H.", "section": "On the hardness of estimating H"}, {"figure_path": "HPvIf4w5Dd/figures/figures_17_1.jpg", "caption": "Figure 1: MDP MR, the hard instance for Theorem 1. Each arrow corresponds to a state-action and next state combination, and is annotated with the mean reward of the action and the probability of the transition. Arrows with a different line style correspond to different actions.", "description": "This figure shows a Markov Decision Process (MDP) used in the proof of Theorem 1 to demonstrate that estimating the optimal bias span H can be computationally hard. The MDP has three states (1, 2, 3) and two actions for each state.  The actions are represented by solid and dashed arrows, with different transition probabilities and rewards for each. The parameter R is used to control whether the optimal policy is the dashed or solid line policy. This is critical because when R is close to 0.5, a very large number of samples are required to distinguish between R < 0.5 and R > 0.5.", "section": "On the hardness of estimating H"}, {"figure_path": "HPvIf4w5Dd/figures/figures_19_1.jpg", "caption": "Figure 1: MDP MR, the hard instance for Theorem 1. Each arrow corresponds to a state-action and next state combination, and is annotated with the mean reward of the action and the probability of the transition. Arrows with a different line style correspond to different actions.", "description": "This figure shows a Markov Decision Process (MDP) used in the proof of Theorem 1.  The MDP has three states and two actions. The transitions and rewards are controlled by a parameter R. The figure demonstrates how the optimal policy, and therefore the optimal bias span, changes depending on whether R is less than or greater than 1/2.  This property makes it difficult to estimate the optimal bias span H efficiently, which is central to the theorem's proof.", "section": "On the hardness of estimating H"}]