[{"figure_path": "boGxvYWZEq/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of different CL settings. (a) In CIL, models classify images within all previously encountered classes. (b) In MTIL, models classify images from both seen and unseen domains based on the given domain-identities. (c) In X-TAIL, models classify images from both seen and unseen domains without any domain-identity hint.", "description": "This figure compares three continual learning (CL) settings: traditional Class-Incremental Learning (CIL), Multi-domain Task-Incremental Learning (MTIL), and the proposed Cross-domain Task-Agnostic Incremental Learning (X-TAIL).  CIL only classifies images from previously seen classes. MTIL classifies images from seen and unseen domains using domain-identity hints.  X-TAIL, however, is a more challenging setting where the model must classify images from both seen and unseen domains *without* any domain-identity information.", "section": "Introduction"}, {"figure_path": "boGxvYWZEq/figures/figures_3_1.jpg", "caption": "Figure 2: Metrics for X-TAIL setting.", "description": "This figure shows the evaluation metrics used in the Cross-domain Task-Agnostic Incremental Learning (X-TAIL) setting.  The matrix visualizes the performance of a continual learning model across multiple domains and learning steps.  Each row represents a learning step, where a new domain's data is introduced. Each column represents a domain. The lower diagonal (orange) represents performance on seen domains; the upper diagonal (grey and green) represents performance on unseen and seen domains, respectively.  'Average' represents the overall average performance. 'Last' represents the final performance on all seen domains. 'Transfer' represents the zero-shot performance on unseen domains.", "section": "3 Cross-domain task-agnostic incremental learning"}, {"figure_path": "boGxvYWZEq/figures/figures_4_1.jpg", "caption": "Figure 3: Pearson correlation coefficients (CCs) for 10 pairs of domain-prototypes.", "description": "This figure shows the Pearson correlation coefficients (CCs) between 10 pairs of domain prototypes.  The three classifiers used are Linear, Primal, and Dual.  The heatmaps visualize the correlation between domain prototypes for each classifier.  The figure aims to demonstrate that the non-linear projection methods (Primal and Dual) enhance the separability of CLIP features from different domains, unlike the Linear approach which exhibits high cross-domain correlations.", "section": "4.1 Motivation"}, {"figure_path": "boGxvYWZEq/figures/figures_5_1.jpg", "caption": "Figure 5: RAIL Overview. (a) During inference, the fusion module utilizes the Zero-shot logits to identify whether a test image is aligned with seen or unseen classes. If classified as a seen class, the Fusion logits combine the RAIL-Adapter logits and the Zero-shot logits; otherwise solely rely on the Zero-shot logits. (b) Primal: at the n-th learning step, features Xe extracted by CLIP's image encoder are projected to higher dimensional \u03a6 via RHL and then update the parameter W and memory M by Theorem 1. (c) Dual: features extracted by CLIP's image encoder update the kernel K, parameter a, and memory Ma by Theorem 2.", "description": "This figure provides an overview of the Regression-based Analytic Incremental Learning (RAIL) method. It shows the inference stage and the training stages for both primal and dual forms. The inference stage shows how the model classifies images based on seen and unseen classes using a fusion module. The training stages illustrate how the model adapts to new domains incrementally using a ridge regression-based adapter. The primal form uses a randomly initialized hidden layer (RHL) for feature projection, while the dual form utilizes a kernel method.", "section": "4 Approach"}, {"figure_path": "boGxvYWZEq/figures/figures_7_1.jpg", "caption": "Figure 6: Accuracy (%) on five domains changes over all learning steps.", "description": "This figure shows the accuracy of different continual learning methods (Primal RAIL, Dual RAIL, ZSCL, and MoE-Adapter) across five different image domains (Caltech101, DTD, EuroSAT, Flowers, and MNIST) over ten learning steps.  Each line represents a method, showing how its accuracy changes as new domains are incrementally added.  It visually demonstrates the performance of each method in the context of cross-domain task-agnostic incremental learning, highlighting the ability (or lack thereof) to maintain accuracy on previously learned domains while adapting to new ones.", "section": "5.1 Comparison results"}, {"figure_path": "boGxvYWZEq/figures/figures_16_1.jpg", "caption": "Figure 8: RHL dimension vs. \"Last\" accuracy (%) averaged on 10 domains.", "description": "This figure shows the relationship between the hidden layer dimension of the Randomly-initialized Hidden Layer (RHL) and the model's performance.  The \"Last\" accuracy, which represents the final accuracy achieved on each domain after all incremental learning, is plotted against different RHL dimensions (1k, 2k, 5k, 10k, 15k, 20k). The graph illustrates that increasing the RHL dimension generally leads to improved accuracy, reaching near saturation beyond a dimension of 10k.  The results suggest a balance between improved model representation power and computational cost when choosing the RHL dimension.", "section": "E.1 RHL dimension"}, {"figure_path": "boGxvYWZEq/figures/figures_16_2.jpg", "caption": "Figure 9: Fusion ratio vs. \"Average\" and \"Last\" accuracy (%) averaged on 10 domains.", "description": "This figure shows the impact of the fusion ratio (beta) on the average and last accuracy of the RAIL model across ten different domains.  The fusion ratio controls the weighting between the RAIL-adapter logits and the CLIP zero-shot logits. The 'Average' accuracy represents the overall performance across all domains and learning steps, while the 'Last' accuracy focuses on the final performance achieved after all domains are learned.  The graph illustrates how different fusion ratios affect model performance, highlighting the optimal balance between leveraging the incremental learning from RAIL-adapter and the zero-shot generalization ability of CLIP.", "section": "E Ablation studies"}, {"figure_path": "boGxvYWZEq/figures/figures_17_1.jpg", "caption": "Figure 5: RAIL Overview. (a) During inference, the fusion module utilizes the Zero-shot logits to identify whether a test image is aligned with seen or unseen classes. If classified as a seen class, the Fusion logits combine the RAIL-Adapter logits and the Zero-shot logits; otherwise solely rely on the Zero-shot logits. (b) Primal: at the n-th learning step, features Xe extracted by CLIP's image encoder are projected to higher dimensional \u03a6 via RHL and then update the parameter W and memory M by Theorem 1. (c) Dual: features extracted by CLIP's image encoder update the kernel K, parameter \u03b1, and memory Ma by Theorem 2.", "description": "This figure provides a high-level overview of the proposed RAIL method.  It shows the inference stage and the training stages for both primal and dual forms of the ridge regression-based adapter.  The inference stage depicts how the fusion module combines zero-shot logits from CLIP and RAIL adapter logits to classify images.  The training stages illustrate the incremental learning process for both primal (using RHL for non-linear projection) and dual (using a kernel method) ridge regressions.", "section": "4 Approach"}]