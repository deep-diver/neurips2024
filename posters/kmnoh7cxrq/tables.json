[{"figure_path": "kMnoh7CXrq/tables/tables_5_1.jpg", "caption": "Table 1: Performance of DenseFormer and the standard architecture of different sizes on OpenWebText2 dataset. The number of millions of parameters is reported as well as the final perplexity. Additionally the number of batches of size 64 that can be processed in one second is reported as a measure of inference speed. The results are based on three runs with different seeds. The mean value is reported with the standard error reported in parenthesis. In terms of perplexity, DenseFormer clearly outperforms a standard Transformer of the same depth as well as standard Transformers with a similar inference speed. While sometimes a deeper model with the standard architecture can match the performance of a shallower DenseFormer (e.g. 72 block standard architecture and 48 block DenseFormer), inference using the shallow DenseFormer remains much faster. The inference speed is significantly improved with negligible effect on perplexity when increasing the dilation factor and DWA period. Adding a scaling factor to all skip connections in the standard architecture (named Skips with Gains) does not yield the same performance boost as DenseFormer highlighting the importance of inter-block connectivity in DenseFormer.", "description": "This table compares the performance of DenseFormer and standard Transformer models of various sizes on the OpenWebText2 dataset.  It shows the number of parameters, perplexity, and inference speed (in batches per second) for each model.  Results demonstrate that DenseFormer consistently outperforms standard Transformers of the same depth and achieves comparable or better perplexity at significantly faster inference speeds, showcasing its superior efficiency.", "section": "4 Results"}, {"figure_path": "kMnoh7CXrq/tables/tables_7_1.jpg", "caption": "Table 1: Performance of DenseFormer and the standard architecture of different sizes on OpenWebText2 dataset. The number of millions of parameters is reported as well as the final perplexity. Additionally the number of batches of size 64 that can be processed in one second is reported as a measure of inference speed. The results are based on three runs with different seeds. The mean value is reported with the standard error reported in parenthesis. In terms of perplexity, DenseFormer clearly outperforms a standard Transformer of the same depth as well as standard Transformers with a similar inference speed. While sometimes a deeper model with the standard architecture can match the performance of a shallower DenseFormer (e.g. 72 block standard architecture and 48 block DenseFormer), inference using the shallow DenseFormer remains much faster. The inference speed is significantly improved with negligible effect on perplexity when increasing the dilation factor and DWA period. Adding a scaling factor to all skip connections in the standard architecture (named Skips with Gains) does not yield the same performance boost as DenseFormer highlighting the importance of inter-block connectivity in DenseFormer.", "description": "This table compares the performance of DenseFormer and standard Transformer models of different sizes on the OpenWebText2 dataset.  The comparison considers several key metrics: number of parameters, perplexity (a measure of model accuracy), and inference speed (measured in batches per second). The results demonstrate that DenseFormer consistently outperforms standard Transformers, especially in terms of inference speed while achieving similar or better perplexity.  The impact of increasing the dilation factor and DWA period (two hyperparameters of DenseFormer) on inference speed is also highlighted.", "section": "4 Results"}, {"figure_path": "kMnoh7CXrq/tables/tables_8_1.jpg", "caption": "Table 1: Performance of DenseFormer and the standard architecture of different sizes on OpenWebText2 dataset. The number of millions of parameters is reported as well as the final perplexity. Additionally the number of batches of size 64 that can be processed in one second is reported as a measure of inference speed. The results are based on three runs with different seeds. The mean value is reported with the standard error reported in parenthesis. In terms of perplexity, DenseFormer clearly outperforms a standard Transformer of the same depth as well as standard Transformers with a similar inference speed. While sometimes a deeper model with the standard architecture can match the performance of a shallower DenseFormer (e.g. 72 block standard architecture and 48 block DenseFormer), inference using the shallow DenseFormer remains much faster. The inference speed is significantly improved with negligible effect on perplexity when increasing the dilation factor and DWA period. Adding a scaling factor to all skip connections in the standard architecture (named Skips with Gains) does not yield the same performance boost as DenseFormer highlighting the importance of inter-block connectivity in DenseFormer.", "description": "This table compares the performance of DenseFormer and standard Transformer models of different sizes on the OpenWebText2 dataset.  The comparison uses perplexity, the number of parameters, and inference speed (measured in batches per second).  Results are averaged across three runs with different random seeds.  DenseFormer consistently outperforms standard Transformers with the same depth and often outperforms deeper Transformers with similar inference speeds. The table also demonstrates that increasing dilation and DWA period improves inference speed with minimal impact on perplexity. Adding scaling to skip connections in the standard architecture does not replicate DenseFormer's performance gains, highlighting the importance of DenseFormer's inter-block connectivity.", "section": "4 Results"}, {"figure_path": "kMnoh7CXrq/tables/tables_15_1.jpg", "caption": "Table 1: Performance of DenseFormer and the standard architecture of different sizes on OpenWebText2 dataset. The number of millions of parameters is reported as well as the final perplexity. Additionally the number of batches of size 64 that can be processed in one second is reported as a measure of inference speed. The results are based on three runs with different seeds. The mean value is reported with the standard error reported in parenthesis. In terms of perplexity, DenseFormer clearly outperforms a standard Transformer of the same depth as well as standard Transformers with a similar inference speed. While sometimes a deeper model with the standard architecture can match the performance of a shallower DenseFormer (e.g. 72 block standard architecture and 48 block DenseFormer), inference using the shallow DenseFormer remains much faster. The inference speed is significantly improved with negligible effect on perplexity when increasing the dilation factor and DWA period. Adding a scaling factor to all skip connections in the standard architecture (named Skips with Gains) does not yield the same performance boost as DenseFormer highlighting the importance of inter-block connectivity in DenseFormer.", "description": "This table compares the performance of DenseFormer and standard Transformer architectures of different sizes on the OpenWebText2 dataset.  It shows the number of parameters, perplexity, and inference speed (batches per second) for each model.  The results demonstrate that DenseFormer achieves lower perplexity and faster inference speed compared to standard Transformers with the same depth or similar inference speed.  Increasing the dilation factor and DWA period further improves inference speed with minimal impact on perplexity.  The table also shows that simply adding scaling factors to skip connections in the standard Transformer architecture does not produce the same performance gains as DenseFormer, highlighting the importance of DenseFormer's inter-block connectivity.", "section": "4 Results"}, {"figure_path": "kMnoh7CXrq/tables/tables_18_1.jpg", "caption": "Table 4: Comparison on PG-19. Comparing DenseFormers and Transformers on the PG19 dataset. The results show similar improvements as the ones observed on the OpenWebText2 dataset. This demonstrates the generality of our results. Those results were obtained using a batch size of 128.", "description": "This table compares the performance of DenseFormers and standard Transformers on the PG-19 dataset.  It shows the perplexity achieved by each model architecture at different depths (24, 48, and 72 layers).  The results demonstrate that DenseFormers consistently outperform standard Transformers in terms of perplexity, showcasing the effectiveness of the proposed architecture across datasets. The batch size used for these experiments was 128.", "section": "4 Results"}, {"figure_path": "kMnoh7CXrq/tables/tables_19_1.jpg", "caption": "Table 5: Start training the DWA weights after N iterations. At initialization, a DenseFormer is the same as a Transformer. We experiment with tuning the DWA weights only after N iterations. This means the model is trained as a Transformer for N iterations, and as a DenseFormer from N to 40k iterations.", "description": "This table shows the effect of delaying the training of DWA weights.  A 4x5-DenseFormer model was trained for 40k iterations. In the experiment, the training was performed as a standard Transformer for N iterations, then switched to DenseFormer training until the 40k iteration mark.  The table shows the perplexity achieved for different values of N, demonstrating that starting DWA training earlier yields better performance.", "section": "B.4 Delaying the Training of DWA Weights"}, {"figure_path": "kMnoh7CXrq/tables/tables_20_1.jpg", "caption": "Table 1: Performance of DenseFormer and the standard architecture of different sizes on OpenWebText2 dataset. The number of millions of parameters is reported as well as the final perplexity. Additionally the number of batches of size 64 that can be processed in one second is reported as a measure of inference speed. The results are based on three runs with different seeds. The mean value is reported with the standard error reported in parenthesis. In terms of perplexity, DenseFormer clearly outperforms a standard Transformer of the same depth as well as standard Transformers with a similar inference speed. While sometimes a deeper model with the standard architecture can match the performance of a shallower DenseFormer (e.g. 72 block standard architecture and 48 block DenseFormer), inference using the shallow DenseFormer remains much faster. The inference speed is significantly improved with negligible effect on perplexity when increasing the dilation factor and DWA period. Adding a scaling factor to all skip connections in the standard architecture (named Skips with Gains) does not yield the same performance boost as DenseFormer highlighting the importance of inter-block connectivity in DenseFormer.", "description": "This table compares the performance of DenseFormer and standard Transformer models of various sizes on the OpenWebText2 dataset.  Key metrics include the number of parameters, perplexity, and inference speed (batches per second).  It demonstrates DenseFormer's superior performance in terms of perplexity and inference speed compared to standard Transformers of the same depth or similar inference speed. The table also shows that simply adding scaling factors to skip connections in standard Transformers does not achieve the same level of improvement as DenseFormer, highlighting the significance of DenseFormer's inter-block connectivity.", "section": "4 Results"}, {"figure_path": "kMnoh7CXrq/tables/tables_20_2.jpg", "caption": "Table 6: Performance of DenseFormer and the standard architecture of different sizes on OpenWebText2 dataset. Using a batch size of 128, and a DWA period of 1. DenseFormer clearly outperforms a standard architecture of the same depth as well as standard architecture with the same inference speed. While sometimes a deeper model with the standard architecture can match the performance of a shallower DenseFormer, inference using the shallow DenseFormer remains much faster.", "description": "This table compares the performance of DenseFormer and standard Transformer architectures of various depths on the OpenWebText2 dataset.  It shows perplexity scores and inference speed (measured in batches per second). The results demonstrate DenseFormer's superior performance in terms of perplexity and speed compared to standard Transformers at various depths and sizes.", "section": "4 Results"}, {"figure_path": "kMnoh7CXrq/tables/tables_22_1.jpg", "caption": "Table 1: Performance of DenseFormer and the standard architecture of different sizes on OpenWebText2 dataset. The number of millions of parameters is reported as well as the final perplexity. Additionally the number of batches of size 64 that can be processed in one second is reported as a measure of inference speed. The results are based on three runs with different seeds. The mean value is reported with the standard error reported in parenthesis. In terms of perplexity, DenseFormer clearly outperforms a standard Transformer of the same depth as well as standard Transformers with a similar inference speed. While sometimes a deeper model with the standard architecture can match the performance of a shallower DenseFormer (e.g. 72 block standard architecture and 48 block DenseFormer), inference using the shallow DenseFormer remains much faster. The inference speed is significantly improved with negligible effect on perplexity when increasing the dilation factor and DWA period. Adding a scaling factor to all skip connections in the standard architecture (named Skips with Gains) does not yield the same performance boost as DenseFormer highlighting the importance of inter-block connectivity in DenseFormer.", "description": "This table compares the performance of DenseFormer and standard Transformer models of various sizes on the OpenWebText2 dataset.  It shows the number of parameters, perplexity, and inference speed (batches per second) for each model.  The results demonstrate DenseFormer's superiority in terms of perplexity and inference speed, especially when considering models with similar depths or inference times. The table also highlights the impact of DenseFormer's architectural improvements, as increasing the dilation factor and DWA period significantly enhances inference speed without sacrificing perplexity.", "section": "4 Results"}]