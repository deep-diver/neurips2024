[{"type": "text", "text": "Nearly Minimax Optimal Submodular Maximization with Bandit Feedback ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Artin Tajdini, Lalit Jain, Kevin Jamieson University of Washington, Seattle, WA {artin, jamieson}@cs.washington.edu, lalitj@uw.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider maximizing an unknown monotonic, submodular set function $f:$ $2^{[n]}\\to[0,1]$ with cardinality constraint under stochastic bandit feedback. At each time $t=1,\\dots,T$ the learner chooses a set $S_{t}\\subset[n]$ with $|S_{t}|\\leq k$ and receives reward $f(S_{t})+\\eta_{t}$ where $\\eta_{t}$ is mean-zero sub-Gaussian noise. The objective is to minimize the learner\u2019s regret with respect to an approximation of the maximum $f(S_{*})$ with $|S_{*}|\\;=\\;k$ , obtained through robust greedy maximization of $f$ . To date, the best regret bound in the literature scales as $k n^{1/3}T^{2/3}$ . And by trivially treating every set as a unique arm one deduces that $\\sqrt{\\binom{n}{k}T}$ is also achievable using standard multi-armed bandit algorithms. In this work, we establish the first minimax lower bound for this setting that scales like $\\tilde{\\Omega}(\\operatorname*{min}_{L\\leq k}(L^{1/3}n^{1/3}T^{2/3}+$ $\\sqrt{\\binom{n}{k-L}T})$ ). For a slightly restricted algorithm class, we prove a stronger regret lower bound of $\\begin{array}{r}{\\tilde{\\Omega}(\\operatorname*{min}_{L\\leq k}(L n^{1/3}T^{2/3}+\\sqrt{\\binom{n}{k-L}T}))}\\end{array}$ . Moreover, we propose an algorithm Sub-UCB that achieves regret $\\begin{array}{r l}{\\lefteqn{\\tilde{\\mathcal{O}}(\\operatorname*{min}_{L\\leq k}(L n^{1/3}T^{2/3}+\\sqrt{\\binom{n}{k-L}T}))}\\quad}&{{}}\\end{array}$ capable of matching the lower bound on regret for the restricted class up to logarithmic factors. ", "page_idx": 0}, {"type": "text", "text": "1 INTRODUCTION ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Optimizing over sets of $n$ ground items given noisy feedback is a common problem. For example, when a patient comes into the hospital with sepsis (bacterial infection of the blood), it is common for a cocktail of $1<k\\le n$ antibiotics to be prescribed. This can be attractive for reasons including 1) the set could be as effective (or more) than a single drug alone, but each unit of the cocktail could be administered at a far lower dosage to avoid toxicity, or 2) could be more robust to resistance by blocking a number of different pathways that would have to be overcome simultaneously, or 3) could cover a larger set of pathogens present in the population. In this setting the prescriber wants to balance exploration with exploitation over different subsets to maximize the number of patients that survive. As a second example, we consider factorial optimization of web-layouts: you have $n$ pieces of content and $k$ locations on the webpage to place them\u2013how do you choose subsets to maximize metrics like click-through rate or engagement? ", "page_idx": 0}, {"type": "text", "text": "Given there are $\\approx\\,n^{k}$ ways to choose $k$ items amongst a set of $n$ , this optimization problem is daunting. It is further complicated by the fact that for any set $S_{t}$ that we evaluate at time $t$ , we only get to observe a noisy realization of $f$ , namely $y_{t}=f(S_{t})\\!+\\!\\eta_{t}$ where $\\eta_{t}$ is mean-zero, sub-Gaussian noise. In the antibiotics case, this could be a Bernoulli indicating whether the patient recovered or not, and in the web-layout case this could be a Bernoulli indicating a click or a (clipped) real number to represent the engagement time on the website. To make this problem more tractable, practitioners make structural assumptions about $f$ . A common assumption is to assume that higherorder interaction terms are negligible [20, 12]. For example, assuming only interactions up to the second degree would mean that there exist parameters $\\theta^{(0)}\\in\\mathbb{R}$ , $\\theta^{(1)}\\in\\mathbb{R}^{n}$ , and $\\boldsymbol{\\theta}^{(2)}\\in\\mathbb{R}^{\\dot{(n)}}$ such that ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\nf(S)=\\theta^{(0)}+\\sum_{i\\in S}\\theta_{i}^{(1)}+\\sum_{i,j\\in S,i\\neq j}\\theta_{i,j}^{(2)}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "However, this model can be very restrictive and even if true, the number of unknowns scales like $n^{2}$ which could still be intractably large. ", "page_idx": 1}, {"type": "text", "text": "An alternative strategy is to remain within a non-parametric class, but reduce our ambitions to measuring performance relative to a different benchmark which is easier to optimize. We say a set function $f:2^{[n]}\\to\\mathbb{R}$ is increasing and submodular if for all $A\\subset B\\subset[n]$ we have $f(A)\\leq f(B)$ and ", "page_idx": 1}, {"type": "equation", "text": "$$\nf(A\\cup B)+f(A\\cap B)\\leq f(A)+f(B).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Such a condition limits how quickly $f$ can grow and captures some notion of diminishing returns. Diminishing returns is reasonable in both the antibiotics and webpage optimization examples. It is instructive to note that a sufficient condition for the parametric form of (1) to be submodular is for $\\operatorname*{max}_{i,j}\\theta_{i,j}^{(2)}\\,\\leq\\,0$ . But in general, $f$ still has $\\approx n^{k}$ degrees of freedom even if it is monotonic and submodular. And it is known that for an unknown $f$ , identifying $S^{*}:=\\arg\\operatorname*{max}_{S\\subset[n]:|S|=k}f(S)$ may require evaluating $f$ as many as $n^{k}$ times. ", "page_idx": 1}, {"type": "text", "text": "The power of submodularity is made apparent through the famous result of [26] which showed that the greedy algorithm which grows a set one item at a time by adding the item with the highest marginal gain returns a solution that is within a $\\scriptstyle(1-e^{-1})$ -multiplicative factor of the optimal solution. That is, if we begin with $S_{g r}^{f}=\\emptyset$ and set $\\begin{array}{r}{S_{g r}^{f}\\leftarrow\\arg\\operatorname*{max}_{i\\in[n]\\setminus S_{g r}^{f}}f(S_{g r}^{f}\\cup\\{i\\})}\\end{array}$ until $|S_{g r}^{f}|=k$ , then $f(S_{g r}^{f})\\geq(1-1/e)f(S_{*}^{f})$ where $S_{*}^{f}:=\\arg\\operatorname*{max}_{S\\in[n]:|S|\\le k}f(S)$ if $f$ is increasing and submodular. This result is complemented by [13] which shows achieving any $\\left(1-e^{-1}+\\epsilon\\right)$ -approximation is NP-Hard. Under additional assumptions like curvature, this guarantee can be strengthened. ", "page_idx": 1}, {"type": "text", "text": "Due to the centrality of the greedily constructed set to the optimization of a submodular function, it is natural to define a performance measure relative to the greedily constructed set. However, as discussed at length in the next section, because we only observe noisy observations of the underlying function, recovering the set constructed greedily from noiseless evaluations is too much to hope for. Consequently, there is a more natural notion of regret against a noisy greedy solution, denoted $R_{\\mathbf{gr}}$ , that actually appears in the proofs of all upper bounds found in the literature for this setting (see the next section for a definition). ", "page_idx": 1}, {"type": "text", "text": "For this notion of regret, previous works have demonstrated that a regret bound of $R_{\\mathbf{gr}}\\ =$ $O(\\mathrm{poly}(k)n^{1/3}T^{2/3})$ is achievable ([28], [36]). This $T^{2/3}$ rate is unusual in multi-armed bandits, where frequently we expect a regret bound to scale as $T^{1/2}$ . On the other hand, by treating each $k$ -subset as a separate arm, one can easily adapt existing algorithms to achieve a regret bound of $\\sqrt{\\binom{n}{k}T}$ . This leads to the following question: ", "page_idx": 1}, {"type": "text", "text": "Does there exist an algorithm that obtains $\\sqrt{n^{r}T}$ regret for $r\\,=\\,o(k)$ on every instance? And if not, what is the optimal dependence on $k$ and $n$ for a bound scaling like $T^{2/3}$ ? ", "page_idx": 1}, {"type": "text", "text": "To address these questions, we prove a minimax lower bound and complement the result with an algorithm achieving a matching upper bound. To be precise, the contributions of this paper include: ", "page_idx": 1}, {"type": "text", "text": "\u2022 A minimax lower bound demonstrating that $R_{\\mathbf{gr}}\\ =\\ \\tilde{\\Omega}\\Big(\\operatorname*{min}_{0\\le L\\le k}(L^{1/3}n^{1/3}T^{2/3}\\ +$ $\\sqrt{\\binom{n}{k-L}T})$ . In words, for small $T$ , a $T^{2/3}$ regret bound is inevitable, for large $T$ the $\\sqrt{\\binom{n}{k}T}$ bound is optimal, with an interpolating regret bound for in between. \u2013 For slightly restricted class of algorithms with non-adaptive greedy error threshold, we have the improved $R_{\\mathbf{gr}}={\\tilde{\\Omega}}({\\Big(}{\\operatorname*{min}}_{0\\leq L\\leq k}(L n^{1/3}T^{2/3}+{\\sqrt{{\\binom{n}{k-L}}T}}){\\Big)}$ ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose an algorithm that for any increasing, submodular $f$ , we have $\\begin{array}{r l}{R_{\\mathbf{gr}}}&{{}=}\\end{array}$ $\\begin{array}{r}{\\tilde{\\mathcal{O}}\\operatorname*{min}_{0\\leq L\\leq k}(L n^{1/3}T^{2/3}+\\sqrt{\\binom{n}{k-L}T})}\\end{array}$ . As this matches our lower bound, we conclude that this is the first provably tight algorithm for optimizing increasing, submodular functions with bandit feedback. Existing algorithms construct a set by greedily adding $k$ items. Our main insight is that it is actually optimal to build up a set up to a size $i^{*}$ and then for the remaining stages play sets of size $k$ that include the initial set of size $i^{*}$ . Our choice of $i^{*}$ is directly motivated by our lower bound. ", "page_idx": 2}, {"type": "text", "text": "In what remains, we will formally define the problem, discuss the related work, and then move on to the statement of the main theoretical results. Experiments and conclusions follow. ", "page_idx": 2}, {"type": "text", "text": "1.1 Problem Statement ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $[n]=\\{1,\\ldots,n\\}$ denote the set of base arms, $T$ be the time horizon, and $k$ be a given cardinality constraint. At time $t$ , the agent selects a set $S_{t}\\subset[n]$ where $|S_{t}|\\leq k$ , and observes reward $f(S_{t})\\!+\\!\\eta_{t}$ where $\\eta_{t}$ is i.i.d. mean-zero 1-sub-Gaussian noise, and $f:2^{[n]}\\to[0,1]$ is an unknown monotone non-decreasing submodular function defined for all sets of cardinality at most $k$ . ", "page_idx": 2}, {"type": "text", "text": "Ideally, our goal would be to minimize the regret relative to pulling the best set $S^{*}:=$ arg $:\\operatorname*{max}_{|S|\\leq k}f(S)$ at each time. In general, even if we had the ability to evaluate the true function $f(\\cdot)$ (i.e. without noise), maximizing a submodular function with a cardinality constraint is NP-hard. However, greedy algorithms which sequentially add points, i.e. $S^{(i+1)}~=$ arg $\\operatorname*{max}_{a\\notin S^{(i)}}$ $f(S^{(i)}\\cup a),1\\,\\le\\,i\\,\\le\\,k$ guarantee that $f(S^{(k)})\\,\\geq\\,\\alpha f(S^{\\star})$ with $\\alpha\\,\\geq\\,1\\,-\\,1/e$ in worst-case. Unfortunately, since we do not know $f(\\cdot)$ and instead only have access to noisy observations, running the greedy algorithm on any estimate ${\\hat{f}}(\\cdot)$ may not necessarily guarantee an $\\alpha=(1-1/e)$ -approximation to $\\overline{{f}}(S^{\\ast})^{1}$ . ", "page_idx": 2}, {"type": "text", "text": "Consequently, a natural notion to address noisy observations is an $\\epsilon$ -approximate greedy set for $\\epsilon\\in[0,\\bar{1}]^{k}$ . We define the following collection of sets of size $k$ ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S^{k,\\epsilon}=\\{S=S^{(k)}\\supset\\dots\\supset S^{(1)},|S^{(i)}|=i,}\\\\ &{\\qquad\\qquad\\operatorname*{max}_{a\\notin S^{(i)}}f(S^{(i)}\\cup\\{a\\})-f(S^{(i+1)})\\leq\\epsilon_{i}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Intuitively, any $S\\in S^{k,\\epsilon}$ can be thought of as being constructed from a process that adds an element at stage $i$ which is $\\epsilon_{i}$ -optimal compared to the Greedy algorithm run on $f$ . Such a set naturally arises as the output of the Greedy algorithm run on an approximation $\\hat{f}$ . This set enjoys the following guarantee. ", "page_idx": 2}, {"type": "text", "text": "Lemma 1.1. (Theorem $6$ in [36]) For any $\\mathbf{\\epsilon}\\mathbf{\\epsilon}\\mathbf{\\epsilon}\\mathbf{\\geq0}\\in\\mathbb{R}^{k}$ , and $S_{g r}^{k,\\epsilon}\\in S^{k,\\epsilon}$ , we have ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(S_{g r}^{k,\\epsilon})+\\mathbf{1}^{T}\\boldsymbol{\\epsilon}\\geq(1-e^{-1})f(S^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Lemma 1.1 is a noise-robust analogous result to the approximation ratio of the perfect greedy algorithm of [26] that says $f(S_{\\mathbf{gr}}^{k,0})\\geq(1-e^{-1})f(S^{*})$ . Note that $|S^{k,\\epsilon}|$ is non-decreasing in $\\epsilon_{i}$ for all $i\\in[k]$ , so identifying a set in $S^{k,\\epsilon}$ is in some sense easier for a larger ${\\mathbf{1}}^{T}{\\boldsymbol{\\epsilon}}$ . Thus, to define an appropriate definition of regret, the measure must balance the facts that comparing with the noiseless greedy approximation in $\\bar{S}^{k,0}$ may be impossible, but should account for identifying a set in $S^{k,\\epsilon}$ is strictly easier for larger ${\\bf1}^{T}{\\boldsymbol\\epsilon}$ . Inspired by the above lemma we define robust greedy regret ", "page_idx": 2}, {"type": "equation", "text": "$$\nR_{\\mathbf{gr}}:=\\operatorname*{min}_{\\epsilon\\geq\\mathbf{0},S_{\\mathbf{gr}}^{k,\\epsilon}\\in S^{k,\\epsilon}}R(S_{\\mathbf{gr}}^{k,\\epsilon})+T\\mathbf{1}^{T}\\epsilon\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ", "page_idx": 2}, {"type": "equation", "text": "$$\nR(S):=\\sum_{t=1}^{T}f(S)-f(S_{t}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "table", "img_path": "Vn0FWRImra/tmp/e142763a778272791e211a59cf4e37bce7b7244c5b85ebca77f3ce4ad39b7ebe.jpg", "table_caption": [], "table_footnote": ["Table 1: Best known regret bounds for combinatorial multiarmed bandits under different assumptions. By lemma 1.1 our upperbound can also be stated for $R_{1-e^{-1}}$ . We note that our lower bound proven for the stochastic setting immediately applies to the adversarial setting in the table. "], "page_idx": 3}, {"type": "text", "text": "This notion of regret captures the fact that if the algorithm plays a set in $S^{k,\\epsilon}$ then they may be incurring up to 1T\u03f5 extra regret. Note that when $\\epsilon=0$ achieves the minimum (which can happen if the \u201cgaps\u201d between the greedily added element and all other elements at each stage is large) then this notion of regret is relative to the greedy set constructed in the noiseless setting. ", "page_idx": 3}, {"type": "text", "text": "The definition of regret in (3) is not novel to our paper. This notion is implicitly used in [36] in the proofs of Lemma 3 for the full-feedback setting and Theorem 13 for the bandit feedback setting, [28] in Theorem 4.1, [31] in Theorem 1, [27] in Theorem 2 for the full-feedback setting and Theorem 4 for bandit feedback, and [29] in Theorem 1. However, readers of these papers will note that they report their results not in terms of $R_{\\mathbf{gr}}$ , but $\\alpha$ -Regret: for an $\\alpha\\,\\in\\,[0,1]$ , define $\\alpha$ -regret by, $\\begin{array}{r}{R_{\\alpha}:=\\sum_{t=1}^{T}\\alpha f(S^{*})-f(S_{t})}\\end{array}$ where $S^{*}:=\\arg\\operatorname*{max}_{|S|\\leq k}f(S)$ . Using Lemma 1, one immediately has tha t $R_{\\alpha}\\leq R_{\\mathbf{gr}}$ for $\\alpha=(1-e^{-1})$ . Thus, an upper bound on (3) immediately results in an upper bound on $R_{\\alpha}$ , which is precisely what previous works exploit to obtain their upper bounds on $R_{\\alpha}$ . ", "page_idx": 3}, {"type": "text", "text": "To summarize: all the analyses of these previous works concentrate on showing an upper bound on $R_{\\mathbf{gr}}$ , and only at the last step argue that $R_{\\alpha}\\leq R_{\\mathbf{gr}}$ , and report an upper bound on $R_{\\alpha}$ . But $R_{\\alpha}$ can be a very loose lower bound on $R_{\\mathbf{gr}}$ ! For instance, when the function is modular (the inequalities of submodularity are tight), and the gap between the best set and worst set is equal to $\\Delta<e^{-1}$ , then a random selection algorithm would get zero or even negative $R_{\\alpha}$ regret, while $R_{\\mathbf{gr}}$ would be linear $\\Delta T$ , which is more natural. Thus, in studying regret against approximations attained by an offline step-wise greedy procedure, $R_{g r}$ can be a more appropriate measure than $R_{\\alpha}$ ", "page_idx": 3}, {"type": "text", "text": "1.2 Related Work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "There has been several works on combinatorial multi-armed bandits with submodular assumptions and different feedback assumptions. Table 1 summarizes of the most relevant results as well as the results of this paper. For monotonic submodular maximization specifically, previous work use Lemma 1.1 with appropriate $\\epsilon$ to prove an upper bound on expected $R_{\\alpha}$ -regret when the greedy result with perfect information gives an $\\alpha$ -approximation of the actual maximum value. ", "page_idx": 3}, {"type": "text", "text": "Stochastic In the stochastic setting, when the expected reward function is submodular and monotonic, [28] proposed an explore-then-commit algorithm with full-bandit feedback that achieves $R_{\\mathbf{gr}}\\,=\\,\\mathcal{O}(k^{\\Bar{4}/3}\\Bar{T}^{2/3}n^{1/3})^{2}$ . Recently, [31] showed with the same explore-then-commit algorithm with different parameters, $R_{\\mathbf{gr}}={\\mathcal{O}}(k n^{1/3}T^{2/3}+k n^{2/3}T^{1/3}d)$ is possible with delay feedback parameter of $d$ . Without the monotonicity, [15] achieves $R_{\\alpha}\\,=\\,\\mathcal{O}(n T^{2/3})$ with bandit feedback for $\\alpha=1/2$ . There have also been several works in the semi-bandit feedback setting ([42], [45]), and others such as getting the marginal gain of each element after each query. ", "page_idx": 3}, {"type": "text", "text": "Adversarial In the adversarial setting, the environment chooses an arbitrary sequence of monotone submodular functions $\\{f_{1},\\ldots,f_{T}\\}$ , and the goal is to minimize regret against an approximation of the reward of the best set in hindsight ([17], [19], [38], [41]). [37] showed $\\mathcal{O}(\\dot{k}\\sqrt{T n\\log n})$ $R_{(1-e^{-1})}$ -regret is possible with partially transparent feedback(where after each round, $f(S^{(i)})$ for all $i$ is revealed instead of only $f(S^{(k)}))$ and $\\mathcal{O}(k n^{1/3}T^{2/3})\\,R_{(1-e^{-1})}$ -regret for the bandit-feedback setting. [27] proposed a generalized algorithm with $\\tilde{\\mathcal{O}}(k n^{2/3}T^{2/3})$ $R_{(1-e^{-1})}$ -regret with full bandit feedback, and showed all explore-then-commit greedy algorithms have $\\Omega(T^{2/3})$ regret, when applied to our setting. Without the monotone assumption, [27] gets $\\mathcal{O}(n T^{2/3})~R_{(1/2)}.$ -regret with bandit feedback. The upper-bound results in the adversarial setting doesn\u2019t naturally lead to results in the stochastic setting as the function is submodular and monotone only in expectation in the stochastic setting. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Continuous Submodular There are several works on online maximization of the continuous extensions of submodular set functions to a compact subspace such as Lov\u00e1sz and multilinear extensions([3], [14]). With a stronger assumption of DR-submodularity, it\u2019s possible to achieve higher approximation ratio guarantees and lower regret bounds ([7], [8], [33]). [41] uses multilinear extension to achieve $\\bar{O(T^{2/3})}$ $R_{(1-e^{-1})}$ -regret for adversarial submodular maximization with partition matroid constraint. ", "page_idx": 4}, {"type": "text", "text": "Low-degree polynomial In general reward functions without the submodular assumption, [12] showed if the reward function is a $d\\!.$ -degree polynomial, $\\Theta\\big(\\operatorname*{min}(\\sqrt{n^{d}T},\\sqrt{n^{k}T})\\big)$ regret is optimal. ", "page_idx": 4}, {"type": "text", "text": "2 LOWER BOUND ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Theorem 2.1. For any $n\\geq4$ , $k\\,\\leq\\,\\lfloor n/3\\rfloor$ , satisfying $512k^{7}n\\,\\leq\\,T\\,\\in\\,\\mathbb{N},$ , let $\\mathcal{F}$ denote the set of submodular functions that are non-decreasing and bounded by $[0,1]$ for sets of size $k$ or less, with $f({\\boldsymbol{\\varnothing}})=0$ . Then ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\mathbb{A}\\mathbb{g}}\\operatorname*{sup}_{f\\in\\mathcal{F}}\\mathbb{E}[R_{g r}]\\ge\\frac{1}{16}(k-i^{*})^{1/3}T^{2/3}n^{1/3}e^{-8}+\\frac{1}{4}T^{1/2}\\sqrt{\\binom{n-k}{i^{*}}}e^{-2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the infimum is over all randomized algorithms and the supremum is over the functions in $\\mathcal{F}$ , and $i^{*}\\in[k]$ is the largest value satisfying ${\\frac{16}{n^{2}k^{6}}}\\left({n-k\\atop i^{*}}^{3}\\right)^{3}\\leq T$ . ", "page_idx": 4}, {"type": "text", "text": "The lowerbound is intuitively a mix of the greedy explore-then-commit algorithm for the first $k-i^{*}$ arms, and then a standard MAB algorithm between all superarms of cardinality $k$ that include those elements. For small $T$ (i.e. $T=\\mathcal{O}(n^{4}))$ the regret would be $\\Omega(k^{1/3}n^{1/3}T^{2/3})$ , and for large $T({\\mathrm{i.e.}}$ $T\\,=\\,\\Omega(n^{3k-2}))$ the regret would be $\\Omega({\\binom{n}{k}}^{1/2}T^{1/2})$ . This lowerbound also immediately gives a lower bound for the adversarial setting where $f_{i}\\;=\\;f+\\mathcal{N}(0,1)$ is the function chosen by the environment at time $i$ . ", "page_idx": 4}, {"type": "text", "text": "Proof Sketch We construct a hard instance so that at each cardinality a single set gives an elevated reward. Focusing on $k=2$ for illustration, the instance would be the following: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{H}_{0}:=\\left\\{\\begin{array}{l l}{f(\\{i\\})=1/2}&{\\mathrm{~if~}i\\in\\{1\\}}\\\\ {f(\\{i\\})=1/2-\\Delta}&{\\mathrm{~if~}i\\in[n]\\setminus\\{1\\}}\\\\ {f(\\{i,j\\})=3/4}&{\\mathrm{~if~}(i,j)=(1,2)}\\\\ {f(\\{i,j\\})=3/4-\\Delta}&{\\mathrm{~if~}(i,j)\\in\\binom{[n]}{2}\\setminus\\{(1,2)\\}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\Delta$ is the gap of the best set that we will tune based on $T$ . Pulling any arm of cardinality less than 2 would incur $\\Omega(1)$ regret, however, since there are only $n$ such sets (compared to $\\binom{n}{2}$ sets of size 2), pulling these simple arms give more information on the optimal set. ", "page_idx": 4}, {"type": "text", "text": "For a set of alternative instances, we choose a set of size $k$ and elevate its reward by $2\\Delta$ . We also elevate every prefix set of a permutation of this set by $2\\Delta$ so that the new set can be found by a ", "page_idx": 4}, {"type": "text", "text": "greedy algorithm. Again, for $k=2$ , and any $\\{\\hat{i},\\hat{j}\\}\\in[n]\\backslash\\{1,2\\}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{H}_{\\widehat{i},\\widehat{j}}:=\\left\\{\\begin{array}{l l}{f(\\{i\\})=1/2}&{\\mathrm{~if~}i\\in\\{1\\}}\\\\ {f(\\{i\\})=1/2+\\Delta}&{\\mathrm{~if~}i\\in\\{\\widehat{i}\\}}\\\\ {f(\\{i\\})=1/2-\\Delta}&{\\mathrm{~if~}i\\in[n]\\setminus\\{1,\\widehat{i}\\}}\\\\ {f(\\{i,j\\})=3/4}&{\\mathrm{~if~}(i,j)=(1,2)}\\\\ {f(\\{i,j\\})=3/4+\\Delta}&{\\mathrm{~if~}(i,j)=\\widehat{(i,\\widehat{j})}}\\\\ {f(\\{i,j\\})=3/4-\\Delta}&{\\mathrm{~Otherwise~}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that, if $\\Delta\\,<\\,\\frac{1}{16}$ for the $k\\,=\\,2$ instance, All the functions are submodular, as $f(\\{a,b\\})\\mathrm{~-~}$ $\\begin{array}{r}{f\\bigl(\\{b\\}\\bigr)\\leq\\frac{1}{4}+2\\Delta\\leq\\ensuremath{1}/2-\\Delta\\leq f\\bigl(\\{a\\}\\bigr)-f\\bigl(\\{\\phi\\}\\bigr)}\\end{array}$ for any $a,b\\in[n]$ . ", "page_idx": 5}, {"type": "text", "text": "For $\\mathbf{H}_{0}$ , if $\\epsilon_{i}<\\Delta$ for all $i\\in$ [2], then $\\begin{array}{r}{f_{\\mathcal{H}_{0}}(S_{g r}^{2,\\epsilon})=\\frac{3}{4}}\\end{array}$ as the noisy greedy finds the best arm, and otherwise 1T \u03f5 \u2265\u2206, so min\u03f5\u22650 fH0(Sg2,r\u03f5 ) + 1T \u03f5 = 43. Similarly, $\\begin{array}{r}{\\operatorname*{min}_{\\epsilon\\geq0}f_{\\mathbf{H}_{\\widehat{i},\\widehat{j}}}(S_{g r}^{2,\\epsilon})+\\mathbf{1}^{T}\\epsilon=}\\end{array}$ ${\\textstyle{\\frac{3}{4}}}+\\Delta$ . So for these instances $R_{\\mathbf{gr}}=R(S^{*})$ . ", "page_idx": 5}, {"type": "text", "text": "We show that if the $\\mathrm{KL}$ divergence between an alternate instance and $\\mathbf{H}_{0}$ is small, then the algorithm cannot distinguish between the two environments and the maximum regret of the two would be $\\Omega(\\Delta T)$ . Let $\\mathbb{P}_{\\widehat{i},\\widehat{j}},\\mathbb{E}_{\\widehat{i},\\widehat{j}}$ be the probability and expectation under $\\mathbf{H}_{\\widehat{i},\\widehat{j}}$ , respectively when executing some fixed  al gori t hm with observations being corrupted by stand a rd Gaussian noise. Then $\\begin{array}{r}{K L(\\bar{\\mathbb{P}}_{0}|\\mathbb{P}_{\\widehat{i},\\widehat{j}})=\\frac{\\Delta^{2}}{2}\\left(\\bar{\\mathbb{E}}_{0}[T_{\\widehat{i}}]+4\\mathbb{E}_{0}[T_{\\widehat{i},\\widehat{j}}]\\right)}\\end{array}$ for $k=2$ , where $T_{S}$ is the number of pulls of set $S$ , and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathfrak{T}_{0}[R_{\\mathbf{gr}}]+\\mathbb{E}_{\\widehat{\\iota},\\widehat{\\boldsymbol{j}}}[R_{\\mathbf{gr}}]\\geq\\frac{1}{2}\\sum_{i=1}^{n}\\mathbb{E}_{0}[T_{i}]+\\frac{\\Delta T}{2}\\Big(\\mathbb{P}_{0}(T_{1,2}\\leq\\frac{T}{2})+\\mathbb{P}_{\\widehat{\\iota},\\widehat{\\boldsymbol{j}}}(T_{1,2}>\\frac{T}{2})\\Big)}\\\\ &{\\displaystyle\\geq\\frac{1}{2}\\sum_{i=1}^{n}\\mathbb{E}_{0}[T_{i}]+\\frac{\\Delta T}{4}\\exp(-K L(\\mathbb{P}_{0}|\\mathbb{P}_{\\widehat{i},\\widehat{\\boldsymbol{j}}}))=\\frac{1}{2}\\sum_{i=1}^{n}\\mathbb{E}_{0}[T_{i}]+\\frac{\\Delta T}{4}\\exp\\Big(-2\\Delta^{2}\\big(\\mathbb{E}_{0}[T_{i}]+\\mathbb{E}_{0}[T_{\\widehat{i},\\widehat{\\boldsymbol{j}}}]\\big)\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Since $\\widehat{i},\\widehat{j}$ were arbitrary, the following Lemma shows that there exist a pair that are pulled for small numbe r  of times in expectation (see Lemma A.2 for general $k$ ). ", "page_idx": 5}, {"type": "text", "text": "Lemma 2.2. There exists a pair $\\widehat{i},\\widehat{j}$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{0}[T_{\\widehat{i}}]+\\mathbb{E}_{0}[T_{\\widehat{i},\\widehat{j}}]\\leq\\frac{2\\sum_{i}\\mathbb{E}_{0}[T_{i}]}{n-2}+\\frac{T}{\\binom{n-2}{2}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof. For a pair $(i,j)$ , define $Q_{(i,j)}:=\\mathbb{E}_{0}[T_{i}]+\\mathbb{E}_{0}[T_{i,j}]$ . Then the sum of this term for all pairs not equal to $1,2$ would be ", "page_idx": 5}, {"type": "equation", "text": "$$\nQ:=\\sum_{(i,j)\\neq(1,2)}Q_{(i,j)}\\leq(n-3)\\sum_{i\\neq(1,2))}\\mathbb{E}_{0}[T_{i}]+\\sum_{i,j\\neq1,2}\\mathbb{E}_{0}[T_{i,j}]\\leq(n-3)\\sum_{i}\\mathbb{E}_{0}[T_{i}]+T\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then by Pigeonhole principal there exist a pair $\\widehat{i},\\widehat{j}$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\nQ_{(\\widehat{i},\\widehat{j})}\\leq\\frac{Q}{\\binom{n-2}{2}}\\leq\\frac{2}{n-2}\\sum_{i}\\mathbb{E}_{0}[T_{i}]+\\frac{T}{\\binom{n-2}{2}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Using the lemma, for some $(\\widehat{i},\\widehat{j})$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{0}[R_{\\mathbf{gr}}]+\\mathbb{E}_{\\hat{\\imath},\\hat{\\jmath}}[R_{\\mathbf{gr}}]\\geq\\frac{1}{2}\\sum_{i=1}^{n}\\mathbb{E}_{0}[T_{i}]+\\frac{\\Delta T}{4}\\exp\\Big(-2\\Delta^{2}\\big(\\frac{2}{n-2}\\sum_{i}\\mathbb{E}_{0}[T_{i}]+\\frac{T}{\\binom{n-2}{2}}\\big)\\Big)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We choose an appropriate $\\Delta$ based on value of $i^{*}$ . ", "page_idx": 5}, {"type": "text", "text": "\u2022 If $i^{*}=1$ , then for $\\Delta=T^{-1/3}n^{1/3}$ , we have $\\begin{array}{r}{\\frac{2\\Delta^{2}T}{\\binom{n-2}{2}}\\leq1}\\end{array}$ . So either the KL divergence is less than 2, then the regret is lowerbounded by $\\Delta{\\cal T}e^{-2}=T^{2/3}n^{1/3}e^{-2}$ , or for KL divergence to be larger than 2 we would have $\\begin{array}{r}{\\sum_{i}\\mathbb{E}_{0}[T_{i}]\\ge\\frac{1}{4}T^{2/3}n^{1/3}}\\end{array}$ , which from the above equation shows the regret is $\\Omega(T^{2/3}n^{1/3})$ .  This can be extended for expected value of pulls of each cardinality lower than $i^{*}+1$ for general $k$ . ", "page_idx": 6}, {"type": "text", "text": "\u2022 If $\\begin{array}{r l r}{i^{*}}&{{}=}&{2}\\end{array}$ , then it can be shown that the term $\\begin{array}{r}{{\\frac{1}{2}}\\sum_{i=1}^{n}\\mathbb{E}_{0}[T_{i}]\\ +\\ {\\frac{\\Delta T}{4}}\\exp\\left(\\ -\\ \\right.}\\end{array}$ $\\begin{array}{r}{2\\Delta^{2}\\big(\\frac{2}{n-2}\\sum_{i}\\mathbb{E}_{0}[T_{i}]+(T-\\sum_{i=1}^{n}\\mathbb{E}_{0}[T_{i}])/\\binom{n-2}{2}\\big)\\big)}\\end{array}$ with $\\Delta\\;=\\;\\sqrt{{\\binom{n-2}{2}}/T}$ minimizes when $\\begin{array}{r}{\\sum_{i=1}^{n}\\mathbb{E}_{0}[T_{i}]=0}\\end{array}$ i.e. zero single arm sets being pulled in expectation, so the regret would  be $\\bar{T^{1/2}}\\binom{n-2}{2}^{1/2}\\exp(-2)$ . ", "page_idx": 6}, {"type": "text", "text": "This shows that the expected regret is $\\tilde{\\Omega}(\\operatorname*{min}_{i}(i^{1/3}n^{1/3}T^{2/3}+\\sqrt{{\\binom{n}{k-i}}T}))$ . The instance of general $k$ , and the detailed proof is in appendix A.1. \u25a1 ", "page_idx": 6}, {"type": "text", "text": "We define an algorithm to be in non-adaptive greedy error-threshold class against $R_{\\mathbf{gr}}$ regret, if it selects $\\epsilon_{1}^{\\prime},\\ldots,\\epsilon_{k}^{\\prime}$ at the start only dependent on parameters $T,n,k$ before any arm pulls, and minimizes regret against $f(S_{\\mathbf{gr}}^{k,\\epsilon^{\\prime}})+\\mathbf{1}^{T}\\epsilon^{\\prime}$ . All the algorithms from previous work in the literature fall within this restricted class, and with this extra assumption we can prove a stronger lower bound. Theorem 2.3. For any $n\\geq4$ , $k\\,\\leq\\,\\lfloor n/3\\rfloor$ , satisfying $512k^{9}n\\,\\leq\\,T\\,\\in\\,\\mathbb{N},$ , let $\\mathcal{F}$ denote the set of submodular functions that are non-decreasing and bounded by $[0,1]$ for sets of size $k$ or less, with $f({\\boldsymbol{\\varnothing}})=0$ . Then ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\mathsf{A l g\\,e N A E T}}\\operatorname*{sup}_{f\\in\\mathcal{F}}\\mathbb{E}[R_{g r}]\\geq\\frac{1}{288}(k-i^{*})T^{2/3}n^{1/3}e^{-10}+\\frac{1}{4}T^{1/2}\\sqrt{\\binom{n-k}{i^{*}}}e^{-2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the infimum is over all randomized algorithms with non-adaptive greedy error threshold selection, and the supremum is over the functions in $\\mathcal{F}$ , and $i^{*}\\,\\in\\,[k]$ is the largest value satisfying $\\begin{array}{r}{\\frac{16}{n^{2}k^{6}}\\left({n-k}\\right)^{3}\\leq T.}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "3 UCB UPPER BOUND ", "text_level": 1, "page_idx": 6}, {"type": "table", "img_path": "Vn0FWRImra/tmp/c7b537850ffbef0290513b18a9c327fdc1995b5a5e4c2c4988af11c10d9c39c3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "A natural approach to minimizing regret is to take an Explore-Then-Commit strategy motivated by the greedy algorithm. Such an algorithm would be the following - proceed in $k$ rounds. Set $S^{0}={\\bar{\\varnothing}}$ . ", "page_idx": 6}, {"type": "text", "text": "In round $i$ pull each set in the collection $\\{S^{i-1}\\cup\\{a\\}:a\\in[n]\\backslash S^{i-1}\\}$ , $m$ times. Use these samples to update our estimate $\\hat{f}$ of $f$ on these sets, and set $S^{(i)}\\gets\\arg\\operatorname*{max}_{a\\in[n]\\backslash S^{i-1}}\\hat{f}(S^{i-1}\\cup\\{a\\})$ . This approach has been pursued by existing works [28], and with an appropriate choice of $m$ results in $\\bar{O}(k n^{1/3}T^{2/3})$ regret. ", "page_idx": 7}, {"type": "text", "text": "The disadvantage of this approach is that it can not achieve the correct trade-off between $\\sqrt{n^{k}T}$ and $k n^{1/3}T^{2/3}$ exhibited by the lower bound. Motivated by the statement of the lower bound, our algorithm SUB-UCB attempts to interpolate between these different regret regimes. The critical quantity is $i^{*}$ . For the first $k-i^{*}$ cardinalities, our algorithm plays a UCB style strategy which more or less follows the ETC strategy described in the previous paragraph. After that, it defaults to a UCB algorithm on all subsets containing $S^{k-i^{*}}$ , a total of $\\bar{(}^{n-k+i^{*}})$ possible arms. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3.1. For any $l\\leq k$ , SUB-UCB guarantees ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{g r}]\\leq(1+4\\sqrt{2})l T^{2/3}n^{1/3}(\\log T)^{1/3}+65\\sqrt{T\\binom{n-k}{k-l}\\log T}+\\frac{32}{15}\\binom{n-k}{k-l}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "when $m=T^{2/3}n^{-2/3}\\log T^{1/3}$ . ", "page_idx": 7}, {"type": "text", "text": "Proof Sketch We show that for $\\epsilon\\,:=\\,2\\sqrt{2\\log(2k n T^{2})/m}$ , the greedy part of SUB-UCB with high probability adds an $\\epsilon$ -optimal arm in each step. Defining event $G$ to be $|\\hat{\\mu}_{S}\\,-\\,f(S)|\\;\\leq$ $\\sqrt{2T_{S}\\log(2k n T^{2})}$ for all iterations, we prove that this event is true with a probability of at least 1 \u2212T1 . ", "page_idx": 7}, {"type": "text", "text": "On Event $G$ , We show that an $\\epsilon$ -good arm is selected at each step of the greedy algorithm for $\\epsilon=$ 2 log(2mknT 2). Let a be a sub-optimal arm with expected reward value more than 2 $2\\sqrt{\\frac{2\\log(2k n T^{2})}{m}}$ from the best arm in the $i$ -th step i.e. $\\Delta_{S^{(i)},a}\\;:=\\;\\mathrm{max}_{a^{\\prime}}\\,f(S^{(i)}\\cup\\{a^{\\prime}\\})\\;-\\;f(S^{(i)}\\cup\\{a\\})\\;\\geq$ $2\\sqrt{\\frac{2\\log(2k n T^{2})}{m}}$ . Then if arm $a$ is added in $i$ -th step, we have $U_{a}(t)\\,\\geq\\,U_{a^{*}}(t)\\,\\geq\\,f(S^{(i))\\cup\\{a^{*}\\}}$ , and therefore, ", "page_idx": 7}, {"type": "equation", "text": "$$\nU_{a}(t)-f(S^{(i)}\\cup\\{a\\})\\geq\\Delta_{S^{(i)},a}>2\\sqrt{\\frac{2\\log(2k n T^{2})}{m}},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "so $\\begin{array}{r}{\\hat{\\mu}_{S^{(i)}\\cup\\{a\\}}-f(S^{(i)}\\cup\\{a\\})>\\sqrt{\\frac{2\\log(2k n T^{2})}{m}}}\\end{array}$ 2 log(2mknT 2). This is a contradiction with event G, so on event G such an arm cannot be selected. Lastly, we expand the regret of two stages. As UCB in the second part of the algorithm has the regret of $\\begin{array}{r}{65\\sqrt{T\\binom{n-k}{k-l}\\log T}+\\frac{32}{15}\\binom{n-k}{k-l}}\\end{array}$ against the best arm containing $S^{(l)}(\\sec{[24]})$ , it is an upper bound for the regret against the greedy solution were the first $l$ steps select an $\\epsilon$ -good arm, and the last $k-l$ steps select the best arm, so on event $G$ the regret can be written against a set in $S^{k,\\epsilon}$ where ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{1}^{T}\\epsilon=l\\epsilon+(k-l)0=2l\\sqrt{\\frac{2\\log(2k n T^{2})}{m}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Therefore, the expected regret $\\mathbb{E}[R_{\\mathbf{gr}}]$ on event $G$ can be written as ", "page_idx": 7}, {"type": "equation", "text": "$$\n2T l\\sqrt{\\frac{2\\log(2k n T^{2})}{m}}+m n(k-i^{*})+65\\sqrt{T\\binom{n-k}{k-l}\\log T}+\\frac{32}{15}\\binom{n-k}{k-l},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for any choice of $m$ and $l$ . So for $m~=~T^{2/3}n^{-2/3}\\log^{1/3}(2k n T^{2})$ the above term becomes $\\tilde{\\mathcal{O}}(l T^{2/3}n^{1/3}+\\sqrt{T\\binom{n}{k-l}})$ . The detailed proof is in Appendix B \u25a1 ", "page_idx": 7}, {"type": "text", "text": "4 EXPERIMENTS ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "For the experiments we compare SUB-UCB $(l)$ for different greedy stop levels $l$ , SUB-UCB $\\left(k-\\right.$ $i^{*}$ ) which selects the best stop level based on the regret analysis, the ETCG (explore-then-commit greedy) algorithm from [28], and UCB on all size $k$ arms. Each arm pull has a 1-Gaussian noise, with 50 trials for each setting. The expected reward functions are the following. ", "page_idx": 7}, {"type": "image", "img_path": "Vn0FWRImra/tmp/bb053598eaccd0095be82989bd430b5cc30e576028c56fa3ff5703e903654bf9.jpg", "img_caption": ["Figure 1: Regret comparison for weighted set cover with $n=15$ and $k=4$ "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Functions: ", "page_idx": 8}, {"type": "text", "text": "\u2022 The Unique greedy path hard instance i.e. ", "page_idx": 8}, {"type": "equation", "text": "$$\nf(S)=\\left\\{\\sum_{i=1}^{|S|}\\frac{1}{k+i}\\quad S=\\{1,\\dots,|S|\\}\\right.\\qquad}\\\\ {\\left.\\sum_{i=1}^{|S|}\\frac{1}{k+i}+\\frac{1}{100}\\quad S=\\{1,\\dots,|S|\\}.\\right.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "This function is inspired by the hard instance in the proof of our lower-bound. Note that this particular parameterization is submodular when $k\\leq7$ , not for general $k$ . \u2022 Weighted set cover function i.e. $\\begin{array}{r}{f_{\\mathcal{C}}(S)=\\sum_{C\\in\\mathcal{C}}w(C)\\mathbf{1}\\{S\\cap C\\neq\\emptyset\\}}\\end{array}$ for a partition $\\mathcal{C}$ of $[n]$ and weight function $w$ on the partition . For $n=15$ and $k=4$ , we use the partitions of size $5,5,4,1$ with weights of $1/1\\bar{0},1/10,2/10,6/10$ respectively. ", "page_idx": 8}, {"type": "text", "text": "Results: As illustrated in figure 1, we observe that our algorithm with the level selection of $k-i^{*}$ outperforms both ETCG and naive UCB on all size $k$ arms, as it combines the advantages of greedy approach for small $T\\mathrm{s}$ and UCB on many super arms for large $T$ . For smaller $T\\mathrm{s}$ compared to $\\binom{\\bar{n}}{k}$ , both SUB-UCB and ETCG outperform normal UCB as it doesn\u2019t have enough budget to find optimal sets of size $k$ , so it gets linear regret(as the other two get $O(T^{2/3}))$ . However, as $T$ becomes larger the reverse happens as $\\binom{n}{k}T^{1/2}$ becomes smaller than $T^{2/3}$ , but SUB-UCB adopts to $T$ and continues to outperform the two until it converges with naive UCB for very large $T$ . ", "page_idx": 8}, {"type": "image", "img_path": "Vn0FWRImra/tmp/8aab970aabb670fe01217e18a87af80a1041f2416756b7c7b403cb2470de0223.jpg", "img_caption": ["Figure 2: Comparison between all SUB-UCB greedy stop cardinality choices for the unique greedy path function with $n\\:=\\:20\\$ and $k\\,=\\,5$ . The worst-case optimal stop cardinality $l\\;=\\;k\\,-\\,i^{*}$ is highlighted "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "In figure 2, we compare the performance of SUB-UCB for different choices of greedy stop cardinality, and observe that the best choice gradually decreases from $k$ to 0 as $T$ gets larger, and $k-i^{*}$ is a practical selection of the best stop cardinality before running the algorithm. Note that the defined stop level was chosen to minimize the worst-case bound on the regret, and if the gaps between arms on a particular instance are larger than the worst case, this stop level could be conservative. So $k\\!-\\!i^{*}$ is near the optimal stop level, and not the exact one as seen in these figures. Also, the empirical standard derivation is much smaller than ${\\mathcal{O}}(T^{1/2})$ due to the regret symmetry of non-optimal sets at each cardinality, and it\u2019s not visible in the plots. ", "page_idx": 9}, {"type": "text", "text": "5 CONCLUSION ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper we showed that $\\mathrm{min}_{L}(L^{1/3}T^{2/3}n^{1/3}+\\sqrt{\\binom{n}{k-L}T})$ , ignoring logarithmic factors, is a lower bound on the regret against robust greedy solutions of stochastic submodular functions, and a stronger lower bound if the algorithm class is slightly restricted. We also matched this bound with an algorithm. This work is the first minimax lower bound for submodular bandits, and beyond closing the $k^{2/3}$ gap between the general lowerbound and upperbound, it remains open to prove similar minimax optimal bounds in settings with different types of constraint such as matroid, or in general, any offline-to-online greedy procedure that is robust to local noise (e.g. Non-monotonic submodular maximization where the greedy approach gets a $1/2$ -approximation of the function, or DR-submodular optimization for the continuous setting which also has a $(1-e^{-1})$ -approximation). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper is based on work supported by Microsoft Grant for Customer Experience Innovation. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Mridul Agarwal, Vaneet Aggarwal, Christopher J. Quinn, and Abhishek Umrawal. DART: aDaptive Accept RejecT for non-linear top-K subset identification, November 2020. arXiv:2011.07687 [cs, stat].   \n[2] Mridul Agarwal, Vaneet Aggarwal, Christopher J. Quinn, and Abhishek K. Umrawal. Stochastic Top- $\\mathbb{S}\\mathbb{K}\\mathbb{S}$ Subset Bandits with Linear Space and Non-Linear Feedback, October 2021. arXiv:1811.11925 [cs, stat].   \n[3] Francis Bach. Submodular functions: from discrete to continuous domains. Math. Program., 175(12):419459, May 2019.   \n[4] Maria-Florina Balcan and Nicholas J A Harvey. Learning Submodular Functions.   \n[5] Eric Balkanski and Yaron Singer. The adaptive complexity of maximizing a submodular function. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1138\u20131151, Los Angeles CA USA, June 2018. ACM.   \n[6] Andrew An Bian, Joachim M. Buhmann, Andreas Krause, and Sebastian Tschiatschek. Guarantees for Greedy Maximization of Non-submodular Functions with Applications, May 2019. arXiv:1703.02100 [cs, math].   \n[7] Andrew An Bian, Kfir Y. Levy, Andreas Krause, and Joachim M. Buhmann. Continuous drsubmodular maximization: structure and algorithms. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, page 486496, Red Hook, NY, USA, 2017. Curran Associates Inc.   \n[8] Andrew An Bian, Baharan Mirzasoleiman, Joachim Buhmann, and Andreas Krause. Guaranteed Non-convex Optimization: Submodular Maximization over Continuous Domains. In Aarti Singh and Jerry Zhu, editors, Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages 111\u2013120. PMLR, 20\u201322 Apr 2017. [9] Lin Chen, Hamed Hassani, and Amin Karbasi. Online continuous submodular maximization. In Amos Storkey and Fernando Perez-Cruz, editors, Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning Research, pages 1896\u20131905. PMLR, 09\u201311 Apr 2018.   \n[10] Wei Chen, Wei Hu, Fu Li, Jian Li, Yu Liu, and Pinyan Lu. Combinatorial Multi-Armed Bandit with General Reward Functions. In Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016.   \n[11] Wei Chen, Yajun Wang, Yang Yuan, and Qinshi Wang. Combinatorial Multi-Armed Bandit and Its Extension to Probabilistically Triggered Arms, March 2016. arXiv:1407.8339 [cs].   \n[12] Xi Chen, Yanjun Han, and Yining Wang. Adversarial Combinatorial Bandits with General Non-linear Reward Functions, January 2021. arXiv:2101.01301 [cs, stat].   \n[13] Uriel Feige. A threshold of ln n for approximating set cover. J. ACM, 45(4):634652, jul 1998.   \n[14] Moran Feldman and Amin Karbasi. Continuous submodular maximization: Beyond drsubmodularity. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1404\u20131416. Curran Associates, Inc., 2020.   \n[15] Fares Fourati, Vaneet Aggarwal, Christopher Quinn, and Mohamed-Slim Alouini. Randomized greedy learning for non-monotone stochastic submodular maximization under full-bandit feedback. In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent, editors, Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, volume 206 of Proceedings of Machine Learning Research, pages 7455\u20137471. PMLR, 25\u201327 Apr 2023.   \n[16] Michel X. Goemans, Nicholas J. A. Harvey, Satoru Iwata, and Vahab Mirrokni. Approximating Submodular Functions Everywhere. In Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 535\u2013544. Society for Industrial and Applied Mathematics, January 2009.   \n[17] Daniel Golovin, Andreas Krause, and Matthew Streeter. Online submodular maximization under a matroid constraint with application to learning assignments, 2014.   \n[18] Botao Hao, Tor Lattimore, and Mengdi Wang. High-Dimensional Sparse Linear Bandits, September 2021. arXiv:2011.04020 [cs, math, stat].   \n[19] Nicholas Harvey, Christopher Liaw, and Tasuku Soma. Improved Algorithms for Online Submodular Maximization via First-order Regret Bounds. In Advances in Neural Information Processing Systems, volume 33, pages 123\u2013133. Curran Associates, Inc., 2020.   \n[20] Daniel N Hill, Houssam Nassif, Yi Liu, Anand Iyer, and SVN Vishwanathan. An efficient bandit algorithm for realtime multivariate optimization. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1813\u20131821, 2017.   \n[21] Emilie Kaufmann, Olivier Capp\u00e9, and Aur\u00e9lien Garivier. On the Complexity of Best Arm Identification in Multi-Armed Bandit Models, November 2016. arXiv:1407.4443 [cs, stat].   \n[22] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Machine learning, 49(2):209\u2013232, 2002.   \n[23] Andreas Krause and Daniel Golovin. Submodular Function Maximization. In Lucas Bordeaux, Youssef Hamadi, and Pushmeet Kohli, editors, Tractability, pages 71\u2013104. Cambridge University Press, 1 edition, February 2014.   \n[24] Tor Lattimore and Csaba Szepesvari. Bandit algorithms. 2017.   \n[25] Tatsuya Matsuoka, Shinji Ito, and Naoto Ohsaka. Tracking Regret Bounds for Online Submodular Optimization. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, pages 3421\u20133429. PMLR, March 2021. ISSN: 2640-3498.   \n[26] George L Nemhauser and Laurence A Wolsey. Submodular set functions, matroids and the greedy algorithm: Tight worst-case bounds and some generalizations of the rado-edmonds theorem. Combinatorica, 3(3-4):257\u2013268, 1978.   \n[27] Rad Niazadeh, Negin Golrezaei, Joshua Wang, Fransisca Susan, and Ashwinkumar Badanidiyuru. Online Learning via Offline Greedy Algorithms: Applications in Market Design and Optimization, February 2023. arXiv:2102.11050 [cs, math, stat].   \n[28] Guanyu Nie, Mridul Agarwal, Abhishek Kumar Umrawal, Vaneet Aggarwal, and Christopher John Quinn. An Explore-then-Commit Algorithm for Submodular Maximization Under Full-bandit Feedback. 2022.   \n[29] Guanyu Nie, Yididiya Y Nadew, Yanhui Zhu, Vaneet Aggarwal, and Christopher John Quinn. A framework for adapting offline algorithms to solve combinatorial multi-armed bandit problems with bandit feedback. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR.org, 2023.   \n[30] Stephen U. Pasteris, Alberto Rumi, Fabio Vitale, and Nicol\u00f2 Cesa-Bianchi. Sum-max submodular bandits. In Sanjoy Dasgupta, Stephan Mandt, and Yingzhen Li, editors, Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238 of Proceedings of Machine Learning Research, pages 2323\u20132331. PMLR, 02\u201304 May 2024.   \n[31] Mohammad Pedramfar and Vaneet Aggarwal. Stochastic Submodular Bandits with Delayed Composite Anonymous Bandit Feedback, March 2023. arXiv:2303.13604 [cs].   \n[32] Tim Roughgarden and Joshua R. Wang. An optimal learning algorithm for online unconstrained submodular maximization. In S\u00e9bastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 1307\u20131325. PMLR, 06\u201309 Jul 2018.   \n[33] Omid Sadeghi, Prasanna Raut, and Maryam Fazel. Improved Regret Bounds for Online Submodular Maximization, June 2021. arXiv:2106.07836 [cs, math, stat].   \n[34] Max Simchowitz, Kevin Jamieson, and Benjamin Recht. Best-of-K Bandits, March 2016. arXiv:1603.02752 [cs, stat].   \n[35] Adish Singla, Sebastian Tschiatschek, and Andreas Krause. Noisy submodular maximization via adaptive sampling with applications to crowdsourced image collection summarization. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI\u201916, page 20372043. AAAI Press, 2016.   \n[36] Matthew Streeter and Daniel Golovin. An Online Algorithm for Maximizing Submodular Functions:. Technical report, Defense Technical Information Center, Fort Belvoir, VA, December 2007.   \n[37] Matthew Streeter and Daniel Golovin. An Online Algorithm for Maximizing Submodular Functions. In Advances in Neural Information Processing Systems, volume 21. Curran Associates, Inc., 2008.   \n[38] Matthew Streeter, Daniel Golovin, and Andreas Krause. Online learning of assignments. In Y. Bengio, D. Schuurmans, J. Lafferty, C. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems, volume 22. Curran Associates, Inc., 2009.   \n[39] Maxim Sviridenko, Jan Vondr\u00e1k, and Justin Ward. Optimal approximation for submodular and supermodular optimization with bounded curvature, December 2014. arXiv:1311.4728 [cs].   \n[40] Zoya Svitkina and Lisa Fleischer. Submodular approximation: sampling-based algorithms and lower bounds, May 2010. arXiv:0805.1071 [cs].   \n[41] Zongqi Wan, Jialin Zhang, Wei Chen, Xiaoming Sun, and Zhijie Zhang. Bandit multi-linear DR-submodular maximization and its applications on adversarial submodular bandits. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 35491\u201335524. PMLR, 23\u201329 Jul 2023.   \n[42] Zheng Wen, Branislav Kveton, Michal Valko, and Sharan Vaswani. Online influence maximization under independent cascade model with semi-bandit feedback. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, page 30263036, Red Hook, NY, USA, 2017. Curran Associates Inc.   \n[43] Mingrui Zhang, Lin Chen, Hamed Hassani, and Amin Karbasi. Online continuous submodular maximization: From full-information to bandit feedback. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[44] Qixin Zhang, Zengde Deng, Zaiyi Chen, Haoyuan Hu, and Yu Yang. Stochastic continuous submodular maximization: Boosting via non-oblivious function. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 26116\u201326134. PMLR, 17\u201323 Jul 2022.   \n[45] Junlong Zhu, Qingtao Wu, Mingchuan Zhang, Ruijuan Zheng, and Keqin Li. Projection-free decentralized online learning for submodular maximization over time-varying networks. Journal of Machine Learning Research, 22(51):1\u201342, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Lowerbound proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Proof of Theorem 2.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For any $\\{x_{1},x_{2},\\ldots,x_{k}\\}\\in\\left({\\stackrel{[n]\\setminus\\{1,\\ldots,k\\}}{k}}\\right)$ , define instance $\\mathcal{H}_{0},\\mathcal{H}_{(x_{1},\\dots,x_{k})},\\mathcal{H}_{(x_{i+1},\\dots,x_{k})}$ with reward functions as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{c}{f_{\\mathcal{H}_{0}}(S):=\\left\\{\\!H_{|S|+k}-H_{k}=\\sum_{i=1}^{|S|}\\frac{1}{k+i}\\quad S=\\{1,2,\\ldots,|S|\\}\\!\\!\\right\\}}\\\\ {\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\textstyle H_{n}=\\sum_{k=1}^{n}{\\frac{1}{k}}$ is the $n$ -th harmonic number. ", "page_idx": 13}, {"type": "text", "text": "Lemma A.1. If $\\Delta\\le(1/8k^{2})$ then $\\mathcal{H}_{0}$ and $\\mathcal{H}_{(x_{1},\\dots,x_{k})}$ are submodular. ", "page_idx": 13}, {"type": "text", "text": "Proof. for any $S\\subsetneq T\\subset[n]$ where $|T|<k$ (the function is only defined on sets of cardinality at most $k$ ) and $x\\not\\in T$ we have to show $\\begin{array}{r}{\\dot{f}(\\dot{S}+x)-f(S)\\geq f(T+\\dot{x})-f(T)}\\end{array}$ . ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{^{\\mathrm{r}}(T+x)-f(T)\\le\\displaystyle\\frac{1}{|T|+k+1}+2\\Delta\\le\\displaystyle\\frac{1}{|T|+k+1}+\\displaystyle\\frac{1}{4k^{2}}\\le\\displaystyle\\frac{1}{|T|+k}-\\displaystyle\\frac{1}{4k^{2}}\\le\\displaystyle\\frac{1}{|T|+k}-2\\Delta}}\\\\ {{\\le\\displaystyle\\frac{1}{|S|+1+k}-2\\Delta\\le f(S+x)-f(S)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For $\\mathcal{H}_{0}$ if $\\epsilon_{i}\\,<\\,\\Delta$ at each step $i$ of the greedy arm selection, then $S_{\\mathbf{gr}}^{k,\\epsilon}\\,=\\,\\{1,\\ldots,k\\}$ , otherwise $f_{\\mathcal{H}_{0}}(S_{\\mathbf{gr}}^{k,\\epsilon})+\\mathbf{1}^{T}\\epsilon\\geq H_{2k}-H_{k}+\\Delta-\\Delta=H_{2k}-H_{k}=f_{\\mathcal{H}_{0}}(\\{1,\\dots,k\\})$ . $\\mathrm{So}\\;\\mathrm{min}_{\\epsilon}\\;f_{\\mathcal{H}_{0}}(S_{\\mathbf{gr}}^{k,\\epsilon})\\;+$ $\\mathbf{1}^{T}\\epsilon\\;=\\;f_{\\mathcal{H}_{0}}(\\{1,\\dots,k\\})$ . This means that we can compute our regret against $f_{\\mathcal{H}_{0}}(\\{1,\\ldots,k\\})$ . Similarly, $\\begin{array}{r}{\\operatorname*{min}_{\\epsilon}f_{\\mathcal{H}_{(x_{1},\\ldots,x_{k})}}(S_{\\mathbf{gr}}^{k,\\epsilon})+\\mathbf{1}^{T}\\epsilon=H_{2k}-H_{k}+\\Delta=f_{\\mathcal{H}_{(x_{1},\\ldots,x_{k})}}(\\{x_{1},\\ldots,x_{k}\\})}\\end{array}$ showing that we can compute our regret against $\\{x_{1},\\cdot\\cdot\\cdot,x_{k}\\}$ . ", "page_idx": 13}, {"type": "text", "text": "Let $\\mathbb{E}_{0}$ and $\\mathbb{E}_{(x_{1},\\dots,x_{k})}$ denote the probability law under $\\mathcal{H}_{0}$ and $\\mathcal{H}_{(x_{1},\\dots,x_{k})}$ , respectively. For any $S\\subset[n]$ let $T_{S}$ denote the random variable describing the number of time the set $S$ is played by a policy $\\pi$ . Define $\\begin{array}{r}{T_{i}:=\\sum_{\\cal S\\subset[n]:|\\cal S|=i}T_{\\cal S}}\\end{array}$ . ", "page_idx": 13}, {"type": "text", "text": "Then by the definition of $\\mathcal{H}_{0}$ we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{0}[R_{\\mathbf{gr}}]\\geq\\displaystyle\\sum_{i=1}^{k-1}(f_{\\mathcal{H}_{0}}(1,\\ldots,k)-\\operatorname*{max}_{S:|S|=i}f_{\\mathcal{H}_{0}}(S))\\mathbb{E}_{0}[T_{i}]+\\displaystyle\\sum_{S:|S|=k}(f_{\\mathcal{H}_{0}}(\\{1,\\ldots,k\\})-f_{\\mathcal{H}_{0}}(S))\\mathbb{E}_{0}[T_{i}]}\\\\ &{\\qquad\\geq\\displaystyle\\sum_{i=1}^{k-1}\\Big(\\sum_{j=i+1}^{k}1/(k+j)\\Big)\\mathbb{E}_{0}[T_{i}]+\\Delta\\sum_{\\{y_{1},\\ldots,y_{k}\\}\\neq\\{1,\\ldots,k\\}}\\mathbb{E}_{0}[T_{\\{y_{1},\\ldots,y_{k}\\}}]}\\\\ &{\\qquad\\geq\\displaystyle\\sum_{i=1}^{k-1}\\frac{k-i}{2k}\\mathbb{E}_{0}[T_{i}]+\\frac{\\Delta T}{2}\\mathbb{P}_{0}(T_{\\{1,\\ldots,k\\}}\\leq T/2)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Similarly for $\\mathcal{H}_{(x_{1},\\dots,x_{k})}$ we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\{x_{1},\\ldots,x_{k}\\}}[R_{\\mathbf{gr}}]}\\\\ &{\\;\\;\\geq\\displaystyle\\sum_{i=1}^{k-1}(f_{\\mathcal{H}(x_{1},\\ldots,x_{k})}(\\{x_{1},\\ldots,x_{k}\\})-\\operatorname*{max}_{|S|=i}f_{\\mathcal{H}(x_{1},\\ldots,x_{k})}(S))\\mathbb{E}_{(x_{1},\\ldots,x_{k})}[T_{i}]}\\\\ &{\\;\\;+\\displaystyle\\sum_{S:|S|=k}(f_{\\mathcal{H}(x_{1},\\ldots,x_{k})}(\\{x_{1},\\ldots,x_{k}\\})-f_{\\mathcal{H}(x_{1},\\ldots,x_{k})}(S))\\mathbb{E}_{0}[T_{S}]}\\\\ &{\\;\\;\\geq\\displaystyle\\sum_{i}^{k-1}(\\sum_{j=i+1}^{k}1/(k+j))\\mathbb{E}_{\\{x_{1},\\ldots,x_{k}\\}}[T_{i}]+\\Delta\\sum_{\\{y_{1},\\ldots,y_{k}\\}\\neq\\{x_{1},\\ldots,x_{k}\\}}\\mathbb{E}_{\\{x_{1},\\ldots,x_{k}\\}}[T_{\\{y_{1},\\ldots,y_{k}\\}}]}\\\\ &{\\;\\;\\geq\\displaystyle\\frac{\\Delta T}{2}\\mathbb{P}_{\\{x_{1},\\ldots,x_{k}\\}}(T_{\\{x_{1},\\ldots,k\\}}>T/2).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma A.2. For any $i\\leq k$ here exist a sequence $(x_{i},\\ldots,x_{k})$ , where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j=i}^{k}\\mathbb{E}_{0}[T_{\\{1,\\dots,i-1,x_{i},\\dots,x_{j}\\}}]}\\\\ &{\\displaystyle\\leq\\frac{1}{n-k}\\mathbb{E}_{0}[T_{i}]+\\frac{2}{(n-k)(n-k-1)}\\mathbb{E}_{0}[T_{i+1}]+\\frac{4}{(n-k)(n-k-1)}\\sum_{j=i+2}^{k-1}\\frac{k-j}{2k}\\mathbb{E}_{0}[T_{j}]+\\frac{T}{\\binom{n-k-1}{k-i+1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. For $i\\,\\leq\\,k$ and a sequence $(x_{i},...,x_{k})$ , define $\\begin{array}{r}{Q_{(x_{i},\\dotsc,x_{k})}\\,:=\\,\\sum_{j=i}^{k}\\mathbb{E}_{0}[T_{\\{1,\\dotsc,i-1,x_{i},\\dotsc,x_{j}\\}}].}\\end{array}$ Then we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{Q}:=\\sum_{(x_{i},\\ldots,x_{k})\\neq(i,\\ldots,k)}Q_{(x_{i},\\ldots,x_{k})}\\leq\\sum_{j=i}^{k-1}\\frac{(n-k-j+i-1)!(j-i+1)!}{(n-2k+i-1)!}\\mathbb{E}_{0}[T_{j}]+((k-i+1)!)\\mathbb{E}_{0}[T_{k}].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then by Pigeonhole principle, the exists a sequence $(x_{i},\\ldots,x_{k})$ such that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{(x_{i},\\ldots,x_{k})\\leq\\atop(n,\\ldots,x)\\leq n}\\frac{Q}{(n-(n-k)!)}}}\\\\ &{\\leq\\sum_{j=i}^{k-1}\\frac{(n-k-j+i-1)!(j-i+1)!}{(n-k)!}\\mathbb{g}_{0}[T_{j}]+\\frac{(n-2k+i-1)!(k-i+1)!}{(n-k)!}\\mathbb{E}[T_{k}]}\\\\ &{\\leq\\frac{1}{n-k}\\mathbb{E}_{0}[T_{i}]+\\frac{1}{(n-k)(n-k-1)}\\sum_{j=i+1}^{k-1}\\frac{(j-i)(j-i-1)}{(n-k-2)}\\mathbb{E}[T_{j}]+\\frac{1}{{\\binom{n-k}{n-i+1}}}\\mathbb{E}[T_{k}]}\\\\ &{\\leq\\frac{1}{n-k}\\mathbb{E}_{0}[T_{i}]+\\frac{2}{(n-k)(n-k-1)}\\mathbb{E}_{0}[T_{i+1}]}\\\\ &{\\quad+\\frac{4}{(n-k)(n-k-1)}\\sum_{j=i+2}^{k-1}\\frac{k-j}{2k}\\mathbb{E}_{0}[T_{j}]+\\frac{T}{{\\binom{n-k}{n-i+1}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma A.3. For $\\mathcal{H}_{0}$ and $\\mathcal{H}_{(x_{1},\\dots,x_{k})}$ defined above, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nK L(\\mathbb{P}_{0}|\\mathbb{P}_{\\{x_{i},\\ldots,x_{k}\\}})=2\\Delta^{2}\\sum_{j=i}^{k-1}\\mathbb{E}_{0}[T_{1,\\ldots,i-1,x_{i},\\ldots,x_{j}}]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K L(\\mathbb{P}_{0}|\\mathbb{P}_{\\{x_{i},\\ldots,x_{k}\\}})=\\displaystyle\\sum_{S:|S|\\leq k}\\mathbb{E}_{0}[T_{S}]K L(P_{0}(S)|P_{\\{x_{i},\\ldots,x_{k}\\}}(S))}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\sum_{j=i}^{k}2\\Delta^{2}\\mathbb{E}_{0}[T_{1,\\ldots,i-1,x_{i},\\ldots,x_{j}}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "(lemma 15.1 in [24]) ", "page_idx": 14}, {"type": "text", "text": "where $P_{0}(S)=\\mathcal{N}(f_{\\mathcal{H}_{0}}(S),1)$ and $P_{\\{x_{i},...,x_{k}\\}}(S)=\\mathcal N(f_{\\mathcal{H}_{(x_{i},...,x_{k})}}(S),1)$ are the reward distributions of arm $S$ in $\\mathcal{H}_{0}$ and $\\mathcal{H}_{(x_{i},\\dots,X_{k})}$ respectively. ", "page_idx": 15}, {"type": "text", "text": "Using two above lemmas, we have, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\operatorname*{max}\\left(\\mathbb{E}_{\\lfloor\\alpha\\rfloor+\\frac{1}{2}\\rfloor+\\frac{3}{2}\\sqrt{\\alpha}}\\sum_{i,j\\in\\{i,j\\}}\\sum_{\\ell=1}^{\\infty}\\mathbb{E}_{\\lfloor\\alpha\\rfloor+\\frac{3}{2}\\sqrt{\\alpha}}\\prod_{\\ell=i+1}^{\\mathcal{I}}\\left\\{P_{i j}\\right\\}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }}\\\\ &{{2\\sum_{1\\le\\le\\frac{\\alpha+1+i+1}{\\alpha},\\frac{\\alpha+2+3}{}\\alpha,}\\sum_{i,j\\in\\{i,j^{\\prime}\\}}\\prod_{\\ell=i}^{i}\\mathbb{E}_{\\lfloor\\alpha\\rfloor+1}+\\mathbb{E}_{\\lfloor\\alpha\\rfloor+\\frac{3}{2}\\sqrt{\\alpha}}\\prod_{\\ell=i+1}^{\\mathcal{I}}\\left\\{P_{i j}\\right\\}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ 2\\operatorname*{max}\\left\\{P_{i j}\\le\\frac{\\alpha+1}{2}\\left\\{P_{i j}-1\\right\\},\\frac{\\beta}{2}\\sum_{l=1}^{\\infty}\\mathbb{E}_{\\lfloor\\alpha\\rfloor+1}^{\\top}\\right\\})\\ \\ \\ \\ \\ \\ \\mathrm{(Eq~ap~of~\\ensuremath{prad~\\ensuremath{prad~\\ensuremath{prad~\\ensuremath{q}}}}~T2)}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\frac{2}{3}\\sum_{i\\le \n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For $\\mathrm{~1~\\leq~\\it~i~\\leq~k~-~i^{*}~+~1~}$ , ${\\frac{n^{2/3}T^{1/3}}{\\binom{n-k}{k-i+1}}}\\ \\ \\leq\\ \\ 1$ by definition of $i^{*}$ ; so either the maximum regret is larger than $\\textstyle{\\frac{1}{4}}T^{2/3}(k\\,-\\,i^{*})^{1/3}n^{1/3}\\exp(-8)$ , which proves the theorem, or $\\scriptstyle{\\frac{1}{n-k}}\\mathbb{E}_{0}[T_{i}]\\ +$ $\\begin{array}{r}{\\frac{2}{(n-k)(n-k-1)}\\mathbb{E}_{0}[T_{i+1}]+\\frac{4}{(n-k)(n-k-1)}\\sum_{j=i+2}^{k-1}\\frac{k-j}{2k}\\mathbb{E}_{0}[T_{j}]\\geq3(1/\\Delta^{2})}\\end{array}$ jk=\u2212i1+2k2\u2212kj E0[Tj] \u22653(1/\u22062). If the third term is larger tha\u2212n $1/\\Delta^{2}$ \u2212, then $\\begin{array}{r l r}{\\sum_{j=i+2}^{k-1}\\frac{k-j}{2k}\\mathbb{E}_{0}[T_{j}]}&{{}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\sum_{j=i+2^{\\ast}}^{}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!$ which proves the lowerbound as (k in\u2217)2/3 \u2265(k \u2212i\u2217)1/3. Therefore, the only remaining case is that either the first or second term is $\\geq1/\\Delta^{2}$ . This means that for $1\\leq i\\leq k-i^{*}+1$ , either $\\begin{array}{r}{\\mathbb{E}_{0}[T_{i}]\\,\\ge\\,\\frac{1}{4}(k-i^{*})^{-2/3}T^{2/3}n^{1/3}}\\end{array}$ or $\\begin{array}{r}{\\mathbb{E}_{0}[T_{i+1}]\\geq\\frac{1}{8}(n-k-1)(k-i^{*})^{-2/3}T^{2/3}n^{1/3}\\geq\\frac{1}{4}(k-i^{*})^{-2/3}T^{2/3}n^{1/3}}\\end{array}$ . Therefore, for at least ha $\\begin{array}{r l}&{\\mathrm{lf~of~the~}1\\stackrel{\\circ}{\\leq}i\\leq k-i^{*}+1,\\mathbb{E}_{0}^{\\prime}[T_{i}]\\geq\\frac{1}{4}(k-i^{*})^{-2/3}T^{2/3}n^{1/3},\\mathrm{and}}\\end{array}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{0}[R_{\\mathbf{gr}}]\\geq\\sum_{j=1}^{k-i^{*}+1}\\frac{k-j}{2k}\\mathbb{E}_{0}[T_{j}]\\geq\\frac{1}{8}(k-i^{*})^{1/3}T^{2/3}n^{1/3}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that since $T\\geq512k^{7}n$ , we have $\\begin{array}{r}{\\Delta\\le(k n/T)^{1/3}\\le\\frac{1}{8k^{2}}}\\end{array}$ , so the functions with this selection of $\\Delta$ are submodular. ", "page_idx": 15}, {"type": "text", "text": "We now lower bound the regret in a different way. Let $\\begin{array}{r}{\\lambda:=\\frac{\\sum_{j=k-i^{*}+1}^{k-1}\\frac{k-i}{2k}\\mathbb{E}_{0}[T_{i}]}{T}}\\end{array}$ , then $\\lambda\\leq1$ , and using lemma A.2 we have that there exists a selection of $(x_{i},\\ldots,x_{k})$ such that, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{j=i}^{k}\\mathbb{E}_{0}[T_{\\{1,\\dots,i-1,x_{i},\\dots,x_{j}\\}}]}\\qquad}&{}\\\\ &{\\leq\\frac{1}{n-k}\\mathbb{E}_{0}[T_{i}]+\\frac{2}{(n-k)(n-k-1)}\\mathbb{E}_{0}[T_{i+1}]+\\frac{4}{(n-k)(n-k-1)}\\sum_{j=i+2}^{k-1}\\frac{k-j}{2k}\\mathbb{E}_{0}[T_{j}]+\\frac{4}{\\binom{n-1}{k-1}}}\\\\ &{\\leq\\frac{4}{n-k}\\sum_{j=i}^{k-1}\\frac{k-j}{2k}\\mathbb{E}_{0}[T_{j}]+\\frac{T}{\\binom{n-k}{k-i+1}}=\\frac{4}{\\binom{n-k}{n-k}}\\lambda T+\\frac{T}{\\binom{n-k}{k-i+1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "So ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\operatorname*{max}\\left(\\mathbb{E}_{0}|{R_{\\mathfrak{p}}}|_{1\\leq i\\leq k,(\\alpha_{i}-\\alpha_{k})\\in\\{i,\\alpha_{k}-i,\\alpha_{k}\\}}\\mathbb{E}_{[i_{0}]\\leq(i_{-\\infty},\\dots,\\alpha_{k})}[R_{\\mathfrak{p}}]\\right)}\\\\ &{\\geq\\underset{1\\leq i\\leq k,(\\alpha_{i}-\\alpha_{k})\\in\\{i,\\alpha_{k}-i,\\beta_{k}\\}}{\\sum_{i\\in\\{i,\\alpha_{i}-\\beta_{k}\\}}}E_{0}|R_{\\mathfrak{p}}|+\\mathbb{E}_{[(\\alpha_{i}-\\alpha_{k})\\times[R_{\\mathfrak{p}}]}}\\\\ &{\\geq\\underset{1\\leq i\\leq1,1\\leq i\\leq k,(\\alpha_{i}-\\alpha_{k})\\in\\{i,\\alpha_{k}\\}}{\\sum_{i\\in\\{i,1\\}}}\\lambda T+\\frac{\\Delta T}{2}\\exp\\left(-2\\Delta^{2}\\sum_{j=1}^{k}\\mathbb{E}_{0}|T_{(1,\\dots,i-1,\\dots,x_{j})}|\\right)}\\\\ &{\\geq\\underset{\\lambda\\in\\{0,1\\}}{\\operatorname*{min}}\\operatorname*{max}\\lambda T+\\frac{\\Delta T}{2}\\exp\\left(-2\\Delta^{2}(\\frac{4}{n-k}X T+\\frac{T}{(k-i-1)})\\right)}\\\\ &{\\geq\\underset{\\lambda\\in\\{0,1\\}}{\\operatorname*{min}}\\operatorname*{max}\\lambda T+\\frac{\\Delta T}{2}+\\frac{1}{2}T^{1/2}\\bigg(\\frac{n-k}{k-i+1}\\bigg)^{1/2}\\exp\\left(-2\\frac{4\\Delta(\\frac{n-k}{k-i+1})}{(n-k)}-2\\right)}\\\\ &{\\geq\\underset{\\lambda\\in\\{0,1\\}}{\\operatorname*{min}}\\operatorname*{max}\\lambda T+\\frac{\\operatorname*{max}}{2^{1/2}}(\\frac{n-k}{k}-1)^{1/2}(\\frac{n-k}{k-i+1})^{1/2}\\exp\\left(-2\\frac{4\\Delta(\\frac{n-k}{k-i+1})}{(n-k)}-2\\right)}\\\\ &{\\geq\\frac{1}{2}{T^{1/2}}\\bigg(\\frac{n-k}{i}\\bigg)^{1/2}e^{-2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The last inequality holds as $\\begin{array}{r}{\\log(\\frac{4T^{1/2}\\binom{n-k}{k-i+1}^{3/2}}{(n-k)T})\\leq0}\\end{array}$ , and the function relative to $\\lambda$ is convex, $\\lambda=0$ minimizes in the last inequality. Combining the two parts of the proof we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{max}\\Big(\\mathbb{E}_{0}[R_{\\mathbf{gr}}],\\underset{1\\leq i\\leq k,(x_{i},\\ldots,x_{k})\\not=(i,\\ldots,k)}{\\operatorname*{max}}\\mathbb{E}_{\\{x_{i},\\ldots,x_{k}\\}}[R_{\\mathbf{gr}}]\\Big)}\\\\ &{\\geq\\operatorname*{max}\\big(\\frac{1}{8}(k-i^{*})^{1/3}T^{2/3}n^{1/3}e^{-8},\\frac{1}{2}T^{1/2}\\binom{n-k}{i^{*}}\\big)^{1/2}e^{-2}\\big)}\\\\ &{\\geq\\frac{1}{16}(k-i^{*})^{1/3}T^{2/3}n^{1/3}e^{-8}+\\cfrac{1}{4}T^{1/2}\\binom{n-k}{i^{*}}^{1/2}e^{-2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.2 Proof of Theorem 2.3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We generalize the lowerbound distance of Theorem 2.1 by having the gap $\\Delta_{i}$ in cardinality $i$ . For any $\\left\\{x_{1},x_{2},...\\,,x_{k}\\right\\}\\;\\in\\;\\left({}^{[n]\\backslash\\{1,...,k\\}}\\right)$ , define instance $\\mathcal{H}_{0},\\mathcal{H}_{(x_{1},\\dots,x_{k})},\\mathcal{H}_{(x_{i+1},\\dots,x_{k})}$ with reward functions as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{\\mathcal{H}_{0}}(S):=\\left\\{H_{|S|+k}-H_{k}=\\sum_{i=1}^{|S|}\\frac{1}{k+i}\\quad S=\\{1,2,\\dots,|S|\\}\\right.}\\\\ &{\\left.\\begin{array}{l l}{H_{|S|+k}-H_{k}-\\Delta_{|S|}}&{\\mathrm{Otherwise}}\\\\ {H_{|S|+k}-H_{k}+\\Delta_{|S|}}&{S=\\{x_{1},x_{2},\\dots,x_{|S|}\\}}\\end{array}\\right.}\\\\ &{f_{\\mathcal{H}_{(x_{1},\\dots,x_{k})}}(S):=\\left\\{\\begin{array}{l l}{H_{|S|+k}-H_{k}}&{S=\\{1,2,\\dots,|S|\\}}\\\\ {H_{|S|+k}-H_{k}}&{S=\\{1,2,\\dots,|S|\\}}\\\\ {H_{|S|+k}-H_{k}-\\Delta_{|S|}}&{\\mathrm{Otherwise}}\\end{array}\\right.}\\\\ &{f_{\\mathcal{H}_{(x_{i+1},\\dots,x_{k})}}(S):=\\left\\{\\begin{array}{l l}{H_{|S|+k}-H_{k}+\\Delta_{|S|}}&{S=\\{1,\\dots,i,x_{i+1},\\dots,x_{|S|}\\}}\\\\ {H_{|S|+k}-H_{k}}&{S=\\{1,2,\\dots,|S|\\}}\\\\ {H_{|S|+k}-H_{k}-\\Delta_{|S|}}&{\\mathrm{Otherwise}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The KL divergance between reward distribution of two instances is similarly: ", "page_idx": 17}, {"type": "equation", "text": "$$\nK L(\\mathbb{P}_{0}|\\mathbb{P}_{\\{x_{i},\\ldots,x_{k}\\}})=\\sum_{j=i}^{k}2\\Delta_{j}^{2}\\mathbb{E}_{0}[T_{1,\\ldots,i-1,x_{i},\\ldots,x_{j}}]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma A.4. For any $i\\leq k$ here exist a sequence $(x_{i},\\ldots,x_{k})$ , where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{=i}^{k}\\Delta_{j}^{2}\\mathbb{E}_{0}[T_{\\{1,\\dots,i-1,x_{i},\\dots,x_{j}\\}}]\\le\\displaystyle\\frac{1}{n-k}\\Delta_{i}^{2}\\mathbb{E}_{0}[T_{i}]+\\displaystyle\\frac{2}{(n-k)(n-k-1)}\\Delta_{i+1}^{2}\\mathbb{E}_{0}[T_{i+1}]}&{}\\\\ {\\displaystyle+\\,\\frac{6}{(n-k)(n-k-1)(n-k-2)}\\Delta_{i+2}^{2}}&{}\\\\ {\\displaystyle}&{+\\,\\displaystyle\\frac{12}{(n-k)(n-k-1)(n-k-2)}\\sum_{j=i+3}^{k-1}\\displaystyle\\frac{k-j}{2k}\\Delta_{j}^{2}\\mathbb{E}_{0}[T_{j}]+\\displaystyle\\frac{\\Delta_{k}^{2}T}{\\binom{n-k}{k-i+1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. For $i\\leq k$ and a sequence $(x_{i},...,x_{k})$ , define $\\begin{array}{r}{Q_{(x_{i},\\dotsc,x_{k})}:=\\sum_{j=i}^{k}\\Delta_{j}^{2}\\mathbb{E}_{0}[T_{\\{1,\\dotsc,i-1,x_{i},\\dotsc,x_{j}\\}}].}\\end{array}$ Then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{\\substack{i_{1},\\ldots,x_{k}\\right)\\neq(i_{1},\\ldots,k)}}Q_{(x_{i},\\ldots,x_{k})}\\leq\\sum_{j=i}^{k-1}\\frac{(n-k-j+i-1)!(j-i+1)!}{(n-2k+i-1)!}\\Delta_{j}^{2}\\mathbb{E}_{0}[T_{j}]+((k-i+1)!)\\Delta_{k}^{2}\\mathbb{E}_{0}[T_{k}]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then by Pigeonhole principle, the exists a sequence $(x_{i},\\ldots,x_{k})$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathsf{J}_{(x_{i},\\ldots,x_{n})}\\le\\frac{2}{(n-2\\!+\\!1\\!+\\!1)!}}&{}\\\\ &{\\le\\displaystyle\\sum_{j=i}^{k-1}\\frac{(n-k-j+i-1)!(j-i+1)!}{(n-k)!}\\le\\!\\frac{1}{\\Delta_{j}^{2}\\mathsf{E}_{0}[Z_{j}]}+\\frac{(n-2k+i-1)!(k-i+1)!}{(n-k)!}\\Delta_{k}^{2}\\mathbb{E}[\\frac{1}{2}}\\\\ &{\\le\\displaystyle\\frac{1}{n-k}\\Delta_{i}^{2}\\mathsf{E}_{0}[Z_{i}]+\\frac{1}{(n-k)!(n-k-1)}\\sum_{j=i+1}^{k-1}\\frac{(j-i)(j-i-1)}{(n-k-2)!}\\Delta_{j}^{2}\\mathbb{E}[Z_{j}]+\\frac{1}{{(n-k-1)!}}\\mathbb{E}}\\\\ &{\\le\\displaystyle\\frac{1}{n-k}\\Delta_{i}^{2}\\mathsf{E}_{0}[Z_{i}]+\\frac{2}{(n-k)!(n-k-1)}\\Delta_{i}^{2}\\mathsf{E}_{0}[Z_{i+1}]}\\\\ &{\\quad+\\frac{6}{(n-k)(n-k-1)(n-k-2)}\\Delta_{i}^{2}\\mathsf{A}_{2}}\\\\ &{\\quad+\\frac{12}{(n-k)(n-k-1)(n-k-2)}\\displaystyle\\sum_{j=1}^{k-1}\\frac{k-j}{2k}\\Delta_{j}^{2}\\mathsf{E}_{0}[Z_{j}]+\\frac{\\Delta_{k}^{2}T}{{(n-k-1)!}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We now assign $\\Delta_{i}$ for lower cardinalities based on the value of $\\Delta_{k}$ . If $\\mathbf{1}^{T}\\epsilon^{\\prime}\\le2\\Delta_{k}$ , $\\ker i\\leq k{-1}$ , we assign $\\Delta_{i}=\\epsilon_{i}^{\\prime}$ , so a greedy procedure with $\\epsilon^{\\prime}$ will retrieve the best set, hence $f_{\\mathcal{H}_{0}}(S_{\\mathbf{gr}}^{k,\\epsilon^{\\prime}})+\\mathbf{1}^{T}\\boldsymbol\\epsilon^{\\prime}\\geq$ fH0({1, . . . , k}) and fH(xi,...,x )( $f_{\\mathcal{H}_{(x_{i},\\ldots,x_{k})}}(S_{\\mathbf{gr}}^{k,\\epsilon^{\\prime}})\\!+\\!\\mathbf{1}^{T}\\epsilon^{\\prime}\\geq f_{\\mathcal{H}_{(x_{i},\\ldots,x_{k})}}(\\{1,\\ldots,i-1,x_{i}\\ldots,x_{k}\\})$ . Otherwise, since the gap of any set of size $k$ and the best set is at most $2\\Delta_{k}$ for both $\\mathcal{H}_{0}$ and $\\mathcal{H}_{(x_{i},\\dots,x_{k})}$ , $f_{\\mathcal{H}_{0}}(S_{\\mathbf{gr}}^{k,\\epsilon^{\\prime}})+\\mathbf{1}^{T}\\epsilon^{\\prime}\\geq H_{2k}-H_{k}-\\Delta_{k}+2\\Delta_{k}=f_{\\mathcal{H}_{0}}(\\{1,\\ldots,k\\})$ and $f_{\\mathcal{H}_{(x_{i},\\ldots,x_{k})}}(S_{\\mathbf{gr}}^{k,\\epsilon^{\\prime}})+\\mathbf{1}^{T}\\boldsymbol{\\epsilon}^{\\prime}\\geq$ $H_{2k}-H_{k}-\\Delta_{k}+2\\Delta_{k}\\ge f_{\\mathcal{H}_{(x_{i},\\dots,x_{k})}}(\\{1,\\dots,i-1,x_{i}\\dots,x_{k}\\})$ ; so for $i\\leq k-1$ , and we assign $\\begin{array}{r}{\\Delta_{i}=\\frac{\\Delta_{k}}{k}}\\end{array}$ . Therefore, in both cases $R_{\\mathbf{gr}}\\geq R(S^{*})$ , and we give a lower bound for $R(S^{*})$ . ", "page_idx": 17}, {"type": "text", "text": "For the first part of the lower bound, we\u2019ll assign $\\begin{array}{r}{\\Delta_{k}=(k-i^{*})(\\frac{n}{T})^{1/3}}\\end{array}$ . Now similarly to proof of Theorem 2.1, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\operatorname*{max}\\left(\\mathbb{E}_{\\eta}|\\mathbb{E}_{\\psi}|_{\\lambda\\leq\\xi\\leq\\xi,\\eta\\leq m,\\eta}\\mathrm{exp}_{i(\\xi-\\eta,\\eta)}\\mathrm{E}_{\\psi(\\xi,\\eta,\\eta)}\\big[\\psi_{\\eta}\\right]\\right)}\\\\ &{\\geq_{1\\leq\\xi\\leq\\xi,(\\xi,\\eta,\\eta,\\eta)\\leq m,\\eta}\\mathrm{exp}_{i(-1)}\\frac{\\Delta_{\\xi}\\mathrm{T}}{2}\\mathrm{exp}\\left(-2\\frac{\\xi}{\\eta}\\mathrm{e}^{2\\xi}\\mathrm{exp}_{i}|\\mathcal{F}_{(1,\\eta-\\xi,\\eta,\\eta-\\eta,\\eta)}\\right)}\\\\ &{\\geq\\frac{\\Delta_{\\xi}\\mathrm{T}}{2}\\frac{\\mathrm{imp~ax~}}{1\\leq\\xi\\leq m}\\mathrm{exp}\\left(-2(\\frac{1}{n-k}\\frac{\\Delta_{\\xi}\\mathrm{T}}{2}|\\mathcal{F}_{1}|+(\\frac{2}{n-k})^{2}\\frac{2}{n-k})\\mathrm{e}^{2\\xi_{1}\\xi_{1}}\\mathrm{exp}_{i}|\\right.}\\\\ &{\\qquad+\\frac{\\delta}{(n-k)(n-k-1)(n-k-2)}\\mathrm{exp}_{i(\\xi-\\eta,\\eta)}\\mathrm{exp}_{i(1)}^{2}\\mathrm{exp}_{i(1)}-\\frac{\\xi^{-1}}{2}\\mathrm{exp}_{i(1)}^{2}\\mathrm{exp}_{i(1)}}\\\\ &{+\\frac{\\delta\\mathrm{L}^{2}}{(n-k)^{3}}\\right)}\\\\ &{\\geq\\frac{\\mathrm{imp~ax~}}{1\\leq\\eta\\leq m}\\mathrm{e}^{2(k-1)/2^{(3)}m^{(1)/2}\\mathrm{exp}}\\left(-2(\\frac{1}{n-k}\\delta^{2}|\\mathcal{F}_{1}|+\\frac{2}{(n-k)(n-k-1)}\\alpha_{i}^{2}\\mathrm{exp}_{i(1)+1}}\\\\ &{\\qquad+\\frac{\\delta}{(n-k)(n-k-1)(n-k-2)}\\Delta_{i}^{2}+\\frac{1}{(n-k)(n-k-1)(n-k-2)}\\frac{k-1}{2}\\frac{k}{2}\\mathrm{e}^{2}|\\mathcal{F}_{2}|\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\mathrm{(S u p~a x~\\textunderscore~\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For 1 \u2264 i \u2264 k \u2212i\u2217+ 1, (k\u2212i\u2217)n2n2k/3T 1/3 $\\begin{array}{r}{\\frac{(k-i^{*})^{2}n^{2/3}T^{1/3}}{\\binom{n-k}{k-i+1}}\\;\\leq\\;\\frac{k^{2}n^{2/3}T^{1/3}}{\\binom{n-k}{k-i+1}}\\;\\leq\\;1}\\end{array}$ by definition of $i^{*}$ ; so either the maximum regret is larger than $\\begin{array}{r}{\\frac{1}{4}T^{2/3}(k-i^{*})n^{1/3}\\exp(-10)}\\end{array}$ , which proves the theorem, or ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{n-k}\\Delta_{i}^{2}\\mathbb{E}_{0}[T_{i}]+\\frac{2}{(n-k)(n-k-1)}\\Delta_{i+1}^{2}\\mathbb{E}_{0}[T_{i+1}]+\\frac{6}{(n-k)(n-k-1)(n-k-2)}\\Delta_{i+2}^{2}}\\\\ {\\displaystyle+\\,\\frac{12}{(n-k)(n-k-1)(n-k-2)}\\sum_{j=i+3}^{k-1}\\frac{k-j}{2k}\\Delta_{j}^{2}\\mathbb{E}_{0}[T_{j}]\\Delta_{j}^{2}\\mathbb{E}_{0}[T_{j}]\\geq4}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If the forth term is larger than 1, then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{k}^{2}\\displaystyle\\sum_{j=i+3}^{k-1}\\frac{k-j}{2k}\\mathbb{E}_{0}[T_{j}]\\geq\\displaystyle\\sum_{j=i+3}^{k-1}\\frac{k-j}{2k}\\Delta_{j}^{2}\\mathbb{E}_{0}[T_{j}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\frac{(n-k)(n-k-1)(n-k-2)}{12}\\geq\\frac{n^{3}}{96}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$\\begin{array}{r}{0\\,\\sum_{j=i+3}^{k-1}\\frac{k-j}{2k}\\mathbb{E}_{0}[T_{j}]\\ge\\frac{n^{3}}{96}\\frac{1}{\\Delta_{k}^{2}}\\ge\\frac{1}{96}(k-i^{*})n^{1/3}T^{2/3}}\\end{array}$ which proves the lower bound. ", "page_idx": 18}, {"type": "text", "text": "Therefore, the only remaining case is that at least one of the first three terms is $\\geq1$ . This means that for 1 \u2264i \u2264k \u2212i\u2217+ 1, either E0[Ti] \u2265 2n\u22062 , or E0[Ti+1] \u2265 4\u2206n2 $\\begin{array}{r}{\\mathbb{E}_{0}[T_{i+1}]\\,\\ge\\,\\frac{n}{4\\Delta_{i+1}^{2}}\\bigl(n-k-1\\bigr)\\,\\ge\\,\\frac{n}{4\\Delta_{i+1}^{2}}}\\end{array}$ , or E0[Ti+2] \u2265 12\u2206ni2+2 (n \u2212k \u22121)(n \u2212k \u22122) \u226512\u2206ni2+2 . ", "page_idx": 18}, {"type": "text", "text": "Therefore, for at least $1/3$ of the $\\begin{array}{r}{1\\leq i\\leq k-i^{*}+1,\\mathbb{E}_{0}[T_{i}]\\,\\geq\\,\\frac{n}{12\\Delta_{i}^{2}}}\\end{array}$ . Let $I$ be all cardinalities in which this inequality holds(so $\\begin{array}{r}{|I|\\geq\\frac{k-i^{*}}{3},}\\end{array}$ ; since $\\begin{array}{r}{\\sum_{i=1}^{k-1}\\Delta_{i}\\leq2\\Delta_{k}}\\end{array}$ , using Lemma C.1, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{0}[R_{\\mathbf{gr}}]\\geq\\sum_{j=1}^{k-i^{*}+1}\\frac{k-j}{2k}\\mathbb{E}_{0}[T_{j}]\\geq\\sum_{j\\in I}\\frac{k-j}{2k}\\frac{n}{12\\Delta_{j}^{2}}\\geq\\frac{1}{288}(k-i^{*})T^{2/3}n^{1/3}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For the second part of the lower bound, using $\\Delta_{i}\\leq2\\Delta_{k}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nK L(\\mathbb{P}_{0}|\\mathbb{P}_{\\{x_{i},\\ldots,x_{k}\\}})\\le8\\Delta_{k}^{2}\\sum_{j=i}^{k-1}\\mathbb{E}_{0}[T_{1,\\ldots,i-1,x_{i},\\ldots,x_{j}}]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": ", and the rest of the argument follows the proof of 2.1. ", "page_idx": 19}, {"type": "text", "text": "B Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. We use the notation $l=k-i^{*}$ to match our lowerbound, however as $k-i^{*}$ is arbitrary, it can be used for any other choice of $l$ as well. Define the event $\\begin{array}{r}{G:=\\bigcap_{i=1}^{k}\\bigcap_{a\\in[n]\\backslash S^{(i-1)}}\\bigcap_{t=1}^{T}{\\dot{g_{i,a,t}}}}\\end{array}$ where ", "page_idx": 19}, {"type": "equation", "text": "$$\ng_{i,a,t}:=\\left\\{\\Big|\\sum_{\\substack{s\\leq t;I_{s}=S^{(i-1)}\\cup\\{a\\}}}(r_{s}-f(S^{(i-1)}\\cup\\{a\\}))\\Big|\\leq\\sqrt{2T_{S^{(i-1)}\\cup\\{a\\}}(t)\\log(2k n T^{2})}\\right\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now note that if $X_{s}$ are i.i.d. sub-Gaussian random variables then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{P}(G^{\\tau})\\leq\\sum_{i=1}^{k}\\mathbb{P}\\Big(\\bigcup_{s=i}^{j}\\bigcup_{s=i}^{\\tau}g_{i,a,t}^{\\tau}\\Big)}\\qquad\\hfill}\\\\ &{=\\sum_{i=1}^{k}\\sum_{s\\in\\{\\binom{j}{1}\\}}\\mathbb{P}\\Big(\\bigcup_{s=i}^{j}\\bigcup_{s=i}^{\\tau}g_{i,a,t}^{\\tau}|S^{(i-1)}=S\\Big)\\mathbb{P}(S^{(i-1)}=S)}\\\\ &{\\leq\\frac{k}{\\sum_{i=1}^{k}S(\\binom{j}{1})\\alpha(e||S^{\\tau}|)}\\leq\\frac{\\mathbb{P}\\Big(\\bigcup_{s=i}^{k}\\bigcup_{s=i}^{k-1}S^{\\tau}(\\mathcal{S}^{(i-1)}=S)\\Big)\\mathbb{P}(S^{(i-1)}=S)}{t_{i}!}}\\\\ &{\\leq\\sum_{i=1}^{k}\\sum_{s\\in\\{\\binom{j}{1}\\}}\\sum_{s\\in\\{|\\alpha|\\}}\\mathbb{P}\\Big(\\bigcup_{s=i}^{j}\\bigcup_{s=i}^{\\tau}|S^{(i-1)}=S\\Big)\\mathbb{P}(S^{(i-1)}=S)}\\\\ &{\\leq\\sum_{i=1}^{k}\\sum_{s\\in\\{\\binom{j}{1}\\}}\\sum_{s\\in\\{|\\alpha|\\}}\\mathbb{P}\\Big(\\bigcup_{s=1}^{T}\\bigcup_{s=1}^{k}\\big|\\sum_{s=1}^{j}\\mathbb{P}\\big(\\mathcal{Z}\\mathrm{I}[\\mathcal{H}\\alpha^{(j-1)}]\\big)\\Big)\\mathbb{P}(S^{(i-1)}=S)}\\\\ &{\\leq\\sum_{i=1}^{k}\\sum_{s\\in\\{\\binom{j}{1}\\}}\\sum_{s\\in\\{|\\alpha|\\}}\\sum_{s\\in\\{1}}^{T}\\frac{1}{t_{i}!}\\mathbb{P}(S^{(i-1)}=S)\\leq1/T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $\\mathcal{E}_{i}$ be the event that the arm selected at the $i$ -th step of the algorithm is within $2\\sqrt{\\frac{2\\log(2k n T^{2})}{m}}$ of the best possible arm at that step, i.e. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{E}_{i}=\\left\\{\\operatorname*{max}_{a\\notin S^{(i-1)}}f(S^{(i-1)}\\cup\\{a\\})-f(S^{(i)})\\leq2\\sqrt{\\frac{2\\log(2k n T^{2})}{m}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We prove that on event $G,\\cup_{i\\in[k-i^{*}]}\\mathcal{E}_{i}$ is true. ", "page_idx": 19}, {"type": "text", "text": "Let $a$ be a sub optimal arm with value more than 2 log(2mknT 2) from the best arm in the i-th step. That is, if $a^{*}:=\\arg\\operatorname*{max}_{a^{\\prime}}f(S^{(i)}\\cup\\{a^{\\prime}\\})$ and $\\Delta_{S^{(i)},a}:=f(S^{(i)}\\cup\\{a^{*}\\})-f(S^{(i)}\\cup\\{a\\})$ , then assume that $\\begin{array}{r}{\\Delta_{S^{(i)},a}\\geq2\\sqrt{\\frac{2\\log\\left(2k n T^{2}\\right)}{m}}}\\end{array}$ 2 log(2mknT 2). Then on event G and arm a being added in i-th step, ", "page_idx": 19}, {"type": "equation", "text": "$$\nU_{a}(t)\\geq U_{a^{*}}(t)\\geq f(S^{(i)}\\cup\\{a^{*}\\})=f(S^{(i)}\\cup\\{a\\})+\\Delta_{S^{(i)},a}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which implies ", "page_idx": 19}, {"type": "equation", "text": "$$\nU_{a}(t)-f(S^{(i)}\\cup\\{a\\})\\geq\\Delta_{S^{(i)},a}>2\\sqrt{\\frac{2\\log(2k n T^{2})}{m}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "But this implies that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{S^{(i)}\\cup\\{a\\}}-f(S^{(i)}\\cup\\{a\\})>\\sqrt{\\frac{2\\log(2k n T^{2})}{m}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which is a contradiction of event $G$ . Thus, on event $G$ such an arm cannot be selected. ", "page_idx": 19}, {"type": "text", "text": "As UCB in the second part of the algorithm has the regret of $\\begin{array}{r}{65\\sqrt{T\\binom{n-k}{k-l}\\log T}+\\frac{32}{15}\\binom{n-k}{k-l}}\\end{array}$ against $S^{(k)}$ which is the best size $k$ arm containing $S^{(k-i^{*})}$ (see [24]), on event $G$ , it is an upper bound for the regret against the greedy solution were the first $k-i^{*}$ steps select an $\\epsilon_{}$ -good arm, and the last $i^{*}$ steps select the best arm, so on event $G$ the regret can be written against a set in $S^{k,\\epsilon}$ where ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{1}^{T}\\epsilon=(k-i^{*})\\epsilon=2(k-i^{*})\\sqrt{\\frac{2\\log{(2k n T^{2})}}{m}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, we upper bound the regret relative to $\\begin{array}{r}{f(S^{k})+2(k-i^{*})\\sqrt{\\frac{2\\log(2k n T^{2})}{m}}}\\end{array}$ 2 log(2mknT 2), as by Lemma 1.1 From the while lcoop  \u2212condition in the algorithm, we have |Ti| \u2264 a\u2208/S(i\u22121) min \u22062S(i1\u22121),a, ${\\textstyle\\frac{1}{c}}(1-e^{-c})f(S^{*})$ be the set of times where we pulled a set of cardinality $i$ . ", "page_idx": 20}, {"type": "text", "text": "$(n+1-i)m$ for $i\\leq k-i^{*}$ . For $\\begin{array}{r}{\\epsilon=2\\sqrt{\\frac{2\\log(2k n T^{2})}{m}}}\\end{array}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[R_{\\pmb{w}}]\\leq\\mathbb{P}[G^{*}]T+\\mathbb{E}[R_{\\pmb{w}}\\mathbf{1}(G)]\\leq\\frac{1}{T}T+\\mathbb{E}[R_{\\pmb{w}}\\mathbf{1}(G)]}\\\\ &{\\qquad\\leq1+\\displaystyle\\sum_{i=1}^{k-i}\\sum_{t\\in\\mathcal{T}_{h}}(f(\\delta^{(k)})+(k-i^{*})c)-f(S^{(i-1)}\\cup\\{a_{i}\\}))+\\sum_{t\\in\\mathcal{T}_{h}}(f(S^{(k)})+(k-i^{*})c)\\cdot}\\\\ &{\\qquad\\leq1+(k-i^{*})c T+m n(k-i^{*})f(S^{(k)})+\\displaystyle\\sum_{t\\in\\mathcal{T}_{h}}f(S^{(k)})-f(S_{i})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(f(S^{(i-1)}\\cup\\{a_{i}\\}))\\geq0}\\\\ &{\\qquad\\leq2T(k-i^{*})\\sqrt{\\frac{2\\log(2k n T^{2})}{m}}+m n(k-i^{*})+65\\sqrt{T\\binom{n}{i+1}}+\\frac{32\\,n-k}{15}+1}\\\\ &{\\qquad\\leq2T^{2(3n^{1})/3}(k-i^{*})(\\log(2k n T^{2}))^{1/3}+\\sqrt{8\\widetilde{T}^{2/3}n^{1/3}}(k-i^{*})(\\log(2k n T^{2}))^{1/3}}\\\\ &{\\qquad\\qquad+65\\sqrt{T\\binom{n}{i+1}}+\\frac{32\\,n-k}{15}+1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "C Auxiliary Lemmas ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lemma C.1. For any sequence of numbers $a_{1},\\ldots,a_{n}$ bounded between $(0,1],\\,I f\\textstyle\\sum_{i}a_{i}\\leq C\\leq1,$ , then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}{\\frac{1}{a_{i}^{2}}}\\geq{\\frac{n^{3}}{C^{2}}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. If there exists $\\begin{array}{r l r}{j,k}&{{}\\in}&{[n]}\\end{array}$ such that $\\begin{array}{r l r}{a_{j}}&{{}<}&{a_{k}}\\end{array}$ , then for a new sequence $\\begin{array}{r l}{a_{i}^{\\prime}}&{{}=}\\end{array}$ $\\left\\{\\!\\!\\begin{array}{l l}{a_{i}}&{i\\notin\\{j,k\\}}\\\\ {\\frac{a_{j}+a_{k}}{2}}&{i\\in\\{j,k\\}}\\end{array}\\!\\!\\right.$ we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum a_{i}^{-2}-\\sum a_{i}^{\\prime-2}=a_{j}^{-2}+a_{k}^{-2}-2\\frac{4}{(a_{j}+a_{k})^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\displaystyle=\\frac{2+\\overbrace{a_{j}^{2}a_{k}^{-2}+a_{j}^{-2}a_{k}^{2}}^{>2}+2\\overbrace{(a_{j}^{-1}a_{k}+a_{j}a_{k}^{-1})-8}^{>2}}{a_{j}^{2}+a_{k}^{2}+2a_{j}a_{k}}>0}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, the infimum value of $\\sum{a_{i}^{-2}}$ over all such sequences is when all elements are equal, and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}{\\frac{1}{a_{i}^{2}}}\\geq n{\\bigl(}{\\frac{n}{\\sum a_{i}}}{\\bigr)}^{2}\\geq{\\frac{n^{3}}{C^{2}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We\u2019ve stated all our results and the assumptions for them in the introduction. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: For our theoretical results, the assumptions are clearly stated, and our results are within these assumptions. We also state the experimental limitation of our algorithm in the experiments section. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All the assumptions for each theorem are stated, a proof sketch is provided in the main text, and full proofs are in the appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes all the algorithms that are used in experiments and all the function definitions that are used as inputs are clearly written. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [No] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper is a theoretical work, and the experiments apply the proposed algorithm in main text on synthetic functions which are fully defined in the main text. There are no datasets used, and no code beyond the main algorithm implementation. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We explain the functions used as inputs, and the synthetic noise applied to them. There are no other setting details as this is mainly a theory paper and the experiments are trivial. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The empirical SDs are included in the plots, although hardly visible because of the logarithmic scale. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: There are no experiments that need anything more than a laptop. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Yes this paper conforms, in every respect, with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This work is mainly theoretical, and there are no direct social impacts. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper doesn\u2019t pose any such risks as it is theoretical. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: No assets are used in this work. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: No new assets are released in this work. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: he paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}]