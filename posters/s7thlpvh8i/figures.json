[{"figure_path": "S7THlpvH8i/figures/figures_2_1.jpg", "caption": "Figure 2: The variance of the GNS estimator for different Bbig (left) and Bsmall (right) sizes. Bbig = l and Bsmall = s in legends. Stderr is estimated using a jackknife resampling method for ratio estimators [12]. For the same number of samples processed, a smaller Bsmall always has a lower standard error, while the size of the large batch, Bbig does not affect the standard error.", "description": "This figure shows the variance of the Gradient Noise Scale (GNS) estimator as a function of the batch sizes used for computation (Bbig and Bsmall).  The left panel demonstrates that the standard error of the GNS estimator decreases as Bsmall decreases, while it is unaffected by changes in Bbig.  The right panel shows the same trend for different values of Bbig.", "section": "2.1 Gradient Noise Scale"}, {"figure_path": "S7THlpvH8i/figures/figures_5_1.jpg", "caption": "Figure 3: FLOP cost of computing per-example gradient norms. (Left) Total FLOP cost. (Right) Proportional cost versus one model forward and backward pass. The FLOP cost of Simultaneous per-example gradient norms is strictly dominant to alternative methods (left) and the ratio of this additional cost to the FLOP cost of processing the entire model does not depend on context length (right).", "description": "This figure compares the FLOP cost of three methods for computing per-example gradient norms against the total FLOP cost of a forward and backward pass of a neural network model. The left panel shows the total FLOP cost for each method, while the right panel shows the ratio of the per-example gradient norm FLOP cost to the total model FLOP cost.  The \"Simultaneous\" method consistently outperforms the other methods in terms of FLOPs, especially for longer sequences. The ratio of the additional FLOP cost to the total model FLOP cost remains relatively constant across different sequence lengths for the Simultaneous method.", "section": "3 Simultaneous Per-example Gradient Norms"}, {"figure_path": "S7THlpvH8i/figures/figures_5_2.jpg", "caption": "Figure 4: Total I/O cost of computing per-example gradient norms, assuming gradients and parameters are stored with 4 bytes of precision. The relative IO cost of Simultaneous per-example gradient norms is less than Li et al. [36] for very long contexts for all model scales, approximately equivalent for models of 10B parameters and 4096 context length, and higher for shorter contexts with larger models. The IO cost of LN (LayerNorm) per-example gradient norms alone is much lower than either method.", "description": "This figure compares the I/O cost of three different methods for computing per-example gradient norms: the method proposed in the paper (Simultaneous), the method by Li et al. [36], and a method that only considers LayerNorm layers (LN only).  The results show that the Simultaneous method is more efficient for longer sequences and larger models.  The LN only method is significantly more efficient than the others, suggesting that focusing on LayerNorm layers is sufficient for accurate GNS estimation. The I/O cost is shown as a proportion of the model's forward pass I/O cost.", "section": "3.1 FLOPs and I/O Costs"}, {"figure_path": "S7THlpvH8i/figures/figures_6_1.jpg", "caption": "Figure 5: GNS phase plot: Linear/Embedding layers are separated from LayerNorm layers by row. Component estimators of Equations 4 and 5 are shown (left) with the GNS over the course of training on the (right).", "description": "This figure shows the GNS (Gradient Noise Scale) phase plot.  It visualizes the relationship between two estimators of GNS (||G||\u00b2 and S) and the overall GNS, across different layers of a neural network during training. The left plots show the component estimators for linear/embedding layers and LayerNorm layers separately. The right plots show the overall GNS trends for each layer type and a combined GNS, providing a visual representation of how the different components contribute to the overall gradient noise scale over the training process.", "section": "4 Gradient Noise Scale in Transformer Language Models"}, {"figure_path": "S7THlpvH8i/figures/figures_7_1.jpg", "caption": "Figure 6: During the middle of training a 111M parameter language model on OpenWebText, the learning rate, \u03b5 or batch size, B were varied, restarting the run from the same point. This Figure replicates an experiment from McCandlish et al. [39] showing how varying the ratio causes changes in the measured GNS, but here only due to changes in the learning rate. Changes in the batch size do not have the predicted effect.", "description": "This figure replicates an experiment from a previous work which shows how changing the ratio of batch size to learning rate affects the Gradient Noise Scale (GNS). The authors of this paper varied the learning rate and batch size independently, keeping their ratio constant, and found that only changes in the learning rate affected the GNS, while changes in the batch size did not.", "section": "GNS Correlates Between Layer Types"}, {"figure_path": "S7THlpvH8i/figures/figures_8_1.jpg", "caption": "Figure 7: Regression of total GNS using the GNS of each layer type. (Left) GNS of each layer type and the total GNS are plotted against the number of tokens processed for varying EMA alpha settings. (Center & Right) The slope and Pearson\u2019s correlation coefficient of the regression of the total GNS against the GNS of each layer type, respectively, as a function of the same EMA alpha values. The total GNS (black) on the left is predicted well by individual layer types as indicated by the correlation coefficients (right), however the type with slope closest to 1 is LayerNorm (center), only overestimating the GNS by less than 40% across EMA alpha values.", "description": "This figure shows the relationship between the total gradient noise scale (GNS) of a transformer model and the GNS of individual layer types (Attention, LayerNorm, MLP, Embedding). The left panel plots the GNS for each layer type and the total GNS against the number of tokens processed, for different values of EMA alpha (a smoothing parameter). The center and right panels show the slope and Pearson correlation coefficient, respectively, of the regression of total GNS against each layer type's GNS, as a function of EMA alpha.  The results indicate that the total GNS is strongly correlated with the GNS of individual layer types, especially LayerNorm, suggesting that monitoring LayerNorm's GNS can provide a good estimate of the overall GNS.", "section": "4 Gradient Noise Scale in Transformer Language Models"}, {"figure_path": "S7THlpvH8i/figures/figures_8_2.jpg", "caption": "Figure 8: Comparison of average time taken for a LayerNorm forward and backward pass with gradient accumulation when using PyTorch's native implementation versus our custom kernel computing per-example gradient norms in tandem. Measured on an Nvidia H100 GPU.", "description": "This figure compares the performance of PyTorch's built-in LayerNorm implementation against a custom kernel developed by the authors. The custom kernel is designed to simultaneously compute per-example gradient norms alongside the standard forward and backward passes of LayerNorm. The comparison is performed across varying dimensionalities (768, 2048, and 4096), showing the average time taken for both implementations.  The results demonstrate that the custom kernel achieves comparable or better performance than PyTorch's implementation, especially at higher dimensionalities. This highlights the efficiency of the proposed approach, which enables gradient noise scale (GNS) estimation with near-zero overhead.", "section": "5.1 Universal GNS with Zero Overhead"}, {"figure_path": "S7THlpvH8i/figures/figures_9_1.jpg", "caption": "Figure 9: (Left) Linear batch size schedule tracking the GNS over 2.2 billion tokens processed. Loss is plotted over a smoothed range from 3 runs using different Seeds. (Right) The number of tokens saved over the fixed batch size run to achieve the same loss.", "description": "This figure shows the results of an experiment comparing a fixed batch size training schedule to one where the batch size increases linearly with the number of tokens processed. The left plot shows the training loss for both schedules over the course of training. The right plot shows the number of tokens saved by using the linear batch size schedule compared to the fixed batch size schedule, to achieve the same training loss.  The results demonstrate the effectiveness of the linear batch size schedule in reducing training time.", "section": "5 Batch Size Scheduling"}, {"figure_path": "S7THlpvH8i/figures/figures_16_1.jpg", "caption": "Figure 10: The loss of models trained on OpenWebText with 70M, 111M and 161M parameters. The learning rate was varied to find the minima in the loss at each model scale. The optimal learning rate for each model size is annotated.", "description": "This figure shows the validation loss of three different-sized language models (70M, 111M, and 161M parameters) trained on the OpenWebText dataset. The x-axis represents the learning rate used during training, and the y-axis represents the validation loss achieved.  Each model size has multiple data points, each representing a different learning rate tested. The optimal learning rate for each model size that resulted in the lowest validation loss is annotated on the plot. The goal was to determine the optimal learning rate for each model size while maintaining a constant total number of FLOPs.", "section": "C Language Model Experiment Details"}, {"figure_path": "S7THlpvH8i/figures/figures_17_1.jpg", "caption": "Figure 5: GNS phase plot: Linear/Embedding layers are separated from LayerNorm layers by row. Component estimators of Equations 4 and 5 are shown (left) with the GNS over the course of training on the (right).", "description": "This figure shows the relationship between the different components of the gradient noise scale (GNS) and the overall GNS over the course of training a language model.  The left side shows the individual components (||G||\u00b2 and S) for Linear/Embedding layers and LayerNorm layers separately. The right side shows the overall GNS calculated from these components. This visualization helps to understand how the different layer types contribute to the overall GNS and how these components change during training.", "section": "4 Gradient Noise Scale in Transformer Language Models"}, {"figure_path": "S7THlpvH8i/figures/figures_18_1.jpg", "caption": "Figure 12: Two \u201cstudent", "description": "This figure shows the results of an experiment comparing two student networks trained with and without Flash Attention. Both networks start with similar weights but the teacher network has additional noise added to its weights.  The plot shows that the network using Flash Attention diverges from the teacher network as training progresses, while the network without Flash Attention remains close. The different plots illustrate several metrics like bias norms, distances to the teacher, and the difference in distances between the two student networks.", "section": "4 Gradient Noise Scale in Transformer Language Models"}, {"figure_path": "S7THlpvH8i/figures/figures_18_2.jpg", "caption": "Figure 12: Two \u201cstudent", "description": "This figure shows the results of an experiment designed to simulate the divergence observed when using Flash Attention. Two networks, a teacher and a student, are trained. The student network is identical to the teacher except for a small amount of added noise to the teacher's QKV projection bias. The plots show how the bias norms, distances to the teacher, distances between the networks (student models using Flash Attention and a control network without it), and the difference in distances to the teacher between the two networks change over training iterations. The results show that, in this simulated scenario, the student using Flash Attention diverges.", "section": "4.1 The Temperature of Training"}, {"figure_path": "S7THlpvH8i/figures/figures_20_1.jpg", "caption": "Figure 5: GNS phase plot: Linear/Embedding layers are separated from LayerNorm layers by row. Component estimators of Equations 4 and 5 are shown (left) with the GNS over the course of training on the (right).", "description": "This figure visualizes the relationship between the gradient noise scale (GNS) and its component estimators (||G||2 and S) over the course of training a transformer model.  It shows the GNS and its components for both linear/embedding layers and LayerNorm layers separately. The plots show that the GNS of LayerNorm layers strongly correlates with the total GNS, implying the efficiency of focusing only on the LayerNorm layers for practical GNS tracking. The 'combined' trace (black) shows that LayerNorm and the other layers' GNS estimates (colored traces) trend similarly during training.", "section": "4 Gradient Noise Scale in Transformer Language Models"}, {"figure_path": "S7THlpvH8i/figures/figures_21_1.jpg", "caption": "Figure 9: (Left) Linear batch size schedule tracking the GNS over 2.2 billion tokens processed. Loss is plotted over a smoothed range from 3 runs using different Seeds. (Right) The number of tokens saved over the fixed batch size run to achieve the same loss.", "description": "This figure shows the results of an experiment comparing a fixed batch size training schedule with a linear batch size schedule that increases linearly with the number of tokens processed. The left plot shows the training loss for both schedules, smoothed over three runs with different random seeds. The right plot shows the number of tokens saved by using the linear batch size schedule compared to the fixed batch size schedule to achieve the same loss.", "section": "5 Batch Size Scheduling"}, {"figure_path": "S7THlpvH8i/figures/figures_21_2.jpg", "caption": "Figure 7: Regression of total GNS using the GNS of each layer type. (Left) GNS of each layer type and the total GNS are plotted against the number of tokens processed for varying EMA alpha settings. (Center & Right) The slope and Pearson's correlation coefficient of the regression of the total GNS against the GNS of each layer type, respectively, as a function of the same EMA alpha values. The total GNS (black) on the left is predicted well by individual layer types as indicated by the correlation coefficients (right), however the type with slope closest to 1 is LayerNorm (center), only overestimating the GNS by less than 40% across EMA alpha values.", "description": "This figure shows the relationship between the total Gradient Noise Scale (GNS) of a transformer model and the GNS of individual layer types (Attention, LayerNorm, MLP, Embedding). The left panel plots the GNS values against the number of tokens processed for various EMA smoothing factors (alpha). The central and right panels show the regression slope and Pearson correlation coefficient between the total GNS and each individual layer type's GNS as a function of the EMA alpha.  The results indicate a strong correlation between the total GNS and the GNS of the LayerNorm layers, with the LayerNorm GNS being a particularly good predictor of the overall GNS.", "section": "4 Gradient Noise Scale in Transformer Language Models"}, {"figure_path": "S7THlpvH8i/figures/figures_21_3.jpg", "caption": "Figure 16: 1.3B GPT model train on OpenWebText using 8 H100s, trained twice. (Left) Per-example gradient norms for all layers were gathered to replicate the analysis in Figure 7. (Right) Per-example gradient norms were gathered for only LayerNorm layers, then compared to the GNS computed using traditional DDP methods.", "description": "This figure compares the gradient noise scale (GNS) estimation methods for a larger 1.3B parameter GPT model trained on OpenWebText dataset using 8 H100 GPUs.  The left panel shows per-example gradient norms across all layer types, similar to the analysis in Figure 7. The right panel focuses on LayerNorm layers' per-example gradient norms and compares the GNS with the traditional method based on Distributed Data Parallel (DDP), demonstrating that LayerNorm layers are highly predictive of the overall GNS even in this larger model, and that the proposed method is efficient.", "section": "Larger Scale Training"}]