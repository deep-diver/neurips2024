[{"type": "text", "text": "ParallelEdits: Efficient Multi-Aspect Text-Driven Image Editing with Attention Grouping ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mingzhen Huang, Jialing Cai, Shan Jia, Vishnu Suresh Lokhande\u2217, Siwei Lyu\u2217 University at Buffalo, State University of New York, USA ", "page_idx": 0}, {"type": "image", "img_path": "cCL92OPlDz/tmp/2002902e95ec40c40e40b91b16b8168bfd8c99f8aea8da042751f1c7895b31ba.jpg", "img_caption": ["Figure 1: Multi-aspect text-driven image editing. Multiple edits in images pose a significant challenge in existing models (such as DirectInverison [1] and InfEdit [2]), as their performance downgrades with an increasing number of aspects. In contrast, our ParallelEdits can achieve precise multi-aspect image editing in 5 seconds. The symbol $\\otimes$ denotes a swap action, the symbol $\\bigoplus$ denotes an object addition action, and the symbol $\\ominus$ denotes an object deletion. Arrows $(\\rightarrow)$ on the image highlight the aspects edited by our method. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Text-driven image synthesis has made significant advancements with the development of diffusion models, transforming how visual content is generated from text prompts. Despite these advances, text-driven image editing, a key area in computer graphics, faces unique challenges. A major challenge is making simultaneous edits across multiple objects or attributes. Applying these methods sequentially for multi-aspect edits increases computational demands and efficiency losses. In this paper, we address these challenges with significant contributions. Our main contribution is the development of ParallelEdits, a method that seamlessly manages simultaneous edits across multiple attributes. In contrast to previous approaches, ParallelEdits not only preserves the quality of single attribute edits but also significantly improves the performance of multitasking edits. This is achieved through innovative attention distribution mechanism and multi-branch design that operates across several processing heads. Additionally, we introduce the PIE-Bench $^{++}$ dataset, an expansion of the original PIE-Bench dataset, to better support evaluating image-editing tasks involving multiple objects and attributes simultaneously. This dataset is a benchmark for evaluating text-driven image editing methods in multifaceted scenarios. Codes are available at: https://mingzhenhuang.github.io/projects/ParallelEdits.html. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Recently, text-driven image editing has experienced remarkable growth, driven by advances in diffusion-based image generative models. This technique involves modifying existing images based on textual prompts to alter objects, their attributes, and the relationships among various objects. The latest methods [3, 1, 4] can produce edited images that closely match the semantic content described in the prompts while keeping the rest of the image unchanged. Unlike early image editing approaches that required image matting to precisely extract foreground objects using alpha mattes [5], text-driven editing offers a less labor-intensive alternative. User-provided textual prompts guide the edits, with auxiliary inputs like masks facilitating localized modifications [6]. ", "page_idx": 1}, {"type": "text", "text": "While these methods have showcased promising results, existing methods typically focus on editing a single aspect in the source image. An \u201caspect\u201d refers to a specific attribute or entity within the textual prompt that describes the image and can be modified, such as object type, color, material, pose, or relationship. However, the ability to edit multiple aspects through text prompts is rarely explored. We introduce the concept of multi-aspect text-driven image editing to address this gap. Multi-aspect image editing is essential due to the rich and diverse content and structure of digital images, as well as the varied requirements of users. For example, it always occurs that users wish to modify multiple attributes or regions in an image, such as adding a necktie to a cat and changing the background wall to a beach (Fig. 1, Left), or removing a man and replacing a mountain with a castle in the right example. Unlike traditional editing methods (e.g., [1, 2]) that focus on a single aspect, multi-aspect editing allows users to manipulate various aspects simultaneously. Different from full text-to-image synthesis [7, 8], which involves creating content from scratch, multi-aspect editing works with the source image to ensure essential content preservation. It bridges the gap between single-aspect editing and full synthesis, catering to a wide range of editing scenarios. ", "page_idx": 1}, {"type": "text", "text": "However, we observe that directly applying the single-aspect text-driven image editing methods in cases where multiple image aspects must be modified often does not yield satisfactory results. A straightforward solution to this problem is to apply the single aspect editing method sequentially \u2013 we can order the aspects to be modified and use a single-aspect editing method to change the aspects one by one. Although sequential applications of single-aspect text-driven image editing methods can modify multiple aspects of an image, they may introduce significantly higher computational overhead. More importantly, the order of the aspects modified may affect the quality \u2013 changes to later aspects may undo the early ones or accumulate the errors and artifacts, thus reducing the effectiveness of the final editing results, as the last two rows of Fig. 5 and Table 1 show. ", "page_idx": 1}, {"type": "text", "text": "In this work, we introduce ParallelEdits as an efficient and effective solution to the problem of multi-aspect text-driven image editing. This method is based on a crucial insight that the editing step can occur in parallel with the image\u2019s diffusion steps. Therefore, in ParallelEdits, we build image aspect editing into the diffusion steps to accelerate the editing process. ParrallelEdits is based on an architecture with a fixed number of additional branches dedicated to handling rigid, non-rigid, and style changes. This design ensures scalability independent of the number of prompt aspects altered. In addition, we employ an attention aggregator to accurately assess editing difficulty and route aspects to appropriate branches within the ParallelEdits framework, ensuring precise and efficient editing. To enable subsequent research and evaluation of multi-aspect text-driven image editing methods, we also build the PIE-Bench $^{++}$ dataset, an extension of the PIE-Bench [1] that has 700 images with detailed text prompts and tailored to facilitate simultaneous edits across multiple image aspects. We propose evaluation metrics and benchmark different text-driven image editing methods on PIE-Bench++. The ParallelEdits outperforms the state-of-the-art image editing methods on PIE-Bench $^{++}$ . ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Diffusion Models for Text-Driven Image Editing. Text-driven image editing aims to manipulate local regions of an image based on textual prompts. The editing has two main goals: ensuring the edits align with provided instructions and preserving essential content. Diffusion models [9] have gained popularity as a preferred image editing model for their capacity for generating high-quality samples by incorporating diverse conditions, especially using text [10, 11, 2, 12, 13, 14, 1]. This involves transforming the images into the latent space and generating regions using diffusion models conditioned by the text prompt while ensuring accurate reconstruction of unmodified regions during editing. To avoid the edited image deviating from original image, early text-driven image editing typically requires user-specified masks as additional condition [15, 16, 17] or training [18, 19, 20] to guided the editing process, which constrain their potential zero-shot application. To address this limitation, recent editing models, such as InfEdit [2], PnP [21], Direct Inversion [1] follow the work Prompt-to-Prompt (P2P) [3], which proposed to obtain an attention map from the cross attention process and either swap or refine the attention map from text prompt for image editing. This design automatically obtains the editing mask and only allows image editing using a text prompt. Another method, MasaCtrl [4], converts existing self-attention in diffusion models into mutual self-attention for non-rigid consistent image synthesis and editing, enabling to query correlated local contents and textures from source images for consistency. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Multi-Aspect Image Editing. While current image editing models have shown promising results in their text-driven image editing benchmarks, we have observed that they work well on single-attribute editing while struggling to edit multiple aspects, especially when editing multiple objects (as shown in Fig. 1). We attribute this limitation to the following reasons. First, existing methods use the attention mask to direct where edits should be made. With multiple attributes, the editing area may expand significantly, incorporating extensive semantic information or scattered regions that are challenging to edit using a single mask. Second, employing a fixed mask from cross-attention maps struggles with edits involving changes in region size (such as pose adjustments), while using an adaptive mask faces challenges in maintaining edit fidelity. Therefore, integrating various attention masks for accurate multi-attribute editing presents a challenging technical problem. Early studies [22, 23] have employed GAN models such as StyleGAN2 [24] to edit multiple attributes in faces. The multiple-attribute editing is realized by training the GAN model with supervised multi-class training and a training dataset of image and attribute vector pairs. This solution heavily relies on the training sets and has limitations in generalizing to new editing types. Few recent works achieve multi-aspect editing with additional inputs: [25] leverages rich text to edit multiple objects and [26] pre-processes the image with grounding to localize multiple edited regions for multi-aspect editing. However, the editing performance highly relies on additional input beyond plain text, either from user input or other off-the-shelf models. A recent work [27] proposes an iterative multi-granular image editor, where a diffusion model can faithfully follow a series of image editing instructions from a user. However, this interactive editing pipeline will result in significant computational overhead. ", "page_idx": 2}, {"type": "text", "text": "Image Editing with Multiple Branches. In the literature [4, 3], image editing processes have been conducted by implementing a dual-branch approach. This methodology involves segregating source and target branches throughout the editing process. Specifically, the source branch is reverted to $z_{0}$ , while the trajectory of the target branch is iteratively adjusted. By computing the distance from the source branch, the calibration of the target branch occurs at each time-step. Our observation underscores the disparity between the effectiveness of a dual branch in enhancing the editing process and its failure in multi-aspect editing. A singular target branch proves inadequate in calibrating fully from the source branch, leading to imperfect incorporation of all aspects into the image. Hence, our primary proposition advocates for multi-aspect editing by utilizing multiple target branches. Each target branch\u2019s trajectory is meticulously calibrated, with simpler concepts addressed in the initial branches and more complex aspects deferred to subsequent ones. In the following section, we will delve deeper into this concept. ", "page_idx": 2}, {"type": "text", "text": "3 Diffusion-based Image Generation and Editing ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We are provided with an image sample $x_{0}$ which transforms the latent space via an encoder/decoder pair $\\mathcal{E}/\\mathcal{D}$ , such that $z_{0}\\,=\\,\\bar{\\mathcal{E}}(x_{0})$ . Here, $z_{0}$ represents the latent representation of the image $x_{0}$ With a slight abuse of notation, we approximate the reconstructed image $\\scriptstyle{\\bar{x}}_{0}$ as $\\mathcal{D}(\\bar{z}_{0})$ , where $\\bar{z}_{0}$ denotes the reconstructed version of $z_{0}$ . These operations are integral to the latent diffusion model [9]. The diffusion process constitutes two steps: the forward step incrementally adds zero-mean white Gaussian noise with time-varying variance to the latent vector $z$ according to discrete-time $t^{*}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\nz_{t}=\\sqrt{\\alpha_{t}}z_{0}+\\sqrt{1-\\alpha_{t}}\\epsilon\\quad\\mathrm{with}\\quad\\epsilon\\sim\\mathcal{N}(0,I),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "$\\alpha_{1:T}$ represents a variance schedule for $t$ drawn from the interval $[1,T]$ . The variance schedule can be different, such as linear or cosine quadratic [28]. The backward step is an iterative process to remove ", "page_idx": 2}, {"type": "text", "text": "\\*Diffusion process is rigorously defined as a continuous-time stochastic differential equation, but in practice often implemented with discrete-time updates. ", "page_idx": 2}, {"type": "text", "text": "the noise from the data progressively. Using the same variance schedule $\\alpha_{1:T}$ as in the forward \u221astep, a noise schedule $\\sigma_{1:T}$ and a parameterized noise prediction network $\\epsilon_{\\theta}$ with coefficients $c_{\\mathrm{pred}}=\\sqrt{\\alpha_{t-1}}$ , $c_{\\mathrm{dir}}=\\sqrt{1-\\alpha_{t-1}-\\sigma_{t}^{2}}$ , and $c_{\\mathrm{noise}}=\\sigma_{t}$ , the backward step corresponds to the following process: ", "page_idx": 3}, {"type": "equation", "text": "$$\nz_{t-1}=\\underbrace{c_{\\mathrm{pred}}f_{\\theta}(z_{t},t)}_{\\mathrm{predicting}~\\bar{z}_{0}}+\\underbrace{c_{\\mathrm{dir}}\\epsilon_{\\theta}(z_{t},t)}_{\\mathrm{adjust\\;along}~z_{t}}+\\underbrace{c_{\\mathrm{noise}}\\epsilon_{t}}_{\\mathrm{random\\;noise}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The noise schedule $\\sigma_{1:T}$ comprises hyperparameters requiring careful selection based on factors like image dimensions or desired performance [29][30]. In the framework of Denoising Diffusion Implicit Models (DDIM) [31], the function $f_{\\theta}$ is employed for the predictio\u221an and reconstruction of $\\bar{z}_{0}$ , based on the input $z_{t}$ . Specifically, we have $\\begin{array}{r}{\\bar{z}_{0}=f_{\\theta}(z_{t},t)=\\frac{1}{\\sqrt{\\alpha_{t}}}z_{t}-\\frac{\\sqrt{1-\\alpha_{t}}}{\\sqrt{\\alpha_{t}}}\\epsilon_{\\theta}(z_{t},t)}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Consistency Models for Inversion-free Image Editing. Consistency models [32, 33] have been introduced to expedite the generation process through a consistent distillation approach. These models exhibit a self-consistency property, ensuring that samples along the same trajectory map to the same initial point. Specifically, the function $f_{\\theta}$ is rendered self-consistent by satisfying $f_{\\theta}(z_{t},t)=z_{0}$ for a given sample $z_{t}$ at timestep $t$ . As a result, the self-consistency property yields a closed-form solution for the noise predictor \u03f5\u03b8. We denote this particular \u03f5\u03b8 as \u03f5cons, which is derived as \u03f5cons = zt\u221a\u22121\u2212\u03b1\u03b1tz 0. Since $\\epsilon^{\\mathrm{{cons}}}$ is not parameterized and contains the ground-truth $z_{0}$ , Xu et al. [2] propose starting directly with random noise, i.e., $z_{T}\\sim\\mathcal{N}(0,\\mathbf{I})$ , at the last time-step $T$ , which is particularly advantageous for image-editing tasks as it eliminates the need for inversion from $z_{0}$ to $z_{T}$ . Therefore, starting with $z_{\\tau}=z_{T}\\sim\\bar{\\mathcal{N}}(0,\\mathbf{I})$ , the sampling process proceeds as follows: ", "page_idx": 3}, {"type": "text", "text": "After many iterations, the final output is $z$ . Furthermore, [2] demonstrates that the dual-branch paradigm (involving a source and a target branch) used in image editing tasks can be executed in an inversion-free manner. We will delve into this, along with our method description, in Section 4.2.2. ", "page_idx": 3}, {"type": "text", "text": "4 Multi-Aspect Image Editing ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Problem Definition ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The input to the multi-aspect image editing task includes a source image $\\mathcal{Z}_{s r c})$ , the source prompt, and a set of edits to be applied to the source image, indicating the changes from the source prompt to target prompt. A text prompt (whether source or target) comprises several independent tokens, of which only a subset is editable. We refer to these editable tokens as Aspects. ", "page_idx": 3}, {"type": "text", "text": "Definition 4.1 (Aspect). We define an $i^{\\mathrm{th}}$ aspect $\\mathcal{A}_{s r c}^{i}$ in the source prompt (or the $j^{\\mathrm{th}}$ aspect $\\mathcal{A}_{e d t}^{j}$ in the target prompt) as any entity that can be substituted, deleted, or inserted into the text prompt, resulting in a meaningful sentence structure. ", "page_idx": 3}, {"type": "text", "text": "Several examples of tokens corresponding to aspects or not are given in Fig. 3. In other words, aspects correspond to single or multiple tokens representing object color, pose, material, content, background, image style, etc. An editing operation $E^{i\\rightarrow j}$ between the editing pair $(\\mathcal{A}_{s r c}^{i},\\mathcal{A}_{e d t}^{j})$ as $E^{i\\to j}\\in\\{\\otimes,\\oplus,\\ominus,\\oslash\\}$ . Here, $\\otimes$ denotes a swap action, $\\bigoplus$ denotes an object addition action, $\\ominus$ denotes object deletion, and $\\oslash$ indicates no change in the aspect. Such an editing operation can be inferred directly by appropriately mapping the source and target prompts, or it can be provided as metadata [3, 34]. The editing task is considered successful if the edited source image, $\\mathcal{Z}_{e d t}$ , reflects the required edits while preserving the unaffected aspects of the original image. ", "page_idx": 3}, {"type": "text", "text": "4.2 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Figure 2 outlines the overall pipeline of our method, which has three steps. In the first step (Sec. 4.2.1), we perform aspect grouping using attention maps generated by running a few iterations of the diffusion process. The aspects in the source image are put into up to $N$ groups, each processed by a distinct branch. The second step (Sec. 4.2.2) demonstrates how each branch, which receives a specific group of aspects, performs inversion-free editing. In the last step (Sec. 4.2.3), we perform the necessary adjustments for enabling cross-branch interaction and elucidate the significance of such interaction. ", "page_idx": 3}, {"type": "image", "img_path": "cCL92OPlDz/tmp/a695b43d9d17686b0fa4ec9332c70c3cfa7cc8beefcb80c48b995bb1a603fdae.jpg", "img_caption": ["Figure 2: Pipeline. Our method, ParallelEdits, takes a source image, source prompt, and target prompt as input and produces an edited image. The target prompt specifies the edits needed in the source image. Attention maps for all edited aspects are first collected. Aspect Grouping (see Section 4.2.1) categorizes each aspect into one of $N$ groups (in the above figure, $N=5$ ). Each group is then assigned a branch and the branch-level updates are detailed in Section 4.2.2. Each branch can be viewed either as a rigid editing branch, non-rigid editing branch, or global editing branch. Finally, adjustments to query/key/value at the self-attention and cross-attention layers are made, as illustrated in the figure and described in Section 4.2.3. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4.2.1 Aspect Grouping ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We would like to group aspects in a prompt into $N$ distinct groups using the cross-attention maps of the diffusion UNet [35] to characterize the spatial layouts as in previous studies [36]. Given an editing operation $E^{i\\rightarrow j}$ between the source aspect $\\mathcal{A}_{s r c}^{i}$ and the target aspect $\\mathcal{A}_{e d t}^{j}$ s,  frwoem  obbottaih nt hteh es ocuorrcree sapnodn tdairngge t apttreonmtipotns as $\\bar{\\mathcal{M}}_{s r c}^{i}$ and $\\bar{\\mathcal{M}}_{e d t}^{j}$ , respectively. The attention map $\\mathcal{M}$ is defined by the query feature $\\hat{Q}$ and key feature $\\hat{K}$ from the cross-attention as ${\\mathcal M}\\,=\\,\\mathrm{softmax}\\left(\\frac{\\hat{Q}\\hat{K}^{T}}{\\sqrt{d}}\\right)$ The binarized attention map $\\bar{\\mathcal{M}}$ is obtained by normalizing $\\mathcal{M}$ and thresholding its values. Our aspect grouping proceeds in two steps, ", "page_idx": 4}, {"type": "text", "text": "Step 1. Assign a type for every editing operation $(\\mathbf{E^{i\\rightarrowj}})$ . We consider three possible types of edits, in line with previous works [4], namely a global edit, a local rigid edit or a local non-rigid edit. Rigid local edits, such as changing an object\u2019s color or texture, do not alter the layout of objects. Conversely, non-rigid local edits modify the layout of objects, such as adding or deleting objects or changing object poses. Global edits affect background and style changes. The type assignment for the editing operation $\\left(E^{i\\rightarrow{\\bar{j}}}\\right)$ is determined by the followin ", "page_idx": 4}, {"type": "image", "img_path": "cCL92OPlDz/tmp/f83cad0c76a9ca9ccd0d2d949c25f2ed7c9cb355683f26bddeb525d9a7e5b0de.jpg", "img_caption": ["Figure 3: Aspects and Aspect Grouping. In a text prompt, there are multiple independent tokens, with only some being editable, known as aspects and are underlined in the above example. These aspects can be added, deleted, or swapped between the source and target prompts. Pairs of source and target aspects are grouped into branches, and the methodology for aspect grouping is explained in Section 4.2.1. "], "img_footnote": [], "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{type}(E^{i\\rightarrow j})=\\left\\{\\!\\!\\!\\begin{array}{l l}{\\mathrm{global~edit}}&{...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................... \n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $\\phi$ represent mIoU [37], while $\\gamma$ indicates the alpha mattes of attention maps. $\\lambda$ and $\\beta$ are tunable hyperparameters. For further details, please refer to the supplementary Sec. D. ", "page_idx": 4}, {"type": "text", "text": "Step 2. Categorize every editing operation $(\\mathbf{E^{i\\rightarrowj}})$ into $\\mathbf{N}$ groups. For each editing operation $(E^{i\\to j})$ of a specific type, we assess whether $\\phi(\\bar{\\mathcal{M}}_{e d t}^{j},\\bar{\\mathcal{M}}_{e d t}^{k})\\;\\ge\\;\\lambda$ to determine if there exists substantial overlap between any pair of attention maps of that type. If significant overlap is detected, the attention maps are grouped together. On the other hand, if attention maps are isolated like the \"boat\" and \"mountain\" in Fig. 3 are categorized into separate groups due to small overall. Therefore, we have a total of $N$ groups. Each group has a dedicated branch, resulting in a total of $N\\,>\\,2$ branches. ", "page_idx": 5}, {"type": "text", "text": "4.2.2 Inversion-Free Multi-Branch Editing ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We use a set of $N$ branches indexed by $n$ . The source branch undergoes a DDCM sampling process [2], but the $n^{\\mathrm{th}}$ target branch is calibrated to its $(n-1)^{\\mathrm{th}}$ target branch. The $N-$ way target branch calibration can occur simultaneously, saving significant compute time. For the DDCM sampling process of the $n^{\\mathrm{th}}$ branch, it has the form of Section 3, Step $\\bar{\\textcircled{1}}$ : ", "page_idx": 5}, {"type": "image", "img_path": "cCL92OPlDz/tmp/2a689237330f5a26374eba5b482b752a7a6f124213d43edf88dff6a3636fcc93.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Let us break down Eq. 4 step by step. $n=1$ representing the source branch, we have $z(1)^{\\mathrm{edt}}=z^{\\mathrm{src}}$ and $\\epsilon(1)_{\\tau}^{\\mathrm{edt}}=\\epsilon_{\\tau}^{\\mathrm{src}}$ . Also, $z(1)_{\\tau}^{\\mathrm{edt}}\\,=\\,z_{\\tau}^{\\mathrm{src}}$ , which at time step $\\tau=\\tau_{1}$ , is random noise drawn from $\\mathcal{N}(0,\\mathbf{I})$ . Similarly, when $n=N$ , $z(N)^{\\mathrm{edt}}$ represents the final calibrated/edited image containing all the required aspect edits after repeating for $\\tau\\in\\{\\tau_{1},\\tau_{2},\\dots\\tau_{T}\\}$ timesteps. The noise addition on any target branch remains the same as Step $\\textcircled{2}$ , i.e., $z(n)_{\\tau}^{\\mathrm{edt}}=\\sqrt{\\alpha_{\\tau}}z(n)^{\\mathrm{edt}}+\\sqrt{1-\\alpha_{\\tau}}\\epsilon$ where $\\epsilon\\sim\\mathcal{N}(0,\\bf{I})$ . For $1<n<N$ , we have $\\epsilon(n)_{\\tau}^{\\mathrm{edt}}=\\epsilon_{\\theta}(z(n)_{\\tau}^{\\mathrm{edt}},\\tau)$ , where $\\epsilon_{\\theta}$ represents a parameterized noise predictor network (details in the Appendix Sec. D). A key observation is that the difference in the parameterized noise at the $n^{\\mathrm{th}}$ branch and $(n-1)^{\\mathrm{th}}$ branch is utilized to calculate $z(n)^{\\mathrm{edt}}$ in (4). Finally, $\\epsilon(n)_{\\tau}^{\\mathrm{cons}}$ is defined by $\\epsilon(n)_{\\tau}^{\\mathrm{cons}}=\\big(z(n)_{\\tau}^{\\mathrm{edt}}-\\sqrt{\\alpha_{\\tau}}\\hat{z}(n-1)^{\\mathrm{edt}}\\big)/\\sqrt{1-\\alpha_{\\tau}}$ . Unlike the dual-branch setup in [2], the reference initial input is the estimated latent from the previous branch at a previous diffusion denoising iteration as indicated by $\\hat{z}(n-1)^{\\mathrm{edt}}$ . ", "page_idx": 5}, {"type": "text", "text": "4.2.3 Cross-Branch Interactions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For rigid local branches, the cross-attention map $\\mathcal{M}_{n}^{i}$ from the previous branch is either switched or injected into the current branch, akin to the method used in P2P [3]. This approach facilitates local edits while preserving structural consistency. For non-rigid local branches, we observe that the query features in the shallow layers of UNet [35] can effectively query correlated local contents and textures from the prior branch\u2019s latent features, ensuring consistency. Consequently, the key and value features from the prior branch are retained in the current branch to maintain consistent editing. We use a non-rigid editing branch to manage non-rigid local edits. In the current branch $n$ , textures from the previous branch $(n-1)$ are preserved by replacing the $K_{n-1}$ and $V_{n-1}$ features from the last branch with the $K_{n}$ and $V_{n}$ features in the current branch. Only the query features are preserved to maintain layout semantic correspondence. Additionally, the attention mask $\\mathcal{M}_{n-1}$ from the previous branch\u2019s cross-attention layer is used to guide the editing process by adding it to $\\mathcal{M}_{n}$ , thereby converting the object layout from $\\mathcal{M}_{n-1}$ to ${\\mathcal{M}}_{n}$ . This step is crucial for object removal or shape modification edits, where the object mask is derived from the previous branch. For all global branches, there is no replacement of attention features or masks, and the attention mask is not used to guide the editing process, as the entire image is intended to be altered. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "PIE-Bench $^{++}$ Dataset. We introduce a new dataset, PIE-Bench $^{++}$ , derived from PIE-Bench [1] and dedicated to evaluate the performance of multi-aspect image editing. The PIE-Bench dataset contains 700 images and prompts with single-aspect editing including object-level manipulations (addition, deletion, or alteration), attribute-level manipulations (changes in content, pose, color, and material), and image-level manipulations that modify background and overall style. Our PIE-Bench $^{++}$ extends ", "page_idx": 5}, {"type": "image", "img_path": "cCL92OPlDz/tmp/a80ee4145543e9fd27b21e8cff531f7b344fe39f220afda736eb2a100c5d900a.jpg", "img_caption": ["Figure 4: Qualitative results of ParallelEdits. We denote the edits in arrows with edit actions and aspects for each pair of images. The last image pair is a failure case of ParallelEdits. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "PIE-Bench by enabling multi-aspect edits: $57\\%$ of our dataset have two aspect edits per prompt, $19\\%$ have more than two edits, and the remaining $24\\%$ have a signle aspect edit. For additional details and examples of the PIE-Bench $^{++}$ dataset, please refer to the supplementary material. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics. We introduce two new metrics designed for evaluating multi-aspect text-driven image editing, alongside standard evaluation metrics. ", "page_idx": 6}, {"type": "text", "text": "(a) Aspect Accuracy-LLaVA. Drawing inspiration from the remarkable capability of large vision language models in comprehending intricate semantics within images, we propose to innovatively leverage them as an \u201comniscient\u201d agent equipped with extensive knowledge to understand various attributes of images. We use the LLaVA [38] model, trained on visual grounding tasks, to evaluate the accuracy of multi-aspect image editing. Given a text prompt with multiple aspects, such as \u201cA [pink] [taxi] with [colorful] [flowers] on top\u201d, we provide the following prompt with the edited image to the LLaVA model: \u201cDoes the image match the elements in [ ]: A [pink] [taxi] with [colorful] [flowers] on top? Return a list of numbers where 1 is matched and $O$ is unmatched.\u201d We then parse the returned list and compute its average to determine the aspect accuracy. We name this new evaluation metric as AspAcc-LLaVA. Examples and detailed explanations of this evaluation metric are available in the supplementary material. ", "page_idx": 6}, {"type": "text", "text": "(b) Aspect Accuracy-CLIP. We also use the similarity of the CLIP [39] to evaluate if an attribute has been successfully edited. Given an edited image $\\mathcal{T}_{e d t}$ and the target prompt $\\mathcal{P}_{e d t}$ with $k$ edited aspects $A_{e d t}$ , every time we remove an aspect $\\mathcal{A}_{e d t}^{j}$ from $\\mathcal{P}_{e d t}$ and revert it back to $\\mathcal{A}_{s r c}^{i}$ as $\\hat{\\mathcal{P}}_{e d t}$ . We then extract the CLIP [39] similarity between the edited image $I_{e d t}$ and two prompts, i.e., $s_{1}=C L I P(\\mathcal{T}_{e d t},\\mathcal{P}_{e d t})$ and $s_{2}=C L I P(\\mathcal{T}_{e d t},\\hat{\\mathcal{P}}_{e d t})$ . We expect $s_{1}>s_{2}$ if the aspect $\\mathcal{A}_{e d t}^{j}$ has been successfully edited. Thus, the aspect accuracy is $\\frac{k_{s}}{k}$ when a total of $k_{s}$ aspects have been successfully edited among target edits. Note that in the case of an edited or added object that also involves changes in attributes (such as color or material), we consider it a successful edit only if both the object and its attributes have been successfully modified. We name this metric as AspAcc-CLIP. ", "page_idx": 6}, {"type": "text", "text": "(c) Standard Metrics. Several standard metrics widely used for evaluating text-image similarity and image quality are considered, including PSNR, LPIPS [40], MSE, and SSIM [41]. We also use the CLIP [39] score to measure the image-text alignment performance. Additionally, the bi-directional CLIP (D-CLIP) score [42] is reported, which is formulated as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{cos}\\langle\\mathrm{CLIP}_{\\mathrm{img}}\\left(\\mathbb{Z}_{e d t}\\right)-\\mathrm{CLIP}_{\\mathrm{img}}\\left(\\mathbb{Z}_{s r c}\\right),\\mathrm{CLIP}_{\\mathrm{text}}\\left(\\mathcal{P}_{e d t}\\right)-\\mathrm{CLIP}_{\\mathrm{text}}(\\mathcal{P}_{s r c})\\rangle\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "5.1 Quantitative Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first conduct experiment on the PIE-Bench $^{++}$ dataset to compare our method with the state-of-theart text-driven image editing methods combining their corresponding inversion method leads to best performance, including DDIM $\\pm$ MasaCtrl [4], DDIM $^{+}$ Prompt-to-Prompt (P2P) [3], DDIM $^+$ Plug-andPlay (PnP) [21], StyleDiffusion (StyleD) [43]+P2P, Null-text Inversion (NTI) $[34]\\!+\\!\\mathrm{P2P}$ , DirectInverison $(\\mathrm{DI})[1]{+}\\mathrm{PnP}_{:}$ , and InfEdit [2]. An intuitive way to improve off-the-shelf image editing methods is to apply the single-aspect editing method sequentially. We follow [27] to adapt existing image editing methods into sequential editing processes, where these methods are applied multiple times to achieve multi-aspect editing. Each time, only one aspect is edited. Table 1 presents the metrics in terms of text-image similarity (i.e., CILP and D-CLIP scores), computational efficiency, and aspect accuracy. Our ParallelEdits model outperforms all baselines in editing effectiveness, with a slightly longer runtime than the InfEdit model. Even though sequential editing better aligns the target prompt than their vanilla methods, it significantly increases computational overhead and may propagate editing errors over time. Moreover, although the sequential editing is conducted in the latent space, it would introduce more noise and artifacts to the edited image. Hence, their performance in all editing quality metrics was inferior to our method. ", "page_idx": 6}, {"type": "image", "img_path": "cCL92OPlDz/tmp/8162f7b3c98099bd40a164905d12c253f5929cebcde68a1627bc8eac07accfd4.jpg", "img_caption": ["Figure 5: Qualitative results comparison. Current methods fail to edit multiple aspects effectively, even using sequential edits (noted as \\*). Methods marked with $\\star\\star$ taking additional inputs other than source image and plain text. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "cCL92OPlDz/tmp/4b09dbb2056fe09c9685475227aa32b1019e73cacf364e12cac36394bd2eb980.jpg", "table_caption": ["Table 1: Comparison results in multi-aspect image editing on the PIE-Bench $^{++}$ dataset. Computational efficiency is abbreviated as Eff., and \\* denotes the method using sequential editing. The best performance is highlighted in bold and the second best performance is underlined. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.2 Qualitative Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Fig. 4 presents several examples of our method\u2019s multi-aspect editing on the PIE-Bench $^{++}$ dataset. The results demonstrate the effectiveness of our method in handling multiple and varied types of edits across diverse image content. Fig. 5 further compares our method with several state-of-the-art models and one popular multi-modal large language model, GPT-4V [44], by providing the source image, source prompt, and target prompt to guide the image editing. The Rich-text [25] model differs from other models, which uses rich-text prompt to edit the image generated from the plain (source) text prompt. The results show that current image editing models even with sequential editing fail to edit multiple aspects, while multi-modal large language models fail to preserve the content of source image. Our method achieves visually convincing results by successfully editing different attributes with good content preservation. ", "page_idx": 7}, {"type": "image", "img_path": "cCL92OPlDz/tmp/349f1f1c5661026fff3a72c8cd2e6d150ae390a1d13949a23be88c11304f5428.jpg", "img_caption": ["Figure 6: Comparison across different numbers of editing aspects. We also include the comparison in PIE-Bench dataset. Our proposed method is robust to different numbers of editing aspects. ", "Table 2: Comparison results in terms of background and aspects preservation. The results from sequential editing is noted as green. ParallelEdits achieves state-of-the-art performance on multi-aspect editing while preserving the background and content consistency. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "cCL92OPlDz/tmp/7373b075e8755524cb8fcb7babd6bb335850317eceabf0b7715d909365592123.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.3 Ablation Study and Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "(a) Impact of Editing Aspect Number. We first examine the performance of our ParallelEdits and baseline methods on various editing aspect numbers by comparing CLIP and LLaVA-based aspect accuracies on the original PIE-Bench [1] and our PIE-Bench $^{++}$ datasets. The bar charts in Fig. 6 show the outstanding performance of our method across all settings, including single-aspect editing on two datasets and multi-aspect editing. Takeaway: the proposed ParallelEdits demonstrates robustness across varying numbers of editing aspects. ", "page_idx": 8}, {"type": "text", "text": "(b) Evaluation on Perservation. We follow [1] to evaluate the background preservation. We first use the PSNR, LPIPS [40], MSE and SSIM [41] to evaluate the background preservation. We measure that metric on a subset of images of our proposed PIE-Bench $^{++}$ dataset where the background can be well defined in that image, e.g., no image style or background editing, and the background is visible after aspect editing. The results are shown in Table 2, where we compare our method with the top performance methods in Table 1. Moreover, we adopt the similar way as calculating the AspAcc-LLaVA to prompt LLaVA [38] for evaluating how the unchanged aspect preserves in the edited image. We also calculate the CLIP [39] score between the target image and the text prompt after removing all edited aspects. The results are reported in Table 2 noted as CLIP and LLaVA, respectively. Takeaway: preservation is even maintained in ParallelEdits. ", "page_idx": 8}, {"type": "text", "text": "(c) Branches numbers and aspect grouping. To demonstrate the effectiveness of our multi-branch design and early aspect grouping, we design additional ablation studies for our method in threefold. (1) We only use one single non-rigid branch to conduct all edits; (2) we remove the aspect categorization process from the pipeline and use the same non-rigid branch for each edit; (3) we adopt one single branch for different type of edits without using any auxillary branches which results a total of three branches (also see Section B for more details). Takeaway: As shown in Table 3, the multi-branch design and aspect grouping play a significant role in enhancing the performance of our proposed ParallelEdits. ", "page_idx": 8}, {"type": "table", "img_path": "cCL92OPlDz/tmp/7679ec10a621dbe72cf8c5ed1c8b48b4b81354f9a88ef4540355b0cd40deb328.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "cCL92OPlDz/tmp/2b48b5ffa765ffc6f28b7de14c35186d1c461db9f3e655832b7261e91b77dbce.jpg", "table_caption": ["Table 3: Ablation studies on branch numbers and aspect grouping. "], "table_footnote": ["Table 4: Comparison on each category in PIE-Bench $++.$ . Our ParallelEdits achieves the best performance on most of the categories from the dataset. "], "page_idx": 9}, {"type": "text", "text": "(d) Performance comparison on each category. Recall that our dataset includes nine different categories for editing. We compare the performance of baseline models and our approach across the nine categories, as presented in Table 4. Takeaway: Our proposed ParallelEdits achieves state-of-theart performance across most categories. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Failure Cases. The proposed ParallelEdits has several limitations. First, it cannot handle the text editing in the image, as shown in the last image pair of Fig. 4. Second, ParallelEdits fails to edit dramatic background changes, as examples shown in the supplementary material. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose a new research task, multi-aspect text-driven image editing, to modify multiple object types, attributes, and relationships. We introduce a dedicated method, ParallelEdits, to multi-aspect text-driven image editing as an effective and efficient solution to this problem. Due to the lack of evaluation benchmark, we introduce PIE-Bench $^{++}$ , an improved version of PIE-Bench [1] tailored for simultaneous multiple-aspect edits within images. ParallelEdits achieves better quality and performance than existing methods on proposed PIE-Bench++. Our work introduces ParallelEdits, a novel approach that adeptly handles multiple attribute edits simultaneously, preserving the quality of edits across single and multiple attributes through a unique attention grouping mechanism without adding computational complexity. There are several future works we would like to explore. First, different aspects of an image have a specific semantic order. Editing these aspects according to their intrinsic order will simplify the editing process. Secondly, the current ParallelEdits still has limitations, as shown in Fig. 4. It will be of interest to study approaches to improve these aspects. ", "page_idx": 9}, {"type": "text", "text": "Ethics Statement. In anticipation of contributing to the academic community, we plan to make the dataset and associated code publicly available for research. Nonetheless, we acknowledge the potential for misuse, particularly by those aiming to generate misinformation using our methodology. We will release our code under an open-source license with explicit stipulations to mitigate this risk. These conditions will prohibit the distribution of harmful, offensive, or dehumanizing content or negatively representing individuals, their environments, cultures, religions, and so forth through the use of our model weights. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement. This work was supported in part by the National Science Foundation (NSF) Projects under grants SaTC-2153112, No.1822190, and TIP-2137871. Prof. Lokhande thanks support provided by University at Buffalo Startup funds. We thank Sudhir Kumar Yarram for the insightful discussions on the project. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Direct inversion: Boosting diffusion-based editing with 3 lines of code. 2023.   \n[2] Sihan Xu, Yidong Huang, Jiayi Pan, Ziqiao Ma, and Joyce Chai. Inversion-free image editing with natural language. arXiv preprint arXiv:2312.04965, 2023.   \n[3] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. 2022.   \n[4] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 22560\u201322570, October 2023.   \n[5] Jizhizi Li, Jing Zhang, and Dacheng Tao. Referring image matting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 22448\u201322457, 2023.   \n[6] Yuhao Liu, Jiake Xie, Xiao Shi, Yu Qiao, Yujie Huang, Yong Tang, and Xin Yang. Tripartite information mining and integration for image matting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7555\u20137564, 2021.   \n[7] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10696\u201310706, 2022.   \n[8] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-toimage generation via hierarchical transformers. Advances in Neural Information Processing Systems, 35:16890\u201316902, 2022.   \n[9] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.   \n[10] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusionbased semantic image editing with mask guidance. In The Eleventh International Conference on Learning Representations, 2023.   \n[11] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In ACM SIGGRAPH 2023 Conference Proceedings, pages 1\u201311, 2023.   \n[12] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6007\u20136017, 2023.   \n[13] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusionbased semantic image editing with mask guidance. arXiv preprint arXiv:2210.11427, 2022.   \n[14] Thao Nguyen, Yuheng Li, Utkarsh Ojha, and Yong Jae Lee. Visual instruction inversion: Image editing via image prompting. Advances in Neural Information Processing Systems, 36, 2023.   \n[15] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11461\u201311471, 2022.   \n[16] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18208\u201318218, 2022.   \n[17] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.   \n[18] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning method for denoising diffusion probabilistic models. arXiv preprint arXiv:2108.02938, 2021.   \n[19] Sihan Xu, Ziqiao Ma, Yidong Huang, Honglak Lee, and Joyce Chai. Cyclenet: Rethinking cycle consistency in text-guided diffusion for image manipulation. Advances in Neural Information Processing Systems, 36, 2024.   \n[20] Min Zhao, Fan Bao, Chongxuan Li, and Jun Zhu. Egsde: Unpaired image-to-image translation via energy-guided stochastic differential equations. Advances in Neural Information Processing Systems, 35:3609\u20133623, 2022.   \n[21] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1921\u20131930, 2023.   \n[22] Hao Wang, Guosheng Lin, Ana Garc\u00eda del Molino, Anran Wang, Zehuan Yuan, Chunyan Miao, and Jiashi Feng. Maniclip: Multi-attribute face manipulation from text. arXiv preprint arXiv:2210.00445, 2022.   \n[23] Siavash Khodadadeh, Shabnam Ghadar, Saeid Motiian, Wei-An Lin, Ladislau B\u00f6l\u00f6ni, and Ratheesh Kalarot. Latent to latent: A learned mapper for identity preserving editing of multiple face attributes in stylegan-generated images. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 3184\u20133192, 2022.   \n[24] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8110\u20138119, 2020.   \n[25] Songwei Ge, Taesung Park, Jun-Yan Zhu, and Jia-Bin Huang. Expressive text-to-image generation with rich text. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7545\u20137556, 2023.   \n[26] Hangeol Chang, Jinho Chang, and Jong Chul Ye. Ground-a-score: Scaling up the score distillation for multi-attribute editing. arXiv preprint arXiv:2403.13551, 2024.   \n[27] KJ Joseph, Prateksha Udhayanan, Tripti Shukla, Aishwarya Agarwal, Srikrishna Karanam, Koustava Goswami, and Balaji Vasan Srinivasan. Iterative multi-granular image editing using diffusion models. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 8107\u20138116, 2024.   \n[28] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pages 8162\u20138171. PMLR, 2021.   \n[29] Ting Chen. On the importance of noise scheduling for diffusion models. arXiv preprint arXiv:2301.10972, 2023.   \n[30] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35:26565\u201326577, 2022.   \n[31] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \n[32] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023.   \n[33] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023.   \n[34] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6038\u20136047, 2023.   \n[35] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234\u2013241. Springer, 2015.   \n[36] Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang, Gefei Yang, Karun Kumar, Pontus Stenetorp, Jimmy Lin, and Ferhan Ture. What the daam: Interpreting stable diffusion using cross attention. arXiv preprint arXiv:2210.04885, 2022.   \n[37] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014.   \n[38] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023.   \n[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[40] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586\u2013595, 2018.   \n[41] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600\u2013 612, 2004.   \n[42] Chen Henry Wu and Fernando De la Torre. Unifying diffusion models\u2019 latent space, with applications to cyclediffusion and guidance. arXiv preprint arXiv:2210.05559, 2022.   \n[43] Zhizhong Wang, Lei Zhao, and Wei Xing. Stylediffusion: Controllable disentangled style transfer via diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7677\u20137689, 2023.   \n[44] OpenAI. Gpt-4v(ision) system card. 2023.   \n[45] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. arXiv preprint arXiv:2211.09794, 2022.   \n[46] Narek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Splicing vit features for semantic appearance transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10748\u201310757, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A ParallelEdits: The Algorithm ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section we provide Algorithm 1: Early Aspect Grouping and Algorithm 2: ParallelEdits on a particular branch. These algorithms describe the overall idea behind ParallelEdits. They are also pictorially illustrated in Figures 2 and 3 of the main paper. Let us denote an arbitrary branch and the timestep in the diffusion process by $n$ and $t$ respectively. Firstly, in Algorithm 1, we demonstrate how Early Aspect Grouping is conducted over the attention maps. Recall that we refer to this as \u201cearly\" aspect grouping because only a few steps (maximum of 5) are sufficient to perform the grouping. This phase of ParallelEdits takes as an input, the edit action set $\\{E^{i\\to j}\\}$ and the corresponding cross-attention maps for every token $\\mathbf{A}_{s r c}^{j}$ , and outputs the grouped edit actions set $\\bar{A}_{e d t}^{c}$ . Recall from Section 4 of the paper that $E^{i\\to j}\\in\\{\\otimes,\\oplus,\\ominus,\\oslash\\}$ , with $\\otimes$ denoting a swap action, $\\bigoplus$ denoting an add action, $\\ominus$ denoting aspect deletion, and $\\oslash$ indicating no change in the aspect. Once grouped edit actions set is computed, it is fed into Algorithm 1 to conduct multi-aspect editing and obtain the edited latent features. In Algorithm 2, we implement several operations on the attention masks, similar to the P2P method [3], and describe them as follows. ", "page_idx": 13}, {"type": "text", "text": "Replace: Swapping token attention mask $\\mathcal{M}_{n-1}$ in the prompt from previous branch, overriding $\\mathcal{M}_{n}$ ; ", "page_idx": 13}, {"type": "text", "text": "Refine: Injecting only the attention mask that corresponds to the unchanged part of the prompt from $\\mathcal{M}_{n-1}$ to $\\mathcal{M}_{n}$ ; ", "page_idx": 13}, {"type": "text", "text": "Retain: Keeping the attention mask $\\mathcal{M}_{n}$ unchanged. ", "page_idx": 13}, {"type": "table", "img_path": "cCL92OPlDz/tmp/5914f3332c48ae32a8fe75f35324e5be63cf7d83af80b4d6d6d0869db449c8c3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Some More Details on ParallelEdits ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In the literature [4, 3], image editing processes have been conducted through the implementation of a dual-branch approach. This method involves utilizing a source and target branches for editing. ", "page_idx": 13}, {"type": "text", "text": "Input: Denoising UNet $\\varepsilon_{\\theta}$ , Grouped edit action $\\bar{A}_{e d t}^{c}$ , $\\triangleright$ Output from early aspect grouping Latent feature in previous branch and previous timestep ztn\u22121, ztn\u22121, Cross attention maps $\\{\\mathcal{M}\\}$ , Self attention features $Q_{n-1},K_{n-1},V_{n-1}$ , Edit type list: rigid-edit, non-rigid-edit, global-edit   \n1: $\\bar{\\mathcal{M}_{n}}\\overleftarrow{\\varepsilon_{\\theta}}(\\bar{\\mathcal{A}}_{e d t}^{c},\\tilde{z}_{n}^{t-1},t-1)$   \n2: if $\\bar{A}_{e d t}^{c}\\in$ global-edit then $\\triangleright$ This is a global edit   \n3: retain $\\left(\\mathcal{M}_{n}\\right)$ $\\triangleright$ Do not switch attention maps for global edits   \n4: else if $\\bar{A}_{e d t}^{c}\\in$ non-rigid-edit then $\\triangleright$ This is a non-rigid edit   \n5: replace $(\\mathcal{M}_{n-1},\\mathcal{M}_{n}$ )   \n6: else if $\\bar{A}_{e d t}^{c}\\in$ rigid-edit then \u25b7This is a rigid edit   \n7: $\\{Q_{n},\\overleftarrow{K}_{n},V_{n}\\}\\leftarrow\\{Q_{n},K_{n-1},V_{n-1}\\}$   \n8: refine $(\\mathcal{M}_{n-1},\\mathcal{M}_{n})$ )   \n11901::: ${\\bar{\\mathcal{M}}}_{n}\\gets{\\tt b i n a r i z e}(\\sum_{m=0}^{m\\leq n}{\\mathcal{M}}_{m})$ $z_{n}^{t}\\leftarrow\\bar{\\mathcal{M}}_{n}\\odot z_{n}^{t}+\\left(\\bar{1}-\\bar{\\mathcal{M}}_{n}\\right)\\odot z_{n-1}^{t}$   \nOutput: Latent feature $z_{n}^{t}$ ", "page_idx": 14}, {"type": "text", "text": "Specifically, the source branch is reverted to $z_{0}$ , while the trajectory of the target branch is iteratively adjusted. By computing the distance from the source branch and $\\epsilon^{\\mathrm{{cons}}}$ with Latent Consistency Model [32], the target branch is calibrated at each time step. ", "page_idx": 14}, {"type": "text", "text": "Our experiments, as seen in Section 5 of the main paper, show the ineffectiveness of a dual-branch procedure for multi-aspect editing tasks. Specifically, a single target branch is inadequate, leading to imperfection in the target image. Thereby we advocate multi-aspect editing through the use of multiple target branches. Each target branch handles a group of aspects, with simpler aspects such as non-rigid local edits directed to initial branches, and more complex aspects such as rigid local edits deferred to subsequent ones. Note that however, all the branches operate simultaneously. ", "page_idx": 14}, {"type": "text", "text": "Auxiliary Rigid / Non-Rigid Branches. In the main paper, it was noted that there was one dedicated branch for each type of edit: non-rigid, rigid, and global edit. The Early Aspect Grouping algorithm 1 classifies aspects into these three categories. Our experiments revealed that sometimes, due to low overlap between attention maps, aspects may not always be grouped into dedicated rigid or non-rigid branches. In such cases, it becomes necessary to include an auxiliary branch to handle the ungrouped aspects. Therefore, ParallelEdits may involve a single rigid branch and additional auxiliary branches to manage ungrouped aspects, and similarly, a single non-rigid branch and supplementary auxiliary branches to address ungrouped aspects. An ablation study on auxiliary branches is provided in Table 3. ", "page_idx": 14}, {"type": "text", "text": "C More Details on Evaluation Metrics ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we describe more details of our evaluation metrics. ", "page_idx": 14}, {"type": "text", "text": "LLaVA aspect accuracy. We show how we leverage LLaVA [38] to evaluate the multi-aspect editing accuracy in Fig. 7 and Fig. 8, we also prompt LLaVA [38] for explanation to human readers. LLaVA [38] could provide detailed summary for the image and also explanations for the mismatched between edited aspects and image. ", "page_idx": 14}, {"type": "text", "text": "Other evaluation metrics. Moreover, even though the Structure Distance [46] has been used in PIE-Bench [1] to evaluate the structure between source and target image while ignoring appearance information, it could not serve as a good evaluation metric for multi-aspect editing. This is due to the fact that the structure of multi-aspect edited target image may necessitate substantial modifications, particularly when it involves adding or removing multiple objects. ", "page_idx": 14}, {"type": "text", "text": "\ud83c\udf0b LLaVA [1,1,1,1] ", "page_idx": 15}, {"type": "text", "text": "Please provide some explanation. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\ud83c\udf0b LLaVA ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Theimagematches the elements in thedescription asfollows: 1.A sailboat is present in theimage,which is silhouetted against the evening glow. The evening glow is visible in the sky, creating a beautiful backdrop for the sailboat. 3.The mountains in thebackground are covered with snow,which is consistent with the description. 4.The overall scene isa winter scene,asindicated bythe snow-covered mountains and thecalm,cold appearance ofthe water Therefore, all elements in the description are matched by the image, resulting in a list of 4 ones. ", "page_idx": 15}, {"type": "image", "img_path": "cCL92OPlDz/tmp/24343acc119e4b8f3fda8a337f8e241fc3d154ca2a49a3314055e9f142b7e5c3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "\ud83c\udf0b LLaVA [1,1,1,1] ", "page_idx": 15}, {"type": "text", "text": "Please provide some explanation. ", "page_idx": 15}, {"type": "text", "text": "\ud83c\udf0b LLaVA ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The image matches all the elements in the given description: 1. A man is standing in a boat. 2. The man is silhouetted against the sunset. 3. There are ducks on the water. 4.The background features a mountain range,which could be described as Alps-like.   \nTherefore,the elements in the description are all present in the image, and the list would be [1, 1, 1, 1]. ", "page_idx": 15}, {"type": "image", "img_path": "cCL92OPlDz/tmp/0f715a8c6920043fd7933ac4b823c24c628b330cb762a5d1b6347367e21a6d75.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "LLaVA [1,1,1] ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Please provide some explanation. ", "page_idx": 15}, {"type": "text", "text": "\ud83c\udf0b LLaVA ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The image shows a boat on a lake with a house and a mountain in the background. The mountain appears to be covered with snow, which could be described as an \"ice mountain.\" The elements in the brackets match the description provided:   \nAA  hboouats ei si sp rviessieblnet  iinn  tthhee  ibmacakgge.r   \nA house is visible in the background.   \nThe mountain in the background is covered with snow, which could be described as an \"ice mountain.\" ", "page_idx": 15}, {"type": "text", "text": "Therefore, the elements in the brackets match the description, and the list would be [1,1, 1]. ", "page_idx": 15}, {"type": "text", "text": "Figure 7: Examples of prompting LLaVA for aspect accuracy measurement in cases of successful editing.   \nLLaVA can effectively illustrate if and how the target image and edits are misaligned. ", "page_idx": 15}, {"type": "text", "text": "\ud83c\udf0b LLaVA [1,0,0] ", "page_idx": 16}, {"type": "text", "text": "Please provide some explanation. ", "page_idx": 16}, {"type": "text", "text": "\ud83c\udf0b LLaVA Certainly! Here's an explanation for the elements in the image and their match with the elements in the provided format: l1e, a\"v[emso. nSeoy,  ttrheise  eplleanmte]\"n t-  isT hmea itmcahgeed .shows a plant that is commonly known as a money tree. It is a type of bonsai tree with a twisted trunk and glossy green leaves.So,this element is matched. specifies that the word should be written on the notebook, which is not the case here. The word is actually written on a whiteboard or a similar sur that is placed in front of the notebook. Therefore, this element is unmatched. that is plcedinfrontof thetebookTherefore,thislmentsunmathe. 3, \"[black table]\" - The table in the image is not black; it is a light color, possibly gray or beige. So, this element is unmatched. Therefore, the list of matches would be [1, 0, 0]. ", "page_idx": 16}, {"type": "text", "text": "\ud83c\udf0b LLaVA [1,1,0] ", "page_idx": 16}, {"type": "text", "text": "\ud83c\udf0b LLaVA ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Certainly! The image you've provided features a logo that is composed of an $\\times$ shape, which matches the first element in the description. The X is colored in   \na shade of pink, which matches the second element in the description. However, the background of the image is not light; it is dark, which does not match   \nthe third element in the description. Therefore, the list of matches would be: 1, Logo is an X- Matched 2, X is pink - Matched 3, Background is light - Not Matched   \nThis results in the list [1, 1, 0]. ", "page_idx": 16}, {"type": "text", "text": "Figure 8: Examples of prompting LLaVA for aspect accuracy measurement in cases of unsuccessful editing. ", "page_idx": 16}, {"type": "text", "text": "D Implementation Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Our proposed ParallelEdits is based on the Latent Consistency Model [32], with the publicly available ${\\bf L}{\\bf C}{\\bf\\bar{M}}^{\\dagger}$ which is finetuned from Stable Diffusion v1.5. We then follow [2] to leverage their proposed inversion-free technique in ParallelEdits for image editing. During sampling, we perform LCM sampling [32] with 15 denoising steps, and the classifier-free guidance (CFG) is set to 4.0. ParallelEdits can control the editing strength by adjusting the CFG . There\u2019s a trade-off between achieving satisfactory inversion and robust editing ability. A higher CFG tends to produce stronger editing effects but may lower inversion results and identity preservation. We also set the hyper-parameter $\\theta$ as 0.9 and $\\beta$ as 0.8 in our experiments, where $\\theta,\\beta$ are used to determine the edit type of a given edit action. ", "page_idx": 16}, {"type": "text", "text": "In the inversion-free multi-branch editing approach, for $1<n<N$ , the noise estimation is also conditioned on a text conditioning $c_{n}$ in branch $n$ . This can be expressed as $\\epsilon(n)_{\\tau}^{\\mathrm{edt}}=\\epsilon_{\\theta}\\big(z(n)_{\\tau}^{\\mathrm{edt}},\\tau,c_{n}\\big)$ . ", "page_idx": 16}, {"type": "text", "text": "\u2020https://huggingface.co/SimianLuo/LCMDreamshaperv7 ", "page_idx": 16}, {"type": "text", "text": "Here, $c_{1}$ corresponds to the source prompt, $c_{N}$ corresponds to the target prompt, and $c_{n}$ represents the prompt that includes all aspect edits up to branch $n$ . ", "page_idx": 17}, {"type": "text", "text": "E Additional Details of PIE-Bench++ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "E.1 PIE-Bench $^{++}$ Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Unlike existing benchmarks that primarily focus on single-aspect edits, PIE-Bench $^{++}$ is tailored to multiple aspect edits, reflecting the complexities inherent in real-world editing tasks. Our enhanced dataset, PIE-Bench $^{++}$ , builds upon the PIE-Bench [1] by incorporating 700 images across nine diverse categories, covering both natural and artificial scenes, with a significant focus on multi-aspect editing scenarios. Specifically, the Change Object category involves swapping objects in the scene with different yet reasonable alternatives. Add Object adds new elements to the scene. Delete Object focuses on removing objects, testing the model\u2019s ability to erase elements seamlessly. Change Object Content alters the content of specific objects, such as changing the design on a shirt or the pattern on a wall. Change Object Pose includes changes in the shape of objects, humans, or animals. Change Object Color assesses the model\u2019s ability to apply accurate color changes. Change Object Material evaluates the rendering of different textures and materials. Change Background involves editing scenarios where there is a distinct foreground object and a main background. This type of edit focuses on seamlessly integrating new background elements while preserving the integrity of the foreground object. Change Image Style involves the application of style transfer techniques to the entire image while ensuring the original content remains intact. For example, this could involve transforming a photograph to adopt a cartoon style. Each category is carefully curated to provide a comprehensive evaluation of the dataset\u2019s multi-aspect editing capabilities, the summary of the dataset is shown in Table 5. ", "page_idx": 17}, {"type": "text", "text": "E.2 Dataset Annotation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The annotation process involves a primary annotator who labels the source prompt, describing the original image, and the target prompt, which outlines the desired modifications to generate the target image. The target prompt is carefully annotated to include all editing pairs expected to be reflected in the target image. Subsequently, a second annotator reviews the annotations for accuracy and consistency, ensuring the reliability of the dataset. The majority of target prompts in PIE-Bench $^{++}$ feature at least two edited aspects. Nevertheless, within the categories that solely changing background and image styles, the number of edits is usually constrained to one or two aspects. This limitation is due to the intrinsic characteristics of these attributes, such as each image having only one background or style. ", "page_idx": 17}, {"type": "text", "text": "Annotation format details. Each image in the dataset annotation is associated with key elements as shown in Fig. 9: a source prompt, a target prompt, an edit action, and a mapping of aspects. The edit action specifies the position index in the source prompt where changes are to be made, the type of edit to be applied, and the operation required to achieve the desired outcome. The aspect mapping connects objects undergoing editing to their respective modified attributes, enabling the identification of which objects are subject to editing. ", "page_idx": 17}, {"type": "image", "img_path": "cCL92OPlDz/tmp/e685b381fa7754c15a836a0505e0544ba2a7ba8d29b4c63e815aca07939389d7.jpg", "img_caption": ["Figure 9: Annotation examples from PIE-Bench++. Each annotation containing a Source Prompt, Target Prompt, Edit Action, and Aspect Mapping. Edit action contains the specific instructions including the desired modification index in source prompt as position, edit type among 9 catergories and the action $\\in\\{\\otimes,\\oplus,\\ominus\\}$ . The aspect mapping indicts the pair between object and attribute. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "F Additional Qualitative Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We also provide more qualitative results in Fig. 10, showing the effectiveness of our proposed method in handling multi-aspect editing tasks. These examples showcase the model\u2019s proficiency in executing intricate edits. For instance, as depicted in Fig. 10 (b), our method successfully removes a cup while accurately reconstructing the obscured parts of the lamp behind it. In Fig. 10 (a), the model demonstrates its ability to swap and add aspects, while preserving the composition of the scene. The results underscore the model\u2019s adeptness in interpreting and executing sophisticated editing instructions, leading to visually consistent and contextually ftiting edited images. Additional, we also provide the results for sequential editing methods with different editing order in Fig. ??. ", "page_idx": 19}, {"type": "image", "img_path": "cCL92OPlDz/tmp/2a9812bb94daf901baa7f6f4aece9e88e84b8537fb9d521d6af28b13a4ba4a5e.jpg", "img_caption": ["Figure 10: Qualitative results from ParallelEdits. ParallelEdits is able to swap, add and delete multiple aspects. The last image pair is a failure case of ParallelEdits. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "cCL92OPlDz/tmp/2d1af06c7f1bdf2e6636be6c9691c9c33c2f6960104ec1835fe2d40a8cf725b7.jpg", "img_caption": ["Figure 11: Sequential editing using single-aspect text-driven image editing methods. The sequential editing might accumulate errors and undo previous edits. It also fails to edit significantly overlapped objects. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "cCL92OPlDz/tmp/b9c61c93a6c698f993c4129aa3f39fcd38944800f10dff93524396577930e311.jpg", "img_caption": ["Figure 12: Sequential editing with different orders. Sequential editing with different orders can yield varying final results. Additionally, it may lead to error accumulation and potentially overwrite previous edits. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We include the limitation and failure cases of the work in Sec. 5 ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper does not include theoretical result. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We fully disclose all the information needed to reproduce the main experimental results ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The code and data will be open-sourced for academic use. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines ( https://nips.cc/pu blic/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ( https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We specify all the training and test details Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The paper reports error bars ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper provide sufficient information on the computer resources Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal group, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The code follows the NeurIPS Code of Ethics. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The paper includes the discussion of potential societal impacts. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 24}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: CC-BY 4.0 for PIE-Bench. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The documentation provided alongside the assets Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]