{"references": [{"fullname_first_author": "R. Agarwal", "paper_title": "On-policy distillation of language models: Learning from self-generated mistakes", "publication_date": "2024", "reason": "This paper is foundational to the concept of on-policy distillation for language models, a key technique in the paper's methodology."}, {"fullname_first_author": "G. Hinton", "paper_title": "Distilling the knowledge in a neural network", "publication_date": "2015", "reason": "This seminal work introduced the concept of knowledge distillation, which is central to the paper's approach and widely used in model compression."}, {"fullname_first_author": "J. Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020", "reason": "This paper established crucial scaling laws for large language models, directly influencing the paper's discussion on the computational demands of LLMs."}, {"fullname_first_author": "H. Touvron", "paper_title": "LLaMA: Open and efficient foundation language models", "publication_date": "2023", "reason": "The LLaMA model series is used in the paper's experiments, making this foundational paper essential to understanding the models employed."}, {"fullname_first_author": "Y. Gu", "paper_title": "Knowledge distillation of large language models", "publication_date": "2024", "reason": "This paper directly addresses knowledge distillation for LLMs, providing a key comparative method within the paper's experimental setup."}]}