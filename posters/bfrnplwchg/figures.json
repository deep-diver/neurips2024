[{"figure_path": "bFrNPlWchg/figures/figures_1_1.jpg", "caption": "Figure 1: Left: Proposed Long Video MAE Decoder Masking. We leverage a jointly trained adaptive tokenizer and importance module to define a decoder mask and token targets for a video MAE pre-training strategy. The resulting sparsification in tokens (only 15%) allows pre-training with long videos (128-frames) and results in substantial performance gains. Right: Decoder masking and memory in long-video (128 frames) pre-training. We report memory and FLOPs per-device for a batch size of 1 using different decoder mask ratios and ViT-B architecture.", "description": "This figure illustrates the proposed long video masked autoencoder (LVMAE) decoder masking strategy. The left panel shows the architecture, highlighting the adaptive tokenizer and importance module used to select the most important tokens for reconstruction. This selective masking enables training on longer video sequences (128 frames). The right panel shows the memory and computational cost (GFLOPs) trade-offs associated with different decoder masking ratios, demonstrating the efficiency of the proposed adaptive strategy for long video training.", "section": "3 Approach"}, {"figure_path": "bFrNPlWchg/figures/figures_4_1.jpg", "caption": "Figure 2: Illustration of Adaptive FSQ-MagViT Training. FSQ-MagViT adaptive tokenizer includes MAGVIT encoder and CNN-based token scorer with a differentiable top-k selection layer designating importance of tokens. During tokenizer training unselected tokens zeroed out and video is reconstructed using MAGVIT decoder. We then freeze this adaptive tokenizer and use it to generate target tokens for scalable pre-training of video MAE.", "description": "This figure illustrates the training process of the Adaptive FSQ-MagViT tokenizer.  The tokenizer, a combination of a MAGVIT encoder and a CNN-based token scorer, assigns importance scores to video tokens. A differentiable top-k selection layer then chooses the most important tokens. During training, the less important tokens are masked (set to zero), and the MAGVIT decoder reconstructs the video from the remaining tokens and the learned importance scores. After training, this tokenizer is frozen and used to generate target tokens for the video MAE pre-training phase.", "section": "3.3 Adaptive Finite Scalar Quantized VAE for Saliency and Reconstruction Targets"}, {"figure_path": "bFrNPlWchg/figures/figures_17_1.jpg", "caption": "Figure 3: Our Adaptive tokenizer visualized. We visualize the tokens masks by masking the corresponding input video (repeating frames to much the latent temporal dimension).", "description": "This figure shows how the adaptive tokenizer selects tokens for a video masked autoencoder. The top row displays the original video frames. The bottom row displays the selected tokens after the adaptive tokenizer has processed the video.  The masked areas of the video are represented as black.  The figure demonstrates that the tokenizer focuses on the most important parts of the video for reconstruction, rather than randomly selecting tokens.", "section": "A.6 Visualization of adaptive mask"}, {"figure_path": "bFrNPlWchg/figures/figures_18_1.jpg", "caption": "Figure 4: Token selection strategies visualized. We can see that flow based token selection can be dominated by large background motion. Randomly selected masks are unable to focus the tokens on the interesting parts of the video. In contrast, we see that the adaptively selected tokens reflect well what is changing in the video.", "description": "This figure compares different token selection strategies for video masked autoencoders.  It shows how flow-based methods struggle with background motion, while random masking fails to focus on important parts of the video. The authors' adaptive method (ours) highlights the relevant changes in the video sequence. This visualizes the benefits of their proposed content-dependent masking.", "section": "A.6 Visualization of adaptive mask"}]