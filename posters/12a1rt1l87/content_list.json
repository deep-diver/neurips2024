[{"type": "text", "text": "Are Large-scale Soft Labels Necessary for Large-scale Dataset Distillation? ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Lingao Xiao1,3 and Yang $\\mathbf{He}^{1,2,3\\ast}$ ", "page_idx": 0}, {"type": "text", "text": "1CFAR, Agency for Science, Technology and Research, Singapore 2IHPC, Agency for Science, Technology and Research, Singapore 3National University of Singapore xiao_lingao@u.nus.edu, he_yang@cfar.a-star.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In ImageNet-condensation, the storage for auxiliary soft labels exceeds that of the condensed dataset by over 30 times. However, are large-scale soft labels necessary for large-scale dataset distillation? In this paper, we first discover that the high within-class similarity in condensed datasets necessitates the use of large-scale soft labels. This high within-class similarity can be attributed to the fact that previous methods use samples from different classes to construct a single batch for batch normalization (BN) matching. To reduce the within-class similarity, we introduce class-wise supervision during the image synthesizing process by batching the samples within classes, instead of across classes. As a result, we can increase within-class diversity and reduce the size of required soft labels. A key benefit of improved image diversity is that soft label compression can be achieved through simple random pruning, eliminating the need for complex rule-based strategies. Experiments validate our discoveries. For example, when condensing ImageNet-1K to 200 images per class, our approach compresses the required soft labels from 113 GB to 2.8 GB ( $40\\times$ compression) with a $2.6\\%$ performance gain. Code is available at: https://github.com/he-y/soft-label-pruning-for-dataset-distillation. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We are pacing into the era of ImageNet-level condensation, and the previous works [1, 2, 3, 4, 5] fail in scaling up to large-scale datasets due to extensive memory constraint. Until recently, Yin et al.[6] decouple the traditional distillation scheme into three phases. First, a teacher model is pretrained with full datasets (squeeze phase). Second, images are synthesized by matching the Batch Normalization (BN) statistics from the teacher and student models (recover phase). Third, auxiliary data such as soft labels are pregenerated from different image augmentations to create abundant supervision for posttraining (relabel phase). ", "page_idx": 0}, {"type": "text", "text": "However, the auxiliary data are $30\\times$ larger than the distilled data in ImageNet-1K. To ", "page_idx": 0}, {"type": "image", "img_path": "12A1RT1L87/tmp/1bfb19b07cf15453df774f1d9c4afc107825bdd3737415d95e09f23211058c9a.jpg", "img_caption": ["Figure 1: The relationship between performance and total storage of auxiliary information needed. Our method achieves SOTA performance with fewer soft labels than images. "], "img_footnote": [], "page_idx": 0}, {"type": "image", "img_path": "12A1RT1L87/tmp/708df98d7784b4e8111549eaf2d952e65a5395b17cff540c472f18fc0f97e7f8.jpg", "img_caption": ["Figure 2: Visual comparison between $\\mathrm{SRe^{2}L}$ and the proposed method. The classes are hammer shark (top), pineapple (middle), and pomegranate (bottom). Our method is more visually diverse. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "attain correct and effective supervision, the exact augmentations and soft labels of every training epoch are stored [6, 7, 8, 9, 10]. The required soft label storage is the colored circles in Fig. 1. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we consider whether large-scale soft labels are necessary, and what causes the excessive requirement of these labels? To answer these questions, we provide an analysis of the distilled images using $\\mathrm{SRe^{2}L}$ [7], and we find that within-class diversity is at stake as shown in Fig. 2. To be more precise, we analyze the similarity using Feature Cosine Similarity and Maximum Mean Discrepancy in Sec. 3.2. The high similarity of images within the same class requires extensive data augmentation to provide different supervision. ", "page_idx": 1}, {"type": "text", "text": "To address this issue, we propose Label Pruning for Large-scale Distillation (LPLD). Specifically, we modified the algorithms by batching images within the same class, leveraging the fact that different classes are naturally independent. Furthermore, we introduce class-wise supervision to align our changes. In addition, we have explored different label pruning metrics and found that simple random pruning was performed on par with carefully selected labels. To further increase diversity, we improve the label pool by introducing randomness in a finer granularity (i.e., batch-level). Our method effectively distills the images while requiring less label storage compared to image storage, as shown in Fig. 1. ", "page_idx": 1}, {"type": "text", "text": "The key contributions of this work are: (1) To the best of our knowledge, it is the first work to introduce label pruning to large-scale dataset distillation. (2) We discover that high within-class diversity necessitates large-scale soft labels. (3) We re-batch images and introduce class-wise supervision to improve data diversity, allowing random label pruning to be effective with an improved label pool. (4) Our LPLD method achieves SOTA performance using a lot less label storage, and it is validated with extensive experiments on various networks (e.g., ResNet, EfficientNet, MobileNet, and Swin-V2) and datasets (e.g., Tiny-ImageNet, ImageNet-1K, and ImageNet-21K). ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Dataset Distillation. DD [1] first introduces dataset distillation, which aims to learn a synthetic dataset that is equally effective but much smaller in size. The matching objectives include performance matching [1, 11, 12, 13, 14], gradient matching [4, 15, 16, 17], distribution or feature matching [5, 2, 18], trajectory matching [3, 19, 20], representative matching [21, 22], loss-curvature matching [23], and Batch-Norm matching[6, 7, 9, 10]. ", "page_idx": 1}, {"type": "text", "text": "Dataset Distillation of Large-Scale Datasets. Large-scale datasets scale up in terms of image size and the number of total images, incurring affordable memory consumption for most of the welldesigned matching objectives targeted for small datasets. MTT [3] is able to condense Tiny-ImageNet (ImageNet-1K subsets with images downsampled to $64\\times64$ and 200 classes). IDC [24] conducts experiments on ImageNet-10, which contains an image size of $224\\times224$ but has only 10 classes. TESLA [20] manages to condense the full ImageNet-1K dataset by exactly computing the unrolled gradient with constant memory or complexity. $\\mathrm{SRe^{2}L}$ [6] decouples the bilevel optimization into three phases: 1) squeezing, 2) recovering, and 3) relabeling. The proposed framework surpasses TESLA [20] by a noticeable margin. CDA [7] improves the recovering phase by introducing curriculum learning. RDED [8] replaces the recovering phase with an optimization-free approach by concatenating selected image patches. SC-DD [10] uses self-supervised models as recovery models. Existing methods [7, 8, 10] place high emphasis on improving the recovering phase; however, the problem of the relabeling phase is overlooked: a large amount of storage is required for the relabeling phase. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Label Compression. The problem of excessive storage seems to be fixed if the teacher model generates soft labels immediately used by the student model on the fly. However, when considering the actual use case of distilled datasets (i.e., Neural Architecture Search), using pre-generated labels enjoys speeding up training and reduced memory cost. More importantly, the generated labels can be repeatedly used. FKD [25] employs label quantization to store only the top- $k$ logits. In contrast, our method retains full logits, offering an orthogonal approach to quantization. A comparison to FKD is provided in Appendix D.3. Unlike FerKD [26], which removes some unreliable soft labels, our strategy targets higher pruning ratios. ", "page_idx": 2}, {"type": "text", "text": "Comparison with G-VBSM [9]. In one recent work, G-VBSM also mentioned re-batching the images within classes; however, the motivation is that having a single image in a class is insufficient [9]. It re-designed the loss by introducing a model pool, matching additional statistics from convolutional layers, and updating the statistics of synthetic images using exponential moving averages (EMA). Additionally, an ensemble of models is involved in both the data synthesis and relabel phase, requiring a total of $N$ forward propagation from $N$ different models, where $N=4$ is used for ImageNet-1K experiments. On the other hand, we aim to improve the within-class data diversity for reducing soft label storage. Furthermore, to account for the re-batching operation, we introduce class-wise supervision while all G-VBSM statistics remain global. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The conventional Batch Normalization (BN) transformation is defined as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\ny=\\gamma\\left(\\frac{x-\\mu}{\\sqrt{\\sigma^{2}+\\epsilon}}\\right)+\\beta,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\gamma$ and $\\beta$ are parameters learned during training, $\\mu$ and $\\sigma^{2}$ are the mean and variance of the input features, and $\\epsilon$ is a small constant to prevent division by zero. Additionally, the running mean and running variance are maintained during network training and subsequently utilized as $\\mu$ (mean) and $\\sigma^{2}$ (variance) during the inference phase, given that the true mean and variance of the test data are not available. ", "page_idx": 2}, {"type": "text", "text": "The matching object of SRe2L [7] follows DeepInversion [27], which optimizes synthetic datasets by matching the models\u2019 layer-wise BN statistics: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{BN}}(\\widetilde{\\pmb{x}})=\\displaystyle\\sum_{l}\\|\\mu_{l}(\\widetilde{\\pmb{x}})-\\mathbb{E}\\left(\\mu_{l}\\mid\\mathcal{T}\\right)\\|_{2}+\\displaystyle\\sum_{l}\\|\\sigma_{l}^{2}(\\widetilde{\\pmb{x}})-\\mathbb{E}\\left(\\sigma_{l}^{2}\\mid\\mathcal{T}\\right)\\|_{2}}\\\\ &{\\quad\\quad\\quad\\approx\\displaystyle\\sum_{l}\\left\\|\\mu_{l}(\\widetilde{\\pmb{x}})-\\mathbf{B}\\mathbf{N}_{l}^{\\mathrm{RM}}\\right\\|_{2}+\\displaystyle\\sum_{l}\\left\\|\\sigma_{l}^{2}(\\widetilde{\\pmb{x}})-\\mathbf{B}\\mathbf{N}_{l}^{\\mathrm{RV}}\\right\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the BN\u2019s running mean $\\mathbf{B}\\mathbf{N}_{l}^{\\mathrm{RM}}$ and running variance $\\mathbf{BN}_{l}^{\\mathrm{RV}}$ are used to approximate the expected mean $\\mathbb{E}\\left(\\mu_{l}\\mid T\\right)$ and expected variance $\\mathbb{E}\\left(\\sigma_{l}^{2}\\mid\\tau\\right)$ of the original dataset $\\tau$ , repsectively. The BN loss matches BN for layers $l$ , and $\\mu_{l}(\\widetilde{\\pmb{x}})$ and $\\sigma_{l}^{2}(\\widetilde{\\pmb{x}})$ are the mean and variance of the synthetic images $\\widetilde{\\mathbfit{x}}$ . ", "page_idx": 2}, {"type": "text", "text": "The BN loss term is used as a regularization term applied to the classification loss $\\mathcal{L}_{\\mathrm{CE}}$ . Therefore, the matching objective is: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\underset{\\widetilde{\\pmb{x}}}{\\arg\\operatorname*{min}}\\,\\underbrace{\\ell\\left(\\pmb{\\theta}_{\\mathcal{T}}\\left(\\widetilde{\\pmb{x}}\\right),\\pmb{y}\\right)}_{\\mathcal{L}_{\\mathrm{CE}}}+\\alpha\\cdot\\mathcal{L}_{\\mathrm{BN}}\\left(\\widetilde{\\pmb{x}}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\theta_{7}$ is the model pretrained on the original dataset $\\tau$ . The symbol $\\alpha$ is a small factor controlling the regularization strength of BN loss. ", "page_idx": 2}, {"type": "image", "img_path": "", "img_caption": ["Figure 3: MMD visualization. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.2 Diversity Analysis on Synthetic Dataset ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.2.1 Similarity within Synthetic Dataset: Feature Cosine Similarity ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A critical aspect of image diversity is how similar or different the images are within the same class. To quantify this, we utilize the feature cosine similarity measure defined above. Lower cosine similarity values between images within the same class indicate greater diversity, as the images are less similar to one another. This relationship is formally stated as follows: ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. The lower feature cosine similarity of images indicates higher diversity because the images are less similar to one another. ", "page_idx": 3}, {"type": "text", "text": "Feature Cosine similarity can be formally put as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{cos\\:similarity}:=\\frac{f(\\widetilde{\\mathbf{x}}_{c})\\cdot f(\\widetilde{\\mathbf{x}}_{c}^{\\prime})}{\\Vert f(\\widetilde{\\mathbf{x}}_{c})\\Vert\\Vert f(\\widetilde{\\mathbf{x}}_{c}^{\\prime})\\Vert}=\\frac{\\sum_{i=1}^{n}f(\\widetilde{\\mathbf{x}}_{c,i})\\ f(\\widetilde{\\mathbf{x}}^{\\prime}{}_{c,i})}{\\sqrt{\\sum_{i=1}^{n}f(\\widetilde{\\mathbf{x}}_{c,i})^{2}}\\sqrt{\\sum_{i=1}^{n}f(\\widetilde{\\mathbf{x}}^{\\prime}{}_{c,i})^{2}}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\widetilde{\\mathbf{x}}_{c}$ and $\\widetilde{\\mathbf{\\mathbf{x}}}_{\\mathit{c}}^{\\prime}$ are two images from the same class $c$ , $f(\\cdot)$ are the features extracted from a pretrained model, and $n$ is the feature dimension. ", "page_idx": 3}, {"type": "text", "text": "3.2.2 Similarity between Synthetic and Original Dataset: Maximum Mean Discrepancy ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The similarity between images is not the only determinant of diversity since images can be dissimilar to each other yet not representative of the original dataset. Therefore, to further validate the diversity of our synthetic dataset, we consider an additional metric: the Maximum Mean Discrepancy (MMD) between synthetic datasets and original datasets. This measure helps evaluate how well the synthetic data represents the original data distribution. The following proposition clarifies the relationship between MMD and dataset diversity: ", "page_idx": 3}, {"type": "text", "text": "Proposition 2. A lower MMD suggests that the synthetic dataset captures a broader range of features similar to the original dataset, indicating greater diversity. ", "page_idx": 3}, {"type": "text", "text": "The empirical approximation of MMD can be formally defined as [28, 29], ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{MMD}^{2}\\left(P_{\\mathcal{T}},P_{\\mathcal{S}}\\right)=\\hat{\\mathcal{K}}_{\\mathcal{T},\\mathcal{T}}+\\hat{\\mathcal{K}}_{\\mathcal{S},\\mathcal{S}}-2\\hat{\\mathcal{K}}_{\\mathcal{T},\\mathcal{S}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{\\hat{\\mathcal{K}}_{X,Y}=\\frac{1}{|X|\\cdot|Y|}\\sum_{i=1}^{|X|}\\sum_{j=1}^{|Y|}K\\left(f\\left(x_{i}\\right),f\\left(y_{j}\\right)\\right)}\\end{array}$ with $\\{x_{i}\\}_{i-1}^{|X|}\\sim X,\\{y_{i}\\}_{i=1}^{|Y|}\\sim Y$ . $\\tau$ and $\\boldsymbol{S}$ denote real and synthetic datasets, respectively; $\\kappa$ is the reproducing kernel (e.g., Gaussian kernel); $_{P}$ is the feature (embedding) distribution, and $f(\\cdot)$ is the feature representation extracted by model $\\theta$ , where $f(\\mathcal T)\\sim P_{\\mathcal T},f(\\ensuremath{\\boldsymbol{S}})\\stackrel{!}{\\sim}\\ensuremath{\\boldsymbol{P}}_{\\!\\mathcal{S}}$ . ", "page_idx": 3}, {"type": "text", "text": "3.3 Label Pruning for Large-scale Distillation (LPLD) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.3.1 Diverse Sample Generation via Class-wise Supervision ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The previous objective function follows Eq. 3; it uses a subset of classes $\\displaystyle{\\mathcal{B}}_{c}$ to match the BN statistics of the entire dataset, and images in the same class are independently generated, causing an low image diversity within classes. However, inspired by He et al.[30], images in the same class should work collaboratively, and images that are optimized individually (see Baseline $\\mathbf{B}$ in work [30]) do not lead to the optimal performance when IPC (Images Per Class) gets larger. ", "page_idx": 3}, {"type": "image", "img_path": "12A1RT1L87/tmp/92ae96cd873c7c391e141054816197d1eca668de1d65aa0885143689f093f55c.jpg", "img_caption": ["Figure 4: Illustration of existing methods (left, grey) and the proposed method (right, blue). Existing methods (i.e., $\\mathrm{SRe^{2}L}$ , CDA) independently generate along the IPC (Image-Per-Class) dimension, causing a high similarity between images of the same class. The proposed method allows images of the same class to collaborate, leaving different classes naturally independent. In addition, synthetic images are updated under class-wise supervision. The classification loss is omitted for simplicity. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Step 1: Re-batching Images within Class. Subsequently, to obtain a collaborative effect among different images of the same class, we sample images from the same class and provide the images with class-wise supervision [4, 24]. Fig. 4 illustrates the changes. ", "page_idx": 4}, {"type": "text", "text": "Step 2: Introducing Class-wise Supervision. However, the running mean and variance approximate the original dataset\u2019s expected mean and variance in a global aspect. The matching objective becomes sub-optimal in class-wise matching situation. To this end, we propose to track BN statistics for each class separately. Since we only track the running mean and variance, the extra storage is marginal even when up to 1K classes in ImageNet-1K (see Appendix E.2 and E.4). ", "page_idx": 4}, {"type": "text", "text": "Step 3: Class-wise Objective Function. The new class-wise objective function is modified from Eq. 3, which has two loss functions. First, we compute the classification loss (i.e., the Cross-Entropy Loss) with BN layers using global statistics to ensure effective supervision. Second, we compute BN loss by matching class-wise BN statistics. The modified parts are highlighted in blue color, and the objective function is formally put as, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\underset{\\widetilde{\\boldsymbol{x}}_{c}}{\\arg\\operatorname*{min}}\\left(-\\displaystyle\\sum_{i=1}^{N}{\\boldsymbol{y}_{c,i}\\log\\left(\\mathrm{softmax}\\left(\\theta_{T}\\left(\\frac{\\widetilde{\\boldsymbol{x}}_{c,i}-\\boldsymbol{\\mathbf{B}}\\mathbf{N}_{\\mathrm{global}}^{\\mathrm{RM}}}{\\sqrt{\\mathbf{B}\\mathbf{N}_{\\mathrm{global}}^{\\mathrm{RV}}+\\epsilon}}\\right)\\right)\\right)}_{c}\\right)}\\\\ &{}&{+\\left.\\alpha\\cdot\\displaystyle\\sum_{l}\\left(\\left\\|\\mu_{l}(\\widetilde{\\boldsymbol{x}}_{c})-\\mathbf{B}\\mathbf{N}_{l,c}^{\\mathrm{RM}}\\right\\|_{2}+\\left\\|\\sigma_{l}^{2}(\\widetilde{\\boldsymbol{x}}_{c})-\\mathbf{B}\\mathbf{N}_{l,c}^{\\mathrm{RV}}\\right\\|_{2}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "", "img_caption": ["Cross-Entropy Loss with Global BN Statistics ", "Batch Norm Loss with Class-wise BN Statistics "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "We want to emphasize that even though we are adjusting the BN loss with class-wise statistics, the global statistics of the dataset are still taken into account. The output logits for calculating CE loss are produced using global statistics. This is because altering $\\mu$ and $\\sigma$ without fine-tuning $\\gamma$ and $\\beta$ could lead to a decline in model performance, resulting in less effective supervision. ", "page_idx": 4}, {"type": "text", "text": "Theoretical Number of Updates for Stable Class-wise BN Statistics. Traditional BN layers do not compute class-wise statistics; therefore, we need to either keep track of the class-wise statistics while training a model from scratch or compute these statistics using a pretrained model. We prefer the latter as the former requires extensive computing resources. To understand how many BN statistics updates are needed, we can first look at the update rules of BN running statistics for a class $c$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{B}\\mathbf{N}_{l,c}^{\\mathrm{RM}}\\leftarrow(1-\\epsilon)\\cdot\\mathbf{B}\\mathbf{N}_{l,c}^{\\mathrm{RM}}+\\epsilon\\cdot\\mu_{l}(\\mathbf{x}_{c}),}\\\\ {\\mathbf{B}\\mathbf{N}_{l,c}^{\\mathrm{RV}}\\leftarrow(1-\\epsilon)\\cdot\\mathbf{B}\\mathbf{N}_{l,c}^{\\mathrm{RV}}+\\epsilon\\cdot\\sigma_{l}^{2}(\\mathbf{x}_{c}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "12A1RT1L87/tmp/912751093dee485012b0a70a061e5edfa839dfb1e3b4c37329597f2bb1d203b8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 5: Illustration of two random processes in label pruning with improved label pool. First, we need a smaller soft label pool due to the storage budget. We can conduct pruning at two levels: (1) epoch-level and (2) batch-level. Batch-level pruning can provide a more diverse label pool since augmentations (e.g., Mixup or CutMix) are different across batches. The illustrated pruning ratio is $25\\%$ ; the crossed-out labels denote the pruned labels, and the remaining form the label pool. Second, we randomly sample soft labels for model training. ", "page_idx": 5}, {"type": "text", "text": "where $\\epsilon$ is the momentum. Since the momentum factor for the current batch statistics is usually set to a small value (i.e., $\\epsilon=0.1$ ), we can theoretically compute existing running statistics that can be statistically significant after how many updates, assuming all other factors are fixed. ", "page_idx": 5}, {"type": "text", "text": "Since the running statistics are computed per class, we provide the theoretical number of updates required to stabilize all class statistics (see Appendix A for the proof): ", "page_idx": 5}, {"type": "equation", "text": "$$\nn\\geq\\operatorname*{max}\\left(\\underbrace{-2\\ln\\left(\\frac{T}{2}\\right)}_{\\delta^{2}\\operatorname*{min}(q_{c})},\\quad\\underbrace{\\ln\\left(\\frac{C}{\\tau}\\right)}_{\\mathrm{BN\\,Convergence}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $n$ is the number updates needed, $q_{c}$ is the probability that class $c$ appears in a batch, $T$ is a probability threshold, $\\varepsilon$ is the momentum parameter in Batch Normalization, $\\delta$ is the acceptable relative deviation (where $0\\leq\\delta\\leq1\\rangle$ ), $C$ is some constant, and $\\tau$ is the desired convergence tolerance for the BN statistics. How Eq. 8 guides our experiment design is detailed in Appendix E.3. ", "page_idx": 5}, {"type": "text", "text": "3.3.2 Random Label Pruning with Improved Label Pool ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Excelling in Both Similarity Measures. By adopting the changes provided in Sec. 3.3.1, our synthetic dataset is more diverse and representative than the existing methods. First, our dataset exhibits smaller feature cosine similarity within classes compared to datasets produced by existing methods, as shown in Table 1. This indicates that our synthetic images are less similar to each other and, thus, more diverse. Second, our dataset exhibits a significantly lower MMD shown in Fig 3 compared to datasets produced by existing methods. This suggests that our synthetic dataset better captures the feature distribution of the original dataset. After obtaining a diverse dataset, the next move is to address superfluous soft labels. ", "page_idx": 5}, {"type": "text", "text": "Random Label Pruning. Different from dataset pruning metrics, which many wield training dynamics [31, 32], label pruning is inherently different since the labels in different epochs are independently generated or evaluated. Subsequently, these methods do not directly apply, and we modify these metrics to determine which epochs contain the most useful augmentations and soft labels. Through empirical study, we find that using soft labels carefully pruned from different metrics is no better than simple random pruning. As a result, we can discard complex rule-based pruning metrics, attaining both simplicity and efficiency. After obtaining the soft label pool, we have to decide which labels will be used. Following the previous random pruning scheme, we randomly sample the labels for model training in order to ensure diversity and avoid any prior knowledge. ", "page_idx": 5}, {"type": "text", "text": "Improved Label Pool. Considering that random selection may be the most efficient choice, we rethink the diversity of the label pool, as labels at the epoch-level are not the finest elements. ", "page_idx": 5}, {"type": "text", "text": "Table 2: Tiny-ImageNet label pruning results. The standard deviation is attained from three different runs. \u2020 denotes the reported results. ", "page_idx": 6}, {"type": "table", "img_path": "12A1RT1L87/tmp/87e15ca83f7bbda2fb33b789c5b2313dbc3b31bf02a0cc4b5912b3bb2bbd63a0.jpg", "table_caption": ["(a) Comparison between SOTA methods. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "12A1RT1L87/tmp/5204f9903935938e7fdd75c9000e25756a24409e849816ac12f1d73bbe35e473.jpg", "table_caption": ["(b) Experiments on larger networks. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "12A1RT1L87/tmp/fa61a6a240dedb14b88637038928d4e521fef6a74f3555ea3f893b110d537f75.jpg", "table_caption": ["Table 3: ImageNet-1K label pruning result. Our method consistently shows a better performance under various pruning ratios. The validation model is ResNet-18. $^\\dagger$ denotes the reported results. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "The augmentations such as CutMix and Mixup are performed at the batch level, where the same augmentations are applied to images within the same batch and are different across batches. Therefore, we improve the label pool by allowing batches in different epochs to form a new epoch. The improved label pool breaks the fixed batch orders and the fixed combination of augmentations within an epoch, allowing a more diverse training process while reusing the labels. Our label pruning method is illustrated in Fig. 5. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experiment Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset details can be found in Appendix B and detailed settings are provided in Appendix C.   \nComputing resources used for experiments can be found in Appendix E.5. ", "page_idx": 6}, {"type": "text", "text": "Dataset. Our experiment results are evaluated on Tiny-ImageNet [33], ImageNet-1K [34], and ImageNet-21K-P [35]. We follow the data pre-processing procedure of $\\bar{\\mathrm{SRe^{2}L}}$ [6] and CDA [7]. ", "page_idx": 6}, {"type": "text", "text": "Squeeze. We modify the pretrained model by adding class-wise BN running mean and running variance; since they are not involved in computing the BN statistics, they do not affect performance. As mentioned in Sec. 3.3.1, we compute class-wise BN statistics by training for one epoch with model parameters kept frozen. ", "page_idx": 6}, {"type": "text", "text": "Recover. We perform data synthesis following Eq. 6. The batch size for the recovery phase is the same as the IPC. Besides, we adhere to the original setting in $\\mathrm{SRe^{2}L}$ . ", "page_idx": 6}, {"type": "text", "text": "Relabel. We use pretrained ResNet18 [36] for all experiments as the relabel model except otherwise stated. For Tiny-ImageNet and ImageNet-1K, we use Pytorch pretrained model. For ImageNet-21K-P, we use Timm pretrained model. ", "page_idx": 6}, {"type": "text", "text": "Validate. For validation, we adhere to the hyperparameter settings of CDA [7]. ", "page_idx": 6}, {"type": "text", "text": "Pruning Setting. For label pruning, we exclude the last batch (usually with an incomplete batch size) of each epoch from the label pool. There are two random processes: (1) Random candidate selection from all batches. (2) Random reuse of candidate labels. ", "page_idx": 6}, {"type": "text", "text": "Table 5: Comparison between different pruning metrics. Results are obtained from ImageNet-1K IPC10 and validated using ResNet-18. (b) Calibration of label pool. ", "page_idx": 7}, {"type": "table", "img_path": "12A1RT1L87/tmp/bed9f7c6c43262c65bddf3e14b20d27272b145ebf18df55b2cb64a532b92fdcf.jpg", "table_caption": ["(a) Random pruning vs. Pruning metrics at $40\\times$ "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "12A1RT1L87/tmp/61f99f676e722a49301b7ce4b97cfdb2b2eba5255a5814fd7024fcab3bdaa44e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 Primary Result ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Tiny-ImageNet. Table 2a presents a comparison between the label pruning outcomes on TinyImageNet for our approach, $\\mathrm{SRe^{2}L}$ [6], and the subsequent work, CDA [7]. Our method not only consistently surpasses $\\mathrm{SRe^{2}L}$ across identical pruning ratios but also achieves comparable results to $\\mathrm{SRe^{2}L}$ while using $40\\times$ fewer labels. When compared to CDA, our method exhibits closely matched performance, yet it demonstrates superior accuracy preservation. For instance, at a $40\\times$ label reduction, our method secures a notable $7.5\\%$ increase in accuracy over CDA, even though the improvement stands at a mere $0.1\\%$ at the $1\\times$ benchmark. Table 2b provides the pruning results on ResNet50 and ResNet101. Although there are consistent improvements observed when compared to ResNet18, scaling to large networks does not necessarily bring improvements. ", "page_idx": 7}, {"type": "text", "text": "ImageNet-1K. Table 3 compares the ImageNet-1K pruning results with SOTA methods on ResNet18. Our method outperforms other SOTA methods at various pruning ratios and different IPCs. More importantly, our method consistently exceeds the unpruned version of $\\mathrm{SRe^{2}L}$ with $30\\times$ less storage. Such a result is not impressive at first glance; however, when considering the actual storage, the storage is reduced from 29G to 0.87G. In addition, we notice the performance at $10\\times$ (or $90\\%$ ) pruning ratio degrades slightly, especially for large IPCs. For example, merely $0.2\\%$ performance degradation on IPC200 using ResNet18. Pruning results of larger IPCs can be found in Appendix D.2. ", "page_idx": 7}, {"type": "text", "text": "4.3 Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Ablation Study. Table 4 presents the ablation study of the proposed method. Row 1 is the implementation of $\\mathrm{SRe^{2}L}$ under CDA\u2019s hyperparameter settings. Row 2 is simply re-ordering the loops, and the performance at $1\\times$ is improved; nevertheless, when considering the extreme pruning ratio (i.e., $100\\!\\times\\!)$ , it falls short of the existing method. Row 3 computes class-wise BN running statistics in the \u201csqueeze\u201d phase, ", "page_idx": 7}, {"type": "table", "img_path": "12A1RT1L87/tmp/ac7c60de4e3d319821f84d393ab9318820cab0350b1b3ed2304a5fa2c758cd4c.jpg", "table_caption": ["Table 4: Ablation study of the proposed method. C denotes using class-wise matching. CS denotes suing class-wise supervision. ILP denotes using an improved label pool. (IPC50, ResNet18, ImageNet-1K). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "and these class-wise statistics are used as supervision in the \u201crecover\u201d phase. A steady improvement is observed. Row 4 allows pre-generated labels to be sampled at batch level from different epochs, further boosting the performance. Refer to Appendix D.1 for an expanded version of ablation. ", "page_idx": 7}, {"type": "text", "text": "Label Pruning Metrics. From Table 5a, we empirically find that using different metrics explained in Appendix E.1 is no better than random pruning. In addition, as mentioned in FerKD [25], calibrating the searching space by discarding a portion of easy or hard images can be beneficial. We conduct a similar experiment to perform random pruning on a calibrated label pool, and the metric for determining easy or hard images is \u201cconfidence\u201d. However, as shown in Table 5b, no such range can consistently outperform the non-calibrated ones (last row). An interesting observation is that the label pruning law at large pruning ratio seems to coincide partially with data pruning, where removing hard labels becomes beneficial [37]. ", "page_idx": 7}, {"type": "text", "text": "Generalization. Table 6a shows the performance under large compression rates. Smaller IPC datasets suffer more from label pruning since it requires more augmentation and soft label pairs to boost data diversity. Furthermore, label pruning results on ResNet50 are provided in Table 6b. ", "page_idx": 7}, {"type": "table", "img_path": "12A1RT1L87/tmp/518470c6bf49787855006128e03fc0f80e2d0a7e8ab3d7f2baad74fdcc039d9e.jpg", "table_caption": ["(a) Large pruning rate. (b) Label pruning results on ResNet-50. (c) Cross-architecture result. IPC50. ", "Table 6: Additional ImageNet-1K label pruning results "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "12A1RT1L87/tmp/b525c51ca43691acaee6a0d4525dcf018ebe8340d13d98c908e4a7c8672d7427.jpg", "table_caption": ["Table 7: Label pruning result on ImageNet-21K- Table 8: Label pruning for optimization-free P, using ResNet-18. I denotes image storage. L method. \u201cOurs\u201d uses improved label pool. denotes label storage. \u2020 denotes reported results. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Not only scaling to large networks of the same family (i.e., ResNet) but Table 6c also demonstrates the generalization capability of the proposed method across different network architectures. An analogous trend is evident in the context of label pruning: comparable performance is achieved with $10\\times$ fewer labels. This reinforces the statement that the necessity for extensive augmentations and labels can be significantly reduced if the dataset exhibits sufficient diversity. ", "page_idx": 8}, {"type": "text", "text": "Large Dataset. ImageNet-21K-P has 10,450 classes, significantly increasing the disk storage as each soft label stores a probability of 10,450 classes. The IPC20 dataset leads to a 1.2 TB (i.e., 1285 GB) label storage, making the existing framework less practical. However, with the help of our method, it can surpass $\\mathrm{SRe^{2}L}$ [6] by a large margin despite using $40\\times$ less storage. For example, we attain an $8.9\\%$ accuracy improvement on IPC20 with label storage reduced from 1285 GB to 32 GB. ", "page_idx": 8}, {"type": "text", "text": "Pruning for Optimization-Free Approach. RDED [8] is an optimization-free approach during the \u201crecover\u201d phase. However, extensive labels are still required for post-evaluation. To prune labels, consistent improvements are observed using the improved label pool, as shown in Table 8. ", "page_idx": 8}, {"type": "text", "text": "Comparison with G-VBSM [9]. Compared to G-VBSM [9], which uses an ensemble of 4 models to recover and relabel, our method outperforms it at various pruning ratios with only a single model (see Table 9). Furthermore, the techniques used for G-VBSM apply to our method. By adopting label generation with ensemble and a loss function of $\\mathbf{\\Psi}^{*}\\mathbf{M}\\mathbf{S}\\mathbf{E}\\mathbf{+}\\mathbf{0}.1\\ \\times\\ \\mathbf{G}\\mathbf{T}^{*}$ [9], our method can be further improved by a large margin on IPC10 of ImageNet-1K, using ResNet18. Implementation details can be found in Appendix C.4. ", "page_idx": 8}, {"type": "table", "img_path": "12A1RT1L87/tmp/90d20195b5ae6770c010bdf11e1b296e2d1cf74e87f20ef360a4440ec39f8413.jpg", "table_caption": ["Table 9: Compare with GVBSM [9]. \u201cOurs+\u201d uses ensemble and $\\mathrm{MSE+GT}$ loss. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Visualization. Fig. 2b visualizes our method on three classes. More visualizations are provided in Appendix F. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To answer the question \u201cwhether large-scale soft labels are necessary for large-scale dataset distillation?\u201d, we conduct diversity analysis on synthetic datasets. The high within-class similarity is observed and necessitates large-scale soft labels. Our LPLD method re-batches images within classes and introduces class-wise BN supervision during the image synthesis phase to address this issue. These changes improve data diversity, so that simple random label pruning can perform on par with complex rule-based pruning metrics. Additionally, we randomly conduct pruning on an improved label pool. Finally, LPLD is validated by extensive experiments, serving a strong baseline that takes into account actual storage. Limitations and future works are provided in Appendix E.6. The ethics statement and broader impacts can be found in Appendix E.7. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by A\\*STAR Career Development Fund (CDF) under C233312004, in part by the National Research Foundation, Singapore, and the Maritime and Port Authority of Singapore / Singapore Maritime Institute under the Maritime Transformation Programme (Maritime AI Research Programme \u2013 Grant number SMI-2022-MTP-06). The computational work for this article was partially performed on resources of the National Supercomputing Centre (NSCC), Singapore (https://www.nscc.sg). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. Dataset distillation. arXiv preprint arXiv:1811.10959, 2018.   \n[2] Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang, Shuo Wang, Guan Huang, Hakan Bilen, Xinchao Wang, and Yang You. Cafe: Learning to condense dataset by aligning features. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 12196\u201312205, 2022.   \n[3] George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A. Efros, and Jun-Yan Zhu. Dataset distillation by matching training trajectories. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., 2022. [4] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with gradient matching. In Proc. Int. Conf. Learn. Represent., 2021. [5] Bo Zhao and Hakan Bilen. Dataset condensation with distribution matching. In Proc. IEEE Winter Conf. Appl. Comput. Vis., pages 6514\u20136523, 2023. [6] Zeyuan Yin, Eric Xing, and Zhiqiang Shen. Squeeze, recover and relabel: Dataset condensation at imagenet scale from a new perspective. In Proc. Adv. Neural Inform. Process. Syst., 2023. [7] Zeyuan Yin and Zhiqiang Shen. Dataset distillation in large data era. arXiv preprint arXiv:2311.18838, 2023. [8] Peng Sun, Bei Shi, Daiwei Yu, and Tao Lin. On the diversity and realism of distilled dataset: An efficient dataset distillation paradigm. arXiv preprint arXiv:2312.03526, 2023.   \n[9] Shitong Shao, Zeyuan Yin, Muxin Zhou, Xindong Zhang, and Zhiqiang Shen. Generalized large-scale data condensation via various backbone and statistical matching. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., 2024.   \n[10] Muxin Zhou, Zeyuan Yin, Shitong Shao, and Zhiqiang Shen. Self-supervised dataset distillation: A good compression is all you need. arXiv preprint arXiv:2404.07976, 2024.   \n[11] Timothy Nguyen, Zhourong Chen, and Jaehoon Lee. Dataset meta-learning from kernel ridge-regression. In Proc. Int. Conf. Learn. Represent., 2021.   \n[12] Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee. Dataset distillation with infinitely wide convolutional networks. In Proc. Adv. Neural Inform. Process. Syst., pages 5186\u20135198, 2021.   \n[13] Yongchao Zhou, Ehsan Nezhadarya, and Jimmy Ba. Dataset distillation using neural feature regression. In Proc. Adv. Neural Inform. Process. Syst., 2022.   \n[14] Noel Loo, Ramin Hasani, Alexander Amini, and Daniela Rus. Efficient dataset distillation using random feature approximation. In Proc. Adv. Neural Inform. Process. Syst., 2022.   \n[15] Zixuan Jiang, Jiaqi Gu, Mingjie Liu, and David Z Pan. Delving into effective gradient matching for dataset condensation. arXiv preprint arXiv:2208.00311, 2022.   \n[16] Saehyung Lee, Sanghyuk Chun, Sangwon Jung, Sangdoo Yun, and Sungroh Yoon. Dataset condensation with contrastive signals. In Proc. Int. Conf. Mach. Learn., pages 12352\u201312364, 2022.   \n[17] Noel Loo, Ramin Hasani, Mathias Lechner, and Daniela Rus. Dataset distillation with convexified implicit gradients. In Proc. Int. Conf. Mach. Learn., 2023.   \n[18] Ganlong Zhao, Guanbin Li, Yipeng Qin, and Yizhou Yu. Improved distribution matching for dataset condensation. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 7856\u20137865, 2023.   \n[19] Jiawei Du, Yidi Jiang, Vincent TF Tan, Joey Tianyi Zhou, and Haizhou Li. Minimizing the accumulated trajectory error to improve dataset distillation. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., 2023.   \n[20] Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh. Scaling up dataset distillation to imagenet1k with constant memory. In Proc. Int. Conf. Mach. Learn., pages 6565\u20136590. PMLR, 2023.   \n[21] Yanqing Liu, Jianyang Gu, Kai Wang, Zheng Zhu, Wei Jiang, and Yang You. DREAM: Efficient dataset distillation by representative matching. arXiv preprint arXiv:2302.14416, 2023.   \n[22] Murad Tukan, Alaa Maalouf, and Margarita Osadchy. Dataset distillation meets provable subset selection. arXiv preprint arXiv:2307.08086, 2023.   \n[23] Seungjae Shin, Heesun Bae, Donghyeok Shin, Weonyoung Joo, and Il-Chul Moon. Losscurvature matching for dataset selection and condensation. In International Conference on Artificial Intelligence and Statistics, pages 8606\u20138628, 2023.   \n[24] Jang-Hyun Kim, Jinuk Kim, Seong Joon Oh, Sangdoo Yun, Hwanjun Song, Joonhyun Jeong, Jung-Woo Ha, and Hyun Oh Song. Dataset condensation via efficient synthetic-data parameterization. In Proc. Int. Conf. Mach. Learn., 2022.   \n[25] Zhiqiang Shen and Eric Xing. A fast knowledge distillation framework for visual recognition. In Proc. Eur. Conf. Comput. Vis., pages 673\u2013690, 2022.   \n[26] Zhiqiang Shen. Ferkd: Surgical label adaptation for efficient distillation. In Proc. Int. Conf. Comput. Vis., pages 1666\u20131675, 2023.   \n[27] Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj K Jha, and Jan Kautz. Dreaming to distill: Data-free knowledge transfer via deepinversion. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 8715\u20138724, 2020.   \n[28] Hansong Zhang, Shikun Li, Pengju Wang, Dan Zeng, and Shiming Ge. M3d: Dataset condensation by minimizing maximum mean discrepancy. In Proc. AAAI Conf. Artif. Intell., pages 9314\u20139322, 2024.   \n[29] Tian Qin, Zhiwei Deng, and David Alvarez-Melis. Distributional dataset distillation with subtask decomposition. arXiv preprint arXiv:2403.00999, 2024.   \n[30] Yang He, Lingao Xiao, Joey Tianyi Zhou, and Ivor Tsang. Multisize dataset condensation. In Proc. Int. Conf. Learn. Represent., 2024.   \n[31] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J. Gordon. An empirical study of example forgetting during deep neural network learning. In Proc. Int. Conf. Learn. Represent., 2019.   \n[32] Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet: Finding important examples early in training. In Proc. Adv. Neural Inform. Process. Syst., pages 20596\u201320607, 2021.   \n[33] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.   \n[34] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 248\u2013255. Ieee, 2009.   \n[35] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. arXiv preprint arXiv:2104.10972, 2021.   \n[36] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 770\u2013778, 2016.   \n[37] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. In Proc. Adv. Neural Inform. Process. Syst., pages 19523\u201319536, 2022.   \n[38] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pages 6105\u20136114. PMLR, 2019.   \n[39] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 4510\u20134520, 2018.   \n[40] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12009\u201312019, 2022.   \n[41] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 9729\u20139738, 2020.   \n[42] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.   \n[43] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training. In Proc. Int. Conf. Mach. Learn., pages 10096\u201310106. PMLR, 2021.   \n[44] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 6848\u20136856, 2018.   \n[45] Geoff Pleiss, Tianyi Zhang, Ethan Elenberg, and Kilian Q Weinberger. Identifying mislabeled data using the area under the margin ranking. In Proc. Adv. Neural Inform. Process. Syst., pages 17044\u201317056, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proof ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We aim to determine a lower bound on the number of batches (updates) $n$ required to ensure that the Batch Normalization (BN) statistics for each class in the ImageNet dataset converge within a specified tolerance $\\tau$ , with high probability. The dataset has a varying number of images per class, affecting the probability of each class appearing in a batch during sampling. ", "page_idx": 12}, {"type": "text", "text": "A.1 Preliminary Analysis ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Defining Class Probabilities: Let $p_{c}$ denote the probability that a randomly selected image from the dataset belongs to class $c$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\np_{c}=\\frac{\\mathrm{Number~of~images~in~class~}c}{\\mathrm{Total~number~of~images~in~the~dataset}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Due to the unequal distribution of images across classes, $p_{c}$ varies among classes. ", "page_idx": 12}, {"type": "text", "text": "Probability of Class Appearance in a Batch: When sampling a batch of size $B$ , the probability that class $c$ does not appear in the batch is $(1-p_{c})^{B}$ . Therefore, the probability that class $c$ appears in the batch is: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q_{c}=1-(1-p_{c})^{B}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "This represents the likelihood that at least one image from class $c$ is included in a given batch. ", "page_idx": 12}, {"type": "text", "text": "Number of Batches: Let $n$ be the total number of batches sampled during training. ", "page_idx": 12}, {"type": "text", "text": "We assume that batches are sampled independently with replacement from the dataset. Under this assumption, each batch is an independent trial where class $c$ appears with probability $q_{c}$ . Therefore, the number of batches $M$ where class $c$ appears follows a binomial distribution: ", "page_idx": 12}, {"type": "equation", "text": "$$\nM\\sim\\mathrm{Binomial}(n,q_{c}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Remark: In practice, batches are often sampled without replacement within an epoch, introducing dependency between batches. However, for large datasets where the total number of images $N$ is significantly larger than the batch size $B$ and the number of batches $n$ , the dependence becomes negligible. In such cases, the binomial distribution serves as a reasonable approximation. ", "page_idx": 12}, {"type": "text", "text": "The expected value of $M$ is: ", "page_idx": 12}, {"type": "equation", "text": "$$\nE[M]=n q_{c}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "A.2 Chernoff Bound ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "To ensure that $M$ is not significantly less than its expected value $E[M]$ , we apply the Chernoff bound: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(M\\leq(1-\\delta)E[M]\\right)\\leq\\exp\\left(-{\\frac{\\delta^{2}E[M]}{2}}\\right),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\delta\\in(0,1)$ represents the acceptable relative deviation from the expected value. This bound provides a way to quantify the probability that a random variable deviates from its expected value, which is crucial for making high-confidence guarantees. ", "page_idx": 12}, {"type": "text", "text": "To ensure the probability that $M$ is less than $(1-\\delta)E[M]$ is at most $T_{1}$ , we set: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\exp\\left(-\\frac{\\delta^{2}n q_{c}}{2}\\right)\\leq T_{1}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Solving for $n$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\nn\\geq\\frac{-2\\ln(T_{1})}{\\delta^{2}q_{c}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "To ensure this condition holds for all classes, we use the minimum value of $q_{c}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\nn\\geq\\frac{-2\\ln(T_{1})}{\\delta^{2}\\operatorname*{min}(q_{c})}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.3 BN Convergence ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "BN Statistics Update: Batch Normalization updates its running statistics using an exponential moving average. The update rule for the BN statistics of class $c$ at iteration $t+1$ is: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{B}\\mathbf{N}_{c}^{t+1}=(1-\\varepsilon)\\mathbf{B}\\mathbf{N}_{c}^{t}+\\varepsilon\\mathbf{B}\\mathbf{\\hat{N}}_{c}^{t+1},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\varepsilon$ is the momentum parameter, and $\\hat{\\mathbf{B}}\\mathbf{N}_{c}^{t+1}$ is the BN statistics estimated from the current batch for class $c$ . ", "page_idx": 13}, {"type": "text", "text": "Since BN statistics for class $c$ are updated only when class $c$ appears in the batch, we consider only the updates corresponding to those batches. After $M$ such updates: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{B}\\mathbf{N}_{c}^{t+M}=(1-\\varepsilon)^{M}\\mathbf{B}\\mathbf{N}_{c}^{t}+\\varepsilon\\sum_{k=1}^{M}(1-\\varepsilon)^{M-k}\\mathbf{B}\\mathbf{\\hat{N}}_{c}^{k},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\hat{\\mathbf B}\\mathbf{\\hat{N}}_{c}^{k}$ is the BN statistics estimated in the $k$ -th batch containing class $c$ . ", "page_idx": 13}, {"type": "text", "text": "Assuming $\\hat{\\mathbf{B}}\\mathbf{\\hat{N}}_{c}^{k}$ is an unbiased estimator of the true BN statistics $\\mu_{c}$ when class $c$ appears, the expected value is: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{E[{\\bf B}{\\bf N}_{c}^{t+M}]=(1-\\varepsilon)^{M}{\\bf B}{\\bf N}_{c}^{t}+\\mu_{c}\\left[1-(1-\\varepsilon)^{M}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Convergence Within Tolerance: To ensure that the BN statistics converge within a tolerance $\\tau$ of the true statistics $\\mu_{c}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left|E[{\\bf B}{\\bf N}_{c}^{t+M}]-\\mu_{c}\\right|\\leq\\tau.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left|E[{\\bf B N}_{c}^{t+M}]-\\mu_{c}\\right|=(1-\\varepsilon)^{M}\\left|{\\bf B N}_{c}^{t}-\\mu_{c}\\right|,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and assuming $\\left|\\mathbf{B}\\mathbf{N}_{c}^{t}-\\mu_{c}\\right|\\leq C$ for some constant $C$ , we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1-\\varepsilon)^{M}C\\leq\\tau.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Taking natural logarithms: ", "page_idx": 13}, {"type": "equation", "text": "$$\nM\\ln(1-\\varepsilon)+\\ln(C)\\leq\\ln(\\tau).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Using the approximation $\\ln(1-\\varepsilon)\\approx-\\varepsilon$ for small $\\varepsilon$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n-M\\varepsilon+\\ln(C)\\leq\\ln(\\tau).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Solving for $M$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\nM\\geq M_{0}=\\frac{\\ln\\left(\\frac{C}{\\tau}\\right)}{\\varepsilon}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Origin of $M_{0}$ : Here, $M_{0}$ is derived from the BN convergence requirement that ensures: ", "page_idx": 14}, {"type": "equation", "text": "$$\n(1-\\varepsilon)^{M_{0}}\\left|\\mathrm{BN}_{c}^{t}-\\mu_{c}\\right|\\leq\\tau.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "It represents the minimum number of updates required for the BN statistics of class $c$ to converge within the desired tolerance $\\tau$ . ", "page_idx": 14}, {"type": "text", "text": "A.4 Combining Bounds ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Event Definitions: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 Let $E_{1}$ be the event that class $c$ appears in sufficient batches (as guaranteed by the Chernoff bound). \u2022 Let $E_{2}$ be the event that the BN statistics for class $c$ converge within the desired tolerance $\\tau$ . ", "page_idx": 14}, {"type": "text", "text": "Target Probability: We aim to ensure that both events occur simultaneously with high probability: ", "page_idx": 14}, {"type": "equation", "text": "$$\nP(E_{1}\\cap E_{2})\\geq1-T.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Union Bound Application: For any two events, the probability of their intersection satisfies: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{P(E_{1}\\cap E_{2})=1-P(\\overline{{E_{1}\\cap E_{2}}})}&{}\\\\ {=1-P(\\overline{{E_{1}}}\\cup\\overline{{E_{2}}})}&{}\\\\ {\\geq1-P(\\overline{{E_{1}}})-P(\\overline{{E_{2}}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Error Probability Allocation: For simplicity, we allocate the total acceptable failure probability $T$ equally between the two events: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{P(\\overline{{E_{1}}})\\leq\\frac{T}{2}}&{{}(\\mathrm{allocated~to~Chernoff~bound}),}\\\\ {P(\\overline{{E_{2}}})\\leq\\frac{T}{2}}&{{}(\\mathrm{allocated~to~BN~convergence}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Chernoff Bound Analysis: For event $E_{1}$ , we require that the probability of class $c$ appearing in fewer than the expected number of batches is at most $\\textstyle{\\frac{T}{2}}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\nP\\left(M\\leq(1-\\delta)n q_{c}\\right)\\leq\\frac{T}{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Applying the Chernoff bound: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\exp\\left(-\\frac{\\delta^{2}n q_{c}}{2}\\right)\\leq\\frac{T}{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Solving for $n$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle-\\frac{\\delta^{2}n q_{c}}{2}\\leq\\ln\\left(\\frac{T}{2}\\right),}\\\\ {\\displaystyle n\\geq\\frac{-2\\ln\\left(\\frac{T}{2}\\right)}{\\delta^{2}q_{c}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "BN Convergence Requirement: For event $E_{2}$ , we require that the number of batches $M$ where class $c$ appears is sufficient for BN convergence: ", "page_idx": 14}, {"type": "equation", "text": "$$\nM\\geq M_{0}=\\frac{\\ln\\left(\\frac{C}{\\tau}\\right)}{\\varepsilon}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To ensure that this condition holds when event $E_{1}$ occurs, we use the fact that, with probability at least $\\textstyle1-{\\frac{T}{2}}$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\nM\\geq(1-\\delta)n q_{c}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, to guarantee $M\\geq M_{0}$ , we require: ", "page_idx": 15}, {"type": "equation", "text": "$$\n(1-\\delta)n q_{c}\\geq M_{0}=\\frac{\\ln\\left(\\frac{C}{\\tau}\\right)}{\\varepsilon}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Solving for $n$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nn\\geq\\frac{\\ln\\left(\\frac{C}{\\tau}\\right)}{(1-\\delta)\\varepsilon q_{c}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Final Combined Bound: To ensure that both conditions hold for all classes, we use $\\operatorname*{min}(q_{c})$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nn\\geq\\operatorname*{max}\\left(\\underbrace{-2\\ln\\left(\\frac{T}{2}\\right)}_{\\delta^{2}\\operatorname*{min}(q_{c})},\\quad\\underbrace{\\ln\\left(\\frac{C}{\\tau}\\right)}_{\\mathrm{BN\\,Convergence}}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\delta$ represents the acceptable relative deviation from the expected number of batches, $\\varepsilon$ is the momentum parameter in BN updates, $T$ denotes the acceptable total failure probability $(T=T_{1}{+}T_{2})$ , $\\tau$ is the convergence threshold for BN statistics, $C$ represents an upper bound on $\\left|{\\bf B}{\\bf N}_{c}^{t}-\\mu_{c}\\right|$ at initialization, and $\\operatorname*{min}(q_{c})$ represents the minimum probability that a class appears in a batch. ", "page_idx": 15}, {"type": "text", "text": "This bound ensures: ", "page_idx": 15}, {"type": "text", "text": "\u2022 With probability at least $\\textstyle1-{\\frac{T}{2}}$ , each class $c$ appears in at least $(1-\\delta)n q_{c}$ batches (event $E_{1}$ occurs).   \n\u2022 With probability at least $\\textstyle1-{\\frac{T}{2}}$ , the BN statistics for each class $c$ converge within tolerance $\\tau$ (event $E_{2}$ occurs).   \n\u2022 By the union bound, both events $E_{1}$ and $E_{2}$ occur simultaneously with probability at least $1-T$ . ", "page_idx": 15}, {"type": "text", "text": "B Dataset Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We perform experiments on the following three datasets: ", "page_idx": 15}, {"type": "text", "text": "\u2022 Tiny-ImageNet [33] is the subset of ImageNet-1K containing 500 images per class of a total of 200 classes, and spatial sizes of images are downsampled to $64\\times64$ . \u2022 ImageNet-1K [34] contains 1,000 classes and 1,281,167 images in total. The image sizes are resized to $224\\times224$ . \u2022 ImageNet-21K-P [35] is the pruned version of ImageNet-21K, containing 10,450 classes and 11,060,223 images in total. Images are sized to $224\\times224$ resolution. ", "page_idx": 15}, {"type": "text", "text": "C Hyperparameter Settings ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 ImageNet-1K ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table 10: Squeezing and class-wise BN statistics of ImageNet-1K ", "page_idx": 15}, {"type": "table", "img_path": "12A1RT1L87/tmp/10e0181ba5b4b7d6a99faa45f47dfbe534c633d86a692f7e10c8b482568a5fc2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "12A1RT1L87/tmp/29ece93ddab44b514a9d6ef278e6f86a1570a6aa8efe49d3c822a3f1f488d965.jpg", "table_caption": ["Table 11: Data Synthesis of ImageNet-1K "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "12A1RT1L87/tmp/cd8e89c704b1fff72a88137bc20d64593beb5eab9d7ccd8eaeff9887187781e5.jpg", "table_caption": ["Table 12: Relabel and Validation of ImageNet-1K "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "We use Pytorch pretrained ResNet-18 [36], with a Top-1 accuracy of $69.76\\%$ , as both the recovery and relabeling model. Class-wise BN statistics are computed using a modified version of the training script of the source provided in Table 10. The recovery, or data synthesis, phase is provided in Table 11, which follows CDA [7] except by changing the batch size to an IPC-dependent size. Relabel and validation processes share the same setting as provided in Table 12. ", "page_idx": 16}, {"type": "text", "text": "C.2 Tiny-ImageNet ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "12A1RT1L87/tmp/a3d9870598fde309399715611aa52e2619e50c89a420b375bfe9173f35fb2202.jpg", "table_caption": ["Table 13: Squeezing and class-wise BN statistics of Tiny-Imagenet. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "12A1RT1L87/tmp/05a89532c7f98fc66a7d1ab3cae4ad45596fa6f82fff1a556c8ca7a06049ae2c.jpg", "table_caption": ["Table 15: Relabel and Validation of Tiny-ImageNet. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Following $\\mathrm{SRe^{2}L}$ and CDA [7], we use a modified version of ResNet-18 [41] for Tiny-ImageNet. We modify the training script from Table 13 to compute class-wise BN statistics. The pretrained model has a Top-1 accuracy of $59.47\\%$ , and the model is used for data synthesis and relabel/validation as shown in Table 14 and Table 15, respectively. Note that for the validation phase, a warm-up of 5 epochs is added with a different learning rate scheduler (i.e., linear). ", "page_idx": 17}, {"type": "text", "text": "C.3 ImageNet-21K-P ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "12A1RT1L87/tmp/d1568f928b190462260dfc2eda88e72301cbe1876fd3457f207908d2e7c9e963.jpg", "table_caption": ["Table 16: Squeezing and class-wise BN statistics of Imagenet-21K-P. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Following CDA [7], we use ResNet-18 trained for 80 epochs initialized with well-trained ImageNet1K weight [35]. Class-wise BN statistics are computed using a modified version of the training script of the source provided in Table 16. The pretrained ResNet-18 on ImageNet-21K-P has a Top-1 accuracy of $38.1\\%$ , and the model is used for data synthesis and relabel/validation as shown in Table 17 and Table 18, respectively. Note that CutMix used in ImageNet-1K is replaced with CutOut [42], and a relatively large label smooth of 0.2 is used during the ImageNet-21K-P pretraining phase. We incorporate the same changes to the relabel/validation phase of the synthetic dataset. ", "page_idx": 17}, {"type": "text", "text": "C.4 Implementation of Baselines ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "RDED. RDED [8] has several different changes to the $\\mathrm{SRe^{2}L}$ settings. (1) The batch size is adjusted according to the IPC size (i.e., 100 for IPC10 and 200 for IPC50). (2) It uses additional augmentation (i.e., ShufflePatch to shuffle the position of patches). Such augmentations are considered additional storage since the exact order of patch shuffling needs to be stored. (3) Weaker augmentation (i.e., a larger lower bound for the random area of the resized crop). (4) A smoothed learning rate scheduler. We adhere to all the changes for experiments regarding RDED. ", "page_idx": 18}, {"type": "text", "text": "G-VBSM. In Table 9, we adopt the several techniques used for G-VBSM [9]. (1) Soft labels are generated with an ensemble of models. Specifically, we use ResNet18 [36], MobileNetV2 [39], EfficientNet-B0 [43], ShuffleNetV2-0.5 [44]. (2) Logit Normalization is used to keep the same label storage. (3) A different $\\mathrm{MSE+\\gamma\\times}$ GT loss replaces KL divergence, where $\\gamma=0.1$ . ", "page_idx": 18}, {"type": "text", "text": "D Additional Experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Ablation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Table 19: Ablation study of the proposed method. C denotes using class-wise matching. CS denotes suing class-wise supervision. ILP denotes using an improved label pool. (IPC50, ResNet18, ImageNet-1K). ", "page_idx": 18}, {"type": "table", "img_path": "12A1RT1L87/tmp/a8e8f037d0317c4a4609fccd26b819520d03d8e1b1e868e274b1c5dd569b9bae.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 19 presents an expanded version of Table 4. The row highlighted in grey outlines the ablation study on class-wise supervision, demonstrating that the ILP component (Improved Label Pool) enhances performance independently of class-wise supervision. ", "page_idx": 18}, {"type": "text", "text": "D.2 Scaling on Large IPCs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Table 20: Experiment on the scalability of large IPCs. T denotes the total storage of images and labels, and storage is measured in GB. The validation model is ResNet18. ", "page_idx": 18}, {"type": "table", "img_path": "12A1RT1L87/tmp/0d631aedd63fb692435a32a13e46b54d2aed23a1d49b86aa0585c2f5e8f98fc2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 20 demonstrates that our method exhibits commendable scalability across large IPCs. We observe non-marginal enhancements when deploying even larger IPCs, such as IPC300 and IPC400. Moreover, our approach achieves nearly identical accuracy levels \u2014 specifically, $65.3\\%$ vs. $65.2\\%$ \u2014 when comparing the use of IPC300 at $1\\times$ with $\\mathrm{IPC400}$ at $30\\times$ less labels. Compared to the full ImageNet-1K dataset, our method preserves a large portion of the accuracy with $10\\times$ less storage. This performance is achieved despite the vastly different storage requirements of 178G and 13G, respectively, indicating a higher flexibility of IPC choice with a fixed storage budget. ", "page_idx": 18}, {"type": "text", "text": "D.3 Comparison with Fast Knowledge Distillation [25] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The label quantization technique mentioned in Fast Knowledge Distillation (FKD) [25] is orthogonal to the proposed method for several reasons. Firstly, as demonstrated in Table 21, there are six components related to soft labels. FKD only compresses the prediction logits (component 6), while the our method addresses all six components. ", "page_idx": 18}, {"type": "text", "text": "Secondly, even for the overlapping storage component (component 6: prediction logits), the compression targets differ between FKD and our method, as shown in Table 22. The total stored prediction logits can be approximated by the formula: number_of_condensed_images $\\times$ number_of_augmentations $\\times$ dimension_of_logits. FKD\u2019s label quantization focuses on compressing the dimension_of_logits, whereas the proposed label pruning method focuses on compressing the number_of_augmentations. ", "page_idx": 19}, {"type": "table", "img_path": "12A1RT1L87/tmp/520f70c8ac5f56d3173e3059dd5076f4d1675ec4f767678b4cdbb95b521323f0.jpg", "table_caption": ["Table 21: Different storage components between FKD and the proposed method. FKD, originally for model distillation, requires storage only for components 1, 2, and 6. Adapting it to dataset distillation requires additional storage for components 3, 4, and 5. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "12A1RT1L87/tmp/0ad3016056e07d1bc2b400d1f687099a059b10ae265af4f0261675165280dc67.jpg", "table_caption": ["Table 22: Breakdown explanation for component 6 (prediction logits) storage between FKD\u2019s label quantization and the proposed label pruning. The number of condensed images is computed by $\\mathbf{N}=$ IPC $\\times$ number_of_classes. FKD\u2019s compression target is dimension_of_logits, while the proposed method\u2019s target is number_of_augmentations. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Although FKD\u2019s approach is orthogonal to our method, a comparative analysis was conducted to better understand their relative performance. Table 23 presents a detailed comparison between FKD\u2019s two label quantization strategies (Marginal Smoothing and Marginal Re-Norm) and the proposed method. It is important to note that FKD only compresses component 6, with the compression rate related to hyper-parameter $K$ . Components 1-5 remain uncompressed ( $1\\times$ rate) in FKD. Additionally, FKD\u2019s quantized logits store both values and indices, so their actual storage is doubled, and their compression rate is halved. ", "page_idx": 19}, {"type": "text", "text": "This analysis has yielded two key observations. First, our method demonstrates higher accuracy at comparable compression rates. For IPC10, our method achieves $32.70\\%$ accuracy at $10\\times$ compression, while FKD only reaches $18.10\\%$ at $8.2\\times$ compression. Second, our method exhibits better compression at similar accuracy levels. On IPC10, our method attains $20.20\\%$ accuracy at $40\\times$ compression, whereas FKD achieves $19.04\\%$ at just $4.5\\mathrm{x}$ compression. ", "page_idx": 19}, {"type": "table", "img_path": "12A1RT1L87/tmp/7abd16db9e599e1aa066c07a7677ea650368d3d832efbe10d8b7e8cf9fe02142.jpg", "table_caption": ["Table 23: Comparison between FKD\u2019s two label quantization strategies (Marginal Smoothing and Marginal Re-Norm) and ours. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Additional Information ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "E.1 Label Pruning Metrics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We determine labels according to the statistics of the auxiliary information: ", "page_idx": 20}, {"type": "text", "text": "1. correct: the number of correctly classified images [31]   \n2. diff: the absolute difference between the Top-2 outputs   \n3. signed_diff: the signed difference between Top-2 output [45]   \n4. cut_ratio: the cut-mix ratio   \n5. confidence: the value of the largest output [26]. ", "page_idx": 20}, {"type": "text", "text": "These metrics serve for the baselines compared to random label pruning in Table 5 After knowing the metric, knowing which data type to prune (i.e., \u201ceasy\u201d, \u201chard\u201d, or \u201cuniform\") is important. Additionally, FerKD [26] argues the reliability of generated soft labels and proposes to use neither too easy nor too hard samples. ", "page_idx": 20}, {"type": "text", "text": "E.2 Image and Label Storage ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Table 24: Image and label storage. I denotes image storage. L denotes label storage. \u201cRatio\u201d is label-to-image ratio. ", "page_idx": 20}, {"type": "table", "img_path": "12A1RT1L87/tmp/832a01330f55b6bb1d81317b26d23d70b566c96b2bf5e41b883030dc11689bd5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table. 24 shows that stored labels are more than $10\\times$ , $30\\times$ , and $200\\times$ sized of the image storage, depending on the number of classes of the dataset. ", "page_idx": 20}, {"type": "text", "text": "E.3 Theoretical Analysis on the Number of Updates ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Our experiments are grounded in a careful analysis of the number of updates required for stable Batch Normalization (BN) statistics. We begin by examining the derived bound from Eq. 8: ", "page_idx": 20}, {"type": "equation", "text": "$$\nn\\geq\\operatorname*{max}\\left(\\underbrace{-2\\ln\\left(\\frac{T}{2}\\right)}_{\\delta^{2}\\operatorname*{min}(q_{c})},\\quad\\underbrace{\\ln\\left(\\frac{C}{\\tau}\\right)}_{\\mathrm{BN\\,Convergence}}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "To evaluate this bound, we substitute the following values: ", "page_idx": 20}, {"type": "text", "text": "\u2022 $T=0.05$ (acceptable total failure probability, corresponding to $95\\%$ confidence)   \n\u2022 $\\delta=0.2$ (acceptable relative deviation from the expected number of batches)   \n\u2022 $\\varepsilon=0.1$ (momentum parameter in BN) $\\operatorname*{min}(p_{c})=\\frac{732}{1,281,167}\\approx0.0005711$ (ratio of the least number of images in a class to total images)   \n\u2022 $B=256$ (batch size)   \n\u2022 $\\operatorname*{min}(q_{c})=1-(1-\\operatorname*{min}(p_{c}))^{B}$ (minimum probability that any class appears in a batch) ", "page_idx": 20}, {"type": "text", "text": "First, we compute $\\operatorname*{min}(q_{c})$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\operatorname*{min}(q_{c})=1-(1-\\operatorname*{min}(p_{c}))^{B}}\\\\ &{=1-(1-0.0005711)^{256}}\\\\ &{=1-(0.9994289)^{256}}\\\\ &{\\approx1-e^{-256\\times0.0005711}\\quad({\\mathrm{since~min}}(p_{c}){\\mathrm{~is~small}})}\\\\ &{=1-e^{-0.1462}}\\\\ &{\\approx1-0.8639=0.1361.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, $\\operatorname*{min}(q_{c})\\approx0.1361$ . ", "page_idx": 21}, {"type": "text", "text": "Next, we compute the two parts of the bound separately. ", "page_idx": 21}, {"type": "text", "text": "From Chernoff Bound Term: Given that we allocate the total failure probability $T$ equally between the two events, we have $T/2=0.025$ . ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\qquad-2\\ln\\left({\\frac{T}{2}}\\right)}\\\\ &{\\qquad\\geq{\\frac{7}{\\delta^{2}\\operatorname*{min}(q_{c})}}}\\\\ &{\\qquad={\\frac{-2\\ln(0.025)}{(0.2)^{2}\\times0.1361}}}\\\\ &{\\qquad={\\frac{-2\\times\\left(-3.6889\\right)}{0.04\\times0.1361}}{\\quad\\mathrm{(since~ln}(0.025)=-3.6889)}}\\\\ &{\\qquad={\\frac{7.3778}{0.005444}}}\\\\ &{\\qquad\\approx1.355.2.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From BN Convergence Term: We need to specify $C$ and $\\tau$ . Let\u2019s assume: ", "page_idx": 21}, {"type": "text", "text": "\u2022 $C=1$ (an upper bound on $|{\\bf B}{\\bf N}_{c}^{t}-\\mu_{c}|$ at initialization, as the running mean is typically   \ninitialized to zero)   \n\u2022 $\\tau=0.01$ (desired convergence tolerance) ", "page_idx": 21}, {"type": "text", "text": "Compute the numerator: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\ln\\left({\\frac{C}{\\tau}}\\right)=\\ln\\left({\\frac{1}{0.01}}\\right)=\\ln(100)=4.6052.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now, compute the denominator: ", "page_idx": 21}, {"type": "equation", "text": "$$\n(1-\\delta)\\varepsilon\\operatorname*{min}(q_{c})=(1-0.2)\\times0.1\\times0.1361=0.8\\times0.1\\times0.1361=0.010888.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Compute the second part: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle{\\ln{\\left(\\frac{C}{\\tau}\\right)}}}\\\\ {\\displaystyle{n\\geq\\frac{\\ln{\\left(\\frac{C}{\\tau}\\right)}}{(1-\\delta)\\varepsilon\\operatorname*{min}(q_{c})}}}\\\\ {\\displaystyle{=\\frac{4.6052}{0.010888}}}\\\\ {\\displaystyle{\\approx423.08.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Final Bound: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "$n\\geq\\operatorname*{max}\\left(1,355.2,\\,423.08\\right)=1,355.2\\approx1,356$ (rounding up to the nearest whole number). ", "page_idx": 21}, {"type": "text", "text": "This theoretical result indicates that approximately 1, 356 batches are needed for stable BN statistics with the specified parameters. ", "page_idx": 22}, {"type": "text", "text": "Practical Implications: This observation leads to a key insight: pretrained models have already undergone sufficient updates to achieve stable BN statistics. Specifically, in the context of ImageNet1K: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{Updates\\per\\epoch}=\\frac{1,281,167}{256}\\approx5,005\\,\\mathrm{updates}>1,356.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since one epoch consists of approximately 5, 005 updates, which is substantially more than the theoretical requirement of 1, 356 batches, we can confirm that a single epoch of training is sufficient for the BN statistics of each class to converge within the desired tolerance with high probability. ", "page_idx": 22}, {"type": "text", "text": "E.4 Class-wise Statistics Storage ", "text_level": 1, "page_idx": 22}, {"type": "table", "img_path": "12A1RT1L87/tmp/d27d9ec6b2e36de61beecdbdd975c7aba31d7018fe7ec597b59e0d978a786cef.jpg", "table_caption": ["Table 25: Additional storage required for class-wise statistics. The model is ResNet-18, and storage is measured in MB. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "The additional storage allocation for class-specific statistics is detailed in Table 25. It is observed that this storage requirement escalates with an increase in the number of classes. However, this is a one-time necessity during the recovery phase and becomes redundant once the synthetic data generation is completed. ", "page_idx": 22}, {"type": "text", "text": "E.5 Computing Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Experiments are performed on 4 A100 80G GPU cards. We notice that for Tiny-ImageNet, there is a slight performance drop when multiple GPU cards are used with DataParallel in PyTorch. Therefore, we use 4 GPU cards for ImageNet-1K and ImageNet-21K-P experiments and 1 GPU card for all Tiny-ImageNet experiments. ", "page_idx": 22}, {"type": "text", "text": "E.6 Limitation and Future Work ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We recognize that there are several limitations and potential areas for further investigation. Firstly, while our work significantly reduces the required storage, the process for generating the soft labels is still necessary, as we randomly select from this label space. Secondly, reducing the required labels may not directly lead to a reduced training speed, as the total training epochs remain the same in order to achieve the best performance. Future work is warranted to reduce label storage as well as the required training budget simultaneously. ", "page_idx": 22}, {"type": "text", "text": "E.7 Ethics Statement and Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Our research study focuses on dataset distillation, which aims to preserve data privacy and reduce computing costs by generating small synthetic datasets that have no direct connection to real datasets. However, this approach does not usually generate datasets with the same level of accuracy as the full datasets. ", "page_idx": 22}, {"type": "text", "text": "In addition, our work in reducing the size of soft labels and enhancing image diversity can have a positive impact on the field by making large-scale dataset distillation more efficient, thereby reducing storage and computational requirements. This efficiency can facilitate broader access to advanced machine learning techniques, potentially fostering innovation across diverse sectors. ", "page_idx": 22}, {"type": "text", "text": "F Visualization ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we present visualizations of the datasets used in our experiments. Due to the different matching objectives, datasets of different IPCs should have distinct images. Therefore, we provide the visualization of different IPCs. Figure 6 shows randomly sampled images from ImageNet-1K at various IPC. Figure 7 depicts the Tiny-ImageNet dataset with IPC50 and IPC100. Figure 8 provides visualizations of ImageNet-21K-P at IPC10 and IPC20. ", "page_idx": 23}, {"type": "text", "text": "F.1 ImageNet-1K ", "text_level": 1, "page_idx": 23}, {"type": "image", "img_path": "12A1RT1L87/tmp/0eedad94bfc88dc7a8b727738376925019344938294d8013af1da63c009b92d4.jpg", "img_caption": ["Figure 6: Visualization of ImageNet-1K. Images are randomly sampled. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "F.2 Tiny-ImageNet ", "text_level": 1, "page_idx": 24}, {"type": "image", "img_path": "12A1RT1L87/tmp/457aae085ec84783313e2e1817b7e47fa771fa3ed70b2352aa7f2a4042a90b4a.jpg", "img_caption": ["Figure 7: Visualization of Tiny-ImageNet. Images are randomly sampled. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "F.3 ImageNet-21K-P ", "text_level": 1, "page_idx": 24}, {"type": "image", "img_path": "12A1RT1L87/tmp/dba9adab2f632e36213d8941f50599d2a26f05c5370c131518e7def376b47f2c.jpg", "img_caption": ["Figure 8: Visualization of ImageNet-21K-P. Images are randomly sampled. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 25}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 25}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 25}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 25}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 25}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 25}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 26}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 26}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 27}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: [TODO] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: [TODO] ", "page_idx": 29}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] Justification: [TODO] ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] Justification: [TODO] ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]