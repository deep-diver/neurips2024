[{"figure_path": "PquRXu9pQ6/figures/figures_3_1.jpg", "caption": "Figure 1: The pipeline of Ex-MCR. (a) We extend leaf spaces to base space via the overlapping modalities. The base space is frozen and the leaf spaces are aligned to the base space via projectors. (b) When extending the audio-text space to the text-image space, we iteratively use texts, audio, and images as queries to retrieve and aggregate the corresponding semantically consistent embeddings. The pseudo embedding pairs generated from different modality data are shuffled together to build the final various modality-centric data pool.", "description": "This figure illustrates the architecture of Ex-MCR, a method for extending multi-modal contrastive representations.  Panel (a) shows how leaf spaces (pre-trained models for specific modalities) are extended into a base space (a pre-trained model for a set of overlapping modalities) using projectors and preserving alignment. The base space is kept frozen during training. Panel (b) details the various modality-centric aggregating process, which involves iteratively using modalities as queries to find and aggregate semantically similar embeddings from other modalities to create a comprehensive data pool for training.", "section": "3.2 Enhancing Alignment Learning Pipeline"}, {"figure_path": "PquRXu9pQ6/figures/figures_8_1.jpg", "caption": "Figure 1: The pipeline of Ex-MCR. (a) We extend leaf spaces to base space via the overlapping modalities. The base space is frozen and the leaf spaces are aligned to the base space via projectors. (b) When extending the audio-text space to the text-image space, we iteratively use texts, audio, and images as queries to retrieve and aggregate the corresponding semantically consistent embeddings. The pseudo embedding pairs generated from different modality data are shuffled together to build the final various modality-centric data pool.", "description": "This figure illustrates the Ex-MCR pipeline.  Panel (a) shows how leaf spaces (containing modalities not present in the base space) are extended into the base space via an overlapping modality.  The base space remains frozen. Panel (b) details the method used when extending the audio-text space, showing the iterative querying of texts, audio, and images to generate semantically consistent embeddings, and how these are combined to create the modality-centric data pool.", "section": "3 Extending Multi-modal Contrastive Representations"}, {"figure_path": "PquRXu9pQ6/figures/figures_8_2.jpg", "caption": "Figure 1: The pipeline of Ex-MCR. (a) We extend leaf spaces to base space via the overlapping modalities. The base space is frozen and the leaf spaces are aligned to the base space via projectors. (b) When extending the audio-text space to the text-image space, we iteratively use texts, audio, and images as queries to retrieve and aggregate the corresponding semantically consistent embeddings. The pseudo embedding pairs generated from different modality data are shuffled together to build the final various modality-centric data pool.", "description": "This figure illustrates the Ex-MCR pipeline.  Panel (a) shows how leaf spaces (containing modalities not shared between the leaf and base spaces) are extended into a base space (a pre-trained model like CLIP) through an overlapping modality.  Panel (b) details the 'Various Modality-centric Aggregating' strategy, where audio, text, and image data are used iteratively as queries to find related embeddings from other modalities to improve alignment and build a comprehensive representation.", "section": "3.2 Enhancing Alignment Learning Pipeline"}]