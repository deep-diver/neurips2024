[{"type": "text", "text": "Wasserstein Distributionally Robust Optimization Through the Lens of Structural Causal Models and Individual Fairness ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ahmad-Reza Ehyaei Max Planck Institute for Intelligent Systems, T\u00fcbingen AI Center, Germany ahmad.ehyaei@tuebingen.mpg.de ", "page_idx": 0}, {"type": "text", "text": "Golnoosh Farnadi\u2217 Mila Qu\u00e9bec AI Institute ; McGill University, Montr\u00e9al, Canada farnadig@mila.quebec ", "page_idx": 0}, {"type": "text", "text": "Samira Samadi Max Planck Institute for Intelligent Systems, T\u00fcbingen AI Center, Germany ssamadi@tuebingen.mpg.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, Wasserstein Distributionally Robust Optimization (DRO) has garnered substantial interest for its efficacy in data-driven decision-making under distributional uncertainty. However, limited research has explored the application of DRO to address individual fairness concerns, particularly when considering causal structures and sensitive attributes in learning problems. To address this gap, we first formulate the DRO problem from causality and individual fairness perspectives. We then present the DRO dual formulation as an efficient tool to convert the DRO problem into a more tractable and computationally efficient form. Next, we characterize the closed form of the approximate worst-case loss quantity as a regularizer, eliminating the max-step in the min-max DRO problem. We further estimate the regularizer in more general cases and explore the relationship between DRO and classical robust optimization. Finally, by removing the assumption of a known structural causal model, we provide finite sample error bounds when designing DRO with empirical distributions and estimated causal structures to ensure efficiency and robust learning. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine learning models must address discrimination because they often reflect and amplify biases present in their training datasets [31]. These biases can significantly influence decisions in domains such as healthcare [30], education [3], recruitment [18], and lending services [6]. Consequently, these decisions disproportionately affect individuals based on sensitive attributes like race or gender, perpetuating systemic discrimination. ", "page_idx": 0}, {"type": "text", "text": "To address and quantify unfairness, researchers have developed concepts like group fairness and individual fairness [45, 4]. Group fairness aims to achieve equitable outcomes across demographic groups, while individual fairness ensures that similar individuals receive similar treatment. Formally, with $\\nu$ as the feature space and $\\boldsymbol{\\wp}$ as the label space, a model $h:\\mathcal{V}\\to\\mathcal{V}$ ensures individual fairness ", "page_idx": 0}, {"type": "text", "text": "if it satisfies the condition in [20]: ", "page_idx": 1}, {"type": "equation", "text": "$$\nd{y}(h(v),h(v^{\\prime}))\\leq L d{\\nu}(v,v^{\\prime})\\quad{\\mathrm{for~all~}}v,v^{\\prime}\\in\\mathcal{V},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $d_{\\nu}$ and $d{\\boldsymbol{y}}$ are dissimilarity functions, often referred to as fair metrics on the input and output spaces. These functions capture the proximity of individuals and $L\\in\\mathbb{R}^{+}$ is a Lipschitz constant. The metric $d_{\\nu}$ reflects the intuition about which instances should be considered similar by the model. ", "page_idx": 1}, {"type": "text", "text": "Due to challenges in defining such metrics, group fairness is often prioritized in fairness literature because it more straightforwardly addresses observable disparities among distinct groups, making measurement and implementation easier in practice [8]. Therefore, it is crucial to study and formulate individual fairness under different assumptions in machine learning. ", "page_idx": 1}, {"type": "text", "text": "Individual fairness can be achieved through robust optimization methods such as Wasserstein DRO, which has gained significant attention for its applications in learning and decision-making [55, 43]. DRO incorporates a regularization term to mitigate overfitting [17, 26, 59]. By using a fair metric as the transportation cost function in computing the Wasserstein distance, models are designed to deliver consistent performance across varied data distributions, ensuring similar individuals receive comparable outcomes, thus satisfying individual fairness. ", "page_idx": 1}, {"type": "text", "text": "Incorporating causal structures and sensitive attributes into data models complicates using an individual fair metric as a cost function within the DRO framework. The fair metric must account for perturbations in sensitive attributes based on counterfactuals to ensure counterfactual fairness [22]. This can violate the positive-definite property, where $d(v,v^{\\prime})=0$ implies $v=v^{\\prime}$ , a key assumption in many DRO theorems [55, 43]. ", "page_idx": 1}, {"type": "text", "text": "Although previous works [42, 67, 70, 69, 57] have attempted to apply DRO to address individual fairness, they often do not explore the implications when causal structures and sensitive attributes are present in the learning problem. These studies are typically limited to linear Structural Causal Model (SCM) with specific metrics and do not discuss the form of the regularizer for other classical DRO theorems when using a fair metric. To accurately compare our work with related studies, we will postpone this discussion until after presenting our results in Section 4.1. ", "page_idx": 1}, {"type": "text", "text": "1.1 Our Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this work, we adopt the definition of a fair metric from [22] to define a Causally Fair Dissimilarity Function (CFDF), which delineates how to establish a fair metric through causality and sensitive attributes. Using CFDF, we introduce Causally Fair DRO and present a strong duality theorem for our approach. Under mild assumptions about CFDF and causal structure, we demonstrate that the DRO regularizer can be estimated, or in some cases can be explicitly solved. This estimation often leads to being more practical and computationally efficient than solving the min-max problem in (4), as supported by advancements in algorithms from previous research such as [14, 15]. Finally, Our numerical analysis of both real and synthetic data demonstrates the practicality of our theoretical framework in real-world applications. (\u00a7 5). In summary, the main contributions of this work are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Define a causally fair dissimilarity function, an individual fair metric incorporating causal structures and sensitive attributes (Def. 1), along with its representation form (Prop. 1). \u2022 Define a causally fair DRO problem with a causally fair dissimilarity function cost $(\\S\\,4)$ . \u2022 Present the strong duality theorem for causally fair DRO (Thm. 1). \u2022 Provide the exact regularizer for linear SCM under mild conditions for the loss function in regression and classification problems (Thm. 2 and Thm. 3). \u2022 Estimate the first-order causally fair DRO regularizer for non-linear SCM (Thm. 4). \u2022 Provide the relation between classical robust optimization and causally fair DRO (Prop. 2). \u2022 Demonstrate that under unknown SCM assumptions, by estimating the SCM or cost function, we have finite sample guarantees for convergence of empirical DRO problems (Thm. 5). ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries & Notations ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Data Model. Let $\\textbf{V}\\in\\mathcal{V}$ denote a vector of feature space (predictor variables) and let $\\mathbf Y\\in\\mathcal D$ represent the response variable, such that $\\mathbf{Z}=(\\mathbf{V},\\mathbf{Y})$ comprises the observation variables with an underlying probability $\\mathbb{P}_{*}$ . Furthermore, assume that the feature vector $\\mathbf{V}=(\\mathbf{A},\\mathbf{X})$ comprises both sensitive attributes $\\mathbf A\\in\\mathcal A$ and non-sensitive attributes $\\mathbf{X}\\in\\mathcal{X}$ . Let $\\{z^{i}=^{\\dot{}}(v^{i},y^{\\dot{\\iota}})\\}_{i=1}^{N}$ represent the observations used to construct the empirical distribution $\\mathbb{P}_{N}$ , defined as $\\begin{array}{r}{\\mathbb{P}_{N_{\\!}}:={\\frac{1}{N}}\\sum_{i=1}^{N}\\delta_{z^{i}},}\\end{array}$ where $\\delta_{z}$ is the Dirac delta function. Given a loss function $\\ell:\\mathcal{Z}\\times\\Theta\\to\\mathbb{R}$ , the risk function for a parameter $\\theta\\in\\Theta$ and a probability measure $\\mathbb{P}$ is $\\mathcal{R}(\\mathbb{P},\\theta)=\\mathbb{E}_{\\mathbb{P}}\\left[\\ell(Z,\\theta)\\right]$ . This leads to the common empirical risk minimization approach. This method seeks to find the minimizer $\\theta_{N}^{\\,\\mathrm{erm}}$ within the set $\\theta_{N}^{\\mathrm{\\,erm}}\\in\\arg\\operatorname*{min}_{\\theta\\in\\Theta}\\mathcal{R}(\\mathbb{P}_{N},\\theta)$ , as an empirical way to obtaining the optimal solution $\\theta_{*}$ , which is given by $\\theta_{*}=\\operatorname*{inf}_{\\theta\\in\\Theta}\\mathcal{R}(\\mathbb{P}_{*},\\theta)$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Assume the feature space is represented by a structural causal model (SCM) $\\mathcal{M}\\quad=$ $\\langle\\mathcal{G},\\mathbf{V},\\mathbf{U},\\mathbb{P}_{\\mathbf{U}}\\rangle$ [51]. This model includes structural equations $\\{\\mathbf{V}_{i}:=f_{i}(\\mathbf{V}_{\\mathrm{Pa}(i)},\\mathbf{U}_{i})\\}_{i=1}^{n}$ , which delineate the causal relations among an endogenous variable $\\mathbf{V}_{i}$ , its causal predecessors $\\mathbf{V}_{\\mathrm{Pa}(i)}$ , and an exogenous variable $\\mathbf{U}_{i}$ representing unobservable factors. The model\u2019s structure is encapsulated in a directed acyclic graph $\\mathcal{G}$ . Exogenous variables are posited as mutually independent, enabling $\\mathbb{P}_{\\mathbf{U}}$ to be expressed as $\\bar{\\prod}_{i=1}^{n}\\bar{\\mathbb{P}}\\mathbf{U}_{i}$ , assuming causal sufficiency and excluding hidden confounders [53]. ", "page_idx": 2}, {"type": "text", "text": "Counterfactuals. In causal structures, data perturbation is achieved through counterfactuals, which are derived from interventions in SCMs. These interventions, conducted using $d o$ -calculus, include both hard and soft types [51]. Hard interventions fix a subset $\\mathcal{T}\\subseteq\\{1,\\ldots,\\bar{n}\\}$ of features $\\mathbf{V}_{\\mathcal{T}}$ to a constant $\\tau$ , modifying their causal connections within the causal graph while maintaining the structural equations of other features [51]. This type of intervention is denoted as $\\mathcal{M}^{d o(\\mathbf{V}_{\\mathcal{Z}}:=\\tau)}$ and its structural equations are obtained by: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\{\\mathbf{V}_{i}:=\\tau_{i},\\quad\\forall i\\in\\mathbb{Z};\\quad\\mathbf{V}_{i}:=f_{i}(\\mathbf{V_{Pa}}_{(i)},\\mathbf{U}_{i}),\\quad\\forall i\\notin\\mathbb{Z}\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Soft interventions, on the other hand, adjust the functions in the structural equations, such as through additive interventions, without disrupting existing causal links [53]. In an additive (or shift) intervention, a value $\\Delta\\in\\mathbb{R}^{n}$ is added to each feature within the SCM to enact manipulation: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\{\\mathbf{V}_{i}:=f_{i}(\\mathbf{V_{Pa}}_{(i)},\\mathbf{U}_{i})+\\Delta_{i}\\}_{i=1}^{n}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In SCMs, counterfactuals are computed by modifying structural equations to reflect hard interventions on specific variables, thus exploring what would occur if the intervention was applied. Under the assumption of acyclicity, a unique function $F:\\mathcal{U}\\to\\mathcal{V}$ exists such that $F(u)=v$ . Acyclicity remains unchanged by either hard or shift interventions, allowing for the existence of modified functions $F^{d o(\\mathbf{V}_{\\mathcal{T}}:=\\tau)}$ and $F^{d o(\\mathbf{V}_{\\mathbb{Z}}+=\\Delta)}$ corresponding to these interventions, respectively. The counterfactual outcome for a hard intervention can thus be calculated using $\\mathbf{CF}(v,\\tau)=F^{d o(\\mathbf{V}_{\\mathcal{Z}:=\\tau})}(F^{-1}(v))$ , and similarly, for a shift intervention, it is defined as $\\mathbf{CF}(v,\\Delta)$ . These interventions are frequently applied in this analysis. ", "page_idx": 2}, {"type": "text", "text": "Counterfactuals involving the modification of sensitive attributes (termed twins) are essential for addressing individual-level fairness [40, 64]. Twins are generated by altering the sensitive attribute from $a$ to $a^{\\prime}$ across its domain $\\boldsymbol{\\mathcal{A}}$ . For any instance, $v\\in\\mathcal{V}$ , a set of counterfactual twins is produced as $\\{\\ddot{v}_{a}\\,=\\,\\mathrm{CF}(v,a)\\,:\\,a\\,\\in\\,{\\cal A}\\}$ , facilitating the analysis of fairness by comparing outcomes under different sensitive attribute values. ", "page_idx": 2}, {"type": "text", "text": "Counterfactual Identifiability. To estimate the effects of interventions from observational data, counterfactuals must be identifiable within a causal framework. A notable example of such identifiable SCMs is the additive noise models (ANMs), which suggest that structural equations can be represented as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\{\\mathbf{V}_{i}:=f_{i}(\\mathbf{V_{Pa}}_{(i)})+\\mathbf{U}_{i}\\}_{i=1}^{n}\\implies\\mathbf{U}=(I-f)(\\mathbf{V})\\implies\\quad\\mathbf{V}=(I-f)^{-1}(\\mathbf{U})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "leading to a bijective mapping between $U_{i}$ and $V_{i}$ , ensuring no loss of information from exogenous to endogenous variables[50]. This relationship implies that $\\mathbf{V}$ can be derived from $\\mathbf{U}$ through a bijective reduced-form mapping $F=(I-f)^{-1}$ , where $I(x)=x$ is the identity function. Besides ANM, there are other counterfactually identifiable models such as LSNM [34] and PNL [71]. However, for the sake of simplicity, our focus remains on ANM. Linear SCMs is a specific instance of ANMs, characterized by linear functions $f_{i}$ . ", "page_idx": 2}, {"type": "text", "text": "Individual Fairness Through Robustness. In machine learning, individual fairness [20] is achieved through robustness by ensuring that similar individuals receive similar outcomes, regardless of variations in their inputs. This concept aligns with the notion of Lipschitz continuity in decision functions (Eq. 1), where small changes in input should not lead to excessively large changes in output. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Depending on how the uncertainty set is defined, various types of robust optimization can be employed. In adversarially robust optimization [44, 7], the uncertainty set is defined by introducing a slight perturbation $\\delta$ based on the metric $d$ to the input data. The goal is to find the optimal $\\theta$ that minimizes risk even under the worst-case perturbation quantity: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\delta}^{a d v}(\\mathbb{P},\\theta)=\\underset{v\\sim\\mathbb{P}}{\\mathbb{E}}\\left[\\operatorname*{sup}_{d^{p}(v,v+\\Delta)\\leq\\delta}\\ell(v+\\Delta,y,\\theta)\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $p\\in[0,\\infty]$ . This formulation ensures that the optimization considers the maximum potential loss within the defined perturbation bounds. ", "page_idx": 3}, {"type": "text", "text": "In counterfactually robust optimization [40, 37, 64, 23, 24], the uncertainty set is generated by twins, which are obtained by creating counterfactuals concerning all levels of the sensitive attribute. In this scenario, the worst-case loss quantity is obtained by calculating the maximum loss over the twins of the input data: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\delta}^{c f}(\\mathbb{P},\\theta)=\\underset{v\\sim\\mathbb{P}}{\\mathbb{E}}\\left[\\operatorname*{sup}_{a\\in\\mathcal{A}}\\ell(\\ddot{v}_{a},y,\\theta)\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Distributionally Robust Optimization [43, 55] is a data-driven approach designed to minimize the discrepancies between in-sample and out-of-sample expected losses, using ambiguity sets based on Wasserstein distances. Consider a lower semi-continuous cost function $c(\\cdot,\\cdot):\\mathcal{Z}\\stackrel{-}{\\times}\\mathcal{\\bar{Z}}\\rightarrow[0,\\infty]$ that satisfies $c(z,z)=0$ for all $z\\in{\\mathcal{Z}}$ , serving as a fair metric. The optimal transport cost between two distributions $\\mathbb{P},\\mathbb{Q}\\in\\mathcal{P}(\\mathcal{Z})$ , is represented by: ", "page_idx": 3}, {"type": "equation", "text": "$$\nW_{c,p}\\left(\\mathbb{P},\\mathbb{Q}\\right)\\triangleq\\operatorname*{min}_{\\pi\\in\\mathcal{P}(\\mathcal{Z}\\times\\mathcal{Z})}\\left\\{\\left({\\mathbbsmall\\left(}\\mathbb{E}_{(z,z^{\\prime})\\sim\\pi}\\left[c^{p}(z,z^{\\prime})\\right]\\right)^{\\frac{1}{p}}:\\pi_{1}=\\mathbb{P},\\pi_{2}=\\mathbb{Q}\\right\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $\\pi\\in{\\mathcal{P}}({\\mathcal{Z}}\\times{\\mathcal{Z}})$ denotes the set of all joint probability distributions, and $\\pi_{1}$ and $\\pi_{2}$ are the marginals of $\\pi$ under first and second coordinates [54, 63]. When $c(z,z^{\\prime})$ acts as a metric (in mathematics term) on $\\mathcal{Z}$ , $W_{c,p}$ is called the Wasserstein distance [63]. ", "page_idx": 3}, {"type": "text", "text": "An important ingredient in the DRO formulation is the description of the distributional uncertainty region $\\mathbb{B}_{\\delta}(\\mathbb{P})$ that is defined by optimal transport cost: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{B}_{\\delta}(\\mathbb{P}):=\\{\\mathbb{Q}\\in\\mathcal{P}(\\mathcal{V}):W_{c,p}(\\mathbb{Q},\\mathbb{P})\\leq\\delta\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "DRO problem minimizes worst-case loss quantity: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\delta}(\\mathbb{P},\\boldsymbol{\\theta})\\triangleq\\operatorname*{sup}_{\\mathbb{Q}\\in\\mathbb{B}_{\\delta}(\\mathbb{P})}\\big\\{\\mathbf{E}_{\\mathbb{Q}}[\\ell(\\mathbf{Z},\\boldsymbol{\\theta})]\\big\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and obtained the $\\theta_{N}^{\\mathrm{\\tiny~dro}}~\\in~\\mathrm{arg}\\operatorname*{min}_{\\theta\\in\\Theta}\\mathcal{R}_{\\delta}(\\mathbb{P}_{N},\\theta)$ . The main tool in DRO is the strong duality theorem [26, 46], which converts an infinite-dimensional problem into a finite optimization problem. The theorem states that: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbb{Q}\\in\\mathbb{B}_{\\delta}(\\mathbb{P})}\\left\\{\\underset{v\\sim\\mathbb{Q}}{\\mathbb{E}}[\\psi(v)]\\right\\}=\\operatorname*{inf}_{\\lambda\\geq0}\\left\\{\\lambda\\delta^{p}+\\underset{v\\sim\\mathbb{P}}{\\mathbb{E}}[\\psi_{\\lambda}(v)]\\right\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\psi_{\\lambda}(v)$ is defined as $\\psi_{\\lambda}(v):=\\operatorname*{sup}_{v^{\\prime}\\in\\mathcal{V}}\\;\\{\\psi(v^{\\prime})-\\lambda d^{p}(v,v^{\\prime})\\}.$ ", "page_idx": 3}, {"type": "text", "text": "3 Causally Fair Dissimilarity Function ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The key to robust optimization and individual fairness is the metric that measures individual similarity. This section outlines the properties of such a metric in a causal framework to protect sensitive attributes. We begin with an illustrative example. ", "page_idx": 3}, {"type": "text", "text": "Example 1 Let $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$ represent two SCMs describing the relationships among the variables gender $(\\mathbf G)$ , education $(\\mathbf{E})$ , and income (I). $\\mathcal{M}_{1}$ models these variables as independent, whereas $\\mathcal{M}_{2}$ specifies a linear causal relationship: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{M}_{1}=\\left\\{\\begin{array}{l l}{\\mathbf{G}:=\\mathbf{U}_{G},\\quad\\mathbf{U}_{G}\\sim\\mathcal{B}(0.5)}\\\\ {\\mathbf{E}:=\\mathbf{U}_{E},\\quad\\mathbf{U}_{E}\\sim\\mathcal{N}(0,1)\\;,}\\\\ {\\mathbf{I}:=\\mathbf{U}_{I},\\quad\\mathbf{U}_{I}\\sim\\mathcal{N}(0,1)}\\end{array}\\right.\\,\\mathcal{M}_{2}=\\left\\{\\begin{array}{l l}{\\mathbf{G}:=\\mathbf{U}_{G},\\quad}&{\\mathbf{U}_{G}\\sim\\mathcal{B}(0.5)}\\\\ {\\mathbf{E}:=\\mathbf{G}+\\mathbf{U}_{E},\\quad}&{\\mathbf{U}_{E}\\sim\\mathcal{N}(0,1)\\;,}\\\\ {\\mathbf{I}:=\\mathbf{G}+2\\mathbf{E}+\\mathbf{U}_{I},\\quad\\mathbf{U}_{I}\\sim\\mathcal{N}(0,1)}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Where $\\mathbf{U}_{G}$ represents the population distribution of gender, modeled by a Bernoulli distribution, while $\\mathbf{U}_{E}$ and $\\mathbf{U}_{I}$ are intrinsic talents for academic and income achievements, respectively, modeled by normal distributions. To compare individuals, let\u2019s consider the $L_{1}$ norm on non-sensitive attributes $(d(v,v^{\\prime})=|e-e^{\\prime}|+|i-i^{\\prime}|)$ . If two individuals have less than a 0.1 unit difference, they are deemed similar. Now, consider an individual with data $\\boldsymbol{v}\\,=\\,(M,1,1)$ . Based on experience, we expect that a perturbation in educational talent by . $05$ units will not significantly alter this individual\u2019s status. We model this perturbation with a shift intervention $\\Delta=(0,.05,0)$ . In Model $^{\\,l}$ , the result $C F(v,\\Delta)=(M,1.05,1)$ is considered similar to v. However, in Model 2, $C\\dot{F}(v,\\Delta)=(M,1.05,1.1)$ results in a distance of $d(v,c F(v,\\Delta))=0.15,$ , indicating dissimilarity. In the presence of causality, one attribute can be amplified multiple times in the final feature space. Therefore, we need to control our intuition of dissimilarity between the exogenous variables and the feature space. ", "page_idx": 4}, {"type": "text", "text": "To protect against gender bias, we need to ensure that people with the same intrinsic characteristics but different genders behave similarly. This is modeled by a counterfactual change in gender. In Model $^{\\,l}$ , $\\bar{c}\\boldsymbol{F}(\\boldsymbol{v},\\boldsymbol{F})\\;=\\;(F,1,1)$ shows no difference $(d(v,\\ddot{v}_{F})\\;=\\;\\dot{0},$ ). However, in Model 2, $C F(v,F)=(F,\\dot{0},-2)$ results $d(v,\\ddot{v}_{F})=4$ , which means that they are not similar. ", "page_idx": 4}, {"type": "text", "text": "The example 1 demonstrates that in the presence of causality and protected variables, the standard $l_{p}$ -norm or any metric fails to accurately capture the intuition of similarity. In these scenarios, a dissimilarity function should incorporate counterfactuals and uniformly control for non-sensitive perturbations to effectively capture proximity. This approach is further elaborated in the following definition. Before proceeding, we introduce some notation. For a vector $v$ or $u$ , we define $P_{\\mathcal{A}}(\\cdot)$ and $P_{\\mathcal{X}}(\\cdot)$ as the projections onto the sensitive and non-sensitive parts, respectively. ", "page_idx": 4}, {"type": "text", "text": "Definition 1 (Causally Fair Dissimilarity Function) Let $d:\\mathcal{V}\\times\\mathcal{V}\\to[0,\\infty]$ be a dissimilarity function defined on the feature space $\\mathcal{V}_{i}$ , generated by a SCM $\\mathcal{M}$ . Let A denote a set of sensitive attributes, and $\\mathcal{T}$ represent their corresponding index within $\\{1,\\ldots,n\\}$ . The metric is called $a$ causally fair dissimilarity function or CFDF if it adheres to the following properties: ", "page_idx": 4}, {"type": "text", "text": "\u2022 Zero Dissimilarity for Twin Pairs: For any $v\\in\\mathcal{V}$ and $a\\in\\mathbf{A}$ , the dissimilarity $d(v,\\ddot{v}_{a})$ between an instance and its twins is zero. ", "page_idx": 4}, {"type": "text", "text": "\u2022 Guaranteed Similarity for Minor Perturbations: For every $v\\in\\mathcal{V}$ and any $\\delta>0$ , there exists an \u03f5 such that for any sufficiently small intervention $(\\lVert\\boldsymbol{\\Delta}\\rVert\\leq\\epsilon)$ on the non-sensitive attributes ${}^{\\prime}P_{A}(\\Delta)=0,$ ), the distance $\\bar{d}(v,c F(v,\\Delta))$ remains less than $\\delta$ . ", "page_idx": 4}, {"type": "text", "text": "To understand the shape of $d$ under the assumptions of Def. 1, we must first recognize that the CFDF needs to be defined on a larger space than $\\mathrm{Range}(\\mathcal{M})$ [22]. This is because, generally, when $\\mathcal{M}$ is intervened upon by some sensitive attribute level $a$ , we have $\\begin{array}{r}{\\mathrm{Range}(\\mathcal{M})\\subseteq\\bigcup_{a\\in\\mathcal{A}}\\mathrm{Range}(\\mathcal{M}^{d o(\\mathbf{A}:=a)})}\\end{array}$ . The complete space encompassing all counterfactual values can be defined as follows. ", "page_idx": 4}, {"type": "text", "text": "Definition 2 (Parent-Free Sensitive Attribute SCM) Consider $\\mathcal{M}$ with sensitive attributes indexed by $\\mathcal{T}$ . The parent-free sensitive attribute SCM denoted as $\\begin{array}{r}{\\mathcal{M}_{\\mathrm{0}},}\\end{array}$ , is derived from $\\mathcal{M}$ by removing the causal effects of parents of sensitive attributes and replacing their exogenous variables with indigenous ones. The structural equations for $\\mathcal{M}_{\\mathrm{0}}$ are as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{V}_{i}^{0}:=\\left\\{\\!\\!\\!\\begin{array}{l l}{\\mathbf{U}_{i}\\ \\ \\ }&{\\mathbf{U}_{i}:=\\mathbf{V}_{i}\\sim\\mathbb{P}_{\\mathbf{V}_{i}},\\quad\\,i\\in\\mathbb{Z}}\\\\ {f_{i}(\\mathbf{V}_{p a(i)}^{0})+\\mathbf{U}_{i}\\quad\\mathbf{U}_{i}\\sim\\mathbb{P}_{\\mathbf{U}_{i}},}&{\\,\\,i\\notin\\mathbb{Z}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The exogenous space corresponding to $\\mathcal{M}_{0}$ , denoted by $\\d\\mathcal{U}_{0}$ , includes the sensitive attributes and the non-sensitive parts of the exogenous variables of $\\mathcal{M}$ . This space called the semi-latent space, is constructed as $\\mathcal{U}_{0}=\\mathcal{A}\\times\\mathcal{U}_{\\mathcal{X}}$ , where $\\mathcal{U}_{\\mathcal{X}}$ is the non-sensitive part of the exogenous space in $\\mathcal{M}$ . ", "page_idx": 4}, {"type": "text", "text": "If we know the structural equations of $\\mathcal{M}$ , we can first map the CFDF to the exogenous space. In this space, the exogenous variables are assumed to be independent. Therefore, we can design a dissimilarity function for each variable separately and then combine them using product topology (\u00a7.2 [48]). Following this intuition, we introduce the bijective map $g:\\mathcal{V}\\to\\mathcal{U}_{0}$ from the feature space to the semi-latent space, along with its inverse, defined as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\ng_{i}(v):=\\left\\{\\!v_{i}\\qquad\\right.}\\\\ {F_{i}(v)}&{\\left.i\\notin\\mathbb{Z}\\:\\:^{\\bigstar}\\!\\:,\\quad g_{i}^{-1}(u):=\\left\\{\\!\\!\\!\\begin{array}{l l}{u_{i}}&{i\\in\\mathbb{Z}}\\\\ {f_{i}(g_{\\mathbf{pa}(i)}^{-1}(u))+u_{i}}&{i\\notin\\mathbb{Z}}\\end{array}\\!\\!\\right.}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "If all sensitive attributes have no parents, the semi-latent space is equivalent to the exogenous space, and $g=F^{-1}$ . The counterfactual with respect to $\\mathbf{\\mathcal{M}}_{0}$ is denoted by $\\mathbf{CF}_{0}(v,\\Delta)$ . We can now present the following proposition to determine the shape of $d$ . ", "page_idx": 5}, {"type": "text", "text": "Proposition 1 Let $\\mathcal{M}$ be an ANM, with g as its corresponding map to the semi-latent space $^{6}$ , and $P_{\\mathcal{X}}(u)$ the projection of vector u to the non-sensitive part $\\mathcal{U}_{\\mathcal{X}}$ . Then: ", "page_idx": 5}, {"type": "text", "text": "$(i)$ If $d_{\\mathcal{X}}$ is a continuous dissimilarity function on diagonal $\\mathcal{U}_{\\mathcal{X}}\\times\\mathcal{U}_{\\mathcal{X}}$ , then the function d defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nd(v,v^{\\prime})=d_{X}(P_{X}(g(v)),P_{X}(g(v^{\\prime})))\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "satisfies the definitions of a CFDF. ", "page_idx": 5}, {"type": "text", "text": "$(i i)$ If $d:\\mathcal{V}\\times\\mathcal{V}\\rightarrow[0,\\infty]$ satisfies the CFDF definition and the triangle inequality property, then $d$ can be represented as a dissimilarity function $d_{\\mathcal{X}}$ dependent solely on the non-sensitive components $\\mathcal{U}_{\\mathcal{X}}$ i.e., $\\bar{d}(v,v^{\\prime})=d_{X}(P_{X}(g(v)),\\dot{P_{X}}(g(v^{\\prime})))$ . ", "page_idx": 5}, {"type": "text", "text": "Since $d_{\\mathcal{X}}$ is defined on independent coordinates, its relation to the components is less complex than the CFDF $d$ . We assume the dissimilarity function $d_{\\mathcal{X}}(x,x^{\\prime})$ is translation-invariant. Therefore, for simplicity, we assume $d_{\\mathcal{X}}(x^{\\prime},x)=\\|x^{\\prime}-x\\|$ . The dual of $\\Vert\\cdot\\Vert$ is defined as $\\|x\\|_{*}=\\operatorname*{sup}_{x^{\\prime}}\\{x^{\\dot{T}}x^{\\prime}\\mid$ $\\|x^{\\prime}\\|\\leq\\bar{1}\\}$ . Now we establish our assumptions about the SCM and its CFDF. ", "page_idx": 5}, {"type": "text", "text": "Assumption 1 (i) $\\mathcal{M}$ is an ANM with known structural equations and a semi-latent map $g$ . ", "page_idx": 5}, {"type": "text", "text": "(iii) Cost function over $\\mathcal{Z}$ has form $c((v,y),(v^{\\prime},y^{\\prime}))=d(v,v^{\\prime})+\\infty\\cdot|y-y^{\\prime}|.$ . ", "page_idx": 5}, {"type": "text", "text": "Remark 1 All results of this work apply to the homogeneous dissimilarity function (Def. 6), which includes a broad family of dissimilarity functions, such as norms. ", "page_idx": 5}, {"type": "text", "text": "4 Causally Fair Distributionally Robust Optimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To find out the impact of the CFDF in DRO problems, we first consider the dual form of the worst-case loss quantity, which simplifies the infinite-dimensional primal problem into a more tractable and computationally manageable form. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 (Causally Fair Strong Duality) If Assumption $^{\\,l}$ is satisfied, then for any reference probability distribution $\\mathbb{P}$ and any function $\\psi:\\mathcal{V}\\to\\mathbb{R}$ that is both upper semi-continuous and $L_{1}$ -integrable, the following duality holds: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbb{Q}\\in\\mathbb{B}_{\\delta}(\\mathbb{P})}\\left\\{\\underset{v\\sim\\mathbb{Q}}{\\mathbb{E}}[\\psi(v)]\\right\\}=\\operatorname*{inf}_{\\lambda\\geq0}\\left\\{\\lambda\\delta^{p}+\\underset{v\\sim\\mathbb{P}}{\\mathbb{E}}\\left[\\operatorname*{sup}_{a\\in\\mathcal{A}}\\psi_{\\lambda}(\\ddot{v}_{a})\\right]\\right\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\psi_{\\lambda}(v)$ is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\psi_{\\lambda}(v):=\\operatorname*{sup}_{\\Delta\\in\\mathcal{X}}\\left\\{\\psi(C F_{0}(v,\\Delta))-\\lambda^{p}d(v,C F_{0}(v,\\Delta))\\right\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and ${c r_{\\mathrm{0}}}$ is counterfactual regarding parent-free SCM $\\mathcal{M}_{\\mathrm{0}}$ . ", "page_idx": 5}, {"type": "text", "text": "Remark 2 The intuition behind the above formula is as follows: In the case where all features are independent, let $v=(a,x)$ . The CFDF should exhibit no difference between $(a,x)$ and $(a^{\\prime},x)$ for each $a,a^{\\prime}\\in A$ . Consequently, the distance metric satisfies $d((a,x),(a^{\\prime},x^{\\prime}))=d_{x}(x,x^{\\prime})$ . Under this condition, the classical strong duality theorem (Eq. 5) provides the following relationship: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\psi_{\\lambda}(v)=\\operatorname*{sup}_{(a^{\\prime},x^{\\prime})\\in\\mathcal{V}}\\left\\{\\psi((a^{\\prime},x^{\\prime}))-\\lambda d_{\\mathcal{X}}^{p}(x,x^{\\prime})\\right\\}=\\operatorname*{sup}_{a\\in\\mathcal{A}}\\left\\{\\operatorname*{sup}_{\\Delta\\in\\mathcal{X}}\\psi((a,x+\\Delta))-\\lambda d_{\\mathcal{X}}^{p}(x,x+\\Delta)\\right\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "When we incorporate causal structure instead of coordinating a and $x$ , the two dimensions ${\\ddot{v}}_{a}$ and $C F_{0}(v,\\Delta)$ are replaced accordingly. ", "page_idx": 5}, {"type": "text", "text": "In the DRO formulation, the worst-case loss is expressed in a dual form and can act as a regularizer for parameter learning. Explicitly solving the dual problem eliminates the need to compute the worst-case distribution, resulting in faster, more efficient learning algorithms [14, 16, 62, 73]. Before presenting the general theorem, the next two theorems show that, under mild conditions, the dual formula for specific loss functions in classification and regression problems can be explicitly solved. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 (Higher Order Linear Loss) Given Assumptions $^{\\,l}$ , let $\\mathcal{M}$ be a linear SCM and the loss function $\\ell(z,\\theta)^{p}$ , where $\\ell(z,\\theta)$ is of the form $h(y-\\langle\\theta,v\\rangle)$ or $h(y\\cdot\\langle\\theta,v\\rangle)$ for functions $h(t)$ such as $\\left|t\\right|$ , $\\operatorname*{max}(0,t)$ , $|t-\\tau|$ , or m $\\operatorname{1ax}(0,t-\\tau)$ for some $\\tau\\geq0$ , and $p\\in[1,\\infty)$ . Then the $D R O$ problem $^{4}$ can be reduced to: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\delta}(\\mathbb{P}_{N},\\theta)=\\left\\{\\begin{array}{l l}{\\left(\\mathcal{R}_{\\delta}^{c f}(\\mathbb{P}_{N},\\theta)^{\\frac{1}{p}}+\\delta\\left\\|P_{\\mathcal{X}}(M^{T}\\theta)\\right\\|_{*}\\right)^{p},}&{\\mathrm{diam}\\left(\\mathcal{A}\\right)<\\infty}\\\\ {\\left(\\mathcal{R}(\\mathbb{P}_{N},\\theta)^{\\frac{1}{p}}+\\delta\\left\\|P_{\\mathcal{X}}(M^{T}\\theta)\\right\\|_{*}\\right)^{p},}&{\\ s.t.\\quad P_{\\mathcal{A}}(M^{T}\\theta)=0;\\quad\\mathrm{diam}\\left(\\mathcal{A}\\right)=\\infty}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $M$ is the corresponding matrix for the linear map $g^{-1}$ (see Eq. 6). ", "page_idx": 6}, {"type": "text", "text": "Remark 3 In real-world datasets, the sensitive part always satisfies diam $(A)<\\infty$ . According to the above theorem, $\\mathcal{R}_{\\delta}(\\mathbb{P}_{N},\\theta)\\ge\\mathcal{R}_{\\delta}^{c f}(\\mathbb{P}_{N},\\theta)$ . For practical applications, if the worst-case loss must not exceed a certain value, we can replace $\\infty$ with some constant in the above theorem. ", "page_idx": 6}, {"type": "text", "text": "Example 2 Here are specific examples of the above theorem. We offer a framework to study the equivalence between the worst-case loss in the DRO problem, with the cost function derived from the CFDF, and the regularization scheme for classification and regression problems. ", "page_idx": 6}, {"type": "table", "img_path": "piOzFx9whU/tmp/308b17800aa3fe3cada14e9f8386ffa01e18a9e970b72689485eef3e1f4c6842.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "The Thm. 2 can be extended to the non-linear regression loss function. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3 (Nonlinear Loss) Let assumptions 1 be satisfied, with $p=1$ , $\\mathcal{M}$ linear with matrix $M$ corresponding to map $g^{-1}$ , and a loss function $\\ell(z,\\theta)$ of the form $\\bar{h}(y-\\langle\\theta,v\\rangle)$ for regression and $h(y\\cdot\\langle\\theta,v\\rangle)$ for classification, where $h$ has the following two properties: ", "page_idx": 6}, {"type": "text", "text": "(ii) There exists sequence of $\\{t_{k}\\}_{k=1}^{\\infty}$ goes to $\\infty$ such that for each $t_{0}~\\in~\\mathbb{R}$ we have $l i m_{k\\rightarrow\\infty}\\frac{\\vert h(t_{0}+t_{k})-h(t_{0})\\vert}{\\vert t_{k}\\vert}=\\bar{L_{h}}.$ ", "page_idx": 6}, {"type": "text", "text": "By the above assumption, DRO problem $^{4}$ can be reduced as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\delta}(\\mathbb{P}_{N},\\theta)=\\left\\{\\begin{array}{l l}{\\mathcal{R}_{\\delta}^{c f}(\\mathbb{P}_{N},\\theta)+\\delta L_{h}\\left\\lVert P_{\\chi}(M^{T}\\theta)\\right\\rVert_{*},}&{\\mathrm{diam}(\\boldsymbol{A})<\\infty}\\\\ {\\mathcal{R}(\\mathbb{P}_{N},\\theta)+\\delta L_{h}\\left\\lVert P_{\\chi}(M^{T}\\theta)\\right\\rVert_{*},}&{\\boldsymbol{s}.t.~~~P_{A}(M^{T}\\theta)=0;~~~\\mathrm{diam}(\\boldsymbol{A})=\\infty}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Example 3 The following forms of the loss function satisfy the conditions of $h$ in Thm. 3: ", "page_idx": 6}, {"type": "text", "text": "Now, the first-order estimation of the regularizer for non-linear SCM and loss function is ready to be stated. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4 (First-Order Estimation of DRO Regularizer) Assume M has structural equation $f$ , which $f$ and loss function $\\ell$ are both twice continuously differentiable respect to non-sensitive ", "page_idx": 6}, {"type": "table", "img_path": "piOzFx9whU/tmp/632a376742fcb756a6875b6ed5d33e4f3fda62adf1eb89e038b19fbd107a6d8b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "attributes, diam $(u)<\\infty$ and $c$ satisfies the assumption $^{\\,l}$ with $p\\in[2,\\infty]$ . The necessary condition for the existence of a finite DRO solution is that for each $v\\in\\mathcal{V}$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{a\\in\\mathcal{A}}\\left\\{\\ell(\\ddot{v}_{a},y,\\theta)\\right\\}<\\infty.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "By these conditions, the worst-case loss quantity is equal to: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\delta}(\\mathbb{P}_{N},\\theta)=\\underset{v\\sim\\mathbb{P}_{N}}{\\mathbb{E}}\\left[\\operatorname*{sup}_{a\\in\\mathcal{A}}\\ell(\\ddot{v}_{a},y,\\theta)\\right]+\\delta\\cdot\\bigg(\\underset{v\\sim\\mathbb{P}_{N}}{\\mathbb{E}}\\left[\\operatorname*{sup}_{a\\in\\mathcal{A}}\\lVert\\nabla^{c r}\\ell(\\ddot{v}_{a},y,\\theta)\\rVert_{*}^{q}\\right]\\bigg)^{1/q}+O(\\delta^{2}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the $O(\\delta^{2})$ term is uniform over all $\\theta\\in\\Theta$ , $q$ is p\u2019s conjugate, and the gradient $\\nabla^{c r}\\ell$ equals to: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\nabla^{c r}\\ell(v,y,\\theta)=\\operatorname*{lim}_{\\Delta\\to0}{\\frac{\\ell(c F_{0}(v,\\Delta),y,\\theta)-\\ell(v,y,\\theta)}{\\|\\Delta\\|}}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where ${c r_{\\mathrm{0}}}$ is counterfactual regarding parent-free SCM $\\mathcal{M}_{0}$ . ", "page_idx": 7}, {"type": "text", "text": "By applying Prop. 2 from Gao\u2019s work [28], the next proposition presents the relationship between classical adversarial optimization 3 and DRO for CFDF. ", "page_idx": 7}, {"type": "text", "text": "Proposition 2 (Approximation by Robust Optimization) Suppose $\\boldsymbol{\\mathcal{A}}$ is a finite set and let $\\{(v^{i},\\bar{y}^{i})\\}_{i=1}^{N}$ be observational data. Under Assumption $^{\\,l}$ , assume that for the loss function $\\ell$ there exist constants $L,M\\ge0$ such that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\ell(v,y,\\theta)-\\ell(v^{\\prime},y,\\theta)|<L d^{p}(v,v^{\\prime})+M\\quad f o r\\,a l l\\quad v,v^{\\prime}\\in\\mathcal{V}\\,a n d\\,p\\in[1,\\infty).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For an arbitrary $K\\in\\mathbb{N}$ , consider the adversarial loss within the setting: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{R}}_{\\delta}^{a d v}(\\mathbb{P}_{N}):=\\operatorname*{sup}_{(w^{i k})_{i,k}\\in\\tilde{B}_{\\delta}}\\left\\{\\frac{1}{N K}\\sum_{i=1}^{N}\\sum_{k=1}^{K}\\operatorname*{sup}_{a\\in A}\\ell(\\ddot{w}_{a}^{i k},y_{i},\\theta)\\right\\},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the uncertainty set ${\\tilde{B}}_{\\delta}$ is defined as: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\tilde{B}_{\\delta}:=\\left\\{(w^{i k})_{i,k}:\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{k=1}^{K}d^{p}(v^{i},w^{i k})\\leq\\delta,\\,w^{i k}\\in\\mathcal{V}\\right\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Then, the DRO can be approximated by adversarial optimization as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{R}}_{\\delta}^{a d v}(\\mathbb{P}_{N})\\le\\mathcal{R}_{\\delta}(\\mathbb{P}_{N})\\le\\tilde{\\mathcal{R}}_{\\delta}^{a d v}(\\mathbb{P}_{N})+\\frac{L D+M}{N K},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $D$ is independent of $K$ . ", "page_idx": 7}, {"type": "text", "text": "One of the main challenges in designing DRO for SCMs is that the CFDF depends on the causal structure. When the functional structure is unknown, it must be estimated from data. This empirical estimation impacts the DRO learning process. Therefore, it is crucial to control the uniform convergence error of the DRO problem between the true metric and distribution and the DRO estimated from the data. The following theorem guarantees learning from sample data, but certain assumptions need to be established first. ", "page_idx": 7}, {"type": "text", "text": "(ii) The loss function $\\ell$ is uniformly bounded: there exists a positive constant $M$ such that $0\\leq\\ell(z,\\theta)\\leq M$ for all $\\theta\\in\\Theta$ . Moreover, $\\ell$ is Lipschitz with respect to the counterfactual in $\\mathcal{M}_{0}$ ; that is, there exists a constant $L$ such that: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\ell(v,y,\\theta)-\\ell(C F_{0}(v,\\Delta),y,\\theta)|\\leq\\|\\ell\\|_{\\mathrm{Lip}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "(iii) $\\hat{d}$ is an estimation of the CFDF such that, with probability $1-\\epsilon$ , there exists $M_{d}$ such that, at a rate of $N^{-\\eta}$ , the discrepancy is uniformly bounded by: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\forall v,v^{\\prime}\\in\\mathcal{V}:\\quad|d(v,v^{\\prime})-\\hat{d}(v,v^{\\prime})|\\leq M_{d}N^{-\\eta},\\quad f o r\\,s o m e\\quad\\eta>0.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The following theorem states that the efforts to estimate the metric or causal structures and the parameter \u03b8\u02c6 Ndr o, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{N}^{\\mathrm{\\tiny~dro}}:=\\operatorname*{inf}_{\\theta\\in\\Theta}\\left\\{\\operatorname*{sup}_{\\mathbb{Q}:W_{\\hat{c},p}(\\mathbb{Q},\\mathbb{P}_{N})\\leq\\delta}\\ \\underset{z\\sim\\mathbb{Q}}{\\mathbb{E}}[\\ell(z,\\theta)]\\right\\}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Where $\\hat{c}$ is the $\\hat{d}$ corresponding cost on $\\mathcal{Z}$ , leading to the estimation of the true parameters of the DRO problem. To state our result, we need the Dudley entropy integral [61], which measures the complexity of the loss function class. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5 (Learning Finite Sample Guarantee) With assumption $^{\\,I}$ and 2, then for $\\hat{\\theta}_{N}^{\\mathrm{{dro}}}$ we have: $\\mathcal{R}_{\\delta}(\\mathbb{P}_{*},\\hat{\\theta}_{N}^{\\mathrm{dro}})-\\operatorname*{inf}_{\\theta\\in\\Theta}\\mathcal{R}_{\\delta}(\\mathbb{P}_{*},\\theta)\\le N^{-1/2}\\left[c_{0}+c_{1}\\delta^{1-p}+c_{2}\\delta^{1-p}N^{-\\eta+1/2}+c_{3}\\sqrt{\\log(2/\\epsilon)}\\right],$ With probability at least $1-2\\epsilon$ . With ${\\mathfrak{C}}({\\mathcal{L}})$ denoting the Dudley entropy integral for the function class $\\{\\ell(\\cdot,\\theta):\\theta\\in\\Theta\\}$ , the constants $c_{0},\\,c_{1}$ and $c_{2}$ are identified as follows: $c_{0}:=96\\mathfrak{C}(\\mathcal{L}),\\;c_{1}:=96L\\cdot\\operatorname{diam}\\left(\\mathcal{V}\\right)^{p},\\;c_{2}:=2p L\\cdot\\operatorname{diam}\\left(\\mathcal{V}\\right)^{p-1}\\cdot M_{d},\\;\\;a n d\\;\\;c_{3}:=2\\sqrt{2}\\times M.$ ", "page_idx": 8}, {"type": "text", "text": "The final theorem completes our framework, enabling us to perform DRO on real-world datasets without knowing the SCM structures while providing performance bounds. ", "page_idx": 8}, {"type": "text", "text": "4.1 Related Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Causally Fair Dissimilarity Function. Various studies have addressed the specification and learning of individual fair metrics, such as [33, 68, 70, 47], but their construction based on causal structure and sensitive attributes remains unclear. Our work adopts and extends the concept of a causal fair metric, as discussed in the works [23, 24]. ", "page_idx": 8}, {"type": "text", "text": "DRO and Individual Fairness. Previous works, such as [68, 70, 47], address the DRO problem with an individual fairness metric but are limited to linear SCMs and $p=2$ . These studies do not discuss the duality theorem or regularizers. Additionally, [42] studied DRO, but its connection to causality remains unclear. ", "page_idx": 8}, {"type": "text", "text": "Strong Duality Theorem. Various versions of the strong duality theorem have been explored in prior works. For instance, in [59, 46, 9, 14, 27, 28, 66], the cost function must be a metric or [74] has convex property. Additionally, in [72, 11, 58], the distance function $d$ must be positive-definite, meaning $d(v,v^{\\prime})\\,=\\,0$ if and only if $v\\,=\\,v^{\\prime}$ . However, these conditions are not met for CFDF, necessitating a new formulation of the duality theorem 1. ", "page_idx": 8}, {"type": "text", "text": "DRO as Regularizer. Previous works on using DRO as a regularizer, explicitly solved [60, 14, 16, 29] or through $\\mathbf{k}\\cdot$ -order estimation [5, 9, 6, 66, 27], only consider cases where the cost function is derived from a metric or a positive-definite dissimilarity function. Therefore, their theorems do not apply directly to our CFDF. We present new results in Theorems 2, 3, and 4 tailored for our cases. ", "page_idx": 8}, {"type": "text", "text": "Finite Sample Guarantee. Various works provide bounds on the performance of DRO solutions with finite samples [41, 25, 10, 12], but these do not apply to our CFDF due to previously mentioned reasons. The studies [68, 70, 47] offer performance bounds only for the case of linear SCMs with $p=2$ . Therefore, we present a general case in Theorem 5. ", "page_idx": 8}, {"type": "text", "text": "Optimal Transport and Causality. Recent works [39, 35, 13, 32, 21, 1, 2] on causal optimal transport focus on the causal structure of the transport map or plan, which differs from our problem. In our case, causality pertains to the transportation cost derived from SCMs. ", "page_idx": 8}, {"type": "text", "text": "5 Numerical Studies ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In our numerical studies, we evaluate the impact of using causally fair DRO to mitigate individual unfairness, henceforth referred to as CDRO. We compare CDRO\u2019s performance against Empirical Risk Minimization (ERM), non-causal Adversarial Learning (AL) [44], and the Ross method [56]. Our experiments employ real-world datasets, namely the Adult [38] and COMPAS [65] datasets, pre-processed according to [19]. Additionally, we use a synthetic dataset for linear SCM (LIN) with formulations detailed in Appendix C.1. We first fit a linear structural equation model for both the ", "page_idx": 9}, {"type": "image", "img_path": "piOzFx9whU/tmp/cc4c1e9a072eb386b0a80521babcf319ef06b58236860126f406c30b4937afbe.jpg", "img_caption": ["Figure 1: Displays the findings from our numerical experiment, assessing the performance of DRO across different models and datasets. (left) Bar plot showing the comparison of models based on the unfair area percentage (lower values are better) for $\\Delta=.05$ . (right) Bar plot comparing methods by prediction accuracy performance (higher values are better). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Adult and COMPAS datasets. Logistic regression is employed for classification, and performance is evaluated based on accuracy. Fairness is assessed using the Unfair Area Index (UAI), which is defined by the following equation: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\mathbf{U}_{\\Delta}:=\\mathbb{P}\\big(\\{v\\in\\mathcal{V}:\\quad\\exists v^{\\prime}\\in\\mathcal{V}\\quad\\mathrm{s.t.}\\quad d(v,v^{\\prime})\\leq\\Delta\\quad\\land\\quad h(v)\\neq h(v^{\\prime})\\big\\}\\big).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "We evaluate UAI across different $\\Delta$ values, specifically 0.05 and 0.01. Additionally, we calculate the UAI for scenarios where no sensitive attributes are considered, representing the percentage of non-robust data. Detailed computational experiment procedures are provided in Section C.1. ", "page_idx": 9}, {"type": "text", "text": "Our experiments, conducted using 100 different seeds, are summarized in Table 1. Figures 1, 2 and 3 illustrate that the CDRO method achieves a lower unfair area $(U_{\\Delta})$ for $\\Delta=.05$ , and $\\Delta=0.01$ in all scenarios. Although CDRO shows slightly lower accuracy than ERM, this trade-off is a common observation in several studies [52]. Additional results can be found in $\\S\\,{\\mathrm{C}}.6$ . ", "page_idx": 9}, {"type": "text", "text": "6 Discussion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our study introduces a novel framework for causally fair DRO, integrating causal structures and sensitive attributes into the DRO paradigm. This framework is supported by several theoretical advancements, including a strong duality theorem, explicit regularizer formulation, first-order regularizer estimation, and finite sample guarantees with unknown SCMs, enhancing its efficiency and practicality for real-world applications. Our experimental results demonstrate its effectiveness in various settings, highlighting its potential for mitigating biases in machine learning models. ", "page_idx": 9}, {"type": "text", "text": "Despite the promising results, our study has several limitations that warrant further investigation. Firstly, the assumption of an additive noise model may not capture the complexity of all realworld causal relationships, posing challenges in computing additive interventions in general SCMs. Secondly, while Theorem 2 and Theorem 3 could be extended to more general cases, we omitted these extensions to avoid complexity. Lastly, further work is needed to explore the relationship between our method and causal optimal transport [13, 32]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors thank the Max Planck Institute for Intelligent Systems, T\u00fcbingen AI Center, for supporting this project. Partial funding support was also provided by the Canada CIFAR AI Chair program. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Beatrice Acciaio, Julio Backhoff-Veraguas, and Anastasiia Zalashko. Causal optimal transport and its links to enlargement of flitrations and continuous-time stochastic optimization. Stochastic Processes and their Applications, 130(5):2918\u20132953, 2020.   \n[2] Julio Backhoff, Mathias Beiglbock, Yiqing Lin, and Anastasiia Zalashko. Causal transport in discrete time and applications. SIAM Journal on Optimization, 27(4):2528\u20132562, 2017.   \n[3] Ryan S Baker and Aaron Hawn. Algorithmic bias in education. International Journal of Artificial Intelligence in Education, pages 1\u201341, 2022.   \n[4] Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and machine learning: Limitations and opportunities. MIT Press, 2023.   \n[5] Daniel Bartl, Mathias Beiglb\u00f6ck, and Gudmund Pammer. The wasserstein space of stochastic processes. arXiv preprint arXiv:2104.14245, 2021.   \n[6] Robert Bartlett, Adair Morse, Richard Stanton, and Nancy Wallace. Consumer-lending discrimination in the fintech era. Journal of Financial Economics, 143(1):30\u201356, 2022.   \n[7] Dimitris Bertsimas, David B Brown, and Constantine Caramanis. Theory and applications of robust optimization. SIAM review, 53(3):464\u2013501, 2011.   \n[8] Reuben Binns. On the apparent confilct between individual and group fairness. In Proceedings of the 2020 conference on fairness, accountability, and transparency, pages 514\u2013524, 2020.   \n[9] Jose Blanchet, Yang Kang, and Karthyek Murthy. Robust wasserstein profile inference and applications to machine learning. Journal of Applied Probability, 56(3):830\u2013857, 2019.   \n[10] Jose Blanchet, Jiajin Li, Sirui Lin, and Xuhui Zhang. Distributionally robust optimization and robust statistics. arXiv preprint arXiv:2401.14655, 2024.   \n[11] Jose Blanchet and Karthyek Murthy. Quantifying distributional model risk via optimal transport. Mathematics of Operations Research, 44(2):565\u2013600, 2019.   \n[12] Jose Blanchet, Karthyek Murthy, and Nian Si. Confidence regions in wasserstein distributionally robust estimation. Biometrika, 109(2):295\u2013315, 2022.   \n[13] Patrick Cheridito and Stephan Eckstein. Optimal transport and wasserstein distances for causal models. arXiv preprint arXiv:2303.14085, 2023.   \n[14] Hong Chu, Meixia Lin, and Kim-Chuan Toh. Wasserstein distributionally robust optimization and its tractable regularization formulations. arXiv preprint arXiv:2402.03942, 2024.   \n[15] Hong TM Chu, Kim-Chuan Toh, and Yangjing Zhang. On regularized square-root regression problems: distributionally robust interpretation and fast computations. The Journal of Machine Learning Research, 23(1):13885\u201313923, 2022.   \n[16] Hong TM Chu, Kim-Chuan Toh, and Yangjing Zhang. On regularized square-root regression problems: distributionally robust interpretation and fast computations. Journal of Machine Learning Research, 23(308):1\u201339, 2022.   \n[17] Zac Cranko, Zhan Shi, Xinhua Zhang, Richard Nock, and Simon Kornblith. Generalised Lipschitz regularisation equals distributional robustness. In International Conference on Machine Learning, pages 2178\u20132188. PMLR, 2021.   \n[18] Jeffrey Dastin. Amazon scraps secret ai recruiting tool that showed bias against women. In Ethics of data and analytics, pages 296\u2013299. Auerbach Publications, 2022.   \n[19] Ricardo Dominguez-Olmedo, Amir H Karimi, and Bernhard Sch\u00f6lkopf. On the adversarial robustness of causal algorithmic recourse. In International Conference on Machine Learning, pages 5324\u20135342. PMLR, 2022.   \n[20] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, pages 214\u2013226, 2012.   \n[21] Stephan Eckstein and Gudmund Pammer. Computational methods for adapted optimal transport. The Annals of Applied Probability, 34(1A):675\u2013713, 2024.   \n[22] Ahmad-Reza Ehyaei, Golnoosh Farnadi, and Samira Samadi. Causal fair metric: Bridging causality, individual fairness, and adversarial robustness. arXiv preprint arXiv:2310.19391, 2023.   \n[23] Ahmad-Reza Ehyaei, Amir-Hossein Karimi, Bernhard Sch\u00f6lkopf, and Setareh Maghsudi. Robustness implies fairness in causal algorithmic recourse. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, pages 984\u20131001, 2023.   \n[24] Ahmad-Reza Ehyaei, Kiarash Mohammadi, Amir-Hossein Karimi, Samira Samadi, and Golnoosh Farnadi. Causal adversarial perturbations for individual fairness and robustness in heterogeneous data spaces. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, 10, pages 11847\u201311855, 2024.   \n[25] Rui Gao. Finite-sample guarantees for wasserstein distributionally robust optimization: Breaking the curse of dimensionality. Operations Research, 71(6):2291\u20132306, 2023.   \n[26] Rui Gao, Xi Chen, and Anton J Kleywegt. Wasserstein distributionally robust optimization and variation regularization. arXiv preprint arXiv:1712.06050, 2017.   \n[27] Rui Gao, Xi Chen, and Anton J Kleywegt. Wasserstein distributionally robust optimization and variation regularization. Operations Research, 2022.   \n[28] Rui Gao and Anton Kleywegt. Distributionally robust stochastic optimization with wasserstein distance. Mathematics of Operations Research, 48(2):603\u2013655, 2023.   \n[29] Camilo Andr\u00e9s Garc\u00eda Trillos and Nicol\u00e1s Garc\u00eda Trillos. On the regularized risk of distributionally robust learning over deep neural networks. Research in the Mathematical Sciences, 9(3):54, 2022.   \n[30] Milena A Gianfrancesco, Suzanne Tamang, Jinoos Yazdany, and Gabriela Schmajuk. Potential biases in machine learning algorithms using electronic health record data. JAMA internal medicine, 178(11):1544\u20131547, 2018.   \n[31] Melissa Hall, Laurens van der Maaten, Laura Gustafson, Maxwell Jones, and Aaron Adcock. A systematic study of bias amplification. arXiv preprint arXiv:2201.11706, 2022.   \n[32] Bingyan Han. Distributionally robust risk evaluation with a causality constraint and structural information. arXiv preprint arXiv:2203.10571, 2022.   \n[33] Christina Ilvento. Metric learning for individual fairness. In 1st Symposium on Foundations of Responsible Computing (FORC 2020). Schloss-Dagstuhl-Leibniz Zentrum f\u00fcr Informatik, 2020.   \n[34] Alexander Immer, Christoph Schultheiss, Julia E Vogt, Bernhard Sch\u00f6lkopf, Peter B\u00fchlmann, and Alexander Marx. On the identifiability and estimation of causal location-scale noise models. arXiv preprint arXiv:2210.09054, 2022.   \n[35] Yifan Jiang. Duality of causal distributionally robust optimization: the discrete-time case. arXiv preprint arXiv:2401.16556, 2024.   \n[36] Olav Kallenberg and Olav Kallenberg. Foundations of modern probability, volume 2. Springer, 1997.   \n[37] Amir-Hossein Karimi, Bernhard Sch\u00f6lkopf, and Isabel Valera. Algorithmic recourse: from counterfactual explanations to interventions. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 353\u2013362, 2021.   \n[38] Ronny Kohavi and Barry Becker. Uci adult data set. UCI Meachine Learning Repository, 5, 1996.   \n[39] Daniel Kr\u0161ek and Gudmund Pammer. General duality and dual attainment for adapted transport. arXiv preprint arXiv:2401.11958, 2024.   \n[40] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In Advances in Neural Information Processing Systems, pages 4069\u20134079, 2017.   \n[41] Jaeho Lee and Maxim Raginsky. Minimax statistical learning with wasserstein distances. Advances in Neural Information Processing Systems, 31, 2018.   \n[42] Peizhao Li, Ethan Xia, and Hongfu Liu. Learning antidote data to individual unfairness. In International Conference on Machine Learning, pages 20168\u201320181. PMLR, 2023.   \n[43] Fengming Lin, Xiaolei Fang, and Zheming Gao. Distributionally robust optimization: A review on theory and applications. Numerical Algebra, Control and Optimization, 12(1):159\u2013212, 2022.   \n[44] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.   \n[45] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on bias and fairness in machine learning. ACM computing surveys (CSUR), 54(6):1\u201335, 2021.   \n[46] Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization using the wasserstein metric: Performance guarantees and tractable reformulations. Mathematical Programming, 171(1):115\u2013166, 2018.   \n[47] Debarghya Mukherjee, Mikhail Yurochkin, Moulinath Banerjee, and Yuekai Sun. Two simple ways to learn individual fairness metrics from data. In International Conference on Machine Learning, pages 7097\u20137107. PMLR, 2020.   \n[48] James R Munkres. Topology, 2nd edn of [mr0464128], 2000.   \n[49] Razieh Nabi and Ilya Shpitser. Fair inference on outcomes. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 1, 2018.   \n[50] Arash Nasr-Esfahany, Mohammad Alizadeh, and Devavrat Shah. Counterfactual identifiability of bijective causal models. In International Conference on Machine Learning, pages 25733\u2013 25754. PMLR, 2023.   \n[51] Judea Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, 2009.   \n[52] Dana Pessach and Erez Shmueli. A review on fairness in machine learning. ACM Computing Surveys (CSUR), 55(3):1\u201344, 2022.   \n[53] Jonas Peters, Dominik Janzing, and Bernhard Sch\u00f6lkopf. Elements of causal inference: foundations and learning algorithms. The MIT Press, 2017.   \n[54] Gabriel Peyr\u00e9, Marco Cuturi, et al. Computational optimal transport. Center for Research in Economics and Statistics Working Papers, 2017-86, 2017.   \n[55] Hamed Rahimian and Sanjay Mehrotra. Frameworks and results in distributionally robust optimization. Open Journal of Mathematical Optimization, 3:1\u201385, 2022.   \n[56] Alexis Ross, Himabindu Lakkaraju, and Osbert Bastani. Learning models for actionable recourse. Advances in Neural Information Processing Systems, 34:18734\u201318746, 2021.   \n[57] Anian Ruoss, Mislav Balunovic, Marc Fischer, and Martin Vechev. Learning certified individually fair representations. In Advances in Neural Information Processing Systems, 2020.   \n[58] Soroosh Shafieezadeh-Abadeh, Liviu Aolaritei, Florian D\u00f6rfler, and Daniel Kuhn. New perspectives on regularization and computation in optimal transport-based distributionally robust optimization. arXiv preprint arXiv:2303.03900, 2023.   \n[59] Soroosh Shafieezadeh-Abadeh, Daniel Kuhn, and Peyman Mohajerin Esfahani. Regularization via mass transportation. Journal of Machine Learning Research, 20(103):1\u201368, 2019.   \n[60] Soroosh Shafieezadeh Abadeh, Peyman M Mohajerin Esfahani, and Daniel Kuhn. Distributionally robust logistic regression. Advances in Neural Information Processing Systems, 28, 2015.   \n[61] Michel Talagrand. Upper and lower bounds for stochastic processes, volume 60. Springer, 2014.   \n[62] Peipei Tang, Chengjing Wang, Defeng Sun, and Kim-Chuan Toh. A sparse semismooth newton based proximal majorization-minimization algorithm for nonconvex square-root-loss regression problem. The Journal of Machine Learning Research, 21(1):9253\u20139290, 2020.   \n[63] C\u00e9dric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009.   \n[64] Julius Von K\u00fcgelgen, Amir-Hossein Karimi, Umang Bhatt, Isabel Valera, Adrian Weller, and Bernhard Sch\u00f6lkopf. On the fairness of causal algorithmic recourse. In Proceedings of the AAAI conference on artificial intelligence, volume 36, 9, pages 9584\u20139594, 2022.   \n[65] Anne L Washington. How to argue with an algorithm: Lessons from the compas-propublica debate. Colo. Tech. LJ, 17:131, 2018.   \n[66] Qinyu Wu, Jonathan Yu-Meng Li, and Tiantian Mao. On generalization and regularization via wasserstein distributionally robust optimization. arXiv preprint arXiv:2212.05716, 2022.   \n[67] Samuel Yeom and Matt Fredrikson. Individual fairness revisited: transferring techniques from adversarial robustness. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, 2021.   \n[68] Mikhail Yurochkin, Amanda Bower, and Yuekai Sun. Training individually fair ml models with sensitive subspace robustness. arXiv preprint arXiv:1907.00020, 2019.   \n[69] Mikhail Yurochkin, Amanda Bower, and Yuekai Sun. Training individually fair ml models with sensitive subspace robustness. In International Conference on Learning Representations, 2020.   \n[70] Mikhail Yurochkin and Yuekai Sun. Sensei: Sensitive set invariance for enforcing individual fairness. In International Conference on Learning Representations, 2021.   \n[71] K Zhang and A Hyv\u00e4rinen. On the identifiability of the post-nonlinear causal model. In 25th Conference on Uncertainty in Artificial Intelligence (UAI 2009), pages 647\u2013655. AUAI Press, 2009.   \n[72] Luhao Zhang, Jincheng Yang, and Rui Gao. A simple and general duality proof for wasserstein distributionally robust optimization. arXiv preprint arXiv:2205.00362, 2022.   \n[73] Yangjing Zhang, Ning Zhang, Defeng Sun, and Kim-Chuan Toh. An efficient hessian based algorithm for solving large-scale sparse group lasso problems. Mathematical Programming, 179:223\u2013263, 2020.   \n[74] Jianzhe Zhen, Daniel Kuhn, and Wolfram Wiesemann. A unified theory of robust and distributionally robust optimization via the primal-worst-equals-dual-best principle. Operations Research, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Supplementary Theoretical Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Notations. In this work random variables are denoted by bold letters (e.g., V), their corresponding probability spaces by calligraphic letters (e.g., $\\mathcal{V},$ , and instances by normal letters (e.g., $v$ ). The space of probability measures on $\\mathcal{V}$ is represented by $\\mathcal{P}(\\mathcal{V})$ and probability measures by blackboard bold letters (e.g., $\\mathbb{P}$ ). ", "page_idx": 14}, {"type": "text", "text": "Non-Sensitive Part. Let $F:\\mathcal{U}\\to\\mathcal{V}$ be the reduced-form map of $\\mathcal{M}$ . The vector $v$ decomposes into sensitive and non-sensitive parts, $v=(a,x)$ , and we have a corresponding decomposition in the exogenous space denoted by $\\boldsymbol{u}=\\left(u_{a},u_{x}\\right)$ and $\\mathcal{U}_{A},\\mathcal{U}_{X}$ are corresponding spaces. Using the ANM model, we can assume that both $\\nu$ and $\\boldsymbol{\\mathcal{U}}$ are equivalent, and therefore, the non-sensitive feature space is the same as the non-sensitive part of the exogenous space. If $\\mathbb{P}$ is a probability measure in $\\mathcal{P}(\\mathcal{V})$ , then $(\\mathbb{P})_{\\mathcal{X}}$ refers to the marginal probability over the non-sensitive part. We also refer to $\\mathbb{Q})_{\\mathcal{X}}$ for marginal probability over the non-sensitive part of the exogenous space. ", "page_idx": 14}, {"type": "text", "text": "Definition 3 (Push-forward Measure) Let $\\mathbb{P}\\in\\mathcal{P}(\\mathcal{V})$ , $\\mathbb{Q}\\in{\\mathcal{P}}({\\mathcal{U}})$ be two probability measures and $T:\\mathcal{V}\\to\\mathcal{U}$ is map, the measure $\\mathbb{Q}$ is called the push-forward of $\\mathbb{P}$ through $T$ is denoted by $T_{\\#}\\mathbb{P}$ if: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{Q}(B)=\\mathbb{P}(T^{-1}(B)),\\quad\\forall B\\subset\\mathcal{U}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Definition 4 (Set of Couplings) The set $\\Gamma(\\mathbb{P},\\mathbb{Q})$ represents the couplings of probability distributions $\\mathbb{P}\\in\\mathcal{P}(\\mathcal{V}),\\,\\mathbb{Q}\\in\\mathcal{P}(\\mathcal{U})$ , comprising distributions over $\\nu\\times\\mathcal{U}$ with margins $\\mathbb{P}$ and $\\mathbb{Q}$ . A measure $\\pi$ belongs to $\\Gamma(\\mathbb{P},\\mathbb{Q})$ if and only if ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pi(A\\times\\mathcal{U})=\\mathbb{P}(A)\\quad a n d\\quad\\pi(\\mathcal{V}\\times B)=\\mathbb{Q}(B)\\quad\\forall A\\subset\\mathcal{V},B\\subset\\mathcal{U}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By extension, a random pair $(X,Y)\\sim\\pi_{\\!\\!\\!\\operatorname{\\rho}}$ , where $\\pi\\in\\Gamma(\\mathbb{P},\\mathbb{Q})$ , will also be called a coupling of $\\mathbb{P}$ and $\\mathbb{Q}$ . ", "page_idx": 14}, {"type": "text", "text": "Definition 5 (Diameter of a Set) Let $A$ be a set in a metric space with a distance function d. The diameter of $A$ , denoted diam $(A)$ , is defined as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nd i a m(A)=\\operatorname*{sup}\\{d(x,y):x,y\\in A\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where sup represents the supremum of the set of distances $d(x,y)$ for all pairs $(x,y)$ in $A$ . ", "page_idx": 14}, {"type": "text", "text": "Definition 6 (Homogeneous dissimilarity function) Let $\\Lambda$ be an extended-valued function $\\Lambda\\colon\\mathcal{X}\\rightarrow[0,\\infty]$ on a real vector space $\\mathcal{X}$ with absolutely homogeneous assumption i.e. $\\Lambda(t x)=$ $\\left|t\\right|\\Lambda(x)$ for any $t\\in\\mathbb R$ and $z\\in\\mathcal{X}$ . In addition, $\\Lambda$ is proper it means there exists $x_{0}\\in\\mathcal{X}$ such that $\\Lambda(x_{0})=1$ . The cost function $d:\\mathcal{X}\\times\\mathcal{X}\\rightarrow[0,\\infty]$ is called Homogeneous dissimilarity function if is defined as $d(x^{\\prime},x):=\\Lambda(x^{\\prime}-x)$ for any $x^{\\prime},x\\in\\mathcal{X}$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma 1 If $\\mathcal{M}$ is an additive noise model with mutually independent exogenous variables, then the parent-free sensitive $\\mathbf{\\mathcal{M}}_{0}$ attribute model retains both of these properties. Moreover, the map-reduced form mapping of $\\mathcal{M}_{\\mathrm{0}}$ is equivalent to $g^{-1}$ , where $g$ represents the mapping to the semi-latent space. ", "page_idx": 14}, {"type": "text", "text": "Proof. First, $M_{0}$ is an additive noise model because its structure is derived from the initial equations of $\\mathcal{M}$ by removing those equations related to the sensitive attributes and replacing the exogenous variable $\\mathbf{U}_{i}$ by $\\mathbf{V}_{i}$ . ", "page_idx": 14}, {"type": "text", "text": "Regarding the mutual independence of the exogenous variables, in the original model $\\mathcal{M}$ , the variables $V_{i}$ for $i\\in\\mathcal{Z}$ are not independent of $\\mathbf{{U}}_{j}$ for $j\\not\\in{\\mathcal{Z}}$ if they have parents. However, assuming a hard intervention for each instance of $V_{i}$ \u2014 where a do-action is executed for this intervention \u2014 it implies that the intervened variable $V_{i}$ can be considered independent from the other variables. Therefore, since we apply hard interventions to all sensitive variables, we can assume that $V_{i}$ for $i\\in\\mathcal{Z}$ are mutually independent, and also that $V_{i}$ are independent of $\\mathbf{{U}}_{j}$ for all $j\\not\\in{\\mathcal{T}}$ . ", "page_idx": 14}, {"type": "text", "text": "Finally, by referencing equations 6, it is observable that the map-reduced form mapping of $\\mathbf{\\mathcal{M}}_{0}$ is equivalent to the inverse of the map to the semi-latent space. ", "page_idx": 14}, {"type": "text", "text": "Lemma 2 Let $\\mathcal{M}$ be an additive noise model with a mapping g to semi-latent space $\\mathcal{U}_{\\mathrm{0}}$ . Assume $\\mathcal{M}$ includes the sensitive attributes A and other non-sensitive attributes $\\mathbf{X}$ that belong to the vector space $\\mathcal{X}$ . Consider $v\\,=\\,(a,x)$ as an instance in $\\mathcal{M}$ , and let $\\Delta\\in\\mathcal{X}$ represent a shift intervention value. Then, the counterfactual corresponding to additive shit is obtained by: ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\nP_{\\mathcal{X}}(C F(v,\\Delta))=P_{\\mathcal{X}}(g^{-1}(g(v)+(0,\\Delta))).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover, if $a^{\\prime}\\in A$ represents another level of sensitive attributes, then the hard intervention concerning $\\mathbf{A}:=\\boldsymbol{a}^{\\prime}$ is achieved by: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\ddot{v}_{a^{\\prime}}=C F(v,d o(\\mathbf{A}{:=}a^{\\prime}))=g^{-1}((a^{\\prime},P_{X}(g(v)))).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. In additive noise models, an additive intervention can be conceptualized as adding a value $\\delta$ to the exogenous variables, while all structural equations remain unchanged. Consequently, during such an intervention, the reduced-form mapping $F_{\\mathcal{M}^{\\Delta}}$ of the intervened SCM remains unchanged. Therefore by definition of intervention, it follows that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{CF}(v,d o(\\mathbf{X}+=\\Delta))=F_{M^{\\Delta}}(F^{-1}(v)+(0,\\Delta))=F(F^{-1}(v)+(0,\\Delta)).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $F$ and $g^{-1}$ are coincide in non-sensitive coordinates then ${\\cal P}\\chi(\\psi(F^{-1}(v)\\,+\\,(0,\\Delta)))\\,=$ $P_{\\mathcal{X}}(g^{-1}(g(v)+(0,\\Delta)))$ and it completes the first part. In this case, we denote $F$ as $M$ , which is an invertible matrix. Consequently, the counterfactual $\\mathbf{CF}(v,d o(\\mathbf{X}{+}{=}\\Delta))$ can be expressed as $v+M^{-1}(0,\\Delta)$ . ", "page_idx": 15}, {"type": "text", "text": "To prove the second part, when intervention is performed on sensitive attributes, $\\mathcal{M}$ transforms into a parent-free sensitive attribute model where the sensitive attribute $a$ is replaced by $a^{\\prime}$ . Given that the map-reduced form of the parent-free sensitive attribute model aligns with $g^{-1}$ , and since $g$ and $F^{-1}$ coincide on the non-sensitive parts, the counterfactual can be expressed as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\ddot{v}_{a^{\\prime}}=\\mathbf{CF}\\big(v,d o(\\mathbf{A}{:}=a^{\\prime})\\big)=g^{-1}\\big((a^{\\prime},P_{X}(g(v)))\\big).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B Proof Section ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Proposition 1. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "(i) If $d$ adheres to Eq. 7, it means that for each $v~\\in~\\mathcal{V}$ , the mapping $g(v)~=~(a,x)$ . For its counterfactual ${\\ddot{v}}_{a}$ , we have $g(\\ddot{v}_{a^{\\prime}})=(a^{\\prime},x)$ . Using Eq. 7, we can express: ", "page_idx": 15}, {"type": "equation", "text": "$$\nd(v,\\ddot{v}_{a})=d_{\\mathcal{X}}(P_{\\mathcal{X}}(g(v)),P_{\\mathcal{X}}(g(\\ddot{v}_{a})))=d_{\\mathcal{X}}(x,x)=0\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This demonstrates that $d$ retains the first property of Def. 1. ", "page_idx": 15}, {"type": "text", "text": "Additionally, since $d_{\\mathcal{X}}$ is continuous, for each $x$ and any $\\epsilon>0$ , there exists a $\\delta>0$ such that if $||\\Delta||<\\delta$ , then $d_{\\mathcal{X}}(x,x+\\Delta)<\\epsilon.$ Referencing Lemma 2 and the formulation of $d$ , it follows that for $||\\Delta||<\\delta$ and for each $a\\in{\\mathcal{A}}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nd(v,\\mathbf{CF}(v,\\Delta))=d(P_{\\mathcal{X}}(g(v)),P_{\\mathcal{X}}(g(\\mathbf{CF}(v,\\Delta))))=d_{\\mathcal{X}}(x,x+\\Delta)<\\epsilon\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, it satisfies property (ii) of the CFDF. ", "page_idx": 15}, {"type": "text", "text": "(ii) Let\u2019s consider a CFDF denoted as $d:\\mathcal{V}\\times\\mathcal{V}\\,\\rightarrow\\,\\mathbb{R}$ , with an embedding $\\mathbf{g}:\\mathcal{V}\\to\\mathcal{Q}$ that maps from the feature space to a semi-latent space. We define $d^{\\ast}$ as the pull-back of $d$ onto $\\mathcal{Q}$ , $d^{*}({\\boldsymbol{q}}_{1},{\\boldsymbol{q}}_{2})=d({\\boldsymbol{g}}^{-1}({\\boldsymbol{q}}_{1}),\\dot{{\\boldsymbol{g}}}^{-1}({\\boldsymbol{q}}_{2}))$ where $d^{\\ast}$ is a dissimilarity function, and we aim to clarify which properties it inherits from Def. 1. We utilize a decomposition of $\\mathcal{Q}$ into $A\\times X$ , where $\\boldsymbol{q}=\\boldsymbol{g}^{-1}(\\boldsymbol{v})$ and $v\\in\\mathcal{V}$ , denoting $q$ as $(a,x)$ . Property (i) of the CFDF ensures: ", "page_idx": 15}, {"type": "equation", "text": "$$\nd(v,\\ddot{v}_{a^{\\prime}})=d^{*}((a,x),(a^{\\prime},x))=0\\quad\\forall a^{\\prime}\\in\\mathcal{A}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This property confirms that $d^{*}$ is insensitive to changes in the sensitive part $\\boldsymbol{\\mathcal{A}}$ . To demonstrate, consider any two points $q_{1}\\,=\\,(a_{1},x_{1})$ and $q_{2}\\,=\\,(a_{2},x_{2})$ , with an arbitrary $a_{0}\\in A$ . By triangle property of dissimilarity function $d$ it can be seen: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d^{*}\\big((a_{1},x_{1}),(a_{2},x_{2})\\big)\\leq d^{*}\\big((a_{1},x_{1}),(a_{0},x_{1})\\big)+d^{*}\\big((a_{0},x_{1}),(a_{2},x_{2})\\big)\\Longrightarrow}\\\\ &{\\qquad\\qquad\\qquad\\qquad d^{*}\\big((a_{1},x_{1}),(a_{2},x_{2})\\big)\\leq d^{*}\\big((a_{0},x_{1}),(a_{2},x_{2})\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here, $d^{*}((s_{1},x_{1}),(s_{0},x_{1}))$ is zero due to property (i). Similarly, we can argue: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d^{*}((a_{0},x_{1}),(a_{2},x_{2}))\\leq d^{*}((a_{0},x_{1}),(a_{1},x_{1}))+d^{*}((a_{1},x_{1}),(a_{2},x_{2}))\\Longrightarrow}\\\\ &{\\qquad\\qquad\\qquad\\qquad d^{*}((a_{0},x_{1}),(a_{2},x_{2}))\\leq d^{*}((a_{1},x_{1}),(a_{2},x_{2}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This results in $d^{*}((a_{1},x_{1}),(a_{2},x_{2}))=d^{*}((a_{0},x_{1}),(a_{2},x_{2}))$ . With similar reasoning, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{I^{*}((a_{1},x_{1}),(a_{2},x_{2}))=d^{*}((a_{1},x_{1}),(a_{0},x_{2}))\\Longrightarrow d^{*}((a_{1},x_{1}),(a_{2},x_{2}))=d^{*}((a_{0},x_{1}),(a_{0},x_{2}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, $d^{*}$ is invariant to the sensitive subspace. If $d_{\\mathcal{X}}$ is the dissimilarity function induced by $d^{*}$ on $\\mathcal{X}$ , then $d^{*}((a_{1},x_{1}),(a_{2},x_{2}))=d_{\\mathcal{X}}(x_{1},\\bar{x_{2}})$ . In accordance with Lemma 2, the second property of Def. 1 states that for each $\\epsilon>0$ , there exists a $\\delta$ such that if $|\\Delta|<\\delta$ , then $d_{\\mathcal{X}}(x,x+\\Delta)<\\epsilon$ . This property demonstrates the continuity of $d_{\\mathcal{X}}$ along the diagonal. ", "page_idx": 16}, {"type": "text", "text": "Finally, $d$ can be embedded in semi-latent space and described by another dissimilarity function on it that only depends on the non-sensitive part of exogenous space: ", "page_idx": 16}, {"type": "equation", "text": "$$\nd(v,w)=d_{X}(P_{X}(g(v)),P_{X}(g(w)))\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 16}, {"type": "text", "text": "Lemma 3 (Transformation by a Bijective Map) Let $g:\\mathcal{V}\\to\\mathcal{U}$ be an invertible function and let the transportation cost function c be constructed by $c(v,v^{\\prime})=d(g(v),g(v^{\\prime}))$ where $d$ is a metric on the space U. For every $\\mathbb{P},\\mathbb{Q}\\in\\mathcal{P}(\\mathcal{V})$ , the following equation holds: ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{c,p}(\\mathbb{P},\\mathbb{Q})=W_{d,p}(g_{\\#}\\mathbb{P},g_{\\#}\\mathbb{Q})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $W_{c}$ and $W_{d}$ represent the Wasserstein distances with respect to the metrics c and $d,$ , respectively. ", "page_idx": 16}, {"type": "text", "text": "Proof. By the definition of the Wasserstein distance, ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{c,p}(\\mathbb{P},\\mathbb{Q})=\\operatorname*{inf}_{\\pi\\in\\Gamma(\\mathbb{P},\\mathbb{Q})}\\int_{\\mathcal{V}\\times\\mathcal{V}}c^{p}(v,v^{\\prime})\\,d\\pi(v,v^{\\prime}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Substituting $c(v,v^{\\prime})=d(g(v),g(v^{\\prime}))$ and $u=g(v)$ gives: ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{c,p}(\\mathbb{P},\\mathbb{Q})=\\operatorname*{inf}_{\\pi\\in\\Gamma(\\mathbb{P},\\mathbb{Q})}\\int_{\\mathcal{V}\\times\\mathcal{V}}d^{p}(g(v),g(v^{\\prime}))\\,d\\pi(v,v^{\\prime}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Consider a coupling $\\pi$ of $\\mathbb{P}$ and $\\mathbb{Q}$ . Define a measure $\\tilde{\\pi}$ on $\\mathcal{U}\\!\\times\\!\\mathcal{U}$ by $\\tilde{\\pi}(A{\\times}B)=\\pi(g^{-1}(A){\\times}g^{-1}(B))$ . $\\tilde{\\pi}$ is a coupling of $g_{\\#}\\mathbb{P}$ and $g_{\\#}\\mathbb{Q}$ because: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{\\pi}(A\\times\\mathcal{U})=\\pi(g^{-1}(A)\\times\\mathcal{V})=g_{\\#}\\mathbb{P}(A);\\quad\\tilde{\\pi}(\\mathcal{U}\\times B)=\\pi(\\mathcal{V}\\times g^{-1}(B))=g_{\\#}\\mathbb{Q}(B).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, the $p$ -Wasserstein distance for the push-forward measures is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{inf}_{\\pi\\in\\Gamma(\\mathbb{P},\\mathbb{Q})}\\displaystyle\\int_{\\mathcal{V}\\times\\mathcal{V}}d^{p}(g(v),g(v^{\\prime}))\\,d\\pi(v,v^{\\prime})=\\displaystyle\\operatorname*{inf}_{\\tilde{\\pi}\\in\\Gamma(g_{\\#}\\mathbb{P},g_{\\#}\\mathbb{Q})}\\displaystyle\\int_{\\mathcal{U}\\times\\mathcal{U}}d^{p}(u,u^{\\prime})\\,d\\tilde{\\pi}(u,u^{\\prime})}\\\\ {=W_{d,p}(g_{\\#}\\mathbb{P},g_{\\#}\\mathbb{Q}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\tilde{\\pi}$ arises from $\\pi$ via $g$ , and $g$ is invertible and measure-preserving in this context, the values in the integrals of the definitions of $W_{c,p}(\\mathbb{P},\\mathbb{Q})$ and $W_{d,p}(g_{\\#}\\mathbb{P},g_{\\#}\\mathbb{Q})$ match. Thus, we have shown that $W_{c,p}(\\bar{\\mathbb{P}},\\mathbb{Q})=W_{d,p}(g_{\\#}\\mathbb{P},g_{\\#}\\mathbb{Q})$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma 4 (Optimal Transportation Cost on Subspace) Let $\\mathcal{U}\\subseteq\\mathbb{R}^{n}$ and suppose $\\boldsymbol{\\mathcal{U}}$ is decomposed into two subspaces, $\\mathcal{U}=(\\mathcal{A},\\mathcal{X})$ , where $\\boldsymbol{\\mathcal{A}}$ corresponds to the subset of some coordinates and $\\mathcal{X}$ to its complements. Let $P_{\\mathcal{X}}$ denote the projection function onto the $\\mathcal{X}$ space, i.e., $P_{\\mathcal{X}}(u)$ projects $u\\in\\mathcal{U}$ onto $\\mathcal{X}$ components. Define a cost function $c(u,u^{\\prime})=d(P_{\\mathcal{X}}(u),P_{\\mathcal{X}}(u^{\\prime}))$ , where $d$ is a cost function on the space $\\mathcal{X}$ . Consider probability measures $\\mathbb{P},\\mathbb{Q}\\in\\mathcal{P}(\\mathcal{U})$ , and define $\\mathbb{P}_{\\mathcal{X}}=P_{\\mathcal{X}\\#}\\mathbb{P}$ and $\\mathbb{Q}_{\\mathcal{X}}=P_{\\mathcal{X}\\#}\\mathbb{Q}$ as the pushforward measures of $\\mathbb{P}$ and $\\mathbb{Q}$ under the projection $P_{\\mathcal{X}}$ , respectively, placing them in $\\mathcal{P}(\\mathcal{X})$ . Let $\\pi_{\\mathcal{X}}^{*}$ be the optimal transport plan concerning the Wasserstein distance $W_{d}(\\mathbb{P}_{\\mathcal{X}},\\mathbb{Q}_{\\mathcal{X}})$ . Then, any transport plan $\\pi\\in{\\mathcal{P}}({\\mathcal{U}}\\times{\\mathcal{U}})$ , whose marginal distribution over $\\mathcal X\\times\\mathcal X$ equals $\\pi_{\\mathcal{X}}^{*}$ , should also be an optimal solution for the Wasserstein distance $W_{c}({\\mathbb{P}},\\mathbb{Q})$ concerning the cost function $c$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Given any coupling $\\pi\\in\\Gamma(\\mathbb{P},\\mathbb{Q})$ , we consider elements $u=(a,x)$ and $u^{\\prime}=(a^{\\prime},x^{\\prime})$ in $\\mathcal{U}=\\mathcal{A}\\times\\mathcal{X}$ . The cost function $c$ is defined by $c((a,x),(a^{\\prime},x^{\\prime}))=d(x,\\dot{x}^{\\prime})$ , where $d$ is a metric on the space $\\mathcal{X}$ . By definition of optimal transport cost $W_{c}({\\mathbb{P}},\\mathbb{Q})$ we have: ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\pi\\in\\Gamma(\\mathbb{P},\\mathbb{Q})}{\\operatorname*{sup}}\\left\\lbrace\\int_{\\mathcal{U}\\times\\mathcal{U}}c((a,x),(a^{\\prime},x^{\\prime}))\\,d\\pi\\right\\rbrace=\\underset{\\pi\\in\\Gamma(\\mathbb{P},\\mathbb{Q})}{\\operatorname*{sup}}\\left\\lbrace\\int_{\\mathcal{U}\\times\\mathcal{U}}d(x,x^{\\prime})\\,d\\pi\\right\\rbrace=}\\\\ &{\\underset{\\pi\\in\\Gamma(\\mathbb{P},\\mathbb{Q})}{\\operatorname*{sup}}\\left\\lbrace\\int_{\\mathcal{X}\\times\\mathcal{X}}\\left(\\int_{A\\times\\mathcal{A}}d(x,x^{\\prime})\\,d\\pi((a,a^{\\prime})|\\mathbf{X}=x,\\mathbf{X}^{\\prime}=x^{\\prime})\\right)d(\\pi)_{X\\times\\mathcal{X}}\\right\\rbrace=}\\\\ &{\\underset{\\pi\\in\\Gamma(\\mathbb{P},\\mathbb{Q})}{\\operatorname*{sup}}\\left\\lbrace\\int_{\\mathcal{X}\\times\\mathcal{X}}d(x,x^{\\prime})d(\\pi)_{X\\times\\mathcal{X}}\\right\\rbrace=\\underset{\\pi\\in\\Gamma(\\mathbb{P}),x,(\\mathbb{Q})}{\\operatorname*{sup}}\\left\\lbrace\\int_{\\mathcal{X}\\times\\mathcal{X}}d(x,x^{\\prime})\\pi\\right\\rbrace}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $(\\mathbb{P})_{\\mathcal{X}}$ and $(\\pi)_{\\mathcal{X}\\times\\mathcal{X}}$ is marginal distribution over $\\mathcal{X}$ and $\\mathcal X\\times\\mathcal X$ respectively. $\\pi(.|\\mathbf{X}=x,\\mathbf{X}^{\\prime}=$ $x^{\\prime}$ ) is conditional distribution of $\\pi$ condition to the first and second $\\mathcal{X}$ components equal to $x$ and $x^{\\prime}$ . We observe that this integral effectively only depends on the $\\mathcal{X}$ component since the cost function $c$ does not involve $\\boldsymbol{\\mathcal{A}}$ . Hence, we reduce the expression to: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\pi\\in\\Gamma((\\mathbb{P})_{\\mathcal{X}},(\\mathbb{Q})_{\\mathcal{X}})}\\left\\{\\int_{\\mathcal{X}\\times\\mathcal{X}}d(x,x^{\\prime})\\pi\\right\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The Eq. 11 shows that the optima cost function of $W_{c}({\\mathbb{P}},\\mathbb{Q})$ equals to $W_{c}((\\mathbb{P})_{\\mathcal{X}},(\\mathbb{Q})_{\\mathcal{X}})$ . Therefore if $\\pi_{\\mathcal{X}}^{*}$ be the optimal transport plan for $\\mathbb{P}_{\\mathcal{X}}$ to $\\mathbb{Q}_{\\mathcal{X}}$ with respect to $d$ on $\\mathcal{X}$ , then any coupling $\\pi$ in $\\mathcal{U}\\times\\mathcal{U}$ that its marginal distribution $(\\pi)_{\\mathcal{X}\\times\\mathcal{X}}$ equals $\\pi_{\\mathcal{X}}^{*}$ is the solution of optimal transport. It results that the conditional distribution $\\pi((.,.)|\\mathbf{X}=x,\\mathbf{X}^{\\prime}=x^{\\prime})$ could be any distribution. This completes the proof. ", "page_idx": 17}, {"type": "text", "text": "Lemma 5 Let $X$ and $A$ be sets, and let $f:X\\times A\\to\\mathbb{R}$ be a function. Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in X}\\operatorname*{sup}_{a\\in A}f(x,a)=\\operatorname*{sup}_{a\\in A}\\operatorname*{sup}_{x\\in X}f(x,a).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Define: ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\nL=\\operatorname*{sup}_{x\\in X}\\operatorname*{sup}_{a\\in A}f(x,a)\\quad{\\mathrm{and}}\\quad R=\\operatorname*{sup}_{a\\in A}\\operatorname*{sup}_{x\\in X}f(x,a).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To show that $L=R$ , we need to prove that $L\\leq R$ and $R\\leq L$ . Consider any $x\\in X$ and $a\\in A$ . By definition, $f(x,a)\\leq\\operatorname*{sup}_{a\\in A}f(x,a)$ for each fixed $x$ . Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\nf(x,a)\\leq\\operatorname*{sup}_{a\\in A}f(x,a)\\leq\\operatorname*{sup}_{x\\in X}\\operatorname*{sup}_{a\\in A}f(x,a)=R.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $f(x,a)$ was arbitrary, we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{a\\in A}f(x,a)\\leq R\\quad{\\mathrm{for~all~}}x\\in X,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and thus, ", "page_idx": 17}, {"type": "equation", "text": "$$\nL=\\operatorname*{sup}_{x\\in X}\\operatorname*{sup}_{a\\in A}f(x,a)\\leq R.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, for any fixed $a\\in A$ , $f(x,a)\\leq\\operatorname*{sup}_{x\\in X}f(x,a)$ . Hence, ", "page_idx": 17}, {"type": "equation", "text": "$$\nf(x,a)\\leq\\operatorname*{sup}_{x\\in X}f(x,a)\\leq\\operatorname*{sup}_{a\\in A}\\operatorname*{sup}_{x\\in X}f(x,a)=L.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "As before, since $f(x,a)$ was arbitrary, we conclude: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in X}f(x,a)\\leq L\\quad{\\mathrm{for~all~}}a\\in A,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and thus, ", "page_idx": 17}, {"type": "equation", "text": "$$\nR=\\operatorname*{sup}_{a\\in A}\\operatorname*{sup}_{x\\in X}f(x,a)\\leq L.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $L\\leq R$ and $R\\leq L$ , it follows that $L=R$ . Therefore, we have proven that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in X}\\operatorname*{sup}_{a\\in A}f(x,a)=\\operatorname*{sup}_{a\\in A}\\operatorname*{sup}_{x\\in X}f(x,a).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This demonstrates the Principle of the Iterated Suprema. ", "page_idx": 17}, {"type": "text", "text": "B.1 Proof of Theorem 1. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We prove the assertion in two steps: first, we assume that none of the sensitive attributes have parents, and second, we address and prove the general case. When all sensitive attributes do not have parents, in this case by definition2 semi-latent space equivalent with exogenous space and therefore $g=F^{-1}$ . First, we show that the worst-case loss quantity can be decomposed into sensitive and non-sensitive components like as below equation: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbb{Q}\\in\\mathbb{B}_{\\delta}\\left(\\mathbb{P}\\right)}\\left\\{\\underset{v\\sim\\mathbb{Q}}{\\mathbb{E}}\\left[\\psi(v)\\right]\\right\\}=\\operatorname*{sup}_{\\mathbb{Q}\\in\\mathbb{B}_{\\delta}\\left((F_{\\#}^{-1}\\mathbb{P})_{X}\\right)}\\left\\{\\underset{u_{x}\\sim\\mathbb{Q}}{\\mathbb{E}}\\left[\\operatorname*{sup}_{u_{a}\\in\\mathcal{U}_{A}}\\left\\{\\psi\\left(F\\left((u_{a},u_{x})\\right)\\right)\\right\\}\\right]\\right\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By the assumption, the CFDF has a form $d(v,v^{\\prime})=d_{\\mathcal{X}}(P_{\\mathcal{X}}(F^{-1}(v)),P_{\\mathcal{X}}(F^{-1}(v^{\\prime})))$ . By Def. 2 in a case that sensitive attributes have no parents then the semi-latent space coincides with exogenous space and the map between feature space and semi-latent space equals $g=F^{-1}$ . Therefore in the following equations, we use $g$ instead of $F^{-1}$ . Moreover since $g$ is invertible by Lemma 3, we can write: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{0\\in\\mathbb{B}_{\\delta}(\\mathbb{R})}{\\operatorname*{sup}}\\left\\lbrace\\underset{v^{\\prime}\\sim0}{\\mathbb{E}}[\\psi(v^{\\prime})]\\right\\rbrace=\\underset{0\\in\\mathbb{B}_{\\delta}(g_{\\#}\\mathbb{P})}{\\operatorname*{sup}}\\left\\lbrace\\underset{u^{\\prime}\\sim0}{\\mathbb{E}}[\\psi(F(u^{\\prime}))]\\right\\rbrace=}\\\\ &{\\quad\\underset{\\pi\\in\\mathcal{P}(\\mathcal{U}\\times\\mathcal{U})}{\\operatorname*{sup}}\\left\\lbrace\\underset{u^{\\prime}\\sim\\pi_{2}}{\\mathbb{E}}[\\psi(F(u^{\\prime}))]\\;\\middle|\\;u\\sim g_{\\#}\\mathbb{P}_{(u,u^{\\prime})\\sim\\pi}[\\tilde{d}_{x}(u,u^{\\prime})]\\le\\delta\\right\\rbrace=}\\\\ &{\\quad\\underset{\\pi\\in\\mathcal{P}(\\mathcal{U}\\times\\mathcal{U})}{\\operatorname*{sup}}\\left\\lbrace\\underset{u^{\\prime}\\sim\\pi_{2}}{\\mathbb{E}}[\\psi(F((u_{a}^{\\prime},u_{x}^{\\prime})))]\\;\\middle|\\;u\\sim g_{\\#}\\mathbb{P}_{(u,u^{\\prime})\\sim\\pi}\\mathbb{E}_{(u,u_{x}^{\\prime})}[d_{x}(u_{x},u_{x}^{\\prime})]\\le\\delta\\right\\rbrace=}\\\\ &{\\quad\\underset{\\pi\\in\\mathcal{P}(\\mathcal{U}\\times\\mathcal{U})}{\\operatorname*{sup}}\\left\\lbrace\\int_{u}\\psi(F((u_{a}^{\\prime},u_{x}^{\\prime})))\\;d\\pi_{2}(u^{\\prime})\\;\\middle|\\;u\\sim g_{\\#}\\mathbb{P}_{(u,u_{x}^{\\prime})\\sim\\pi\\times x}[d_{x}(u_{x},u_{x}^{\\prime})]\\le\\delta\\right\\rbrace=*,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\tilde{d}$ be a cost function on $\\boldsymbol{\\mathcal{U}}$ defined as $\\tilde{d}(u,u^{\\prime})=d_{X}(P_{X}(u),P_{X}(u^{\\prime}))$ , $\\pi_{2}$ denotes the marginal distribution on second part and $\\mathbb{B}_{\\delta}(g_{\\#}\\mathbb{P})=\\{\\mathbb{Q}\\in\\mathcal{P}(\\mathcal{U}):W_{\\tilde{d}}(\\mathbb{Q},g_{\\#}\\mathbb{P})\\le\\delta\\}$ . Using the disintegration theorem ( [36] Chapter 3), the joint distribution $\\stackrel{\\leftarrow}{\\pi_{2}}\\,\\cdot$ can be decomposed into the product of the conditional distribution of $\\mathcal{U}_{A}$ given $\\mathcal{U}_{\\mathcal{X}}$ and the marginal distribution on $\\mathcal{U}_{\\mathcal{X}}$ . Therefore we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{U}}\\psi(F((u_{a}^{\\prime},u_{x}^{\\prime})))\\,d\\pi_{2}(u^{\\prime})=\\int_{\\mathcal{U}_{x}}\\left(\\int_{\\mathcal{U}_{A}}\\psi(F((u_{a},u_{x})))\\,d\\pi_{2}(u_{a}^{\\prime}|\\mathbf{U}_{X}=u_{x}^{\\prime})\\right)d\\chi(\\pi_{2})_{X}(u_{x}^{\\prime}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $(\\pi_{2})_{\\mathcal{X}}$ is the marginal distribution of $\\pi_{2}$ over the non-sensitive part and $\\pi_{2}(u_{a}^{\\prime}|\\mathbf{U}_{\\mathcal{X}}\\,=\\,u_{x}^{\\prime})$ is a conditional distribution of the sensitive part of exogenous space condition by $\\mathbf{U}_{\\mathcal{X}}=u_{x}^{\\prime}$ . By disintegration formula, $({^*})$ can be rewritten as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{x_{2}\\in P(U)}{\\operatorname*{sup}}\\left\\lbrace\\int_{u}\\psi(F((u_{\\alpha}^{\\prime},u_{x}^{\\prime})))d\\pi_{2}(u^{\\prime})\\Big|\\pi\\in\\mathcal{P}(U\\times\\mathcal{U}),\\pi_{1}=g_{\\#}\\mathbb{P}_{\\{u_{\\alpha},u_{x}^{\\prime}\\}\\sim\\pi}\\big[\\mathbb{d}(u_{x},u_{x}^{\\prime})\\big]:\\right.}\\\\ &{\\left.=\\underset{\\pi_{2}\\in P(U)}{\\operatorname*{sup}}\\left\\lbrace\\int_{u_{\\mathcal{X}}}\\left(\\int_{u_{\\alpha}}\\psi(F(u_{\\alpha},u_{x}))\\,d\\pi_{2}(u_{\\alpha}^{\\prime}|\\mathbf{U}_{x}=u_{x}^{\\prime})\\right)d_{X}(\\pi_{2})_{X}(u_{x}^{\\prime})\\,\\right|}\\\\ &{\\pi\\in\\mathcal{P}(U\\times\\mathcal{U}),\\pi_{1}=g_{\\#}\\mathbb{P}_{\\{u_{\\alpha},u_{x}^{\\prime}\\}\\sim\\pi}\\big[d(u_{x},u_{x}^{\\prime})\\big]\\le\\delta\\right\\rbrace}\\\\ &{=\\underset{(\\pi_{2})\\times\\in P(U_{\\mathcal{X}})}{\\operatorname*{sup}}\\left\\lbrace\\int_{u_{\\mathcal{X}}}\\frac{\\operatorname*{sup}}{\\operatorname*{min}(\\mathbf{\\bar{x}},u_{x}^{\\prime}=u_{x}^{\\prime})}\\,\\Big\\lbrace\\int_{u_{\\alpha}}\\psi(F((u_{\\alpha}^{\\prime},u_{x}^{\\prime})))d\\pi_{2}(u_{\\alpha}^{\\prime}|\\mathbf{U}_{x}=u_{x}^{\\prime})\\Big\\rbrace d(\\pi_{2})_{X}(u_{x}^{\\prime})\\,\\right|}\\\\ &{\\pi\\in\\mathcal{P}(U\\times\\mathcal{U}),\\pi_{2}=(g_{\\#}\\mathbb{P})_{X},\\big(\\mu_{\\alpha},u_{x}^{\\prime}\\big)\\mathbb{P}_{(u_{\\alpha},u_{x}^{\\prime})\\sim\\pi}\\big[d_{X}(u_{x},u_{x}^{\\prime})\\big]\\le\\delta\\biggr\\rbrace}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $d_{\\mathcal{X}}$ depends only on the non-sensitive components, it follows from Lemma 4 that $\\pi_{2}(.|\\mathbf{U}_{\\mathcal{X}}=$ $u_{x}^{\\prime}$ ) can achieve any distribution. Moreover, since it does not depend on the Wasserstein distance in each coupling, the marginal distribution of the sensitive attribute can be considered independent of the marginal distribution of the non-sensitive attributes. Therefore, the supremum over $\\pi_{2}(.|\\mathbf{U}_{\\mathcal{X}}=u_{x}^{\\prime})$ of integral equals the supremum of $\\psi(F((u_{a}^{\\prime},u_{x}^{\\prime})))$ ) over all values of $u_{a}^{\\prime}$ . Furthermore, the distribution $\\pi_{2}(u_{a}^{\\prime}|u_{x}^{\\prime})$ does not influence the value of the Wasserstein distance. Based on these points, the last equation can be rewritten as: ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\pi_{2}\\in P(\\mathcal{U}_{\\mathcal{X}})}{\\operatorname*{sup}}\\Bigg\\{\\int_{\\mathcal{U}_{\\mathcal{X}}}\\underset{u_{*}^{\\prime}\\in\\mathcal{U}_{\\mathcal{A}}}{\\operatorname*{sup}}\\psi(F((u_{u_{*}^{\\prime}}^{\\prime},u_{x_{*}^{\\prime}}^{\\prime})))d\\pi_{2}\\;\\Big|\\;\\pi\\in\\mathcal{P}(\\mathcal{U}_{\\mathcal{X}}\\times\\mathcal{U}_{\\mathcal{X}})\\,,}\\\\ {\\pi_{1}=(g_{\\#}\\mathbb{P})\\,\\chi,}\\\\ {\\pi_{1}=(g_{\\#}\\mathbb{P})\\,\\chi,}\\\\ {\\underset{\\pi_{2}\\in P(\\mathcal{U}_{\\mathcal{X}})}{\\operatorname*{sup}}\\,\\Bigg\\{\\underset{u_{*}^{\\prime}\\in\\mathcal{V}_{\\mathcal{A}}}{\\operatorname*{lim}}\\,[d(u_{x_{*}^{\\prime}},u_{x_{*}^{\\prime}}^{\\prime})]\\leq\\delta\\Bigg\\}=}\\\\ {\\underset{\\pi_{2}\\in P(\\mathcal{U}_{\\mathcal{X}})}{\\operatorname*{sup}}\\,\\Bigg\\{\\underset{u_{*}^{\\prime}\\in\\mathcal{V}_{\\mathcal{A}}}{\\left\\mathbb{E}}\\,\\Bigg[\\underset{u_{*}^{\\prime}\\in\\mathcal{U}_{\\mathcal{A}}}{\\operatorname*{sup}}\\,\\psi(F((u_{*}^{\\prime},u_{x_{*}^{\\prime}}^{\\prime})))\\Bigg]\\;\\Bigg|\\;\\pi\\in\\mathcal{P}(\\mathcal{U}_{\\mathcal{X}}\\times\\mathcal{U}_{\\mathcal{X}})\\,,}\\\\ {\\pi_{1}=(g_{\\#}\\mathbb{P})\\,\\chi,}\\\\ {\\pi_{1}=(g_{\\#}\\mathbb{P})\\,\\underset{(u_{*},u_{*}^{\\prime})\\sim\\mathcal{V}_{\\mathcal{A}}}{\\left\\{\\sum_{\\sigma_{1},\\sigma_{1}^{\\prime}\\in\\mathcal{V}_{\\mathcal{A}}}\\,[d(u_{*},u_{*}^{\\prime})]\\leq\\delta\\right\\}}=}\\\\ {\\underset{\\mathcal{Q}\\in\\mathcal{B}_{\\mathcal{X}}}{\\operatorname*{sup}}\\,\\Bigg\\{\\underset{u_{*}^{\\prime}\\in\\mathcal{V}_{\\mathcal{A}}}{\\left\\mathrm{lim}}\\,[\\underset{u_{*}^{\\prime}\\in\\mathcal{V}_{\\mathcal{A}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The last equation concludes the proof of Eq. 12. Similarly, by altering the order of integration in Eq. 13, we arrive at the following equation: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbb{Q}\\in\\mathbb{B}_{\\delta}\\left(\\mathbb{P}\\right)}\\left\\{\\underset{v\\sim\\mathbb{Q}}{\\mathbb{E}}\\left[\\psi(v)\\right]\\right\\}=\\operatorname*{sup}_{u_{a}\\in\\mathcal{U}_{A}}\\left\\{\\operatorname*{sup}_{\\mathbb{Q}\\in\\mathbb{B}_{\\delta}\\left(\\left(g_{\\#}\\mathbb{P}\\right)x\\right)}\\left\\{\\underset{u_{x}\\sim\\mathbb{Q}}{\\mathbb{E}}\\left[\\psi\\left(F\\left((u_{a},u_{x})\\right)\\right)\\right]\\right\\}\\right\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To proceed with the proof, we utilize the strong duality theorem. There are various kinds of duality theorems for DRO, but we apply the one proposed by Blanchet et al. [11]. ", "page_idx": 19}, {"type": "text", "text": "Strong duality [11]. Suppose the transportation cost $c:\\mathcal{Z}\\times\\mathcal{Z}\\to[0,\\infty]$ satisfies $c(z,z)=0$ for all $z\\in{\\mathcal{Z}}$ and lower semi-continuous. Then for any reference probability distribution $\\mathbb{P}$ and upper semi-continuous $\\psi:\\mathcal{Z}\\to\\mathbb{R}$ satisfying $\\mathbb{E}[f(\\mathbf{Z})]<\\infty$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\mathbb{Q}\\in\\mathbb{B}_{\\delta}(\\mathbb{P})}{\\operatorname*{sup}}\\mathbb{E}\\left[\\psi(\\mathbf{Z})\\right]=\\underset{\\lambda\\geq0}{\\operatorname*{inf}}\\ \\ \\lambda\\delta+\\frac{\\mathbb{E}}{\\mathbb{P}}\\left[\\psi_{\\lambda}(\\mathbf{Z})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\psi_{\\lambda}(z):=\\operatorname*{sup}_{z^{\\prime}\\in{\\mathcal{Z}}}\\big\\{\\psi(z^{\\prime})-\\lambda c(z,z^{\\prime})\\big\\}.$ . ", "page_idx": 19}, {"type": "text", "text": "Based on the assumption about the CFDF, where only $d(v,\\ddot{v}_{a^{\\prime}})=0$ , it follows that $d(x,x^{\\prime})=0$ only if $x=x^{\\prime}$ . Therefore, we can apply the duality theorem to Eq. 12. According to the duality theorem, it can be expressed as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{u_{a}\\in\\mathcal{U}_{A}}{\\operatorname*{sup}}\\left\\{\\underset{\\mathbb{Q}\\in\\mathbb{B}_{\\delta}((g_{\\#}\\mathbb{P})_{X})}{\\operatorname*{sup}}\\left\\{\\underset{u_{x}\\sim\\mathbb{Q}}{\\mathbb{E}}[\\psi\\left(F\\left((u_{a},u_{x}))\\right)]\\right\\}\\right\\}=\\underset{\\lambda\\geq0}{\\operatorname*{inf}}\\left\\{\\begin{array}{l l}{\\lambda\\delta+\\underset{u_{x}\\sim(g_{\\#}\\mathbb{P})_{X}}{\\mathbb{E}}[\\eta_{\\lambda}(u_{x})]\\right\\}}\\\\ {\\times t\\mathrm{here}\\ \\eta_{\\lambda}(u_{x})\\ =\\ \\operatorname*{sup}_{u_{x}^{\\prime}\\in\\mathcal{U}_{X}}\\left\\{\\operatorname*{sup}_{u_{a}\\in\\mathcal{U}_{A}}\\left\\{\\psi(F((u_{a},u_{x}^{\\prime}))\\right\\}-\\lambda d_{X}(u_{x},u_{x}^{\\prime})\\right\\}\\cdot\\mathrm{\\normalfont~By~using~len}}\\end{array}\\right.}\\\\ &{\\operatorname*{sup}_{u_{x}^{\\prime}\\in\\mathcal{U}_{X}}\\left\\{\\operatorname*{sup}_{u_{a}\\in\\mathcal{U}_{A}}\\left\\{\\psi(F((u_{a},u_{x}^{\\prime})))\\right\\}\\right\\}=\\operatorname*{sup}_{u_{a}\\in\\mathcal{U}_{A}}\\left\\{\\operatorname*{sup}_{u_{x}^{\\prime}\\in\\mathcal{U}_{X}}\\left\\{\\psi(F((u_{a},u_{x}^{\\prime}))\\right\\}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now, since sensitive attributes don\u2019t have parents then two spaces $\\mathcal{U}_{A}$ and $\\boldsymbol{\\mathcal{A}}$ are equal. By applying Lemma 2, we can replace the above equation with hard and soft interventions as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta_{\\lambda}(u_{x})=\\displaystyle\\operatorname*{sup}_{a\\in A}\\left\\{\\operatorname*{sup}_{u_{x}^{*}\\in\\mathcal{U}_{x}}\\left\\{\\psi(F((a,u_{x}^{\\prime})))-\\lambda d_{X}(u_{x},u_{x}^{\\prime})\\right\\}\\right\\}=}\\\\ &{\\operatorname*{sup}_{a\\in A}\\left\\{\\operatorname*{sup}_{\\alpha}\\left\\{\\psi(F((a,u_{x}+\\Delta)))-\\lambda d_{X}(u_{x},u_{x}+\\Delta)\\right\\}\\right\\}=}\\\\ &{\\left.\\begin{array}{l}{\\displaystyle\\operatorname*{sup}_{a\\in A}\\left\\{\\operatorname*{sup}_{x}\\psi(F((a,u_{x}+\\Delta)))-\\lambda d_{X}(P_{x}((a,u_{x}+\\Delta)))\\right\\}=}\\\\ {\\displaystyle\\operatorname*{sup}_{a\\in A}\\left\\{\\operatorname*{sup}_{a\\in A}\\left(F((a,u_{x}+\\Delta))-\\lambda d_{X}(P_{x}((a,u_{x})),P_{X}((a,u_{x}+\\Delta)))\\right\\}=}\\\\ {\\displaystyle\\operatorname*{sup}_{a\\in A}\\left\\{\\operatorname*{sup}_{x}\\psi(F((a,u_{x}+\\Delta)))-\\lambda d_{X}(P_{x}(g^{-1}(g(a,u_{x})))),P_{X}(g^{-1}(g(a,u_{x}+\\Delta))))\\right\\}=}\\\\ {\\displaystyle\\operatorname*{sup}_{a\\in A}\\left\\{\\operatorname*{sup}_{A\\in\\mathcal{U}_{x}}\\psi(\\operatorname*{cr}(\\tilde{v}^{a},\\Delta))-\\lambda d_{X}(P_{x}(g^{-1}(g(a,u_{x})))),P_{X}(g^{-1}(g(a,u_{x}+\\Delta)))\\right\\}=}\\\\ {\\displaystyle\\operatorname*{sup}_{a\\in A}\\left\\{\\operatorname*{sup}_{A\\in\\mathcal{U}_{x}}\\psi(\\operatorname{CF}(\\tilde{v}^{a},\\Delta))-\\lambda c(\\tilde{v}^{a},\\operatorname{CF}(\\tilde{v}^{a},\\Delta))\\right\\}=\\displaystyle\\operatorname*{sup}_{a\\in A}\\left\\{\\operatorname{sup}_{\\lambda}(\\tilde{v}^{a})\\right\\}}\\end{array}\\right.(17)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Where $\\begin{array}{r}{\\tilde{\\psi}_{\\lambda}(\\ddot{v}^{a}):=\\operatorname*{sup}_{\\Delta\\in\\mathcal{U}_{\\mathcal{X}}}\\psi(\\mathbf{CF}(\\ddot{v}^{a},\\Delta))-\\lambda d(\\ddot{v}^{a},\\mathbf{CF}(\\ddot{v}^{a},\\Delta))}\\end{array}$ . The equations $F((a,u_{x}+\\Delta))=$ $\\mathbf{CF}(v,\\Delta)$ and $F((a^{\\prime},u_{x}{+}\\Delta))={\\bf C}{\\bf F}(\\ddot{v}^{a^{\\prime}},\\Delta)$ hold true according to Lemma 2. Finally, by substituting $\\ddot{v}^{a}=\\mathbf{C}\\mathbf{F}(v,a)$ into the equation, we prove the equation: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbb{Q}\\in\\mathbb{B}_{\\delta}(\\mathbb{P})}\\left\\{\\underset{v\\sim\\mathbb{Q}}{\\mathbb{E}}[\\psi(v)]\\right\\}=\\operatorname*{inf}_{\\lambda\\geq0}\\left\\{\\lambda\\delta^{p}+\\underset{v\\sim\\mathbb{P}}{\\mathbb{E}}\\left[\\operatorname*{sup}_{a\\in\\mathcal{A}}\\psi_{\\lambda}(\\ddot{v}_{a})\\right]\\right\\},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\psi_{\\lambda}(v)$ is defined as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\psi_{\\lambda}(v):=\\operatorname*{sup}_{\\Delta\\in\\mathcal{X}}\\left\\{\\psi(\\mathbf{CF}_{0}(v,\\Delta))-\\lambda^{p}d(v,\\mathbf{CF}_{0}(v,\\Delta))\\right\\},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since, in this case, $C F$ is equivalent to $\\mathbf{CF}_{0}$ , this completes the proof for case one. ", "page_idx": 20}, {"type": "text", "text": "Now consider the scenario where sensitive attributes have parents. Eq. 17 shows in strong duality computation it needs to compute function in intervened $\\mathcal{M}$ concerning the sensitive attributes levels. In this case, instead of using the structural causal model $\\mathcal{M}$ , it is sufficient to employ the parent-free sensitive attribute SCM (Def. 2), $\\mathbf{\\mathcal{M}}_{0}$ . $\\mathcal{M}_{\\mathrm{0}}$ aligns with the semi-latent space and is compatible with the representation form outlined in Proposition 1. By adopting this strategy, we transform $\\mathcal{M}$ into a model where sensitive attributes do not have parents. The proof procedure for $\\mathbf{\\mathcal{M}}_{0}$ remains the same as for $\\mathcal{M}$ and completes the proof. ", "page_idx": 20}, {"type": "text", "text": "Lemma 6 Let $(\\mathcal{Z},c)$ be a space with cost function $c$ , $\\mathbb{P}_{N}$ an empirical probability measure based on observations $\\{z_{i}\\}_{i=1}^{N}$ , and define $\\mathbb{Q}$ as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{Q}=\\mathbb{P}_{N}-\\frac{1}{N}\\delta_{z_{1}}+\\frac{1}{N}\\delta_{z_{1}^{\\prime}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\delta_{z_{1}}$ and $\\delta_{z_{1}^{\\prime}}$ are Dirac measures at $z_{1}$ and $z_{1}^{\\prime}$ , respectively. Then, the $p$ -Wasserstein distance between $\\mathbb{P}_{N}$ and $\\textcircled{\\scriptsize{1}}$ is given by: ", "page_idx": 20}, {"type": "equation", "text": "$$\nW_{c,p}(\\mathbb{P}_{N},\\mathbb{Q})=(\\frac{1}{N})^{\\frac{1}{p}}c(z_{1},z_{1}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. The definition of the $p$ -Wasserstein distance between two probability measures $\\mathbb{P}_{N}$ and $\\mathbb{Q}$ is: ", "page_idx": 20}, {"type": "equation", "text": "$$\nW_{c,p}(\\mathbb{P}_{N},\\mathbb{Q})=\\left(\\operatorname*{inf}_{\\pi\\in\\Gamma(\\mathbb{P}_{N},\\mathbb{Q})}\\int_{\\mathcal{Z}\\times\\mathcal{Z}}c(z,z^{\\prime})^{p}\\,d\\pi(z,z^{\\prime})\\right)^{\\frac{1}{p}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\Gamma(\\mathbb{P}_{N},\\mathbb{Q})$ represents the set of all couplings of $\\mathbb{P}_{N}$ and $\\mathbb{Q}$ . Since $\\mathbb{Q}$ is obtained by transferring a mass of $\\textstyle{\\frac{1}{N}}$ from $z_{1}$ to $z_{1}^{\\prime}$ , the optimal transport plan under the constraint that $\\mathbb{P}_{N}$ and $\\mathbb{Q}$ differ only at two points involves only moving the mass $\\textstyle{\\frac{1}{N}}$ from $z_{1}$ to $z_{1}^{\\prime}$ . The cost of this transportation is $c(z_{1},z_{1}^{\\prime})^{p}$ , and because the entire mass $\\textstyle{\\frac{1}{N}}$ is being moved: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{Z}\\times\\mathcal{Z}}c(z,z^{\\prime})^{p}\\,d\\pi(z,z^{\\prime})=c(z_{1},z_{1}^{\\prime})^{p}\\cdot\\frac{1}{N}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, substituting this into the formula for $W_{p}$ , we obtain: ", "page_idx": 20}, {"type": "equation", "text": "$$\nW_{c,p}(\\mathbb{P}_{N},\\mathbb{Q})=\\left(c(z_{1},z_{1}^{\\prime})^{p}\\cdot{\\frac{1}{N}}\\right)^{\\frac{1}{p}}=\\left({\\frac{1}{N}}\\right)^{\\frac{1}{p}}c(z_{1},z_{1}^{\\prime}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "thus proving the lemma. ", "page_idx": 20}, {"type": "text", "text": "Lemma 7 Assume that $f(a,x)$ is convex in $x$ for each fixed a and continuous in both a and x. Also, assume $f$ is uniformly continuous in $x$ across $a$ . If $A$ is compact, then the function defined by ", "page_idx": 20}, {"type": "equation", "text": "$$\ng(x)=\\operatorname*{sup}_{a\\in A}f(a,x)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "is convex and continuous in $x$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. To show that $g(x)$ is convex, consider any $x_{1},x_{2}$ in the domain and $\\lambda\\in[0,1]$ . By the definition of supremum and the convexity of $f(a,x)$ in $x$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\nf(a,\\lambda x_{1}+(1-\\lambda)x_{2})\\leq\\lambda f(a,x_{1})+(1-\\lambda)f(a,x_{2}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Taking the supremum over $a$ in $A$ on both sides, we get: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{a\\in A}f(a,\\lambda x_{1}+(1-\\lambda)x_{2})\\leq\\operatorname*{sup}_{a\\in A}(\\lambda f(a,x_{1})+(1-\\lambda)f(a,x_{2})).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using the properties of supremum, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{a\\in A}f(a,\\lambda x_{1}+(1-\\lambda)x_{2})\\leq\\lambda\\operatorname*{sup}_{a\\in A}f(a,x_{1})+(1-\\lambda)\\operatorname*{sup}_{a\\in A}f(a,x_{2}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, ", "page_idx": 21}, {"type": "equation", "text": "$$\ng(\\lambda x_{1}+(1-\\lambda)x_{2})\\leq\\lambda g(x_{1})+(1-\\lambda)g(x_{2}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "proving that $g(x)$ is convex. ", "page_idx": 21}, {"type": "text", "text": "To show continuity of $g(x)$ at a point $x_{0}$ , consider any sequence $\\{x_{n}\\}$ converging to $x_{0}$ . Since $f$ is uniformly continuous in $x$ , given $\\epsilon>0$ , there exists $\\delta>0$ such that for all $x,y$ with $|x-y|<\\delta$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n|f(a,x)-f(a,y)|<\\epsilon\\quad{\\mathrm{for}}\\,{\\mathrm{all}}\\,a\\in A.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, ", "page_idx": 21}, {"type": "equation", "text": "$$\nf(a,x_{n})<f(a,x_{0})+\\epsilon\\quad\\mathrm{and}\\quad f(a,x_{0})<f(a,x_{n})+\\epsilon\\quad\\mathrm{for~all~}a\\in A\\mathrm{~and~}|x_{n}-x_{0}|<\\delta.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Taking the supremum over $a$ in $A$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\ng(x_{n})\\leq g(x_{0})+\\epsilon\\quad{\\mathrm{and}}\\quad g(x_{0})\\leq g(x_{n})+\\epsilon.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This implies ", "page_idx": 21}, {"type": "equation", "text": "$$\n|g(x_{n})-g(x_{0})|\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "establishing the continuity of $g(x)$ at $x_{0}$ . ", "page_idx": 21}, {"type": "text", "text": "Hence, we conclude that $\\operatorname*{sup}_{a\\in A}f(a,x)$ is convex and continuous in $x$ . ", "page_idx": 21}, {"type": "text", "text": "B.2 Proof of Theorem 2. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Let\u2019s consider the $\\ell(v,y,\\theta)\\,=\\,h({\\mathbf Y}\\,-\\,\\langle\\theta,{\\mathbf V}\\rangle)$ (or in abbreviation $\\ell(y))$ or $h({\\bf Y}\\cdot\\langle\\boldsymbol{\\theta},{\\bf V}\\rangle)$ where $h:\\mathbb{R}\\to\\mathbb{R}$ has one of the forms $|t|$ , $\\operatorname*{max}(0,t),\\,|t-\\tau|$ , or $\\operatorname*{nax}(0,t-\\tau)$ for some $\\tau\\geq0$ ", "page_idx": 21}, {"type": "text", "text": "First consider the case diam $\\left({\\mathcal{A}}\\right)=\\infty$ . By property of CFDF for each Since the distance of $v$ by its twins ${\\ddot{v}}_{a}$ is zero.Let $z\\ \\in\\ \\{z_{i}\\}$ If $\\mathbb{P}_{N}$ the empirical distribution by lemma 6 it can be seen for observation $Z=(v,y)$ the distribution ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{Q}_{a}=\\mathbb{P}_{N}-\\frac{1}{N}\\delta_{z}+\\frac{1}{N}\\delta_{(\\vec{v}_{a},y_{1})}\\Longrightarrow W_{c,p}(\\mathbb{Q}_{a},\\mathbb{P}_{N})=0\\Longrightarrow\\mathbb{Q}_{a}\\in\\mathbb{B}_{\\delta}(\\mathbb{P}_{N}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This equation results that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\delta}(\\mathbb{P}_{N},\\theta)\\geq\\operatorname*{sup}_{a\\in A}\\mathcal{R}_{\\delta}(\\mathbb{Q}_{a},\\theta)\\geq\\mathcal{R}(\\mathbb{P}_{N},\\theta)-\\frac{1}{N}\\ell(v_{1})+\\frac{1}{N}\\operatorname*{sup}_{a\\in A}\\ell(\\ddot{v}_{a})|\\geq\\frac{1}{N}\\operatorname*{sup}_{a\\in A}\\ell(\\ddot{v}_{a})|\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $u\\,=\\,(u_{\\mathcal{A}},u_{\\mathcal{X}})$ such that $v\\,=\\,M u$ . By definition of hard intervention ${\\ddot{v}}_{a}$ is obtained by the formula ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ddot{v}_{a}=(M-M_{\\mathbf{pa}})\\times(u-(0,\\ldots,\\overbrace{a-u_{A}}^{=\\alpha},\\ldots,0)^{T})}\\\\ &{=M\\times u-\\underbrace{M\\times(0,\\ldots,\\overbrace{\\alpha,\\ldots,\\alpha,\\ldots,0)}^{M_{A}\\times\\alpha}-\\overbrace{M_{\\mathbf{pa}}\\times\\overbrace{u}^{e}+M_{\\mathbf{pa}}\\times(0,\\ldots,\\alpha,\\ldots,0)}^{C}}_{\\mathrm{~}}0}\\\\ &{=v-M_{A}\\times\\alpha-C}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $M_{\\mathbf{pa}}$ refers to the effect of parents of sensitive variables and $M_{A}$ is the columns of matrix $M$ related to sensitive attributes that show the effects of sensitive attributes on non-sensitive variables. By substituting that last equation in the loss function we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\delta}(\\mathbb{P}_{N},\\theta)\\geq\\frac{1}{N}\\operatorname*{sup}_{a\\in\\mathcal{A}}\\ell(v-M_{A}\\times\\alpha-C)\\geq\\frac{1}{N}\\operatorname*{sup}_{\\alpha\\to\\infty}O(\\theta^{T}M_{A}\\times\\alpha)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $B$ is some constant value. with the assumptions about loss function, all of them by choosing proper $z_{1}$ , its behavior when $\\alpha$ is large enough is linear so can be approximated by its input value. Now Since the diam $\\left({\\mathcal{A}}\\right)=\\infty$ therefore the value of $\\alpha$ goes to the infinity. Therefore to prevent the value of $\\mathcal{R}_{\\delta}(\\mathbb{P}_{N},\\theta)$ it needs that the expression $\\theta^{T}M_{A}^{\\bar{\\,}}\\times\\alpha=0$ in the other word the $\\mathrm{\\Delta}P_{A}(M^{T}\\theta)$ needs to be zero. This condition implies that for all $a\\in A$ we have $\\ell(v)=\\ell(\\ddot{v}_{a})$ . By using strong duality 8 we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbb{Q}\\in\\mathbb{B}_{\\delta}(\\mathbb{P})}\\left\\{\\underset{v\\sim\\mathbb{Q}}{\\mathbb{E}}[\\ell(v)]\\right\\}=\\operatorname*{inf}_{\\lambda\\geq0}\\left\\{\\lambda\\delta+\\underset{v\\sim\\mathbb{P}}{\\mathbb{E}}\\left[\\operatorname*{sup}_{a\\in\\mathcal{A}}\\ell_{\\lambda}(\\ddot{v}_{a})\\right]\\right\\}=\\operatorname*{inf}_{\\lambda\\geq0}\\left\\{\\lambda\\delta+\\underset{v\\sim\\mathbb{P}}{\\mathbb{E}}[\\ell_{\\lambda}(v)]\\right\\},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{\\lambda}(v)=\\underset{\\Delta\\in\\mathcal{X}}{\\operatorname*{sup}}\\left\\lbrace\\ell(\\mathbf{C}\\mathbf{F}(v,\\Delta))-\\lambda d(v,\\mathbf{C}\\mathbf{F}(v,\\Delta))\\right\\rbrace=}\\\\ &{\\qquad\\qquad\\operatorname*{sup}_{\\Delta\\in\\mathcal{X}}h(\\theta^{T}M_{0}((u_{a},u_{x}+\\Delta)))-\\lambda d_{\\mathcal{X}}(u_{x},u_{x}+\\Delta)=}\\\\ &{\\qquad\\quad\\underset{\\Delta\\in\\mathcal{X}}{\\operatorname*{sup}}h(P_{\\mathcal{X}}(M^{T}\\theta)^{T}(u_{x}+\\Delta))-\\lambda d_{\\mathcal{X}}(u_{x},u_{x}+\\Delta)=}\\\\ &{\\qquad\\quad\\mathrm{sup}\\ h(\\langle\\theta_{0},u_{x}+\\Delta\\rangle)-\\lambda\\left\\|\\Delta\\right\\|=h_{\\lambda}(u_{x})}\\\\ &{\\qquad\\quad\\mathrm{sup}\\ h(\\langle\\theta_{0},u_{x}+\\Delta\\rangle)-\\lambda\\left\\|\\Delta\\right\\|=h_{\\lambda}(u_{x})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In the equation 19, $M_{0}$ is reduced-form mapping of the parent-free sensitive attribute $\\mathcal{M}_{\\mathrm{0}}$ . By the definition it is easy in both SCM, the effect of the sensitive attributes is equal therefore $P_{A}(M_{0}^{T}\\dot{\\theta})=$ $P_{A}(M^{T}\\theta)=0$ . Moreover since in $\\mathcal{M}_{0}$ the structure of non-sensitive attribute has not changed then $\\dot{P_{\\mathcal{X}}}(M_{0}^{T}\\theta)=P_{\\mathcal{X}}(M^{T}\\theta)$ . Finally, by substituting $\\theta_{0}=P_{\\mathcal{X}}(M^{T}\\theta)$ , it can be seen that the problem of finding worst-case loss quantity converts to the regular problem in space $\\mathcal{X}$ . This problem was solved previously in works of [14, 58, 25, 66]. By using Theorem 3.2 and 3.3 and Proposition 4.1 and 4.2, of work by Chu et al. [14], we can write ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\lambda\\geq0}{\\operatorname*{inf}}\\left\\{\\lambda\\delta+\\underset{v\\sim\\mathbb{P}_{N}}{\\mathbb{E}}[\\ell_{\\lambda}(v)]\\right\\}=\\underset{\\lambda\\geq0}{\\operatorname*{inf}}\\left\\{\\lambda\\delta+\\underset{u_{x}\\sim(g_{\\#}\\mathbb{P}_{N})_{x}}{\\mathbb{E}}[h_{\\lambda}(u_{x})]\\right\\}=}\\\\ &{\\qquad\\Big(\\mathcal{R}((g_{\\#}\\mathbb{P}_{N})_{x},\\theta_{0})^{\\frac{1}{p}}+\\delta\\left\\Vert\\theta_{0}\\right\\Vert_{*}\\Big)^{p}=\\Big(\\mathcal{R}(\\mathbb{P}_{N},\\theta)^{\\frac{1}{p}}+\\delta\\left\\Vert P_{\\mathcal{X}}(M^{T}\\theta)\\right\\Vert_{*}\\Big)^{p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The equality $\\mathcal{R}((g_{\\#}\\mathbb{P}_{N})_{\\mathcal{X}},\\theta_{0})=\\mathcal{R}(\\mathbb{P}_{N},\\theta)$ holds by definition and property $P_{A}(M^{T}\\theta)=0$ . The last equation completes the proof of the first case. ", "page_idx": 22}, {"type": "text", "text": "Now let\u2019s consider the case that diam $(A)<\\infty$ . ", "page_idx": 22}, {"type": "text", "text": "Case: $p\\in(1,\\infty)$ . Lets consider Eq. 16 it implies that: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\delta}(\\mathbb{P})=\\operatorname*{inf}_{\\lambda\\geq0}\\left\\{\\begin{array}{l l}{\\lambda\\delta+\\underset{u_{x}\\sim(g_{\\#}\\mathbb{P})_{x}}{\\mathbb{E}}\\left[\\tilde{\\ell}_{\\lambda}(u_{x})\\right]\\right\\}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where, $\\begin{array}{r}{\\tilde{\\ell}(u_{x})=\\operatorname*{sup}_{u_{a}\\in\\mathcal{U}_{A}}\\left\\{\\ell(M(u_{a},u_{x}^{\\prime})\\right\\}}\\end{array}$ and $\\begin{array}{r}{\\eta_{\\lambda}(u_{x})=\\operatorname*{sup}_{\\Delta\\in\\mathcal{U}_{\\mathcal{X}}}\\tilde{\\ell}(u_{x}+\\Delta)-\\lambda d_{\\mathcal{X}}(u_{x},u_{x}+\\Delta)\\}}\\end{array}$ By assumption, the whole type of loss functions are form $h(\\langle\\theta,\\bar{v}\\rangle)$ and convex and continuous. $h$ can be written by $M$ in the form $h(\\langle\\theta,M(u_{a},u_{x})\\rangle)$ . Then $\\begin{array}{r}{\\tilde{\\ell}(u_{x})=\\operatorname*{sup}_{u_{a}\\in\\mathcal{U}_{A}}h(P_{A}(M^{T}\\theta)u_{a}+}\\end{array}$ $P_{\\mathcal{X}}(M^{T}\\theta)u_{x})$ . Since all forms of function are uniformly continuous concerning the $u_{x}$ then lemma 7 implies that the $\\tilde{\\ell}(u_{x})$ is still continuous and convex. Theorem 6 and 7 of work [66] states that: ", "page_idx": 22}, {"type": "text", "text": "Theorem. [66] Let $\\ell:\\mathbb{R}\\rightarrow\\mathbb{R}$ be a non-negative, Lipschitz continuous and convex function. For an integer $p\\in(1,\\infty)$ , suppose that for any $\\mathbb{P}\\in\\mathcal{P}(\\mathcal{X})$ , and $\\epsilon\\geq0$ , we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbb{Q}\\in\\mathbb{B}_{\\delta}(\\mathbb{P})}\\mathbb{E}_{x\\sim\\mathbb{Q}}[\\ell^{p}(\\theta^{T}x)]=\\left(\\left(\\mathbb{E}_{x\\sim\\mathbb{P}}[\\ell^{p}(\\theta^{T}x)]\\right)^{1/p}+\\delta\\left\\Vert\\theta\\right\\Vert_{*}\\right)^{p}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By applying above theorem in Eq. 20 it can be seen: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}_{\\delta}(\\mathbb{P})=\\left(\\left(\\mathbb{E}_{u_{x}\\sim(g_{\\#}\\mathbb{P})_{x}}[\\tilde{\\ell}^{p}(u_{x})]\\right)^{1/p}+\\delta\\left\\|P_{\\mathcal{X}}(M^{T}\\theta)\\right\\|_{*}\\right)^{p}=}\\\\ {\\left(\\left(\\mathbb{E}_{v\\sim\\mathbb{P}[\\operatorname*{sup}_{a\\in\\mathcal{A}}^{p}]}\\right)^{1/p}+\\delta\\left\\|P_{\\mathcal{X}}(M^{T}\\theta)\\right\\|_{*}\\right)^{p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Case: $p=1$ . To complete the proof we use Theorem 2 and Corollary 2 of Gao et al. work [27]. ", "page_idx": 23}, {"type": "text", "text": "Theorem [27] If $\\ell$ is Lipschitz $|\\ell(x_{1})-\\ell(x_{2})|\\leq L\\left\\|x_{k}-x_{0}\\right\\|$ and satisfies tightness at infinity, i.e. for every $v_{0}$ there exists sequence $\\{v_{k}\\}_{k=1}^{\\infty}\\in\\mathcal{V}$ such that $\\|x_{k}-x_{0}\\|\\to\\infty$ we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\|x_{k}-x_{0}\\|\\to\\infty}{\\frac{|\\ell(x_{k})-\\ell(x_{0})|}{\\|x_{k}-x_{0}\\|}}=L\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "then we have $\\mathcal{R}_{\\delta}(\\mathbb{P})=\\mathcal{R}(\\mathbb{P})+\\delta.L$ . ", "page_idx": 23}, {"type": "text", "text": "Now back to the Eq. 20. It is necessary to show that $\\tilde{\\ell}$ satisfies the Gao\u2019s theorem. Let $h(t)$ be one of the functions | $t|,(t-\\tau)_{+}$ , $(|t|-\\tau)_{+}$ . All of loss function can be written as form $h(y-\\langle\\theta,v\\rangle)$ or $h(y.\\,\\langle\\theta,v\\rangle)$ . It is easy to check that $h$ is Lipschitz with constant 1 and there exist $t_{k}$ such that for each $t_{0}$ we have $l i m_{k\\rightarrow\\infty}\\frac{\\vert h(t_{0}+t_{k})-h(t_{0})\\vert}{\\vert t_{k}\\vert}=1.$ ", "page_idx": 23}, {"type": "text", "text": "To use this theorem for $\\tilde{\\ell}$ , we need to prove that $\\tilde{\\ell}$ is Lipschitz and has a tightness condition at infinity. Since the $h$ is Lipschitz we know that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\iota_{u_{a}}\\in\\mathcal{U}_{A}:}&{|h(P_{A}(M^{T}\\theta)u_{a}+P_{X}(M^{T}\\theta)u_{x})-h(P_{A}(M^{T}\\theta)u_{a}+P_{X}(M^{T}\\theta)u_{x}^{\\prime})|\\leq}\\\\ &{|P_{X}(M^{T}\\theta)(u_{x}-u_{x}^{\\prime})|\\leq\\left\\|P_{X}(M^{T}\\theta)\\right\\|_{*}\\left\\|u_{x}-u_{x}^{\\prime}\\right\\|\\Rightarrow}\\\\ &{\\quad\\quad\\quad\\quad\\operatorname*{sup}_{u\\in\\mathcal{U}_{A}}|h(P_{A}(M^{T}\\theta)u_{a}+P_{X}(M^{T}\\theta)u_{x})-h(P_{A}(M^{T}\\theta)u_{a}+P_{X}(M^{T}\\theta)u_{x}^{\\prime})|}\\\\ &{\\quad\\quad\\quad\\quad\\operatorname*{sup}_{u\\in\\mathcal{U}_{A}}|h(P_{A}(M^{T}\\theta)u_{a}+P_{X}(M^{T}\\theta)u_{x})-h(P_{A}(M^{T}\\theta)u_{a}+P_{X}(M^{T}\\theta)u_{x}^{\\prime})|=}\\\\ &{\\quad\\quad\\quad\\operatorname*{sup}_{u\\in\\mathcal{U}_{A}}h(P_{A}(M^{T}\\theta)u_{a}+P_{X}(M^{T}\\theta)u_{x})-\\displaystyle\\operatorname*{sup}_{u\\in\\mathcal{U}_{A}}h(P_{A}(M^{T}\\theta)u_{a}+P_{X}(M^{T}\\theta)u_{x}^{\\prime})|=}\\\\ &{\\quad\\quad\\quad\\operatorname*{sup}_{u\\in\\mathcal{U}_{A}}h(P_{A}(M^{T}\\theta)u_{a}+P_{X}(M^{T}\\theta)u_{x})-\\displaystyle\\operatorname*{sup}_{u\\in\\mathcal{U}_{A}}h(P_{A}(M^{T}\\theta)u_{a}+P_{X}(M^{T}\\theta)u_{x}^{\\prime})|=}\\\\ &{|\\tilde{e}(u_{x})-\\tilde{\\ell}(u_{x}^{\\prime})|\\leq\\left\\|P_{X}(M^{T}\\theta)\\right\\|_{*}\\left\\|u_{x}-u_{x}^{\\prime}\\right\\|\\Rightarrow\\quad\\tilde{\\ell}\\quad\\mathrm{is~Lipbschitz}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "To satisfy the condition of Gao\u2019s theorem, it remains to show for $u_{x}^{0}$ there exists sequence $\\{u_{x}^{k}\\}_{k=1}^{\\infty}$ such that $\\operatorname*{lim}_{\\|u_{x}^{k}-u_{x}^{0}\\|\\to\\infty}\\frac{|\\tilde{\\ell}(u_{x}^{k})-\\tilde{\\ell}(u_{x}^{0})|}{\\|u_{x}^{k}-u_{x}^{0}\\|}=L$ To prove it we consider that for function $h$ there exists sequence such that $l i m_{k\\rightarrow\\infty}\\frac{|h(t_{0}+t_{k})-h(t_{0})|}{|t_{k}|}=1$ . consider the specific point $u_{x}^{0}$ and $u_{a}\\in\\mathcal{U}_{A}$ . Lets define $t_{0}=P_{A}(M^{T}\\theta)u_{a}+P_{\\mathcal{X}}(M^{T}\\theta)u_{x}^{0}$ . Then there exists $t_{k}$ that satisfies infinity tightness. Therefore there exist $\\Delta_{k}\\in\\mathcal{U}_{\\mathcal{X}}$ such that $P_{\\chi}(M^{T}\\theta)u_{x}^{k}-P_{\\chi}(M^{T}\\theta)u_{x}^{0}\\,=\\,t_{k}$ therefore it can be written: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\!=\\!\\!I i m_{k\\rightarrow\\infty}\\!\\!\\!\\frac{\\displaystyle\\left\\vert\\!\\frac{h\\left(t_{0}+t_{k}\\right)-h\\left(t_{0}\\right)}{\\vert t_{k}\\vert}\\right\\vert_{\\infty}}{\\vert t_{k}\\vert}=}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In the above equation, since we have uniform convergence, we can change the limit and supremum. The last equation shows that there exits sequence of $\\bar{\\{}u_{x}^{k}\\}_{k=1}^{\\infty}$ satisfies tightness condition in $\\infty$ . By applying Gao\u2019s theorem we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\delta}(\\mathbb{P})=\\mathbb{E}_{u_{x}\\sim(g_{\\#}\\mathbb{P})_{x}}[\\tilde{\\ell}(u_{x})]+\\delta\\left\\|P_{X}(M^{T}\\theta)\\right\\|_{*}=\\mathbb{E}_{v\\sim\\mathbb{P}[\\operatorname*{sup}_{a\\in\\mathcal{A}}\\ell^{p}(\\tilde{v}_{a})]}+\\delta\\left\\|P_{X}(M^{T}\\theta)\\right\\|_{*}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The last equation completes the proof. ", "page_idx": 24}, {"type": "text", "text": "B.3 Proof of Theorem 3. ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The case diam $\\left({\\mathcal{A}}\\right)=\\infty$ corresponds exactly to the first part of the proof of Theorem 2, with the only difference being that in that theorem we have $\\|h\\|_{\\mathrm{Lip}}=1$ , while in this case, we have $\\|h\\|_{\\mathrm{Lip}}=L_{h}$ . Therefore we have the below equation: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\delta}(\\mathbb{P})=\\mathcal{R}(\\mathbb{P})+L_{h}\\left\\|P_{\\mathcal{X}}(M^{T}\\theta)\\right\\|_{*}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now consider the case diam $(A)<\\infty$ . To prove our assertion, we use Eq. 16 and it implies that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\delta}(\\mathbb{P})=\\operatorname*{inf}_{\\lambda\\geq0}\\left\\{\\begin{array}{l l}{\\lambda\\delta+\\underset{u_{x}\\sim(g_{\\#}\\mathbb{P})_{x}}{\\mathbb{E}}\\left[\\tilde{\\ell}_{\\lambda}(u_{x})\\right]\\right\\}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\begin{array}{r}{\\tilde{\\ell}(u_{x})=\\operatorname*{sup}_{u_{a}\\in\\mathcal{U}_{A}}\\big\\{\\ell(M(u_{a},u_{x}^{\\prime})\\big\\}\\,\\mathrm{and}\\,\\eta_{\\lambda}(u_{x})=\\operatorname*{sup}_{\\Delta\\in\\mathcal{U}_{x}}\\tilde{\\ell}(u_{x}+\\Delta)-\\lambda d_{X}(u_{x},u_{x}+\\Delta)\\big\\}.}\\end{array}$ To compute the right side of the above equation, we use the theorem 3.2 of work [14] that states. ", "page_idx": 24}, {"type": "text", "text": "Theorem [14]. Let $\\mathcal{Z}_{N}:=\\{z_{1},\\ldots,z_{n}\\}\\subset\\mathcal{Z}$ be a given dataset and $\\mathbb{P}_{N}$ be the corresponding empirical distribution. In addition, let $c(\\cdot,\\cdot)$ be a cost function on ${\\mathcal{Z}}\\times{\\mathcal{Z}}$ and $\\delta\\in(0,\\infty)$ be a scalar. Suppose the loss function $\\ell:\\mathcal{Z}\\times\\Theta\\to\\mathbb{R}$ , where satisfies the following assumptions: ", "page_idx": 24}, {"type": "text", "text": "(A1) $\\ell$ is Lipschitz respect o the cost function $d$ at set $\\mathcal{Z}_{N}$ with $L_{\\theta}^{\\mathcal{Z}_{N}}\\in(0,\\infty)$ ; ", "page_idx": 24}, {"type": "text", "text": "(A2) for any $\\epsilon\\in(0,L_{\\theta}^{\\mathcal{Z}_{N}})$ and each $z_{i}\\in\\mathcal{Z}_{N}$ , there exists $\\tilde{z}_{i}\\in\\mathcal{Z}$ such that $\\delta\\leq d(\\tilde{z}_{i},z_{i})<\\infty$ and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\ell(\\tilde{z}_{i})-\\ell(z_{i})\\geq(L_{\\theta}^{\\mathcal{Z}_{N}}-\\epsilon)c(\\tilde{z}_{i},z_{i}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbb{P}\\colon W_{d,1}(\\mathbb{Q},\\mathbb{P}_{N})\\leq\\delta}\\mathrm{E}_{\\mathbb{Q}}[\\ell(\\mathbf{Z},\\theta)]=\\mathrm{E}_{\\mathbb{P}_{N}}[\\ell(\\mathbf{Z},\\theta)]+L_{\\theta}^{\\mathcal{Z}_{N}}\\delta.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "To use the above theorem we need that $\\tilde{\\ell}$ satisfies conditions (A1) and (A2). ", "page_idx": 24}, {"type": "text", "text": "A1. To prove the Lipschitz condition it can be seen: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall u_{a}\\in\\mathcal{U}_{\\mathcal{X}}:\\ \\ |h(y-\\theta^{T}M(u_{a},u_{x}))-h(y-\\theta^{T}M(u_{a},u_{x}^{\\prime}))|}\\\\ &{\\qquad\\qquad\\leq L_{h}|P_{\\mathcal{X}}(M^{T}\\theta)(u_{a}-u_{a}^{\\prime})|\\leq L_{h}\\left\\|P_{\\mathcal{X}}(M^{T}\\theta)\\right\\|_{*}\\left\\|u_{a}-u_{a}^{\\prime}\\right\\|\\Rightarrow}\\\\ &{\\qquad\\operatorname*{sup}_{u_{a}\\in\\mathcal{U}_{\\mathcal{X}}}\\left|h(y-\\theta^{T}M(u_{a},u_{x}))-h(y-\\theta^{T}M(u_{a},u_{x}^{\\prime}))\\right|=}\\\\ &{\\qquad\\quad|\\tilde{\\ell}(u_{a})-\\tilde{\\ell}(u_{x}^{\\prime})|\\leq L_{h}\\left\\|P_{\\mathcal{X}}(M^{T}\\theta)\\right\\|_{*}\\left\\|u_{x}-u_{x}^{\\prime}\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The case $h(y.\\,\\langle\\theta,y\\rangle)$ is similar so we omit it. ", "page_idx": 24}, {"type": "text", "text": "A2. To check that $\\tilde{\\ell}$ satisfies (A2) condition we show that there exists sequence $\\{\\Delta^{k}\\}$ such that the $\\left\\|\\Delta_{k}\\right\\|\\to\\infty$ for every $v\\in\\mathcal{V}$ and sequence $v_{k}=\\mathbf{C}\\mathbf{F}(v,\\Delta_{k})$ and we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\nl i m_{k\\rightarrow\\infty}\\frac{\\left|h(y-\\langle\\theta,v_{k}\\rangle)-h(y-\\langle\\theta,v\\rangle)\\right|}{d(v_{k},v)}=L_{h}.\\left\\|P_{\\mathcal{X}}(\\boldsymbol{M}^{T}\\theta)\\right\\|_{*}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By assumption about $h$ we have: For each $t_{0}~\\in~\\mathbb{R}$ there exists sequence of $\\{t_{k}\\}_{k=1}^{\\infty}$ goes to $\\infty$ then $l i m_{k\\rightarrow\\infty}\\frac{|h(t_{0}+t_{k})-h(t_{0})|}{|t_{k}|}\\,=\\,L_{h}$ = Lh. By changing variable v = M(ua, ux). Let t0 = ", "page_idx": 24}, {"type": "text", "text": "$y-\\theta^{T}M(u_{a},u_{x})$ and $\\Delta_{k}\\in\\mathcal{X}$ such that $P_{\\mathcal{X}}(M^{T}\\theta)\\Delta_{k}=t_{k}$ it is clear $\\Delta_{k}$ exist. No if we define $\\boldsymbol{v}_{k}=\\mathbf{C}\\mathbf{F}(v,\\Delta_{k})$ we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{h}=l i m_{k\\rightarrow\\infty}\\frac{\\left|h\\left(t_{0}+t_{k}\\right)-h\\left(t_{0}\\right)\\right|}{\\left|t_{k}\\right|}=}\\\\ &{\\ \\ \\ \\ l i m_{k\\rightarrow\\infty}\\frac{\\left|h\\left(y-\\theta^{T}M\\left(u_{a},u_{x}\\right)+P_{x}\\left(M^{T}\\theta\\right)\\Delta_{k}\\right)-h\\left(y-\\theta^{T}M\\left(u_{a},u_{x}\\right)\\right)\\right|}{\\left|P_{x}\\left(M^{T}\\theta\\right)\\Delta_{k}\\right|}=}\\\\ &{\\ l i m_{k\\rightarrow\\infty}\\frac{\\left|h\\left(y-\\theta^{T}M\\left(u_{a},u_{x}+\\Delta_{k}\\right)\\right)-h\\left(y-\\theta^{T}M\\left(u_{a},u_{x}\\right)\\right)\\right|}{\\left|P_{x}\\left(M^{T}\\theta\\right)\\Delta_{k}\\right|}=}\\\\ &{\\ l i m_{k\\rightarrow\\infty}\\frac{\\left|h\\left(y-\\theta^{T}v_{k}\\right)-h\\left(y-\\theta^{T}v\\right)\\right|}{\\left|\\|P_{x}\\left(M^{T}\\theta\\right)\\right|}=l i m_{k\\rightarrow\\infty}\\frac{\\left|h\\left(y-\\langle\\theta,v_{k}\\rangle\\right)-h\\left(y-\\langle\\theta,v\\rangle\\right)\\right|}{\\left\\|P_{x}\\left(M^{T}\\theta\\right)\\right\\|_{*}d\\left(v_{k},v\\right)}}\\\\ &{\\ \\ \\ \\implies l i m_{k\\rightarrow\\infty}\\frac{\\left|h\\left(y-\\langle\\theta,v_{k}\\rangle\\right)-h\\left(y-\\langle\\theta,v\\rangle\\right)\\right|}{d\\left(v_{k},v\\right)}=\\left\\|P_{x}(M^{T}\\theta)\\right\\|_{*}.L_{h}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The Last equation is valid because, by Holder inequality, there exists $\\Delta$ such that for all $\\lambda\\Delta$ the Holder inequality converts to equality. Now it is sufficient that find proper $\\lambda$ such that $\\lambda_{k}\\ =$ $t_{k}/\\left\\Vert P_{\\mathcal{X}}(M^{\\acute{T}}\\theta)\\bar{\\Delta}\\right\\Vert_{*}$ so by define $\\Delta_{k}=\\lambda_{k}\\Delta$ we find sequence that holds the assertion. ", "page_idx": 25}, {"type": "text", "text": "The case of $h(y.\\,\\langle\\theta,v\\rangle$ is similar. By discussion of the first part, we can find $\\Delta_{k}$ . Now since we have binary classification, so $y\\in\\{-1,1\\}$ we define $\\tilde{\\Delta}_{k}=\\mathrm{sign}(y)\\Delta_{k}$ therefore for such $\\Delta_{k}$ , we have $y.P_{\\dot{\\mathcal{X}}}(M^{T}\\theta)\\tilde{\\Delta}_{k})=t_{k}$ . By assumption, it can be written as: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{h}=l i m_{k\\rightarrow\\infty}\\frac{\\left|h\\left(t_{0}+t_{k}\\right)-h\\left(t_{0}\\right)\\right|}{\\left|t_{k}\\right|}=}\\\\ &{l i m_{k\\rightarrow\\infty}\\frac{\\left|h\\left(y,\\theta^{T}M\\left(u_{a},u_{x}\\right)+y,P_{x}\\left(M^{T}\\theta\\right)\\widehat{\\Delta}_{k}\\right)-h\\left(y,\\theta^{T}M\\left(u_{a},u_{x}\\right)\\right)\\right|}{\\left|P_{x}\\left(M^{T}\\theta\\right)\\widehat{\\Delta}_{k}\\right|}=}\\\\ &{l i m_{k\\rightarrow\\infty}\\frac{\\left|h\\left(y,\\theta^{T}M\\left(u_{a},u_{x}\\right)+y,P_{x}\\left(M^{T}\\theta\\right)\\widehat{\\Delta}_{k}\\right)-h\\left(y,\\theta^{T}M\\left(u_{a},u_{x}\\right)\\right)\\right|}{\\left|P_{x}\\left(M^{T}\\theta\\right)\\Delta_{k}\\right|}=}\\\\ &{l i m_{k\\rightarrow\\infty}\\frac{\\left|h\\left(y,\\theta^{T}M\\left(u_{a},u_{x}\\right)-\\Delta_{k}\\right)-h\\left(y,\\theta^{T}M\\left(u_{a},u_{x}\\right)\\right)\\right|}{\\left|P_{x}\\left(M^{T}\\theta\\right)\\Delta_{k}\\right|}=}\\\\ &{l i m_{k\\rightarrow\\infty}\\frac{\\left|h\\left(y,\\theta^{T}v_{k}\\right)-h\\left(y,\\theta^{T}v\\right)\\right|}{\\left|P_{x}\\left(M^{T}\\theta\\right)\\right|}=l i m_{k\\rightarrow\\infty}\\frac{\\left|h\\left(y,\\theta^{T}v\\right)-h\\left(y-\\theta\\right)\\right|}{\\left|P_{x}\\left(M^{T}\\theta\\right)\\right|_{*}d\\left(v_{k},v\\right)}}\\\\ &{\\implies l i m_{k\\rightarrow\\infty}\\frac{\\left|h\\left(y-\\theta,v\\right)\\right|-h\\left(y-\\theta,v\\right)\\right|}{d\\left(v_{k},v\\right)}=\\left|P_{x}(M^{T}\\theta)\\right|_{*}\\mathcal{A}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let $\\ell(v)=h(y-P_{\\mathcal{X}}(M^{T}\\theta)u_{x}-P_{\\mathcal{A}}(M^{T}\\theta)u_{a})$ . By assumption $h$ is Lipschitz so it $\\ell$ is Lipschitz concerning each $u_{x}$ and $u_{a}$ and $\\mathcal{U}_{A}$ is bounded so it is compact. Then these properties imply uniformly continuous so we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\iota_{a}\\in\\mathcal{U}_{A}}{\\operatorname*{sup}}l i m_{k\\to\\infty}\\frac{\\left|h(y-\\langle\\theta,v_{k}\\rangle)-h(y-\\langle\\theta,v\\rangle)\\right|}{d(v_{k},v)}=}\\\\ &{i m_{k\\to\\infty}\\underset{u_{a}\\in\\mathcal{U}_{A}}{\\operatorname*{sup}}\\frac{\\left|h(y-\\langle\\theta,v_{k}\\rangle)-h(y-\\langle\\theta,v\\rangle)\\right|}{d(v_{k},v)}=l i m_{k\\to\\infty}\\frac{\\left|\\tilde{\\ell}(u_{x})-\\tilde{\\ell}(\\Delta_{k})\\right|}{\\left\\|u_{x}-\\Delta_{k}\\right\\|}=\\left\\|P_{\\mathcal{X}}(M^{T}\\theta)\\right\\|_{*}.L_{h}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The last equation satisfies the $\\left(A_{2}\\right)$ condition because since $l i m_{k\\rightarrow\\infty}\\frac{|\\tilde{\\ell}(u_{x})-\\tilde{\\ell}(\\Delta_{k})|}{\\left\\|u_{x}-\\Delta_{k}\\right\\|}\\ \\ =$ $\\left\\|P_{\\mathcal{X}}(M^{T}\\theta)\\right\\|_{*}.I$ $L_{h}$ in other hand we have $|\\tilde{\\ell}(\\boldsymbol{u_{a}})-\\tilde{\\ell}(\\boldsymbol{u_{x}^{\\prime}})|\\le L_{h}\\left\\|P_{\\mathcal{X}}(\\boldsymbol{M^{T}\\theta})\\right\\|_{*}\\left\\|\\boldsymbol{u_{x}}-\\boldsymbol{u_{x}^{\\prime}}\\right\\|$ , then for each $\\epsilon$ there exist $\\Delta_{k}$ such that $|\\tilde{\\ell}(u_{x})-\\tilde{\\ell}(\\Delta_{k})|>(L_{h}\\left\\|P_{\\mathcal{X}}(M^{T}\\theta)\\right\\|_{*}\\left\\|u_{x}-\\Delta_{k}\\right\\|-\\epsilon)\\left\\|u_{x}-\\Delta_{k}\\right\\|$ Now we can use Chu\u2019s Theorem and it implies that: $\\mathcal{R}_{\\delta}(\\mathbb{P})=\\mathcal{R}^{c f}(\\mathbb{P})+L_{h}\\left\\|P_{\\mathcal{X}}(M^{T}\\theta)\\right\\|_{*}$ . The classification case is the same and it completes the proof. ", "page_idx": 25}, {"type": "text", "text": "B.4 Proof of Theorem 4. ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Let\u2019s prove the necessary condition. By first part proof of theorem 2, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\delta}(\\mathbb{P}_{N},\\theta)\\geq\\frac{1}{N}\\operatorname*{sup}_{a\\in\\mathcal{A}}\\ell(\\ddot{v}_{a},y,\\theta).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, for a finite solution to exist for the DRO problem, it is necessary that: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{a\\in A}\\ell(\\ddot{v}_{a},y,\\theta)<\\infty.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To prove Eq. 10, we use again some parts of the proof of strong duality theorem1 and idea of proof of theorem 9.1 of Garcia\u2019s work [29]. It states: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\delta}(\\mathbb{P}_{N})=\\operatorname*{sup}_{\\mathbb{Q}\\in\\mathbb{B}_{\\delta}(\\mathbb{P}_{N})}\\left\\{\\underset{v\\sim\\mathbb{Q}}{\\mathbb{E}}[\\ell(v^{\\prime},y,\\theta)]\\right\\}=\\operatorname*{sup}_{\\mathbb{Q}_{\\alpha}\\in\\mathbb{B}_{\\delta}((g_{\\#}\\mathbb{P}_{N})_{\\mathcal{X}})}\\left\\{\\underset{u_{\\alpha}^{\\prime}\\sim\\mathbb{Q}_{\\alpha}}{\\mathbb{E}}\\left[\\ell\\left(g^{-1}\\left((u_{\\alpha}^{\\prime},u_{x}^{\\prime})\\right),y,\\theta,\\right)\\right]\\right\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where by discussion in lemma 4, we can suppose that $\\mathbb{Q}_{x}$ and $\\mathbb{Q}_{a}$ are independent of each other. For simplicity, we define $J(u_{x},u_{a},\\theta)=\\ell(g^{-\\bar{1}}((u_{a},u_{x})),y,\\theta)$ . By assumption, since the $f$ is twice differentiable, then $(I-f)^{-1}$ is also twice differentiable. Because the function $g$ is obtained by $I-f$ by removing the functional structure of sensitive attributes and is a set identity function instead of them, there are two functions $g$ and $g^{-1}$ . These results show that combination $\\ell(\\dot{g}^{-1})$ is also twice differentiable, so the gradient of $J$ exists concerning the $u_{x}$ . By using Taylor\u2019s expansion theorem if $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ is the function that has gradient then the first order estimation of $f$ equals: ", "page_idx": 26}, {"type": "equation", "text": "$$\nf(\\boldsymbol{x}+\\boldsymbol{h})=f(\\boldsymbol{x})+\\nabla f(\\boldsymbol{x})^{\\top}\\boldsymbol{h}+\\int_{0}^{1}(\\nabla f(\\boldsymbol{x}+t\\boldsymbol{h})-\\nabla f(\\boldsymbol{x}))^{\\top}\\boldsymbol{h}\\,d t.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Let $u_{x}\\sim(g_{\\#}\\mathbb{P})_{\\mathcal{X}}$ by writing Taylor\u2019s expansion around $u_{x}$ we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[J(u_{x}^{\\prime},u_{a}^{\\prime},\\theta)\\right]=\\mathbb{E}\\left[J(u_{x},u_{a}^{\\prime},\\theta)+\\nabla_{x}J(u_{x},u_{a}^{\\prime},\\theta)\\cdot(u_{x}^{\\prime}-u_{x})\\right.}\\\\ &{\\qquad+\\left.\\int_{0}^{1}\\{\\nabla_{x}J(u_{x}+\\lambda(u_{x}^{\\prime}-u_{x}),u_{a}^{\\prime},\\theta)-\\nabla_{x}J(u_{x},u_{a}^{\\prime},\\theta)\\}\\cdot(u_{x}^{\\prime}-u_{x})d\\lambda\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since the diam $(u)<\\infty$ we can suppose that the space $\\boldsymbol{\\mathcal{U}}$ is compact. By assumption $J$ is twice differentiable, so $\\dot{\\nabla}_{x}J(.,u_{a},\\theta)$ is Lipschitz with constant $\\|\\nabla_{x}J(.,u_{a},\\theta)\\|_{\\mathrm{Lip}}$ that there exist $L<\\infty$ such that $\\Vert\\nabla_{x}J(.,u_{a},\\theta)\\Vert_{\\mathrm{Lip}}\\leq L$ . By these assumptions, it can be written: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\int_{0}^{1}\\{\\nabla_{x}J(u_{x}+\\lambda(u_{x}^{\\prime}-u_{x}),u_{a}^{\\prime},\\theta)-\\nabla_{x}J(u_{x},u_{a}^{\\prime},\\theta)\\}\\cdot(u_{x}^{\\prime}-u_{x})d\\lambda\\right]\\le}\\\\ &{~~\\mathbb{E}\\left[\\displaystyle\\int_{0}^{1}\\|\\nabla_{x}J(u_{x}+\\lambda(u_{x}^{\\prime}-u_{x}),u_{a}^{\\prime},\\theta)-\\nabla_{x}J(u_{x},u_{a}^{\\prime},\\theta)\\|\\|u_{x}^{\\prime}-u_{x}\\|_{e}^{2}d\\lambda\\right]=}\\\\ &{~~\\mathbb{E}\\left[\\displaystyle\\int_{0}^{1}\\|\\nabla_{x}J(.,u_{a}^{\\prime},\\theta)\\|_{\\mathrm{Lip}}\\|u_{x}^{\\prime}-u_{x}\\|_{e}^{2}d\\lambda\\right]=\\mathbb{E}\\left[\\displaystyle\\frac{1}{2}\\|\\nabla_{x}J(.,u_{a}^{\\prime},\\theta)\\|_{\\mathrm{Lip}}\\|u_{x}^{\\prime}-u_{x}\\|_{e}^{2}\\right]\\le}\\\\ &{~~\\displaystyle\\frac{C}{2}\\|\\nabla_{x}J(.,u_{a}^{\\prime},\\theta)\\|_{\\mathrm{Lip}}\\mathbb{E}\\left[\\|u_{x}^{\\prime}-u_{x}\\|^{2}\\right]\\le\\displaystyle\\frac{C L}{2}\\mathbb{E}\\left[\\|u_{x}^{\\prime}-u_{x}\\|^{2}\\right]\\le{O}(\\delta^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $C$ is a constant that arises from the equivalence of norms in $\\mathbb{R}^{d}$ , it means that there exists $C\\ \\|.\\|_{e}\\ \\leq\\ C\\left\\|.\\right\\|$ . The inequality $\\mathbb{E}\\left[\\|\\dot{u_{x}^{\\prime}}-u_{x}\\|^{2}\\right]~\\leq~\\delta^{2}$ is valid because by definition $\\mathbb{Q}_{x}\\;\\in\\;\\mathbb{B}_{\\delta}\\big((g_{\\#}\\mathbb{P}_{N})_{\\mathcal{X}}\\big)$ and the cost function in the space $\\mathcal{U}_{\\mathcal{X}}$ is expressed by the $\\left\\Vert u_{x}^{\\prime}-u_{x}\\right\\Vert$ so by definition of $\\mathbb{B}_{\\delta}\\big((\\bar{g}_{\\#}\\mathbb{P}_{N})_{\\mathcal{X}}\\big)$ , for $p\\geq2$ by applying Jensen\u2019s inequality we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{Q}_{x}\\in\\mathbb{B}_{\\delta}\\big((g_{\\#}\\mathbb{P}_{N})_{\\mathcal{X}}\\big)\\Rightarrow\\mathbb{E}_{\\mathbb{Q}_{x}}\\left[\\|u_{x}^{\\prime}-u_{x}\\|^{p}\\right]^{\\frac{1}{p}}\\leq\\delta\\Rightarrow}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\mathbb{E}_{\\mathbb{Q}_{x}}\\left[\\|u_{x}^{\\prime}-u_{x}\\|^{2}\\right]\\leq\\mathbb{E}_{\\mathbb{Q}_{x}}\\left[\\|u_{x}^{\\prime}-u_{x}\\|^{p}\\right]^{\\frac{2}{p}}\\leq\\delta^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since by assumption $\\nabla J_{x}$ is uniformly Lipschitz for different value of $\\theta$ and $u_{a}$ therefore we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\delta}(\\mathbb{P}_{N})=\\operatorname*{sup}_{\\mathbb{Q}_{x}\\in\\mathbb{B}_{\\delta}((g_{\\#}\\mathbb{P}_{N})_{X})}\\left\\{\\mathbb{E}\\left[J(u_{x},u_{a}^{\\prime},\\theta)+\\nabla_{x}J(u_{x},u_{a}^{\\prime},\\theta)\\cdot(u_{x}^{\\prime}-u_{x})\\right]\\right\\}+O(\\delta^{2}),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for $O(\\delta^{2})$ independent of $\\theta$ and $u_{a}$ . The first expression of the above equation has the simple form: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbb{Q}_{\\alpha}\\in\\mathcal{P}(\\mathcal{U}_{A})}{\\operatorname*{sup}}\\left\\{\\underset{u_{\\alpha}\\sim(g_{\\#}\\mathbb{P}_{N})_{x}}{\\mathbb{E}}[J(u_{x},u_{a}^{\\prime},\\theta)]\\right\\}=\\underset{u_{x}\\sim(g_{\\#}\\mathbb{P}_{N})x}{\\mathbb{E}}\\left[\\underset{\\mathbb{Q}_{\\alpha}\\in\\mathcal{P}(\\mathcal{U}_{A})}{\\operatorname*{sup}}\\left\\{\\underset{u_{\\alpha}^{\\prime}\\sim\\mathbb{Q}_{\\alpha}}{\\mathbb{E}}[J(u_{x},u_{a}^{\\prime},\\theta)]\\right\\}\\right]=}\\\\ &{\\underset{u_{x}\\sim(g_{\\#}\\mathbb{P}_{N})x}{\\mathbb{E}}\\left[\\underset{u_{\\alpha}^{\\prime}\\in\\mathcal{U}_{A}}{\\operatorname*{sup}}\\left\\{J(u_{x},u_{a}^{\\prime},\\theta)\\right\\}\\right]=\\underset{u_{x}\\sim(g_{\\#}\\mathbb{P}_{N})x}{\\mathbb{E}}\\left[\\underset{u_{\\alpha}^{\\prime}\\in\\mathcal{U}_{A}}{\\operatorname*{sup}}\\left\\{\\ell(g^{-1}((u_{x},u_{a}^{\\prime})),y,\\theta)\\right\\}\\right]=}\\\\ &{\\underset{v\\sim\\mathbb{P}_{N}}{\\mathbb{E}}\\left[\\underset{u\\in\\mathcal{A}}{\\operatorname*{sup}}\\,\\ell(\\ddot{v}_{a},y,\\theta)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The only term in the above equation that still depends on the $\\mathbb{Q}$ is that term $\\nabla_{x}J(u_{x},u_{a}^{\\prime},\\theta)\\!\\cdot\\!(u_{x}^{\\prime}\\!-\\!u_{x})$ . To remove this term we use extended H\u00f6lder inequality with the expectation that can be expressed using the following formula: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}[|X Y|]\\leq(\\mathbb{E}[|X|^{p}])^{\\frac{1}{p}}(\\mathbb{E}[|Y|^{q}])^{\\frac{1}{q}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and equality holds if and only if there exist constants $c\\geq0$ such that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n|Y|=c|X|^{\\frac{p}{q}}\\quad{\\mathrm{almost~surely,}}\\quad c\\geq0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By using H\u00f6lder inequality, with the same reasoning we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{sup}_{\\mathbb{Q}_{\\kappa}\\in\\mathbb{B}_{\\mathcal{A}}}\\left(\\underset{\\mathbb{Q}_{\\kappa}^{\\star}\\sim\\mathbb{Q}_{\\kappa}}{\\mathbb{E}}\\left[\\nabla_{x}J(u_{x},u_{\\alpha}^{\\prime},\\theta)\\cdot(u_{x}^{\\prime}-u_{x})\\right]\\right\\}=}\\\\ &{\\underset{\\mathbb{Q}_{\\kappa}\\in\\mathbb{B}_{\\mathcal{A}}}{\\mathbb{E}}\\left(\\underset{\\mathbb{Q}_{\\kappa}^{\\star}\\sim\\mathbb{Q}_{\\kappa}}{\\mathbb{E}}\\left[\\begin{array}{l}{\\operatorname*{sup}_{\\alpha}\\left\\{\\nabla_{x}J(u_{x},u_{\\alpha}^{\\prime},\\theta)\\cdot(u_{x}^{\\prime}-u_{x})\\right\\}\\right\\}=}\\\\ {\\vdots}\\\\ {\\underset{\\mathbb{Q}_{\\kappa}^{\\star}\\sim\\mathbb{Q}_{\\kappa}}{\\mathbb{E}}\\left[\\begin{array}{l}{\\operatorname*{sup}_{\\alpha}\\left\\{\\nabla_{x}J(u_{x},u_{\\alpha}^{\\prime},\\theta)\\cdot(u_{x}^{\\prime}-u_{x})\\right\\}\\right\\}=}\\\\ {\\vdots}\\\\ {\\underset{\\mathbb{Q}_{\\kappa}^{\\star}\\sim\\mathcal{N}_{\\beta}}{\\mathbb{E}}\\left[\\underset{u_{x}^{\\star}\\sim\\mathcal{U}_{\\kappa}}{\\mathbb{E}}\\left\\{\\|\\nabla_{x}J(u_{x},u_{\\alpha}^{\\prime},\\theta)\\|_{\\mathbb{H}}^{2}\\right\\}\\right]\\right\\}\\right)^{\\frac{1}{n}}\\underset{\\mathbb{Q}_{\\kappa}\\in\\mathbb{B}_{\\mathcal{A}}\\left(\\mathcal{Q}_{\\kappa}\\mathcal{P}_{\\kappa}\\mathcal{N}\\right)\\mathcal{N}}{\\underbrace{\\left\\{\\left(\\begin{array}{l l}{\\displaystyle\\sum_{u_{x}^{\\prime}\\sim\\mathcal{U}_{\\kappa}}\\left[\\|u_{x}^{\\prime}-u_{x}\\|^{p}\\right]\\right)^{\\frac{1}{n}}\\right\\}}}_{\\in\\mathcal{V}_{\\kappa}\\left(\\mathcal{Q}_{\\kappa}\\mathcal{P}_{\\kappa}\\mathcal{N}\\right)\\mathcal{N}}}}\\\\ &{=\\delta\\left(\\underset{u_{x}\\sim(\\mathcal{Q}_{\\kappa}\\mathcal{P})\\mathcal{N}}{\\mathbb{E}}\\left[\\begin{array}{l}{\\operatorname*{sup}_{\\alpha}\\left\\{\\|\\nabla_{x}J(u_{x},u_{\\alpha}^{\\prime},\\theta)\\|_{\\mathbb\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where equality can be attained whenever $1\\ \\leq\\ p\\ \\leq\\ \\infty$ for a proper choice of $u_{x}^{\\prime}\\mathrm{~-~}u_{x}$ with $(\\mathbf{E}[\\|u_{x}^{\\prime}-u_{x}\\|^{p}])^{1/p}=\\delta$ . Therefore, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\delta}(\\mathbb{P}_{N})=\\underset{v\\sim\\mathbb{P}_{N}}{\\mathbb{E}}\\left[\\operatorname*{sup}_{a\\in\\mathcal{A}}\\ell(\\ddot{v}_{a},y,\\theta)\\right]+\\delta\\left(\\underset{v\\sim\\mathbb{P}_{N}}{\\mathbb{E}}[\\operatorname*{sup}_{a\\in\\mathcal{A}}\\{\\|\\nabla^{\\mathrm{cr}}\\ell(\\ddot{v},y,\\theta)\\|_{*}^{q}\\}]\\right)^{1/q}+O(\\delta^{2})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\nabla^{\\mathrm{cr}}\\ell(v,y,\\theta)=\\operatorname*{lim}_{\\Delta\\to0}{\\frac{\\ell({\\bf C F}_{0}(v,\\Delta))-f(v)}{\\|\\Delta\\|}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "the last equation completes the proofs. ", "page_idx": 27}, {"type": "text", "text": "B.5 Proof of Proposition 2. ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "By Eq. 16 we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\delta}(\\mathbb{P})=\\operatorname*{inf}_{\\lambda\\geq0}\\left\\{\\begin{array}{l l}{\\lambda\\delta+\\underset{u_{x}\\sim(g_{\\#}\\mathbb{P})_{x}}{\\mathbb{E}}\\left[\\tilde{\\ell}_{\\lambda}(u_{x})\\right]\\right\\}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where, $\\tilde{\\ell}(u_{x})\\,=\\,\\mathrm{sup}_{u_{a}\\in\\mathcal{U}_{A}}\\left\\{\\ell(M(u_{a},u_{x}^{\\prime})\\right\\}$ and $\\eta_{\\lambda}(u_{x})\\,=\\,\\operatorname*{sup}_{\\Delta\\in\\mathcal{U}_{x}}\\,\\tilde{\\ell}(u_{x}+\\Delta)-\\lambda d_{X}(u_{x},u_{x}\\,+$ $\\Delta)\\}$ . To prove we use Corollary 2 [28] for the $\\tilde{\\ell}$ . First, we need to show that $\\tilde{\\ell}$ satisfies the condition of Corollary 2 [28]. By assumption for $v^{\\prime}\\;\\in\\;\\mathcal{V},\\;L,M\\;\\geq\\;0$ such that $\\left|\\ell(v,y,\\theta)\\right.-$ ", "page_idx": 27}, {"type": "text", "text": "$\\ell(v^{\\prime},y,\\theta)|<L d^{p}(v,v^{\\prime})+M$ for all $v\\in\\mathcal{V}$ and $p\\in[1,\\infty)$ . By setting $v=g^{-1}((u_{a},u_{a}))$ and $v^{\\prime}=g^{-1}\\bigl((u_{a}^{\\prime},u_{a}^{\\prime})\\bigr)$ , and for simplicity $\\ell(v)=\\ell(v,y,\\theta)$ we have: $\\begin{array}{r l}&{|\\ell(g^{-1}((u_{a},u_{x})))-\\ell(g^{-1}((u_{a}^{\\prime},u_{x}^{\\prime})))|<L\\,\\|u_{x}-u_{x}^{\\prime}\\|^{p}+M,\\quad\\forall u_{x}\\in\\mathcal{U}_{X},\\ u_{a}\\in\\mathcal{U}_{A}\\Rightarrow}\\\\ &{|\\tilde{\\ell}(u_{x})-\\tilde{\\ell}(u_{x}^{\\prime})|\\leq L\\,\\|u_{x}-u_{x}^{\\prime}\\|^{p}+M}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "where $\\tilde{\\ell}(u_{x})=s u p_{u_{a}\\in\\mathcal{U}_{A}}\\ell(g^{-1}((u_{a},u_{x})))$ . This equation implies that $\\tilde{\\ell}$ satisfies the condition of corollary 2. So in consequence of corollary 2 and the equation 16 if we define uncertainty set: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\tilde{B}_{\\delta}=\\left\\{(\\omega_{x}^{i k})_{i,k}:\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{k=1}^{K}\\left\\|u_{x}^{i}-\\omega^{i k}\\right\\|\\leq\\delta,\\,\\omega^{i k}\\in\\mathcal{U}_{X}\\right\\}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since the casual fair metric $d$ and loss function $\\tilde{\\ell}$ do not depend on the sensitive part then ${\\tilde{B}}_{\\delta}$ is equivalent to the below uncertainty set. ", "page_idx": 28}, {"type": "equation", "text": "$$\nB_{\\delta}=\\left\\{(w^{i k})_{i,k}:{\\frac{1}{N}}\\sum_{i=1}^{N}\\sum_{k=1}^{K}d^{p}(v_{i},w^{i k})\\leq\\delta,\\,w^{i k}\\in\\mathcal{V}\\right\\}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By applying the uncertainty ${\\tilde{B}}_{\\delta}$ for $\\tilde{\\ell}$ , the robust optimization problem has a form: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathcal{R}_{\\delta}^{a d v}(\\mathbb{P}_{N})=}}\\\\ {{\\displaystyle\\operatorname*{sup}_{(\\omega^{i k})_{i,k}\\in\\tilde{B}_{\\delta}}\\left\\{\\frac{1}{N K}\\sum_{i=1}^{N}\\sum_{k=1}^{K}\\tilde{\\ell}(\\omega^{i k})\\right\\}=\\operatorname*{sup}_{(\\omega^{i k})_{i,k}\\in\\tilde{B}_{\\delta}}\\left\\{\\frac{1}{N K}\\sum_{i=1}^{N}\\sum_{k=1}^{K}\\operatorname*{max}_{u_{\\alpha}\\in\\mathcal{U}_{A}}\\ell(g^{-1}((u_{\\alpha},\\omega^{i k})))\\right\\}=}}\\\\ {{\\displaystyle\\operatorname*{sup}_{(w^{i k})_{i,k}\\in B_{\\delta}}\\left\\{\\frac{1}{N K}\\sum_{i=1}^{N}\\sum_{k=1}^{K}\\operatorname*{max}_{u\\in A}\\ell(\\tilde{w}_{a}^{i k})))\\right\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and finally for ${\\tilde{\\mathcal{R}}}_{\\delta}^{a d v}(\\mathbb{P}_{N})$ we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{R}}_{\\delta}^{a d v}(\\mathbb{P}_{N})\\le\\mathcal{R}_{\\delta}^{n}(\\mathbb{P}_{N})\\le\\tilde{\\mathcal{R}}_{\\delta}^{a d v}(\\mathbb{P}_{N})+\\frac{L D+M}{N K},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and it completes the proof. ", "page_idx": 28}, {"type": "text", "text": "Lemma 8 Assume that the cost functions c and $\\hat{c}$ satisfy $|c(x,x^{\\prime})-\\hat{c}(x,x^{\\prime})|<\\alpha$ for all $x,x^{\\prime}\\in\\mathbb{R}^{n}$ and for some $\\alpha\\geq0$ . Then, for any $\\lambda\\geq0$ , the difference between the $\\lambda$ -conjugates of $f$ with respect to c and $\\hat{c}$ is bounded by $\\lambda\\alpha$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|f_{\\lambda}(x)-\\hat{f}_{\\lambda}(x)|\\le\\lambda\\alpha\\ \\ \\,f o r\\,a l l\\,x\\in\\mathbb{R}^{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. To prove the proposition, we consider any $x\\in\\mathbb{R}^{n}$ and examine the definitions of $f_{\\lambda}(x)$ and $\\hat{f}_{\\lambda}(x)$ . Begin by expressing the bounds on $\\hat{c}$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\widehat{c}(x,x^{\\prime})\\leq c(x,x^{\\prime})+\\alpha\\quad\\mathrm{and}\\quad\\widehat{c}(x,x^{\\prime})\\geq c(x,x^{\\prime})-\\alpha.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "From these inequalities, for any $x^{\\prime}\\in\\mathbb{R}^{n}$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\nf(x^{\\prime})-\\lambda{\\hat{c}}(x,x^{\\prime})\\geq f(x^{\\prime})-\\lambda(c(x,x^{\\prime})+\\alpha)=f(x^{\\prime})-\\lambda c(x,x^{\\prime})-\\lambda\\alpha,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\nf(x^{\\prime})-\\lambda{\\hat{c}}(x,x^{\\prime})\\leq f(x^{\\prime})-\\lambda(c(x,x^{\\prime})-\\alpha)=f(x^{\\prime})-\\lambda c(x,x^{\\prime})+\\lambda\\alpha.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Taking the supremum over all $x^{\\prime}$ in the above expressions, we obtain: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\hat{f}_{\\lambda}(x)\\geq f_{\\lambda}(x)-\\lambda\\alpha\\quad\\mathrm{and}\\quad\\hat{f}_{\\lambda}(x)\\leq f_{\\lambda}(x)+\\lambda\\alpha.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "These two bounds together imply: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|f_{\\lambda}(x)-\\hat{f}_{\\lambda}(x)|\\le\\lambda\\alpha.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, the proof is complete, showing that the difference between the $\\lambda$ -conjugates of $f$ is indeed bounded by $\\lambda\\alpha$ . ", "page_idx": 28}, {"type": "text", "text": "Lemma 9 Let $c,\\hat{c}:\\mathbb{R}^{n}\\,\\times\\,\\mathbb{R}^{n}\\,\\to\\,\\mathbb{R}$ be two functions such that $|c(x,y)-\\hat{c}(x,y)|\\,<\\,\\alpha$ for all $\\boldsymbol{x},\\boldsymbol{y}\\in\\mathbb{R}^{n}$ and some $\\alpha>0$ . For any real number $p\\geq1$ , the following inequality holds: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|c(x,y)^{p}-\\hat{c}(x,y)^{p}|\\leq p\\cdot M^{p-1}\\cdot\\alpha,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $M\\geq\\operatorname*{max}\\{|c(x,y)|,|{\\hat{c}}(x,y)|\\}$ for all $x,y$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. Consider the functions $c$ and $\\hat{c}$ and any $\\boldsymbol{x},\\boldsymbol{y}\\in\\mathbb{R}^{n}$ . By the hypothesis, we have $\\left|c(x,y)-\\right|$ $\\hat{c}(x,y)|<\\alpha$ . To find a bound on the difference of their powers, apply the mean value theorem to the function $f(t)=t^{p}$ , which is differentiable over $\\mathbb{R}$ (or over $\\mathbb{R}^{+}$ if $p$ is not an integer). The derivative of $f$ is $f^{\\prime}(t)=p t^{p-1}$ . ", "page_idx": 29}, {"type": "text", "text": "Since $f$ is continuously differentiable, there exists some $\\xi$ between $c(x,y)$ and $\\hat{c}(x,y)$ such that ", "page_idx": 29}, {"type": "equation", "text": "$$\nf(c(x,y))-f(\\hat{c}(x,y))=f^{\\prime}(\\xi)\\cdot(c(x,y)-\\hat{c}(x,y)).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Therefore, ", "page_idx": 29}, {"type": "equation", "text": "$$\n|c(x,y)^{p}-\\hat{c}(x,y)^{p}|=|p\\xi^{p-1}(c(x,y)-\\hat{c}(x,y))|.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Using the bound $|c(x,y)-\\hat{c}(x,y)|<\\alpha$ and noting that $\\xi$ must be within the range of values between $c(x,y)$ and $\\hat{c}(x,y)$ , we have $\\dot{\\xi}^{p-1}\\le M^{p-1}$ . Thus, ", "page_idx": 29}, {"type": "equation", "text": "$$\n|c(x,y)^{p}-\\hat{c}(x,y)^{p}|\\leq p M^{p-1}|c(x,y)-\\hat{c}(x,y)|<p M^{p-1}\\alpha.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Lemma 10 ([41]) Fix some $\\mathbb{P}\\in\\mathcal{P}(\\mathcal{Z})$ , $\\theta\\in\\Theta$ and $\\lambda^{*}\\geq0$ via ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda^{*}:=\\operatorname*{argmin}_{\\lambda\\geq0}\\left\\{\\lambda\\delta^{p}+\\mathbb{E}_{\\mathbb{P}}[\\ell_{\\lambda}(\\mathbf{Z},\\theta)]\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then under $\\ell$ Lipschitz and diam $(\\mathcal{Z})<\\infty$ assumptions, we have $\\lambda^{*}\\leq L\\delta^{-(p-1)}$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. First, note that: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\lambda^{*}\\delta^{p}\\leq\\lambda^{*}\\delta^{p}+\\mathbb{E}_{\\mathbb{P}}\\left[\\operatorname*{sup}_{z^{\\prime}\\in\\mathcal{Z}}\\left\\lbrace\\ell(z^{\\prime},\\theta)-\\ell(z,\\theta)-\\lambda^{*}c^{p}(z,z^{\\prime})\\right\\rbrace\\right]=*,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "since the left-hand side, is greater than the case where $z^{\\prime}=z$ so it is positive. By the optimality of $\\lambda^{*}$ in $\\ell$ , the right-hand side can be further upper-bounded as follows for any $\\lambda\\geq0$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ast\\leq\\lambda\\delta^{p}+\\mathbb{E}_{\\mathbb{P}}\\left[\\underset{z^{\\prime}\\in\\mathcal{Z}}{\\operatorname*{sup}}\\{\\ell(z^{\\prime},\\theta)-\\ell(z,\\theta)-\\lambda c^{p}(z,z^{\\prime})\\}\\right]}\\\\ &{\\quad\\leq\\lambda\\delta^{p}+\\mathbb{E}_{\\mathbb{P}}\\left[\\underset{z^{\\prime}\\in\\mathcal{Z}}{\\operatorname*{sup}}\\left\\lbrace L c(z,z^{\\prime})-\\lambda c^{p}(z,z^{\\prime})\\right\\rbrace\\right]}\\\\ &{\\quad\\leq\\lambda\\delta^{p}+\\underset{t\\geq0}{\\operatorname*{sup}}\\{L t-\\lambda t^{p}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "using the Lipschitz property for the second line and setting $t=c(z,z^{\\prime})$ in the third line. If $p=1$ , by setting $\\lambda=L$ , we obtain: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\lambda^{*}\\delta\\leq L\\delta+\\operatorname*{sup}_{t\\geq0}\\{L t-L t\\}=L\\delta,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which implies $\\lambda^{*}\\leq L$ . For $p>1$ , using the optimal value $t=(L/p\\lambda)^{1/(p-1)}$ , we derive: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\lambda^{*}\\delta^{p}\\leq\\lambda\\delta^{p}+L^{\\frac{p}{p-1}}p^{-\\frac{p}{p-1}}(p-1)\\lambda^{-\\frac{1}{p-1}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Minimizing the right-hand side with $\\lambda=L/p\\delta^{p-1}$ yields: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\lambda^{*}\\delta^{p}\\leq L\\delta\\Rightarrow\\lambda^{*}\\leq L\\delta^{-(p-1)},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "resulting in the stated bound on $\\lambda^{*}$ . ", "page_idx": 29}, {"type": "text", "text": "Lemma 11 If $f:A\\times\\mathbb{R}^{n}\\to\\mathbb{R}$ is Lipschitz with respect to $x$ uniformly in $a$ , i.e., there exists a constant $L$ such that for all $a\\in A$ and for all $\\boldsymbol{x},\\boldsymbol{y}\\in\\mathbb{R}^{n}$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n|f(a,x)-f(a,y)|\\leq L\\|x-y\\|,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "then $\\operatorname*{sup}_{a\\in A}f(a,x)$ is also Lipschitz in $x$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. Let $F(x)=\\operatorname*{sup}_{a\\in A}f(a,x)$ . We aim to show that there exists a constant $L^{\\prime}$ such that for all $\\boldsymbol{x},\\boldsymbol{y}\\in\\mathbb{R}^{n}$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n|F(x)-F(y)|\\leq L^{\\prime}\\|x-y\\|.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Since $f$ is Lipschitz continuous with respect to $x$ uniformly in $a$ with Lipschitz constant $L$ , it holds for each $a\\in A$ and any $\\boldsymbol{x},\\boldsymbol{y}\\in\\mathbb{R}^{n}$ that ", "page_idx": 30}, {"type": "equation", "text": "$$\n|f(a,x)-f(a,y)|\\leq L\\|x-y\\|.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Consider $F(x)$ and $F(y)$ . By definition, ", "page_idx": 30}, {"type": "equation", "text": "$$\nF(x)=\\operatorname*{sup}_{a\\in A}f(a,x)\\quad{\\mathrm{and}}\\quad F(y)=\\operatorname*{sup}_{a\\in A}f(a,y).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For any $a\\in A$ , since $|f(a,x)-f(a,y)|\\leq L||x-y||$ , we can infer that ", "page_idx": 30}, {"type": "equation", "text": "$$\nf(a,x)\\leq f(a,y)+L||x-y||.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Taking the supremum over all $a\\in A$ on both sides, we obtain ", "page_idx": 30}, {"type": "text", "text": "Similarly, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~F(x)\\leq F(y)+L\\|x-y\\|.}\\\\ &{~}\\\\ &{F(y)\\leq F(x)+L\\|x-y\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Combining these two inequalities, we find ", "page_idx": 30}, {"type": "equation", "text": "$$\n|F(x)-F(y)|\\leq L\\|x-y\\|.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, $F(x)=\\operatorname*{sup}_{a\\in A}f(a,x)$ is Lipschitz continuous with Lipschitz constant $L$ . This completes the proof. ", "page_idx": 30}, {"type": "text", "text": "B.6 Proof of Theorem 5. ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Let define define $\\begin{array}{r}{\\hat{\\mathcal{R}}_{\\delta}(\\mathbb{P},\\theta):=\\operatorname*{sup}_{\\mathbb{Q}:W_{\\hat{c},p}(\\mathbb{Q},\\mathbb{P})\\leq\\delta}\\mathbb{E}_{\\mathbb{Q}}[\\ell(\\mathbf{Z},\\theta)]}\\end{array}$ , the worst-case loss quantity over estimation of the metric $d$ and $\\begin{array}{r}{\\theta_{*}:=\\operatorname*{inf}_{\\theta\\in\\Theta}\\left\\{\\operatorname*{sup}_{\\mathbb{Q}:W_{c,p}(\\mathbb{Q},\\mathbb{P}_{*})\\leq\\delta}\\mathbb{E}_{\\mathbb{Q}}[\\ell(\\mathbf{Z},\\theta)]\\right\\}}\\end{array}$ . Therefore by definition, we can write: ", "page_idx": 30}, {"type": "text", "text": "$\\begin{array}{r}{\\mathcal{R}_{\\delta}(\\mathbb{P}_{*},\\hat{\\theta}_{N}^{\\mathrm{dro}})-\\mathcal{R}_{\\delta}(\\mathbb{P}_{*},\\theta_{*})\\le\\mathcal{R}_{\\delta}(\\mathbb{P}_{*},\\hat{\\theta}_{N}^{\\mathrm{dro}})-\\hat{\\mathcal{R}}_{\\delta}(\\mathbb{P}_{N},\\hat{\\theta}_{N}^{\\mathrm{dro}})-(\\mathcal{R}_{\\delta}(\\mathbb{P}_{*},\\theta_{*})-\\hat{\\mathcal{R}}_{\\delta}(\\mathbb{P}_{N},\\theta_{*}))}\\end{array}$ (23) because we have $\\begin{array}{r}{\\hat{\\mathcal{R}}_{\\delta}(\\mathbb{P}_{N},\\hat{\\theta}_{N}^{\\mathrm{\\,dro}})\\le\\mathcal{R}_{\\delta}(\\mathbb{P}_{*},\\theta_{*})}\\end{array}$ . ", "page_idx": 30}, {"type": "text", "text": "We estimate two expression $|\\mathcal{R}_{\\delta}(\\mathbb{P}_{*},\\hat{\\theta}_{N}^{\\mathrm{\\tiny~dro}})-\\hat{\\mathcal{R}}_{\\delta}(\\mathbb{P}_{N},\\hat{\\theta}_{N}^{\\mathrm{\\tiny~dro}})|$ and $\\vert\\mathcal{R}_{\\delta}(\\mathbb{P}_{*},\\theta_{*})-\\mathcal{\\hat{R}}_{\\delta}(\\mathbb{P}_{N},\\theta_{*})\\vert$ . By the general strong duality theorem [26] we have: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{R}_{\\delta}(\\mathbb{P}_{*},\\theta_{*})-\\hat{\\mathcal{R}}_{\\delta}(\\mathbb{P}_{N},\\theta_{*})=\\underset{\\mathbb{Q}:\\boldsymbol{W}_{*,p}(\\mathbb{Q},\\mathbb{P}_{*})\\leq\\delta}{\\operatorname*{sup}}\\mathbb{E}_{\\mathbb{Q}}[\\ell(\\mathbf{Z},\\theta_{*})]-\\underset{\\mathbb{Q}:\\boldsymbol{W}_{\\widehat{\\mathcal{S}},p}(\\mathbb{Q},\\mathbb{P}_{N})\\leq\\delta}{\\operatorname*{sup}}\\mathbb{E}_{\\mathbb{Q}}[\\ell(\\mathbf{Z},\\theta_{*})]=}&{}\\\\ {\\underset{\\lambda\\geq0}{\\operatorname*{inf}}\\left\\lbrace\\lambda\\delta^{p}+\\mathbb{E}_{\\mathbb{P}_{*}}\\left[\\ell_{\\lambda}^{c}(\\mathbf{Z},\\theta_{*})\\right]\\right\\rbrace-\\underset{\\lambda\\geq0}{\\operatorname*{inf}}\\left\\lbrace\\lambda\\delta^{p}+\\mathbb{E}_{\\mathbb{P}_{N}}\\left[\\ell_{\\lambda}^{\\widehat{c}}(\\mathbf{Z},\\theta_{*})\\right]\\right\\rbrace\\leq}&{}\\\\ {\\lambda_{N}\\delta^{p}+\\mathbb{E}_{\\mathbb{P}_{*}}\\left[\\ell_{\\lambda_{N}}^{c}(\\mathbf{Z},\\theta_{*})\\right]\\right\\rbrace-\\left(\\lambda_{N}\\delta^{p}+\\mathbb{E}_{\\mathbb{P}_{N}}\\left[\\ell_{\\lambda_{N}}^{\\widehat{c}}(\\mathbf{Z},\\theta_{*})\\right]\\right)=}&{}\\\\ {\\mathbb{E}_{\\mathbb{P}_{*}}\\left[\\ell_{\\lambda_{N}}^{c}(\\mathbf{Z},\\theta_{*})\\right]\\right\\rbrace-\\mathbb{E}_{\\mathbb{P}_{N}}\\left[\\ell_{\\lambda_{N}}^{\\widehat{c}}(\\mathbf{Z},\\theta_{*})\\right]}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the $\\lambda_{N}:=\\mathrm{arginf}_{\\lambda\\geq0}\\left\\{\\lambda\\delta^{p}+\\mathbb{E}_{\\mathbb{P}_{N}}\\left[\\ell_{\\lambda}^{\\hat{c}}(\\mathbf{Z},\\boldsymbol{\\theta}_{\\ast})\\right]\\right\\}$ ", "page_idx": 30}, {"type": "text", "text": "By assumption 2 and lemma 9, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\ell_{\\lambda_{N}}^{c}(z,\\theta)-\\ell_{\\lambda_{N}}^{\\hat{c}}(z,\\theta)|=\\bigg|\\displaystyle\\operatorname*{sup}_{v^{\\prime}\\in\\mathcal{V}}\\ell(z^{\\prime},y,\\theta)-\\lambda_{N}d^{p}(v^{\\prime},v)-\\displaystyle\\operatorname*{sup}_{v^{\\prime}\\in\\mathcal{V}}\\ell(v^{\\prime},y,\\theta)-\\lambda_{N}\\hat{d}^{p}(v^{\\prime},v)\\bigg|}\\\\ &{\\le\\displaystyle\\operatorname*{sup}_{v^{\\prime}\\in\\mathcal{V}}\\lambda_{N}|d^{p}(v^{\\prime},v)-\\hat{d}^{p}(v^{\\prime},v)|\\le\\lambda_{N}\\,p\\,\\mathrm{diam}\\,(\\mathcal{V})^{p-1}\\ M_{d}\\,N^{-\\eta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This implies ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\cal R}_{\\delta}(\\mathbb{P}_{*},\\boldsymbol{\\theta}_{*})-\\hat{\\mathcal{R}}_{\\delta}(\\mathbb{P}_{N},\\boldsymbol{\\theta}_{*})\\le{\\mathbb E}_{P_{*}}\\left[\\ell_{\\lambda_{N}}^{c}(\\mathbf{Z},\\boldsymbol{\\theta})\\right]-{\\mathbb E}_{P_{N}}\\left[\\ell_{\\lambda_{N}}^{c}(\\mathbf{Z},\\boldsymbol{\\theta})\\right]+\\lambda_{N}\\,p\\,\\mathrm{diam}\\,(\\boldsymbol{\\gamma})^{p-1}\\,\\,M_{d}\\,N^{-\\eta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "If define $\\lambda_{*}:=\\operatorname*{arginf}_{\\lambda\\geq0}\\left\\{\\lambda\\delta^{p}+\\mathbb{E}_{\\mathbb{P}}\\big[\\ell_{\\lambda}^{c}(\\mathbf{Z},\\theta_{*})\\big]\\right\\}$ , similarly, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathcal{R}}_{\\delta}(\\mathbb{P}_{N},\\boldsymbol{\\theta}_{*})-\\mathcal{R}_{\\delta}(\\mathbb{P}_{*},\\boldsymbol{\\theta}_{*})\\le\\mathbb{E}_{\\mathbb{P}_{N}}\\left[\\ell_{\\lambda_{*}}^{\\hat{c}}(\\mathbf{Z},\\boldsymbol{\\theta})\\right]-\\mathbb{E}_{\\mathbb{P}_{*}}\\left[\\ell_{\\lambda_{*}}^{c}(\\mathbf{Z},\\boldsymbol{\\theta})\\right]\\le}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\mathbb{E}_{\\mathbb{P}_{N}}\\left[\\ell_{\\lambda_{*}}^{c}(\\mathbf{Z},\\boldsymbol{\\theta})\\right]-\\mathbb{E}_{\\mathbb{P}_{*}}\\left[\\ell_{\\lambda_{*}}^{c}(\\mathbf{Z},\\boldsymbol{\\theta})\\right]+\\lambda_{*}\\,p\\,\\mathrm{diam}\\left(\\mathcal{V}\\right)^{p-1}\\ M_{d}\\,N^{-\\eta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We need to estimate the $\\lambda_{N}$ and $\\lambda_{*}$ . By using strong duality theorem by Eq. 16 we have: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\delta}(\\mathbb{P})=\\operatorname*{inf}_{\\lambda\\geq0}\\left\\{\\begin{array}{l l}{\\lambda\\delta^{p}+\\underset{u_{x}\\sim(g_{\\#}\\mathbb{P})_{X}}{\\mathbb{E}}\\left[\\tilde{\\ell}_{\\lambda}(u_{x})\\right]\\right\\}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "So instead the solve problem for $\\ell$ is it sufficient to prove our result for $\\begin{array}{r l}{\\tilde{\\ell}(u_{x})}&{{}=}\\end{array}$ $\\begin{array}{r}{\\operatorname*{sup}_{u_{a}\\in\\mathcal{U}_{A}}\\ell(g^{-1}((u_{a},u_{x}))}\\end{array}$ . At first, we show that $\\tilde{\\ell}$ is Lipschitz on the space $\\mathcal{U}_{\\mathcal{X}}$ concerning the norm $\\lVert.\\rVert$ . By assumption, for each $a\\in A$ the function $\\ell(g^{-1}((u_{a},u_{x}))$ is also Lipschitz: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\ell(g^{-1}((u_{a},u_{x})),y,\\theta)-\\ell(g^{-1}((u_{a},u_{x}+\\Delta)),y,\\theta)\\right\\|=}\\\\ &{\\|\\ell(\\mathbf{CF}(v,a),y,\\theta)-\\ell(\\mathbf{CF}(v,a,\\Delta),y,\\theta)\\|\\le L d(\\mathbf{CF}(v,a),\\mathbf{CF}(v,a,\\Delta))=L\\left\\|\\Delta\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now by using lemma 11 it can be concluded that the function $\\begin{array}{r}{\\tilde{\\ell}(u_{x})=\\operatorname*{sup}_{u_{a}\\in\\mathcal{U}_{A}}\\ell(g^{-1}((u_{a},u_{x}))}\\end{array}$ also has Lipschitz property with constant $L$ . By Applying lemma 10 for equation 24 and $(g_{\\#}\\mathbb{P}_{N})_{\\mathcal{X}}$ , it can be seen $\\lambda_{N},\\lambda_{*}\\leq L\\delta^{-(p-1)}$ . Therefore until now, we have two inequalities: ", "page_idx": 31}, {"type": "text", "text": "| $\\begin{array}{r l}&{{\\cal R}_{\\delta}(\\mathbb{P}_{*},\\theta_{*})-\\hat{\\cal R}_{\\delta}(\\mathbb{P}_{N},\\theta_{*})\\leq\\mathbb{E}_{P_{*}}\\left[\\ell_{\\lambda_{N}}^{c}({\\bf Z},\\theta)\\right]-\\mathbb{E}_{P_{N}}\\left[\\ell_{\\lambda_{N}}^{c}({\\bf Z},\\theta)\\right]+\\lambda_{N}\\,p\\,\\mathrm{diam}\\,(\\mathcal{V})^{p-1}\\,\\,M_{d}\\,N^{-\\eta},}\\\\ &{\\hat{\\cal R}_{\\delta}(\\mathbb{P}_{N},\\theta_{*})-{\\cal R}_{\\delta}(\\mathbb{P}_{*},\\theta_{*})\\leq\\mathbb{E}_{\\mathbb{P}_{N}}\\left[\\ell_{\\lambda_{n}}^{c}({\\bf Z},\\theta)\\right]-\\mathbb{E}_{\\mathbb{P}_{*}}\\left[\\ell_{\\lambda_{n}}^{c}({\\bf Z},\\theta)\\right]+\\lambda_{*}\\,p\\,\\mathrm{diam}\\,(\\mathcal{V})^{p-1}\\,\\,M_{d}\\,N^{-\\eta}\\Rightarrow}\\\\ &{{\\cal R}_{\\delta}(\\mathbb{P}_{*},\\theta_{*})-\\hat{\\cal R}_{\\delta}(\\mathbb{P}_{N},\\theta_{*})|\\leq\\underset{f\\in\\mathcal{L}^{c}}{\\operatorname*{sup}}\\,|\\int_{\\mathcal{Z}}f(z)d(\\mathbb{P}_{N}-\\mathbb{P}_{*})(z)|+L\\delta^{1-p}\\,p\\,\\mathrm{diam}\\,(\\mathcal{V})^{p-1}\\,\\,M_{d}\\,N^{-\\eta},}\\end{array}$ where $\\mathcal{L}^{c}=\\{\\ell_{\\lambda}^{c}(\\cdot,\\theta):\\lambda\\in[0,L\\delta^{1-p}],\\theta\\in\\Theta\\}$ is the DR loss class. ", "page_idx": 31}, {"type": "text", "text": "In the remaining part of the proof, we estimate $\\begin{array}{r}{\\operatorname*{sup}_{f\\in{\\mathscr{L}}^{c}}|\\int_{{\\mathcal{Z}}}f(z)d(\\mathbb{P}_{*}-\\mathbb{P}_{N})(z)|}\\end{array}$ using conventional methods from statistical learning theory. According to assumption 2, the functions within $\\mathcal{F}$ are limited as shown: ", "page_idx": 31}, {"type": "equation", "text": "$$\n0\\leq\\ell_{\\lambda}^{c}(v,\\theta)\\leq\\operatorname*{sup}_{v^{\\prime}\\in\\mathcal{V}}\\ell(v^{\\prime},y,\\theta)-\\lambda d(v,v^{\\prime})\\leq\\operatorname*{sup}_{v^{\\prime}\\in\\mathcal{V}}\\ell(v^{\\prime},y,\\theta)\\leq M.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "similar to the proof Theorem 3 [41], by utilizing the bounded-differences inequality and symmetrization, we derive that: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{f\\in\\mathcal{L}^{c}}\\left|\\int_{\\mathcal{Z}}f(z)d(\\mathbb{P}_{N}-\\mathbb{P}_{*})(z)\\right|\\le2\\Re_{N}(\\mathcal{L}^{c})+M\\sqrt{\\frac{\\log\\frac{2}{\\epsilon}}{2N}}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "holds with a probability of at least $1-\\epsilon$ , where $\\Re_{n}(\\mathcal{L}^{c})$ represents the Rademacher complexity of $\\mathcal{L}^{c}$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Re_{\\mathfrak{N}}(\\mathcal{L}^{c})=\\mathbb{E}\\left[\\operatorname*{sup}_{f\\in\\mathcal{L}^{c}}\\frac{1}{N}\\sum_{i=1}^{N}\\sigma_{i}f(Z_{i})\\right].\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In the proof of Theorem 2 [41], the authors has proved: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Re_{N}(\\mathcal{L}^{c})\\leq\\frac{24\\mathfrak{C}(\\mathcal{L})}{\\sqrt{N}}+\\frac{24L.\\mathrm{diam}\\left(\\mathcal{V}\\right)^{p}}{\\sqrt{N}\\delta^{p-1}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where ${\\mathfrak{C}}({\\mathcal{L}})$ , the entropy integral of of loss class. By applying this result it can be written: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\delta}(\\mathbb{P}_{*},\\theta_{*})-\\hat{\\mathcal{R}}_{\\delta}(\\mathbb{P}_{N},\\theta_{*})|\\leq M\\sqrt{\\frac{\\log\\frac{2}{\\epsilon}}{2N}}+\\frac{48\\mathfrak{C}(\\mathcal{L})}{\\sqrt{n}}+\\frac{48L\\operatorname{diam}{(\\mathcal{V})}^{p}}{\\sqrt{n}\\delta^{p-1}}+L\\delta^{1-p}\\,p\\operatorname{diam}{(\\mathcal{V})}^{p-1}\\,\\,M_{d}\\,N^{-r}\\,,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Since the prove does depend on value of $\\theta$ , then $|\\mathcal{R}_{\\delta}(\\mathbb{P}_{*},\\hat{\\theta}_{N}^{\\mathrm{\\tiny~dro}})-\\hat{\\mathcal{R}}_{\\delta}(\\mathbb{P}_{N},\\hat{\\theta}_{N}^{\\mathrm{\\tiny~dro}})|$ also satisfies in the above inequality. By combining two terms with probability $1-\\epsilon$ we have, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathfrak{L}_{\\delta}(\\mathbb{P}_{*},\\hat{\\theta}_{N}^{\\mathrm{dro}})-\\mathcal{R}_{\\delta}(\\mathbb{P}_{*},\\theta_{*})\\le M\\sqrt{\\frac{2\\log\\frac{2}{\\epsilon}}{N}}+\\frac{96\\mathfrak{C}(\\mathcal{L})}{\\sqrt{n}}+\\frac{96L.\\mathrm{diam}\\left(\\mathcal{V}\\right)^{p}}{\\sqrt{n}\\delta^{p-1}}+2L\\delta^{1-p}\\,p\\,\\mathrm{diam}\\left(\\mathcal{V}\\right)^{p-1}\\,M_{d}\\,N,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Since by assumption, with probability $1-\\epsilon$ we have inequality ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\forall z,z^{\\prime}\\in|c(z,z^{\\prime})-\\hat{c}(z,z^{\\prime})|\\leq M_{d}N^{-\\eta},\\quad\\eta>0\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "therefore by probability $1-2\\epsilon$ the main inequality is true and it completes the proof. ", "page_idx": 31}, {"type": "text", "text": "C Numerical Analysis Supplementary ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "C.1 Synthetic Data Models ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The structural equations used to generate the SCMs in $\\S\\ S$ are listed below. For the LIN SCM, we generate the protected feature $\\mathbf{A}$ and variables $\\mathbf{X}_{i}$ according to the following structural equations: ", "page_idx": 32}, {"type": "text", "text": "\u2022 linear SCM (LIN): ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\mathbf{A}:=\\mathbf{U}_{\\mathbf{A}},\\qquad}&{\\mathbf{U}_{\\mathbf{A}}\\sim\\mathcal{B}(0.5)}\\\\ {\\mathbf{X}_{1}:=2\\mathbf{A}+U_{1},}&{U_{1}\\sim\\mathcal{N}(0,1)}\\\\ {\\mathbf{X}_{2}:=\\mathbf{A}-\\mathbf{X}_{1}+\\mathbf{U}_{2},}&{\\mathbf{U}_{2}\\sim\\mathcal{N}(0,1)}\\\\ {\\mathbf{Y}\\sim\\mathcal{B}((1+e x p(-(\\mathbf{X}_{1}+\\mathbf{X}_{2}))^{-1})}&{}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Here, $\\,B(p)$ represents Bernoulli random variables with probability $p$ , and $\\mathcal{N}(\\mu,\\sigma^{2})$ represents normal random variables with mean $\\mu$ and variance $\\sigma^{2}$ . To generate the ground truth $h(\\mathbf{A},X_{1},X_{2})$ , we use a linear model for the LIN method. In all the synthetic models considered, we treat $\\mathbf{A}$ as a binary-sensitive attribute. ", "page_idx": 32}, {"type": "text", "text": "C.2 Real-World Data ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In our research, we have utilized the Adult dataset [38] and the COMPAS dataset [65] for our experimental analysis. To employ these datasets, we initially constructed an SCM based on the causal graph proposed by Nabi et al. [49]. For the Adult dataset, we incorporate features such as sex, age, native-country, marital-status, education-num, hours-per-week, and consider gender as a sensitive attribute. In the case of the COMPAS dataset, the utilized features comprise age, race, sex, and priors count, which function as variables. Additionally, sex is considered a sensitive attribute. ", "page_idx": 32}, {"type": "text", "text": "For classification purposes, we apply data standardization before the learning process. ", "page_idx": 32}, {"type": "image", "img_path": "piOzFx9whU/tmp/fce852301fdf008e94fff4ee557f40e0f5d8712165929538b0106bdd056e8f14.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "In these above equations, $\\beta_{i j}$ are the coefficients for the linear combinations of the $X$ variables, and $U_{i}$ are the exogenous variables. ", "page_idx": 32}, {"type": "text", "text": "C.3 Training Methods ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In our study, we utilize various training objectives to train decision-making classifiers, with loss function $\\ell(v)$ . The training objectives are as follows: ", "page_idx": 32}, {"type": "text", "text": "\u2022 Empirical Risk Minimization (ERM): This approach minimizes the expected risk concerning the classifier parameters $\\psi$ , represented by ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\theta\\in\\Theta}\\mathbb{E}_{z\\sim\\mathbb{P}}[\\ell(z,\\theta)]\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "\u2022 Adversarial Learning (AL): This method trains the model to withstand or defend against adversarial perturbations, represented by ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\theta\\in\\Theta}\\left\\{\\mathbb{E}_{z\\sim\\mathbb{P}}\\left[\\operatorname*{sup}_{\\|\\Delta\\|\\leq\\delta}\\ell(z+\\Delta,\\theta)\\right]\\right\\}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "\u2022 ROSS: Based on the work of Ross et al. [56], this method minimizes the expected risk along with an adversarial perturbation term, represented by ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\theta\\in\\Theta}\\left\\{\\mathbb{E}_{(v,y)\\sim\\mathbb{P}}\\left[\\ell(v,y,\\theta)+\\operatorname*{inf}_{\\|\\Delta\\|\\leq\\delta}\\ell(v+\\Delta,1,\\theta)\\right]\\right\\}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "\u2022 CDRO: Our approach, as described in this paper, is formulated as follows: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\theta\\in\\Theta}\\left\\{\\operatorname*{sup}_{\\mathbb{Q}\\in\\mathbb{B}_{\\delta}(\\mathbb{P})}\\mathbf{E}_{z\\sim\\mathbb{Q}}[\\ell(z,\\theta)]\\right\\}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For our loss function \u2113, we use the binary cross-entropy loss. ", "page_idx": 33}, {"type": "text", "text": "C.4 Hyperparameter Tuning ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "The majority of the experimental setup is based on the work of Ehyaei et al. [22]. For each dataset and its respective label, we use a generalized linear model (GLM). Each training objective is applied to four different datasets, using 100 different random seeds. The optimization process is performed using the Adam optimizer with a learning rate of $10^{-3}$ and a batch size of 100. After optimizing the benchmark time and considering the training rate, we set the number of epochs to 10 to ensure comparability in benchmarking. ", "page_idx": 33}, {"type": "text", "text": "C.5 Metrics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "To assess the performance of various training methods concerning accuracy, unfair area, counterfactual fairness, and adversarial robustness, we employ seven distinct metrics as outlined below: ", "page_idx": 33}, {"type": "text", "text": "\u2022 Acc: The accuracy of the classifier, is represented as a percentage.   \n\u2022 $U_{\\delta}$ : The proportion of data points within the unfair area with a radius of $\\delta$ . $U_{\\delta}:=\\mathbb{P}\\big(\\{v\\in\\mathcal{V}:\\quad\\exists v^{\\prime}\\in\\mathcal{V}\\quad\\mathrm{s.t.}\\quad d(v,v^{\\prime})\\le\\delta\\quad\\land\\quad h(v)\\neq h(v^{\\prime})\\big\\}\\big).$   \n\u2022 $R_{\\delta}$ : The fraction of data points that are vulnerable to adversarial perturbations within a radius of $\\delta$ . This metric coincides with the unfair area in cases where no sensitive attribute is considered. $\\begin{array}{r}{R_{\\delta}:=\\mathbb{P}\\big(\\{v\\in\\mathcal{V}:\\quad\\exists\\Delta\\in\\mathcal{V}\\quad\\mathrm{s.t.}\\quad d(v,\\mathbf{C}\\mathbf{F}(v,\\Delta))\\leq\\delta\\quad\\land\\quad h(v)\\neq h(\\mathbf{C}\\mathbf{F}(v,\\Delta))\\}\\big).}\\end{array}$   \n\u2022 $C F$ : The percentage of data points that exhibit counterfactual unfairness. This metric aligns with the unfair area when the perturbation radius is zero. ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathrm{CF}:=\\mathbb{P}\\big(\\{v\\in\\mathcal{V}:\\quad\\exists a\\in\\mathcal{A}\\quad\\mathrm{s.t.}\\quad h(v)\\not=h(\\ddot{v}_{a})\\}\\big).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "C.6 Additional Results ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this section, we present additional simulation results. CDRO performs well across all datasets except for the $R_{\\delta}$ measure in the Adult dataset, likely because the linear model does not fti the SCM well. Nevertheless, CDRO demonstrates robustness and counterfactual fairness, as shown in Table 1, making it the preferred model when balancing both accuracy and fairness. ", "page_idx": 33}, {"type": "image", "img_path": "piOzFx9whU/tmp/9149b85d5def6ec4419a4692c51d7d000857acc7fbb72d917f7d7deef5d67b98.jpg", "img_caption": ["Figure 2: Displays the findings from our numerical experiment, assessing the performance of DRO across different models and datasets. (left) Counterfactual unfair area percentage (lower values are better). (right) Non-robust area performance of classifier (higher values are better) for $\\Delta=.05$ . "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "piOzFx9whU/tmp/5bcd884e4d7cfba88316247744deb0e0a46da6ab7135c1d08378c7aee51c60f3.jpg", "img_caption": ["Figure 3: Displays the findings from our numerical experiment, assessing the performance of DRO across different models and datasets. (Left) Bar plot showing the comparison of models based on the unfair area percentage $U(\\delta)$ (lower values are better) at $\\Delta=.01$ . (Right) Bar plot showing the comparison of models based on the robustness area percentage $R(\\delta)$ (lower values are better) at $\\Delta=.01$ . "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Broader Impact Statement ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Our theoretical framework bridges adversarial robustness, distributional robustness, individual fairness, and causality, aligning with the core pillars of responsible AI. By demonstrating the connection between these areas, we aim to inspire further research at their intersection and contribute to the development of safer, more equitable AI models for society. This approach holds the promise of improving decision-making under uncertainty while ensuring fairness and mitigating the impact of adversarial perturbations. ", "page_idx": 34}, {"type": "text", "text": "However, we acknowledge several limitations and ethical implications inherent in our approach. While our method produces fair and robust predictions under specific conditions, it is important to note that it fundamentally relies on a machine learning model, which may inherit the same vulnerabilities as the original model in areas not explicitly addressed in this work such as multiplicity, ", "page_idx": 34}, {"type": "table", "img_path": "piOzFx9whU/tmp/ab86ee5447ae397406662834bf8eac12c7e8393873693b95f268267ac1cba33b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "Table 1: The table presents the results of our numerical experiment, comparing various trainers based on their input sets in terms of accuracy (Acc, higher values are better), unfairness areas $(U_{.05}$ , lower values are better), unfairness areas $U_{.01}$ , lower values are better), Counterfactual Unfair area (CF, lower values are better), the non-robust percentage concerning adversarial perturbation with radii 0.05 $\\mathcal{R}_{.05}$ , lower values are better), and the non-robust percentage concerning adversarial perturbation with radii 0.01 $(\\mathcal{R}_{.01}$ , lower values are better). The top-performing techniques for each trainer, dataset, and metric are highlighted in bold. The findings demonstrate that CDRO excels in reducing unfair areas. The average standard deviation for CDRO is .029, while for the other methods, it is .031. ", "page_idx": 34}, {"type": "text", "text": "privacy breaches, lack of explainability, and safety/security concerns. Additionally, our work operates under simplifying assumptions regarding the fairness notion. In real-world applications, fairness is a complex and context-dependent concept. Therefore, it is essential to define fairness carefully and consider multiple dimensions of fairness when applying our approach. We emphasize that this work is a proof of concept, and we strongly recommend involving diverse stakeholders, including ethicists, domain experts, and affected communities, before applying our approach to high-risk application domains. It\u2019s important for users to be aware of these limitations and potential biases that might not be fully addressed by our framework. ", "page_idx": 35}, {"type": "text", "text": "D NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our claims in the abstract and the introduction match the body of the text, the provided proofs, and the reported experimental results. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We discuss the main limitations of our work in the conclusion. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We provide general assumptions in the beginning, and all the lemmas and theorems have either a full proof or a general intuition in the main body of the paper, with the remainder of proofs in the appendix. ", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We mention the used datasets and models, the random seeds, and details about the data splits in the main body of the paper and the appendix. Information about libraries and used algorithms is also provided. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We mention the used datasets and models, the random seeds, and details about the data splits in the main body of the paper and the appendix. Information about libraries and used algorithms is also provided. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We give detailed results in the form of tables (with mean and std), while we only plot the mean values to not overcrowd the figures and just report the standard deviation in the table caption. ", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [No] ", "page_idx": 37}, {"type": "text", "text": "Justification: The experiments are hardware agnostic and use fairly small models and datasets. ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We include a broader impact statement in the appendix. ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our paper does not create new data or models. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We properly credit the used datasets. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We documented the code. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}]