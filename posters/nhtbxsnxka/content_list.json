[{"type": "text", "text": "Single-Loop Stochastic Algorithms for Difference of Max-Structured Weakly Convex Functions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Quanqi Hu 1 Qi Qi 2 Zhaosong Lu 3 Tianbao Yang 1 ", "page_idx": 0}, {"type": "text", "text": "1 Department of Computer Science & Engineering, Texas A&M University 2 Department of Computer Science, The University of Iowa 3 Department of Industrial and Systems Engineering, University of Minnesota {quanqi-hu, tianbao-yang}@tamu.edu qi-qi@uiowa.edu zhaosong@umn.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we study a class of non-smooth non-convex problems in the form of $\\begin{array}{r}{\\operatorname*{min}_{x}[\\overbar{\\mathrm{max}}_{y\\in\\mathcal{Y}}\\phi(x,y)-\\operatorname*{max}_{z\\in\\mathcal{Z}}\\psi(x,z)]}\\end{array}$ , where both $\\bar{\\Phi(x)}=\\operatorname*{max}_{y\\in\\mathcal{Y}}\\phi(x,y)$ and $\\Psi(x)=\\operatorname*{max}_{z\\in{\\mathcal{Z}}}\\psi(x,z)$ are weakly convex functions, and $\\phi(x,y),\\psi(x,z)$ are strongly concave functions in terms of $y$ and $z$ , respectively. It covers two families of problems that have been studied but are missing single-loop stochastic algorithms, i.e., difference of weakly convex functions and weakly convex stronglyconcave min-max problems. We propose a stochastic Moreau envelope approximate gradient method dubbed SMAG, the first single-loop algorithm for solving these problems, and provide a state-of-the-art non-asymptotic convergence rate. The key idea of the design is to compute an approximate gradient of the Moreau envelopes of $\\Phi,\\Psi$ using only one step of stochastic gradient update of the primal and dual variables. Empirically, we conduct experiments on positive-unlabeled (PU) learning and partial area under ROC curve (pAUC) optimization with an adversarial fairness regularizer to validate the effectiveness of our proposed algorithms. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we consider a class of non-convex, non-smooth problems in the following form ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb R^{d_{x}}}\\big\\{F(x):=\\operatorname*{max}_{y\\in\\mathcal{Y}}\\phi(x,y)-\\operatorname*{max}_{z\\in\\mathcal{Z}}\\psi(x,z)\\big\\},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where the sets $\\mathcal{V}\\,\\subset\\,\\mathbb{R}^{d_{\\boldsymbol{y}}},\\,\\mathcal{Z}\\,\\subset\\,\\mathbb{R}^{d_{\\boldsymbol{z}}}$ are convex and compact, and the two component functions $\\phi(x,y)$ and $\\psi(x,z)$ are weakly-convex in terms of $x$ and strongly-concave in the terms of $y$ and $z$ , respectively. Both component functions are in expectation forms, i.e., $\\phi(x,y)=\\mathbb{E}_{\\xi\\sim\\mathcal{D}_{\\phi}}[\\phi(\\bar{x},y;\\xi)]$ and $\\psi(x,z)\\,=\\,\\mathbb{E}_{\\zeta\\sim\\mathcal{D}_{\\psi}}[\\psi(x,z;\\zeta)]$ . We refer to this class of problems as the Difference of MaxStructured Weakly Convex Functions (DMax) Optimization. DMax optimization unifies two emerging families of problems in optimization field, difference-of-weakly-convex (DWC) optimization ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d_{x}}}\\{F(x):=\\phi(x)-\\psi(x)\\},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "and weakly-convex-strongly-concave (WCSC) min-max optimization ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d_{x}}}\\left\\{F(x):=\\operatorname*{max}_{y\\in\\mathcal{Y}}\\phi(x,y)\\right\\}.\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Thus, DMax optimization has a wide range of applications in machine learning and AI, including applications of DWC optimization (e.g., positive-unlabeled (PU) Learning [39], non-convex sparsity-promoting regularizers [39], Boltzmann machines [26]) and applications of min-max optimization (e.g., adversarial learning [31, 22], distributional robust learning [8, 28], learning with non-decomposable loss [28]). In recent years, the scale of data and models significantly increased, leading to the demand of more efficient optimization methods. However, all existing stochastic methods for DWC optimization and non-smooth WCSC min-max optimization with state-of-the-art non-asymptotic convergence rate $O(\\epsilon^{-4})$ are double-loop. As a result, these methods are complex regarding the implementation and require extensive hyperparameter tuning. To close this gap, we propose a single-loop stochastic algorithm for DMax optimization and provide non-asymptotic convergence analysis to match the state-of-the-art non-asymptotic convergence rate. ", "page_idx": 0}, {"type": "table", "img_path": "NhtBXSNXKA/tmp/8525e3f7e69d6459ecfd156067ffa398f0ab7b86ead0077ef3d075c5fed85232.jpg", "table_caption": ["Table 1: Comparison with existing stochastic methods for solving DWC problems with nonasymptotic convergence guarantee. \u2217The method SBCD is designed to solve a problem in the form of $\\mathrm{min}_{x}\\{\\mathrm{min}_{y}\\,\\phi(x,y)-\\mathrm{min}_{z}\\,\\psi(x,z)\\}$ with a specific formulation of $\\phi$ and $\\psi$ . However, the method and analysis can be generalized to solving non-smooth DWC problems. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The main challenges of designing a single-loop method for DMax optimization are threefold. 1) given the weakly-convex nature of the component functions, their difference $F(x)$ is not necessarily weakly-convex, resulting in a non-smooth non-convex optimization problem. 2) the component functions $\\operatorname*{max}_{y\\in{\\mathcal{Y}}}\\phi(x,y)$ and $\\operatorname*{max}_{z\\in{\\mathcal{Z}}}\\psi(x,z)$ require solving maximization subproblems, making unbiased estimations of their subgradients inaccessible. 3) existing work on non-smooth problems with DC or/and min-max structures heavily rely on inner loops to solve subproblems to a certain accuracy. ", "page_idx": 1}, {"type": "text", "text": "To address the first challenge, we apply Moreau envelope smoothing technique [24, 3] to the component functions individually and take their difference as a smooth approximation of the original objective. Inspired by existing work [32, 45], we show that solving the original DMax problem can be achieved by solving this smooth approximation. Consequently, the problem is transformed into a smooth problem with two layer of nested optimization structure, the Moreau envelope and the maximization from the min-max structure. In order to avoid inner-loop, we perform only one step of update for each of the nested optimization problems. Our analysis leverages the fast convergence of strongly convex/concave problems, proving that single-step updates are sufficient to achieve a state-of-the-art convergence rate. Although the Moreau envelope smoothing is not new for solving DC and min-max optimization [32, 45, 47, 43], the existing results either require double loops [32, 45] or require smoothness of the objective function [47, 43]. ", "page_idx": 1}, {"type": "text", "text": "Contributions. We summarize the main contribution of this work as following. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We construct a new framework DMax optimization that unifies the DWC optimization and WCSC min-max optimization. Based on a Moreau envelope smoothing technique, we propose a singleloop stochastic algorithm, namely SMAG, for DMax optimization in non-smooth setting, which achieves $O(\\epsilon^{-4})$ convergence rate.   \n\u2022 We show that the proposed method leads to the first single-loop stochastic algorithms for DWC optimization and non-smooth WCSC min-max optimization achieving $\\mathcal{O}(\\epsilon^{-4})$ convergence rate.   \n\u2022 Finally, we present experimental results on applications including Positive-Unlabeled (PU) Learning and partial AUC optimization with an adversarial fairness regularizer to validate the effectiveness of our proposed algorithms. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Stochastic DC Optimization. DWC can be converted into Difference-of-convex (DC) programming. DC programming was initially introduced in [33] and has been extensively studied since then. A comprehensive review on the developments of DC programming can be found in [18]. Despite the rich literature on DC programming, DC in stochastic setting has rarely been mentioned until recently. Most of the existing studies on stochastic DC optimization are based on the classical method, DC Algorithm (DCA) in deterministic DC optimization. The main idea of DCA is to approximate the DC problem by a convex problem by taking the linear approximation of the second component. In other words, DCA solves $\\operatorname*{min}_{x}$ $\\{\\phi(\\overset{.}{x})-\\langle\\bar{\\nabla}\\phi(x_{k}),x\\rangle\\}$ to update $x_{k}$ and thus forms a double-loop algorithm. [34] first proposed stochastic DCA (SDCA) for solving large sum problems of non-convex smooth functions, which was further generalized to solving large sum non-smooth problems in [16]. [15] is the first work that allows both components in DC problems to be non-smooth. The authors proposed a SDCA scheme in the aggregated update style, where all past information needs to be stored for constructing future subproblems. [17] improved the efficiency of the SDCA scheme by removing the need of storing historical information. So far, none of the above work provides non-asymptotic convergence guarantee. The first non-asymptotic convergence analysis was established in [26]. The authors proposed a stochastic proximal DC algorithm (SPD), which modifies SDCA by adding an extra quadratic term after linearizing the second component function, and proved that SPD has a convergence rate of $O(\\epsilon^{-4})$ . The main drawback of their analysis is that they need the smoothness assumption of the first component function. With very similar algorithm design, [39] managed to partially relax the smoothness assumption. Given at least one of the two component functions having $\\nu$ -H\u00f6lder continuous gradient, i.e., $\\|\\nabla f(x)-\\nabla f(x^{\\prime})\\|\\,\\leq\\,\\|x-x^{\\prime}\\|^{\\nu}$ for all $x,x^{\\prime}$ , they proved a convergence rate of $O(\\epsilon^{-4/\\nu})$ . In fact, the H\u00f6lder continuous gradient assumption is still fairly strong as some of the common non-smooth functions do not satisfy, for example the hinge loss function. ", "page_idx": 1}, {"type": "table", "img_path": "NhtBXSNXKA/tmp/9ae1d5ed91adb64d10c5af96104241579ab971d138a6198f9a55b36ac5f128fe.jpg", "table_caption": ["Table 2: Comparison with existing stochastic methods for solving non-convex non-smooth min-max problems. The objective function is in the form of $\\phi(x,y)=f(x,\\bar{y})\\!-\\!g(y)\\!+\\!h(x)$ . NS and S stand for non-smooth and smooth respectively, and NSP means non-smooth and its proximal mapping is easily solved. WC, C stand for weakly-convex and convex respectively. WCSC stands for weakly-convexstrongly-concave, SSC stands for smooth and strongly concave and WCC means weakly-convexconcave. Note that Epoch-GDA and SMAG studies the general formulation $\\phi(x,y)=f({\\dot{x}},y)$ . "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Recently, another approach to tackling the non-smoothness in DC problems has been considered. Following the smoothing technique in non-smooth weakly-convex optimization literature [3], [32, 25] constructed Moreau envelope smoothing approximations for both of the component functions respectively and established non-asymptotic convergence analysis under deterministic setting and the assumption that either one component function is smooth or the proximal-point subproblems can be solved exactly. Following a similar idea, [45] studied a problem in the form of $\\scriptstyle\\operatorname*{min}_{x}F(x)\\;:=$ $\\mathrm{min}_{y}\\,\\phi(x,y)\\to\\mathrm{\\bar{m}{i n}}_{z}\\,\\psi(x,z)$ , where $\\phi$ and $\\psi$ are in some specific formulations, and proposed a double-loop algorithm with $O(\\epsilon^{-6})$ convergence rate. Although the $\\phi$ and $\\psi$ are non-smooth, their analysis heavily relies on the properties in the given formulation, especially the structures in the dual variables $y,z$ , thus is not trivial to generalize. ", "page_idx": 2}, {"type": "text", "text": "Note that none of the aforementioned work is able to solve the DMax problem, as they require unbiased stochastic gradient estimations of the two component functions, which are not accessible in DMax due to the presence of the maximization structure. ", "page_idx": 2}, {"type": "text", "text": "Stochastic Non-smooth Weakly-Convex-Strongly-Concave Min-Max Optimization. Stochastic WCSC min-max optimization has been an emerging topic in recent years. Most of the existing works focuses on the smooth setting, i.e., the objective is smooth [12, 19, 49, 43, 47, 43] or the stochastic gradient oracles are Lipschitz continuous [23, 42, 11, 21, 38]. To the best of our knowledge, [29] is the first work that considers non-smooth WCSC min-max problems. They considered a special structure where the maximization over $y$ given $x$ can be simply solved and it is solved with $\\mathcal{O}(1/\\epsilon^{2})$ times. They proposed a nested method Proximally Guided Stochastic Mirror Descent Method (PGSMD) that achieves a convergence rate of $\\mathcal{O}(\\epsilon^{-4})$ . Later, [40] further relaxed the assumption by removing the requirement of the special structure, and proved that their nested method Epoch-GDA has a similar convergence rate of $O(\\epsilon^{-4})$ . Another line of work studies a special case of the general non-smooth non-convex min-max optimization, where the objective is assumed to be composite, i.e., $\\phi(x,y)=f(x,y)-g(y)+h(x)$ , so that $f$ is smooth while $g,h$ are potentially non-smooth [1, 48]. Both works established $O(\\epsilon^{-4})$ convergence rate, and assume $f$ is smooth and strongly concave, $g$ and $h$ are convex but potentially non-smooth and their proximal mappings can be easily solved. However, none of them is applicable to the general non-smooth WCSC min-max optimization. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Notations. For simplicity, we denote $\\Phi(x):=\\operatorname*{max}_{y\\in\\mathcal{Y}}\\phi(x,y)$ , $\\Psi(x):=\\operatorname*{max}_{z\\in{\\mathcal{Z}}}\\psi(x,z),y^{*}(\\cdot):=$ arg $\\operatorname*{max}_{y\\in\\mathcal{y}}\\phi(\\cdot,y)$ , and $\\begin{array}{r}{z^{*}(\\cdot):=\\arg\\operatorname*{max}_{z\\in\\mathcal{Z}}\\psi(\\cdot,z)}\\end{array}$ . We use $\\|\\cdot\\|$ to denote the Euclidean norm of a vector and $P_{\\mathcal{C}}(\\cdot)$ to denote the Euclidean projection onto a closed set $\\mathcal{C}$ . We use the following definitions of general subgradient and subdifferential [3, 30]. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (subgradient and subdifferential). Consider a function $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}\\cup\\{\\infty\\}$ and a point $x$ with finite $f(x)$ . A vector $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ is a general subgradient of $f$ at $x$ if ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(y)\\geq f(x)+\\langle v,y-x\\rangle+o(\\|y-x\\|)\\quad{\\mathrm{as~}}y\\to x.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The subdifferential $\\partial f(x)$ is the set of subgradients of $f$ at point $x$ . ", "page_idx": 3}, {"type": "text", "text": "For simplicity, we abuse the notation $\\partial f(x)$ to denote one subgradient from the corresponding subdifferential when no confusion could be caused. We use $\\tilde{\\partial}^{\\aa}f(x)$ and $\\tilde{\\nabla}f(x)$ to represent an unbiased stochastic estimator of the subgradient $\\partial f(x)$ and the gradient $\\nabla f(x)$ respectively. A function $f:\\mathcal{D}\\rightarrow\\mathbb{R}$ is said to be $L$ -smooth if $\\|\\nabla f(x)-\\nabla f(x^{\\prime})\\|\\le L\\|x-x^{\\prime}\\|$ for all $x,x^{\\prime}\\in\\mathcal{D}$ . A function $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}\\cup\\{\\infty\\}$ is $\\delta$ -weakly convex if $f(\\cdot)+\\frac{\\delta}{2}\\|\\cdot\\|^{2}$ is convex. A mapping $\\mathcal{M}:\\mathcal{D}\\rightarrow\\mathbb{R}^{l}$ is said to be $C$ -Lipschitz continuous if $\\|\\mathcal{M}(x)-\\mathcal{M}(x^{\\prime})\\|\\le C\\|x-x^{\\prime}\\|$ for all $x,x^{\\prime}\\in\\mathcal{D}$ . ", "page_idx": 3}, {"type": "text", "text": "Consider solving a non-smooth problem $\\operatorname*{min}_{x}f(x)$ . One of the main challenges is that the $\\epsilon\\cdot$ - stationary point, i.e., a point $x$ such that d $\\mathrm{ist}(0,\\partial f(x))\\leq\\epsilon$ , which is the typical goal for smooth problems, may not exist in the neighborhood of its optimal solution. A classical counter example would be $f(x)=|x|$ , where for $\\epsilon\\in[0,1)$ the only $\\epsilon$ -stationary point is the optimal solution $x=0$ . A standard solution to this issue in weakly-convex setting is to use a relaxed convergence criteria, that is to find a point no more than $\\epsilon$ away from an $\\epsilon$ -stationary point. This is called a nearly $\\epsilon$ -stationary point, and is widely used in non-smooth weakly-convex optimization literature [4, 29, 41, 50, 51, 19]. In fact, finding a nearly $\\epsilon$ -stationary point for $f(x)$ can be achieved by finding an $\\epsilon$ -stationary point of $f_{\\gamma}(\\boldsymbol{x})$ , the Moreau envelope of $f(x)$ . Assume function $f$ is $\\delta$ -weakly-convex, then its Moreau envelope and proximal map are given by ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{\\gamma}(x):=\\operatorname*{min}_{x^{\\prime}}\\Big\\{f(x^{\\prime})+\\frac{1}{2\\gamma}\\|x^{\\prime}-x\\|^{2}\\Big\\},\\quad\\mathsf{p r o x}_{\\gamma f}(x):=\\arg\\operatorname*{min}_{x^{\\prime}}\\Big\\{f(x^{\\prime})+\\frac{1}{2\\gamma}\\|x^{\\prime}-x\\|^{2}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Existing work [3] has shown that with $\\gamma\\in(0,\\delta^{-1})$ and ${\\hat{x}}=\\operatorname{prox}_{\\gamma f}(x)$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla f_{\\gamma}(x)=\\gamma^{-1}(x-\\hat{x}),\\quad f(\\hat{x})\\leq f(x),\\quad\\mathsf{d i s t}(0,\\partial f(\\hat{x}))\\leq\\Vert\\nabla f_{\\gamma}(x)\\Vert.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Moreover, $\\mathrm{prox}_{\\gamma f}(x)$ is $\\textstyle{\\frac{1}{1-\\gamma\\delta}}$ - Lipschitz continuous [32]. ", "page_idx": 3}, {"type": "text", "text": "Now we consider the DMax problem (1). By Danskin\u2019s Theorem, the weak convexity assumption of $\\phi(\\cdot,y)$ and $\\psi(\\cdot,z)$ naturally leads to the weak convexity of $\\Phi(\\cdot)$ and $\\Psi(\\cdot)$ . Since the weak convexity assumption of component functions does not guarantee the weak convexity of their difference function $F(x)$ , one may neither 1) use nearly $\\epsilon$ -stationary point of $F(x)$ as the convergence metric, nor 2) directly apply Moreau envelope smoothing technique to $F(x)$ . To tackle the first issue, we follow the existing work [45] to use the following convergence metric for non-smooth DWC problems. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.2 (Definition 2 in [45]). Given $\\epsilon\\mathrm{~>~0~}$ , we say $x$ is a nearly $\\epsilon$ -critical point of $\\operatorname*{min}_{x}\\{F(x)\\ :=\\ \\Phi(x)\\,-\\,\\Psi(x)\\}$ if there exist $v,x^{\\prime},x^{\\prime\\prime}$ such that $v\\ \\in\\ \\partial\\dot{\\Phi}(x^{\\prime})\\,-\\,\\partial\\Psi(\\bar{x}^{\\prime\\prime})$ and $\\operatorname*{max}\\{\\mathbb{E}\\|v\\|,\\mathbb{E}\\|x-x^{\\prime}\\|,\\mathbb{E}\\|x-x^{\\prime\\prime}\\|\\}\\leq\\epsilon.$ ", "page_idx": 3}, {"type": "text", "text": "To tackle the second issue, we take the Moreau envelope of $\\Phi(\\cdot)$ and $\\Psi(\\cdot)$ individually and define the smooth approximation of $F(x)$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\nF_{\\gamma}(x)=\\Phi_{\\gamma}(x)-\\Psi_{\\gamma}(x).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The recent work [32] has proven that $F_{\\gamma}(x)$ is indeed smooth. ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 Stochastic Moreau Envelope Approximate Gradient Method (SMAG) ", "page_idx": 4}, {"type": "text", "text": "1: Input Initial points: $x_{\\phi}^{0}$ , $x_{\\psi}^{0}$ , $x_{0}$ , y0, $z_{0}$ . Hyper-parameters: $\\gamma,\\eta_{0},\\eta_{1}$ .   \n2: Stochastic (sub)gradients: $\\tilde{\\partial}_{x}\\phi,\\tilde{\\nabla}_{y}\\phi,\\tilde{\\partial}_{x}\\psi,\\tilde{\\nabla}_{z}\\psi$ .   \n3: for $t=0,\\dots,T-1$ do   \n4: $\\begin{array}{r l}&{x_{\\phi}^{t+1}=x_{\\phi}^{t}-\\eta_{1}(\\tilde{\\partial}_{x}\\phi(x_{\\phi}^{t},y_{t})+\\frac{1}{\\gamma}(x_{\\phi}^{t}-x_{t}))}\\\\ &{y_{t+1}=P_{y}\\big(y_{t}+\\eta_{1}\\tilde{\\nabla}_{y}\\phi(x_{\\phi}^{t},y_{t})\\big)}\\\\ &{x_{\\psi}^{t+1}=x_{\\psi}^{t}-\\eta_{1}(\\tilde{\\partial}_{x}\\psi(x_{\\psi}^{t},z_{t})+\\frac{1}{\\gamma}(x_{\\psi}^{t}-x_{t}))}\\\\ &{z_{t+1}=P_{Z}\\big(z_{t}+\\eta_{1}\\tilde{\\nabla}_{z}\\psi(x_{\\psi}^{t},z_{t})\\big)}\\\\ &{G_{t+1}=\\frac{1}{\\gamma}\\big(x_{t}-x_{\\phi}^{t+1}\\big)\\ -\\ \\frac{1}{\\gamma}\\big(x_{t}-x_{\\psi}^{t+1}\\big)}\\end{array}$   \n5:   \n6:   \n7:   \n8:   \n9: $x_{t+1}=x_{t}-\\eta_{0}G_{t+1}$   \n10: end for   \n11: return xt\u00af\u03d5 or xt with $\\bar{t}$ uniformly sampled from $\\{1,\\ldots,T\\}$ ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.3 (Proposition EC.1.2 in [32]). Assume $\\Phi(\\cdot)$ and $\\Psi(\\cdot)$ are $\\delta_{\\phi},\\delta_{\\psi}$ -weakly convex respectively. Then F\u03b3(x) = \u03a6\u03b3(x) \u2212\u03a8\u03b3(x) is LF -smooth, where LF =\u03b3\u2212\u03b32 mi2n{\u03b4\u03c8,\u03b4\u03d5}. ", "page_idx": 4}, {"type": "text", "text": "Moreover, one can show that a good approximate stationary point $x$ of $F_{\\gamma}(\\cdot)$ and a good approximation point $x^{\\prime}$ to the proximal points $\\mathrm{prox}_{\\gamma\\Phi}(x)$ and $\\mathrm{prox}_{\\gamma\\Psi}(x)$ can guarantee that $x^{\\prime}$ is a nearly $\\epsilon$ -critical point of $\\mathrm{min}_{\\hat{x}}\\,F(\\hat{x})$ . ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.4 (Lemma 3 in [45]). Assume $\\Phi(\\cdot)$ and $\\Psi(\\cdot)$ are $\\delta_{\\phi},\\delta_{\\psi}$ -weakly convex respectively, and $0<\\gamma<\\operatorname*{min}\\{\\delta_{\\phi}^{-1},\\delta_{\\psi}^{-1}\\}$ . If $x$ is a vector such that $\\mathbb{E}[\\|\\nabla F_{\\gamma}(x)\\|^{2}]\\leq\\operatorname*{min}\\{1,\\gamma^{-2}\\}\\epsilon^{2}/4,$ , and $x^{\\prime}$ is $a$ vector such that $\\mathbb{E}[\\|x^{\\prime}-p r o x_{\\gamma\\Phi}(x)\\|^{2}]\\le\\epsilon^{2}/4\\,o r\\,\\mathbb{E}[\\|x^{\\prime}-p r o x_{\\gamma\\Psi}(x)\\|^{2}]\\le\\epsilon^{2}/4$ , then $x^{\\prime}$ is a nearly $\\epsilon$ -critical point of $\\mathrm{min}_{\\hat{x}}\\,F(\\hat{x})$ . ", "page_idx": 4}, {"type": "text", "text": "4 Algorithms and Convergence ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Since we aim to minimize the smooth function $F_{\\gamma}(x)$ , the natural strategy is to perform gradient descent to update the variable $x$ . Following from the properties of Moreau envelope, the gradient of $F_{\\gamma}(x)$ is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla F_{\\gamma}(x)=\\ \\frac{1}{\\gamma}(x-\\mathrm{prox}_{\\gamma\\Phi}(x))\\ \\ -\\ \\frac{1}{\\gamma}(x-\\mathrm{prox}_{\\gamma\\Psi}(x))\\ \\ ,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the blue component is the gradient of $\\Phi_{\\gamma}(x)$ and the green component is the gradient of $\\Psi_{\\gamma}(x)$ . However, the proximal points $\\mathrm{prox}_{\\gamma\\Psi}(x)$ and $\\mathrm{prox}_{\\gamma\\Phi}(x)$ are not accessible in general. Indeed, these proximal points are the optimal solutions to $\\begin{array}{r}{\\operatorname*{min}_{x^{\\prime}}\\{\\dot{\\Phi}(x^{\\prime})\\!+\\!\\frac{1}{2\\gamma}||x\\!-\\!x^{\\prime}||^{2}\\}}\\end{array}$ and $\\begin{array}{r}{\\operatorname*{min}_{x^{\\prime}}\\{\\Psi(x^{\\prime})\\!+\\!\\frac{1}{2\\gamma}\\|x\\!-\\!}\\end{array}$ $x^{\\prime}\\|^{2}\\}$ respectively, and $\\Phi(\\cdot)$ and $\\Psi(\\cdot)$ are typically not accessible because they are the value functions of possibly sophisticated maximization problems. Thus, we maintain two variables $x_{\\phi}^{t}$ and $x_{\\psi}^{t}$ as the estimators of $\\ p\\mathrm{rox}_{\\gamma\\Phi}(x_{t})$ and $\\ p\\mathrm{rox}_{\\gamma\\Psi}(x_{t})$ respectively, and maintain another two variables $y_{t}$ and $z_{t}$ as the estimators of $\\begin{array}{r}{\\arg\\operatorname*{max}_{y\\in\\mathcal{Y}}\\phi(\\mathbf{prox}_{\\gamma\\Phi}(x_{t}),y)}\\end{array}$ and $\\arg\\operatorname*{max}_{z\\in\\mathcal{Z}}\\psi(\\mathbf{prox}_{\\gamma\\Psi}(x_{t}),z)$ respectively. At each iteration, we update $x_{\\phi}^{t}$ and $x_{\\psi}^{t}$ by one step of stochastic gradient descent, and update $y_{t}$ and $z_{t}$ by one step of stochastic gradient ascent. Finally, we compute the gradient estimator $\\begin{array}{r}{\\bar{G}_{t+1}=\\frac{1}{\\gamma}(\\bar{x}_{t}-x_{\\phi}^{t+\\bar{1}})-\\frac{1}{\\gamma}(x_{t}-x_{\\psi}^{t+\\bar{1}})}\\end{array}$ of $\\nabla F_{\\gamma}(x_{t})$ and update $x_{t}$ by one step of gradient descent. The resulting algorithm is presented in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "DWC Optimization. For DWC problem (2), the associated functions $\\Phi(\\cdot)=\\phi(\\cdot)$ and $\\Psi(\\cdot)=\\psi(\\cdot)$ are directly accessible. Thus the variables $y_{t}$ and $z_{t}$ in SMAG are no longer needed. The simplified SMAG algorithm for DWC optimization is presented in Algorithm 2. ", "page_idx": 4}, {"type": "text", "text": "WCSC Min-Max Optimization. For WCSC Min-Max problem (3), the second component function $\\Psi=0$ can be ignored, and thus variables $x_{\\psi}^{t}$ and $z_{t}$ are no longer needed. However, this brings a change to the gradient of $F_{\\gamma}(x_{t})$ as it now becomes ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla F_{\\gamma}(x_{t})=\\gamma^{-1}(x_{t}-\\mathrm{prox}_{\\gamma\\Phi}(x_{t})).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The simplified SMAG algorithm for WCSC Min-Max optimization is presented in Algorithm 3. ", "page_idx": 5}, {"type": "table", "img_path": "NhtBXSNXKA/tmp/5683dffb33dfe0c67bd2b3f40187d66d53304d11da3d4da6f1deab4951159fc3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.1 Convergence Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present convergence results for Algorithms 1-3. To proceed, we make the following assumption for DMax problem (1). ", "page_idx": 5}, {"type": "text", "text": "Assumption 4.1. Considering DMax problem (1), we assume that ", "page_idx": 5}, {"type": "text", "text": "(i) $\\phi(\\cdot,y)$ is $\\delta_{\\phi}$ -weakly convex, and $\\psi(\\cdot,z)$ is $\\delta_{\\psi}$ -weakly convex.   \n(ii) $\\phi(x,\\cdot)$ is $\\mu_{\\phi}$ -strongly concave, and $\\psi(x,\\cdot)$ is $\\mu_{\\psi}$ -strongly concave.   \n(iii) $\\phi(x,y)$ and $\\psi(x,z)$ are differentiable in terms of $y$ and $z$ respectively, $\\nabla_{y}\\phi(\\cdot,y)$ is $L_{\\phi,y x}$ - Lipschitz continuous, and $\\nabla_{z}\\psi(\\cdot,z)$ is $L_{\\psi,z x}$ -Lipschitz continuous.   \n(iv) There exists a constant $F_{\\gamma}^{\\ast}>-\\infty$ such that $F_{\\gamma}^{*}\\leq F_{\\gamma}(x)$ for all $x$ .   \n(v) There exists a finite constant $M$ such that $\\mathbb{E}\\|\\tilde{\\partial}_{x}\\phi(x,y)\\|^{2}\\,\\le\\,M^{2},\\,\\mathbb{E}\\|\\tilde{\\nabla}_{y}\\phi(x,y)\\|^{2}\\,\\le\\,M^{2},$ $\\mathbb{E}\\|\\tilde{\\partial}_{x}\\psi(x,z)\\|^{2}\\leq M^{2},\\mathbb{E}\\|\\tilde{\\nabla}_{z}\\psi(x,z)\\|^{2}\\leq M^{2}$ for all $x\\in\\mathbb{R}^{d_{x}}$ , $y\\in\\mathcal{V}$ and $z\\in{\\mathcal{Z}}$ . ", "page_idx": 5}, {"type": "text", "text": "It shall be noted that Assumption 4.1(iii) only requires partial smoothness of $\\phi$ and $\\psi$ , and is to ensure the Lipschitz continuity of $\\begin{array}{r}{y^{*}(\\cdot):=\\arg\\operatorname*{max}_{y\\in\\mathcal{Y}}\\phi(\\cdot,y)}\\end{array}$ and $\\boldsymbol{z}^{*}(\\cdot):=\\arg\\operatorname*{max}_{\\boldsymbol{z}\\in\\mathcal{Z}}\\psi(\\cdot,\\boldsymbol{z})$ . This follows from existing results. ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.2 (Lemma 4.3 in [19]). Consider problem $\\operatorname*{max}_{y\\in{\\hat{y}}}f(x,y)$ for any $x\\,\\in\\,\\mathbb{R}^{d_{x}}$ , where $\\hat{y}\\subset\\mathbb{R}^{d_{y}}$ is a closed convex set. Assume that $f(x,y)$ is $\\mu$ -strongly concave in y for each $x\\in\\mathbb{R}^{d_{x}}$ , and $\\nabla_{y}f(\\cdot,y)$ is $L_{y x}$ -Lipschitz for each $y\\in\\hat{\\mathcal{V}}$ . Then arg $\\operatorname*{max}_{y}f(\\cdot,y)$ is $\\frac{L_{y x}}{\\mu}$ -Lipschitz continuous. ", "page_idx": 5}, {"type": "text", "text": "A Lipschitz smooth function $f(x,y)$ is guaranteed to have Lipschitz continuous partial gradient $\\nabla_{y}f(\\cdot,y)$ , while the reverse statement is not necessarily true. For example, consider a function $f(x,y)=y^{\\top}h(x)-g(y)$ with non-smooth $C$ -Lipschitz continuous $h(\\cdot)$ and strongly convex $g$ . Then $f(x,y)$ is non-smooth but the partial subgradient $\\nabla_{y}f(\\cdot,y)=h(\\cdot)-\\overleftarrow{\\nabla}g(y)$ is Lipschitz continuous with respect to the first argument. Another example is given by $f(x,y)=f_{1}(x)+f_{2}(x,y)$ , where $f_{1}$ is weakly convex and $f_{2}$ is smooth and strongly concave in terms of $y$ . The latter is indeed seen in our considered application for pAUC maximization with adversarial fairness. In fact, one may replace Assumption 4.1(iii) by directly assuming that $y^{\\ast}(\\cdot)$ and $z^{*}(\\cdot)$ are Lipschitz continuous. In addition, Assumption 4.1(v) is standard in non-smooth optimization literature [3, 39, 10]. ", "page_idx": 5}, {"type": "text", "text": "Here we give a brief outline of the convergence analysis. First of all, we present a standard result [7]. Lemma 4.3. Suppose that $F_{\\gamma}(\\cdot)$ is $L_{F}$ -smooth and $x_{t+1}=x_{t}-\\eta_{0}G_{t+1}$ with $\\begin{array}{r}{0<\\eta_{0}\\leq\\frac{1}{2L_{F}}}\\end{array}$ . Then we have ", "page_idx": 5}, {"type": "equation", "text": "$$\nF_{\\gamma}(x_{t+1})\\leq F_{\\gamma}(x_{t})+\\frac{\\eta_{0}}{2}\\|\\nabla F_{\\gamma}(x_{t})-G_{t+1}\\|^{2}-\\frac{\\eta_{0}}{2}\\|\\nabla F_{\\gamma}(x_{t})\\|^{2}-\\frac{\\eta_{0}}{4}\\|G_{t+1}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This implies that the key to bounding the gradient $\\|\\nabla F_{\\gamma}(x_{t})\\|^{2}$ is to obtain a recursive bound for the gradient estimation error $\\|\\nabla F_{\\gamma}(x_{t})-G_{t+1}\\|^{2}$ . Following from the true gradient formulation 5, we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|\\nabla F_{\\gamma}(x_{t})-G_{t+1}\\|^{2}\\leq\\frac{2}{\\gamma^{2}}\\left(\\|x_{\\phi}^{t+1}-\\mathrm{{prox}}_{\\gamma\\Phi}(x_{t})\\|^{2}+\\|x_{\\psi}^{t+1}-{\\mathrm{prox}}_{\\gamma\\Psi}(x_{t})\\|^{2}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In other words, the error of the gradient estimation $G_{t+1}$ can be bounded by the estimation errors $\\|x_{\\phi}^{t+1}-\\mathrm{prox}_{\\gamma\\Phi}(x_{t})\\|^{2}$ $x_{\\phi}^{t+1}$ xt\u03c8+1. Thus, we construct recursive bound for the proximal point estimation errors and $\\|x_{\\psi}^{t+1}-\\mathrm{prox}_{\\gamma\\Psi}(x_{t})\\|^{2}$ individually. In fact, these two errors share almost identical analysis due to similar assumptions and updates. Here we only present the result for function $\\phi$ , as the result for $\\psi$ directly follows. ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.4. Suppose that Assumption 4.1 holds, $0<\\gamma<1/\\delta_{\\phi}$ , and $\\begin{array}{r}{\\eta_{1}\\le\\frac{\\gamma^{2}\\left(1/\\gamma-\\delta_{\\phi}\\right)}{2}}\\end{array}$ . Then the sequences $\\{x_{t}\\},\\,\\{y_{t}\\},\\,\\{x_{\\phi}^{t}\\}$ and $\\{G_{t}\\}$ generated by Algorithm $^{\\,l}$ satisfy ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|x_{\\phi}^{t+1}-p r o x_{\\gamma\\Phi}(x_{t})\\|^{2}+\\mathbb{E}_{t}\\|y_{t+1}-y^{*}(p r o x_{\\gamma\\Phi}(x_{t}))\\|^{2}}\\\\ &{\\leq(1-\\frac{\\eta_{1}\\left(1/\\gamma-\\delta_{\\phi}\\right)}{2})\\mathbb{E}\\|x_{\\phi}^{t}-p r o x_{\\gamma\\Phi}(x_{t-1})\\|^{2}+(1-\\eta_{1}\\mu_{\\phi})\\mathbb{E}\\|y_{t}-y^{*}(p r o x_{\\gamma\\Phi}(x_{t-1}))\\|^{2}}\\\\ &{\\quad+\\left(\\frac{2\\eta_{0}^{2}}{\\eta_{1}\\gamma^{2}\\left(1/\\gamma-\\delta_{\\phi}\\right)^{3}}+\\frac{L_{\\phi,y x}^{2}\\eta_{0}^{2}}{\\eta_{1}\\mu_{\\phi}^{3}\\gamma^{2}\\left(1/\\gamma-\\delta_{\\phi}\\right)^{2}}\\right)\\mathbb{E}\\|G_{t}\\|^{2}+12M^{2}\\eta_{1}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Finally, combining Lemma 4.3, inequality (6) and Lemma 4.4 yields the following convergence result for Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.5. Suppose that Assumption 4.1 holds, $0\\,<\\,\\gamma\\,<\\,\\operatorname*{min}\\{\\delta_{\\phi}^{-1},\\delta_{\\psi}^{-1}\\}$ , $\\eta_{1}\\,=\\,\\mathcal{O}(\\epsilon^{2})$ , and $\\eta_{0}~=~\\tau\\eta_{1}$ . Then after $T\\,\\geq\\,{\\mathcal{O}}(\\epsilon^{-4})$ iterations, the sequences $\\left\\{x_{t}\\right\\}$ , $\\{x_{\\phi}^{t}\\}$ and $\\{\\boldsymbol{x}_{\\psi}^{t}\\}$ generated by Algorithm $^{\\,l}$ satisfy $\\mathbb{E}[\\|x_{\\phi}^{\\bar{t}}-p r o x_{\\gamma\\Phi}(x_{\\bar{t}-1})\\|^{2}+\\|x_{\\psi}^{\\bar{t}}-p r o x_{\\gamma\\Psi}(x_{\\bar{t}-1})\\|^{2}+\\|\\nabla F_{\\gamma}(x_{\\bar{t}-1})\\|^{2}]\\le$ $\\operatorname*{min}\\{1,\\gamma^{-2}\\}\\epsilon^{2}/4,$ , and the outputs $x_{\\phi}^{\\bar{t}}$ and $x_{\\psi}^{\\bar{t}}$ are both nearly $\\epsilon$ -critical points of problem $(I)$ . ", "page_idx": 6}, {"type": "text", "text": "Since DMax optimization is a unified framework covering DWC optimization and WCSC min-max optimization, the convergence results of Algorithms 2 and 3 directly follow from Theorem 4.5. To present them, we first provide a reduced version of Assumption 4.1 for DWC problem (2). ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.6. Considering DWC problem (2), we assume that ", "page_idx": 6}, {"type": "text", "text": "(i) $\\phi(\\cdot)$ is $\\delta_{\\phi}$ -weakly convex, and $\\psi(\\cdot)$ is $\\delta_{\\psi}$ -weakly convex.   \n(ii) There exists a constant $F_{\\gamma}^{\\ast}>-\\infty$ such that $F_{\\gamma}^{*}\\leq F_{\\gamma}(x)$ for all $x$ .   \n(iii) There exists a finite constant $M$ such that $\\mathbb{E}\\|\\tilde{\\partial}\\phi(x)\\|^{2}\\leq M^{2}$ and $\\mathbb{E}\\|\\tilde{\\partial}\\psi(x)\\|^{2}\\leq M^{2}$ for all x \u2208Rdx. ", "page_idx": 6}, {"type": "text", "text": "By setting $\\phi(x,y)=\\phi(x)$ and $\\psi(x,z)=\\psi(x)$ , namely independent of $y$ and $z$ , in DMax problem (1), we obtain the following convergence result for Algorithm 2, which is an immediate consequence of Theorem 4.5. ", "page_idx": 6}, {"type": "text", "text": "Corollary 4.7. Suppose that Assumption 4.6 holds, $0\\,<\\,\\gamma\\,<\\,\\mathrm{min}\\{\\delta_{\\phi}^{-1},\\delta_{\\psi}^{-1}\\}$ , $\\eta_{1}=\\mathcal{O}(\\epsilon^{2})$ , and $\\eta_{0}=\\tau\\eta_{1}$ . Then after $T\\geq\\mathcal{O}(\\epsilon^{-4})$ iterations, the outputs $x_{\\phi}^{\\bar{t}}$ and $x_{\\psi}^{\\bar{t}}$ of Algorithm 2 are both nearly $\\epsilon$ -critical points of problem (2). ", "page_idx": 6}, {"type": "text", "text": "For WCSC min-max problem (3), we reduce Assumption 4.1 to the following. ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.8. Considering WCSC min-max problem (3), we assume that ", "page_idx": 6}, {"type": "text", "text": "(i) $\\phi(\\cdot,y)$ is $\\delta_{\\phi}$ -weakly convex, and $\\phi(x,\\cdot)$ is $\\mu_{\\phi}$ -strongly concave.   \n(ii) $\\phi(x,y)$ is differentiable in terms of $y$ , and $\\nabla_{y}\\phi(\\cdot,y)$ is $L_{\\phi,y x}$ -Lipschitz continuous.   \n(iii) There exists a constant $F_{\\gamma}^{\\ast}>-\\infty$ such that $F_{\\gamma}^{*}\\leq F_{\\gamma}(x)$ for all $x$ .   \n(iv) There exists a finite constant $M$ such that $\\mathbb{E}\\|\\tilde{\\partial}_{x}\\phi(x,y)\\|^{2}\\leq M^{2}$ and $\\mathbb{E}\\|\\tilde{\\nabla}_{y}\\phi(x,y)\\|^{2}\\leq M^{2}$ for all $x\\in\\mathbb{R}^{d_{x}}$ and $y\\in\\mathcal{V}$ . ", "page_idx": 6}, {"type": "text", "text": "By setting $\\psi(x,z)\\;=\\;0$ in DMax problem (1), we obtain the following convergence result for Algorithm 3, which is an immediate consequence of Theorem 4.5. ", "page_idx": 7}, {"type": "text", "text": "Corollary 4.9. Suppose that Assumption 4.8 holds, $0<\\gamma<1/\\delta_{\\phi}$ , $\\eta_{1}=\\mathcal{O}(\\epsilon^{2})$ , and $\\eta_{0}\\,=\\,\\tau\\eta_{1}$ , Then after $T\\geq\\mathcal{O}(\\epsilon^{-4})$ iterations, the output $x_{\\bar{t}}$ of Algorithm $^3$ is a nearly $\\epsilon$ -stationary point of problem (3). ", "page_idx": 7}, {"type": "text", "text": "It shall be mentioned that for WCSC min-max problem (3, we use nearly $\\epsilon$ -stationary point as the convergence metric. This is standard in weakly-convex optimization literature [3]. ", "page_idx": 7}, {"type": "text", "text": "5 Applications ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we introduce two applications of DMax optimization, PU learning for DWC optimization and partial AUC optimization with adversarial fairness regularization for WCSC min-max optimization. We also show experimental results on both applications. ", "page_idx": 7}, {"type": "text", "text": "5.1 Positive-Unlabeled Learning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In binary classification task, the optimization problem is commonly formulated as the minimization of empirical risk, i.e., $\\begin{array}{r}{\\operatorname*{min}_{\\mathbf{w}\\in\\mathbb{R}^{d}}\\frac{1}{|S|}\\!\\!\\dot{\\sum}_{\\mathbf{x}_{i}\\in S}\\ell(\\dot{\\mathbf{w}};\\mathbf{x}_{i},y_{i})}\\end{array}$ where $\\ell(\\mathbf{w};\\mathbf{x}_{i},y_{i})$ is the loss given the model parameter w on a data point $\\mathbf{x}_{i}$ and its ground truth label $y_{i}$ . Given the scenario where only positive data $S_{+}$ are observed, then the standard approach becomes problematic. One way to address this issue is to utilize unlabeled data $\\textstyle S_{u}$ to construct unbiased risk estimators. To be specific, [13] formulated the PU learning problem as following ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w}\\in\\mathbb{R}^{d}}\\frac{\\pi_{p}}{n_{+}}\\sum_{\\mathbf{x}_{i}\\in\\mathcal{S}_{+}}\\left[\\ell(\\mathbf{w};\\mathbf{x}_{i},+1)-\\ell(\\mathbf{w};\\mathbf{x}_{i},-1)\\right]+\\frac{1}{n_{u}}\\sum_{\\mathbf{x}_{j}^{u}\\in\\mathcal{S}_{u}}\\ell(\\mathbf{w};x_{j}^{u},-1)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $n_{+}=|S_{+}|$ , $n_{u}\\,=\\,|S_{u}|$ , $\\pi_{p}\\,=\\,P r(y\\,=\\,1)$ is the prior probability of the positive class. If $\\ell(\\mathbf{w};\\mathbf{x},y)$ is weakly convex in terms of w, then Problem (7) is a DWC problems. In particular, in our experiments we consider linear classification model and hinge loss. ", "page_idx": 7}, {"type": "text", "text": "Baselines. We implemented five baselines and compared them with our proposed method SMAG for DWC optimization. The first baseline, stochastic gradient descent (SGD), does not have theoretical convergence guarantee for DWC problems. However, since it is the fundamental method for convex optimization, we include it to show its performance. We also implemented existing stochastic methods for solving DC or DWC problems with non-smooth components, including SDCA [26], SSDC-SPG [39], SSDC-Adagrad [39] and SBCD [45]. ", "page_idx": 7}, {"type": "text", "text": "Datasets. We use four multi-class classification datasets, Fashion-MNIST [36], MNIST [5] CIFAR10 [14] and FER2013 [6]. To fit them in binary classification task, we consider the first five classes as negative for Fashion-MNIST, MNIST and CIFAR10, and the first four classes as negative for FER2013. For Fashion-MNIST, MNIST, CIFAR10, we follow the standard train-test split. For FER2013, we take the first 25709 samples as the training data, and the rest as for testing. ", "page_idx": 7}, {"type": "text", "text": "Setup. For all datasets, we use a batch size of 64 and set $\\pi_{p}=0.5$ . We train 40 epochs and decay the learning rate by 10 at epoch 12 and 24. The learning rates of SGD, SDCA, SSDC-SPG and SSDC-Adagrad, the learning rate of the inner loop of SBCD (i.e., $\\mu\\eta_{t}/(\\mu+\\eta_{t}))$ , and $\\eta_{1}$ in SMAG are all tuned from $\\{10,1,0.2,0.1,0.01,0.001\\}$ . The learning rate of the outer loop in SDCA and $\\eta_{0}$ in SMAG are tuned from $\\{0.1,0.5,0.9\\}$ . The numbers of inner loops for all double-loop methods are tuned from $\\{2,5,10\\}$ . The $\\mu$ in SBCD, $1/\\gamma$ in SSDC-SPG and SSDC-Adagrad, $\\gamma$ in SMAG are tuned in $\\{0.05,0.1,0.2,0.5,1,2\\}$ . We run 4 trails for each setting and plot the average curves. ", "page_idx": 7}, {"type": "text", "text": "Results. We plot the curves of training losses in Figure 1. For all tested datasets, the performance of SMAG surpasses the baselines. Among the baselines, SBDC is the generally the next best choice. However, since SBDC is a double-loop method, it has one more hyperparameter compared to SMAG. We also present the ablation study of SMAG regarding the parameter $\\gamma$ in Figure 2 included in the Appendix. ", "page_idx": 7}, {"type": "image", "img_path": "NhtBXSNXKA/tmp/afeef954bff9a8f0926f7bec616dc77d7f04387bd5a0ed748d47df3e18fd1972.jpg", "img_caption": ["Figure 1: Training Curves of PU Learning "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.2 Partial AUC Maximization with Fairness Regularization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "AUC Maximization aims to maximize the area under the curve of true positive rate (TPR) vs false positive rate (FPR). It has been studied extensively [44, 46, 20, 9] and has shown great success in large-scale real-world tasks, e.g., medical image classification [46] and molecular properties prediction [35]. One-way partial AUC (OPAUC) is an extension of AUC that has a primary interest in the curve corresponding to low FPR. To be specific, OPAUC restrict the FPR to the region $[0,\\rho]$ where $\\rho\\in(0,1)$ . A recent work [52] proposed to formulate OPAUC problem into a non-smooth weakly convex optimization problem using conditional-value-at-risk (CVaR) based distributionally robust optimization (DRO). The formulation is given by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w},\\mathbf{s}\\in\\mathbb{R}^{n_{+}}}F_{\\mathrm{pauc}}(\\mathbf{w},\\mathbf{s})=\\frac{1}{n_{+}}\\sum_{\\mathbf{x}_{i}\\in S_{+}}\\left(s_{i}+\\frac{1}{\\rho n_{-}}\\sum_{\\mathbf{x}_{j}\\in S_{-}}(L(\\mathbf{w};\\mathbf{x}_{i},\\mathbf{x}_{j})-s_{i})_{+}\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $S_{+},S_{-}$ are the sets of positive and negative samples respectively, $n_{+}=|S_{+}|,n_{-}=|S_{-}|$ , and w denotes the weights of encoder network and classification layer. The pairwise surrogate loss is defined by $L(\\mathbf{w};\\mathbf{x}_{i},\\mathbf{x}_{j})=\\ell(h(\\mathbf{w},\\mathbf{x}_{i})-h(\\mathbf{w},\\mathbf{x}_{j}))$ and we use squared hinge loss as the surrogate loss, i.e., $\\ell(\\cdot)=(c-\\cdot)^{2}$ , where $c>0$ is a parameter. ", "page_idx": 8}, {"type": "text", "text": "However, directly solving the above problem may end up with a model that is unfair with respect to some protected groups (e.g., female patients). Hence, we consider a formulation that incorporates an adversarial fairness regularization: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{u}}F_{\\mathrm{fair}}(\\mathbf{w},\\mathbf{w}_{a}):=\\mathbb{E}_{(\\mathbf{x},a)\\sim\\mathcal{D}_{a}}\\left\\{\\mathbb{I}(a=1)\\log(\\sigma(\\mathbf{w},\\mathbf{w}_{a},\\mathbf{x}))+\\mathbb{I}(a=-1)\\log(1-\\sigma(\\mathbf{w},\\mathbf{w}_{a},\\mathbf{x}))\\right\\},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\sigma(\\mathbf{w},\\mathbf{w}_{a},\\mathbf{x})$ denotes a predicted probability that the data has a sensitive attribute $a=1$ by using a classification head $\\mathbf{w}_{a}$ on top of the encoded representation of $\\mathbf{x}$ . This adversarial fairness regularization has been demonstrated effective for promoting fairness [37]. As a result, we consider OPAUC problem with a fairness regularization: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w},\\mathbf{s}\\in\\mathbb{R}^{n}+}\\operatorname*{max}_{\\mathbf{w}_{a}}F_{\\mathrm{pauc}}(\\mathbf{w},\\mathbf{s})+\\alpha F_{\\mathrm{fair}}(\\mathbf{w},\\mathbf{w}_{a})+\\frac{\\lambda_{0}}{2}\\|\\mathbf{w}_{a}\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "It is clear that the problem is WCSC. ", "page_idx": 8}, {"type": "text", "text": "Baseline. We implement our proposed method SMAG for solving OPAUC problem (8) and OPAUC problem with adversarial fairness regularization (9). We refer the former as $\\mathrm{SMAG^{*}}$ and the latter as SMAG. The baseline on OPAUC problem (8) is SOPA, proposed in [52]. The baselines on OPAUC problem with adversarial fairness regularization (9) are SGDA [19] and Epoch-GDA [40]. ", "page_idx": 8}, {"type": "text", "text": "Dataset. CelebA contains $200\\mathbf{k}$ celebrity face images with 40 binary attributes each, including the gender-sensitive attribute denoted as Male. In our experiments, we conduct experiments on three independent attribute prediction tasks: Attractive, Big Nose, and Bags Under Eyes, which have high Pearson correlations [2, 27] with the sensitive attribute Male. We divide the dataset into training, validation, and test data with an $80\\%/10\\%/10\\%$ split. ", "page_idx": 8}, {"type": "text", "text": "Setup. For all experiments, we adopt ResNet-18 as our backbone model architecture and initialize it with ImageNet pre-trained weights. The batch size is 128. We set the FPR upper bound to be $\\rho=0.3$ . We train the model for 3 epochs with cosine decay learning rates for all baselines. The regularizer parameter $\\alpha$ is tuned in 0.1, 0.2, 0.5 for SGDA, Epoch-GDA, and SMAG, and the adversarial learning rates are tuned in 0.001, 0.01, 0.1. $\\alpha\\,=\\,0$ for SOPA and $\\mathrm{SMAG^{*}}$ . The initial learning rates for optimizing w are tuned in 0.1, 0.01, 0.001 for all methods, while the weight interpolation parameters, i.e., $\\gamma$ in Epoch-GDA and SMAG, are also tuned in $0.1,0.01,0.001$ . The inner loop step is tuned in $\\{5,10,15\\}$ for Epoch-GDA. $\\eta_{1}$ in SMAG are tuned from $\\{10,1,0.2,0.1,0.01,0.001\\}$ . ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Results. We report the experimental results on three fairness metrics [27], equalized odds difference (EOD), equalized opportunity (EOP), and demographic disparity (DP) in Table 3. We observe that SMAG consistently achieves the highest pAUC score and lowest disparities metrics across all tasks compared to all other baseline min-max methods. ", "page_idx": 9}, {"type": "text", "text": "Table 3: Mean $\\pm$ std of fairness results on CelebA test dataset with Attractive and Big Nose task labels, and Male sensitive attribute. Results are reported on 3 independent runs. We use bold font to denote the best result and use underline to denote the second best. Results on Bags Under Eyes are included in the appendix due to limited space. ", "page_idx": 9}, {"type": "table", "img_path": "NhtBXSNXKA/tmp/f1354eb1df3f9f1e7945c706d7bbddaaba4089c14f1057988fee5c5889f606ba.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this study, we have introduced a new framework namely DMax optimization, that unifies DWC optimization and non-smooth WCSC min-max optimization. We proposed a single-loop stochastic method for solving DMax optimization and presented a novel convergence analysis showing that the proposed method achieves a non-asymptotic convergence rate of $\\bar{\\mathcal O}(\\epsilon^{-4})$ . Experimental results on two applications, PU learning and OPAUC optimization with adversarial fairness regularization demonstrate strong performance of our method. One limitation of this work is the strong convexity assumption on the $\\phi(x,\\cdot)$ and $\\psi(x,\\cdot)$ . This strong assumption may limit the applicability of our method. Future work will focus on exploring DMax optimization with weaker assumptions. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank anonymous reviewers for constructive comments. Q. Hu and T. Yang were partially supported by the National Science Foundation Career Award 2246753, the National Science Foundation Award 2246757, 2246756 and 2306572. Z. Lu was partially supported by the National Science Foundation Award IIS-2211491, the Office of Naval Research Award N00014-24-1-2702, and the Air Force Office of Scientific Research Award FA9550-24-1-0343. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Radu Ioan Bo,t and Axel B\u00f6hm. Alternating proximal-gradient steps for (stochastic) nonconvexconcave minimax problems. SIAM J. Optim., 33:1884\u20131913, 2020.   \n[2] Luigi Celona, Simone Bianco, and Raimondo Schettini. Fine-grained face annotation using deep multi-task cnn. Sensors, 18(8):2666, 2018.   \n[3] Damek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of weakly convex functions, 2018.   \n[4] Damek Davis and Benjamin Grimmer. Proximally guided stochastic subgradient method for nonsmooth, nonconvex problems. SIAM Journal on Optimization, 29(3):1908\u20131930, 2019.   \n[5] Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141\u2013142, 2012.   \n[6] Ian J. Goodfellow, Dumitru Erhan, Pierre Luc Carrier, Aaron Courville, Mehdi Mirza, Ben Hamner, Will Cukierski, Yichuan Tang, David Thaler, Dong-Hyun Lee, Yingbo Zhou, Chetan Ramaiah, Fangxiang Feng, Ruifan Li, Xiaojie Wang, Dimitris Athanasakis, John Shawe-Taylor, Maxim Milakov, John Park, Radu Ionescu, Marius Popescu, Cristian Grozea, James Bergstra, Jingjing Xie, Lukasz Romaszko, Bing Xu, Zhang Chuang, and Yoshua Bengio. Challenges in representation learning: A report on three machine learning contests, 2013.   \n[7] Zhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, and Tianbao Yang. A novel convergence analysis for algorithms of the adam family and beyond, 2022.   \n[8] Mert G\u00fcrb\u00fczbalaban, A. Ruszczynski, and Landi Zhu. A stochastic subgradient method for distributionally robust non-convex and non-smooth learning. Journal of Optimization Theory and Applications, 194:1014 \u2013 1041, 2022.   \n[9] Quanqi Hu, Yongjian Zhong, and Tianbao Yang. Multi-block min-max bilevel optimization with applications in multi-task deep auc maximization. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 29552\u201329565. Curran Associates, Inc., 2022.   \n[10] Quanqi Hu, Dixian Zhu, and Tianbao Yang. Non-smooth weakly-convex finite-sum coupled compositional optimization. ArXiv, abs/2310.03234, 2023.   \n[11] Feihu Huang, Shangqian Gao, Jian Pei, and Heng Huang. Accelerated zeroth-order momentum methods from mini to minimax optimization. ArXiv, abs/2008.08170, 2020.   \n[12] Chi Jin, Praneeth Netrapalli, and Michael Jordan. What is local optimality in nonconvexnonconcave minimax optimization? In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 4880\u20134889. PMLR, 13\u201318 Jul 2020.   \n[13] Ryuichi Kiryo, Gang Niu, Marthinus Christoffel du Plessis, and Masashi Sugiyama. Positiveunlabeled learning with non-negative risk estimator. ArXiv, abs/1703.00593, 2017.   \n[14] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[15] Hoai An Le Thi, Van Ngai Huynh, Tao Pham Dinh, and Hoang Phuc Hau Luu. Stochastic difference-of-convex-functions algorithms for nonconvex programming. SIAM Journal on Optimization, 32(3):2263\u20132293, 2022.   \n[16] Hoai An Le Thi, Hoai Minh Le, Duy Nhat Phan, and Bach Tran. Stochastic dca for minimizing a large sum of dc functions with application to multi-class logistic regression. Neural Networks, 132:220\u2013231, 2020.   \n[17] Hoai An Le Thi, Hoang Phuc Hau Luu, and Tao Pham Dinh. Online stochastic dca with applications to principal component analysis. IEEE Transactions on Neural Networks and Learning Systems, 35(5):7035\u20137047, 2024.   \n[18] Hoai An Le Thi and Tao Pham Dinh. Dc programming and dca: thirty years of developments. Mathematical Programming, 169, 01 2018.   \n[19] Tianyi Lin, Chi Jin, and Michael Jordan. On gradient descent ascent for nonconvex-concave minimax problems. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 6083\u20136093. PMLR, 13\u201318 Jul 2020.   \n[20] Mingrui Liu, Zhuoning Yuan, Yiming Ying, and Tianbao Yang. Stochastic auc maximization with deep neural networks. arXiv preprint arXiv:1908.10831, 2019.   \n[21] Luo Luo, Haishan Ye, Zhichao Huang, and Tong Zhang. Stochastic recursive gradient descent ascent for stochastic nonconvex-strongly-concave minimax problems. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 20566\u201320577. Curran Associates, Inc., 2020.   \n[22] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks, 2019.   \n[23] Gabriel Mancino-Ball and Yangyang Xu. Variance-reduced accelerated methods for decentralized stochastic double-regularized nonconvex strongly-concave minimax problems. $A r X i\\nu$ , abs/2307.07113, 2023.   \n[24] J.J. Moreau. Proximit\u00e9 et dualit\u00e9 dans un espace hilbertien. Bulletin de la Soci\u00e9t\u00e9 Math\u00e9matique de France, 93:273\u2013299, 1965.   \n[25] Abdellatif Moudaf.i A Regularization of DC Optimization. Pure and Applied Functional Analysis, 2022.   \n[26] Atsushi Nitanda and Taiji Suzuki. Stochastic Difference of Convex Algorithm and its Application to Training Deep Boltzmann Machines. In Aarti Singh and Jerry Zhu, editors, Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages 470\u2013478. PMLR, 20\u201322 Apr 2017.   \n[27] Sungho Park, Jewook Lee, Pilhyeon Lee, Sunhee Hwang, Dohyung Kim, and Hyeran Byun. Fair contrastive learning for facial attribute classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10389\u201310398, 2022.   \n[28] Hassan Rafique, Mingrui Liu, Qihang Lin, and Tianbao Yang. Non-convex min-max optimization: Provable algorithms and applications in machine learning. arXiv preprint arXiv:1810.02060, 2018.   \n[29] Hassan Rafique, Mingrui Liu, Qihang Lin, and Tianbao Yang. Weakly-convex\u2013concave min\u2013max optimization: provable algorithms and applications in machine learning. Optimization Methods and Software, 37(3):1087\u20131121, 2022.   \n[30] R.T. Rockafellar, M. Wets, and R.J.B. Wets. Variational Analysis. Grundlehren der mathematischen Wissenschaften. Springer Berlin Heidelberg, 2009.   \n[31] Aman Sinha, Hongseok Namkoong, Riccardo Volpi, and John Duchi. Certifying some distributional robustness with principled adversarial training, 2020.   \n[32] Kaizhao Sun and Xu Andy Sun. Algorithms for difference-of-convex programs based on difference-of-moreau-envelopes smoothing. INFORMS J. Optim., 5:321\u2013339, 2022.   \n[33] Pham Dinh Tao and El Bernoussi Souad. Algorithms for solving a class of nonconvex optimization problems. methods of subgradients. North-holland Mathematics Studies, 129:249\u2013271, 1986.   \n[34] Hoai An Le Thi, Hoai Minh Le, Duy Nhat Phan, and Bach Tran. Stochastic DCA for the large-sum of non-convex functions problem and its application to group variable selection in classification. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3394\u20133403. PMLR, 06\u201311 Aug 2017.   \n[35] Zhengyang Wang, Meng Liu, Youzhi Luo, Zhao Xu, Yaochen Xie, Limei Wang, Lei Cai, Qi Qi, Zhuoning Yuan, Tianbao Yang, and Shuiwang Ji. Advanced graph and sequence neural networks for molecular property prediction and drug discovery. Bioinformatics, 38(9):2579\u20132586, 02 2022.   \n[36] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. ArXiv, abs/1708.07747, 2017.   \n[37] Qizhe Xie, Zihang Dai, Yulun Du, Eduard H. Hovy, and Graham Neubig. Controllable invariance through adversarial feature learning. In Neural Information Processing Systems, 2017.   \n[38] Tengyu Xu, Zhe Wang, Yingbin Liang, and H. Vincent Poor. Enhanced first and zeroth order variance reduced algorithms for min-max optimization. ArXiv, abs/2006.09361, 2020.   \n[39] Yi Xu, Qi Qi, Qihang Lin, Rong Jin, and Tianbao Yang. Stochastic optimization for DC functions and non-smooth non-convex regularizers with non-asymptotic convergence. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 6942\u20136951. PMLR, 2019.   \n[40] Yan Yan, Yi Xu, Qihang Lin, Wei Liu, and Tianbao Yang. Optimal epoch stochastic gradient descent ascent methods for min-max optimization. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 5789\u20135800. Curran Associates, Inc., 2020.   \n[41] Yan Yan, Yi Xu, Qihang Lin, Wei Liu, and Tianbao Yang. Sharp analysis of epoch stochastic gradient descent ascent methods for min-max optimization. arXiv preprint arXiv:2002.05309, 2020.   \n[42] Junchi Yang, Xiang Li, and Niao He. Nest your adaptive algorithm for parameter-agnostic nonconvex minimax optimization. ArXiv, abs/2206.00743, 2022.   \n[43] Junchi Yang, Antonio Orvieto, Aurelien Lucchi, and Niao He. Faster single-loop algorithms for minimax optimization without strong concavity. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors, Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages 5485\u20135517. PMLR, 28\u201330 Mar 2022.   \n[44] Tianbao Yang and Yiming Ying. AUC maximization in the era of big data and AI: A survey. ACM Comput. Surv., 55(8):172:1\u2013172:37, 2023.   \n[45] Yao Yao, Qihang Lin, and Tianbao Yang. Large-scale optimization of partial auc in a range of false positive rates, 2022.   \n[46] Zhuoning Yuan, Yan Yan, Milan Sonka, and Tianbao Yang. Large-scale robust deep auc maximization: A new surrogate loss and empirical studies on medical image classification, 2021.   \n[47] Jiawei Zhang, Peijun Xiao, Ruoyu Sun, and Zhiquan Luo. A single-loop smoothed gradient descent-ascent algorithm for nonconvex-concave min-max problems. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 7377\u20137389. Curran Associates, Inc., 2020.   \n[48] Xuan Zhang, Necdet Serhat Aybat, and Mert Gurbuzbalaban. Sapd+: An accelerated stochastic method for nonconvex-concave minimax problems. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 21668\u201321681. Curran Associates, Inc., 2022.   \n[49] Xuan Zhang, Necdet Serhat Aybat, and Mert G\u00fcrb\u00fczbalaban. Sapd+: An accelerated stochastic method for nonconvex-concave minimax problems. In Neural Information Processing Systems, 2022.   \n[50] Xuan Zhang, Necdet Serhat Aybat, and Mert Gurbuzbalaban. Sapd+: An accelerated stochastic method for nonconvex-concave minimax problems, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[51] Renbo Zhao. A primal-dual smoothing framework for max-structured non-convex optimization, 2022. ", "page_idx": 13}, {"type": "text", "text": "[52] Dixian Zhu, Gang Li, Bokun Wang, Xiaodong Wu, and Tianbao Yang. When AUC meets DRO: Optimizing partial AUC for deep learning with non-convex convergence guarantee. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 27548\u201327573. PMLR, 17\u201323 Jul 2022. ", "page_idx": 13}, {"type": "text", "text": "A Convergence Analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Recall that $\\begin{array}{r}{\\mathbf{\\Psi}(x):=\\operatorname*{max}_{y\\in\\mathcal{Y}}\\phi(x,y),\\,\\Psi(x):=\\operatorname*{max}_{z\\in\\mathcal{Z}}\\psi(x,z),\\,y^{*}(\\cdot):=\\operatorname{arg\\,max}_{y\\in\\mathcal{Y}}\\phi(\\cdot,y)}\\end{array}$ , and $\\boldsymbol{z}^{*}(\\cdot):=\\arg\\operatorname*{max}_{\\boldsymbol{z}\\in\\mathcal{Z}}\\psi(\\cdot,\\boldsymbol{z})$ . Before presenting the proof of Theorem 4.5, we first give the proof of the proximal point estimation error bounds. As we have stated the bound for $\\|x_{\\phi}^{t+1}-\\mathrm{prox}_{\\gamma\\Phi}(x_{t})\\|^{2}$ in Lemma 4.4, here we present the corresponding lemma for $\\|x_{\\psi}^{t+1}-\\mathsf{p r o x}_{\\gamma\\Psi}(\\dot{x}_{t})\\|^{2}$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma A.1. Suppose that Assumption 4.1 holds, $0<\\gamma<1/\\delta_{\\psi}$ , and \u03b32(1/\u03b3\u2212\u03b4\u03c8). Then the sequences $\\{x_{t}\\},\\,\\{z_{t}\\},\\,\\{x_{\\psi}^{t}\\}$ and $\\{G_{t}\\}$ generated by Algorithm $^{\\,I}$ satisfy ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|x_{\\psi}^{t+1}-p r o x_{\\gamma\\Psi}(x_{t})\\|^{2}+\\mathbb{E}\\|z_{t+1}-z^{*}(p r o x_{\\gamma\\Psi}(x_{t}))\\|^{2}}\\\\ &{\\le(1-\\frac{\\eta_{1}\\left(1/\\gamma-\\delta_{\\psi}\\right)}{2})\\mathbb{E}\\|x_{\\psi}^{t}-p r o x_{\\gamma\\Psi}(x_{t-1})\\|^{2}+(1-\\eta_{1}\\mu_{\\psi})\\mathbb{E}\\|z_{t}-z^{*}(p r o x_{\\gamma\\Psi}(x_{t-1}))\\|^{2}}\\\\ &{\\quad+\\left(\\frac{2\\eta_{0}^{2}}{\\eta_{1}\\gamma^{2}(1/\\gamma-\\delta_{\\psi})^{3}}+\\frac{L_{\\psi,z x}^{2}\\eta_{0}^{2}}{\\eta_{1}\\mu_{\\psi}^{3}\\gamma^{2}(1/\\gamma-\\delta_{\\psi})^{2}}\\right)\\mathbb{E}\\|G_{t}\\|^{2}+12M^{2}\\eta_{1}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since Lemma 4.4 and Lemma A.1 share the same proof strategy, we only present the proof of Lemma 4.4. ", "page_idx": 14}, {"type": "text", "text": "A.1 Proof of Lemma 4.4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. Recall that $\\Phi(x)\\,=\\,\\operatorname*{max}_{y\\in\\mathcal{Y}}\\phi(x,y)$ and $\\boldsymbol{y}^{*}(\\cdot)\\,=\\,\\arg\\operatorname*{max}_{\\boldsymbol{y}\\in\\mathcal{Y}}\\boldsymbol{\\phi}(\\cdot,\\boldsymbol{y})$ . Observe from Assumption 4.1(i) that $\\Phi$ is $\\delta_{\\phi}$ -weakly convex. It then follows that $\\mathrm{prox}_{\\gamma\\Phi}(\\cdot)$ is $1/(1-\\gamma\\delta_{\\phi})$ -Lipschitz continuous. By this, Assumption 4.1(iii) and Lemma 4.2, it is not hard to see that $y^{*}(\\mathfrak{p r o x}_{\\gamma\\Phi}(\\cdot))$ is $L_{\\phi,y x}/(\\mu_{\\phi}(1-\\gamma\\delta_{\\phi}))$ -Lipschitz continuous. ", "page_idx": 14}, {"type": "text", "text": "For notational convenience, we let ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Phi_{t}(x,y)=\\phi(x,y)+\\frac{1}{2\\gamma}\\|x-x_{t}\\|^{2},}\\\\ {\\displaystyle x_{\\Phi,t}^{*}=\\mathrm{prox}_{\\gamma\\Phi}(x_{t}),\\quad y_{t}^{*}=y^{*}(\\mathrm{prox}_{\\gamma\\Phi}(x_{t})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In view of (10) and the update rule of $x_{\\phi}^{t+1}$ , one has ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}\\|x_{\\phi}^{t+1}-x_{\\Phi,t}^{*}\\|^{2}=\\mathbb{E}_{t}\\|x_{\\phi}^{t}-\\eta_{1}\\tilde{\\partial}_{x}\\Phi_{t}(x_{\\phi}^{t},y_{t})-x_{\\Phi,t}^{*}\\|^{2}}\\\\ &{\\,=\\|x_{\\phi}^{t}-x_{\\Phi,t}^{*}\\|^{2}-2\\mathbb{E}_{t}\\langle\\eta_{1}\\tilde{\\partial}_{x}\\Phi_{t}(x_{\\phi}^{t},y_{t}),x_{\\phi}^{t}-x_{\\Phi,t}^{*}\\rangle+\\mathbb{E}_{t}\\|\\eta_{1}\\tilde{\\partial}_{x}\\Phi_{t}(x_{\\phi}^{t},y_{t})\\|^{2}}\\\\ &{\\,\\le\\|x_{\\phi}^{t}-x_{\\Phi,t}^{*}\\|^{2}+2\\eta_{1}\\underbrace{\\langle\\partial_{x}\\Phi_{t}(x_{\\phi}^{t},y_{t}),x_{\\Phi,t}^{*}-x_{\\phi}^{t}\\rangle}_{(A)}+8M^{2}\\eta_{1}^{2}+\\frac{2\\eta_{1}^{2}}{\\gamma^{2}}\\|x_{\\phi}^{t}-x_{\\Phi,t}^{*}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we use the inequality ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}\\|\\tilde{\\partial}_{x}\\Phi_{t}(x_{\\phi}^{t},y_{t})\\|^{2}=\\mathbb{E}_{t}\\|\\tilde{\\partial}_{x}\\phi(x_{\\psi}^{t},y_{t})+\\frac{1}{\\gamma}(x_{\\phi}^{t}-x_{t})\\|^{2}}\\\\ &{\\quad=\\mathbb{E}_{t}\\|\\tilde{\\partial}_{x}\\phi(x_{\\phi}^{t},y_{t})+\\frac{1}{\\gamma}(x_{\\phi}^{t}-x_{t})-\\partial_{x}\\phi(x_{\\Phi,t}^{*},y_{t}^{*})-\\frac{1}{\\gamma}(x_{\\Phi,t}^{*}-x_{t})\\|^{2}}\\\\ &{\\quad\\leq4\\mathbb{E}_{t}\\|\\tilde{\\partial}\\phi(x_{\\phi}^{t},y_{t})\\|^{2}+4\\|\\partial_{x}\\phi(x_{\\Phi,t}^{*},y_{t}^{*})\\|^{2}+\\frac{2}{\\gamma^{2}}\\|x_{\\phi}^{t}-x_{\\Phi,t}^{*}\\|^{2}}\\\\ &{\\quad\\leq8M^{2}+\\frac{2}{\\gamma^{2}}\\|x_{\\phi}^{t}-x_{\\Phi,t}^{*}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By $(\\gamma^{-1}-\\delta_{\\phi})$ -strong convexity of $\\Phi_{t}(\\cdot,y)$ and the definition of $\\boldsymbol{x}_{\\Phi,t}^{*}$ in (10), one has ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\partial_{x}\\Phi_{t}(x_{\\phi}^{t},y_{t}),x_{\\Phi,t}^{*}-x_{\\phi}^{t}\\rangle\\leq\\Phi_{t}(x_{\\Phi,t}^{*},y_{t})-\\Phi_{t}(x_{\\phi}^{t},y_{t})-\\displaystyle\\frac{(1/\\gamma-\\delta_{\\phi})}{2}\\|x_{\\Phi,t}^{*}-x_{\\phi}^{t}\\|^{2},}\\\\ &{0\\leq\\Phi_{t}(x_{\\phi}^{t},y_{t}^{*})-\\Phi_{t}(x_{\\Phi,t}^{*},y_{t}^{*})-\\displaystyle\\frac{(1/\\gamma-\\delta_{\\phi})}{2}\\|x_{\\Phi,t}^{*}-x_{\\phi}^{t}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Summing up these two inequalities gives ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(A)\\leq\\Phi_{t}(x_{\\Phi,t}^{*},y_{t})-\\Phi_{t}(x_{\\phi}^{t},y_{t})+\\Phi_{t}(x_{\\phi}^{t},y_{t}^{*})-\\Phi_{t}(x_{\\Phi,t}^{*},y_{t}^{*})-(1/\\gamma-\\delta_{\\phi})\\|x_{\\Phi,t}^{*}-x_{\\phi}^{t}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Notice from the definition of $y_{t}^{*}$ in (10) that there exists a particular subgradient $\\nabla_{y}\\phi(x_{\\Phi,t}^{*},y_{t}^{*})$ such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y_{t}^{*}=P_{\\mathcal{Y}}\\bigl(y_{t}^{*}+\\eta_{1}\\nabla_{y}\\phi\\bigl(x_{\\Phi,t}^{*},y_{t}^{*}\\bigr)\\bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using this and the update rule of $y_{t+1}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}\\|y_{t+1}-y_{t}^{*}\\|^{2}=\\mathbb{E}_{t}\\|P_{y}(y_{t}+\\eta_{1}\\tilde{\\nabla}_{y}\\Phi(x_{\\phi}^{t},y_{t}))-y_{t}^{*}\\|^{2}}\\\\ &{=\\mathbb{E}_{t}\\|P_{y}(y_{t}+\\eta_{1}\\tilde{\\nabla}_{y}\\Phi_{t}(x_{\\phi}^{t},y_{t}))-P_{y}(y_{t}^{*}+\\eta_{1}\\nabla_{y}\\Phi_{t}(x_{\\Phi,t}^{*},y_{t}^{*}))\\|^{2}}\\\\ &{\\le\\mathbb{E}_{t}\\|y_{t}+\\eta_{1}\\tilde{\\nabla}_{y}\\Phi_{t}(x_{\\phi}^{t},y_{t})-(y_{t}^{*}+\\eta_{1}\\nabla_{y}\\Phi_{t}(x_{\\Phi,t}^{*},y_{t}^{*}))\\|^{2}}\\\\ &{\\le\\|y_{t}-y_{t}^{*}\\|^{2}+2\\eta_{1}\\langle\\nabla_{y}\\Phi_{t}(x_{\\phi}^{t},y_{t})-\\nabla_{y}\\Phi_{t}(x_{\\Phi,t}^{*},y_{t}^{*}),y_{t}-y_{t}^{*}\\rangle}\\\\ &{\\quad+\\eta_{1}^{2}\\mathbb{E}_{t}\\|\\tilde{\\nabla}_{y}\\Phi_{t}(x_{\\phi}^{t},y_{t})-\\nabla_{y}\\Phi_{t}(x_{\\Phi,t}^{*},y_{t}^{*})\\|^{2}}\\\\ &{\\le\\|y_{t}-y_{t}^{*}\\|^{2}+2\\eta_{1}\\underbrace{\\langle\\nabla_{y}\\Phi_{t}(x_{\\phi}^{t},y_{t})-\\nabla_{y}\\Phi_{t}(x_{\\Phi,t}^{*},y_{t}^{*}),y_{t}-y_{t}^{*}\\rangle}_{(B)}+4\\eta_{1}^{2}M^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By $\\mu_{\\phi}$ -strong concavity of $\\Phi_{t}(x,\\cdot)$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(B)=\\langle-\\nabla_{y}\\Phi_{t}(x_{\\phi}^{t},y_{t}),y_{t}^{*}-y_{t}\\rangle+\\langle-\\nabla_{y}\\Phi_{t}(x_{\\Phi,t}^{*},y_{t}^{*}),y_{t}-y_{t}^{*}\\rangle}\\\\ &{\\quad\\leq-\\Phi_{t}(x_{\\phi}^{t},y_{t}^{*})+\\Phi_{t}(x_{\\phi}^{t},y_{t})-\\frac{\\mu_{\\phi}}{2}\\|y_{t}^{*}-y_{t}\\|^{2}}\\\\ &{\\quad\\quad-\\Phi_{t}(x_{\\Phi,t}^{*},y_{t})+\\Phi_{t}(x_{\\Phi,t}^{*},y_{t}^{*})-\\frac{\\mu_{\\phi}}{2}\\|y_{t}^{*}-y_{t}\\|^{2}}\\\\ &{\\quad\\quad=-\\Phi_{t}(x_{\\phi}^{t},y_{t}^{*})+\\Phi_{t}(x_{\\phi}^{t},y_{t})-\\Phi_{t}(x_{\\Phi,t}^{*},y_{t})+\\Phi_{t}(x_{\\Phi,t}^{*},y_{t}^{*})-\\mu_{\\phi}\\|y_{t}^{*}-y_{t}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining (12) and (14) yields ", "page_idx": 15}, {"type": "equation", "text": "$$\n(A)+(B)\\leq-(1/\\gamma-\\delta_{\\phi})\\|x_{\\Phi,t}^{*}-x_{\\phi}^{t}\\|^{2}-\\mu_{\\phi}\\|y_{t}^{*}-y_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using this inequality, (11) and (13), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|x_{6}^{\\varepsilon+1}-x_{6}^{\\varepsilon}\\|^{2}+\\mathbb{E}\\|y_{6}+\\eta_{6}^{\\varepsilon}\\|y_{7}^{2}\\|^{2}}\\\\ &{\\leq(1-2\\eta_{1}(1/\\gamma-\\delta_{6}))(2\\eta_{7}^{2}/\\gamma)\\|x_{6}^{\\varepsilon}-x_{6}^{\\varepsilon}\\|^{2}+(1-2\\eta_{1}\\mu_{6})\\|y_{7}^{\\varepsilon}-y_{6}\\|^{2}+12M^{2}\\eta_{7}^{2}}\\\\ &{\\overset{(a)}{\\leq}(1-\\eta_{1}(1/\\gamma-\\delta_{6}))\\|x_{6}^{\\varepsilon}-x_{6}^{\\varepsilon}\\|^{2}+(1-2\\eta_{1}\\mu_{6})\\big\\|y_{7}^{\\varepsilon}-y_{6}\\|^{2}+12M^{2}\\eta_{7}^{2}}\\\\ &{\\overset{(b)}{\\leq}(1-\\eta_{1}(1/\\gamma-\\delta_{6}))\\bigg(\\left(1+\\frac{\\eta_{1}(1/\\gamma-\\delta_{6})}{2}\\right)\\|x_{6}^{\\varepsilon}-x_{6}^{\\varepsilon}\\|^{2}+\\bigg(1+\\frac{2}{\\eta_{1}(1/\\gamma-\\delta_{6})}\\bigg)\\|x_{6}^{\\varepsilon}-x_{6}^{2}\\bigg)}\\\\ &{\\quad+(1-2\\eta_{1}\\mu_{6})\\big((1+\\eta_{1}\\mu_{6})\\|y_{7}-y_{6}^{\\varepsilon}\\|^{2}+(1+(\\eta_{1}\\mu_{6})^{-1})\\|y_{7}^{\\varepsilon}-y_{6}^{\\varepsilon}\\|^{2}\\big)+12M^{2}\\eta_{7}^{2}}\\\\ &{\\overset{(c)}{\\leq}\\left(1-\\frac{\\eta_{1}(1/\\gamma-\\delta_{6})}{2}\\right)\\|x_{6}^{\\varepsilon}-x_{6}^{\\varepsilon}-1\\|^{2}+\\frac{2}{\\eta_{1}(1/\\gamma-\\delta_{6})}\\big[1\\varepsilon_{6}^{2}-x_{6}^{\\varepsilon}\\|^{2}}\\\\ &{\\quad+(1-\\eta_{1}\\mu_{6})\\|y_{7}-y_{6}^{\\varepsilon}\\|^{2}+ \n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $(a)$ follows from the assumption $\\begin{array}{r}{\\eta_{1}\\,\\le\\,\\frac{\\gamma^{2}(1/\\gamma-\\delta_{\\phi})}{2}}\\end{array}$ , $(b)$ uses the fact that $\\|a+b\\|^{2}\\leq(1+$ $\\begin{array}{r}{\\alpha)\\|a\\|^{2}+(1+\\frac{1}{\\alpha})\\|b\\|^{2}}\\end{array}$ for any $\\alpha>0$ , (c) follows from bounding the coefficient of each term from above, and $(d)$ uses $1\\big/(1-\\gamma\\delta_{\\phi})$ -Lipschitz continuity of $\\mathrm{prox}_{\\gamma\\Phi}(\\cdot)$ , $L_{\\phi,y x}/(\\mu_{\\phi}(1-\\gamma\\delta_{\\phi}))$ -Lipschitz continuity of $y^{*}(\\mathrm{prox}_{\\gamma\\Phi}(\\cdot))$ and the update rule of $x_{t}$ . ", "page_idx": 15}, {"type": "text", "text": "Finally taking expectation over all randomness yields the desired result. ", "page_idx": 15}, {"type": "text", "text": "A.2 Proof of Theorem 4.5 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We first present a detailed version of Theorem 4.5. ", "page_idx": 16}, {"type": "text", "text": "Theorem A.2. Consider Problem 1 and assume Assumption 4.1 holds. Suppose that the parameters $\\gamma,\\,\\eta_{0}$ and $\\eta_{1}$ in Algorithm 1 are chosen as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\tilde{\\Gamma}}<\\gamma<\\operatorname*{min}\\{\\delta_{\\phi}^{-1},\\delta_{\\psi}^{-1}\\},\\quad\\alpha=\\operatorname*{min}\\left\\{\\displaystyle\\frac{1/\\gamma-\\delta_{\\phi}}{4},\\displaystyle\\frac{1/\\gamma-\\delta_{\\psi}}{4},\\displaystyle\\mu_{\\phi},\\mu_{\\psi}\\right\\},}\\\\ &{\\mathbb{\\tilde{\\Gamma}}=\\operatorname*{min}\\left\\{\\displaystyle\\frac{\\gamma^{2}\\alpha^{2}}{4},\\displaystyle\\frac{\\mu_{\\phi}^{1.5}\\gamma^{2}\\alpha^{1.5}}{4L_{\\phi,y x}},\\displaystyle\\frac{\\mu_{\\psi}^{1.5}\\gamma^{2}\\alpha^{1.5}}{4L_{\\psi,z x}}\\right\\},\\quad\\nu=\\operatorname*{min}\\left\\{1,\\displaystyle\\frac{2\\tau}{\\gamma^{2}\\alpha}\\right\\},\\,L_{F}=\\displaystyle\\frac{2}{\\gamma-\\gamma^{2}\\operatorname*{min}\\{\\delta_{\\psi},\\delta_{\\phi}\\}},}\\\\ &{\\mathbb{\\tilde{\\Gamma}}=\\operatorname*{min}\\left\\{\\displaystyle\\frac{\\gamma^{2}(1/\\gamma-\\delta_{\\phi})}{2},\\displaystyle\\frac{\\gamma^{2}(1/\\gamma-\\delta_{\\psi})}{2},\\displaystyle\\frac{1}{2L_{F}\\tau},\\displaystyle\\frac{\\operatorname*{min}\\{1,\\gamma^{2}\\}\\operatorname*{min}\\{\\alpha,\\tau\\}\\nu\\alpha}{768\\tau M^{2}}\\epsilon^{2}\\right\\},\\quad\\eta_{0}=\\tau\\eta_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}(\\mathbb{E}\\|x_{\\phi}^{t+1}-p r o x_{\\gamma\\Phi}(x_{t})\\|^{2}+\\mathbb{E}\\|x_{\\psi}^{t+1}-p r o x_{\\gamma\\Psi}(x_{t})\\|^{2}+\\mathbb{E}\\|\\nabla F_{\\gamma}(x_{t})\\|^{2})\\leq\\operatorname*{min}\\{1,\\gamma^{-2}\\}\\frac{\\epsilon^{2}}{4},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and consequently $x_{\\phi}^{\\bar{t}}$ and $x_{\\psi}^{\\bar{t}}$ are both nearly $\\epsilon$ -critical points of problem (1), whenever ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Gamma\\geq\\frac{16(F_{\\gamma}(x_{0})-F_{\\gamma}^{*}+P_{0})}{\\operatorname*{min}\\{1,\\gamma^{-2}\\}\\operatorname*{min}\\{\\alpha,\\tau\\}\\nu\\epsilon^{2}}\\operatorname*{max}\\left\\{\\frac{2}{\\gamma^{2}(1/\\gamma-\\delta_{\\phi})},\\frac{2}{\\gamma^{2}(1/\\gamma-\\delta_{\\psi})},2L_{F}\\tau,\\frac{768\\tau M^{2}}{\\operatorname*{min}\\{1,\\gamma^{2}\\}\\operatorname*{min}\\{\\alpha,\\tau\\}},2L_{F}\\tau,\\frac{2}{\\operatorname*{max}\\{1,\\gamma^{2}\\}\\operatorname*{min}\\{1,\\gamma\\epsilon^{2}\\}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with ", "page_idx": 16}, {"type": "equation", "text": "$$\nP_{0}=\\frac{2\\eta_{0}}{\\eta_{1}\\gamma^{2}\\alpha}\\left(\\mathbb{E}\\|x_{\\phi}^{1}-p r o x_{\\gamma\\Phi}(x_{0})\\|^{2}+\\mathbb{E}\\|y_{1}-y_{0}^{*}\\|^{2}+\\mathbb{E}\\|x_{\\psi}^{1}-p r o x_{\\gamma\\Psi}(x_{0})\\|^{2}+\\mathbb{E}\\|z_{1}-z_{0}^{*}\\|^{2}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. For notational convenience, let ", "page_idx": 16}, {"type": "equation", "text": "$$\nx_{\\Psi,t}^{*}=\\mathrm{prox}_{\\gamma\\Psi}(x_{t}),\\quad z_{t}^{*}=\\arg\\operatorname*{max}_{z\\in\\mathcal{Z}}\\psi(x_{\\Psi,t}^{*},z).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From Proposition 3.3, we know that $F_{\\gamma}(\\cdot)$ is $L_{F}$ -smooth. By this, $\\begin{array}{r}{0<\\eta_{0}\\leq\\frac{1}{2L_{F}}}\\end{array}$ , and Lemma 4.3, one has ", "page_idx": 16}, {"type": "equation", "text": "$$\nF_{\\gamma}(x_{t+1})\\leq F_{\\gamma}(x_{t})+\\frac{\\eta_{0}}{2}\\|\\nabla F_{\\gamma}(x_{t})-G_{t+1}\\|^{2}-\\frac{\\eta_{0}}{2}\\|\\nabla F_{\\gamma}(x_{t})\\|^{2}-\\frac{\\eta_{0}}{4}\\|G_{t+1}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Notice that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla F_{\\gamma}(x_{t})=\\gamma^{-1}(\\mathrm{prox}_{\\gamma\\Psi}(x_{t})-x_{t}+x_{t}-\\mathrm{prox}_{\\gamma\\Phi}(x_{t}))=\\gamma^{-1}(\\mathrm{prox}_{\\gamma\\Psi}(x_{t})-\\mathrm{prox}_{\\gamma\\Phi}(x_{t})),}\\\\ &{G_{t+1}=\\gamma^{-1}(x_{\\psi}^{t+1}-x_{\\phi}^{t+1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using these, (10) and (16), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla F_{\\gamma}(x_{t})-G_{t+1}\\|^{2}=\\|\\gamma^{-1}(\\mathrm{prox}_{\\gamma\\Psi}(x_{t})-\\mathrm{prox}_{\\gamma\\Phi}(x_{t}))-\\gamma^{-1}(x_{\\psi}^{t+1}-x_{\\phi}^{t+1})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\|\\gamma^{-1}(x_{\\Psi,t}^{*}-x_{\\Phi,t}^{*})-\\gamma^{-1}(x_{\\psi}^{t+1}-x_{\\phi}^{t+1})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\gamma^{-2}\\left(\\|x_{\\Psi,t}^{*}-x_{\\psi}^{t+1}\\|^{2}+\\|x_{\\Phi,t}^{*}-x_{\\phi}^{t+1}\\|^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It follows from this and (17) that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[F_{\\gamma}(x_{t+1})]\\le\\mathbb{E}[F_{\\gamma}(x_{t})]+\\frac{\\eta_{0}}{\\gamma^{2}}\\mathbb{E}\\|x_{\\psi}^{t+1}-x_{\\Psi,t}^{*}\\|^{2}+\\frac{\\eta_{0}}{\\gamma^{2}}\\mathbb{E}\\|x_{\\phi}^{t+1}-x_{\\Phi,t}^{*}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad-\\left.\\frac{\\eta_{0}}{2}\\mathbb{E}\\|\\nabla F_{\\gamma}(x_{t})\\|^{2}-\\frac{\\eta_{0}}{4}\\mathbb{E}\\|G_{t+1}\\|^{2}.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let $\\boldsymbol{x}_{\\Phi,t}^{*}$ and $y_{t}^{*}$ be defined in (10). Invoking Lemma 4.4, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|x_{\\phi}^{t+2}-x_{\\Phi,t+1}^{*}\\|^{2}+\\mathbb{E}\\|y_{t+2}-y_{t+1}^{*}\\|^{2}}\\\\ &{\\leq\\left(1-\\frac{\\eta_{1}\\left(1/\\gamma-\\delta_{\\phi}\\right)}{2}\\right)\\mathbb{E}\\|x_{\\phi}^{t+1}-x_{\\Phi,t}^{*}\\|^{2}+(1-\\eta_{1}\\mu_{\\phi})\\mathbb{E}\\|y_{t+1}-y_{t}^{*}\\|^{2}}\\\\ &{\\quad+\\left(\\frac{2\\eta_{0}^{2}}{\\eta_{1}\\gamma^{2}(1/\\gamma-\\delta_{\\phi})^{3}}+\\frac{L_{\\phi,y x}^{2}\\eta_{0}^{2}}{\\eta_{1}\\mu_{\\phi}^{3}\\gamma^{2}(1/\\gamma-\\delta_{\\phi})^{2}}\\right)\\mathbb{E}\\|G_{t+1}\\|^{2}+12M^{2}\\eta_{1}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Recall that $\\boldsymbol{x}_{\\Psi,t}^{*}$ and $z_{t}^{*}$ are defined in (16). By Lemma A.1, one has ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|x_{\\psi}^{t+2}-x_{\\Psi,t+1}^{*}\\|^{2}+\\mathbb{E}\\|z_{t+2}-z_{t+1}^{*}\\|^{2}}\\\\ &{\\leq\\left(1-\\frac{\\eta_{1}\\left(1/\\gamma-\\delta_{\\psi}\\right)}{2}\\right)\\mathbb{E}\\|x_{\\psi}^{t+1}-x_{\\Psi,t}^{*}\\|^{2}+(1-\\eta_{1}\\mu_{\\psi})\\mathbb{E}\\|z_{t+1}-z_{t}^{*}\\|^{2}}\\\\ &{\\quad+\\left(\\frac{2\\eta_{0}^{2}}{\\eta_{1}\\gamma^{2}(1/\\gamma-\\delta_{\\psi})^{3}}+\\frac{L_{\\psi,z x}^{2}\\eta_{0}^{2}}{\\eta_{1}\\mu_{\\psi}^{3}\\gamma^{2}(1/\\gamma-\\delta_{\\psi})^{2}}\\right)\\mathbb{E}\\|G_{t+1}\\|^{2}+12M^{2}\\eta_{1}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\alpha$ be given in the statement of this theorem. Using this and the last two inequalities above, we have ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|x_{\\phi}^{t+2}-x_{\\Phi,t+1}^{*}\\|^{2}+\\mathbb{E}\\|y_{t+2}-y_{t+1}^{*}\\|^{2}}\\\\ &{\\leq(1-\\alpha\\eta_{1})\\big(\\mathbb{E}\\|x_{\\phi}^{t+1}-x_{\\Phi,t}^{*}\\|^{2}+\\mathbb{E}\\|y_{t+1}-y_{t}^{*}\\|^{2}\\big)}\\\\ &{\\quad+\\left(\\frac{2\\eta_{0}^{2}}{\\eta_{1}\\gamma^{2}(1/\\gamma-\\delta_{\\phi})^{3}}+\\frac{L_{\\phi,y x}^{2}\\eta_{0}^{2}}{\\eta_{1}\\mu_{\\phi}^{3}\\gamma^{2}(1/\\gamma-\\delta_{\\phi})^{2}}\\right)\\mathbb{E}\\|G_{t+1}\\|^{2}+12M^{2}\\eta_{1}^{2},}\\\\ &{\\mathbb{E}\\|x_{\\psi}^{t+2}-x_{\\Psi,t+1}^{*}\\|^{2}+\\mathbb{E}\\|z_{t+2}-z_{t+1}^{*}\\|^{2}}\\\\ &{\\leq(1-\\alpha\\eta_{1})\\big(\\mathbb{E}\\|x_{\\psi}^{t+1}-x_{\\Psi,t}^{*}\\|^{2}+\\mathbb{E}\\|z_{t+1}-z_{t}^{*}\\|^{2}\\big)}\\\\ &{\\quad+\\left(\\frac{2\\eta_{0}^{2}}{\\eta_{1}\\gamma^{2}(1/\\gamma-\\delta_{\\psi})^{3}}+\\frac{L_{\\psi,z}^{2}\\eta_{0}^{2}}{\\eta_{1}\\mu_{\\psi}^{3}\\gamma^{2}(1/\\gamma-\\delta_{\\psi})^{2}}\\right)\\mathbb{E}\\|G_{t+1}\\|^{2}+12M^{2}\\eta_{1}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Summing up inequalities (19), $(20)\\!\\times\\!\\frac{2\\eta_{0}}{\\eta_{1}\\gamma^{2}\\alpha}$ and $(21)\\!\\times\\!\\frac{2\\eta_{0}}{\\eta_{1}\\gamma^{2}\\alpha}$ \u03b71\u03b320\u03b1 yields ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[F_{r}(x_{t+1})]+\\frac{2\\eta_{0}}{\\eta_{1}\\gamma^{2}\\alpha}\\left(\\mathbb{E}\\|x_{0}^{\\varepsilon+2}-x_{\\varepsilon,t+1}^{*}\\|^{2}+\\mathbb{E}\\|y_{t+2}-y_{t+1}^{*}\\|^{2}\\right)}\\\\ &{\\quad+\\frac{2\\eta_{0}}{\\eta_{1}\\gamma^{2}\\alpha}\\bigg(\\mathbb{E}\\|x_{0}^{\\varepsilon+2}-x_{\\varepsilon,t+1}^{*}\\|^{2}+\\mathbb{E}\\|z_{t+2}-z_{t+1}^{*}\\|^{2}\\bigg)}\\\\ &{\\leq\\mathbb{E}[F_{r}(x_{t})]+\\frac{2\\eta_{0}}{\\eta_{1}\\gamma^{2}\\alpha}\\left(1-\\frac{\\eta_{1}\\alpha}{2}\\right)\\left(\\mathbb{E}\\|x_{0}^{\\varepsilon+1}-x_{\\varepsilon,t}^{*}\\|^{2}+\\mathbb{E}\\|y_{t+1}-y_{t}^{*}\\|^{2}\\right)}\\\\ &{\\quad+\\frac{2\\eta_{0}}{\\eta_{1}\\gamma^{2}\\alpha}\\left(1-\\frac{\\eta_{1}\\alpha}{2}\\right)\\left(\\mathbb{E}\\|x_{0}^{\\varepsilon+1}-x_{\\varepsilon,t}^{*}\\|^{2}+\\mathbb{E}\\|y_{t+1}-y_{t}^{*}\\|^{2}\\right)}\\\\ &{\\quad+\\bigg(\\frac{4\\eta_{0}}{\\eta_{1}\\gamma^{2}\\alpha}+\\frac{4\\eta_{0}^{3}}{\\eta_{1}\\gamma^{3}\\alpha}+\\frac{4\\eta_{0}^{3}}{\\eta_{1}\\gamma^{2}\\alpha}(\\mathbb{A}|\\gamma_{t}-\\delta_{\\theta}|^{3}+\\frac{2L_{\\theta,x x}^{2}\\eta_{0}^{3}}{\\eta_{1}\\beta_{0}^{3}\\gamma^{4}\\alpha(1/\\gamma-\\delta_{\\theta})^{2}}}\\\\ &{\\quad+\\frac{2L_{\\theta,x x}^{2}\\eta_{0}^{3}}{\\eta_{1}\\beta_{0}^{3}\\gamma^{4}\\alpha(1/\\gamma-\\delta_{\\theta})}-\\frac{\\eta_{0}}{\\eta_{1}\\gamma}\\mathbb{E}\\|z_{t+1}\\|^{2}}\\\\ &{\\quad-\\frac{\\eta_{0} \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We now introduce a potential function ", "page_idx": 17}, {"type": "equation", "text": "$$\nP_{t}=\\frac{2\\eta_{0}}{\\eta_{1}\\gamma^{2}\\alpha}\\Bigg(\\mathbb{E}\\|x_{\\phi}^{t+1}-x_{\\Phi,t}^{*}\\|^{2}+\\mathbb{E}\\|y_{t+1}-y_{t}^{*}\\|^{2}+\\mathbb{E}\\|x_{\\psi}^{t+1}-x_{\\Psi,t}^{*}\\|^{2}+\\mathbb{E}\\|z_{t+1}-z_{t}^{*}\\|^{2}\\Bigg),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and rewrite inequality (22) as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[F_{\\gamma}(x_{t+1})]+P_{t+1}}\\\\ &{\\;\\leq\\mathbb{E}[F_{\\gamma}(x_{t})]+(1-\\beta)P_{t}-\\beta\\mathbb{E}\\|\\nabla F_{\\gamma}(x_{t})\\|^{2}+\\frac{48\\eta_{0}\\eta_{1}M^{2}}{\\gamma^{2}\\alpha}}\\\\ &{\\quad+\\left(\\frac{\\eta_{0}^{3}}{\\eta_{1}^{2}\\gamma^{4}\\alpha^{4}}+\\frac{L_{\\phi,y x}^{2}\\eta_{0}^{3}}{\\eta_{1}^{2}\\mu_{\\phi}^{3}\\gamma^{4}\\alpha^{3}}+\\frac{L_{\\psi,z x}^{2}\\eta_{0}^{3}}{\\eta_{1}^{2}\\mu_{\\psi}^{3}\\gamma^{4}\\alpha^{3}}-\\frac{\\eta_{0}}{4}\\right)\\mathbb{E}\\|G_{t+1}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\beta=\\operatorname*{min}\\left\\{\\frac{\\eta_{1}\\alpha}{2},\\frac{\\eta_{0}}{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This inequality, together with the choice of $\\eta_{0}$ and $\\tau$ specified in this theorm, yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{E[F_{\\gamma}(x_{t+1})]+P_{t+1}\\leq\\mathbb{E}[F_{\\gamma}(x_{t})]+(1-\\beta)P_{t}-\\beta\\mathbb{E}\\|\\nabla F_{\\gamma}(x_{t})\\|^{2}+\\frac{48\\eta_{0}\\eta_{1}M^{2}}{\\gamma^{2}\\alpha}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Taking average of these inequalities over $t=0,\\dots,T-1$ yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}(P_{t}+\\mathbb{E}\\|\\nabla F_{\\gamma}(x_{t})\\|^{2})\\leq\\frac{F_{\\gamma}(x_{0})-F_{\\gamma}^{*}+P_{0}}{\\beta T}+\\frac{48\\eta_{0}\\eta_{1}M^{2}}{\\beta\\gamma^{2}\\alpha},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we use $F_{\\gamma}^{*}\\leq F_{\\gamma}(x_{T})$ due to Assumption 4.1(iii). Recall that $\\eta_{0}=\\tau\\eta_{1}$ and $\\begin{array}{r}{\\nu=\\operatorname*{min}\\{1,\\frac{2\\tau}{\\gamma^{2}\\alpha}\\}}\\end{array}$ . Using these, (23) and (25), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{T}\\sum_{t=0}^{T-1}(\\mathbb{E}\\|x_{\\phi}^{t+1}-x_{\\Phi,t}^{*}\\|^{2}+\\mathbb{E}\\|x_{\\psi}^{t+1}-x_{\\Psi,t}^{*}\\|^{2}+\\mathbb{E}\\|\\nabla F_{\\gamma}(x_{t})\\|^{2})}\\\\ &{\\displaystyle\\leq\\frac{1}{\\nu T}\\sum_{t=0}^{T-1}(P_{t}+\\mathbb{E}\\|\\nabla F_{\\gamma}(x_{t})\\|^{2})\\leq\\frac{F_{\\gamma}(x_{0})-F_{\\gamma}^{*}+P_{0}}{\\nu\\beta T}+\\frac{48\\eta_{0}\\eta_{1}M^{2}}{\\nu\\beta\\gamma^{2}\\alpha}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By (24) and the choice of $\\alpha,\\eta_{0}$ and $\\eta_{1}$ specified in this theorem, one has ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\sin\\{1,\\gamma^{-2}\\}\\nu\\beta\\gamma^{2}\\alpha\\epsilon^{2}}{384\\eta_{0}M^{2}}=\\frac{\\operatorname*{min}\\{1,\\gamma^{2}\\}\\operatorname*{min}\\left\\{\\frac{\\eta_{1}\\alpha}{2},\\frac{\\eta_{1}\\tau}{2}\\right\\}\\nu\\alpha\\epsilon^{2}}{384\\eta_{1}\\tau M^{2}}=\\frac{\\operatorname*{min}\\{1,\\gamma^{2}\\}\\operatorname*{min}\\left\\{\\alpha,\\tau\\right\\}\\nu\\alpha}{768\\tau M^{2}}\\epsilon^{2}\\geq\\eta_{1},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which implies that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{48\\eta_{0}\\eta_{1}M^{2}}{\\nu\\beta\\gamma^{2}\\alpha}\\leq\\operatorname*{min}\\{1,\\gamma^{-2}\\}\\frac{\\epsilon^{2}}{8}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Suppose that $T$ satisfies (15). It then follows from (24), $\\eta_{0}=\\tau\\eta_{1}$ , and the expression of $\\eta_{1}$ that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{~\\,\\,\\,\\,}\\geq\\frac{16(F_{\\gamma}(x_{0})-F_{\\gamma}^{*}+P_{0})}{\\operatorname*{min}\\{1,\\gamma^{-2}\\}\\operatorname*{min}\\{\\alpha,\\tau\\}\\nu\\epsilon^{2}}\\operatorname*{max}\\left\\{\\frac{2}{\\gamma^{2}(1/\\gamma-\\delta_{\\phi})},\\frac{2}{\\gamma^{2}(1/\\gamma-\\delta_{\\psi})},2L_{F}\\tau,\\frac{768\\tau M^{2}}{\\operatorname*{min}\\{1,\\gamma^{2}\\}\\operatorname*{min}\\{\\alpha,\\tau\\}},\\frac{2}{\\gamma^{2}(1/\\gamma-\\delta_{\\phi})}\\right\\}}\\\\ &{\\,\\,\\,\\,=\\frac{8(F_{\\gamma}(x_{0})-F_{\\gamma}^{*}+P_{0})}{\\operatorname*{min}\\{1,\\gamma^{-2}\\}\\nu\\beta\\epsilon^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which implies that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{F_{\\gamma}(x_{0})-F_{\\gamma}^{*}+P_{0}}{\\nu\\beta T}\\leq\\operatorname*{min}\\{1,\\gamma^{-2}\\}\\frac{\\epsilon^{2}}{8}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, for any $T$ satisfying (15), one has ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}(\\mathbb{E}\\|x_{\\phi}^{t+1}-x_{\\Phi,t}^{*}\\|^{2}+\\mathbb{E}\\|x_{\\psi}^{t+1}-x_{\\Psi,t}^{*}\\|^{2}+\\mathbb{E}\\|\\nabla F_{\\gamma}(x_{t})\\|^{2})\\le\\operatorname*{min}\\{1,\\gamma^{-2}\\}\\frac{\\epsilon^{2}}{4},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which together with $\\boldsymbol{x}_{\\Phi,t}^{*}=\\mathfrak{p r o x}_{\\gamma\\Phi}(\\boldsymbol{x}_{t})$ and $x_{\\Psi,t}^{*}=\\mathfrak{p r o x}_{\\gamma\\Psi}(x_{t})$ yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}(\\mathbb{E}\\|x_{\\phi}^{t+1}-\\mathrm{{prox}}_{\\gamma\\Phi}(x_{t})\\|^{2}+\\mathbb{E}\\|x_{\\psi}^{t+1}-{\\mathrm{prox}}_{\\gamma\\Psi}(x_{t})\\|^{2}+\\mathbb{E}\\|\\nabla F_{\\gamma}(x_{t})\\|^{2})\\leq\\operatorname*{min}\\{1,\\gamma^{-2}\\}\\frac{\\epsilon^{2}}{4}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $\\bar{t}$ is uniformly sampled from $\\{1,\\ldots,T\\}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|x_{\\phi}^{\\bar{t}}-\\mathrm{prox}_{\\gamma\\Phi}(x_{\\bar{t}-1})\\|^{2}+\\|x_{\\psi}^{\\bar{t}}-\\mathrm{prox}_{\\gamma\\Psi}(x_{\\bar{t}-1})\\|^{2}+\\|\\nabla F_{\\gamma}(x_{\\bar{t}-1})\\|^{2}]\\leq\\operatorname*{min}\\{1,\\gamma^{-2}\\}\\frac{\\epsilon^{2}}{4}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It then follows from Lemma 3.4 that $x_{\\phi}^{\\bar{t}}$ and $x_{\\psi}^{\\bar{t}}$ are both nearly $\\epsilon$ -critical points of problem (1). ", "page_idx": 18}, {"type": "text", "text": "A.3 Proof of Corollary 4.7 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We present a detailed version of Corollary 4.7 ", "page_idx": 19}, {"type": "text", "text": "Corollary A.3. Consider Problem 2 and assume Assumption 4.6 holds. Suppose that the parameters $\\gamma,\\,\\eta_{0}$ and $\\eta_{1}$ in Algorithm 2 are chosen as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{1}<\\gamma<\\operatorname*{min}\\{\\delta_{\\phi}^{-1},\\delta_{\\psi}^{-1}\\},\\quad\\alpha=\\operatorname*{min}\\left\\{\\displaystyle\\frac{1/\\gamma-\\delta_{\\phi}}{2},\\displaystyle\\frac{1/\\gamma-\\delta_{\\psi}}{2}\\right\\},\\quad\\tau=\\displaystyle\\frac{\\gamma^{2}\\alpha^{2}}{4},\\quad\\nu=\\operatorname*{min}\\left\\{1,\\displaystyle\\frac{2\\tau}{\\gamma^{2}\\alpha}\\right\\}}\\\\ &{\\mathbb{1}=\\operatorname*{min}\\left\\{\\displaystyle\\frac{\\gamma^{2}(1/\\gamma-\\delta_{\\phi})}{2},\\displaystyle\\frac{\\gamma^{2}(1/\\gamma-\\delta_{\\psi})}{2},\\displaystyle\\frac{1}{2L_{F}\\tau},\\displaystyle\\frac{\\operatorname*{min}\\{1,\\gamma^{2}\\}\\operatorname*{min}\\left\\{\\alpha,\\tau\\right\\}\\nu\\alpha}{768\\tau M^{2}}\\epsilon^{2}\\right\\},\\quad\\eta_{0}=\\tau\\eta_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}(\\mathbb{E}\\|x_{\\phi}^{t+1}-p r o x_{\\gamma\\phi}(x_{t})\\|^{2}+\\mathbb{E}\\|x_{\\psi}^{t+1}-p r o x_{\\gamma\\psi}(x_{t})\\|^{2}+\\mathbb{E}\\|\\nabla F_{\\gamma}(x_{t})\\|^{2})\\le\\operatorname*{min}\\{1,\\gamma^{-2}\\}\\frac{\\epsilon^{2}}{4},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and consequently $x_{\\phi}^{\\bar{t}}$ and $x_{\\psi}^{\\bar{t}}$ are both nearly $\\epsilon$ -critical points of problem (2), whenever ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Gamma\\geq\\frac{16(F_{\\gamma}(x_{0})-F_{\\gamma}^{*}+P_{0})}{\\operatorname*{min}\\{1,\\gamma^{-2}\\}\\operatorname*{min}\\{\\alpha,\\tau\\}\\nu\\epsilon^{2}}\\operatorname*{max}\\left\\{\\frac{2}{\\gamma^{2}(1/\\gamma-\\delta_{\\phi})},\\frac{2}{\\gamma^{2}(1/\\gamma-\\delta_{\\psi})},2L_{F}\\tau,\\frac{768\\tau M^{2}}{\\operatorname*{min}\\{1,\\gamma^{2}\\}\\operatorname*{min}\\{\\alpha,\\tau\\}},2L_{F}\\tau,\\frac{2}{\\operatorname*{max}\\{1,\\gamma^{2}\\}\\operatorname*{min}\\{1,\\gamma\\epsilon^{2}\\}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with ", "page_idx": 19}, {"type": "equation", "text": "$$\nP_{0}=\\frac{2\\tau}{\\gamma^{2}\\alpha}\\left(\\mathbb{E}\\|x_{\\phi}^{1}-p r o x_{\\gamma\\phi}(x_{0})\\|^{2}+\\mathbb{E}\\|x_{\\psi}^{1}-p r o x_{\\gamma\\psi}(x_{0})\\|^{2}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since problem (2) and Algorithm 2 are special cases of problem (1) and Algorithm 1 respectively, Corollary A.3 directly follows from Theorem A.2. ", "page_idx": 19}, {"type": "text", "text": "A.4 Proof of Corollary 4.9 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We present a detailed version of Corollary 4.9 ", "page_idx": 19}, {"type": "text", "text": "Corollary A.4. Consider Problem 3 and assume Assumption 4.8 holds. Suppose that the parameters $\\gamma,\\,\\eta_{0}$ and $\\eta_{1}$ in Algorithm 3 are chosen as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1<\\gamma<\\delta_{\\phi}^{-1},\\quad\\alpha=\\operatorname*{min}\\left\\{\\displaystyle\\frac{1/\\gamma-\\delta_{\\phi}}{2},\\mu_{\\phi}\\right\\},\\quad\\tau=\\operatorname*{min}\\left\\{\\displaystyle\\frac{\\gamma^{2}\\alpha^{2}}{4},\\displaystyle\\frac{\\mu_{\\phi}^{1.5}\\gamma^{2}\\alpha^{1.5}}{4L_{\\phi,y x}}\\right\\},\\quad\\nu=\\operatorname*{min}\\left\\{1,\\displaystyle\\frac{2\\tau}{\\gamma^{2}\\alpha^{2}},\\displaystyle\\frac{\\mu_{\\phi}^{1.5}\\gamma^{2}\\alpha^{1.5}}{4L_{\\phi,y x}}\\right\\},}\\\\ &{\\displaystyle\\kappa=\\operatorname*{min}\\left\\{\\displaystyle\\frac{\\gamma^{2}(1/\\gamma-\\delta_{\\phi})}{2},\\displaystyle\\frac{1}{2L_{F}\\tau},\\displaystyle\\frac{\\operatorname*{min}\\left\\{1,\\gamma^{2}\\right\\}\\operatorname*{min}\\left\\{\\alpha,\\tau\\right\\}\\nu\\alpha}{384\\tau M^{2}}\\epsilon^{2}\\right\\},\\quad\\eta_{0}=\\tau\\eta_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}(\\mathbb{E}\\|x_{\\phi}^{t+1}-p r o x_{\\gamma F}(x_{t})\\|^{2}+\\mathbb{E}\\|\\nabla F_{\\gamma}(x_{t})\\|^{2})\\leq\\operatorname*{min}\\{1,\\gamma^{-2}\\}\\frac{\\epsilon^{2}}{4},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and consequently $x_{\\bar{t}}$ is a nearly $\\epsilon$ -critical point of problem (3), whenever ", "page_idx": 19}, {"type": "equation", "text": "$$\nT\\geq\\frac{16(F_{\\gamma}(x_{0})-F_{\\gamma}^{*}+P_{0})}{\\operatorname*{min}\\{1,\\gamma^{-2}\\}\\operatorname*{min}\\{\\alpha,\\tau\\}\\nu\\epsilon^{2}}\\operatorname*{max}\\left\\{\\frac{2}{\\gamma^{2}(1/\\gamma-\\delta_{\\phi})},2L_{F}\\tau,\\frac{384\\tau M^{2}}{\\operatorname*{min}\\{1,\\gamma^{2}\\}\\operatorname*{min}\\{\\alpha,\\tau\\}\\nu\\alpha\\epsilon^{2}}\\right\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with ", "page_idx": 19}, {"type": "equation", "text": "$$\nP_{0}=\\frac{2\\eta_{0}}{\\eta_{1}\\gamma^{2}\\alpha}\\left(\\mathbb{E}\\|x_{\\phi}^{1}-p r o x_{\\gamma F}(x_{0})\\|^{2}+\\mathbb{E}\\|y_{1}-y_{0}^{*}\\|^{2}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. This proof is similar to that of Theorem A.2 except that the inequality (18) is replaced by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla F_{\\gamma}(x_{t})-G_{t+1}\\|^{2}=\\bigg\\|\\frac{1}{\\gamma}(x_{t}-\\mathrm{prox}_{\\gamma F}(x_{t}))-\\frac{1}{\\gamma}(x_{t}-x_{\\phi}^{t+1})\\bigg\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{1}{\\gamma^{2}}\\|\\mathrm{prox}_{\\gamma F}(x_{t})-x_{\\phi}^{t+1}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B More Experimental Results ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "NhtBXSNXKA/tmp/eb18de493ab83b7a23810965d6543359c5382dd056b584a37d6b0f5a1e644bfe.jpg", "img_caption": ["Figure 2: Ablation Study of SMAG for PU Learning "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 4: Mean $\\pm$ std of fairness results on CelebA test dataset with Bags Under Eyes task labels, and Male sensitive attribute. Results are reported on 3 independent runs. We use bold font to denote the best result and use underline to denote the second best. ", "page_idx": 20}, {"type": "table", "img_path": "NhtBXSNXKA/tmp/ec6b30fb91fc2f7971fdf9c21613f8b2a40ae5f4afe7f2e5fc7a9b6f02254959.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 21}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 21}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 21}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 21}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 21}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 21}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We have detailed explanation on all items described in the abstract presented in the paper. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We discussed the limitation of this in the conclusion section. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Assumptions are presented in the main paper. The detailed theorem statements and proofs are presented in the appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide all the information needed to reproduce the experimental results in the application section. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The code is included in the supplemental material. The data we used are public datasets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 23}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provided all the implementation details in the application section. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We ran multiple trails for each experiment setting and present the error bars in the plot or table. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Yes. Compute resources information is provided in the application section. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pre-trained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: For all assets we used in this paper, we cited their original source. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]