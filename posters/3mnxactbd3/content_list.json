[{"type": "text", "text": "B-ary Tree Push-Pull Method is Provably Efficient for Distributed Learning on Heterogeneous Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "RunzeYou School of Data Science The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen) runzeyou@link.cuhk.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Shi Pu School of DataScience The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen) pushi@cuhk.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper considers the distributed learning problem where a group of agents cooperatively minimizes the summation of their local cost functions based on peer-to-peer communication. Particularly, we propose a highly efficient algorithm, termed $\\mathbf{\\nabla}^{\\leftarrow}\\mathbf{B}$ -ary Tree Push-Pull\u2019 (BTPP), that employs two B-ary spanning trees for distributing the information related to the parameters and stochastic gradients across the network. The simple method is efficient in communication since each agent interacts with at most $(B+1)$ neighbors per iteration. More importantly, BTPP achieves linear speedup for smooth nonconvex and strongly convex objective functions with only ${\\tilde{O}}(n)$ and $\\tilde{O}(1)$ transient iterations, respectively, significantly outperforming the state-of-the-art results to the best of our knowledge. Our code is available at https://github.com/ryou98/BTPP. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we consider a group of agents, labeled as ${\\mathcal{N}}:=\\{1,2,...,n\\}$ , in which each agent $i$ holds its own local cost function $f_{i}:\\mathbb{R}^{p}\\to\\mathbb{R}$ and communicates only within its direct neighborhood. We investigate how the agents collaborate to locate $x\\in\\mathbb{R}^{p}$ that minimizes the average of all the cost functions: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{p}}f(x)\\left(=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(x)\\right),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $f_{i}(x):=\\mathbb{E}_{\\xi_{i}\\sim\\mathcal{D}_{i}}\\left[F_{i}(x;\\xi_{i})\\right]$ . Here $\\xi_{i}$ denotes the local data of agent $i$ that follows the local distribution $\\mathcal{D}_{i}$ Data hetrogeneity exists if $\\{\\mathcal{D}_{i}\\}_{i=1}^{n}$ are not identical. ", "page_idx": 0}, {"type": "text", "text": "To solve problem (1), we assume each agent $i$ queries a stochastic oracle $({\\mathcal{S}}{\\mathcal{O}})$ to obtain noisy gradient samples. Stochastic gradients appear in many areas including online distributed learning [23, 3], reinforcement learning [17, 15], generative modeling [5, 6], and parameter estimation [2, 27]. Assumption 1.1 ensures that the gradient estimator $g_{i}(x;\\xi_{i})$ remains unbiased with a bounded variance for any given $\\mathbf{x}$ , while independent samples $\\xi_{i}$ are gathered continuously over time. In addition, the assumption is critical in simulation-based optimization as gradient estimation often encounters noise from multiple sources, such as modeling and discretization errors, or limitations due to finite sample sizes in Monte-Carlo methods [7]. ", "page_idx": 0}, {"type": "text", "text": "Modern optimization and machine learning typically involve tremendous data samples and model parameters. The scale of these problems calls for efficient distributed algorithms across multiple computing nodes. Recently, distributed algorithms dealing with problem (1) have been studied extensively in the literature; see, e.g., [19, 14, 4, 34]. Traditional distributed learning approaches typically follow a centralized master-worker setup, where each worker node communicates with a (virtual) central server [12]. However, such a communication pattern incurs significant communication overheads and long latency, especially when the training requires a large number of computing nodes. ", "page_idx": 1}, {"type": "text", "text": "Decentralized learning is an emerging paradigm to save communication costs, where the computing nodes are connected through a certain network topology (e.g., ring, grid, hypercube). Decentralized algorithms do not rely on central servers: the agents maintain the similarity among their copies of model parameters through peer-to-peer messages passing by communicating locally with immediate neighbors in the network. Such a setup allows each node to communicate with only a few peers and hence incurs much lower communication overhead [1]. Moreover, it offers strong promise for new applications, allowing agents to collaboratively train a model while respecting the data locality and privacy of each contributor. ", "page_idx": 1}, {"type": "text", "text": "Specifically, in decentralized stochastic gradient methods, the agents share their local stochastic gradient updates through gossip communication [32]. At every iteration, the local updates are sent to the neighbors of each agent who iteratively propagate the information through the network. Typically, the agents employ iterative gossip averaging of their neighbors? models with their own, where the averaging weights are chosen to ensure asymptotic uniform distribution of each update across the network. However, local averaging is less effective in \u201cmixing\u201d information which makes decentralized algorithms converge slower than their centralized counterparts. Generally speaking, the network topology determines both the number of per-iteration communications and the convergence rates of decentralized algorithms, leading to a trade-off. For example, a densely-connected graph enables decentralized methods to converge faster but results in less efficient communication since each node needs to communicate with more neighbors. By contrast, a sparsely-connected topology results in a slower convergence rate but also reduces the per-iteration communication cost [19, 21, 35]. In particular, for smooth and non-convex objective functions, it has been shown that decentralized stochastic gradient methods (with arbitrary topology) can achieve the same convergence rate as the centralized SGD method, but only after an initial period of iterations has passed [14, 34, 20]. The number of transient iterations (transient time) heavily depends on the network topology, and thus a practical decentralized stochastic gradient algorithm should aim to minimize the transient time while keeping the number of per-iteration communications small (e.g., over a a sparselyconnected topology). Such an observation has motivated several recent works, which consider network topologieswith $\\Theta(1)$ per-iteration communications (or degree) for each node; see, e.g., [34, 26]. ", "page_idx": 1}, {"type": "text", "text": "This work considers an alternative mechanism to gossip averaging, called \u201cB-ary Tree Push-Pull\" (BTPP), inherited from the Push-Pull method in [22, 33]. Rather than relaying the messages over one graph at every iteration, BTPP uses two B-ary trees ${\\mathit{g}}_{\\mathcal{R}}$ and $\\mathcal{G}_{\\mathcal{C}}$ ) to spread the information about the parameters and the stochastic gradients, respectively. Each agent assigned in the B-ary tree acts as a worker on an assembly line. The model parameters are transmitted through the graph $\\mathcal{G}_{\\mathcal{R}}$ from the parent nodes to the child nodes. Meanwhile, the stochastic gradients are computed under the current model parameters and accumulated through the inverse graph of $\\mathcal{G}_{\\mathcal{R}}$ denoted as $\\mathcal{G}_{C}$ . BTPP can be viewed as a semi-(de)centralized approach given the critical role of node 1. Notably, the corresponding mixing matrices of $\\mathcal{G}_{\\mathcal{R}}$ and $\\mathcal{G}_{C}$ only consist of O's and $\\mathrm{1\\,\\dot{s}}$ ,which together with the B-ary Tree topology design, results in high algorithmic efficiency. We show BTPP achieves an $\\tilde{\\mathcal{O}}(n)$ transient time under smooth nonconvex objective functions with $\\Theta(1)$ per-iteration communications for each agent. By comparison, the state-of-the-art transient time is $\\bar{\\mathcal{O}}(n^{3})$ (see Table 1). ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Decentralized Learning Decentralized Stochastic Gradient Descent (DsGD) type algorithms are increasingly popular for accelerating the training of large-scale machine learning models [14, 34, 10] . These algorithms have been adapted under a range of practical settings, including those discussed in [1, 16]. However, DSGD suffers from data heterogeneity [9], which triggers more advanced techniques such as EXTRA [25], Exact-Diffusion $\\mathrm{\\Omega^{2}}$ [13], and gradient tracking [19]. The Push-Pull method [22, 33] which enjoys broad topological requirements was introduced for deterministic decentralized optimization under strongly convex objectives. This work particularly takes advantage of the fexibility in the network design of Push-Pull, utilizing the B-ary tree family, while considering stochastic gradients for minimizing smooth nonconvex objectives. ", "page_idx": 1}, {"type": "table", "img_path": "3MnXAcTBD3/tmp/47f62501154d7253dcdee401c4beb2e64c21242f7fcf2bdd1d55c17c85d3d9d5.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison of different algorithms for distributed stochastic optimization under smooth nonconvex objectives. \u201cPer-iter Comm.\u2019 denotes the number of per-iteration communications or neighbors (degree) for each agent. \u201cBased Graph\u201d represents the number of required graph topologies during the entire training procedure. \u201c\"Trans. Iter.?\u2019 represents the number of transient iterations. The notation $\\tilde{\\mathcal{O}}(\\cdot)$ hides all the polylogarithmic factors. "], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Topology Design  Decentralized stochastic gradient algorithms often rely on gossip averaging over various topologies such as rings, grids, and tori [18]. The hypercube graph [30] strikes a balance between the communication efficiency and the consensus rate, but the network size is constrained to be the power of two. The work in [34] re-examined the static exponential graph with $\\Theta(\\ln(n))$ degree and introduced a one-peer exponential graph with $\\Theta(1)$ degree while preserving the consensus properties under the specific requirement of $n$ . The paper [28] proposed a base- $(k+1)$ graph as an enhancement that achieves similar convergence rate as in [34] under arbitrary network size by sequentially employing multiple graph topologies (splitting an all-connected graph into $\\Theta(\\ln(n))$ different subgraphs). DSGD-CECA [4] requires roughly $\\lceil\\log_{2}(n)\\rceil$ rounds of message passing for global averaging with $\\Theta(n)$ network topologies. OD(OU)-EquiDyn [26] introduces algorithms that employ various topologies to achieve network-size independent consensus rates. RelaySGD [31] offers a relay-based algorithm that ensures $\\Theta(1)$ per-iteration communication across different topologies. ", "page_idx": 2}, {"type": "text", "text": "The above-mentioned methods all enjoy comparable convergence rates with centralized SGD (and thus achieves linear speedup) when the number of iterations $T$ is large enough. The transient times are generally in the order of $\\tilde{\\mathcal{O}}(n^{3})$ under smooth nonconvex objectives (see Table 1) and $\\tilde{\\mathcal{O}}(n)$ under smooth strongly convex objectives (see Table 2). ", "page_idx": 2}, {"type": "text", "text": "Note that the above works and this paper generally consider training machine learning modes within high-performance data-center clusters, in which the network topology can be fully controlled: any two nodes can directly communicate over the network when necessary. By comparison, in some other scenarios, the underlying network topology is fixed, and the communication between two nodes is constrained (e.g., in wireless sensor networks, internet of vehicles, etc). ", "page_idx": 2}, {"type": "text", "text": "1.2 Main Contribution ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This paper introduces a novel distributed stochastic gradient algorithm, termed \u201cB-ary Tree Push-Pull\" (BTPP), which is provably efficient for solving the distributed learning problem (1) under arbitrary network size. The main contribution is summarized as follows: ", "page_idx": 2}, {"type": "text", "text": "\u00b7 BTPP incurs a $\\Theta(1)$ communication overhead per-iteration for each agent. Specifically, any agent in the network communicates with at most $(B+1)$ neighbors, where $B$ can befreely chosen to fit different settings. Generally speaking, larger $B$ increases the per-iteration communication cost but reduces the transient time at the same time. ", "page_idx": 2}, {"type": "table", "img_path": "3MnXAcTBD3/tmp/31febd8268e225ebb6ca841ca94b65ea88f4139a91fb16f6b15bd4bf52ad1268.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Table 2: Comparison of different algorithms for distributed stochastic optimization under smooth strongly convex objectives. The notation $\\tilde{\\mathcal{O}}(\\cdot)$ hides all the polylogarithmic factors inheriting from [26, 9]. ", "page_idx": 3}, {"type": "text", "text": "\u00b7 We show BTPP enjoys an $\\tilde{\\mathcal{O}}(n)$ transient time or iteration complexity under smooth nonconvex objectives and an $\\tilde{\\mathcal{O}}(1)$ transient time or iteration complexity under smooth strongly convex objectives. Such results outperform the baselines: see Table 1 and Table 2. The improvement is significant since the transient time greatly impacts the algorithmic performance, especially under large $n$   \n\u00b7 The convergence analysis for BTPP is non-trivial, partly due to the fact that the algorithm admits two different network topologies for communicating the model parameters and the (stochastic) gradient trackers respectively. Instead of constructing the induced matrix norms $\\Vert\\cdot\\Vert_{\\mathcal{R}}$ and $\\|\\cdot\\|c$ as in [22], the analysis is performed under $\\lVert\\cdot\\rVert_{2}$ and $\\left\\|\\cdot\\right\\|_{F}$ only by carefully treating the matrix products and related terms. ", "page_idx": 3}, {"type": "text", "text": "1.3  Notation and Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Throughout the paper, vectors default to columns if not otherwise specified. Let each agent $i$ hold a local copy $x_{i}\\in\\mathbb{R}^{p}$ of the decision variable and an auxiliary variable $y_{i}\\in\\mathbb{R}^{p}$ ._Their values at iteration $k$ are denoted by $x_{i}^{(k)}$ and $y_{i}^{(k)}$ , respectively. We let $\\mathbf{X}=\\left[x_{1},x_{2},\\cdot\\cdot\\cdot\\,,x_{n}\\right]^{\\top}\\in\\mathbb{R}^{n\\times p}$ \uff0c $\\mathbf{Y}=$ $\\left[y_{1},y_{2},\\cdot\\cdot\\cdot\\mathbf{\\delta},y_{n}\\right]^{\\top}\\in\\mathbb{R}^{n\\times p}$ , and 1 denotes the column vector with allentries equal to 1. We also define the aggregated gradients at the local variables as $\\nabla F(\\mathbf{X}):=[\\nabla f_{1}(x_{1}),\\nabla f_{2}(x_{2}),\\cdot\\cdot\\cdot\\,,\\nabla f_{n}(x_{n})]^{\\top}\\in$ $\\mathbb{R}^{n\\times p}$ , where $\\textstyle F(\\mathbf{X})\\;:=\\;\\sum_{i=1}^{n}f_{i}(x_{i})$ . In addition, denote $\\pmb{\\xi}\\,:=\\,[\\xi_{1},\\xi_{2},\\cdot\\cdot\\cdot\\,,\\xi_{n}]^{\\mathrm{~\\scriptsize~1~}}$ \uff0c ${\\bf G}({\\bf X},\\pmb{\\xi})\\;:=\\;$ $\\left[g_{1}(x_{1},\\xi_{1}),g_{2}(x_{2},\\xi_{2}),\\cdot\\cdot\\cdot\\,,g_{n}(x_{n},\\xi_{n})\\right]^{\\intercal}\\in\\mathbb{R}^{n\\times p}$ . For the conciseness of presentation, we also use $\\mathbf{G}^{\\left(t\\right)}$ to represent $\\mathbf{G}(\\mathbf{X}^{(t)},\\pmb{\\xi}^{(t)})$ . The term $\\langle a,b\\rangle$ stands for the inner product of two vectors $a,b\\in\\mathbb{R}^{p}$ For matrices, $\\lVert\\cdot\\rVert_{2}$ and $\\|\\cdot\\|_{F}$ represent the spectral norm and the Frobenius norm respectively, which degenerate to the Euclidean norm for vectors. For simplicity, any square matrix with power O is the unit matrix I with the same dimension if not otherwise specified. ", "page_idx": 3}, {"type": "text", "text": "We assume each agent $i$ is able to obtain noisy gradient samples of the form $g_{i}(x,\\xi_{i})$ that satisfies the following assumption. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1.1. For all $i\\in\\mathcal{N}$ and $x\\in\\mathbb{R}^{p}$ , each random vector $\\xi_{i}$ is independent and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\xi_{i}\\sim\\mathcal{D}_{i}}\\left[g_{i}(x,\\xi_{i})|x\\right]=\\nabla f_{i}(x),\\;\\mathbb{E}_{\\xi_{i}\\sim\\mathcal{D}_{i}}\\left[\\left\\|g_{i}(x,\\xi_{i})-\\nabla f_{i}(x)\\right\\|^{2}|x\\right]\\le\\sigma^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for some $\\sigma^{2}>0$ ", "page_idx": 3}, {"type": "text", "text": "Regarding the individual objective functions $f_{i}$ , we make the following standard assumption. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1.2. Each $f_{i}(x):\\mathbb{R}^{p}\\to\\mathbb{R}$ is lower bounded with $L$ -Lipschitz continuous gradients, i.e., for any $x,x^{\\prime}\\in\\mathbb R^{p}$ \uff0c ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|\\nabla f_{i}(x)-\\nabla f_{i}(x^{\\prime})\\|\\le L\\,\\|x-x^{\\prime}\\|\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We also consider the following standard assumption regarding strongly convexity. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1.3. For any $x,y\\in\\mathbb{R}^{p}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(y)\\geq f(x)+\\left\\langle\\nabla f(x),y-x\\right\\rangle+{\\frac{\\mu}{2}}\\left\\Vert y-x\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "A directed graph $\\mathcal{G}(\\mathcal{N},\\mathcal{E})$ consists of a set of $n$ nodes $\\mathcal{N}$ and a set of directed edges $\\mathcal{E}\\subseteq\\mathcal{N}\\times\\mathcal{N}$ where an edge $(j,i)\\in\\mathcal{E}$ indicates that node $j$ can directly send information to node $i$ . To facilitate the local averaging procedure, each graph can be associated with a non-negative weight matrix $\\mathbf{W}=[w_{i j}]\\in\\breve{\\mathbb{R}}^{n\\breve{\\times}n}$ , whose element $w_{i j}$ is non-zero only if $(j,i)\\in\\mathcal{E}$ . Similarly, a non-negative weight matrix W corresponds to a directed graph denoted by $\\mathcal{G}_{\\mathbf{W}}$ . For a given graph $\\mathcal{G}_{\\mathbf{W}}$ , the inneighborhood and out-neighborhood of node $i\\in\\mathcal{N}$ are given by $\\mathcal{N}_{\\mathbf{W},i}^{i n}:=\\{j\\in\\mathcal{N}:(j,i)\\in\\mathcal{E}\\}$ and $\\mathcal{N}_{\\mathbf{W},i}^{o u t}:=\\{j\\in\\mathcal{N}\\,\\underline{{\\cdot}}\\,(i,j)\\in\\mathcal{E}\\}$ respectively. The degreeof node $i$ isthe number ofits in-neighbors or out-neighbors. For example, in a one-peer graph, the degree of each node is at most 1. ", "page_idx": 4}, {"type": "text", "text": "1.4  Organization of the Paper ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The rest of this paper is organized as follows. In Section 2, we introduce the B-ary Tree Push-Pull algorithm and present its main convergence results. The sketch of analysis is presented in Section 3, and numerical experiments are provided in Section 4. We conclude the paper in Section 5. ", "page_idx": 4}, {"type": "text", "text": "2  B-ary Tree Push-Pull Method ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "2.1 Communication Graphs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The proposed B-ary Tree Push-Pull method makes use of two spanning trees as communication graphs: $\\mathcal{G}_{\\mathcal{R}}$ and $\\mathcal{G}_{c}$ , which correspond to two mixing matrices $\\mathcal{R}$ and $\\mathcal{C}$ , respectively. Specifically, we consider B-ary tree graphs with arbitrary number of nodes $n$ and depth $d$ . The root node is labeled as 1 for convenience, and we index the nodes layer-by-layer. The additional nodes are placed at the last layer if the tree is not full. Figure 1 ilustrates the assignment of 10 nodes when $B=2$ . In the Pull Tree $\\mathcal{G}_{\\mathcal{R}}$ (the left ones), each node has 1 parent node and $B$ child nodes (except the ones in the last layer). The root node 1 has no parent node. In the Push Tree $\\mathcal{G}_{\\mathcal{C}}$ (the right ones), each node has 1 child node and $B$ parent nodes (except the ones in the last layer). It can be seen that the tree $\\mathcal{G}_{\\mathcal{C}}$ is identical to $\\mathcal{G}_{\\mathcal{R}}$ with all the edges reversing directions. Note that only node 1 has a self-loop. ", "page_idx": 4}, {"type": "image", "img_path": "3MnXAcTBD3/tmp/240025009041b964874223d6a1a2b01d64450bf0c1c55818358c68fd9be5272e.jpg", "img_caption": ["Figure 1: Two spanning trees with 10 nodes when $B=2$ . On the left is $\\mathcal{G}_{\\mathcal{R}}$ , and the right one is $\\mathcal{G}_{C}$ "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "2.2  Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We consider the following distributed stochastic gradient method (Algorithm 1) for solving problem (1). At every iteration $t$ each agent $i$ pulls the state information from its in-neighborhood $\\mathcal{N}_{\\mathcal{R},i}^{i n}$ pushes its (stochastic) gradient tracker $y_{i}$ to the out-neighborhood $\\mathcal{N}_{\\mathcal{C}_{\\lambda}^{\\textit{i}}}^{o u t}$ , and updates its local variables $x_{i}$ and $y_{i}$ based on the received information. The agents aim to find the $\\epsilon_{}$ -stationary point jointly by performing local computation and exchanging information through two spanning trees. ", "page_idx": 4}, {"type": "text", "text": "More specifically, in the pull tree $\\mathcal{G}_{\\mathcal{R}}$ , each node $i$ pulls the updated model from its parent node along the tree Note that $\\bar{N}_{\\mathcal{R},i}^{i n}$ consisofnlydeheparntodehushee $\\mathcal{G}_{\\mathcal{C}}$ the inverse of the $\\mathrm{Pull}$ Tree, in which each node collects and aggregates the gradient trackers from its parent nodes. Due to the tree structure, only. $y_{1}^{t}$ aggregates and tracks the stochastic gradients across the entire network, which will be made clear from the analysis. The implementation of the algoritraaawa $x_{2}^{(\\bar{t}+1)}=x_{1}^{(t)}-\\gamma y_{1}^{(t)}$ and y2 $y_{2}^{(t+1)}=y_{4}^{(t)}+y_{5}^{(t)}+g_{2}(x_{2}^{(t+1)};\\xi_{2}^{(t+1)})-g_{2}(x_{2}^{(t)};\\xi_{2}^{(t)}).$ ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 B-ary Tree Push-Pull Method (BTPP) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "1: Each agent $i$ initalwiy $x_{i}^{(0)}=x^{(0)}\\in\\mathbb R^{p}$ $y_{i}^{(0)}=g_{i}(x_{i}^{(0)},\\xi_{i}^{(0)})\\in$ $\\mathbb{R}^{p}$ after drawing a random sample $\\xi_{i}^{(0)}$ , stepsize $\\gamma>0$ and number of iterations $T$ ", "page_idx": 5}, {"type": "text", "text": "2: for iteration $t=0,1,2,\\ldots,T-1$ do   \n3: for agent $i$ in parallel do   \n6: Independently draw a random sample $\\xi_{i}^{(t+1)}$   \n7: Update parameters through ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle x_{i}^{(t+1)}=\\sum_{j\\in\\mathcal{N}_{\\mathcal{R},i}^{i n}}\\left(x_{j}^{(t)}-\\gamma y_{j}^{(t)}\\right)}}\\\\ {{\\displaystyle y_{i}^{(t+1)}=\\sum_{j\\in\\mathcal{N}_{\\mathcal{C},i}^{i n}}y_{j}^{(t)}+g_{i}(x_{i}^{(t+1)};\\xi_{i}^{(t+1)})-g_{i}(x_{i}^{(t)};\\xi_{i}^{(t)})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "8: end for ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "9: end for ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "$x_{1}^{(T)}$ ", "page_idx": 5}, {"type": "text", "text": "We can write Algorithm 1 in the following compact form: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{X}^{(t+1)}\\mathbf{\\Lambda}=\\mathcal{R}\\left(\\mathbf{X}^{(t)}-\\gamma\\mathbf{Y}^{(t)}\\right)}\\\\ &{\\mathbf{Y}^{(t+1)}\\mathbf{\\Lambda}=\\mathcal{C}\\mathbf{Y}^{(t)}+\\mathbf{G}(\\mathbf{X}^{(t+1)},\\underline{{\\xi}}^{(t+1)})-\\mathbf{G}(\\mathbf{X}^{(t)},\\underline{{\\xi}}^{(t)})}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where ${\\bf Y}^{(0)}={\\bf G}({\\bf X}^{(0)},\\pmb{\\xi}^{(0)})$ and $\\mathcal{R},\\mathcal{C}\\in\\mathbb{R}^{n\\times n}$ are non-negative matrices whose elements are given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n[\\mathcal{R}]_{i,j}=\\left\\{\\begin{array}{l l}{1}&{\\mathrm{if}\\ i\\in\\{B j+1-B+[B]\\}\\cap[n]\\ \\mathrm{or}\\ i=j=1}\\\\ {0}&{\\mathrm{otherwise}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and $\\mathcal{C}=\\mathcal{R}^{\\top}$ which corresponds to $\\mathcal{G}_{c}$ , the inverse tree of $\\mathcal{G}_{\\mathcal{R}}$ . It can be seen that $\\mathcal{R}$ is a row-stochastic matrix that only consists of $0\\,\\mathrm{\\dot{s}}$ and 1's, and $\\mathcal{C}$ is column stochastic. For example, the mixing matrices corresponding to the graphs in Figure 1 are given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left|\\begin{array}{c c c c c c c c}{1}&{2}&{1}&{1}&{1}&&&\\\\ &&&&&{1}&{1}&&\\\\ &&&&&&&{1}&{1}&\\\\ &&&&&&&&{1}\\end{array}\\right|\\,,\\,\\,\\,\\mathscr{C}=\\left(\\begin{array}{c c c c c c c c c}{1}&{1}&{1}&{1}&&&&\\\\ &&&{1}&{1}&&&&\\\\ &&&&&&{1}&{1}&&\\\\ &&&&&&&&{1}\\end{array}\\right)\\,,\\,\\,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the unspecified elements are zeros. ", "page_idx": 5}, {"type": "text", "text": "2.3 Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The main convergence properties of BTPP are summarized in the following two theorems, where the second result assumes strongly convexity on $f$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 2.1. For the BTPP algorithm outlined in Algorithm 1 implemented on $B$ ary treegraphs $\\mathcal{G}_{\\mathcal{R}}$ and $\\mathcal{G}_{C}$ \uff0c assume Assumption 1.1 and Assumption 1.2 hold.  Let $\\gamma\\ =$ ", "page_idx": 5}, {"type": "text", "text": "$\\begin{array}{r l r}{\\mathrm{min}\\big\\{\\bigg(\\frac{\\Delta_{f}}{3\\sigma^{2}L n(T+1)}\\bigg)^{\\frac{1}{2}}\\,,\\bigg(\\frac{\\Delta_{f}}{1500n^{2}d^{6}\\sigma^{2}L^{2}(T+1)}\\bigg)^{\\frac{1}{3}}\\,,\\frac{1}{100n d^{3}L}\\bigg\\}}\\end{array}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{\\Gamma+1}\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\nabla f(x_{1}^{(t)})\\right\\|_{2}^{2}\\leq\\frac{32\\sqrt{\\Delta_{f}\\sigma^{2}L}}{\\sqrt{n(T+1)}}+\\frac{240d^{2}\\left(\\sigma^{2}L^{2}\\Delta_{f}^{2}\\right)^{\\frac{1}{3}}}{(\\sqrt{n}(T+1))^{\\frac{2}{3}}}+\\frac{800d^{3}L\\Delta_{f}}{T+1}+\\frac{\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right\\|_{F}^{2}}{n(T+1)},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\Delta_{f}:=f(\\mathbf{x}_{1}^{(0)})-f^{*}$ and $d=\\lfloor\\log_{B}(n)\\rfloor$ represents the diameter of the graphs. ", "page_idx": 6}, {"type": "text", "text": "Remark 2.2. Based on the convergence rate in (3) of BTPP, we can derive that when $T\\,=$ $\\Theta(n\\log^{12}(n))$ , the term $\\textstyle{\\mathcal{O}}({\\frac{1}{\\sqrt{n T}}})$ dominates the remaining terms up to a constat scalar. This implies that BTPP achieves linear speedup after ${\\mathcal{O}}\\left(n\\log^{12}(n)\\right)$ transient iterations. ", "page_idx": 6}, {"type": "text", "text": "Remark 2.3. The convergence rate in (3) is related to the branch size $B$ .For larger $B$ ,the diameter $d=\\lfloor\\log_{B}(n)\\rfloor$ becomes smaller, which results in more efficient transmission of information and fewer transient iterations. However, the per-iteration communication cost is relatively larger. When $B$ is smaller, the communication burden for each agent at every iteration is lighter, but the transient time is larger. Therefore, in practice, the communication cost and convergence rate can be balanced by considering a proper $B$ ", "page_idx": 6}, {"type": "text", "text": "Theorem 2.4. For the BTPP algorithm outlined in Algorithm $^{\\,l}$ implemented on $B$ -ary tree graphs $\\mathcal{G}_{\\mathcal{R}}$ and $\\mathcal{G}_{\\mathcal{C}}$ \uff0cassume Assumption 1.1, Assumption 1.2 and Assumption 1.3 hold. Let $\\begin{array}{r}{\\gamma=\\operatorname*{min}\\left\\{\\frac{1}{100n d^{2}\\kappa L},\\frac{16\\log\\left(n(T+1)^{2}\\right)}{n(T+1)\\mu}\\right\\}}\\end{array}$ and $T\\geq2d$ The following convergence result holds: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathbb{E}\\left\\|x_{1}^{(T)}-x^{*}\\right\\|^{2}\\leq\\frac{2240\\sigma^{2}\\log(n(T+1)^{2})}{n\\left(T+1\\right)\\mu^{2}}+\\frac{26880000d^{6}\\kappa^{2}\\sigma^{2}\\left(\\log(n(T+1)^{2})\\right)^{2}}{n\\left(T+1\\right)^{2}\\mu^{2}}}\\\\ {\\displaystyle+\\operatorname*{max}\\left\\{\\exp(-\\frac{T}{800d^{2}\\kappa^{2}}),\\frac{40}{n\\left(T+1\\right)^{2}}\\right\\}\\left(\\left\\|x_{1}^{(0)}-x^{*}\\right\\|^{2}+\\frac{1}{n L^{2}}\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right\\|_{F}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark2.5.The convergence ratein(4)impliesthat $\\begin{array}{r l r}{\\mathbb{E}\\left\\|x_{1}^{(T)}-x^{*}\\right\\|^{2}}&{{}}&{\\leq}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "$\\begin{array}{r}{\\tilde{\\mathcal{O}}\\left(\\frac{1}{n T}+\\frac{1}{n T^{2}}+\\exp\\left(-T\\right)\\right),}\\end{array}$ where $\\tilde{\\mathcal{O}}$ hides the constants and polylogarithmic factors.  The transient timeisthus $\\tilde{O}(1)$ .ie,the number o iteraions efore the term $\\mathcal{O}(\\frac{1}{n T})$ dominatesthe remaining terms. Such a transient time also outperforms the state-of-the-art results. ", "page_idx": 6}, {"type": "text", "text": "3  Analysis of B-ary Tree Push-Pull ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we study the convergence of BTPP and prove Theorem 2.1 by analyzing the properties of the weight matrices $\\mathcal{R}$ and $\\mathcal{C}$ tevlftaggregatdce $\\begin{array}{r}{\\sum_{t=0}^{T}\\underline{{\\mathbb{E}}}\\left\\|\\boldsymbol{\\Pi}_{\\mathbf{u}}\\boldsymbol{\\mathbf{X}}^{(t)}\\right\\|_{F}^{2}.}\\end{array}$ and the expected inner products of the stochastic gradients between different layers. The approach is different from those employed in [22, 19, 26], where the analysis considers two special matrix norms related to $\\mathcal{R}$ and $\\mathcal{C}$ , respectively. Such a distinction is because BTPP works with two B-ary trees and iterates in a layer-wise manner, while most other works consider connected graphs. ", "page_idx": 6}, {"type": "text", "text": "Our analysis starts with characterizing the weight matrices $\\mathcal{R}$ and $\\mathcal{C}$ , as delineated in the following lemmas. It is important to note that for any given $n$ and a specific integer $B$ , we can determine an integer d satisfying B-1 $\\begin{array}{r}{\\frac{B^{d}-1}{B-1}<n\\leq\\frac{B^{d+1}-1}{B-1}}\\end{array}$ which is the diameter of the graphs. ", "page_idx": 6}, {"type": "text", "text": "Notice that $\\mathcal{R}$ has a unique non-negative left eigenvector $\\mathbf{u}^{\\top}$ (w.r.t. eigenvalue 1) with $\\mathbf{u}^{\\top}\\mathbf{1}=$ $n$ . More specifically, $\\mathbf{u}\\,=\\,[n,0,\\cdots\\,,0]^{\\top}$ , which is also the unique right eigenvector of $\\mathcal{C}$ (w.r.t. eigenvalue 1), denoted by $\\mathbf{v}$ for the clarity of presentation. Following the above observations, it is revealed in Lemma3.1 that the 2-norm of the matrix $\\begin{array}{r}{{\\mathcal{R}}-{\\frac{1}{n}}\\mathbf{1}\\mathbf{u}^{\\top}}\\end{array}$ with exponent $k$ remains bounded by $\\sqrt{n}$ and equals zero when $k$ exceeds $d-1$ ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.1. Given a positive integer $k$ the 2-norm of the matrix $\\begin{array}{r}{{\\mathcal{R}}^{k}-{\\frac{1}{n}}\\mathbf{1}\\mathbf{u}^{\\top}}\\end{array}$ satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|\\mathcal{R}^{k}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\|_{2}\\left\\{\\overset{\\leq}{\\quad}\\sqrt{n}\\quad k\\leq d-1\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Similar result applies to the matrix $\\begin{array}{r}{\\mathcal{C}^{k}-\\frac{1}{n}{\\bf v1}^{\\top}}\\end{array}$ . Consequently, we introduce the mixing matrices $\\mathbf{\\Pi}\\Pi_{\\mathbf{u}},\\Pi_{\\mathbf{v}}$ based on the eigenvectors $\\mathbf{u},\\mathbf{v}$ , which play a crucial role in the follow-up analysis. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\Pi_{\\mathbf{u}}:=\\mathbf{I}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top},\\;\\Pi_{\\mathbf{v}}:=\\mathbf{I}-\\frac{1}{n}\\mathbf{v}\\mathbf{1}^{\\top}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The following lemmas delineate the critical elements for constraining the average expected norms of the objective function as formulated in (1), i.e., $\\begin{array}{r}{\\frac{1}{T+1}\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\nabla f(x_{1}^{(t)})\\right\\|_{2}^{2}}\\end{array}$ . Lemma 3.2 and Lemma 3.3 provide bounds on the expressions $\\begin{array}{r}{\\sum_{t=0}^{T}\\mathbb{E}\\|\\bar{\\mathbf{X}}^{(t+1)}-\\bar{\\mathbf{X}}^{(t)}\\|_{F}^{2}}\\end{array}$ and $\\sum_{t=0}^{T}\\left|\\left|\\mathbf{I}\\mathbf{I}_{\\mathbf{u}}\\mathbf{X}^{(t)}\\right|\\right|_{F}^{2}$ where $\\begin{array}{r}{\\bar{\\mathbf{X}}^{\\left(t\\right)}:=\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\mathbf{X}^{\\left(t\\right)}}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "Lemma3.2. Suppose Assumption 1.1 holds and $\\begin{array}{r}{\\gamma\\leq\\frac{1}{10n d L}}\\end{array}$ we have the fllowing inequality: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t=0}^{T}\\mathbb{E}\\|\\bar{\\mathbf{X}}^{(t+1)}-\\bar{\\mathbf{X}}^{(t)}\\|_{F}^{2}\\leq6\\gamma^{2}n^{2}\\sigma^{2}(T+1)+50\\gamma^{2}n^{2}d^{2}L^{2}\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\mathbf{I}_{\\mathbf{u}}\\mathbf{X}^{(t)}\\right\\|_{F}^{2}}}\\\\ &{+\\,6\\gamma^{2}n^{2}d\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right\\|_{F}^{2}+15\\gamma^{2}n^{3}\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\nabla f(x_{1}^{(t)})\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Lemma3.3. Suppose Assumption 1.1 holds and $\\begin{array}{r}{\\gamma\\leq\\frac{1}{40n d^{2}L}}\\end{array}$ we have for $d\\geq2$ that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle}&{\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\Pi_{\\mathbf{u}}\\mathbf{X}^{(t)}\\right\\|_{F}^{2}\\leq300\\gamma^{2}n^{2}d^{4}(T+1)\\sigma^{2}+20\\gamma^{2}n^{3}d^{2}\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\|\\nabla f(x_{1}^{(t)})\\|_{2}^{2}}\\\\ &{\\displaystyle\\qquad\\qquad+\\,6n d\\left\\|\\Pi_{\\mathbf{u}}\\mathbf{X}^{(0)}\\right\\|_{F}^{2}+40\\gamma^{2}n^{2}d^{3}\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "From the design of BTPP, there is an inherent delay in the transmission of information from layer $k$ to layer 1. As information traverses through the $\\mathbf{B}$ -ary trees, the delay becomes evident. Specifically, for nodes at layer $k$ , their information requires an additional $k$ iterations to successfully reach and impact node 1, as demonstrated in Lemma 3.4. ", "page_idx": 7}, {"type": "text", "text": "Lemma 3.4. For any integer $t>1$ we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sum_{m=1}^{\\operatorname*{min}\\{t,d\\}}\\mathbb{E}\\left\\langle\\nabla f(x_{1}^{(t)}),\\left(\\frac{\\mathbf{u}^{\\top}}{n}-\\mathbf{1}^{\\top}\\right)\\mathbf{A}_{m}\\left(\\mathbf{G}^{(t-m)}-\\nabla\\mathbf{F}(\\mathbf{X}^{(t-m)})\\right)\\right\\rangle=0,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where ${\\bf A}_{m}=\\mathcal{C}^{m}-\\mathcal{C}^{m-1}$ and $\\mathbf{A}_{1}=\\boldsymbol{\\mathcal{C}}-\\mathbf{I}$ ", "page_idx": 7}, {"type": "text", "text": "Building on the preceding lemmas, we are in a position to establish the main convergence result for the BTPP algorithm. This involves upper bounding the expected norm for the gradient of the objective function evaluated at &(t) . To show the result, we integrate the findings from Lemma 3.2, Lemma 3.3, and Lemma 3.4, as detailed in Lemma 3.5. ", "page_idx": 7}, {"type": "text", "text": "Lemma3.5. Suppose Asumption1.1 and Assumption 1.2hold and $\\begin{array}{r}{\\gamma\\leq\\frac{1}{100n d^{3}L}}\\end{array}$ wehave ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{T+1}\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\nabla f(x_{1}^{(t)})\\right\\|_{2}^{2}\\leq\\frac{8\\Delta_{f}}{\\gamma n(T+1)}+24\\gamma\\sigma^{2}L+2000\\gamma^{2}n d^{6}\\sigma^{2}L^{2}}\\\\ &{\\displaystyle\\qquad+\\,\\frac{400d^{3}L^{2}\\left\\|\\mathbf{I}_{\\mathbf{u}}\\mathbf{X}^{(0)}\\right\\|_{F}^{2}}{T+1}+\\frac{56\\gamma d^{3}L\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right\\|_{F}^{2}}{T+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark 3.6. Lemma 3.5 implies that the transient time of BTPP under Assumption 1.2 is influenced by the fourth term in the upper bound: $\\frac{400d^{3}L^{2}\\left\\lVert\\mathbf{m}_{\\mathbf{u}}\\mathbf{X}^{(0)}\\right\\rVert_{F}^{2}}{T+1}$ which is related to the initial consensus error. Therefore, we initialize all the agents with the same solution $x^{(0)}$ ", "page_idx": 7}, {"type": "text", "text": "Under strong convexity of $f$ , we have the following key lemma. ", "page_idx": 7}, {"type": "text", "text": "Lemma.SupseAsso1 Asn12adAstn13hld $\\begin{array}{r}{\\gamma\\leq\\frac{1}{100n d^{2}\\kappa L}}\\end{array}$ wehave ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|x_{1}^{(T)}-x^{*}\\right\\|^{2}\\leq\\left(1-\\frac{n\\gamma\\mu}{4}\\right)^{T}\\left\\|x_{1}^{(0)}-x^{*}\\right\\|^{2}}\\\\ &{\\quad+\\,7\\gamma^{2}n\\sigma^{2}\\left(T+1\\right)+21000\\gamma^{3}n^{2}d^{6}\\kappa L\\sigma^{2}\\left(T+1\\right)}\\\\ &{\\quad+\\,80\\gamma^{2}n d^{3}\\left(1-\\frac{n\\gamma\\mu}{4}\\right)^{T-d}\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right\\|+420\\gamma^{3}n^{2}d^{3}\\kappa L\\left\\|\\Pi_{\\mathbf{u}}\\mathbf{X}^{(0)}\\right\\|_{F}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\kappa:=L/\\mu$ is the conditional number. ", "page_idx": 8}, {"type": "text", "text": "4 Numerical Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This section presents experimental results to compare the B-ary Tree Push-Pull method with other popular algorithms on logistic regression with synthetic data and deep learning tasks with real data. ", "page_idx": 8}, {"type": "text", "text": "4.1 Logistic Regression ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We compare the performance of BTPP against other algorithms listed in Table 1 for logistic regression with non-convex regularization [26]. The objective functions $f_{i}:\\mathbb{R}^{p}\\to\\mathbb{R}$ aregivenby ", "page_idx": 8}, {"type": "equation", "text": "$$\nf_{i}(\\boldsymbol{x}):=\\frac{1}{J}\\sum_{j=1}^{J}\\ln\\left(1+\\exp(-y_{i,j}h_{i,j}^{\\top}\\boldsymbol{x})\\right)+R\\sum_{k=1}^{p}\\frac{x_{[k]}^{2}}{1+x_{[k]}^{2}},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $x_{[k]}$ is the $k$ th element of $x$ and $\\left\\{(h_{i,j},y_{i,j})\\right\\}_{j=1}^{J}$ repreent t lalatakpt bynde $i$ To control the data heterogeneity across the nodes, we first let each node $i$ be associated with a local logistic regression model with parameter $\\tilde{x}_{i}$ generated by $\\tilde{x}_{i}=\\tilde{x}+v_{i}$ , where $\\tilde{x}\\sim\\mathcal{N}(0,\\mathbf{I}_{p})$ is a common random vector, and $v_{i}\\,\\sim\\mathcal{N}(0,\\sigma_{h}^{2}\\mathbf{I}_{p})$ are random vectors generated independently. Therefore, $\\{v_{i}\\}$ decide the dissimilarities between $\\tilde{x}_{i}$ , and larger $\\sigma_{h}$ generally amplifies the difference. After fixing $\\{\\tilde{x}_{i}\\}$ , local data samples are generated that follow distinct distributions. For node $i$ , the feature vectors are generated as $\\bar{h_{i,j}}\\sim\\mathcal{N}(0,\\mathbf{I}_{p})$ , and $z_{i,j}\\sim\\mathcal{U}(0,1)$ . Then, the labels $y_{i,j}\\in\\{-1,1\\}$ are set to satisfy $z_{i,j}\\leq1+\\exp(-y_{i,j}h_{i,j}^{\\top}\\tilde{x}_{i})$ ", "page_idx": 8}, {"type": "text", "text": "In the simulations, the parameters are set as follows: $n=100$ $p=500$ $J=1000$ $R=0.01$ , and $\\sigma_{h}=0.8$ . All the algorithms initialize with the same stepsize $\\gamma=0.3$ , except BTPP, which employs a modified stepsize $\\gamma/n$ . Such an adjustment is due to BTPP's update mechanism, which incorporates a tracking estimator that effectively accumulates $n$ times the averaged stochastic gradients as the number of iterations increases. This can also be seen from the stepsize choice in Theorem 2.1.1 Additionally, we implement a stepsize decay of $60\\%$ every 100 iterations to facilitate convergence. ", "page_idx": 8}, {"type": "text", "text": "In Figure 2, the gradient norm is used as a metric to gauge the algorithmic performance of each algorithm. The left panel of Figure 2 illustrates the comparative performance of various algorithms, highlighting that BTPP (in red) achieves faster convergence than the other algorithms with $\\Theta(1)$ degree and closely approximates the performance of the centralized SGD algorithm (i.e., DSGDFullyConnected). The right panel of Figure 2 demonstrates the behavior of BTPP when increasing thebranch size $B$ . It is observed that with larger $B$ , the convergence trajectory of BTPP more closely aligns with that of centralized SGD, corroborating the prediction of the theoretical analysis. ", "page_idx": 8}, {"type": "text", "text": "4.2 Deep Learning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We apply BTPP and the other algorithms to solve the image classification task with CNN over MNIST dataset [11]. We run all experiments on a server with eight Nvidia RTX 3090 GPUs. The network contains two convolutional layers with max pooling and ReLu and two feed-forward layers. In particular, we consider a heterogeneous data setting, where data samples are sorted based on their labels and partitioned among the agents. The local batch size is set to 8 with 24 agents in total. The learning rate is 0.01 for all the algorithms except BTPP (which employs a modified stepsize $\\gamma/n)$ ", "page_idx": 8}, {"type": "image", "img_path": "3MnXAcTBD3/tmp/5ae77d25e94cdaad46f3571f179110f61538bddc9b3a2e7be4ea19571164ef1b.jpg", "img_caption": ["Figure 2: Left: performance of algorithms for logistic regression with nonconvex regularization, where the dotted lines correspond to algorithms whose degrees are not $\\Theta(1)$ . We let the branch size $B=2$ in BTPP, $\\eta=0.5$ in OD-EquiDyn, $k=2$ inBase- $(k+1)$ , and perform RelaySGD on a binary tree graph for fairness. Right: performance of BTPP with different branch size $B$ "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "for fairness. Additionally, the starting model is enhanced by pre-training using the SGD optimizer on the entire MNIST dataset for several iterations. Figure 3 illustrates the training loss and the test accuracy curves. Comparing the performance of different algorithms, it can be seen that BTPP (in red) and DSGT with ODEquiDyn (based on $\\Theta(n)$ graphs) achieve faster convergence than the other algorithms with $\\Theta(1)$ degree and closely approximate the performance of centralized SGD(i.e., DSGD-FullyConnected). ", "page_idx": 9}, {"type": "image", "img_path": "3MnXAcTBD3/tmp/66d631bd58f7ba62ed2d4ee29bbe3e2e464d176f2f71b11120f3d6e6cf5f24d7.jpg", "img_caption": ["Figure 3: Train loss and test accuracy of different algorithms for training CNN on MNIST, where the dotted lines correspond to the algorithms whose degrees are not $\\Theta(1)$ . We perform BTPP with $B=2$ , ODEquiDyn with $\\eta=0.5$ ,Base- $(k+1)$ with $k=2$ , and RelaySGD on a binary tree graph for fairness. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Remark 4.1. Higher accuracy can be achieved for BTPP and other methods when using the momentum technique, or when the data heterogeneity is removed, meaning that samples are randomly assigned to each agent. Additional experiments demonstrating the performance of various algorithms across different tasks and scenarios are provided in Appendix B. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper proposes a novel algorithm for distributed learning over heterogeneous data, named BTPP. The convergence is theoretically analyzed for smooth non-convex stochastic optimization. The results demonstrate that, at the minimal communication cost per iteration, BTPP achieves linear speedup in the number of nodes $n$ , and the transient times behaves as ${\\tilde{O}}(n)$ and $\\tilde{O}(1)$ respectively for smooth nonconvex and strongly convex objectives, outperforming the state-of-the-art results. Numerical experiments further validate the efficiency of BTPP. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] M. AsSRAN, N. LOIZOU, N. BALLAS, AND M. RABBAT, Stochastic gradient push for distributed deep learning, in International Conference on Machine Learning, PMLR, 2019, pp. 344-353.   \n[2] L. BoTToU, Stochastic gradient descent tricks, in Neural Networks: Tricks of the Trade: Second Edition, Springer, 2012, p. 421-436.   \n[3] J. DEAN, G. CORRADO, R. MONGA, K. CHEN, M. DEVIN, M. MAO, M. RANZATO, A. SENIOR, P. TUCKER, K. YANG, ET AL., Large scale distributed deep networks, Advances in neural information processing systems, 25 (2012).   \n[4] L. DING, K. JIN, B. YING, K. YUAN, AND W. YIN, Dsgd-ceca: Decentralized sgd with communication-optimal exact consensus algorithm, arXiv preprint arXiv:2306.00256, (2023). [5] 1. GOODFELLOW, J. POUGET-ABADIE, M. MIRZA, B. XU, D. WARDE-FARLEY, S. OZAIR, A. CoURVILLE, AND Y. BENGIO, Generative adversarial nets, Advances in neural information processing systems, 27 (2014).   \n[6] D. P. KINGMA, M. WELLING, ET AL., An introduction to variational autoencoders, Foundations and Trends $^\\mathrm{\\textregistered}$ in Machine Learning, 12 (2019), pp. 307-392.   \n[7] J. P. KLEIJNEN, Design and analysis of simulation experiments, Springer, 2018.   \n[8]  A. KOLOsKOVA, T. LIN, AND S. U. STICH, An improved analysis of gradient tracking for decentralized machine learning, Advances in Neural Information Processing Systems, 34 (2021), pp. 11422-11435.   \n[9] A. KOLOSKOVA, N. LOIZOU, S. BOREIRI, M. JAGGI, AND S. STICH, A unifed theory of decentralized sgd with changing topology and local updates, in International Conference on Machine Learning, PMLR, 2020, p. 5381-5393.   \n[10] A. KOLOSKOVA, S. STICH, AND M. JAGGI, Decentralized stochastic optimization and gossip algorihms withcompressed communication, in International Conference onMachineLeaning, PMLR, 2019, Pp. 3478-3487.   \n[11] Y. LECUN, C. CORTEs, C. BURGES, ET AL., Mnist handwritten digit database, 2010.   \n[12] M. LI, D. G. ANDERSEN, J. W. PARK, A. J. SMOLA, A. AHMED, V. JOSIFOVSKI, J. LONG E. J. SHEKITA, AND B.-Y. SU, Scaling distributed machine learning with the parameter server, in 11th USENIX Symposium on operating systems design and implementation (OSDI 14), 2014, Pp. 583-598.   \n[13] Z. L1, W. SHI, AND M. YAN, A decentralized proximal-gradient method with network independent step-sizes and separated convergence rates, IEEE Transactions on Signal Processing, 67 (2019), Pp. 4494-4506.   \n[14] X. LIAN, C. ZHANG, H. ZHANG, C.-J. HSIEH, W. ZHANG, AND J. LIU, Can decentralized algorithms outperformcentralizedalgorithms?acase studyfordecentralizedparallstochastic gradient descent, Advances in neural information processing systems, 30 (2017).   \n[15] T. P. LILLICRAP, J. J. HUNT, A. PRITZEL, N. HEESS, T. EREZ, Y. TASSA, D. SILVER, AND D. WIERsTRA, Continuous control with deep reinforcement learning, arXiv preprint arXiv:1509.02971, (2015).   \n[16] T. LIN, S. P. KARIMIREDDY, S. U. STICH, AND M. JAGGI, Quasi-global momentum: Accelerating decentralized deep learning on heterogeneous data, arXiv preprint arXiv:2102.04761, (2021).   \n[17] V. MNIH, K. KAVUKCUOGLU, D. SILVER, A. GRAVES, I. ANTONOGLOU, D. WIERSTRA, AND M. RIEDMILLER, Playing atari with deep reinforcement learning, arXiv preprint arXiv:1312.5602, (2013).   \n[18] A. NEDIC, A. OLSHEVSKY, AND M. G. RABBAT, Network topology and communicationcomputation tradeoffs in decentralized optimization, Proceedings of the IEEE, 106 (2018), pp. 953-976.   \n[19]  S. PU AND A. NEDIC, Distributed stochastic gradient tracking methods, Mathematical Programming, 187 (2021), pPp. 409-457.   \n[20] S. PU, A. OLSHEVsKY, AND I. C. PASCHALIDIS, Asymptotic network independence in distributed stochastic optimization formachine learning: Examining distributed and centralized stochastic gradient descent, IEEE signal processing magazine, 37 (2020), pp. 114-122.   \n[21] S. PU, A. OLSHEVsKY, AND I. C. PAsCHALIDIS, A sharp estimate on the transient time of distributed stochastic gradient descent, IEEE Transactions on Automatic Control, 67 (2021), pp. 5900-5915.   \n[22]  S. PU, W. SHI, J. XU, AND A. NEDIC, Push-pull gradient methods for distributed optimization in networks, IEEE Transactions on Automatic Control, 66 (2020), pp. 1-16.   \n[23] B. RECHT, C. RE, S. WRIGHT, AND F. NIU, Hogwild!: A lock-free approach to parallizing stochastic gradient descent, Advances in neural information processing systems, 24 (2011).   \n[24] S. REsNICK, A probability path, Springer, 2019.   \n[25] W. SHI, Q. LING, G. WU, AND W. YIN, Extra: An exact first-order algorithm for decentralized consensus optimization, SIAM Journal on Optimization, 25 (2015), pp. 944-966.   \n[26] Z. SONG, W. LI, K. JIN, L. SHI, M. YAN, W. YIN, AND K. YUAN, Communication-efficient topologies for decentralized learning with $o(1)$ consensus rate, Advances in Neural Information Processing Systems, 35 (2022), pp. 1073-1085.   \n[27] N. SRIVASTAVA, G. HINTON, A. KRIZHEVSKY, I. SUTSKEVER, AND R. SALAKHUTDINOV, Dropout: a simple way to prevent neural networks from overfiting, The journal of machine learning research, 15 (2014), pp. 1929-1958.   \n[28] Y. TAKEZAWA, R. SATO, H. BAO, K. NIWA, AND M. YAMADA, Beyond exponential graph: Communication-effcient topologiesfordecentralized learning viafniteimeconvergence,ariv preprint arXiv:2305.11420, (2023).   \n[29] H. TANG, X. LIAN, M. YAN, C. ZHANG, AND J. LIU, : Decentralized training over decentralized data, in International Conference on Machine Learning, PMLR, 2018, pp. 4848- 4856.   \n[30] L.TREVIsAN, Lecture notes on graph partitioning, expanders and spectral methods,University of California, Berkeley, https://people. eecs. berkeley. edu/luca/books/expanders-2016. pdf, (2017).   \n[31] T. VOGELS, L. HE, A. KOLOSKOVA, S. P. KARIMIREDDY, T. LIN, S. U. STICH, AND M. JAGGI, Relaysum for decentralized deep learning on heterogeneous data, Advances in Neural Information Processing Systems, 34 (2021), pp. 28004-28015.   \n[32] L. XIAO AND S. BOYD, Fast linear iterations for distributed averaging, Systems & Control Letters, 53 (2004), pp. 65-78.   \n[33] R. XIN AND U. A. KHAN, A linear algorithm for optimization over directed graphs with geometric convergence, IEEE Control Systems Letters, 2 (2018), pp. 315-320.   \n[34] B. YING, K. YUAN, Y. CHEN, H. HU, P. PAN, AND W. YIN, Exponential graph is provably efficient for decentralized deep training, Advances in Neural Information Processing Systems, 34 (2021), pp. 13975-13987.   \n[35] K. YUAN, S. A. ALGHUNAIM, AND X. HUANG, Removing data heterogeneity infuence enhances network topology dependence of decentralized sgd, Journal of Machine Learning Research, 24 (2023), pp. 1-53. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A  Convergence Analysis of BTPP ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we aim to demonstrate the convergence results of BTPP through a three-step process. First, we explore the key properties of matrices $\\mathcal{R}$ and $\\mathcal{C}$ , acquainting readers with several operations that will be frequently utilized in the subsequent parts. Then, we introduce various technical tools essential for the analysis. Finally, we delve into proving the convergence results supported by a series of pertinent lemmas. ", "page_idx": 12}, {"type": "text", "text": "A.1  Properties of the Weight Matrices ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this part, we first demonstrate that $\\mathcal{R}\\in\\mathbb{R}^{n\\times n}$ possesses a set of properties ( the matrix $\\mathcal{C}=\\mathcal{R}^{\\top}$ shares similar properties). Then, we utilize the established tools to prove the crucial result presented in Lemma 3.1. Lastly, we provide clarifications on certain matrix operations that will be frequently employed in deriving the convergence results. ", "page_idx": 12}, {"type": "text", "text": "It is important to note that for any given $n$ and specific integer $B$ , the diameter of the corresponding B-ary tree graphd (the distance frm the last layer node tonode 1 atisfies $\\begin{array}{r}{\\frac{B^{d}-1}{B-1}<n\\leq\\frac{\\'B^{d+1}-1}{B-1}}\\end{array}$ To investigate the properties of $\\mathcal{R}$ and $\\mathcal{C}$ , we will introduce the column vector $\\mathbf{e}_{\\mathcal{T}}\\in\\mathbb{R}^{n}$ , where each element of $\\mathbf{e}_{\\mathcal{T}}$ is equal to 1 for indices $i\\in\\mathcal{Z}$ and O otherwise. Define the index sets ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\mathcal{Z}}_{1,k}=\\left\\{1:\\frac{B^{k+1}-1}{B-1}\\right\\},}}\\\\ {{\\displaystyle{\\mathcal{Z}}_{i,k}=\\left\\{\\left(\\frac{B^{k+1}-1}{B-1}+(i-2)B^{k}+1\\right):\\left(\\frac{B^{k+1}-1}{B-1}+(i-1)B^{k}\\right)\\right\\},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $k_{1}:k_{2}$ is the arithmetic progression from $k_{1}$ to $k_{2}$ with difference 1. We can then define the matrix $\\mathbf{Z}_{k}\\in\\mathbb{R}^{n\\times n}$ as a composite of several column vectors arranged in the following format: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbf{Z}_{k}=\\left[\\mathbf{e}_{{Z}_{1,k}},\\mathbf{e}_{{Z}_{2,k}},\\cdot\\cdot\\cdot\\mathbf{\\Phi},\\mathbf{e}_{{Z}_{n,k}}\\right].\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "This closed-form expression of $\\mathcal{R}$ with any power $k$ is shown in Lemma A.1 which aids in developing further properties. ", "page_idx": 12}, {"type": "text", "text": "Lemma A.1. For the pull matrix $\\mathcal{R}$ corresponding to the $B$ -ary tree $\\mathcal{G}_{\\mathcal{R}}$ , given any positive index $k$ wehave ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{R}^{k}=\\mathbf{Z}_{k}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. We prove the lemma by induction. First, it is obvious that $\\mathcal{R}=\\mathbf{Z}_{1}$ by the definition of $\\mathcal{R}$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~~\\mathcal{R}_{i j}=1}\\\\ &{\\mathrm{iff}~i\\in\\{B j+1-B+[B]\\}\\cap[n]\\;\\mathrm{or}\\;i=j=1}\\\\ &{\\mathrm{iff}\\;B(j-1)+2\\leq i\\leq B j+1,\\;i\\in[n]\\;\\mathrm{or}\\;i=j=1}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Now assume the statement is true for $k=j$ . Then, for $k=j+1$ ,wehave ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{R}^{j+1}=\\mathcal{R}^{j}\\ast\\mathcal{R}=\\mathbf{Z}_{j}\\mathbf{Z}_{1}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Denote $[\\mathbf{Z}_{j}\\cdot\\mathbf{Z}_{1}]_{i}$ as the $i$ -th column of $\\mathbf{Z}_{j}\\cdot\\mathbf{Z}_{1}$ . To establish the result, we only need to demonstrate that the two matrices, $\\mathcal{R}^{k+1}$ and $\\mathbf{Z}_{k+1}$ , have the same column entries. For $i=1$ \uff0c ", "page_idx": 12}, {"type": "equation", "text": "$$\n[\\mathbf Z_{j}\\mathbf Z_{1}]_{1}=\\mathbf Z_{j}[\\mathbf Z_{1}]_{1}=\\sum_{i=1}^{B+1}[\\mathbf Z_{j}]_{i}=\\sum_{i=1}^{B+1}\\mathbf e_{{\\cal Z}_{i,j}}=\\mathbf e_{\\cup_{i=1}^{B+1}{\\cal Z}_{i,j}}=\\mathbf e_{{\\cal Z}_{1,j+1}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "For $i>1$ ,we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[\\mathbf{Z}_{j}\\mathbf{Z}_{1}]_{i}=\\mathbf{Z}_{j}[\\mathbf{Z}_{1}]_{i}=\\displaystyle\\sum_{m\\in\\mathbb{Z}_{i,1}}[\\mathbf{Z}_{j}]_{m}=\\sum_{m=(i-1)B+2}^{i B+1}[\\mathbf{Z}_{j}]_{m}}\\\\ &{=\\displaystyle\\sum_{m=(i-1)B}^{i B-1}\\mathbf{e}_{\\mathbb{Z}_{m,j}}=\\mathbf{e}_{\\mathbb{V}_{m=(i-1)B}^{i B-1}}\\mathbb{Z}_{m,j}=\\mathbf{e}_{\\mathbb{Z}_{i,j+1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Thus, we conclude that $\\mathcal{R}^{k+1}=\\mathbf{Z}_{k+1}$ ", "page_idx": 12}, {"type": "text", "text": "Corollary A.2 below reveals that when the power $k$ exceeds $d,\\mathcal{R}^{k}$ transforms into a matrix where the first column is entirely composed of ones, while all the other columns consist of zeros. ", "page_idx": 13}, {"type": "text", "text": "Corollary A.2. For $k=d,$ we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{R}^{k}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}=\\mathbf{0}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where Ois the matrix with all entries equal $\\boldsymbol{O}$ ", "page_idx": 13}, {"type": "text", "text": "Proof. From Lemma A.1, we have for the $i$ -th column of $\\begin{array}{r}{{\\mathcal{R}}^{k}-{\\frac{1}{n}}\\mathbf{1}\\mathbf{u}^{\\top}}\\end{array}$ that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left[{\\mathcal{R}}^{k}-{\\frac{1}{n}}\\mathbf{1}\\mathbf{u}^{\\top}\\right]_{i}=\\left\\{{\\begin{array}{r}{-\\mathbf{e}_{\\frac{B^{k+1}-1}{B-1}+1:n}\\,i=1}\\\\ {\\qquad\\qquad\\mathbf{e}_{Z_{i,k}}\\;i>1}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For $k=d$ , the first $n$ elements of all the columns remain O, which implies the desired result. ", "page_idx": 13}, {"type": "text", "text": "Now, we are ready to prove Lemma 3.1: ", "page_idx": 13}, {"type": "text", "text": "Proof of Lemma 3.1. For any integer $k\\leq d-1$ , define ", "page_idx": 13}, {"type": "equation", "text": "$$\nn_{0}:=\\left\\lfloor\\frac{n-\\frac{B^{k+1}-1}{B-1}}{B^{k}}\\right\\rfloor.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This ensure hat $\\begin{array}{r}{\\frac{B^{k+1}-1}{B-1}+n_{0}B^{k}\\,\\leq\\,n}\\end{array}$ and $\\begin{array}{r}{\\frac{B^{k+1}-1}{B-1}+(n_{0}+1)B^{k}\\,>\\,n}\\end{array}$ s that only the frst $(n_{0}+2)$ -th columns of $\\mathcal{R}^{k}$ consist of non-zero elements. Note that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\|\\mathbf{x}\\|_{2}=1}\\left\\{\\|\\left(\\mathcal{R}^{k}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)\\mathbf{x}\\|_{2}^{2}\\right\\}=\\operatorname*{max}_{\\mathbf{x}}\\left\\{\\frac{\\|\\left(\\mathcal{R}^{k}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)\\mathbf{x}\\|_{2}^{2}}{\\|\\mathbf{x}\\|_{2}^{2}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, we focus on the non-zero elements of the matrix $\\begin{array}{r}{{\\mathcal{R}}^{k}-{\\frac{1}{n}}\\mathbf{1}\\mathbf{u}^{\\top}}\\end{array}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{{\\frac{|\\left({\\mathcal{R}}^{k}-{\\frac{1}{n}}\\mathbf{1}\\mathbf{u}^{\\top}\\right)\\mathbf{x}||_{2}^{2}}{\\|\\mathbf{x}\\|_{2}^{2}}}}&{={\\frac{1}{\\|\\mathbf{x}\\|_{2}^{2}}}\\left(\\sum_{j=2}^{n+1}B^{k}(x_{j}-x_{1})^{2}+\\left[n-{\\cfrac{B^{k+1}-1}{B-1}}-B^{k}n_{0}\\right](x_{n_{0}+2}-x_{1})^{2}\\right)}\\end{array}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\tilde{\\mathbf{x}}=(x_{1},\\cdots,x_{n_{0}+2})$ is the truncated $\\mathbf{x}$ , and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Sigma=\\left(\\begin{array}{c c c c}{n-\\frac{B^{k+1}-1}{B-1}}&{-B^{k}}&{\\cdots}&{-\\left[n-\\frac{B^{k+1}-1}{B-1}-B^{k}n_{0}\\right]}\\\\ {-B^{k}}&{B^{k}}&&\\\\ {\\vdots}&&{\\ddots}&\\\\ {-\\left[n-\\frac{B^{k+1}-1}{B-1}-B^{k}n_{0}\\right]}&&&{\\left[n-\\frac{B^{k+1}-1}{B-1}-B^{k}n_{0}\\right]}\\end{array}\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the unspecified elements are all zeros. Since $\\Sigma$ is symmetric, all the eigenvalues are real. We show by contradiction that any eigenvalue $\\lambda$ of $\\Sigma$ is upper bounded by $n$ . Otherwise, if there exists $\\lambda>n$ , we denote $\\mathbf{x}$ as the corresponding eigenvector of $\\lambda$ . Then, we have from $\\Sigma\\mathbf{x}=\\lambda\\mathbf{x}$ that ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\begin{array}{l}{{\\lambda}x_{1}=\\left(n-{\\cfrac{B^{k+1}-1}{B-1}}\\right)x_{1}-B^{k}x_{2}-\\cdots-\\left[n-{\\cfrac{B^{k+1}-1}{B-1}}-B^{k}n_{0}\\right]x_{n_{0}+2}}\\\\ {{\\lambda}x_{2}=-B^{k}x_{1}+B^{k}x_{2}}\\\\ {{\\lambda}x_{3}=-B^{k}x_{1}+B^{k}x_{3}}\\\\ {\\qquad\\cdots}\\\\ {{\\vdots}}\\\\ {{\\overleftarrow{n}}_{0}+2=-\\left[n-{\\cfrac{B^{k+1}-1}{B-1}}-B^{k}n_{0}\\right]x_{1}+\\left[n-{\\cfrac{B^{k+1}-1}{B-1}}-B^{k}n_{0}\\right]x_{n_{0}+2}.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Without loss of generality, assume $x_{1}\\neq0$ . Then, by substituting the other relations into the first one, wehave ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\lambda=n-\\frac{B^{k+1}-1}{B-1}+\\sum_{i=1}^{n_{0}}\\frac{B^{2k}}{\\lambda-B^{k}}+\\frac{\\left[n-\\frac{B^{k+1}-1}{B-1}-B^{k}n_{0}\\right]^{2}}{\\lambda-\\left[n-\\frac{B^{k+1}-1}{B-1}-B^{k}n_{0}\\right]}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "With the fact that $\\lambda>n$ , there holds ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda\\leq n-\\frac{B^{k+1}-1}{B-1}+\\frac{n_{0}B^{2k}}{n-B^{k}}+\\frac{\\Big[n-\\frac{B^{k+1}-1}{B-1}-B^{k}n_{0}\\Big]^{2}}{n-\\Big[n-\\frac{B^{k+1}-1}{B-1}-B^{k}n_{0}\\Big]}}\\\\ &{\\quad=n-\\frac{B^{k+1}-1}{B-1}+\\frac{n_{0}B^{2k}}{n-B^{k}}+\\frac{n^{2}}{\\frac{B^{k+1}-1}{B-1}+B^{k}n_{0}}-2n+\\frac{B^{k+1}-1}{B-1}+B^{k}n_{0}}\\\\ &{\\quad=\\frac{n B^{k}n_{0}}{n-B^{k}}+\\frac{n\\Big(n-\\frac{B^{k+1}-1}{B-1}-B^{k}n_{0}\\Big)}{\\frac{B^{k+1}-1}{B-1}+B^{k}n_{0}}}\\\\ &{\\quad\\leq\\frac{n B^{k}n_{0}}{n-B^{k}}+\\frac{n}{n-B^{k}}\\left(n-\\frac{B^{k+1}-1}{B-1}-B^{k}n_{0}\\right)}\\\\ &{\\quad=n\\frac{n-B^{k+1}-1}{n-B^{k}}<n,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is a contradiction. Thus, we have $\\lambda\\leq n$ . It follows that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\tilde{\\mathbf{x}}^{\\top}\\Sigma\\tilde{\\mathbf{x}}}{\\|\\mathbf{x}\\|^{2}}\\leq\\frac{\\tilde{\\mathbf{x}}^{\\top}\\Sigma\\tilde{\\mathbf{x}}}{\\|\\tilde{\\mathbf{x}}\\|^{2}}\\leq\\lambda_{\\operatorname*{max}}(\\Sigma)\\leq n.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "From the fact that the square root function is monotonically increasing on $[0,\\infty)$ ,wehave ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\mathcal{R}^{k}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\|_{2}^{2}=\\operatorname*{max}_{\\|\\mathbf{x}\\|_{2}=1}\\left\\{\\|\\left(\\mathcal{R}^{k}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)\\mathbf{x}\\|_{2}^{2}\\right\\}\\leq n,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which implies that $\\begin{array}{r}{\\|\\mathcal{R}^{k}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\|_{2}\\leq\\sqrt{n}}\\end{array}$ for $k\\leq d-1$ and $\\begin{array}{r}{\\|\\mathcal{R}^{k}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\|_{2}=0}\\end{array}$ otherwise by Corollary A.2. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "The transformations described in Corollary A.3 below are straightforward. ", "page_idx": 14}, {"type": "text", "text": "Corollary A.3. For any integer $m>0$ ,we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Pi_{\\mathbf{u}}\\mathcal{R}=\\Pi_{\\mathbf{u}}\\left(\\mathcal{R}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)=\\left(\\mathcal{R}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)\\Pi_{\\mathbf{u}},}\\\\ &{\\quad\\Pi_{\\mathbf{u}}\\mathcal{R}^{m}=\\Pi_{\\mathbf{u}}\\left(\\mathcal{R}^{m}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)=\\Pi_{\\mathbf{u}}\\left(\\mathcal{R}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)^{m}=\\left(\\mathcal{R}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)^{m}\\Pi_{\\mathbf{u}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To simplify the convergence analysis, we introduce the matrix $\\mathbf{A}_{i}$ defined as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{A}_{i}=\\mathcal{C}^{i}-\\mathcal{C}^{i-1}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for $i=1,2,\\cdots,d.$ Specifically, $\\mathbf{A}_{1}=\\boldsymbol{\\mathcal{C}}-\\mathbf{I}$ . Consequently, Corollary A.4 below can be directly deduced from Lemma A.1 and Corollary A.3. ", "page_idx": 14}, {"type": "text", "text": "Corollary A.4. For $i=1,\\cdots,d,$ we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left({\\frac{\\mathbf{u}^{\\top}}{n}}-\\mathbf{1}^{\\top}\\right)\\mathbf{A}_{i}={\\left\\{\\begin{array}{l l}{\\mathbf{e}_{\\frac{B^{i}-1}{B-1}+1:{\\frac{B^{i+1}-1}{B-1}}}^{\\top}}&{i\\leq d-1}\\\\ {\\qquad\\mathbf{e}_{\\frac{B^{d}-1}{B-1}+1:n}^{\\top}}&{i=d}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Intuitively, Corollary A.4 illustrates that $\\left(\\frac{\\mathbf{u}^{\\top}}{n}-\\mathbf{1}^{\\top}\\right)\\mathbf{A}_{i}$ serves as an indicator vector representing the $(i+1)$ -th layer of the graph. ", "page_idx": 14}, {"type": "text", "text": "A.2  Supporting Inequalities and Lemmas ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma A.5 and Lemma A.6 below are frequently employed for bounding the norms of matrix summations and multiplications. Their proofs rely on the Cauchy-Schwartz inequality and the definitions of matrix norms $\\|\\cdot\\|_{2}$ and $\\|\\cdot\\|_{F}$ ", "page_idx": 15}, {"type": "text", "text": "Lemma A.5. For an arbitrary set of m matrices $\\{\\mathbf{A}_{i}\\}_{i=1}^{m}$ with the same size, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{i=1}^{m}\\mathbf{A}_{i}\\right\\|_{F}^{2}\\leq m\\sum_{i=1}^{m}\\left\\|\\mathbf{A}_{i}\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. By the definition of Frobenius norm, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{i=1}^{m}\\mathbf{A}_{i}\\right\\|_{F}\\leq\\sum_{i=1}^{m}\\left\\|\\mathbf{A}_{i}\\right\\|_{F}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Taking squares on both sides and invoking the Cauchy-Schwarz inequality, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{i=1}^{m}\\mathbf{A}_{i}\\right\\|_{F}^{2}\\leq\\left(\\sum_{i=1}^{m}\\|\\mathbf{A}_{i}\\|_{F}\\right)^{2}\\leq m\\sum_{i=1}^{m}\\|\\mathbf{A}_{i}\\|_{F}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma A.6. Let A, B be two real matrices whose sizes match. Then, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\mathbf{AB}\\|_{F}\\leq\\|\\mathbf{A}\\|_{2}\\,\\|\\mathbf{B}\\|_{F}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Let $\\mathbf{A}=\\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^{H}$ be the singular value decomposition of $\\mathbf{A}$ , with the largest singular value $\\sigma_{\\mathrm{max}}$ and hence $\\left\\|\\mathbf{A}\\right\\|_{2}=\\sigma_{\\operatorname*{max}}$ . Then, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{A}\\mathbf{B}\\right\\|_{F}^{2}=\\left\\|\\mathbf{U}\\Sigma\\mathbf{V}^{H}\\mathbf{B}\\right\\|_{F}^{2}=\\operatorname{trace}\\left(\\left(\\mathbf{U}\\Sigma\\mathbf{V}^{H}\\mathbf{B}\\right)^{H}\\left(\\mathbf{U}\\Sigma\\mathbf{V}^{H}\\mathbf{B}\\right)\\right)}\\\\ &{\\qquad\\qquad=\\operatorname{trace}\\left(\\left(\\Sigma\\mathbf{V}^{H}\\mathbf{B}\\right)^{H}\\left(\\Sigma\\mathbf{V}^{H}\\mathbf{B}\\right)\\right)=\\left\\|\\Sigma\\mathbf{V}^{H}\\mathbf{B}\\right\\|_{F}^{2}}\\\\ &{\\qquad\\qquad\\leq\\sigma_{\\operatorname*{max}}^{2}\\|\\mathbf{V}^{H}\\mathbf{B}\\|_{F}^{2}=\\sigma_{\\operatorname*{max}}^{2}\\mathrm{trace}\\left(\\mathbf{B}^{\\top}\\mathbf{V}\\mathbf{V}^{H}\\mathbf{B}\\right)}\\\\ &{\\qquad=\\sigma_{\\operatorname*{max}}^{2}\\mathrm{trace}\\left(\\mathbf{B}^{\\top}\\mathbf{B}\\right)=\\sigma_{\\operatorname*{max}}^{2}\\left\\|\\mathbf{B}\\right\\|_{F}^{2}}\\\\ &{\\qquad=\\left\\|\\mathbf{A}\\right\\|_{2}^{2}\\left\\|\\mathbf{B}\\right\\|_{F}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which implies the desired result ", "page_idx": 15}, {"type": "text", "text": "Lemma A.7 below will be used in the last step for deriving the convergence rate of BTPP. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.7. Let $A,B,C$ and $\\alpha$ be positive constants and $T$ be a positive integer Define function ", "page_idx": 15}, {"type": "equation", "text": "$$\ng(\\gamma)=\\frac{A}{\\gamma(T+1)}+B\\gamma+C\\gamma^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\gamma\\in(0,\\frac{1}{\\alpha}]}g(\\gamma)\\leq2\\left(\\frac{A B}{T+1}\\right)^{\\frac{1}{2}}+2C^{\\frac{1}{3}}\\left(\\frac{A}{T+1}\\right)^{\\frac{2}{3}}+\\frac{\\alpha A}{T+1},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where theuper bound an e achieved by chosing $\\begin{array}{r}{\\gamma=\\operatorname*{min}\\Bigg\\{\\left(\\frac{A}{B(T+1)}\\right)^{\\frac{1}{2}},\\left(\\frac{A}{C(T+1)}\\right)^{\\frac{1}{3}},\\frac{1}{\\alpha}\\Bigg\\}.}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Proof. See Lemma 26 in [8] for a reference. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.8 is a technical result related to random variables. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.8. Consider three random variables $X,Y,$ and $Z$ Assumethat $Z$ is independentwith $(X,Y)$ . Let h and g be functions such that the conditional expectation $\\mathbb{E}[g(Y,Z)~|~Y]=0$ Wehave ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left(h(X)g(Y,Z)\\right)=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. It implies by the condition $Z\\perp\\!\\!\\!\\perp(X,Y)$ that $\\sigma(Z)$ $\\sigma(X,Y)$ . Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}\\left[h(X)g(Y,Z)|Y\\right]=\\mathbb{E}\\left\\{\\mathbb{E}\\left[h(X)g(Y,Z)|X,Y\\right]|Y\\right\\}\\,}\\\\ &{}&{=\\mathbb{E}\\left\\{h(X)\\mathbb{E}\\left[g(Y,Z)|X,Y\\right]|Y\\right\\}.\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It suffices to show ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname{\\mathbb{E}}\\left[g(Y,Z)|X,Y\\right]=\\operatorname{\\mathbb{E}}\\left[g(Y,Z)|Y\\right](=0).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let $f_{g}(y)=\\mathbb{E}\\left(g(y,Z)\\right)$ . Since $\\sigma(Z)$ $\\sigma(X,Y)$ , we have $\\sigma(Z)$ $\\sigma(Y)$ . Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{g}(Y)=\\mathbb{E}\\left(g(Y,Z)|Y\\right)=\\mathbb{E}\\left[g(Y,Z)|X,Y\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which follows directly from (10.17) in [24]. Thus, by the Tower Rule, we reach the statement as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left(h(X)g(Y,Z)\\right)=\\mathbb{E}\\left\\{\\mathbb{E}\\left(h(X)g(Y,Z)|Y\\right)\\right\\}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.3 Proofs of Key Lemmas ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we prove several key lemmas for proving the main convergence result of BTPP. ", "page_idx": 16}, {"type": "text", "text": "A.3.1 Preparation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Algorithm 1, as encapsulated by the equations in (2), can be succinctly expressed in the following matrix form: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{c}{\\mathbf{X}^{(t+1)}}\\\\ {\\mathbf{Y}^{(t+1)}}\\end{array}\\right)=\\left(\\begin{array}{c c}{\\mathcal{R}}&{-\\gamma\\mathcal{R}}\\\\ {\\mathbf{0}}&{\\mathcal{C}}\\end{array}\\right)\\left(\\begin{array}{c}{\\mathbf{X}^{(t)}}\\\\ {\\mathbf{Y}^{(t)}}\\end{array}\\right)+\\left(\\begin{array}{c}{\\mathbf{0}}\\\\ {\\mathbf{G}^{(t+1)}-\\mathbf{G}^{(t)}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By repeatedly applying equation (5) starting from time step $t$ and going back to time step O, we arrive at the following relation: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{c}{\\mathbf{X}^{\\left(t\\right)}}\\\\ {\\mathbf{Y}^{\\left(t\\right)}}\\end{array}\\right)=\\left(\\begin{array}{c c}{\\mathcal{R}}&{-\\gamma\\mathcal{R}}\\\\ {\\mathbf{0}}&{\\mathcal{C}}\\end{array}\\right)^{t}\\left(\\begin{array}{c}{\\mathbf{X}^{\\left(0\\right)}}\\\\ {\\mathbf{Y}^{\\left(0\\right)}}\\end{array}\\right)+\\sum_{m=0}^{t-1}\\left(\\begin{array}{c c}{\\mathcal{R}}&{-\\gamma\\mathcal{R}}\\\\ {\\mathbf{0}}&{\\mathcal{C}}\\end{array}\\right)^{t-m-1}\\left(\\begin{array}{c}{\\mathbf{0}}\\\\ {\\mathbf{G}^{\\left(m+1\\right)}-\\mathbf{G}^{\\left(m\\right)}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For the sake of clarity, we start with introducing some simple definitions. Any matrix raised to the power of O is defined as the identity matrix $\\mathbf{I}$ which matches the original matrix in dimension. The only exceptions are $\\begin{array}{r}{\\left(\\mathcal{R}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)^{0}:=\\mathbf{II_{u}}}\\end{array}$ and $\\begin{array}{r}{\\left(\\mathcal{C}-\\frac{1}{n}\\mathbf{v}\\mathbf{1}^{\\top}\\right)^{0}:=\\bar{\\Pi_{\\mathbf{v}}}}\\end{array}$ for convenience. Furthermore, we introduce the following terms: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{X}}^{(t)}:=\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\mathbf{X}^{(t)},\\;\\bar{\\mathbf{Y}}^{(t)}:=\\frac{1}{n}\\mathbf{v}\\mathbf{1}^{\\top}\\mathbf{Y}^{(t)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that, for any given integer $m>0$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{c c}{\\mathcal{R}}&{-\\gamma\\mathcal{R}}\\\\ {\\mathbf{0}}&{\\mathcal{C}}\\end{array}\\right)^{m}=\\left(\\begin{array}{c c}{\\mathcal{R}^{m}}&{-\\gamma\\sum_{j=1}^{m}\\mathcal{R}^{j}\\mathcal{C}^{m-j}}\\\\ {\\mathbf{0}}&{\\mathcal{C}^{m}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As a result, given the initial condition $\\mathbf{Y}^{(0)}=\\mathbf{G}^{(0)}$ , we can deduce the outcomes of $\\mathbf{X}^{(t)}$ and $\\mathbf{Y}^{(t)}$ as follows. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\bf X}^{(t)}=\\mathscr{R}^{t}{\\bf X}^{(0)}-\\gamma\\displaystyle\\sum_{m=0}^{t-2}\\sum_{j=1}^{t-m-1}\\mathscr{R}^{j}\\mathscr{C}^{t-m-1-j}\\left[{\\bf G}^{(m+1)}-{\\bf G}^{(m)}\\right]-\\gamma\\displaystyle\\sum_{j=1}^{t}\\mathscr{R}^{j}\\mathscr{C}^{t-j}{\\bf G}^{(0)},}\\\\ &{{\\bf Y}^{(t)}=\\displaystyle\\sum_{m=0}^{t-1}\\mathscr{C}^{t-m-1}\\left[{\\bf G}^{(m+1)}-{\\bf G}^{(m)}\\right]+\\mathscr{C}^{t}{\\bf G}^{(0)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, after multiplying $\\Pi_{\\mathbf{u}}$ and $\\Pi_{\\mathbf{v}}$ to equation (6) and equation (7) respectively, and invoking Corollary A.3, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Pi_{\\mathbf{u}}\\mathbf{X}^{(i)}=\\left(\\mathcal{R}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)^{t}\\mathbf{X}^{(i)}}&{\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A.3.2 Analysis of the Variance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Denote by $\\mathcal{F}_{t}$ the $\\sigma$ -algebra generated by $\\{\\xi_{0},\\cdot\\cdot\\cdot,\\xi_{t-1}\\}$ , and define $\\mathbb{E}\\left[\\cdot|\\mathcal{F}_{t}\\right]$ as the conditional expectation given $\\mathcal{F}_{t}$ . Lemma A.9 provides an estimate for the variance of the gradient estimator $G\\bar{(\\mathbf{X}^{(t)},\\pmb{\\xi}^{(t)})}$ ", "page_idx": 17}, {"type": "text", "text": "Lemma A.9. Under Assumption 1.1, for any given power $k\\leq d-1$ wehave for all $t\\geq0$ that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|\\left(\\bigmathcal{C}-\\frac{1}{n}\\mathbf{v1}^{\\top}\\right)^{k}\\left(\\mathbf{G}(\\mathbf{X}^{(t)},\\xi^{(t)})-\\nabla\\mathbf{F}(\\mathbf{X}^{(t)})\\right)\\right\\|_{F}^{2}\\,|\\,\\mathcal{F}_{t}\\right]\\leq2n\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. For any given $t$ and $i\\neq j$ , due to the independently drawn sample $\\xi_{i}^{(t)}$ ), we have that (t) is independent of $(\\mathcal{F}_{t},\\xi_{j}^{(t)})$ , and thus $\\xi_{i}^{(t)}$ is independent of (a, 2,) . Hence, invoking Lemma A.8 and Assumption 1.1 yields ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\nabla F(x_{i}^{(t)};\\xi_{i}^{(t)})-\\nabla f_{i}(x_{i}^{(t)})\\bigg\\vert x_{i}^{(t)}\\right]=\\mathbb{E}\\left[\\nabla F(x_{i}^{(t)};\\xi_{i}^{(t)})-\\nabla f_{i}(x_{i}^{(t)})\\bigg\\vert\\mathcal{F}_{t}\\right]=0,}\\\\ &{\\mathbb{E}\\left\\langle\\nabla F(x_{i}^{(t)};\\xi_{i}^{(t)})-\\nabla f_{i}(x_{i}^{(t)}),\\nabla F(x_{j}^{(t)};\\xi_{j}^{(t)})-\\nabla f_{j}(x_{j}^{(t)})\\right\\rangle=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, for any index set ${\\mathcal{T}}\\subseteq\\{1,2,\\cdots\\,,n\\}$ wehave $\\begin{array}{r}{\\mathbb{E}\\|\\mathbf{e}_{\\mathcal{T}}^{\\top}\\left(\\mathbf{G}^{(t)}-\\nabla\\mathbf{F}(\\mathbf{X}^{(t)})\\right)\\|_{2}^{2}\\leq|\\mathcal{T}|\\sigma^{2}.}\\end{array}$ Notice that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\left(\\boldsymbol{\\mathcal{C}}-\\frac{1}{n}{\\mathbf{v}}{\\mathbf{1}}^{\\top}\\right)^{k}\\left({\\mathbf{G}}^{(t)}-\\nabla{\\mathbf{F}}({\\mathbf{X}}^{(t)})\\right)\\right\\|_{F}^{2}=\\left\\|{\\mathbf{e}}_{\\frac{B^{k+1}-1}{B-1}+1:n}\\left({\\mathbf{G}}^{(t)}-\\nabla{\\mathbf{F}}({\\mathbf{X}}^{(t)})\\right)\\right\\|_{2}^{2}}\\\\ {+\\displaystyle\\sum_{i=2}^{n}\\left\\|{\\mathbf{e}}_{\\mathcal{X}_{i,k}}\\left({\\mathbf{G}}^{(t)}-\\nabla{\\mathbf{F}}({\\mathbf{X}}^{(t)})\\right)\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, we obtain the desired result by invoking Lemma A.1 and Corollary A.2 after taking expectation on both sides of the above relation: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|\\left(\\mathcal{C}-\\frac{1}{n}\\mathbf{v1}^{\\top}\\right)^{j}\\left(\\mathbf{G}^{(t)}-\\nabla\\mathbf{F}(\\mathbf{X}^{(t)})\\right)\\right\\|_{F}^{2}\\leq2\\left(n-\\frac{B^{k+1}-1}{B-1}\\right)\\sigma^{2}\\leq2n\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Under Assumption 1.1 and the randomly selected samples, Lemma A.9 and Corollary A.10 below provide an initial estimation for the variance terms. The proof of Corollary A.10 is directly from the analysis in Appendix A.3.2 and Corollary A.4. ", "page_idx": 18}, {"type": "text", "text": "Corollary A.10. Under Assumption 1.1, we have for all $t\\geq0$ that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{d}\\mathbb{E}\\|\\left(\\frac{\\mathbf{u}^{\\top}}{n}-\\mathbf{1}^{\\top}\\right)A_{k}\\left(\\mathbf{G}(\\mathbf{X}^{(t)},\\pmb{\\xi}^{(t)})-\\nabla\\mathbf{F}(\\mathbf{X}^{(t)})\\right)\\|_{2}^{2}\\leq(n-1)\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "A.3.3 Proof of Lemma 3.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. Notice that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{{\\mathbf{X}}}^{(t+1)}-\\bar{{\\mathbf{X}}}^{(t)}=-\\gamma\\frac{1}{n}{\\mathbf{1}}{\\mathbf{u}}^{\\top}{\\mathbf{Y}}^{(t)}=-\\gamma\\frac{1}{n}{\\mathbf{1}}{\\mathbf{u}}^{\\top}\\left[{\\mathbf{I}}_{\\mathbf{v}}{\\mathbf{Y}}^{(t)}+\\frac{1}{n}{\\mathbf{v}}{\\mathbf{1}}^{\\top}{\\mathbf{Y}}^{(t)}\\right]}\\\\ &{\\qquad=-\\gamma\\frac{1}{n}{\\mathbf{1}}{\\mathbf{u}}^{\\top}{\\mathbf{I}}_{\\mathbf{v}}{\\mathbf{Y}}^{(t)}-\\gamma{\\mathbf{1}}{\\mathbf{1}}^{\\top}{\\mathbf{Y}}^{(t)}=-\\gamma{\\mathbf{1}}\\left(\\frac{{\\mathbf{u}}^{\\top}}{n}-{\\mathbf{1}}^{\\top}\\right){\\mathbf{I}}_{\\mathbf{v}}{\\mathbf{Y}}^{(t)}-\\gamma{\\mathbf{1}}{\\mathbf{1}}^{\\top}{\\mathbf{Y}}^{(t)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Multiplying ${\\bf1^{\\top}}$ on both sides of equation (7), we have $\\mathbf{1}^{\\top}\\mathbf{Y}^{(t)}=\\mathbf{1}^{\\top}\\mathbf{G}^{(t)}$ for any integer $t$ Thus, in light of equation (9), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(\\mathbf{t}+\\mathbf{1})-\\mathbf{X}^{(i)}=-\\gamma\\cdot\\left(\\mathbf{1}\\left(\\mathbf{\\Delta}\\mathbf{u}^{\\mathrm{T}}\\right)-\\mathbf{1}^{\\mathrm{T}}\\right)\\mathbf{T}_{\\infty}\\mathbf{V}^{(i)}}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, taking the F-norm and expectation on both sides, we have from Lemma A.5 that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|\\bar{{\\mathbf{X}}}^{(t+1)}-\\bar{{\\mathbf{X}}}^{(t)}\\|_{F}^{2}\\leq5\\gamma^{2}n\\mathbb{E}\\bigg\\|\\displaystyle\\sum_{m=1}^{\\lfloor t\\rfloor\\operatorname*{min}\\{t,d\\}}\\bigg(\\displaystyle\\frac{{\\mathbf{u}}^{\\top}}{n}-1^{\\top}\\bigg)\\operatorname{A}_{m}\\bigg({\\mathbf{G}}^{(t-m)}-\\nabla{\\mathbf{F}}({\\mathbf{X}}^{(t-m)})\\bigg)\\bigg\\|_{F}^{2}}\\\\ &{\\qquad+5\\gamma^{2}n\\mathbb{E}\\bigg\\|\\bigg(\\displaystyle\\frac{{\\mathbf{u}}^{\\top}}{n}-1^{\\top}\\bigg)\\operatorname*{max}{\\displaystyle\\sum_{m=\\operatorname*{max}\\{0,t-d\\}}^{t-1}\\bigg(C-\\frac{1}{n}{\\mathbf{v}}^{\\top}\\bigg)^{t-m-1}\\left[\\nabla{\\mathbf{F}}({\\mathbf{X}}^{(m+1)})-\\nabla{\\mathbf{F}}({\\mathbf{X}}^{(m)})\\right]}}\\\\ &{\\qquad+5\\gamma^{2}n\\mathbb{E}\\bigg\\|\\bigg(\\displaystyle\\frac{{\\mathbf{u}}^{\\top}}{n}-1^{\\top}\\bigg)\\bigg({\\mathcal{C}}-\\frac{1}{n}{\\mathbf{v}}{\\mathbf{I}}^{\\top}\\bigg)^{t}\\nabla{\\mathbf{F}}({\\mathbf{X}}^{(0)})\\bigg\\|_{F}^{2}}\\\\ &{\\qquad+5\\gamma^{2}n\\mathbb{E}\\bigg\\|\\displaystyle\\frac{{\\mathbf{u}}^{\\top}}{n}\\left(\\nabla{\\mathbf{F}}({\\mathbf{X}}^{(t)})-{\\mathbf{G}}^{(t)}\\right)\\bigg\\|_{F}^{2}+5\\gamma^{2}n\\mathbb{E}\\bigg\\|{\\mathbf{1}}^{\\top}\\mathbf{F}({\\mathbf{X}}^{(t)})\\bigg\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that, invoking Lemma A.9 and Corollary A.10 yields ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|\\displaystyle\\sum_{m=1}^{\\operatorname*{min}\\{t,d\\}}\\left(\\frac{\\mathbf{u}^{\\top}}{n}-\\mathbf{1}^{\\top}\\right)\\mathbf{A}_{m}\\left(\\mathbf{G}^{(t-m)}-\\nabla\\mathbf{F}(\\mathbf{X}^{(t-m)})\\right)\\right\\|_{F}^{2}}\\\\ &{=\\displaystyle\\sum_{m=1}^{\\operatorname*{min}\\{t,d\\}}\\mathbb{E}\\left\\|\\left(\\frac{\\mathbf{u}^{\\top}}{n}-\\mathbf{1}^{\\top}\\right)\\mathbf{A}_{m}\\left(\\mathbf{G}^{(t-m)}-\\nabla\\mathbf{F}(\\mathbf{X}^{(t-m)})\\right)\\right\\|_{F}^{2}}\\\\ &{\\leq(n-1)\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "From Assumption 1.2, Lemma A.6 and Lemma A.5, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|\\left(\\frac{\\mathbf{u}^{\\top}}{n}-\\mathbf{I}^{\\top}\\right)\\ldots\\right\\|_{m=\\operatorname*{max}\\{0,t-d\\}}^{t-1}\\left(\\mathcal{C}-\\frac{1}{n}\\mathbf{v}^{\\top}\\right)^{t}{\\mathrm{,-}m}\\left[\\nabla\\mathbf{F}(\\mathbf{X}^{(m+1)})-\\nabla\\mathbf{F}(\\mathbf{X}^{(m+1)})\\right]\\right\\|_{F}^{2}}\\\\ &{\\leq d\\frac{t-1}{m-m\\operatorname*{max}\\{0,t-d\\}}\\mathbb{E}\\left\\|\\left(\\frac{\\mathbf{u}^{\\top}}{n}-\\mathbf{I}^{\\top}\\right)\\left(\\mathcal{C}-\\frac{1}{n}\\mathbf{v}^{\\top}\\right)^{t-m-1}\\left[\\nabla\\mathbf{F}(\\mathbf{X}^{(m+1)})-\\nabla\\mathbf{F}(\\mathbf{X}^{(m+1)})\\right]\\right\\|_{F}^{2}}\\\\ &{\\leq d\\frac{t-1}{m-m\\operatorname*{max}\\{0,t-d\\}}\\mathbb{E}\\left\\|\\left(\\frac{\\mathbf{u}^{\\top}}{n}-\\mathbf{I}^{\\top}\\right)\\left(\\mathcal{C}-\\frac{1}{n}\\mathbf{v}^{\\top}\\right)^{t-m-1}\\right\\|_{2}^{2}\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(m+1)})-\\nabla\\mathbf{F}(\\mathbf{X}^{(m)})\\right\\|_{F}^{2}}\\\\ &{\\overset{\\leq n}m{\\mathrm{.~}}^{t-1}\\underbrace{\\frac{t-1}{2}}_{\\operatorname*{max}\\{0,t-d\\}}\\mathbb{E}\\left\\|\\mathbf{X}^{(m+1)}-\\mathbf{X}^{(m)}\\right\\|_{F}^{2}}\\\\ &{\\leq n d L^{2}\\underbrace{\\sum_{j=1}^{t-1}}_{\\operatorname*{max}\\{0,t-d\\}}\\left(\\mathbb{E}\\left\\|\\mathbf{X}^{(m+1)}-\\mathbf{X}^{(m+1)}\\right\\|_{F}^{2}}\\\\ &{\\leq n d L^{2}\\underbrace{\\sum_{j=1}^{t-1}}_{\\operatorname*{max}\\{0,t-d\\}}3\\left(\\mathbb{E}\\left\\|\\mathbf{X}^{(m+1)}-\\mathbf{\\tilde{X}}^{(m+1)}\\right\\|_{F}^{2}+\\mathbb{E}\\left\\|\\mathbf{X}^{(m)}-\\bar{\\mathbf{X}}^{(m)}\\right\\|_{F}^{2}+\\mathbb\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, summing over $t$ in (10) from 0 to $T$ , combining all the inequalities above, and invoking Assumption 1.1 and Assumption 1.2, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=0}^{T}\\mathbb{E}\\|\\mathbf{S}^{(t+1)}-\\mathbf{X}^{(t)}\\|_{F}^{2}\\leq5^{\\prime}2^{n}(n-1)\\sigma^{2}(T+1)+307^{2}n^{2}d^{2}L\\sum_{t=0}^{T}\\mathbb{E}\\Big\\|\\mathbf{I}_{\\mathbf{u}}\\mathbf{X}^{(t)}\\Big\\|_{F}^{2}}\\\\ &{\\quad+15^{\\prime}2^{n}d^{2}L^{2}\\displaystyle\\sum_{s=0}^{T}\\mathbb{E}\\Big\\|\\mathbf{X}^{(t+1)}-\\mathbf{X}^{(t)}\\Big\\|_{F}^{2}}\\\\ &{\\qquad\\qquad\\operatorname*{min}\\left(\\underbrace{A(t-1)}_{\\ensuremath{\\mathbb{R}}}\\mathbb{E}\\Big(\\mathbf{X}^{(t)}\\Big)\\Big\\|_{F}^{2}+5^{\\prime}2^{n}n^{2}(T+1)\\right.}\\\\ &{\\qquad+\\left.5\\gamma^{2}n^{2}\\displaystyle\\sum_{c=0}^{T}\\left(2\\mathbb{E}\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(t)})\\right\\|_{m}^{2}+17\\nabla\\mathbf{F}(\\mathbf{X}^{(t)})\\right\\|_{2}^{2}+2n^{2}\\mathbb{E}\\left\\|\\frac{1}{n}^{1}\\nabla\\mathbf{F}(\\mathbf{X}^{(t)})\\right\\|_{2}^{2}\\right)}\\\\ &{\\qquad+5\\gamma^{2}n^{2}\\upsilon\\left(\\!\\!\\ \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\begin{array}{r}{\\gamma\\leq\\frac{1}{10n d L}}\\end{array}$ , we have $\\textstyle15\\gamma^{2}n^{2}d^{2}L^{2}\\leq{\\frac{1}{6}}$ , and the desired result follows. ", "page_idx": 19}, {"type": "text", "text": "A.3.4Proof of Lemma 3.3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. We show the upper bound for $\\mathbb{E}\\left\\|\\mathbf{\\boldsymbol{\\Pi}}\\mathbf{u}\\mathbf{X}^{(t)}\\right\\|_{F}^{2}$ by studying equation (8) and bound the F-norm of each term respectively. From Corollary A.2, we can change the power of $\\begin{array}{r}{{\\mathcal{R}}-{\\frac{1}{n}}\\mathbf{1}\\mathbf{u}^{\\top}}\\end{array}$ to at most ", "page_idx": 19}, {"type": "text", "text": "$d-2$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{I}_{\\mathbf{u}}\\mathbf{X}^{(t)}=\\!(\\mathcal{R}^{t}-\\displaystyle\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top})\\mathbf{X}^{(0)}-\\gamma\\displaystyle\\sum_{m=0}^{t-2\\!\\!\\operatorname*{min}\\{t-m-1,d-1\\}}\\!(\\mathcal{R}-\\displaystyle\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top})^{j}\\mathcal{C}^{t-m-1-j}\\left[\\mathbf{G}^{(m+1)}-\\mathbf{G}^{(m)}\\right]}\\\\ &{\\qquad\\qquad\\qquad-\\gamma\\displaystyle\\sum_{j=1}^{\\operatorname*{min}\\{t,d-1\\}}\\left(\\mathcal{R}-\\displaystyle\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)^{j}\\mathcal{C}^{t-j}\\mathbf{G}^{(0)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, we derive the following decomposition by pairing the gradients with each of the stochastic gradients in order to use Assumption 1.1. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{\\frac{m!}{\\rho!}\\end{array}\\mathrm{Let}\\quad h}\\\\ &{=-\\frac{\\gamma+\\alpha}{2}\\quad\\mathrm{et}\\quad\\Biggl(\\kappa-\\frac{1}{\\eta}\\Biggr)^{2}\\mathrm{in}\\nabla^{2}\\alpha^{2}\\mathrm{Let}^{-2\\mathrm{i}k}\\nabla^{2}\\alpha^{2}}\\\\ &{-\\frac{\\rho!}{\\eta!}\\ \\Biggl(\\kappa-\\frac{1}{2}\\Biggr)^{2}\\left(\\kappa-\\frac{1}{2}\\right)^{2}\\mathrm{i}\\alpha^{2}\\mathrm{Let}^{-2\\mathrm{i}k}\\nabla^{2}\\alpha^{2}}\\\\ &{-\\frac{\\rho!}{\\rho!}\\ \\Biggl(\\kappa-\\frac{1}{2}\\Biggr)^{2}\\left(\\left(\\kappa-\\frac{1}{2}\\right)^{2}\\right)^{2}\\left(\\left(\\kappa-\\frac{1}{2}\\right)^{2}\\right)^{2}\\mathrm{et}\\nabla^{2}\\alpha^{2}}\\\\ &{-\\frac{\\rho!}{\\eta!}\\Biggl(\\kappa-\\frac{1}{2}\\Biggr)^{2}\\left(\\left(\\kappa-\\frac{1}{2}\\right)^{2}\\right)^{2}\\mathrm{i}\\alpha^{2}\\mathrm{Let}^{-\\mathrm{i}k}\\nabla^{2}\\alpha^{2}}\\\\ &{-\\gamma\\Biggr[\\frac{\\rho!}{\\rho!}\\Biggr]\\left(-\\frac{1}{2}\\Biggr)^{2}\\left(\\kappa-\\frac{1}{2}\\Biggr)^{2}\\mathrm{i}\\alpha^{2}\\mathrm{Let}^{-\\mathrm{i}k}\\nabla^{2}\\alpha^{2}}\\\\ &{-\\gamma\\Biggr[\\frac{\\rho!}{\\rho!}\\Biggr]\\left(\\kappa^{2}\\right)\\Biggr[(\\left(\\kappa-\\frac{1}{2}\\right)^{2}+\\kappa^{2})\\left(\\left(\\kappa-\\frac{1}{2}\\right)^{2}+\\kappa^{2}\\right)}\\\\ &{\\qquad-\\gamma\\Biggr[\\frac{\\rho!}{\\rho!}\\Biggr]\\left(\\kappa^{2}\\right)\\Biggr]\\times\\Biggl[\\left(\\kappa-\\frac{1}{2}\\right)^{2}\\Biggr(\\left(\\kappa-\\frac{1}{2}\\right)^{2}+\\kappa^{2}\\right)}\\\\ &{\\qquad-\\gamma\\Biggr[\\frac{\\rho!}{\\rho!}\\Biggr]\\left(\\kappa^{2}\\right)\\Biggr[(\\left(\\kappa-\\frac{1}{2}\\right)^{2}+\\kappa^{2})\\left(\\left(\\kappa-\\frac{1}{2}\\right)^{2}+\\kappa^{2}\\right)^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the terms from $\\mathbf{Q}_{0,t,1}$ to $\\mathbf{Q}_{0,t,4}$ and from $\\mathbf{Q}_{1,t}$ to $\\mathbf{Q}_{4,t}$ correspond to each term following $\\begin{array}{r}{\\left(\\mathcal{R}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)^{\\top}\\mathbf{X}^{(0)}}\\end{array}$ one-by-one. ", "page_idx": 20}, {"type": "text", "text": "We assume that $d\\geq2$ since $d=1$ makes the summation illegal (summing over $j$ from a positive number to a non-positive number), in which case $\\mathbf{I}_{\\mathbf{u}}\\mathbf{X}^{(t)}$ degenerates to $(\\mathcal{R}^{t}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top})\\mathbf{X}^{(0)}$ and hence by Corollary A.2, there is no consensus error, i.e. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T}\\left\\Vert\\mathbf{I}\\mathbf{u}\\mathbf{X}^{(t)}\\right\\Vert_{F}^{2}=0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For $d\\geq2$ Lemma A.11 - Lemma A.13 below introduce the upper bounds for the F-norms of $\\mathbf{Q}_{1,t}$ $\\mathbf{Q}_{2,t}$ and $\\mathbf{Q}_{3,t}+\\mathbf{Q}_{0,t,2}$ ,summing from $t=0$ to $T$ . Lemma A.14 establishes a similar upper bound for the $\\boldsymbol{\\mathrm{F}}.$ -norm of $\\mathbf{Q}_{0,t,1}+\\mathbf{Q}_{0,t,3}$ . Furthermore, Lemma A.15 provides the upper bound for the F-norm of $\\mathbf{Q}_{0,t,4}+\\mathbf{Q}_{4,t}$ ", "page_idx": 20}, {"type": "text", "text": "Lemma A.11. For any iteration number $T$ wehave ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T}\\mathbb{E}\\|\\mathbf{Q}_{1,t}\\|_{F}^{2}\\leq32n^{2}d^{4}(T+1)\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. To make the summation legal given $d\\geq2$ weneed $d-1\\geq t-m-d$ ,which implies that $m\\geq t+1-2d$ .Then, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Q}_{1,t}=\\displaystyle\\sum_{m=0}^{t-2\\mathrm{\\min}\\{t-m-1,d-1\\}}\\left(\\mathcal{R}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)^{j}\\left(\\mathcal{C}-\\frac{1}{n}\\mathbf{v}\\mathbf{1}^{\\top}\\right)^{t-m-1-j}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\left[\\mathbf{G}^{(m+1)}-\\nabla\\mathbf{F}(\\mathbf{X}^{(m+1)})+\\nabla\\mathbf{F}(\\mathbf{X}^{(m)})-\\mathbf{G}^{(m)}\\right]}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{m=\\mathrm{max}\\{t+1-2d,0\\}\\,j=\\mathrm{max}\\{1,t-m-d\\}}^{t-2\\mathrm{\\min}\\{t-m-1,d-1\\}}\\left(\\mathcal{R}-\\frac{1}{n}\\mathbf{u}^{\\top}\\right)^{j}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\left(\\mathcal{C}-\\frac{1}{n}\\mathbf{v}\\mathbf{1}^{\\top}\\right)^{t-m-1-j}\\left[\\mathbf{G}^{(m+1)}-\\nabla\\mathbf{F}(\\mathbf{X}^{(m+1)})+\\nabla\\mathbf{F}(\\mathbf{X}^{(m)})-\\mathbf{G}^{(m)}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Invoking Lemma A.5, Lemma 3.1 and Lemma A.9, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|\\mathbf{Q}_{1,t}\\|_{F}^{2}\\leq2(d-1)^{2}\\underset{m=\\operatorname*{max}\\{t+1-2d,0\\}}{\\overset{t-2}{\\sum}}\\underset{j=\\operatorname*{max}\\{1,t-m-d\\}}{\\operatorname*{min}}\\mathbb{E}\\left\\|\\bigg(\\mathcal{R}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\bigg)^{j}\\right\\|_{2}^{2}.}\\\\ &{\\quad\\quad\\quad\\left\\|\\bigg(\\mathcal{C}-\\frac{1}{n}\\mathbf{v}\\mathbf{1}^{\\top}\\bigg)^{t-m-1-j}\\left[\\mathbf{G}^{(m+1)}-\\nabla\\mathbf{F}(\\mathbf{X}^{(m+1)})+\\nabla\\mathbf{F}(\\mathbf{X}^{(m)})-\\mathbf{G}^{(m)}\\right]\\bigg\\|_{F}^{2}}\\\\ &{\\leq2n(d-1)^{2}\\underset{m=\\operatorname*{max}\\{t+1-2d,0\\}}{\\overset{t-1}{\\sum}}\\underset{j=\\operatorname*{max}\\{1,t-m-d\\}}{\\operatorname*{min}}\\mathbb{E}\\frac{(t-m-1,d-1)}{8n\\sigma^{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\left.\\leq\\rho\\mathbf{\\Omega}^{2}\\mathbf{\\Omega}^{2}\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Summing over $t$ from 0 to $T$ , we get the desired result. ", "page_idx": 21}, {"type": "text", "text": "Lemma A.12. For any $\\begin{array}{r}{\\gamma\\leq\\frac{1}{20n d L}}\\end{array}$ 20ndL, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\|\\mathbf{Q}_{2,t}\\|_{F}^{2}\\leq52n^{2}d^{4}L^{2}\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\Pi_{\\mathbf{u}}\\mathbf{X}^{(t)}\\right\\|_{F}^{2}+144\\gamma^{2}n^{4}d^{4}(T+1)L^{2}\\sigma^{2}}&{}\\\\ {\\displaystyle+\\left.360\\gamma^{2}n^{5}d^{4}L^{2}\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\frac{1}{n}\\nabla\\mathbf{F}(\\bar{\\mathbf{X}}^{(t)})\\right\\|_{2}^{2}+360\\gamma^{2}n^{4}d^{5}L^{2}\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Similar to the proof of Lemma A.11, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathfrak{Q}_{2,t}=\\sum_{m=\\operatorname*{max}\\{t+1-2d,0\\}}^{t-1}\\sum_{j=\\operatorname*{max}\\{1,t-m-d\\}}^{\\operatorname*{min}\\{t-m-1,d-1\\}}\\left(\\mathcal{R}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)^{j}\\left(\\mathscr{C}-\\frac{1}{n}\\mathbf{v}\\mathbf{1}^{\\top}\\right)^{t-m-1-j}\\left[\\nabla\\mathbf{F}(\\mathbf{X}^{(m+1)})-\\frac{1}{n}\\mathbf{v}\\mathbf{1}^{\\top}\\right].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Invoking Lemma A.5 and Assumption 1.2, we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|\\mathbf{Q}_{2,t}\\|_{F}^{2}\\leq4n^{2}d^{3}\\underset{m=\\operatorname*{max}\\{t+1-2d,0\\}}{\\overset{t-1}{\\sum}}\\mathbb{E}\\left[\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(m+1)})-\\nabla\\mathbf{F}(\\mathbf{X}^{(m)})\\|_{F}^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq4n^{2}d^{3}L_{\\underset{m=\\operatorname*{max}\\{t+1-2d,0\\}}{\\overset{t-1}{\\sum}}}^{\\begin{array}{r l}&{\\mathbb{E}}\\left\\|\\mathbf{X}^{(m+1)}-\\bar{\\mathbf{X}}^{(m+1)}+\\bar{\\mathbf{X}}^{(m)}-\\mathbf{X}^{(m)}+\\bar{\\mathbf{X}}^{(m+1)}-\\bar{\\mathbf{X}}^{(m)}\\right\\|}\\\\ &{\\qquad\\leq24n^{2}d^{3}L_{\\underset{m=\\operatorname*{max}\\{t+1-2d,0\\}}{\\overset{t}{\\sum}}}^{\\begin{array}{r l}&{\\mathbb{E}\\left\\|\\mathbf{T}_{\\mathbf{u}}\\mathbf{X}^{(t)}\\right\\|_{F}^{2}+12n^{2}d^{3}L_{\\underset{m=\\operatorname*{max}\\{t+1-2d,0\\}}{\\overset{t-1}{\\sum}}}\\mathbb{E}\\left\\|\\bar{\\mathbf{X}}^{(m+1)}\\right\\|_{F}}\\end{array}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It follows by summing over $t$ from 0 to $T$ and applying Lemma 3.2 that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\|\\mathbf{Q}_{2,t}\\|_{F}^{2}\\leq48n^{2}d^{4}L^{2}\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\Pi_{\\mathbf{u}}\\mathbf{X}^{(t)}\\right\\|_{F}^{2}+24n^{2}d^{4}L^{2}\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\bar{\\mathbf{X}}^{(t+1)}-\\bar{\\mathbf{X}}^{(t)}\\right\\|_{F}^{2}}\\\\ {\\leq\\left(48n^{2}d^{4}L^{2}+1200\\gamma^{2}n^{4}d^{6}L^{4}\\right)\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\Pi_{\\mathbf{u}}\\mathbf{X}^{(t)}\\right\\|_{F}^{2}+144\\gamma^{2}n^{4}d^{4}(T+1)L^{2}\\sigma^{2}}\\\\ {\\displaystyle+\\left.360\\gamma^{2}n^{5}d^{4}L^{2}\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\nabla f(x_{1}^{(t)})\\right\\|_{2}^{2}+144\\gamma^{2}n^{4}d^{5}L^{2}\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hene mdete codonthat $\\begin{array}{r}{\\gamma\\leq\\frac{1}{20n d L}}\\end{array}$ there olds $1200\\gamma^{2}n^{4}(d-1)^{6}L^{4}\\leq4n^{2}(d-1)^{4}L^{2}$ ", "page_idx": 22}, {"type": "text", "text": "Lemma A.13. For any $T$ ,we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T}\\mathbb{E}\\|\\mathbf{Q}_{3,t}+\\mathbf{Q}_{0,t,2}\\|_{F}^{2}\\leq d^{2}n^{2}(T+1)\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. By definition, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbf{Q}_{3,t}+\\mathbf{Q}_{0,t,2}=\\sum_{j=1}^{\\operatorname*{min}\\{t,d-1\\}}\\left(\\mathcal{R}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)^{j}\\frac{1}{n}\\mathbf{v}\\mathbf{1}^{\\top}\\left(\\mathbf{G}^{(0)}-\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right)+}\\\\ &{\\displaystyle\\sum_{m=0}^{t-2\\,\\operatorname*{min}\\{t-m-1,d-1\\}}\\left(\\mathcal{R}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)^{j}\\frac{\\mathbf{v}\\mathbf{1}^{\\top}}{n}\\left[\\mathbf{G}^{(m+1)}-\\nabla\\mathbf{F}(\\mathbf{X}^{(m+1)})+\\nabla\\mathbf{F}(\\mathbf{X}^{(m)})-\\mathbf{G}^{(m)}\\right]}\\\\ &{\\displaystyle=\\sum_{m=\\operatorname*{max}\\{t-d,0\\}}^{t-1}\\left(\\mathcal{R}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)^{t-m}\\frac{\\mathbf{v1}^{\\top}}{n}\\left[\\mathbf{G}^{(m)}-\\nabla\\mathbf{F}(\\mathbf{X}^{(m)})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, invoking Lemma A.5 and Lemma 3.1, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|\\mathbf{Q}_{3,t}\\|_{F}^{2}\\leq n d\\sum_{m=\\operatorname*{max}\\{t-d,0\\}}^{t-1}\\mathbb{E}\\|\\frac{\\mathbf{v}}{n}\\|_{2}^{2}\\cdot\\|\\mathbf{1}^{\\top}\\left(\\mathbf{G}^{(m+1)}-\\nabla\\mathbf{F}(\\mathbf{X}^{(m+1)})\\right)\\|_{F}^{2}\\leq d^{2}n^{2}\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "After summing over $t$ from 0 to $T$ , we get the desired result. ", "page_idx": 22}, {"type": "text", "text": "Lemma A.14. For any $T$ we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\mathbf{Q}_{0,t,1}+\\mathbf{Q}_{0,t,3}\\right\\|_{F}^{2}\\leq4n^{2}d^{3}\\sigma^{2}+4n^{2}d^{3}\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Note that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\mathbf{Q}_{0,t,1}+\\mathbf{Q}_{0,t,3}\\right\\|_{F}^{2}\\leq2\\left\\|\\mathbf{Q}_{0,t,1}\\right\\|_{F}^{2}+2\\left\\|\\mathbf{Q}_{0,t,3}\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We show the uper bounds for $\\sum_{t=0}^{T}\\left\\|\\mathbf{Q}_{0,t,i}\\right\\|_{F}^{2}$ where $i=1,3$ respectivly. Based on Corollary A.2, Lemma A.5 and Lemma A.9, we have the foliowing result: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{Q}_{0,t,1}\\|_{F}^{2}=\\left\\|\\displaystyle\\sum_{j=1}^{\\operatorname*{min}\\{t,d-1\\}}\\left(\\mathcal{R}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)^{j}\\left(\\boldsymbol{c}-\\frac{1}{n}\\mathbf{v1}^{\\top}\\right)^{t-j}\\left(\\mathbf{G}^{(0)}-\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right)\\right\\|_{F}^{2}}\\\\ &{\\qquad=\\left\\|\\displaystyle\\sum_{j=\\operatorname*{max}\\{1,t,d-d+1\\}}^{\\operatorname*{min}\\{t,d-1\\}}\\left(\\mathcal{R}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)^{j}\\left(\\boldsymbol{c}-\\frac{1}{n}\\mathbf{v1}^{\\top}\\right)^{t-j}\\left(\\mathbf{G}^{(0)}-\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right)\\right\\|_{F}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\operatorname*{min}\\{t,d-1\\}}\\\\ &{\\qquad\\qquad\\qquad\\sum_{j=\\operatorname*{max}\\{1,t-d+1\\}}^{\\operatorname*{min}\\{t,d-1\\}}\\left\\|\\left(\\boldsymbol{c}-\\frac{1}{n}\\mathbf{v1}^{\\top}\\right)^{t-j}\\left(\\mathbf{G}^{(0)}-\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right)\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, recall that the summation is legal only when $t\\leq2(d-2)$ .We have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T}\\mathbb{E}\\left\\lVert\\mathbf{Q}_{0,t,1}\\right\\rVert_{F}^{2}\\le\\sum_{t=0}^{\\operatorname*{min}\\{T,2(d-2)\\}}n d\\sum_{j=\\operatorname*{max}\\{1,t-d+1\\}}^{\\operatorname*{min}\\{t,d-1\\}}\\left\\lVert\\left(\\mathcal{C}-\\frac{1}{n}\\mathbf{v}\\mathbf{1}^{\\top}\\right)^{t-j}\\left(\\mathbf{G}^{(0)}-\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right)\\right\\rVert_{F}^{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Similarly, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=0}^{\\top}\\mathbb{E}\\left\\|\\mathbf{Q}_{0,t,3}\\right\\|_{F}^{2}}\\\\ &{\\displaystyle\\qquad\\sum_{t=0}^{\\operatorname*{min}\\{T,2(d-1)\\}}\\underset{\\substack{j=\\operatorname*{max}\\{1,t-d+1\\}}}{\\operatorname*{min}\\{t,d-1\\}}\\left\\|\\left(\\mathcal{R}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)^{j}\\right\\|_{2}^{2}\\left\\|\\left(\\mathcal{C}-\\frac{1}{n}\\mathbf{v}^{1\\top}\\right)^{t-j}\\right\\|_{2}^{2}\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right\\|_{F}^{2}}\\\\ &{\\displaystyle\\qquad\\leq2n^{2}d^{3}\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining the above upper bounds leads to the final result. ", "page_idx": 23}, {"type": "text", "text": "Lemma A.15. For any $T$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\mathbf{Q}_{0,t,4}+\\mathbf{Q}_{4,t}\\right\\|_{F}^{2}\\leq2n^{2}d^{2}L^{2}\\sum_{t=0}^{T}\\mathbb{E}\\|\\mathbf{I}_{\\mathbf{u}}\\mathbf{X}^{(t)}\\|_{F}^{2}+2n^{3}d^{2}\\sum_{t=0}^{T}\\mathbb{E}\\|f(x_{1}^{(m)})\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Note that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbf{Q}_{0,t,4}+\\mathbf{Q}_{4,t}=\\sum_{m=0}^{t-2\\operatorname*{min}\\{t-m-1,d-1\\}}\\left(\\mathbf{\\mathcal{R}}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)^{j}\\frac{\\mathbf{v}\\mathbf{1}^{\\top}}{n}\\left[\\nabla\\mathbf{F}(\\mathbf{X}^{(m+1)})-\\nabla\\mathbf{F}(\\mathbf{X}^{(m)})\\right]}\\quad}&{}\\\\ {\\quad}&{\\displaystyle\\operatorname*{min}\\{t,d-1\\}\\left(\\mathcal{R}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)^{j}\\frac{1}{n}\\mathbf{v}\\mathbf{1}^{\\top}\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})}\\\\ {\\quad}&{=\\displaystyle\\sum_{m=\\operatorname*{max}\\{t-d,0\\}}^{t-1}\\left(\\mathcal{R}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)^{t-m}\\frac{\\mathbf{v}\\mathbf{1}^{\\top}}{n}\\nabla\\mathbf{F}(\\mathbf{X}^{(m)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last equality comes from extending the summation in the first line and telescoping the summation. Consequently, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{\\displaystyle\\sum_{0,t,4}+Q_{4,t}=\\sum_{m=\\operatorname*{max}\\{t-d,0\\}}^{t-1}\\left(\\mathcal{R}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\right)^{t-m}\\frac{\\mathbf{v}\\mathbf{k}^{\\top}}{n}\\left(\\nabla\\mathbf{F}(\\mathbf{X}^{(m)})-\\nabla\\mathbf{F}(\\bar{\\mathbf{X}}^{(m)})+\\nabla\\mathbf{F}(\\bar{\\mathbf{X}}^{(m)})\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, taking the $\\boldsymbol{\\mathrm F}$ -norm on both sides and invoking Lemma A.6, Lemma A.5 and Lemma 3.2 as before,we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathbf{Q}_{0,t,4}+\\mathbf{Q}_{4,t}|\\displaystyle\\|_{F}^{2}\\leq d n\\sum_{m=\\operatorname*{max}\\{t-d,0\\}}^{t-1}\\left(2\\|\\frac{\\mathbf{v}\\mathbf{H}^{\\top}}{n}\\left[\\nabla\\mathbf{F}(\\mathbf{X}^{(m)})-\\nabla\\mathbf{F}(\\bar{\\mathbf{X}}^{(m)})\\right]\\|_{F}^{2}+2\\|\\frac{\\mathbf{v}\\mathbf{H}^{\\top}}{n}\\nabla\\mathbf{F}(\\bar{\\mathbf{X}}^{(m)})\\|_{F}^{2}\\right)}\\\\ &{\\leq2d n^{2}L^{2}\\displaystyle\\sum_{m=\\operatorname*{max}\\{t-d,0\\}}^{t-1}\\|\\mathbf{I}\\mathbf{u}\\mathbf{X}^{(m)}\\|_{F}^{2}+2d n^{3}\\displaystyle\\sum_{m=\\operatorname*{max}\\{t-d,0\\}}^{t-1}\\|\\nabla f(x_{1}^{(m)})\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Taking expectation on both sides and summing over $t$ from 0 to $T$ ,we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\mathbf{Q}_{0,t,4}+\\mathbf{Q}_{4,t}\\right\\|_{F}^{2}\\leq2n^{2}d^{2}L^{2}\\sum_{t=0}^{T}\\mathbb{E}\\|\\mathbf{I}_{\\mathbf{u}}\\mathbf{X}^{(t)}\\|_{F}^{2}+2n^{3}d^{2}\\sum_{t=0}^{T}\\mathbb{E}\\|\\nabla f(x_{1}^{(m)})\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Back to equation (13), note that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\Vert\\Pi_{\\mathbf{u}}\\mathbf{X}^{\\left(t\\right)}\\right\\Vert_{F}^{2}\\leq6\\left\\Vert(\\mathcal{R}^{\\top}-\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top})^{t}\\mathbf{X}^{\\left(0\\right)}\\right\\Vert_{F}^{2}+6\\gamma^{2}\\left\\Vert\\mathbf{Q}_{0,t,1}+\\mathbf{Q}_{0,t,3}\\right\\Vert_{F}^{2}}\\\\ &{\\qquad+\\left.6\\gamma^{2}\\left\\Vert\\mathbf{Q}_{1,t}\\right\\Vert_{F}^{2}+6\\gamma^{2}\\left\\Vert\\mathbf{Q}_{2,t}\\right\\Vert_{F}^{2}+6\\gamma^{2}\\left\\Vert\\mathbf{Q}_{3,t}+\\mathbf{Q}_{0,t,2}\\right\\Vert_{F}^{2}+6\\gamma^{2}\\left\\Vert\\mathbf{Q}_{0,t,4}+\\mathbf{Q}_{4,t}\\right\\Vert_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Taking full expectation on both sides, summing over $t$ from 0 to $T$ and combining Lemma A.11 to Lemma A.14, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{n=0}^{T}\\left\\|\\mathbf{I}_{\\mathbf{u}}\\mathbf{x}^{(t)}\\right\\|_{F}^{2}\\leq6\\displaystyle\\sum_{t=0}^{\\operatorname*{min}(T-1)}\\mathbb{E}\\left\\|\\left(\\mathcal{R}^{\\top}-\\frac{1}{n}\\mathbf{u}^{\\top}\\right)^{t}\\mathbf{X}^{(t)}\\right\\|_{F}^{2}+6\\displaystyle\\sum_{t=0}^{T}\\frac{\\mathbb{T}}{\\log1}\\big\\|\\mathbf{Q}_{4,t}\\big\\|_{F}^{2}+6\\displaystyle{\\gamma}^{2}\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\mathbf{Q}_{4,t}\\right\\|_{F}^{2}+}&{}\\\\ {+6\\displaystyle{\\gamma}^{2}\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\mathbf{Q}_{3,t}+\\mathbf{Q}_{0,t,2}\\right\\|_{F}^{2}+6\\gamma^{2}\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\mathbf{Q}_{0,t,4}+\\mathbf{Q}_{4,t}\\right\\|_{F}^{2}+6\\displaystyle{\\gamma}^{2}\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\mathbf{Q}_{0,t,1}+\\mathbf{Q}_{0,t}\\right\\|_{F}}&{}\\\\ {\\displaystyle\\leq6n d\\left\\|\\mathbf{I}_{\\mathbf{u}}\\mathbf{X}^{(t)}\\right\\|_{F}^{2}+192\\gamma^{2}n^{2}d^{4}(T+1)\\sigma^{2}+312\\gamma^{2}n^{2}d^{4}L^{2}\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\mathbf{H}_{\\mathbf{u}}\\mathbf{x}^{(t)}\\right\\|_{F}^{2}}&{}\\\\ {+1000\\sigma^{2}i^{4}n^{4}d^{4}(T+1)L^{2}\\sigma^{2}+2160\\gamma^{4}n^{5}d^{4}L^{2}\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\nabla f(x^{(t)})\\right\\|_{2}^{2}}&{}\\\\ {+2160\\gamma^{4}n^{4}d^{5}L^{2}\\left\\|\\nabla\\mathbb{F}(\\mathbf{X}^{(t)})\\right\\|_{F}^{2}+6\\gamma^{2}n^{2}d^{2}(T+1)\\sigma^{2}+24\\gamma^{2}n^{2}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For $\\begin{array}{r}{\\gamma\\leq\\frac{1}{40n d^{2}L}}\\end{array}$ , which implies that $\\begin{array}{r}{312\\gamma^{2}n^{2}d^{4}L^{2}+12\\gamma^{2}n^{2}d^{2}L^{2}\\leq\\frac{1}{4}}\\end{array}$ , we can simplify equation (14) as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=0}^{T}\\Big\\|\\mathbf{\\Pi}_{\\mathbf{u}}\\mathbf{X}^{(t)}\\Big\\|_{F}^{2}\\leq300\\gamma^{2}n^{2}d^{4}(T+1)\\sigma^{2}+20\\gamma^{2}n^{3}d^{2}\\sum_{t=0}^{T}\\mathbb{E}\\|\\nabla f(x_{1}^{(t)})\\|_{2}^{2}}}\\\\ &{}&{+\\,6n d\\left\\|\\Pi_{\\mathbf{u}}\\mathbf{X}^{(0)}\\right\\|_{F}^{2}+40\\gamma^{2}n^{2}d^{3}\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right\\|_{F}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which implies the desired result. ", "page_idx": 24}, {"type": "text", "text": "A.3.5Proof of Lemma 3.4 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof. Notice that ", "page_idx": 24}, {"type": "equation", "text": "$$\nx_{1}^{(t)}=x_{1}^{(t-1)}-\\gamma y_{1}^{(t-1)}=x_{1}^{(t-1)}-\\gamma\\sum_{i\\in\\mathbb{Z}_{1,1}}y_{1}^{(t-2)}-\\gamma g_{1}(x_{1}^{(t-1)},\\xi_{1}^{(t-1)})+\\gamma g_{1}(x_{1}^{(t-2)},\\xi_{1}^{(t-2)}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, x $\\boldsymbol{x}_{1}^{(t)}$ does not depend on $\\xi_{i}^{t-1}$ for $i\\neq1$ . We iterate the above procedure to get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle x_{1}^{(t)}=x_{1}^{(t-1)}-\\gamma g_{1}\\big(x_{1}^{(t-1)},\\xi_{1}^{(t-1)}\\big)+\\gamma g_{1}\\big(x_{1}^{(t-2)},\\xi_{1}^{(t-2)}\\big)}}\\\\ {{\\displaystyle\\qquad-\\gamma\\sum_{i\\in\\mathbb{Z}_{1,2}}y_{i}^{(t-3)}-\\gamma\\sum_{i\\in\\mathbb{Z}_{1,1}}g_{i}\\big(x_{1}^{(t-2)},\\xi_{1}^{(t-2)}\\big)+\\gamma\\sum_{i\\in\\mathbb{Z}_{1,1}}g_{i}\\big(x_{1}^{(t-3)},\\xi_{1}^{(t-3)}\\big)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Similar to $\\boldsymbol{x}_{1}^{(t)}$ , we know that \u03b1(t-1) does not depend on (t- \uff0c $i\\neq1$ Hence $\\boldsymbol{x}_{1}^{(t)}$ is independent with $\\xi_{i}^{(t-2)}$ 10 $i\\notin\\mathcal{T}_{1,1}$ $\\boldsymbol{x}_{1}^{(t)}$ is independentwih $\\xi_{i}^{(t-k)}$ for $i\\not\\in{\\cal T}_{1,k}$ ", "page_idx": 24}, {"type": "text", "text": "Consgen $Z=\\{\\xi_{i}^{(t-k)},i\\in\\mathcal{I}_{1,k},i\\notin\\mathcal{I}_{1,k-1}\\}$ $X=x_{1}^{(t)}$ \uff0c $Y=\\{x_{i}^{(t-k)},i\\in[n]\\}$$Z$ $(X,Y)$", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\langle\\nabla f({x}_{1}^{(t)}),{\\mathbf{e}}_{{\\mathcal{T}}_{1,k}/{\\mathcal{T}}_{1,k-1}}\\left({\\mathbf{G}}^{(t-k)}-\\nabla{\\mathbf{F}}({\\mathbf{X}}^{(t-k)})\\right)\\right\\rangle=0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, invoking Corollary A.4, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{m=1}^{\\operatorname*{min}\\{t,d\\}}\\mathbb{E}\\left\\langle\\nabla f(x_{1}^{(t)}),\\left(\\frac{\\mathbf{u}^{\\top}}{n}-\\mathbf{1}^{\\top}\\right)\\mathbf{A}_{m}\\left(\\mathbf{G}^{(t-m)}-\\nabla\\mathbf{F}(\\mathbf{X}^{(t-m)})\\right)\\right\\rangle}\\\\ &{\\displaystyle=\\sum_{m=1}^{\\operatorname*{min}\\{t,d\\}}\\mathbb{E}\\left\\langle\\nabla f(x_{1}^{(t)}),\\mathbf{e}_{Z_{1,m}/Z_{1,m-1}}\\left(\\mathbf{G}^{(t-m)}-\\nabla\\mathbf{F}(\\mathbf{X}^{(t-m)})\\right)\\right\\rangle=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "A.3.6Proof of Lemma 3.5 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proof. By Assumption 1.2, the function $\\textstyle f:={\\frac{1}{n}}\\sum_{i=1}^{n}f_{i}$ is $L$ -smooth. Then, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}f(x_{1}^{(t+1)})\\le\\mathbb{E}f(x_{1}^{(t)})+\\mathbb{E}\\langle\\nabla f(x_{1}^{(t)}),x_{1}^{(t+1)}-x_{1}^{(t)}\\rangle+\\frac{L}{2}\\mathbb{E}\\|x_{1}^{(t+1)}-x_{1}^{(t)}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For the last term, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad{\\mathbb{E}}\\|x_{1}^{(t+1)}-x_{1}^{(t)}\\|_{2}^{2}={\\mathbb{E}}\\|\\frac{1}{n}\\mathbf{u}^{\\top}\\left({\\mathbf{X}}^{(t+1)}-{\\mathbf{X}}^{(t)}\\right)\\|_{2}^{2}}\\\\ &{\\quad\\!\\!=\\!\\frac{1}{n}{\\mathbb{E}}\\|\\frac{1}{n}\\mathbf{1}\\mathbf{u}^{\\top}\\left({\\mathbf{X}}^{(t+1)}-{\\mathbf{X}}^{(t)}\\right)\\|_{F}^{2}=\\frac{1}{n}{\\mathbb{E}}\\|\\bar{\\mathbf{X}}^{(t+1)}-\\bar{\\mathbf{X}}^{(t)}\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For the second last term, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\langle\\nabla f(x_{1}^{(t)}),x_{1}^{(t+1)}-x_{1}^{(t)}\\rangle=\\mathbb{E}\\langle\\nabla f(x_{1}^{(t)}),\\frac{\\mathbf{u}^{\\top}}{n}\\left(\\mathbf{X}^{(t+1)}-\\mathbf{X}^{(t)}\\right)\\rangle=\\mathbb{E}\\langle\\nabla f(x_{1}^{(t)}),-\\gamma\\frac{\\mathbf{u}^{\\top}}{n}\\mathbf{Y}^{(t)}\\rangle}\\\\ &{=\\!\\mathbb{E}\\langle\\nabla f(x_{1}^{(t)}),-\\gamma\\left(\\frac{\\mathbf{u}^{\\top}}{n}-\\mathbf{1}^{\\top}\\right)\\mathbf{Y}^{(t)}\\rangle+\\mathbb{E}\\langle\\nabla f(x_{1}^{(t)}),-\\gamma\\mathbf{1}^{\\top}\\mathbf{Y}^{(t)}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We now bound the two terms in the above equation. Firstly ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\langle\\nabla f(x_{1}^{(t)}),-\\gamma\\left(\\frac{\\mathbf{u}^{\\top}}{n}-\\mathbf{1}^{\\top}\\right)\\mathbf{Y}^{(t)}\\rangle=\\gamma\\mathbb{E}\\langle\\nabla f(x_{1}^{(t)}),-\\left(\\frac{\\mathbf{u}^{\\top}}{n}-\\mathbf{1}^{\\top}\\right)\\mathbf{I}_{\\mathbf{v}}\\mathbf{Y}^{(t)}\\rangle.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Recall that by equation (9), ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma\\mathbb{E}\\langle\\nabla f(x_{1}^{(t)}),-\\left(\\frac{\\mathbf{u}^{\\mathsf{T}}}{n}-1\\right)\\mathbf{I}_{n}\\mathbf{Y}^{(t)}\\rangle}\\\\ &{=\\!\\gamma\\mathbb{E}\\Bigg\\langle\\nabla f(x_{1}^{(t)}),-\\left(\\frac{\\mathbf{u}^{\\mathsf{T}}}{n}-1\\right)^{\\mathsf{m i n}}\\!\\Bigg[\\!\\frac{\\mathbf{u}^{\\mathsf{T}}(t,t)}{n-1}\\mathbf{\\delta}_{\\mathbf{a}}\\mathbf{G}^{(t-m)}-\\left(\\frac{\\mathbf{u}^{\\mathsf{T}}}{n}-1\\!\\!\\!\\nabla\\right)\\mathbf{G}^{(t)}\\Bigg\\rangle}\\\\ &{=-\\gamma\\mathbb{E}\\Bigg\\langle\\nabla f(x_{1}^{(t)}),\\left(\\frac{\\mathbf{u}^{\\mathsf{T}}}{n}-1\\right)^{\\mathsf{T}}\\Bigg(\\mathbf{G}^{(t)}-\\nabla\\mathbf{F}(\\mathbf{X}^{(t)})\\Bigg)\\Bigg\\rangle}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!-\\gamma\\mathbb{E}\\Bigg\\langle\\nabla f(x_{1}^{(t)}),\\left(\\frac{\\mathbf{u}^{\\mathsf{T}}}{n}-1\\right)^{\\mathsf{m i n}}\\!\\Bigg[\\!\\frac{\\mathbf{u}^{\\mathsf{T}}(t,t)}{n-1}\\mathbf{\\delta}_{\\mathbf{a}}\\mathbf{\\delta}_{\\mathbf{a}}\\left(\\mathbf{G}^{(t-m)}-\\nabla\\mathbf{F}(\\mathbf{X}^{(t-m)})\\right)\\Bigg\\rangle}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!-\\gamma\\mathbb{E}\\Bigg\\langle\\nabla f(x_{1}^{(t)}),\\left(\\frac{\\mathbf{u}^{\\mathsf{T}}}{n}-1\\right)^{\\mathsf{T}}\\nabla\\mathbf{F}(\\mathbf{X}^{(t)})\\Bigg\\rangle}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!-\\gamma\\mathbb{E}\\Bigg\\langle\\nabla f(x_{1}^{(t)}),\\left(\\frac{\\mathbf{u}^{\\mathsf{T}}}{n}-1\\right)^{\\mathsf{m i n}}\\mathbb{I}_{n}^{(t)}\\Bigg(\\mathbf{X}^{(t)}\\Bigg)}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!-\\gamma\\mathbb{E}\\Bigg\\langle\\nabla f(x_{1}^{(t)}),\\left(\\frac{\\mathbf{u}^{\\mathsf{T}}}{n}-1\\right)^{\\mathsf{m i n}}\\mathbb{I}_{n}^{(t)}\\Bigg\\rangle}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We bound the four terms above one by one. For the first term, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\langle\\nabla f({x}_{1}^{(t)}),-\\left(\\frac{{\\mathbf{u}}^{\\top}}{n}-{\\mathbf{1}}^{\\top}\\right)\\left({\\mathbf{G}}^{(t)}-\\nabla{\\mathbf{F}}({\\mathbf{X}}^{(t)})\\right)\\rangle=0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For the second one, invoking Lemma 3.4, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\langle\\nabla f({x}_{1}^{(t)}),\\left(\\frac{\\mathbf{u}^{\\top}}{n}-\\mathbf{1}^{\\top}\\right)\\displaystyle\\sum_{m=1}^{\\operatorname*{min}\\{t,d\\}}\\mathbf{A}_{m}\\left(\\mathbf{G}^{(t-m)}-\\nabla\\mathbf{F}(\\mathbf{X}^{(t-m)})\\right)\\right\\rangle}\\\\ &{=\\displaystyle\\sum_{m=1}^{\\operatorname*{min}\\{t,d\\}}\\mathbb{E}\\left\\langle\\nabla f({x}_{1}^{(t)}),\\left(\\frac{\\mathbf{u}^{\\top}}{n}-\\mathbf{1}^{\\top}\\right)\\mathbf{A}_{m}\\left(\\mathbf{G}^{(t-m)}-\\nabla\\mathbf{F}(\\mathbf{X}^{(t-m)})\\right)\\right\\rangle=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For the last two terms in (17), we have as: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\;\\mathbb{E}\\left\\langle\\nabla f({x}_{1}^{(t)}),\\left(\\frac{\\mathbf{u}^{\\top}}{n}-\\mathbf{1}^{\\top}\\right)\\nabla\\mathbf{F}(\\mathbf{X}^{(t)})\\right\\rangle-\\mathbb{E}\\left\\langle\\nabla f({x}_{1}^{(t)}),\\left(\\frac{\\mathbf{u}^{\\top}}{n}-\\mathbf{1}^{\\top}\\right)\\sum_{m=1}^{\\operatorname*{min}\\{t,d\\}}\\mathbf{A}_{m}\\nabla\\mathbf{F}(\\mathbf{X}^{(t-m)})\\right\\rangle}\\\\ &{=\\displaystyle\\sum_{m=1}^{\\operatorname*{min}\\{t,d\\}}\\mathbb{E}\\left\\langle\\nabla f({x}_{1}^{(t)}),-\\left(\\frac{\\mathbf{u}^{\\top}}{n}-\\mathbf{1}^{\\top}\\right)\\left(\\mathcal{C}-\\frac{1}{n}\\mathbf{v1}^{\\top}\\right)^{m-1}\\left(\\nabla\\mathbf{F}(\\mathbf{X}^{(t-m+1)})-\\nabla\\mathbf{F}(\\mathbf{X}^{(t-m)})\\right)\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By the Cauchy-Schwartz inequality, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{m=1}{\\operatorname*{min}}\\underbrace{\\left\\{t,d\\right\\}}_{\\mathbf{S}}\\Bigg\\langle\\nabla f(x_{1}^{(t)}),-\\left(\\frac{\\mathbf{u}^{\\top}}{n}-\\mathbf{1}^{\\top}\\right)\\left(\\mathcal{C}-\\frac{1}{n}\\mathbf{v}\\mathbf{1}^{\\top}\\right)^{m-1}\\left(\\nabla\\mathbf{F}(\\mathbf{X}^{(t-m+1)})-\\nabla\\mathbf{F}(\\mathbf{X}^{(t-m)})\\right)\\Bigg\\rangle}\\\\ &{\\overset{\\operatorname*{min}}{\\leq}\\sum_{m=1}^{n}\\Bigg\\{\\frac{n}{2d}\\mathbb{E}\\left\\|\\nabla f(x_{1}^{(t)})\\right\\|_{2}^{2}+\\frac{d}{2n}\\mathbb{E}\\left\\|\\left(\\frac{\\mathbf{u}^{\\top}}{n}-\\mathbf{1}^{\\top}\\right)\\left(\\mathcal{C}-\\frac{1}{n}\\mathbf{v}\\mathbf{1}^{\\top}\\right)^{m-1}\\right\\|_{2}^{2}\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(t-m+1)})-\\nabla\\mathbf{F}(\\mathbf{X}^{(t-m+1)})\\right)}\\\\ &{\\overset{\\leq}-\\frac{n}{2}\\mathbb{E}\\left\\|\\nabla f(x_{1}^{(t)})\\right\\|_{2}^{2}+\\frac{d L^{2}}{2}\\underset{m=1}{\\overset{\\operatorname*{max}}{\\sum}}\\mathbb{E}\\left\\|\\mathbf{X}^{(t-m+1)}-\\mathbf{X}^{(t-m)}\\right\\|_{F}^{2}}\\\\ &{\\overset{\\leq}\\frac{n}{2}\\mathbb{E}\\left\\|\\nabla f(x_{1}^{(t)})\\right\\|_{2}^{2}+\\frac{3d L^{2}}{2}\\underset{m=1}{\\overset{\\operatorname*{max}}{\\sum}}\\mathbb{E}\\left(\\left\\|\\mathbf{I}_{\\mathbf{u}}\\mathbf{X}^{(t-m+1)}\\right\\|_{F}^{2}+\\left\\|\\Pi_{\\mathbf{u}}\\mathbf{X}^{(t-m)}\\right\\|_{F}^{2}+\\left\\|\\bar{\\mathbf{X}}^{(t-m+1)}-\\bar{\\mathbf{X}}^{(t-m+1)}\\right\\|_{F}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus, combining the above inequalities together yields ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t=0}^{T}\\gamma\\mathbb{E}\\langle\\nabla f(x_{1}^{(t)}),-\\left(\\frac{\\mathbf{u}^{\\top}}{n}-\\mathbf{1}^{\\top}\\right)\\Pi_{\\mathbf{v}}\\mathbf{Y}^{(t)}\\rangle\\leq\\frac{n\\gamma}{2}\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\nabla f(x_{1}^{(t)})\\right\\|_{2}^{2}}}\\\\ &{+3\\gamma d^{2}L^{2}\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\Pi_{\\mathbf{u}}\\mathbf{X}^{(t)}\\right\\|_{F}^{2}+3\\gamma d^{2}L^{2}\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\bar{\\mathbf{X}}^{(t+1)}-\\bar{\\mathbf{X}}^{(t)}\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Secondly, for the second term in (16), ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~\\mathbb{E}\\langle\\nabla f({x}_{1}^{(t)}),-\\gamma\\mathbf{1}^{\\top}\\mathbf{Y}^{(t)}\\rangle=\\mathbb{E}\\langle\\nabla f({x}_{1}^{(t)}),-\\gamma\\mathbf{1}^{\\top}\\mathbf{G}^{(t)}\\rangle=\\mathbb{E}\\langle\\nabla f({x}_{1}^{(t)}),-\\gamma\\mathbf{1}^{\\top}\\nabla\\mathbf{F}(\\mathbf{X}^{(t)})\\rangle}\\\\ &{=-n\\gamma\\mathbb{E}\\|\\nabla f({x}_{1}^{(t)})\\|_{2}^{2}-n\\gamma\\mathbb{E}\\langle\\nabla f({x}_{1}^{(t)}),\\frac{1}{n}\\mathbf{1}^{\\top}\\nabla\\mathbf{F}(\\mathbf{X}^{(t)})-\\frac{1}{n}\\mathbf{1}^{\\top}\\nabla\\mathbf{F}(\\bar{\\mathbf{X}}^{(t)})\\rangle}\\\\ &{\\leq-n\\gamma\\mathbb{E}\\|\\nabla f({x}_{1}^{(t)})\\|_{2}^{2}+\\gamma\\frac{n}{4}\\mathbb{E}\\|\\nabla f({x}_{1}^{(t)})\\|_{2}^{2}+2\\gamma\\mathbb{E}\\|\\nabla\\mathbf{F}({\\mathbf{X}}^{(t)})-\\nabla\\mathbf{F}(\\bar{\\mathbf{X}}^{(t)})\\|_{F}^{2}}\\\\ &{\\leq-n\\gamma\\mathbb{E}\\|\\nabla f({x}_{1}^{(t)})\\|_{2}^{2}+\\gamma\\frac{n}{4}\\mathbb{E}\\|\\nabla f({x}_{1}^{(t)})\\|_{2}^{2}+2\\gamma L^{2}\\mathbb{E}\\|\\mathbf{I}_{\\mathbf{u}}\\mathbf{X}^{(t)}\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Summing over $t$ from 0 to $T$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T}\\mathbb{E}\\langle\\nabla f(x_{1}^{(t)}),-\\gamma\\mathbf{1}^{\\top}\\mathbf{Y}^{(t)}\\rangle\\leq-\\frac{3n\\gamma}{4}\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\nabla f(x_{1}^{(t)})\\right\\|_{2}^{2}+2\\gamma L^{2}\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\Pi_{\\mathbf{u}}\\mathbf{X}^{(t)}\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "It yields by summing over $t$ from O to $T$ on both sides of equation (15) that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Xi f(x_{1}^{(T+1)})-\\mathbb{E}f(x_{1}^{0})\\leq\\sum_{t=0}^{T}\\mathbb{E}\\langle\\nabla f(x_{1}^{(t)}),x_{1}^{(t+1)}-x_{1}^{(t)}\\rangle+\\frac{L}{2n}\\sum_{t=0}^{T}\\mathbb{E}\\|\\bar{\\mathbf{X}}^{(t+1)}-\\bar{\\mathbf{X}}^{(t)}\\|_{F}^{2}}\\\\ {\\displaystyle\\quad\\leq\\sum_{t=0}^{T}\\mathbb{E}\\langle\\nabla f(x_{1}^{(t)}),-\\gamma\\mathbf{1}^{\\top}\\mathbf{Y}^{(t)}\\rangle+\\sum_{t=0}^{T}\\gamma\\mathbb{E}\\langle\\nabla f(x_{1}^{(t)}),-\\left(\\frac{\\mathbf{u}^{\\top}}{n}-\\mathbf{1}^{\\top}\\right)\\mathbf{I}_{\\mathbf{v}}\\mathbf{Y}^{(t)}\\rangle+\\frac{L}{2n}\\sum_{t=0}^{T}\\mathbb{E}\\|\\bar{\\mathbf{X}}^{(t+1)}\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "With the results above, given $\\Delta_{f}=f(x^{0})-f^{*}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\Delta_{f}\\leq-\\displaystyle\\frac{n\\gamma}{4}\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\nabla f(x_{1}^{(t)})\\right\\|_{2}^{2}+\\displaystyle\\frac{L}{2n}\\sum_{t=0}^{T}\\mathbb{E}\\|\\bar{\\mathbf{X}}^{(t+1)}-\\bar{\\mathbf{X}}^{(t)}\\|_{F}^{2}}\\\\ &{\\qquad\\qquad+\\displaystyle5\\gamma d^{2}L^{2}\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\Pi_{\\mathbf{u}}\\mathbf{X}^{(t)}\\right\\|_{F}^{2}+3\\gamma d^{2}L^{2}\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\bar{\\mathbf{X}}^{(t+1)}-\\bar{\\mathbf{X}}^{(t)}\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Invoking Lemma 3.2, we have for $\\begin{array}{r}{\\gamma\\leq\\frac{1}{100n d^{3}L}\\bigl(\\leq\\frac{1}{10n d L}\\bigr)}\\end{array}$ that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\!-\\Delta_{f}\\leq n\\gamma\\left(45\\gamma^{2}n^{2}d^{2}L^{2}+8\\gamma n L-\\frac{1}{4}\\right)\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\nabla f(x_{1}^{(t)})\\right\\|_{2}^{2}}\\\\ &{\\!+\\left(150\\gamma^{3}n^{2}d^{4}L^{4}+25\\gamma^{2}n d^{2}L^{3}+5\\gamma d^{2}L^{2}\\right)\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\mathbf{I}_{\\mathbf{u}}\\mathbf{X}^{(t)}\\right\\|_{F}^{2}}\\\\ &{\\!+3\\gamma^{2}n(T+1)L\\sigma^{2}+18\\gamma^{3}n^{2}d^{2}L^{2}(T+1)\\sigma^{2}}\\\\ &{\\!+\\left(18\\gamma^{3}n^{2}d^{3}L^{2}+3\\gamma^{2}n L\\right)\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right\\|_{F}^{2},}\\\\ &{\\!\\mathrm{d}\\mathrm{s}\\,\\mathrm{that}\\,150\\gamma^{3}n^{2}d^{4}L^{4}+25\\gamma^{2}n d^{2}L^{3}+5\\gamma d^{2}L^{2}\\leq6\\gamma d^{2}L^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where it ho ", "page_idx": 27}, {"type": "text", "text": "Invoking Lemma 33, we have for $\\begin{array}{r}{\\gamma\\leq\\frac{1}{100n d^{3}L}(\\leq\\frac{1}{40n d^{2}L})}\\end{array}$ that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\Delta_{f}\\leq n\\gamma\\left(120\\gamma^{2}n^{2}d^{4}L^{2}+45\\gamma^{2}n^{2}d^{2}L^{2}+8\\gamma n L-\\displaystyle\\frac{1}{4}\\right)\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\nabla f(x_{1}^{(t)})\\right\\|_{2}^{2}}\\\\ &{\\qquad\\qquad+\\left.3\\gamma^{2}n(T+1)L\\sigma^{2}+18\\gamma^{3}n^{2}d^{2}L^{2}(T+1)\\sigma^{2}+1800\\gamma^{3}n^{2}d^{6}(T+1)\\sigma^{2}L^{2}\\right.}\\\\ &{\\qquad\\qquad+\\left.\\left(240\\gamma^{3}n^{2}d^{5}L^{2}+18\\gamma^{3}n^{2}d^{3}L^{2}+3\\gamma^{2}n L\\right)\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right\\|_{F}^{2}}\\\\ &{\\qquad\\qquad+\\left.36\\gamma n d^{3}L^{2}\\left\\|\\Pi_{\\mathbf{u}}\\mathbf{X}^{(0)}\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, for $\\begin{array}{r}{\\gamma\\leq\\frac{1}{100n d^{3}L}}\\end{array}$ 100nd3L, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{120\\gamma^{2}n^{2}d^{4}L^{2}+45\\gamma^{2}n^{2}d^{2}L^{2}+8\\gamma n L-\\displaystyle\\frac{1}{4}\\leq-\\displaystyle\\frac{1}{8}}}\\\\ {{\\gamma n\\left(240\\gamma^{2}n d^{5}L^{2}+18\\gamma^{2}n d^{3}L^{2}+3\\gamma L\\right)\\leq7\\gamma^{2}n d^{2}L.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "After re-arranging the terms, we conclude that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{T+1}\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\nabla f(x_{1}^{(t)})\\right\\|_{2}^{2}\\leq\\frac{8\\Delta_{f}}{\\gamma n(T+1)}+24\\gamma\\sigma^{2}L+2000\\gamma^{2}n d^{6}\\sigma^{2}L^{2}}\\\\ &{\\displaystyle\\qquad+\\,\\frac{400d^{3}L^{2}\\left\\|\\mathbf{I}_{\\mathbf{u}}\\mathbf{X}^{(0)}\\right\\|_{F}^{2}}{T+1}+\\frac{56\\gamma d^{3}L\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right\\|_{F}^{2}}{T+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "A.3.7 Proof of Lemma 3.7 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Let $x^{*}\\,=\\,\\arg\\operatorname*{min}_{x}\\,f(x)$ .We stat with analying the behavior of $\\|\\boldsymbol{x}_{1}^{(t)}-\\boldsymbol{x}^{*}\\|^{2}$ afte btaining Lemma 3.4. It holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|x_{1}^{(t+1)}-x^{*}\\right\\|^{2}=\\left\\|x_{1}^{(t)}-x^{*}\\right\\|^{2}+2\\left\\langle x_{1}^{(t)}-x^{*},x_{1}^{(t+1)}-x_{1}^{(t)}\\right\\rangle+\\left\\|x_{1}^{(t+1)}-x_{1}^{(t)}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "To deal with the critical inner product, similar to the decomposition in Equation (16), we have, by replacing $\\nabla f(x_{1}^{(t)})$ Wwih $x_{1}^{(t)}-x^{*}$ in Equatio (17), and ivoking Lemma 3.4 as we have done in Lemma 3.5, that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\langle x_{1}^{(t)}-x^{*},x_{1}^{(t+1)}-x_{1}^{(t)}\\right\\rangle}\\\\ &{=-\\left\\langle x_{1}^{(t)}-x^{*},\\gamma\\left(\\frac{\\mathbf{u}^{\\top}}{n}-\\mathbf{1}^{\\top}\\right)\\Pi_{\\mathbf{v}}\\mathbf{Y}^{(t)}\\right\\rangle-\\left\\langle x_{1}^{(t)}-x^{*},\\gamma\\mathbf{1}^{\\top}\\mathbf{Y}^{(t)}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\left.\\mathbb{E}\\left\\langle x_{1}^{(t)}-x^{*},\\gamma\\left(\\frac{\\mathbf{u}^{\\mathsf{T}}}{n}-\\mathbf{1}^{\\mathsf{T}}\\right)\\Pi_{\\mathbf{v}}\\mathbf{Y}^{(t)}\\right\\rangle}\\\\ &{=-\\gamma\\mathbb{E}\\left\\langle x_{1}^{(t)}-x^{*},\\left(\\frac{\\mathbf{u}^{\\mathsf{T}}}{n}-\\mathbf{1}^{\\mathsf{T}}\\right)\\nabla\\mathbf{F}(\\mathbf{X}^{(t)})\\right\\rangle}\\\\ &{\\quad-\\left.\\gamma\\mathbb{E}\\left\\langle x_{1}^{(t)}-x^{*},\\left(\\frac{\\mathbf{u}^{\\mathsf{T}}}{n}-\\mathbf{1}^{\\mathsf{T}}\\right)\\frac{\\operatorname*{min}\\{t,d\\}}{m=1}\\mathbf{A}_{m}\\nabla\\mathbf{F}(\\mathbf{X}^{(t)})\\right\\rangle}\\\\ &{\\quad\\underbrace{\\varepsilon\\frac{n\\gamma\\mu}{4}\\mathbb{E}\\left\\|x_{1}^{(t)}-x^{*}\\right\\|^{2}+\\frac{3d L^{2}}{\\mu}\\gamma\\sum_{m=1}^{\\operatorname*{min}\\{t,d\\}}}_{m=1}\\mathbb{E}\\left(\\left\\|\\Pi_{\\mathbf{u}}\\mathbf{X}^{t-m+1}\\right\\|^{2}+\\left\\|\\Pi_{\\mathbf{u}}\\mathbf{X}^{t-m}\\right\\|^{2}+\\left\\|\\bar{\\mathbf{X}}^{t-m+1}-\\bar{\\mathbf{X}}^{t-m}\\right\\|^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Notice that, first by strong convexity of $f$ (Assumption 1.3) and then by $L$ -smoothness (Assumption 1.2), there holds ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\left\\langle x_{1}^{(t)}-x^{*},\\nabla f(x_{1}^{(t)})\\right\\rangle\\leq f(x^{*})-f(x_{1}^{(t)})-\\displaystyle\\frac{\\mu}{2}\\left\\Vert x_{1}^{(t)}-x^{*}\\right\\Vert^{2}}\\\\ &{\\leq-\\displaystyle\\frac{1}{2L}\\left\\Vert\\nabla f(x_{1}^{(t)})\\right\\Vert^{2}-\\displaystyle\\frac{\\mu}{2}\\left\\Vert x_{1}^{(t)}-x^{*}\\right\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\gamma\\mathbb{E}\\left\\langle x_{1}^{(t)}-x^{*},\\mathbf{1}^{\\top}\\mathbf{Y}^{(t)}\\right\\rangle=-\\gamma\\mathbb{E}\\left\\langle x_{1}^{(t)}-x^{*},\\mathbf{1}^{\\top}\\nabla\\mathbf{F}(\\mathbf{X}^{(t)})\\right\\rangle}\\\\ &{=-n\\gamma\\mathbb{E}\\left\\langle x_{1}^{(t)}-x^{*},\\nabla f(x_{1}^{(t)})\\right\\rangle-\\gamma\\mathbb{E}\\left\\langle x_{1}^{(t)}-x^{*},\\mathbf{1}^{\\top}\\nabla\\mathbf{F}(\\mathbf{X}^{(t)})-\\mathbf{1}^{\\top}\\nabla\\mathbf{F}(\\bar{\\mathbf{X}}^{(t)})\\right\\rangle}\\\\ &{\\leq-\\frac{n\\gamma\\mu}{2}\\mathbb{E}\\left\\|x_{1}^{(t)}-x^{*}\\right\\|^{2}-\\frac{n\\gamma}{2L}\\left\\|\\nabla f(x_{1}^{(t)})\\right\\|^{2}+\\frac{n\\gamma\\mu}{8}\\mathbb{E}\\left\\|x_{1}^{(t)}-x^{*}\\right\\|^{2}+\\frac{2L^{2}\\gamma}{\\mu}\\mathbb{E}\\left\\|\\Pi_{\\mathbf{u}}\\mathbf{X}^{(t)}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, plugging the above results into Equation (18), with $\\kappa:=L/\\mu$ as the conditional number, it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|x_{1}^{(t+1)}-x^{*}\\right\\|^{2}\\leq\\left(1-\\displaystyle\\frac{n\\gamma\\mu}{4}\\right)\\mathbb{E}\\left\\|x_{1}^{(t)}-x^{*}\\right\\|^{2}-\\displaystyle\\frac{n\\gamma}{L}\\mathbb{E}\\left\\|\\nabla f(x_{1}^{(t)})\\right\\|^{2}+\\displaystyle\\frac{1}{n}\\mathbb{E}\\left\\|\\bar{\\mathbf{X}}^{(t+1)}-\\bar{\\mathbf{X}}^{(t)}\\right\\|_{F}^{2}}\\\\ &{\\quad+\\displaystyle6d\\kappa L\\gamma\\displaystyle\\sum_{m=1}^{\\operatorname*{min}\\{t,d\\}}\\mathbb{E}\\left\\|\\bar{\\mathbf{X}}^{(t-m+1)}-\\bar{\\mathbf{X}}^{(t-m)}\\right\\|_{F}^{2}+20d\\kappa L\\gamma\\displaystyle\\sum_{m=0}^{\\operatorname*{min}\\{t,d\\}+1}\\mathbb{E}\\left\\|\\mathbf{I}_{\\mathbf{u}}\\mathbf{X}^{t-m}\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We derive the convergence result by several standard steps as follows. ", "page_idx": 28}, {"type": "text", "text": "St1.abof $\\begin{array}{r}{\\frac{1}{2}\\overset{\\cdot}{\\leq}1-\\frac{n\\gamma\\mu}{4}\\leq1}\\end{array}$ $\\begin{array}{r}{\\gamma\\leq\\frac{1}{10n d^{2}\\kappa L}}\\end{array}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|x_{1}^{(T)}-x^{*}\\right\\|^{2}\\leq\\left(1-\\frac{n\\gamma\\mu}{4}\\right)^{T}\\left\\|x_{1}^{(0)}-x^{*}\\right\\|^{2}-\\frac{n\\gamma}{2L}\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\nabla f(x_{1}^{(t)})\\right\\|^{2}}\\\\ &{\\quad+\\left(\\frac{1}{n}+6d^{2}\\kappa L\\gamma\\right)\\displaystyle\\sum_{t=0}^{T}\\left(1-\\frac{n\\gamma\\mu}{4}\\right)^{T-t}\\mathbb{E}\\left\\|\\bar{\\mathbf{X}}^{(t+1)}-\\bar{\\mathbf{X}}^{(t)}\\right\\|_{F}^{2}}\\\\ &{\\quad+\\,60d^{2}\\kappa L\\gamma\\displaystyle\\sum_{t=0}^{T}\\left(1-\\frac{n\\gamma\\mu}{4}\\right)^{T-t}\\mathbb{E}\\left\\|\\Pi_{\\mathbf{u}}\\mathbf{X}^{t}\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Step 2. Torene Lmma3.2, we start with Equation (11) multiplied by the coefcit $\\begin{array}{r}{\\left(1-\\frac{n\\gamma\\mu}{4}\\right)^{T-t}}\\end{array}$ before summing over $t$ in Equation (10) from 0 to $T$ .Then, we have, for $\\begin{array}{r}{\\gamma\\ \\leq\\ \\frac{1}{10n d^{2}\\kappa L}\\ (\\frac{1}{2}\\ \\leq}\\end{array}$ $\\begin{array}{r}{1-\\frac{n\\gamma\\mu}{4}\\leq1}\\end{array}$ 0 $\\begin{array}{r}{\\gamma\\leq\\frac{1}{10n d^{2}\\kappa L})}\\end{array}$ , that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\ell=0}^{T}\\left(1-\\frac{n\\gamma\\mu}{4}\\right)^{T-t}\\mathbb{E}\\|\\bar{\\mathbf{X}}^{(t+1)}-\\bar{\\mathbf{X}}^{(t)}\\|_{F}^{2}\\leq6\\gamma^{2}n^{2}\\sigma^{2}(T+1)+50\\gamma^{2}n^{2}d^{2}L^{2}\\displaystyle\\sum_{t=0}^{T}\\left(1-\\frac{n\\gamma\\mu}{4}\\right)^{T-t}\\mathbb{E}\\left\\|\\bar{\\mathbf{X}}^{(t)}-\\bar{\\mathbf{X}}^{(t)}\\right\\|}\\\\ &{\\displaystyle\\qquad+\\,6\\gamma^{2}n^{2}\\displaystyle\\sum_{t=0}^{d}\\left(1-\\frac{n\\gamma\\mu}{4}\\right)^{T-t}\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right\\|_{F}^{2}+15\\gamma^{2}n^{3}\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\nabla f(x_{1}^{(t)})\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we get the result which only modifies the coeficient of the term $\\big\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\big\\|_{F}^{2}$ ", "page_idx": 29}, {"type": "text", "text": "Implementing the above result, we have, for $\\begin{array}{r}{\\gamma\\leq\\frac{1}{10n d^{2}\\kappa L}\\left(\\leq\\frac{1}{10n d L}\\right)}\\end{array}$ \uff0c ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|x_{1}^{(T)}-x^{*}\\right\\|^{2}\\leq\\left(1-\\frac{n\\gamma\\mu}{4}\\right)^{T}\\left\\|x_{1}^{(0)}-x^{*}\\right\\|^{2}}\\\\ &{\\quad+\\left(-\\frac{n\\gamma}{2L}+15\\gamma^{2}n^{2}+90\\gamma^{3}n^{3}d^{2}\\kappa L\\right)\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\nabla f(x_{1}^{(t)})\\right\\|^{2}}\\\\ &{\\quad+6\\gamma^{2}n\\sigma^{2}\\left(T+1\\right)+36\\gamma^{3}n^{2}d^{2}\\kappa L\\sigma^{2}\\left(T+1\\right)}\\\\ &{\\quad+\\left(60\\gamma d^{2}\\kappa L+50\\gamma^{2}n d^{2}L^{2}+300\\gamma^{3}n^{2}d^{4}\\kappa L^{3}\\right)\\displaystyle\\sum_{t=0}^{T}\\left(1-\\frac{n\\gamma\\mu}{4}\\right)^{T-t}\\mathbb{E}\\left\\|\\Pi_{\\mathbf{u}}\\mathbf{X}^{t}\\right\\|_{F}^{2}}\\\\ &{\\quad+\\left(6\\gamma^{2}n d+36\\gamma^{3}n^{2}d^{3}\\kappa L\\right)\\left(1-\\frac{n\\gamma\\mu}{4}\\right)^{T-d}\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Step 3. Similarly, we refine Lemma 3.3 as follows. By multiplying $\\begin{array}{r}{\\left(1-\\frac{n\\gamma\\mu}{4}\\right)^{T-t}}\\end{array}$ before summing over $t$ in Equation (14), we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=0}^{T}\\left(1-\\frac{n\\gamma\\mu}{4}\\right)^{T-t}\\left\\|\\mathbf{I}_{\\mathbf{u}}\\mathbf{X}^{(t)}\\right\\|_{F}^{2}\\leq300\\gamma^{2}n^{2}d^{4}(T+1)\\sigma^{2}+20\\gamma^{2}n^{3}d^{2}\\displaystyle\\sum_{t=0}^{T}\\mathbb{E}\\|\\nabla f(x_{1}^{(t)})\\|_{2}^{2}}\\\\ {\\displaystyle\\qquad+\\left.6n d\\left\\|\\mathbf{I}_{\\mathbf{u}}\\mathbf{X}^{(0)}\\right\\|_{F}^{2}+40\\gamma^{2}n^{2}d^{3}\\left(1-\\frac{n\\gamma\\mu}{4}\\right)^{T-d}\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right\\|_{F}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Implementing the above result, we have for $\\begin{array}{r}{\\gamma\\leq\\frac{1}{40n d^{2}\\kappa L}}\\end{array}$ that ", "page_idx": 29}, {"type": "equation", "text": "$$\n60\\gamma d^{2}\\kappa L+50\\gamma^{2}n d^{2}L^{2}+300\\gamma^{3}n^{2}d^{4}\\kappa L^{3}\\leq70\\gamma d^{2}\\kappa L,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left\\|x_{1}^{(T)}-x^{*}\\right\\|^{2}\\leq\\left(1-\\frac{n\\gamma\\mu}{4}\\right)^{T}\\left\\|x_{1}^{(0)}-x^{*}\\right\\|^{2}}\\\\ {\\displaystyle\\quad+\\left(-\\frac{n\\gamma}{2L}+15\\gamma^{2}n^{2}+90\\gamma^{3}n^{3}d^{2}\\kappa L+1400\\gamma^{3}n^{3}d^{4}\\kappa L\\right)\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\nabla f(x_{1}^{(t)})\\right\\|^{2}}\\\\ {\\displaystyle\\quad+6\\gamma^{2}n\\sigma^{2}\\left(T+1\\right)+36\\gamma^{3}n^{2}d^{2}\\kappa L\\sigma^{2}\\left(T+1\\right)+21000\\gamma^{3}n^{2}d^{6}\\kappa L\\sigma^{2}\\left(T+1\\right)}\\\\ {\\displaystyle\\quad+\\left(6\\gamma^{2}n d+36\\gamma^{3}n^{2}d^{3}\\kappa L+2800\\gamma^{3}n^{2}d^{5}\\kappa L\\right)\\left(1-\\frac{n\\gamma\\mu}{4}\\right)^{T-d}\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right\\|+420\\gamma^{3}n^{2}d^{3}\\kappa L\\left\\|\\Pi\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Stetth $\\begin{array}{r}{\\sum_{t=0}^{T}\\left\\|\\nabla f(x_{1}^{(t)})\\right\\|^{2}}\\end{array}$ $\\begin{array}{r}{\\gamma\\leq\\frac{1}{100n d^{2}\\kappa L}}\\end{array}$ \uff0c derived as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\,\\frac{n\\gamma}{2L}+15\\gamma^{2}n^{2}+90\\gamma^{3}n^{3}d^{2}\\kappa L+1400\\gamma^{3}n^{3}d^{4}\\kappa L}\\\\ &{=\\!\\!\\frac{n\\gamma}{2L}\\left(-1+30\\gamma n L+180\\gamma^{2}n^{2}d^{2}\\kappa L^{2}+2800\\gamma^{2}n d^{4}\\kappa L\\right)}\\\\ &{\\leq\\!\\!\\frac{n\\gamma}{2L}\\left(\\!-1+\\frac{30}{100}+\\frac{180}{10^{4}}+\\frac{2800}{10^{4}}\\right)\\leq\\!-\\frac{\\gamma n}{20L}<0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, it holds that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|x_{1}^{(T)}-x^{*}\\right\\|^{2}\\leq\\left(1-\\frac{n\\gamma\\mu}{4}\\right)^{T}\\left\\|x_{1}^{(0)}-x^{*}\\right\\|^{2}}\\\\ &{\\quad+\\left.7\\gamma^{2}n\\sigma^{2}\\left(T+1\\right)+21000\\gamma^{3}n^{2}d^{6}\\kappa L\\sigma^{2}\\left(T+1\\right)}\\\\ &{\\quad+\\left.80\\gamma^{2}n d^{3}\\left(1-\\frac{n\\gamma\\mu}{4}\\right)^{T-d}\\right\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right\\|+420\\gamma^{3}n^{2}d^{3}\\kappa L\\left\\|\\Pi_{\\mathbf{u}}\\mathbf{X}^{(0)}\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "A.4 Proof of the Convergence Results ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "A.4.1 Proof of Theorem 2.1 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Invoking Lemma 3.5, with identica itia values $x_{i}^{(0)}$ that implies $\\big\\|\\mathbf{I}\\mathbf{H}_{\\mathbf{u}}\\mathbf{X}^{(0)}\\big\\|_{F}^{2}=0$ , we have $\\frac{1}{{\\cal{T}}+1}\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\nabla f(x_{1}^{(t)})\\right\\|_{2}^{2}\\leq\\frac{8\\Delta_{f}}{\\gamma n(T+1)}+24\\gamma\\sigma^{2}L+2\\cdot10^{4}\\gamma^{2}n d^{6}\\sigma^{2}L^{2}+\\frac{56\\gamma d^{3}L\\left\\|\\nabla{\\bf{F}}({\\bf{X}}^{(0)})\\right\\|_{F}^{2}}{T+1}$ Referring to Lemma 26 in [8], as stated in Lemma A.7, by taking $\\begin{array}{r l r}{A}&{{}=}&{\\frac{8\\Delta_{f}}{n}}\\end{array}$ $\\begin{array}{r c l c r c l}{B}&{=}&{{24\\sigma^{2}L},}&{C}&{=}&{{20000n d^{6}\\sigma^{2}L^{2}}}\\end{array}$ and $\\begin{array}{r l r}{\\alpha}&{{}=}&{100n d^{3}L}\\end{array}$ whenconsidering $\\gamma\\quad=$ $\\begin{array}{r}{\\operatorname*{min}\\{\\left(\\frac{\\Delta_{f}}{3\\sigma^{2}L n(T+1)}\\right)^{\\frac{1}{2}},\\left(\\frac{\\Delta_{f}}{1500n^{2}d^{6}\\sigma^{2}L^{2}(T+1)}\\right)^{\\frac{1}{3}}}\\end{array}$ \\* 100T , we have $\\frac{1}{\\Gamma+1}\\sum_{t=0}^{T}\\mathbb{E}\\left\\|\\nabla f(x_{1}^{(t)})\\right\\|_{2}^{2}\\leq\\frac{32\\sqrt{\\Delta_{f}\\sigma^{2}L}}{\\sqrt{n(T+1)}}+\\frac{240d^{2}\\left(\\sigma^{2}L^{2}\\Delta_{f}^{2}\\right)^{\\frac{1}{3}}}{(\\sqrt{n}(T+1))^{\\frac{2}{3}}}+\\frac{800d^{3}L\\Delta_{f}}{T+1}+\\frac{\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right\\|_{F}^{2}}{n(T+1)}.$ ", "page_idx": 30}, {"type": "text", "text": "A.4.2 Proof of Theorem 2.4 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Invoking Lemma 3.7, with identical values x that implies $\\big\\|\\mathbf{I}\\mathbf{H}_{\\mathbf{u}}\\mathbf{X}^{(0)}\\big\\|_{F}^{2}=0$ we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|x_{1}^{(T)}-x^{*}\\right\\|^{2}\\leq\\left(1-\\displaystyle\\frac{n\\gamma\\mu}{4}\\right)^{T}\\left\\|x_{1}^{(0)}-x^{*}\\right\\|^{2}}\\\\ &{\\quad+\\left.7\\gamma^{2}n\\sigma^{2}\\left(T+1\\right)+21000\\gamma^{3}n^{2}d^{6}\\kappa L\\sigma^{2}\\left(T+1\\right)\\right.}\\\\ &{\\quad+\\left.80\\gamma^{2}n d^{3}\\left(1-\\displaystyle\\frac{n\\gamma\\mu}{4}\\right)^{T-d}\\right\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Considering $\\begin{array}{r}{\\gamma=\\operatorname*{min}\\left\\{\\frac{1}{100n d^{2}\\kappa L},\\frac{16\\log\\left(n(T+1)^{2}\\right)}{n(T+1)\\mu}\\right\\}}\\end{array}$ 16log(n(T+1) ,forT \u22652d we get that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(1-\\cfrac{n\\mu\\gamma}{4}\\right)^{T}\\leq\\operatorname*{max}\\left\\{\\left(1-\\cfrac{1}{400d^{2}\\kappa^{2}}\\right)^{T},\\left(1-\\cfrac{4\\log\\left(n\\left(T+1\\right)^{2}\\right)}{T+1}\\right)^{T}\\right\\}}\\\\ &{\\qquad\\qquad\\leq\\operatorname*{max}\\left\\{\\exp(-\\cfrac{T}{400d^{2}\\kappa^{2}}),\\cfrac{40}{n\\left(T+1\\right)^{2}}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(1-\\cfrac{n\\mu\\gamma}{4}\\right)^{T-d}\\leq\\operatorname*{max}\\left\\{\\left(1-\\cfrac{1}{400d^{2}\\kappa^{2}}\\right)^{T/2},\\left(1-\\cfrac{4\\log\\left(n\\left(T+1\\right)^{2}\\right)}{T+1}\\right)^{T/2}\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\operatorname*{max}\\left\\{\\exp(-\\cfrac{T}{800d^{2}\\kappa^{2}}),\\cfrac{10}{n\\left(T+1\\right)^{2}}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where we use the fact $(1-{\\frac{1}{x}})^{x}\\leq e^{-1}$ and $1-x\\leq\\exp(-x)$ for any $x\\in\\mathbb{R}_{+}$ . Then, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathbb{E}\\left\\|x_{1}^{(T)}-x^{*}\\right\\|^{2}\\leq\\frac{2240\\sigma^{2}\\log(n(T+1)^{2})}{n\\left(T+1\\right)\\mu^{2}}+\\frac{26880000d^{6}\\kappa^{2}\\sigma^{2}\\left(\\log(n(T+1)^{2})\\right)^{2}}{n\\left(T+1\\right)^{2}\\mu^{2}}}\\\\ {\\displaystyle+\\operatorname*{max}\\left\\{\\exp(-\\frac{T}{800d^{2}\\kappa^{2}}),\\frac{40}{n\\left(T+1\\right)^{2}}\\right\\}\\left(\\left\\|x_{1}^{(0)}-x^{*}\\right\\|^{2}+\\frac{1}{n L^{2}}\\left\\|\\nabla\\mathbf{F}(\\mathbf{X}^{(0)})\\right\\|_{F}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "B Additional Experiments ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "For the problem of training a CNN on the MNIST dataset, we have further compared the real-time performance of BTPP with other representative methods. The experiments are conducted on a server equipped with eight Nvidia RTX 3090 GPUs and two Intel Xeon Gold 4310 CPUs, where the communication between GPUs follows the topology requirement of each algorithm. We measure the running time including GPU computation and communication for 13,0o0 iterations. The experimental settings are consistent with those described in Section 4.2. From Figure 4, BTPP outperforms the other algorithms concerning the running time. Additionally, we evaluate BTPP with various branch sizes $B$ , concluding that for relatively small values of $n$ ,abranch size of $B=2$ ismost effective. ", "page_idx": 30}, {"type": "image", "img_path": "3MnXAcTBD3/tmp/540651b8cdd6c89bbe2291da1e1d9e69f8b15748c628317776bcc7e14fd969bd.jpg", "img_caption": ["Figure 4: Real-time performance of BTPP (with different branch size $B$ ) compared with related methods when training CNN over MNIST. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "Furthermore, we consider training VGG13 on the CIFAR10 dataset, with $n=8$ and a batch size of 16. The learning rate and topology configurations are consistent with those described in Section 4.2. Additionally, the case of BTPP with $B\\,=\\,8$ is equivalent to DSGD in a fully connected setting, meaning that they produce identical outputs when using the same random seed. Figure 5 and Figure 6 illustrate that BTPP beats competing algorithms in terms of the convergence rate (against iteration number) and running time. Moreover, a branch size of $B=2$ is optimal. ", "page_idx": 31}, {"type": "image", "img_path": "3MnXAcTBD3/tmp/b5676e6fc7859ab0622251dc4c45f9726fba6810ad5a57b3f0fd43b752b1031e.jpg", "img_caption": ["Figure 5: Performance of BTPP (with different branch size B) compared with related methods for training VGG13 over CIFAR10. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "We further demonstrate that the performance of BTPP can be improved by incorporating a momentum term (with momentum parameter set to 0.9) when data heterogeneity exists or by removing the data heterogeneity, which involves randomly assigning samples to each agent; see Figure 7 and Figure 8. ", "page_idx": 31}, {"type": "image", "img_path": "3MnXAcTBD3/tmp/53ead5ebc8028fe6bba63a215c482789ac40c55481dfd61d6b35e148b2f0ff96.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Figure 6: Real-time performance of BTPP (with different branch size $B$ ) compared with related methods when training VGG13 over CIFAR10. ", "page_idx": 32}, {"type": "image", "img_path": "3MnXAcTBD3/tmp/df47de7fb5756dea7654062e4d8b6819c7fdc769ccbe71b39949541dc6d9ad63.jpg", "img_caption": ["Figure 7: Performance of BTPP with branch size $B=2$ under various configurations when training CNN over MNIST. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "3MnXAcTBD3/tmp/b6a424dca9d1aefa42439d3bc2443bdfa5a191a7897008a74a2ea78da6c80039.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Figure 8: Performance of BTPP with branch size $B=2$ under various configurations when training VGG13 0ver CIFAR10. ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Claims ", "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our paper meticulously delineates its primary contributions in both the abstract (lines 8-9) and the introduction (subsection 1.2, lines 105-121). ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper. ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. \u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. \u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: At the end of section 1.1, lines 100-104. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The assumptions are in subsection 1.3, lines 129-132. The proofs of all theoretical results are in the Appendix and subsection 2.1. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. ", "page_idx": 33}, {"type": "text", "text": "\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We present the experiment details in section 4 and upload our code in the link shown in the abstract. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b)If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct thedataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We provide our code with the link shown in lines 10-11. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code. \u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details. ", "page_idx": 34}, {"type": "text", "text": "\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We provide all the experimental details and settings in Section 4 and our code link. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We have repeated some experiments with different seeds and reported the averaged performance. See Section 4 for details. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified. ", "page_idx": 35}, {"type": "text", "text": "\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates). \u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: See line 255 for reference. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We have cited the original paper that produced the code package or dataset. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]