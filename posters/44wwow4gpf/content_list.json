[{"type": "text", "text": "Learning symmetries via weight-sharing with doubly stochastic tensors ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Putri A. van der Linden1,\u2217 Alejandro Garc\u00eda-Castellanos1 Sharvaree Vadgama1 Thijs P. Kuipers2,3 Erik J. Bekkers1 ", "page_idx": 0}, {"type": "text", "text": "1Amsterdam Machine Learning Lab, University of Amsterdam 2Department of Biomedical Engineering and Physics, Amsterdam UMC, the Netherlands 3Department of Radiology and Nuclear Medicine, Amsterdam UMC, the Netherlands ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Group equivariance has emerged as a valuable inductive bias in deep learning, enhancing generalization, data efficiency, and robustness. Classically, group equivariant methods require the groups of interest to be known beforehand, which may not be realistic for real-world data. Additionally, baking in fixed group equivariance may impose overly restrictive constraints on model architecture. This highlights the need for methods that can dynamically discover and apply symmetries as soft constraints. For neural network architectures, equivariance is commonly achieved through group transformations of a canonical weight tensor, resulting in weight sharing over a given group $G$ . In this work, we propose to learn such a weightsharing scheme by defining a collection of learnable doubly stochastic matrices that act as soft permutation matrices on canonical weight tensors, which can take regular group representations as a special case. This yields learnable kernel transformations that are jointly optimized with downstream tasks. We show that when the dataset exhibits strong symmetries, the permutation matrices will converge to regular group representations and our weight-sharing networks effectively become regular group convolutions. Additionally, the flexibility of the method enables it to effectively pick up on partial symmetries. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Equivariance has emerged as a beneficial inductive bias in deep learning, enhancing performance across a variety of tasks. By constraining the function space to adhere to specific symmetries, models not only generalize better but also achieve greater parameter efficiency [8, 9]. For instance, integrating group symmetry principles into generative models has enhanced sample generation and efficient learning of data distributions, particularly benefiting areas such as vision and molecular generation [11, 5]. ", "page_idx": 0}, {"type": "text", "text": "The most well-known and transformative models in equivariant deep learning are convolutional neural networks (CNNs) [17], which achieve translation equivariance by translating learnable kernels to every position in the input. This design ensures that the weights defining the kernels are shared across all translations, so that if the input is translated, the output features are correspondingly translated; in other words, equivariance is achieved through weight sharing. In the seminal work by Cohen and Welling [9], this concept was extended to generalize weight-sharing under any discrete group of symmetries, resulting in the group-equivariant CNN (G-CNN). G-CNNs enable G-equivariance to a broader range of symmetries, such as rotation, reflection, and scale [9, 4, 32, 27], thereby expanding the applicability of CNNs to more complex data transformations. ", "page_idx": 0}, {"type": "text", "text": "However, the impact of G-CNNs is closely tied to the presence of specific inductive biases in the data. When exact symmetries, such as E(3) group symmetries in molecular point cloud data, are known to exist, G-CNNs excel. Yet, for many types of data, including natural images and sequence data, these exact symmetries are not present, leading to overly constrained models that can suffer in performance [30, 22, 2, 29]. In scenarios with limited data and for certain critical downstream tasks, having appropriate inductive biases becomes even more crucial. ", "page_idx": 1}, {"type": "text", "text": "To avoid overly constraining models, symmetries must be chosen carefully to match those in the input data. This requires prior knowledge of these symmetries, which may not always be available. Furthermore, different symmetries at different scales can coexist, making manual determination of these symmetries highly impractical. To address this, multiple works have proposed partial or relaxed G-CNNs [22, 6, 2, 30]. Such models are initialized to be fully equivariant to some groups and learn from the data to partially break equivariance on a per-layer basis where necessary. However, these methods still require specifying which symmetries to include and can only achieve equivariance to subsets of these symmetries. ", "page_idx": 1}, {"type": "text", "text": "In this work, we tackle the challenge of specifying group symmetries upfront by introducing a general weight-sharing scheme. Our method can represent G-CNNs as a special case but is not limited to exact equivariance constraints, offering greater flexibility in handling various symmetries in the data. Inspired by the idea that group equivariance for finite groups can be achieved through weight-sharing patterns on a set of base weights [21], we propose learning the symmetries directly from the data on a per-layer basis, requiring no prior knowledge of the possible symmetries. ", "page_idx": 1}, {"type": "text", "text": "We leverage the fact that regular group representations act as permutations and that the expectation of random variables defined over this set of permutations is a doubly stochastic matrix [7]. This implies that regular partial group transformation can be approximated by a stack of doubly stochastic matrices which essentially act as (soft) permutation matrices. Consequently, we learn a set of doubly stochastic matrices through the Sinkhorn operator [26], resulting in weight-sharing under learnable group transformations. ", "page_idx": 1}, {"type": "text", "text": "We summarize our contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a novel weight-sharing scheme that can adapt to group actions when certain symmetry transformations are present in the data, enhancing model flexibility and performance. \u2022 We present empirical results on image benchmarks, demonstrating the effectiveness of our approach in learning relevant weight-sharing schemes when there are clear symmetries. \u2022 The proposed method outpaces models configured with known symmetries in environments where they are only partially present. Moreover, in the absence of predefined symmetries, it adeptly identifies effective weight-sharing patterns, matching the performance of fully flexible, non-weight-sharing models. \u2022 We provide analyses of the learned symmetries on some controlled toy settings. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Partial or relaxed equivariance Methods such as [22, 6, 2] learn partial equivariance by learning distributions over transformations, and thereby aim to learn partial or relaxed equivariances from data by sampling some group elements more often than others. [30] tries to relax equivariance by introducing learnable equivariance-breaking components. [10, 29] Relax equivariance constraints by parameterizing layers as (linear) combinations of fully flexible, non-constrained components and constrained equivariant components. Finally, several works model soft invariances through learning the amount of data augmentation in the data or model relevant for a given task, either through learned distributions on the group or (automatic) hyperparameter selection [6, 12, 20]. However, these methods require pre-specified sets of symmetry transformations and/or group structure to be known beforehand. In contrast, we aim to pick up the relevant symmetry transformations during training. ", "page_idx": 1}, {"type": "text", "text": "Symmetry discovery methods [24, 19] learn the group structure via (irreducible) group representations. [24] proposed to learn the Fourier transform of finite compact commutative groups and their corresponding bispectrum by learning to separate orbits on our dataset. This approach can be extended to non-commutative finite groups leveraging advanced unitary representation theory [19]. However, these methods are constrained to finite-dimensional groups and require specific orbit-predicting datasets. In contrast, our approach learns a relaxation of regular group representations\u2014as opposed to irreducible representations. Moreover, our approach is not merely capable of learning symmetries, it subsequently utilizes them in a regular group-convolution-type architecture. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Weight-sharing methods Previous studies have demonstrated that equivariance to finite groups can be achieved through weight-sharing schemes applied to model parameters. Notably, the works in [21], [34], and [33] provide foundational insights into this approach. In [34], weight-sharing patterns are learned by using a matrix that operates on flattened canonical weight tensors, effectively inducing weight sharing. They additionally prove that for finite groups, there are weight-sharing matrices capable of implementing the corresponding group convolution. However, their approach requires learning these patterns through meta-learning and modeling the weight-sharing matrix as an unconstrained tensor. In contrast, our method learns weight sharing directly in conjunction with the downstream task and enforces the matrix to be doubly stochastic, thereby representing soft permutations by design. ", "page_idx": 2}, {"type": "text", "text": "[33] presents an approach closely aligned with ours, where a weight-sharing scheme is learned that is characterized by row-stochastic entries. Their method involves both inner- and outer-loop optimization and demonstrates the ability to uncover relevant weight-sharing patterns in straightforward scenarios. However, their approach does not support joint optimization of the canonical weights and weight-sharing pattern, and they acknowledge difficulties in extending their method to higher input dimensionalities. Unlike [33], we enforce both row and column stochasticity. Additionally, we can optimize for the sharing pattern and weight tensors jointly, and successfully apply our approach to more interesting data domains such as image processing. ", "page_idx": 2}, {"type": "text", "text": "In [28], group actions are integrated directly into the learning process of the downstream task. This method involves learning a set of generator matrices that operate via matrix multiplication on flattened input vectors. However, this approach constrains the operators to members of finite cyclic groups, which inherently limits their ability to represent more complex group structures. Furthermore, this restriction precludes the possibility of modeling partial equivariances, reducing the flexibility and applicability of the model to more diverse or complex scenarios. ", "page_idx": 2}, {"type": "text", "text": "3 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We begin by revisiting group convolutional methods in the context of image processing, followed by their relation to weight-sharing schemes. We then proceed to briefly cover the Sinkhorn operator, which is the main mechanism through which we acquire weight-sharing schemes. Some familiarity with group theory is assumed, and essential concepts will be outlined in the following. ", "page_idx": 2}, {"type": "text", "text": "Groups We are interested in (symmetry) groups, which are algebraic constructs that consist of a set $G$ and a group product\u2014which we denote as a juxtaposition\u2014that satisfies certain axioms, such as the existence of an identity element $e\\in G$ such that for all $g\\in G$ we have $e g=g e=g$ , closure such that for all $g,h\\in G$ we have $g h\\in G$ , the existence of an inverse $g^{-1}$ for each $g$ such that $g^{-1}g=e$ , and associativity such that for all $g,h,i\\in G$ we have $(g h)i=g(h i)$ . ", "page_idx": 2}, {"type": "text", "text": "Representations In the context of geometric deep learning [8], it is most useful to think of groups as transformation groups, and the group structure describes how transformations relate to each other. Specifically, group representations $\\rho:G\\to G L(V)$ are concrete operators that transform elements in a vector space $V$ in a way that adheres to the group structure (they are group homomorphisms). That is, to each group element $g$ , we can associate a linear transformation $\\rho(g)\\,\\in\\,G L(\\bar{V})$ , with $G L(V)$ the set of linear invertible transformations on vector space $V$ . ", "page_idx": 2}, {"type": "text", "text": "Group convolution Concretely, such representations can be used to define group convolutions. Consider feature maps $f:X\\to{\\dot{\\mathbb{R}}}^{D}$ over some domain on which a group action is defined, i.e., over a $G$ -space. E.g., for images (signals over $X=\\mathbb{R}^{2}$ ) we could consider the group $G=(\\mathbb{R}^{2},+)$ of translations, which acts on $X$ via $g x=x+y$ , with $g=(y)$ a translation by $\\boldsymbol{y}\\in\\mathbb{R}^{2}$ . While the group $G$ merely defines how two transformations $g,h\\in G$ applied one after the other correspond to a net translation $g h\\in G$ , a representation $\\rho$ concretely describes how data is transformed. E.g., signals $f:X\\to\\mathbb{R}$ can be transformed via the left-regular representation $[\\rho(g)f](x):=f(g^{-1}x)$ , which in the case of images and the translation group is given by $[\\rho(g)f](x)=f(\\overbar{x}-y)$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "In general, group convolution is defined as transforming a base kernel under every possible group action, and for every transformation taking the inner product with the underlying data via ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Big|\\,G\\mathrm{-convolution,inner~product~form:}\\Big|\\qquad\\qquad\\qquad\\qquad(k\\star_{G}f)(g)=\\langle\\rho(g)k,f\\rangle\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with $\\langle\\cdot,\\cdot\\rangle$ denoting the inner product. For images, which are essentially functions over the group $G=(\\mathbb{R}^{2},+)$ , and taking $\\textstyle\\langle k,{\\bar{f}}\\rangle:=\\int_{X}k(x)f({\\bar{x}})\\mathrm{d}x$ the standard inner product, Eq. (1) boils down to the standard cross-correlation operator2: ", "page_idx": 3}, {"type": "table", "img_path": "44WWOW4GPF/tmp/375c268b219cec7bef7d6c259506a718363395c861203cee9655113b275ad3af.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle({k\\star f})({g})=\\int_{G}k({g}^{-1}h)f(h)\\mathrm{d}h}\\\\ {\\displaystyle({k\\star f})({x})=\\int_{\\mathbb R^{2}}k({x}^{\\prime}-x)f({x}^{\\prime})\\mathrm{d}{x}^{\\prime}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Semi-direct product groups When equivariance to larger symmetry groups is desired, e.g. in the case of $G=S E(2)$ roto-translation equivariance for images with domain $X=\\mathbb{R}^{2}$ , a lifting convolution can be used to generate signals over the group $G$ . In essence it is still of the form of (2), however integration is over $X$ instead of over $G$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{\\Big[G\\!-\\!\\operatorname{lifting}\\,\\mathrm{Convolution:}\\Big]}{\\Big[S E(2)\\!-\\!\\operatorname{lifting}\\,\\mathrm{Convolution:}}\\qquad}&{(k\\star f)(g)=\\displaystyle\\int_{{\\cal R}^{2}}k(g^{-1}x)f(x)\\mathrm{d}x}\\\\ {\\displaystyle\\frac{\\big[S E(2)\\!-\\!\\operatorname{lifting}\\,\\mathrm{Convolution:}\\Big]}{(S E(2)\\!-\\!\\operatorname{lifting}\\,\\mathrm{Convolution:}}\\qquad}&{(k\\star f)(x,{\\bf R})=\\displaystyle\\int_{{\\mathbb R}^{2}}k({\\bf R}^{-1}(x^{\\prime}-x))f(x^{\\prime})\\mathrm{d}x^{\\prime}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with $g\\,=\\,(x,\\mathbf{R})\\,\\in\\,(\\mathbb{R}^{2},+)\\,\\rtimes\\,S O(2)$ . The roto-translation group is an instance of a semi-direct product group (denoted with $\\rtimes$ ) between the translation and rotation group, which has the practical benefit that a stack of rotated kernels can be precomputed [9], and the translation part efficiently be taken care of via optimized Conv2D operators. Namely via $(k\\star f)(x,{\\bf R}_{i})=\\mathsf{C o n v2D}[k_{i},f]$ , with $k_{i}:=k(\\mathbf{R}_{i}^{-1}x)$ . This trick can also be applied for full group convolutions (2). ", "page_idx": 3}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our objective is to uncover the underlying symmetries of datasets whose exact symmetries may not be known, in a manner that is both parameter-efficient and free from rigid group constraints. To achieve this, we re-examine regular representations and their critical role in generating various instantiations of the fundamental group convolution equation (1). Moving from the continuous setting to concrete instantiations, we derive weight-sharing from a finite-dimensional vector of base weights by interpreting regular representations as permutations. We further analyze the characteristics of this weight-sharing approach, proposing the use of doubly stochastic matrices. This analysis forms the foundation for developing weight-sharing layers that adaptively learn dataset symmetries. ", "page_idx": 3}, {"type": "text", "text": "4.1 Weight-sharing through permutations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the current section, we first establish the connection between representations, weight-sharing and permutations. We then proceed to provide practical instantiations as ingredients for the proposed weight-sharing convolutional layers. ", "page_idx": 3}, {"type": "text", "text": "Weight-sharing through learnable representations To achieve weight-sharing over a finite set of symmetries, we define learnable representations $\\rho:G\\to G L(V)$ . Specifically, we assign a learnable transformation (permutation matrix) to each element in $G$ . It is important to note that we refer to $G$ and $\\rho$ as a \"group\" and \"representation\" in a loose sense, as we relax the homomorphism property and do not initially endow $G$ with a group product. Consequently, the collection of linear transformations does not form a group representation a priori. However, our proposed method is capable of modeling this structure in principle. ", "page_idx": 3}, {"type": "text", "text": "Regular representations as permutations Consider the case of a continuous linear Lie group $G$ , e.g. of rotations $S O(2)$ , and a real signal $f:G\\to\\mathbb{R}$ over it. This signal is to be considered an infinite dimensional vector with continuous \"indices\" $g\\in G$ that index the vector elements $f(g)$ . To emphasize the resemblance of the regular representation to permutation matrices we write it as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left[\\mathrm{Regular\\;representation\\;in\\;integral\\;form:}\\right]\\qquad\\qquad\\left[\\rho(g)f\\right](i)=\\int_{G}P_{g}(i,h)f(h)\\mathrm{d}h\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with $P_{g}(i,h)=\\delta_{g-1i}(h)$ a kernel that for each $g$ maps $h$ to a new \"index\" $i\\in G$ , where the Dirac delta essentially codes for the group product as $\\delta_{g^{-1}h}(i)$ is non-zero only if $g^{-1}i=h\\Leftrightarrow g\\cdot h=i$ . ", "page_idx": 4}, {"type": "text", "text": "The discrete counterpart of such an integral transform is matrix-vector multiplication with a matrix $\\rho(g)={\\bf P}_{g}$ with entries $P_{g h i}=1$ if $g h=i$ and zero otherwise. As a concrete example, consider a signal $f:G\\to\\mathbb{R}$ over the discrete group $G=C_{4}$ of cyclic permutations of size 4, i.e., a periodic signal of length four, then we could vectorize it as $f\\rightarrow\\mathbf{v}$ with entries $v_{g}=f(g)$ and the regular representation becomes a simple cyclic permutation matrix with ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{P}_{0}={\\left(\\begin{array}{l l l l}{1}&{0}&{0}&{0}\\\\ {0}&{1}&{0}&{0}\\\\ {0}&{0}&{1}&{0}\\\\ {0}&{0}&{0}&{1}\\end{array}\\right)}\\,,\\ \\ \\mathbf{P}_{1}={\\left(\\begin{array}{l l l l}{0}&{0}&{0}&{1}\\\\ {1}&{0}&{0}&{0}\\\\ {0}&{1}&{0}&{0}\\\\ {0}&{0}&{1}&{0}\\end{array}\\right)}\\,,\\ \\ \\mathbf{P}_{2}={\\left(\\begin{array}{l l l l}{0}&{0}&{1}&{0}\\\\ {0}&{0}&{0}&{1}\\\\ {1}&{0}&{0}&{0}\\\\ {0}&{1}&{0}&{0}\\end{array}\\right)}\\,,\\ \\ \\mathbf{P}_{3}={\\left(\\begin{array}{l l l l}{0}&{1}&{0}&{0}\\\\ {0}&{0}&{1}&{0}\\\\ {0}&{0}&{0}&{1}\\\\ {1}&{0}&{0}&{0}\\end{array}\\right)}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We refer to the collection of permutation matrices $\\mathbf{P}\\in[0,1]^{|G|\\times|G|\\times|G|}$ as the permutation tensor. ", "page_idx": 4}, {"type": "text", "text": "Recall that the regular convolution operator (2) is not only defined for signals over groups $G$ but for $G$ -spaces $X$ , in general, as in Eq. (4). It requires a representation that acts on signals (convolution kernels) over $X$ . However, since the group $G$ acts by automorphisms (bijections) on the space $X$ , we can define the regular representation\u2014as before\u2014using permutation integral with $P_{g}(x,x^{\\prime})=\\delta_{g^{-1}x^{\\prime}}$ that effectively sends old \"indices\" $x^{\\prime}$ to their new location $x$ , or concretely via permutation matrices $\\mathbf{P}$ of shape $|{\\dot{G}}|\\times|X|\\times|X|$ , where $X$ is the domain of the signal that is transformed\u2014which can also be $G$ . ", "page_idx": 4}, {"type": "text", "text": "In practice, it is common that we perform a discretization of continuous signals $f$ , e.g., when we work with images, we discretize the continuous signal $f:\\mathbb{R}^{2}\\to\\mathbb{R}^{C}$ to $\\mathbf{f}:\\overline{{\\mathbb{Z}^{2}}}\\to\\mathbb{R}^{C}$ . Therefore, the group action over the discretized signal should approximate the group action over the continuous signal. We can see that in some cases, the group representation of the action on the discrete signal can still be implemented using permutation matrices. E.g., $90^{\\circ}$ rotations (in $C_{4}$ ) applied to images merely permute the pixels. However, for finer discretizations, e.g. using $45^{\\circ}$ rotations, interpolation can be used as a form of approximate permutations [16, Fig. 2]. ", "page_idx": 4}, {"type": "text", "text": "Weight sharing In a discrete group setting, we then see that convolution (1) is obtained by multiplications with matrices obtained by stacking permuted base kernel weights $\\mathbf{w}\\in\\mathbb{R}^{|X|}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r}{\\frac{\\left|\\mathbf{D}\\mathbf{iscrete}\\;G\\mathbf{\\cdotconvolution:}\\right|}{\\mathbf{D}\\mathbf{iscrete}\\;G\\mathbf{\\cdotconvolution:}}\\,}&{}&&{\\mathbf{f}^{o u t}=\\mathbf{W}\\mathbf{f}^{i n}\\,,\\quad\\mathrm{with}\\quad\\mathbf{W}=\\mathbf{P}\\mathbf{w}:=\\left(\\begin{array}{c}{(\\mathbf{P}_{0}\\mathbf{w})^{T}}\\\\ {(\\mathbf{P}_{1}\\mathbf{w})^{T}}\\\\ {\\vdots}\\end{array}\\right)\\in\\mathbb{R}^{|G|\\times|X|}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "E.g., a group convolution over $C_{4}$ is implemented with a matrix of the form $\\mathbf{W}=\\left(\\begin{array}{l}{w_{0}\\ w_{1}\\ w_{2}\\ w_{3}}\\\\ {w_{3}\\ w_{0}\\ w_{1}\\ w_{2}}\\\\ {w_{2}\\ w_{3}\\ w_{0}\\ w_{1}}\\\\ {w_{1}\\ w_{2}\\ w_{3}\\ w_{4}}\\end{array}\\right)$ ", "page_idx": 4}, {"type": "text", "text": "Regular representations allow for element-wise activations An important practical element of using regular representations to define group convolutions is that permutations commute with element-wise activations, namely, $[\\rho(g)\\sigma(\\check{f})](\\dot{i})=\\sigma(f)(g^{-1}i)=\\sigma(\\dot{f}(g^{-1}i))=\\sigma([\\rho(g)f](i))$ . In contrast, steerable methods\u2014based on irreducible representations (cf. App. A.1)\u2014require specialized activation functions so as not to break group equivariance. Such activations in practice are not as effective as the classic element-wise activations such as ReLU [31]: they may introduce discretization artifacts that disrupt exact equivariance, ultimately constraining the method\u2019s expressivity [5]. Hence, when learning weight-sharing schemes\u2014as is our objective\u2014it is preferred to achieve weight-sharing using regular representations without the risk of breaking equivariance by using standard activation functions. ", "page_idx": 4}, {"type": "image", "img_path": "44WWOW4GPF/tmp/a5f17e3cda5616a92c94b66b259909090442aa55ee2be7ca5f25b67fa8f7bdd3.jpg", "img_caption": ["Figure 1: Kernel stacks are acquired through a learned weight-sharing scheme applied to a set of flattened base kernels. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.2 Learnable doubly stochastic tensors ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Having established the link between regular representations and soft permutations, we now motivate the use of doubly stochastic matrices as natural candidates for their implementation. Specifically, we utilize the fact that the expected value of random variables over this set of permutations yields a doubly stochastic matrix [7]. ", "page_idx": 5}, {"type": "text", "text": "Doubly stochastic matrices Let $\\mathcal{S}_{\\infty}$ $(S_{n})$ denote respectively the system of infinite $(n\\times n)$ doubly stochastic matrices, i.e. matrices $S\\equiv\\{s_{i j}\\in[0,1]:i,j=1,2,\\ldots(,n)\\}$ such that $\\textstyle\\sum_{j}s_{i j}=1$ , and $\\textstyle\\sum_{i}s_{i j}=1$ . Let $\\mathcal{P}_{\\infty}\\left(\\mathcal{P}_{n}\\right)$ denote respectively the system of infinite $(n\\times n)$ permutation matrices, i.e. matrices $P\\equiv\\{p_{i j}\\in\\{0,1\\}:i,j=1,2,\\ldots(,n)\\}$ such that $\\textstyle\\sum_{j}p_{i j}=1$ , and $\\textstyle\\sum_{i}p_{i j}=1$ . Note that for any permutation tensor $\\mathbf{P}\\,\\in\\,\\mathbb{R}^{|G|\\times|X|\\times|X|}$ , where $X$ is the domain of the signal that is transformed by the group $G$ , then $\\mathbf{P}_{g}\\in\\mathcal{P}_{|X|}$ for every $g\\in G$ . ", "page_idx": 5}, {"type": "text", "text": "Then, by Birkhoff\u2019s Theorem [7], and its extension to infinite dimensional matrices, commonly called Birkhoff\u2019s Problem 111 [13, 23, 14, 3], we have that any convex combination of permutation matrices will be equal to a doubly stochastic matrix, i.e, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sum_{P\\in{\\mathcal P}_{n}}\\lambda(P)P=S\\in{\\mathcal S}_{n},\\;\\operatorname{with}\\;\\sum_{P\\in{\\mathcal P}_{n}}\\lambda(P)=1\\quad(\\forall n\\in\\mathbb{N}\\cup\\{+\\infty\\})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda(P)$ gives a probability measure supported on a finite subset of the set of permutation matrices $\\mathcal{P}$ . Therefore we may state that ", "page_idx": 5}, {"type": "text", "text": "Using doubly stochastic matrices, we can model approximate equivariance as defined in [22]. ", "page_idx": 5}, {"type": "text", "text": "I.e., let $S$ be a random variable over $\\{\\mathbf{P}_{g}\\in\\mathcal{P}_{|X|}\\,\\mid\\,g\\,\\in\\,G\\}$ with a finitely supported probability measure $\\mathbb{P}[S=\\mathbf{P}_{g}]=\\lambda(\\mathbf{P}_{g})$ for every $g\\in G$ , then $\\begin{array}{r}{\\mathbf{S}=\\mathbb{E}[S]=\\sum_{g\\in G}\\mathbb{P}[S=\\mathbf{P}_{g}]\\mathbf{P}_{g}}\\end{array}$ is a doubly stochastic matrix. We want to note that S can be seen as a generalization of the convolution matrix presented in [18]. ", "page_idx": 5}, {"type": "text", "text": "Sinkhorn operator The Sinkhorn operator [26, 1] transforms an arbitrary matrix to a doubly stochastic one through iterative row and column normalization, provided that the number of iterations is large enough. That is, initialize a tensor ${\\bf X}\\in\\mathbb{R}^{N\\times N}$ , then it will converge to a doubly stochastic tensor via the following algorithm: ", "page_idx": 5}, {"type": "equation", "text": "$$\nS^{0}(\\mathbf{X})=\\exp(\\mathbf{X})\\,,\\qquad\\quad S^{l}(\\mathbf{X})=T_{c}(T_{r}(S^{l-1}(\\mathbf{X})))\\,,\\qquad\\quad S_{N}\\quad\\ni\\quad\\mathbf{S}=\\operatorname*{lim}_{l\\to\\infty}S^{l}(\\mathbf{X})\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $T_{c}$ and $T_{r}$ the normalization operators over the rows and columns, respectively, defined as $T_{c}\\,=\\,X\\odot{\\bf1}_{N}{\\bf1}_{N}^{T}{\\bf X}$ and $T_{r}\\,=\\,{\\bf X}\\odot{\\bf X}\\,{\\bf1}_{N}{\\bf1}_{N}^{T}$ , where $\\oslash$ denotes elementwise division, $\\mathrm{sum}_{c}(\\cdot)$ sum c (X) sumr(X) ", "page_idx": 5}, {"type": "text", "text": "$\\mathrm{sum}_{r}(\\cdot)$ perform column-wise and row-wise summation, respectively. ", "page_idx": 5}, {"type": "image", "img_path": "44WWOW4GPF/tmp/bf70688dea698161b916b38e2a800766dce6d768e9d0f998f48e71f25d70cd40.jpg", "img_caption": ["Figure 2: Learned kernels from the lifting layer of WSCNN, applied to rotated MNIST and reshaped to [No.elem., $C_{o u t}]$ . Since $\\mathbf{R}_{1}$ is set as the identity operator, the first column displays the raw kernels. ", "Figure 3: Comparison of $C_{4}$ representations and the representation stack learned by the lifting layer on the rotated MNIST dataset. Top: learned representations. Bottom: permutations for $C_{4}$ on $d=25$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Our proposal: Weight Sharing Convolutional Neural Networks Having established the foundational elements, we now define our Weight Sharing Convolutional Neural Networks (WSCNNs). We let $\\Theta_{i}^{l}\\in\\mathbb{R}^{|X|\\times|X|}$ be a collection of learnable parameters that parametrize the representation stack of the layer $l$ as $\\mathbf{R}^{l}=(S^{K}(\\Theta_{0}^{l}),S^{K}(\\Theta_{1}^{l}),...,S^{K}(\\Theta_{N}^{l}))^{T}\\,\\in\\,[0,1]^{|G|\\times|X|\\times|X|}$ . i.e., we parameterize this tensor as a stack of $|G|$ approximate doubly stochastic $|X|$ -dimensional matrices, wherein stochasticity is enforced via $K$ applications of the Sinkhorn operator. We also define a set of learnable base weights $\\pmb{\\theta}^{l}\\in\\mathbb{R}^{|X|\\times C_{o u t}\\times C_{i n}}$ . The WSCNN layer is then simply given by (7) with $\\mathbf{P}$ and w respectively replaced by $\\mathbf{R}^{l}$ and $\\pmb{\\theta}^{l}$ . ", "page_idx": 6}, {"type": "text", "text": "We further note that on image data $|X|$ can be large, making the discrete matrix form implementation computationally demanding. Hence, we consider semi-direct product group parametrizations for $G$ , in which we let $G$ be of the form $(\\mathbb{R}^{n},+)\\rtimes H$ , with $H$ a learnable (approximate) group. Then, the representation stacks will merely be of shape $|H|\\times|X^{\\prime}|\\times|X^{\\prime}|$ , with $|H|\\ll|G|$ the size of the sub-group and $\\left|X^{\\prime}\\right|$ the number of pixels that support the convolution kernel. A WSCNN layer is then efficiently implemented via a $\\mathsf{C o n v2D}[\\mathbf{f},\\mathbf{R}^{l}\\pmb{\\theta}^{l}]$ . For group convolutions (after the lifting layer) the representation stacks will be of shape $|\\dot{H^{\\prime}}|\\times(|\\dot{X^{\\prime}}|\\times|\\bar{H}|)\\times(|X^{\\prime}|\\times|H|)$ . Computational scaling requirements can be found in Appendix C.4. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first demonstrate that the proposed weight sharing method can effectively pick up on useful weight sharing patterns when trained on image datasets with different equivariance priors. We then proceed to show the method can effectively handle settings where partial symmetries are present in the data, and further analyze the learned weight-sharing structures on a suite of toy datasets. Model architectures, regularizers (norm, ent) and design choices can be found in Appendix C.4 and C.1, respectively. An analysis of computational requirements can be found in Appendix C.4 ", "page_idx": 6}, {"type": "text", "text": "5.1 Image datasets and equivariance priors ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We assess the efficacy of our proposed weight-sharing scheme in recognizing data symmetries through experiments on datasets subjected to various data augmentations. Specifically, we evaluate our model on MNIST images that have been rotated (with full $S O(2)$ rotations) and scaled (with scaling factors between [0.3, 1.0]). We categorize these datasets based on their data symmetry characteristics: MNIST with rotation and scaling as datasets with known symmetries, and CIFAR-10 with filps as a dataset with unknown symmetries. ", "page_idx": 6}, {"type": "text", "text": "For RotatedMNIST, we regard a C4-group convolutional model as a benchmark since it has been equipped with a subgroup of the underlying data symmetries a priori. Additionally, we contrast our results with a non-constrained CNN model which has double the number of channels, allowing for free optimization without symmetry constraints. As such, our evaluations are benchmarked against two distinct models: 1) a group convolutional model that is equivariant to discrete rotations, embodying fixed equivariance constraints, and 2) a standard CNN that adheres only to translational equivariance, without additional constraints. ", "page_idx": 6}, {"type": "table", "img_path": "44WWOW4GPF/tmp/17184a2898296e1933450f97c3c9372975edbcc99da26ef8fbc087837e895cbf.jpg", "table_caption": ["Table 1: Test accuracy on MNIST for both rotation and scaling transformations. Additional parameters induced by weight-sharing are marked $(+)$ . Parameter counts denoted in millions (M) or thousands (K). Best-performing models (equivalent within $<1\\%$ ) marked bold. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "44WWOW4GPF/tmp/133874584b0ae2e54d5d580b8ff9cb793621679aaf776862e5efc770f8b39c65.jpg", "table_caption": ["Table 2: Test accuracy on CIFAR-10. Number of elements denotes the number of group elements used in group convolutional models. Additional parameters induced by weight-sharing are marked $(+)$ . Parameter counts denoted in millions (M) or thousands (K). Best-performing models (equivalent within $<1\\%$ ) marked bold. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "This experimental setup positions the standard CNN as the most flexible model lacking predefined inductive biases. In contrast, the group convolutional neural network (GCNN) is characterized by fixed weight sharing, while our proposed weight-sharing CNN (WSCNN) introduces a semi-flexible, learnable weight-sharing mechanism. Note that we explicitly distinguish between the number of free model and kernel parameters and the additional parameters introduced by our weight sharing scheme throughout results (marked by $+.$ ). ", "page_idx": 7}, {"type": "text", "text": "Results can be found in Tab. 1. When there is a clear misalignment between the model and data symmetries, the constraints imposed by the model hinder performance, as demonstrated by the C4-GCNN on the scaled MNIST dataset. Notably, our proposed method consistently achieves high performance across all datasets without requiring fixed group specifications. Furthermore, visual inspection of the learned kernels indicates that WSCNN adapts to the underlying data symmetries by effectively rotating kernels, as shown in Figure 2. Additionally, analysis of the learned representation stack reveals that it closely resembles elements of $C_{4}$ permutations, further demonstrating the model\u2019s capability to internalize and replicate data transformations (see Figure 3 and Appendix B.3). ", "page_idx": 7}, {"type": "text", "text": "Additionally, we test the model on CIFAR-10, representing a dataset with possibly more complex/unknown symmetry structures, and CIFAR-10 with horizontal filps (which cannot be represented by $C_{N}$ transformations). Results can be found in Tab. 2, with detailed training and model specifications available in Appendix C.4. We compare against the (possibly misspecified) C4-GCNN, and several unconstrained CNNs models with varying number of channels: 1) CNN-32 matched in free kernel size, 2) CNN-64 matched in parameter budget, and 3) CNN-128 matched in effective kernel size (calculated as $|G|\\times\\mathrm{channels}=4\\times32=128)$ ). Despite the kernel constraints in WSCNN, it achieves performance comparable to that of the unconstrained 128-channel CNN (within $<1\\%$ accuracy), at a significantly smaller parameter budget $.2.1\\,\\mathrm{M}$ vs. $6.5\\;\\mathrm{M}$ ). ", "page_idx": 7}, {"type": "text", "text": "5.2 Learning partial equivariances ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We show our method is able to pick up on partial symmetries by testing it on MNIST with rotations sampled from a subset of SO(2) and compare it to the C4-GCNN. Additionally, we show results on CIFAR-10 with horizontal filps, which is a commonly used train augmentation. Results can be found in Tab. 3 and Tab. 4. ", "page_idx": 7}, {"type": "table", "img_path": "44WWOW4GPF/tmp/1b823fc93975cb13846e3ccbee7039212cc8120d040d93a595f7d6be3fdf9abc.jpg", "table_caption": ["Table 3: Test accuracy on MNIST with partial rotations. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "44WWOW4GPF/tmp/b8758318276311e61feaacfc19f65f884ff8c328fd1236a8827a497b70f0ccbd.jpg", "table_caption": ["Table 4: Test accuracy on CIFAR-10 with horizontal filps. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Additionally, We proceed to test the model\u2019s capability to detect data symmetries by applying it to a suite of toy problems, wherein the datasets comprise noisy $G$ -transformed samples. Details on the data generation framework are provided in Appendix B. Our testing employs a single-layer setup aimed at learning a collection of kernels that ideally match each data sample, considering inherent noise. This involves training the model to identify a set of base kernels and their pertinent transformations, effectively adapting to the variations presented by the toy problems. ", "page_idx": 8}, {"type": "text", "text": "6 Discussion and Future Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We demonstrated a method that can effectively identifies underlying symmetries in data, even without strict group constraints. Our approach is uniquely capable of learning both partial and approximate symmetries, which more closely mirrors the complexity found in real-world datasets. Utilizing doubly stochastic matrices to adapt kernel weights for convolutions, our method offers a flexible means of learning representation stacks, accommodating both known and unknown structures within the data. This adaptability makes it possible to detect useful patterns, although these may not always be interpretable in traditional group-theoretic terms due to the absence of predefined structures in the representation stack. ", "page_idx": 8}, {"type": "text", "text": "Limitations include computational requirements, which scale quadratically with the size of the group and the kernel size, posing challenges in scenarios with large groups or high-resolution data. As such, in this work we have designed the representation stack to be uniform across the channel dimension. However, this prevents learning of other commonly used image transformations such as color jitter. Furthermore, the need for task-specific regularization to manage entropy scaling during the learning of representations introduces complexity in hyperparameter tuning, which can be a barrier in some applications. Additionally, we observed that representations in later layers may show minimal diversity, suggesting that further innovation in regularization strategies might be necessary to enhance the distinctiveness of learned features across different layers. ", "page_idx": 8}, {"type": "text", "text": "For future work, we aim to enhance our method by implementing hierarchical weight-sharing across layers and promoting group equivariance more systematically. One promising direction is to leverage the concept of a Cayley tensor, akin to [19], to identify and reuse learned group structures across different layers of the network. This approach would not only impose a more unified and coherent group structure within the model but also potentially reduce the computational overhead associated with learning separate representations for each layer. By encouraging a shared group structure throughout the network, we anticipate improvements in both performance and interpretability, paving the way for more robust and efficient symmetry-aware learning systems. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "SV and AGC are supported by the Hybrid Intelligence Center, a 10-year program funded by the Dutch Ministry of Education, Culture and Science through the Netherlands Organisation for Scientific Research. The computational results presented have been achieved in part using the Snellius cluster at SURFsara. Additionally, we thank Alexander Timans for helpful discussions. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Ryan Prescott Adams and Richard S. Zemel. Ranking via sinkhorn propagation, 2011.   \n[2] James Urquhart Allingham, Bruno Kacper Mlodozeniec, Shreyas Padhy, Javier Antor\u00e1n, David Krueger, Richard E. Turner, Eric Nalisnick, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. A generative model of symmetry transformations, 2024.   \n[3] Daiki Asakura. An infinite dimensional birkhoff\u2019s theorem and locc-convertibility. IEICE Technical Report; IEICE Tech. Rep., 2016.   \n[4] Erik J. Bekkers, Maxime W. Lafarge, Mitko Veta, Koen A. J. Eppenhof, Josien P. W. Pluim, and Remco Duits. Roto-translation covariant convolutional networks for medical image analysis. CoRR, abs/1804.03393, 2018. URL http://arxiv.org/abs/1804.03393.   \n[5] Erik J Bekkers, Sharvaree Vadgama, Rob Hesselink, Putri A Van der Linden, and David W. Romero. Fast, expressive se(n) equivariant networks through weight-sharing in positionorientation space. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $=$ dPHLbUqGbr.   \n[6] Gregory W. Benton, Marc Finzi, Pavel Izmailov, and Andrew Gordon Wilson. Learning invariances in neural networks. CoRR, abs/2010.11882, 2020. URL https://arxiv.org/ abs/2010.11882.   \n[7] Garrett Birkhoff. Three observations on linear algebra. Univ. Nac. Tacuman, Rev. Ser. A, 5: 147\u2013151, 1946. URL https://cir.nii.ac.jp/crid/1573387450959988992.   \n[8] Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Velickovic. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. CoRR, abs/2104.13478, 2021. URL https: //arxiv.org/abs/2104.13478.   \n[9] Taco S. Cohen and Max Welling. Group equivariant convolutional networks. CoRR, abs/1602.07576, 2016. URL http://arxiv.org/abs/1602.07576.   \n[10] Marc Finzi, Gregory W. Benton, and Andrew Gordon Wilson. Residual pathway priors for soft equivariance constraints. CoRR, abs/2112.01388, 2021. URL https://arxiv.org/abs/ 2112.01388.   \n[11] Emiel Hoogeboom, Victor Garcia Satorras, Cl\u00e9ment Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3d, 2022.   \n[12] Alexander Immer, Tycho F. A. van der Ouderaa, Gunnar R\u00e4tsch, Vincent Fortuin, and Mark van der Wilk. Invariance learning in deep neural networks with differentiable laplace approximations, 2022. URL https://arxiv.org/abs/2202.10638.   \n[13] J.R. Isbell. Infinite Doubly Stochastic Matrices. Canadian Mathematical Bulletin, 5(1):1\u20134, January 1962. ISSN 0008-4395, 1496-4287. doi: 10.4153/ CMB-1962-001-4. URL https://www.cambridge.org/core/product/identifier/ S0008439500050992/type/journal_article.   \n[14] David G. Kendall. On Infinite Doubly-Stochastic Matrices and Birkhoffs Problem 111. Journal of the London Mathematical Society, s1-35(1):81\u201384, 1960. ISSN 1469-7750. doi: 10.1112/ jlms/s1-35.1.81. URL https://onlinelibrary.wiley.com/doi/abs/10.1112/jlms/ s1-35.1.81. _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1112/jlms/s1-35.1.81.   \n[15] David M. Knigge, David W Romero, and Erik J Bekkers. Exploiting redundancy: Separable group convolutional networks on lie groups. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 11359\u201311386. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/ v162/knigge22a.html.   \n[16] Maxime W Lafarge, Erik J Bekkers, Josien PW Pluim, Remco Duits, and Mitko Veta. Rototranslation equivariant convolutional networks: Application to histopathology image analysis. Medical Image Analysis, 68:101849, 2021.   \n[17] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998. doi: 10.1109/5.726791.   \n[18] Yue Liu. Homomorphism of independent random variable convolution and matrix multiplication, 2023.   \n[19] Giovanni Luca Marchetti, Christopher Hillar, Danica Kragic, and Sophia Sanborn. Harmonics of learning: Universal fourier features emerge in invariant networks. arXiv preprint arXiv:2312.08550, 2023.   \n[20] Eric Nalisnick and Padhraic Smyth. Learning priors for invariance. In Amos Storkey and Fernando Perez-Cruz, editors, Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning Research, pages 366\u2013375. PMLR, 09\u201311 Apr 2018. URL https://proceedings.mlr.press/v84/ nalisnick18a.html.   \n[21] Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos. Equivariance through parametersharing, 2017.   \n[22] David W. Romero and Suhas Lohit. Learning equivariances and partial equivariances from data. CoRR, abs/2110.10211, 2021. URL https://arxiv.org/abs/2110.10211.   \n[23] P. R\u00e9v\u00e9sz. A probabilistic solution of problem 111. of G. Birkhoff. Acta Mathematica Academiae Scientiarum Hungarica, 13(1):187\u2013198, March 1962. ISSN 1588-2632. doi: 10.1007/BF02033637. URL https://doi.org/10.1007/BF02033637.   \n[24] Sophia Sanborn, Christian Shewmake, Bruno Olshausen, and Christopher Hillar. Bispectral neural networks. arXiv preprint arXiv:2209.03416, 2022.   \n[25] Jean-Pierre Serre. Linear representations of finite groups. Springer-Verlag, New York, 1977. ISBN 0-387-90190-6. Translated from the second French edition by Leonard L. Scott, Graduate Texts in Mathematics, Vol. 42.   \n[26] Richard Sinkhorn. A relationship between arbitrary positive matrices and doubly stochastic matrices. Annals of Mathematical Statistics, 35:876\u2013879, 1964. URL https://api. semanticscholar.org/CorpusID:120846714.   \n[27] Ivan Sosnovik, Artem Moskalev, and Arnold W. M. Smeulders. DISCO: accurate discrete scale convolutions. CoRR, abs/2106.02733, 2021. URL https://arxiv.org/abs/2106.02733.   \n[28] Emmanouil Theodosis, Karim Helwani, and Demba Ba. Learning linear groups in neural networks, 2023.   \n[29] Tycho F. A. van der Ouderaa, Alexander Immer, and Mark van der Wilk. Learning layer-wise equivariances automatically using gradients, 2023.   \n[30] Rui Wang, Robin Walters, and Rose Yu. Approximately equivariant networks for imperfectly symmetric dynamics. CoRR, abs/2201.11969, 2022. URL https://arxiv.org/abs/2201. 11969.   \n[31] Maurice Weiler and Gabriele Cesa. General e (2)-equivariant steerable cnns. Advances in neural information processing systems, 32, 2019.   \n[32] Daniel E. Worrall and Max Welling. Deep scale-spaces: Equivariance over scale. CoRR, abs/1905.11697, 2019. URL http://arxiv.org/abs/1905.11697.   \n[33] Raymond A. Yeh, Yuan-Ting Hu, Mark Hasegawa-Johnson, and Alexander Schwing. Equivariance discovery by learned parameter-sharing. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors, Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages 1527\u2013 1545. PMLR, 28\u201330 Mar 2022. URL https://proceedings.mlr.press/v151/yeh22b. html.   \n[34] Allan Zhou, Tom Knowles, and Chelsea Finn. Meta-learning symmetries by reparameterization. CoRR, abs/2007.02933, 2020. URL https://arxiv.org/abs/2007.02933. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "A Preliminaries ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "A.1 Irreducible representations ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "In this section, we closely follow the mathematical preliminaries outlined in [31]. For a comprehensive reference on Representation Theory, see [25]. ", "page_idx": 11}, {"type": "text", "text": "Equivalent representations Two representations $\\rho$ and $\\rho^{\\prime}$ of a group $G$ are considered equivalent if there exists a similarity transform such that: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall g\\in G\\quad\\rho(g)=Q\\rho^{\\prime}(g)Q^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where $Q$ represents a change of basis matrix. ", "page_idx": 11}, {"type": "text", "text": "Irreps A matrix representation is called reducible if it can be decomposed as: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\rho(g)=Q^{-1}(\\rho_{1}(g)\\oplus\\rho_{2}(g))Q^{-1}=Q\\left(\\!\\!\\begin{array}{c c}{{\\rho_{1}(g)}}&{{0}}\\\\ {{0}}&{{\\rho_{2}(g)}}\\end{array}\\!\\!\\right)Q^{-1}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where $Q$ is a change of basis matrix. If the sub-representations $\\rho_{1}$ and $\\rho_{2}$ cannot be further decomposed, they are termed irreducible representations (irreps). The set of all irreducible representations of a group $G$ is denoted as $\\hat{G}$ . ", "page_idx": 11}, {"type": "text", "text": "Additionally, any representation $\\rho:G\\to G L(V)$ of a compact group $G$ can be expressed as: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\rho(G)=Q\\left[\\bigoplus_{j\\in\\mathbb{Z}}\\rho_{j}\\right]Q^{-1}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where $\\mathcal{T}$ is an index set (possibly with repetitions) over $\\hat{G}$ . ", "page_idx": 11}, {"type": "text", "text": "Similarly to what is showed in Section 4, a representation $\\rho\\,:\\,G\\,\\rightarrow\\,\\mathbb{R}^{d\\times d}$ can be viewed as a collection of $d^{2}$ functions over $G$ . The Peter-Weyl theorem asserts that the collection of functions formed by the matrix entries of all irreps in $\\hat{G}$ spans the space of all square-integrable functions over $G$ . For most groups, these entries form an orthogonal basis, allowing any function $f:G\\to\\mathbb{R}$ to be written as: ", "page_idx": 11}, {"type": "equation", "text": "$$\nf(g)=\\sum_{\\rho_{j}\\in\\hat{G}}\\sum_{m,n<d_{j}}w_{j,m,n}\\cdot\\sqrt{d_{j}}[\\rho_{j}(g)]_{m n}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where $d_{j}$ is the dimension of the irrep $\\rho_{j}$ , while $m,n$ index the entries of $\\rho_{j}$ . Note that this expression corresponds to the inverse Fourier transform and that the coefficients $w_{j,m,n}$ can be obtained by the Fourier transform of $f$ with respect to the basis functions $\\{[\\rho_{j}(g)]_{m n}\\}_{j\\in\\mathbb{Z}}$ . ", "page_idx": 11}, {"type": "text", "text": "Connection with regular representations It can be shown that the regular representations can be decomposed using the corresponding irreps as follows: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\rho_{\\mathrm{reg}}(g)=Q^{-1}\\left[\\bigoplus_{p_{j}}^{d_{j}}\\bigoplus\\rho_{j}\\right]Q\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where $Q$ performs the Fourier transform, while $Q^{-1}$ performs the inverse Fourier transform. This implies that when functions $f:G\\to\\mathbb{R}$ are considered as vectors in $\\mathbb{R}^{|G|}$ , with a basis where each axis corresponds to a group element, then, as we have seen in Section 4, the group action results in a permutation of these axes. However, applying the Fourier transform changes the basis so that $G$ acts independently on different subsets of the axes, resulting in the action being represented by a block-diagonal matrix, which is the direct sum of irreps. ", "page_idx": 11}, {"type": "text", "text": "B Toy problems ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "B.1 Data generation processes ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "For the construction of the toy problems, we look at two types of data-generating processes: ", "page_idx": 11}, {"type": "text", "text": "Equivariant data. Assume a canonical vector $\\hat{\\bf x}$ and corresponding label $\\hat{\\mathbf{y}}$ . We assume these vectors transform under a known group $G$ , such that the data-generating process is as follows: ", "page_idx": 12}, {"type": "text", "text": "Sample group action: $\\begin{array}{r l}&{g\\sim\\mu(G),}\\\\ &{\\mathbf{x}=\\rho^{x}(g)\\hat{\\mathbf{x}}+\\epsilon}\\\\ &{\\mathbf{y}=\\rho^{y}(g)\\hat{\\mathbf{y}}}\\end{array}$ Apply group action to feature with noise: Apply group action to label: ", "page_idx": 12}, {"type": "text", "text": "with $\\rho^{x},\\rho^{y}$ the representations of $G$ acting on the feature and label space, respectively. ", "page_idx": 12}, {"type": "image", "img_path": "44WWOW4GPF/tmp/59e63365ff292139d135f53a09f6b1c6bae253223fe0f60a89fc24508aa3ef96.jpg", "img_caption": ["Figure 4: Samples of the two tasks. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "Evaluation metrics To assess whether our method effectively captures the underlying data symmetries, we analyze the learned weight-sharing structure by comparing it to known ground truth patterns. Since our model does not impose associativity or any strict group constraints on the representation stack, it may learn to represent mixtures or interpolations of group elements. Hence, in general, we will not observe a relevant algebraic structure if we produce a Cayley table based on the learned representations as done in [24, 19]. ", "page_idx": 12}, {"type": "text", "text": "Therefore, we will use an approach that allows us to capture the flexibility of our representations. To this end, we will examine each representation matrix to determine how closely it resembles a convolution matrix [18] associated with a random variable defined over some specific group $G$ . We employ the set of group actions from G, represented as doubly stochastic tensors {Pgkt }|kG=|1, as a reference framework to quantify the fit and alignment of our model\u2019s representations with these predefined group actions. As such, for each learned weight sharing tensor $\\mathbf{R}_{i}^{l}$ , we calculate the fit $\\begin{array}{r}{\\hat{\\mathbf{P}}_{i}=\\sum_{k}^{|G|}c_{k}\\mathbf{P}_{k}^{g t}}\\end{array}$ and acquire coefficients $c_{k}>0$ , such that $\\sum_{k}^{\\vert G\\vert}c_{k}=1$ in a constrained linear regression setup by minimizing $||\\hat{\\mathbf{P}}_{i}-\\mathbf{R}_{i}^{l}||_{2}$ . ", "page_idx": 12}, {"type": "text", "text": "B.2 Additional results: toy problems ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We conducted experiments on various signals subjected to different transformations, including: a 1D signal with cyclic shifts (exemplary samples shown in Fig. 4a), a 2D signal with $C_{8}$ rotations (illustrated in Fig. 7), and a 3D voxel grid enhanced by 24 cube symmetries. In each scenario, the learned kernel stack accurately matched the data samples, achieving perfect accuracy. Figure 6 displays the learned representations for localized shifts in the 1D signal, while Figure 8 presents the learned kernel stack for the 2D signal dataset. ", "page_idx": 12}, {"type": "image", "img_path": "44WWOW4GPF/tmp/34bafbda6eaba7c91054bed87ad3443ef07193d289c024060ba90322e0aa08fb.jpg", "img_caption": ["(a) Shift-dataset with uniformly sampled (b) Shift-dataset with partially sampled group elements. group elements. ", "Figure 5: Coefficient responses of learned representations and their base transformations. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Furthermore, to assess whether our model can identify partial group structures, we evaluate it using two distinct datasets: a 1D signal enhanced with cyclical shifts, and a $3\\times3\\times3$ voxel grid subjected to rotations from $C4\\times C4$ . For the shift dataset, we utilize the complete set of cyclical shifts as the ground truth representations. Figure $5(\\mathrm{a},\\mathrm{b})$ displays the coefficients for the shift dataset. Specifically, part (a) shows coefficients when trained uniformly across group elements, while part (b) illustrates coefficients using only the first half of the group elements, where the group no longer retains cyclic properties or satisfies closure. Given that our method does not assume cyclic groups or group closure, it effectively captures the relevant group transformations even for partial transformations. App. B.4 shows the coefficients for the base representations of $C_{4}\\times C_{4}\\times C_{4}$ cube symmetries. Since the data augmentation only applied $C_{4}\\times C_{4}$ transformations, the method predominantly identifies elements corresponding to these transformations, as highlighted by the red line. ", "page_idx": 13}, {"type": "image", "img_path": "44WWOW4GPF/tmp/df6f4b7346724edf0a327552a6defabc66ac970b9a2d93238f659e4d4a5dfbef.jpg", "img_caption": ["Figure 6: Representations learned for 1D equivariant shift task "], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "44WWOW4GPF/tmp/b0aa89c8c1a875f466897acfd2b8419da3fe8d61985c12fa542fb13d4f758e16.jpg", "img_caption": ["Figure 7: Samples of the 2D-signal dataset. "], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "44WWOW4GPF/tmp/8c82a5a106364e04f4090f7f9dafff7a6853b93d067d500ecf55fae28821e860.jpg", "img_caption": ["Figure 9: Ground-truth permutation matrices for $C_{4}$ rotations for $5\\times5$ spatial kernel, implementing a shift-twist operator. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "44WWOW4GPF/tmp/50279b201be07c7f71b6899bc065fe0e4627f9705783a66ca95f938c01ca2eb6.jpg", "img_caption": ["Figure 10: Learned representations for the weight-sharing G-conv layers. Top to bottom: first to last layer learned representation stacks. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.4 Additional results: learning partial equivariance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Fig. 11 shows the coefficients for the base representations of $C_{4}\\times C_{4}\\times C_{4}$ cube symmetries. The $\\mathbf{X}$ -axis quantifies the permutation representation for each element consisting of the number of 90-degree flips around each x, y or z axis. ", "page_idx": 15}, {"type": "image", "img_path": "44WWOW4GPF/tmp/5c3ea3977f3a9557815446b54268b890c3f790a217daa7533e878c653f0d475e.jpg", "img_caption": ["Figure 11: Cube dataset with $C_{4}\\times C_{4}$ rotations with $C_{4}\\times C_{4}\\times C_{4}$ base elements. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.5 Convolution matrix decomposition for Fig. 3. ", "page_idx": 16}, {"type": "image", "img_path": "44WWOW4GPF/tmp/5f3ac217b1f2c0355ea8a5b024706589e80e67cc375d8c01c2b4fac74b9ac07d.jpg", "img_caption": ["Figure 12: Coefficients for $C_{4}$ representations for $d=25$ and learned representations of the lifting layer of the rotatedMNIST experiment. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Architectural details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In the current section, we outline some design choices used across all experiments. Code is available at https://github.com/computri/learnable-weight-sharing. Firstly, when defining the learnable representation stack $\\mathbf{R}$ , we have found that anchoring an identity element aids in distinguishing the base kernel from its potential transformations. This approach is based on the intuition of starting with a learnable base kernel and subsequently learning its transformations and we have found that it aids optimization. ", "page_idx": 16}, {"type": "text", "text": "C.1 Regularizers ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We test two regularizers on the representations $\\mathbf{R}$ , which are similar to those used by [33]: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Entropy Regularizer: The primary motivation for using the entropy regularizer is to encourage sparsity in our weight-sharing schemes, which helps the matrices approximate actual permutation matrices rather than simply being doubly stochastic. This approach stems from the intuition for some transformations, the weight-sharing schemes should mimic soft-permutation matrices. The effectiveness of this sparsity depends on the specific group transformations relevant to the task\u2014for example, C4 rotations are typically represented by exact permutation matrices. In contrast, $C_{N}$ rotations or scale transformations might require interpolation, thus aligning more closely with soft permutation matrices. Our experimental results indicate that the utility of this regularizer varies with the underlying transformations in the data; for instance, it is not beneficial for scale transformations in the MNIST dataset, as anticipated. The entropy regularizer is of the following form: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{ent}(\\mathbf{R})=-\\sum_{i j k}^{N,D,D}\\mathbf{R}_{i j k}\\cdot\\log(\\mathbf{R}_{i j k})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "\u2022 Normalization Regularizer: Empirically, we have found the normalization regularizer essential for reducing the number of iterations needed by the Sinkhorn operator to ensure the matrices are row and column-normalized. Without this regularizer, the tensors either fail to achieve double stochasticity or require an excessively high number of Sinkhorn iterations to do so. The normalization regularizer is of the following form: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{norm}({\\bf R})=\\frac{1}{D}\\sum_{i}^{D}\\mathrm{sum}_{r}({\\bf R})_{i}^{2}+\\mathrm{sum}_{c}({\\bf R})_{i}^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Where $\\mathrm{sum}_{r}$ , $\\mathrm{sum}_{c}$ are defined as in 4.2. ", "page_idx": 17}, {"type": "text", "text": "C.2 MNIST ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Model architecture For all MNIST experiments, a simple 5-block CNN was used. Each block uses a kernel size of 5 and is succeeded by instance norm and ReLU activation, respectively. After the final convolution block, any spatial and group dimensions are reduced through a global average pooling operation, and a single linear layer is used as a classification head. For the group convolutional model and our weight sharing model, the default hidden channel dimension in the blocks was set to 32 unless otherwise stated, and 64 in the regular CNN models. ", "page_idx": 17}, {"type": "text", "text": "Training details The models used a learning rate of 1e-2 and were trained for 100 epochs. All the experiments were done on a single GPU with 24GB memory under six hours. ", "page_idx": 17}, {"type": "text", "text": "C.3 CIFAR10 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Model architecture We used the ResNet architecture as in [15] Appendix B.1, except that we swapped the final global max pooling operator with a global mean pooling. However, in contrast to [15], we use regular discrete kernels instead of continuous kernel parameterizations. ", "page_idx": 17}, {"type": "text", "text": "Training details Following [15], we trained the models for 200 epochs using a learning rate of 1e-4. All the experiments were done on a single GPU with 24GB memory under six hours. ", "page_idx": 17}, {"type": "text", "text": "C.4 Computational Demands ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Fig. 13 14 show the computational scaling analysis of our weight sharing layer, comparing it to a group convolutional model of the same dimensions. We highlight that regular group convolutions can be implemented via weight-sharing schemes, resulting in equal computational demands for both approaches. Since the weight-sharing approach applies the group transformation in parallel across all elements (as a result of matrix multiplication and reshape operations), our method can prove quite efficient. Regarding memory allocation, group convolutions are often implemented using for-loops over the group actions, and this sequential method imposes a less heavy memory burden since the operation is applied in series per group element. However, although scaling is quadratic w.r.t. the number of group elements and the domain size for weight-sharing, we mitigate this issue by use of the typically lower-dimensional support of convolutional filters (i.e., and ), rendering our approach practical for a wider range of settings. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "image", "img_path": "44WWOW4GPF/tmp/82ef36fa0ca05f75b94c2330ae733c76b2ddeec04aeda88ffe532cd20929e9c5.jpg", "img_caption": ["Figure 13: Memory allocation at inference time for a C4-GCNN layer and a weight sharing layer, for different number of group elements $|G|$ and different kernel sizes. The input is $32\\times3\\times100\\times100$ (batch $\\times$ channels $\\times$ height $\\times$ width) ", "", ""], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "44WWOW4GPF/tmp/c459f46698cdbee60bd4802b996cb39088c2382754db03107d8885b89b01371c.jpg", "img_caption": ["Figure 14: Execution time at inference time for a C4-GCNN layer and a weight sharing layer, for different number of group elements $|G|$ and different kernel sizes. The input is $32\\times3\\times100\\times100$ (batch $\\times$ channels $\\times$ height $\\times$ width) "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: We do not give new theoretical results per see, the proposed method has a theoretical formulation, that is give in detail in the paper and appendix. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 22}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We do not use proprietary datasets or code ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]