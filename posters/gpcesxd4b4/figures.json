[{"figure_path": "gPCesxD4B4/figures/figures_2_1.jpg", "caption": "Figure 1: Active geo-localization across different goal modalities. The agent must navigate to the goal (yellow dot) based on partial aerial glimpses, i.e. the full area is never observed in its entirety.", "description": "This figure illustrates the active geo-localization task across different goal modalities.  The agent (e.g., a UAV) uses a sequence of aerial images to locate a target. The target's location is specified in one of three modalities: aerial image, ground-level image, or natural language text.  The agent's field of view is limited, so it must navigate efficiently to find the target.", "section": "3 Active Geo-localization Setup"}, {"figure_path": "gPCesxD4B4/figures/figures_3_1.jpg", "caption": "Figure 2: GASP strategy for pretraining LLMs for AGL.", "description": "This figure illustrates the Goal-Aware Supervised Pretraining (GASP) strategy used to pretrain Large Language Models (LLMs) for the Goal-Conditioned Partially Observable Markov Decision Process (GC-POMDP) problem of active geo-localization.  The GASP strategy involves two steps: 1) generating a random trajectory using a random policy, and 2) training the LLM to predict the next best action at each step based on the entire history. The LLM uses a CLIP-based Multi-Modal Feature Extractor (CLIP-MMFE) to process aerial image observations.  The training objective is to minimize the cross-entropy loss between the LLM's predicted actions and a set of optimal actions. The figure shows the input embeddings to the LLM, the LLM architecture, and the cross-entropy loss function.", "section": "4 Proposed Framework for Goal Modality Agnostic Active Geo-localization"}, {"figure_path": "gPCesxD4B4/figures/figures_5_1.jpg", "caption": "Figure 3: Our proposed GOMAA-Geo framework for goal modality agnostic active geo-localization.", "description": "This figure shows the GOMAA-Geo framework's architecture, illustrating how it handles different goal modalities (aerial, ground-level images, and text) to achieve active geo-localization. The process begins with the CLIP-MMFE which handles the different modalities by aligning their representations. The aligned representations are then fed into an LLM, which predicts an action based on the history of observed states and actions. Finally, an actor-critic network refines the action selection, considering the current state and the goal.", "section": "4 Proposed Framework for Goal Modality Agnostic Active Geo-localization"}, {"figure_path": "gPCesxD4B4/figures/figures_7_1.jpg", "caption": "Figure 4: Active geo-localization across different goal modalities. The agent must navigate to the goal (yellow dot) based on partial aerial glimpses, i.e. the full area is never observed in its entirety.", "description": "This figure illustrates the active geo-localization task across different modalities. The agent, using only sequential observations of aerial images, needs to find the goal which can be specified using various modalities such as aerial images, ground level images, or natural language descriptions. The agent can only observe a small part of the search area at each time step.", "section": "Active Geo-localization Setup"}, {"figure_path": "gPCesxD4B4/figures/figures_8_1.jpg", "caption": "Figure 4: Example exploration behavior of GOMAA-Geo across different goal modalities. The stochastic policy selects actions probabilistically, whereas the argmax policy selects the action with the highest probability.", "description": "The figure showcases four different exploration strategies used by GOMAA-Geo to reach a goal, categorized by different goal modalities. The first column demonstrates an argmax policy, which always selects the action with the highest probability of success, thus resulting in a deterministic path. The remaining columns illustrate stochastic policies, introducing randomness in action selection, leading to four distinct but successful trajectories.", "section": "Further Analyses and Ablation Studies"}, {"figure_path": "gPCesxD4B4/figures/figures_8_2.jpg", "caption": "Figure 1: Active geo-localization across different goal modalities. The agent must navigate to the goal (yellow dot) based on partial aerial glimpses, i.e., the full area is never observed in its entirety.", "description": "This figure illustrates the task of active geo-localization. An agent (like a UAV) needs to find a target within a search area using only a sequence of partial aerial views (limited field of view).  The goal can be described in multiple ways: as an aerial image, a ground-level image, or natural language. The agent must efficiently use the sequence of observations to find the goal as quickly as possible.", "section": "3 Active Geo-localization Setup"}, {"figure_path": "gPCesxD4B4/figures/figures_9_1.jpg", "caption": "Figure 4: Example exploration behavior of GOMAA-Geo across different goal modalities. The stochastic policy selects actions probabilistically, whereas the argmax policy selects the action with the highest probability.", "description": "This figure shows four example scenarios of GOMAA-Geo's exploration behavior using both stochastic and argmax policies across different goal modalities. In each scenario, the agent starts at the same location and attempts to reach the goal. The stochastic policy is shown in different colors for multiple trials (different colored lines) to show the variability introduced by its probabilistic action selection.  The argmax policy, in contrast, always chooses the most probable action at each step, resulting in a more direct path to the goal. The figure aims to illustrate how different policy choices lead to different exploration patterns, even when the underlying model and goal are the same.", "section": "Further Analyses and Ablation Studies"}, {"figure_path": "gPCesxD4B4/figures/figures_14_1.jpg", "caption": "Figure 1: Active geo-localization across different goal modalities. The agent must navigate to the goal (yellow dot) based on partial aerial glimpses, i.e. the full area is never observed in its entirety.", "description": "This figure illustrates the active geo-localization task across different modalities. An agent (e.g., a UAV) must locate a goal within a search area using only sequential observations of aerial sub-images.  The goal is specified in one of several modalities (natural language, ground-level image, or aerial image), but its exact location (pg) within the search area is unknown. The agent can move to different positions (pt) within the area.  The challenge is to efficiently find the goal (pt = pg) with limited localization time.  The figure visually represents the task with three examples: a ground-level image, an aerial image, and natural language text, all indicating a distinct goal.", "section": "3 Active Geo-localization Setup"}, {"figure_path": "gPCesxD4B4/figures/figures_14_2.jpg", "caption": "Figure 9: Example exploration behavior of GOMAA-Geo across different goal modalities. In this case, the agent successfully reaches the goal for all three goal modalities in the minimum number of steps (red line).", "description": "This figure shows four different trials of the GOMAA-Geo model exploring a search area to find a goal.  The goal is specified in three different modalities: an image, a textual description, and an aerial image.  The red line shows the optimal path taken by the argmax policy. The other lines depict alternative paths taken by the stochastic policy in four different runs.  The figure demonstrates the agent's ability to successfully locate the goal in all three modalities using the minimum number of steps.", "section": "B More Visualizations of Exploration Behavior of GOMAA-Geo across different Goal Modalities"}, {"figure_path": "gPCesxD4B4/figures/figures_15_1.jpg", "caption": "Figure 4: Example exploration behavior of GOMAA-Geo across different goal modalities. The stochastic policy selects actions probabilistically, whereas the argmax policy selects the action with the highest probability.", "description": "This figure visualizes how the GOMAA-Geo agent explores the environment to reach a goal specified in different modalities (text, ground-level image, aerial image). The figure shows four different stochastic policy trials and one argmax policy trial.  The stochastic policy demonstrates the agent's ability to explore probabilistically, while the argmax policy represents the most likely path based on the learned model's predictions. The visualization helps understand the agent's decision-making process across various goal modalities.", "section": "Further Analyses and Ablation Studies"}, {"figure_path": "gPCesxD4B4/figures/figures_15_2.jpg", "caption": "Figure 10: Example exploration behaviors of GOMAA-Geo. The agent is successful in all cases.", "description": "This figure showcases four different exploration trajectories generated by the GOMAA-Geo agent using a stochastic policy (trials 1-4) and a deterministic argmax policy. Each trajectory shows the path taken by the agent (indicated by different colored lines) to reach the goal, starting from the same initial point.  All four stochastic policy runs and the argmax policy run successfully reach the goal.  The figure demonstrates the agent's ability to find a successful route to the goal, even when using a stochastic policy that introduces randomness into the action selection process. This highlights the robustness of the GOMAA-Geo method.", "section": "D More Visualizations of Exploration Behavior of GOMAA-Geo"}, {"figure_path": "gPCesxD4B4/figures/figures_16_1.jpg", "caption": "Figure 5: Examples of successful exploration behaviors of GOMAA-Geo.", "description": "This figure visualizes the exploration trajectories generated by GOMAA-Geo's stochastic and argmax policies for a specific start and goal pair. The stochastic policy selects actions probabilistically, while the argmax policy chooses the action with the highest probability.  The figure demonstrates the different exploration paths taken by the agent in multiple trials using the stochastic policy and contrasts them with the more direct, deterministic path of the argmax policy. This illustrates the exploration-exploitation trade-off inherent in reinforcement learning agents and how a stochastic policy can lead to discovering alternative, potentially more efficient, paths to the goal.", "section": "Visualizing the exploration behavior of GOMAA-Geo"}, {"figure_path": "gPCesxD4B4/figures/figures_16_2.jpg", "caption": "Figure 1: Active geo-localization across different goal modalities. The agent must navigate to the goal (yellow dot) based on partial aerial glimpses, i.e., the full area is never observed in its entirety.", "description": "This figure illustrates the task of active geo-localization with goals specified in different modalities (aerial image, ground-level image, natural language description).  An agent (e.g., a UAV) uses a sequence of aerial images to locate a goal. The agent's field of view is limited (as shown by the grid overlay), and it must navigate efficiently to the goal. The goal is only partially observed through the agent's limited view.", "section": "Active Geo-localization Setup"}, {"figure_path": "gPCesxD4B4/figures/figures_17_1.jpg", "caption": "Figure 14: Performance of GOMAA-Geo across different values of search budget B.", "description": "This figure shows the performance of the GOMAA-Geo model across different search budget sizes (B). The x-axis represents different values of C (distance from start to goal) and B (search budget), and the y-axis represents the success rate. The box plot shows the distribution of success rates across multiple independent trials for each configuration. For both C=5 and C=6, the success rate generally increases as the search budget B increases.", "section": "G Performance Comparison of GOMAA-Geo with Varying Search Budget B"}, {"figure_path": "gPCesxD4B4/figures/figures_19_1.jpg", "caption": "Figure 3: Our proposed GOMAA-Geo framework for goal modality agnostic active geo-localization.", "description": "The figure shows the GOMAA-Geo framework, illustrating its components: CLIP-MMFE for multi-modal feature extraction, GASP (Goal-Aware Supervised Pretraining) for LLM pre-training, and the actor-critic network for planning.  The framework enables the agent to learn a goal-conditioned latent representation using historical data and goal specifications. The trained model can generalize to different goal modalities (text, ground-level, aerial images).", "section": "4 Proposed Framework for Goal Modality Agnostic Active Geo-localization"}, {"figure_path": "gPCesxD4B4/figures/figures_20_1.jpg", "caption": "Figure 16: Locations of the samples in our MM-GAG dataset.", "description": "This figure shows the geographical distribution of the 73 ground-level images collected for the MM-GAG dataset.  The images are sourced from various locations across the globe, indicating a diverse range of geographical contexts represented in the dataset.  This diversity is important for evaluating the generalizability of the GOMAA-Geo model to unseen environments and real-world scenarios.", "section": "LMM-GAG Dataset Details"}, {"figure_path": "gPCesxD4B4/figures/figures_21_1.jpg", "caption": "Figure 17: Boxplot of different methods for different choices of C using Masa Dataset.", "description": "This boxplot visualizes the performance of different active geo-localization methods (Random, DiT, AirLoc, PPO, and GOMAA-Geo) across various start-to-goal distances (C = 4, 5, 6, 7, 8) using the Masa dataset. Each box represents the distribution of success ratios obtained across multiple trials for each method and distance. The boxplot clearly shows that GOMAA-Geo consistently outperforms all other baselines in terms of success rate.", "section": "Experiments and Results"}, {"figure_path": "gPCesxD4B4/figures/figures_22_1.jpg", "caption": "Figure 17: Boxplot of different methods for different choices of C using Masa Dataset.", "description": "The figure shows the performance comparison of GOMAA-Geo with different baseline methods across different evaluation settings using the Masa dataset. The boxplots represent the distribution of the success ratios obtained from five independent experimental trials for each method across various start-to-goal distances (C). The results show that GOMAA-Geo significantly outperforms all the baseline methods across all evaluation settings.", "section": "Experiments and Results"}, {"figure_path": "gPCesxD4B4/figures/figures_23_1.jpg", "caption": "Figure 4: Example exploration behavior of GOMAA-Geo across different goal modalities. The stochastic policy selects actions probabilistically, whereas the argmax policy selects the action with the highest probability.", "description": "This figure visualizes the exploration behavior of the GOMAA-Geo model across four different goal modalities: aerial image, ground-level image, and natural language text. Each column represents a different goal modality, and each row illustrates the agent's path (trajectory) during a single trial.  The stochastic policy (represented by different colored lines) allows the agent to explore multiple paths probabilistically by selecting actions based on their probabilities.  In contrast, the argmax policy uses a deterministic approach, selecting the action with the highest probability at each step. The figure aims to showcase how GOMAA-Geo can handle different goal specifications while learning effective exploration strategies.", "section": "Further Analyses and Ablation Studies"}, {"figure_path": "gPCesxD4B4/figures/figures_23_2.jpg", "caption": "Figure 4: Example exploration behavior of GOMAA-Geo across different goal modalities. The stochastic policy selects actions probabilistically, whereas the argmax policy selects the action with the highest probability.", "description": "The figure shows four examples of how the GOMAA-Geo model explores the environment in order to reach the goal. The goal is represented as a yellow dot in each case. The top row shows the argmax policy, where the agent deterministically chooses the action with the highest predicted probability. The bottom row shows four trials of the stochastic policy, where the agent randomly selects an action, with the probability of each action determined by the policy. The different colors of the trajectories represent the different trials of the stochastic policy. This figure illustrates the stochastic nature of the exploration process and how the agent learns to find the goal effectively.", "section": "Further Analyses and Ablation Studies"}]