[{"figure_path": "RJEC9fZ9Ma/tables/tables_8_1.jpg", "caption": "Table 1: An ablation study of Mixup method for training ResNet50 on CIFAR-100 using different classifiers and loss functions. The numbers in the second row are imbalance ratios. The parameters are fixed with f = 20, and \u03b8 = 0.2", "description": "This table presents the results of an ablation study conducted to evaluate the impact of using the Mixup training method in conjunction with different classifiers and loss functions on the ResNet50 model trained on the CIFAR-100 dataset.  The study examines various imbalance ratios (0.005, 0.01, 0.02) and compares the performance of different methods (CE, SETF, CAL) both with and without Mixup. The parameters f and \u03b8 for the CAL method are fixed at 20 and 0.2 respectively.", "section": "4.4 Long-Tailed Classification"}, {"figure_path": "RJEC9fZ9Ma/tables/tables_9_1.jpg", "caption": "Table 2: Long-tailed classification accuracy (%) with ResNet and DenseNet on CIFAR-10 and CIFAR-100.", "description": "This table shows the results of long-tailed classification experiments using ResNet and DenseNet architectures on CIFAR-10 and CIFAR-100 datasets.  The results are presented for different imbalance ratios (0.005, 0.01, and 0.02) and compare the performance of three methods: CE (standard cross-entropy loss), SETF (Simplex-Encoded-Labels Interpolation), and CAL (the proposed Cosine-Aware Loss).  The table highlights the improved performance of CAL, especially in the more challenging imbalanced scenarios.", "section": "4.4 Long-Tailed Classification"}, {"figure_path": "RJEC9fZ9Ma/tables/tables_9_2.jpg", "caption": "Table 3: A comparison of several recent methods of long-tail classification trained on ResNet50. f = 20 and \u03b8 = 0.2 are fixed. The values without \u00b1 are that we did not reproduce.", "description": "This table compares the performance of different long-tail classification methods on three datasets (CIFAR-10, CIFAR-100, and ImageNet) using ResNet50 as the backbone network.  The methods compared include several recent approaches (LDAM-DRW, KCL, SETF, ARBLoss) and the proposed method (CAL).  The imbalance ratio (\u03c4) varies across different columns.  Note that some values are missing because the authors did not reproduce those results.  The hyperparameters f and \u03b8 are fixed at 20 and 0.2, respectively.", "section": "4.4 Long-Tailed Classification"}, {"figure_path": "RJEC9fZ9Ma/tables/tables_24_1.jpg", "caption": "Table 4: Loss P vs Cosine Regression Loss on CIFAR-100", "description": "This table presents the long-tail classification results obtained using three different loss functions (Loss P without Mixup, Loss P with Mixup, and CAL) on the CIFAR-100 dataset at different imbalance ratios (\u03c4 = 0.005, 0.01, 0.02). The results are presented as the accuracy with standard deviations, showcasing the performance of each loss function under various data imbalance levels.", "section": "4.4 Long-Tailed Classification"}, {"figure_path": "RJEC9fZ9Ma/tables/tables_26_1.jpg", "caption": "Table 5: Long-tailed classification accuracy (%) with ResNet and DenseNet on SVHN and STL-10.", "description": "This table presents the results of long-tailed image classification experiments using two different network architectures (ResNet and DenseNet) on two datasets (SVHN and STL-10).  The results show the accuracy achieved at different levels of class imbalance (0.005, 0.01, 0.02), comparing the performance of a standard cross-entropy loss function (CE), a fixed-classifier SETF method, and the proposed CAL method.", "section": "4.4 Long-Tailed Classification"}, {"figure_path": "RJEC9fZ9Ma/tables/tables_26_2.jpg", "caption": "Table 6: ResNet50's accuracy changes with the parameter tuple (f, \u03b8) on CIFAR100, \u03c4 = 0.005", "description": "This table presents the ResNet50 accuracy on CIFAR-100 with an imbalance ratio of 0.005, varying hyperparameters f and \u03b8 in the proposed Cosine Regression Loss.  It demonstrates how the choice of these hyperparameters impacts the model's performance on the long-tailed classification task.", "section": "4.4 Long-Tailed Classification"}, {"figure_path": "RJEC9fZ9Ma/tables/tables_28_1.jpg", "caption": "Table 7: CAL vs RBL in long-tail classification trained on ResNet50. f = 20 and \u03b8 = 0.2 are fixed. The values without \u00b1 are that we did not reproduce. '-' represents a missing value.", "description": "This table compares the performance of CAL and RBL methods on CIFAR-10 and CIFAR-100 datasets with different imbalance ratios.  It highlights the impact of the post-hoc logit adjustment in RBL and shows that CAL outperforms RBL in some settings, but RBL with post-hoc logit adjustment is better in others.", "section": "4.4 Long-Tailed Classification"}]