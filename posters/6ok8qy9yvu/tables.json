[{"figure_path": "6OK8Qy9yVu/tables/tables_6_1.jpg", "caption": "Table 1: Performance of FL algorithms with full network and partial network updates.", "description": "The table presents a comparison of the performance of three federated learning algorithms (FedAvg, FedProx, and FedMoon) using both full network updates (FNU) and partial network updates (PNU) proposed by the authors.  The comparison is done across four different training cycles (C) on three datasets (CIFAR-10, CIFAR-100, and TinyImageNet).  The metrics used for comparison include the best accuracy achieved (Best Acc.), communication cost (Comm.), and computation cost (Comp.).", "section": "4.1 Main Properties"}, {"figure_path": "6OK8Qy9yVu/tables/tables_7_1.jpg", "caption": "Table 1: Performance of FL algorithms with full network and partial network updates.", "description": "This table presents a comparison of the performance of three classic federated learning algorithms (FedAvg, FedProx, and FedMoon) using both full network updates (FNU) and partial network updates (PNU) introduced by the FedPart method.  The results are shown across three datasets (CIFAR-10, CIFAR-100, and TinyImageNet) and four training cycles.  For each algorithm and update method, the table reports best achieved accuracy, communication cost (in GB), and computational cost (in TFLOPs).  This allows for a comprehensive comparison of the effectiveness and efficiency of FedPart compared to traditional methods.", "section": "4.1 Main Properties"}, {"figure_path": "6OK8Qy9yVu/tables/tables_7_2.jpg", "caption": "Table 1: Performance of FL algorithms with full network and partial network updates.", "description": "This table compares the performance of three classic federated learning algorithms (FedAvg, FedProx, and FedMoon) using both full network updates (FNU) and partial network updates (PNU) with the proposed FedPart method.  The comparison is made across three datasets (CIFAR-10, CIFAR-100, and TinyImageNet) and considers several metrics: Best Accuracy, Communication Cost, and Computation Cost.  The results demonstrate the performance improvement and efficiency gains achieved with FedPart.", "section": "4.1 Main Properties"}, {"figure_path": "6OK8Qy9yVu/tables/tables_7_3.jpg", "caption": "Table 1: Performance of FL algorithms with full network and partial network updates.", "description": "The table compares the performance of three classic federated learning algorithms (FedAvg, FedProx, and FedMoon) using both full network updates (FNU) and partial network updates (PNU) with the proposed FedPart method.  The results show the best accuracy, communication cost, and computation cost for different datasets (CIFAR-10, CIFAR-100, TinyImageNet) and training cycles (C).  FedPart consistently outperforms FNU across all algorithms and datasets, demonstrating significant improvements in accuracy and reductions in communication and computation costs.", "section": "4.1 Main Properties"}, {"figure_path": "6OK8Qy9yVu/tables/tables_8_1.jpg", "caption": "Table 1: Performance of FL algorithms with full network and partial network updates.", "description": "This table compares the performance of three classic federated learning algorithms (FedAvg, FedProx, and FedMoon) using both full network updates (FNU) and partial network updates (PNU) with the FedPart method.  The results are shown for four different training cycles (C) on three datasets: CIFAR-10, CIFAR-100, and TinyImageNet.  Metrics include best accuracy, communication cost (Comm.), and computation cost (Comp.).  The table highlights the superior performance of FedPart, achieving higher accuracy while significantly reducing both communication and computation overhead.", "section": "4.1 Main Properties"}, {"figure_path": "6OK8Qy9yVu/tables/tables_8_2.jpg", "caption": "Table 1: Performance of FL algorithms with full network and partial network updates.", "description": "This table presents the performance comparison of three Federated Learning (FL) algorithms (FedAvg, FedProx, and FedMoon) using both full network updates (FNU) and the proposed partial network updates (PNU) method (FedPart).  The results are shown across different datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet), with each algorithm evaluated over five training cycles (C). For each algorithm and cycle, the table displays the best accuracy achieved (Best Acc.), communication cost (Comm.), and computational cost (Comp.). This table demonstrates the superior performance of FedPart compared to FNU, achieving higher accuracy with significantly reduced communication and computational overheads.", "section": "4.1 Main Properties"}, {"figure_path": "6OK8Qy9yVu/tables/tables_9_1.jpg", "caption": "Table 1: Performance of FL algorithms with full network and partial network updates.", "description": "This table compares the performance of three classic federated learning algorithms (FedAvg, FedProx, and FedMoon) using both full network updates (FNU) and partial network updates (PNU) with the FedPart method. The comparison is done across four different training cycles (C) and three benchmark datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet).  Metrics include the best accuracy achieved (Best Acc.), total upstream transmission volume required (Comm.), and total floating-point computation required (Comp.).", "section": "4.1 Main Properties"}, {"figure_path": "6OK8Qy9yVu/tables/tables_9_2.jpg", "caption": "Table 9: Average and Max PSNRs of reconstructed images for FedAvg and FedPart models.", "description": "This table presents the average and maximum Peak Signal-to-Noise Ratio (PSNR) values obtained from reconstructing images using the Deep Leakage from Gradients (DLG) attack. The results are compared between the FedAvg-100 model (full network updates) and the FedPart(5C) model (partial network updates).  The comparison is made using all parameters for FedAvg-100, and only a subset of parameters (#1 (conv) and #10 (fc)) for FedPart(5C). Lower PSNR values indicate better privacy protection.", "section": "4.4 Impact on Privacy Protection"}, {"figure_path": "6OK8Qy9yVu/tables/tables_19_1.jpg", "caption": "Table 1: Performance of FL algorithms with full network and partial network updates.", "description": "This table compares the performance of three classic federated learning algorithms (FedAvg, FedProx, and FedMoon) using both full network updates (FNU) and the proposed partial network updates (PNU) method (FedPart).  It shows the best accuracy achieved, communication cost (in GB), and computation cost (in TFLOPs) for each algorithm and update strategy across four training cycles on three different datasets (CIFAR-10, CIFAR-100, and TinyImageNet).  The results demonstrate the effectiveness of FedPart in improving convergence speed and accuracy while reducing communication and computational overhead.", "section": "4.1 Main Properties"}, {"figure_path": "6OK8Qy9yVu/tables/tables_19_2.jpg", "caption": "Table 1: Performance of FL algorithms with full network and partial network updates.", "description": "This table presents a comparison of the performance of three Federated Learning (FL) algorithms (FedAvg, FedProx, and FedMoon) using both full network updates (FNU) and partial network updates (PNU).  The table shows the best accuracy achieved, the communication cost (in GB), and the computation cost (in TFLOPs) for each algorithm and update method across four different training cycles (C) on three benchmark datasets (CIFAR-10, CIFAR-100, and TinyImageNet). It demonstrates the performance improvements and efficiency gains achieved by the proposed FedPart method with partial network updates.", "section": "4.1 Main Properties"}, {"figure_path": "6OK8Qy9yVu/tables/tables_19_3.jpg", "caption": "Table 12: Performance of FL algorithms with full network and partial network updates under extreme data heterogeneity (Dirichlet, a = 0.1)", "description": "This table presents the performance of federated learning algorithms (FedAvg and FedProx) with full network updates (FNU) and partial network updates (using the FedPart method) under extreme data heterogeneity (Dirichlet distribution with parameter \u03b1 = 0.1).  It shows the best accuracy achieved for each algorithm and update method across different numbers of training rounds (C). The results demonstrate the performance of FedPart, particularly considering its computational and communication efficiency improvements, even under the challenging non-IID data distribution.", "section": "F.3 Analysis under Extreme Data Heterogeneity"}, {"figure_path": "6OK8Qy9yVu/tables/tables_20_1.jpg", "caption": "Table 1: Performance of FL algorithms with full network and partial network updates.", "description": "This table compares the performance of three classic federated learning algorithms (FedAvg, FedProx, and FedMoon) using both full network updates (FNU) and partial network updates (PNU) with the proposed FedPart method.  The comparison is done across three datasets (CIFAR-10, CIFAR-100, and TinyImageNet) and considers various metrics: Best Accuracy (Best Acc.), Communication cost (Comm.), and Computation cost (Comp.). The results showcase FedPart's superior performance in terms of accuracy, communication efficiency, and computational efficiency compared to traditional FNU methods. ", "section": "4.1 Main Properties"}]