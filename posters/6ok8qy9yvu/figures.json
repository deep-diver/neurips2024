[{"figure_path": "6OK8Qy9yVu/figures/figures_1_1.jpg", "caption": "Figure 1: Update step sizes for each iteration. The experiment uses the ResNet-8 model with 20,000 CIFAR-100 images distributed in an i.i.d. manner across 40 clients.", "description": "This figure shows the update step sizes of a ResNet-8 model trained using traditional federated learning with full network updates (a) and compared to partial network updates (b). The x-axis represents the training iteration, and the y-axis represents the update step size.  In (a), the step sizes increase significantly after each parameter averaging, indicating layer mismatch. (b) shows that partial network updates significantly reduce the increase in step size after averaging.", "section": "1 Introduction"}, {"figure_path": "6OK8Qy9yVu/figures/figures_1_2.jpg", "caption": "Figure 1: Update step sizes for each iteration. The experiment uses the ResNet-8 model with 20,000 CIFAR-100 images distributed in an i.i.d. manner across 40 clients.", "description": "This figure shows a comparison of update step sizes during training between traditional federated learning with full network updates and the proposed FedPart method with partial network updates.  The y-axis represents the magnitude of the update steps, and the x-axis represents the training iteration. The figure illustrates that full network updates cause larger fluctuations in update step sizes after each averaging step, suggesting a layer mismatch problem. In contrast, the partial network updates in FedPart lead to a smoother and more stable convergence process.", "section": "1 Introduction"}, {"figure_path": "6OK8Qy9yVu/figures/figures_1_3.jpg", "caption": "Figure 2: Mechanism for layer mismatch in FedAvg and FedPart.", "description": "This figure illustrates the layer mismatch problem in federated learning. (a) shows how FedAvg (Federated Averaging), with full network updates, leads to layer mismatch.  Local models on clients have layers that cooperate well, but after averaging parameters, the global model's layers may not cooperate effectively due to differences in training on heterogeneous data. (b) demonstrates FedPart (Federated Partial), a proposed method using partial network updates, which helps mitigate this issue. By training only specific layers in each round, FedPart promotes better alignment between layers and reduces mismatch.", "section": "1 Introduction"}, {"figure_path": "6OK8Qy9yVu/figures/figures_4_1.jpg", "caption": "Figure 3: Strategy for selecting trainable layers.", "description": "This figure illustrates the strategy used for selecting trainable layers in FedPart.  It shows three approaches: Full Network Updates (all layers trained simultaneously), Sequential Training (one layer trained at a time, cycling through the layers), and Multi-round Cycle Training (repeating the sequential training process multiple times). The visual representation uses colored boxes to distinguish between trainable and untrainable layers at each round.", "section": "3.3 Selecting Trainable Layers"}, {"figure_path": "6OK8Qy9yVu/figures/figures_13_1.jpg", "caption": "Figure 4: Model architecture and layer partitioning about our ResNet-8 and ResNet-18 model.", "description": "This figure shows the architecture of ResNet-8 and ResNet-18 models used in the experiments. It also illustrates how the layers are partitioned for partial network updates in the FedPart method.  ResNet-8 is divided into 10 layers, with each layer containing trainable parameters (weights and biases of convolutional and BN layers). ResNet-18 has more layers, following a similar partitioning scheme. The figure helps visualize how the partial network updates are performed, focusing on selecting and training only a subset of layers during each training round.  The numbering helps understand the order in which layers are trained sequentially in the FedPart approach.", "section": "Implementation Details"}, {"figure_path": "6OK8Qy9yVu/figures/figures_14_1.jpg", "caption": "Figure 5: Model architecture and layer partitioning for language transformer.", "description": "This figure illustrates the architecture of a language transformer model and how its layers are partitioned for partial network updates in the FedPart method.  The model consists of an embedding layer followed by multiple encoder blocks, each composed of self-attention, layer normalization, linear transformation, and dropout layers.  The partitioning scheme shows how these layers can be grouped and updated sequentially during each training round of FedPart, enhancing training efficiency and reducing communication overhead. The final layer is a fully-connected layer for classification. ", "section": "3 Method"}, {"figure_path": "6OK8Qy9yVu/figures/figures_17_1.jpg", "caption": "Figure 6: Activation maximization images of different channels within different layers.", "description": "This figure visualizes the results of activation maximization on different channels within different layers of the ResNet-8 model trained using four different methods: FedAvg-100 (full network training), FedPart(No Init. 1C) (partial network updates without initial full network training), FedPart(1C) (partial network updates with one cycle of initial full network training), and FedPart(5C) (partial network updates with five cycles of initial full network training).  The visualization demonstrates how different training methods affect the semantic information captured by different layers. It shows that FedAvg-100 captures low-level semantic features in shallower layers and higher-level features in deeper layers. In contrast, FedPart(No Init. 1C) shows disrupted features, whereas FedPart(1C) and FedPart(5C) progressively recover the hierarchical information extraction capability of FedAvg-100.", "section": "4.3 Visualization Results"}, {"figure_path": "6OK8Qy9yVu/figures/figures_17_2.jpg", "caption": "Figure 7: Convolutional kernel visualization results of 5 planes in the first convolutional layer. Each plane include three color channels of image.", "description": "This figure visualizes the convolutional kernels in the first convolutional layer of four different models: FedAvg-100 (full network training), FedPart(No Init. 1C) (partial network training without initial full network updates), FedPart(1C) (partial network training with one cycle of initial full network updates), and FedPart(5C) (partial network training with five cycles of initial full network updates).  It shows how the kernel characteristics change depending on the training approach. FedAvg-100 shows kernels primarily functioning as edge/corner detectors. In contrast, the FedPart models without initial full network updates show more random and irregular patterns. With the inclusion of initial full network updates and multiple cycles, the FedPart kernels gradually begin to resemble those in FedAvg-100, indicating improved coordination and function.", "section": "4.3 Visualization Results"}, {"figure_path": "6OK8Qy9yVu/figures/figures_18_1.jpg", "caption": "Figure 8: The reconstructed images from DLG attacks on full network of FedAvg-100 and different partial network of FedPart(5C).", "description": "This figure visualizes the results of a Deep Leakage from Gradients (DLG) attack on different model settings.  The leftmost column shows the original images used in the attack. The subsequent columns show reconstructed images generated by DLG using gradient information from: (1) FedAvg-100 (full network updates), (2) FedPart(5C) with only layer #1 (convolutional) updated, (3) FedPart(5C) with only layer #9 (convolutional) updated, and (4) FedPart(5C) with only layer #10 (fully connected) updated. The quality of the reconstructed images reflects the amount of information leaked during the training process.  The goal is to show that FedPart (partial network updates) reduces the information leakage and thus improves privacy protection compared to FedAvg (full network updates).", "section": "E Robustness to Privacy Attack"}]