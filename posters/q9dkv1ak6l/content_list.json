[{"type": "text", "text": "Small steps no more: Global convergence of stochastic gradient bandits for arbitrary learning rates ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jincheng Mei 1 Bo Dai 1, 3 Alekh Agarwal 2 Sharan Vaswani 5 Anant Raj 6 Csaba Szepesv\u00e1ri 1 4 Dale Schuurmans 1 4 ", "page_idx": 0}, {"type": "text", "text": "1Google DeepMind 2Google Research 3Georgia Institute of Technology 4University of Alberta 5Simon Fraser University 6Indian Institute of Science ", "page_idx": 0}, {"type": "text", "text": "{jcmei,bodai,alekhagarwal,szepi,schuurmans}@google.com vaswani.sharan@gmail.com anantraj@iisc.ac.in ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We provide a new understanding of the stochastic gradient bandit algorithm by showing that it converges to a globally optimal policy almost surely using any constant learning rate. This result demonstrates that the stochastic gradient algorithm continues to balance exploration and exploitation appropriately even in scenarios where standard smoothness and noise control assumptions break down. The proofs are based on novel findings about action sampling rates and the relationship between cumulative progress and noise, and extend the current understanding of how simple stochastic gradient methods behave in bandit settings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The stochastic gradient method has been ubiquitous in the field of machine learning for decades [4]. When applied to reinforcement learning (RL), a representative instantiation of stochastic gradient is the well known policy gradient [32] (or REINFORCE [34]) algorithm, where in each iteration an online sample is gathered using the current policy, from which a gradient estimate is obtained to conduct parameter updates. In the simplest setting of a stochastic bandit problem [16], where decisions matter only for one step, the REINFORCE policy gradient method becomes equivalent to the stochastic gradient bandit algorithm [31, Section 2.8]. Compared to other statistical methods, such as the upper confidence bound algorithm (UCB, [14, 3]), and Thompson sampling (TS, [33, 2]), the stochastic gradient bandit algorithm is conceptually simpler and more computationally efficient, as it does not calculate exploration bonuses nor posterior distributions. Moreover, the stochastic gradient method is highly scalable and naturally applicable to large scale neural networks [29, 30]. ", "page_idx": 0}, {"type": "text", "text": "However, unlike UCB or TS, the stochastic gradient bandit algorithm does not have an equivalently well established and comprehensive theoretical footing. Given its pervasive success and widespread application in RL [30] and fine-tuning for large language models [26, 27]), it remains an important question to understand the success of stochastic gradient based algorithms in bandit-like settings, not only to bridge the gap between theory and practice, but also to identify more effective and robust variants. In this paper, we make a significant contribution to the theoretical understanding of the stochastic gradient bandit algorithm, bringing its justification closer to that of other less scalable but theoretically well established methods. In particular, we establish the surprising result that: ", "page_idx": 0}, {"type": "text", "text": "Since learning rate is the only tuning parameter in the stochastic gradient bandit algorithm, this result offers a remarkable robustness for the method, that it converges to a near optimal policy, irrespective of the value of this hyperparameter! Analysis of this algorithm is challenging because it requires techniques for simultaneously handling non-convex optimization, stochastic approximation, and the exploration-exploitation trade-off. Prior theoretical work on the stochastic gradient algorithm has primarily focused on non-convex convex optimization and stochastic approximation, but understanding the simultaneous effect on exploration has been largely lacking. ", "page_idx": 1}, {"type": "text", "text": "Recently, significant progress has been made in establishing global convergence results for policy gradient (PG) methods. For example, it has been shown that using exact gradients, Softmax PG converges to a globally optimal policy asymptotically as the number of iterations $t$ goes to infinity [1]. Subsequent work has demonstrated that the asymptotic rate of convergence is $O(1/t)$ [23], albeit with problem and initialization dependent constants [22, 17]. The rate and constant dependence in the true gradient setting have been improved via several techniques, including entropy regularization [23], normalization [21], and using natural gradient (mirror descent) [1, 6, 15]. ", "page_idx": 1}, {"type": "text", "text": "Unfortunately, in the online stochastic setting, where the policy gradient has to be estimated using the current policy to collect samples, these accelerated methods all obtain worse asymptotic results than the standard Softmax PG [20], failing to converge to a global optimum without careful design choices [19]. Yet in the same setting, standard Softmax PG has been shown to succeed in its simplest form, provided only that a sufficiently small learning constant rate $\\eta\\in\\Theta(1)$ is used [24]. ", "page_idx": 1}, {"type": "text", "text": "For stochastic gradient based methods, decaying or sufficiently small learning rates are used by almost all current approaches, motivated by classical convergence analyses from stochastic optimization [28, 12, 39, 38, 9, 37, 36, 24, 8]. Stationary point convergence is guaranteed for learning rates sufficiently small with respect to the smoothness of the objective function, while also decaying to zero at a precise rate if noise in the gradient estimator persists. In addition to appropriate learning rate control, many other techniques have been developed to control the effects of gradient noise, including regularization [38, 9], variance reduction [37], and carefully considering growth conditions [36, 24]. ", "page_idx": 1}, {"type": "text", "text": "The technical challenges we face in the current study can be understood in the following aspects: (1) Using an arbitrarily large constant learning rate for online stochastic gradient optimization immediately renders the smoothness and noise control techniques mentioned above inapplicable. (2) With any constant learning rate $\\eta>0$ , the question of whether oscillation or convergence will ultimately occur needs to be addressed before even considering whether any convergence is to a global optimum. This additional level of complexity arises because the optimization objective is not necessarily improved monotonically in expectation. Finally, (3) The gradient bandit algorithm does not use any exploration bonus, which means that new techniques are required to demonstrate that it adequately balances the exploration-exploitation trade-off. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we resolve the above difficulties by uncovering intriguing exploration properties of stochastic gradient when using any constant learning rate. In particular, we establish the following. ", "page_idx": 1}, {"type": "text", "text": "(i) In the stochastic online setting, with probability 1, the stochastic gradient bandit algorithm will not keep sampling any single action forever, implying that it will exhibit a minimal form of exploration without any further modification. This asymptotic event (as $t\\to\\infty$ ) happens with probability 1 and holds for any constant learning rate $\\eta>0$ .   \n(ii) This result can then be leveraged to show that, as a consequence, given any constant learning rate, the stochastic gradient bandit algorithms will converge to the globally optimal policy as $t\\to\\infty$ , with probability 1. That is, the probability of sub-optimal actions decays to 0, even though some of them are taken infinitely often asymptotically. ", "page_idx": 1}, {"type": "text", "text": "2 Setting and Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider the stochastic multi-armed bandit problem [16], specified by $K$ actions and a true mean reward vector $r\\in\\mathbb{R}^{K}$ , where for each action $a\\in[K]:=\\{1,2,\\ldots,K\\}$ , ", "page_idx": 1}, {"type": "equation", "text": "$$\nr(a)=\\int_{-R_{\\mathrm{max}}}^{R_{\\mathrm{max}}}x\\cdot P_{a}(x)\\mu(d x),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $R_{\\mathrm{max}}>0$ is the reward range, $\\mu$ is a finite measure over $[-R_{\\mathrm{max}},R_{\\mathrm{max}}]$ , and $P_{a}(x)\\geq0$ is the probability density function with respect to $\\mu$ . We use $R_{a}$ to denote the reward distribution for ", "page_idx": 1}, {"type": "text", "text": "Algorithm 1 Gradient bandit algorithm (without baselines) Input: initial parameters $\\theta_{1}\\in\\mathbb{R}^{K}$ , learning rate $\\eta>0$ . Output: policies $\\pi_{\\theta_{t}}=\\mathrm{softmax}(\\theta_{t})$ . while $t\\geq1$ do Sample an action $a_{t}\\sim\\pi_{\\theta_{t}}(\\cdot)$ and observe reward $R_{t}(a_{t})\\sim P_{a_{t}}$ . for all $a\\in[K]$ do if $a=a_{t}$ then $\\theta_{t+1}(a)\\gets\\theta_{t}(a)+\\eta\\cdot(1-\\pi_{\\theta_{t}}(a))\\cdot R_{t}(a_{t}).$ else $\\theta_{t+1}(a)\\gets\\theta_{t}(a)-\\eta\\cdot\\pi_{\\theta_{t}}(a)\\cdot R_{t}(a_{t}).$ end if end for end while ", "page_idx": 2}, {"type": "text", "text": "action $a$ defined by the density $P_{a}$ and base measure $\\mu$ . The goal is to find a policy $\\pi_{\\theta}\\in[0,1]^{K}$ to achieve high expected reward, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta\\in\\mathbb{R}^{K}}\\pi_{\\theta}^{\\top}r,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pi_{\\theta}$ is parameterized by $\\boldsymbol\\theta\\in\\mathbb{R}^{K}$ . ", "page_idx": 2}, {"type": "text", "text": "The gradient bandit algorithm. A natural idea to optimize Eq. (2) is to use stochastic gradient ascent, which is shown in Algorithm 1 and known as the gradient bandit algorithm [31, Section 2.8]. In Algorithm 1, in each iteration $t\\geq1$ , the probability of pulling arm $a\\in[K]$ is given as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi_{\\theta_{t}}(a)=[\\mathrm{softmax}(\\theta_{t})](a):=\\frac{\\exp\\{\\theta_{t}(a)\\}}{\\sum_{a^{\\prime}\\in[K]}\\exp\\{\\theta_{t}(a^{\\prime})\\}},\\quad\\mathrm{~for~all~}a\\in[K],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\theta_{t}\\ \\in\\ \\mathbb{R}^{K}$ is the parameter vector to be updated. The following proposition shows that Algorithm 1 is an instance of stochastic gradient ascent with an unbiased gradient estimator [31, 24]. ", "page_idx": 2}, {"type": "text", "text": "Proposition 1 (Proposition 2.3 of [24]). Algorithm $^{\\,l}$ is equivalent to the following update, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\theta_{t+1}\\leftarrow\\theta_{t}+\\eta\\cdot\\frac{d\\,\\pi_{\\theta_{t}}^{\\top}\\hat{r}_{t}}{d\\theta_{t}}=\\theta_{t}+\\eta\\cdot\\left(d i a g(\\pi_{\\theta_{t}})-\\pi_{\\theta_{t}}\\pi_{\\theta_{t}}^{\\top}\\right)\\hat{r}_{t},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbb{E}_{t}\\left[{\\frac{d\\;\\pi_{\\theta_{t}}^{\\top}{\\hat{r}}_{t}}{d\\theta_{t}}}\\right]={\\frac{d\\;\\pi_{\\theta_{t}}^{\\top}r}{d\\theta_{t}}}$ , and $\\mathbb{E}_{t}[\\cdot]$ is defined with respect to randomness from on-policy sampling $a_{t}\\,\\sim\\,\\pi_{\\theta_{t}}(\\cdot)$ and reward sampling $R_{t}(a_{t})\\,\\sim\\,P_{a_{t}}$ . The Jacobian of $\\theta\\;\\mapsto\\;\\pi_{\\theta}\\;:=\\;\\mathrm{softmax}(\\theta)$ is $\\begin{array}{r}{\\left(\\frac{d\\ \\pi_{\\theta}}{d\\theta}\\right)^{\\top}\\ =\\ d i a g(\\pi_{\\theta})\\,-\\,\\pi_{\\theta}\\pi_{\\theta}^{\\top}\\ \\in\\ \\mathbb{R}^{K\\times K}}\\end{array}$ , and I{\u03c0at(=aa)} \u00b7 Rt(a) for all a \u2208 [K] is the importance sampling $(I S)$ estimator, and we set $R_{t}(a)=0$ for all $a\\neq a_{t}$ . ", "page_idx": 2}, {"type": "text", "text": "Known results on the convergence of the gradient bandit algorithm. Since Eq. (2) corresponds to a smooth non-concave maximization problem over $\\boldsymbol\\theta\\in\\mathbb{R}^{K}$ [23], using Algorithm 1 with decaying learning rates is sufficient to guarantee convergence to a stationary point [28, 25, 12, 39]. However, this is insufficient to ensure the globally optimal solution of Eq. (2) is reached, since there exist multiple stationary points. More recently, guarantees of convergence to a globally optimal policy have been developed for PG methods in the true gradient setting [1, 23], where the algorithm has access to exact mean rewards. These results were later extended to achieve global convergence guarantees (almost surely) in the stochastic setting [38, 9, 37, 36, 24, 18]. However, these extended results have required decaying or sufficiently small learning rates, motivated by exploiting smoothness and combating the inherent noise in stochastic gradients. ", "page_idx": 2}, {"type": "text", "text": "Despite these previous assumptions, there exists empirical and theoretical evidence that using a large learning rate in the stochastic gradient bandit algorithm is a viable option. For example, it has been observed that softmax policies learn even with extremely large learning rates such as $2^{14}$ [11]. For logistic regression on linearly separable data, the objective has an exponential tail and the minimizer is unbounded, yet it has been shown that gradient descent with iteration dependent learning rate $\\eta\\in\\Theta(t)$ achieves accelerated ${\\cal O}(1/t^{2})$ convergence [35]. Though the objective in Eq. (2) has similar properties, unlike logistic regression, the problem we are considering is non-concave, so the same techniques cannot be directly applied. The most related results are from [24], which proved that with a small problem specific constant learning rate, Algorithm 1 achieves convergence to a globally optimal policy almost surely. However, as mentioned, the learning rate choices in [24] rely on assumptions of (non-uniform) smoothness and noise growth conditions (their Lemmas 4.2, 4.3, and 4.6), which cannot be directly applied here for a large learning rate. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Consequently, the use of large learning rates appear to render existing results and techniques inapplicable. Furthermore, with a large constant learning rate, it is unclear whether Algorithm 1 will converge to any stationary point, or the iterates will keep oscillating. If the algorithm does converge, it is also not clear what effect large step-sizes have on exploration, and whether the algorithm will converge to the optimal arm in such cases. Resolving these questions requires new results that characterize the behavior of Algorithm 1, since the classical optimization and stochastic approximation convergence theories are no longer applicable, as explained. ", "page_idx": 3}, {"type": "text", "text": "3 Asymptotic Global Convergence of Gradient Bandit Algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We have seen that solving the non-concave maximization problem Eq. (2) using Algorithm 1 with any constant (potentially large) learning rate requires ideas beyond classical optimization theory. Here, we take a different perspective to investigate how Algorithm 1 samples actions. For analysis, we make the following assumption about the reward distribution. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 (True mean reward has no ties). For all $i,j\\in[K],$ , if $i\\neq j$ , then $r(i)\\neq r(j)$ . ", "page_idx": 3}, {"type": "text", "text": "Remark 1. Removing Assumption 1 remains an open question for future work, while we believe that Algorithm 1 works without Assumption 1. One piece of evidence to support this conjecture is that even in the exact gradient setting, the set of initializations where Softmax PG approaches non-strict one-hot policies has zero measure. ", "page_idx": 3}, {"type": "text", "text": "3.1 Failure Mode of Aggressive Updates ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "It has been observed that several accelerated PG methods in the true gradient setting, including natural PG [13, 1] and normalized PG [21], obtain worse results than standard softmax PG if combined with online sampling $a_{t}\\sim\\pi_{\\theta_{t}}(\\cdot)$ using constant learning rates [20]. The failure mode in these cases is that the update is too aggressive and commits to a sub-optimal arm without sufficiently exploring all arms. This results in a non-trivial probability of sampling one action forever, i.e., there exists a potentially sub-optimal action $a\\in[K]$ , such that with some constant probability, $a_{t}=a$ for all $t\\geq1$ . Such an outcome implies that $\\pi_{\\theta_{t}}(a)\\to1$ as $t\\to\\infty$ [20, Theorem 3]. Since $a\\in[K]$ could be a sub-optimal action with $\\textstyle r(a)<r(a^{*})={\\operatorname*{max}}_{a\\in[K]}\\,r(a)$ , this results in a lack of exploration, and consequently, methods such as natural PG and normalized PG are not guaranteed to converge to the optimal action $a^{*}:=\\arg\\operatorname*{max}_{a\\in[K]}r(a)$ with probability 1. ", "page_idx": 3}, {"type": "text", "text": "3.2 Stochastic Gradient Automatically Avoids Lack of Exploration ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our first key finding is that Algorithm 1 does not keep sampling one action forever, no matter how large the constant learning rate is. This property avoids the problem of a lack of exploration, in the sense that Algorithm 1 will at least explore more than one action infinitely often. At first glance, this might not seem like a strong property, since the algorithm might somehow explore only sub-optimal actions forever. However, we will argue below that this property coupled with additional arguments is sufficient to guarantee convergence to the globally optimal policy. ", "page_idx": 3}, {"type": "text", "text": "Let us now formally prove the above property. By Algorithm 1, for all $a\\in[K]$ , for all $t\\geq1$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\theta_{t+1}(a)\\gets\\theta_{t}(a)+\\left\\{\\begin{array}{r l}{\\eta\\cdot(1-\\pi_{\\theta_{t}}(a))\\cdot R_{t}(a),\\quad\\mathrm{if~}a_{t}=a,}\\\\ {-\\eta\\cdot\\pi_{\\theta_{t}}(a)\\cdot R_{t}(a_{t}),\\quad\\mathrm{otherwise}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We define $N_{t}(a)$ as the number of times action $a\\in[K]$ is sampled up to iteration $t\\geq1$ , i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\nN_{t}(a):=\\sum_{s=1}^{t}\\mathbb{I}\\left\\{a_{s}=a\\right\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and its asymptotic limit $\\begin{array}{r}{N_{\\infty}(a):=\\operatorname*{lim}_{t\\to\\infty}N_{t}(a)}\\end{array}$ , which could possibly be infinity. For all $a\\in[K]$ , we have either $N_{\\infty}(a)=\\infty$ or $N_{\\infty}(a)<\\infty$ , meaning that $a\\in[K]$ is sampled infinitely often or only finitely many times asymptotically. First, we prove the following Lemma 1, which shows that if an action $a\\in[K]$ is sampled only finitely many times as $t\\to\\infty$ , then the parameter corresponding to action $a$ is also finite, i.e., $\\operatorname*{sup}_{t\\geq1}|\\theta_{t}(\\bar{a})|<\\stackrel{\\cdot}{\\infty}$ . ", "page_idx": 4}, {"type": "text", "text": "Lemma 1. Using Algorithm $^{\\,l}$ with any constant $\\eta\\in\\Theta(1)$ , if $N_{\\infty}(a)<\\infty$ for an action $a\\in[K]$ , then we have, almost surely, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq1}\\theta_{t}(a)<\\infty,\\;a n d\\ \\operatorname*{inf}_{t\\geq1}\\theta_{t}(a)>-\\infty.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Lemma 1 will be used multiple times in the subsequent convergence arguments. ", "page_idx": 4}, {"type": "text", "text": "Proof sketch. Since we assume action $a\\in[K]$ is sampled finitely many times, the update given in the case depicted by Eq. (5) happens finitely many times. Each update is bounded since the sampled reward is in $[-R_{\\mathrm{max}},R_{\\mathrm{max}}]$ by Eq. (1), and the learning rate is a constant, i.e., $\\eta\\in\\Theta(1)$ In Algorithm 1, $\\theta_{t}(a)$ is still updated even when $a_{t}~\\neq~a$ , with the corresponding update given by the case depicted by Eq. (6). Therefore, whether $\\theta_{t}(a)$ is bounded depends on the cumulative probability $\\scriptstyle\\sum_{s=1}^{t}\\pi_{\\theta_{s}}(a)$ being summable as $t\\,\\rightarrow\\,\\infty$ . According to the extended Borel-Cantelli lemma (Le mma 3), we have, almost surely, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Big\\{\\sum_{t\\ge1}\\pi_{\\theta_{t}}(a)=\\infty\\Big\\}=\\{N_{\\infty}(a)=\\infty\\}\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which implies (by taking complements) that $\\begin{array}{r}{\\sum_{t\\geq1}\\pi_{\\theta_{t}}(a)<\\infty}\\end{array}$ if and only if $N_{\\infty}(a)<\\infty$ . Therefore, if $a\\in[K]$ is sampled finitely often, $\\theta_{t}(a)$ will be updated in a bounded manner (using Eqs. (5) and (6)) as $t\\to\\infty$ , hence establishing Lemma 1. Detailed proofs for this lemma, as well as for all other results in this paper can be found in the appendix. ", "page_idx": 4}, {"type": "text", "text": "Given Lemma 1, we can then establish the above-mentioned finding about the exploration effect of Algorithm 1 in Lemma 2. ", "page_idx": 4}, {"type": "text", "text": "Lemma 2 (Avoiding a lack of exploration). Using Algorithm $^{\\,l}$ with any $\\eta\\in\\Theta(1)$ , there exists at least a pair of distinct actions $i,j\\in[K]$ and $i\\neq j$ , such that, almost surely, ", "page_idx": 4}, {"type": "equation", "text": "$$\nN_{\\infty}(i)=\\infty,\\;a n d\\;N_{\\infty}(j)=\\infty.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Proof sketch. The argument for the existence of one such action is straightforward, since by the pigeonhole principle, if there are finitely many actions, i.e., $K<\\infty$ , there must be at least one action $i\\in[K]$ that is sampled infinitely often as $t\\to\\infty$ . ", "page_idx": 4}, {"type": "text", "text": "The argument for the existence of a second such action is by contradiction. Suppose that all the other actions $j~\\in~[K]$ with $j\\neq i$ are sampled only finitely many times as $t\\,\\rightarrow\\,\\infty$ . According to Lemma 1, their corresponding parameters must remain finite, i.e., $\\mathrm{sup}_{t\\geq1}\\left|\\theta_{t}(j)\\right|\\,<\\,\\infty$ for all $j\\in[K]$ with $j\\neq i$ . Now consider $\\theta_{t}(i)$ . By assumption, the second update case for this parameter, Eq. (6), happens only finitely often, since Eq. (6) can only occur when $a_{t}\\neq i$ . Therefore, the key question is whether the cumulative probability $\\begin{array}{r}{\\sum_{s=1}^{t}\\left(1-\\pi_{\\theta_{s}}(i)\\right)}\\end{array}$ involved in the first case of the update, Eq. (5), is summable as $t\\to\\infty$ . Note that $\\begin{array}{r}{\\sum_{s=1}^{t}\\left(1-\\pi_{\\theta_{s}}(i)\\right)=\\sum_{s=1}^{t}\\sum_{j\\neq i}\\pi_{\\theta_{s}}(j)}\\end{array}$ , which is indeed summable as $t\\to\\infty$ , by the assumption and Eq. (9). This implies that action $i$ , which is sampled infinitely often, achieves a parameter magnitude, $\\mathrm{sup}_{t\\geq1}\\left|\\theta_{t}(i)\\right|<\\infty$ , that remains bounded as $t\\to\\infty$ . Using the softmax parameterization Eq. (3) in the above argument, we conclude that for all $a\\in[K]$ , $\\operatorname*{inf}_{t\\geq1}\\pi_{\\theta_{t}}(a)>0$ , i.e., every action\u2019s probability remains bounded away from zero, and hence is not summable. Using Eq. (9), this implies that every action is sampled infinitely often, which contradicts the assumption that only action $i$ is sampled infinitely often as $t\\to\\infty$ . ", "page_idx": 4}, {"type": "text", "text": "Discussion. Lemma 2 implies that Algorithm 1 is not an aggressive method in the sense of [20], no matter how large the learning rate is, as long as it is constant, i.e., $\\eta\\in\\Theta(1)$ . According to [20, Theorem 7], even if we fix the sampling in Algorithm 1 to a sub-optimal action $\\boldsymbol{a}\\in[K]$ forever, i.e., $a_{t}=a$ for all $t\\geq1$ , its probability will not approach 1 faster than ${\\bar{O}}(1/t)$ , i.e., $1-\\bar{\\pi_{\\theta_{t}}}(\\dot{a})\\in\\Omega(1/t)$ . This means that there must be at least one another action $a^{\\prime}\\in[K]$ with $a^{\\prime}\\neq a$ , such that $a^{\\prime}$ will also be sampled infinitely often. A more intuitive explanation is that the $(1-\\pi_{\\theta_{t}}(a))$ term in Eq. (5) will be near 0, which slows the speed of committing to a deterministic policy on $a$ whenever $\\pi_{\\theta_{t}}(a)$ is close to 1, which encourages exploration. Such natural exploratory behavior arises in Algorithm 1 because of the softmax Jacobian $\\mathrm{diag}(\\pi_{\\theta})-\\pi_{\\theta}\\pi_{\\theta}^{\\top}$ in the update shown in Proposition 1, which determines the growth order of $\\theta_{t}(a)$ for all $a\\,\\in\\,[K]$ as $t\\to\\infty$ , making the effect of a constant learning rate $\\eta\\in\\Theta(1)$ asymptotically inconsequential. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.3 Warm up: Global Asymptotic Convergence when $K=2$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now consider the simplest case, where we have only two possible actions. According to Lemma 2, each of the two actions must be sampled infinitely often as $t\\to\\infty$ . We now illustrate the second key result, that for both actions $a\\in[K]$ , the random sequence $\\{\\theta_{t}(a)\\}_{t\\geq1}$ follows the direction of the expected gradient for sufficiently large $t\\geq1$ almost surely. The proof uses a technique that has been previously used in [19, 24] for small learning rates, but here we observe that the same technique continues to work for Algorithm 1 no matter how large the learning rate is, as long as $\\eta\\in\\Theta(1)$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Let $K=2$ and $r(1)>r(2)$ . Using Algorithm $^{\\,l}$ with any $\\eta\\in\\Theta(1)$ , we have, almost surely, $\\pi_{\\theta_{t}}(a^{*})\\to1$ as $t\\to\\infty$ , where $a^{*}:=\\arg\\operatorname*{max}_{a\\in[K]}r(a)$ (equal to Action 1 in this case). ", "page_idx": 5}, {"type": "text", "text": "Proof sketch. According to Lemma 2, $N_{\\infty}(1)\\,=\\,N_{\\infty}(2)\\,=\\,\\infty$ . Denote the the reward gap as $\\Delta:=\\,r(a^{*})-\\operatorname*{max}_{a\\neq a^{*}}\\,r\\bar{(a)}>0$ , which becomes $\\Delta\\,=\\,r(1)\\,-\\,r(2)$ for two actions. Since the stochastic gradient is unbiased (Proposition 1), we have, for all $t\\geq1$ (detailed calculations omitted), ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}[\\theta_{t+1}(a^{*})]=\\theta_{t}(a^{*})+\\eta\\cdot\\pi_{\\theta_{t}}(a^{*})\\cdot\\big(r(a^{*})-\\pi_{\\theta_{t}}^{\\top}r\\big)}\\\\ &{\\qquad\\qquad\\qquad=\\theta_{t}(a^{*})+\\eta\\cdot\\pi_{\\theta_{t}}(a^{*})\\cdot\\Delta\\cdot(1-\\pi_{\\theta_{t}}(a^{*}))>\\theta_{t}(a^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "A similar calculation shows that, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t}[\\theta_{t+1}(2)]=\\theta_{t}(2)-\\eta\\cdot\\pi_{\\theta_{t}}(2)\\cdot\\Delta\\cdot(1-\\pi_{\\theta_{t}}(2))<\\theta_{t}(2),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which means that $\\theta_{t}(a^{*})$ is monotonically increasing in expectation and $\\theta_{t}(2)$ is monotonically decreasing in expectation. In other words, $\\{\\theta_{t}(a^{*})\\}_{t\\geq1}$ is a sub-martingale, while $\\{\\theta_{t}(2)\\}_{t\\geq1}$ is a super-martingale. However, since $\\theta_{t}\\:\\in\\:\\mathbb{R}^{K}$ is unbounded, Doob\u2019s martingale convergence results cannot be directly applied, so we pursue a different argument. Following [19, 24], given an action $a\\in[K]$ , we define $\\bar{P}_{t}(a):=\\mathbb{E}_{t}[\\bar{\\theta}_{t+1}(a)]-\\theta_{t}(a)$ as the \u201cprogress\u201d, and define $W_{t}(a):=$ $\\theta_{t}(a)-\\mathbb{E}_{t-1}[\\theta_{t}(a)]$ as the \u201cnoise\u201d, where $\\partial_{t}(a)\\ '=W_{t}(a)+P_{t-1}(a)+\\theta_{t-1}(a)$ . By recursion we can determine that, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\theta_{t}(a)=\\mathbb{E}[\\theta_{1}(a)]+\\sum_{s=1}^{t}W_{s}(a)+\\sum_{s=1}^{t-1}P_{s}(a),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "i.e., $\\theta_{t}(a)$ is the result of \u201ccumulative progress\u201d and \u201ccumulative noise\u201d. According to [24, Theorem C.3], the cumulative noise term can be bounded by using martingale concentration, where the order of the corresponding confidence interval is smaller than the order of the cumulative progress. Therefore, the summation will always be determined by the cumulative progress as $t~\\rightarrow~\\infty$ . According to the calculations in Eqs. (12) and (13), we have $P_{t}(a^{*})\\;>\\;\\bar{0}$ and $P_{t}(2)\\,<\\,0$ , both of which are not summable. As a result, $\\theta_{t}(a^{*})\\,\\to\\,\\infty$ and $\\theta_{t}(2)\\,\\to\\,-\\infty$ as $t\\,\\rightarrow\\,\\infty$ , which implies that $\\begin{array}{r}{\\frac{\\pi_{\\theta_{t}}(a^{*})}{\\pi_{\\theta_{t}}(2)}=\\exp\\{\\theta_{t}(a^{*})-\\theta_{t}(a)\\}\\to\\infty}\\end{array}$ , hence $\\pi_{\\theta_{t}}(a^{*})\\to1$ as $t\\to\\infty$ . ", "page_idx": 5}, {"type": "text", "text": "3.4 Global Asymptotic Convergence for all $K\\geq2$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The illustrative two-action case shows that if $\\pi_{\\theta_{t}}^{\\top}r\\in(r(2),r(a^{*}))$ and if both actions are sampled infinitely often, then we have, almost surely $\\theta_{t}(a^{*})\\to\\infty$ and $\\theta_{t}(2)\\rightarrow-\\infty$ as $t\\to\\infty$ . However, the question at the beginning of Section 3.2 remains: when $K>2$ , if the two actions sampled infinitely often in Lemma 2 are both sub-optimal, will that result in a similar failure mode to the one described in Section 3.1? The answer is no, which follows from our third key finding, which is based on another contradiction-based argument that establishes almost sure convergence to a globally optimal policy in the general $K>2$ case. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Given $K\\ge2$ , using Algorithm $^{\\,l}$ with any $\\eta\\in\\Theta(1)$ , we have, almost surely, $\\pi_{\\theta_{t}}(a^{*})\\to$ 1 as $t\\to\\infty$ , where $a^{*}=\\arg\\operatorname*{max}_{a\\in[K]}r(a)$ is the optimal action. ", "page_idx": 5}, {"type": "text", "text": "Proof sketch. We consider two cases: $N_{\\infty}(a^{*})\\;<\\;\\infty$ and $N_{\\infty}(a^{*})\\;=\\;\\infty$ , corresponding to whether the optimal action is sampled finitely or infinitely often as $t\\,\\rightarrow\\,\\infty$ . We argue that the first case $(N_{\\infty}\\Bar{(a^{*})}<\\infty)$ is impossible, while for the second case $(N_{\\infty}(a^{*})=\\infty)$ we prove that $\\theta_{t}(a^{*})-\\theta_{t}(a)\\rightarrow\\infty$ for all $a\\in[K]$ with $r(a)<r(a^{*})$ , which implies $\\pi_{\\theta_{t}}(a^{*})\\to1$ as $t\\to\\infty$ . ", "page_idx": 6}, {"type": "text", "text": "First case. Suppose that $N_{\\infty}(a^{*})<\\infty$ . We argue that this is impossible via contradiction. Given the assumption and Lemma 2 we know there must be at least two other sub-optimal actions $i_{1},i_{2}\\in[K]$ , $i_{1}\\neq i_{2}$ , such that $N_{\\infty}(i_{1})=N_{\\infty}(i_{2})=\\infty$ . In particular, let $\\begin{array}{r}{i_{1}=\\arg\\operatorname*{min}_{a\\in[K],N_{\\infty}(a)=\\infty}r(a)}\\end{array}$ and $i_{2}=\\arg\\operatorname*{max}_{a\\in[K],N_{\\infty}(a)=\\infty}r(a)$ , hence $r(i_{1})<r(i_{2})<r(a^{*})$ . By Lemma 6 (see Appendix) we will also have $r(i_{1})<\\pi_{\\theta_{t}}^{\\top}r<r(i_{2})$ for sufficiently large $t\\geq1$ , which implies for action $i_{1}$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{t}[\\theta_{t+1}(i_{1})]=\\theta_{t}(i_{1})+\\eta\\cdot\\pi_{\\theta_{t}}(i_{1})\\cdot\\big(r(i_{1})-\\pi_{\\theta_{t}}^{\\top}r\\big)<\\theta_{t}(i_{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for sufficiently large $t\\geq1$ , which further implies that $\\mathrm{sup}_{t\\geq1}\\,\\theta_{t}(i_{1})\\,<\\,\\infty$ . Meanwhile, for the optimal action $a^{*}$ , the assumption and Lemma 1 imply that $\\operatorname*{inf}_{t\\geq1}\\theta_{t}(a^{*})>-\\infty$ . Combining these two observations gives, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq1}\\frac{\\pi_{\\theta_{t}}(i_{1})}{\\pi_{\\theta_{t}}(a^{*})}=\\operatorname*{sup}_{t\\geq1}\\exp\\{\\theta_{t}(i_{1})-\\theta_{t}(a^{*})\\}<\\infty\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "On the other hand, since $N_{\\infty}(i_{1})\\ =\\ \\infty$ and $N_{\\infty}(a^{*})\\ <\\ \\infty$ (by assumption), we then have $\\begin{array}{r}{\\operatorname*{sup}_{t\\geq1}\\exp\\{\\theta_{t}(i_{1})-\\theta_{t}(a^{*})\\}=\\stackrel{\\cdot}{\\infty}}\\end{array}$ by Lemma 5 (see Appendix), which contradicts Eq. (16). ", "page_idx": 6}, {"type": "text", "text": "Second case. Suppose that $N_{\\infty}(a^{*})=\\infty$ . We will argue that $\\pi_{\\theta_{t}}(a^{*})\\to1$ as $t\\to\\infty$ almost surely. First, according to Lemma 2, there exists at least one sub-optimal action $i_{1}\\in[K],$ , $i_{1}\\neq a^{*}$ , such that $N_{\\infty}(i_{1})=\\infty$ . Let $\\begin{array}{r}{i_{1}=\\arg\\operatorname*{min}_{a\\in[K],N_{\\infty}(a)=\\infty}r(a)}\\end{array}$ . By Lemma 6 and the definition of $a^{*}$ , we have $r(i_{1})<\\pi_{\\theta_{t}}^{\\top}r<r(a^{*})$ for all sufficiently large $t\\geq1$ . Since $N_{\\infty}(i_{1})=\\infty$ , using similar calculations to Eqs. (13) and (14) in Theorem 1, we have, $\\theta_{t}(i_{1})\\rightarrow-\\infty$ as $t\\to\\infty$ . We also have, inft\u22651 \u03b8t(a\u2217) > \u2212\u221eas t \u2192\u221e. Hence, \u03c0\u03c0\u03b8\u03b8tt((ai1)) $\\begin{array}{r}{\\frac{\\pi_{\\theta_{t}}(a^{*})}{\\pi_{\\theta_{t}}(i_{1})}=\\exp\\{\\theta_{t}(a^{*})-\\theta_{t}(i_{1})\\}\\to\\infty}\\end{array}$ as $t\\to\\infty$ . ", "page_idx": 6}, {"type": "text", "text": "Define $A_{\\infty}:=\\{a\\in[K]\\mid N_{\\infty}(a)=\\infty\\}$ as the set of actions that are sampled infinitely often, and note that $|\\mathcal{A}_{\\infty}|\\geq2$ by Lemma 2. Sort the action indices in $A_{\\infty}$ according to their expected reward values in descending order, i.e., ", "page_idx": 6}, {"type": "equation", "text": "$$\nr(a^{*})>r(i_{|A_{\\infty}|-1})>r(i_{|A_{\\infty}|-2})>\\cdot\\cdot>r(i_{2})>r(i_{1}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Assumption 1 is used here to prevent two arms from having the same reward and thus guarantee the inequalities are strict in Eq. (17). Next, using similar calculations as in Lemma 6, we have, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pi_{\\theta_{t}}^{\\top}r-r(i_{2})>\\pi_{\\theta_{t}}(a^{*})\\cdot\\left[r(a^{*})-r(i_{2})-\\sum_{a^{-}\\in A^{-}(i_{2})}\\frac{\\pi_{\\theta_{t}}(a^{-})}{\\pi_{\\theta_{t}}(a^{*})}\\cdot\\left(r(i_{2})-r(a^{-})\\right)\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{A}^{-}(i_{2}):=\\{a^{-}\\in[K]:r(a^{-})<r(i_{2})\\}}\\end{array}$ is the set of actions that have lower mean reward than $i_{2}\\in[K]$ , and note that $i_{1}\\in A^{-}(i_{2})$ . Using the above definitions, we can conclude that $i_{1}$ is the only arm in $\\dot{A}^{-}(i_{2})$ that has been sampled infinitely often. According to Lemma 5, for all $a^{-}\\in A^{-}(i_{2})$ with a\u2212 \u0338= i1, we have, \u03c0\u03c0\u03b8\u03b8tt((aa\u2212)) $\\frac{\\pi_{\\theta_{t}}(a^{*})}{\\pi_{\\theta_{t}}(a^{-})}\\;\\rightarrow\\;\\infty$ as $t\\,\\rightarrow\\,\\infty$ , since $N_{\\infty}(a^{*})\\,=\\,\\infty$ (by assumption) and $N_{\\infty}(a^{-})<\\infty$ (by Eq. (17)). Therefore, for all sufficiently large $t$ , the probability ratio in Eq. (18) $\\frac{\\pi_{\\theta_{t}}(a^{*})}{\\pi_{\\theta_{t}}(a-\\prime)}\\rightarrow\\infty$ for all $a^{-}\\in A^{-}(i_{2})$ , which implies that, for all sufficiently large $t\\geq1$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pi_{\\theta_{t}}^{\\top}r-r(i_{2})>0.5\\cdot\\pi_{\\theta_{t}}(a^{*})\\cdot\\big(r(a^{*})-r(i_{2})\\big)>0.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We have thus shown that $\\pi_{\\theta_{t}}^{\\top}r>r(i_{2})$ . Recall that we had previously proved that $\\pi_{\\theta_{t}}^{\\top}r\\,>\\,r(i_{1})$ Hence, we will apply this argument recursively: after this point, $i_{2}\\,\\in\\,[K]$ will become the new $\\begin{array}{r}{\\cdots i_{1}\\in[K]^{\\ast}}\\end{array}$ , and a similar inequality to Eq. (13) will then hold for $i_{2}\\in[K]$ from similar calculations to Eqs. (13) and (14) in Theorem 1, establishing $\\theta_{t}(i_{2})\\rightarrow-\\infty$ as $t\\to\\infty$ . This will imply that for all sufficiently large $t\\geq1$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pi_{\\theta_{t}}^{\\top}r-r(i_{3})>0.5\\cdot\\pi_{\\theta_{t}}(a^{*})\\cdot\\big(r(a^{*})-r(i_{3})\\big)>0.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Continuing the recursive argument, we can conclude for all actions $a\\in A_{\\infty}$ with $a\\neq a^{*}$ that $\\frac{\\pi_{\\theta_{t}}(a^{*})}{\\pi_{\\theta_{t}}(a)}\\rightarrow\\infty$ ) \u2192\u221eas t \u2192\u221e. Meanwhile, for all actions a \u0338\u2208A\u221e, Lemma 5 also shows that \u03c0\u03c0\u03b8\u03b8t((aa\u2217)) $\\frac{\\pi_{\\theta_{t}}(a^{*})}{\\pi_{\\theta_{t}}(a)}\\rightarrow\\infty$ as $t\\to\\infty$ . Combining these two results yields the conclusion that for all sub-optimal actions $a\\in[K]$ with $r(a)<r(a^{*})$ we have $\\frac{\\pi_{\\theta_{t}}(a^{*})}{\\pi_{\\theta_{t}}(a)}\\rightarrow\\infty$ as $t\\to\\infty$ , which implies $\\pi_{\\theta_{t}}(a^{*})\\to1$ as $t\\to\\infty$ . Thus, we have established almost sure convergence to the globally optimal policy. ", "page_idx": 6}, {"type": "text", "text": "Discussion. Lemma 2 is important to prove that the optimal arm will be sampled infinitely often. In particular, Lemma 2 guarantees $|\\mathcal{A}_{\\infty}|\\ge2$ and the existence of $i_{1}$ in Eq. (15), which can then be used to construct the contradiction in Eq. (16). Without Lemma 2, $|\\mathcal{A}_{\\infty}|$ might be equal to 1 and the failure mode in Section 3.1 can occur, resulting in Algorithm 1 not sampling the optimal action infinitely often as $t\\to\\infty$ . ", "page_idx": 7}, {"type": "text", "text": "4 Simulation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To validate and enhance the theoretical findings, we ran experiments with a four action bandit environment $K=4,$ with a true mean reward vector of $r=^{^{\\circ}}(0.2,0.05,-0.1,-0.4)^{^{\\intercal}}\\in\\mathbb{R}^{4}$ . The reward distribution $P_{a}$ for arm $1\\le a\\le4$ is Gaussian, centered at $r(a)$ and with a standard deviation of 0.1. The environment is chosen to illustrate various phenomenon, which we discuss after presenting the results. The algorithm is Algorithm 1 with $\\boldsymbol{\\theta}_{1}=\\dot{\\mathbf{0}}\\in\\mathbb{R}^{K}$ . ", "page_idx": 7}, {"type": "text", "text": "For comparison, assuming that the random rewards belong to the $[-1,1]$ interval, the only result for the stochastic gradient bandit algorithm [24, Lemma 4.6] that allowed a constant learning rate required that the learning rate be less than \u03b7c =40\u00b7K3/\u22062\u00b7R3  =1 $\\begin{array}{r}{\\eta_{c}=\\frac{\\Delta^{2}}{40\\cdot K^{3/2}\\cdot R_{\\mathrm{max}}^{3}}=\\frac{9}{128000}\\approx0.00007}\\end{array}$ , where we used $\\Delta=0.15$ , $K=4$ , and $R_{\\operatorname*{max}}=1$ . While technically, the result does not apply to our case where the reward distributions have unbounded support, the probability of the reward landing outside of $[-1,1]$ is in the order of $10^{-9}$ . Choosing $R_{\\mathrm{max}}$ to be larger, this probability falls extremely quickly, which suggests that the above threshold is generous. For the experiments we use the learning rates $\\eta\\in\\{1,10,100,1000\\}$ , that are several orders of magnitudes larger than $\\eta_{c}$ . For each learning rate, we plot the outcome of 10 runs, corresponding to different random seeds. Each run lasts $10^{6}(\\stackrel{\\_}{\\approx}e^{14})$ iterations. The log-suboptimality gaps for the $4\\times10$ cases are shown on Figures 1a-1d, where they are plotted against the logarithm of time. Additional results for $K=2$ arms are shown in Appendix C. Note that in this example small sub-optimality implies that the optimal arm is chosen with high probability. In what follows, we discuss the results in the plots. ", "page_idx": 7}, {"type": "text", "text": "Asymptotic convergence. For the smaller learning rates of $\\eta=1$ and 10, all 10 seeds rapidly and steadily converge, reaching a sub-optimality of $e^{-14}$ or less. For $\\eta=100$ and 1000, most of the runs reach even small error even faster, but some runs are \u201cstuck\u201d even after $10^{6}$ steps. Note that this does not contradict the theoretical result; nor do we suspect numerical issues. As seen for the case of $\\eta=100$ , even after a long phase with little to no progress, a run can \u201crecover\u201d (see the grey curve). In fact, it is reasonable to expect that the price of increasing the learning rate is larger variance; as seen in these plots (subplots (a) and (b) are also attesting to this). Differences between learning rates are further discussed below. ", "page_idx": 7}, {"type": "text", "text": "Non-monotone objective value. Using a very small learning rate guarantees monotonic improvement (in expectation) in the policy\u2019s expected reward [24]. Conversely, a large learning rate results in non-monotonic evolution of the expected rewards $\\{\\pi_{\\theta_{t}}^{\\top}r\\}_{t\\geq1}$ , even in the final stages of convergence, as can be seen clearly for the learning rates of $\\eta=1$ and 10 in Figures 1a and 1b. For larger $\\eta$ , the non-monotone behavior happens over longer periods and is less visible in the plots. This is because using large learning rates causes the policy to rapidly increase the parameters for some action, after which the gradient becomes small, limiting further progress. ", "page_idx": 7}, {"type": "text", "text": "Rate of convergence. This work establishes almost sure convergence to the globally optimal policy as $t\\to\\infty$ without giving any particular rate of convergence. Figures 1a and 1b, where the log-log plot has a slope of nearly $-1$ , give some evidence that an $\\bar{O}(1/t)$ asymptotic rate is achieved. In general, such a rate cannot be improved in terms of $t$ [14]. However, more work is needed to rigorously quantify the asymptotic convergence rate. ", "page_idx": 7}, {"type": "text", "text": "Different learning rates. Two observations can be made from Figure 1 regarding the effect of using different $\\eta$ values: First, during the final stage of convergence when $r(\\boldsymbol{a}^{*})-\\pi_{\\boldsymbol{\\theta}_{t}}^{\\daleth}r\\approx0$ , using larger $\\eta$ results in faster convergence on average. As $\\eta$ increases, the order of $\\log\\left(\\dot{r}(a^{*})-\\pi_{\\theta_{t}}^{\\top}r\\right)$ also changes from $e^{-14}$ $(\\eta=1)$ , to $e^{-20}$ $\\eta=100)$ , and $e^{-200}$ $\\mathit{\\Pi}_{\\mathcal{V}}=1000)$ . We conjecture that the asymptotic rate of convergence has an ${\\cal O}(1/\\eta)$ dependence. Second, using larger learning rates can take a longer time to enter the final stage of convergence. When $\\eta=1$ or 10, all curves quickly enter the final stage of $r(\\boldsymbol{a}^{*})-\\pi_{\\boldsymbol{\\theta}_{t}}^{\\top}r\\approx0$ . However, for larger $\\eta$ values, $1/10$ runs $\\mathit{\\Pi}_{\\mathcal{\\Pi}}(\\mathit{\\Pi}_{\\mathcal{I}}=100)$ and $3/\\dot{1}0$ runs $(\\eta=1000)$ result in $r(a^{*})-\\pi_{\\theta_{t}}^{\\top}r$ values far from 0 even after $10^{6}$ iterations. These runs take orders of magnitude more iterations to eventually achieve $r(a^{*})-\\pi_{\\theta_{t}}^{\\top}r\\approx0$ . These situations correspond to the policy $\\pi_{\\theta_{t}}$ getting stuck near sub-optimal corners of the simplex, meaning that $\\pi_{\\theta_{t}}(i)\\approx1$ for a sub-optimal action $i\\in[K]$ with $r(i)<r(a^{*})$ . In such cases, even Softmax PG with the true gradient can remain stuck on a sub-optimal plateau for an extremely long time [23]. However, the reason why larger learning rates lead to longer plateaus in the stochastic setting remains unclear. ", "page_idx": 7}, {"type": "image", "img_path": "q9dKv1AK6l/tmp/943de237bb61d484e3ee9cbf73eaa7eb23e322fcfaab323bee6bb205ef8206c5.jpg", "img_caption": ["Figure 1: Log sub-optimality gap, $\\log\\left(r(a^{*})-\\pi_{\\theta_{t}}^{\\mathsf{T}}r\\right)$ , plotted against the logarithm of time, $\\log t$ , in a 4-action problem with various learning rates, $\\eta$ . Each subplot shows a run with a specific learning rate. The curves in a subplot correspond to 10 different random seeds. Theory predicts that essentially all seeds will lead to a curve converging to zero ( $-\\infty$ in these plots). For a discussion of the results, see the text., "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Trade-offs and multi-stage chracterizations of convergence. Given the above observations, there appears to exist a trade-off for $\\eta$ : larger $\\eta$ values result in faster convergence during the final stage where $r(\\boldsymbol{a}^{*})-\\pi_{\\boldsymbol{\\theta}_{t}}^{\\top}r\\approx0$ , but at the the cost of taking far longer to enter this final stage of convergence. Since asymptotic convergence results are insufficient for explaining these subtleties in a satisfactory manner, a more refined analysis that considers the different stages of convergence is required. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusions and Future Directions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This work refines our understanding of stochastic gradient bandit algorithms by proving that it converges to a globally optimal policy almost surely with any constant learning rate. Our new proof strategy based on the asymptotics of sample counts opens new directions for better characterizing exploration effects of stochastic gradient methods, while also suggesting interesting new questions. Immediate next steps are to characterize the asymptotic rate of convergence under large constant learning rates. Characterizing the multiple stages of convergence remains another interesting future direction. One interesting possibility is that there might exist an optimal time-dependent scheme for increasing the learning rate (such as $\\eta\\in O(\\log t))$ to accelerate convergence, rather than use a constant $\\eta\\in O(1)$ . This is corroborated by our experiments: As seen in Figure 1, small learning rates perform better during the early stages of optimization, while larger learning rates achieve faster convergence during the final stage. Other directions include extending our bandit results to the more general RL setting [34], as well as extending our results for the softmax tabular parameterization to handle function approximation [1]. ", "page_idx": 8}, {"type": "text", "text": "Limitations: While this work establishes a surprising asymptotic convergence result for any constant learning rate, it does not shed light on the convergence rate, nor on the effect of different learning rates on the convergence. Moreover, our analysis is limited to multi-armed bandits, and does not immediately extend to the general RL setting. These aspects are the main limitations of this paper. ", "page_idx": 9}, {"type": "text", "text": "Broader impact: This is primarily theoretical work on a fundamental algorithm that is used broadly in RL applications. We expect these results to improve the research community\u2019s understanding of the basic stochastic gradient bandit method. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Jincheng Mei would like to thank Ramki Gummadi for providing feedback on a draft of this manuscript. Csaba Szepesv\u00e1ri and Dale Schuurmans gratefully acknowledge funding from the Canada CIFAR AI Chairs Program, Amii and NSERC. Sharan Vaswani acknowledges the support from the NSERC Discovery Grant (RGPIN-2022-04816). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research, 22(98):1\u201376, 2021.   \n[2] Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit problem. In Conference on learning theory, pages 39\u20131. JMLR Workshop and Conference Proceedings, 2012.   \n[3] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2):235\u2013256, 2002.   \n[4] L\u00e9on Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT\u20192010: 19th International Conference on Computational StatisticsParis France, August 22-27, 2010 Keynote, Invited and Contributed Papers, pages 177\u2013186. Springer, 2010.   \n[5] Leo Breiman. Probability. SIAM, 1992.   \n[6] Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi. Fast global convergence of natural policy gradient methods with entropy regularization. Operations Research, 70(4):2563\u20132578, 2022.   \n[7] Nicol\u00f2 Cesa-bianchi and Claudio Gentile. Improved risk tail bounds for on-line algorithms. In Y. Weiss, B. Sch\u00f6lkopf, and J. Platt, editors, Advances in Neural Information Processing Systems, volume 18. MIT Press, 2005.   \n[8] Denis Denisov and Neil Walton. Regret analysis of a markov policy gradient algorithm for multi-arm bandits. arXiv preprint arXiv:2007.10229, 2020.   \n[9] Yuhao Ding, Junzi Zhang, and Javad Lavaei. Beyond exact gradients: Convergence of stochastic soft-max policy gradient methods with entropy regularization. arXiv preprint arXiv:2110.10117, 2021.   \n[10] David A. Freedman. On Tail Probabilities for Martingales. The Annals of Probability, 3(1):100 \u2013 118, 1975.   \n[11] Shivam Garg, Samuele Tosatto, Yangchen Pan, Martha White, and A Rupam Mahmood. An alternate policy gradient estimator for softmax policies. arXiv preprint arXiv:2112.11622, 2021.   \n[12] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013.   \n[13] Sham M Kakade. A natural policy gradient. In Advances in neural information processing systems, pages 1531\u20131538, 2002.   \n[14] Tze Leung Lai, Herbert Robbins, et al. Asymptotically efficient adaptive allocation rules. Advances in applied mathematics, 6(1):4\u201322, 1985.   \n[15] Guanghui Lan. Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes. Mathematical programming, 198(1):1059\u20131106, 2023.   \n[16] Tor Lattimore and Csaba Szepesv\u00e1ri. Bandit algorithms. Cambridge University Press, 2020.   \n[17] Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Softmax policy gradient methods can take exponential time to converge. In Conference on Learning Theory, pages 3107\u20133110. PMLR, 2021.   \n[18] Michael Lu, Matin Aghaei, Anant Raj, and Sharan Vaswani. Towards principled, practical policy gradient for bandits and tabular mdps. arXiv preprint arXiv:2405.13136, 2024.   \n[19] Jincheng Mei, Wesley Chung, Valentin Thomas, Bo Dai, Csaba Szepesvari, and Dale Schuurmans. The role of baselines in policy gradient optimization. Advances in Neural Information Processing Systems, 35:17818\u201317830, 2022.   \n[20] Jincheng Mei, Bo Dai, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. Understanding the effect of stochasticity in policy optimization. Advances in Neural Information Processing Systems, 34:19339\u201319351, 2021.   \n[21] Jincheng Mei, Yue Gao, Bo Dai, Csaba Szepesvari, and Dale Schuurmans. Leveraging non-uniformity in first-order non-convex optimization. In International Conference on Machine Learning, pages 7555\u20137564. PMLR, 2021.   \n[22] Jincheng Mei, Chenjun Xiao, Bo Dai, Lihong Li, Csaba Szepesv\u00e1ri, and Dale Schuurmans. Escaping the gravitational pull of softmax. Advances in Neural Information Processing Systems, 33:21130\u201321140, 2020.   \n[23] Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence rates of softmax policy gradient methods. In International Conference on Machine Learning, pages 6820\u20136829. PMLR, 2020.   \n[24] Jincheng Mei, Zixin Zhong, Bo Dai, Alekh Agarwal, Csaba Szepesvari, and Dale Schuurmans. Stochastic gradient succeeds for bandits. arXiv preprint arXiv:2402.17235, 2024.   \n[25] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on optimization, 19(4):1574\u20131609, 2009.   \n[26] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.   \n[27] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.   \n[28] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400\u2013407, 1951.   \n[29] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pages 1889\u20131897, 2015.   \n[30] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[31] Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT Press, 2018.   \n[32] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999.   \n[33] William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285\u2013294, 1933.   \n[34] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229\u2013256, 1992.   \n[35] Jingfeng Wu, Peter L Bartlett, Matus Telgarsky, and Bin Yu. Large stepsize gradient descent for logistic loss: Non-monotonicity of the loss improves optimization efficiency. arXiv preprint arXiv:2402.15926, 2024.   \n[36] Rui Yuan, Robert M Gower, and Alessandro Lazaric. A general sample complexity analysis of vanilla policy gradient. In International Conference on Artificial Intelligence and Statistics, pages 3332\u20133380. PMLR, 2022.   \n[37] Junyu Zhang, Chengzhuo Ni, Csaba Szepesvari, Mengdi Wang, et al. On the convergence and sample efficiency of variance-reduced policy gradient method. Advances in Neural Information Processing Systems, 34:2228\u20132240, 2021.   \n[38] Junzi Zhang, Jongho Kim, Brendan O\u2019Donoghue, and Stephen Boyd. Sample efficient reinforcement learning with reinforce. arXiv preprint arXiv:2010.11364, 2020.   \n[39] Kaiqing Zhang, Alec Koppel, Hao Zhu, and Tamer Basar. Global convergence of policy gradient methods to (almost) locally optimal policies. SIAM Journal on Control and Optimization, 58(6):3586\u20133612, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Lemma 1. Using Algorithm 1 with any constant $\\eta\\in\\Theta(1)$ , if $N_{\\infty}(a)<\\infty$ for an action $a\\in[K]$ , then we have, almost surely, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq1}\\theta_{t}(a)<\\infty,\\;\\mathrm{and}\\;\\operatorname*{inf}_{t\\geq1}\\theta_{t}(a)>-\\infty.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. Suppose $N_{\\infty}(a)<\\infty$ for an action $a\\in[K]$ . According to Algorithm 1, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\theta_{t+1}(a)\\gets\\theta_{t}(a)+\\left\\{\\eta\\cdot(1-\\pi_{\\theta_{t}}(a))\\cdot R_{t}(a),\\right.\\mathrm{~if~}a_{t}=a\\,,}\\\\ {\\left.-\\eta\\cdot\\pi_{\\theta_{t}}(a)\\cdot R_{t}(a_{t}),\\right.\\qquad\\mathrm{otherwise}\\,,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and let ", "page_idx": 12}, {"type": "equation", "text": "$$\nI_{t}(a):={\\binom{1,\\quad\\mathrm{if}\\ a_{t}=a\\,,}{0,\\quad\\mathrm{otherwise}\\,.}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "According to Eq. (22), we have, for all $t\\geq1$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\theta_{t}(a)-\\theta_{1}(a)=\\sum_{s=1}^{t-1}I_{s}(a)\\cdot\\eta\\cdot(1-\\pi_{\\theta_{s}}(a))\\cdot R_{s}(a)+\\sum_{s=1}^{t-1}(1-I_{s}(a))\\cdot(-\\eta)\\cdot\\pi_{\\theta_{s}}(a)\\cdot R_{s}(a_{s}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Using triangle inequality, we have, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\theta_{t}(a)-\\theta_{1}(a)|\\leq\\sum_{s=1}^{t-1}\\Big|I_{s}(a)\\cdot\\eta\\cdot(1-\\pi_{\\theta_{s}}(a))\\cdot R_{s}(a)\\Big|+\\sum_{s=1}^{t-1}\\Big|\\left(1-I_{s}(a)\\right)\\cdot(-\\eta)\\cdot\\pi_{\\theta_{s}}(a)\\cdot R_{s}(a_{s})\\Big|}}\\\\ &{}&{\\quad\\quad(25)}\\\\ &{}&{\\leq\\eta\\cdot R_{\\operatorname*{max}}\\cdot\\displaystyle\\sum_{s=1}^{t-1}I_{s}(a)+\\eta\\cdot R_{\\operatorname*{max}}\\cdot\\displaystyle\\sum_{s=1}^{t-1}\\pi_{\\theta_{s}}(a)}\\\\ &{}&{\\quad\\quad=\\eta\\cdot R_{\\operatorname*{max}}\\cdot\\Big(N_{t-1}(a)+\\displaystyle\\sum_{s=1}^{t-1}\\pi_{\\theta_{s}}(a)\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "By assumption, we have, ", "page_idx": 12}, {"type": "equation", "text": "$$\nN_{\\infty}(a):=\\operatorname*{lim}_{t\\to\\infty}N_{t}(a)<\\infty.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "According to the extended Borel-Cantelli Lemma 3, we have, almost surely, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{\\infty}\\pi_{\\theta_{t}}(a):=\\operatorname*{lim}_{t\\to\\infty}\\sum_{s=1}^{t}\\pi_{\\theta_{s}}(a)<\\infty.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Combining Eqs. (25), (28) and (29), we have, almost surely, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq1}|\\theta_{t}(a)-\\theta_{1}(a)|<\\infty,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "which implies that, almost surely, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq1}|\\theta_{t}(a)|\\leq\\operatorname*{sup}_{t\\geq1}|\\theta_{t}(a)-\\theta_{1}(a)|+|\\theta_{1}(a)|<\\infty.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Lemma 2 (Avoiding lack of exploration). Using Algorithm 1 with any $\\eta\\in\\Theta(1)$ , there exists at least a pair of distinct actions $i,j\\in[K]$ and $i\\neq j$ , such that, almost surely, ", "page_idx": 12}, {"type": "equation", "text": "$$\nN_{\\infty}(i)=\\infty,\\;\\mathrm{and}\\;N_{\\infty}(j)=\\infty.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. First, we have, for all $t\\geq1$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{t=\\displaystyle\\sum_{s=1}^{t}\\displaystyle\\sum_{a\\in[K]}I_{s}(a)}&&{\\Big(\\displaystyle\\sum_{a\\in[K]}I_{t}(a)=1\\mathrm{~for~all~}t\\ge1\\Big)}\\\\ &{~~=\\displaystyle\\sum_{a\\in[K]}\\displaystyle\\sum_{s=1}^{t}I_{s}(a)}\\\\ &{~~=\\displaystyle\\sum_{a\\in[K]}N_{t}(a).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By pigeonhole principle, there exists at least one action $i\\in[K]$ , such that, almost surely, ", "page_idx": 13}, {"type": "equation", "text": "$$\nN_{\\infty}(i):=\\operatorname*{lim}_{t\\to\\infty}N_{t}(i)=\\infty.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We argue the existence of another action by contradiction. Suppose for all the other actions $j\\in[K]$ and $j\\neq i$ , we have $N_{\\infty}(j)<\\infty$ . According to the extended Borel-Cantelli Lemma 3, we have, almost surely, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{\\infty}\\pi_{\\theta_{t}}(j):=\\operatorname*{lim}_{t\\to\\infty}\\sum_{s=1}^{t}\\pi_{\\theta_{s}}(j)<\\infty.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "According to the update Eq. (22), we have, for all $t\\geq1$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\theta_{t}(i)-\\theta_{1}(i)=\\sum_{s=1}^{t-1}I_{s}(i)\\cdot\\eta\\cdot(1-\\pi_{\\theta_{s}}(i))\\cdot R_{s}(i)+\\sum_{s=1}^{t-1}(1-I_{s}(i))\\cdot(-\\eta)\\cdot\\pi_{\\theta_{s}}(i)\\cdot R_{s}(a_{s}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By triangle inequality, we have, ", "page_idx": 13}, {"type": "equation", "text": "$$\n|\\theta_{t}(i)-\\theta_{1}(i)|\\leq\\sum_{s=1}^{t-1}\\Big|I_{s}(i)\\cdot\\eta\\cdot(1-\\pi_{\\theta_{s}}(i))\\cdot R_{s}(i)\\Big|+\\sum_{s=1}^{t-1}\\Big|\\left(1-I_{s}(i)\\right)\\cdot(-\\eta)\\cdot\\pi_{\\theta_{s}}(i)\\cdot R_{s}(a_{s})\\Big|\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\le\\eta\\cdot R_{\\operatorname*{max}}\\cdot\\sum_{s=1}^{t-1}(1-\\pi_{\\theta_{s}}(i))+\\eta\\cdot R_{\\operatorname*{max}}\\cdot\\sum_{s=1}^{t-1}(1-I_{s}(i))}}\\\\ &{=\\eta\\cdot R_{\\operatorname*{max}}\\cdot\\biggl(\\displaystyle\\sum_{s=1}^{t-1}\\sum_{j\\neq i}\\pi_{\\theta_{s}}(j)+\\displaystyle\\sum_{s=1}^{t-1}\\sum_{j\\neq i}I_{s}(j)\\biggr)}\\\\ &{=\\eta\\cdot R_{\\operatorname*{max}}\\cdot\\biggl(\\displaystyle\\sum_{j\\neq i}\\sum_{s=1}^{t-1}\\pi_{\\theta_{s}}(j)+\\displaystyle\\sum_{j\\neq i}N_{t-1}(j)\\biggr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Combing Eqs. (36) and (38) and the assumption of $N_{\\infty}(j)<\\infty$ for all $j\\neq i$ , we have, almost surely, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq1}|\\theta_{t}(i)|\\leq\\operatorname*{sup}_{t\\geq1}|\\theta_{t}(i)-\\theta_{1}(i)|+|\\theta_{1}(i)|<\\infty.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since $N_{\\infty}(j)<\\infty$ for all $j\\neq i$ by assumption, and according to Lemma 1, we have, almost surely, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq1}\\left|\\theta_{t}(j)\\right|<\\infty.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Combining Eqs. (42) and (43), we have, for all action $a\\in[K]$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq1}|\\theta_{t}(a)|<\\infty,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which implies that, there exists $c>0$ and $c\\in O(1)$ , such that, for all $a\\in[K]$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{t\\geq1}\\pi_{\\theta_{t}}(a)=\\operatorname*{inf}_{t\\geq1}\\frac{\\exp\\{\\theta_{t}(a)\\}}{\\sum_{a^{\\prime}\\in[K]}\\exp\\{\\theta_{t}(a^{\\prime})\\}}\\geq c>0.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore, for all action $a\\in[K]$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{\\infty}\\pi_{\\theta_{t}}(a):=\\operatorname*{lim}_{t\\rightarrow\\infty}\\sum_{s=1}^{t}\\pi_{\\theta_{s}}(a)}\\\\ &{\\quad\\quad\\quad\\quad\\geq\\displaystyle\\operatorname*{lim}_{t\\rightarrow\\infty}\\sum_{s=1}^{t}c}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\operatorname*{lim}_{t\\rightarrow\\infty}t\\cdot c}\\\\ &{\\quad=\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "According to the extended Borel-Cantelli Lemma 3, we have, almost surely, for all action $a\\in[K]$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\nN_{\\infty}(a)=\\infty,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is a contradiction with the assumption of $N_{\\infty}(j)<\\infty$ for all the other actions $j\\in[K]$ with $j\\neq i$ . Therefore, there exists another action $j\\in[K]$ with $j\\neq i$ , such that $N_{\\infty}(j)=\\dot{\\infty}$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Theorem 1. Let $K=2$ and $r(1)>r(2)$ . Using Algorithm 1 with any $\\eta\\in\\Theta(1)$ , we have, almost surely, $\\pi_{\\theta_{t}}(a^{*})\\to1$ as $t\\to\\infty$ , where $a^{*}:=\\arg\\operatorname*{max}_{a\\in[K]}r(a)$ (equal to Action 1 in this case). ", "page_idx": 14}, {"type": "text", "text": "Proof. The proof uses the same notations as of [24, Theorem 5.1]. ", "page_idx": 14}, {"type": "text", "text": "According to Lemma 2, we have, $N_{\\infty}(1)=\\infty$ and $N_{\\infty}(2)=\\infty$ , i.e., both of the two actions are sampled for infinitely many times as $t\\to\\infty$ . ", "page_idx": 14}, {"type": "text", "text": "Let $\\mathcal{F}_{t}$ be the $\\sigma$ -algebra generated by $a_{1},R_{1}(a_{1}),\\cdots,a_{t-1},R_{t-1}(a_{t-1})\\colon$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{F}_{t}=\\sigma(\\{a_{1},R_{1}(a_{1}),\\cdot\\cdot\\cdot\\cdot,a_{t-1},R_{t-1}(a_{t-1})\\})\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that $\\theta_{t}$ and $I_{t}$ (defined by Eq. (23)) are $\\mathcal{F}_{t}$ -measurable for all $t\\geq1$ . Let $\\mathbb{E}_{t}[\\cdot]$ denote the conditional expectation with respect to $\\mathcal{F}_{t}\\colon\\mathbb{E}_{t}[X]=\\mathbb{E}[X|\\mathcal{F}_{t}]$ . Define the following notations, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{W_{t}(a):=\\theta_{t}(a)-\\mathbb{E}_{t-1}[\\theta_{t}(a)],}&&{\\mathrm{~(\\'*{noise}')~}}\\\\ &{P_{t}(a):=\\mathbb{E}_{t}[\\theta_{t+1}(a)]-\\theta_{t}(a).}&&{\\mathrm{~(\\'*progress\")~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For each action $a\\in[K]$ , for $t\\geq2$ , we have the following decomposition, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\theta_{t}(a)=W_{t}(a)+P_{t-1}(a)+\\theta_{t-1}(a).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By recursion we can determine that, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\theta_{t}(a)=\\mathbb{E}[\\theta_{1}(a)]+\\sum_{s=1}^{t}W_{s}(a)+\\sum_{s=1}^{t-1}P_{s}(a),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "while we also have, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\theta_{1}(a)=\\underbrace{\\theta_{1}(a)-\\mathbb{E}[\\theta_{1}(a)]}_{W_{1}(a)}+\\mathbb{E}[\\theta_{1}(a)],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and $\\mathbb{E}[\\theta_{1}(a)]$ accounts for potential randomness in initializing $\\boldsymbol{\\theta}_{1}\\in\\mathbb{R}^{K}$ . According to Proposition 1, we have, ", "page_idx": 14}, {"type": "equation", "text": "$$\nP_{t}(a)=\\mathbb{E}_{t}[\\theta_{t+1}(a)]-\\theta_{t}(a)=\\eta\\cdot\\pi_{\\theta_{t}}(a)\\cdot\\left(r(a)-\\pi_{\\theta_{t}}^{\\top}r\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which implies that, for all $t\\geq1$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\nP_{t}(a^{*})>0>P_{t}(2).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Next, we show that $\\begin{array}{r}{\\sum_{s=1}^{t}P_{s}(a^{*})\\to\\infty}\\end{array}$ as $t\\to\\infty$ by contradiction. Suppose that, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{\\infty}P_{t}(a^{*})<\\infty.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For the optimal action $a^{*}=1$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{t}P_{s}(a^{*})=\\sum_{s=1}^{t}\\eta\\cdot\\pi_{\\theta_{s}}(a^{*})\\cdot\\big(r(a^{*})-\\pi_{\\theta_{s}}^{\\top}r\\big)}}\\\\ &{}&{=\\eta\\cdot\\Delta\\cdot\\displaystyle\\sum_{s=1}^{t}\\pi_{\\theta_{s}}(a^{*})\\cdot\\big(1-\\pi_{\\theta_{s}}(a^{*})\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\Delta:=r(a^{*})-\\operatorname*{max}_{a\\neq a^{*}}r(a)=r(1)-r(2)>0$ is the reward gap. Denote that, for all $t\\geq1$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nV_{t}(a^{*}):=\\frac{5}{18}\\cdot\\sum_{s=1}^{t-1}\\pi_{\\theta_{s}}(a^{*})\\cdot(1-\\pi_{\\theta_{s}}(a^{*})).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "According to Lemma 8 (using Eqs. (58) to (60) and (62)), we have, almost surely, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq1}\\left|\\theta_{t}(a^{*})\\right|<\\infty.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For the sub-optimal action (Action 2), we have, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=1}^{t}P_{s}(2)=\\sum_{s=1}^{t}\\eta\\cdot\\pi_{\\theta_{s}}(2)\\cdot(r(2)-\\pi_{\\theta_{s}}^{\\top}r)}\\\\ &{\\quad\\quad\\quad\\quad=-\\eta\\cdot\\Delta\\cdot\\displaystyle\\sum_{s=1}^{t}\\pi_{\\theta_{s}}(2)\\cdot(1-\\pi_{\\theta_{s}}(2))}\\\\ &{\\quad\\quad\\quad\\quad=-\\displaystyle\\sum_{s=1}^{t}P_{s}(a^{*}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and also denote, for all $t\\geq1$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nV_{t}(2):=\\frac{5}{18}\\cdot\\sum_{s=1}^{t-1}\\pi_{\\theta_{s}}(2)\\cdot(1-\\pi_{\\theta_{s}}(2))=V_{t}(a^{*}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "According to Lemma 8 (using Eqs. (58), (59), (64) and (67)), we have, almost surely, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq1}\\left|\\theta_{t}(2)\\right|<\\infty.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining Eqs. (63) and (68), there exists $c>0$ and $c\\in O(1)$ , such that, for all $a\\in\\{a^{*},2\\}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{t\\geq1}\\pi_{\\theta_{t}}(a)=\\operatorname*{inf}_{t\\geq1}\\frac{\\exp\\{\\theta_{t}(a)\\}}{\\sum_{a^{\\prime}\\in[K]}\\exp\\{\\theta_{t}(a^{\\prime})\\}}\\geq c>0,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which implies that, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{\\infty}\\pi_{\\theta_{t}}(a^{*})\\cdot(1-\\pi_{\\theta_{t}}(a^{*})):=\\operatorname*{lim}_{t\\to\\infty}\\sum_{s=1}^{t}\\pi_{\\theta_{s}}(a^{*})\\cdot(1-\\pi_{\\theta_{s}}(a^{*}))}\\\\ &{\\ge\\displaystyle\\operatorname*{lim}_{t\\to\\infty}\\sum_{s=1}^{t}c\\cdot c}\\\\ &{=\\displaystyle\\operatorname*{lim}_{t\\to\\infty}t\\cdot c^{2}}\\\\ &{=\\infty,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which is a contradiction with the assumption of Eq. (59). Therefore, we have, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}P_{s}(a^{*})\\to\\infty,\\;\\mathrm{as}\\;t\\to\\infty.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "According to Lemma 9 (using Eqs. (58), (60), (62) and (74)), we have, almost surely, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\theta_{t}(a^{*})\\to\\infty,\\;\\mathrm{as}\\;t\\to\\infty.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Similarly, according to Lemma 10 (using Eqs. (58), (64), (67) and (74)), we have, almost surely, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\rightarrow\\infty}\\theta_{t}(2)=-\\infty.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi_{\\theta_{t}}(a^{*})=\\frac{\\pi_{\\theta_{t}}(a^{*})}{\\pi_{\\theta_{t}}(2)+\\pi_{\\theta_{t}}(a^{*})}}\\\\ &{\\qquad\\qquad=\\frac{1}{\\exp\\{\\theta_{t}(2)-\\theta_{t}(a^{*})\\}+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining Eqs. (75) to (77), we have, almost surely, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pi_{\\theta_{t}}(a^{*})\\to1,\\;\\mathrm{as}\\;t\\to\\infty.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Theorem 2. Under Assumption 1, given $K\\ge2$ , using Algorithm 1 with any $\\eta\\in\\Theta(1)$ , we have, almost surely, $\\pi_{\\theta_{t}}(a^{*})\\to1$ as $t\\to\\infty$ , where $a^{*}=\\arg\\operatorname*{max}_{a\\in[K]}r(a)$ is the optimal action. ", "page_idx": 16}, {"type": "text", "text": "Proof. Similar to the proof of Theorem 1, we define $\\mathcal{F}_{t}$ to be the $\\sigma$ -algebra generated by $a_{1}$ , $R_{1}(a_{1})$ , \u00b7 \u00b7 \u00b7 , $a_{t-1}$ , $R_{t-1}{\\left(a_{t-1}\\right)}$ i.e. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{F}_{t}=\\sigma(\\{a_{1},R_{1}(a_{1}),\\cdot\\cdot\\cdot\\cdot,a_{t-1},R_{t-1}(a_{t-1})\\})\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that $\\theta_{t}$ and $I_{t}$ (defined by Eq. (23)) are $\\mathcal{F}_{t}$ -measurable for all $t\\geq1$ . Let $\\mathbb{E}_{t}[\\cdot]$ denote the conditional expectation with respect to $\\mathcal{F}_{t}\\colon\\mathbb{E}_{t}[X]=\\mathbb{E}[X|\\mathcal{F}_{t}]$ . Define the following notations, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{W_{t}(a):=\\theta_{t}(a)-\\mathbb{E}_{t-1}[\\theta_{t}(a)],}&&{\\mathrm{~(\\'*{noise}')~}}\\\\ &{P_{t}(a):=\\mathbb{E}_{t}[\\theta_{t+1}(a)]-\\theta_{t}(a).}&&{\\mathrm{~(\\'*progress\")~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For each action $a\\in[K]$ , for $t\\geq2$ , we have the following decomposition, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\theta_{t}(a)=W_{t}(a)+P_{t-1}(a)+\\theta_{t-1}(a).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By recursion we can determine that, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\theta_{t}(a)=\\mathbb{E}[\\theta_{1}(a)]+\\sum_{s=1}^{t}W_{s}(a)+\\sum_{s=1}^{t-1}P_{s}(a),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "while we also have, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\theta_{1}(a)=\\underbrace{\\theta_{1}(a)-\\mathbb{E}[\\theta_{1}(a)]}_{W_{1}(a)}+\\mathbb{E}[\\theta_{1}(a)],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and $\\mathbb{E}[\\theta_{1}(a)]$ accounts for potential randomness in initializing $\\boldsymbol{\\theta}_{1}\\in\\mathbb{R}^{K}$ . For an action $a\\in[K]$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A^{+}(a):=\\left\\{a^{+}\\in[K]:r(a^{+})>r(a)\\right\\},}\\\\ {\\ A^{-}(a):=\\left\\{a^{-}\\in[K]:r(a^{-})<r(a)\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Define $A_{\\infty}$ as the set of actions which are sampled for infinitely many times as $t\\to\\infty$ , i.e., ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{\\infty}:=\\left\\{a\\in[K]\\mid N_{\\infty}(a)=\\infty\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "First part. We show that $N_{\\infty}(a^{*})=\\infty$ by contradiction. ", "page_idx": 16}, {"type": "text", "text": "Suppose $a^{\\ast}\\notin A_{\\infty}$ i.e. $N_{\\infty}(a^{*})<\\infty$ . According to Lemma 2, we have, $|\\mathcal{A}_{\\infty}|\\geq2$ . Since $a^{\\ast}\\notin A_{\\infty}$ by assumption, there must be at least two other sub-optimal actions $i_{1},i_{2}\\in[K]$ , $i_{1}\\neq i_{2}$ , such that, ", "page_idx": 16}, {"type": "equation", "text": "$$\nN_{\\infty}(i_{1})=N_{\\infty}(i_{2})=\\infty.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In particular, define ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{i_{1}:=\\underset{a\\in[K],}{\\arg\\operatorname*{min}}~r(a),}\\\\ &{\\qquad\\quad\\overset{N\\,\\ldots\\,(a)=\\infty}{N\\,\\infty}}\\\\ &{i_{2}:=\\underset{a\\in[K],}{\\arg\\operatorname*{max}}~r(a).}\\\\ &{\\qquad\\quad\\quad\\,N_{\\infty}(a)\\!=\\!\\infty}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using the update for arm $i_{1}$ , we know that, for all $s\\geq1$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\nP_{s}(i_{1})=\\eta\\cdot\\pi_{\\theta_{s}}(i_{1})\\cdot(r(i_{1})-\\pi_{\\theta_{s}}^{\\top}r)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By definition and because of Assumption 1, we have that $r(i_{1})\\,<\\,r(i_{2})\\,<\\,r(a^{*})$ . Furthermore, according to Lemma 6, we have, for sufficiently large $\\tau\\geq1$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{r(i_{1})<\\pi_{\\theta_{t}}^{\\top}r<r(i_{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which implies that after some large enough $\\tau\\geq1$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=\\tau}^{t}P_{s}(i_{1})=\\sum_{s=\\tau}^{t}\\eta\\cdot\\pi_{\\theta_{s}}(i_{1})\\cdot\\big(r(i_{1})-\\pi_{\\theta_{s}}^{\\top}r\\big)}\\\\ &{\\quad\\quad\\quad\\quad\\quad<0.\\quad\\quad\\mathrm{(by~Eq.~(92))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle r(i_{1})-\\pi_{\\theta_{s}}^{\\top}r=\\sum_{a\\neq i_{1}}\\pi_{\\theta_{s}}(a)\\left[r(i_{1})-r(a)\\right]}}\\\\ {{\\displaystyle\\qquad\\qquad=-\\sum_{a^{+}\\in A^{+}(i_{1})}\\pi_{\\theta_{s}}(a^{+})\\cdot(r(a^{+})-r(i_{1}))+\\sum_{a^{-}\\in A^{-}(i_{1})}\\pi_{\\theta_{s}}(a^{-})\\cdot(r(i_{1})-r(a^{-})).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "From Eq. (89), we have, ", "page_idx": 17}, {"type": "equation", "text": "$$\nA_{\\infty}\\subseteq{\\mathcal{A}}^{+}(i_{1})\\cup\\{i_{1}\\},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which implies that, for all $a^{-}\\in\\mathcal{A}^{-}(i_{1})$ , we have, ", "page_idx": 17}, {"type": "equation", "text": "$$\nN_{\\infty}(a^{-})<\\infty.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that, by definition, $i_{2}\\in\\mathcal{A}^{+}(i_{1})$ , and ", "page_idx": 17}, {"type": "equation", "text": "$$\nN_{\\infty}(i_{2})=\\infty.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In order to bound Eq. (95) using the above relations, note that, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{a^{-}\\in\\mathcal{A}^{-}(i_{1})}\\frac{\\pi_{\\theta_{s}}(a^{-})}{\\sum_{a^{+}\\in\\mathcal{A}^{+}(i_{1})}\\pi_{\\theta_{s}}(a^{+})\\cdot\\big(r(a^{+})-r(i_{1})\\big)}\\cdot\\big(r(i_{1})-r(a^{-})\\big)}}\\\\ &{}&{<\\sum_{a^{-}\\in\\mathcal{A}^{-}(i_{1})}\\frac{\\pi_{\\theta_{s}}(a^{-})}{\\pi_{\\theta_{s}}(i_{2})\\cdot\\big(r(i_{2})-r(i_{1})\\big)}\\cdot\\big(r(i_{1})-r(a^{-})\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "(Fewer terms in the denominator) ", "page_idx": 17}, {"type": "text", "text": "By Lemma 5, for large enough $\\tau\\geq1$ , for all $a^{-}$ , $\\begin{array}{r}{\\frac{\\pi_{\\theta_{s}}(a^{-})}{\\pi_{\\theta_{s}}(i_{2})}\\leq\\frac{1}{|A^{-}(i_{1})|}}\\end{array}$ \u00b7 r(i2)\u2212r(i1). Hence, \u00b7 ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\displaystyle\\sum_{a^{-}\\in A^{-}(i_{1})}\\frac{1}{2}\\cdot\\frac{1}{|A^{-}(i_{1})|}\\cdot\\frac{r(i_{2})-r(i_{1})}{r(i_{1})-r(a^{-})}\\cdot\\frac{r(i_{1})-r(a^{-})}{r(i_{2})-r(i_{1})}}\\\\ &{=\\displaystyle\\frac{1}{2}}\\\\ &{\\Longrightarrow\\displaystyle\\sum_{a^{-}\\in A^{-}(i_{1})}\\pi_{\\theta_{s}}(a^{-})\\cdot(r(i_{1})-r(a^{-}))\\leq\\displaystyle\\frac{1}{2}\\sum_{a^{+}\\in A^{+}(i_{1})}\\pi_{\\theta_{s}}(a^{+})\\cdot(r(a^{+})-r(i_{1})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combining Eqs. (94), (95) and (102), we have, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=\\tau}^{t}P_{s}(i_{1})=\\sum_{s=\\tau}^{t}\\eta\\cdot\\pi_{\\theta_{s}}(i_{1})\\cdot(r(i_{1})-\\pi_{\\theta_{s}}^{\\top}r)}\\\\ &{\\qquad\\qquad\\le-\\displaystyle\\frac{\\eta}{2}\\cdot\\sum_{s=\\tau}^{t}\\pi_{\\theta_{s}}(i_{1})\\sum_{a^{+}\\in A^{+}(i_{1})}\\pi_{\\theta_{s}}(a^{+})\\cdot(r(a^{+})-r(i_{1}))}\\\\ &{\\qquad\\qquad\\le-\\displaystyle\\frac{\\eta\\cdot\\Delta}{2}\\cdot\\sum_{s=\\tau}^{t}\\pi_{\\theta_{s}}(i_{1})\\sum_{a^{+}\\in A^{+}(i_{1})}\\pi_{\\theta_{s}}(a^{+})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Delta:=\\operatorname*{min}_{\\substack{i,j\\in[K],\\,i\\neq j}}|r(i)-r(j)|>0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Bounding the variance for arm $i_{1}$ , we have that for all large enough $\\tau\\geq1$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{V_{t}(i_{1}):=\\displaystyle\\frac{5}{18}\\cdot\\sum_{s=\\tau}^{t-1}\\pi_{\\theta_{s}}(i_{1})\\cdot(1-\\pi_{\\theta_{s}}(i_{1}))}}\\\\ {{\\displaystyle\\qquad=\\frac{5}{18}\\cdot\\sum_{s=\\tau}^{t-1}\\pi_{\\theta_{s}}(i_{1})\\cdot\\left(\\sum_{a^{-}\\in A^{-}(i_{1})}\\pi_{\\theta_{s}}(a^{-})+\\sum_{a^{+}\\in A^{+}(i_{1})}\\pi_{\\theta_{s}}(a^{+})\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using similar calculations as for the progress term, we have, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{a^{-}\\in A^{-}(i_{1})}\\frac{\\pi_{\\theta_{s}}(a^{-})}{\\sum_{a^{+}\\in A^{+}(i_{1})}\\pi_{\\theta_{s}}(a^{+})}<\\sum_{a^{-}\\in A^{-}(i_{1})}\\frac{\\pi_{\\theta_{s}}(a^{-})}{\\pi_{\\theta_{s}}(i_{2})}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By Lemma 5, for large enough \u03c4 \u22651, for all a\u2212, \u03c0\u03c0\u03b8s((ai2)) \u2264 $\\begin{array}{r}{\\frac{\\pi_{\\theta_{s}}(a^{-})}{\\pi_{\\theta_{s}}(i_{2})}\\leq\\frac{13}{5}\\,\\frac{1}{|A^{-}(i_{1})|}}\\end{array}$ 153 |A\u22121(i1)|. Hence, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\leq\\displaystyle\\sum_{a^{-}\\in A^{-}(i_{1})}\\frac{13}{5}\\cdot\\frac{1}{|A^{-}(i_{1})|}}}\\\\ {{=\\displaystyle\\frac{13}{5}}}\\\\ {{\\Longrightarrow\\displaystyle\\sum_{a^{-}\\in A^{-}(i_{1})}\\pi_{\\theta_{s}}(a^{-})\\leq\\frac{13}{5}\\sum_{a^{+}\\in A^{+}(i_{1})}\\pi_{\\theta_{s}}(a^{+})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining Eqs. (108) and (111), we have that for large enough $\\tau\\geq1$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{t}(i_{1})=\\displaystyle\\frac{5}{18}\\cdot\\sum_{s=\\tau}^{t-1}\\pi_{\\theta_{s}}(i_{1})\\cdot(1-\\pi_{\\theta_{s}}(i_{1}))}\\\\ {\\le\\displaystyle\\sum_{s=\\tau}^{t-1}\\pi_{\\theta_{s}}(i_{1})\\sum_{a^{+}\\in A^{+}(i_{1})}\\pi_{\\theta_{s}}(a^{+}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "According to Lemma 12 (using Eqs. (94), (105) and (113)), we have, almost surely, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq1}\\theta_{t}(i_{1})<\\infty.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $N_{\\infty}(a^{*})<\\infty$ by assumption, and according to Lemma 1, we have, almost surely, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{t\\geq1}\\theta_{t}(a^{*})>-\\infty.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining Eqs. (114) and (115), we have, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq1}\\frac{\\pi_{\\theta_{t}}(i_{1})}{\\pi_{\\theta_{t}}(a^{*})}=\\operatorname*{sup}_{t\\geq1}\\exp\\{\\theta_{t}(i_{1})-\\theta_{t}(a^{*})\\}<\\infty\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "On the other hand, by Lemma 5, we have, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq1}\\frac{\\pi_{\\theta_{t}}(i_{1})}{\\pi_{\\theta_{t}}(a^{*})}=\\infty\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which contradicts Eq. (116). Hence, the assumption that $N_{\\infty}(a^{*})<\\infty$ cannot hold. This completes the proof by contradiction, and implies that $N_{\\infty}(a^{*})=\\infty$ . ", "page_idx": 18}, {"type": "text", "text": "Second part. With $a^{*}\\in A_{\\infty}$ i.e. $N_{\\infty}(a^{*})=\\infty$ , we now argue that $\\pi_{\\theta_{t}}(a^{*})\\to1$ as $t\\to\\infty$ almost surely. ", "page_idx": 19}, {"type": "text", "text": "According to Lemma 2, we have, $|\\mathcal{A}_{\\infty}|\\geq2$ . Since $a^{*}\\in A_{\\infty}$ by assumption, there must be at least one sub-optimal action $i_{1}\\in[K]$ with $r(i_{1})<r(a^{*})$ , such that, ", "page_idx": 19}, {"type": "equation", "text": "$$\nN_{\\infty}(i_{1})=\\infty.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In particular, define ", "page_idx": 19}, {"type": "equation", "text": "$$\ni_{1}:=\\underset{a\\in[K],}{\\arg\\operatorname*{min}}\\ r(a).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "According to Lemma 6, we have, for all sufficiently large $\\tau\\geq1$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\nr(i_{1})<\\pi_{\\theta_{t}}^{\\top}r<r(a^{*}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using the same arguments as in the first part (except that $i_{2}$ in Eq. (98) is replaced with $a^{*}$ ), both Eqs. (105) and (113) hold. ", "page_idx": 19}, {"type": "text", "text": "Next, we argue that $\\begin{array}{r}{\\sum_{s=\\tau}^{t}\\pi_{\\theta_{s}}\\bigl(i_{1}\\bigr)\\sum_{a^{+}\\in A^{+}(i_{1})}\\pi_{\\theta_{s}}(a^{+})\\to\\infty}\\end{array}$ as $t\\to\\infty$ by contradiction. ", "page_idx": 19}, {"type": "text", "text": "Suppose ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{t=\\tau}^{\\infty}\\pi_{\\theta_{t}}(i_{1})\\sum_{a^{+}\\in\\mathcal{A}^{+}(i_{1})}\\pi_{\\theta_{t}}(a^{+})<\\infty.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "According to Lemma 8 (using Eqs. (94), (105), (113) and (121)), we have, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq1}\\left|\\theta_{t}(i_{1})\\right|<\\infty.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Calculating the progress and variance for arm $a^{*}$ , for $t\\geq1$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{P_{t}(a^{*})=\\eta\\cdot\\pi_{\\theta_{t}}(a^{*})\\cdot\\big(r(a^{*})-\\pi_{\\theta_{t}}^{\\top}r\\big)}\\\\ &{\\qquad\\qquad\\geq\\eta\\cdot\\Delta\\cdot\\pi_{\\theta_{t}}(a^{*})\\cdot\\big(1-\\pi_{\\theta_{t}}(a^{*})\\big).}&&{\\big(\\Delta:=r(a^{*})-\\displaystyle\\operatorname*{max}_{a\\not=a^{*}}r(a)\\big)}\\\\ &{\\qquad\\qquad\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Denote that, for all $t\\geq1$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\nV_{t}(a^{*}):=\\frac{5}{18}\\cdot\\sum_{s=1}^{t-1}\\pi_{\\theta_{s}}(a^{*})\\cdot(1-\\pi_{\\theta_{s}}(a^{*})).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "According to Lemma 11 (using Eqs. (123) and (126)), we have, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{t\\geq1}\\theta_{t}(a^{*})>-\\infty.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining Eqs. (122) and (127), we have, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq1}\\frac{\\pi_{\\theta_{t}}(i_{1})}{\\pi_{\\theta_{t}}(a^{*})}=\\operatorname*{sup}_{t\\geq1}\\exp\\{\\theta_{t}(i_{1})-\\theta_{t}(a^{*})\\}<\\infty,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For all $t\\geq1$ , $|\\theta_{t}(i_{1})|<\\infty$ and since there is at least one arm $(a^{*})$ s.t. $\\operatorname*{inf}_{t\\geq1}\\theta_{t}(a)>-\\infty$ , there exists $\\epsilon>0$ and $\\epsilon\\in O(1)$ , such that, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq1}\\pi_{\\theta_{t}}(i_{1})<1-2\\,\\epsilon.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "According to Eq. (118), we know that $N_{\\infty}(i_{1})\\,=\\,\\infty$ and for all $a^{-}\\,\\in\\,\\mathcal{A}^{-}(i_{1})$ , $N_{\\infty}(a^{-})\\,<\\infty$ .   \nUsing Lemma 5, we have, for all large enough $t\\geq1$ , $\\begin{array}{r}{\\frac{\\pi_{\\theta_{t}}(a^{-})}{\\pi_{\\theta_{t}}(i_{1})}<\\frac{\\epsilon}{|A^{-}(i_{1})|}}\\end{array}$ . ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c l c r}{{\\pi_{\\theta_{t}}(i_{1})+\\displaystyle\\sum_{a^{+}\\in A^{+}(i_{1})}\\pi_{\\theta_{t}}(a^{+})=1-\\pi_{\\theta_{t}}(i_{1})\\displaystyle\\sum_{a^{-}\\in A^{-}(i_{1})}\\displaystyle\\frac{\\pi_{\\theta_{t}}(a^{-})}{\\pi_{\\theta_{t}}(i_{1})}>1-\\pi_{\\theta_{t}}(i_{1})\\,\\epsilon}}\\\\ {{\\Longrightarrow\\ \\pi_{\\theta_{t}}(i_{1})+\\displaystyle\\sum_{a^{+}\\in A^{+}(i_{1})}\\pi_{\\theta_{t}}(a^{+})\\geq1-\\epsilon.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining Eqs. (129) and (131), we have, for all large enough $t\\geq1$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{a^{+}\\in{\\cal A}^{+}(i_{1})}\\pi_{\\theta_{t}}(a^{+})\\geq\\epsilon,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which implies that, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=\\tau}^{\\infty}\\pi_{\\theta_{s}}(i_{1})\\sum_{a^{+}\\in A^{+}(i_{1})}\\pi_{\\theta_{s}}(a^{+})\\ge\\epsilon\\cdot\\sum_{s=\\tau}^{\\infty}\\pi_{\\theta_{s}}(i_{1})}}\\\\ &{}&{=\\infty,\\qquad(\\mathrm{since}~N_{\\infty}(i_{1})=\\infty~\\mathrm{and}~\\mathrm{by}~\\mathrm{Lemma}~3)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which contradicts the assumption of Eq. (121). This completes the proof by contradiction, and therefore, we have, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{s=\\tau}^{t}\\pi_{\\theta_{s}}(i_{1})\\sum_{a^{+}\\in\\mathcal{A}^{+}(i_{1})}\\pi_{\\theta_{s}}(a^{+})\\rightarrow\\infty,\\ \\mathrm{as}\\ t\\rightarrow\\infty.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "According to Lemma 10 (using Eqs. (105), (113) and (135)), we have, almost surely, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\theta_{t}(i_{1})\\to-\\infty,\\;\\mathrm{as}\\;t\\to\\infty.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Combining Eqs. (127) and (136), we have, almost surely, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\pi_{\\theta_{t}}(a^{*})}{\\pi_{\\theta_{t}}(i_{1})}=\\exp\\{\\theta_{t}(a^{*})-\\theta_{t}(i_{1})\\}\\to\\infty,\\;\\mathrm{as\\}t\\to\\infty.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, we have proved that if $N_{\\infty}(i_{1})=\\infty$ and $r(i_{1})<\\pi_{\\theta_{t}}^{\\top}r<r(a^{*})$ for sufficiently large $\\tau\\geq1$ , then, $\\frac{\\pi_{\\theta_{t}}(a^{*})}{\\pi_{\\theta_{t}}(i_{1})}\\rightarrow\\infty$ , as $t\\to\\infty$ . ", "page_idx": 20}, {"type": "text", "text": "In order to use this argument recursively, consider sorting the action indices in $A_{\\infty}$ according to their descending expected reward values, ", "page_idx": 20}, {"type": "equation", "text": "$$\nr(a^{*})>r(i_{|A_{\\infty}|-1})>r(i_{|A_{\\infty}|-2})>\\cdot\\cdot>r(i_{2})>r(i_{1}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We know that, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\pi_{\\theta_{t}}^{\\top}r-r(i_{2})=\\displaystyle\\sum_{a\\neq i_{2}}\\pi_{\\theta_{t}}(a)\\cdot(r(a)-r(i_{2}))}}\\\\ &{=\\displaystyle\\sum_{a=\\epsilon,A-(i_{2})}\\pi_{\\theta_{t}}(a^{-})\\cdot(r(a^{-})-r(i_{2}))+\\displaystyle\\sum_{a^{+}\\in A^{+}(i_{2})}\\pi_{\\theta_{t}}(a^{+})\\cdot(r(a^{+})-r(i_{2}))}\\\\ &{}&{(14\\,\\\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "According to Lemma 5, for all $a^{-}\\in\\mathcal{A}^{-}(i_{2})$ with $a^{-}\\neq i_{1}$ , we have, ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\frac{\\pi_{\\theta_{t}}(a^{*})}{\\pi_{\\theta_{t}}(a^{-})}}\\to\\infty,\\;\\mathrm{as}\\;t\\to\\infty.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Combining Eqs. (137) and (143), we have, for all $a^{-}\\in A^{-}(i_{2})$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\frac{\\pi_{\\theta_{t}}(a^{*})}{\\pi_{\\theta_{t}}(a^{-})}}\\to\\infty,\\;\\mathrm{as}\\;t\\to\\infty.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, for all sufficiently large $\\tau\\geq1$ , for all $a^{-}\\in A^{-}(i_{2})$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\pi_{\\theta_{t}}(a^{-})}{\\pi_{\\theta_{t}}(a^{*})}\\leq\\frac{1}{2\\left|\\mathcal{A}^{-}(i_{2})\\right|}\\,\\frac{r(a^{*})-r(i_{2})}{r(i_{2})-r(a^{-})}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Combining Eqs. (142) and (145), we have, for all sufficiently large $t\\geq1$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pi_{\\theta_{t}}^{\\top}r-r(i_{2})>\\pi_{\\theta_{t}}(a^{*})\\cdot\\frac{r(a^{*})-r(i_{2})}{2}>0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence we have, for all sufficiently large $\\tau\\geq1$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\nr(i_{2})<\\pi_{\\theta_{t}}^{\\top}r<r(a^{*})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Comparing Eqs. (120) and (147), we can use a similar argument for $i_{2}$ and conclude that, for sufficiently large \u03c4 \u22651, then, \u03c0\u03c0\u03b8\u03b8tt((ai2)) $\\frac{\\pi_{\\theta_{t}}(a^{*})}{\\pi_{\\theta_{t}}(i_{2})}\\rightarrow\\infty$ , as $t\\to\\infty$ . This further implies that ", "page_idx": 21}, {"type": "equation", "text": "$$\nr(i_{3})<\\pi_{\\theta_{t}}^{\\top}r<r(a^{*}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Continuing this recursive argument, we have, for all actions $a\\in A_{\\infty}$ with $a\\neq a^{*}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\frac{\\pi_{\\theta_{t}}(a^{*})}{\\pi_{\\theta_{t}}(a)}}\\to\\infty,\\,\\operatorname{as}t\\to\\infty.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Meanwhile, according to Lemma 5, we have, for all actions $a\\not\\in A_{\\infty}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\frac{\\pi_{\\theta_{t}}(a^{*})}{\\pi_{\\theta_{t}}(a)}}\\to\\infty,\\,\\operatorname{as}t\\to\\infty.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combining Eqs. (149) and (150), we have, for all sub-optimal actions $a\\in[K]$ with $r(a)<r(a^{*})$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\frac{\\pi_{\\theta_{t}}(a^{*})}{\\pi_{\\theta_{t}}(a)}}\\to\\infty,\\,\\operatorname{as}t\\to\\infty.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, note that, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pi_{\\theta_{t}}(a^{*})=\\frac{\\pi_{\\theta_{t}}(a^{*})}{\\sum_{a\\in[K]:\\ r(a)<r(a^{*})}\\pi_{\\theta_{t}}(a)+\\pi_{\\theta_{t}}(a^{*})}}\\\\ {=\\frac{1}{\\sum_{a\\in[K]:\\ r(a)<r(a^{*})}\\frac{\\pi_{\\theta_{t}}(a)}{\\pi_{\\theta_{t}}(a^{*})}+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combining Eqs. (151) and (153), we have, almost surely, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pi_{\\theta_{t}}(a^{*})\\to1,\\;\\mathrm{as}\\;t\\to\\infty.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "B Miscellaneous Extra Supporting Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma 3 (Extended Borel-Cantelli Lemma, Corollary 5.29 of [5]). Let $({\\mathcal{F}}_{n})_{n\\geq1}$ be a filtration, $A_{n}\\in{\\mathcal{F}}_{n}$ . Then, almost surely, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\{\\omega\\,:\\,\\omega\\in A_{n}\\,\\,i n f u n i t e l y\\,o f t e n\\,\\right\\}=\\left\\{\\omega\\,:\\,\\sum_{n=1}^{\\infty}\\mathbb{P}(A_{n}|\\mathcal{F}_{n})\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma 4 (Freedman\u2019s inequality [10, 7], Theorem C.3 of [24]). Let $X_{1},X_{2},.\\,.$ . be a sequence of random variables, such that for all $t\\geq1$ , $|X_{t}|\\leq1/2$ . Define ", "page_idx": 22}, {"type": "equation", "text": "$$\nS_{n}:=\\left|\\sum_{t=1}^{n}\\mathbb{E}[X_{t}|X_{1},\\ldots,X_{t-1}]-X_{t}\\right|\\quad a n d\\quad V_{n}:=\\sum_{t=1}^{n}\\operatorname{Var}[X_{t}|X_{1},\\ldots,X_{t-1}].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, for all $\\delta>0$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\exists\\,n:\\;S_{n}\\geq6\\,{\\sqrt{\\left(V_{n}+{\\frac{4}{3}}\\right)\\,\\log\\left({\\frac{V_{n}+1}{\\delta}}\\right)}}+2\\log\\left({\\frac{1}{\\delta}}\\right)+{\\frac{4}{3}}\\log3\\right)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma 5. Using Algorithm $^{\\,l}$ , for any two different actions $i,j\\in[K]$ with $i\\neq j$ , if $N_{\\infty}(i)=\\infty$ and $N_{\\infty}(j)<\\infty$ , then we have, almost surely, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq1}\\frac{\\pi_{\\theta_{t}}(i)}{\\pi_{\\theta_{t}}(j)}=\\infty.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We prove the result by contradiction. Suppose ", "page_idx": 22}, {"type": "equation", "text": "$$\nc:=\\operatorname*{sup}_{t\\geq1}\\frac{\\pi_{\\theta_{t}}(i)}{\\pi_{\\theta_{t}}(j)}<\\infty.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "According to the extended Borel-Cantelli Lemma 3, we have, for all $a\\in[K]$ , almost surely, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Big\\{\\sum_{t\\ge1}\\pi_{\\theta_{t}}(j)=\\infty\\Big\\}=\\{N_{\\infty}(j)=\\infty\\}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, taking complements, we have, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Big\\{\\sum_{t\\ge1}\\pi_{\\theta_{t}}(j)<\\infty\\Big\\}=\\big\\{N_{\\infty}(j)<\\infty\\big\\}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "also holds almost surely, which implies that, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\sum_{t=1}^{\\infty}\\pi_{\\theta_{t}}(i)=\\infty,~\\mathrm{and}}\\\\ {\\displaystyle\\sum_{t=1}^{\\infty}\\pi_{\\theta_{t}}(j)<\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, we have, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{\\infty}\\pi_{\\theta_{t}}(i)=\\displaystyle\\sum_{t=1}^{\\infty}\\pi_{\\theta_{t}}(j)\\cdot\\frac{\\pi_{\\theta_{t}}(i)}{\\pi_{\\theta_{t}}(j)}}\\\\ {\\displaystyle\\qquad\\qquad\\leq c\\cdot\\displaystyle\\sum_{t=1}^{\\infty}\\pi_{\\theta_{t}}(j)}\\\\ {\\displaystyle\\qquad<\\infty,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which is a contradiction with Eq. (161). ", "page_idx": 22}, {"type": "text", "text": "Lemma 6. Using Algorithm $^{\\,l}$ , we have, almost surely, for all large enough $t\\geq1$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\nr(i_{1})<\\pi_{\\theta_{t}}^{\\top}r<r(i_{2}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $i_{1},i_{2}\\in[K]$ and $i_{1}\\neq i_{2}$ are the action indices defined as, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{i_{1}:=\\underset{a\\in[K],}{\\arg\\operatorname*{min}}~r(a),}\\\\ &{\\qquad\\quad\\overset{N\\,\\ldots\\,(a)=\\infty}{N\\,\\infty}}\\\\ &{i_{2}:=\\underset{a\\in[K],}{\\arg\\operatorname*{max}}~r(a).}\\\\ &{\\qquad\\quad\\quad\\,N_{\\infty}(a)\\!=\\!\\infty}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Define $A_{\\infty}$ as the set of actions which are sampled for infinitely many times as $t\\to\\infty$ , i.e., ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{\\infty}:=\\left\\{a\\in[K]\\mid N_{\\infty}(a)=\\infty\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "According to Lemma 2, we have $|\\mathcal{A}_{\\infty}|\\geq2$ , which implies that $i_{1}\\neq i_{2}$ . ", "page_idx": 23}, {"type": "text", "text": "Given any sub-optimal action $i\\in[K]$ with $r(i)<r(a^{*})$ , we partition the remaining actions into two parts using $r(i)$ . ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A^{+}(i):=\\left\\{a^{+}\\in[K]:r(a^{+})>r(i)\\right\\},}\\\\ {\\ A^{-}(i):=\\left\\{a^{-}\\in[K]:r(a^{-})<r(i)\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By definition, we have, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{A_{\\infty}\\subseteq A^{+}(i_{1})\\cup\\{i_{1}\\},\\;\\mathrm{and}}}\\\\ {{A_{\\infty}\\subseteq A^{-}(i_{2})\\cup\\{i_{2}\\}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "First part. $r(i_{1})<\\pi_{\\theta_{t}}^{\\top}r$ . ", "page_idx": 23}, {"type": "text", "text": "If $\\textstyle r(i_{1})=\\operatorname*{min}_{a\\in[K]}r(a)$ , i.e., $i_{1}$ is the \u201cworst action\u201d, then $r(i_{1})<\\pi_{\\theta_{t}}^{\\top}r$ holds trivially. Otherwise, suppose $r(i_{1})\\neq\\operatorname*{min}_{a\\in[K]}r(a)$ . We have, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\pi_{\\theta_{t}}^{\\top}r-r(i_{1})=\\sum_{a^{+}\\in A^{+}(i_{1})}\\pi_{\\theta_{t}}(a^{+})\\cdot(r(a^{+})-r(i_{1}))-\\sum_{a^{-}\\in A^{-}(i_{1})}\\pi_{\\theta_{t}}(a^{-})\\cdot(r(i_{1})-r(a^{-})).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Consider the non-empty set $A_{\\infty}\\cap A^{+}(i_{1})$ . Pick an action $j_{1}\\in\\mathcal{A}_{\\infty}\\cap A^{+}(i_{1})$ , and ignore all the other actions $a^{+}\\in\\bar{A^{+}}(i_{1})$ with $a^{+}\\neq j_{1}$ in the above equation. We have, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi_{\\theta_{t}}^{\\top}r-r(i_{1})>\\pi_{\\theta_{t}}(j_{1})\\cdot(r(j_{1})-r(i_{1}))-\\displaystyle\\sum_{a^{-}\\in A^{-}(i_{1})}\\pi_{\\theta_{t}}(a^{-})\\cdot(r(i_{1})-r(a^{-}))}\\\\ &{\\qquad\\qquad=\\pi_{\\theta_{t}}(j_{1})\\cdot\\displaystyle\\bigg[r(j_{1})-r(i_{1})-\\displaystyle\\sum_{a^{-}\\in A^{-}(i_{1})}\\frac{\\pi_{\\theta_{t}}(a^{-})}{\\pi_{\\theta_{t}}(j_{1})}\\cdot(r(i_{1})-r(a^{-}))\\bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the first inequality is because of $r(a^{+})-r(i_{1})>0$ for all $a^{+}\\in\\mathcal{A}^{+}(i_{1})$ by Eq. (170). Note that $N_{\\infty}(j_{1})=\\infty$ and $\\dot{N}_{\\infty}(a^{-})<\\infty$ . According to Lemma 5, we have, for all large enough $t\\geq1$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{a^{-}\\in A^{-}(i_{1})}\\frac{\\pi_{\\theta_{t}}(a^{-})}{\\pi_{\\theta_{t}}(j_{1})}\\cdot(r(i_{1})-r(a^{-}))=\\sum_{a^{-}\\in A^{-}(i_{1})}(r(i_{1})-r(a^{-}))\\Big/\\frac{\\pi_{\\theta_{t}}(j_{1})}{\\pi_{\\theta_{t}}(a^{-})}}}\\\\ &{<\\sum_{a^{-}\\in A^{-}(i_{1})}\\left(r(i_{1})-r(a^{-})\\right)\\cdot\\frac{1}{\\left|A^{-}(i_{1})\\right|}\\cdot\\frac{r(j_{1})-r(i_{1})}{r(i_{1})-r(a^{-})}\\cdot\\frac{1}{2}}\\\\ &{=\\frac{r(j_{1})-r(i_{1})}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining Eqs. (175) and (177), we have, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\pi_{\\theta_{t}}^{\\top}r-r(i_{1})>\\pi_{\\theta_{t}}(j_{1})\\cdot\\frac{r(j_{1})-r(i_{1})}{2}>0,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last inequality is because $j_{1}\\in\\mathcal{A}^{+}(i_{1})$ . ", "page_idx": 24}, {"type": "text", "text": "Second part. $\\pi_{\\theta_{t}}^{\\top}r<r(i_{2})$ . ", "page_idx": 24}, {"type": "text", "text": "The arguments are similar to the first part. If $r(i_{2})\\,=\\,r(a^{*})$ , i.e., $i_{2}$ is the optimal action, then $\\pi_{\\theta_{t}}^{\\top}r<r(i_{2})$ holds trivially. Otherwise, suppose $r(i_{2})\\neq r(a^{*})$ . We have, ", "page_idx": 24}, {"type": "equation", "text": "$$\nr(i_{2})-\\pi_{\\theta_{t}}^{\\top}r=\\sum_{a^{-}\\in A^{-}(i_{2})}\\pi_{\\theta_{t}}(a^{-})\\cdot(r(i_{2})-r(a^{-}))-\\sum_{a^{+}\\in A^{+}(i_{2})}\\pi_{\\theta_{t}}(a^{+})\\cdot(r(a^{+})-r(i_{2})).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Consider the non-empty set $A_{\\infty}\\cap A^{-}(i_{2})$ . Pick an action $j_{2}\\in\\mathcal{A}_{\\infty}\\cap A^{-}(i_{2})$ , and ignore all the other actions $a^{-}\\in\\bar{A}^{-}(i_{2})$ with $a^{-}\\neq j_{2}$ in the above equation. We have, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r(i_{2})-\\pi_{\\theta_{t}}^{\\top}r\\geq\\pi_{\\theta_{t}}(j_{2})\\cdot(r(i_{2})-r(j_{2}))-\\displaystyle\\sum_{a^{+}\\in A^{+}(i_{2})}\\pi_{\\theta_{t}}(a^{+})\\cdot(r(a^{+})-r(i_{2}))}\\\\ &{\\qquad\\qquad=\\pi_{\\theta_{t}}(j_{2})\\cdot\\bigg[r(i_{2})-r(j_{2})-\\displaystyle\\sum_{a^{+}\\in A^{+}(i_{2})}\\frac{\\pi_{\\theta_{t}}(a^{+})}{\\pi_{\\theta_{t}}(j_{2})}\\cdot(r(a^{+})-r(i_{2}))\\bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the first inequality is because of $r(i_{2})-r(a^{-})>0$ for all $a^{-}\\in\\mathcal{A}^{-}(i_{2})$ by Eq. (170). Note that $N_{\\infty}(j_{2})=\\infty$ and $N_{\\infty}(a^{+})<\\infty$ . According to Lemma 5, we have, for all large enough $t\\geq1$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{a^{+}\\in A^{+}(i_{2})}\\frac{\\pi_{\\theta_{t}}(a^{+})}{\\pi_{\\theta_{t}}(j_{2})}\\cdot(r(a^{+})-r(i_{2}))=\\sum_{a^{+}\\in A^{+}(i_{2})}\\big(r(a^{+})-r(i_{2})\\big)\\Big/\\frac{\\pi_{\\theta_{t}}(j_{2})}{\\pi_{\\theta_{t}}(a^{+})}}\\\\ &{\\quad\\quad<\\displaystyle\\sum_{a^{+}\\in A^{+}(i_{2})}\\big(r(a^{+})-r(i_{2})\\big)\\cdot\\frac{1}{\\big|A^{+}(i_{2})\\big|}\\cdot\\frac{r(i_{2})-r(j_{2})}{r(a^{+})-r(i_{2})}\\cdot\\frac{1}{2}}\\\\ &{\\quad\\quad=\\frac{r(i_{2})-r(j_{2})}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combining Eqs. (182) and (184), we have, ", "page_idx": 24}, {"type": "equation", "text": "$$\nr(i_{2})-\\pi_{\\theta_{t}}^{\\top}r\\geq\\pi_{\\theta_{t}}(j_{2})\\cdot\\frac{r(i_{2})-r(j_{2})}{2}>0,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequality is because $j_{2}\\in\\mathcal{A}^{-}(i_{2})$ . ", "page_idx": 24}, {"type": "text", "text": "For the following lemmas, we will use the notation defined in the proofs for Theorems 1 and 2. ", "page_idx": 24}, {"type": "text", "text": "Lemma 7 (Concentration of noise). Given an action $a\\in[K]$ . We have, with probability at least $1-\\delta$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\langle t:}&{{}\\left|\\displaystyle\\sum_{s=1}^{t}W_{s+1}(a)\\right|\\leq36\\ \\eta\\ R_{\\operatorname*{max}}\\ \\sqrt{(V_{t}(a)+4/3)\\log\\left(\\frac{V_{t}(a)+1}{\\delta}\\right)}+12\\ \\eta\\ R_{\\operatorname*{max}}\\ \\log(1/\\delta)+8\\ \\eta\\ R_{\\operatorname*{max}}\\ \\log(1/\\delta)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where ", "page_idx": 24}, {"type": "equation", "text": "$$\nV_{t}(a):=\\frac{5}{18}\\cdot\\sum_{s=1}^{t}\\pi_{\\theta_{s}}(a)\\cdot(1-\\pi_{\\theta_{s}}(a)),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\nW_{t}(a):=\\theta_{t}(a)-\\mathbb{E}_{t-1}[\\theta_{t}(a)]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "is originally defined by Eq. (52). ", "page_idx": 24}, {"type": "text", "text": "Proof. First, note that, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t}[W_{t+1}(a)]=0,{\\mathrm{~for~all~}}t\\geq0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Using the update Eq. (22), we have, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{t+1}(a)=\\theta_{t+1}(a)-\\mathbb{E}_{t}[\\theta_{t+1}(a)]}\\\\ &{\\quad\\quad=\\theta_{t}(a)+\\eta\\cdot\\left(I_{t}(a)-\\pi_{\\theta_{t}}(a)\\right)\\cdot R_{t}(a_{t})-\\left(\\theta_{t}(a)+\\eta\\cdot\\pi_{\\theta_{t}}(a)\\cdot\\left(r(a)-\\pi_{\\theta_{t}}^{\\top}r\\right)\\right)}\\\\ &{\\quad\\quad=\\eta\\cdot\\left(I_{t}(a)-\\pi_{\\theta_{t}}(a)\\right)\\cdot R_{t}(a_{t})-\\eta\\cdot\\pi_{\\theta_{t}}(a)\\cdot\\left(r(a)-\\pi_{\\theta_{t}}^{\\top}r\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "According to Eq. (1), we have, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|W_{t+1}(a)|\\leq3\\,\\eta\\cdot R_{\\operatorname*{max}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The conditional variance of noise is, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname{Var}[W_{t+1}(a)|\\mathcal{F}_{t}]:=\\mathbb{E}_{t}[(W_{t+1}(a))^{2}]}\\\\ &{\\qquad\\le2\\,\\eta^{2}\\cdot\\mathbb{E}_{t}[(I_{t}(a)-\\pi_{\\theta_{t}}(a))^{2}\\cdot R_{t}(a_{t})^{2}]+2\\,\\eta^{2}\\cdot\\pi_{\\theta_{t}}(a)^{2}\\cdot\\big(r(a)-\\pi_{\\theta_{t}}^{\\top}r\\big)^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the inequality is by $(a+b)^{2}\\leq2a^{2}+2b^{2}$ . Next, we have, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{t}[(I_{t}(a)-\\pi_{\\theta_{t}}(a))^{2}\\cdot R_{t}(a_{t})^{2}]=\\pi_{t}(a)\\cdot(1-\\pi_{t}(a))^{2}\\cdot r(a)^{2}+\\displaystyle\\sum_{a^{\\prime}\\neq a}\\pi_{\\theta_{t}}(a^{\\prime})\\cdot\\pi_{t}(a)^{2}\\cdot r(a^{\\prime})^{2}}&{}\\\\ {\\vdots}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(15^{2}/5)}\\\\ {\\leq R_{\\operatorname*{max}}^{2}\\cdot\\Big(\\pi_{t}(a)\\cdot(1-\\pi_{t}(a))^{2}+(1-\\pi_{t}(a))\\cdot\\pi_{t}(a)^{2}\\Big)}&{(15^{2}/5)}\\\\ {=R_{\\operatorname*{max}}^{2}\\cdot\\pi_{\\theta_{t}}(a)\\cdot(1-\\pi_{\\theta_{t}}(a)),}&{(20/5)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left|\\boldsymbol{r}(a)-\\boldsymbol{\\pi}_{\\theta_{t}}^{\\top}\\boldsymbol{r}\\right|=\\Bigg|\\sum_{a^{\\prime}\\neq a}\\pi_{\\theta_{t}}(a^{\\prime})\\cdot\\left(\\boldsymbol{r}(a)-\\boldsymbol{r}(a^{\\prime})\\right)\\Bigg|}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\displaystyle\\sum_{a^{\\prime}\\neq a}\\pi_{\\theta_{t}}(a^{\\prime})\\cdot\\left|\\boldsymbol{r}(a)-\\boldsymbol{r}(a^{\\prime})\\right|}\\\\ &{\\quad\\quad\\quad\\quad\\leq2\\,R_{\\operatorname*{max}}\\cdot\\sum_{a^{\\prime}\\neq a}\\pi_{\\theta_{t}}(a^{\\prime})}\\\\ &{\\quad\\quad\\quad\\quad=2\\,R_{\\operatorname*{max}}\\cdot\\left(1-\\pi_{\\theta_{t}}(a)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining Eqs. (196), (198) and (201), we have, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{Var}[W_{t+1}(a)|\\mathcal{F}_{t}]\\leq2\\ \\eta^{2}\\cdot R_{\\operatorname*{max}}^{2}\\cdot\\pi_{\\theta_{t}}(a)\\cdot(1-\\pi_{\\theta_{t}}(a))+8\\ \\eta^{2}\\cdot R_{\\operatorname*{max}}^{2}\\cdot\\pi_{\\theta_{t}}(a)^{2}\\cdot(1-\\pi_{\\theta_{t}}(a))^{2}}&{}\\\\ {(20\\cdot\\mathrm{Var}[W_{t}])\\leq10\\ \\eta^{2}\\cdot R_{\\operatorname*{max}}^{2}\\cdot\\pi_{\\theta_{t}}(a)\\cdot(1-\\pi_{\\theta_{t}}(a)).}&{}&{\\mathrm{(200~0~0~0~0~0~)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let $\\begin{array}{r}{X_{t+1}(a):=\\frac{W_{t+1}(a)}{6\\mathrm{~}\\eta\\cdot R_{\\operatorname*{max}}}}\\end{array}$ Wt+1(a). Then we have, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{c}{|X_{t+1}(a)|\\leq1/2,\\mathrm{~and}}\\\\ {\\mathrm{Var}[X_{t+1}(a)|\\mathcal{F}_{t}]\\leq\\displaystyle\\frac{5}{18}\\cdot\\pi_{\\theta_{t}}(a)\\cdot(1-\\pi_{\\theta_{t}}(a)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "According to Lemma 4, there exists an event ${\\mathcal{E}}_{1}$ such that $\\operatorname*{Pr}({\\mathcal{E}}_{1})\\geq1-\\delta$ , and when ${\\mathcal{E}}_{1}$ holds, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\forall t:\\quad\\left|\\sum_{s=1}^{t}X_{s+1}(a)\\right|\\leq6\\,{\\sqrt{(V_{t}(a)+4/3)\\log\\left({\\frac{V_{t}(a)+1}{\\delta}}\\right)}}+2\\,\\log(1/\\delta)+{\\frac{4}{3}}\\log3,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which implies that, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\forall t:\\quad\\left|\\sum_{s=1}^{t}W_{s+1}(a)\\right|\\leq36\\ \\eta\\ R_{\\operatorname*{max}}\\ {\\sqrt{(V_{t}(a)+4/3)\\log\\left({\\frac{V_{t}(a)+1}{\\delta}}\\right)}}+12\\ \\eta\\ R_{\\operatorname*{max}}\\ \\log(1/\\delta)+4/3.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\begin{array}{r}{V_{t}(a):=\\frac{5}{18}\\cdot\\sum_{s=1}^{t}\\pi_{\\theta_{s}}(a)\\cdot(1-\\pi_{\\theta_{s}}(a)).}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "Lemma 8 (Bounded progress). For any action $a\\in[K]$ with $N_{\\infty}(a)=\\infty,$ , if there exists $c>0$ and $\\tau<\\infty$ , such that, for all $t\\geq\\tau$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\left|P_{s}(a)\\right|\\geq c\\cdot V_{t}(a),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $V_{t}(a)$ is defined in Eq. (189), and if also ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{\\infty}\\left|P_{t}(a)\\right|<\\infty,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "then we have, almost surely, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq1}|\\theta_{t}(a)|<\\infty.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. According to Eq. (55) and triangle inequality, we have, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|\\theta_{t}(a)\\right|\\leq\\left|\\mathbb{E}[\\theta_{1}(a)]\\right|+\\Big|\\sum_{s=1}^{t}W_{s}(a)\\Big|+\\Big|\\sum_{s=1}^{t-1}P_{s}(a)\\Big|.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "According to Lemma 7, there exists an event ${\\mathcal{E}}_{1}$ , such that $\\operatorname*{Pr}({\\mathcal{E}}_{1})\\geq1-\\delta$ , and when ${\\mathcal{E}}_{1}$ holds, we have, for all $t\\ge\\tau+1$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}W_{s}(a)\\Bigg|\\le36\\ \\eta\\ R_{\\mathrm{max}}\\ \\sqrt{(V_{t-1}(a)+4/3)\\cdot\\log\\left(\\frac{V_{t-1}(a)+1}{\\delta}\\right)}+12\\ \\eta\\ R_{\\mathrm{max}}\\ \\log(1/\\delta)+8\\ \\eta\\ R_{\\mathrm{max}}\\ \\log(1/\\delta)\\ .\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "According to Eq. (212), as $t\\to\\infty$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Big|\\sum_{s=1}^{t-1}P_{s}(a)\\Big|\\leq\\sum_{s=1}^{t-1}|P_{s}(a)|<\\infty.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "According to Eqs. (211), (215) and (216), we have, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|\\sum_{s=1}^{t}W_{s}(a)\\right|<\\infty.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Combining Eqs. (214), (216) and (217), we have, as $t\\to\\infty$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n|\\theta_{t}(a)|<\\infty.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Take any $\\omega\\in\\mathcal{E}:=\\{N_{\\infty}(a)=\\infty\\}$ . Because $\\mathbb{P}({\\mathcal{E}}\\setminus({\\mathcal{E}}\\cap{\\mathcal{E}}_{1}))\\le\\mathbb{P}(\\Omega\\setminus{\\mathcal{E}}_{1})\\le\\delta\\rightarrow0$ as $\\delta\\rightarrow0$ , we have that $\\mathbb{P}$ -almost surely for all $\\omega\\in{\\mathcal{E}}$ there exists $\\delta>0$ such that $\\omega\\in\\mathcal{E}\\cap\\mathcal{E}_{1}$ while Eq. (215) also holds for this $\\delta$ . Take such a $\\delta$ . We have, almost surely, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq1}|\\theta_{t}(a)|<\\infty.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma 9 (Unbounded positive progress). For any action $a\\in[K]$ with $N_{\\infty}(a)=\\infty$ , if there exists $c>0$ and $\\tau<\\infty$ , such that, for all $t\\geq\\tau$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\nP_{t}(a)>0,\\ a n d\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{s=\\tau}^{t}P_{s}(a)\\geq c\\cdot V_{t}(a),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $V_{t}(a)$ is defined in Eq. (189), and if also, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{t=\\tau}^{\\infty}P_{t}(a)=\\infty,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "then we have, almost surely, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\theta_{t}(a)\\to\\infty,\\,a s\\,t\\to\\infty.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. According to Eq. (55), ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\theta_{t}(a)=\\mathbb{E}[\\theta_{1}(a)]+\\sum_{s=1}^{t}W_{s}(a)+\\sum_{s=1}^{t-1}P_{s}(a).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "According to Lemma 7, there exists an event ${\\mathcal{E}}_{1}$ , such that $\\operatorname*{Pr}({\\mathcal{E}}_{1})\\geq1-\\delta$ , and when ${\\mathcal{E}}_{1}$ holds, we have, for all $t\\ge\\tau+1$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{=1}^{t}W_{s}(a)\\geq-36\\,\\eta\\,R_{\\operatorname*{max}}\\,\\underbrace{\\sqrt{(V_{t-1}(a)+4/3)\\cdot\\log\\left(\\frac{V_{t-1}(a)+1}{\\delta}\\right)}}_{\\diamondsuit}-12\\,\\eta\\,R_{\\operatorname*{max}}\\,\\log(1/\\delta)-8\\,\\eta\\,R_{\\operatorname*{max}}\\,\\log(1/\\delta).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By Eq. (221), as $t\\to\\infty$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t-1}P_{s}(a)\\rightarrow\\infty,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and the speed of $\\begin{array}{r}{\\sum_{s=1}^{t-1}P_{s}(a)\\to\\infty}\\end{array}$ is strictly faster than $\\heartsuit\\to\\infty$ , according to Eq. (220). This implies that $\\theta_{t}(a){\\overline{{\\to}}}\\,\\tilde{\\infty}$ , as a result of \u201ccumulative progress\u201d dominates \u201ccumulative noise\u201d. ", "page_idx": 27}, {"type": "text", "text": "Take any $\\omega\\in\\mathcal{E}:=\\{N_{\\infty}(a)=\\infty\\}$ . Because $\\mathbb{P}({\\mathcal{E}}\\setminus({\\mathcal{E}}\\cap{\\mathcal{E}}_{1}))\\le\\mathbb{P}(\\Omega\\setminus{\\mathcal{E}}_{1})\\le\\delta\\rightarrow0$ as $\\delta\\rightarrow0$ , we have that $\\mathbb{P}$ -almost surely for all $\\omega\\in{\\mathcal{E}}$ there exists $\\delta>0$ such that $\\omega\\in\\mathcal{E}\\cap\\mathcal{E}_{1}$ while Eq. (224) also holds for this $\\delta$ . Take such a $\\delta$ . We have, almost surely, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\theta_{t}(a)\\to\\infty,\\;\\mathrm{as}\\;t\\to\\infty.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Lemma 10 (Unbounded negative progress). For any action $a\\in[K]$ with $N_{\\infty}(a)\\,=\\infty,$ , if there exists $c>0$ and $\\tau<\\infty$ , such that, for all $t\\geq\\tau$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\nP_{t}(a)<0,\\;a n d\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n-\\sum_{s=\\tau}^{t}P_{s}(a)\\geq c\\cdot V_{t}(a),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $V_{t}(a)$ is defined in Eq. (189), and if also, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{t=\\tau}^{\\infty}P_{t}(a)=-\\infty,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "then we have, almost surely, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\theta_{t}(a)\\rightarrow-\\infty,\\,a s\\,t\\to\\infty.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. The proof follows almost the same arguments for Lemma 9. ", "page_idx": 27}, {"type": "text", "text": "Lemma 11 (Positive progress). For any action $a\\in[K]$ with $N_{\\infty}(a)=\\infty$ , if there exists $c>0$ and $\\tau<\\infty$ , such that, for all $t\\geq\\tau$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\nP_{t}(a)>0,\\ a n d\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{s=\\tau}^{t}P_{s}(a)\\geq c\\cdot V_{t}(a),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $V_{t}(a)$ is defined in Eq. (189), then we have, almost surely, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{t\\geq1}\\theta_{t}(a)>-\\infty.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. First case: if $\\textstyle\\sum_{t=1}^{\\infty}P_{t}(a)<\\infty$ , then according to Lemma 8, we have, almost surely, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq1}|\\theta_{t}(a)|<\\infty,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which implies Eq. (232). ", "page_idx": 27}, {"type": "text", "text": "Second case: if $\\textstyle\\sum_{t=1}^{\\infty}P_{t}(a)=\\infty$ , then according to Lemma 9, we have, almost surely, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\theta_{t}(a)\\to\\infty,\\,\\,{\\mathrm{as\\,}}t\\to\\infty,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which also implies Eq. (232). ", "page_idx": 27}, {"type": "text", "text": "Lemma 12 (Negative progress). For any action $a\\in[K]$ with $N_{\\infty}(a)=\\infty,$ , if there exists $c>0$ and $\\tau<\\infty$ , such that, for all $t\\geq\\tau$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\nP_{t}(a)<0,\\;a n d\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n-\\sum_{s=\\tau}^{t}P_{s}(a)\\geq c\\cdot V_{t}(a),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $V_{t}(a)$ is defined in Eq. (189), then we have, almost surely, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq1}\\theta_{t}(a)<\\infty.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. First case: if $\\textstyle-\\sum_{t=1}^{\\infty}P_{t}(a)<\\infty$ , then according to Lemma 8, we have, almost surely, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq1}|\\theta_{t}(a)|<\\infty,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which implies Eq. (237). ", "page_idx": 28}, {"type": "text", "text": "Second case: if $\\textstyle-\\sum_{t=1}^{\\infty}P_{t}(a)=\\infty$ , then according to Lemma 10, we have, almost surely, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\theta_{t}(a)\\to-\\infty,\\;\\mathrm{as}\\;t\\to\\infty,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which also implies Eq. (237). ", "page_idx": 28}, {"type": "text", "text": "C Additional simulation results ", "text_level": 1, "page_idx": 28}, {"type": "image", "img_path": "q9dKv1AK6l/tmp/b4f971a1f9523e416ce1e50a484d4fd6765f9de87037e97e0ae60397bf3b336b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 2: Visualization in a two-action stochastic bandit problem. Here the rewards are defined as $(-0.05,-0.25)$ . Other details are same as for Figure 1. Figures 2a and 2a are based on a single run, while Figure 2c averages across 10 runs. Note that $\\log\\left(r(a^{*})-\\pi_{\\theta_{t}}^{\\top}r\\right)\\approx10^{-33}$ at the final stages on Figure 2b. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: In our view, the abstract and introduction summarize the main results, as well as the technical novelty in obtaining these results. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: See Section 5. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We describe the setting and assumptions clearly. The main results are backed by proof sketches in the main text and detailed proofs in the appendix. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our experiments consist of simple simulations, for which all the details are included in Section 4. The algorithm studied here is remarkably simple and well-known, so the experiments should be straightforward to reproduce with these details, without any code. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: Given that we are doing simulations with a very classical algorithm, for which we give all simulation details and hyperparameters, we do not believe that code is necessary to reproduce the results. Our data is simulated, and hence easily reproduced, given the distribution which we explicitly describe. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Please see the details in Section 4. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: There are no comparisons for which significance needs to be demonstrated, but we provide evidence for the convergence by repeating 10 independent runs, which are presented in the plots. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [No] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our experiments are simply run on a single laptop or Python notebook, without requiring any particular infrastructure. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The research does not involve human subjects or introduces any new datasets. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: See Section 5. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 32}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: No data or models are released. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: No existing assets are used. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: No human subjects or crowdsourcing is involved in this work. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: No human subjects or crowdsourcing is involved in this work. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}]