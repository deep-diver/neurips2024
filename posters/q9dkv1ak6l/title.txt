Small steps no more: Global convergence of stochastic gradient bandits for arbitrary learning rates