[{"figure_path": "rCXTkIhkbF/figures/figures_0_1.jpg", "caption": "Figure 1: GPT2s training using Adam with weight decay or CPR (Kappa-IP). AdamCPR outperforms AdamW with the same budget and only requires 2/3 of the budget to reach the same score.", "description": "This figure shows the perplexity of a GPT-2 small language model during training using two different optimization methods: AdamW (with weight decay) and AdamCPR (with the Kappa-IP constraint initialization).  The plot demonstrates that AdamCPR achieves a lower perplexity (better performance) than AdamW within the same number of optimization steps (training budget).  Furthermore, it highlights that AdamCPR reaches the same perplexity level as AdamW trained for 300k steps, but using only 200k steps. This demonstrates the efficiency of the AdamCPR optimization method.", "section": "1 Introduction"}, {"figure_path": "rCXTkIhkbF/figures/figures_4_1.jpg", "caption": "Figure 1: GPT2s training using Adam with weight decay or CPR (Kappa-IP). AdamCPR outperforms AdamW with the same budget and only requires 2/3 of the budget to reach the same score.", "description": "This figure compares the performance of three different optimization methods: AdamW with weight decay, AdamCPR with Kappa-IP, and AdamW with a larger training budget.  The y-axis represents perplexity (a measure of how well the model predicts the next token in a sequence), and the x-axis shows the number of optimization steps. The plot demonstrates that AdamCPR (using the Constrained Parameter Regularization method with Kappa-IP initialization) achieves a lower perplexity than AdamW with the same training budget, and reaches a similar perplexity score as AdamW with a larger budget using only two-thirds of the steps. This highlights the efficiency and effectiveness of the AdamCPR method.", "section": "1 Introduction"}, {"figure_path": "rCXTkIhkbF/figures/figures_5_1.jpg", "caption": "Figure 1: GPT2s training using Adam with weight decay or CPR (Kappa-IP). AdamCPR outperforms AdamW with the same budget and only requires 2/3 of the budget to reach the same score.", "description": "This figure shows the training curves of a GPT-2 small language model trained using the Adam optimizer with either weight decay (AdamW) or the proposed Constrained Parameter Regularization (CPR) method. The x-axis represents the number of optimization steps, and the y-axis represents the perplexity, a measure of the model's performance. The figure demonstrates that AdamCPR with Kappa-IP initialization achieves the same perplexity as AdamW but with fewer optimization steps (approximately 2/3 of the steps). This illustrates CPR's efficiency in achieving comparable results with reduced computational cost.", "section": "1 Introduction"}, {"figure_path": "rCXTkIhkbF/figures/figures_7_1.jpg", "caption": "Figure 1: GPT2s training using Adam with weight decay or CPR (Kappa-IP). AdamCPR outperforms AdamW with the same budget and only requires 2/3 of the budget to reach the same score.", "description": "The figure shows the perplexity (a measure of how well a language model predicts a sequence of words) over the course of training a GPT-2 small language model.  Two different optimization methods are compared: AdamW (a widely used optimizer with weight decay) and AdamCPR (the proposed method with constrained parameter regularization). AdamCPR consistently achieves lower perplexity than AdamW, indicating better model performance. Notably, AdamCPR reaches a similar perplexity to AdamW with fewer optimization steps, highlighting its efficiency.", "section": "1 Introduction"}, {"figure_path": "rCXTkIhkbF/figures/figures_8_1.jpg", "caption": "Figure 1: GPT2s training using Adam with weight decay or CPR (Kappa-IP). AdamCPR outperforms AdamW with the same budget and only requires 2/3 of the budget to reach the same score.", "description": "This figure shows the perplexity of a GPT2s language model trained using two different optimizers: AdamW (with weight decay) and AdamCPR (with Kappa-IP constraint initialization).  The x-axis represents the number of optimization steps, and the y-axis represents the perplexity (a lower perplexity indicates better performance). AdamCPR achieves a lower perplexity than AdamW using the same number of optimization steps.  Furthermore, AdamCPR reaches a similar level of perplexity as AdamW using approximately 2/3 of the optimization steps, demonstrating its efficiency.", "section": "1 Introduction"}, {"figure_path": "rCXTkIhkbF/figures/figures_9_1.jpg", "caption": "Figure 1: GPT2s training using Adam with weight decay or CPR (Kappa-IP). AdamCPR outperforms AdamW with the same budget and only requires 2/3 of the budget to reach the same score.", "description": "The figure shows the perplexity (a measure of how well a language model predicts a sequence of words) over the number of optimization steps during the training of a GPT-2 small language model.  Two different optimization methods are compared: AdamW (a popular optimizer with weight decay) and AdamCPR (the proposed method using constrained parameter regularization).  AdamCPR (using the Kappa-IP initialization strategy) achieves a lower perplexity than AdamW, indicating better performance, and reaches the same perplexity with fewer optimization steps (approximately 2/3 the number of steps). This demonstrates the efficiency of AdamCPR in achieving the same level of performance with reduced computational cost.", "section": "1 Introduction"}, {"figure_path": "rCXTkIhkbF/figures/figures_15_1.jpg", "caption": "Figure 1: GPT2s training using Adam with weight decay or CPR (Kappa-IP). AdamCPR outperforms AdamW with the same budget and only requires 2/3 of the budget to reach the same score.", "description": "This figure shows the perplexity (a measure of how well a language model predicts text) over the course of training a GPT-2 small (GPT2s) language model. Two optimization methods are compared: AdamW (a popular optimizer with weight decay) and AdamCPR (the proposed method using constrained parameter regularization).  The results demonstrate that AdamCPR achieves a lower perplexity (better performance) than AdamW with the same training budget (optimization steps).  Furthermore, AdamCPR reaches the same perplexity as AdamW using only two-thirds of the training steps, indicating improved efficiency.", "section": "1 Introduction"}, {"figure_path": "rCXTkIhkbF/figures/figures_16_1.jpg", "caption": "Figure C.2: A comparison of different \u03bb update rates \u03bc in the training of a GPT2s model. We see three weight matrices during the training with AdamCPR. We also see how \u03bb regulates the constraint of the bound on the squared L2 norm. The bottom two plots show the training and validation loss.", "description": "This figure compares the training dynamics of AdamCPR with different values of the Lagrange multiplier update rate (\u03bc) on a GPT2s model.  The top three panels display the squared L2-norm of the weight matrices for three different layers of the network over training iterations. These demonstrate the effect of \u03bc on constraining the weight matrices. The bottom two panels show the training and validation loss curves, illustrating the overall performance achieved with varying values of \u03bc. The results show that the algorithm's stability and overall performance are largely insensitive to the specific choice of the \u03bc parameter within a wide range.", "section": "C Experiments on the Sensitivity of the Update Rate \u03bc"}, {"figure_path": "rCXTkIhkbF/figures/figures_17_1.jpg", "caption": "Figure 1: GPT2s training using Adam with weight decay or CPR (Kappa-IP). AdamCPR outperforms AdamW with the same budget and only requires 2/3 of the budget to reach the same score.", "description": "This figure shows the training curves of a GPT-2 small language model trained using the Adam optimizer with either weight decay (AdamW) or constrained parameter regularization (CPR) using the Kappa-IP initialization strategy. The y-axis represents the perplexity, a measure of the model's performance, and the x-axis represents the number of optimization steps. The figure demonstrates that CPR outperforms AdamW, achieving a similar perplexity score with fewer optimization steps, suggesting improved training efficiency.", "section": "1 Introduction"}, {"figure_path": "rCXTkIhkbF/figures/figures_19_1.jpg", "caption": "Figure 1: GPT2s training using Adam with weight decay or CPR (Kappa-IP). AdamCPR outperforms AdamW with the same budget and only requires 2/3 of the budget to reach the same score.", "description": "The figure shows the training curves of a GPT2s model trained using two different optimizers: AdamW (with weight decay) and AdamCPR (with the Kappa-IP constraint initialization).  The x-axis represents the number of optimization steps, and the y-axis shows the perplexity.  The results demonstrate that AdamCPR achieves lower perplexity (better performance) than AdamW using fewer optimization steps, indicating improved efficiency and potentially better generalization.", "section": "1 Introduction"}, {"figure_path": "rCXTkIhkbF/figures/figures_20_1.jpg", "caption": "Figure 1: GPT2s training using Adam with weight decay or CPR (Kappa-IP). AdamCPR outperforms AdamW with the same budget and only requires 2/3 of the budget to reach the same score.", "description": "This figure compares the performance of the AdamW optimizer with weight decay against the AdamCPR optimizer with the Kappa-IP initialization strategy.  The y-axis represents the perplexity of a GPT2s language model during training, a measure of how well the model predicts the next word in a sequence. The x-axis shows the number of optimization steps.  The figure demonstrates that AdamCPR achieves a lower perplexity (better performance) than AdamW, and reaches the same level of performance with approximately two-thirds the number of training steps.", "section": "1 Introduction"}, {"figure_path": "rCXTkIhkbF/figures/figures_21_1.jpg", "caption": "Figure 1: GPT2s training using Adam with weight decay or CPR (Kappa-IP). AdamCPR outperforms AdamW with the same budget and only requires 2/3 of the budget to reach the same score.", "description": "This figure compares the training performance of GPT-2 small model using AdamW (Adam with weight decay) and AdamCPR (Adam with Constrained Parameter Regularization) using the Kappa-IP initialization strategy.  The x-axis represents the number of optimization steps, and the y-axis shows the perplexity, a measure of how well the model predicts the next token in a sequence.  The results demonstrate that AdamCPR achieves a lower perplexity (better performance) than AdamW, and it reaches the same perplexity with significantly fewer optimization steps. This highlights the efficiency of CPR in deep learning optimization.", "section": "1 Introduction"}, {"figure_path": "rCXTkIhkbF/figures/figures_22_1.jpg", "caption": "Figure 1: GPT2s training using Adam with weight decay or CPR (Kappa-IP). AdamCPR outperforms AdamW with the same budget and only requires 2/3 of the budget to reach the same score.", "description": "The figure shows the training curves of perplexity vs. optimization steps for three different training methods: AdamW with a weight decay of 200k steps, AdamW with a weight decay of 300k steps, and AdamCPR with Kappa-IP and 200k steps. AdamCPR outperforms AdamW with the same budget (200k steps) and reaches the same score as AdamW with 300k steps by only using 2/3 of the budget.", "section": "1 Introduction"}, {"figure_path": "rCXTkIhkbF/figures/figures_23_1.jpg", "caption": "Figure 2: Percentage of correct labels (\u2191) of a ResNet18 trained on CIFAR100 with AdamW and AdamCPR with Kappa-IP or Kappa-WS. We use a learning rate warm-up of 500 steps and the best Kappa-WS value is 2\u00d7 the warm-up steps. We report the mean of three runs with random seeds. We see that both CPR versions outperform weight decay", "description": "This figure displays the performance of AdamW and AdamCPR (with Kappa-IP and Kappa-WS initialization strategies) on the CIFAR100 image classification task using a ResNet18 model.  The results show the percentage of correctly classified labels, plotted against various weight decay values (for AdamW) and warm start steps (for Kappa-WS). The experiment used a learning rate warm-up of 500 steps, and the optimal Kappa-WS value was determined to be twice the warm-up steps. The mean accuracy over three separate runs with different random seeds is reported, demonstrating that both CPR approaches surpass the performance of AdamW across a range of hyperparameter settings.", "section": "5.1 Train an Image Classification Model (CIFAR100)"}, {"figure_path": "rCXTkIhkbF/figures/figures_23_2.jpg", "caption": "Figure 1: GPT2s training using Adam with weight decay or CPR (Kappa-IP). AdamCPR outperforms AdamW with the same budget and only requires 2/3 of the budget to reach the same score.", "description": "The figure shows the perplexity (a measure of how well a language model predicts a sequence of words) over training steps for three different training methods using the GPT-2 small model.  AdamW (with weight decay) and AdamCPR (with Kappa-IP initialization) are compared. AdamCPR shows lower perplexity than AdamW with the same number of training steps, demonstrating superior performance.  Another AdamW training run is shown for comparison, highlighting the significant advantage of AdamCPR using the same training budget. It demonstrates that CPR using Kappa-IP can reach the same score while using less than two-thirds of the optimization steps.", "section": "1 Introduction"}, {"figure_path": "rCXTkIhkbF/figures/figures_24_1.jpg", "caption": "Figure 2: Percentage of correct labels (\u2191) of a ResNet18 trained on CIFAR100 with AdamW and AdamCPR with Kappa-IP or Kappa-WS. We use a learning rate warm-up of 500 steps and the best Kappa-WS value is 2\u00d7 the warm-up steps. We report the mean of three runs with random seeds. We see that both CPR versions outperform weight decay", "description": "This figure shows the performance of AdamW and AdamCPR optimizers on the CIFAR100 image classification task.  Two variants of CPR are compared: Kappa-IP (hyperparameter-free) and Kappa-WS (one hyperparameter). The experiment uses a ResNet18 model, and the results show that both CPR methods outperform AdamW with weight decay across a range of learning rates and weight decay values.", "section": "5.1 Train an Image Classification Model (CIFAR100)"}, {"figure_path": "rCXTkIhkbF/figures/figures_25_1.jpg", "caption": "Figure 1: GPT2s training using Adam with weight decay or CPR (Kappa-IP). AdamCPR outperforms AdamW with the same budget and only requires 2/3 of the budget to reach the same score.", "description": "The figure shows the training curves of GPT-2 small model using Adam optimizer with weight decay and Adam optimizer with CPR (Constrained Parameter Regularization). The CPR method uses Kappa-IP initialization strategy, which is a hyperparameter-free method. The results show that Adam with CPR outperforms AdamW, achieving the same perplexity with only 2/3 of the optimization steps (training budget). This demonstrates the effectiveness of CPR in improving the optimization process of deep learning models.", "section": "1 Introduction"}, {"figure_path": "rCXTkIhkbF/figures/figures_27_1.jpg", "caption": "Figure 1: GPT2s training using Adam with weight decay or CPR (Kappa-IP). AdamCPR outperforms AdamW with the same budget and only requires 2/3 of the budget to reach the same score.", "description": "The figure shows the training curves of a GPT-2 small language model trained with the Adam optimizer using either weight decay (AdamW) or constrained parameter regularization (CPR).  The CPR method uses the Kappa-IP initialization strategy. The graph plots perplexity against the number of optimization steps.  The results demonstrate that AdamCPR achieves lower perplexity (better performance) than AdamW within the same training budget (number of steps), and it reaches a comparable level of performance with approximately 2/3 of the computational cost.", "section": "1 Introduction"}, {"figure_path": "rCXTkIhkbF/figures/figures_34_1.jpg", "caption": "Figure 1: GPT2s training using Adam with weight decay or CPR (Kappa-IP). AdamCPR outperforms AdamW with the same budget and only requires 2/3 of the budget to reach the same score.", "description": "This figure shows the training curves for a GPT-2 small language model trained using the Adam optimizer with either weight decay (AdamW) or Constrained Parameter Regularization (CPR) using the Kappa-IP initialization strategy.  The y-axis represents the perplexity, a measure of how well the model predicts the next word in a sequence.  The x-axis represents the number of optimization steps. The figure demonstrates that CPR achieves a lower perplexity (better performance) than AdamW with the same training budget (number of steps), and even reaches the same performance level with only two-thirds of the computational resources.", "section": "1 Introduction"}]