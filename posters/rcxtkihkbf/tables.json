[{"figure_path": "rCXTkIhkbF/tables/tables_6_1.jpg", "caption": "Table 1: Comparison of AdamW and AdamCPR in a DeiT [28] pertaining on ImageNet. We train a small (22M parameters) and a base model (86M) with different regularization parameters.", "description": "This table compares the performance of AdamW and AdamCPR on DeiT-Small (22M parameters) and DeiT-Base (86M parameters) models trained on the ImageNet dataset.  It shows the Top-1 Accuracy achieved using AdamW with different weight decay values (0.005, 0.05, 0.5) and AdamCPR with Kappa WS initialization (1x, 2x, 4x lr-warmup) and Kappa IP initialization.  The results highlight the impact of different regularization strategies on model accuracy in pre-training.", "section": "5.2 Train an Image Classification Model (ImageNet)"}, {"figure_path": "rCXTkIhkbF/tables/tables_7_1.jpg", "caption": "Table 2: Comparison of AdamW and AdamCPR for CLIP finetuning on ImageNet. We report the top-1 accuracy and follow the hyperparameters and schedule from WiSE-FT [32].", "description": "This table presents the results of a fine-tuning experiment using the CLIP model on the ImageNet dataset.  It compares the performance of AdamW (with varying weight decay values) against AdamCPR (using Kappa WS and Kappa IP initializations, each with different multiples of the learning rate warmup). The top-1 accuracy is reported for each configuration, highlighting the effectiveness of AdamCPR in this fine-tuning task.  The hyperparameter settings used in this experiment adhere to those defined in the WiSE-FT paper.", "section": "5.3 Fine-tuning a CLIP model"}, {"figure_path": "rCXTkIhkbF/tables/tables_15_1.jpg", "caption": "Table C.1: Comparison of different values for the update rate \u03bc of AdamCPR. We run experiments with GPT2s with 50k total steps, a learning rate warmup of 2.5k steps, and a kappa warm start of 5k steps.", "description": "This table presents the results of experiments on the sensitivity of the update rate (\u03bc) in the CPR method.  The experiments used the GPT2s model and involved 50,000 total training steps, a learning rate warm-up of 2,500 steps, and a kappa warm start of 5,000 steps.  Four different values for \u03bc were tested (10, 1, 0.1, and 0.01), and the accuracy and perplexity scores are reported for each value.", "section": "C Experiments on the Sensitivity of the Update Rate \u03bc"}, {"figure_path": "rCXTkIhkbF/tables/tables_18_1.jpg", "caption": "Table 1: Comparison of AdamW and AdamCPR in a DeiT [28] pertaining on ImageNet. We train a small (22M parameters) and a base model (86M) with different regularization parameters.", "description": "This table compares the performance of AdamW and AdamCPR on the ImageNet dataset using two different DeiT models: a small model with 22 million parameters and a base model with 86 million parameters.  It shows the top-1 accuracy achieved by each optimizer for different weight decay values (for AdamW) and different Kappa-WS and Kappa-IP values (for AdamCPR). The table highlights the impact of different regularization strategies on the accuracy of pre-trained vision transformers.", "section": "5.2 Train an Image Classification Model (ImageNet)"}, {"figure_path": "rCXTkIhkbF/tables/tables_26_1.jpg", "caption": "Table F.1: Hyperparameters for the DeiT small experiments on ImageNet.", "description": "This table lists the hyperparameters used for training the DeiT-Small model on the ImageNet dataset.  It specifies settings for both AdamW and AdamCPR optimizers, including weight decay values, learning rate, warmup epochs, training epochs, batch size, and data augmentation techniques.  The table also shows the initialization parameters and methods for the CPR optimizer's constraint values.", "section": "F Experiments on Image Classification (ImageNet)"}, {"figure_path": "rCXTkIhkbF/tables/tables_26_2.jpg", "caption": "Table 1: Comparison of AdamW and AdamCPR in a DeiT [28] pertaining on ImageNet. We train a small (22M parameters) and a base model (86M) with different regularization parameters.", "description": "This table presents the results of comparing AdamW and AdamCPR optimizers on DeiT (Data-efficient Image Transformer) models for ImageNet image classification.  Two model sizes are used: a small model with 22 million parameters and a base model with 86 million parameters.  The comparison focuses on the Top-1 accuracy achieved using different weight decay values (for AdamW) and different initialization strategies for CPR (Kappa WS and Kappa IP).  The table highlights the performance gains achieved with CPR compared to AdamW under various conditions.", "section": "5.2 Train an Image Classification Model (ImageNet)"}, {"figure_path": "rCXTkIhkbF/tables/tables_28_1.jpg", "caption": "Table H.1: Comparison of AdamW, AdamCPR, AdaDecay, AWD, and AMOS on GPT2s trained on OpenWebText. For AdamW and AdamCPR we report the mean across three random seeds. For the other methods, only a single seed is reported. The number next to the optimizer name is the weight decay coefficient \u03b3 except for AdamCPR, here it is the number of warm start steps s for Kappa-WS.", "description": "This table compares the performance of different optimization methods (AdamW, AdamCPR with Kappa-WS and Kappa-IP, AdaDecay, AWD, AMOS) on a GPT2s language model trained using the OpenWebText dataset.  The results are presented as perplexity scores, a lower score indicating better performance.  For AdamW and AdamCPR, average perplexity over three runs is reported, while for other optimizers a single run's results are shown.  The numbers next to each method indicate the corresponding weight decay coefficient (\u03b3) for AdamW, AdaDecay, AWD, and AMOS, while for AdamCPR it indicates the number of warm-up steps used for the Kappa-WS initialization strategy.  Note that AMOS resulted in NaN (Not a Number) perplexity values across the board.", "section": "Experiments on Language Modelling"}, {"figure_path": "rCXTkIhkbF/tables/tables_29_1.jpg", "caption": "Table 1: Comparison of AdamW and AdamCPR in a DeiT [28] pertaining on ImageNet. We train a small (22M parameters) and a base model (86M) with different regularization parameters.", "description": "This table presents the results of comparing AdamW and AdamCPR optimizers on DeiT models (small and base sizes) for ImageNet image classification.  The models were trained with varying weight decay values for AdamW and different Kappa initialization strategies for AdamCPR (Kappa WS and Kappa IP).  The table shows the Top-1 accuracy achieved for each model size and optimizer configuration.", "section": "5.2 Train an Image Classification Model (ImageNet)"}, {"figure_path": "rCXTkIhkbF/tables/tables_30_1.jpg", "caption": "Table 1: Comparison of AdamW and AdamCPR in a DeiT [28] pertaining on ImageNet. We train a small (22M parameters) and a base model (86M) with different regularization parameters.", "description": "This table presents a comparison of the AdamW and AdamCPR optimizers on DeiT models (small and base versions) trained on the ImageNet dataset.  It showcases the performance (Top-1 accuracy) achieved using different weight decay values for AdamW and different parameter initialization strategies (Kappa-WS and Kappa-IP) for AdamCPR.  The results highlight the impact of different regularization techniques on the models' performance.", "section": "Experiments on Image Classification (ImageNet)"}, {"figure_path": "rCXTkIhkbF/tables/tables_30_2.jpg", "caption": "Table 1: Comparison of AdamW and AdamCPR in a DeiT [28] pertaining on ImageNet. We train a small (22M parameters) and a base model (86M) with different regularization parameters.", "description": "This table compares the performance of AdamW and AdamCPR optimizers on DeiT models (small and base sizes) trained on the ImageNet dataset.  It shows the top-1 accuracy achieved using different weight decay values (for AdamW) and different Kappa initialization methods (for AdamCPR). The results highlight the impact of various regularization strategies on model accuracy for different model sizes.", "section": "5.2 Train an Image Classification Model (ImageNet)"}, {"figure_path": "rCXTkIhkbF/tables/tables_30_3.jpg", "caption": "Table 1: Comparison of AdamW and AdamCPR in a DeiT [28] pertaining on ImageNet. We train a small (22M parameters) and a base model (86M) with different regularization parameters.", "description": "This table compares the performance of AdamW and AdamCPR optimizers on DeiT models (small and base versions) for ImageNet image classification.  It shows the top-1 accuracy achieved using different weight decay values for AdamW and different Kappa (WS and IP) initializations for AdamCPR. The results highlight the performance gains using AdamCPR compared to AdamW, particularly with the hyperparameter-free Kappa IP.", "section": "5.2 Train an Image Classification Model (ImageNet)"}, {"figure_path": "rCXTkIhkbF/tables/tables_31_1.jpg", "caption": "Table 1: Comparison of AdamW and AdamCPR in a DeiT [28] pertaining on ImageNet. We train a small (22M parameters) and a base model (86M) with different regularization parameters.", "description": "This table compares the performance of AdamW and AdamCPR optimizers on DeiT models (small and base versions) for ImageNet image classification.  It shows the top-1 accuracy achieved using different weight decay values (for AdamW) and different initialization strategies (Kappa WS, Kappa IP) for AdamCPR. The results demonstrate that AdamCPR with appropriate parameter initialization can surpass the performance of AdamW.", "section": "5.2 Train an Image Classification Model (ImageNet)"}, {"figure_path": "rCXTkIhkbF/tables/tables_32_1.jpg", "caption": "Table 1: Comparison of AdamW and AdamCPR in a DeiT [28] pertaining on ImageNet. We train a small (22M parameters) and a base model (86M) with different regularization parameters.", "description": "This table presents the results of comparing AdamW and AdamCPR optimizers on DeiT models (small and base sizes) for ImageNet image classification.  Different weight decay values are used for AdamW, while AdamCPR utilizes the Kappa-WS and Kappa-IP initialization strategies. The table shows Top-1 accuracy achieved by each configuration, highlighting the performance differences between the optimizers and initialization methods.", "section": "5.2 Train an Image Classification Model (ImageNet)"}, {"figure_path": "rCXTkIhkbF/tables/tables_33_1.jpg", "caption": "Table 1: Comparison of AdamW and AdamCPR in a DeiT [28] pertaining on ImageNet. We train a small (22M parameters) and a base model (86M) with different regularization parameters.", "description": "This table compares the performance of AdamW and AdamCPR on the ImageNet dataset using DeiT, a vision transformer model. Two model sizes are used: small (22M parameters) and base (86M parameters).  Different weight decay values are tested for AdamW, while AdamCPR uses the Kappa-WS and Kappa-IP initialization strategies. The top-1 accuracy is reported for each configuration.", "section": "5.2 Train an Image Classification Model (ImageNet)"}]