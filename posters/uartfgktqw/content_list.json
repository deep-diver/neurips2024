[{"type": "text", "text": "MagR: Weight Magnitude Reduction for Enhancing Post-Training Quantization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Aozhong Zhang1 Naigang Wang2 Yanxia Deng1 Xin Li1 Zi Yang1 Penghang $\\mathbf{Yin^{1}}$ 1University at Albany, SUNY 2 IBM T. J. Watson Research Center {azhang3, ydeng5, xli48, zyang8, pyin}@albany.edu nwang@us.ibm.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we present a simple optimization-based preprocessing technique called Weight Magnitude Reduction (MagR) to improve the performance of posttraining quantization. For each linear layer, we adjust the pre-trained floating-point weights by solving a channel-wise $\\ell_{\\infty}$ -regularized optimization problem. This process greatly diminishes the maximum magnitude of the weights and smooths out outliers, while preserving the layer\u2019s output. The preprocessed weights exhibit reduced range, which facilitates the subsequent quantization process. To implement MagR, we address the $\\ell_{\\infty}$ -regularization by employing an efficient proximal gradient descent algorithm. Unlike existing preprocessing methods that involve linear transformations and subsequent post-processing steps, which can introduce significant overhead at inference time, MagR functions as a non-linear transformation, eliminating the need for any additional post-processing. This ensures that MagR introduces no overhead whatsoever during inference. Our experiments demonstrate that MagR achieves state-of-the-art performance on the Llama family of models. For example, we achieve a Wikitext2 perplexity of 5.95 on the LLaMA2-70B model for per-channel INT2 weight quantization without incurring any inference overhead. The code is available at https://github.com/AozhongZhang/MagR ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have achieved outstanding performance across a broad range of applications, demonstrating remarkable success. However, their unprecedented model size has led to many computation operations and substantial memory footprints, becoming significant barriers to their practical deployment and adoption in production environments. Accordingly, it is highly desirable to develop efficient model compression techniques for LLMs so they can be more widely deployed in resource-limited scenarios. Among the various techniques to compress and accelerate deep neural networks (DNNs), low-precision quantization has proven to be highly effective across numerous application domains and is widely adopted for accelerating DNNs. For LLMs, the inference runtime is dominated by the token generation process, where output tokens are produced sequentially, one at a time. This process is known to be memory bandwidth bound [3, 19]. As a result, the quantization of LLMs has primarily focused on reducing the bit-width of model weights, with the dual goals of lowering the model\u2019s footprint to enable deployment on resource-constrained devices and decreasing the memory bandwidth requirements to improve computational efficiency and accelerate inference. ", "page_idx": 0}, {"type": "text", "text": "The enormous computational demands for pre-training and fine-tuning Large Language Models (LLMs) have led to the emergence of Post-Training Quantization (PTQ) [4, 15, 22, 24, 27, 31, 41, 51, 52, 53, 43, 40], as a promising solution for quantizing these models. Unlike Quantization Aware Training (QAT) [7, 9, 12, 18, 21, 23, 46, 47, 48, 49], which is designed to minimize a global training loss for quantization parameters, PTQ directly applies low-precision calibration to a pretrained full-precision model using a minimal set of calibration samples. By aiming to identify an optimal quantized model locally through the minimization of a simplified surrogate loss, PTQ offers computational savings and resource efficiency compared to QAT. However, PTQ often lags behind QAT in accuracy, particularly for ultra-low precision lower than 4-bit. Thus, it remains an open problem to achieve an improved balance between cost and performance for PTQ-based approaches. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Motivation. To achieve state-of-the-art performance, the latest advances in PTQ [8, 25, 26, 36, 42] have proposed applying a linear transformation to process the pre-trained weights within a linear layer. This strategy of linear transformation aims to make the weights more suitable for the subsequent quantization procedure by reducing their magnitudes and suppressing outliers. In a nutshell, given the features $\\mathbf{\\deltaX}$ and weights $W$ , one constructs linear transformation $_T$ such that $_{T W}$ is better conditioned than $W$ in terms of being quantization-friendly. Such designs of $_T$ include diagonal matrices (so-called channel-wise scaling) [25, 36, 42], random transformations [8, 39], and finite frames [1, 13]. Then, quantization is performed on $^{T W}$ instead of the original weights $W$ . To preserve the layer\u2019s output, however, the inverse transformation ${\\pmb T}^{-1}$ has to be in turn applied to the features $\\mathbf{\\deltaX}$ , namely, ", "page_idx": 1}, {"type": "equation", "text": "$$\nX W=(X T^{-1})(T W)\\approx(X T^{-1}){\\mathcal{Q}}(T W),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "with $\\mathcal{Q}(T W)$ being the quantized weights. PTQ done this way requires modifications on the original neural architecture, which involves additional computations of $X T^{-1}$ and extra memory storage for $T^{-1}$ at inference time. As a result, these steps introduce overhead that offsets the benefits provided by quantization. This raises a natural question: ", "page_idx": 1}, {"type": "text", "text": "Can we effectively process the weights at the preprocessing stage to facilitate quantization without introducing inference overhead? ", "page_idx": 1}, {"type": "text", "text": "To address this problem, we propose a simple optimization-based technique called Weight Magnitude Reduction (MagR). MagR functions as a non-linear transformation on weights without altering the original features/activations. The optimization program is designed to find new weights with minimal maximum magnitude, i.e., the $\\ell_{\\infty}$ norm, while preserving the layer\u2019s outputs. ", "page_idx": 1}, {"type": "text", "text": "Contributions. We propose a non-linear approach, MagR, based on channel-wise $\\ell_{\\infty}$ -regularized least squares, to reduce the quantization scale without compromising the performance of pre-trained model, facilitating subsequent weight quantization while requiring no post-processing or inference overhead. See Figure 1 for comparing weight magnitudes before and after applying MagR. To address the $\\ell_{\\infty}$ -regularization problem, we develop an efficient and parallelizable proximal gradient descent algorithm that involves computing $\\ell_{1}$ -ball projections at each iteration. Specifically, MagR preprocessing on a single Nvidia A100 GPU takes merely $15\\;\\mathrm{min}$ for LLaMA2-7B and $3.5\\,\\mathrm{hr}$ for the 70B model. Our results on INT weight-quantization demonstrate that MagR can significantly boost the performance in the sub-4bit regime when combined with fast gradient-free methods for layer-wise PTQ, such as rounding-to-nearest (RTN) [30] and OPTQ [16]. This approach achieves performance for weight quantization at least comparable to state-of-the-art PTQ methods on natural language processing (NLP) tasks, including gradient-based methods using block-wise reconstruction. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Recently, as the sizes of language models are exploding, there has been growing interest in developing post-training quantization (PTQ) methods [8, 16, 25, 26, 36, 44, 45] for large-scale AI models like large language models (LLMs) to reduce the model sizes and accelerate inference by representing weight matrices in low precision. PTQ methods directly find the low-precision representation of the model without re-training, thereby preferred by extreme large-scale AI models. The OPTQ [16] uses approximate second-order information to calibrate the quantization. The method successfully compresses LLMs into 3 or 4 bits and can achieve reasonable accuracy in 2 bits. Researchers have found that the extreme values and the distribution of the weight entries highly affect the quantization errors and the quantized model quality. The original weight can be converted into a more quantizationfriendly one by linear transformations. The approach can significantly reduce the quantization errors while bringing more time overhead during inference because of the linear transformation. OmniQuant [36] proposes learnable weight clippings and equivalent transformations to avoid the influence of extreme values. AWQ [25] searches for the most significant entries in the weight by looking at the activation and selects the scales that protect these entries. SmoothQuant [44] passes the difficulty in activation quantization to weights by an equivalent linear transformation. QuIP [8], AffineQuant [26] and FrameQuant [1] apply a linear transformation before quantization to make the transformed weight quantization-friendly. These approaches achieve high performance for extreme bits, like 2 bits, but introduce additional inference overhead though the transformation is carefully designed to be efficient. OmniQuant [36] and AffineQuant [26] can be adopted for weight-activation quantization by considering the activations in the proposed methods. The work [45] introduces a low-rank compensation method on top of other quantization methods, which employs low-rank matrices to reduce quantization errors with a minimal increase in model size. By modeling the quantization residual as an $\\ell_{\\infty}$ -bounded perturbation, [2] proposes applying an $\\ell_{1}$ penalty on the gradient of loss to enhance quantization robustness. ", "page_idx": 1}, {"type": "image", "img_path": "UARTFgkTqW/tmp/83e2673ac8dd634a896c8b1a0f5d54aac39dbb85031c1c30f5ea660e2713490e.jpg", "img_caption": ["Figure 1: Motivation behind MagR: we can effectively reduce the magnitude of weights at the preprocessing stage. Each point denotes the maximum magnitude before ( $x$ -coordinate) and after $y$ -coordinate) applying MagR within a sampled channel (or column) of the weight matrix from three random layers of LLaMa2-7B [38]. These column-wise maximum magnitudes are typically more than halved through MagR. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The works most closely related to ours are [20] and [27], both utilizing $\\ell_{\\infty}$ norm to regularize or constrain the weight range to a smaller scale. The Range Regularization $(\\mathbf{R}^{2})$ method [20] applies an $\\ell_{\\infty}$ penalty or its variants to the conventional network loss to regularize the weight range during end-to-end model pre-training, optimized via SGD. However, this approach becomes practically infeasible for large-scale models. In [27], a layer-wise pre-processing technique is proposed, which involves solving an intractable $\\ell_{0}$ -minimization problem while constraining the $\\ell_{\\infty}$ -norm of weights. ", "page_idx": 2}, {"type": "text", "text": "3 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "First, we clarify the mathematical notations that will be used throughout this paper: ", "page_idx": 2}, {"type": "text", "text": "Notations. We denote vectors by bold small letters and matrices by bold capital ones. For a positive integer $n$ , $[n]:=\\{1,2,...,n\\}$ denotes the set containing all positive integers up to $n$ . For any two vectors $\\mathbf{\\Delta}x,y\\in\\mathbb{R}^{n}$ , $\\textstyle\\langle x,y\\rangle:=\\sum_{i=1}^{n}x_{i}y_{i}$ is the inner product. We denote by $\\|x\\|:={\\sqrt{\\langle x,x\\rangle}}=$ $\\sqrt{\\textstyle\\sum_{i=1}^{n}x_{i}^{2}}$ the Euclidean norm ; $\\textstyle\\|\\pmb{x}\\|_{1}:=\\sum_{i=1}^{n}|x_{i}|$ is the $\\ell_{1}$ -norm; $\\|\\pmb{x}\\|_{\\infty}:=\\operatorname*{max}_{1\\leq i\\leq n}|x_{i}|$ is the $\\ell_{\\infty}$ -norm. For any matrix $\\pmb{X}\\in\\mathbb{R}^{m\\times n}$ , $\\pmb{X}^{\\top}\\in\\mathbb{R}^{n\\times m}$ is the transpose. We denote the spectrum norm of $\\mathbf{\\deltaX}$ by $\\|X\\|=\\sigma_{\\operatorname*{max}}(X)$ , which equals its maximum singular value. Its Frobenius norm is given by $\\begin{array}{r}{\\|\\boldsymbol{X}\\|_{\\mathrm{F}}=\\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n}X_{i,j}^{2}}}\\end{array}$ . Moreover, for vectors $\\textbf{\\em x}$ and $y,x\\odot y:=(x_{1}y_{1},\\dots,x_{n}y_{n})\\in\\mathbb{R}^{n}$ denotes the Hadamard or element-wise product, and likewise for two matrices. ", "page_idx": 2}, {"type": "text", "text": "Layerwise PTQ. Post-training quantization via layerwise reconstruction calls for solving a least squares problem with a discrete constraint. For the pre-trained weights $W$ within a linear layer, we aim to find the quantized weights $W_{q}$ that minimize the following function ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{{W_{q}\\in\\mathbb{Q}}}\\;\\|X W_{q}-X W\\|_{\\mathrm{F}}^{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pmb{X}\\in\\mathbb{R}^{(b\\cdot l)\\times m}$ is the feature matrix associated with a batch of calibration data consisting of $b$ samples stacked together, and each data sample is represented by an $l\\times m$ sub-matrix. $\\mathbb{Q}\\subset\\mathbb{R}^{m\\times n}$ is an appropriate set of all feasible quantized weights. ", "page_idx": 2}, {"type": "text", "text": "The most straightforward PTQ technique, known as RTN, involves directly rounding the weight matrix $W$ without utilizing any additional data. An improvement over RTN was introduced by AWQ [25], which enhances the quantization process by incorporating channel-wise scaling on $W$ . ", "page_idx": 3}, {"type": "text", "text": "Thanks to the simplicity of the layer-wise formulation (1), several efficient gradient-free algorithms [4, 16, 51, 53] have been recently proposed to address layer-wise quantization, including OPTQ. Built on top of OPTQ, QuIP subjects $\\mathbf{\\deltaX}$ and $W$ to random orthogonal transformations to produce \u201cincoherent\" weight and Hessian matrices, leading to superior accuracy with sub-4bit quantization. However, this advantage comes with a trade-off; during inference, QuIP requires random orthogonal transformations on the feature inputs of linear layers, rendering noticeably slower throughput compared to OPTQ. ", "page_idx": 3}, {"type": "text", "text": "Uniform Quantizer. Given a set of points $\\pmb{w}\\in\\mathbb{R}^{m}$ , the commonly-used (asymmetric) uniform quantizer [9] defines the quantization step \u03b4 = max(w2)b\u2212\u22121min(w) and zero-point $z=\\left\\lfloor\\frac{\\operatorname*{min}({\\pmb w})}{\\delta}\\right\\rceil$ , and it quantizes $\\pmb{w}$ onto the scaled integer grids $\\mathbb{Q}=\\{z\\cdot\\delta,(z+1)\\cdot\\delta,\\ldots,(z+(2^{b}-1))\\cdot\\bar{\\delta}\\}^{m}$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\pmb w}_{q}=\\delta\\cdot\\left(\\mathrm{clamp}\\left(\\left\\lfloor\\frac{{\\pmb w}}{\\delta}\\right\\rceil-z,0,2^{b}-1\\right)+z\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In per-channel (or per-group) PTQ, the quantization step $\\delta$ is conventionally calculated based on the channel-wise (or group-wise, respectively) minimum and maximum values of the pre-trained weights $W$ , as defined above, and remains constant throughout the quantization procedure. ", "page_idx": 3}, {"type": "text", "text": "4 The Proposed Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present the Weight Magnitude Reduction (MagR) method based on $\\ell_{\\infty}$ -norm regularization, which is applied just before the quantization step within each linear layer. The intuition behind MagR is based on the following simple estimate of the layer-wise quantization error. Given the feature/activation matrix $\\mathbf{\\deltaX}$ , the quantizer $\\mathcal{Q}$ , and any pre-trained weights ${\\pmb w}\\in\\mathbb{R}^{m}$ , we have: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{w_{q}\\in\\mathbb{Q}}\\|X w_{q}-X w\\|\\leq\\|X(\\mathcal{Q}(w)-w)\\|\\leq\\|X\\|\\|\\mathcal{Q}(w)-w\\|\\leq\\frac{\\sigma_{\\operatorname*{max}}(X)\\sqrt{m}}{2}\\delta,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{\\delta\\,=\\,\\frac{\\operatorname*{max}({\\pmb w})-\\operatorname*{min}({\\pmb w})}{2^{b}-1}}\\end{array}$ is the quantization step size. This shows that reducing the range of weights helps to suppress the quantization error. With this in mind, MagR preprocessing is designed to achieve two key effects: ", "page_idx": 3}, {"type": "text", "text": "\u2022 First, it effectively reduces the channel-wise (or column-wise) maximum magnitude of the weights, as illustrated by Figure 1.   \n\u2022 Second, it preserves the model\u2019s original performance with minimal accuracy loss after preprocessing. Table 1 demonstrates that MagR preprocessing maintains the perplexity of the pre-trained models, with only minor degradation. ", "page_idx": 3}, {"type": "table", "img_path": "UARTFgkTqW/tmp/5c2baf64b9dd37cc812ca16698dde8b32c393dad436ab1299c13b3e0b2bf0c72.jpg", "table_caption": ["Table 1: A comparison of perplexity (PPL) for the original pre-trained and the MagR-processed LLaMA2 models. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "4.1 Approximately Rank-Deficient Feature Matrix ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To illustrate the idea behind the proposed MagR method, let us consider a pre-trained weight vector $\\hat{\\pmb{w}}\\in\\mathbb{R}^{m}$ of a linear layer and the associated feature input matrix $\\mathbf{\\deltaX}$ . MagR leverages the fact that the feature matrix $\\mathbf{\\deltaX}$ across all layers of LLMs is approximately rank-deficient. Specifically, if $\\mathbf{\\deltaX}$ is exactly rank-deficient, the linear system modeling the layer\u2019s output, $X w=X\\hat{w}$ with variables $\\pmb{w}$ , generally has infinitely many solutions. That is, for any $\\pmb{\\nu}$ in the non-trivial kernel space of $\\mathbf{\\deltaX}$ , we have that $\\pmb{w}=\\hat{\\pmb{w}}+\\pmb{\\nu}$ preserves the layer\u2019s output. Among all solutions, MagR aims to identify the weight vector $\\pmb{w}$ with the smallest extreme value in magnitude. ", "page_idx": 3}, {"type": "table", "img_path": "UARTFgkTqW/tmp/5c72b235417bb3eed365490a9015c79908af14e414a4df792b29fd423de055fb.jpg", "table_caption": ["Table 2: The statistics of (approximate) fraction ranks in percentage $(\\%)$ of feature matrix $\\mathbf{\\deltaX}$ across all layers of LLaMA models. All feature matrices are approximately rank-deficient with a fraction rank less than $100\\%$ . Some of them are highly low-rank with a fraction rank $\\approx1\\%$ . "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "In [8], the authors empirically observed that the Hessian matrix $X^{\\top}X$ is approximately low-rank across all layers in open pre-trained (OPT) models [54]. Here we examined the feature matrix of LLaMA models [37, 38]. Our approximate fraction rank of the feature matrix $\\mathbf{\\deltaX}$ is defined as the fraction of singular values of $\\mathbf{\\deltaX}$ such that $\\sigma(X)>0.01\\cdot\\sigma_{\\operatorname*{max}}(X)$ . Table 2 illustrates that all feature matrices extracted from LLaMA models are indeed rank-deficient according to this definition. ", "page_idx": 4}, {"type": "text", "text": "4.2 MagR via $\\ell_{\\infty}$ -Regularization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Let us consider the quantization of a weight vector for simplicity. Given pre-trained weight vector $\\hat{w}$ , we would like to find a new set of weights $\\pmb{w}$ with the smallest maximum magnitude, such that the layer output is preserved up to a small error $\\varepsilon>0$ , i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{w}\\in\\mathbb{R}^{m}}\\|\\pmb{w}\\|_{\\infty}\\quad\\mathrm{subject}\\,\\mathrm{to}\\quad\\|X\\pmb{w}-X\\pmb{\\hat{w}}\\|\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To efficiently implement MagR, we consider the following mathematically equivalent $\\ell_{\\infty}$ - regularization problem instead: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{w}\\in\\mathbb{R}^{m}}\\frac{1}{2}\\|\\pmb{X}\\pmb{w}-\\pmb{X}\\hat{\\pmb{w}}\\|^{2}+\\alpha\\|\\pmb{w}\\|_{\\infty}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\alpha>0$ serves as the regularization parameter, balancing fidelity against the $\\ell_{\\infty}$ regularizer. To maintain the output of the layer, $\\alpha$ should typically be set to a small value. Indeed, let $\\pmb{w}^{*}$ be the minimizer of (2), we have that the $\\ell_{2}$ error of the layer\u2019s output introduced by MagR is $O({\\sqrt{\\alpha}})$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\boldsymbol{X}\\boldsymbol{w}^{*}-\\boldsymbol{X}\\hat{\\boldsymbol{w}}\\|\\leq\\sqrt{\\|\\boldsymbol{X}\\boldsymbol{w}^{*}-\\boldsymbol{X}\\hat{\\boldsymbol{w}}\\|^{2}+2\\alpha\\|\\boldsymbol{w}^{*}\\|_{\\infty}}\\qquad}\\\\ {\\leq\\sqrt{\\|\\boldsymbol{X}\\hat{\\boldsymbol{w}}-\\boldsymbol{X}\\hat{\\boldsymbol{w}}\\|^{2}+2\\alpha\\|\\hat{\\boldsymbol{w}}\\|_{\\infty}}=\\sqrt{2\\alpha\\|\\hat{\\boldsymbol{w}}\\|_{\\infty}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\|\\hat{\\pmb{w}}\\|_{\\infty}$ is a constant independent of $\\alpha$ , and the second inequality uses that $\\pmb{w}^{*}$ is the minimizer. ", "page_idx": 4}, {"type": "text", "text": "Proximal Gradient Descent. Note that $\\ell_{\\infty}$ -norm is a convex but non-differentiable function. In theory, the optimization problem (2) can be simply solved by a subgradient algorithm, but it is significantly slower than the more sophisticated proximal gradient algorithm which matches the convergence rate of standard gradient descent. ", "page_idx": 4}, {"type": "text", "text": "With the step size $\\eta>0$ , proximal gradient descent [32] takes the following iteration: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\pmb w}^{k+1}=\\operatorname{prox}_{\\eta\\alpha\\parallel\\cdot\\parallel\\infty}\\bigg({\\pmb w}^{k}-\\eta\\,\\nabla_{\\pmb w}\\frac{1}{2}\\|{\\pmb X}{\\pmb w}-{\\pmb X}\\hat{\\pmb w}\\|^{2}\\bigg|_{{\\pmb w}={\\pmb w}^{k}}\\bigg)}\\\\ &{\\quad\\quad=\\operatorname{prox}_{\\eta\\alpha\\parallel\\cdot\\parallel\\infty}\\bigg({\\pmb w}^{k}-\\eta\\cdot{\\pmb X}^{\\top}{\\pmb X}({\\pmb w}^{k}-\\hat{\\pmb w})\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathrm{prox}_{t\\|\\cdot\\|_{\\infty}}$ with the scalar $t>0$ is the (scaled) proximal operator of $\\ell_{\\infty}$ -norm function, defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{prox}_{t\\parallel\\cdot\\parallel\\infty}(\\pmb{v}):=\\arg\\operatorname*{min}_{\\pmb{x}\\in\\mathbb{R}^{m}}\\frac{1}{2}\\|\\pmb{x}-\\pmb{v}\\|^{2}+t\\|\\pmb{x}\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To ensure the convergence of (3), it is sufficient to choose the step size ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\eta\\le\\frac{1}{\\lambda_{\\operatorname*{max}}(X^{\\top}X)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda_{\\operatorname*{max}}(X^{\\top}X)$ is the maximum eigenvalue of $X^{\\top}X$ . ", "page_idx": 5}, {"type": "text", "text": "Proximal Operator of $\\ell_{\\infty}$ -Norm. It remains to determine the proximal operator of $\\ell_{\\infty}$ -norm. It turns out we can compute it by leveraging the celebrated Moreau decomposition [29, 32]: for any $t>0$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{prox}_{t\\parallel\\cdot\\parallel\\infty}(\\pmb{v})=\\pmb{v}-t\\cdot\\operatorname{proj}_{\\parallel\\cdot\\parallel_{1}\\leq1}\\left(\\frac{\\pmb{v}}{t}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "That is, computing the proximal operator of $\\ell_{\\infty}$ norm amounts to evaluating the projection onto $\\ell_{1}$ ball, which is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{proj}_{\\|\\cdot\\|_{1}\\leq1}(v):=\\arg\\operatorname*{min}_{x\\in\\mathbb{R}^{m}}\\|x-v\\|^{2}\\quad{\\mathrm{subject~to~}}\\ \\ \\|x\\|_{1}\\leq1.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Fortunately, computing projection onto the $\\ell_{1}$ ball is an established task, and there are several efficient algorithms available. For example, see [11] and the references therein. Here we adopted a simple algorithm of $O(m\\log m)$ time complexity as in [14], which supports parallelizable or vectorized implementation for the projections of a batch of weight vectors, i.e., a weight matrix, as will be described in the next subsection. The implementation mainly involves sorting and soft-thresholding [50]; see Algorithm 3 and its derivation in Appendix A.1 for the details. ", "page_idx": 5}, {"type": "text", "text": "MagR for Weight Matrix. In practical implementation of MagR, we preprocess the entire weight matrix $W=[{\\bf\\bar{w}}_{1},\\dots,{\\bf w}_{n}]\\in\\mathrm{~\\mathbb{R}}^{m\\times n}$ within each linear layer. For per-channel quantization (or per-column quantization in our setting), the $\\ell_{\\infty}$ penalty is imposed column-wise on the weight matrix to reduce the quantization scale of each channel. That is, MagR amounts to solving ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{W}\\in\\mathbb{R}^{m\\times n}}\\frac{1}{2}\\|\\pmb{X}\\pmb{W}-\\pmb{X}\\hat{\\pmb{W}}\\|_{\\mathrm{F}}^{2}+\\alpha\\sum_{j=1}^{n}\\|\\pmb{w}_{j}\\|_{\\infty}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In this case, we take the following iteration: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W^{k+1}=\\operatorname{prox}_{\\eta\\alpha\\parallel\\cdot\\parallel\\infty}\\left(W^{k}-\\eta\\cdot X^{\\top}X(W^{k}-\\hat{W})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the proximal operator $\\mathrm{prox}_{t\\|\\cdot\\|_{\\infty}}$ and the corresponding projection proj\u2225\u00b7 $\\lVert_{1}\\leq1$ in (4) are applied column-wise to the matrix input. Hereby we summarize MagR for processing one linear layer in Algorithm 1 with the column-wise $\\ell_{1}$ -ball projection as detailed in Algorithm 2, which generalizes Algorithm 3 in Appendix A.1, by handling matrix inputs (or batches of vectors). ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Per-channel MagR for one linear layer. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Input: Pre-trained weight matrix $\\hat{\\pmb{W}}\\in\\mathbb{R}^{m\\times n}$ ; Hessian matrix $\\pmb{H}=\\pmb{X}^{\\top}\\pmb{X}\\in\\mathbb{R}^{m\\times m}$ ; max iteration number $K$ ; step size $\\begin{array}{r}{\\eta\\stackrel{}{=}\\frac{1}{\\lambda_{\\mathrm{max}}(H)}}\\end{array}$ ; penalty parameter $\\alpha>0$ . ", "page_idx": 5}, {"type": "text", "text": "Output: Preprocessed weights $\\dot{W}\\in\\mathbb{R}^{m\\times n}$ . ", "page_idx": 5}, {"type": "text", "text": "1: Initialize $\\boldsymbol{W}^{0}=\\hat{\\boldsymbol{W}}$ .   \n2: for $k=0,\\ldots,K-1$ do   \n3: $\\begin{array}{r l}&{V^{k}=\\dot{W}^{k}-\\eta\\cdot H(W^{k}-\\hat{W})}\\\\ &{W^{k+1}=V^{k}-\\eta\\alpha\\cdot\\mathrm{proj}_{\\lVert\\cdot\\rVert_{1}\\leq1}\\left(\\frac{V^{k}}{\\eta\\alpha}\\right)}\\end{array}$   \n4: ", "page_idx": 5}, {"type": "text", "text": "gradient descent step ", "page_idx": 5}, {"type": "text", "text": "6: return $\\boldsymbol{W}=\\boldsymbol{W}^{K}$ ", "page_idx": 5}, {"type": "text", "text": "Extension to Per-Group Quantization. By using more float scaling factors, per-group quantization becomes a preferred strategy for mitigating accuracy loss at extremely low bit-widths. In this approach, a weight vector $\\mathbf{w}\\in\\mathbb{R}^{m}$ is segmented into groups of weights, each containing $d$ elements, with all weights within a group sharing a common scaling factor for quantization. Here, per-group MagR applies an $\\ell_{\\infty}$ penalty to each vector of grouped weights. Consequently, the $\\ell_{1}$ -ball projection is independently performed on these vectors, while maintaining the gradient descent step unchanged. We note that the group-wise $\\ell_{1}$ -ball projection can be easily done using Algorithm 2, with an additional reshaping of the input V \u2208Rm\u00d7n into Rd\u00d7( md \u00b7n). ", "page_idx": 5}, {"type": "text", "text": "Algorithm 2 Column-wise projection onto the unit $\\ell_{1}{\\mathrm{-}}{\\mathsf{b a l l}}$ ", "page_idx": 6}, {"type": "text", "text": "Input: Matrix $V\\in\\mathbb{R}^{m\\times n}$ ; the radius of $\\ell_{1}$ ball, $\\epsilon=1$ .   \nOutput: $W\\in\\mathbb{R}^{m\\times n}$ such that all columns $\\|\\pmb{w}_{j}\\|_{1}\\leq\\epsilon,\\forall j\\in[n]$ .   \n1: Create a binary mask $M\\in\\mathbb{R}^{m\\times n}$ filtering out the columns of $V$ with $\\lVert\\pmb{v}_{j}\\rVert_{1}\\leq\\epsilon$ . 2: Sort $|V|$ column-wise in descending order into $U$ .   \n3: Find index $\\begin{array}{r}{\\rho_{j}=\\operatorname*{max}\\Big\\{i\\in[m]:\\dot{u_{i,j}}>\\frac{1}{i}\\left(\\sum_{r=1}^{i}{u_{r,j}}-\\epsilon\\right)\\Big\\},\\forall\\,j\\in[n]}\\end{array}$   \n4: Define $\\begin{array}{r}{\\theta_{j}=\\frac{1}{\\rho_{j}}\\left(\\sum_{r=1}^{\\rho_{j}}u_{r,j}-\\epsilon\\right)\\!,\\forall j\\in[n]}\\end{array}$   \n5: Tile $\\pmb\\theta\\in\\mathbb{R}^{n}$ into $\\Theta\\in\\mathbb{R}^{m\\times n}$ along the row.   \n6: Compute $W=(1-M)\\odot V+\\bar{M}\\odot\\mathrm{sgn}(V)\\odot\\mathrm{max}\\{|V|-\\Theta,0\\}$   \n7: return W ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Overview. We tested the proposed MagR for INT4, INT3, and INT2 weight quantization. In our notations, the weight and activation bits are denoted by $\\epsilon_{\\ensuremath{\\mathrm{~W~}}}\\mathfrak{\\times}$ and $\\bullet_{\\tt A}\\,\\bullet$ , respectively. Additionally, we implemented group-wise weight quantization with the group size denoted by $^{\\epsilon}\\mathsf{g}^{,}$ . For example, W2A16g128 signifies INT2 weight and FP16 activation (i.e., INT2 weight-only quantization) with a group size of 128. ", "page_idx": 6}, {"type": "text", "text": "We employed our MagR processing approach on top of the two gradient-free PTQ methods in main text, RTN and OPTQ [16], to quantize the LLaMA1 (7B-65B) [37] and LLaMA2 (7B-70B) [38] model families. In the Appendix A.2, we extend MagR with QuIP [8] $(\\mathrm{MagR+QuIP})$ to quantize LLaMA2 (7B-70B) model families. By applying MagR on top of RTN $(\\mathbf{MagR+RTN})$ ), we achieved better results than AWQ [25] for per-channel INT3 and INT4 weight quantization. Additionally, MagR combined with OPTQ $\\scriptstyle(\\mathrm{MagR+OPTQ})$ ) achieved state-of-the-art performance for INT3 and INT4 qunatization. To enhance the per-channel INT2 quantization, we ran 30 additional iterations of coordinate descent algorithm [4, 51] on top of OPTQ, which we denote by $\\scriptstyle\\mathrm{MagR+OPTQ^{\\dag}}$ . It turns out $\\mathrm{MagR+OPTQ^{\\dag}}$ is superior to both Omniquant [36] and QuIP [8] in terms of perplexity (Table 9), and falls just short of QuIP in zero-shot tasks for 13B and 70B models (Table 4). Note that QuIP uses random orthogonal transformations (so-called Incoherence Processing) to process both the weights and features, resulting in $1.5\\times$ slower throughput than OPTQ. In contrast, MagR-based method does not introduce any overhead whatsoever compared with OPTQ. ", "page_idx": 6}, {"type": "text", "text": "In conclusion, our MagR-based PTQ method is intuitive yet effective in compressing models into extreme bit-widths, while maintaining performance without introducing any inference overhead. ", "page_idx": 6}, {"type": "text", "text": "Datasets and Evaluation. Following the previous work [16, 25, 36], we evaluate the quantized model on language generation tasks on WikiText2 [28] and C4 [33]. Additionally, we test its performance on zero-shot tasks, including PIQA [5], ARC (Easy and Challenge) [10], and Winogrande [35]. For the language generation experiments, our implement is based on the OPTQ\u2019s [16] repository, which is built using PyTorch. For executing all zero-shot tasks, we adhere to the lm-eval-harness [17]. ", "page_idx": 6}, {"type": "text", "text": "Baseline: For the language generation task, we compare our method with RTN, OPTQ [16], AWQ [25] and OmniQuant [36] on LLaMA1 and LLaMA2 models. In addition to the aforementioned methods, we also conduct a comparison with QuIP [8] on the LLaMA2-70B model. In the zero-shot task, we focus on four individual tasks and compare the average accuracy across all four tasks with Omniquant [36]. ", "page_idx": 6}, {"type": "text", "text": "Implementation details. We utilized the HuggingFace implementations of the LLaMA1 and LLaMA2 models and perform quantization on a single NVIDIA A100 GPU with 80GB of memory. Following the OPTQ method, we load one block consisting of 7 linear layers into GPU memory at a time. In line with previous work [8, 16], the input matrix $\\mathbf{\\deltaX}$ is obtained by propagating the calibration data through the quantized layers. ", "page_idx": 6}, {"type": "text", "text": "The choice of parameters. To ensure that the MagR-processed layer output $X W$ is faithful to the original $X\\hat{W}$ , we need to use a tiny penalty parameter $\\alpha$ in (2). For per-channel quantization, $\\alpha$ was fixed to be $10^{-3}$ in our experiments, but we did find that setting it to a smaller value of $5\\times10^{-4}$ or $10^{-4}$ can sometimes slightly improve the perplexity (with a relative change of $<1\\%$ in ppl). Similarly for per-group quantization, we set $\\alpha$ to $10^{-4}$ , while reducing it to $5\\times\\overline{{10^{-5}}}$ or $10^{-5}$ could sometimes also slightly improve the perplexity. An ablation study on $\\alpha$ is provided in the Appendix A.2. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Furthermore, we used a multiplicative scalar $\\beta<1$ to decay the standard quantization step $\\delta=$ max(w2)b\u2212\u22121min(w)(or equivalently, the quantization scale) of the quantizer. In other words, our \u03b4 = \u03b2 \u00b7 max(w2)b\u2212\u22121min(w). It has been shown in existing works [21, 34] that, optimal quantization step for binary or ternary quantization yielding the minimum quantization error is not given by max(w2)b\u2212\u22121min(w). Shrinking $\\delta$ at low bit-width results in a more clustered quantization grid lattice that ftis the weights better, which leads to a smaller overall error. In general, $\\beta$ is positively correlated with the bit-width used. For per-channel quantization, the best $\\beta\\,\\in\\,[0.8,0.85]$ on INT2 quantization, whereas the empirically optimal $\\beta$ is around 0.9 for INT3 quantization. As for INT4, $\\beta$ is simply set to 1, that is, we used the standard quantization step. In addition, for per-group quantization, we chose $\\beta=0.95$ for both INT2 and INT3 quantization. The ablation study of $\\beta$ is in the Appendix A.2. We observed that this refinement on the quantization step $\\delta$ significantly improves the performance of the PTQ method. In addition, the iteration number $K$ in Algorithm 1 was set to 150 across all the experiments. ", "page_idx": 7}, {"type": "table", "img_path": "UARTFgkTqW/tmp/6490bac2c742489db227439a22264c5e545f8c503f0c50c23a3e8b84dee075f6.jpg", "table_caption": ["Table 3: Perplexity of quantized LLaMA2 models on Wikitext2 and C4. We report WikiText2 and C4 perplexity in this table. LLaMA1 resutls can be found in the Appendix. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.1 Language Generation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We concentrate our analysis on perplexity-based tasks. The results for the LLaMA2 family with context length of 2048, are elaborated in Table 9, while those for LLaMA1 are provided in Appendix Table 6. As evidenced by the tables, the MagR preprocessing consistently improve the performance of the baselines RTN and OPTQ. Moreover, $\\mathbf{MagR+OPTQ}$ consistently outperforms most baseline across the LLaMA family models for both per-channel and per-group weight quantization. Particularly, for ", "page_idx": 7}, {"type": "text", "text": "INT2, $\\scriptstyle\\mathrm{MagR+OPTQ^{\\dag}}$ performs 30 additional coordinate descent (CD) iterations [4, 51] on top of OPTQ to refine the solution, surpassing all baselines. ", "page_idx": 8}, {"type": "text", "text": "Furthermore, $\\scriptstyle\\mathrm{MagR+RTN}$ achieves performance comparable to OPTQ. Notably, it outperforms AWQ by a significant margin in INT3 quantization, implying that MagR proves more effective as a preprocessing method compared to channel-wise scaling. ", "page_idx": 8}, {"type": "table", "img_path": "UARTFgkTqW/tmp/2d6465aaaa3898e6497b34b7c38664758f9a026e100d90cc37267f11a10c8418.jpg", "table_caption": ["Table 4: Multi-task results of quantized LLaMA2 models. This table reports the accuracy of 4 zero-shot tasks. Perplexity results can be found in the Appendix. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.2 Zero-Shot Tasks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We evaluated the performance of quantized models on several zero-shot tasks. The results are reported in Table 4. Similar to previous observations, the proposed MagR demonstrates superior performance on most models compared to OmniQuant, with a small gap compared to QuIP [8]. Nonetheless, it is reasonable and commendable that our algorithm achieves results close to QuIP without introducing any inference overhead. It is possible to further improve our approach based on the insight behind QuIP [8] \u2014 i.e., quantization beneftis from incoherent weight and Hessian matrices; see Table 9 for the results in the appendix. ", "page_idx": 8}, {"type": "text", "text": "5.3 Preprocessing and Quantization Runtime ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We report the execution time of MagR+RTN and MagR $^{+}$ OPTQ on a single NVIDIA A100 GPU in Table 5. For example, it typically took 0.5-7.5 hours for MagR $^{+}$ OPTQ to quantize the LlaMA2 models. We note that the integration of MagR can markedly enhance the performance of the standard OPTQ [16]. It is noted that MagR $^{+}$ OPTQ\u2020 for INT2 weight quantization requires a longer runtime due to the additional CD iterations, extending the quantization process for LLaMA2-70B to $31\\;\\mathrm{hr}.$ . It also reveals that the preprocessing overhead for quantizing the LLaMA2 models (7B-70B) amounts to approximately $15\\;\\mathrm{min}$ , $30\\,\\mathrm{min}$ , and $3.5\\,\\mathrm{hr}$ , respectively. In comparison, our total runtime is roughly half of that of the gradient-based method, OmniQuant [36], while achieving at least comparable results. Moreover, MagR introduces no post-processing step or overhead during inference. ", "page_idx": 8}, {"type": "table", "img_path": "UARTFgkTqW/tmp/497b11b7de46d4c404a371733412e5ed1c9ee67649be2f81825b57158c305812.jpg", "table_caption": ["Table 5: The runtime of $\\mathbf{MagR+RTN}$ , $\\mathbf{MagR+OPTQ}$ , and $\\mathbf{MagR+OPTQ^{\\dag}}$ on an Nvidia A100 GPU, with comparisons to their vanilla counterparts, namely, RTN and OPTQ. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Concluding Remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we proposed MagR, based on $\\ell_{\\infty}$ -regularization, to significantly reduce the maximum weight magnitude of pre-trained LLMs within each layer while preserving their output. MagR is designed to enhance the accuracy of backpropagation-free PTQ methods that use layer-wise reconstruction, such as RTN and OPTQ. MagR produces a more clustered distribution of weights and leads to a smaller quantization step, thereby facilitating the subsequent PTQ task. To solve the $\\ell_{\\infty}$ -regularization problem, we used the classical proximal gradient descent algorithm with $\\ell_{1}$ -ball projections, tailored to handle matrix variables efficiently. Our experiments on LLaMA family validated the effectiveness of the MagR approach, achieving the state-of-the-art performance on NLP tasks. Remarkably, unlike existing weight preprocessing techniques that require performing an inverse transformation on features during inference, MagR eliminates the need for post-processing and incurs no overhead. This renders MagR more practical for the deployment of quantized models. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partially supported by NSF grants DMS-2208126, DMS-2110836, IIS-2110546, CCSS-2348046, SUNY-IBM AI Research Alliance Grant, and a start-up grant from SUNY Albany. We would also like to thank SUNY Albany for providing access to the Nvidia A100 GPUs. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Harshavardhan Adepu, Zhanpeng Zeng, Li Zhang, and Vikas Singh. Framequant: Flexible low-bit quantization for transformers. arXiv preprint arXiv:2403.06082, 2024.   \n[2] Milad Alizadeh, Arash Behboodi, Mart Van Baalen, Christos Louizos, Tijmen Blankevoort, and Max Welling. Gradient $\\ell_{1}$ regularization for quantization robustness. arXiv preprint arXiv:2002.07520, 2020.   \n[3] Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, and Yuxiong He. Deepspeed inference: Enabling efficient inference of transformer models at unprecedented scale, 2022.   \n[4] Kayhan Behdin, Ayan Acharya, Aman Gupta, Sathiya Keerthi, and Rahul Mazumder. Quantease: Optimization-based quantization for language models\u2013an efficient and intuitive algorithm. arXiv preprint arXiv:2309.01885, 2023.   \n[5] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432\u20137439, 2020.   \n[6] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.   \n[7] Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos. Deep learning with low precision by half-wave gaussian quantization. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5918\u20135926, 2017.   \n[8] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M De Sa. Quip: 2-bit quantization of large language models with guarantees. In Advances in Neural Information Processing Systems, 2023.   \n[9] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085, 2018.   \n[10] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.   \n[11] Laurent Condat. Fast projection onto the simplex and the $\\ell_{1}$ ball. Mathematical Programming, 158(1):575\u2013 585, 2016.   \n[12] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. Advances in neural information processing systems, 28, 2015.   \n[13] Wojciech Czaja and Sanghoon Na. Frame quantization of neural networks. arXiv preprint arXiv:2404.08131, 2024.   \n[14] John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efficient projections onto the l 1-ball for learning in high dimensions. In Proceedings of the 25th international conference on Machine learning, pages 272\u2013279, 2008.   \n[15] Elias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning. Advances in Neural Information Processing Systems, 35:4475\u20134488, 2022.   \n[16] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Optq: Accurate quantization for generative pre-trained transformers. In The Eleventh International Conference on Learning Representations, 2022.   \n[17] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPof,i Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac\u2019h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 2023.   \n[18] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations. Journal of Machine Learning Research, 18(187):1\u201330, 2018.   \n[19] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W. Mahoney, Yakun Sophia Shao, and Amir Gholami. Full stack optimization of transformer inference: a survey, 2023.   \n[20] Arnav Kundu, Chungkuk Yoo, Srijan Mishra, Minsik Cho, and Saurabh Adya. R2 loss: Range restriction loss for model compression and quantization. arXiv preprint arXiv:2303.08253, 2023.   \n[21] Fengfu Li, Bin Liu, Xiaoxing Wang, Bo Zhang, and Junchi Yan. Ternary weight networks. arXiv preprint arXiv:1605.04711, 2016.   \n[22] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426, 2021.   \n[23] Zhijian Li, Biao Yang, Penghang Yin, Yingyong Qi, and Jack Xin. Feature affinity assisted knowledge distillation and quantization of deep neural networks on label-free data. IEEE Access, 2023.   \n[24] Chen Lin, Bo Peng, Zheyang Li, Wenming Tan, Ye Ren, Jun Xiao, and Shiliang Pu. Bit-shrinking: Limiting instantaneous sharpness for improving post-training quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16196\u201316205, 2023.   \n[25] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.   \n[26] Yuexiao Ma, Huixia Li, Xiawu Zheng, Feng Ling, Xuefeng Xiao, Rui Wang, Shilei Wen, Fei Chao, and Rongrong Ji. Affinequant: Affine transformation quantization for large language models. arXiv preprint arXiv:2403.12544, 2024.   \n[27] Johannes Maly and Rayan Saab. A simple approach for quantizing neural networks. Applied and Computational Harmonic Analysis, 66:138\u2013150, 2023.   \n[28] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.   \n[29] Jean Jacques Moreau. D\u00e9composition orthogonale d\u2019un espace hilbertien selon deux c\u00f4nes mutuellement polaires. Comptes rendus hebdomadaires des s\u00e9ances de l\u2019Acad\u00e9mie des sciences, 255:238\u2013240, 1962.   \n[30] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In International Conference on Machine Learning, pages 7197\u20137206. PMLR, 2020.   \n[31] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1325\u20131334, 2019.   \n[32] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and trends\u00ae in Optimization, 1(3):127\u2013239, 2014.   \n[33] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1\u201367, 2020.   \n[34] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In European conference on computer vision, pages 525\u2013542. Springer, 2016.   \n[35] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106, 2021.   \n[36] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language models. arXiv preprint arXiv:2308.13137, 2023.   \n[37] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[38] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[39] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks. arXiv preprint arXiv:2402.04396, 2024.   \n[40] Haoxuan Wang, Yuzhang Shang, Zhihang Yuan, Junyi Wu, and Yan Yan. Quest: Low-bit diffusion model quantization via efficient selective finetuning. arXiv preprint arXiv:2402.03666, 2024.   \n[41] Naigang Wang, Chi-Chun Charlie Liu, Swagath Venkataramani, Sanchari Sen, Chia-Yu Chen, Kaoutar El Maghraoui, Vijayalakshmi Viji Srinivasan, and Leland Chang. Deep compression of pre-trained transformer models. Advances in Neural Information Processing Systems, 35:14140\u201314154, 2022.   \n[42] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145, 2023.   \n[43] Junyi Wu, Haoxuan Wang, Yuzhang Shang, Mubarak Shah, and Yan Yan. Ptq4dit: Post-training quantization for diffusion transformers. arXiv preprint arXiv:2405.16005, 2024.   \n[44] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087\u201338099. PMLR, 2023.   \n[45] Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong He. Exploring post-training quantization in llms from comprehensive study to low rank compensation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 19377\u201319385, 2024.   \n[46] Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and Jack Xin. Understanding straight-through estimator in training activation quantized neural nets. In International Conference on Learning Representations, 2019.   \n[47] Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi, and Jack Xin. Binaryrelax: A relaxation approach for training deep neural networks with quantized weights. SIAM Journal on Imaging Sciences, 11(4):2205\u20132223, 2018.   \n[48] Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi, and Jack Xin. Blended coarse gradient descent for full quantization of deep neural networks. Research in the Mathematical Sciences, 6:1\u201323, 2019.   \n[49] Penghang Yin, Shuai Zhang, Yingyong Qi, and Jack Xin. Quantization and training of low bit-width convolutional neural networks for object detection. arXiv preprint arXiv:1612.06052, 2016.   \n[50] Wotao Yin, Stanley Osher, Donald Goldfarb, and Jerome Darbon. Bregman iterative algorithms for $\\ell_{1}$ - minimization with applications to compressed sensing. SIAM Journal on Imaging sciences, 1(1):143\u2013168, 2008.   \n[51] Aozhong Zhang, Zi Yang, Naigang Wang, Yingyong Qi, Jack Xin, Xin Li, and Penghang Yin. Comq: A backpropagation-free algorithm for post-training quantization. arXiv preprint arXiv:2403.07134, 2024.   \n[52] Jinjie Zhang and Rayan Saab. Spfq: A stochastic algorithm and its error analysis for neural network quantization. arXiv preprint arXiv:2309.10975, 2023.   \n[53] Jinjie Zhang, Yixuan Zhou, and Rayan Saab. Post-training quantization for neural networks with provable guarantees. SIAM Journal on Mathematics of Data Science, 5(2):373\u2013399, 2023.   \n[54] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Projection of Vectors Onto $\\ell_{1}$ -Ball ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we show how to compute the projection onto the unit $\\ell_{1}$ -Ball. That is, for any fixed $\\pmb{v}\\in\\mathbb{R}^{m}$ , we solve the optimization problem: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{x}\\in\\mathbb{R}^{m}}\\;\\|\\pmb{x}-\\pmb{v}\\|^{2}\\quad\\mathrm{subject\\,to}\\quad\\|\\pmb{x}\\|_{1}\\leq1.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Consider the Lagrangian $\\begin{array}{r}{\\mathcal{L}(\\pmb{x},\\lambda)=\\frac{1}{2}||\\pmb{x}-\\pmb{v}||^{2}+\\lambda(||\\pmb{x}||_{1}-1)}\\end{array}$ , where $\\lambda\\in\\mathbb{R}$ is the lagrange multiplier. Let $x^{*}$ be the optimal solution of (5), then there exits $\\lambda^{*}$ such that the following Karush\u2013Kuhn\u2013Tucker (KKT) conditions [6] hold: ", "page_idx": 13}, {"type": "text", "text": "\u2022 Stationarity: $\\begin{array}{r}{\\mathbf{0}\\in\\partial_{x}\\mathcal{L}(x^{*},\\lambda^{*})\\Leftrightarrow x^{*}=\\mathrm{sgn}(v)\\odot\\mathrm{max}\\{|v|-\\lambda^{*},0\\}}\\end{array}$ \u2022 Slackness: $\\lambda^{*}\\left(\\lvert\\lvert\\pmb{x}^{*}\\rvert\\rvert_{1}-1\\right)=0\\Leftrightarrow\\lambda^{*}=0$ or $\\|\\pmb{x}^{*}\\|_{1}=1$ . \u2022 Primal feasibility: $\\|\\pmb{x}^{*}\\|_{1}-1\\leq0\\Leftrightarrow\\|\\pmb{x}^{*}\\|_{1}\\leq1$ \u2022 Dual feasibility: $\\lambda^{*}\\geq0\\Leftrightarrow\\lambda^{*}=0$ or $\\lambda^{*}>0$ ", "page_idx": 13}, {"type": "text", "text": "where sgn is the signum function applied element-wise on vectors, i.e., ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{sgn}({\\pmb v})_{i}=\\left\\{\\begin{array}{l l}{1}&{\\mathrm{if}\\;v_{i}>0,}\\\\ {0}&{\\mathrm{if}\\;v_{i}=0,}\\\\ {-1}&{\\mathrm{if}\\;v_{i}<0.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We note that the stationarity condition: ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\pmb x}^{*}=\\mathrm{sgn}({\\pmb v})\\odot\\mathrm{max}\\{|{\\pmb v}|-\\lambda^{*},0\\}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "gives the projection of $\\pmb{v}$ onto the unit $\\ell_{1}$ ball, provided $\\lambda^{*}$ is known. Therefore, what remains is to find $\\lambda^{*}$ : ", "page_idx": 13}, {"type": "text", "text": "\u2022 Case I: $\\lambda^{*}=0$ . We have $\\boldsymbol{\\pmb{x}}^{*}=\\pmb{v}$ , which corresponds to the case that $\\lVert\\pmb{v}\\rVert_{1}\\leq1$ . \u2022 Case II: $\\lambda^{*}>0$ . The slackness condition yields $\\|\\pmb{x}^{*}\\|_{1}=1$ , i.e., $\\textstyle\\sum_{i=1}^{n}\\left|x_{i}^{*}\\right|=1$ , or equivalently, $\\begin{array}{r}{\\underset{-i=1}{\\neg{n}}\\,|\\mathrm{sgn}(v_{i})(|v_{i}|-\\lambda^{*})_{+}|=\\sum_{i=1}^{n}(|v_{i}|-\\lambda^{*})_{+}^{\\cdot}=1}\\end{array}$ with $x_{+}:=\\operatorname*{max}\\{x,0\\}$ . That is, $\\lambda^{*}$ is the root of the piece-wise linear equation: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}(|v_{i}|-\\lambda)_{+}=1,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which can be solved by sorting. ", "page_idx": 13}, {"type": "text", "text": "In summary, Algorithm 3 details the implementation of projecting $\\pmb{v}\\in\\mathbb{R}^{m}$ onto a general $\\ell_{1}$ -ball with radius $\\epsilon$ In Step 4, we specifically compute the root $\\lambda^{*}$ (or $\\theta$ ) of (6) for Case II. ", "page_idx": 13}, {"type": "text", "text": "Algorithm 3 Projection onto $\\ell_{1}$ -ball. ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Input: Vector $\\pmb{v}\\in\\mathbb{R}^{m}$ ; the radius of $\\ell_{1}$ ball, $\\epsilon=1$ .   \nOutput: $\\pmb{w}\\in\\mathbb{R}^{m}$ such that $\\lVert\\pmb{w}\\rVert_{1}\\leq\\epsilon$ .   \n1: if $\\lVert\\pmb{v}\\rVert_{1}>\\epsilon$ then   \n2: Sort $|\\boldsymbol{v}|$ into $\\pmb{\\mu}$ such that $\\mu_{1}\\geq\\mu_{2}\\geq...\\geq\\mu_{m}$ .   \n3: Find index $\\begin{array}{r}{\\rho=\\operatorname*{max}\\left\\{i\\in[m]:\\mu_{i}>\\frac{1}{i}\\left(\\sum_{r=1}^{i}\\mu_{r}-\\epsilon\\right)\\right\\}}\\end{array}$ 4: Define $\\begin{array}{r}{\\theta=\\frac{1}{\\rho}\\left(\\sum_{r=1}^{\\rho}\\dot{\\mu}_{r}-\\epsilon\\right)}\\end{array}$   \n5: Compute $\\pmb{w}^{'}=\\mathrm{sgn}(\\pmb{v})\\odot\\operatorname*{max}\\{|\\pmb{v}|-\\theta,0\\}$   \n6: else ", "page_idx": 13}, {"type": "text", "text": "7: ${\\pmb w}={\\pmb v}$ ", "page_idx": 13}, {"type": "text", "text": "8: end if ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "9: return w ", "page_idx": 13}, {"type": "text", "text": "A.2 Additional Experimental Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table 6 shows the results for WikiText2 and C4 perplexity on the LLaMA1. ", "page_idx": 13}, {"type": "table", "img_path": "UARTFgkTqW/tmp/fca3b935b74b8c8c0782f0227e1a6c10c0207c6ec20e4aaf77454533ff5299d4.jpg", "table_caption": ["Table 6: Weight-only quantization Results of WikiText2 and C4 on LLaMA1 Models. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "UARTFgkTqW/tmp/1e6f14997f45143bcac91ede116ef055accbbff9eb105eec728e281c7eabff30.jpg", "table_caption": ["Table 7: The perplexity of quantized LLaMa2-7B models for different $\\alpha$ values. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.3 Ablation Study ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Impact of the parameter $\\alpha$ . The tiny penalty parameter $\\alpha$ balances the trade-off between output discrepancy and the maximum magnitude of the weights. We carry out experiments on channel-wise quantization for differernt $\\alpha$ on LLaMA2-7B. The choice of $\\alpha$ is independent of the bit-width. As shown in Table 7, we can find that both too large and too small $\\alpha$ will lead performance degeneration. Compared to INT4, fluctuations in alpha at INT3 result in greater performance fluctuations. Fortunately, $\\alpha=0.001$ works well for all channel-wise quantization. ", "page_idx": 14}, {"type": "text", "text": "Impact of the parameter $\\beta$ . We shrink the quantization step to reduce the overall quantization error by a multiplicative scalar $\\beta$ . To investigate the influence of $\\beta$ , we experiment with different value of $\\beta$ at INT3 and INT2 channel-wise quantization. As shown in Table 8, $\\beta$ is positively correlated with the bit-width. Specifically, the best $\\beta$ is around 0.9 for INT3 quantization and for INT2 quantization the optimal $\\beta$ is around 0.8. ", "page_idx": 14}, {"type": "text", "text": "Table 8: The perplexity of quantized LLaMa2-7B models for different $\\beta$ values. ", "page_idx": 15}, {"type": "table", "img_path": "UARTFgkTqW/tmp/ad663a0b348b1b145556df3a9db350ec2ea7202986af54b8579fd1f12470fb9e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "UARTFgkTqW/tmp/d5ab63cd0c2535054fad0c655e80cc2dc2141d15042d1efec3f1e87cde0851af.jpg", "img_caption": ["Impact of MagR on Quantization Error. To explore how MagR affects quantization error, we compared the errors with and without MagR by randomly select five layers from LLaMA2 models. As illustrated in Figure 2, quantization error is notably reduced across all layers with the application of MagR. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 2: Layer-wise quantization errors (root mse) for $\\mathbf{MagR+OPTQ}$ and OPTQ, respectively, for 4-bit quantization. The layers are selected randomly for visualization, but improvement is consistent across all layers. ", "page_idx": 15}, {"type": "text", "text": "The adaptive capacity of MagR. We investigated the combined effects of MagR and QuIP. As illustrated in Table 9, incorporating MagR significantly enhances the performance of QuIP, leading to improved quantization results for the LLaMA2 models family. ", "page_idx": 15}, {"type": "table", "img_path": "UARTFgkTqW/tmp/c3153ed980a2518f2063d1e9c493dc861d60c4a4a9ad392b28c08cf100f854e9.jpg", "table_caption": ["Table 9: Perplexity of $\\mathbf{MagR+QuIP}$ for LLaMA2 models on Wikitext2 and C4. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The proposed method is shown in Section 4 and demonstrated by experiments in Section 5. They accurately reflect the contributions discussed in the abstract and introduction. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: The proposed method needs mild parameter tuning to achieve the best results. It is discussed in Section 5. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper does not include any theoretical results. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: Implementation details and parameter setup are clearly stated in Section 5. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: Please refer https://github.com/AozhongZhang/MagR Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Implementation details and parameter setup are clearly stated in Section 5. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [No] ", "page_idx": 18}, {"type": "text", "text": "Justification: In Section 5, we report the test results of the quantized models. Given the full-precision model, our algorithm produces the quantized model in a deterministic way. Hence, we don\u2019t have statistical significance to report in this paper. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Implementation details are clearly stated in Section 5. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We conform with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Our method is an efficient algorithm to compress large LLMs into low-precision. It has wide applications in saving computing resources and broaden the applications of LLMs. The impacts are discussed in Section 1 and 6. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our paper introduces a method to quantize large AI models. We don\u2019t have models or data to release. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. \u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The data and code used in this paper are open to the public. In section 5, we illustrate properly what we use. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: We do not release any new assets. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]