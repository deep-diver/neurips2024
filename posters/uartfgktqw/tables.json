[{"figure_path": "UARTFgkTqW/tables/tables_3_1.jpg", "caption": "Table 1: A comparison of perplexity (PPL) for the original pre-trained and the MagR-processed LLaMA2 models.", "description": "This table compares the perplexity scores achieved on the Wikitext2 and C4 datasets using the original pre-trained LLaMA2 models and those processed with the MagR method.  The perplexity metric indicates how well the language model predicts the next word in a sequence, with lower scores representing better performance. The table shows results for three different sizes of LLaMA2 models (7B, 13B, and 70B parameters).", "section": "4 The Proposed Method"}, {"figure_path": "UARTFgkTqW/tables/tables_4_1.jpg", "caption": "Table 2: The statistics of (approximate) fraction ranks in percentage (%) of feature matrix X across all layers of LLaMA models. All feature matrices are approximately rank-deficient with a fraction rank less than 100%. Some of them are highly low-rank with a fraction rank \u2248 1%.", "description": "This table shows the statistics of the approximate fraction ranks of feature matrices (X) across all layers in various LLaMA models.  The fraction rank is calculated as the percentage of singular values of X that are greater than 0.01 times the maximum singular value.  The table demonstrates that the feature matrices in all LLaMA models are approximately rank-deficient, with some exhibiting very low ranks.", "section": "4.1 Approximately Rank-Deficient Feature Matrix"}, {"figure_path": "UARTFgkTqW/tables/tables_7_1.jpg", "caption": "Table 1: A comparison of perplexity (PPL) for the original pre-trained and the MagR-processed LLaMA2 models.", "description": "This table compares the perplexity scores achieved on the Wikitext2 and C4 datasets using the original pre-trained LLaMA2 models (7B, 13B, and 70B parameters) and those same models after being processed by the MagR method.  Lower perplexity scores indicate better performance.", "section": "4 The Proposed Method"}, {"figure_path": "UARTFgkTqW/tables/tables_8_1.jpg", "caption": "Table 4: Multi-task results of quantized LLaMA2 models. This table reports the accuracy of 4 zero-shot tasks. Perplexity results can be found in the Appendix.", "description": "This table presents the results of  zero-shot tasks on four datasets (ARC-C, ARC-E, PIQA, Winogrande) for quantized LLaMA2 models with varying bit-widths (2, 3, 4 bits) for weights and 16 bits for activations.  It compares the performance of OmniQuant, QuIP, MagR+OPTQ, and MagR+OPTQ+.  Note that perplexity results are available in the appendix.", "section": "5.2 Zero-Shot Tasks"}, {"figure_path": "UARTFgkTqW/tables/tables_9_1.jpg", "caption": "Table 5: The runtime of MagR+RTN, MagR+OPTQ, and MagR+OPTQ\u2020 on an Nvidia A100 GPU, with comparisons to their vanilla counterparts, namely, RTN and OPTQ.", "description": "This table shows the runtime comparison of different quantization methods on three different sizes of LLAMA2 models.  It compares the original RTN and OPTQ methods to their enhanced versions that incorporate the MagR pre-processing technique.  It also shows the additional time required when using MagR+OPTQ\u2020, which includes extra coordinate descent iterations for further optimization.  The table highlights that while MagR adds pre-processing time, its inference time overhead is negligible. ", "section": "5 Experiments"}, {"figure_path": "UARTFgkTqW/tables/tables_14_1.jpg", "caption": "Table 3: Perplexity of quantized LLaMA2 models on Wikitext2 and C4. We report WikiText2 and C4 perplexity in this table. LLaMA1 resutls can be found in the Appendix.", "description": "This table presents a comparison of the perplexity scores achieved by different quantization methods (OPTQ, OmniQuant, QuIP, MagR+OPTQ, etc.) on the LLaMA2 model family (7B, 13B, and 70B parameters) for different bit-widths (W2A16, W3A16, W4A16) and group sizes (g128).  The perplexity is a measure of how well the model predicts a sequence of words, with lower scores indicating better performance.  The results are broken down by dataset (Wikitext2 and C4) and model size, allowing for a detailed performance comparison across various settings and methods.", "section": "5 Experiments"}, {"figure_path": "UARTFgkTqW/tables/tables_14_2.jpg", "caption": "Table 7: The perplexity of quantized LLaMa2-7B models for different \u03b1 values.", "description": "This table shows the perplexity scores on the WikiText2 and C4 datasets for different values of the regularization parameter \u03b1,  with weight and activation bit-widths of 4/16 and 3/16. Lower perplexity indicates better performance. The results demonstrate the impact of \u03b1 on the model's performance after applying the MagR preprocessing technique.", "section": "5 Experiments"}, {"figure_path": "UARTFgkTqW/tables/tables_15_1.jpg", "caption": "Table 8: The perplexity of quantized LLaMa2-7B models for different \u03b2 values.", "description": "This table presents the perplexity results on WikiText2 and C4 datasets for the quantized LLaMa2-7B model with different values of the beta (\u03b2) parameter. Beta is used as a multiplicative scalar to decay the standard quantization step in the quantizer.  The table shows how varying the \u03b2 parameter affects the model's performance across different bit-widths (3-bit and 2-bit) for weight-only quantization.", "section": "5 Experiments"}, {"figure_path": "UARTFgkTqW/tables/tables_15_2.jpg", "caption": "Table 9: Perplexity of MagR+QuIP for LLAMA2 models on Wikitext2 and C4.", "description": "This table presents the perplexity scores achieved by QuIP and MagR+QuIP on the WikiText2 and C4 datasets, using various bit-width settings for weights (W2, W3, W4) while keeping activations at 16 bits.  It demonstrates the impact of incorporating MagR into the QuIP algorithm on model performance across different model sizes (7B and 13B parameters). Lower perplexity indicates better performance.", "section": "5.1 Language Generation"}]