[{"figure_path": "sn3UrYRItk/tables/tables_18_1.jpg", "caption": "Table B.1: GLUE tasks with ROBERTa. For our experiments with RoBERTa models, finetuned on GLUE tasks, we use the following setup:", "description": "This table details the hyperparameters and settings used for training RoBERTa-Large models on GLUE tasks using the LoRA method.  It specifies the model, learning rates, beta values for AdamW optimizer, epsilon, learning rate schedule, weight decay, training batch size, number of epochs, LoRA rank, alpha, dropout, target modules, sequence length, random seeds, and precision.  These parameters define the specific configurations of the experiments conducted in the study.", "section": "B.1.2 GLUE tasks with ROBERTa"}, {"figure_path": "sn3UrYRItk/tables/tables_18_2.jpg", "caption": "LORA Hyperparameters", "description": "This table shows the hyperparameters used for the Low-Rank Adaptation (LoRA) method in the experiments described in the paper.  The LoRA rank determines the dimensionality of the low-rank approximation used to adapt the model parameters.  LoRA \u03b1 is a scaling factor. LoRA Dropout refers to the dropout rate used during training to regularize the model.  Finally, Target Modules specifies which modules of the model are adapted using the LoRA technique.", "section": "4.1 GLUE tasks with ROBERTa"}, {"figure_path": "sn3UrYRItk/tables/tables_18_3.jpg", "caption": "Table 1 LoRA Hyperparameters", "description": "This table lists the hyperparameters used for the Low-Rank Adaptation (LoRA) experiments in the paper.  It details settings like the rank of the LoRA matrices, the alpha scaling factor, dropout rate, and which modules were targeted for adaptation. These settings are crucial to the parameter efficiency of LoRA and influence its performance.", "section": "4.1 GLUE tasks with ROBERTa"}, {"figure_path": "sn3UrYRItk/tables/tables_19_1.jpg", "caption": "Table B.1: Training Algorithm Details", "description": "This table details the training algorithm used for the experiments, specifically focusing on hyperparameters like learning rates, betas for AdamW optimizer, epsilon value, learning rate schedule, weight decay, batch size, and number of epochs.  These settings are consistent across various model and dataset combinations, except where otherwise specified in the paper.", "section": "B.1.3 TinyLlama WikiText-2"}, {"figure_path": "sn3UrYRItk/tables/tables_19_2.jpg", "caption": "LORA Hyperparameters", "description": "This table lists the hyperparameters used for the Low-Rank Adaptation (LoRA) method in the experiments.  It specifies the rank of the low-rank matrices (LoRA Rank), the scaling factor for the rank decomposition (LoRA \u03b1), the dropout rate applied to the LoRA layers (LoRA Dropout), and the specific modules within the language model that the LoRA adaptation is applied to (Target Modules).  These settings are crucial for controlling the tradeoff between model performance and the number of trainable parameters.", "section": "4.2 Llama"}, {"figure_path": "sn3UrYRItk/tables/tables_19_3.jpg", "caption": "Table 1: Hyperparameters for the experiments with TinyLlama model on WikiText-2.", "description": "This table lists the hyperparameters used in the experiments involving the TinyLlama model on the WikiText-2 dataset.  It details the settings for the training algorithm (AdamW), including learning rates, beta values, epsilon, learning rate schedule, weight decay, batch size, number of epochs, and LORA parameters (rank, alpha, dropout, and target modules).  Additionally, it provides hyperparameters for other aspects of the experiment, including sequence length, the number of random seeds used, and the precision (BF16).", "section": "B.1.3 TinyLlama WikiText-2"}, {"figure_path": "sn3UrYRItk/tables/tables_20_1.jpg", "caption": "Training Algorithm Details", "description": "This table details the hyperparameters used for training the Llama-7b model on the GSM8k dataset.  It specifies the range of learning rates tested, the beta parameters for AdamW optimizer, epsilon value, learning rate schedule, weight decay, training batch size, and the number of epochs.", "section": "B.1.5 Llama-7b GSM8k"}, {"figure_path": "sn3UrYRItk/tables/tables_20_2.jpg", "caption": "Table 1 LORA Hyperparameters", "description": "This table shows the hyperparameters used for the Low-Rank Adaptation (LoRA) experiments in the paper.  It lists the LoRA rank, the scaling factor alpha, the dropout rate, and the target modules that were updated during the training.  These settings are crucial for the parameter-efficient fine-tuning of large language models, as detailed in the paper.", "section": "4.2 Llama"}, {"figure_path": "sn3UrYRItk/tables/tables_20_3.jpg", "caption": "Table 1: LORA Hyperparameters", "description": "This table lists the hyperparameters used for the Low-Rank Adaptation (LoRA) experiments in the paper.  It includes details like the rank of the LoRA matrices, the scaling factor, dropout rate, and which modules were targeted for adaptation.", "section": "4 Experiments with Language Models"}, {"figure_path": "sn3UrYRItk/tables/tables_21_1.jpg", "caption": "Table B.1: Training Algorithm Details", "description": "This table shows the hyperparameters used for training the Llama-7b model on the GSM8k dataset.  It lists the learning rates, beta1, beta2, epsilon values used in the AdamW optimizer, the learning rate schedule, weight decay, training batch size, and the number of epochs.", "section": "B.1.5 Llama-7b GSM8k"}, {"figure_path": "sn3UrYRItk/tables/tables_21_2.jpg", "caption": "Table 1 LoRA Hyperparameters", "description": "This table shows the hyperparameters used for the Low-Rank Adaptation (LoRA) experiments in the paper.  The LoRA rank specifies the dimensionality reduction applied to the weight matrices.  LoRA \u03b1 is a scaling factor for the update of the LoRA matrices. LoRA Dropout refers to the dropout rate used during training of the LoRA weights. Finally, Target Modules specifies the layers within the model to which LoRA is applied.", "section": "4.2 Llama"}, {"figure_path": "sn3UrYRItk/tables/tables_21_3.jpg", "caption": "Table 2: Other Hyperparameters", "description": "This table lists the hyperparameters used in the experiments described in section 4.1 of the paper.  These hyperparameters were used for fine-tuning the RoBERTa-large model on GLUE tasks using LORA. The table includes details about sequence length, random seeds, and precision settings.", "section": "4.1 GLUE tasks with ROBERTa"}]