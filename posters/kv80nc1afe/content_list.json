[{"type": "text", "text": "Adaptive Passive-Aggressive Framework for Online Regression with Side Information ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Runhao Shi, Jiaxi Ying, Daniel P. Palomar The Hong Kong University of Science and Technology {rshiaf, jx.ying}@connect.ust.hk, palomar@ust.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The Passive-Aggressive (PA) method is widely used in online regression problems for handling large-scale streaming data, typically updating model parameters in a passive-aggressive manner based on whether the error exceeds a predefined threshold. However, this approach struggles with determining optimal thresholds and adapting to complex scenarios with side information, where tracking accuracy is not the sole metric in the regression model. To address these challenges, we introduce a novel adaptive framework that allows finer adjustments to the weight vector in PA using side information. This framework adaptively selects the threshold parameter in PA, theoretically ensuring convergence to the optimal setting. Additionally, we present an efficient implementation of our algorithm that significantly reduces computational complexity. Numerical experiments show that our model achieves outstanding performance associated with the side information while maintaining low tracking error, demonstrating marked improvements over traditional PA methods across various scenarios. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Online learning techniques, initially introduced by Zinkevich (2003), have gained significant popularity due to their robustness in adversarial environments and efficiency in processing large streaming data (Shalev-Shwartz et al., 2012; Orabona, 2019; Hazan, 2022). In the online learning framework, an online player continuously makes decisions and receives corresponding losses, aiming to minimize regret. Regret, in this context, refers to the worst-case discrepancy in performance compared to the best-fixed decision in hindsight, measuring the overhead of identifying the best-fixed decision. ", "page_idx": 0}, {"type": "text", "text": "These techniques have found widespread application in modeling regression problems for streaming data, enabling practical applications across various fields (Herbster, 2001; Crammer et al., 2006; Shalev-Shwartz and Ben-David, 2014). They are extensively applied in diverse domains such as portfolio selection (Li et al., 2012; Li and Hoi, 2012), malicious URL detection (Ma et al., 2009; Zhao and Hoi, 2013), and time series prediction (Anava et al., 2013, 2015; Hazan et al., 2018; Lale et al., 2020; Tsiamis and Pappas, 2022; Zhang et al., 2024). Without relying on strong assumptions, online regression models demonstrate robustness with regret guarantees in challenging scenarios. Furthermore, their incremental learning schemes make them highly adaptable to streaming data, eliminating the need to retrain the entire dataset and resulting in significant efficiency advantages. ", "page_idx": 0}, {"type": "text", "text": "One well-known online regression method is the Passive-Aggressive (PA) algorithm (Crammer et al., 2006). PA employs a passive-aggressive updating scheme to learn a weight vector for linear regression problems. It passively maintains the previous weight below a certain threshold and aggressively updates the weight when the loss exceeds the threshold. However, determining an appropriate threshold can be challenging. A small threshold prioritizes real-time tracking accuracy but may lead to overftiting and sensitivity to noise, compromising long-term tracking accuracy. Additionally, the selected weight may impact factors beyond accuracy in practical model performance. When additional metrics and side information are available for evaluating performance, PA may struggle to achieve a more nuanced weight selection. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address the aforementioned challenges, we propose an Adaptive Passive-Aggressive online regression framework with Side information (APAS) to achieve the following objectives: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Novel APAS framework: We introduce a novel APAS framework that integrates side information into PA to enhance weight evaluation and selection. This framework adaptively selects the threshold parameter in PA, enabling it to achieve outstanding performance associated with the side information while maintaining a low tracking error. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Efficient algorithm: We develop an efficient algorithm using the successive convex approximation (SCA, Scutari et al., 2013) to accelerate the computation of APAS. This algorithm rapidly converges to the optimal point, allowing flexibility in selecting measurements to integrate side information. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Regret bound: We derive an $O({\\sqrt{T}})$ regret bound for our APAS framework for non-convex loss functions, ensuring the robustness and effectiveness of APAS theoretically. This regret bound matches the optimal regret bound for non-convex loss functions. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Extensive experiments: We conduct an enhanced index tracking task on both synthetic and real financial datasets to validate the effectiveness and efficiency of APAS, which demonstrates the impressive performance of APAS in achieving high returns while maintaining small tracking errors. ", "page_idx": 1}, {"type": "text", "text": "Notation: Matrices and vectors are represented by bold letters. $[T]$ denotes the set $\\{1,2,\\ldots,T\\}$ . The weight vector at time $t$ is denoted by $\\mathbf{w}_{t}\\in\\mathcal{W}$ . The instance and target in an online regression problem are denoted by $\\mathbf{x}_{t}\\,\\in\\,\\mathbb{R}^{N}$ and $y_{t}\\in\\mathbb{R}$ , respectively. The proximal operator and Moreau envelope associated with $\\lambda h$ are denoted as $\\mathbf{prox}_{\\lambda h}$ and $M_{\\lambda h}$ , respectively. The Euclidean projection of vector $\\mathbf{u}\\in\\mathbb{R}^{N}$ onto the set $\\mathcal{W}$ is denoted by $\\begin{array}{r}{\\Pi_{\\mathcal{W}}(\\mathbf{u})=\\arg\\operatorname*{min}_{\\mathbf{w}\\in\\mathcal{W}}||\\mathbf{w}-\\mathbf{u}||_{2}^{2}}\\end{array}$ . For a continuous function $f(x)$ , the set of subderivatives at point $a$ is denoted as $\\partial f(a)$ . The left derivative at $a$ is denoted by \u2202\u2212f(a) = limx\u2192a\u2212 $\\begin{array}{r}{\\partial_{-}f(a)=\\operatorname*{lim}_{x\\to a^{-}}\\frac{f(x)-f(a)}{x-a}}\\end{array}$ . The derivative at point $a$ , if it exists, is denoted as $f^{\\prime}(a)$ . ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Online Learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Online learning is a mathematical framework designed to address optimization problems where objective functions change over time. In this context, an online learner sequentially makes decisions $b_{t}$ based on historical loss and receives a new loss $f_{t}(\\boldsymbol{b}_{t})$ after making the decision. The performance of an online learning algorithm is evaluated using the concept of regret $R_{T}$ , which quantifies the discrepancy between the algorithm\u2019s performance and that of an optimal static parameter setting: ", "page_idx": 1}, {"type": "equation", "text": "$$\nR_{T}=\\sum_{t=1}^{T}f_{t}(b_{t})-\\operatorname*{min}_{b\\in\\mathcal{X}}\\left(\\sum_{t=1}^{T}f_{t}(b)\\right),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathcal{X}$ denotes the feasible set. An online learning strategy converges to the optimal static parameter setting if $R_{T}=o(T)$ , indicating the average performance gap diminishes as the number of iterations $T$ approaches infinity. In the case of convex loss functions, different regularization functions can be employed to achieve various optimal regret bounds, depending on the assumptions about the curve of the loss function (Zinkevich, 2003; Hazan et al., 2007; Hazan and Seshadhri, 2007, 2009). Adaptive regularization methods, which select the regularization term dynamically, have also been proposed and widely adopted in various domains (Duchi et al., 2010, 2011; Van Erven and Koolen, 2016). ", "page_idx": 1}, {"type": "text", "text": "2.2 Passive-Aggressive Method ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The Passive-Aggressive (PA) method is a popular online algorithm utilized for regression problems involving streaming data (Crammer et al., 2006). In an online regression problem, we receive an instance $\\mathbf{x}_{t}\\,\\in\\,\\mathbb{R}^{N}$ and predict the target value $\\mathbf{w}_{t}^{\\mathsf{T}}\\mathbf{x}_{t}$ using the incrementally learned vector ${\\bf w}_{t}$ , where the ground truth is $y_{t}$ . PA predicts the next weight vector by solving the following optimization problem: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\boldsymbol{\\widehat{\\mathbf{w}}}_{t+1}=\\underset{\\mathbf{w}\\in\\mathbb{R}^{N}}{\\arg\\operatorname*{min}}\\ \\frac{1}{2}\\|\\mathbf{w}-\\mathbf{w}_{t}\\|_{2}^{2}\\qquad\\mathrm{subject}\\;\\mathrm{to}\\quad\\ell_{\\varepsilon}(\\mathbf{w};(\\mathbf{x}_{t},y_{t}))=0,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\ell_{\\varepsilon}$ is the $\\varepsilon$ -insensitive hinge loss function defined as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\ell_{\\varepsilon}\\left(\\mathbf{w};(\\mathbf{x},y)\\right)=\\left\\{\\mathop{0}_{\\left|\\mathbf{w}^{\\mathsf{T}}\\mathbf{x}-y\\right|-\\varepsilon}\\quad\\mathrm{~otherwise}.\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Intuitively, PA performs an aggressive update when the discrepancy between the predicted value and the ground truth exceeds the threshold $\\varepsilon$ , and passively maintains the previous weight when the discrepancy is within the threshold $\\varepsilon$ . A smaller threshold may prioritize real-time tracking accuracy but could result in overfitting and compromise long-term performance. Therefore, the selection of the threshold $\\varepsilon$ significantly influences the performance. Additionally, relying solely on tracking accuracy without considering side information may limit the method\u2019s potential performance. ", "page_idx": 2}, {"type": "text", "text": "3 Proposed Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present a novel framework that incorporates the side information into PA for evaluating and selecting weight vector $\\mathbf{w}_{t+1}$ and threshold $\\varepsilon$ . This framework adaptively selects select $\\varepsilon$ by balancing real-time tracking accuracy and side performance, achieving performance comparable to the optimal parameter setting, supported by theoretical regret guarantees. Additionally, we propose an efficient method based on the successive convex approximation technique, which significantly reduces time complexity and accelerates computation. ", "page_idx": 2}, {"type": "text", "text": "3.1 PAS Framework ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present a novel Passive-Aggressive with Side information (PAS) framework that considers the trade-off between real-time tracking accuracy and side performance. PAS builds upon two variations of PA, each providing closed-form solutions for a given value of $\\varepsilon$ , without imposing any constraints as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)=\\left\\{\\mathbf{w}_{t}\\mathbf{\\Sigma}\\leq\\varepsilon,\\qquad\\qquad\\qquad\\qquad\\qquad\\quad|\\mathbf{w}_{t}^{\\mathsf{T}}\\mathbf{x}_{t}-y_{t}|\\leq\\varepsilon,\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\tau_{t}=\\left\\{\\left(\\left|\\mathbf{w}_{t}^{\\mathsf{T}}\\mathbf{x}_{t}-y_{t}\\right|-\\varepsilon\\right)/\\|\\mathbf{x}_{t}\\|_{2}^{2}\\right.\\;}\\\\ {\\left.\\left(\\left|\\mathbf{w}_{t}^{\\mathsf{T}}\\mathbf{x}_{t}-y_{t}\\right|-\\varepsilon\\right)/\\left(\\left\\|\\mathbf{x}_{t}\\right\\|_{2}^{2}+\\frac{1}{2C}\\right)\\right.}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and PA-II refers to a robust PA method with an aggressiveness constant $C$ . For the regression problem with constraints, the final weight is determined by performing a projection onto the feasible set $\\mathcal{W}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{w}_{t+1}(\\varepsilon)=\\underset{\\mathbf{w}\\in\\mathcal{W}}{\\arg\\operatorname*{min}}\\|\\mathbf{w}-\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Although Crammer et al. (2006) does not include a projection operation, we demonstrate in Appendix D that PA with lazy projection still achieves a comparable bound to Crammer et al. (2006). ", "page_idx": 2}, {"type": "text", "text": "Suppose that at each round $t$ , we have a lower semi-continuous convex function $h_{t}(\\mathbf{w})$ that quantifies the performance associated with the side information. To leverage this information and enhance the performance of the weight selection, we integrate $h_{t}(\\mathbf{w})$ into the projection step of the PA method and propose the PAS framework for selecting the next weight vector: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{w}_{t+1}(\\varepsilon)=\\mathop{\\underset{\\mathbf{w}\\in\\mathcal{W}}{\\mathrm{arg}\\,\\mathrm{min}}}_{\\mathbf{w}\\in\\mathcal{W}}\\left(h_{t}(\\mathbf{w})+\\frac{1}{2\\lambda}{\\left\\|\\mathbf{w}-\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)\\right\\|_{2}^{2}}\\right)=\\mathrm{prox}_{\\lambda h_{t}}\\left(\\widehat{\\mathbf{w}}_{t+1}\\left(\\varepsilon\\right)\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathrm{prox}_{\\lambda h_{t}}$ denotes the proximal operator. In PAS, $\\lambda$ serves as the trade-off parameter that quantifies the preference between tracking accuracy and side performance. When $h_{t}(\\mathbf{w})$ is set as a constant, the PAS model essentially simplifies to the original PA method with lazy projection, as shown in Equation (4). By leveraging the proximal operator, we can explicitly integrate side performance into the weight selection process by modifying $h_{t}(\\mathbf{w})$ . ", "page_idx": 2}, {"type": "text", "text": "From another perspective, $\\|\\mathbf{w}-\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)\\|_{2}^{2}$ can be viewed as a regularization term that passively aligns with the trend of the ground truth. In contrast, $h_{t}(\\mathbf{w})$ serves as the primary loss measurement, acting as the main driver for aggressively updating the weight vector. To understand how the weight vector $\\mathbf{w}_{t+1}(\\boldsymbol{\\varepsilon})$ is selected, we discuss the following two scenarios: ", "page_idx": 2}, {"type": "image", "img_path": "kV80nC1afE/tmp/bcfffce7f2e526a2f485d97ba7c7fa36974bb54a43c0a9f5a84de0667e54a2bf.jpg", "img_caption": ["Figure 1: Adaptive learning scheme of APAS. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "\u2022 If $|\\mathbf{w}_{t}^{\\mathsf{T}}\\mathbf{x}_{t}-y_{t}|\\,\\leq\\,\\varepsilon$ , we have $\\begin{array}{r}{\\mathbf{w}_{t+1}(\\varepsilon)=\\arg\\operatorname*{min}_{\\mathbf{w}\\in\\mathcal{W}}\\;\\big(h_{t}(\\mathbf{w})+\\frac{1}{2\\lambda}\\|\\mathbf{w}-\\mathbf{w}_{t}\\|_{2}^{2}\\big)}\\end{array}$ . This implies that we aim to passively maintain the same weight setting as the previous round while aggressively updating the weight to improve side performance for small real-time tracking errors. \u2022 If $|\\mathbf{w}_{t}^{\\mathsf{T}}\\mathbf{x}_{t}\\;-\\;y_{t}|\\;>\\;\\;\\varepsilon$ , we have $\\begin{array}{r l r}{{\\bf w}_{t+1}(\\varepsilon)}&{=}&{\\arg\\operatorname*{min}_{{\\bf w}\\in\\mathcal{W}}\\left(h_{t}({\\bf w})\\;+\\;\\frac{1}{2\\lambda}\\|{\\bf w}\\;-\\;{\\bf w}_{t}\\|_{2}^{2}\\;-\\right.}\\end{array}$ $\\begin{array}{r}{\\frac{1}{\\lambda}\\mathbf{w}^{\\top}\\mathrm{sign}\\left[y_{t}-\\mathbf{w}_{t}^{\\top}\\mathbf{x}_{t}\\right]\\tau_{t}\\mathbf{x}_{t}+\\mathrm{const}\\right)}\\end{array}$ . This implies that we aim to passively maintain the same weight setting as the previous round while aggressively updating the weight to improve real-time tracking accuracy and side performance for large real-time tracking errors. ", "page_idx": 3}, {"type": "text", "text": "This framework ensures that while the selected weight passively follows the general trend of the data through the $\\ell_{2}$ -norm, it actively seeks to improve performance based on the side information, thus achieving a balance between stability and adaptability. Consequently, $\\mathbf{w}_{t+1}(\\boldsymbol{\\varepsilon})$ corresponds to the point that defines the infimum of the trade-off between side performance $h_{t}$ and real-time tracking accuracy. The infimum is essentially the Moreau Envelope (Parikh et al., 2014), which we define as the loss function with respect to $\\varepsilon$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{t}(\\varepsilon)=\\operatorname*{inf}_{\\mathbf{w}\\in\\mathcal{W}}\\left[h_{t}(\\mathbf{w})+\\frac{1}{2\\lambda}\\|\\mathbf{w}-\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)\\|_{2}^{2}\\right]=M_{\\lambda h_{t}}\\left(\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, ${M}_{\\lambda{h}_{t}}$ represents the Moreau Envelope of $\\lambda h_{t}$ with respect to $\\lambda\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)$ . In this way, we establish a connection between the determined weight vector $\\mathbf{w}_{t+1}(\\boldsymbol{\\varepsilon})$ and loss function $f_{t}(\\varepsilon)$ with $\\varepsilon$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Adaptive PAS ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The parameter $\\varepsilon$ is a crucial component in PAS, as it determines the weight selection $\\mathbf{w}_{t+1}(\\boldsymbol{\\varepsilon})$ and the performance evaluation $f_{t}(\\varepsilon)$ . While the trade-off parameter $\\lambda$ has an intuitive interpretation, the process of setting $\\varepsilon$ is less straightforward. A smaller threshold $\\varepsilon$ may prioritize real-time tracking accuracy but could lead to overftiting, affecting long-term accuracy and compromising side performance. Conversely, a larger $\\varepsilon$ might stabilize performance but result in underftiting. Hence, our objective is to develop an adaptive algorithm that can dynamically choose the value of $\\varepsilon$ based on the designed loss function $f_{t}(\\varepsilon)$ . To facilitate the dynamic selection of $\\varepsilon$ , we introduce the following assumptions: ", "page_idx": 3}, {"type": "text", "text": "Assumption 1. The feasible domain $\\mathcal{D}$ of the parameter $\\varepsilon$ is bounded with $\\mathcal{D}=[\\nu,D]$ and $\\nu>0$ .   \nAssumption 2. The subderivatives of $f_{t}(\\varepsilon)$ is bounded, such that $\\operatorname*{sup}_{\\varepsilon\\in{\\mathcal{D}},t\\in[T]}|\\partial f_{t}(\\varepsilon)|\\leq G.$ . ", "page_idx": 3}, {"type": "text", "text": "Our proposed adaptive parameter updating scheme for $\\mathbf{w}_{t+1}$ and $\\varepsilon_{t+1}$ is as follows: At each round $t$ , we receive information up to time $t$ and use it to select the next weight vector $\\mathbf{w}_{t+1}(\\varepsilon_{t})$ with $\\varepsilon_{t}$ . Subsequently, we update $\\varepsilon_{t+1}$ based on the loss $f_{t}(\\varepsilon_{t})$ . The overall procedure is illustrated in Figure 1. Under Assumptions 1 and 2, the updating rule for $\\varepsilon_{t+1}$ is formulated as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\varepsilon_{t+1}=\\Pi_{\\mathcal{D}}\\left[\\varepsilon_{t}-\\eta_{t}\\tilde{g}_{t}(\\varepsilon_{t})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Pi_{\\mathcal{D}}[\\varepsilon]=\\operatorname*{min}\\left\\{\\operatorname*{max}\\left\\{\\varepsilon,\\nu\\right\\},D\\right\\}$ , $\\begin{array}{r}{\\eta_{t}=\\frac{\\zeta_{t}\\sqrt{D}}{G\\sqrt{\\nu t}}}\\end{array}$ , and $\\zeta_{t}=\\Pi_{\\mathcal{D}}\\left[|{\\bf w}_{t}^{\\sf T}{\\bf x}_{t}-y_{t}|\\right]$ . Here, $\\tilde{g}_{t}(\\varepsilon)$ is a modified derivative of $f_{t}(\\varepsilon)$ , which is defined as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{g}_{t}(\\varepsilon):=\\left\\{\\!\\!\\!\\begin{array}{l l}{f_{t}^{\\prime}(\\varepsilon)\\quad}&{\\mathrm{if}\\;\\varepsilon<\\zeta_{t},}\\\\ {\\operatorname*{max}\\{0,\\partial_{-}f_{t}(\\zeta_{t})\\}\\quad}&{\\mathrm{otherwise}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Since $\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)$ is a continuous piecewise function with respect to $\\varepsilon$ , being affine on $\\varepsilon<\\zeta_{t}$ and constant otherwise, and $M_{\\lambda h_{t}}\\left(\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)\\right)$ is a strongly convex function, their composition $f_{t}(\\varepsilon)$ becomes a piecewise-convex function. Thus, $f_{t}(\\varepsilon)$ is differentiable and strongly convex for $\\varepsilon<\\zeta_{t}$ , and constant otherwise. The derivative of $f_{t}(\\varepsilon)$ for $\\varepsilon<\\zeta_{t}$ can be calculated using the chain rule: ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{t}^{\\prime}(\\varepsilon)=\\frac{\\partial M_{\\lambda h_{t}}\\left(\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)\\right)}{\\partial\\varepsilon}=\\frac{\\partial M_{\\lambda h_{t}}\\left(\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)\\right)}{\\partial\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)}\\frac{\\partial\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)}{\\partial\\varepsilon}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The derivative of the Moreau Envelope ${M}_{\\lambda{h}_{t}}$ with respect to $\\widehat{\\bf w}_{t+1}(\\varepsilon)$ can be calculated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\partial M_{\\lambda h_{t}}\\left(\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)\\right)}{\\partial\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)}=\\frac{1}{\\lambda}\\left(\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)-\\mathrm{prox}_{\\lambda h_{t}}\\left(\\widehat{\\mathbf{w}}_{t+1}\\left(\\varepsilon\\right)\\right)\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To summarize, the overall updating scheme for $\\varepsilon_{t+1}$ and weight vector $\\mathbf{w}_{t+1}$ is outlined in Algorithm 1. This adaptive mechanism enables the framework to adjust dynamically to changing environments, eliminating the need for a manually set static threshold. Intuitively, the update process for $\\varepsilon$ works as follows: If the real-time tracking error is lower than $\\varepsilon_{t}$ (i.e., $\\zeta_{t}\\leq\\varepsilon_{t}$ ), then according to Equation (8), the derivative $\\tilde{g}_{t}\\big(\\varepsilon_{t}\\big)\\geq0$ . Based on the update rule in Equation (7), this suggests that $\\varepsilon_{t}$ is reduced to avoid underestimating the tracking accuracy. Conversely, when the real-time tracking error exceeds $\\varepsilon_{t}$ , the update mechanism adjusts $\\varepsilon_{t}$ to strike a balance between minimizing side loss and maintaining tracking accuracy. ", "page_idx": 4}, {"type": "table", "img_path": "kV80nC1afE/tmp/766c9baa150cce02cb5079771622368eebf1d9453cbdb11ed0d061778c685600.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.3 Efficient Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Algorithm 1 requires the calculation of the proximal operator at each iteration (see Line 5), which can be computationally expensive. While this problem can be addressed directly using the Interior Point Method (IPM) with an off-the-shelf solver (Nemirovski, 2004), it generally incurs a high-order time complexity of ${\\cal O}(N^{3.5})$ , making it inefficient for large-scale problems. ", "page_idx": 4}, {"type": "text", "text": "To improve efficiency, we propose an algorithm that utilizes the Successive Convex Approximation (SCA) framework to accelerate computation (Scutari et al., 2013). SCA reduces time complexity by iteratively optimizing a more manageable surrogate function in place of the original objective function until convergence is reached (Sun et al., 2016; Scutari and Sun, 2018). We denote the objective function of Problem (5) as $\\begin{array}{r}{u_{t}(\\mathbf{w})=h_{t}(\\mathbf{w})+\\frac{1}{2\\lambda}\\|\\mathbf{w}-\\widehat{\\mathbf{w}}_{t+1}(\\boldsymbol{\\varepsilon})\\|_{2}^{2}}\\end{array}$ . To apply SCA, the surrogate function, denoted as $\\tilde{u}_{t}(\\mathbf{w}\\mid\\mathbf{w}^{k})$ , should be strongly convex and satisfy the condition that $\\nabla\\tilde{u}_{t}(\\mathbf{\\bar{w}}^{k}\\,|\\,\\mathbf{w}^{k})=\\nabla u_{t}(\\mathbf{w}^{k})$ , ensuring the gradients match at $\\mathbf{w}^{k}$ . ", "page_idx": 4}, {"type": "text", "text": "We employ the first-order Taylor expansion to approximate $h_{t}(\\mathbf{w})$ , defining the surrogate function $\\tilde{u}_{t}(\\mathbf{w}\\mid\\mathbf{w}^{k})$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{u}_{t}(\\mathbf{w}\\,|\\,\\mathbf{w}^{k})=\\frac{1}{2\\lambda}\\mathbf{w}^{\\mathsf{T}}\\mathbf{w}-\\left(\\frac{1}{\\lambda}\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)-\\nabla h_{t}(\\mathbf{w}^{k})\\right)^{\\mathsf{T}}\\mathbf{w}+\\mathrm{const.}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "By simplifying the formulation, we iteratively optimize the following surrogate problem instead: ", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{q}^{k}=\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)-\\lambda\\nabla h_{t}(\\mathbf{w}^{k})$ . When the feasible set $\\mathcal{W}$ exhibits special geometric properties, such as being a probability simplex or a hyperplane, the optimization problem in Equation (11) admits a closed-form solution, as provided in Appendix C (Palomar and Fonollosa, 2005; Duchi et al., 2008). ", "page_idx": 5}, {"type": "table", "img_path": "kV80nC1afE/tmp/dca9dc5c853d87b7829e4c656ce0964d1a81ad9232c3af7cf052f6e5df4bcc3a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "The overall procedure for efficiently calculating (5) is encapsulated in Algorithm 2. Empirically, this method converges very quickly, reaching the optimal point within only a few iterations. Additionally, it does not require calculating the objective value of $h_{t}(\\mathbf{w})$ , making it more flexible for incorporating side information. By setting $\\gamma^{k+1}=\\gamma^{k}(1-\\rho\\gamma^{k})$ with $\\rho\\,\\in\\,(0,1)$ and $\\gamma^{0}<1/\\rho$ , Algorithm 2 guarantees convergence to the optimal point of Problem (5). This convergence behavior is analyzed in Proposition 1, with the proof provided in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Proposition 1. With $\\gamma^{k}\\,\\in\\,(0,1].$ , $\\gamma^{k}\\,\\rightarrow\\,0$ and $\\textstyle\\sum_{k}\\gamma^{k}=+\\infty$ , Algorithm 2 converges in a finite number of iterations to an optimal solution of (5) or every limit point of the sequence $\\{\\mathbf{w}^{k}\\}_{k=1}^{\\infty}$ (at least one such point exists) is an optimal solution of (5). ", "page_idx": 5}, {"type": "text", "text": "3.4 Regret Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The loss function $f_{t}(\\varepsilon)$ in the APAS framework is piecewise convex, leading to a scenario where it is generally non-convex and non-smooth. In g\u221aeneral convex online learning settings, optimal regret bounds are well-established, typically $\\bar{O(\\sqrt{T})}$ for $T$ iterations. However, achieving these optimal regret bounds in non-convex online learning scenarios poses significant challenges due to the inherent difficulties in optimizing non-convex functions. Strategies to address these challenges often involve either working with a restricted class of loss functions or focusing on a computationally feasible notion of regret (Hazan et al., 2017; Gao et al., 2018). Additionally, some approaches dealing with general non-convex losses rely on access to sampling oracles, which are impractical in many real-world applications (Maillard and Munos, 2010; Krichene et al., 2015; Agarwal et al., 2019; Suggala and Netrapalli, 2020). Despite recent advances, obtaining optimal regret bounds in non-convex settings remains an open and active area of research. ", "page_idx": 5}, {"type": "text", "text": "In our work, we demonstrate that Algorithm 1 can achieve the optimal $O({\\sqrt{T}})$ regret bound. Our approach is novel in that it does not rely on restrictive assumptions or oracles. Instead, it leverages the properties of the function curve and quasi-convexity, as detailed in Proposition 3 and Proposition 4 in Appendix A. Although $f_{t}(\\varepsilon)$ is non-convex and non-smooth, its behavior along the function curve enables us to derive favorable regret bounds, achieved by carefully designing the learning rate $\\eta_{t}$ and the updating rule for $\\varepsilon_{t+1}$ . The following theorem formalizes the regret bound of our approach: ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Under Assumptions $^{\\,l}$ and 2, Algorithm 1 achieves the following regret bound for $T\\geq1$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nR_{T}=\\sum_{t=1}^{T}f_{t}(\\varepsilon_{t})-\\operatorname*{min}_{\\varepsilon\\in\\mathcal{D}}\\sum_{t=1}^{T}f_{t}(\\varepsilon)\\leq2\\sqrt{\\frac{D^{3}G^{2}}{\\nu}}\\sqrt{T}=O(\\sqrt{T}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $D,\\,\\nu,$ , and $G$ are constants defined in Assumptions $^{\\,I}$ and 2. ", "page_idx": 5}, {"type": "text", "text": "The proof of Theorem 2 is provided in Appendix A. Theorem 2 ensures that Algorithm 1 achieves performance that is comparable to the optimal parameter setting over the long term. Crucially, our approach does not depend on restrictive assumptions or external oracles. Instead, it dynamically adjusts the parameter $\\varepsilon$ by responding to real-time changes, allowing for optimal performance in both stable and volatile environments. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To demonstrate the performance of our proposed methods, we conduct experiments using stock lists from S&P 500 and NASDAQ 100 from Yahoo! FinanceTM for an enhanced index-tracking task. Enhanced index tracking is a passive portfolio selection strategy that aims to enhance returns by incorporating tactical tilts towards specific styles, while still maintaining a portfolio that closely mirrors an index (Dose and Cincotti, 2005; Benidis et al., 2017, 2018; Xu et al., 2022). ", "page_idx": 6}, {"type": "text", "text": "In our experiments, the instance $\\mathbf{x}_{t}$ represents the stock return at time $t$ , where $\\boldsymbol{x}_{t,i}\\quad=$ $(p_{t,i}-p_{t-1,i})/p_{t-1,i}$ with $p_{t,i}$ denoting the price of asset $i$ at time $t$ . The target value $y_{t}$ is the index return at time $t$ . In the enhanced index tracking task, we sequentially select the portfolio weight ${\\bf w}_{t}$ at each iteration to mimic the trend of the index $y_{t}$ , where the feasible set is the probability simplex $\\mathcal{W}=\\left\\{\\mathbf{w}\\in\\mathbb{R}^{N}\\mid\\mathbf{1}^{\\mathsf{T}}\\mathbf{w}=1,\\mathbf{w}\\succeq0\\right\\}$ . To achieve a higher return, rather than merely tracking the index, we define the side information as the negative log return, i.e., $h_{t}(\\mathbf{w})=-\\log(1+\\mathbf{x}_{t}^{\\sf T}\\mathbf{w})$ . ", "page_idx": 6}, {"type": "text", "text": "We measure the performance of different methods using tracking error and excess cumulative return. The tracking error is quantified by the magnitude of the daily tracking error (MDTE), computed by: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{Tracking\\;Error}=\\frac{1}{T}\\sqrt{\\sum_{t=1}^{T}\\left(\\mathbf{w}_{t}^{\\mathsf{T}}\\mathbf{x}_{t}-y_{t}\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The excess cumulative return is used to assess the performance relative to the tracking index, which represents the discrepancy between the logarithmic cumulative return of the strategy and the index: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname{Excess}\\operatorname{Cumulative}\\operatorname{Return}=\\sum_{t=1}^{T}\\log\\left({1+\\mathbf{w}_{t}^{\\mathsf{T}}\\mathbf{x}_{t}}\\right)-\\sum_{t=1}^{T}\\log\\left({1+y_{t}}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Benchmark: In addition to the base model PA, we compare the performance with two versions of SLAIT: SLAIT-ETE and SLAIT-DR (Benidis et al., 2017). SLAIT-ETE focuses on tracking accuracy, while SLAIT-DR aims to replicate the index while avoiding excessively large drawdowns. ", "page_idx": 6}, {"type": "text", "text": "4.1 Synthetic Data Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We generate synthetic data by sampling $\\mathbf{x}_{t}\\sim\\mathcal{N}({\\pmb{\\mu}},{\\pmb{\\Sigma}})$ , where $\\pmb{\\mu}\\in\\mathbb{R}^{N}$ and $\\boldsymbol{\\Sigma}\\in\\mathbb{R}^{N\\times N}$ are the sample mean and sample covariance matrix calculated from the real market data from the S&P 500. The corresponding index value is generated by: ", "page_idx": 6}, {"type": "equation", "text": "$$\ny_{t}=\\mathbf{x}_{t}^{\\mathsf{T}}\\mathbf{w}^{\\star}+\\omega,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\omega\\sim\\mathcal{N}(0,\\delta^{2})$ represents Gaussian noise, and $\\mathbf{w}^{\\star}$ is the true weight of the index components. We generate 50 datasets to test the average performance of different methods, with each dataset containing $T=200$ observations and $N=100$ dimensions. The training set consists of $50\\%$ of the data, while the test set contains the remaining $50\\%$ . Both SLAIT-ETE and SLAIT-DR use a rolling training window of 100-day observations, rebalanced every 3 days. ", "page_idx": 6}, {"type": "text", "text": "Figure 2 presents the performance comparison and ablation experiments of the proposed APAS framework against benchmarks on the synthetic dataset. Specifically, Figure 2a illustrates the comparison of excess return and tracking error for APAS and the benchmarks, where the curve for APAS is generated by varying the trade-off parameter $\\lambda$ . For small $\\lambda$ , APAS exhibits relatively low tracking error, while for large $\\lambda$ , APAS achieves higher returns with a slight sacrifice in accuracy. Compared to the benchmarks, APAS demonstrates higher excess cumulative return for the same level of tracking error and lower tracking error for the same level of excess cumulative return. ", "page_idx": 6}, {"type": "text", "text": "Figure 2a also shows how varying the trade-off parameter $\\lambda$ affects the balance between side performance (measured as excess cumulative return) and tracking error. Generally, $\\lambda$ can be selected based on the specific problem\u2019s considerations, such as the magnitude of side information and the desired balance between minimizing tracking error and maximizing side performance. In practice, $\\lambda$ can be determined using domain knowledge and cross-validation. For example, if a specific range of tracking error is desired, the bisection method can be employed during cross-validation to identify the value of $\\lambda$ that maximizes side performance while meeting the tracking error requirement. ", "page_idx": 6}, {"type": "text", "text": "Figure 2b compares the performance of the fixed parameter setting with the adaptive one, where PAS refers to the non-adaptive version of APAS with fixed $\\varepsilon$ . The closer the curve is to the top left, the better the performance. Even without knowing the optimal parameter setting for $\\varepsilon$ , the adaptive $\\varepsilon$ updating scheme in APAS ensures relatively good performance. ", "page_idx": 6}, {"type": "image", "img_path": "kV80nC1afE/tmp/bfb8d7c59520058c65f2fb62e4edb9425d711523ab9a706d727a9edf1261111f.jpg", "img_caption": ["Figure 2: Comparison of tracking error and excess cumulative return on the synthetic dataset. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "kV80nC1afE/tmp/f14ee3a04606b1f9630b309540444ec4e108d6d9ab2e033f9d497bda6ec45483.jpg", "img_caption": ["Figure 3: Tracking error and excess cumulative return over time $T$ for different methods on the synthetic dataset. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We also compare the trends of tracking error and excess cumulative return over time $T$ in Figure 3. This figure shows that both the PA method and the proposed APAS method exhibit relatively low tracking error. Although the PA method has the minimum tracking error, it achieves the lowest excess cumulative return among all methods. In comparison, the APAS method maintains a comparably low tracking error but with a significantly higher excess cumulative return. ", "page_idx": 7}, {"type": "text", "text": "It is widely acknowledged that heavy-tailed distributions offer a more realistic model for datagenerating processes in financial markets compared to Gaussian distributions (Cardoso et al., 2021, 2022). To further evaluate the performance of APAS in highly volatile and noisy environments, we include a detailed comparison of our proposed methods under various data and noise distributions, available in Appendix E. ", "page_idx": 7}, {"type": "text", "text": "4.2 Real Market Data Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct simulations on two well-known indices using real market data from Yahoo! FinanceTM: the S&P 500 Index and the NASDAQ 100 Index. For the S&P 500 Index, we collect data from 2021-01-01 to 2023-01-01, totaling $T\\,=\\,503$ daily observations with $N\\,=\\,453$ stocks. For the NASDAQ 100 Index, we collect data from 2019-01-01 to 2021-01-01, also totaling $T=503$ daily observations with $N=101$ stocks. For the PA and APAS methods, $50\\%$ of the data is used for training, with weights updated adaptively each day based on the latest data. For the SLAIT-ETE and SLAIT-DR methods, the training lookback period is $50\\%$ of the data, with rebalancing occurring every 10 days. ", "page_idx": 7}, {"type": "text", "text": "Figures 4 and 5 show the performance comparison on the S&P 500 and NASDAQ 100 datasets, respectively. As observed, with a small $\\lambda$ setting, APAS has a comparable tracking error to PA while yielding a better excess cumulative return. With a large $\\lambda$ setting, APAS exhibits a higher tracking error but achieves the best excess cumulative return among all methods. The real market comparisons across different datasets demonstrate that the proposed APAS model provides a superior trade-off between tracking error and excess cumulative return compared to the benchmarks. ", "page_idx": 8}, {"type": "image", "img_path": "kV80nC1afE/tmp/fc6f8764bc4626608c0d0ca7cd3e6bff626ebf53abbf2db46a02efa2aeaa114a.jpg", "img_caption": ["Figure 4: Tracking error and excess cumulative return over time $T$ for different methods on S&P 500 dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "kV80nC1afE/tmp/843fe19e3400a37ec3d3b79fae2e974fe68bb8f0fb08822402ab24c3b7bc7273.jpg", "img_caption": ["Figure 5: Tracking error and excess cumulative return over time $T$ for different methods on NASDAQ 100 dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 Speed Comparison of Acceleration Schemes ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This section evaluates the computational efficiency of our proposed method (Algorithm 2) in Section 3.3 across different problem dimensions $N$ . The benchmarks include the widely-used convex problem solver CVXR Fu et al. (2020), Projected Gradient Descent (PGD), and Alternating Direction Method of Multipliers (ADMM, Boyd et al., 2011). ", "page_idx": 8}, {"type": "text", "text": "We assess the performance of the proposed method over 100 randomized trials, comparing the convergence speed and CPU time (in seconds), as shown in Figure 6. The left panel of Figure 6 illustrates the average convergence gap versus the number of iterations on a dataset with $N=1000$ dimensions, comparing the proposed method with PGD and ADMM. The right panel displays the average CPU time for each method across different problem dimensions $N$ . The results demonstrate that our method converges rapidly to the optimal point, being nearly 100 times faster than CVXR and ADMM and 10 times faster than PGD for high-dimensional data. ", "page_idx": 8}, {"type": "text", "text": "To further assess whether time complexity is affected by including different types of side information, we conduct additional experiments using various forms of side information beyond the log return $h_{t}(\\mathbf{w})=-\\log(1+\\mathbf{r}_{t}^{\\sf T}\\mathbf{w})$ , such as: ", "page_idx": 8}, {"type": "text", "text": "\u2022 Switching cost: $h_{t}(\\mathbf{w})=||\\mathbf{w}-\\mathbf{w}_{t}||_{1}$ ;   \n\u2022 Weighted $\\ell_{1}$ norm: $\\begin{array}{r}{h_{t}(\\mathbf{w})=\\sum_{i=1}^{N}\\rho_{i}|w_{i}|}\\end{array}$ ;   \n\u2022 Group Lasso: $\\begin{array}{r}{h_{t}(\\mathbf{w})=\\sum_{i=1}^{m}\\rho_{i}||w_{|\\mathcal{G}_{i}}||_{2}}\\end{array}$ , where $\\mathcal{G}_{i},\\ldots,\\mathcal{G}_{m}$ are $m$ disjoint groups. ", "page_idx": 9}, {"type": "text", "text": "We evaluate the performance of the proposed efficient method with different types of side information functions over 100 randomized trials, comparing the average CPU time (in seconds) in Table 1. From Table 1, it appears that group Lasso incurs higher CPU times, especially for larger dimensions $N$ , due to the added complexity of calculating norms for disjoint groups. In general, while the type of side information can impact the computational time, the APAS framework maintains efficiency across different scenarios. ", "page_idx": 9}, {"type": "image", "img_path": "kV80nC1afE/tmp/a4606c7214cec5ee4dac8b48ac7ccc21d7ef0a5fd2162f54c2cc63e19d15d425.jpg", "img_caption": ["Figure 6: Average convergence speed and CPU time comparison of 100 randomized trials on $N$ - dimensional datasets of Algorithm 2. "], "img_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "kV80nC1afE/tmp/8aa2118362f7d26fad6a681f29be72b0c7014483f75aa98610467393fb8f59e3.jpg", "table_caption": [], "table_footnote": ["Table 1: Average CPU time (in seconds) for different side information functions over 100 randomized trials of Algorithm 2. "], "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we addressed the limitations of the Passive-Aggressive (PA) algorithm in online regression, particularly in determining the appropriate threshold and integrating side information for weight selection. To tackle these issues, we proposed the APAS framework, which incorporates side information into PA. Our APAS framework adaptively selects the threshold parameter, enabling it to leverage side information for improved performance while mainta\u221aining a low tracking error. We demonstrated the robustness and effectiveness of APAS through an $O({\\sqrt{T}})$ regret bound, even with non-convex loss functions. Additionally, we developed an efficient algorithm that significantly reduced computational complexity without compromising theoretical performance guarantees. Comprehensive experiments on synthetic and real market datasets validated the effectiveness and efficiency of APAS, highlighting its practical applicability across various scenarios. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank the anonymous reviewers for their helpful comments. This work was supported by the Hong Kong GRF 16206123 research grant and the Hong Kong RGC Postdoctoral Fellowship Scheme of Project No. PDFS2425-6S05. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Agarwal, N., Gonen, A., and Hazan, E. (2019). Learning in non-convex games with an optimization oracle. In Conference on Learning Theory, pages 18\u201329. PMLR.   \nAnava, O., Hazan, E., Mannor, S., and Shamir, O. (2013). Online learning for time series prediction. In Conference on Learning Theory, pages 172\u2013184. PMLR.   \nAnava, O., Hazan, E., and Zeevi, A. (2015). Online time series prediction with missing data. In International Conference on Machine Learning, pages 2191\u20132199. PMLR.   \nBenidis, K., Feng, Y., and Palomar, D. P. (2017). Sparse portfolios for high-dimensional financial index tracking. IEEE Transactions on Signal Processing, 66(1):155\u2013170.   \nBenidis, K., Feng, Y., Palomar, D. P., et al. (2018). Optimization methods for financial index tracking: From theory to practice. Foundations and Trends\u00ae in Optimization, 3(3):171\u2013279.   \nBoyd, S., Parikh, N., Chu, E., Peleato, B., Eckstein, J., et al. (2011). Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends\u00ae in Machine learning, 3(1):1\u2013122.   \nCardoso, J. V. d. M., Ying, J., and Palomar, D. P. (2021). Graphical models in heavy-tailed markets. In Advances in Neural Information Processing Systems, volume 34, pages 19989\u201320001.   \nCardoso, J. V. d. M., Ying, J., and Palomar, D. P. (2022). Learning bipartite graphs: Heavy tails and multiple components. In Advances in Neural Information Processing Systems, volume 35, pages 14044\u201314057.   \nCrammer, K., Dekel, O., Keshet, J., Shalev-Shwartz, S., and Singer, Y. (2006). Online passiveaggressive algorithms. Journal of Machine Learning Research, 7(19):551\u2013585.   \nDose, C. and Cincotti, S. (2005). Clustering of financial time series with application to index and enhanced index tracking portfolio. Physica A: Statistical Mechanics and its Applications, 355(1):145\u2013151.   \nDuchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(7).   \nDuchi, J., Shalev-Shwartz, S., Singer, Y., and Chandra, T. (2008). Efficient projections onto the $\\ell_{1}$ -ball for learning in high dimensions. In International Conference on Machine Learning, pages 272\u2013279.   \nDuchi, J. C., Shalev-Shwartz, S., Singer, Y., and Tewari, A. (2010). Composite objective mirror descent. In Conference on Learning Theory, volume 10, pages 14\u201326.   \nFu, A., Narasimhan, B., and Boyd, S. (2020). CVXR: An R package for disciplined convex optimization. Journal of Statistical Software, 94(14):1\u201334.   \nGao, X., Li, X., and Zhang, S. (2018). Online learning with non-convex losses and non-stationary regret. In International Conference on Artificial Intelligence and Statistics, pages 235\u2013243. PMLR.   \nHazan, E. (2022). Introduction to online convex optimization. MIT Press.   \nHazan, E., Agarwal, A., and Kale, S. (2007). Logarithmic regret algorithms for online convex optimization. Machine Learning, 69(2):169\u2013192.   \nHazan, E., Lee, H., Singh, K., Zhang, C., and Zhang, Y. (2018). Spectral filtering for general linear dynamical systems. Advances in Neural Information Processing Systems, 31.   \nHazan, E. and Seshadhri, C. (2007). Adaptive algorithms for online decision problems. In Electronic Colloquium on Computational Complexity, volume 14.   \nHazan, E. and Seshadhri, C. (2009). Efficient learning algorithms for changing environments. In International Conference on Machine Learning, pages 393\u2013400.   \nHazan, E., Singh, K., and Zhang, C. (2017). Efficient regret minimization in non-convex games. In International Conference on Machine Learning, pages 1433\u20131441. PMLR.   \nHerbster, M. (2001). Learning additive models online with fast evaluating kernels. In Computational Learning Theory: 14th Annual Conference on Computational Learning Theory, COLT 2001 and 5th European Conference on Computational Learning Theory, EuroCOLT 2001 Amsterdam, The Netherlands, July 16\u201319, 2001 Proceedings 14, pages 444\u2013460. Springer.   \nKrichene, W., Balandat, M., Tomlin, C., and Bayen, A. (2015). The hedge algorithm on a continuum. In International Conference on Machine Learning, pages 824\u2013832. PMLR.   \nLale, S., Azizzadenesheli, K., Hassibi, B., and Anandkumar, A. (2020). Logarithmic regret bound in partially observable linear dynamical systems. Advances in Neural Information Processing Systems, 33:20876\u201320888.   \nLi, B. and Hoi, S. C. (2012). On-line portfolio selection with moving average reversion. In International Conference on Machine Learning, pages 563\u2013570.   \nLi, B., Zhao, P., Hoi, S. C., and Gopalkrishnan, V. (2012). PAMR: Passive aggressive mean reversion strategy for portfolio selection. Machine Learning, 87:221\u2013258.   \nMa, J., Saul, L. K., Savage, S., and Voelker, G. M. (2009). Identifying suspicious urls: an application of large-scale online learning. In International Conference on Machine Learning, pages 681\u2013688.   \nMaillard, O.-A. and Munos, R. (2010). Online learning in adversarial lipschitz environments. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 305\u2013320. Springer.   \nNemirovski, A. (2004). Interior point polynomial time methods in convex programming. Lecture notes, 42(16):3215\u20133224.   \nOrabona, F. (2019). A modern introduction to online learning. arXiv preprint arXiv:1912.13213.   \nPalomar, D. P. and Fonollosa, J. R. (2005). Practical algorithms for a family of waterfliling solutions. IEEE Transactions on Signal Processing, 53(2):686\u2013695.   \nParikh, N., Boyd, S., et al. (2014). Proximal algorithms. Foundations and trends\u00ae in Optimization, 1(3):127\u2013239.   \nScutari, G., Facchinei, F., Song, P., Palomar, D. P., and Pang, J.-S. (2013). Decomposition by partial linearization: Parallel optimization of multi-agent systems. IEEE Transactions on Signal Processing, 62(3):641\u2013656.   \nScutari, G. and Sun, Y. (2018). Parallel and distributed successive convex approximation methods for big-data optimization. Lecture Notes in Mathematics, C.I.M.E, Springer Verlag series.   \nShalev-Shwartz, S. and Ben-David, S. (2014). Understanding machine learning: from theory to algorithms. Cambridge University Press, USA.   \nShalev-Shwartz, S. et al. (2012). Online learning and online convex optimization. Foundations and Trends\u00ae in Machine Learning, 4(2):107\u2013194.   \nSuggala, A. S. and Netrapalli, P. (2020). Online non-convex learning: Following the perturbed leader is optimal. In Algorithmic Learning Theory, pages 845\u2013861. PMLR.   \nSun, Y., Babu, P., and Palomar, D. P. (2016). Majorization-minimization algorithms in signal processing, communications, and machine learning. IEEE Transactions on Signal Processing, 65(3):794\u2013816.   \nTsiamis, A. and Pappas, G. J. (2022). Online learning of the kalman filter with logarithmic regret. IEEE Transactions on Automatic Control, 68(5):2774\u20132789.   \nVan Erven, T. and Koolen, W. M. (2016). Metagrad: Multiple learning rates in online learning. Advances in Neural Information Processing Systems, 29.   \nXu, F., Ma, J., and Lu, H. (2022). Group sparse enhanced indexation model with adaptive beta value. Quantitative Finance, 22(10):1905\u20131926.   \nZhang, Z., Cutkosky, A., and Paschalidis, Y. (2024). Unconstrained dynamic regret via sparse coding. Advances in Neural Information Processing Systems, 36.   \nZhao, P. and Hoi, S. C. (2013). Cost-sensitive online active learning with application to malicious url detection. In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 919\u2013927.   \nZinkevich, M. (2003). Online convex programming and generalized infinitesimal gradient ascent. In International Conference on Machine Learning, pages 928\u2013936. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In the following sections, we present the theoretical proofs for Theorem 2 and Proposition 1. Additionally, we provide closed-form solutions for Algorithm 2 under special cases not explicitly stated in the main manuscript, along with a detailed specification of the regret bound analysis for the PassiveAggressive (PA) method with lazy projection. Furthermore, we include additional experiments to assess the robustness of the proposed APAS framework under various conditions. ", "page_idx": 13}, {"type": "text", "text": "A Proof of Theorem 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The proof of Theorem 2 relies on the first order bound of $f_{t}(\\varepsilon)$ , shown in the following proposition. Proposition 3. Under Assumption 1 and 2, $f_{t}(\\varepsilon)$ is quasi-convex on $\\mathcal{D}=[\\nu,D]$ . With the definition of $\\tilde{g}_{t}(\\varepsilon)$ in Equation (8) and $\\zeta_{t}=\\Pi_{\\mathcal{D}}\\,\\left[|{\\bf w}_{t}^{\\sf T}{\\bf x}_{t}-y_{t}|\\right]\\mathrm{,}$ , for all $t\\in[T]$ and all $v,u\\in\\mathcal{D}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nf_{t}(v)-f_{t}(u)\\leq\\tilde{g}_{t}(v)(v-\\tilde{u}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\tilde{u}=\\operatorname*{min}\\{u,\\zeta_{t}\\}$ . ", "page_idx": 13}, {"type": "text", "text": "The proof of Proposition 3 is detailed in Appendix A.1. Let $\\varepsilon^{\\star}\\in$ arg $\\begin{array}{r}{\\operatorname*{min}_{\\varepsilon\\in{\\mathcal D}}\\sum_{t=1}^{T}f_{t}(\\varepsilon)}\\end{array}$ . According to Proposition 3, we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\nf_{t}(\\varepsilon_{t})-f_{t}(\\varepsilon^{\\star})\\le\\tilde{g}_{t}(\\varepsilon_{t})(\\varepsilon_{t}-z_{t}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $z_{t}=\\operatorname*{min}\\left\\lbrace\\zeta_{t},\\varepsilon^{\\star}\\right\\rbrace$ . Since $\\boldsymbol{\\varepsilon}_{t+1}=\\Pi_{\\mathcal{D}}\\left[\\boldsymbol{\\varepsilon}_{t}-\\eta_{t}\\tilde{g}_{t}\\big(\\boldsymbol{\\varepsilon}_{t}\\big)\\right]$ and employing the Pythagorean theorem, we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(\\varepsilon_{t+1}-z_{t}\\right)^{2}=\\left(\\Pi_{\\mathcal{D}}\\left[\\varepsilon_{t}-\\eta_{t}\\tilde{g}_{t}(\\varepsilon_{t})\\right]-z_{t}\\right)^{2}\\leq\\left(\\varepsilon_{t}-\\eta_{t}\\tilde{g}_{t}(\\varepsilon_{t})-z_{t}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By properly reformulating the inequality, we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\tilde{g}_{t}(\\varepsilon_{t})(\\varepsilon_{t}-z_{t})\\leq\\phi_{t}(z_{t})-\\psi_{t}(z_{t})+\\frac{\\eta_{t}G^{2}}{2},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\begin{array}{r}{\\phi_{t}(z_{t})=\\frac{\\varepsilon_{t}^{2}-2\\varepsilon_{t}z_{t}}{2\\eta_{t}}}\\end{array}$ and $\\begin{array}{r}{\\psi_{t}(z_{t})=\\frac{\\varepsilon_{t+1}^{2}-2\\varepsilon_{t+1}z_{t}}{2\\eta_{t}}}\\end{array}$ \u03b5t+1\u221222\u03b7\u03b5t+1zt. Summing from t = 1 to T, we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle R_{T}=\\sum_{t=1}^{T}\\left(f_{t}\\left(\\varepsilon_{t}\\right)-f_{t}\\left(\\varepsilon^{\\star}\\right)\\right)}}\\\\ {{\\displaystyle\\qquad\\leq\\sum_{t=1}^{T}\\left(\\phi_{t}(z_{t})-\\psi_{t}(z_{t})+\\frac{\\eta_{t}G^{2}}{2}\\right)}}\\\\ {{\\displaystyle\\qquad=\\phi_{1}(z_{1})-\\psi_{T}(z_{T})+\\sum_{t=2}^{T}\\left(\\phi_{t}(z_{t})-\\psi_{t-1}(z_{t-1})\\right)+\\frac{G^{2}}{2}\\sum_{t=1}^{T}\\eta_{t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thus, we only need to bound $\\phi_{t}(z_{t})-\\psi_{t-1}(z_{t-1})$ . ", "page_idx": 13}, {"type": "text", "text": "Proposition 4. Set $\\begin{array}{r}{\\eta_{t}\\,=\\,\\frac{\\zeta_{t}\\sqrt{D}}{G\\sqrt{\\nu t}}}\\end{array}$ with $\\zeta_{t}\\,=\\,\\Pi_{\\mathcal{D}}\\,\\left[|{\\bf w}_{t}^{\\sf T}{\\bf x}_{t}-y_{t}|\\right]$ . Under Assumptions $^{\\,l}$ and 2, for $\\begin{array}{r}{\\varepsilon^{\\star}\\in a r g\\ m i n_{\\varepsilon\\in\\mathcal D}\\sum_{t=1}^{T}f_{t}(\\varepsilon)}\\end{array}$ , and $z_{t}=\\operatorname*{min}\\{\\zeta_{t},\\varepsilon^{\\star}\\}$ , we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\phi_{t}(z_{t})-\\psi_{t-1}(z_{t-1})\\leq\\frac{D^{2}}{2}\\left(\\frac{1}{\\eta_{t}}-\\frac{1}{\\eta_{t-1}}\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\begin{array}{r}{\\phi_{t}(z_{t})=\\frac{\\varepsilon_{t}^{2}-2\\varepsilon_{t}z_{t}}{2\\eta_{t}}}\\end{array}$ and $\\begin{array}{r}{\\psi_{t}(z_{t})=\\frac{\\varepsilon_{t+1}^{2}-2\\varepsilon_{t+1}z_{t}}{2\\eta_{t}}}\\end{array}$ ", "page_idx": 13}, {"type": "text", "text": "The proof for Proposition 4 is detailed in Appendix A.2. Based on Proposition 4, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nR_{T}\\leq\\frac{D^{2}}{\\eta_{T}}+\\frac{G^{2}}{2}\\sum_{t=1}^{T}\\eta_{t}\\leq2\\sqrt{\\frac{D^{3}G^{2}}{\\nu}}\\sqrt{T}=O(\\sqrt{T}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.1 Proof of Proposition 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. First we show that $f_{t}(\\varepsilon)$ is quasi-convex on $\\mathcal{D}$ . The loss function $f_{t}(\\varepsilon)$ is the Moreau Envelop of $\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)$ , which is given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{t}(\\varepsilon)=M_{\\lambda h_{t}}\\left(\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)\\right)=\\operatorname*{inf}_{\\mathbf{w}\\in\\mathcal{W}}\\left[h_{t}(\\mathbf{w})+\\frac{1}{2\\lambda}\\|\\mathbf{w}-\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)\\|_{2}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, $M_{\\lambda h_{t}}\\left(\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)\\right)$ is strongly convex and smooth with respect to $\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)$ . Furthermore, $\\widehat{\\bf w}_{t+1}(\\varepsilon)$ is a piecewise continuous affine function of $\\varepsilon$ , as shown in Equation (2). It is constant if $\\varepsilon\\ \\geq$ $|\\mathbf{w}_{t}^{\\mathsf{T}}\\dot{\\mathbf{x}}_{t}-y_{t}|$ and an affine function of $\\varepsilon$ otherwise. Since $f_{t}(\\varepsilon)$ is a composite function of the strongly convex function $M_{\\lambda h_{t}}\\left(\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)\\right)$ and the piecewise continuous affine function $\\widehat{\\mathbf{w}}_{t+1}(\\varepsilon)$ , we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{t}(\\varepsilon)=\\left\\{{\\begin{array}{l l}{{\\mathrm{strongly~convex~function~}}}&{\\ \\varepsilon\\in[\\nu,\\zeta_{t})}\\\\ {{\\mathrm{const}}}&{\\ \\varepsilon\\in[\\zeta_{t},D],}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\zeta_{t}=\\Pi_{\\mathcal{D}}\\left[|\\mathbf{w}_{t}^{\\mathsf{T}}\\mathbf{x}_{t}-y_{t}|\\right]$ . Thus, it is straightforward to verify that $f_{t}(\\varepsilon)$ is quasi-convex. ", "page_idx": 14}, {"type": "text", "text": "To verify the inequality (15), we analyze different cases. First, we consider the simplest case where $\\zeta_{t}=\\nu$ , which implies that $|\\mathbf{w}_{t}^{\\mathsf{T}}\\mathbf{x}_{t}-\\dot{y}_{t}|\\leq\\nu$ and $f_{t}(\\varepsilon)$ is a constant on $\\mathcal{D}=[\\nu,D]$ . Since $\\tilde{g}_{t}(\\varepsilon)\\geq0$ according to (8) and $\\tilde{u}=\\nu$ , it is straightforward to verify that: ", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{t}(v)-f_{t}(u)=0\\leq\\tilde{g}_{t}(v)(v-\\tilde{u}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, we consider the case where $\\zeta_{t}=D$ , which implies that $|\\mathbf{w}_{t}^{\\mathsf{T}}\\mathbf{x}_{t}-y_{t}|\\geq D$ and $f_{t}(\\varepsilon)$ is strongly convex on $\\mathcal{D}=[\\nu,D]$ . Here, $\\tilde{u}=u$ , and we consider the following cases: ", "page_idx": 14}, {"type": "text", "text": "1. For $v<\\zeta_{t}$ , we have $\\tilde{g}_{t}(v)=f_{t}^{\\prime}(v)$ , and thus, by convexity: ", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{t}(v)-f_{t}(u)\\leq f_{t}^{\\prime}(v)(v-u)=\\tilde{g}_{t}(v)(v-\\tilde{u}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "2. For $v=\\zeta_{t}$ : if $\\partial_{-}f_{t}(v)\\geq0$ , then $\\tilde{g}_{t}(v)=\\partial_{-}f_{t}(v)$ , and by convexity: ", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{t}(v)-f_{t}(u)\\leq\\partial_{-}f_{t}(v)(v-u)=\\tilde{g}_{t}(v)(v-\\tilde{u}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "If $\\partial_{-}f_{t}(v)<0$ , then $\\tilde{g}_{t}(v)=0$ , and we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{t}(v)-f_{t}(u)\\leq\\partial_{-}f_{t}(v)(v-u)\\leq0=\\tilde{g}_{t}(v)(v-\\tilde{u}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "image", "img_path": "kV80nC1afE/tmp/483acd4d4a8d14c55afab303d3e8a9bd7d074ba2fc8428d0e0ad95d8e630d4e4.jpg", "img_caption": ["Figure 7: Illustration for curves of $f_{t}(\\varepsilon)$ with $\\nu<\\zeta_{t}<D$ . "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Next, we consider the case where $\\nu<\\zeta_{t}<D$ , meaning that $\\zeta_{t}=|\\mathbf{w}_{t}^{\\mathsf{T}}\\mathbf{x}_{t}-y_{t}|$ . Figure 7 illustrates the curve of $f_{t}(\\varepsilon)$ . The curve of the loss function $\\tilde{f}_{t}(\\varepsilon)$ could be divided into two categories: when $\\partial_{-}f_{t}(\\zeta_{t})\\,<\\,0$ , we obtain a convex function, as shown in Figure $_{7\\mathrm{a}}$ ; when $\\partial_{-}f_{t}(\\zeta_{t})\\;\\stackrel{}{\\geq}0$ , we get a quasi-convex function, as shown in Figure $^{7\\mathrm{{b}}}$ . To verify the inequalities (15), we consider the following cases: ", "page_idx": 14}, {"type": "text", "text": "(a) If $\\nu\\,\\leq\\,u\\,<\\,\\zeta_{t}$ , then $\\tilde{u}\\,=\\,\\operatorname*{min}\\{u,\\zeta_{t}\\}\\,=\\,u$ . We can directly verify inequality (15) directly by convexity: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{t}(v)-f_{t}(u)\\leq f_{t}^{\\prime}(v)(v-u)=\\tilde{g}_{t}(v)(v-\\tilde{u}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(b) If $\\zeta_{t}\\le u\\le D$ , then $\\tilde{u}=\\operatorname*{min}\\{u,\\zeta_{t}\\}=\\zeta_{t}$ . By convexity, we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{t}(v)-f_{t}(u)=f_{t}(v)-f_{t}(\\zeta_{t})\\leq f_{t}^{\\prime}(v)(v-\\zeta_{t})=\\tilde{g}_{t}(v)(v-\\tilde{u}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "2. For $\\zeta_{t}\\le v\\le D$ , we have $\\tilde{g}_{t}(v)=\\operatorname*{max}\\left\\lbrace0,\\partial_{-}f_{t}(\\zeta_{t})\\right\\rbrace$ : ", "page_idx": 15}, {"type": "text", "text": "(a) If $\\nu\\leq u<\\zeta_{t}$ and $\\partial_{-}f_{t}(\\zeta_{t})>0$ , then $\\tilde{g}_{t}(v)=\\partial_{-}f_{t}(\\zeta_{t})$ . Thus, by convexity: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{t}(v)-f_{t}(u)=f_{t}(\\zeta_{t})-f_{t}(u)\\le\\partial_{-}f_{t}(\\zeta_{t})(\\zeta_{t}-u)\\le\\partial_{-}f_{t}(\\zeta_{t})(v-u)=\\tilde{g}_{t}(v)(v-\\tilde{u}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If $\\nu\\leq u<\\zeta_{t}$ and $\\partial_{-}f_{t}(\\zeta_{t})\\leq0$ , then $\\tilde{g}_{t}(v)=0$ . Since $f_{t}(\\varepsilon)$ is strongly convex on $[\\nu,\\zeta_{t}]$ , we have $f_{t}(u)>f_{t}(\\zeta_{t})$ . Thus, we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{t}(v)-f_{t}(u)=f_{t}(\\zeta_{t})-f_{t}(u)<0=\\tilde{g}_{t}(v)(v-u)=\\tilde{g}_{t}(v)(v-\\tilde{u}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(b) If $\\zeta_{t}\\le u\\le D$ , it is straightforward to verify that $f_{t}(v)\\!-\\!f_{t}(u)=0$ and $\\tilde{g}_{t}(v)(v\\!-\\!\\zeta_{t})\\geq$ 0. Then we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{t}(v)-f_{t}(u)=0\\leq\\tilde{g}_{t}(v)(v-\\zeta_{t})=\\tilde{g}_{t}(v)(v-\\tilde{u}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, we prove inequality (15). ", "page_idx": 15}, {"type": "text", "text": "A.2 Proof of Proposition 4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. Let $\\begin{array}{r}{\\phi_{t}(z_{t})=\\frac{\\varepsilon_{t}^{2}-2\\varepsilon_{t}z_{t}}{2\\eta_{t}}}\\end{array}$ and $\\begin{array}{r}{\\psi_{t}(z_{t})=\\frac{\\varepsilon_{t+1}^{2}-2\\varepsilon_{t+1}z_{t}}{2\\eta_{t}}}\\end{array}$ , where $\\begin{array}{r}{\\varepsilon^{\\star}\\in\\arg\\operatorname*{min}_{\\varepsilon\\in\\mathcal{D}}\\sum_{t=1}^{T}f_{t}(\\varepsilon)}\\end{array}$ and $z_{t}\\,=\\,\\operatorname*{min}\\{\\zeta_{t},\\varepsilon^{\\star}\\}$ . Let $\\dot{\\zeta}_{t}\\,=\\,\\Pi_{\\mathcal{D}}\\,\\left[|\\mathbf{w}_{t}^{\\mathsf{T}}\\mathbf{x}_{t}-y_{t}|\\right]$ , and consider the following four situations for $\\begin{array}{r}{\\eta_{t}=\\frac{\\zeta_{t}\\sqrt{D}}{G\\sqrt{\\nu t}}}\\end{array}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi_{t}(z_{t})-\\psi_{t-1}(z_{t-1})=\\phi_{t}(\\zeta_{t})-\\psi_{t-1}(\\zeta_{t-1})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{\\varepsilon_{t}^{2}}{2\\eta_{t}}-2\\varepsilon_{t}\\zeta_{t}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{D^{2}}{2}\\left(\\frac{1}{\\eta_{t}}-\\frac{1}{\\eta_{t-1}}\\right)+\\varepsilon_{t}\\left(\\frac{\\zeta_{t-1}}{\\eta_{t-1}}-\\frac{\\zeta_{t}}{\\eta_{t}}\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\frac{D^{2}}{2}\\left(\\frac{1}{\\eta_{t}}-\\frac{1}{\\eta_{t-1}}\\right)+\\frac{G\\sqrt{\\nu}\\varepsilon_{t}}{\\sqrt{D}}\\left(\\sqrt{t-1}-\\sqrt{t}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{D^{2}}{2}\\left(\\frac{1}{\\eta_{t}}-\\frac{1}{\\eta_{t-1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "\u2022 if $\\varepsilon^{\\star}\\geq\\zeta_{t-1}$ and $\\varepsilon^{\\star}<\\zeta_{t}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi_{t}(z_{t})-\\psi_{t-1}(z_{t-1})=\\phi_{t}(\\varepsilon^{\\star})-\\psi_{t-1}(\\zeta_{t-1})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{\\varepsilon_{t}^{2}-2\\varepsilon_{t}\\varepsilon^{\\star}}{2\\eta_{t}}-\\frac{\\varepsilon_{t}^{2}-2\\varepsilon_{t}\\zeta_{t-1}}{2\\eta_{t-1}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\varepsilon_{t}^{2}-2\\varepsilon_{t}\\varepsilon^{\\star}}{2\\eta_{t}}-\\frac{\\varepsilon_{t}^{2}-2\\varepsilon_{t}\\varepsilon^{\\star}}{2\\eta_{t-1}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{D^{2}}{2}\\left(\\frac{1}{\\eta_{t}}-\\frac{1}{\\eta_{t-1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\phi_{t}(z_{t})-\\psi_{t-1}(z_{t-1})=\\phi_{t}(\\zeta_{t})-\\psi_{t-1}(\\varepsilon^{\\star})}&{}\\\\ {=\\frac{\\varepsilon_{t}^{2}-2\\varepsilon_{t}\\zeta_{t}}{2\\eta_{t}}-\\frac{\\varepsilon_{t}^{2}-2\\varepsilon_{t}\\varepsilon^{\\star}}{2\\eta_{t-1}}}&{}\\\\ {\\leq\\frac{\\varepsilon_{t}^{2}-2\\varepsilon_{t}\\zeta_{t}}{2\\eta_{t}}-\\frac{\\varepsilon_{t}^{2}-2\\varepsilon_{t}\\zeta_{t-1}}{2\\eta_{t-1}}}&{}\\\\ {\\leq\\frac{D^{2}}{2}\\left(\\frac{1}{\\eta_{t}}-\\frac{1}{\\eta_{t-1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "\u2022 if $\\varepsilon^{\\star}<\\zeta_{t-1}$ and $\\varepsilon^{\\star}<\\zeta_{t}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi_{t}(z_{t})-\\psi_{t-1}(z_{t-1})=\\phi_{t}(\\varepsilon^{\\star})-\\psi_{t-1}(\\varepsilon^{\\star})}\\\\ &{\\qquad\\qquad\\qquad=\\frac{\\varepsilon_{t}^{2}-2\\varepsilon_{t}\\varepsilon^{\\star}}{2\\eta_{t}}-\\frac{\\varepsilon_{t}^{2}-2\\varepsilon_{t}\\varepsilon^{\\star}}{2\\eta_{t-1}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{D^{2}}{2}\\left(\\frac{1}{\\eta_{t}}-\\frac{1}{\\eta_{t-1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To summarize, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\phi_{t}(z_{t})-\\psi_{t-1}(z_{t-1})\\leq\\frac{D^{2}}{2}\\left(\\frac{1}{\\eta_{t}}-\\frac{1}{\\eta_{t-1}}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B Proof of Proposition 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. Since (Scutari et al., 2013, Assumptions A1-A4) hold and (5) is a convex problem, the proof for Proposition (1) follows directly from (Scutari et al., 2013, Theorem 3). \u53e3 ", "page_idx": 16}, {"type": "text", "text": "C Efficient Euclidean Projection Methods ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Projection onto the Probability Simplex ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proposition 5 (Projection onto Simplex (Palomar and Fonollosa, 2005)). When $\\mathcal{W}=\\{\\mathbf{w}\\in\\mathbb{R}^{N}\\mathrm{~}|$ $\\mathbf{1}^{\\mathsf{T}}\\mathbf{w}=1,\\mathbf{w}\\succeq0\\right\\}$ , problem $(I I)$ has a closed-form solution given by: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{w_{i}^{\\star}=\\left[q_{i}^{k}+\\kappa\\right]_{+}\\quad i=1,\\ldots,N,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{\\kappa\\,=\\,\\frac{1}{\\rho}\\,\\left(1-\\sum_{i=1}^{\\rho}q_{[i]}^{k}\\right)}\\end{array}$ with $\\begin{array}{r}{\\rho\\,=\\,\\operatorname*{max}\\Big\\{1\\leq j\\leq N:q_{[j]}^{k}+\\frac{1}{j}\\left(1-\\sum_{i=1}^{j}q_{[i]}^{k}\\right)>0\\Big\\}.}\\end{array}$ , and $q_{[i]}^{k}$ are the sorted elements of $\\mathbf{\\check{q}}^{k}$ , arranged such that $q_{[1]}^{k}\\geq q_{[2]}^{k}\\geq\\dots\\geq q_{[N]}^{k}$ . ", "page_idx": 16}, {"type": "text", "text": "C.2 Projection onto $\\ell_{1}$ Norm Ball ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proposition 6 (Projection onto $\\ell_{1}$ Norm Ball (Duchi et al., 2008)). When $\\mathcal{W}=\\{\\mathbf{w}\\in\\mathbb{R}^{N}\\mathrm{~|~}\\|\\mathbf{w}\\|_{1}\\leq$ $c\\}$ for some constant $c>0$ , problem $(I I)$ has a closed-form solution given by: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{w_{i}^{\\star}=s i g n(q_{i}^{k})\\left[\\left|q_{i}^{k}\\right|-\\tau\\right]_{+}\\quad i=1,\\ldots,N,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\tau$ is chosen such that $\\begin{array}{r}{\\sum_{i=1}^{N}\\left[\\left|q_{i}^{k}\\right|-\\tau\\right]_{+}=c.}\\end{array}$ . The value of $\\tau$ can be efficiently found by sorting $|q_{i}^{k}|$ and using a bisection search. ", "page_idx": 16}, {"type": "text", "text": "D Regret Analysis of PA with Lazy Projection ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 7. Let $(\\mathbf{x}_{1},y_{1}),\\dots,(\\mathbf{x}_{T},y_{T})$ be an arbitrary sequence of examples, where $\\mathbf{x}_{t}\\in\\mathbb{R}^{N}$ and $y_{t}\\,\\in\\,\\mathbb{R}$ for all $t$ . Let $\\xi_{t}\\,=\\,0$ for $|\\mathbf{w}_{t}^{\\mathsf{T}}\\mathbf{x}_{t}-y_{t}|\\,\\leq\\,\\varepsilon$ and $\\xi_{t}\\,=\\,\\tau_{t}$ in Equation (3) otherwise. Let ", "page_idx": 16}, {"type": "text", "text": "$\\boldsymbol{\\mathscr{W}}\\subseteq\\mathbb{R}^{N}$ be the feasible set of the weight vector and u be an arbitrary vector in $\\mathcal{W}$ . Define $\\ell_{t}=\\ell_{\\varepsilon}(\\mathbf{w}_{t};(\\mathbf{x}_{t},y_{t}))$ and $\\ell_{t}^{\\star}=\\ell_{\\varepsilon}(\\mathbf{u};(\\mathbf{x}_{t},y_{t}))$ . The following bound holds for any $\\mathbf{u}\\in\\mathbb{R}^{N}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\xi_{t}\\left(2\\ell_{t}-\\xi_{t}\\|\\mathbf x_{t}\\|_{2}^{2}-2\\ell_{t}^{\\star}\\right)\\leq\\|\\mathbf u\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. The proof is mainly based on (Crammer et al., 2006, Lemma 6) with minor modification. To facilitate the analysis of the regret bound, we rewrite the recursive updating rule of $\\widehat{\\bf w}_{t+1}$ in PA as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathbf{w}}_{t+1}=\\mathbf{w}_{t}+\\mathrm{sign}\\left[y_{t}-\\mathbf{w}_{t}^{\\mathsf{T}}\\mathbf{x}_{t}\\right]\\xi_{t}\\mathbf{x}_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\xi_{t}\\,=\\,0$ for $|\\mathbf{w}_{t}^{\\mathsf{T}}\\mathbf{x}_{t}-y_{t}|\\,\\leq\\,\\varepsilon$ and $\\xi_{t}\\,=\\,\\tau_{t}$ in Equation (3) otherwise. By projecting on the feasible set $\\mathcal{W}$ , we have the weight generated by PA with lazy projection as the following: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{w}_{t+1}=\\Pi_{\\mathcal{W}}(\\widehat{\\mathbf{w}}_{t+1})=\\underset{\\mathbf{w}\\in\\mathcal{W}}{\\arg\\operatorname*{min}}\\,\\|\\mathbf{w}-\\widehat{\\mathbf{w}}_{t+1}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Without loss of generality, we set $\\widehat{\\bf w}_{1}={\\bf0}$ and define ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Delta_{t}=\\lVert\\mathbf{w}_{t}-\\mathbf{u}\\rVert_{2}^{2}-\\lVert\\mathbf{w}_{t+1}-\\mathbf{u}\\rVert_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Summing from 1 to $T$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}{\\displaystyle\\sum_{t=1}^{T}\\Delta_{t}=\\sum_{t=1}^{T}\\left(\\|\\mathbf{w}_{t}-\\mathbf{u}\\|_{2}^{2}-\\|\\mathbf{w}_{t+1}-\\mathbf{u}\\|_{2}^{2}\\right)}&{}&\\\\ {\\displaystyle=\\|\\mathbf{w}_{1}-\\mathbf{u}\\|_{2}^{2}-\\|\\mathbf{w}_{T+1}-\\mathbf{u}\\|_{2}^{2}}\\\\ &{~}&{\\leq\\|\\mathbf{w}_{1}-\\mathbf{u}\\|_{2}^{2}}\\\\ &{~}&{\\leq\\|\\widehat{\\mathbf{w}}_{1}-\\mathbf{u}\\|_{2}^{2}}&&{[\\mathrm{since~}\\|\\Pi_{\\mathcal{W}}\\left(\\widehat{\\mathbf{w}}_{1}\\right)-\\mathbf{u}\\|_{2}^{2}\\leq\\|\\widehat{\\mathbf{w}}_{1}-\\mathbf{u}\\|_{2}^{2}]}\\\\ &{~}&{\\leq\\|\\mathbf{u}\\|_{2}^{2}.}&&{[\\mathrm{since~}\\widehat{\\mathbf{w}}_{1}=\\mathbf{0}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\tilde{\\Delta}_{t}=\\|\\mathbf{w}_{t}-\\mathbf{u}\\|_{2}^{2}-\\|\\widehat{\\mathbf{w}}_{t+1}-\\mathbf{u}\\|_{2}^{2}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{\\Delta}_{t}\\leq\\|\\mathbf{w}_{t}-\\mathbf{u}\\|_{2}^{2}-\\|\\mathbf{w}_{t+1}-\\mathbf{u}\\|_{2}^{2}=\\Delta_{t}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using the recursive updating rule of $\\widehat{\\bf w}_{t+1}$ in PA, we rewrite $\\tilde{\\Delta}_{t}$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\Delta}_{t}=\\|\\mathbf{w}_{t}-\\mathbf{u}\\|_{2}^{2}-\\|\\mathbf{w}_{t}+\\mathrm{sign}\\left[y_{t}-\\mathbf{w}_{t}^{\\top}\\mathbf{x}_{t}\\right]\\xi_{t}\\mathbf{x}_{t}-\\mathbf{u}\\|_{2}^{2}}\\\\ &{\\quad=-\\|\\mathrm{sign}\\left[y_{t}-\\mathbf{w}_{t}^{\\top}\\mathbf{x}_{t}\\right]\\xi_{t}\\mathbf{x}_{t}\\|_{2}^{2}-2(\\mathbf{w}_{t}-\\mathbf{u})^{\\top}\\mathrm{sign}\\left[y_{t}-\\mathbf{w}_{t}^{\\top}\\mathbf{x}_{t}\\right]\\xi_{t}\\mathbf{x}_{t}}\\\\ &{\\quad=-\\xi_{t}^{2}\\|\\mathbf{x}_{t}\\|_{2}^{2}-2\\cdot\\mathrm{sign}\\left[y_{t}-\\mathbf{w}_{t}^{\\top}\\mathbf{x}_{t}\\right]\\xi_{t}\\left(\\mathbf{w}_{t}^{\\top}\\mathbf{x}_{t}-\\mathbf{u}^{\\top}\\mathbf{x}_{t}\\right)}\\\\ &{\\quad=-\\xi_{t}^{2}\\|\\mathbf{x}_{t}\\|_{2}^{2}-2\\cdot\\mathrm{sign}\\left[y_{t}-\\mathbf{w}_{t}^{\\top}\\mathbf{x}_{t}\\right]\\xi_{t}\\left(\\mathbf{w}_{t}^{\\top}\\mathbf{x}_{t}-y_{t}+y_{t}-\\mathbf{u}^{\\top}\\mathbf{x}_{t}\\right)}\\\\ &{\\quad=-\\xi_{t}^{2}\\|\\mathbf{x}_{t}\\|_{2}^{2}-2\\cdot\\mathrm{sign}\\left[y_{t}-\\mathbf{w}_{t}^{\\top}\\mathbf{x}_{t}\\right]\\xi_{t}\\left(\\mathbf{w}_{t}^{\\top}\\mathbf{x}_{t}-y_{t}\\right)+2\\cdot\\mathrm{sign}\\left[y_{t}-\\mathbf{w}_{t}^{\\top}\\mathbf{x}_{t}\\right]\\xi_{t}\\left(\\mathbf{u}^{\\top}\\mathbf{x}_{t}-y_{t}\\right)}\\\\ &{\\quad=-\\xi_{t}^{2}\\|\\mathbf{x}_{t}\\|_{2}^{2}+2\\xi_{t}\\|\\mathbf{w}_{t}^{\\top}\\mathbf{x}_{t}-y_{t}|+2\\cdot\\mathrm{sign}\\left[y_{t}-\\mathbf{w}_{t}^{\\top}\\mathbf{x}_ \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\xi_{t}\\left(2\\ell_{t}-\\xi_{t}\\|\\mathbf{x}_{t}\\|_{2}^{2}-2\\ell_{t}^{\\star}\\right)\\leq\\sum_{t=1}^{T}\\tilde{\\Delta}_{t}\\leq\\sum_{t=1}^{T}\\Delta_{t}\\leq\\|\\mathbf{u}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "E Robust Analysis on Performance of APAS on Heavy-tailed Data ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To demonstrate the performance of APAS in the presence of high volatility and noisy data, we conducted simulations following the same procedure as in our synthetic data experiments outlined in Section 4.1. ", "page_idx": 17}, {"type": "text", "text": "In Section 4.1, we considered Gaussian noise $\\omega\\sim\\mathcal{N}(0,\\delta^{2})$ and Gaussian-distributed side information data $\\mathbf{r}_{t}\\sim\\mathcal{N}(\\mu,\\Sigma)$ . However, heavy-tailed distributions are generally considered more realistic models of data-generating processes in financial markets than Gaussian distributions (Cardoso et al., 2021, 2022). To evaluate the performance of APAS with highly volatile and noisy data, we generate heavy-tailed noise $\\omega$ and data $\\mathbf{r}_{t}$ based on the Student\u2019s $t$ -distribution, using the same mean and variance settings. The degree of freedom for the Student\u2019s t-distribution is set to 3, representing significant heavy tails. ", "page_idx": 18}, {"type": "text", "text": "Table 2 shows the tracking error of APAS with different combinations of noise and data distributions. Specifically, the column \" $\\mathcal{N}$ noise $+\\,t$ data\" means the noise $\\omega$ is generated by Gaussian distribution and the side information data $\\mathbf{r}_{t}$ is generated by Student\u2019s $t$ -distribution. In general, the difference in tracking error is small, indicating the robust performance of APAS in highly volatile data scenarios. ", "page_idx": 18}, {"type": "table", "img_path": "kV80nC1afE/tmp/c01001f44cfdd3eaef2963cfa48e6620aa371301828b1f7e27b4892e7740a228.jpg", "table_caption": ["Table 2: Tracking error of APAS under different combinations of noise and data distributions. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 3 compares the excess cumulative return under different combinations of noise and data distributions. For heavy-tailed noise (i.e., $t$ -distribution noise), there is a mild performance degradation. Interestingly, for heavy-tailed data, there is a modest improvement, illustrating the robustness of APAS. This is mainly due to the increased chances of outliers in positive side performance for heavy-tailed data. These results show that APAS is robust to heavy-tailed data with adaptivity in tilting the weight towards positive side information. ", "page_idx": 18}, {"type": "table", "img_path": "kV80nC1afE/tmp/7287310256cae5d553ec9e722a168c22e85de5a766b2c930d4ce58db70ab6b94.jpg", "table_caption": [], "table_footnote": ["Table 3: Excess cumulative return of APAS under different combinations of noise and data distributions. "], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The main claims in the abstract and introduction accurately reflect our paper\u2019s contribution and scope. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We have discussed the assumptions and scope for this paper in Section 3 Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The assumptions and a complete proof have been provided in Section 3 and Appendix. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The code, data, and instructions to reproduce the experiments are available in the supplemental material. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] , ", "page_idx": 21}, {"type": "text", "text": "Justification: The code, data, and instructions to reproduce the experiments are available in the supplemental material. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The experimental setting and details have been specified in Section 4. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The details for the error bars have been specified in Section 4. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The experiments are conducted on a PC equipped with a 13th Gen Intel(R) Core(TM) i7-13700 CPU and 16GB of memory. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The research conducted in this paper conforms fully with the NeurIPS Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The societal impacts have been discussed in the Abstract and Introduction. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The licenses for existing assets are mentioned in Section 4. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]