[{"figure_path": "Of4iNAIUSe/figures/figures_0_1.jpg", "caption": "Figure 1: Illustrations of main challenges in resource-aware Fed-SSL.", "description": "This figure illustrates the two main challenges in resource-aware federated self-supervised learning (Fed-SSL): deviated representation abilities and inconsistent representation spaces.  The deviated representation abilities are shown by the different ways that different client models represent the same classes (e.g., cat, dog, tiger). The inconsistent representation spaces are shown by the fact that different clients have different classes, leading to representation spaces that do not align well.", "section": "1 Introduction"}, {"figure_path": "Of4iNAIUSe/figures/figures_1_1.jpg", "caption": "Figure 1: Illustrations of main challenges in resource-aware Fed-SSL.", "description": "This figure illustrates the main challenges in resource-aware federated self-supervised learning (Fed-SSL).  Specifically, it highlights the issues of deviated representation abilities and inconsistent representation spaces arising from heterogeneous client models and class skew.  Client A, B, and C represent different clients with varying model architectures and data distributions. Each client's representation space shows how their models encode similar classes (cat, dog, tiger) differently, leading to inconsistencies when attempting to combine these representations into a unified global representation space on the server.", "section": "1 Introduction"}, {"figure_path": "Of4iNAIUSe/figures/figures_3_1.jpg", "caption": "Figure 2: The overall framework of FedMKD. Clients initialize the model architecture based on the local resource, then self-supervised train the local model using unlabeled local data. The server uses the multi-teacher adaptive knowledge integration distillation to aggregate positive local knowledge to train the global model and then updates local models again according to the alignment module.", "description": "This figure illustrates the workflow of the proposed FedMKD framework.  It shows how each client initializes its model based on available resources, trains it using local unlabeled data, and then the server aggregates the knowledge from different clients using a multi-teacher adaptive knowledge integration distillation method. The server then trains a global model and updates the client models using a global anchored alignment module.", "section": "4 Designed FedMKD Method"}, {"figure_path": "Of4iNAIUSe/figures/figures_5_1.jpg", "caption": "Figure 3: T-SNE visualizations of hidden vectors from different models on CIFAR-10, the data distribution of clients is IID.", "description": "This figure displays t-distributed stochastic neighbor embedding (t-SNE) visualizations of hidden vector representations learned by different models on the CIFAR-10 dataset.  The visualizations show how different models represent data points in a lower dimensional space.  Panel (a) shows the results of standalone training (a single model trained on the full dataset), (b) shows results from the MOON algorithm trained on a partial public dataset, (c) shows results from the FedMKD algorithm trained on an IID (independently and identically distributed) public dataset and (d) shows the results from the FedMKD algorithm trained on a partial public dataset. The IID setting ensures each client has an equal number of samples from each class. The visualizations reveal differences in how the models cluster the data, demonstrating the impact of training methodologies on data representation.", "section": "5 Experiments"}, {"figure_path": "Of4iNAIUSe/figures/figures_8_1.jpg", "caption": "Figure 4: Improvement of clients after involving our proposed FedMKD.", "description": "This figure shows the improvement of client model performance after participating in federated learning using the FedMKD framework.  It compares the testing accuracy of standalone local model training versus the performance of the same clients when trained using FedMKD. The results demonstrate that participating in federated training with FedMKD improves the accuracy of the client models, even for those with different architectures.", "section": "5.4 Improvement of clients"}, {"figure_path": "Of4iNAIUSe/figures/figures_8_2.jpg", "caption": "Figure 5: LDA visualizations of hidden vectors from different models on CIFAR-10.", "description": "This figure shows LDA visualizations of hidden vectors from different models on CIFAR-10.  The left panel illustrates the inconsistent representation spaces between different clients (Client A and Client B) before the application of the FedMKD method.  Overlapping classes such as 'cat' and 'dog' highlight this inconsistency.  The right panel demonstrates how FedMKD creates a unified representation space for the global model, where classes are clearly separated, resolving the inconsistency present in the client-side models.", "section": "5.5 Validation of inconsistent representation spaces"}, {"figure_path": "Of4iNAIUSe/figures/figures_15_1.jpg", "caption": "Figure 3: T-SNE visualizations of hidden vectors from different models on CIFAR-10, the data distribution of clients is IID.", "description": "This figure displays t-distributed stochastic neighbor embedding (t-SNE) visualizations, a dimensionality reduction technique, of feature vectors obtained from various models trained on the CIFAR-10 dataset.  The visualizations help illustrate the learned representation spaces produced by different training methods. Panel (a) shows results from standalone training, (b) shows results from MOON (a baseline method) trained using a partial public dataset, (c) shows results from FedMKD (the proposed method) trained using an independent and identically distributed (IID) public dataset, and (d) shows results from FedMKD trained using a partial public dataset. The visualizations show how well the different methods are able to cluster similar images together, giving a visual representation of their performance in learning effective and compact image representations.", "section": "5 Experiments"}, {"figure_path": "Of4iNAIUSe/figures/figures_19_1.jpg", "caption": "Figure 3: T-SNE visualizations of hidden vectors from different models on CIFAR-10, the data distribution of clients is IID.", "description": "This figure shows t-SNE visualizations of hidden vectors learned by different models on the CIFAR-10 dataset.  The data distribution across clients is IID (independent and identically distributed).  The visualizations compare the hidden vector representations learned under different training scenarios: standalone training, MOON on a partial public dataset, FedMKD on an IID public dataset, and FedMKD on a partial public dataset. The visual separation of clusters indicates how well the different models learn to separate the different classes.", "section": "5 Experiments"}, {"figure_path": "Of4iNAIUSe/figures/figures_19_2.jpg", "caption": "Figure 3: T-SNE visualizations of hidden vectors from different models on CIFAR-10, the data distribution of clients is IID.", "description": "This figure shows t-SNE visualizations of hidden vectors from different models trained on the CIFAR-10 dataset.  The visualizations compare the feature representations learned by different models under various training conditions (standalone training, MOON on a partial public dataset, FedMKD on an IID public dataset, and FedMKD on a partial public dataset). The goal is to illustrate the impact of the training method and data distribution on the resulting feature representations. The IID (independent and identically distributed) data distribution among clients implies that all clients have similar data distributions.", "section": "5 Experiments"}]