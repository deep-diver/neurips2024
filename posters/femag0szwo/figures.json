[{"figure_path": "FEmag0szWo/figures/figures_3_1.jpg", "caption": "Figure 1: An illustrative example of MILP and its graph representation.", "description": "This figure illustrates how a Mixed Integer Linear Program (MILP) problem can be represented as a graph.  The MILP example shows an objective function to minimize and linear constraints involving three variables (x1, x2, x3), with some variables restricted to integer values. The corresponding graph representation is shown, using a bipartite graph structure with constraint nodes (U1, U2) and variable nodes (W1, W2, W3). Edges connect constraint and variable nodes where the corresponding coefficient in the MILP's constraint matrix is non-zero.  The graph nodes are labeled with features relevant to the MILP's constraints and variables (e.g., coefficients, bounds, integer constraints). This graph representation is crucial for applying Graph Neural Networks (GNNs) to solve or analyze MILP problems, as detailed in the paper.", "section": "Graph Representation of MILP"}, {"figure_path": "FEmag0szWo/figures/figures_5_1.jpg", "caption": "Figure 2: An illustrative example of color refinement and partitions. Initially, all variables share a common color due to their identical node attributes, as do the constraint nodes. After a round of the WL test, x1 and x2 retain their shared color, while x3 is assigned a distinct color, as it connects solely to the first constraint, unlike x1 and x2. Similarly, the colors of the two constraints can also be differentiated. Finally, this partition stabilizes, resulting in I = {{1},{2}}, J = {{1,2}, {3}}.", "description": "This figure illustrates the Weisfeiler-Lehman (WL) test, a graph isomorphism test, applied to a Mixed Integer Linear Program (MILP) represented as a graph.  The initial state shows all nodes with the same color.  After one iteration of the WL test, nodes are recolored based on their neighbors' colors, resulting in a refined partitioning of the graph. The final partition, stable after further iterations, is shown, demonstrating how the WL test can differentiate nodes based on their structural relationships within the graph.", "section": "4 Main results"}, {"figure_path": "FEmag0szWo/figures/figures_9_1.jpg", "caption": "Figure 3: Numerical results of MP-GNN and 2-FGNN for SB score fitting. In the right figure, the training error of MP-GNN on MP-intractable examples does not decrease after however many epochs.", "description": "This figure shows the training error curves for both MP-GNN and 2-FGNN models on two different datasets: one containing MP-tractable MILPs and the other containing two MP-intractable MILPs. The plot on the left demonstrates that both models can effectively fit the SB scores of the MP-tractable dataset, while the plot on the right illustrates that only the 2-FGNN model is capable of fitting the SB scores of the MP-intractable dataset. The MP-GNN model shows no improvement in its training error even after many training epochs.", "section": "5 Numerical results"}, {"figure_path": "FEmag0szWo/figures/figures_9_2.jpg", "caption": "Figure 3: Numerical results of MP-GNN and 2-FGNN for SB score fitting. In the right figure, the training error of MP-GNN on MP-intractable examples does not decrease after however many epochs.", "description": "This figure shows the training error curves for both MP-GNN and 2-FGNN models when training on two different datasets: one with MP-tractable MILPs and the other with MP-intractable MILPs.  The left plot demonstrates that both models effectively learn to approximate strong branching (SB) scores on MP-tractable data, achieving low training errors. In contrast, the right plot shows that the MP-GNN fails to reduce training error on MP-intractable data, while the 2-FGNN still successfully learns to approximate SB scores. This illustrates the theoretical findings of the paper about the limitations of MP-GNNs and the capacity of 2-FGNNs for approximating SB.", "section": "5 Numerical results"}]