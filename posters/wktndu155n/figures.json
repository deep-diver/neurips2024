[{"figure_path": "WKTNdU155n/figures/figures_2_1.jpg", "caption": "Figure 1: Overall framework of LLaMo. LLaMo consists of a graph neural network, a multi-level graph projector, and a large language model. It first encodes an input 2D molecular graph with the graph neural network and then converts the encoded graph into molecular graph tokens with the multi-level graph projector. Finally, the large language model generates the instruction-following response given the input SMILES, graph tokens, and the instruction.", "description": "The figure illustrates the architecture of LLaMo, a large language model-based molecular graph assistant.  It shows the three main components: a graph neural network (GNN) encoding a 2D molecular graph; a multi-level graph projector transforming the GNN output into molecular graph tokens; and a large language model generating a response based on the tokens, input SMILES notation, and an instruction. The multi-level projector uses cross-attention to integrate information from multiple GNN layers and motif representations, improving the model's understanding of molecular structure.", "section": "3 LLaMo: Large Language Model-based Molecular Graph Assistant"}, {"figure_path": "WKTNdU155n/figures/figures_3_1.jpg", "caption": "Figure 2: Node representations of graph encoder with 1,2,4,5 layers. As the number of layers increases, node representations collapse.", "description": "This figure visualizes the impact of the number of layers in a graph neural network (GNN) on node representations.  It shows four subfigures (a, b, c, d), each representing the node representations (yellow dots) from a GNN with 1, 2, 4, and 5 layers, respectively, applied to the same molecular graph. As the number of layers increases, the node representations become indistinguishable (they collapse). This phenomenon, known as over-smoothing, highlights a limitation of using only high-level features from deep GNNs for tasks involving local graph information, motivating the use of a multi-level graph projector.", "section": "3.1 Model Architecture"}, {"figure_path": "WKTNdU155n/figures/figures_4_1.jpg", "caption": "Figure 1: Overall framework of LLaMo. LLaMo consists of a graph neural network, a multi-level graph projector, and a large language model. It first encodes an input 2D molecular graph with the graph neural network and then converts the encoded graph into molecular graph tokens with the multi-level graph projector. Finally, the large language model generates the instruction-following response given the input SMILES, graph tokens, and the instruction.", "description": "LLaMo's architecture is shown.  It uses a graph neural network to encode a 2D molecular graph, converting the graph representation into tokens. These are fed, along with SMILES notation and an instruction, into a multi-level graph projector and then a large language model. This generates a response based on the instruction.", "section": "3 LLaMo: Large Language Model-based Molecular Graph Assistant"}, {"figure_path": "WKTNdU155n/figures/figures_4_2.jpg", "caption": "Figure 1: Overall framework of LLaMo. LLaMo consists of a graph neural network, a multi-level graph projector, and a large language model. It first encodes an input 2D molecular graph with the graph neural network and then converts the encoded graph into molecular graph tokens with the multi-level graph projector. Finally, the large language model generates the instruction-following response given the input SMILES, graph tokens, and the instruction.", "description": "This figure shows the architecture of LLaMo.  LLaMo uses a three-stage process to answer questions about molecules. First, a graph neural network encodes a 2D molecular graph (input). Second, a multi-level graph projector transforms the encoded graph into tokens understandable by the large language model. Third, the large language model generates an instruction-following response based on the input SMILES string, graph tokens, and the input question.", "section": "3 LLaMo: Large Language Model-based Molecular Graph Assistant"}, {"figure_path": "WKTNdU155n/figures/figures_5_1.jpg", "caption": "Figure 1: Overall framework of LLaMo. LLaMo consists of a graph neural network, a multi-level graph projector, and a large language model. It first encodes an input 2D molecular graph with the graph neural network and then converts the encoded graph into molecular graph tokens with the multi-level graph projector. Finally, the large language model generates the instruction-following response given the input SMILES, graph tokens, and the instruction.", "description": "This figure illustrates the overall architecture of the LLaMo model, showing the three main components: a graph neural network (GNN) that encodes the 2D molecular graph, a multi-level graph projector that transforms the GNN output into tokens understandable by the language model, and a large language model that generates the final instruction-following response. The input includes SMILES notation, graph tokens and an instruction, and the output is the generated text.", "section": "3 LLaMo: Large Language Model-based Molecular Graph Assistant"}, {"figure_path": "WKTNdU155n/figures/figures_8_1.jpg", "caption": "Figure 4: Visualization of attention maps for samples with coarse-grained caption (left) and fine-grained caption (right). The attention scores of high-level features are relatively high when generating coarse-grained captions, whereas those of low-level features are high for fine-grained captions.", "description": "This figure visualizes the attention maps generated by the model when producing different types of captions (coarse-grained and fine-grained).  The heatmaps show which parts of the molecular graph representation the model focuses on when generating each caption type. The left panel shows the attention for coarse-grained captions (high-level features), and the right panel for fine-grained captions (low-level features).  The visualization demonstrates that the model uses different parts of the graph depending on the desired level of detail in the caption.", "section": "5.3 Analysis"}, {"figure_path": "WKTNdU155n/figures/figures_9_1.jpg", "caption": "Figure 1: Overall framework of LLaMo. LLaMo consists of a graph neural network, a multi-level graph projector, and a large language model. It first encodes an input 2D molecular graph with the graph neural network and then converts the encoded graph into molecular graph tokens with the multi-level graph projector. Finally, the large language model generates the instruction-following response given the input SMILES, graph tokens, and the instruction.", "description": "This figure illustrates the architecture of LLaMo, a large language model-based molecular graph assistant.  It shows the three main components: a graph neural network (GNN) that encodes a 2D molecular graph into node representations; a multi-level graph projector that transforms these representations into graph tokens usable by the language model; and a large language model that generates a response based on the graph tokens, SMILES input, and the given instruction. The multi-level projector is key to bridging the gap between graph and language modalities.", "section": "3 LLaMo: Large Language Model-based Molecular Graph Assistant"}, {"figure_path": "WKTNdU155n/figures/figures_20_1.jpg", "caption": "Figure 1: Overall framework of LLaMo. LLaMo consists of a graph neural network, a multi-level graph projector, and a large language model. It first encodes an input 2D molecular graph with the graph neural network and then converts the encoded graph into molecular graph tokens with the multi-level graph projector. Finally, the large language model generates the instruction-following response given the input SMILES, graph tokens, and the instruction.", "description": "The figure shows the architecture of LLaMo, a large language model-based molecular graph assistant.  It consists of three main components: a graph neural network (GNN) that encodes a 2D molecular graph as input, a multi-level graph projector that transforms the GNN's output into a sequence of molecular graph tokens suitable for the language model, and a large language model (LLM) that generates the final instruction-following response. The input to the system is a combination of the 2D molecular graph, its SMILES representation (a 1D string representation of the molecule), and an instruction (text).", "section": "3 LLaMo: Large Language Model-based Molecular Graph Assistant"}, {"figure_path": "WKTNdU155n/figures/figures_20_2.jpg", "caption": "Figure 1: Overall framework of LLaMo. LLaMo consists of a graph neural network, a multi-level graph projector, and a large language model. It first encodes an input 2D molecular graph with the graph neural network and then converts the encoded graph into molecular graph tokens with the multi-level graph projector. Finally, the large language model generates the instruction-following response given the input SMILES, graph tokens, and the instruction.", "description": "LLaMo is composed of three main components: a graph neural network (GNN) that encodes the input molecular graph, a multi-level graph projector that transforms the graph representation into tokens understandable by the language model, and a large language model (LLM) that generates the final response based on the graph tokens and the input instruction (e.g., a SMILES string and a question about the molecule). The multi-level graph projector is a crucial component that bridges the gap between the graph and language modalities.", "section": "3 LLaMo: Large Language Model-based Molecular Graph Assistant"}, {"figure_path": "WKTNdU155n/figures/figures_20_3.jpg", "caption": "Figure 1: Overall framework of LLaMo. LLaMo consists of a graph neural network, a multi-level graph projector, and a large language model. It first encodes an input 2D molecular graph with the graph neural network and then converts the encoded graph into molecular graph tokens with the multi-level graph projector. Finally, the large language model generates the instruction-following response given the input SMILES, graph tokens, and the instruction.", "description": "LLaMo's architecture is shown, illustrating the flow of information.  A 2D molecular graph and its SMILES representation are input to a graph neural network (GNN). The multi-level graph projector transforms the GNN's output into molecular graph tokens, which are then fed into a large language model along with an instruction.  The language model generates a response based on this combined input.", "section": "3 LLaMo: Large Language Model-based Molecular Graph Assistant"}, {"figure_path": "WKTNdU155n/figures/figures_21_1.jpg", "caption": "Figure 2: Node representations of graph encoder with 1,2,4,5 layers. As the number of layers increases, node representations collapse.", "description": "This figure visualizes the node representations learned by a graph neural network (GNN) with varying numbers of layers (1, 2, 4, and 5). Each subfigure shows the node representations of the same molecular graph. The key observation is that as the number of layers in the GNN increases, the node representations tend to become more similar, eventually collapsing into almost identical representations in the higher-layer GNNs. This phenomenon, known as over-smoothing, is a significant challenge in graph neural networks, limiting their ability to capture fine-grained information. This illustrates the limitations of simply using the output of the final GNN layer for downstream tasks.", "section": "3.1 Model Architecture"}, {"figure_path": "WKTNdU155n/figures/figures_21_2.jpg", "caption": "Figure 1: Overall framework of LLaMo. LLaMo consists of a graph neural network, a multi-level graph projector, and a large language model. It first encodes an input 2D molecular graph with the graph neural network and then converts the encoded graph into molecular graph tokens with the multi-level graph projector. Finally, the large language model generates the instruction-following response given the input SMILES, graph tokens, and the instruction.", "description": "The figure illustrates the architecture of LLaMo, a large language model-based molecular graph assistant.  LLaMo takes three types of input: a 2D molecular graph, a SMILES string (a linear representation of the molecule), and an instruction (a natural language query). The molecular graph is processed by a graph neural network (GNN) that outputs node representations. These are then transformed into graph tokens via a multi-level graph projector that incorporates information from multiple layers of the GNN. These tokens, along with the SMILES and instruction, are provided to a large language model, which then generates a response. The multi-level graph projector is key to bridging the gap between the graph and language modalities.", "section": "3 LLaMo: Large Language Model-based Molecular Graph Assistant"}, {"figure_path": "WKTNdU155n/figures/figures_22_1.jpg", "caption": "Figure 1: Overall framework of LLaMo. LLaMo consists of a graph neural network, a multi-level graph projector, and a large language model. It first encodes an input 2D molecular graph with the graph neural network and then converts the encoded graph into molecular graph tokens with the multi-level graph projector. Finally, the large language model generates the instruction-following response given the input SMILES, graph tokens, and the instruction.", "description": "This figure illustrates the overall architecture of LLaMo, which seamlessly integrates a molecular graph encoder, a multi-level graph projector, and a large language model to generate instruction-following responses. The process begins by encoding a 2D molecular graph using a graph neural network.  The multi-level graph projector then transforms the encoded graph into graph tokens, which are compatible with the large language model. Finally, the large language model generates a response based on the input SMILES string, graph tokens, and instruction.", "section": "3 LLaMo: Large Language Model-based Molecular Graph Assistant"}]