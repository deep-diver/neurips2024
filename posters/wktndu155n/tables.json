[{"figure_path": "WKTNdU155n/tables/tables_6_1.jpg", "caption": "Table 2: Performance (%) of generalist models on three tasks: molecule description generation, IUPAC prediction, and property prediction. Mol. Inst. tuned denotes the molecular instruction-tuned model. * The result is not available since LLaMA2 fails generating numerical outputs. \u2020 denotes the experimental results drawn from Mol-Instruction [48].", "description": "This table presents the performance comparison of various generalist models (including GPT-3.5, GPT-4, Galactica, and LLaMA) on three tasks: molecule description generation, IUPAC name prediction, and property prediction.  The metrics used are BLEU, METEOR, and MAE.  It shows how the proposed LLaMo model outperforms other models, especially when instruction tuning is applied.  The table also notes limitations with some models (LLaMA2 failing to provide numerical output for one metric).", "section": "5.2 Experimental Results"}, {"figure_path": "WKTNdU155n/tables/tables_7_1.jpg", "caption": "Table 3: Performance (%) of specialist models on molecule captioning with the PubChem324k and ChEBI-20 datasets and IUPAC name prediction. Full ft denotes full parameter fine-tuning.", "description": "This table presents the performance of various specialist models on two molecule captioning datasets (PubChem324k and ChEBI-20) and IUPAC name prediction.  The models are evaluated using BLEU and METEOR scores for captioning and METEOR for IUPAC name prediction.  The \"Training type\" column indicates whether full fine-tuning or a low-rank adaptation (LORA) was used.  The table allows for comparison of different models' performance across different datasets and training methods. It highlights the performance of LLaMo in comparison to other state-of-the-art models. ", "section": "5.2 Experimental Results"}, {"figure_path": "WKTNdU155n/tables/tables_7_2.jpg", "caption": "Table 4: Performance comparison according to the projector type.", "description": "This table presents a comparison of the performance of different graph projectors used in the LLaMo model.  It shows the BLEU and METEOR scores for molecule description and IUPAC prediction tasks, along with the MAE for property prediction. The projectors compared include a baseline with no graph, MLPs using different levels of node representation, a resampler, and the proposed multi-level graph projector (MGProj) with and without motif information. The results demonstrate the superiority of the MGProj in capturing multi-level graph structure, leading to improved performance.", "section": "5.3 Analysis"}, {"figure_path": "WKTNdU155n/tables/tables_8_1.jpg", "caption": "Table 5: Ablation studies on training stage and GPT-generated instruction tuning data.", "description": "This table presents the results of ablation studies conducted to evaluate the impact of different training stages and the use of GPT-generated instruction tuning data on the performance of the LLaMo model.  It shows the BLEU and METEOR scores for molecule description generation, IUPAC prediction, and the MAE for property prediction. The results demonstrate the effectiveness of the two-stage training pipeline and the inclusion of GPT-generated data in improving the model's performance across various tasks.", "section": "5 Experiments"}, {"figure_path": "WKTNdU155n/tables/tables_8_2.jpg", "caption": "Table 6: Performance comparison according to the training type.", "description": "This table compares the performance of three different training methods on three tasks: molecule description generation, IUPAC prediction, and property prediction.  The methods are: training without instruction tuning (only Stage 1), multi-task learning, and the authors' proposed instruction-tuning method.  The results show that the instruction-tuning method achieves the best performance across all three tasks, demonstrating the effectiveness of instruction tuning for these molecular tasks.", "section": "5.3 Analysis"}, {"figure_path": "WKTNdU155n/tables/tables_9_1.jpg", "caption": "Table 2: Performance (%) of generalist models on three tasks: molecule description generation, IUPAC prediction, and property prediction. Mol. Inst. tuned denotes the molecular instruction-tuned model. * The result is not available since LLaMA2 fails generating numerical outputs. \u2020 denotes the experimental results drawn from Mol-Instruction [48].", "description": "This table compares the performance of various generalist language models on three molecular tasks: generating descriptions of molecules, predicting IUPAC names, and predicting properties.  The models are evaluated using metrics such as BLEU, METEOR, and MAE.  The table includes both models fine-tuned with molecular instructions and those without, highlighting the impact of instruction tuning on performance.", "section": "5.2 Experimental Results"}, {"figure_path": "WKTNdU155n/tables/tables_15_1.jpg", "caption": "Table 2: Performance (%) of generalist models on three tasks: molecule description generation, IUPAC prediction, and property prediction. Mol. Inst. tuned denotes the molecular instruction-tuned model. * The result is not available since LLaMA2 fails generating numerical outputs. \u2020 denotes the experimental results drawn from Mol-Instruction [48].", "description": "This table presents the performance comparison of various generalist models (including the proposed LLaMo) across three tasks: molecule description generation, IUPAC name prediction, and property prediction.  The performance is measured using BLEU, METEOR, and MAE metrics, reflecting the model's ability to generate descriptions, predict IUPAC names, and predict molecular properties, respectively.  The table also highlights whether the models were fine-tuned using molecular instruction data and notes the limitations of LLaMA2 in certain tasks.", "section": "5.2 Experimental Results"}, {"figure_path": "WKTNdU155n/tables/tables_19_1.jpg", "caption": "Table 2: Performance (%) of generalist models on three tasks: molecule description generation, IUPAC prediction, and property prediction. Mol. Inst. tuned denotes the molecular instruction-tuned model. * The result is not available since LLaMA2 fails generating numerical outputs. \u2020 denotes the experimental results drawn from Mol-Instruction [48].", "description": "This table compares the performance of various large language models (LLMs) on three tasks related to molecular understanding: molecule description generation, IUPAC name prediction, and property prediction.  It shows the BLEU and METEOR scores (higher is better) for description and IUPAC prediction, and MAE (lower is better) for property prediction.  The models are categorized as either generalist (handling all three tasks) or molecular instruction-tuned (specifically trained for molecular tasks). The table helps to demonstrate LLaMo's improved performance compared to other LLMs.", "section": "5.2 Experimental Results"}, {"figure_path": "WKTNdU155n/tables/tables_19_2.jpg", "caption": "Table 2: Performance (%) of generalist models on three tasks: molecule description generation, IUPAC prediction, and property prediction. Mol. Inst. tuned denotes the molecular instruction-tuned model. * The result is not available since LLaMA2 fails generating numerical outputs. \u2020 denotes the experimental results drawn from Mol-Instruction [48].", "description": "This table compares the performance of various large language models (LLMs) on three tasks related to molecular understanding: molecule description generation, IUPAC name prediction, and property prediction.  It contrasts the performance of generalist models (trained on multiple tasks) and those that were specifically fine-tuned for molecular instruction tasks, showing BLEU, METEOR, and MAE scores for each model.", "section": "5.2 Experimental Results"}]