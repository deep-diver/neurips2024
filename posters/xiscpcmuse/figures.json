[{"figure_path": "XIScpCMUse/figures/figures_1_1.jpg", "caption": "Figure 1: MVInpainter addresses 2D/3D editing tasks: (a) novel view synthesis, (b) multi-view object removal, and (c) object insertion and replacement through multi-view consistent inpainting ability. Given one inpainted or edited reference image, MVInpainter spreads it to other masked views without pose conditions. (d) MVInpainter could be applied to real-world 3D scene editing for dense point clouds by Dust3R [76] or Multi-View Stereo (MVS) [9] and 3DGS [35] with consistent generation.", "description": "This figure showcases the capabilities of MVInpainter in handling various 2D and 3D editing tasks.  Panel (a) demonstrates novel view synthesis, where a single edited image is used to generate consistent views. Panel (b) shows multi-view object removal, where the object is removed from multiple views in a consistent manner. Panel (c) illustrates multi-view object insertion/replacement, where objects are inserted or replaced in various viewpoints, maintaining consistency across the views. Lastly, panel (d) demonstrates MVInpainter's application in real-world 3D scene editing using methods such as Dust3R, Multi-View Stereo, and 3D Gaussian Splatting for generating consistent 3D outputs.", "section": "1 Introduction"}, {"figure_path": "XIScpCMUse/figures/figures_3_1.jpg", "caption": "Figure 2: The overall pipeline and main contributions of MVInpainter. We primarily focus on multi-view inpainting, while the 3D reconstruction is detailed in Appendix Sec. C.", "description": "This figure shows the overall workflow of the MVInpainter model.  It starts with a 2D inpainted image, then uses MVInpainter-F (for forward-facing scenes) for optional multi-view object removal.  After that, MVInpainter-O (for object-centric scenes) is used for multi-view synthesis and/or object insertion. Finally, optional 3D reconstruction (using 3DGS and LPIPS) is done.  The main contribution of the paper is highlighted as the multi-view inpainting part.", "section": "3 Approach"}, {"figure_path": "XIScpCMUse/figures/figures_3_2.jpg", "caption": "Figure 3: (a) The overview of the proposed MVInpainter. MVInpainter-O is trained on object-centric data, while MVInpainter-F is trained on forward-facing data with a shared SD-inpainting backbone of different LoRA/motion weights and masking strategies. The object-centric MVInpainter focuses on the object-level NVS, while the forward-facing one is devoted to object removal and scene-level inpainting. (b) The Ref-KV is used in spatial self-attention blocks of denoising U-Net. (c) The slot-attention based flow grouping module is used to learn implicit pose features. Dashed boxes in (b) and (c) mean feature concatenation.", "description": "This figure shows the architecture of MVInpainter, a multi-view consistent inpainting model.  It's composed of three parts: an overview of the model's framework, a detailed look at the Ref-KV (Reference Key & Value) concatenation used in the self-attention blocks, and an explanation of the slot-attention-based flow grouping used for pose-free learning.  MVInpainter-O is trained on object-centric data for object-level novel view synthesis, while MVInpainter-F handles forward-facing data for object removal and scene-level inpainting.", "section": "3 Approach"}, {"figure_path": "XIScpCMUse/figures/figures_5_1.jpg", "caption": "Figure 4: (a) The inference pipeline includes object removal, mask adaption, and object insertion. (b) The illustration of heuristic masking adaption, which is built from yellow points of the closed convex hull. (c) The perspective warping based on the basic plane and the bottom face. All matches are on the basic plane filtered by Grounded-SAM [58] with captions \u201ctable\u201d and \u201ctablecloth\u201d.", "description": "This figure illustrates the inference pipeline of MVInpainter, a multi-view consistent inpainting model.  It shows three key stages: (a) The overall pipeline which includes object removal (using MVInpainter-F), mask adaptation, and object insertion (using MVInpainter-O). (b) Details on the heuristic masking adaptation process, where masks are created based on the convex hull of identified object points.  (c) Shows the perspective warping technique used to adapt the object's perspective across different views, utilizing matches from a basic plane and the bottom face of the object, refined with Grounded-SAM to ensure accurate matching.", "section": "3.4 Inference"}, {"figure_path": "XIScpCMUse/figures/figures_7_1.jpg", "caption": "Figure 5: Object-centric results on CO3D, MVImgNet, and Omni3D. The first row denotes the reference (first column) and other masked inputs, while other results are sampled from LeftRefill [7], Nerfiller [80], ZeroNVS [64], and our MVInpainter. Please zoom-in for details.", "description": "This figure shows a comparison of object-centric novel view synthesis (NVS) results on three datasets (CO3D, MVImgNet, and Omni3D) using four different methods: LeftRefill, Nerfiller, ZeroNVS, and the proposed MVInpainter.  The first row displays the input masked images and the reference images. The subsequent rows present the generated results from each method for each input. The figure highlights the superior performance of MVInpainter in generating visually realistic and consistent views compared to other methods. The Omni3D results showcase MVInpainter's zero-shot generalization capabilities.", "section": "4.1 Object-Centric Results"}, {"figure_path": "XIScpCMUse/figures/figures_8_1.jpg", "caption": "Figure 4: (a) The inference pipeline includes object removal, mask adaption, and object insertion. (b) The illustration of heuristic masking adaption, which is built from yellow points of the closed convex hull. (c) The perspective warping based on the basic plane and the bottom face. All matches are on the basic plane filtered by Grounded-SAM [58] with captions \u201ctable\u201d and \u201ctablecloth\u201d.", "description": "This figure shows the inference pipeline of the proposed MVInpainter model.  It demonstrates the three main stages: object removal, mask adaptation, and object insertion. Panel (b) details the heuristic masking adaptation process, illustrating how masks are created based on the convex hull of the object.  Panel (c) describes the perspective warping technique used to accurately transform the object across multiple views. The process uses matches from a basic plane, which are filtered using the Grounded-SAM method [58], ensuring accurate perspective transformation.", "section": "3.4 Inference"}, {"figure_path": "XIScpCMUse/figures/figures_8_2.jpg", "caption": "Figure 7: Qualitative ablation studies of AnimateDiff initialization and Ref-KV on CO3D.", "description": "This figure shows a qualitative comparison of the results obtained using different components of the MVInpainter model on the CO3D dataset.  The top row displays the reference and masked input images. The second row shows the results without using video priors from AnimateDiff, while the third row presents the results with video priors. The fourth row shows the results without using Ref-KV (Reference Key&Value concatenation), while the fifth row displays the results with Ref-KV.  The comparison illustrates the impact of these components on the appearance and consistency of the generated images across different viewpoints.", "section": "3.2 Multi-View Consistent Inpainting Model"}, {"figure_path": "XIScpCMUse/figures/figures_17_1.jpg", "caption": "Figure 8: Object removal visualization on SPInNeRF. The first row denotes the reference (first column) and other masked inputs, while other results are sampled from the inpainted sequence.", "description": "This figure shows a comparison of object removal results on the SPInNeRF dataset using different methods. The first row displays the masked input images, where the target object is missing. The subsequent rows present the results generated by various methods, including LaMa, MAT, SD-Inpaint, LeftRefill, ProPainter, and the proposed MVInpainter. The figure visually demonstrates the effectiveness of MVInpainter in generating visually appealing and consistent object removal results across multiple views.", "section": "4.2 Forward-Facing Results"}, {"figure_path": "XIScpCMUse/figures/figures_18_1.jpg", "caption": "Figure 1: MVInpainter addresses 2D/3D editing tasks: (a) novel view synthesis, (b) multi-view object removal, and (c) object insertion and replacement through multi-view consistent inpainting ability. Given one inpainted or edited reference image, MVInpainter spreads it to other masked views without pose conditions. (d) MVInpainter could be applied to real-world 3D scene editing for dense point clouds by Dust3R [76] or Multi-View Stereo (MVS) [9] and 3DGS [35] with consistent generation.", "description": "This figure showcases the capabilities of MVInpainter in addressing various 2D and 3D editing tasks.  It demonstrates the method's ability to perform novel view synthesis (generating new views from existing ones), multi-view object removal (consistently removing an object from multiple viewpoints), and multi-view object insertion/replacement (consistently adding or replacing objects across different views). Importantly, the method achieves consistency without explicit camera pose information, making it applicable to real-world 3D scenes using point cloud data.", "section": "1 Introduction"}, {"figure_path": "XIScpCMUse/figures/figures_19_1.jpg", "caption": "Figure 1: MVInpainter addresses 2D/3D editing tasks: (a) novel view synthesis, (b) multi-view object removal, and (c) object insertion and replacement through multi-view consistent inpainting ability. Given one inpainted or edited reference image, MVInpainter spreads it to other masked views without pose conditions. (d) MVInpainter could be applied to real-world 3D scene editing for dense point clouds by Dust3R [76] or Multi-View Stereo (MVS) [9] and 3DGS [35] with consistent generation.", "description": "This figure demonstrates the capabilities of MVInpainter in various 2D and 3D editing tasks.  (a) shows novel view synthesis, where a single edited view is extended to multiple consistent views. (b) illustrates multi-view object removal, showcasing consistent object removal across multiple views. (c) displays multi-view object insertion/replacement, where objects are added or replaced across views maintaining consistency. Finally, (d) shows how the model can extend to real-world 3D scene editing using point clouds generated from methods such as Dust3R and 3DGS, maintaining consistent object manipulation.", "section": "1 Introduction"}, {"figure_path": "XIScpCMUse/figures/figures_20_1.jpg", "caption": "Figure 11: Object replacement results edited by T2I inpainting model and AnyDoor [14].", "description": "This figure shows the results of object replacement in multiple views using a text-to-image (T2I) inpainting model and the AnyDoor method.  Different objects are shown being replaced or inserted into various scenes. The results demonstrate the capability of these methods to seamlessly integrate new objects into existing scenes while maintaining visual consistency across multiple viewpoints.", "section": "4.1 Object-Centric Results"}, {"figure_path": "XIScpCMUse/figures/figures_20_2.jpg", "caption": "Figure 12: Qualitative ablation studies of flow guidance.", "description": "This figure shows a qualitative comparison of the results obtained using different flow guidance strategies in the MVInpainter model. The top row displays the reference images and masked inputs. Subsequent rows show results obtained without flow guidance, with dense flow guidance, and with slot-attention-based flow grouping.  The comparison highlights how the choice of flow guidance impacts the inpainting results, particularly for challenging scenes involving significant viewpoint changes, such as stop signs and laptops.", "section": "4.4 Ablation Study"}, {"figure_path": "XIScpCMUse/figures/figures_21_1.jpg", "caption": "Figure 13: Scene editing results and adaptively warped masks with different captions. Object insertions are all based on the removal results with the caption: \u201cbackground\u201d. Zoom-in for details.", "description": "This figure demonstrates the results of scene editing using the MVInpainter model.  It showcases the model's ability to perform object removal and insertion across multiple views.  The process involves adaptively warping masks to ensure seamless integration with the scene, even with viewpoint changes.  Each row shows a different object (background, brown bag, chocolate cake, hotdog, brown cap, blue bag, apple, baseball bat) that is inserted using the inpainted background image as a reference. The masks are created adaptively to fit the foreground object's shape and perspective in each view.", "section": "4 Experiments"}, {"figure_path": "XIScpCMUse/figures/figures_22_1.jpg", "caption": "Figure 1: MVInpainter addresses 2D/3D editing tasks: (a) novel view synthesis, (b) multi-view object removal, and (c) object insertion and replacement through multi-view consistent inpainting ability. Given one inpainted or edited reference image, MVInpainter spreads it to other masked views without pose conditions. (d) MVInpainter could be applied to real-world 3D scene editing for dense point clouds by Dust3R [76] or Multi-View Stereo (MVS) [9] and 3DGS [35] with consistent generation.", "description": "This figure showcases the four main tasks addressed by the MVInpainter model: novel view synthesis, multi-view object removal, multi-view object insertion/replacement, and 3D scene editing.  It highlights the model's ability to consistently inpaint across multiple views using a single edited reference image, eliminating the need for camera pose information. The final panel demonstrates how this 2D inpainting approach extends to 3D scene editing by using existing 3D reconstruction methods like Dust3R, Multi-View Stereo, and 3D Gaussian Splatting.", "section": "1 Introduction"}, {"figure_path": "XIScpCMUse/figures/figures_22_2.jpg", "caption": "Figure 15: Scene editing results and adaptively warped masks with different captions. Object insertions are all based on the removal results with the caption: \u201cbackground\u201d. Zoom-in for details.", "description": "This figure shows examples of scene editing using MVInpainter.  The top row displays the original images with masks. The subsequent rows demonstrate results from editing the images using various captions. Specifically, object removal and replacement were performed, with the object removals using the caption \u201cbackground\u201d to guide the inpainting. Different masking strategies and object types were applied to showcase the flexibility and generalizability of the MVInpainter method. The images highlight the successful integration of the inpainted objects with the existing scenes, preserving contextual consistency and visual coherence.", "section": "4 Experiments"}, {"figure_path": "XIScpCMUse/figures/figures_23_1.jpg", "caption": "Figure 16: The visualization of frame interpolated object removal and insertion. (a) shows expanding results with (\u00d74) length from 6 inpainted views. (b) denotes the long-range interpolation with fixed conditions (first 7 views).", "description": "This figure visualizes the frame interpolation method used in MVInpainter for handling long sequences.  Part (a) demonstrates expanding results by a factor of four, starting from six initially inpainted views. Part (b) shows long-range interpolation where the first seven views are fixed as conditions for subsequent inpainting.", "section": "B.2 Frame Interpolation"}, {"figure_path": "XIScpCMUse/figures/figures_23_2.jpg", "caption": "Figure 17: Visualization of the color difference issue with different VAE decoders.", "description": "The figure compares the results of using a vanilla VAE decoder and an augmented VAE decoder in the MVInpainter model.  The left shows the improvement on a stairway image with a masked area. The right illustrates the effect on an image of a piece of equipment with a masked region. In both cases, the augmented VAE decoder produced more consistent and visually pleasing inpainting results, mitigating the color differences that were present near the masked boundaries when using the vanilla VAE decoder.", "section": "B.3 Asymmetric VAE Decoder"}, {"figure_path": "XIScpCMUse/figures/figures_23_3.jpg", "caption": "Figure 18: Results of baseline methods with (Baseline) and without inpainting formulation (SD-blend, SD-NVS).", "description": "This figure compares the results of three different methods: the baseline method and two methods without the inpainting formulation (SD-blend and SD-NVS).  The image shows the input images and the outputs generated by each method for two different objects. This is used to highlight the effect of inpainting formulation in achieving high quality multi-view consistent results.", "section": "B.4 Detailed Ablation for Inpainting Formulation"}, {"figure_path": "XIScpCMUse/figures/figures_25_1.jpg", "caption": "Figure 19: Object removal compared to SPIn-NeRF [51].", "description": "This figure compares the object removal results of the proposed method, MVInpainter, with those of SPIn-NeRF, a state-of-the-art NeRF editing method. The comparison highlights the differences in the quality of object removal and the overall visual consistency across multiple views. The top row shows masked input images, while subsequent rows illustrate the results of MVInpainter (with blending of original and inpainted regions) and SPIn-NeRF (with and without blending).  The figure demonstrates MVInpainter's ability to generate visually more pleasing and consistent results compared to SPIn-NeRF, particularly with respect to object removal.", "section": "4.2 Forward-Facing Results"}, {"figure_path": "XIScpCMUse/figures/figures_26_1.jpg", "caption": "Figure 1: MVInpainter addresses 2D/3D editing tasks: (a) novel view synthesis, (b) multi-view object removal, and (c) object insertion and replacement through multi-view consistent inpainting ability. Given one inpainted or edited reference image, MVInpainter spreads it to other masked views without pose conditions. (d) MVInpainter could be applied to real-world 3D scene editing for dense point clouds by Dust3R [76] or Multi-View Stereo (MVS) [9] and 3DGS [35] with consistent generation.", "description": "This figure shows four examples of how MVInpainter can be used for 2D and 3D scene editing. (a) shows novel view synthesis, where a new view of a scene is generated from a single reference image. (b) shows multi-view object removal, where an object is removed from multiple views of a scene. (c) shows multi-view object insertion and replacement, where an object is inserted or replaced in multiple views of a scene. (d) shows how MVInpainter can be applied to real-world 3D scene editing, using point clouds generated by Dust3R or Multi-View Stereo (MVS) and 3DGS.  The key takeaway is that MVInpainter uses a multi-view consistent inpainting approach to bridge 2D and 3D editing, which eliminates the need for explicit camera poses and simplifies the editing process.", "section": "1 Introduction"}, {"figure_path": "XIScpCMUse/figures/figures_26_2.jpg", "caption": "Figure 21: The visualization of 3D scene reconstruction based on MVS. (a) denote point clouds produced by MVSFormer++ [9]. (b) show rendered 3DGS test views.", "description": "This figure shows the results of 3D scene reconstruction using multi-view stereo (MVS) and 3D Gaussian splatting (3DGS).  The (a) part displays point clouds generated by the MVSFormer++ method, which serves as input for the 3DGS. The (b) part presents test views generated by the 3DGS model, demonstrating the final 3D reconstruction from multi-view 2D inpainting results.", "section": "C 3D Scene Reconstruction"}, {"figure_path": "XIScpCMUse/figures/figures_27_1.jpg", "caption": "Figure 1: MVInpainter addresses 2D/3D editing tasks: (a) novel view synthesis, (b) multi-view object removal, and (c) object insertion and replacement through multi-view consistent inpainting ability. Given one inpainted or edited reference image, MVInpainter spreads it to other masked views without pose conditions. (d) MVInpainter could be applied to real-world 3D scene editing for dense point clouds by Dust3R [76] or Multi-View Stereo (MVS) [9] and 3DGS [35] with consistent generation.", "description": "This figure showcases the capabilities of MVInpainter in addressing various 2D and 3D editing tasks.  It demonstrates the ability to perform novel view synthesis, multi-view object removal, and object insertion/replacement using a multi-view consistent inpainting approach. The key aspect is that MVInpainter can extend a single, edited reference image to other views without needing explicit camera pose information, simplifying the process significantly. It further highlights the applicability of MVInpainter to real-world 3D scene editing by leveraging existing 3D reconstruction techniques like Dust3R and Multi-View Stereo.", "section": "1 Introduction"}]