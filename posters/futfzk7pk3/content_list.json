[{"type": "text", "text": "The Power of Extrapolation in Federated Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hanmin Li   \nGenAI Center of Excellence KAUST, Saudi Arabia   \nhanmin.li@kaust.edu.sa Kirill Acharya\u2021   \nGenAI Center of Excellence KAUST, Saudi Arabia   \nacharya.kk@phystech.edu ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Peter Richt\u00e1rik GenAI Center of Excellence KAUST, Saudi Arabia peter.richtarik@kaust.edu.sa ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We propose and study several server-extrapolation strategies for enhancing the theoretical and empirical convergence properties of the popular federated learning optimizer FedProx [Li et al., 2020]. While it has long been known that some form of extrapolation can help in the practice of FL, only a handful of works provide any theoretical guarantees. The phenomenon seems elusive, and our current theoretical understanding remains severely incomplete. In our work, we focus on smooth convex or strongly convex problems in the interpolation regime. In particular, we propose Extrapolated FedProx (FedExProx), and study three extrapolation strategies: a constant strategy (depending on various smoothness parameters and the number of participating devices), and two smoothness-adaptive strategies; one based on the notion of gradient diversity (FedExProx-GraDS), and the other one based on the stochastic Polyak stepsize (FedExProx-StoPS). Our theory is corroborated with carefully constructed numerical experiments. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated learning (FL) is a distributed training approach for machine learning models, where multiple clients collaborate under the guidance of a central server to optimize a loss function [Konec\u02c7n\u00fd et al., 2016, McMahan et al., 2017]. This method allows clients to contribute to model training while keeping their data private, as it avoids the need for direct data sharing. Often, federated optimization is formulated as the minimization of a finite-sum objective function, ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\left\\{f(x):=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(x)\\right\\},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where each $f_{i}:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ is the empirical risk of model $x$ associated with the $i$ -th client. The federated averaging method (FedAvg) is among the most favored strategies for addressing federated learning problems, as proposed by McMahan et al. [2017], Mangasarian and Solodov [1993]. In FedAvg, the server initiates an iteration by selecting a subset of clients for participation in a given round. Each chosen client then proceeds with local training, employing gradient-based techniques like gradient descent (GD) or stochastic gradient descent (SGD) with random reshuffilng, as discussed by Bubeck et al. [2015], Gower et al. [2019], Moulines and Bach [2011], Sadiev et al. [2022]. ", "page_idx": 0}, {"type": "text", "text": "Li et al. [2020] proposed replacing the local training of each client via SGD in FedAvg with the computation of a proximal term, resulting in the FedProx algorithm. ", "page_idx": 1}, {"type": "equation", "text": "$$\nx_{k+1}=\\frac{1}{n}\\sum_{i=1}^{n}\\operatorname{prox}_{\\gamma f_{i}}\\left(x_{k}\\right),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\gamma>0$ is the step size, and the proximal operator is defined as ", "page_idx": 1}, {"type": "equation", "text": "$$\n{\\mathrm{prox}}_{\\gamma f_{i}}\\left(x\\right):={\\mathrm{arg}}\\operatorname*{min}_{z\\in\\mathbb{R}^{d}}\\left\\{f_{i}\\left(z\\right)+{\\frac{1}{2\\gamma}}\\left\\Vert z-x\\right\\Vert^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Contrary to gradient-based methods like $\\mathrm{GD}$ and SGD, algorithms based on proximal operation, such as proximal point method (PPM) [Rockafellar, 1976, Parikh et al., 2014] and stochastic proximal point methods (SPPM) [Asi and Duchi, 2019, Bertsekas, 2011, Khaled and Jin, 2022, Patrascu and Necoara, 2018, Richt\u00e1rik and Tak\u00e1c, 2020] benefti from stability against inaccuracies in learning rate specification [Ryu and Boyd, 2014]. Indeed, for GD and SGD, a step size that is excessively large can result in divergence of the algorithm, whereas a step size that is too small can significantly deteriorate the convergence rate of the algorithm. PPM was formally introduced and popularized by the seminal paper of Rockafellar [1976] to solve the variational inequality problems. In practice, the stochastic variant SPPM is more frequently used. ", "page_idx": 1}, {"type": "text", "text": "It is known that the proximal operator applied to a proper, closed and convex function can be viewed as the projection to some level set of the same function depending on the value of $\\gamma$ . In particular, if we let each $f_{i}$ be the indicator function of a nonempty closed convex set $\\mathcal{X}_{i}$ , then $\\mathrm{prox}_{\\gamma f_{i}}$ (\u00b7) becomes the projection $\\Pi_{\\mathcal{X}_{i}}(\\cdot)$ onto the set $\\mathcal{X}_{i}$ . In this case, FedProx in (2) becomes the parallel projection method for convex feasibility problem [Censor et al., 2001, 2012, Combettes, 1997a, Necoara et al., 2019], if we additionally assume ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathscr{X}:=\\bigcap_{i=1}^{n}\\mathscr{X}_{i}\\neq\\emptyset.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "A well known fact about the parallel projection method is that its empirical efficiency can often be improved by extrapolation [Combettes, 1997a, Necoara et al., 2019]. This involves moving further along the line that connects the last iterate $x_{k}$ and the average projection point, resulting in the iteration ", "page_idx": 1}, {"type": "equation", "text": "$$\nx_{k+1}=x_{k}+\\alpha_{k}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\Pi_{{\\mathcal{X}}_{i}}\\left(x_{k}\\right)-x_{k}\\right),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\alpha_{k}~\\geq~1$ defines extrapolation level. Despite the various heuristic rules proposed over the years for setting $\\alpha_{k}$ [Bauschke et al., 2006, Censor et al., 2001, Combettes, 1997b], which have demonstrated satisfactory practical performance, it was only recently that the theoretical foundation explaining the success of extrapolation techniques for solving convex feasibility problems was unveiled by Necoara et al. [2019], where the authors considered randomized version of (3) named Randomized Projection Method (RPM). The practical success of extrapolation has spurred numerous extensions of existing algorithms. Notably, Jhunjhunwala et al. [2023] combined FedAvg with extrapolation, resulting in FedExP, leveraging insights from an effective heuristic rule [Combettes, 1997b] for setting $\\alpha_{k}$ as follows: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\alpha_{k}=\\frac{\\sum_{i=1}^{n}\\left\\|x_{k}-\\Pi_{\\mathcal{X}_{i}}(x_{k})\\right\\|^{2}}{\\left\\|\\sum_{i=1}^{n}\\left(x_{k}-\\Pi_{\\mathcal{X}_{i}}(x_{k})\\right)\\right\\|^{2}}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "However, the authors did not consider the case of a constant extrapolation parameter, nor did they disclose the relationship between the extrapolation parameter and the stepsize of SGD. The extrapolation parameter can be viewed as a server side stepsize in the context of federated learning, its effectiveness was discussed by Malinovsky et al. [2023]. ", "page_idx": 1}, {"type": "text", "text": "In the field of fixed point methods, extrapolation is also known as over-relaxation [Rechardson, 1911]. It is a technique used to effectively accelerate the convergence of fixed point methods, including gradient algorithms and proximal splitting algorithms [Iutzeler and Hendrickx, 2019, Condat et al., 2023]. ", "page_idx": 1}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our paper contributes in the following ways; for the notations used please refer to Appendix A. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Based on the insights gained from the convex feasibility problem, we extend FedProx to its extrapolated counterpart FedExProx for both convex and strongly1convex interpolation problems (See Table 1). By optimally setting the constant extrapolation parameter, we obtain iteration complexity L\u03b3(1+\u03b3Lmax) 2 in the convex case and O $\\begin{array}{r}{\\mathcal{O}\\left(\\frac{L_{\\gamma}(1+\\gamma L_{\\operatorname*{max}})}{\\mu}\\log\\left(\\frac{1}{\\epsilon}\\right)\\right)}\\end{array}$ in the strongly convex case, when all the clients participate in the training (full participation). We reveal the dependence of the optimal extrapolation parameter on smoothness, indicating that simply averaging the iterates from local training on the server is suboptimal. Instead, extrapolation should be applied to achieve faster convergence. Specifically, compared to FedProx with the same step size \u03b3, our method is always at least 2 +\u03b3L1max $\\begin{array}{r}{2\\^{\\biggr.}+\\frac{1}{\\gamma L_{\\mathrm{max}}}+\\gamma L_{\\mathrm{max}}\\;}\\end{array}$ times better in terms of iteration complexity, see Remark 5. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Our method, FedExProx, improves upon the worst-case iteration complexity $\\mathcal{O}\\left(\\frac{L_{\\mathrm{max}}}{\\epsilon}\\right)$ of FedExP [Jhunjhunwala et al., 2023] to $\\begin{array}{r}{\\mathcal{O}\\left(\\frac{L_{\\gamma}(1+\\gamma L_{\\operatorname*{max}})}{\\epsilon}\\right)}\\end{array}$ (See Table 2). The improvement could lead to acceleration up to a factor of $n$ , see Remark 6. Furthermore, we extend FedExProx to client partial participation setting, showing the dependence of optimal extrapolation parameter on $\\tau$ which is the number of clients participating in the training and the beneftis of a larger $\\tau$ . In particular, we show that compared to the single client setting, with complexity $\\begin{array}{r}{\\bar{\\mathcal{O}}\\left(\\frac{\\bar{L}_{\\operatorname*{max}}}{\\epsilon}\\right)}\\end{array}$ , the full participation version enjoys a speed-up up to a factor of $n$ , see Remark 7. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Our theory uncovers the relationship between the extrapolation parameter and the step size in typical gradient-type methods, leveraging the power of the Moreau envelope. We also recover RPM of Necoara et al. [2019] as a special case in our analysis (see Remark 12), and show that the heuristic outlined in (4), is in fact a step size based on gradient diversity [Horv\u00e1th et al., 2022, Yin et al., 2018] for the Moreau envelopes of client functions. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Building on the insights from Horv\u00e1th et al. [2022], we propose two adaptive rules for determining the extrapolation parameter: based on gradient diversity (FedExProx-GraDS), and the stochastic Polyak step size (FedExProx-StoPS) [Horv\u00e1th et al., 2022, Loizou et al., 2021]. The proposed methods eliminate reliance on the unknown smoothness constant and exhibit \u201csemi-adaptivity\u201d, meaning the algorithm converges with any local step size $\\gamma$ and by selecting a sufficiently large $\\gamma$ , we ensure that we lose at most a factor of 2 in iteration complexity. ", "page_idx": 2}, {"type": "text", "text": "\u2022 We validate our theory with numerical experiments. Numerical evidence suggests that FedExProx achieves a $2\\times$ or higher speed-up in terms of iteration complexity compared to FedProx and improved performance compared to FedExP. The framework and the plots are included in the Appendix. ", "page_idx": 2}, {"type": "text", "text": "1.2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Stochastic gradient descent. SGD [Robbins and Monro, 1951, Ghadimi and Lan, 2013, Gower et al., 2019, Gorbunov et al., 2020] stands as a cornerstone algorithm utilized across the fields of machine learning. In its simplest form, the algorithm is written as $x_{k+1}=x_{k}-\\eta\\cdot g(x_{k})$ , where $\\eta>0$ is a scalar step size, $g(x_{k})$ represents a stochastic estimator of the true gradient $\\nabla f(x_{k})$ . We recover $\\mathrm{GD}$ when $g(x_{k})\\,=\\,\\nabla f(x_{k})$ . The evolution of SGD has been marked by significant advancements since its introduction by Robbins and Monro [1951], leading to various adaptations like stochastic batch gradient descent [Nemirovski et al., 2009] and compressed gradient descent [Alistarh et al., 2017, Khirirat et al., 2018]. Gower et al. [2019] presented a framework for analyzing SGD with arbitrary sampling strategies in the convex setting based on expected smoothness, which was later extended by Gorbunov et al. [2020] to the case of local SGD. While many methods have been crafted to leverage the stochastic nature of $g(x_{k})$ , substantial research efforts are also dedicated to finding a ", "page_idx": 2}, {"type": "text", "text": "Table 1: General comparison of FedExP, RPM and FedExProx in terms of conditions and convergence. Each entry indicates whether the method has the corresponding feature $(\\checkmark)$ or not $(\\pmb{X})$ . We use the sign \u201c\u2014\u201d where a feature is not applicable to the corresponding method. ", "page_idx": 3}, {"type": "table", "img_path": "FuTfZK7PK3/tmp/b80fcd3ab608aa3867a1d0a27a8efd32501edc058168173fc0ff876fefd2ccff.jpg", "table_caption": [], "table_footnote": ["a RPM refers to the randomized projection method of Necoara et al. [2019]. Our method includes it as a special case, see Remark 12 b Convexity: local objective $f_{i}$ is convex, which is the indicator function of the convex set $\\mathscr{X}_{i}$ in RPM. c The strong convexity pertains to $f$ , and for RPM, it indicates that the linear regularity condition is satisfied. d Smoothness: $f_{i}$ is $L_{i}$ -smooth. Our algorithm also applies in the non-smooth case; see Appendix F.2. e Jhunjhunwala et al. [2023] provides no convergence guarantee for client partial participation setting. f The concept of \u201csemi-adaptivity\u201d is explained in Remark 9. "], "page_idx": 3}, {"type": "table", "img_path": "FuTfZK7PK3/tmp/d396551462e60dc0cc066abc2a820c90358d0629410f08d028697bbaa6a6e7f9.jpg", "table_caption": ["Table 2: Comparison of convergence of FedExP, FedProx, FedExProx, FedExProx-GraDS and FedExProx-StoPS. The local step size of FedExP is set to be the largest possible value $^1\\!/\\!6t L$ in the full batch case, where $t$ is the number of local iterations of GD performed. We assume the assumptions of Theorem 1 also hold here. The notations are introduced in Theorem 1 and Theorem 2. The convergence for our methods are described for arbitrary $\\gamma>0$ . We use $K$ to denote the total number of iterations. For FedExProx, optimal constant extrapolation is used. The $\\mathcal{O}\\left(\\cdot\\right)$ notation is hidden for all complexities in this table. "], "table_footnote": ["a The $\\alpha_{k,P}$ here is determined according to the theory of Jhunjhunwala et al. [2023]. b Notice that we always have $\\gamma L_{\\gamma}<1$ , so the complexity of FedProx is strictly worse than FedExProx. c We have $L_{\\gamma}\\left(1+\\gamma L_{\\operatorname*{max}}\\right)\\leq L_{\\operatorname*{max}}$ , see Remark 6. d We leave out a factor of $(1\\!+\\!\\gamma L_{\\operatorname*{max}})\\big/(2\\!+\\!\\gamma L_{\\operatorname*{max}})$ which is a constant between $\\textstyle{\\big(}{\\frac{1}{2}},1{\\big)}$ . e See Remark 11 for a lower bound of $\\alpha_{k,S}$ , using which we can rewrite the rate as $\\frac{L_{\\gamma}(1\\!+\\!\\gamma L_{\\operatorname*{max}})}{K}$ "], "page_idx": 3}, {"type": "text", "text": "better stepsize. An illustration of this is the coordinate-wise adaptive step size Adagrad [Duchi et al., 2011]. Another approach involves employing matrix step size, as demonstrated by Safaryan et al. [2021], Li et al. [2023, 2024]. Our analysis builds on the theory of SGD mainly adapted from Gower et al. [2019] with additional consideration on the upper bound of the step size. ", "page_idx": 3}, {"type": "text", "text": "Stochastic proximal point method. PPM was first introduced by Rockafellar [1976] to address the problems of variational inequalities at its inception. Its transition to stochastic case, motivated by the need to efficiently solve large scale optimization problems, results in SPPM. It is often assumed that the proximity operator can be computed efficiently for the algorithm to be practical. Over the years, SPPM has been the subject of extensive research, as documented by Bertsekas [2011], Bianchi [2016], ", "page_idx": 3}, {"type": "text", "text": "Patrascu and Necoara [2018]. Unlike traditional gradient-based methods, SPPM is more robust to inaccuracies in learning rate specifications, as demonstrated by Ryu and Boyd [2014]. Asi and Duchi [2019] studied APROX, which includes SPPM as the special case using the full proximal model; APROX was later extended into minibatch case by Asi et al. [2020]. However, this extension was based on model averaging rather than iterate averaging. The convergence rate of SPPM has been analyzed in various contexts by Khaled and Jin [2022], Ryu and Boyd [2014], Yuan and Li [2022], revealing that its performance does not surpass that of SGD in non-convex regimes. ", "page_idx": 4}, {"type": "text", "text": "Projection onto convex sets. The projection method originated from efforts to solve systems of linear equations or linear inequalities [Kaczmarz, 1937, Von Neumann, 1949, Motzkin and Schoenberg, 1954]. Subsequently, it was generalized to address the convex feasibility problem [Combettes, 1997b]. Typically, the method involves projecting onto a set $\\mathcal{X}_{i}$ , where $i$ is determined through sampling or other strategies. A particularly relevant method to our paper is the parallel projection method, in which individual projections onto the sets are performed in parallel, and their results are averaged in order to produce the next iterate. It is well-established experimentally that the parallel projection method can be accelerated through extrapolation, with numerous successful heuristics having been proposed to adaptively set the extrapolation parameter [Bauschke et al., 2006, Pierra, 1984]. However, only recently a theory was proposed by Necoara et al. [2019] to explain this phenomenon. Necoara et al. [2019] introduced stochastic reformulations of the convex feasibility problem and revealed how the optimal extrapolation parameter depends on the smoothness of the setting and the size of the minibatch. A better result under a linear regularity condition, which is connected to strong convexity, was also obtained. However, the explanation provided by Necoara et al. [2019] was not satisfactory, as it failed to clarify why adaptive rules based on gradient diversity are effective. ", "page_idx": 4}, {"type": "text", "text": "Moreau envelope. The concept of the Moreau envelope, also known as Moreau-Yosida regularization, was first introduced by Moreau [1965] as a mathematical tool for handling non-smooth functions. A particularly relevant property of the Moreau envelope is that executing proximal minimization algorithms on the original objective is equivalent to applying gradient methods to its Moreau envelope [Ryu and Boyd, 2014]. Based on this observation, Davis and Drusvyatskiy [2019] conducted an analysis of several methods, including SPPM for weakly convex and Lipschitz functions. The properties of the Moreau envelope and its applications have been thoroughly investigated in many works including Jourani et al. [2014], Planiden and Wang [2016, 2019]. Beyond its role in proximal minimization algorithms, the Moreau envelope has been utilized in the contexts of personalized federated learning [T Dinh et al., 2020] and meta-learning [Mishchenko et al., 2023]. ", "page_idx": 4}, {"type": "text", "text": "Adaptive step size. One of the most crucial hyperparameters in training machine learning models with gradient-based methods is the step size. For GD and SGD, determining the step size often depends on the smoothness parameter, which is typically unknown, posing challenges in practical step size selection. There has been a growing interest in adaptive step sizes, leading to the development of numerous adaptive methods that enable real-time computation of the step size. Examples include Adagrad [Duchi et al., 2011], RMSProp [Hinton et al.], and ADAM [Kingma and Ba, 2015]. Recently, several studies have attempted to extend the Polyak step size beyond deterministic settings, leading to the development of the stochastic Polyak step size [Richt\u00e1rik and Tak\u00e1c, 2020, Horv\u00e1th et al., 2022, Loizou et al., 2021, Orvieto et al., 2022]. Gradient diversity, first introduced by Yin et al. [2018], was subsequently analyzed theoretically by Horv\u00e1th et al. [2022]. ", "page_idx": 4}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now introduce the several definitions and assumptions that are used throughout the paper. ", "page_idx": 4}, {"type": "text", "text": "Definition 1 (Proximity operator). The proximity operator of an extended-real-valued function $\\phi:\\mathbb{R}^{d}\\mapsto\\mathbb{R}\\cup\\{+\\infty\\}$ with step size $\\gamma>0$ is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{prox}_{\\gamma\\phi}\\left(x\\right):=\\arg\\operatorname*{min}_{z\\in\\mathbb{R}^{d}}\\left\\{\\phi(z)+\\frac{1}{2\\gamma}\\left\\|z-x\\right\\|^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It is known that for a proper, closed and convex function $\\phi$ , the minimizer of $\\begin{array}{r}{\\phi(z)+\\frac{1}{2\\gamma}\\left\\|z-x\\right\\|^{2}}\\end{array}$ exists and is unique. Throughout this paper, we assume that the proximal operators are evaluated exactly, with no approximation or inexactness. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Extrapolated SPPM $\\scriptstyle\\left({\\mathrm{FedExProx}}\\right)$ ) with partial client participation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "1: Parameters: extrapolation parameter $\\alpha_{k}>0$ , step size for the proximity operator $\\gamma>0$ , starting point $x_{0}\\in\\mathbb{R}^{d}$ , number of clients $n$ , total number of iterations $K$ , number of clients participate in the training $\\tau$ , for simplicity, we use $\\tau$ -nice sampling as an example   \n2: for $k=0,1,2\\ldots K-1$ do   \n3: The server samples $S_{k}\\subseteq\\{1,2,\\ldots,n\\}$ uniformly from all subsets of cardinality $\\tau$   \n4: The server computes ", "page_idx": 5}, {"type": "equation", "text": "$$\nx_{k+1}=x_{k}+\\alpha_{k}\\left(\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\mathrm{prox}_{\\gamma f_{i}}\\left(x_{k}\\right)-x_{k}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "5: end for ", "page_idx": 5}, {"type": "text", "text": "Definition 2 (Moreau envelope). The Moreau envelope of an extended-real-valued function $\\phi:\\mathbb{R}^{d}\\mapsto$ $\\mathbb{R}\\cup\\{+\\infty\\}$ with step size $\\gamma>0$ is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\nM_{\\phi}^{\\gamma}\\left(x\\right):=\\operatorname*{min}_{z\\in\\mathbb{R}^{d}}\\left\\{\\phi(z)+\\frac{1}{2\\gamma}\\left\\Vert z-x\\right\\Vert^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The following assumptions are used in our analysis. We use the notation $[n]$ for the set $\\{1,\\ldots,n\\}$ . ", "page_idx": 5}, {"type": "text", "text": "Assumption 1 (Differentiability). The function $f_{i}$ in (1) is differentiable for all $i\\in[n]$ . ", "page_idx": 5}, {"type": "text", "text": "Assumption 2 (Interpolation regime). There exists $x_{\\star}\\in\\mathbb{R}^{d}$ such that $\\nabla f_{i}(x_{\\star})=0$ for all $i\\in[n]$ . ", "page_idx": 5}, {"type": "text", "text": "Note that Assumption 2 indicates that each $f_{i}$ and $f$ are lower bounded. In this paper, we focus on cases where the interpolation regime holds. This assumption often holds in modern deep learning which are overparameterized where the number of parameters greatly exceeds the number of data points, as justified by Arora et al. [2019], Montanari and Zhong [2022]. Our motivation for this assumption partly arises from the convex feasibility problem [Combettes, 1997a, Necoara et al., 2019], wherein the intersection $\\mathcal{X}$ is presumed nonempty. This is equivalent to assuming that the interpolation regime holds when $f_{i}$ is the indicator function of the nonempty closed convex set $\\mathbf{\\mathcal{X}}_{i}$ . Further motivations derived from the proof for this assumption will be discussed later. ", "page_idx": 5}, {"type": "text", "text": "Assumption 3 (Convexity). The function $f_{i}:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ is convex for all $i\\in[n]$ . This means that for each $f_{i}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n0\\leq f_{i}(x)-f_{i}(y)-\\left\\langle\\nabla f_{i}(y),x-y\\right\\rangle,\\quad\\forall x,y\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Assumption 4 (Smoothness). Function $f_{i}:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ is $L_{i}$ -smooth, $L_{i}>0$ for all $i\\,\\in\\,[n]$ . This means that for each $f_{i}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\nf_{i}(x)-f_{i}(y)-\\langle\\nabla f_{i}(y),x-y\\rangle\\leq\\frac{L_{i}}{2}\\left\\Vert x-y\\right\\Vert^{2},\\quad\\forall x,y\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We will use $L_{\\mathrm{max}}$ to denote $\\operatorname*{max}_{i\\in[n]}L_{i}$ . ", "page_idx": 5}, {"type": "text", "text": "It is important to note that the smoothness assumption here is not necessary to obtain a convergence result, see Appendix F.2 for the detail. We introduce this assumption to highlight how the optimal extrapolation parameter depends on smoothness if it is present. The following strong convexity assumption is introduced that, if adopted, enables us to achieve better results. ", "page_idx": 5}, {"type": "text", "text": "Assumption 5 (Strong convexity). The function $f$ is $\\mu$ -strongly convex, $\\mu>0$ . That is ", "page_idx": 5}, {"type": "equation", "text": "$$\nf(x)-f(y)-\\langle\\nabla f(y),x-y\\rangle\\geq{\\frac{\\mu}{2}}\\left\\|x-y\\right\\|^{2},\\quad\\forall x,y\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We first present our algorithm FedExProx as Algorithm 1. In the subsequent sections, we first present the theory in the stochastic setting for FedExProx with a fixed extrapolation parameter in Section 3. Then we proceed to adaptive versions of our algorithm which eliminates the dependence on the unknown smoothness constant in Section 4. ", "page_idx": 5}, {"type": "text", "text": "3 Constant extrapolation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In order to demonstrate the convergence result of our algorithm in the stochastic setting, we use $\\tau$ -nice sampling as the way of selecting clients for partial participation. This refers to that in each iteration, the server samples a set $S_{k}\\subseteq{\\mathsf{\\bar{\\{}}}}1,2,\\ldots,n{\\bar{\\}}$ uniformly at random from all subsets of size $\\tau$ . We want to emphasize that the sampling strategy here is merely an example, it is possible to use other client sampling strategies. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1. Suppose Assumption 1 (Differentiability), Assumption 2 (Interpolation regime), Assumption 3 (Convexity) and Assumption $^{4}$ (Smoothness) hold. If we use a fixed extrapolation parameter $\\begin{array}{r}{\\alpha_{k}=\\alpha\\in\\left(0,\\frac{2}{\\gamma L_{\\gamma,\\tau}}\\right)}\\end{array}$ and any step size $0<\\gamma<+\\infty,$ , then the average iterate of Algorithm $^{\\,l}$ satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\bar{x}_{K})\\right]-\\operatorname*{inf}{f\\le C\\left(\\gamma,\\tau,\\alpha\\right)}\\cdot\\frac{\\left\\Vert x_{0}-x_{\\star}\\right\\Vert^{2}}{K},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $K$ is the number of iteration, $\\bar{x}_{K}$ is sampled uniformly at random from the first $K$ iterates $\\{x_{0},x_{1},\\dots,x_{K-1}\\}$ , $C\\left(\\gamma,\\tau,\\alpha\\right)$ is defined as ", "page_idx": 6}, {"type": "equation", "text": "$$\nC\\left(\\gamma,\\tau,\\alpha\\right):=\\frac{1+\\gamma L_{\\operatorname*{max}}}{\\alpha\\gamma\\left(2-\\alpha\\gamma L_{\\gamma,\\tau}\\right)}\\quad a n d\\quad L_{\\gamma,\\tau}:=\\frac{n-\\tau}{\\tau(n-1)}\\frac{L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}+\\frac{n(\\tau-1)}{\\tau(n-1)}L_{\\gamma},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $L_{\\mathrm{max}}=\\mathrm{max}_{i}\\,L_{i},\\,.$ $L_{\\gamma}$ is the smoothness constant of $\\begin{array}{r}{M^{\\gamma}\\left(x\\right):=\\frac{1}{n}\\sum_{i=1}^{n}M_{f_{i}}^{\\gamma}\\left(x\\right)}\\end{array}$ . If we fix $\\gamma$ and $\\tau$ the optimal constant extrapolation parameter is given by $\\begin{array}{r}{\\alpha_{\\gamma,\\tau}:=\\frac{1}{\\gamma L_{\\gamma,\\tau}}>1}\\end{array}$ , which results in the following convergence guarantee: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\bar{x}_{K})\\right]-\\operatorname*{inf}{f\\le C(\\gamma,\\tau,\\alpha_{\\gamma,\\tau})}\\cdot\\frac{\\left\\|x_{0}-x_{\\star}\\right\\|^{2}}{K}=L_{\\gamma,\\tau}\\left(1+\\gamma L_{\\operatorname*{max}}\\right)\\cdot\\frac{\\left\\|x_{0}-x_{\\star}\\right\\|^{2}}{K}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The proof of this theorem relies on the reformulation of the update rule in (7), using the identity $\\begin{array}{r}{\\nabla M_{f_{i}}^{\\dot{\\gamma}}\\left(x\\right)=\\frac{1}{\\gamma}\\left(x-\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right)\\right)}\\end{array}$ given in Lemma 2, which holds for any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , into the following form: ", "page_idx": 6}, {"type": "equation", "text": "$$\nx_{k+1}=x_{k}-\\alpha_{k}\\cdot\\gamma\\cdot\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We can then apply our modified theory for SGD given in Theorem 3, which is adapted from Gower et al. [2019], to obtain function value suboptimality in terms of $M^{\\gamma}\\left(x\\right)$ . The results are then translated back to function value suboptimality in terms of $f$ . Note that (8) unveils the connection between the step size of gradient type methods and extrapolation parameter in our case. ", "page_idx": 6}, {"type": "text", "text": "Remark 1. Theorem 1 provides convergence guarantee for Algorithm $^{\\,l}$ in the convex case. If in addition, we assume Assumption $5$ (Strong convexity) holds, the rate can be improved and we obtain linear convergence. See Corollary 1 for the details. ", "page_idx": 6}, {"type": "text", "text": "Remark 2. Theorem 1 indicates convergence for any $0<\\gamma<+\\infty$ . Indeed, as it is proved by Lemma 7, we have $C\\left(\\gamma,\\tau,\\alpha_{\\gamma,\\tau}\\right)=L_{\\gamma,\\tau}$ $(1+\\gamma L_{\\operatorname*{max}})\\,\\leq\\,L_{\\operatorname*{max}}$ holds for any $0<\\gamma<+\\infty$ . In cases where there exists at least one $L_{i}<L_{\\mathrm{max}}$ , we have $C\\left(\\gamma,\\tau,\\alpha_{\\gamma,\\tau}\\right)<L_{\\mathrm{max}}$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 3. One may question the necessity of the interpolation regime assumption. This assumption is crucial to our analysis. Besides allowing us to revisit the convex feasibility problem setting, it also guarantees that $M^{\\gamma}\\left(x\\right)$ has the same set of minimizers as $f(x)$ as illustrated by Lemma 8. It also allows us to improve the upper bound on the step size by a factor of 2 in the SGD theory, which is demonstrated in Theorem 3 in the Appendix. ", "page_idx": 6}, {"type": "text", "text": "Remark 4. From the reformulation presented in (8), we see the best extrapolation parameter is obtained when $\\alpha_{k}{\\gamma}$ is the best step size for SGD running on global objective $M^{\\gamma}\\left(x\\right)$ . Since the best step size is affected by the smoothness and the minibatch size, so is the best extrapolation parameter. ", "page_idx": 6}, {"type": "text", "text": "We can also compare our algorithm with FedProx in the convex overparameterized regime. ", "page_idx": 6}, {"type": "text", "text": "Remark 5. Our algorithm includes FedProx as a special case when $\\alpha=1$ . To recover its result, we simply plug in $\\alpha=1$ , the resulting condition number is $\\begin{array}{r}{C(\\gamma,\\tau,1)=\\frac{1+\\gamma L_{\\operatorname*{max}}}{\\gamma(2-\\gamma L_{\\gamma,\\tau})}}\\end{array}$ . Compared to FedProx, Algorithm 1 with the same $\\gamma>0$ demonstrates superior performance, with the acceleration factor being quantified by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{C(\\gamma,\\tau,1)}{C\\left(\\gamma,\\tau,\\alpha_{\\gamma,\\tau}\\right)}\\geq2+\\frac{1}{\\gamma L_{\\operatorname*{max}}}+\\gamma L_{\\operatorname*{max}}\\geq4.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "See Lemma 14 for the proof. This suggests that the approach of the server averaging all iterates following local computation is suboptimal. ", "page_idx": 7}, {"type": "text", "text": "In the following paragraphs, we study some special cases, ", "page_idx": 7}, {"type": "text", "text": "Full participation case For the full participation case $\\tau=n$ ), using definition from Theorem 1 ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\alpha_{\\gamma,n}=\\frac{1}{\\gamma L_{\\gamma}}>1,\\quad L_{\\gamma,n}=L_{\\gamma},\\quad C\\left(\\gamma,n,\\alpha_{\\gamma,n}\\right)=L_{\\gamma}\\left(1+\\gamma L_{\\mathrm{max}}\\right)\\leq L_{\\mathrm{max}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In this case, we can compare our method with FedExP in the convex overparameterized setting. ", "page_idx": 7}, {"type": "text", "text": "Remark 6. Assume the conditions in Theorem $^{\\,l}$ hold, the worst case iteration complexity of FedExP is given by $\\mathcal{O}\\left(\\frac{L_{\\mathrm{max}}}{\\epsilon}\\right)$ , while for Algorithm $^{\\,l}$ , it is $\\mathcal{O}\\left(\\frac{C(\\gamma,n,\\alpha_{\\gamma,n})}{\\epsilon}\\right)$ . As suggested by Lemma 7, Algorithm 1 has a better iteration complexity $\\left(C\\left(\\gamma,n,\\alpha_{\\gamma,n}\\right)<L_{\\operatorname*{max}}\\right)$ whenever there exists $L_{i}\\neq L_{\\operatorname*{max}}$ for some $i\\in[n]$ , and the acceleration could reach up to a factor of n as suggested by Example 1. In general, the speed-up in the worst case is quantified by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}\\cdot\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\frac{L_{i}}{1+\\gamma L_{i}}\\right)^{-1}\\leq\\frac{L_{\\operatorname*{max}}}{C\\left(\\gamma,n,\\alpha_{\\gamma,n}\\right)}\\leq n\\cdot\\frac{L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}\\cdot\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\frac{L_{i}}{1+\\gamma L_{i}}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Single client case For the single client case ( $\\tau=1$ ), using definition from Theorem 1 ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\alpha_{\\gamma,1}=1+\\frac{1}{\\gamma L_{\\operatorname*{max}}}>1,\\quad L_{\\gamma,1}=\\frac{L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}},\\quad C\\left(\\gamma,1,\\alpha_{\\gamma,1}\\right)=L_{\\operatorname*{max}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark 7. Compared with full and partial client participation, the following relations hold for any $\\tau\\in[n]$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\nC\\left(\\gamma,n,\\alpha_{\\gamma,n}\\right)\\leq C\\left(\\gamma,\\tau,\\alpha_{\\gamma,\\tau}\\right)\\leq C\\left(\\gamma,1,\\alpha_{\\gamma,1}\\right)\\quad a n d\\quad\\alpha_{\\gamma,1}\\leq\\alpha_{\\gamma,\\tau}\\leq\\alpha_{\\gamma,n},\\quad\\forall\\tau\\in[n].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Since the iteration complexity of FedExProx is given by C(\u03b3,\u03c4\u03f5,\u03b1\u03b3,\u03c4 ) , the above inequalities tell us a larger client minibatch size $\\tau$ leads to a larger extrapolation and a better iteration complexity. Specifically, Lemma 7 suggests the improvement over the single client case could be as much as a factor of $n$ $\\begin{array}{r}{\\stackrel{.}{\\iota}(C\\left(\\gamma,n,\\alpha_{\\gamma,n}\\right)=\\frac{1}{n}C\\left(\\gamma,\\dot{1},\\alpha_{\\gamma,1}\\right))}\\end{array}$ as suggested by Example 1. ", "page_idx": 7}, {"type": "text", "text": "4 Adaptive extrapolation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Observe that in Theorem 1, in order to determine the optimal extrapolation, we require the knowledge of $L_{\\gamma,\\tau}$ , which is typically unknown. Although theoretically it suggests that simply averaging the iterates may result in suboptimal performance, in practice, this implication is less significant. To address this issue, we introduced two variants of FedExProx, based on gradient diversity and stochastic Polyak step size, given their relation to the extrapolation parameter in our cases. ", "page_idx": 7}, {"type": "text", "text": "Theorem 2. Suppose Assumption 1 (Differentiability), Assumption 2 (Interpolation regime), Assumption 3 (Convexity) and Assumption $^{4}$ (Smoothness) hold. ", "page_idx": 7}, {"type": "text", "text": "(i) (FedExProx-GraDS): If we are using $\\alpha_{k}=\\alpha_{k,G}$ , where ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\alpha_{k,G}:=\\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\left\\Vert x_{k}-\\mathrm{prox}_{\\gamma f_{i}}\\left(x_{k}\\right)\\right\\Vert^{2}}{\\left\\Vert\\frac{1}{n}\\sum_{i=1}^{n}\\left(x_{k}-\\mathrm{prox}_{\\gamma f_{i}}\\left(x_{k}\\right)\\right)\\right\\Vert^{2}}\\geq1,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "then the iterates of Algorithm $^{\\,l}$ with $\\tau=n$ satisfy ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\bar{x}_{K})\\right]-\\operatorname*{inf}{f\\leq\\frac{1+\\gamma L_{\\operatorname*{max}}}{2+\\gamma L_{\\operatorname*{max}}}\\cdot\\left(\\frac{1}{\\gamma}+L_{\\operatorname*{max}}\\right)\\cdot\\frac{\\|x_{0}-x_{\\star}\\|^{2}}{\\sum_{k=0}^{K-1}\\alpha_{k,G}}},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\bar{x}_{K}$ is chosen randomly from the first $K$ iterates $\\{x_{0},x_{1},...,x_{K-1}\\}$ with probabilities $\\begin{array}{r}{p_{k}=\\alpha_{k,G}\\bigl/\\!\\sum_{k=0}^{K-1}\\alpha_{k,G}}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "(ii) (FedExProx-StoPS): If we are using $\\alpha_{k}=\\alpha_{k,S}$ , where, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\alpha_{k,S}:=\\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\left(M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)-\\operatorname*{inf}M_{f_{i}}^{\\gamma}\\right)}{\\gamma\\left\\Vert\\frac{1}{n}\\sum_{i=1}^{n}\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)\\right\\Vert^{2}}\\geq\\frac{1}{2\\gamma L_{\\gamma}},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "then the iterates of Algorithm $^{\\,l}$ with $\\tau=n$ satisfy ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\bar{x}_{K})\\right]-\\operatorname*{inf}{f}\\le\\left(\\frac{1}{\\gamma}+L_{\\operatorname*{max}}\\right)\\cdot\\frac{\\left\\Vert x_{0}-x_{\\star}\\right\\Vert^{2}}{\\sum_{k=0}^{K-1}\\alpha_{k,S}},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\bar{x}_{K}$ is chosen randomly from the first $K$ iterates $\\{x_{0},x_{1},...,x_{K-1}\\}$ with probabilities $\\begin{array}{r}{p_{k}=\\alpha_{k,S}\\Big/\\!\\sum_{k=0}^{K-1}\\alpha_{k,S}}\\end{array}$ . ", "page_idx": 8}, {"type": "text", "text": "Theorem 2 describes the convergence in the full participation setting. However, we can also extend it to the stochastic setting by implementing a stochastic version of these adaptive step size rules for gradient-based methods [Horv\u00e1th et al., 2022, Loizou et al., 2021]. See Theorem 5 in the Appendix for the details. ", "page_idx": 8}, {"type": "text", "text": "Remark 8. In fact, the adaptive rule based on gradient diversity can be improved by using1+L\u03b3mLamxax instead of $\\frac{1}{\\gamma}$ as the maximum of local smoothness constant of Moreau envelops, resulting in the extrapolation, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\alpha_{k}=\\alpha_{k,G}^{\\prime}:=\\frac{1+\\gamma L_{\\operatorname*{max}}}{\\gamma L_{\\operatorname*{max}}}\\cdot\\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\left\\Vert x_{k}-\\mathrm{prox}_{\\gamma f_{i}}\\left(x_{k}\\right)\\right\\Vert^{2}}{\\left\\Vert\\frac{1}{n}\\sum_{i=1}^{n}\\left(x_{k}-\\mathrm{prox}_{\\gamma f_{i}}\\left(x_{k}\\right)\\right)\\right\\Vert^{2}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "One can obtain a slightly better convergence guarantee than the FedExProx-GraDS case in Theorem 2, see Corollary 2 in the Appendix. However, the requires the knowledge of $L_{\\mathrm{max}}$ in order to compute $\\frac{1\\!+\\!\\gamma L_{\\mathrm{max}}}{\\gamma L_{\\mathrm{max}}}$ ", "page_idx": 8}, {"type": "text", "text": "Remark 9. Note that, compared to classical gradient-based methods, FedExProx-GraDS benefits from \u201csemi-adaptivity\u201d. This refers to the fact that the algorithm converges for any choice of $\\gamma>0$ . Although a smaller \u03b3 hinders convergence, setting it to at least Lm1ax limits the worsening of the convergence to a factor of 2. ", "page_idx": 8}, {"type": "text", "text": "Remark 10. Compared to FedExProx with the optimal constant extrapolation parameter, we gain \u201csemi-adaptivity\u201d here by using the gradient diversity based extrapolation. However, this results in losing the favorable dependence of convergence on $L_{\\gamma}$ and instead establishes a dependence on Lmax. ", "page_idx": 8}, {"type": "text", "text": "Remark 11. For FedExProx-StoPS, as it is suggested by Lemma 20, the convergence depends on the favorable smoothness constant $L_{\\gamma}$ , rather than on $L_{\\mathrm{max}}$ . However, this comes at the price of having to know the minimum of each individual Moreau envelope. ", "page_idx": 8}, {"type": "text", "text": "For a detailed discussion of the adaptive variants of FedExProx, we refer the readers to Appendix F.5. Since one of our starting points is the RPM by Necoara et al. [2019] to solve the convex feasibility problem with non-smooth local objectives, we have also adapted our method to non-smooth cases, as detailed in Theorem 4 in the Appendix. We also provided a discussion of our method in the non-interpolated setting and in the non-convex setting in Appendix F. ", "page_idx": 8}, {"type": "text", "text": "Finally, we support our findings with experiments, see Figure 1 for a simple experiment confirming that FedExProx indeed has a better iteration complexity than FedProx. For more details on the experiments, we refer the readers to Appendix I in the Appendix. Notice that in practice, each local proximity operator can be solved using different oracles. Clients may use GD or SGD to solve the local problem to a certain accuracy. The complexity of this subroutine depends on the local stepsize. If $\\gamma$ is large, the local problem becomes harder to solve because we aim to minimize the local objective itself. Conversely, if it is small, the problem is easier since we do not stray far from the current iterate. As the choice of subroutine affects local computation complexity, comparing it directly with FedExP becomes complicated. Therefore, we compare the iteration complexity (number of communication rounds) of the two algorithms, assuming efficient local computations are carried out by the clients. ", "page_idx": 8}, {"type": "image", "img_path": "FuTfZK7PK3/tmp/b5bf839c0ce3578f9375aea9f482b8a4d4cc01430f4cf914d8b0abf1ca835ba5.jpg", "img_caption": ["Figure 1: Comparison of FedExProx and FedProx in terms of iteration complexity in the full participation setting. The notation $\\gamma$ here denotes the local step size of the proximity operator and $\\alpha_{\\gamma,n}$ is the corresponding optimal extrapolation parameter computed in (9) in the full participation case. In all cases, our proposed algorithm outperforms FedProx, suggesting that the practice of simply averaging the iterates is suboptimal. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "5.1 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our analysis of FedExProx serves as an initial step in adding extrapolation to FedProx, which currently relies on the suboptimal practice of the server merely averaging the iterates. While we discuss the behavior of our algorithm in non-interpolated and non-convex scenarios, our analysis only validates the effectiveness of extrapolation under the interpolation regime assumption. ", "page_idx": 9}, {"type": "text", "text": "5.2 Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "As we have just mentioned, extending our method and analysis beyond interpolation and convex regime is intriguing. In this case, new techniques may be needed for variance reduction. It is also interesting to investigate whether extrapolation can be applied together with client-specific personalization. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The research reported in this publication was supported by funding from King Abdullah University of Science and Technology (KAUST): i) KAUST Baseline Research Scheme, ii) Center of Excellence for Generative AI, under award number 5940, iii) SDAIA-KAUST Center of Excellence in Artificial Intelligence and Data Science. The work was done during Kirill Acharya\u2019s internship at KAUST. Kirill Acharya was also affliiated with MIPT, ISP RAS, Russia. The work of Kirill Acharya was also supported by Grant App. No. 2 to Agreement No. 075-03-2024-214. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic. QSGD: Communication-efficient SGD via gradient quantization and encoding. Advances in Neural Information Processing Systems, 30, 2017.   \nS. Arora, S. Du, W. Hu, Z. Li, and R. Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In International Conference on Machine Learning, pages 322\u2013332. PMLR, 2019.   \nH. Asi and J. C. Duchi. Stochastic (approximate) proximal point methods: Convergence, optimality, and adaptivity. SIAM Journal on Optimization, 29(3):2257\u20132290, 2019.   \nH. Asi, K. Chadha, G. Cheng, and J. C. Duchi. Minibatch stochastic approximate proximal point methods. Advances in Neural Information Processing Systems, 33:21958\u201321968, 2020.   \nH. H. lation algorithm for affine-convex feasibility problems. Numerical Algorithms, 41:239\u2013274, 2006.   \nA. Beck. First-order methods in optimization. SIAM, 2017.   \nD. P. Bertsekas. Incremental proximal methods for large scale convex optimization. Mathematical Programming, 129(2):163\u2013195, 2011.   \nP. Bianchi. Ergodic convergence of a stochastic proximal point algorithm. SIAM Journal on Optimization, 26(4):2235\u20132260, 2016.   \nA. B\u00f6hm and S. J. Wright. Variable smoothing for weakly convex composite functions. Journal of Optimization Theory and Applications, 188:628\u2013649, 2021.   \nS. Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends\u00ae in Machine Learning, 8(3-4):231\u2013357, 2015.   \nY. Censor, T. Elfving, and G. Herman. Averaging strings of sequential iterations for convex feasibility problems. In Studies in Computational Mathematics, volume 8, pages 101\u2013113. Elsevier, 2001.   \nY. Censor, W. Chen, P. L. Combettes, R. Davidi, and G. T. Herman. On the effectiveness of projection methods for convex feasibility problems with linear inequality constraints. Computational Optimization and Applications, 51:1065\u20131088, 2012.   \nP. L. Combettes. Convex set theoretic image recovery by extrapolated iterations of parallel subgradient projections. IEEE Transactions on Image Processing, 6(4):493\u2013506, 1997a.   \nP. L. Combettes. Hilbertian convex feasibility problem: Convergence of projection methods. Applied Mathematics and Optimization, 35(3):311\u2013330, 1997b.   \nL. Condat, D. Kitahara, A. Contreras, and A. Hirabayashi. Proximal splitting algorithms for convex optimization: A tour of recent advances, with new twists. SIAM Review, 65(2):375\u2013435, 2023.   \nD. Davis and D. Drusvyatskiy. Stochastic model-based minimization of weakly convex functions. SIAM Journal on Optimization, 29(1):207\u2013239, 2019.   \nJ. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(7), 2011.   \nS. Ghadimi and G. Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013.   \nE. Gorbunov, F. Hanzely, and P. Richt\u00e1rik. A unified theory of SGD: Variance reduction, sampling, quantization and coordinate descent. In International Conference on Artificial Intelligence and Statistics, pages 680\u2013690. PMLR, 2020.   \nR. M. Gower, N. Loizou, X. Qian, A. Sailanbayev, E. Shulgin, and P. Richt\u00e1rik. SGD: General analysis and improved rates. In International Conference on Machine Learning, pages 5200\u20135209. PMLR, 2019.   \nG. Hinton, N. Srivastava, and K. Swersky. Neural networks for machine learning lecture 6a overview of mini-batch gradient descent.   \nS. Horv\u00e1th, K. Mishchenko, and P. Richt\u00e1rik. Adaptive learning rates for faster stochastic gradient methods. arXiv preprint arXiv:2208.05287, 2022.   \nF. Iutzeler and J. M. Hendrickx. A generic online acceleration scheme for optimization algorithms via relaxation and inertia. Optimization Methods and Software, 34(2):383\u2013405, 2019.   \nD. Jhunjhunwala, S. Wang, and G. Joshi. FedExP: Speeding up federated averaging via extrapolation. In International Conference on Learning Representations, 2023.   \nA. Jourani, L. Thibault, and D. Zagrodny. Differential properties of the moreau envelope. Journal of Functional Analysis, 266(3):1185\u20131237, 2014.   \nS. Kaczmarz. Approximate solution of systems of linear equations. International Journal of Control, 57(6):1269\u20131271, 1937.   \nA. Khaled and C. Jin. Faster federated optimization under second-order similarity. In The Eleventh International Conference on Learning Representations, 2022.   \nA. Khaled and P. Richt\u00e1rik. Better theory for SGD in the nonconvex world. Transactions on Machine Learning Research, 2023.   \nS. Khirirat, H. R. Feyzmahdavian, and M. Johansson. Distributed learning with compressed gradients. arXiv preprint arXiv:1806.06573, 2018.   \nD. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, San Diego, CA, USA, 2015.   \nJ. Konec\u02c7n\u00fd, H. B. McMahan, F. X. Yu, P. Richt\u00e1rik, A. T. Suresh, and D. Bacon. Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492, 8, 2016.   \nH. Li, A. Karagulyan, and P. Richt\u00e1rik. Marina meets matrix stepsizes: Variance reduced distributed non-convex optimization. arXiv preprint arXiv:2310.04614, 2023.   \nH. Li, A. Karagulyan, and P. Richt\u00e1rik. Det-CGD: Compressed gradient descent with matrix stepsizes for non-convex optimization. In International Conference on Learning Representations, 2024.   \nT. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith. Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems, 2:429\u2013450, 2020.   \nN. Loizou, S. Vaswani, I. H. Laradji, and S. Lacoste-Julien. Stochastic Polyak step-size for SGD: An adaptive learning rate for fast convergence. In International Conference on Artificial Intelligence and Statistics, pages 1306\u20131314. PMLR, 2021.   \nG. Malinovsky, K. Mishchenko, and P. Richt\u00e1rik. Server-side stepsizes and sampling without replacement provably help in federated optimization. In Proceedings of the 4th International Workshop on Distributed Machine Learning, DistributedML \u201923, page 85\u2013104, 2023.   \nO. L. Mangasarian and M. V. Solodov. Backpropagation convergence via deterministic nonmonotone perturbed minimization. Advances in Neural Information Processing Systems, 6, 1993.   \nB. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics, pages 1273\u20131282. PMLR, 2017.   \nK. Mishchenko, S. Hanzely, and P. Richt\u00e1rik. Convergence of first-order algorithms for meta-learning with Moreau envelopes. arXiv preprint arXiv:2301.06806, 2023.   \nA. Montanari and Y. Zhong. The interpolation phase transition in neural networks: Memorization and generalization under lazy training. The Annals of Statistics, 50(5):2816\u20132847, 2022.   \nJ.-J. Moreau. Proximit\u00e9 et dualit\u00e9 dans un espace Hilbertien. Bulletin de la Soci\u00e9t\u00e9 Math\u00e9matique de France, 93:273\u2013299, 1965.   \nT. S. Motzkin and I. J. Schoenberg. The relaxation method for linear inequalities. Canadian Journal of Mathematics, 6:393\u2013404, 1954.   \nE. Moulines and F. Bach. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. Advances in Neural Information Processing Systems, 24, 2011.   \nI. Necoara, P. Richt\u00e1rik, and A. Patrascu. Randomized projection methods for convex feasibility: Conditioning and convergence rates. SIAM Journal on Optimization, 29(4):2814\u20132852, 2019.   \nA. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574\u20131609, 2009.   \nA. Orvieto, S. Lacoste-Julien, and N. Loizou. Dynamics of SGD with stochastic Polyak stepsizes: Truly adaptive variants and convergence to exact solution. Advances in Neural Information Processing Systems, 35:26943\u201326954, 2022.   \nN. Parikh, S. Boyd, et al. Proximal algorithms. Foundations and Trends\u00ae in Optimization, 1(3): 127\u2013239, 2014.   \nA. Patrascu and I. Necoara. Nonasymptotic convergence of stochastic proximal point methods for constrained convex optimization. Journal of Machine Learning Research, 18(198):1\u201342, 2018.   \nG. Pierra. Decomposition through formalization in a product space. Mathematical Programming, 28: 96\u2013115, 1984.   \nC. Planiden and X. Wang. Strongly convex functions, Moreau envelopes, and the generic nature of convex functions with strong minimizers. SIAM Journal on Optimization, 26(2):1341\u20131364, 2016.   \nC. Planiden and X. Wang. Proximal mappings and Moreau envelopes of single-variable convex piecewise cubic functions and multivariable gauge functions. Nonsmooth Optimization and Its Applications, pages 89\u2013130, 2019.   \nL. Rechardson. The approximate arithmetical solution by finite difference of physical problems involving differential equations, with an application to the stresses in a masonary dam. R. Soc. London Phil. Trans. A, 210:307\u2013357, 1911.   \nP. Richt\u00e1rik and M. Tak\u00e1c. Stochastic reformulations of linear systems: algorithms and convergence theory. SIAM Journal on Matrix Analysis and Applications, 41(2):487\u2013524, 2020.   \nH. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics, pages 400\u2013407, 1951.   \nR. T. Rockafellar. Monotone operators and the proximal point algorithm. SIAM Journal on Control and Optimization, 14(5):877\u2013898, 1976.   \nE. K. Ryu and S. Boyd. Stochastic proximal iteration: a non-asymptotic improvement upon stochastic gradient descent. Author website, early draft, 2014.   \nA. Sadiev, G. Malinovsky, E. Gorbunov, I. Sokolov, A. Khaled, K. Burlachenko, and P. Richt\u00e1rik. Federated optimization algorithms with random reshuffling and gradient compression. arXiv preprint arXiv:2206.07021, 2022.   \nM. Safaryan, F. Hanzely, and P. Richt\u00e1rik. Smoothness matrices beat smoothness constants: Better communication compression techniques for distributed optimization. Advances in Neural Information Processing Systems, 34:25688\u201325702, 2021.   \nC. T Dinh, N. Tran, and J. Nguyen. Personalized federated learning with Moreau envelopes. Advances in Neural Information Processing Systems, 33:21394\u201321405, 2020.   \nJ. Von Neumann. On rings of operators. reduction theory. Annals of Mathematics, pages 401\u2013485, 1949.   \nD. Yin, A. Pananjady, M. Lam, D. Papailiopoulos, K. Ramchandran, and P. Bartlett. Gradient diversity: a key ingredient for scalable distributed learning. In International Conference on Artificial Intelligence and Statistics, pages 1998\u20132007. PMLR, 2018.   \nY. Yu, X. Zheng, M. Marchetti-Bowick, and E. Xing. Minimizing nonconvex non-separable functions. In Artificial Intelligence and Statistics, pages 1107\u20131115. PMLR, 2015.   \nX. Yuan and P. Li. On convergence of FedProx: Local dissimilarity invariant bounds, non-smoothness and beyond. Advances in Neural Information Processing Systems, 35:10752\u201310765, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Notations 15 ", "page_idx": 13}, {"type": "text", "text": "B Basic Facts 15 ", "page_idx": 13}, {"type": "text", "text": "C Properties of Moreau envelope 16 ", "page_idx": 13}, {"type": "text", "text": "D Technical lemmas 18 ", "page_idx": 13}, {"type": "text", "text": "E Theory of SGD 19 ", "page_idx": 13}, {"type": "text", "text": "F Additional analysis on FedExProx 19 ", "page_idx": 13}, {"type": "text", "text": "F.1 FedExProx in the strongly convex case 19   \nF.2 FedExProx in the non-smooth case 20   \nF.3 Discussion on the non-interpolation case 21   \nF.4 Discussion on the non-convex case . . 21   \nF.5 Additional notes on adaptive variants . . 23   \nF.6 Extension of adaptive variants into client partial participation (PP) setting 24 ", "page_idx": 13}, {"type": "text", "text": "G Missing proofs of theorems and corollaries 25 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "G.1 Proof of Theorem 1 25   \nG.2 Proof of Theorem 2 27   \nG.3 Proof of Theorem 3 29   \nG.4 Proof of Theorem 4 31   \nG.5 Proof of Theorem 5 32   \nG.6 Proof of Corollary 1 35   \nG.7 Proof of Corollary 2 . . 35 ", "page_idx": 13}, {"type": "text", "text": "H Missing proofs of lemmas 36 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "H.1 Proof of Lemma 1 36   \nH.2 Proof of Lemma 2 37   \nH.3 Proof of Lemma 3 37   \nH.4 Proof of Lemma 4 37   \nH.5 Proof of Lemma 5 . 37   \nH.6 Proof of Lemma 6 37   \nH.7 Proof of Lemma 7 38   \nH.8 Proof of Lemma 8 39   \nH.9 Proof of Lemma 9 39   \nH.10 Proof of Lemma 10 39   \nH.11 Proof of Lemma 11 41   \nH.12 Proof of Lemma 12 41   \nH.13 Proof of Lemma 13 41   \nH.14 Proof of Lemma 14 42   \nH.15 Proof of Lemma 15 43   \nH.16 Proof of Lemma 16 43   \nH.17 Proof of Lemma 17 43   \nH.18 Proof of Lemma 18 43   \nH.19 Proof of Lemma 19 43   \nH.20 Proof of Lemma 20 44 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "I Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "I.1 Experiment settings . . 44   \nI.2 Large dimension regime 45   \nI.2.1 Comparison of FedExProx and FedProx 45   \nI.2.2 Comparison of FedExProx with different local step size 47   \nI.2.3 Comparison of FedExProx and its adaptive variants 48 ", "page_idx": 14}, {"type": "text", "text": "A Notations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Throughout the paper, we use the notation $\\left\\Vert\\cdot\\right\\Vert$ to denote the standard Euclidean norm defined on $\\mathbb{R}^{d}$ and $\\langle\\cdot,\\cdot\\rangle$ to denote the standard Euclidean inner product. Given a differentiable function $f:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ , its gradient is denoted as $\\nabla f(x)$ . For a convex function $f:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ , we use $\\partial f(x)$ to denote its subdifferential at $x$ . We use the notation $D_{f}\\left(x,y\\right)$ to denote the Bregman divergence associated with a function $f:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ between $x$ and $y$ . The notation inf $f$ is used to denote the minimum of a function $f:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ . We use $\\mathrm{prox}_{\\gamma f}\\left(x\\right)$ to denote the proximity operator of function $f:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ with $\\gamma>0$ at $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , and $\\boldsymbol{M}_{f}^{\\gamma}\\left(\\boldsymbol{x}\\right)$ to denote the corresponding Moreau Envelope. The notation $\\boxed{\\begin{array}{r l}\\end{array}}$ is used for the infimal convolution of two proper functions. We denote the average of the Moreau envelope of each local objective $f_{i}$ by the notation $M^{\\gamma}:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ . Specifically, we define $M^{\\gamma}\\left(x\\right)\\,=\\,{\\textstyle{\\frac{1}{n}}}\\,\\dot{\\sum_{i=1}^{n}}\\,M_{f}^{\\gamma}\\left(x\\right)$ . Note that $M^{\\gamma}\\left(x\\right)$ has an implicit dependence on $\\gamma$ , its smoothness constant is denoted by $L_{\\gamma}$ . We say an extended real-valued function $f:\\mathbb{R}^{d}\\mapsto\\mathbb{R}\\cup\\{+\\infty\\}$ is proper if there exists $x\\,\\in\\,\\mathbb{R}^{d}$ such that $f(x)\\,<\\,+\\infty$ . We say an extended real-valued function $f:\\mathbb{R}^{d}\\mapsto\\mathbb{R}\\cup\\{+\\infty\\}$ is closed if its epigraph is a closed set. The following Table 3 summarizes the commonly used notations and quantities appeared in this paper. ", "page_idx": 14}, {"type": "text", "text": "B Basic Facts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Fact 1 (First prox theorem). [Beck, 2017, Theorem 6.3] Let $f:\\ensuremath{\\mathbb{R}^{d}}\\mapsto\\ensuremath{\\mathbb{R}}$ be a proper, closed and convex function. Then $\\operatorname{prox}_{f}\\left(x\\right)$ is a singleton for any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ . ", "page_idx": 14}, {"type": "text", "text": "Fact 2 (Second prox theorem). [Beck, 2017, Theorem 6.39] Let $f:\\mathbb{R}^{d}\\mapsto\\mathbb{R}\\cup\\{+\\infty\\}$ be a proper, closed and convex function. Then for any $\\boldsymbol{x},\\boldsymbol{u}\\in\\mathbb{R}^{d}$ , the following three claims are equivalent: ", "page_idx": 14}, {"type": "text", "text": "(i) $u=\\mathrm{prox}_{f}\\;(x).$   \n(ii) $x-u\\in\\partial f(u)$ .   \n(iii) $\\langle x-u,y-u\\rangle\\leq f(y)-f(u)$ for any $\\boldsymbol{y}\\in\\mathbb{R}^{d}$ . ", "page_idx": 14}, {"type": "text", "text": "Fact 3 (Bregman divergence). The Bregman divergence associated with a function $f$ between $x,y\\in\\mathbb{R}^{d}$ is defined as, ", "page_idx": 14}, {"type": "equation", "text": "$$\nD_{f}\\left(x,y\\right):=f(x)-f(y)-\\left\\langle\\nabla f(y),x-y\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "If $f$ is convex, then for any $x,y\\in\\mathbb{R}^{d}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\nD_{f}\\left(x,y\\right)\\geq0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "table", "img_path": "FuTfZK7PK3/tmp/7843f778a32c58bcdd385ad1249e9c58a5d23d584870e43e92fa55f8f7255436.jpg", "table_caption": ["Table 3: Summary of frequently used notations and quantities in this paper. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "If $f$ is convex, $L$ -smooth and differentiable, the following inequalities hold for any $x,y\\in\\mathbb{R}^{d}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{L}\\left\\|\\nabla f(x)-\\nabla f(y)\\right\\|^{2}\\leq D_{f}\\left(x,y\\right)+D_{f}\\left(y,x\\right)\\leq L\\left\\|x-y\\right\\|^{2},}\\\\ &{\\displaystyle\\frac{1}{L}\\left\\|\\nabla f(x)-\\nabla f(y)\\right\\|^{2}\\leq2D_{f}\\left(x,y\\right)\\leq L\\left\\|x-y\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Fact 4 (Increasing function). Let $\\begin{array}{r}{f(x)=\\frac{x}{1+\\gamma x}}\\end{array}$ , where $\\gamma>0$ . Then $f(x)$ is monotone increasing when $x>0$ . ", "page_idx": 15}, {"type": "text", "text": "C Properties of Moreau envelope ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we explore the properties of the Moreau envelope of individual functions $f_{i}$ , and the global objective $\\begin{array}{r}{M^{\\gamma}\\stackrel{}{=}\\frac{1}{n}\\sum_{i=1}^{n}\\dot{M}_{f_{i}}^{\\gamma}}\\end{array}$ . Before that, we present the definition of infimal convolution ", "page_idx": 15}, {"type": "text", "text": "Definition 3 (Infimal convolution). The infimal convolution of two proper functions $f,g:\\mathbb{R}^{d}\\mapsto$ $\\mathbb{R}\\cup\\{+\\infty\\}$ is defined via the following formula ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left(f\\square g\\right)(x)=\\operatorname*{min}_{z\\in\\mathbb{R}^{d}}\\left\\{f(z)+g(x-z)\\right\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "One key observation is that $M_{f}^{\\gamma}$ can be viewed as the infimal convolution of the proper, closed and convex function $f$ and the real-valued convex function $\\textstyle{\\frac{1}{2\\gamma}}\\left\\|\\cdot\\right\\|^{2}$ . This observation enables us to infer the convexity and smoothness of the Moreau envelope from the properties of the original function. First, we present two lemmas about basic properties of Moreau envelope. ", "page_idx": 15}, {"type": "text", "text": "Lemma 1 (Real-valuedness). Let $f:\\mathbb{R}^{d}\\mapsto\\mathbb{R}\\cup\\{+\\infty\\}$ be a proper, closed and convex function. Then its Moreau envelope $M_{f}^{\\gamma}$ for any $\\gamma>0$ is a real-valued function. In particular, the following identity holds for $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ according to the definition of Moreau envelope, ", "page_idx": 16}, {"type": "equation", "text": "$$\nM_{f}^{\\gamma}\\left(x\\right)=f\\left(\\operatorname{prox}_{\\gamma f}\\left(x\\right)\\right)+\\frac{1}{2\\gamma}\\left\\Vert x-\\operatorname{prox}_{\\gamma f}\\left(x\\right)\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma 2 (Differentiability of Moreau envelope). [Beck, 2017, Theorem 6.60] Let $f:\\,\\mathbb{R}^{d}\\,\\mapsto$ $\\mathbb{R}\\cup\\{+\\infty\\}$ be a proper, closed and convex function. Then its Moreau envelope $M_{f}^{\\gamma}$ for any $\\gamma>0$ is $\\frac{1}{\\gamma}$ -smooth, and for any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla M_{f}^{\\gamma}\\left(x\\right)=\\frac{1}{\\gamma}\\left(x-\\mathrm{prox}_{\\gamma f}\\left(x\\right)\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We then focus on the relation between individual $f_{i}$ and $M_{f_{i}}^{\\gamma}$ . The following lemma suggests that the convexity of individual $f_{i}$ guarantees the convexity of $M_{f_{i}}^{\\gamma}$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma 3 (Convexity of Moreau envelope). [Beck, 2017, Theorem 6.55] Let $f:\\mathbb{R}^{d}\\mapsto\\mathbb{R}\\cup\\{+\\infty\\}$ be a proper and convex function. Then $M_{f}^{\\gamma}$ is a convex function. ", "page_idx": 16}, {"type": "text", "text": "It is also true that the smoothness of individual $f_{i}$ indicates the smoothness of $M_{f_{i}}^{\\gamma}$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma 4 (Smoothness of Moreau envelope). Let $f:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ be a convex and $L$ -smooth function.   \nThen $M_{f}^{\\gamma}$ is $\\scriptstyle{\\frac{L}{1+\\gamma L}}$ -smooth. ", "page_idx": 16}, {"type": "text", "text": "One notable fact is that $f_{i}$ and $M_{f_{i}}^{\\gamma}$ have the same set of minimizers. ", "page_idx": 16}, {"type": "text", "text": "Lemma 5 (Minimizer equivalence). Let $f:\\mathbb{R}^{d}\\mapsto\\mathbb{R}\\cup\\{+\\infty\\}$ be a proper, closed and convex function. Then for any $\\gamma>0$ , $f$ and $M_{f}^{\\gamma}$ has the same set of minimizers. ", "page_idx": 16}, {"type": "text", "text": "In addition, $M_{f}^{\\gamma}$ is a global lower bound of $f$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma 6 (Individual lower bound). Let $f:\\mathbb{R}^{d}\\mapsto\\mathbb{R}\\cup\\{+\\infty\\}$ be a proper, closed and convex function. Then the Moreau envelope $M_{f}^{\\gamma}$ satisfies $M_{f}^{\\gamma}\\left(x\\right)\\leq f(x)$ for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ . ", "page_idx": 16}, {"type": "text", "text": "Next, we focus on the global objective $M^{\\gamma}\\left(x\\right)$ . The following lemma bounds its smoothness constant from both above and below. ", "page_idx": 16}, {"type": "text", "text": "Lemma 7 (Global convexity and smoothness). Let each $f_{i}$ be proper, closed convex and $L_{i}$ -smooth. Then $M$ is convex and $L_{\\gamma}$ -smooth with ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\frac{1}{n^{2}}}\\sum_{i=1}^{n}{\\frac{L_{i}}{1+\\gamma L_{i}}}\\leq L_{\\gamma}\\leq{\\frac{1}{n}}\\sum_{i=1}^{n}{\\frac{L_{i}}{1+\\gamma L_{i}}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As a result of the above inequalities, we have the following inequality on the condition number defined in Theorem $^{\\,l}$ which holds for any $\\tau\\in[n]$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\nL_{\\gamma}\\left(1+\\gamma L_{\\operatorname*{max}}\\right)=C\\left(\\gamma,n,\\alpha_{\\gamma,n}\\right)\\leq C\\left(\\gamma,\\tau,\\alpha_{\\gamma,\\tau}\\right)\\leq C\\left(\\gamma,1,\\alpha_{\\gamma,1}\\right)=L_{\\operatorname*{max}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "When there exists at least one $L_{i}\\,<\\,L_{\\mathrm{max}}$ , we have $C\\left(\\gamma,n,\\alpha_{\\gamma,n}\\right)\\,<\\,C\\left(\\gamma,\\tau,\\alpha_{\\gamma,\\tau}\\right)\\,<\\,L_{\\mathrm{max}}\\,=$ $C\\left(\\gamma,1,\\alpha_{\\gamma,1}\\right)$ . Even $L_{i}=L_{\\mathrm{max}}$ holds for all $i\\in[n]$ , there are cases (See Example $^{\\,l}$ in the proof.) that $C\\left(\\gamma,n,\\alpha_{\\gamma,n}\\right)=\\textstyle{\\frac{1}{n}}C\\left(\\gamma,1,\\alpha_{\\gamma,1}\\right)=\\frac{1}{n}{L}_{\\operatorname*{max}}$ . ", "page_idx": 16}, {"type": "text", "text": "A key observation in this case is the generalization of Lemma 5 into the finite-sum setting under the interpolation regime. ", "page_idx": 16}, {"type": "text", "text": "Lemma 8 (Minimizer equivalence). If we let every $f_{i}:\\mathbb{R}^{d}\\mapsto\\mathbb{R}\\cup\\{+\\infty\\}$ be proper, closed and convex, then $\\begin{array}{r}{f(x)=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(x)}\\end{array}$ has the same set of minimizers and minimum as ", "page_idx": 16}, {"type": "equation", "text": "$$\nM^{\\gamma}\\left(x\\right)=\\frac{1}{n}\\sum_{i=1}^{n}M_{f_{i}}^{\\gamma}\\left(x\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "if we are in the interpolation regime and $0<\\gamma<\\infty$ . ", "page_idx": 16}, {"type": "text", "text": "The following lemma generalizes Lemma 6 into the finite-sum setting. ", "page_idx": 17}, {"type": "text", "text": "Lemma 9 (Global lower bound). Let each $f_{i}:\\mathbb{R}^{d}\\mapsto\\mathbb{R}\\cup\\{+\\infty\\}$ be proper, closed and convex. Then the following inequality holds for any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ and $\\gamma>0$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\nM^{\\gamma}\\left(x\\right)\\leq M_{f}^{\\gamma}\\left(x\\right)\\leq f(x).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In addition, if we assume we are in the interpolation regime, then $M^{\\gamma}$ , $M_{f}^{\\gamma}$ and $f$ have the same set of minimizers, for any $x_{\\star}$ in this set of minimizers, the following identity holds, ", "page_idx": 17}, {"type": "equation", "text": "$$\nM^{\\gamma}\\left(x_{\\star}\\right)=M_{f}^{\\gamma}\\left(x_{\\star}\\right)=f(x_{\\star}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Besides the global lower bound provided above, there is also a relation between the function value suboptimality of $M^{\\gamma}$ and $f$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma 10 (Suboptimality bound). Suppose Assumption $^{\\,l}$ (Differentiability), 2 (Interpolation Regime), 3 (Convexity) and 4 (Smoothness) hold, for any minimizer $x_{\\star}$ of $M^{\\gamma}\\left(x\\right)$ , all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , the following inequality holds for each client objective, ", "page_idx": 17}, {"type": "equation", "text": "$$\nM_{f_{i}}^{\\gamma}\\left(x\\right)-M_{f_{i}}^{\\gamma}\\left(x_{\\star}\\right)\\geq\\frac{1}{1+\\gamma L_{i}}\\left(f_{i}(x)-f_{i}(x_{\\star})\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Furthermore, this suggests ", "page_idx": 17}, {"type": "equation", "text": "$$\nM^{\\gamma}\\left(x\\right)-M^{\\gamma}\\left(x_{\\star}\\right)\\geq\\frac{1}{1+\\gamma L_{\\operatorname*{max}}}\\left(f_{i}(x)-f_{i}(x_{\\star})\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A direct consequence of the above function suboptimality bound is the star strong convexity of $M^{\\gamma}$ from the strong convexity of $f$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma 11. (Star strong convexity) Assume Assumption $I$ (Differentiability), Assumption 2 (Interpolation Regime), Assumption $3$ (Convexity), Assumption 4 (Smoothness) and Assumption $^{5}$ (Strong convexity) hold, then the convex function $M^{\\gamma}\\left(x\\right)$ satisfies the following inequality, ", "page_idx": 17}, {"type": "equation", "text": "$$\nM^{\\gamma}\\left(x\\right)-M^{\\gamma}\\left(x_{\\star}\\right)\\geq\\frac{\\mu}{1+\\gamma L_{\\operatorname*{max}}}\\cdot\\frac{1}{2}\\left\\Vert x-x_{\\star}\\right\\Vert^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ and a minimizer $x_{\\star}$ of $M^{\\gamma}\\left(x\\right)$ . ", "page_idx": 17}, {"type": "text", "text": "The star strong convexity property of $M^{\\gamma}$ allows us to improve the sublinear convergence in the convex regime into linear convergence. ", "page_idx": 17}, {"type": "text", "text": "D Technical lemmas ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma 12. Let $f:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ be a proper, closed and convex function. Then $x$ is a minimizer of $f$ if and only if $\\,^{\\prime}x=\\operatorname{prox}_{\\gamma f}\\left(x\\right)$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma 13. Assume we are working with the finite-sum problem $\\textstyle f={\\frac{1}{n}}\\sum_{i=1}^{n}f_{i}$ , where each $f_{i}$ is convex and $L_{i}$ -smooth, $f$ is convex and -smooth. Then the smoothness of $L$ satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\frac{1}{n^{2}}}\\sum_{i=1}^{n}L_{i}\\leq L\\leq{\\frac{1}{n}}\\sum_{i=1}^{n}L_{i},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where both bounds are attainable. ", "page_idx": 17}, {"type": "text", "text": "Lemma 14. Assume that all the conditions mentioned in Theorem 1 hold, then the condition number $C(\\gamma,\\tau,1)$ of FedProx and the condition number $C\\left(\\gamma,\\tau,\\alpha_{\\gamma,\\tau}\\right)$ of the optimal constant extrapolation parameter $\\begin{array}{r}{\\alpha_{\\star}=\\frac{1}{\\gamma L_{\\gamma,\\tau}}}\\end{array}$ satisfy the following inequality, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{C(\\gamma,\\tau,1)}{C\\left(\\gamma,\\tau,\\alpha_{\\gamma,\\tau}\\right)}\\geq2+\\frac{1}{\\gamma L_{\\operatorname*{max}}}+\\gamma L_{\\operatorname*{max}}\\geq4\\quad\\forall\\tau\\in[n].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 15. Assume that all the conditions mentioned in Theorem $^{\\,l}$ hold, then the following inequalities hold, ", "page_idx": 17}, {"type": "equation", "text": "$$\nC\\left(\\gamma,n,\\alpha_{\\gamma,n}\\right)\\leq C\\left(\\gamma,\\tau,\\alpha_{\\gamma,\\tau}\\right)\\leq C\\left(\\gamma,1,\\alpha_{\\gamma,1}\\right),\\quad\\forall\\tau\\in[n],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\alpha_{\\gamma,1}\\geq\\alpha_{\\gamma,\\tau}\\geq\\alpha_{\\gamma,n},\\quad\\forall\\tau\\in[n].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "E Theory of SGD ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In order to prove our main theorem, we partly rely on the theory of SGD. The following theorem on the convergence of SGD with $\\tau$ -nice sampling is adapted from Gower et al. [2019]. We introduce modifications to the proof technique and tailor the theorem specifically to the interpolation regime. In this context, the upper bound on the step size is increased by a factor of 2. We first formulate the algorithm as follows for completeness. ", "page_idx": 18}, {"type": "text", "text": "Algorithm 2 SGD with $\\tau$ -nice sampling ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1: Parameters: learning rate $\\eta>0$ , starting point $x_{0}\\in\\mathbb{R}^{d}$ , minibatch size $\\tau\\in\\{1,2,\\ldots,n\\}$   \n2: for $k=0,1,2,\\ldots$ do   \n3: The server samples $S_{k}\\subseteq\\{1,2,\\ldots,n\\}$ uniformly from all subsets of cardinality $\\tau$   \n4: The server performs one gradient step ", "page_idx": 18}, {"type": "equation", "text": "$$\nx_{k+1}=x_{k}-\\eta\\cdot\\frac{1}{\\tau}\\sum_{\\xi_{i}\\in S_{k}}\\nabla f_{\\xi_{i}}(x_{k}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "5: end for ", "page_idx": 18}, {"type": "text", "text": "Theorem 3. Assume Assumption $^{\\,l}$ (Differentiability), 2 (Interpolation regime), 3 (Convexity), 4 (Smoothness) hold. Additionally, assume $f$ is $L$ -smooth where $\\begin{array}{r}{\\bar{L}\\leq\\frac{1}{n}\\sum_{i=1}^{n}L_{i}}\\end{array}$ .3 If we are running SGD with $\\tau$ -nice sampling using step size $\\eta$ that satisfies $\\begin{array}{r}{0<\\eta<\\frac{2}{A_{\\tau}}}\\end{array}$ , where ", "page_idx": 18}, {"type": "equation", "text": "$$\nA_{\\tau}:=\\frac{n-\\tau}{\\tau(n-1)}L_{\\operatorname*{max}}+\\frac{n(\\tau-1)}{\\tau(n-1)}L,\\qquad a n d\\qquad L_{\\operatorname*{max}}:=\\operatorname*{max}_{i}L_{i},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "then the iterates of Algorithm 2 satisfy ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\bar{x}_{K})\\right]-\\operatorname*{inf}{f\\le\\frac{1}{\\eta(2-\\eta A_{\\tau})}}\\cdot\\frac{\\left\\Vert x_{0}-x_{\\star}\\right\\Vert^{2}}{K},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $K$ is the total number of iterations, $\\bar{x}_{K}$ is chosen uniformly at random from the first $K$ iterates $\\{x_{0},x_{1},\\dots,x_{K-1}\\}$ . If, additionally, we assume the following property (which we will refer to as \u201cstar strong convexity\u201d) holds, then the iterates of Algorithm 2 satisfy ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert x_{K}-x_{\\star}\\right\\Vert^{2}\\right]\\leq\\left(1-\\eta(2-\\eta A_{\\tau})\\cdot\\frac{\\mu}{2}\\right)^{K}\\left\\Vert x_{0}-x_{\\star}\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "F Additional analysis on FedExProx ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we provide some additional details on the analysis of FedExProx and its adaptive variants. ", "page_idx": 18}, {"type": "text", "text": "F.1 FedExProx in the strongly convex case ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The following corollary summarizes the convergence guarantee in the strongly convex case. ", "page_idx": 18}, {"type": "text", "text": "Corollary 1. Suppose the assumptions in Theorem 1 hold, and assume in addition that Assumption $^{5}$ (Strong Convexity) holds, then we achieve linear convergence for the final iterate of Algorithm $^{\\,l}$ , which satisfies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert x_{K}-x_{\\star}\\Vert^{2}\\right]\\leq\\left(1-\\alpha\\gamma(2-\\alpha\\gamma L_{\\gamma,\\tau})\\cdot\\frac{\\mu}{2\\left(1+\\gamma L_{\\operatorname*{max}}\\right)}\\right)^{K}\\left\\Vert x_{0}-x_{\\star}\\right\\Vert^{2},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the definition of $L_{\\gamma,\\tau}$ is given in Theorem 1. Fixing the choice of $\\gamma$ and $\\tau$ , the optimal extrapolation parameter that minimizes the convergence rate is given by $\\begin{array}{r}{\\alpha_{\\gamma,\\tau}=\\frac{1}{\\gamma L_{\\gamma,\\tau}}>1}\\end{array}$ , which results in the following convergence in the strongly convex case: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert x_{K}-x_{\\star}\\right\\Vert^{2}\\right]\\leq\\left(1-\\frac{\\mu}{2L_{\\gamma,\\tau}\\left(1+\\gamma L_{\\operatorname*{max}}\\right)}\\right)^{K}\\left\\Vert x_{0}-x_{\\star}\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "3This is justified by Lemma 13. ", "page_idx": 18}, {"type": "text", "text": "As one can observe, by additionally assuming $\\mu$ strong convexity of the original function $f$ , we improve the sublinear convergence in the convex case into linear convergence. ", "page_idx": 19}, {"type": "text", "text": "F.2 FedExProx in the non-smooth case ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Our analysis also adapts to the non-smooth cases. This is based on the observation that even if we only assume Assumption 1 (differentiability), Assumption 2 (Interpolation Regime) and Assumption 3 (Convexity) hold and do not have additional assumptions on smoothness, still each $M_{f_{i}}^{\\gamma}$ is $\\frac{1}{\\gamma}$ -smooth because of Lemma 2. Thus, the theory of SGD in the convex smooth case still applies. However, there are some differences from the smooth case. For the sake of simplicity, we will mainly focus on the convex non-smooth case with a constant extrapolation parameter, the results in the strongly convex regime and with adaptive extrapolation can be obtained similarly as in the proof of Theorem 1 and Theorem 2. ", "page_idx": 19}, {"type": "text", "text": "Theorem 4. Assume Assumption $^{\\,l}$ (Differentiability), 2 (Interpolation Regime) and 3 (Convexity) hold. If we choose a constant extrapolation parameter $\\alpha_{k}=\\alpha$ satisfying ", "page_idx": 19}, {"type": "equation", "text": "$$\n0<\\alpha<\\frac{2}{\\gamma L_{\\gamma,\\tau}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $L_{\\gamma}$ is the smoothness constant of $\\begin{array}{r}{M^{\\gamma}\\left(x\\right)=\\frac{1}{n}\\sum_{i=1}^{n}M_{f_{i}}^{\\gamma}\\left(x\\right),\\,L_{\\gamma,\\tau}}\\end{array}$ is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\nL_{\\gamma,\\tau}=\\frac{n-\\tau}{\\tau(n-1)}\\cdot\\frac{1}{\\gamma}+\\frac{n(\\tau-1)}{\\tau(n-1)}\\cdot L_{\\gamma}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then the iterates of Algorithm $^{\\,l}$ satisfy ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\gamma M^{\\gamma}\\left(\\bar{x}_{K}\\right)-\\operatorname*{inf}\\gamma M^{\\gamma}\\leq\\frac{1}{\\alpha\\left(2-\\alpha\\gamma L_{\\gamma,\\tau}\\right)}\\cdot\\frac{\\|x_{0}-x_{\\star}\\|^{2}}{K},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\bar{x}_{K}$ is chosen uniformly from the first $K$ iterates $\\{x_{0},x_{1},\\dots,x_{K-1}\\}$ . $I t$ is easy to see that the best $\\alpha$ is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\alpha_{\\star}=\\frac{1}{\\gamma L_{\\gamma,\\tau}}\\ge1,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the corresponding convergence is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\gamma M^{\\gamma}\\left(\\bar{x}_{K}\\right)-\\operatorname*{inf}\\gamma M^{\\gamma}\\leq\\left(\\frac{n-\\tau}{\\tau(n-1)}+\\frac{n(\\tau-1)}{\\tau(n-1)}\\gamma L_{\\gamma}\\right)\\cdot\\frac{\\|x_{0}-x_{\\star}\\|^{2}}{K}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Remark 12. Notice that in this case we recover the convergence result of RPM presented in Necoara et al. [2019] in the convex case. Indeed, if each $f_{i}(x)=\\mathbb{I}_{\\lambda_{i}}\\left(x\\right)$ , then we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right)=\\Pi_{\\mathcal{X}_{i}}\\left(x\\right),\\forall x\\in\\mathbb{R}^{d},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\gamma M_{f_{i}}^{\\gamma}\\left(x\\right)=\\frac{1}{2}\\left\\Vert x-\\Pi_{X_{i}}\\left(x\\right)\\right\\Vert^{2},\\quad a n d\\quad\\gamma M^{\\gamma}\\left(x\\right)=\\frac{1}{2}\\cdot\\frac{1}{n}\\sum_{i=1}^{n}\\left\\Vert x-\\Pi_{X_{i}}\\left(x\\right)\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since we are in the interpolation regime, inf $\\gamma M^{\\gamma}=0,$ , and the convergence result becomes ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac12\\cdot\\frac{1}{n}\\sum_{i=1}^{n}\\|x_{K}-\\Pi_{\\mathcal{X}_{i}}\\left(x_{K}\\right)\\|^{2}\\le\\left(\\frac{n-\\tau}{\\tau(n-1)}+\\frac{n(\\tau-1)}{\\tau(n-1)}\\gamma L_{\\gamma}\\right)\\cdot\\frac{\\|x_{0}-x_{\\star}\\|^{2}}{K}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Notice that here $\\gamma L_{\\gamma}~\\leq~1$ is the smoothness constant associated with each distance function $\\frac{1}{2}\\left\\|x-\\Pi_{\\mathcal{X}_{i}}\\left(x\\right)\\right\\|^{2}$ . The difference in the coefficients on the left-hand side from the original results presented in Necoara et al. [2019] results from different sampling strategies employed. ", "page_idx": 19}, {"type": "text", "text": "A key difference in the non-smooth setting is that extrapolation in some cases may not be beneficiary, as illustrated by the following remark. ", "page_idx": 19}, {"type": "text", "text": "Remark 13. In the non-smooth case, it is possible that $\\gamma L_{\\gamma}=1$ , where the optimal $\\alpha_{\\star}=1$ , in this case, extrapolation will not generate any beneftis. However, as it is mentioned by Necoara et al. [2019], there are many examples where $\\gamma L_{\\gamma}<1$ and extrapolation indeed accelerates the algorithm. This is different from the smooth case, where extrapolation always helps. ", "page_idx": 19}, {"type": "text", "text": "Remark 14. Since we do not assume smoothness, Lemma 10 no longer applies. Therefore, the convergence result is stated in terms of the function value suboptimality of Moreau envelope instead of the original objective $f$ which is used in the smooth case. ", "page_idx": 20}, {"type": "text", "text": "Using a similar approach, it is also possible to obtain a convergence guarantee for FedExProx in the strongly convex non-smooth regime, assuming in addition that $M^{\\gamma}\\left(x\\right)$ is $\\mu_{\\gamma}$ -strongly convex, where we recover the convergence result of RPM in Necoara et al. [2019] in cases where the smooth and linear regularity conditions are both satisfied. The following Table 4 confirms that our analysis of FedExProx recovers the theory of RPM as a special case. ", "page_idx": 20}, {"type": "text", "text": "Table 4: Comparison of iteration complexity of RPM from Necoara et al. [2019] obtained using our theory and the original theory. In both cases, the optimal extrapolation parameter is used. The notation $O(\\cdot)$ is hidden. $\\varepsilon$ is the error level reached by function value suboptimality for convex case, squared distance to the solution for strongly convex case. ", "page_idx": 20}, {"type": "table", "img_path": "FuTfZK7PK3/tmp/e9c1d6a43586efcad2465846fd1578e79d04416862446ba4be0d4f7280dd7a9a.jpg", "table_caption": [], "table_footnote": ["(1) The smoothness here does not refer to each $f_{i}$ being $L_{i}$ -smooth, but $\\gamma M^{\\gamma}$ being $\\gamma L_{\\gamma}$ -smooth. This corresponds to the smooth regularity condition presented in Necoara et al. [2019]. (2) Here the strongly convex setting meaning that the linear regularity condition in Necoara et al. [2019] is satisfied. In our theory, it refers to $M^{\\gamma}\\left(x\\right)$ being $\\mu_{\\gamma}$ -strongly convex with $\\mu_{\\gamma}<L_{\\gamma}$ . "], "page_idx": 20}, {"type": "text", "text": "F.3 Discussion on the non-interpolation case ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For the non-interpolation regime cases, we assume that Assumption 1 (Differentiability), Assumption 3 (Convexity) and Assumption 4 (Smoothness) hold. The differences are listed as follows ", "page_idx": 20}, {"type": "text", "text": "(i) Although $f_{i}$ and $M_{f_{i}}^{\\gamma}$ have the same set of minimizers, $f$ and $M^{\\gamma}$ does not necessarily have the same set of minimizers. This will lead to the convergence of FedExProx to the minimizer $x_{\\star,\\gamma}^{\\prime}$ of $M^{\\gamma}\\left(x\\right)$ instead of $x_{\\star}$ of $f$ . As a result, we will only converge to a neighborhood of the $x_{\\star}$ depending on the specific setting.   \n(ii) Since we are not in the interpolation regime, the upper bound on the step size of SGD with sampling is reduced by a factor of 2. Thus, the optimal extrapolation parameter $\\alpha_{\\star}^{\\prime}$ in the non-interpolated cases is also halved, $\\begin{array}{r}{\\alpha_{\\star}^{\\prime}=\\frac{1}{2}\\alpha_{\\star}}\\end{array}$ . As a result, it is possible that $\\alpha_{\\star}^{\\prime}\\leq1$ . The same phenomenon is also observed in FedExP of Jhunjhunwala et al. [2023], where their heuristic in determining the extrapolation parameter adaptively is also reduced by a factor of 2 in non overparameterized cases. ", "page_idx": 20}, {"type": "text", "text": "Observe that all of the above results in both smooth/non-smooth, interpolated/non-interpolated cases suggests that the practice of server simply averaging the iterates it obtained from local training is suboptimal. ", "page_idx": 20}, {"type": "text", "text": "F.4 Discussion on the non-convex case ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In the non-convex case, we assume Assumption 1 (Differentiability) holds, and we need the following additional assumptions on $f:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ and $f_{i}:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ : ", "page_idx": 20}, {"type": "text", "text": "Assumption 6 (Lower boundedness). Function $f_{i}$ is lower bounded by inf $f_{i}$ . ", "page_idx": 20}, {"type": "text", "text": "Assumption 7 (Weak convexity). Function $f_{i}$ is $\\rho>0$ weakly convex, this means that $\\textstyle f_{i}+{\\frac{\\rho}{2}}\\left\\|\\cdot\\right\\|^{2}\\,i$ s convex. ", "page_idx": 20}, {"type": "text", "text": "We have the following lemma under the above assumptions: ", "page_idx": 20}, {"type": "text", "text": "Lemma 16. [B\u00f6hm and Wright, 2021, Lemma 3.1] Let $f$ be a proper, closed, $\\rho$ -weakly convex function and let $\\gamma<\\frac{1}{\\rho}$ . Then the Moreau envelope $M_{f}^{\\gamma}$ is continuously differentiable on $\\mathbb{R}^{\\dot{d}}$ with ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla M_{f}^{\\gamma}\\left(x\\right)=\\frac{1}{\\gamma}\\left(x-\\mathrm{prox}_{\\gamma f}\\left(x\\right)\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In addition, the Moreau envelope is $\\operatorname*{max}{\\left\\{{\\frac{1}{\\gamma}},{\\frac{\\rho}{1-\\gamma\\rho}}\\right\\}}$ -smooth. We will thereby denote the smoothness constant as L\u03b3,\u03c1. ", "page_idx": 21}, {"type": "text", "text": "Indeed, if the stepsize $\\gamma$ in this case is chosen properly such that $\\scriptstyle{\\frac{1}{\\gamma}}\\,>\\,\\rho$ , then it is straight forward to see the function within the proximity operator $\\mathrm{prox}_{\\gamma f_{i}}$ given by $\\begin{array}{r}{f_{i}+\\frac{1}{2}\\cdot\\frac{1}{\\gamma}\\left\\Vert\\cdot\\right\\Vert^{2}}\\end{array}$ is strongly convex. Thus the proximity operator still results in a singleton. Lemma 16 allows us to again reformulate the original algorithm using the gradient of Moreau envelope. The only difference from the convex regime is that the Moreau envelope $M_{f_{i}}^{\\gamma}$ is not necessarily convex. The following lemmas illustrate the connection between $M_{f_{i}}^{\\gamma}$ and $f_{i}$ : ", "page_idx": 21}, {"type": "text", "text": "Lemma 17. [Yu et al., 2015, Proposition 7] Let $\\gamma~>~0$ , $f$ be a closed, proper function that is lower bounded. Then $M_{f}^{\\gamma}\\ \\leq\\ \\bar{f}$ , in $\\mathrm{f}\\;M_{f}^{\\gamma}\\;=\\;\\operatorname{inf}\\,f,$ , $\\mathrm{arg\\,min}_{x}\\,M_{f}^{\\gamma}\\left(x\\right)\\;=\\;\\mathrm{arg\\,min}_{x}\\,f(x)\\;\\subseteq$ $\\left\\{x:x\\in\\mathrm{prox}_{\\gamma f}\\left(x\\right)\\right\\}$ . ", "page_idx": 21}, {"type": "text", "text": "Lemma 18. Let $f:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ be $\\rho$ -weakly convex with $\\rho>0$ and differentiable. If we take $\\begin{array}{r}{0<\\gamma<\\frac{1}{\\rho}}\\end{array}$ , then $M_{f_{i}}^{\\gamma}$ has the same set of stationary points as $f_{i}$ . ", "page_idx": 21}, {"type": "text", "text": "For the sake of simplicity, we will consider only the full participation case with a constant extrapolation parameter $\\alpha_{k}=\\alpha$ . The following lemma describes the convergence of GD in the non-convex case, which is adapted from the theory of Khaled and Richt\u00e1rik [2023]. ", "page_idx": 21}, {"type": "text", "text": "Lemma 19. Assume function $f$ is $L$ -smooth and lower bounded. If we are running $G D$ with a constant stepsize $\\eta$ satisfying $\\begin{array}{r}{0<\\eta<\\frac{1}{L}}\\end{array}$ . Then for any $K\\geq1$ , the iterates $x_{k}$ of $\\it G D$ satisfy ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq k\\leq K-1}\\mathbb{E}\\left[\\|\\nabla f(x_{k})\\|^{2}\\right]\\leq\\frac{2\\left(f(x_{0})-\\operatorname*{inf}f\\right)}{\\eta K}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now we directly apply Lemma 19 in our case, ", "page_idx": 21}, {"type": "text", "text": "1. Since each $M_{f_{i}}^{\\gamma}$ is $L_{\\gamma,\\rho}$ -smooth, $M^{\\gamma}$ is $L_{\\gamma}$ -smooth with $L_{\\gamma}\\leq L_{\\gamma,\\rho}$ , which result in the following bound on the extrapolation parameter ", "page_idx": 21}, {"type": "equation", "text": "$$\n0<\\alpha<\\frac{1}{\\gamma L_{\\gamma}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Notice that in this case we have the following estimation of $\\gamma L_{\\gamma}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{\\gamma L_{\\gamma}}\\geq\\frac{1}{\\gamma L_{\\gamma,\\rho}}=\\operatorname*{min}\\left\\{1,\\frac{1-\\gamma\\rho}{\\gamma\\rho}\\right\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This suggests that extrapolation may not be much beneficiary in the non-convex case. ", "page_idx": 21}, {"type": "text", "text": "2. The following convergence guarantee can be obtained. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq k\\leq K-1}\\mathbb{E}\\left[\\left\\|\\nabla M^{\\gamma}(x_{k})\\right\\|^{2}\\right]\\leq\\frac{2\\left(M^{\\gamma}\\left(x_{0}\\right)-\\operatorname*{inf}M^{\\gamma}\\right)}{\\alpha\\gamma K}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Notice that by Lemma 17, we know that $M_{f_{i}}^{\\gamma}\\left(x_{0}\\right)\\,\\leq\\,f_{i}\\left(x_{0}\\right)$ . We also have inf $M^{\\gamma}~\\geq$ $\\textstyle{\\frac{1}{n}}\\sum_{i=1}^{n}\\operatorname*{inf}\\,M_{f_{i}}^{\\gamma}~=~{\\frac{1}{n}}\\sum_{i=1}^{n}\\operatorname*{inf}\\,f_{i}$ since inf $M_{f_{i}}^{\\gamma}~=~\\operatorname{inf}f_{i}$ is true for each client by Lemma 17. Thus, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nM^{\\gamma}\\left(x_{0}\\right)-\\operatorname*{inf}M^{\\gamma}\\leq f(x_{0})-\\operatorname*{inf}f+\\operatorname*{inf}f-{\\frac{1}{n}}\\sum_{i=1}^{n}\\operatorname*{inf}f_{i}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We can relax the above convergence guarantee and obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq k\\leq K-1}\\mathbb{E}\\left[\\left\\|\\nabla M^{\\gamma}(x_{k})\\right\\|^{2}\\right]\\leq\\frac{2\\left(f(x_{0})-\\operatorname*{inf}f\\right)}{\\alpha\\gamma K}+\\frac{2\\left(\\operatorname*{inf}f-\\frac{1}{n}\\sum_{i=1}^{n}\\operatorname*{inf}f_{i}\\right)}{\\alpha\\gamma K}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The above convergence guarantee indicates that the algorithm converges to some stationary points of $M^{\\gamma}\\left(x\\right)$ in the non-convex case. ", "page_idx": 21}, {"type": "text", "text": "3. In the non-convex case, we did not assume anything similar to the interpolation regime in the convex case. As a result, we did not know the relation between the set of stationary points of $M^{\\gamma}\\left(x\\right)$ and $f(x)$ , denoted as $\\mathcal{V}^{\\prime}$ and $\\boldsymbol{\\wp}$ , respectively. However, if we assume, in addition, that each stationary point $y^{\\prime}\\in\\mathcal{V}^{\\prime}$ of $M^{\\gamma}$ is also a stationary point of each $M_{f_{i}}^{\\gamma}$ , then $y^{\\prime}$ is also a stationary point of $f_{i}$ according to Lemma 18. Thus, $\\begin{array}{r}{\\nabla f\\left(y^{\\prime}\\right)=\\frac{1}{n}\\sum_{i=1}^{n}\\nabla f_{i}\\left(y^{\\prime}\\right)=0}\\end{array}$ , which indicates $y^{\\prime}\\in\\mathcal{V}$ . As a result, we have $y^{\\prime}\\subseteq\\mathcal{V}$ . This means that under this additional assumption, the algorithm converges to a stationary point of $f$ . ", "page_idx": 22}, {"type": "text", "text": "F.5 Additional notes on adaptive variants ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Notes on gradient diversity variant. In general, the gradient diversity step size $\\eta_{k}$ used in SGD to solve the finite sum minimization problem ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\left\\{f(x)=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(x)\\right\\},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "can be written as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\eta_{k}:=\\frac{1}{L_{\\operatorname*{max}}}\\cdot\\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\left\\Vert\\nabla f_{i}(x_{k})\\right\\Vert^{2}}{\\left\\Vert\\frac{1}{n}\\sum_{i=1}^{n}\\nabla f_{i}(x_{k})\\right\\Vert^{2}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $L_{\\mathrm{max}}$ is the maximum of local smoothness constants. In our case, since each local Moreau envelope is1+L\u03b3iLi -smooth and $\\scriptstyle{\\frac{1}{\\gamma}}-{\\mathrm{smooth}}^{4}$ , we can use both $\\frac{L_{\\operatorname*{max}}}{1\\!+\\!\\gamma L_{\\operatorname*{max}}}$ (here in Corollary 2, if we know $L_{\\mathrm{max}})$ and $\\frac{1}{\\gamma}$ (in original Theorem 2, if we do not know $L_{\\mathrm{max}}$ ) as the maximum of local smoothness. We present the convergence result of Algorithm 1 with the following rule given in (13), ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\alpha_{k,G}^{\\prime}=\\frac{1+\\gamma L_{\\operatorname*{max}}}{\\gamma L_{\\operatorname*{max}}}\\cdot\\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\left\\Vert x_{k}-\\mathrm{prox}_{\\gamma f_{i}}\\left(x_{k}\\right)\\right\\Vert^{2}}{\\left\\Vert\\frac{1}{n}\\sum_{i=1}^{n}\\left(x_{k}-\\mathrm{prox}_{\\gamma f_{i}}\\left(x_{k}\\right)\\right)\\right\\Vert^{2}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Corollary 2. Suppose all the assumptions mentioned in Theorem 2 hold, $i f$ we are using (13) to determine $\\alpha_{k,G}^{\\prime}$ in each iteration for Algorithm $^{\\,l}$ with $\\tau=n$ , then the iterates satisfy ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\bar{x}_{K})\\right]-f^{\\mathrm{inf}}\\leq\\left(\\frac{1}{\\gamma}+L_{\\mathrm{max}}\\right)\\cdot\\frac{\\left\\Vert x_{0}-x_{\\star}\\right\\Vert^{2}}{\\sum_{k=0}^{K-1}\\alpha_{k,G}^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\bar{x}_{K}$ is chosen randomly from the first $K$ iterates $\\{x_{0},x_{1},...,x_{K-1}\\}$ with probabilities $p_{k}=$ $\\alpha_{k,G}^{\\prime}\\Big/\\Sigma_{k=0}^{K-1}\\,\\alpha_{k,G}^{\\prime}$ . ", "page_idx": 22}, {"type": "text", "text": "Notice that compared to the case of FedExProx-GraDS in Theorem 2, the convergence rate given in Corollary 2 is indeed better. This can be seen by comparing them directly, for FedExProx-GraDS, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\bar{x}_{K})\\right]-\\operatorname*{inf}{f\\leq\\frac{1+\\gamma L_{\\operatorname*{max}}}{2+\\gamma L_{\\operatorname*{max}}}\\cdot\\left(\\frac{1}{\\gamma}+L_{\\operatorname*{max}}\\right)\\cdot\\frac{\\|x_{0}-x_{\\star}\\|^{2}}{\\sum_{k=0}^{K-1}\\alpha_{k,G}}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and for Algorithm 1 with $\\alpha_{k,G}^{\\prime}$ given in (13), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f(\\bar{x}_{K})\\right]-f^{\\mathrm{inf}}\\le\\left(\\frac{1}{\\gamma}+L_{\\mathrm{max}}\\right)\\cdot\\frac{\\left\\Vert x_{0}-x_{\\star}\\right\\Vert^{2}}{\\sum_{k=0}^{K-1}\\alpha_{k,G}^{\\prime}}}\\\\ &{\\qquad\\qquad=\\frac{\\gamma L_{\\mathrm{max}}}{1+\\gamma L_{\\mathrm{max}}}\\cdot\\left(\\frac{1}{\\gamma}+L_{\\mathrm{max}}\\right)\\cdot\\frac{\\left\\Vert x_{0}-x_{\\star}\\right\\Vert^{2}}{\\sum_{k=0}^{K-1}\\alpha_{k,G}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\gamma L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}\\leq\\frac{1+\\gamma L_{\\operatorname*{max}}}{2+\\gamma L_{\\operatorname*{max}}},\\quad\\forall\\gamma>0,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "the convergence of Algorithm 1 in the full participation case with (13) given in Corollary 2 is indeed better than FedExProx-GraDS. However, this adaptive rule is only practical when we have the knowledge of local smoothness. ", "page_idx": 22}, {"type": "text", "text": "Notes on stochastic Polyak variant. In this paragraph, we further elaborate on the convergence of FedExProx-StoPS. We start by providing a lower bound on the adaptive extrapolation parameter. ", "page_idx": 23}, {"type": "text", "text": "Lemma 20. Suppose that all assumptions mentioned in Theorem 2 hold, then the following inequalities hold for any $\\bar{x}\\in\\mathbb{R}^{d}$ and $x_{\\star}$ that is a minimizer of $f$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\frac1n\\sum_{i=1}^{n}\\Big(M_{f_{i}}^{\\gamma}\\left(x\\right)-M_{f_{i}}^{\\gamma}\\left(x_{\\star}\\right)\\Big)}{\\gamma\\cdot\\Big\\Vert\\frac1n\\sum_{i=1}^{n}\\nabla M_{f_{i}}^{\\gamma}\\left(x\\right)\\Big\\Vert^{2}}\\ge\\frac{1}{2\\gamma L_{\\gamma}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Using the above lower bound, we can further write the convergence of FedExProx-StoPS as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\bar{x}^{K})\\right]-\\operatorname*{inf}{f}\\leq2L_{\\gamma}\\left(1+2\\gamma L_{\\operatorname*{max}}\\right)\\cdot\\frac{\\|x_{0}-x_{\\star}\\|^{2}}{K}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Observe that we recover the favorable dependence of convergence on the smoothness of $M^{\\gamma}$ . However, this comes at the price of having to know each $M_{f_{i}}^{\\gamma}\\left(x_{\\star}\\right)$ or, equivalently in the interpolation regime, knowing $M^{\\gamma}\\left(x_{\\star}\\right)$ . ", "page_idx": 23}, {"type": "text", "text": "F.6 Extension of adaptive variants into client partial participation (PP) setting ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this subsection, we extend the adaptive variants of FedExProx into the stochastic setting. We will refer to them as FedExProx-GraDS-PP and, FedExProx-StoPS-PP respectively. Specifically, we consider that the server chooses the client using the $\\tau$ -nice sampling strategy we have introduced before in Algorithm 1. The following theorem summarizes the convergence guarantee of FedExProx-GraDS-PP and FedExProx-StoPS-PP in the convex case. Its extension to the strongly convex case where we additionally assume Assumption 5 (Strong convexity) is straight forward. ", "page_idx": 23}, {"type": "text", "text": "Theorem 5. Suppose Assumption 1 (Differentiability), Assumption 2 (Interpolation regime), Assumption 3 (Convexity) and Assumption $^{4}$ (Smoothness) hold. Assume we are running FedExProx with $\\tau$ -nice client sampling. ", "page_idx": 23}, {"type": "text", "text": "(i) (FedExProx-GraDS-PP): If we are using $\\alpha_{k}=\\alpha_{\\tau,k,G}(\\boldsymbol{x}_{k},S_{k})$ , where ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\alpha_{\\tau,k,G}(x_{k},S_{k})=\\frac{\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\left\\Vert x_{k}-\\mathrm{prox}_{\\gamma f_{i}}\\left(x_{k}\\right)\\right\\Vert^{2}}{\\left\\Vert\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\left(x_{k}-\\mathrm{prox}_{\\gamma f_{i}}\\left(x_{k}\\right)\\right)\\right\\Vert^{2}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then the iterates of Algorithm $^{\\,l}$ satisfy ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\bar{x}_{K})\\right]-\\operatorname*{inf}\\,f\\leq\\left(\\frac{1+\\gamma L_{\\operatorname*{max}}}{2+\\gamma L_{\\operatorname*{max}}}\\right)\\cdot\\left(\\frac{1}{\\gamma}+L_{\\operatorname*{max}}\\right)\\cdot\\frac{\\|x_{0}-x_{\\star}\\|^{2}}{\\operatorname*{inf}\\alpha_{\\tau,k,G}\\cdot K},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $K$ is the total number of iteration, ${\\bar{x}}_{K}$ is samples uniformly at random from the first $K$ iterates $\\{x_{0},x_{1},\\dots,x_{K-1}\\}$ , inf $\\alpha_{\\tau,k,G}$ is defined as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{inf}\\alpha_{\\tau,k,G}:=\\operatorname*{inf}_{x\\in\\mathbb{R}^{d},S\\subseteq[n],|S|=\\tau}\\alpha_{\\tau,k,G}\\left(x,S\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "satisfying ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\alpha_{\\tau,k,G}(x_{k},S_{k})\\geq\\operatorname*{inf}\\alpha_{\\tau,k,G}\\geq1.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "(ii) (FedExProx-StoPS-PP): If we are using $\\alpha_{k}=\\alpha_{\\tau,k,S}(\\boldsymbol{x_{k}},S_{k})$ , where ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\alpha_{\\tau,k,S}(x_{k},S_{k})=\\frac{\\frac{1}{\\tau}\\sum_{i=1}^{\\tau}\\left(M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)-\\operatorname*{inf}M_{f_{i}}^{\\gamma}\\right)}{\\gamma\\left\\|\\frac{1}{\\tau}\\sum_{i=1}^{\\tau}\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)\\right\\|^{2}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then the iterates of Algorithm $^{\\,l}$ satisfy ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\bar{x}_{K})\\right]-\\operatorname*{inf}{f}\\leq\\left(\\frac{1}{\\gamma}+L_{\\operatorname*{max}}\\right)\\cdot\\frac{\\|x_{0}-x_{\\star}\\|^{2}}{\\operatorname*{inf}{\\alpha_{\\tau,k,S}\\cdot K}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $K$ is the total number of iteration, $\\bar{x}_{K}$ is sampled uniformly at random from the first $K$ iterates $\\{x_{0},x_{1},\\dots,x_{K-1}\\}$ , inf $\\alpha_{\\tau,k,G}$ is defined as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{inf}\\alpha_{\\tau,k,S}:=\\operatorname*{inf}_{x\\in\\mathbb{R}^{d},S\\subseteq[n],|S|=\\tau}\\alpha_{\\tau,k,S}\\left(x,S\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "satisfying ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\alpha_{\\tau,k,S}(x_{k},S_{k})\\geq\\operatorname*{inf}\\alpha_{\\tau,k,S}\\geq\\frac{1}{2}\\left(1+\\frac{1}{\\gamma L_{\\operatorname*{max}}}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Remark 15. For FedExProx-GraDS-PP, different from the full participation setting, the denominator of the sublinear term on the right-hand side of (20) is replaced by $K\\cdot\\operatorname*{inf}\\alpha_{\\tau,k,G}$ . ", "page_idx": 24}, {"type": "text", "text": "(i) In the single client case $\\prime\\tau=1$ ), we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\alpha_{1,k,G}=\\operatorname*{inf}\\alpha_{1,k,G}=1.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "(ii) In the partial participation case $1<\\tau<n_{\\!}$ ), it is possible that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{inf}\\alpha_{\\tau,k,G}>1,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "resulting in acceleration compared to single client case. ", "page_idx": 24}, {"type": "text", "text": "(iii) For the full participation case $\\prime_{\\tau}=n$ ), we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\alpha_{k,G}=\\alpha_{n,k,G},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{k=0}^{K-1}\\alpha_{k,G}\\geq K\\cdot\\operatorname*{inf}\\alpha_{n,k,G},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "thus the convergence guarantee here is a relaxed version of that presented in Theorem 2. ", "page_idx": 24}, {"type": "text", "text": "A similar discussion also applies to FedExProx-StoPS-PP in the client partial participation setting. Remark 16. For FedExProx-StoPS-PP, different from the full participation setting, the denominator of the sublinear term on the right-hand side of (22) is replaced by $K\\cdot\\operatorname*{inf}\\alpha_{\\tau,k,S}$ . ", "page_idx": 24}, {"type": "text", "text": "(i) In the single client case $\\prime\\tau=1$ ), we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\alpha_{1,k,S}\\geq\\operatorname*{inf}\\alpha_{1,k,G}=\\frac{1}{2}\\left(1+\\frac{1}{\\gamma L_{\\operatorname*{max}}}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "(ii) In the partial participation case $1<\\tau<n_{\\!}$ ), it is possible that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{inf}\\alpha_{\\tau,k,S}>\\frac{1}{2}\\left(1+\\frac{1}{\\gamma L_{\\operatorname*{max}}}\\right),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "resulting in acceleration compared to single client case. ", "page_idx": 24}, {"type": "text", "text": "(iii) For the full participation case $\\tau=n$ ), we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\alpha_{k,S}=\\alpha_{n,k,S},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{k=0}^{K-1}\\alpha_{k,S}\\geq K\\cdot\\operatorname*{inf}\\alpha_{n,k,S},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "thus the convergence guarantee here is a relaxed version of that presented in Theorem 2. ", "page_idx": 24}, {"type": "text", "text": "The following Table 5 summarizes the convergence of new algorithms and their variants appeared in our paper. ", "page_idx": 24}, {"type": "text", "text": "G Missing proofs of theorems and corollaries ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "G.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The proof of this theorem can be divided into three parts. ", "page_idx": 24}, {"type": "text", "text": "Table 5: Summary of convergence of new algorithms appeared in our paper in the convex setting. The $\\mathcal{O}\\left(\\cdot\\right)$ notation is hidden for all complexities in this table. For convergence in the full client participation case, results of Theorem 1 and Theorem 2 are used where the relevant notations are defined. For convergence in the partial participation, the results of Theorem 5 are used. ", "page_idx": 25}, {"type": "table", "img_path": "FuTfZK7PK3/tmp/0b4404e63355320e93e001201f00f3c5eda3ac29e45a8336991672369f9e8985.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Step 1: Reformulate the algorithm using Moreau envelope. We know from Lemma 2 that for any $\\overline{{x}}\\in\\mathbb{R}^{d}$ . ", "text_level": 1, "page_idx": 25}, {"type": "equation", "text": "$$\n\\nabla M_{f_{i}}^{\\gamma}\\left(x\\right)=\\frac{1}{\\gamma}\\left(x-\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right)\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using the above identity, we can rewrite the update rule given in (7) in the following form, ", "page_idx": 25}, {"type": "equation", "text": "$$\nx_{k+1}=x_{k}-\\alpha_{k}\\gamma\\cdot\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The above reformulation suggests that running FedExProx with $\\tau$ -nice sampling strategy is equivalent to running SGD with $\\tau$ -nice sampling to the global objective $\\begin{array}{r}{M^{\\gamma}\\left(x\\right)=\\frac{\\mathrm{f}}{n}\\sum_{i=1}^{\\check{n}}M_{f_{i}}^{\\widecheck{\\gamma}^{\\prime}}(x)}\\end{array}$ with step size $\\alpha_{k}{\\gamma}$ . Now, it seems natural to apply the theory of SGD adapted in Theorem 3. However, before proceeding, we list the properties we know about the global objective $M^{\\gamma}$ and each local objective $\\bar{M}_{f_{i}}^{\\gamma}$ ", "page_idx": 25}, {"type": "text", "text": "1. Each $M_{f_{i}}^{\\gamma}\\left(x\\right)$ is convex. This is a consequence of a direct application of Lemma 3 to each $f_{i}$ . Since $M^{\\gamma}$ is the average of convex functions $M_{f_{i}}^{\\gamma}$ , we conclude that $M^{\\gamma}\\left(x\\right)$ is also convex.   \n2. Each $M_{f_{i}}^{\\gamma}$ (x) is $\\begin{array}{r}{\\frac{L_{i}}{1+\\gamma L_{i}}}\\end{array}$ -smooth, where $L_{i}$ is the smoothness constant of $f_{i}$ . This is proved by applying Lemma 4 to each $f_{i}$ . Drawing on Lemma 13 for justification, it is reasonable to assume M \u03b3 (x) is L\u03b3-smooth with L\u03b3 \u2264n1 in 1+\u03b3Li =1 Li -smooth.   \n3. Each $M_{f_{i}}^{\\gamma}\\left(x\\right)$ has the same set of minimizers and minimum as $f_{i}$ . This result arises from applying Lemma 5 to each function $f_{i}$ .   \n4. Furthermore, if Assumption 2 (Interpolation Regime) holds, $M^{\\gamma}\\left(x\\right)$ and $f(x)$ have the same set of minimizers and minimum. This is demonstrated in Lemma 8.   \naSnted pc 2o:n vAepx,p ei st hceoonrvye xo fa gnrd -nst mtoyopteh .m Feturhtohdesr.morNeo, tdicuee  tthoa tt hhee raes $M_{f_{i}}^{\\gamma}\\left(x\\right)$ ni so $\\frac{L_{i}}{1+\\gamma L_{i}}$ -psolmaotiootnh $M^{\\gamma}\\left(x\\right)$ $L_{\\gamma}$   \nregime, $M^{\\gamma}\\left(x\\right)$ and $f(x)$ have the same set of minimizers. Applying the theory of SGD with $\\tau$ -nice   \nsampling in this case, where ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "equation", "text": "$$\nA_{\\tau}=L_{\\gamma,\\tau}=\\frac{n-\\tau}{\\tau(n-1)}\\cdot\\operatorname*{max}_{i\\in[n]}\\left(\\frac{L_{i}}{1+\\gamma L_{i}}\\right)+\\frac{n(\\tau-1)}{\\tau(n-1)}L_{\\gamma}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Notice that using Fact 4, we know that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[n]}\\left(\\frac{L_{i}}{1+\\gamma L_{i}}\\right)\\overset{\\mathrm{Fact}\\;4}{=}\\frac{L_{\\mathrm{max}}}{1+\\gamma L_{\\mathrm{max}}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "thus $L_{\\gamma}$ can be simplified and written as ", "page_idx": 25}, {"type": "equation", "text": "$$\nL_{\\gamma,\\tau}=\\frac{n-\\tau}{\\tau(n-1)}\\cdot\\frac{L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}+\\frac{n(\\tau-1)}{\\tau(n-1)}L_{\\gamma},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where Lmax = maxi Li. We obtain the following result given that 0 < \u03b1\u03b3 < L\u03b32,\u03c4 in the convex setting, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[M^{\\gamma}\\left(\\bar{x}_{K}\\right)\\right]-M^{\\gamma}\\left(x_{\\star}\\right)\\overset{\\mathrm{Theorem3}}{\\leq}\\frac{1}{\\alpha\\gamma(2-\\alpha\\gamma L_{\\gamma,\\tau})}\\cdot\\frac{\\|x_{0}-x_{\\star}\\|^{2}}{K},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\bar{x}_{K}$ is sampled uniformly at random from the first $K$ iterates $\\{x_{0},x_{1},\\dots,x_{K-1}\\}$ . However, the convergence mentioned pertains to $M^{\\gamma}\\left(x\\right)$ . Given our objective is to solve (1), it is necessary to reinterpret this outcome in terms of $f$ . ", "page_idx": 26}, {"type": "text", "text": "Step 3: Translate the result into function values of $f$ . This step is only needed in the convex setting. We use the lower bound in Lemma 10, ", "text_level": 1, "page_idx": 26}, {"type": "equation", "text": "$$\nM^{\\gamma}\\left(\\bar{x}_{K}\\right)-M^{\\gamma}\\left(x_{\\star}\\right)\\stackrel{(18)}{\\geq}\\frac{1}{1+\\gamma L_{\\operatorname*{max}}}\\left(f(\\bar{x}_{K})-f(x_{\\star})\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "to obtain the following result ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\bar{x}_{K})\\right]-f(x_{\\star})\\leq\\frac{1+\\gamma L_{\\operatorname*{max}}}{\\alpha\\gamma\\left(2-\\alpha\\gamma L_{\\gamma,\\tau}\\right)}\\cdot\\frac{\\left\\Vert x_{0}-x_{\\star}\\right\\Vert^{2}}{K}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Observe that we have ", "page_idx": 26}, {"type": "equation", "text": "$$\nC\\left(\\gamma,\\tau,\\alpha\\right)=\\frac{1+\\gamma L_{\\operatorname*{max}}}{\\alpha\\gamma\\left(2-\\alpha\\gamma L_{\\gamma,\\tau}\\right)},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and its numerator does not depend on $\\alpha$ . If we fix the choice of $\\gamma$ and $\\tau$ , then the denominator is maximized when $\\alpha\\gamma L_{\\gamma,\\tau}\\,=\\,1$ . This yields the optimal constant extrapolation parameter $\\alpha_{\\gamma,\\tau}=$ $\\frac{1}{\\gamma L_{\\gamma,\\tau}}$ and the following convergence corresponding to it ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\bar{x}_{K})\\right]-f(x_{\\star})\\le L_{\\gamma,\\tau}\\left(1+\\gamma L_{\\operatorname*{max}}\\right)\\cdot\\frac{\\|x_{0}-x_{\\star}\\|^{2}}{K}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Finally, notice that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\gamma L_{\\gamma}\\overset{\\mathrm{Lemma~13}}{\\leq}\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\gamma L_{i}}{1+\\gamma L_{i}}<1,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for any $\\gamma>0$ . This suggests that, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma L_{\\gamma,\\tau}=\\displaystyle\\frac{n-\\tau}{\\tau(n-1)}\\cdot\\frac{\\gamma L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}+\\frac{n(\\tau-1)}{\\tau(n-1)}\\gamma L_{\\gamma}}\\\\ &{\\hphantom{\\gamma L_{\\gamma,\\tau}}<\\displaystyle\\frac{n-\\tau}{\\tau(n-1)}+\\frac{n(\\tau-1)}{\\tau(n-1)}=1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which in turn tells us $\\begin{array}{r}{\\alpha_{\\gamma,\\tau}=\\frac{1}{\\gamma L_{\\gamma,\\tau}}>1}\\end{array}$ . This concludes the proof. ", "page_idx": 26}, {"type": "text", "text": "G.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We start with the following decomposition, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|x_{k+1}-x_{\\star}\\right\\|^{2}=\\left\\|x_{k}-\\alpha_{k}\\gamma\\nabla M^{\\gamma}\\left(x_{k}\\right)-x_{\\star}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\left\\|x_{k}-x_{\\star}\\right\\|^{2}-2\\alpha_{k}\\gamma\\left\\langle\\nabla M^{\\gamma}\\left(x_{k}\\right),x_{k}-x_{\\star}\\right\\rangle+\\alpha_{k}^{2}\\gamma^{2}\\left\\|\\nabla M^{\\gamma}\\left(x\\right)\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Case 1: FedExProx-GraDS For gradient diversity based $\\alpha_{k}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\alpha_{k}=\\alpha_{k,G}=\\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|\\gamma\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)\\right\\|^{2}}{\\left\\|\\gamma\\nabla M^{\\gamma}\\left(x_{k}\\right)\\right\\|^{2}}=\\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)\\right\\|^{2}}{\\left\\|\\nabla M^{\\gamma}\\left(x_{k}\\right)\\right\\|^{2}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For the last term of (24), ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\alpha_{k,G}^{2}\\gamma^{2}\\left\\|\\nabla M^{\\gamma}\\left(x_{k}\\right)\\right\\|^{2}=\\alpha_{k,G}\\gamma^{2}\\cdot\\cfrac{1}{n}\\sum_{i=1}^{n}\\left\\|\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)\\right\\|^{2}}}\\\\ &{=\\alpha_{k,G}\\gamma^{2}\\cdot\\cfrac{1}{n}\\sum_{i=1}^{n}\\left\\|\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)-\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)\\right\\|^{2}}\\\\ &{\\overset{(16)}{\\leq}\\alpha_{k,G}\\gamma^{2}\\cdot\\cfrac{1}{n}\\sum_{i=1}^{n}\\cfrac{L_{i}}{1+\\gamma L_{i}}\\left(D_{M_{f_{i}}^{\\gamma}}\\left(x_{k},x_{k}\\right)+D_{M_{f_{i}}^{\\gamma}}\\left(x_{k},x_{k}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last inequality follows from the1+L\u03b3iLi - smoothness of $M_{f_{i}}^{\\gamma}$ given in Lemma 4. We further obtain using Fact 4 that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{k,G}^{2}\\gamma^{2}\\left\\Vert\\nabla M^{\\gamma}\\left(x_{k}\\right)\\right\\Vert^{2}\\overset{\\mathrm{Fact4}}{\\leq}\\alpha_{k,G}\\gamma^{2}\\cdot\\frac{L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}\\cdot\\left(D_{M^{\\gamma}}\\left(x_{k},x_{\\star}\\right)+D_{M^{\\gamma}}\\left(x_{\\star},x_{k}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\alpha_{k,G}\\gamma\\cdot\\frac{\\gamma L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}\\left(D_{M^{\\gamma}}\\left(x_{k},x_{\\star}\\right)+D_{M^{\\gamma}}\\left(x_{\\star},x_{k}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For the second term of (24), we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{-2\\alpha_{k,G}\\gamma\\left\\langle\\nabla M^{\\gamma}\\left(x_{k}\\right),x_{k}-x_{\\star}\\right\\rangle=2\\alpha_{k,G}\\gamma\\left\\langle\\nabla M^{\\gamma}\\left(x_{k}\\right)-\\nabla M^{\\gamma}\\left(x_{\\star}\\right),x_{\\star}-x_{k}\\right\\rangle}\\\\ &{}&{=-2\\alpha_{k,G}\\gamma\\left(D_{M^{\\gamma}}\\left(x_{k},x_{\\star}\\right)+D_{M^{\\gamma}}\\left(x_{\\star},x_{k}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Plugging (26) and (25) into (24), we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left\\Vert x_{k+1}-x_{\\star}\\right\\Vert^{2}\\leq\\left\\Vert x_{k}-x_{\\star}\\right\\Vert^{2}-\\alpha_{k,G}\\gamma\\left(2-\\frac{\\gamma L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}\\right)\\left(D_{M^{\\gamma}}\\left(x_{k},x_{\\star}\\right)+D_{M^{\\gamma}}\\left(x_{\\star},x_{k}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Notice that we know that ", "page_idx": 27}, {"type": "equation", "text": "$$\nD_{M^{\\gamma}}\\left(x_{k},x_{\\star}\\right)\\stackrel{(14)}{=}M^{\\gamma}\\left(x_{k}\\right)-M^{\\gamma}\\left(x_{\\star}\\right),\\quad D_{M^{\\gamma}}\\left(x_{\\star},x_{k}\\right)\\stackrel{(15)}{\\geq}0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "As a result, we obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left\\Vert x_{k+1}-x_{\\star}\\right\\Vert^{2}\\leq\\left\\Vert x_{k}-x_{\\star}\\right\\Vert^{2}-\\alpha_{k,G}\\gamma\\left(2-\\frac{\\gamma L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}\\right)\\left(M^{\\gamma}\\left(x_{k}\\right)-M^{\\gamma}\\left(x_{\\star}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Summing up the above recursion for $k=0,1,...,K-1$ , we notice that many of them will telescope and $M^{\\gamma}\\,\\overline{{(x_{\\star})}}=\\operatorname*{inf}\\,M^{\\gamma}$ due to interpolation regime as it is proved by Lemma 8. Thus, we obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\gamma\\left(2-\\frac{\\gamma L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}\\right)\\sum_{k=0}^{K-1}\\alpha_{k,G}\\left(M^{\\gamma}\\left(x_{k}\\right)-\\operatorname*{inf}{M^{\\gamma}}\\right)\\leq\\left\\Vert x_{0}-x_{\\star}\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Denote $\\begin{array}{r}{p_{k}\\ =\\ \\alpha_{k,G}\\Big/\\!\\sum_{k=0}^{K-1}\\alpha_{k,G}}\\end{array}$ for $k\\;=\\;0,1,...,K\\;-\\;1$ . If we pick $\\bar{x}_{K}$ randomly according to probabilities $p_{k}$ from the first $K$ iterates $\\{x_{0},x_{1},\\dots,x_{K-1}\\}$ , then we can further write the above recursion as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[M^{\\gamma}\\left(\\bar{x}^{K}\\right)\\right]-\\operatorname*{inf}{M^{\\gamma}}\\leq\\frac{1+\\gamma L_{\\operatorname*{max}}}{2+\\gamma L_{\\operatorname*{max}}}\\cdot\\frac{1}{\\gamma}\\cdot\\frac{\\|x_{0}-x_{\\star}\\|^{2}}{\\sum_{k=0}^{K-1}\\alpha_{k,G}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Utilizing Lemma 10, we further obtain, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\bar{x}_{K})\\right]-\\operatorname*{inf}{f}\\le\\frac{1+\\gamma L_{\\operatorname*{max}}}{2+\\gamma L_{\\operatorname*{max}}}\\cdot\\left(\\frac{1}{\\gamma}+L_{\\operatorname*{max}}\\right)\\cdot\\frac{\\|x_{0}-x_{\\star}\\|^{2}}{\\sum_{k=0}^{K-1}\\alpha_{k,G}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The above inequality indicates convergence. Indeed, by convexity of standard Euclidean norm, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\alpha_{k,G}\\geq\\frac{\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\left(x_{k}-\\mathrm{prox}_{\\gamma f_{i}}\\left(x_{k}\\right)\\right)\\right\\|^{2}}{\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\left(x_{k}-\\mathrm{prox}_{\\gamma f_{i}}\\left(x_{k}\\right)\\right)\\right\\|^{2}}=1.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This tells us that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{k=0}^{K-1}\\alpha_{k,G}\\geq K.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Case 2: FedExProx-StoPS For stochastic Polyak step size based $\\alpha_{k,S}$ , since we are in the interpolation regime, by Lemma 9, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\nM^{\\gamma}\\left(x_{\\star}\\right)=\\operatorname*{inf}\\,M^{\\gamma}=\\frac{1}{n}\\sum_{i=1}^{n}\\operatorname*{inf}\\,M_{f_{i}}^{\\gamma}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "As a result, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\alpha_{k}=\\alpha_{k,S}=\\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\left(M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)-\\operatorname*{inf}M_{f_{i}}^{\\gamma}\\right)}{\\gamma\\left\\Vert\\frac{1}{n}\\sum_{i=1}^{n}\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)\\right\\Vert^{2}}=\\frac{M^{\\gamma}\\left(x_{k}\\right)-M^{\\gamma}\\left(x_{\\star}\\right)}{\\gamma\\left\\Vert\\nabla M^{\\gamma}\\left(x_{k}\\right)\\right\\Vert^{2}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We have for the last term of (24), ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\alpha_{k,S}^{2}\\gamma^{2}\\left\\|\\nabla M^{\\gamma}\\left(x_{k}\\right)\\right\\|^{2}=\\alpha_{k,S}\\gamma\\left(M^{\\gamma}\\left(x_{k}\\right)-M^{\\gamma}\\left(x_{\\star}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For the second term of (24), we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-2\\alpha_{k,S^{\\gamma}}\\left\\langle\\nabla M^{\\gamma}\\left(x_{k}\\right),x_{k}-x_{\\star}\\right\\rangle=2\\alpha_{k,S^{\\gamma}}\\left\\langle\\nabla M^{\\gamma}\\left(x_{k}\\right),x_{\\star}-x_{k}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\stackrel{(5)}{\\leq}2\\alpha_{k,S^{\\gamma}}\\left(M^{\\gamma}\\left(x_{\\star}\\right)-M^{\\gamma}\\left(x_{k}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad=-2\\alpha_{k,S^{\\gamma}}\\left(M^{\\gamma}\\left(x_{k}\\right)-M^{\\gamma}\\left(x_{\\star}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the inequality is due to the convexity of $M^{\\gamma}$ . Plugging (28) and (27) into (24), we obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|{\\boldsymbol x}_{{k+1}}-{\\boldsymbol x}_{\\star}\\|^{2}\\leq\\|{\\boldsymbol x}_{k}-{\\boldsymbol x}_{\\star}\\|^{2}-\\alpha_{k,S}\\gamma\\left(M^{\\gamma}\\left({\\boldsymbol x}_{k}\\right)-M^{\\gamma}\\left({\\boldsymbol x}_{\\star}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Summing up the above recursion for $k=0,1,...,K-1$ , we notice that many of them will telescope. Thus, we obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\gamma\\sum_{k=0}^{K-1}\\alpha_{k,S}\\left(M^{\\gamma}\\left(x_{k}\\right)-\\operatorname*{inf}M^{\\gamma}\\right)\\leq\\left\\|x_{0}-x_{\\star}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Denote $\\begin{array}{r}{p_{k}\\ =\\ \\alpha_{k,S}\\Big/\\!\\sum_{k=0}^{K-1}\\alpha_{k,S}}\\end{array}$ for $k\\;=\\;0,1,...,K\\;-\\;1$ . If we sample $\\bar{x}^{K}$ randomly according to probabilities $p_{k}$ from the first $K$ iterates $\\{x_{0},x_{1},\\dots,x_{K-1}\\}$ , we can further write the above recursion as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[M^{\\gamma}\\left(\\bar{x}^{K}\\right)\\right]-\\operatorname*{inf}_{\\gamma}M^{\\gamma}\\leq\\frac{1}{\\gamma}\\cdot\\frac{\\left\\Vert x_{0}-x_{\\star}\\right\\Vert^{2}}{\\sum_{k=0}^{K-1}\\alpha_{k,S}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Utilizing the local bound in Lemma 10, we further obtain, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\bar{x}^{K})\\right]-\\operatorname*{inf}{f\\stackrel{(17)}{\\leq}}\\left({\\frac{1}{\\gamma}}+L_{\\operatorname*{max}}\\right)\\cdot{\\frac{\\left\\|x_{0}-x_{\\star}\\right\\|^{2}}{\\sum_{k=0}^{K-1}\\alpha_{k,S}}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Notice that the above inequality indeed indicates convergence, since ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{k=0}^{K-1}\\alpha_{k,S}=\\sum_{k=0}^{K-1}\\frac{M^{\\gamma}\\left(x_{k}\\right)-M^{\\gamma}\\left(x_{\\star}\\right)}{\\gamma\\left\\|\\nabla M^{\\gamma}\\left(x_{k}\\right)\\right\\|^{2}}\\ge\\frac{1}{2\\gamma L_{\\gamma}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the inequality follows from Lemma 20. The above upper bounds allow us to further write the convergence in (29) as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\bar{x}^{K})\\right]-\\operatorname*{inf}{f}\\leq2L_{\\gamma}\\left(1+2\\gamma L_{\\operatorname*{max}}\\right)\\cdot\\frac{\\|x_{0}-x_{\\star}\\|^{2}}{K}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 28}, {"type": "text", "text": "G.3 Proof of Theorem 3 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We start from the decomposition, ", "text_level": 1, "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\Vert x_{k+1}-x_{\\star}\\right\\Vert^{2}=\\left\\Vert x_{k}-x_{\\star}\\right\\Vert^{2}-2\\eta\\left\\langle x_{k}-x_{\\star},\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\nabla f_{i}(x_{k})\\right\\rangle+\\eta^{2}\\left\\Vert\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\nabla f_{i}(x_{k})\\right\\Vert^{2},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $S_{k}$ is the set sampled at iteration $k$ . Taking expectation conditioned on $x_{k}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{S_{k}}\\left[\\left\\|x_{k+1}-x_{*}\\right\\|^{2}\\right]}\\\\ &{\\quad=\\left\\|x_{k}-x_{*}\\right\\|^{2}-2\\eta\\left\\langle x_{k}-x_{*},\\nabla f(x_{k})-\\nabla f(x_{\\star})\\right\\rangle+\\eta^{2}\\mathbb{E}_{S_{k}}\\left[\\left\\|\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\nabla f_{i}(x_{k})\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We can write the second inner product term as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\langle x_{k}-x_{\\star},\\nabla f(x_{k})-\\nabla f(x_{\\star})\\rangle\\stackrel{(14)}{=}D_{f}\\left(x_{k},x_{\\star}\\right)+D_{f}\\left(x_{\\star},x_{k}\\right),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $D_{f}\\left(x_{k},x_{\\star}\\right)$ denotes the Bregman divergence associated with $f$ between $x_{k}$ and $x_{\\star}$ . For the last squared norm term, we first define the indicator random variable $\\chi_{k,i}$ as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\chi_{k,i}=\\left\\{1,\\ \\ \\ \\mathrm{when}\\,i\\in S_{k},\\right.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since we are in the interpolation regime, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S_{k}}\\left[\\left\\lVert\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\nabla f_{i}(x_{k})\\right\\rVert^{2}\\right]=\\mathbb{E}_{S_{k}}\\left[\\left\\lVert\\frac{1}{\\tau}\\sum_{i=1}^{n}\\chi_{k,i}\\left(\\nabla f_{i}(x_{k})-\\nabla f_{i}(x_{k})\\right)\\right\\rVert^{2}\\right].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Denote $a_{k,i}=\\nabla f_{i}(x_{k})-\\nabla f_{i}(x_{\\star})$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\delta u}\\left[\\left\\|\\frac{1}{r_{u}}\\sum_{i=1}^{n}\\nabla_{j}(\\mathbf{r}_{i}(x_{t})-\\nabla f_{i}(x_{t})_{r_{u}})\\right\\|^{2}\\right]}\\\\ &{\\quad=\\mathbb{E}_{\\delta u}\\left[\\left\\|\\frac{1}{r_{u}}\\sum_{i=1}^{n}\\delta_{u,i}\\right\\|^{2}\\right]}\\\\ &{\\quad=\\frac{1}{r_{u}^{2}}\\mathbb{E}_{\\delta u}\\left[\\sum_{i=1}^{n}\\delta_{u,i}\\right]\\Big(\\underbrace{\\mathbf{r}}_{1\\le s\\le t\\le s}\\sum_{i=t_{s}}\\delta_{u,i}\\delta_{s,i}\\left(\\alpha_{s,i}\\alpha_{s,i}\\right)\\right)}\\\\ &{\\quad=\\frac{1}{r_{u}^{2}}\\sum_{i=1}^{n}\\mathbb{E}_{\\delta u}\\left[\\sum_{i=1}^{d}|{\\alpha}_{s,i}|^{2}+\\sum_{s\\le t\\le s}\\mathbb{E}_{\\delta u}\\left[\\sum_{i=t_{s}}\\delta_{u,i}\\alpha_{s,i}\\right](\\alpha_{s,i}\\alpha_{s,j})\\right.}\\\\ &{\\quad=\\frac{1}{n\\tau_{u}}\\sum_{i=1}^{n}|\\alpha_{s,i}|^{2}+\\frac{\\tau-1}{n\\tau(n-1)}\\left(\\left\\|\\sum_{i=t_{s}}^{n}\\alpha_{s,i}\\right\\|^{2}-\\sum_{i=1}^{n}|\\alpha_{s,i}|^{2}\\right)}\\\\ &{\\quad\\left.=\\frac{n-\\tau}{r(n-1)}\\cdot\\frac{1}{n}\\int_{\\mathbb{R}_{}}\\left[\\alpha_{s,i}\\right]^{2}+\\frac{n(\\tau-1)}{r(n-1)}\\cdot\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\alpha_{s,i}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For the first term above in (31), due to the smoothness and convexity of each $f_{i}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac1n\\sum_{i=1}^{n}\\|a_{k,i}\\|^{2}=\\displaystyle\\frac1n\\sum_{i=1}^{n}\\|\\nabla f_{i}(x_{k})-\\nabla f_{i}(x_{k})\\|^{2}}\\\\ {\\displaystyle\\le\\frac1n\\sum_{i=1}^{n}L_{i}\\left(D_{f_{i}}\\left(x_{\\star},x_{k}\\right)+D_{f_{i}}\\left(x_{k},x_{\\star}\\right)\\right)}\\\\ {\\displaystyle\\le L_{\\operatorname*{max}}\\frac1n\\sum_{i=1}^{n}\\left(D_{f_{i}}\\left(x_{\\star},x_{k}\\right)+D_{f_{i}}\\left(x_{k},x_{\\star}\\right)\\right)}\\\\ {\\displaystyle=L_{\\operatorname*{max}}\\left(D_{f}\\left(x_{\\star},x_{k}\\right)+D_{f}\\left(x_{k},x_{\\star}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the first inequality is obtained as a result of Fact 3. For the second term, we have due to the smoothness and convexity of $f$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}a_{k,i}\\right\\|^{2}=\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\nabla f_{i}(x_{k})-\\nabla f_{i}(x_{\\star})\\right)\\right\\|^{2}}}\\\\ &{}&{=\\left\\|\\nabla f(x_{k})-\\nabla f(x_{\\star})\\right\\|^{2}}\\\\ &{}&{\\le L\\left(D_{f}\\left(x_{\\star},x_{k}\\right)+D_{f}\\left(x_{k},x_{\\star}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the inequality is obtained using Fact 3. Combining the above two inequalities and plugging them into (31), we obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S_{k}}\\left[\\left\\lVert\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\nabla f_{i}(x_{k})\\right\\rVert^{2}\\right]\\le\\left(\\frac{n-\\tau}{\\tau(n-1)}\\cdot L_{\\operatorname*{max}}+\\frac{n(\\tau-1)}{\\tau(n-1)}\\cdot L\\right)\\left(D_{f}\\left(x_{\\star},x_{k}\\right)+D_{f}\\left(x_{k},x_{\\star}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Notice that we already defined $A_{\\tau}$ as ", "page_idx": 30}, {"type": "equation", "text": "$$\nA_{\\tau}=\\frac{n-\\tau}{\\tau(n-1)}\\cdot L_{\\operatorname*{max}}+\\frac{n(\\tau-1)}{\\tau(n-1)}\\cdot L.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Combining (30) and (32), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{S_{k}}\\left[\\left\\|x_{k+1}-x_{\\star}\\right\\|^{2}\\right]\\leq\\left\\|x_{k}-x_{\\star}\\right\\|^{2}-\\eta(2-\\eta A_{\\tau})\\left(D_{f}\\left(x_{\\star},x_{k}\\right)+D_{f}\\left(x_{k},x_{\\star}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "If we require $\\begin{array}{r}{0<\\eta<\\frac{2}{A_{\\tau}}}\\end{array}$ , we have $\\eta(2-\\eta A_{\\tau})\\geq0$ . ", "page_idx": 30}, {"type": "text", "text": "Convex regime. It remains to notice that $D_{f}\\left(x_{k},x_{\\star}\\right)+D_{f}\\left(x_{\\star},x_{k}\\right)\\geq D_{f}\\left(x_{k},x_{\\star}\\right)=f(x_{k})~-$ $f(x_{\\star})\\geq0$ , and we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S_{k}}\\left[\\left\\|x_{k+1}-x_{\\star}\\right\\|^{2}\\right]\\leq\\left\\|x_{k}-x_{\\star}\\right\\|^{2}-\\eta(2-\\eta A_{\\tau})\\left(f(x_{k})-f(x_{\\star})\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Taking expectation again and using tower property, we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|x_{k+1}-x_{\\star}\\right\\|^{2}\\right]\\leq\\mathbb{E}\\left[\\left\\|x_{k}-x_{\\star}\\right\\|^{2}\\right]-\\eta(2-\\eta A_{\\tau})\\left(\\mathbb{E}\\left[f(x_{k})\\right]-\\operatorname*{inf}f\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Unrolling this recurrence, we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\bar{x}_{K})\\right]-\\operatorname*{inf}{f\\le\\frac{1}{\\eta(2-\\eta A_{\\tau})}}\\cdot\\frac{\\left\\Vert x_{0}-x_{\\star}\\right\\Vert^{2}}{K},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $K$ is the total number of iterations, ${\\bar{x}}_{K}$ is selected uniformly at random from the first $K$ iterates $\\{x_{0},x_{1},\\dots,x_{K-1}\\}$ . ", "page_idx": 30}, {"type": "text", "text": "Star strongly convex regime. Due to star strong convexity of $f$ , we further lower bound the Bregman divergence ", "page_idx": 30}, {"type": "equation", "text": "$$\nD_{f}\\left(x_{k},x_{\\star}\\right)=f(x_{k})-f(x_{\\star})\\geq{\\frac{\\mu}{2}}\\left\\|x_{k}-x_{\\star}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S_{k}}\\left[\\left\\Vert x_{k+1}-x_{\\star}\\right\\Vert^{2}\\right]\\leq\\left(1-\\eta(2-\\eta A_{\\tau})\\cdot\\frac{\\mu}{2}\\right)\\left\\Vert x_{k}-x_{\\star}\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Taking expectation again, using tower property we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert x_{k+1}-x_{\\star}\\right\\Vert^{2}\\right]\\leq\\left(1-\\eta(2-\\eta A_{\\tau})\\cdot\\frac{\\mu}{2}\\right)\\mathbb{E}\\left[\\left\\Vert x_{k}-x_{\\star}\\right\\Vert^{2}\\right].\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Unrolling the recurrence, we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|x_{K}-x_{\\star}\\right\\|^{2}\\right]\\leq\\left(1-\\eta(2-\\eta A_{\\tau})\\cdot\\frac{\\mu}{2}\\right)^{K}\\left\\|x_{0}-x_{\\star}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 30}, {"type": "text", "text": "G.4 Proof of Theorem 4 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Since each $f_{i}$ is proper, closed and convex, by Lemma 2, we know that each $M_{f_{i}}^{\\gamma}$ is $\\frac{1}{\\gamma}$ -smooth. Therefore, it is reasonable to assume that $\\begin{array}{r}{M^{\\gamma}=\\frac{1}{n}\\sum_{i=1}^{n}M_{f_{i}}^{\\gamma}}\\end{array}$ is $L_{\\gamma}$ -smooth, with $\\begin{array}{r}{L_{\\gamma}\\leq\\frac{1}{\\gamma}}\\end{array}$ . Applying Theorem 3 in this case, we obtain, ", "page_idx": 30}, {"type": "equation", "text": "$$\nM^{\\gamma}\\left(\\bar{x}_{K}\\right)-\\operatorname*{inf}{M^{\\gamma}}\\overset{\\mathrm{Theorem3}}{\\leq}\\frac{1}{\\alpha\\gamma\\left(2-\\alpha\\gamma L_{\\gamma,\\tau}\\right)}\\cdot\\frac{\\|x_{0}-x_{\\star}\\|^{2}}{K},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\bar{x}_{K}$ is chosen uniformly at random from the first $K$ iterates $\\{x_{0},x_{1},\\dots,x_{K-1}\\}$ , and ", "page_idx": 30}, {"type": "equation", "text": "$$\nL_{\\gamma,\\tau}=\\frac{n-\\tau}{\\tau(n-1)}\\cdot\\frac{1}{\\gamma}+\\frac{n(\\tau-1)}{\\tau(n-1)}\\cdot L_{\\gamma}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Multiplying both sides by $\\gamma$ , we obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\gamma M^{\\gamma}\\left(\\bar{x}_{K}\\right)-\\operatorname*{inf}\\gamma M^{\\gamma}\\leq\\frac{1}{\\alpha\\left(2-\\alpha\\gamma L_{\\gamma,\\tau}\\right)}\\cdot\\frac{\\|x_{0}-x_{\\star}\\|^{2}}{K}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "It is easy to see that the coefficient on the right-hand side is minimized when $\\begin{array}{r}{\\alpha=\\frac{1}{\\gamma L_{\\gamma,\\tau}}}\\end{array}$ \u03b3L1\u03b3,\u03c4 , and the convergence is given by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\gamma M^{\\gamma}\\left(\\bar{x}_{K}\\right)-\\operatorname*{inf}\\gamma M^{\\gamma}\\leq\\left(\\frac{n-\\tau}{\\tau(n-1)}+\\frac{n(\\tau-1)}{\\tau(n-1)}\\cdot\\gamma L_{\\gamma}\\right)\\cdot\\frac{\\|x_{0}-x_{\\star}\\|^{2}}{K}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Notice that $\\begin{array}{r}{L_{\\gamma}\\leq\\frac{1}{\\gamma}}\\end{array}$ . As a result, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\alpha_{\\star}=\\frac{1}{\\gamma L_{\\gamma}}\\ge1.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "G.5 Proof of Theorem 5 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Case of FedExProx-GraDS-PP. We start with the following identity ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left\\|{\\boldsymbol x}_{k+1}-{\\boldsymbol x}_{\\star}\\right\\|^{2}=\\left\\|{\\boldsymbol x}_{k}-{\\boldsymbol x}_{\\star}\\right\\|^{2}-2\\alpha_{\\tau,k,G}\\cdot\\gamma\\left\\langle\\displaystyle\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\nabla{\\boldsymbol M}_{f_{i}}^{\\gamma}\\left({\\boldsymbol x}_{k}\\right),{\\boldsymbol x}_{k}-{\\boldsymbol x}_{\\star}\\right\\rangle}\\\\ {+\\alpha_{\\tau,k,G}^{2}\\cdot\\gamma^{2}\\cdot\\left\\|\\displaystyle\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\nabla{\\boldsymbol M}_{f_{i}}^{\\gamma}\\left({\\boldsymbol x}_{k}\\right)\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For the last term, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\alpha_{\\tau,k,G}^{2}\\cdot\\gamma^{2}\\cdot\\left\\|\\displaystyle\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)\\right\\|^{2}=\\alpha_{\\tau,k,G}\\cdot\\gamma^{2}\\cdot\\displaystyle\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\left\\|\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)\\right\\|^{2}}&{}\\\\ {=\\alpha_{\\tau,k,G}\\cdot\\gamma^{2}\\cdot\\displaystyle\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\left\\|\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)-\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the last step is due to the assumption that we are in the interpolation regime. Using Fact 3, we can further upper bound the above expression, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\alpha_{\\tau,k,G}^{2}\\cdot\\gamma^{2}\\cdot\\left\\|\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)\\right\\|^{2}}}\\\\ &{\\le\\alpha_{\\tau,k,G}\\cdot\\gamma^{2}\\cdot\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\frac{L_{i}}{1+\\gamma L_{i}}\\left(D_{M_{f_{i}}^{\\gamma}}\\left(x_{k},x_{\\star}\\right)+D_{M_{f_{i}}^{\\gamma}}\\left(x_{\\star},x_{k}\\right)\\right)}\\\\ &{\\le\\alpha_{\\tau,k,G}\\cdot\\gamma\\cdot\\frac{\\gamma L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}\\cdot\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\left(D_{M_{f_{i}}^{\\gamma}}\\left(x_{k},x_{\\star}\\right)+D_{M_{f_{i}}^{\\gamma}}\\left(x_{\\star},x_{k}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the last inequality is due to Fact 4. Now we look at the second term in Equation (33). ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle-\\;2\\alpha_{\\tau,k,G}\\cdot\\gamma\\left\\langle\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right),x_{k}-x_{\\star}\\right\\rangle}\\\\ &{\\quad=-2\\alpha_{\\tau,k,G}\\cdot\\gamma\\left\\langle\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\left(\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)-M_{f_{i}}^{\\gamma}\\left(x_{\\star}\\right)\\right),x_{k}-x_{\\star}\\right\\rangle}\\\\ &{\\quad=-2\\alpha_{\\tau,k,G}\\cdot\\gamma\\cdot\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\left(D_{M_{f_{i}}^{\\gamma}}\\left(x_{k},x_{k}\\right)+D_{M_{f_{i}}^{\\gamma}}\\left(x_{\\star},x_{k}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Plugging (34) and (35) into (33), we obtain, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\Vert x_{k+1}-x_{\\star}\\right\\Vert^{2}}\\\\ &{\\quad\\leq\\left\\Vert x_{k}-x_{\\star}\\right\\Vert^{2}-\\alpha_{\\tau,k,G}\\cdot\\gamma\\left(2-\\frac{\\gamma L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}\\right)\\cdot\\frac{1}{\\tau}\\displaystyle\\sum_{i\\in S_{k}}\\left(D_{M_{f_{i}}^{\\gamma}}\\left(x_{k},x_{\\star}\\right)+D_{M_{f_{i}}^{\\gamma}}\\left(x_{\\star},x_{k}\\right)\\right)}\\\\ &{\\quad\\leq\\left\\Vert x_{k}-x_{\\star}\\right\\Vert^{2}-\\alpha_{\\tau,k,G}\\cdot\\gamma\\left(\\frac{2+\\gamma L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}\\right)\\cdot\\frac{1}{\\tau}\\displaystyle\\sum_{i\\in S_{k}}\\left(M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)-M_{f_{i}}^{\\gamma}\\left(x_{\\star}\\right)\\right),\\qquad\\qquad(3-\\gamma)\\left\\Vert\\Omega_{k}^{\\gamma}\\left(x_{k}\\right)\\right\\Vert^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the last inequality is due to ", "page_idx": 32}, {"type": "equation", "text": "$$\nD_{M_{f_{i}}^{\\gamma}}\\left(x_{k},x_{\\star}\\right)\\stackrel{(14)}{=}M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)-M_{f_{i}}^{\\gamma}\\left(x_{\\star}\\right),\\quad\\mathrm{and}\\quad D_{M_{f_{i}}^{\\gamma}}\\left(x_{\\star},x_{k}\\right)\\stackrel{(15)}{\\geq}0.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now we want to lower bound $\\alpha_{\\tau,k,G}$ , notice that it can be viewed as a function of the iterate $x$ and the sampled set $S$ . Therefore, we use the notation ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{inf}\\alpha_{\\tau,k,G}=\\operatorname*{inf}_{\\substack{x\\in\\mathbb{R}^{d},S\\subseteq[n],|S|=\\tau}}\\alpha_{\\tau,k,G}\\left(x,S\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "As a result, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\alpha_{\\tau,k,G}\\geq\\operatorname*{inf}\\alpha_{\\tau,k,G}\\geq1,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the second inequality comes from the convexity of standard Euclidean norm. Plugging this lower bound into (36), we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|x_{k+1}-x_{\\star}\\right\\|^{2}}\\\\ &{\\quad\\leq\\left\\|x_{k}-x_{\\star}\\right\\|^{2}-\\operatorname*{inf}\\alpha_{\\tau,k,G}\\cdot\\gamma\\left(\\displaystyle\\frac{2+\\gamma L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}\\right)\\cdot\\displaystyle\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\left(M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)-M_{f_{i}}^{\\gamma}\\left(x_{\\star}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Taking expectation conditioned on $x_{k}$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{S_{k}}\\left[\\left\\Vert x_{k+1}-x_{\\star}\\right\\Vert^{2}\\right]}\\\\ &{\\quad\\le\\left\\Vert x_{k}-x_{\\star}\\right\\Vert^{2}-\\operatorname*{inf}\\alpha_{\\tau,k,G}\\cdot\\gamma\\left(\\displaystyle\\frac{2+\\gamma L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}\\right)\\cdot\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left(M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)-M_{f_{i}}^{\\gamma}\\left(x_{\\star}\\right)\\right)}\\\\ &{\\quad=\\left\\Vert x_{k}-x_{\\star}\\right\\Vert^{2}-\\operatorname*{inf}\\alpha_{\\tau,k,G}\\cdot\\gamma\\left(\\displaystyle\\frac{2+\\gamma L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}\\right)\\cdot\\left(M^{\\gamma}\\left(x_{k}\\right)-\\operatorname*{inf}M\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the last identity is due to the fact that we are in the interpolation regime. Using Lemma 10, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{S_{k}}\\left[\\left\\Vert x_{k+1}-x_{\\star}\\right\\Vert^{2}\\right]}\\\\ &{\\quad\\le\\left\\Vert x_{k}-x_{\\star}\\right\\Vert^{2}-\\operatorname*{inf}\\alpha_{\\tau,k,G}\\cdot\\gamma\\left(\\displaystyle\\frac{2+\\gamma L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}\\right)\\cdot\\displaystyle\\frac{1}{1+\\gamma L_{\\operatorname*{max}}}\\left(f(x_{k})-\\operatorname*{inf}f\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Taking expectation again and using tower property, we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert x_{k+1}-x_{\\star}\\right\\Vert^{2}\\right]}\\\\ &{\\quad\\leq\\mathbb{E}\\left[\\left\\Vert x_{k}-x_{\\star}\\right\\Vert^{2}\\right]-\\operatorname*{inf}\\alpha_{\\tau,k,G}\\cdot\\gamma\\left(\\frac{2+\\gamma L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}\\right)\\cdot\\frac{1}{1+\\gamma L_{\\operatorname*{max}}}\\mathbb{E}\\left[f(x_{k})-\\operatorname*{inf}f\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Following the same step as Theorem 1, we can unroll the above recurrence and obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\bar{x}_{K})\\right]-\\operatorname*{inf}{f\\le\\left(\\frac{1+\\gamma L_{\\operatorname*{max}}}{2+\\gamma L_{\\operatorname*{max}}}\\right)}\\cdot\\left(\\frac{1}{\\gamma}+L_{\\operatorname*{max}}\\right)\\cdot\\frac{\\|x_{0}-x_{\\star}\\|^{2}}{\\operatorname*{inf}{\\alpha_{\\tau,k,G}}\\cdot K},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $K$ is the total number of iterations, $\\bar{x}_{K}$ is sampled uniformly at random from the first $K$ -iterates $\\{x_{0},x_{1},\\dots,x_{K-1}\\}$ . ", "page_idx": 32}, {"type": "text", "text": "Case of FedExProx-StoPS-PP. We start with the following identity ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|x_{k+1}-x_{\\star}\\right\\|^{2}=\\left\\|x_{k}-x_{\\star}\\right\\|^{2}-2\\alpha_{\\tau,k,S}\\cdot\\gamma\\left\\langle\\displaystyle\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right),x_{k}-x_{\\star}\\right\\rangle}\\\\ {+\\alpha_{\\tau,k,S}^{2}\\cdot\\gamma^{2}\\cdot\\left\\|\\displaystyle\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For the last term of Equation (37), we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\alpha_{\\tau,k,S}^{2}\\cdot\\gamma^{2}\\cdot\\left\\|\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)\\right\\|^{2}=\\alpha_{\\tau,k,S}\\cdot\\gamma\\cdot\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\left(M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)-\\operatorname*{inf}M_{f_{i}}^{\\gamma}\\right)}&{}\\\\ {=\\alpha_{\\tau,k,S}\\cdot\\gamma\\cdot\\frac{1}{\\tau}\\displaystyle\\sum_{i\\in S_{k}}\\left(D_{M_{f_{i}}^{\\gamma}}\\left(x_{k},x_{\\star}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "While for the second term we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\;2\\alpha_{\\tau,k,S}\\cdot\\gamma\\left\\langle\\displaystyle\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right),x_{k}-x_{k}\\right\\rangle}\\\\ &{\\quad=-\\displaystyle2\\alpha_{\\tau,k,S}\\cdot\\gamma\\cdot\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\left(D_{M_{f_{i}}^{\\gamma}}\\left(x_{k},x_{\\star}\\right)+D_{M_{f_{i}}^{\\gamma}}\\left(x_{\\star},x_{k}\\right)\\right)}\\\\ &{\\quad\\overset{(15)}{\\leq}-2\\alpha_{\\tau,k,S}\\cdot\\gamma\\cdot\\displaystyle\\frac{1}{\\tau}\\sum_{i\\in S_{k}}D_{M_{f_{i}}^{\\gamma}}\\left(x_{k},x_{k}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Plugging (38) and (39) into (37), we obtain ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left\\Vert x_{k+1}-x_{\\star}\\right\\Vert^{2}\\leq\\left\\Vert x_{k}-x_{\\star}\\right\\Vert^{2}-\\alpha_{\\tau,k,S}\\cdot\\gamma\\cdot\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\left(M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)-\\operatorname*{inf}M_{f_{i}}^{\\gamma}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Now we want to lower bound $\\alpha_{\\tau,k,S}$ , notice that it can be viewed as a function of the iterate $x$ and the sampled set $S$ . Therefore, we use the notation ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{inf}\\alpha_{\\tau,k,S}=\\operatorname*{inf}_{\\substack{x\\in\\mathbb{R}^{d},S\\subseteq[n],|S|=\\tau}}\\alpha_{\\tau,k,S}\\left(x,S\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "As a result, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\alpha_{\\tau,k,S}\\geq\\operatorname*{inf}\\alpha_{\\tau,k,S}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Notice that since each M f\u03b3i is1+L\u03b3iLi - smooth, we conclude that the function $\\textstyle{\\frac{1}{\\tau}}\\sum_{i\\in S_{k}}M_{f_{i}}^{\\gamma}$ is at least $\\frac{L_{\\mathrm{max}}}{1\\!+\\!\\gamma L_{\\mathrm{max}}}$ -smooth5. Using the smoothness of the mentioned function and Fact 3, a lower bound on inf $\\alpha_{\\tau,k,S}$ is obvious, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{inf}\\alpha_{k,\\tau,S}\\geq\\frac{1}{2\\cdot\\frac{L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}\\gamma}=\\frac{1}{2}\\left(1+\\frac{1}{\\gamma L_{\\operatorname*{max}}}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This means that we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\alpha_{\\tau,k,S}\\geq\\operatorname*{inf}\\alpha_{\\tau,k,S}\\geq\\frac{1}{2}\\left(1+\\frac{1}{\\gamma L_{\\operatorname*{max}}}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Using the above lower bound in (40), we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left\\Vert x_{k+1}-x_{\\star}\\right\\Vert^{2}\\leq\\left\\Vert x_{k}-x_{\\star}\\right\\Vert^{2}-\\operatorname*{inf}\\alpha_{\\tau,k,S}\\cdot\\gamma\\cdot\\frac{1}{\\tau}\\sum_{i\\in S_{k}}\\left(M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)-\\operatorname*{inf}M_{f_{i}}^{\\gamma}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Taking expectation conditioned on $x_{k}$ , and noticing that we are in the interpolation regime, we obtain ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{S_{k}}\\left[\\left\\|x_{k+1}-x_{\\star}\\right\\|^{2}\\right]\\leq\\left\\|x_{k}-x_{\\star}\\right\\|^{2}-\\operatorname*{inf}\\alpha_{\\tau,k,S}\\cdot\\gamma\\cdot\\left(M^{\\gamma}\\left(x_{k}\\right)-\\operatorname*{inf}M\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Using Lemma 10, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S_{k}}\\left[\\left\\Vert x_{k+1}-x_{\\star}\\right\\Vert^{2}\\right]\\overset{\\mathrm{Lemma~l0}}{\\leq}\\left\\Vert x_{k}-x_{\\star}\\right\\Vert^{2}-\\operatorname*{inf}\\alpha_{\\tau,k,S}\\cdot\\frac{\\gamma}{1+\\gamma L_{\\operatorname*{max}}}\\cdot\\left(f(x_{k})-\\operatorname*{inf}f\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Now, following the exact same steps as in the previous case of FedExProx-GraDS, we result in ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\bar{x}_{K})\\right]-\\operatorname*{inf}{f}\\leq\\left(\\frac{1}{\\gamma}+L_{\\operatorname*{max}}\\right)\\cdot\\frac{\\|x_{0}-x_{\\star}\\|^{2}}{\\operatorname*{inf}{\\alpha_{\\tau,k,S}\\cdot K}},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $K$ is the total number of iterations, $\\bar{x}_{K}$ is sampled uniformly at random from the first $K$ -iterates $\\{x_{0},x_{1},\\dots,x_{K-1}\\}$ . ", "page_idx": 34}, {"type": "text", "text": "G.6 Proof of Corollary 1 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "If additionally we assume $f$ is $\\mu$ -strongly convex, then from Lemma 11, we know it indicates the following star strong convexity of $M^{\\gamma}$ holds, ", "page_idx": 34}, {"type": "equation", "text": "$$\nM^{\\gamma}\\left(x\\right)-M^{\\gamma}\\left(x_{\\star}\\right)\\geq\\frac{\\mu}{1+\\gamma L_{\\operatorname*{max}}}\\cdot\\frac{1}{2}\\left\\Vert x-x_{\\star}\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus, we apply Theorem 3 with $\\tau$ -nice sampling in the star strong convexity case, and obtain the following result: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert x_{K}-x_{\\star}\\right\\Vert^{2}\\right]\\overset{\\mathrm{Theorem~3}}{\\leq}\\left(1-\\alpha\\gamma(2-\\alpha\\gamma L_{\\gamma,\\tau})\\cdot\\frac{\\mu}{2\\left(1+\\gamma L_{\\mathrm{max}}\\right)}\\right)^{K}\\left\\Vert x_{0}-x_{\\star}\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Since the convergence here is stated in terms of squared distance to the minimizer, we do not need further transformation. Notice that the convergence rate in this case, ", "page_idx": 34}, {"type": "equation", "text": "$$\n1-\\alpha\\gamma(2-\\alpha\\gamma L_{\\gamma,\\tau})\\cdot\\frac{\\mu}{2\\left(1+\\gamma L_{\\operatorname*{max}}\\right)},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "is also minimized when $\\begin{array}{r}{\\alpha=\\alpha_{\\gamma,\\tau}=\\frac{1}{\\gamma L_{\\gamma,\\tau}}}\\end{array}$ . In case of $\\alpha=\\alpha_{\\gamma,\\tau}$ , the convergence is given by ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert x_{K}-x_{\\star}\\right\\Vert^{2}\\right]\\leq\\left(1-\\frac{\\mu}{2L_{\\gamma,\\tau}\\left(1+\\gamma L_{\\operatorname*{max}}\\right)}\\right)^{K}\\left\\Vert x_{0}-x_{\\star}\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 34}, {"type": "text", "text": "G.7 Proof of Corollary 2 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Similar to the proof of Theorem 2, we start with the following identity ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|x_{k+1}-x_{\\star}\\right\\|^{2}=\\left\\|x_{k}-\\alpha_{k,G}^{\\prime}\\gamma\\nabla M^{\\gamma}\\left(x_{k}\\right)-x_{\\star}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\left\\|x_{k}-x_{\\star}\\right\\|^{2}-\\alpha_{k,G}^{\\prime}\\gamma\\left\\langle\\nabla M^{\\gamma}\\left(x_{k}\\right),x_{k}-x_{\\star}\\right\\rangle+\\left(\\alpha_{k,G}^{\\prime}\\right)^{2}\\gamma^{2}\\left\\|\\nabla M^{\\gamma}\\left(x\\right)\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The extrapolation parameter can be rewritten as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\alpha_{k,G}^{\\prime}=\\frac{1+\\gamma L_{\\operatorname*{max}}}{\\gamma L_{\\operatorname*{max}}}\\cdot\\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\left\\Vert\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)\\right\\Vert^{2}}{\\left\\Vert\\nabla M^{\\gamma}\\left(x_{k}\\right)\\right\\Vert^{2}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We have for the last term of (41), ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\alpha_{k,G}^{\\prime}\\right)^{2}\\gamma^{2}\\left\\Vert\\nabla M^{\\gamma}\\left(x_{k}\\right)\\right\\Vert^{2}}\\\\ &{\\quad=\\alpha_{k,G}^{\\prime}\\gamma\\cdot\\left(\\gamma+\\displaystyle\\frac{1}{L_{\\operatorname*{max}}}\\right)\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left\\Vert\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)\\right\\Vert^{2}}\\\\ &{\\quad=\\alpha_{k,G}^{\\prime}\\gamma\\cdot\\left(\\gamma+\\displaystyle\\frac{1}{L_{\\operatorname*{max}}}\\right)\\cdot\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left\\Vert\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)-\\nabla M\\gamma f_{i}x_{\\star}\\right\\Vert^{2}}\\\\ &{\\quad\\le\\alpha_{k,G}^{\\prime}\\gamma\\cdot\\left(\\gamma+\\displaystyle\\frac{1}{L_{\\operatorname*{max}}}\\right)\\cdot\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\displaystyle\\frac{L_{i}}{1+\\gamma L_{i}}\\left(D_{M_{f_{i}}^{\\gamma}}\\left(x_{k},x_{k}\\right)+D_{M_{f_{i}}^{\\gamma}}\\left(x_{k},x_{k}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the last inequality follows from the1+L\u03b3iLi - smoothness of $M_{f_{i}}^{\\gamma}$ . Utilizing the monotonicity of $\\textstyle{\\frac{x}{1+\\gamma x}}$ , for $x>0$ , we further obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\alpha_{k,G}^{\\prime}\\right)^{2}\\gamma^{2}\\left\\Vert\\nabla M^{\\gamma}\\left(x_{k}\\right)\\right\\Vert^{2}}\\\\ &{\\overset{\\mathrm{Fact4}}{\\leq}\\alpha_{k,G}^{\\prime}\\gamma\\cdot\\left(\\gamma+\\displaystyle\\frac{1}{L_{\\operatorname*{max}}}\\right)\\cdot\\displaystyle\\frac{L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}\\cdot\\frac{1}{n}\\sum_{i=1}^{n}\\left(D_{M_{f_{i}}^{\\gamma}}\\left(x_{k},x_{\\star}\\right)+D_{M_{f_{i}}^{\\gamma}}\\left(x_{\\star},x_{k}\\right)\\right)}\\\\ &{=\\alpha_{k,G}^{\\prime}\\gamma\\cdot\\left(\\gamma+\\displaystyle\\frac{1}{L_{\\operatorname*{max}}}\\right)\\cdot\\displaystyle\\frac{L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}\\cdot\\left(D_{M^{\\gamma}}\\left(x_{k},x_{\\star}\\right)+D_{M^{\\gamma}}\\left(x_{\\star},x_{k}\\right)\\right)}\\\\ &{=\\alpha_{k,G}^{\\prime}\\gamma\\left(D_{M^{\\gamma}}\\left(x_{k},x_{\\star}\\right)+D_{M^{\\gamma}}\\left(x_{\\star},x_{k}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "For the second term of (41), we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-2\\alpha_{k,G}^{\\prime}\\gamma\\left\\langle\\nabla M^{\\gamma}\\left(x_{k}\\right),x_{k}-x_{\\star}\\right\\rangle=2\\alpha_{k,G}^{\\prime}\\gamma\\left\\langle\\nabla M^{\\gamma}\\left(x_{k}\\right),x_{\\star}-x_{k}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=2\\alpha_{k,G}^{\\prime}\\gamma\\left\\langle\\nabla M^{\\gamma}\\left(x_{k}\\right)-\\nabla M^{\\gamma}\\left(x_{\\star}\\right),x_{\\star}-x_{k}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=-2\\alpha_{k,G}^{\\prime}\\gamma\\left(D_{M^{\\gamma}}\\left(x_{k},x_{\\star}\\right)+D_{M^{\\gamma}}\\left(x_{\\star},x_{k}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Plugging (43) and (42) into (41), we obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|{x}_{k+1}-{x}_{\\star}\\right\\|^{2}\\leq\\left\\|{x}_{k}-{x}_{\\star}\\right\\|^{2}-\\alpha_{k,G}^{\\prime}\\gamma\\left(D_{M^{\\gamma}}\\left({x}_{k},{x}_{\\star}\\right)+D_{M^{\\gamma}}\\left({x}_{\\star},{x}_{k}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Notice that we know that ", "page_idx": 35}, {"type": "equation", "text": "$$\nD_{M^{\\gamma}}\\left(x_{k},x_{\\star}\\right)\\stackrel{(14)}{=}M^{\\gamma}\\left(x_{k}\\right)-M^{\\gamma}\\left(x_{\\star}\\right),\\quad D_{M^{\\gamma}}\\left(x_{\\star},x_{k}\\right)\\stackrel{(15)}{\\geq}0.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "As a result, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|x_{k+1}-x_{\\star}\\right\\|^{2}\\leq\\left\\|x_{k}-x_{\\star}\\right\\|^{2}-\\alpha_{k,G}^{\\prime}\\gamma\\left(M^{\\gamma}\\left(x_{k}\\right)-M^{\\gamma}\\left(x_{\\star}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Summing up the above recursion for $k=0,1,...,K-1$ , we notice that many of them telescope, we obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\gamma\\sum_{k=0}^{K-1}\\alpha_{k,G}^{\\prime}\\left(M^{\\gamma}\\left(x_{k}\\right)-\\operatorname*{inf}M^{\\gamma}\\right)\\leq\\left\\|x_{0}-x_{\\star}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Denote $\\begin{array}{r l r}{p_{k}\\!\\!}&{{}=}&{\\!\\!\\alpha_{k,G}^{\\prime}\\Big/\\!\\sum_{k=0}^{K-1}\\alpha_{k,G}^{\\prime}}\\end{array}$ for $k\\;=\\;0,1,...,K\\;-\\;1$ . If we sample ${\\bar{x}}_{K}$ randomly according to probabilities $p_{k}$ from the first $K$ iterates $\\{x_{0},x_{1},\\dots,x_{K-1}\\}$ , we can further write the above recursion as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[M^{\\gamma}\\left(\\bar{x}_{K}\\right)\\right]-\\operatorname*{inf}_{\\gamma}M^{\\gamma}\\leq\\frac{1}{\\gamma}\\cdot\\frac{\\left\\Vert x_{0}-x_{\\star}\\right\\Vert^{2}}{\\sum_{k=0}^{K-1}\\alpha_{k,G}^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Utilizing the local bound in Lemma 10, we further obtain, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\bar{x}^{K})\\right]-f^{\\mathrm{inf}}\\leq\\left(\\frac{1}{\\gamma}+L_{\\mathrm{max}}\\right)\\cdot\\frac{\\left\\|x_{0}-x_{\\star}\\right\\|^{2}}{\\sum_{k=0}^{K-1}\\alpha_{k,G}^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 35}, {"type": "text", "text": "H Missing proofs of lemmas ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "H.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Notice that since $f$ is proper, closed and convex, by Fact 1, $\\mathrm{prox}_{\\gamma f}\\left(x\\right)$ is a singleton. We use the notation $z(x)=\\mathrm{prox}_{\\gamma f}\\left(x\\right)$ . Using the definition of $\\mathrm{prox}_{\\gamma f}\\left(x\\right)$ , we see that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle M_{f}^{\\gamma}\\left(\\boldsymbol{x}\\right)=f(\\boldsymbol{z}(\\boldsymbol{x}))+\\frac{1}{2\\gamma}\\left\\Vert\\boldsymbol{z}(\\boldsymbol{x})-\\boldsymbol{x}\\right\\Vert^{2}}}\\\\ {{\\displaystyle=f\\left(\\mathrm{prox}_{\\gamma f}\\left(\\boldsymbol{x}\\right)\\right)+\\frac{1}{2\\gamma}\\left\\Vert\\mathrm{prox}_{\\gamma f}\\left(\\boldsymbol{x}\\right)-\\boldsymbol{x}\\right\\Vert^{2}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Now, assume $M_{f}^{\\gamma}\\left(x\\right)=+\\infty.$ . We have for any $z\\in\\mathbb{R}^{d}$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n+\\infty=M_{f}^{\\gamma}\\left(x\\right)=f\\left(z(x)\\right)+\\frac{1}{2\\gamma}\\left\\Vert z(x)-x\\right\\Vert^{2}\\leq f(z)+\\frac{1}{2\\gamma}\\left\\Vert z-x\\right\\Vert^{2},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which means that $z$ is also optimal, which contradicts the uniqueness $z(x)\\,=\\,\\mathrm{prox}_{\\gamma f}\\left(x\\right)$ . This indicates that $M_{f}^{\\gamma}\\left(x\\right)<+\\infty$ , thus, it is real-valued, which concludes the proof. ", "page_idx": 35}, {"type": "text", "text": "H.2 Proof of Lemma 2 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Let $f^{\\star}$ be the convex conjugate of $f$ , using Corollary 6.56 in the book by Beck [2017], we have $\\begin{array}{r}{\\left(M_{f}^{\\gamma}\\right)^{\\star}=f^{\\star}+\\frac{\\gamma}{2}\\left\\Vert\\cdot\\right\\Vert^{2}}\\end{array}$ . We know that the convex conjugate of a proper, closed and convex function is also proper closed and convex. As a result, $f^{\\star}+\\textstyle{\\frac{\\gamma}{2}}\\left\\|\\cdot\\right\\|^{2}$ is $\\gamma$ -strongly convex. This indicates that $\\left(M_{f}^{\\gamma}\\right)^{\\star}$ is $\\gamma$ -strongly convex, which implies $M_{f}^{\\gamma}$ is $\\frac{1}{\\gamma}$ -smooth. Notice that we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathrm{prox}_{\\gamma f}\\left(x\\right)=\\arg\\operatorname*{min}_{z\\in\\mathbb{R}^{d}}\\left\\{f(z)+\\frac{1}{2\\gamma}\\left\\Vert z-x\\right\\Vert^{2}\\right\\},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "by the definition of proximity operator. Using Theorem 5.30 from Beck [2017], we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\nabla M_{f}^{\\gamma}\\left(x\\right)=\\frac{1}{\\gamma}\\left(x-\\mathrm{prox}_{\\gamma f}\\left(x\\right)\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This completes the proof. ", "page_idx": 36}, {"type": "text", "text": "H.3 Proof of Lemma 3 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "To prove this lemma, we use Theorem 2.19 in the book by Beck [2017]. From the key observation that $M_{f}^{\\bar{\\gamma}}$ is the infimal convolution of the proper, convex function $f$ and the real-valued convex function $\\textstyle{\\frac{1}{2\\gamma}}\\left\\|\\cdot\\right\\|^{2}$ , we deduce that $M_{f}^{\\gamma}$ is convex. This completes the proof. ", "page_idx": 36}, {"type": "text", "text": "H.4 Proof of Lemma 4 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Let $f^{\\star}$ be the convex conjugate of $f$ . From Corollary 6.56 in the book by Beck [2017], it holds that $\\begin{array}{r}{\\left(M_{f}^{\\gamma}\\right)^{\\star}=f^{\\star}+\\frac{\\gamma}{2}\\left\\Vert\\cdot\\right\\Vert^{2}}\\end{array}$ . Since $f$ is $L$ -smooth, we deduce that $f^{\\star}$ is $\\scriptstyle{\\frac{1}{L}}$ -strongly convex, and thus $\\left(M_{f}^{\\gamma}\\right)^{\\star}$ is $\\begin{array}{r}{\\frac{1}{L}+\\gamma}\\end{array}$ -strongly convex. This suggests that $\\left(M_{f}^{\\gamma}\\right)^{\\star}$ is 1+L\u03b3L-strongly convex, which in turn implies M f\u03b3 is1+L\u03b3L- smooth. This completes the proof. ", "page_idx": 36}, {"type": "text", "text": "H.5 Proof of Lemma 5 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Notice that since $M_{f}^{\\gamma}$ is convex and differentiable, the condition $\\nabla M_{f}^{\\gamma}\\left(x\\right)\\,=\\,0$ gives its set of minimizers. This optimality condition can be written exactly as $x\\,=\\,\\operatorname{prox}_{\\gamma f}\\left(x\\right)$ according to Lemma 2. Using Lemma 12, we know this condition also gives the set of minimizers of $f$ , which suggests that $f$ and $M_{f}^{\\gamma}$ have the same set of minimizers. Pick any $x_{\\star}\\in\\mathbb{R}^{d}$ that is a minimizer of $f$ , using Lemma 1, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\operatorname*{inf}\\boldsymbol{M}_{f}^{\\gamma}=\\boldsymbol{M}_{f}^{\\gamma}\\left(\\boldsymbol{x}_{\\star}\\right)}}\\\\ {{\\phantom{\\operatorname*{inf}\\boldsymbol{M}_{f}^{\\gamma}}=f\\left(\\operatorname{prox}_{\\gamma f}\\left(\\boldsymbol{x}_{\\star}\\right)\\right)+\\displaystyle{\\frac{1}{2\\gamma}}\\left\\Vert\\boldsymbol{x}_{\\star}-\\operatorname{prox}_{\\gamma f}\\left(\\boldsymbol{x}_{\\star}\\right)\\right\\Vert^{2}}}\\\\ {{\\phantom{\\operatorname*{inf}\\boldsymbol{M}_{f}^{\\gamma}}=f(\\boldsymbol{x}_{\\star})=\\operatorname*{inf}f.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This completes the proof. ", "page_idx": 36}, {"type": "text", "text": "H.6 Proof of Lemma 6 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "For any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\displaystyle M_{f}^{\\gamma}\\left({\\boldsymbol{x}}\\right)=\\operatorname*{min}_{z\\in\\mathbb{R}^{d}}\\left\\{{\\boldsymbol{f}}(z)+\\displaystyle\\frac{1}{2\\gamma}\\left\\Vert z-x\\right\\Vert^{2}\\right\\}}}}\\\\ {{\\qquad\\qquad\\leq{\\boldsymbol{f}}({\\boldsymbol{x}})+\\displaystyle\\frac{1}{2\\gamma}\\left\\Vert x-x\\right\\Vert^{2}}}\\\\ {{\\qquad={\\boldsymbol{f}}({\\boldsymbol{x}})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This completes the proof. ", "page_idx": 36}, {"type": "text", "text": "H.7 Proof of Lemma 7 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "From Lemma 3 and Lemma 4, we immediately obtain that each $M_{f_{i}}^{\\gamma}$ is convex and $\\begin{array}{r}{\\frac{L_{i}}{1+\\gamma L_{i}}}\\end{array}$ -smooth. This immediately suggests that $\\begin{array}{r}{M=\\frac{1}{n}\\sum_{i=1}^{n}M_{f_{i}}^{\\gamma}}\\end{array}$ is convex and $L_{\\gamma}$ -smooth with ", "page_idx": 37}, {"type": "equation", "text": "$$\nL_{\\gamma}\\leq\\frac{1}{n}\\sum_{i=1}^{n}\\frac{L_{i}}{1+\\gamma L_{i}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Then by Lemma 13, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n{\\frac{1}{n^{2}}}\\sum_{i=1}^{n}{\\frac{L_{i}}{1+\\gamma L_{i}}}\\sum_{i}^{\\mathrm{Lemma~l3}}L_{\\gamma}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Combing the above two inequalities, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n{\\frac{1}{n^{2}}}\\sum_{i=1}^{n}{\\frac{L_{i}}{1+\\gamma L_{i}}}\\leq L_{\\gamma}\\leq{\\frac{1}{n}}\\sum_{i=1}^{n}{\\frac{L_{i}}{1+\\gamma L_{i}}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We then look at the condition number defined in Theorem 1. It is easy to verify that ", "page_idx": 37}, {"type": "equation", "text": "$$\nC\\left(\\gamma,n,\\alpha_{\\gamma,n}\\right)=L_{\\gamma}\\left(1+\\gamma L_{\\operatorname*{max}}\\right)\\quad\\mathrm{~and,~}\\;\\;C\\left(\\gamma,1,\\alpha_{\\gamma,1}\\right)=L_{\\operatorname*{max}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "As a result, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C\\left(\\gamma,n,\\alpha_{\\gamma,n}\\right)=L_{\\gamma}\\left(1+\\gamma L_{\\operatorname*{max}}\\right)}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}L_{i}\\cdot\\frac{1+\\gamma L_{\\operatorname*{max}}}{1+\\gamma L_{i}}}\\\\ &{\\qquad\\qquad\\leq L_{\\operatorname*{max}}=C\\left(\\gamma,n,1\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Notice that we can write $C\\left(\\gamma,\\tau,\\alpha_{\\gamma,\\tau}\\right)$ as an interpolation between $C\\left(\\gamma,n,\\alpha_{\\gamma,n}\\right)$ and $C\\left(\\gamma,1,\\alpha_{\\gamma,1}\\right)$ , therefore ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{\\gamma}\\left(1+\\gamma L_{\\operatorname*{max}}\\right)\\leq C\\left(\\gamma,n,\\alpha_{\\gamma,n}\\right)\\leq C\\left(\\gamma,\\tau,\\alpha_{\\gamma,\\tau}\\right)\\leq C\\left(\\gamma,1,\\alpha_{\\gamma,1}\\right)=L_{\\operatorname*{max}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In cases where there exists at least one $L_{i}<L_{\\mathrm{max}}$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac1n\\sum_{i=1}^{n}L_{i}\\cdot\\frac{1+\\gamma L_{\\operatorname*{max}}}{1+\\gamma L_{i}}<L_{\\operatorname*{max}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "which is true for all $0<\\gamma<+\\infty$ . Thus, $C\\left(\\gamma,n,\\alpha_{\\gamma,n}\\right)<C\\left(\\gamma,\\tau,\\alpha_{\\gamma,\\tau}\\right)<L_{\\operatorname*{max}}=C\\left(\\gamma,1,\\alpha_{\\gamma,1}\\right)\\!.$ Now we give an example that when all $L_{i}=L_{\\mathrm{max}}$ , still $\\begin{array}{r}{C\\left(\\gamma,n,\\alpha_{\\gamma,n}\\right)=\\frac{1}{n}C\\left(\\gamma,1,\\alpha_{\\gamma,1}\\right)=\\frac{1}{n}L_{\\operatorname*{max}}.}\\end{array}$ . ", "page_idx": 37}, {"type": "text", "text": "Example 1. Consider the setting where $f_{i}:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ is defined as $\\textstyle f_{i}(x)={\\frac{\\theta}{2}}x_{i}^{2}$ for some $\\theta>0$ . Here $x_{i}$ denotes the $i$ -th coordinate of the vector $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , $f:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ is given by $\\begin{array}{r}{f(x)=\\frac{\\theta}{2n}\\left\\|x\\right\\|^{2}}\\end{array}$ . $I t$ is easy to show that for each $f_{i}$ is a convex, $\\theta$ -smooth function and the smoothness constant $\\theta$ cannot be improved since ", "page_idx": 37}, {"type": "equation", "text": "$$\n{\\frac{\\theta}{2}}\\left\\|x\\right\\|^{2}-{\\frac{\\theta}{2}}x_{i}^{2}={\\frac{\\theta}{2}}\\sum_{j\\neq i}x_{j}^{2}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "For $\\begin{array}{r}{f(x)=\\frac{\\theta}{2n}\\left\\|x\\right\\|^{2}}\\end{array}$ , apparently, it is $\\frac{\\theta}{n}$ -smooth and convex. We have the following formula for the Moreau envelope of $f_{i}(x)$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\nM_{f_{i}}^{\\gamma}\\left(x\\right)=\\frac{1}{2}\\cdot\\frac{\\theta}{1+\\gamma\\theta}\\cdot x_{i}^{2}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "As expected, each one of them is convex an d1+\u03b8\u03b3\u03b8-smooth. For M \u03b3 (x), it is given by ", "page_idx": 37}, {"type": "equation", "text": "$$\nM^{\\gamma}\\left(x\\right)=\\frac{1}{n}\\sum_{i=1}^{n}M_{f_{i}}^{\\gamma}\\left(x\\right)=\\frac{1}{2}\\cdot\\frac{\\theta}{n(1+\\gamma\\theta)}\\cdot\\left\\Vert x\\right\\Vert^{2},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "thus, we know it is convex and $\\begin{array}{r}{L_{\\gamma}=\\frac{\\theta}{n(1+\\gamma\\theta)}}\\end{array}$ -smooth. In this case ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{L_{\\operatorname*{max}}}{L_{\\gamma}\\left(1+\\gamma L_{\\operatorname*{max}}\\right)}=\\frac{\\theta}{\\frac{\\theta}{n(1+\\gamma\\theta)}\\cdot(1+\\gamma\\theta)}=n,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "which is ", "page_idx": 37}, {"type": "equation", "text": "$$\nL_{\\gamma}\\left(1+\\gamma L_{\\operatorname*{max}}\\right)=C\\left(\\gamma,n,\\alpha_{\\gamma,n}\\right)=\\frac{1}{n}C\\left(\\gamma,1,\\alpha_{\\gamma,1}\\right)=\\frac{1}{n}L_{\\operatorname*{max}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "H.8 Proof of Lemma 8 ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "By Lemma 5, we know that $f_{i}$ and $M_{f_{i}}^{\\gamma}$ have the same set of minimizers and minimum. Denote the set of minimizers as $\\mathcal{X}_{i}$ , since we are in the interpolation regime, we know that the set of minimizers of $f$ is given by, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathscr{X}=\\bigcap_{i=1}^{n}\\mathscr{X}_{i}\\neq\\emptyset.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Now we prove that every $x$ in $\\mathcal{X}$ is a minimizer of $\\begin{array}{r}{M=\\frac{1}{n}\\sum_{i=1}^{n}M_{f_{i}}^{\\gamma}}\\end{array}$ . This is true since $x\\in\\mathscr{X}$ minimizes each $f_{i}$ , thus $M_{f_{i}}^{\\gamma}$ at the same time. The minimum is given by ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{inf}M={\\frac{1}{n}}\\sum_{i=1}^{n}\\operatorname*{inf}M_{f_{i}}^{\\gamma}={\\frac{1}{n}}\\sum_{i=1}^{n}\\operatorname*{inf}f_{i}=\\operatorname*{inf}f.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We then prove that every $x\\notin\\mathcal{X}$ is not a minimizer of $f$ . If $x\\notin\\mathcal{X}$ , there exists at least one set $\\chi_{j}$ such that $x\\notin\\mathcal{X}_{j}$ . Thus $M_{f_{j}}^{\\dot{\\gamma}}\\left(x\\right)>\\operatorname{inf}M_{f_{j}}^{\\gamma}$ . This indicates that $M^{\\gamma}\\left(x\\right)>\\operatorname*{inf}M$ , which means, $x\\notin\\mathcal{X}$ is not a minimizer of $M$ . ", "page_idx": 38}, {"type": "text", "text": "H.9 Proof of Lemma 9 ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "From Lemma 6, it is clear that $M_{f}^{\\gamma}$ is a global lower bound of $f$ satisfying $M_{f}^{\\gamma}\\left(x\\right)\\leq f(x)$ for any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ and $\\gamma>0$ . Notice that the definition of $M^{\\gamma}$ indicates that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\displaystyle M^{\\gamma}\\left(x\\right)=\\frac{1}{n}\\sum_{i=1}^{n}M_{f_{i}}^{\\gamma}\\left(x\\right)}}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\qquad\\sin\\frac{1}{n}\\sum_{i=1}^{n}\\operatorname*{min}_{\\bar{z}\\in\\mathbb{R}^{n}}\\left\\{f_{i}(z_{i})+\\frac{1}{2\\gamma}\\left\\Vert z_{i}-x\\right\\Vert^{2}\\right\\}}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\end{\\le\\operatorname*{min}_{z\\in\\mathbb{R}^{d}}\\left\\{\\frac{1}{n}\\sum_{i=1}^{n}\\left(f_{i}(z)+\\frac{1}{2\\gamma}\\left\\Vert z-x\\right\\Vert^{2}\\right)\\right\\}}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\end{\\qquad=\\operatorname*{min}_{z\\in\\mathbb{R}^{d}}\\left\\{\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(z)+\\frac{1}{2\\gamma}\\left\\Vert z-x\\right\\Vert^{2}\\right\\}}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "holds for any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ and $\\gamma>0$ . Combining the above result, we have $M^{\\gamma}\\left(x\\right)\\leq M_{f}^{\\gamma}\\left(x\\right)\\leq f(x)$ for any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ and $\\gamma>0$ . Notice that in Lemma 8, we have already shown that $M^{\\gamma}$ and $f$ have the same set of minimizers and minimum in the interpolation regime. A direct application of Lemma 5 indicates that $M_{f}^{\\gamma}$ and $f$ have the same set of minimizers and minimum. Therefore, combining the above statement, we know that $M^{\\gamma},M_{f}^{\\gamma}$ and $f$ have the same set of minimizers and minimum. Thus, for any $x_{\\star}$ belongs to the set of minimizers, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\nM^{\\gamma}\\left(x_{\\star}\\right)=M_{f}^{\\gamma}\\left(x_{\\star}\\right)=f(x_{\\star}).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "This completes the proof. ", "page_idx": 38}, {"type": "text", "text": "H.10 Proof of Lemma 10 ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We start from noticing that according to Lemma 1, the following identity is true for Moreau envelope, ", "page_idx": 38}, {"type": "equation", "text": "$$\nM_{f_{i}}^{\\gamma}\\left(x\\right)\\,=\\,f_{i}(\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right))+\\frac{1}{2\\gamma}\\left\\Vert x-\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right)\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For the second squared norm term, we have the following inequality due to the smoothness of each $f_{i}$ and the fact that $\\begin{array}{r}{\\overleftarrow{\\nabla}f_{i}\\left(\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right)\\right)=\\frac{1}{\\gamma}\\left(x-\\mathrm{prox}_{\\gamma f_{i}}\\left(\\Breve{x}\\right)\\right)}\\end{array}$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{act\\,mat\\,\\,v}J_{i}\\,\\{\\mathrm{prox}_{\\gamma f_{i}}\\,\\{x\\}\\}=\\frac{\\varepsilon}{\\gamma}\\,\\{x-\\mathrm{prox}_{\\gamma f_{i}}\\,\\{x\\}\\},}\\\\ &{\\left\\|x-\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right)\\right\\|^{2}=\\left\\langle x-\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right),x-\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right)\\right\\rangle}\\\\ &{\\phantom{\\left\\|x-\\mathrm{prox}_{\\gamma f_{i}}\\,\\right\\|^{2}}=\\gamma\\left\\langle\\nabla f_{i}\\big(\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right)\\big),x-\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right)\\right\\rangle}\\\\ &{\\phantom{\\left\\|x-\\mathrm{prox}_{\\gamma f_{i}}\\,\\right\\|^{2}}\\geq\\gamma\\left(f_{i}(x)-f_{i}\\left(\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right)\\right)\\right)-\\frac{\\gamma L_{i}}{2}\\left\\lVert x-\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right)\\right\\rVert^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which leads to the following lower bound: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left\\Vert x-\\operatorname{prox}_{\\gamma f_{i}}\\left(x\\right)\\right\\Vert^{2}\\geq\\frac{1}{\\frac{1}{\\gamma}+\\frac{L_{i}}{2}}\\left(f_{i}(x)-f_{i}\\left(\\operatorname{prox}_{\\gamma f_{i}}\\left(x\\right)\\right)\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Plug in the above inequality into (44) and notice that inf $\\textstyle M={\\frac{1}{n}}\\sum_{i=1}^{n}$ inf $\\begin{array}{r}{M_{f_{i}}^{\\gamma}=\\frac{1}{n}\\sum_{i=1}^{n}\\operatorname*{inf}f_{i}}\\end{array}$ , we obtain the following lower bound on $M_{f_{i}}^{\\gamma}\\left(x\\right)$ , ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M_{f_{i}}^{\\gamma}\\left(x\\right)-\\operatorname*{inf}{M_{f_{i}}^{\\gamma}}\\geq f_{i}\\left(\\operatorname{prox}_{\\gamma f_{i}}\\left(x\\right)\\right)+\\displaystyle\\frac{1}{2+\\gamma L_{i}}\\left(f_{i}(x)-f_{i}\\left(\\operatorname{prox}_{\\gamma f_{i}}\\left(x\\right)\\right)\\right)-\\operatorname*{inf}{f_{i}}}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{2+\\gamma L_{i}}\\left(f_{i}(x)-\\operatorname*{inf}{f_{i}}\\right)+\\left(1-\\frac{1}{2+\\gamma L_{i}}\\right)\\left(f_{i}(\\operatorname{prox}_{\\gamma f_{i}}\\left(x\\right))-\\operatorname*{inf}{f_{i}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Now let us look at the term $f_{i}\\left(\\operatorname{prox}_{\\gamma f_{i}}\\left(x\\right)\\right)-\\operatorname{inf}f$ . Using again $L_{i}$ -smoothness of $f_{i}$ , we have ", "page_idx": 39}, {"type": "text", "text": "$f_{i}(x)-f_{i}(\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right))-\\left\\langle\\nabla f_{i}(\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right)),x-\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right)\\right\\rangle\\leq\\frac{L_{i}}{2}\\left\\Vert x-\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right)\\right\\Vert^{2}.$ Notice that $x-\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right)=\\gamma\\nabla f_{i}(\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right))$ . As a result, we have, ", "page_idx": 39}, {"type": "equation", "text": "$$\nf_{i}(x)-\\gamma\\left\\Vert\\nabla f_{i}(\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right))\\right\\Vert^{2}-\\frac{L_{i}\\gamma^{2}}{2}\\left\\Vert\\nabla f_{i}(\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right))\\right\\Vert^{2}\\leq f_{i}(\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right)),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which is ", "page_idx": 39}, {"type": "equation", "text": "$$\nf_{i}(x)-\\operatorname*{inf}{f_{i}}-\\left(\\gamma+{\\frac{\\gamma^{2}L_{i}}{2}}\\right)\\left\\|\\nabla f_{i}(\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right))\\right\\|^{2}\\leq f_{i}\\left(\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right)\\right)-\\operatorname*{inf}{f_{i}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Using the interpolation regime assumption, we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\Vert\\nabla f_{i}\\left(\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right)\\right)\\right\\Vert^{2}=\\left\\Vert\\nabla f_{i}\\left(\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right)\\right)-\\nabla f_{i}(x_{\\star})\\right\\Vert^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq2L_{i}D_{f_{i}}\\left(\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right),x_{\\star}\\right)}\\\\ &{\\qquad\\qquad\\qquad=2L_{i}\\left(f_{i}(\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right))-\\mathrm{inf}\\;f_{i}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the inequality is obtained using Fact 3. As a result, we obtain the following bound, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{i}\\left(\\mathrm{prox}_{\\gamma f_{i}}\\left(x\\right)\\right)-\\operatorname*{inf}f_{i}\\ge\\displaystyle\\frac{1}{1+\\gamma L_{i}(2+\\gamma L_{i})}\\left(f_{i}(x)-\\operatorname*{inf}f_{i}\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{(1+\\gamma L_{i})^{2}}\\left(f_{i}(x)-\\operatorname*{inf}f_{i}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Plug the above lower bound into (45), we obtain ", "page_idx": 39}, {"type": "equation", "text": "$$\nM_{f_{i}}^{\\gamma}\\left(x\\right)-\\operatorname*{inf}M_{f_{i}}^{\\gamma}\\geq\\frac{1}{1+\\gamma L_{i}}\\left(f_{i}(x)-\\operatorname*{inf}{f_{i}}\\right),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Notice that we have $\\begin{array}{r}{M^{\\gamma}\\left(x\\right)\\,=\\,\\frac{1}{n}\\sum_{i=1}^{n}M_{f_{i}}^{\\gamma}\\left(x\\right)}\\end{array}$ . Since we are in the interpolation regime, from Lemma 9, we know that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\operatorname*{inf}M^{\\gamma}=M^{\\gamma}\\left(x_{\\star}\\right)=\\frac{1}{n}\\sum_{i=1}^{n}M_{f_{i}}^{\\gamma}\\left(x_{\\star}\\right)=\\frac{1}{n}\\sum_{i=1}^{n}\\operatorname*{inf}M_{f_{i}}^{\\gamma},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\operatorname*{inf}\\,f=f(x_{\\star})={\\frac{1}{n}}\\sum_{i=1}^{n}f_{i}(x_{\\star})={\\frac{1}{n}}\\sum_{i=1}^{n}\\operatorname*{inf}\\,f_{i}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We average (46) for each $i\\in[n]$ and obtain ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle M^{\\gamma}\\left(\\boldsymbol{x}\\right)-\\operatorname*{inf}{M^{\\gamma}}\\ge\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{1+\\gamma L_{i}}\\left(f_{i}(\\boldsymbol{x})-\\operatorname*{inf}_{\\ f_{i}}f_{i}\\right)}}\\\\ {{\\displaystyle\\ge\\frac{1}{1+\\gamma L_{\\operatorname*{max}}}\\cdot\\frac{1}{n}\\sum_{i=1}^{n}\\left(f_{i}(\\boldsymbol{x})-\\operatorname*{inf}_{\\ f_{i}}f_{i}\\right)}}\\\\ {{\\displaystyle=\\frac{1}{1+\\gamma L_{\\operatorname*{max}}}\\left(f(\\boldsymbol{x})-\\operatorname*{inf}_{\\ f}f\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 39}, {"type": "text", "text": "H.11 Proof of Lemma 11 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "We start with picking any point $x\\,\\in\\,\\mathbb{R}^{d}$ , since we are in the interpolation regime, according to Lemma 9, we have $M^{\\gamma}\\left(x_{\\star}\\right)=f(x_{\\star})$ . Applying Lemma 10, we get ", "page_idx": 40}, {"type": "equation", "text": "$$\nM^{\\gamma}\\left(x\\right)-M^{\\gamma}\\left(x_{\\star}\\right)\\geq\\frac{1}{1+\\gamma L_{\\operatorname*{max}}}\\left(f(x)-f(x_{\\star})\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We know that from the $\\mu$ -strong convexity of $f$ , we have for any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\nf(x)-f(x_{\\star})-\\langle\\nabla f(x_{\\star}),x-x_{\\star}\\rangle\\geq{\\frac{\\mu}{2}}\\left\\|x-x_{\\star}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Notice that since $\\nabla f(x_{\\star})=0$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\nf(x)-f(x_{\\star})\\geq{\\frac{\\mu}{2}}\\left\\|x-x_{\\star}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Combining the above two inequalities (47) and (48), we have ", "page_idx": 40}, {"type": "equation", "text": "$$\nM^{\\gamma}\\left(x\\right)-M^{\\gamma}\\left(x_{\\star}\\right)\\geq\\frac{\\mu}{1+\\gamma L_{\\operatorname*{max}}}\\cdot\\frac{1}{2}\\left\\Vert x-x_{\\star}\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 40}, {"type": "text", "text": "H.12 Proof of Lemma 12 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Notice that $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ is a minimizer of $f$ if and only if $0\\in\\partial f(x)$ . This inclusion holds if and only if $0\\in\\partial\\left(\\gamma f(x)\\right)$ , which can be rewritten as $x-x\\in\\partial\\left(\\gamma f(x)\\right)$ . By the equivalence of (i) and (ii) in Fact 2, the above condition is the same as $x=\\operatorname{prox}_{\\gamma f}\\left(x\\right)$ . ", "page_idx": 40}, {"type": "text", "text": "H.13 Proof of Lemma 13 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Since each $f_{i}$ is $L_{i}$ -smooth, the following function is convex for every $i\\in[n]$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{L_{i}}{2}\\left\\|x\\right\\|^{2}-f_{i}\\left(x\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Thus, ", "page_idx": 40}, {"type": "equation", "text": "$$\n{\\frac{{\\frac{1}{n}}\\sum_{i=1}^{n}L_{i}}{2}}\\left\\|x\\right\\|^{2}-{\\frac{1}{n}}\\sum_{i=1}^{n}f_{i}(x),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "is also a convex function, which indicates $f(x)$ is also $\\textstyle{\\frac{1}{n}}\\sum_{i=1}^{n}L_{i}$ -smooth. This means that ", "page_idx": 40}, {"type": "equation", "text": "$$\nL\\leq\\frac{1}{n}\\sum_{i=1}^{n}L_{i}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Now notice that the $L$ -smoothness of $f$ is equivalent to the following function being convex ", "page_idx": 40}, {"type": "equation", "text": "$$\n{\\frac{n L}{2}}\\left\\|x\\right\\|^{2}-\\sum_{i=1}^{n}f_{i}(x).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Pick any $j\\in[n]$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{n L}{2}\\left\\|x\\right\\|^{2}-\\sum_{i=1}^{n}f_{i}(x)+\\sum_{1\\leq i\\neq j\\leq n}f_{i}(x)=\\frac{n L}{2}\\left\\|x\\right\\|^{2}-f_{j}(x).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Since all functions are convex and the sum of convex functions is convex, ", "page_idx": 40}, {"type": "equation", "text": "$$\n{\\frac{n L}{2}}\\left\\|x\\right\\|^{2}-f_{j}(x),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "is convex, which indicates that $f_{j}(x)$ is also $n L$ -smooth. As a result, for every $j\\in[n]$ , we have $n L\\geq L_{j}$ . Summing up the inequality for every $j\\in[n]$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n{\\frac{1}{n^{2}}}\\sum_{j=1}^{n}L_{j}\\leq L.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Combining (49) and (50), we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n{\\frac{1}{n^{2}}}\\sum_{i=1}^{n}L_{i}\\leq L\\leq{\\frac{1}{n}}\\sum_{i=1}^{n}L_{i}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "In order to demonstrate that both bounds are tight in the above inequality, we consider cases where they are identities. ", "page_idx": 41}, {"type": "text", "text": "(i): Consider the case that each function $\\begin{array}{r}{f_{i}(x)=\\frac{1}{2}\\cdot L_{i}\\cdot\\left\\Vert x\\right\\Vert^{2}}\\end{array}$ , it is easy to see that $f(x)=$ $\\textstyle{\\frac{1}{2}}\\cdot\\left({\\frac{1}{n}}\\sum_{i=1}^{n}L_{i}\\right)\\cdot\\left\\|x\\right\\|^{2}$ . In this case $\\textstyle L={\\frac{1}{n}}\\sum_{i=1}^{n}L_{i}$ , the upper bound is an identity. ", "page_idx": 41}, {"type": "text", "text": "(ii): Consider the case that each function $\\begin{array}{r}{f_{i}(x)=\\frac{1}{2}\\cdot\\theta\\cdot x_{i}^{2}}\\end{array}$ , where $\\theta>0$ is a constant, $x_{i}$ is the $i$ -th coordinate of $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ . In this case $\\textstyle f(x)={\\frac{1}{2}}\\cdot{\\frac{\\theta}{n}}\\left\\|x\\right\\|^{2}$ . It is easy to verify that in this case $L_{i}=\\theta$ , $\\textstyle L={\\frac{1}{n}}\\theta$ . Thus $\\textstyle{\\frac{1}{n^{2}}}\\sum_{i=1}^{n}L_{i}=L$ , the lower bound is an identity. ", "page_idx": 41}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 41}, {"type": "text", "text": "H.14 Proof of Lemma 14 ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "From the definition of $C(\\gamma,\\tau,1)$ and $C\\left(\\gamma,\\tau,\\alpha_{\\gamma,\\tau}\\right)$ , we know that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{C(\\gamma,\\tau,1)}{C\\left(\\gamma,\\tau,\\alpha_{\\gamma,\\tau}\\right)}=\\frac{1}{\\gamma L_{\\gamma,\\tau}\\left(2-\\gamma L_{\\gamma,\\tau}\\right)}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Now let $t=\\gamma L_{\\gamma,\\tau}$ , we have the following bound on $t$ according to the definition of $L_{\\gamma,\\tau}$ given in Theorem 1. ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{l}{t=\\gamma L_{\\gamma,\\tau}}\\\\ {\\displaystyle\\ =\\frac{n-\\tau}{\\tau(n-1)}\\cdot\\frac{\\gamma L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}+\\frac{n(\\tau-1)}{\\tau(n-1)}\\cdot\\gamma L_{\\gamma}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Notice that in Lemma 7, we have shown that ", "page_idx": 41}, {"type": "equation", "text": "$$\nL_{\\gamma}\\overset{\\mathrm{Lemma~7}}{\\leq}\\frac{1}{n}\\sum_{i=1}^{n}\\frac{L_{i}}{1+\\gamma L_{i}},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and due to Fact 4, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\frac{L_{i}}{1+\\gamma L_{i}}\\overset{\\mathrm{Fact\\,4}}{\\leq}\\frac{L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "As a result, ", "page_idx": 41}, {"type": "equation", "text": "$$\nt\\leq\\frac{n-\\tau}{\\tau(n-1)}\\cdot\\frac{\\gamma L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}+\\frac{n(\\tau-1)}{\\tau(n-1)}\\cdot\\frac{\\gamma L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}=\\frac{\\gamma L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}<1.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "It is easy to show that $\\begin{array}{r}{g(t)=\\frac{1}{t(2-t)}}\\end{array}$ is monotone decreasing when $t\\in[0,1]$ , thus ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{C(\\gamma,\\tau,1)}{C\\left(\\gamma,\\tau,\\alpha_{\\gamma,\\tau}\\right)}\\geq\\frac{1}{\\frac{\\gamma L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}\\left(1-\\frac{\\gamma L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}\\right)}}\\\\ &{\\qquad\\qquad\\qquad=2+\\frac{1}{\\gamma L_{\\operatorname*{max}}}+\\gamma L_{\\operatorname*{max}}}\\\\ &{\\qquad\\qquad\\qquad\\stackrel{\\mathrm{AM-GM}}{\\geq}4,}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the last inequality is due to the AM-GM inequality. This concludes the proof. ", "page_idx": 41}, {"type": "text", "text": "H.15 Proof of Lemma 15 ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "As suggested by Lemma 7, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\nC\\left(\\gamma,n,\\alpha_{\\gamma,n}\\right)\\leq C\\left(\\gamma,\\tau,\\alpha_{\\gamma,\\tau}\\right)\\leq C\\left(\\gamma,1,\\alpha_{\\gamma,1}\\right),\\quad\\forall\\tau\\in[n].\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Notice that $\\alpha_{\\gamma,\\tau}$ is given by ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\alpha_{\\gamma,\\tau}=\\frac{1}{\\gamma L_{\\gamma,\\tau}},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and we know that ", "page_idx": 42}, {"type": "equation", "text": "$$\nL_{\\gamma,\\tau}=\\frac{n-\\tau}{\\tau(1-n)}\\cdot\\frac{L_{\\operatorname*{max}}}{1+\\gamma L_{\\operatorname*{max}}}+\\frac{n(\\tau-1)}{\\tau(n-1)}\\cdot L_{\\gamma}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "From Lemma 7 and Fact 4, we know that ", "page_idx": 42}, {"type": "equation", "text": "$$\nL_{\\gamma}\\stackrel{\\mathrm{Lemma~7}}{\\leq}\\frac{1}{n}\\sum_{i=1}^{n}\\frac{L_{i}}{1+\\gamma L_{i}}\\stackrel{\\mathrm{Fact~4}}{\\leq}\\frac{L_{\\mathrm{max}}}{1+\\gamma L_{\\mathrm{max}}}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Consequently, $L_{\\gamma,\\tau}$ decreases as $\\tau$ increases. Therefore, $\\alpha_{\\gamma,\\tau}$ increases with the increase of $\\tau$ , as illustrated by the following inequality ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\alpha_{\\gamma,1}\\leq\\alpha_{\\gamma,\\tau}\\leq\\alpha_{\\gamma,n},\\quad\\forall\\tau\\in[n].\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 42}, {"type": "text", "text": "H.16 Proof of Lemma 16 ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "We refer the readers to the proof of Lemma 3.1 of B\u00f6hm and Wright [2021]. ", "page_idx": 42}, {"type": "text", "text": "H.17 Proof of Lemma 17 ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "We refer the readers to the proof of Proposition 7 of Yu et al. [2015]. ", "page_idx": 42}, {"type": "text", "text": "H.18 Proof of Lemma 18 ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Observe that since $\\begin{array}{r}{0<\\gamma<\\frac{1}{\\rho}}\\end{array}$ , we do have $f+\\textstyle{\\frac{1}{2}}\\cdot{\\frac{1}{\\gamma}}\\left\\Vert\\cdot\\right\\Vert^{2}$ being strongly convex. This indicates that $\\mathrm{prox}_{\\gamma f}$ is always a singleton and therefore $M_{f}^{\\gamma}$ is differentiable, as suggested by Lemma 16. Notice that $x$ is stationary point of $M_{f}^{\\gamma}$ if and only if $\\nabla M_{f}^{\\gamma}\\left(x\\right)\\;=\\;0$ . This is equivalent to ${\\underset{\\gamma}{\\frac{1}{\\gamma}}}\\left(x-\\operatorname{prox}_{\\gamma f}\\left(x\\right)\\right)\\,=\\,0$ , which is $x\\,=\\,\\mathrm{prox}_{\\gamma f}\\left(x\\right)$ . In addition, $x\\,=\\,\\operatorname{prox}_{\\gamma f}\\left(x\\right)$ is equivalent to ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\nabla f(x)+\\frac{1}{\\gamma}\\left(x-x\\right)=0,\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "which is $\\nabla f(x)\\;=\\;0$ . Combining the above statements, we have $\\nabla f(x)\\;=\\;0$ if and only if $\\nabla M_{f}^{\\gamma}\\left(x\\right)=0$ . This suggests that the two functions have the same set of stationary points. ", "page_idx": 42}, {"type": "text", "text": "H.19 Proof of Lemma 19 ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Apply Theorem 1 of Khaled and Richt\u00e1rik [2023], notice that in this case GD satisfy the expected smoothness assumption given in Assumption 2 of Khaled and Richt\u00e1rik [2023] with $A=0$ , $B=1$ and $C=0$ , we obtain that when the step size $\\eta$ satisfies ", "page_idx": 42}, {"type": "equation", "text": "$$\n0<\\eta<\\frac{1}{L B}=\\frac{1}{L},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $L$ is the smoothness constant of $f$ , the iterates of GD satisfy ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq k\\leq K-1}\\mathbb{E}\\left[\\left\\|\\nabla f(x_{k})\\right\\|^{2}\\right]\\leq\\frac{2\\left(f(x_{0})-\\operatorname*{inf}f\\right)}{\\eta K}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "This completes the proof. ", "page_idx": 42}, {"type": "text", "text": "H.20 Proof of Lemma 20 ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Notice that we are in the interpolation regime, by Lemma 8, we know that $f$ and $M^{\\gamma}$ have the same set of minimizers and minimum. As a result, ", "page_idx": 43}, {"type": "equation", "text": "$$\nM^{\\gamma}\\left(x_{\\star}\\right)=\\frac{1}{n}\\sum_{i=1}^{n}M_{f_{i}}^{\\gamma}\\left(x_{\\star}\\right){\\overset{\\mathrm{Lemma~8}}{=}}\\;f(x_{\\star}).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "From the above inequality, we obtain that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\left(M_{f_{i}}^{\\gamma}\\left(x\\right)-M_{f_{i}}^{\\gamma}\\left(x_{\\star}\\right)\\right)}{\\gamma\\cdot\\left\\Vert\\frac{1}{n}\\sum_{i=1}^{n}\\nabla M_{f_{i}}^{\\gamma}\\left(x\\right)\\right\\Vert^{2}}\\overset{(51)}{=}\\frac{M^{\\gamma}\\left(x\\right)-M^{\\gamma}\\left(x_{\\star}\\right)}{\\gamma\\cdot\\left\\Vert\\nabla M^{\\gamma}\\left(x\\right)\\right\\Vert^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Then by the smoothness of $M^{\\gamma}$ and Fact 3, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{M^{\\gamma}\\left(x\\right)-M^{\\gamma}\\left(x_{\\star}\\right)}{\\gamma\\cdot\\left\\Vert\\nabla M^{\\gamma}\\left(x\\right)\\right\\Vert^{2}}\\overset{\\mathrm{Fact}\\,3}{\\geq}\\frac{\\frac{1}{2L_{\\gamma}}\\left\\Vert\\nabla M^{\\gamma}\\left(x\\right)-\\nabla M^{\\gamma}\\left(x_{\\star}\\right)\\right\\Vert^{2}}{\\gamma\\cdot\\left\\Vert\\nabla M^{\\gamma}\\left(x\\right)\\right\\Vert^{2}}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{1}{2\\gamma L_{\\gamma}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Thus, by combining the above inequalities, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\frac1n\\sum_{i=1}^{n}\\Big(M_{f_{i}}^{\\gamma}\\left(x\\right)-M_{f_{i}}^{\\gamma}\\left(x_{\\star}\\right)\\Big)}{\\gamma\\cdot\\Big\\Vert\\frac1n\\sum_{i=1}^{n}\\nabla M_{f_{i}}^{\\gamma}\\left(x\\right)\\Big\\Vert^{2}}\\ge\\frac{1}{2\\gamma L_{\\gamma}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Notice that from the definition of $\\alpha_{k,S}$ for FedExProx-StoPS, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\alpha_{k,S}=\\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\left(M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)-M_{f_{i}}^{\\gamma}\\left(x_{\\star}\\right)\\right)}{\\gamma\\cdot\\left\\Vert\\frac{1}{n}\\sum_{i=1}^{n}\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)\\right\\Vert^{2}}\\geq\\frac{1}{2\\gamma L_{\\gamma}}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Therefore, using the above lower bound, it is straight forward to further relax (12) to ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\bar{x}^{K})\\right]-\\operatorname*{inf}{f}\\leq2L_{\\gamma}\\left(1+2\\gamma L_{\\operatorname*{max}}\\right)\\cdot\\frac{\\|x_{0}-x_{\\star}\\|^{2}}{K}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 43}, {"type": "text", "text": "I Experiments ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "In this section, we describe the settings and results of numerical experiments to demonstrate the effectiveness of our method. ", "page_idx": 43}, {"type": "text", "text": "I.1 Experiment settings ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "We consider the overparameterized linear regression problem in the finite sum setting ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\left\\{f(x)=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(x)\\right\\},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $d$ is the dimension of the problem, $n$ is the total number of clients, each function $f_{i}$ has the following form ", "page_idx": 43}, {"type": "equation", "text": "$$\nf_{i}(x)={\\frac{1}{2}}\\left\\|{\\cal A}_{i}x-b_{i}\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $A_{i}\\in\\mathbb{R}^{n_{i}\\times d}$ , $b_{i}\\in\\mathbb{R}^{n_{i}}$ . Here $n_{i}$ is the number of samples on each client. It is easy to see that for each function $f_{i}$ , we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\nabla f_{i}(x)=A_{i}^{\\top}A_{i}x-A_{i}^{\\top}b_{i},\\quad{\\mathrm{~and~}}\\quad\\nabla^{2}f_{i}(x)=A_{i}^{\\top}A_{i}\\succeq O_{d}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Thus, it follows that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\nabla f(x)=\\frac{1}{n}\\sum_{i=1}^{n}\\left(A_{i}^{\\top}A_{i}x-A_{i}^{\\top}b_{i}\\right),\\mathrm{~and~}\\quad\\nabla^{2}f(x)=\\frac{1}{n}\\sum_{i=1}^{n}A_{i}^{\\top}A_{i}\\succeq O_{d}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "The problem is therefore convex. Notice that one implicit assumption for the class of proximal point methods in practice is that the proximity operator can be computed efficiently. In the setting of linear regression, we have the following closed form formula for the proximity operator $\\operatorname{prox}_{\\gamma f_{i}}$ , which holds for any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\operatorname{prox}_{\\gamma f_{i}}\\left(x\\right)=\\left(A_{i}^{\\top}A_{i}+\\frac{1}{\\gamma}I_{d}\\right)^{-1}\\cdot\\left(A_{i}^{\\top}b_{i}+\\frac{1}{\\gamma}x\\right).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Observe that in the linear regression problem, since we know the closed form expression of each $f_{i}$ and $f$ , we know the corresponding smoothness constant ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{i}=\\lambda_{\\mathrm{max}}\\left(\\mathbf{A}_{i}^{\\top}A_{i}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Notice that from Lemma 1, we have ", "page_idx": 44}, {"type": "equation", "text": "$$\nM_{f_{i}}^{\\gamma}\\left(x\\right)=f_{i}\\left(\\mathrm{prox}_{\\gamma}\\left(f_{i}\\right)\\right)+\\frac{1}{2\\gamma}\\left\\Vert x-\\mathrm{prox}_{\\gamma}\\left(f_{i}\\right)\\left(x\\right)\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Since we know $\\operatorname{prox}_{\\gamma}\\left(f_{i}\\right)$ in closed form using (52), we also know each local Moreau envelope in closed form, and thus the same for n1 in=1 M f\u03b3i. As a result, we can deduce L\u03b3 for M \u03b3. In our experiments, we pick $\\textstyle d\\geq\\sum_{i=1}^{n}n_{i}$ so that we are in the interpolation regime. Each $\\boldsymbol{A}_{i}$ is generated randomly from a uniform distribution between $[0,1)$ , and the corresponding vector $b_{i}$ is also generated from the same uniform distribution. In order to find a minimizer $x_{\\star}$ , we run gradient descent for sufficient amount of iterations. All the codes for the experiments are written in Python 3.11 with NumPy and SciPy package. The code was run on a machine with AMD Ryzen 9 5900HX Radeon Graphics $\\textcircled{a}3.3\\,\\mathrm{GHz}$ and 8 cores 16 threads. For experiment in the small dimension regime, each algorithm considers here only takes seconds to finish. For larger experiments, depending on the specific implementation, the algorithms typically take a few minutes to half an hour to finish. For FedProx, FedExP and our method FedExProx in the full participation case, the algorithm for a specific dataset is deterministic, while in case where client sampling is taken into account, the randomness of the algorithms comes from the specific sampling strategy used. Our code is publicly available at the following link: https://anonymous.4open.science/r/FedExProx-F262/ ", "page_idx": 44}, {"type": "text", "text": "I.2 Large dimension regime ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "In this section we provide the numerical experiments in the large dimension regime, where $n_{i}=20$ for each $i\\in[n]$ , $n=30$ , $d=900$ . ", "page_idx": 44}, {"type": "text", "text": "I.2.1 Comparison of FedExProx and FedProx ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "In this section, we compare the performance of FedProx with our method FedExProx in the full participation case and in the client partial participation case, demonstrating that the extrapolated counterpart outperforms FedProx in iteration complexity. Notice that here we are only concerned with iteration complexity, since the amount of computations is almost the same for the two algorithms. The only difference is that for FedExProx, instead of simply averaging the iterates obtained from each client, the server performs extrapolation. From Figure 2, it is easy to see that our proposed algorithm FedExProx outperforms FedProx, which provides numerical evidence for our theoretical findings. Notably, in order to achieve the small level of function value sub-optimality, FedExProx typically requires only half the number of iterations needed by FedProx, which indicates a factor of 2 speed up in terms of iteration complexity. Another observation is that, $\\alpha_{\\gamma,n}$ is decreasing as $\\gamma$ increases, which suggests that when local step sizes are small, the practice of simply averaging the iterates is far from optimal. ", "page_idx": 44}, {"type": "text", "text": "We also compare the performance of the two algorithms in the client partial participation setting. As one can observe from Figure 3, FedExProx still outperforms FedProx in the client partial participation setting, which further corroborates our theoretical findings. Observe that $\\alpha_{\\gamma,\\tau}$ here increases as $\\tau$ becomes larger, which coincides with our predictions in Remark 7. ", "page_idx": 44}, {"type": "image", "img_path": "FuTfZK7PK3/tmp/77d8cf2795e316a0c02910c116e8438f787c446abe10a2fae0ecf04a2847dcd6.jpg", "img_caption": ["Figure 2: Comparison of convergence of FedExProx and FedProx in terms of iteration complexity in the full participation setting. For this experiment $\\gamma$ is picked from the set $\\{0.0{\\bar{0}}01,0.001,0.01,0.1{\\bar{,}}1,10\\}$ , the $\\alpha_{\\gamma,n}$ indicates the optimal constant extrapolation parameter as defined in Theorem 1. For each choice of $\\gamma$ , the two algorithms are run for $K=10000$ iterations, respectively. "], "img_footnote": [], "page_idx": 45}, {"type": "image", "img_path": "FuTfZK7PK3/tmp/5f2a88afff6f8c0cd2914119bcc6adc3fc4ce1e2f978a55c6cc83b6b7bb62051.jpg", "img_caption": ["Figure 3: Comparison of convergence of FedExProx and FedProx in terms of iteration complexity in the client partial participation setting. For this experiment $\\gamma$ is picked from the set $\\{0.0001,\\dot{0}.001\\}$ , the client minibatch size $\\tau$ is chosen from $\\{10,15,20\\}$ and the $\\alpha_{\\gamma,n}$ indicates the optimal constant extrapolation parameter as defined in Theorem 1. For each choice of $\\gamma$ and $\\tau$ , the two algorithm are run for $K=10000$ iterations, respectively. "], "img_footnote": [], "page_idx": 45}, {"type": "image", "img_path": "FuTfZK7PK3/tmp/759730418d6683e7c46b350c5254fa2ee8c13826b3b726627159d604e135301a.jpg", "img_caption": ["Figure 4: Comparison in terms of iteration complexity for FedExProx with different step sizes $\\gamma$ chosen from $\\left\\{0.0001,0.001,0.01,1,10,100\\right\\}$ in the full participation setting. In the figure, we use FedExP with different iterations of local training $t\\,\\in\\,\\{1,5,10\\}$ as a benchmark in the three sub-figures. The local step size for FedExP is set to be the largest possible value 6tL1max , where $L_{\\mathrm{max}}=\\operatorname*{max}_{i}L_{i}.$ . "], "img_footnote": [], "page_idx": 46}, {"type": "image", "img_path": "FuTfZK7PK3/tmp/cc14c44ce602e80f39031c0c368f20ddf2d4bad5cf14f02bbdf22fdd7ddd2538.jpg", "img_caption": ["Figure 5: Comparison in terms of iteration complexity for FedExProx with different step sizes $\\gamma$ chosen from $\\{0.0001,0.0005,0.01,1,10\\}$ in the client partial participation case. Different client minibatch sizes are used, the minibatch size $\\tau$ is chosen from $\\{5,10,20\\}$ . "], "img_footnote": [], "page_idx": 46}, {"type": "text", "text": "I.2.2 Comparison of FedExProx with different local step size ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "In this section, we compare the performance in terms of iteration complexity for FedExProx with different local step sizes. We also include FedExP as a reference. The local step size of FedExP is chosen to be $\\frac{1}{6t L_{\\operatorname*{max}}}$ , where $t$ is the number of gradient descent iterations performed by each client for local training, $\\tilde{L}_{\\operatorname*{max}}=\\operatorname*{max}_{i}L_{i}$ , where $L_{i}$ is the smoothness constant of $f_{i}$ . ", "page_idx": 46}, {"type": "text", "text": "As one can observe from Figure 4, for our proposed method FedExProx, the larger $\\gamma$ is, the faster it will converge. However, as $\\gamma$ becomes larger, the improvement in iteration complexity becomes trivial at some point. Note that for different $\\gamma$ , the complexities required to compute the proximity operator locally varies and often larger $\\gamma$ requires more computation than smaller $\\gamma$ . Compared to FedExP with the best local step size $\\frac{1}{6t L_{\\mathrm{max}}}$ , FedExProx with a large enough $\\gamma$ is better in terms of iteration complexity. In the case where the computation of proximity operator is efficient, our method has a better computation complexity as well. Notice that small $\\gamma$ leads to slow down of our method, and we do not claim that the iteration complexity of FedExProx is always better than FedExP. However, it is provable that FedExProx indeed has a better worst case iteration complexity. We want to emphasize a key difference between FedExP and our method is that we do not have any constraints on the local step size $\\gamma$ , and our method converges for arbitrary local step size $\\gamma>0$ , while for FedExP, a misspecified step size could lead to divergence. ", "page_idx": 46}, {"type": "text", "text": "We also compare FedExProx with different step sizes in the client sampling case, see Figure 5. However, since there is no explicit convergence guarantee for FedExP in this case, we did not include FedExP in the plot. ", "page_idx": 46}, {"type": "text", "text": "In the client partial participation case, the same behavior of how our proposed algorithm FedExProx changes according to different local step sizes $\\gamma$ is observed. A small $\\gamma$ leads to slow convergence of the algorithm, while for large $\\gamma$ , the convergence is improved. However, at some point, the improvement becomes trivial. ", "page_idx": 46}, {"type": "image", "img_path": "FuTfZK7PK3/tmp/862c86662fc3841a54521e8c9c933d0e2eebf60972132ad8c35500bbbdbb67ba.jpg", "img_caption": ["Figure 6: Comparison of FedExProx, FedExProx-GraDS and FedExProx-StoPS in terms of iteration complexity with different step sizes $\\gamma$ chosen from $\\{0.0005,0.0005,0.05,0.5,1,5\\}$ in the full participation setting. "], "img_footnote": [], "page_idx": 47}, {"type": "text", "text": "I.2.3 Comparison of FedExProx and its adaptive variants ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "In this section, we compare FedExProx and its two adaptive variants FedExProx-GraDS and FedExProx-StoPS. We first focus on the full participation case. Note that in this case, the all the algorithms are deterministic. For FedExProx-GraDS, as it is suggested by Theorem 2, the extrapolation parameter is given by ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\alpha_{k}=\\alpha_{k,G}:=\\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|x_{k}-\\mathrm{prox}_{\\gamma f_{i}}\\left(x_{k}\\right)\\right\\|^{2}}{\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\left(x_{k}-\\mathrm{prox}_{\\gamma f_{i}}\\left(x_{k}\\right)\\right)\\right\\|^{2}}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "The server can use the local iterates it received from each client to compute $\\alpha_{k,G}$ directly. If, in addition, we know $L_{\\mathrm{max}}$ , we can implement a version that has a better theoretical guarantee, ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\alpha_{k,G}:=\\frac{1+\\gamma L_{\\operatorname*{max}}}{\\gamma L_{\\operatorname*{max}}}\\cdot\\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\left\\Vert x_{k}-\\mathrm{prox}_{\\gamma f_{i}}\\left(x_{k}\\right)\\right\\Vert^{2}}{\\left\\Vert\\frac{1}{n}\\sum_{i=1}^{n}\\left(x_{k}-\\mathrm{prox}_{\\gamma f_{i}}\\left(x_{k}\\right)\\right)\\right\\Vert^{2}}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "For FedExProx-StoPS, we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\alpha_{k}=\\alpha_{k,S}=\\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\left(M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)-\\operatorname*{inf}M_{f_{i}}^{\\gamma}\\right)}{\\gamma\\left\\Vert\\frac{1}{n}\\sum_{i=1}^{n}\\nabla M_{f_{i}}^{\\gamma}\\left(x_{k}\\right)\\right\\Vert^{2}}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "In order to implement $\\alpha_{k,S}$ , the server requires each client to send the function value of its Moreau envelope at the current iterate to it, and we need to know each inf $M_{f_{i}}^{\\gamma}$ which, according to Lemma 5, is the same as inf $f_{i}$ . ", "page_idx": 47}, {"type": "text", "text": "From Figure 6, we can observe that in all cases when $\\gamma$ is sufficiently large, FedExProx-StoPS is the best among the three algorithms considered, and FedExProx-GraDS outperforms FedExProx, this provides numerical evidence for the effectiveness of our proposed algorithms. In the cases when $\\gamma$ is small, the convergence of FedExProx-GraDS seems to be better than the other two algorithms. We also plot the difference of extrapolation parameter used by the algorithms in each iteration. From Figure 7, observe that when $\\gamma$ is small, $\\alpha_{k,G}$ is often much larger than $\\alpha_{k,S}$ , resulting in better convergence of FedExProx-GraDS as observed in the first two plots of Figure 6. When $\\gamma$ becomes larger, $\\alpha_{k,G}$ and $\\alpha_{k,S}$ become comparable, and their performance is also comparable, with FedExProx-StoPS slightly better than FedExProx-GraDS. ", "page_idx": 47}, {"type": "image", "img_path": "FuTfZK7PK3/tmp/bbae66df09cb2ceb79160ca3d42325ed91bf87b9150c3210922c349e650a56a7.jpg", "img_caption": ["Figure 7: Comparison of the extrapolation parameter $\\alpha_{k}$ used by FedExProx, FedExProx-GraDS and FedExProx-StoPS in each iteration with different step sizes $\\gamma$ chosen from $\\{0.0005,0.0005,0.05,0.5,1,5\\}$ in the full participation setting. "], "img_footnote": [], "page_idx": 48}, {"type": "text", "text": "", "page_idx": 48}, {"type": "text", "text": "We also conduct the experiment where we take client partial participation into account. We can observe from Figure 8 that in all cases, the two adaptive variants FedExProx-GraDS-PP and FedExProx-StoPS-PP outperform FedExProx in iteration complexity, and between the two adaptive variants, FedExProx-GraDS is the better one almost all the time. However, FedExProx-GraDS seems to be more stable than FedExProx-StoPS, especially when $\\gamma$ is small. ", "page_idx": 48}, {"type": "image", "img_path": "FuTfZK7PK3/tmp/02f0aaab1bcfdf6bb58c017423c587c3464a83cd55fa881fb6be1bdc034f8f84.jpg", "img_caption": ["Figure 8: Comparison of FedExProx, FedExProx-GraDS and FedExProx-StoPS in terms of iteration complexity with different step sizes $\\gamma$ in the client partial participation (PP) setting. The client minibatch size is chosen from $\\{5,10,20\\}$ , for each minibatch size, a step size $\\gamma\\ \\in$ $\\{0.001,0.005,0.1,0.5,1,5,10,50,100,500\\}$ is randomly selected. "], "img_footnote": [], "page_idx": 49}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: The abstract and introduction section accurately reflect the contributions made in this paper, which are mainly presented in Section 3, Section 4 and some parts of the Appendix. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 49}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: The limitations of the work are discussed in Section 5.1. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 50}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Justification: Full set of assumptions and a complete and correct proof are described for every fact, lemma, theorem and corollary appeared in this paper. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 50}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: The details of the experiments are included in the experiment section in Appendix I. The code is also provided in the corresponding link. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 51}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Justification: The details of the experiments are described in detail in Appendix I, and the code is given in the corresponding anonymous link. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 51}, {"type": "text", "text": "", "page_idx": 52}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: All the details of the experiments and link to anonymized repository are provided which is enough to understand the experiment. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 52}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: The details are depicted in the experiment section, and notice that for the full participation case of our proposed methods, it is deterministic for a specific dataset. No errors are needed in this case. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 52}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: The computation resources needed for the experiments are described in the experiment section. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 53}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: The research conducted in this paper conform with the NeurIPS Code of Ethics in every aspect. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 53}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: No potential social impact is expected by the authors. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 53}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 54}, {"type": "text", "text": "Justification: The paper contain no such risks in the authors\u2019 expectation. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 54}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 54}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 54}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: The experiment and code for this paper are well documented. The details of the dataset used is described in detail in the experiment section of the paper. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets. \u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. ", "page_idx": 54}, {"type": "text", "text": "\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 55}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 55}, {"type": "text", "text": "Justification: The paper dose not involve crowdsourcing. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 55}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 55}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 55}]