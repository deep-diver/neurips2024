[{"figure_path": "FuTfZK7PK3/figures/figures_9_1.jpg", "caption": "Figure 1: Comparison of FedExProx and FedProx in terms of iteration complexity in the full participation setting. The notation \u03b3 here denotes the local step size of the proximity operator and \u03b1\u03b3,n is the corresponding optimal extrapolation parameter computed in (9) in the full participation case. In all cases, our proposed algorithm outperforms FedProx, suggesting that the practice of simply averaging the iterates is suboptimal.", "description": "This figure compares the iteration complexity of FedExProx and FedProx for various local step sizes (\u03b3) in the full participation setting (all clients participate in each round).  FedProx is a baseline federated learning optimizer. FedExProx enhances FedProx by incorporating server-side extrapolation. The results demonstrate that FedExProx consistently outperforms FedProx across different step sizes, indicating that the server simply averaging iterates from clients is less effective compared to using extrapolation.", "section": "I Experiments"}, {"figure_path": "FuTfZK7PK3/figures/figures_45_1.jpg", "caption": "Figure 2: Comparison of convergence of FedExProx and FedProx in terms of iteration complexity in the full participation setting. For this experiment \u03b3 is picked from the set {0.0001, 0.001, 0.01, 0.1, 1, 10}, the \u03b1\u03b3,\u03b7 indicates the optimal constant extrapolation parameter as defined in Theorem 1. For each choice of \u03b3, the two algorithms are run for K = 10000 iterations, respectively.", "description": "This figure compares the convergence speed of FedExProx and FedProx in the full participation setting (all clients participate in each round).  The x-axis represents the number of iterations, and the y-axis shows the difference between the function value at the current iteration and the minimum function value (f(xk) - f(x*)). Different plots show the results for various values of the local step size \u03b3 (ranging from 0.0001 to 10). The optimal constant extrapolation parameter \u03b1\u03b3,n  is calculated according to Theorem 1 for each \u03b3.  FedExProx consistently outperforms FedProx, indicating that extrapolation leads to faster convergence.", "section": "I Experiments"}, {"figure_path": "FuTfZK7PK3/figures/figures_45_2.jpg", "caption": "Figure 3: Comparison of convergence of FedExProx and FedProx in terms of iteration complexity in the client partial participation setting. For this experiment \u03b3 is picked from the set {0.0001, 0.001}, the client minibatch size \u03c4 is chosen from {10, 15, 20} and the \u03b1\u03b3,\u03c4 indicates the optimal constant extrapolation parameter as defined in Theorem 1. For each choice of \u03b3 and \u03c4, the two algorithms are run for K = 10000 iterations, respectively.", "description": "This figure compares the convergence speed of FedExProx and FedProx under partial client participation. Different local step sizes (\u03b3) and client minibatch sizes (\u03c4) are tested.  The optimal constant extrapolation parameter (\u03b1\u03b3,\u03c4) from Theorem 1 is used for each combination of \u03b3 and \u03c4.  The results show FedExProx consistently outperforms FedProx across various settings, indicating that simple averaging of client updates (FedProx) is less efficient than incorporating extrapolation (FedExProx).", "section": "I.2.1 Comparison of FedExProx and FedProx"}, {"figure_path": "FuTfZK7PK3/figures/figures_46_1.jpg", "caption": "Figure 4: Comparison in terms of iteration complexity for FedExProx with different step sizes \u03b3 chosen from {0.0001, 0.001, 0.01, 1, 10, 100} in the full participation setting. In the figure, we use FedExP with different iterations of local training t \u2208 {1, 5, 10} as a benchmark in the three sub-figures. The local step size for FedExP is set to be the largest possible value 1/(6tLmax), where Lmax = maxi\u2208[n] Li.", "description": "This figure compares the performance of FedExProx with different step sizes (\u03b3) in the full participation setting. It also includes FedExP as a benchmark, using different numbers of local training iterations (t).  The results show the iteration complexity (number of iterations to reach a certain level of suboptimality) for each algorithm and step size. The optimal step size for FedExP is calculated as 1/(6tLmax), where Lmax is the maximum smoothness constant across all local objective functions.", "section": "I Experiments"}, {"figure_path": "FuTfZK7PK3/figures/figures_46_2.jpg", "caption": "Figure 5: Comparison in terms of iteration complexity for FedExProx with different step sizes \u03b3 chosen from {0.0001, 0.0005, 0.01, 1, 10} in the client partial participation case. Different client minibatch sizes are used, the minibatch size \u03c4 is chosen from {5, 10, 20}.", "description": "This figure compares the performance of FedExProx with different step sizes (\u03b3) in the client partial participation setting.  Multiple subplots show the results for various minibatch sizes (\u03c4 = 5, 10, 20). Each subplot displays curves for several different step sizes, illustrating how the convergence rate changes with varying \u03b3 and \u03c4.  The y-axis represents the function value suboptimality (f(x<sub>k</sub>) - f(x*)), and the x-axis represents the number of iterations.", "section": "I.2.1 Comparison of FedExProx and FedProx"}, {"figure_path": "FuTfZK7PK3/figures/figures_47_1.jpg", "caption": "Figure 6: Comparison of FedExProx, FedExProx-GraDS and FedExProx-StoPS in terms of iteration complexity with different step sizes \u03b3 chosen from {0.0005, 0.0005, 0.05, 0.5, 1, 5} in the full participation setting.", "description": "This figure compares the performance of three different algorithms: FedExProx, FedExProx-GraDS, and FedExProx-StoPS.  All three algorithms aim to minimize a loss function, but they use different extrapolation strategies.  The x-axis represents the number of iterations, and the y-axis shows the difference between the current function value and the minimum function value.  Different lines represent different step sizes (\u03b3), showing how the convergence rate changes with the choice of step size for each algorithm. The results demonstrate that the adaptive extrapolation methods (FedProx-GraDS and FedExProx-StoPS) generally outperform the basic FedExProx approach, especially when the step size (\u03b3) is sufficiently large.", "section": "I.2.3 Comparison of FedExProx and its adaptive variants"}, {"figure_path": "FuTfZK7PK3/figures/figures_48_1.jpg", "caption": "Figure 7: Comparison of the extrapolation parameter \u03b1k used by FedExProx, FedExProx-GraDS and FedExProx-StoPS in each iteration with different step sizes \u03b3 chosen from {0.0005, 0.0005, 0.05, 0.5, 1, 5} in the full participation setting.", "description": "This figure compares the extrapolation parameter \u03b1k used in three different algorithms (FedExProx, FedExProx-GraDS, and FedExProx-StoPS) across various iterations.  Different plots show the results for different values of the step size \u03b3 (0.0005, 0.005, 0.05, 0.5, 1, 5). The y-axis represents the value of \u03b1k (on a logarithmic scale), and the x-axis represents the iteration number.  The plots illustrate how the adaptive extrapolation strategies in FedExProx-GraDS and FedExProx-StoPS adjust the extrapolation parameter \u03b1k across iterations, in contrast to the constant \u03b1k used in FedExProx.", "section": "I.2.3 Comparison of FedExProx and its adaptive variants"}, {"figure_path": "FuTfZK7PK3/figures/figures_49_1.jpg", "caption": "Figure 8: Comparison of FedExProx, FedExProx-GraDS-PP and FedExProx-StoPS-PP in terms of iteration complexity with different step sizes \u03b3 in the client partial participation (PP) setting. The client minibatch size \u03c4 is chosen from {5, 10, 20}, for each minibatch size, a step size \u03b3 \u2208 {0.001, 0.005, 0.1, 0.5, 1, 5, 10, 50, 100, 500} is randomly selected.", "description": "This figure compares the performance of FedExProx and its two adaptive variants (FedProx-GraDS-PP and FedExProx-StoPS-PP) in terms of iteration complexity under various settings.  The experiment considers different step sizes (\u03b3) and client minibatch sizes (\u03c4) in a partial client participation scenario. The results show the convergence rate of each algorithm across different parameter settings. ", "section": "I Experiments"}]