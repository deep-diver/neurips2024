[{"type": "text", "text": "Diffusion of Thought: Chain-of-Thought Reasoning in Diffusion Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiacheng $\\mathbf{Y}\\mathbf{e}^{1}$ ,\u2217 Shansan Gong1,\u2217 Liheng Chen1,\u2217 Lin Zheng1, Jiahui $\\mathbf{Gao}^{2}$ ,Han $\\mathbf{S}\\mathbf{h}\\mathbf{i}^{2}$ ,Chuan $\\mathbf{W}\\mathbf{u}^{1}$ ,Xin Jiang2, Zhenguo $\\mathbf{Li^{2}}$ ,Wei $\\mathbf{Bi}^{3}$ ,Lingpeng Kong1 1 The University of Hong Kong 2 Huawei Noah\u2019s Ark Lab 3 Tencent AI Lab {carsonye, sansa933}@connect.hku.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, diffusion models have garnered significant interest in the field of text processing due to their many potential advantages compared to conventional autoregressive models. In this work, we propose Diffusion-of-Thought (DoT), a novel approach that integrates diffusion models with Chain-of-Thought, a well-established technique for improving the reasoning ability of autoregressive language models. In contrast to autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT allows reasoning steps to diffuse over time through a diffusion language model and offers greater flexibility in trading-off computation for reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication, boolean logic, and grade school math problems, with a small diffusion model outperforming a much larger autoregressive model in both efficiency and accuracy. In addition to that, DoT showcases promising self-correction abilities and beneftis from existing reasoningenhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning with diffusion language models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have had a profound impact on the entire field of artificial intelligence [42, 50], transforming our approach to addressing classical problems in natural language processing and machine learning. Among the most notable aspects of LLMs is their remarkable reasoning ability, which many researchers consider to be a representative emergent capability brought about by LLMs [53]. Chain-of-thought prompting (CoT) [54]), which generates a series of intermediate reasoning steps in autoregressive (AR) way, has emerged as a central technique to support complex reasoning processes in LLMs. Despite advancements, errors in intermediate CoT steps can lead to inaccurate answers [32], posing self-correction difficulties [25], and concerns about CoT\u2019s inefficiency have been highlighted in recent studies [7]. ", "page_idx": 0}, {"type": "text", "text": "Recently, diffusion models have attracted interest in text processing [33, 65, 69] as a result of success in the vision domain and distinctive modeling strengths over autoregressive models [34], offering potential benefits including global planning ability [59, 63], self correction [23] and efficiency [37]. As part of the research community effort, pre-trained diffusion language models such as Plaid [18] and SEDD [37] have shown significant progress in text generation capabilities. Although they have not yet attained the scale and capabilities of existing proprietary autoregressive LLMs like GPT-4 [42], these models have demonstrated performance on par with GPT2 [4] and the scaling law [27] in diffusion language models have been highlighted in Plaid. As a result, it becomes pertinent to explore the following question: can diffusion language models also leverage the CoT-style technique to gain enhanced complex reasoning abilities? ", "page_idx": 0}, {"type": "text", "text": "This work presents a preliminary study on this question. We propose Diffusion of Thought (DoT), an inherent chain-of-thought method tailored for diffusion models. In essence, DoT progressively updates a sequence of latent variables representing thoughts in the hidden space, allowing reasoning steps to diffuse over time in parallel. We also introduce a multi-pass variant of DoT which focuses on generating one thought at a time to compensate for causal bias. To condition on complex queries, instead of using gradient-based classifier guidance [18, 33], DoT trains and samples from the denoising model using the classifier-free guidance as in Gong et al. [15], to provide more reliable controlling signals on exact tokens. ", "page_idx": 1}, {"type": "text", "text": "Furthermore, to improve the self-correcting capability of the diffusion model, DoT integrates training-time sampling algorithms to learn to recover from errors originating from prior or current reasoning steps. This feature offers a fresh angle on the issue of error accumulation [25, 32] inherent in autoregressive models. Finally, we adapt a conditional ODE Solver [39] for DoT during inference time to accelerate the ", "page_idx": 1}, {"type": "image", "img_path": "G0v0TxX01N/tmp/26d2e2819a62745beb2373ac63b8b2e39f8f1954f0b1a4309d88d04e05877bb0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Illustration of reasoning approaches. (a) Answer-only and (b) CoT generate left-toright tokens by prompting autoregressive language model. (c) Implicit CoT replaces horizontal reasoning (CoT) with vertical reasoning from shallow layer to deep layer [7]. (d) DoT generates reasoning path along with the diffusion timesteps. ", "page_idx": 1}, {"type": "text", "text": "inference of continuous diffusion models. We show DoT enjoys flexibility in trading off computation (reasoning time) and performance as more complex problems may necessitate increased computation in reasoning [2, 54]. ", "page_idx": 1}, {"type": "text", "text": "From a methodological standpoint, DoT shares similarities with the recently proposed Implicit CoT approach [7], where the latter learns thoughts in hidden states across transformer layers to improve the time efficiency of autoregressive CoT generation. A schematic illustration of CoT, Implicit CoT, and DoT can be found in Figure 1. ", "page_idx": 1}, {"type": "text", "text": "The main contributions of our paper are threefold: ", "page_idx": 1}, {"type": "text", "text": "1. We first introduce the reasoning technique for diffusion models (DoT), and showcase its advantages in simple reasoning tasks (digit multiplication and boolean logic) when compared to autoregressive CoT and Implicit CoT. DoT achieves up to $27\\times$ speed-up without performance drop (\u00a74.2).   \n2. We further adapt DoT to continuous and discrete diffusion base models, and introduce two training-time sampling algorithms to improve its self-correction ability. DoT exhibits superior performance compared to GPT2 with CoT on grade school math problems, enabling a small diffusion model to outperform a $4.6\\mathrm{x}$ larger autoregressive model, showing the potential of text diffusion models for complex reasoning (\u00a74.3).   \n3. Our analysis demonstrates the flexibility of DoT in the trade-off between reasoning time and performance (\u00a74.4), and showcases DoT\u2019s self-correction capability (\u00a74.6). We also find that self-consistency decoding can further improve DoT and its multi-pass variant (\u00a74.5). ", "page_idx": 1}, {"type": "text", "text": "Although it is challenging for current pre-trained diffusion language models to directly compete with LLMs that are hundreds of times larger in parameter size, our study emphasizes the possibility of their complex reasoning abilities and highlights the substantial potential in developing LLMs that go beyond the autoregressive paradigm. We release all the codes at https://github.com/HKUNLP/diffusionof-thoughts. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "This section introduces key concepts and notations in diffusion models for text generation. Detailed formulations and derivations are provided in Appendix A. ", "page_idx": 1}, {"type": "text", "text": "A typical diffusion model contains the forward and reverse process. For each forward step $q(\\mathbf{z}_{t}|\\mathbf{z}_{t-1})$ , we gradually inject noise into the data representation $\\mathbf{z}_{t-1}$ from the last timestep to obtain $\\mathbf{z}_{t}$ . Here $t=1,2,...,T$ and the larger $t$ corresponds to noisier data. For reverse process, the ultimate goal is to recover the original $\\mathbf{z}_{\\mathrm{0}}$ by denoising $\\mathbf{z}_{t}$ : $\\begin{array}{r}{p_{\\theta}(\\mathbf{z}_{0:T}):=p(\\mathbf{z}_{T})\\prod_{t=1}^{T}p_{\\theta}(\\mathbf{z}_{t-1}|\\mathbf{z}_{t})}\\end{array}$ . We model the learning process $p_{\\theta}\\big(\\mathbf{z}_{t-1}\\big|\\mathbf{z}_{t}\\big)$ using the proposed diffusion model $\\mathbf{z}_{\\theta}(\\mathbf{z}_{t},t)$ . ", "page_idx": 2}, {"type": "text", "text": "Previous text generation using diffusion models almost contains two categories: (1) Continuous diffusion models such as Diffusion-LM [33], which relies on a mapping function between the real values and feasible integral point; (2) Discrete diffusion models like D3PM [1], which directly formulate the problem as the integer program. Continuous diffusion models map the discrete text w into a continuous space through an embedding function $\\operatorname{EMB}(\\mathbf{w})$ , and its inverse operati\u221aon is called rounding. The forward perturbations are applied according to $q(\\mathbf{z}_{t}|\\mathbf{z}_{t-1})\\;=\\;$ $\\dot{\\mathcal{N}}(\\mathbf{z}_{t};\\sqrt{1-\\beta_{t}}\\mathbf{z}_{t-1},\\beta_{t}\\mathbf{I})$ , where $\\beta_{t}~\\in~(0,\\bar{1})$ represents different scales of the Gaussian noise. Plaid [18] is a continuous diffusion language model trained from scratch on 314B tokens with 1024 context size. It is currently the largest scale diffusion language model with 1.3B parameters. In the case of discrete diffusion models, each $\\mathbf{z}_{t}$ is represented as a discrete random variable using one-hot vectors in $\\{0,1\\}^{K}$ , where $K$ denotes the vocabulary size. They define $q\\big(\\mathbf{z}_{t}|\\mathbf{z}_{t-1}\\big)$ through a transition matrix, making it a point mass with probability on an absorbing state [MASK] or a uniform distribution over the vocabulary size. SEDD [37] is a recently trained-from-scratch discrete diffusion language model with small and medium size similar to GPT2. ", "page_idx": 2}, {"type": "text", "text": "For sequence-to-sequence (seq2seq) generation, which involves a pair of sequences $\\mathbf{w}^{x}$ and $\\mathbf{w}^{y}$ , DiffuSeq [15] treats these two sequences as a single one $\\mathbf{w}^{z}\\,=\\,\\mathbf{w}^{[x;y]}$ and uses a left-aligned mask $[\\mathbf{0};\\mathbf{1}]$ during the forward and reverse diffusion process to distinguish them. Unlike traditional diffusion models that corrupt the entire $\\mathbf{z}_{t}$ , DiffuSeq only adds noise to those entries with the mask value of 1 (e.g., $\\mathbf{y}_{t}$ ). This modification, termed partial noising, tailors diffusion models for conditional language generation, and set a difference between the gradient-based token guidance in [33] and [18]. ", "page_idx": 2}, {"type": "text", "text": "3 Diffusion-of-Thoughts ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we begin with an overview of our method and its relationship with other reasoning paradigms (\u00a73.1). We then introduce Diffusion-of-Thoughts (DoT) as well as its multi-pass variant $\\mathrm{\\dot{(DoT^{MP}}}$ ; $\\S3.2)$ , as illustrated in Figure 2. Following this, we outline the implementation of our training (\u00a73.3) and inference (\u00a73.4) protocols. ", "page_idx": 2}, {"type": "text", "text": "3.1 Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Without loss of generality, we use the mathematical problem-solving task as our running example. A problem statement and its correct answer are denoted as s and a, respectively. We employ a language model with parameters $\\theta$ , represented as $p_{\\theta}^{L M}$ , to find the solution for each problem. For regular usage of language models without Chain-of-Thoughts $(\\mathrm{CoT})$ , the final answer a is generated directly as $\\overline{{\\mathbf{a}}}\\sim p_{\\theta}^{L M}\\overline{{(\\mathbf{a}|\\mathbf{s})}}$ . The CoT approach introduces meaningful intermediate steps or rationales $\\mathbf{r}_{1},\\ldots,\\mathbf{r}_{n}$ for language models to bridge s and a, resulting in the output $\\mathbf{a}\\sim p_{\\theta}^{L M}(\\mathbf{a}|\\mathbf{s},\\mathbf{r}_{1\\dots n})$ . For implicit CoT [7], the hidden representations of rationales $\\mathbf{z}_{1},\\ldots..\\mathbf{z}_{n}$ are distilled into transformer layers, leading to a $\\mathbf{\\sigma}_{\\mathsf{l}}\\sim p_{\\theta}^{i C o T}(\\mathbf{a}|\\mathbf{\\dot{s}},\\mathbf{z}_{1\\dots n})$ . Similarly but differently, for DoT, these representations are distributed over diffusion timestep $t$ as $\\mathbf{a}\\sim p_{\\theta}^{D o T}(\\mathbf{a}|\\mathbf{s},\\mathbf{z}_{t})$ , where $\\mathbf{z}_{t}$ corresponds exactly to the noised data in diffusion models. ", "page_idx": 2}, {"type": "text", "text": "3.2 Modeling ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We begin by observing the gradient-based token guidance fails to do accurate conditioning as the model cannot exactly recover each conditioning token (see Table 2). This is vital, especially in mathematical reasoning, as it is expected to perform reasoning based on exact tokens (e.g., numbers) in the problem statement, rather than more compact gradient signals. For this, we adopt DiffuSeqstyle [15] classifier-free conditioning during the fine-tuning of Plaid, where all rationales are generated by the backward diffusion process in parallel, with all the conditional tokens fixed as still. Specifically, the problem context $s$ is concatenated with the rationales $\\mathbf{r}_{1\\ldots n}$ during training and sampling, while the noise is only partially imposed to the rationale part in $\\mathbf{r}_{1\\ldots n}$ , keeping s anchored as the condition. ", "page_idx": 2}, {"type": "image", "img_path": "G0v0TxX01N/tmp/264a24a5c309c61e1480d52bc66b3faa50cf5be441b4c1b4ad7ec28c68905046.jpg", "img_caption": ["Figure 2: Demonstration of DoT pipeline. DoT diffuses all possible thoughts across diffusion timestep t. Multi-pass DoT disentangles each rationale and introduces causal bias. The stacked circles stand for the marginalization over other potential reasoning paths, which is implicitly carried out during the training of diffusion models. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "We further propose a multi-pass (MP) variant of DoT, denoted as $\\mathrm{DoT^{MP}}$ , which generates rationales in a thought-by-thought paradigm. This method disentangles the generation of multiple rationales and introduces casual inductive bias such that later rationale can be guided by stronger condition signals obfy $\\mathbf{r}_{1}\\,\\sim\\,p_{\\theta}^{D o T}(\\mathbf{r}_{1}|\\mathbf{s},\\mathbf{z}_{t}^{r_{1}})$ , t hweh geeren $\\mathbf{z}_{t}^{r_{1}}$ ioisn .t hSep encoiifsiceadl lvy,e icnt otrh er efpirrset speanstsa,t iwoen  goef $\\mathbf{r}_{1}$ aitne  tdhief ffuirssito rna timoondaelle. Then $\\mathbf{r}_{1}$ is connected to s as the condition $\\lbrack\\mathbf{s};\\mathbf{r}_{1}]$ to get $\\mathbf{r}_{2}\\sim p_{\\theta}^{D o T}(\\mathbf{r}_{2}|[\\mathbf{s};\\mathbf{r}_{1}],\\mathbf{z}_{t}^{r_{2}})$ , and then we have $\\lbrack\\mathbf{s};\\mathbf{r}_{1};\\mathbf{r}_{2}]$ . Through multiple iterations, we can get the final answer: a $\\sim p_{\\theta}^{D o T}(\\mathbf{a}|[\\mathbf{s};\\mathbf{r}_{1};...;\\mathbf{r}_{n}],\\mathbf{z}_{t}^{r_{n}})$ . ", "page_idx": 3}, {"type": "text", "text": "3.3 Training ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Scheduled sampling Diffusion models have intrinsic self-correcting capability through the multistep denoising process. To further improve their self-correcting ability, we design a scheduled sampling [3] mechanism tailored for diffusion models such that self-generated error thoughts in previous timesteps are exposed and corrected during the training stage. Formally, for any timesteps $s,t,u$ that satisfy $1<s<t<u<T$ , $\\mathbf{z}_{t}$ is sampled from the forward distribution $q\\left(\\mathbf{z}_{t}\\mid\\mathbf{z}_{0}\\right)$ in the training stage while during inference it is sampled from $q(\\mathbf{z}_{t}\\mid\\mathbf{z}_{\\theta}\\left(\\mathbf{z}_{u};u\\right))$ instead, where $\\mathbf{z}_{\\theta}$ is a denoiser neural network that reparameterizes $\\mathbb{E}_{q}[\\mathbf{z}_{0}|\\mathbf{z}_{t}]$ . The presence of such exposure bias may impede the model\u2019s ability to recover from erroneous thoughts during the generation process as the model $\\mathbf{z}_{\\theta}$ has only been trained on corruptions $\\mathbf{z}_{t}$ diffused from oracle data. To mitigate this problem, we mimic the inference stage with probability $\\epsilon_{i}$ during training depending on the current training step $i$ , and $\\epsilon_{i}$ linearly decays from 1 to $\\epsilon_{m i n}$ . Specifically, for time-step $t$ , we randomly sample a former time-step $u\\in\\{t+1,\\dots,T\\}$ , obtain $\\mathbf{z}_{u}$ by forward noising and perform a model forward pass to get a predicted $\\hat{\\mathbf{z}}_{0}=\\mathbf{z}_{\\theta}\\left(\\mathbf{z}_{u};u\\right))$ . $\\mathbf{z}_{t}$ is then sampled from $q(\\mathbf{\\bar{z}}_{t}\\mid\\hat{\\mathbf{z}}_{0})$ to replace the regular one in loss calculation. Compared with scheduled sampling for autoregressive models, such a mechanism in DoT helps the model to recover from errors by considering global information instead of relying on the left-side tokens. ", "page_idx": 3}, {"type": "text", "text": "Coupled sampling In $\\mathrm{DoT^{MP}}$ , correct previous thoughts are given in the training stage, which is not given during inference. Similar to auto-regressive decoding, $\\mathrm{DoT}^{\\mathrm{MP}}$ may suffer from error accumulation during the thought-by-thought generation process. To enhance the self-correction ability of $\\mathrm{DoT^{MP}}$ , we propose a coupled sampling mechanism by adding noise not only to the current thought but also to previous thoughts during training with some probability. For instance, the previous sequence $\\begin{array}{r}{{\\bf z}_{0}=\\mathrm{EMB}\\big(\\big[{\\bf s};{\\bf r}_{1}\\big]\\big)}\\end{array}$ will be modified to ${\\bf z}_{0}=\\mathrm{EMB}\\big([{\\bf s};{\\bf r}_{1};{\\bf r}_{2}]\\big)$ , with the partial noise being applied to $\\left[\\mathbf{r}_{1};\\mathbf{r}_{2}\\right]$ rather than just the last rationale $\\mathbf{r}_{2}$ . Therefore, the model learns to be robust to errors in $\\mathbf{r}_{1}$ when predicting $\\mathbf{r}_{2}$ , which better aligns with the inference stage. The new $\\mathbf{z}_{\\mathrm{0}}$ will be reparameterized into $\\mathbf{z}_{t}$ as before and other procedures keep the same. ", "page_idx": 3}, {"type": "text", "text": "Training objective Given a set of training data for problem-solving tasks of size $D$ : $\\{\\mathbf{s}^{j},\\mathbf{r}_{1\\dotsc n}^{j},\\mathbf{a}^{j}\\}_{j\\in D}$ , we have two training settings for DoT models: one is training from scratch, while the other is fine-tuning from the pre-trained diffusion model. In both training settings, we share the same training objective. For example, the objective is to minimize the negative variational lower bound $\\mathcal{L}_{\\mathrm{VLB}}\\big(\\mathbf{w}^{z}\\big)$ in continuous diffusion models: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{VLB}}(\\mathbf{w}^{z})=\\!\\!\\mathbb{E}_{q(\\mathbf{z}_{0}|\\mathbf{w}^{z})}\\!\\left[\\underbrace{\\log\\frac{q(\\mathbf{z}_{T}|\\mathbf{w}^{z})}{p_{\\theta}(\\mathbf{z}_{T})}}_{\\mathrm{Prior\\;loss}}+\\underbrace{\\mathcal{L}_{\\mathrm{VLB}}(\\mathbf{z}_{0})}_{\\mathrm{Diffusion\\;loss}}\\underbrace{-\\log p_{\\theta}(\\mathbf{w}^{z}|\\mathbf{z}_{0})}_{\\mathrm{Rounding\\;loss}}\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the rounding loss regularizes the embedding learning and the diffusion loss sums up the KL divergence of each time step $t$ with different weighting terms. Please refer to Appendix A for a detailed training objective formulation of continuous and discrete diffusion models. ", "page_idx": 4}, {"type": "text", "text": "3.4 Inference ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "One of the significant advantages of diffusion models is their inference flexibility. Naturally, more complex problems may necessitate increased computation in reasoning time [2, 54], which can be controlled by setting a larger backward timestep $T$ in DoT. However, continuous diffusion such as Plaid usually requires more timesteps, e.g., 4096 [18], to converge. To accelerate the sampling process of the continuous diffusion, we adapt the ODE Solver [38, 39] into a conditional form to fit the conditional training process (detailed in Appendix A.4). Moreover, sharing a similar idea of MBR [30], self-consistency [52] boosts the performance of CoT significantly by generating and aggregating multiple samples. In the context of diffusion models, we can also expect its potential improvement using self-consistency, thanks to their ability to naturally produce diverse responses [15]. After sampling $m$ times to obtain multiple reasoning pathways $(\\mathbf{r}_{i;1...n},\\mathbf{a}_{i})$ from DoT, self-consistency involves marginalizing over $\\mathbf{r}_{i;1...n}$ by taking a majority vote over ${\\bf a}_{i}$ , i.e., arg maxa $\\begin{array}{r}{\\sum_{i=1}^{m}\\mathbb{1}(\\mathbf{a}_{i}=a)}\\end{array}$ We consider this as the most \u201cconsistent\u201d answer among the candidate set of $m$ answers. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We conduct experiments on both simple multi-digit multiplication and boolean logic reasoning as well as complex grade school math problems, to explore the reasoning paradigm in diffusion models. ", "page_idx": 4}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Datasets and Metrics. Following Deng et al. [7], we employ the four-digit $(4\\times4)$ and five-digit $\\mathrm{(5\\times5)}$ multiplication problems from the BIG-bench benchmark [49], known to be challenging for LLMs to solve without CoT. Given that arithmetic reasoning is just one type of the reasoning ability, we also incorporate a boolean logical reasoning task [68]. For more complex tasks, grade school math problems require both language understanding and mathematical reasoning, so we adopt the widely-used GSM8K dataset [6]. We use the augmented training data from Deng et al. [7] and keep all original test sets unchanged. The statistics are listed in Appendix B.1. For both datasets, we use accuracy to measure the exact match accuracy of predicting the final answer, and throughput to measure the number of samples processed per second (it/sec) during inference with a batch size of 1. ", "page_idx": 4}, {"type": "text", "text": "Base Models. When training from scratch, we follow DiffuSeq2to use a 12-layer Transformer [51] encoder with similar size as GPT2-small (124M). We also use Plaid3(1.3B) [18], SEDD-small4 (170M) and SEED-medium (424M) [37] as pre-trained diffusion language models for further finetuning. Both Plaid and SEDD are pre-trained on OpenWebText [10, 13], which is similar to that in GPT2, and the pre-training perplexity of Plaid and SEDD-small is on par with GPT2-small. ", "page_idx": 4}, {"type": "text", "text": "Baselines. We consider Answer-only and CoT as reasoning paradigms for comparison. Another important baseline is Implicit CoT [7], which distills thoughts into transformer layers to accelerate CoT reasoning. We use GPT-2 [4] at various scales (i.e., small 124M, medium 355M, and large 774M) as model baselines, known as conventional autoregressive language models. We mainly consider fine-tuning the model due to the relatively small model size, but we also consider prompting the strong commercial LLM ChatGPT gpt-3.5-turbo-1106 using CoT few-shot demonstrations for completeness. We use 5-shot in the few-shot setting. ", "page_idx": 4}, {"type": "table", "img_path": "G0v0TxX01N/tmp/4dbc29ee4804e4911e8a2ab8e91b98d840caf2f29968e8a74c80e2d956c1d2cc.jpg", "table_caption": ["Table 1: The main results on different problem-solving reasoning tasks. Acc $(\\uparrow)$ is to measure the exact match accuracy of the predicted final answer. Throughput $(\\uparrow)$ measures the number of samples processed per second during test with batch size equals to 1. The baseline results for Mult. and GSM8K datasets are taken from the implicit CoT paper [7] and have been validated for reproducibility by us. Bracketed numbers indicate the self-consistency results. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Implementation Details. During tokenization, we treat all the digits as individual tokens. For $\\mathrm{Do}\\bar{\\mathrm{T}}^{\\mathrm{MP}}$ , we append a special token $\\mathrm{<EOS>}$ to the last thought, so when the model generates a thought followed by $\\tt{-E O S>}$ , it stops generating further, which enables the model to decide the number of rationales dynamically. We conduct all the experiments on 8 NVIDIA V100-32G GPUs. During training, we set $\\epsilon_{m i n}$ to be 0.95 as we find decreasing the probability of oracle demonstration hinders model training. We choose coupled sampling $\\gamma=0.01,k=1$ and self-consistency $m=20$ . Following Plaid, we also adopt self-conditioning [5] during training. During inference, we set both the temperature of the score and output logit to 0.5 to sharpen the predicted output distribution while maintaining the ability to generate diverse samples. The sampling timesteps $T$ is dynamic. By default, we set it to be 64. Considering that simple tasks do not necessitate an excessively large number of steps, we opt to reduce $T$ while ensuring there is no notable performance drop. Other details are in Appendix B.3. ", "page_idx": 5}, {"type": "text", "text": "4.2 Results on Digit Multiplication and Boolean Logic ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first train DoT for digit multiplication tasks and a boolean logical reasoning task as the preliminary investigation, as shown in the left part of Table 1. We observe that neither ChatGPT nor the distilled Implicit CoT model can reach $100\\%$ accuracy. GPT-2 can be fine-tuned to achieve high accuracy but sacrifices throughput during CoT. Interestingly, DoT can attain $100\\%$ accuracy for these tasks while maintaining significant throughput with diffusion sampling steps set at 1 for multiplication datasets and 2 for the boolean logical dataset, achieving maximum $27\\times$ speed-up compared to GPT-2. This preliminary finding indicates that DoT performs well in modeling exact math computation or boolean logic reasoning and benefits from its computational efficiency. ", "page_idx": 5}, {"type": "table", "img_path": "G0v0TxX01N/tmp/ab017ed67b239a549afef432e22fc65e2bdc3bd62eceee62258e7b55ddc10ac6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "G0v0TxX01N/tmp/0af81c5f5e526ea2b96bfe41092e48740ede16ac670c0a856ea8d145db5c0b8c.jpg", "img_caption": ["Figure 3: The effectiveness of ODE solver in speedup inference of Plaid DoT. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.3 Results on Grade School Math ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now move on to a much more complex grade school math task GSM8K as shown in the right part of Table 1. We first consider training DoT from scratch as in the previous tasks, but we are only able to achieve an accuracy of around $5\\%$ , which is much lower than the fine-tuned version of GPT-2. This indicates the pre-trained natural language understanding capability is vital for grade school math. Once DoT is extended based on the pre-trained diffusion language models Plaid and SEDD, the performance is significantly improved after fine-tuning, where the DoT based on SEDD-medium outperforms similar-sized GPT2-medium with CoT by around $10\\%$ . Additionally, multi-pass DoT, with casual bias, performs slightly better than single-pass one on Plaid, while the latter is more efficient. The performance gap between SEDD and Plaid also highlights the importance of the training objective in pretraining diffusion LMs. Finally, we find that self-consistency further yields substantial improvements in DoT models owing to the diverse generations of diffusion model (\u00a74.5). ", "page_idx": 6}, {"type": "text", "text": "We further explore several alternatives and conduct an ablation study as in Table 2 when fine-tuning Plaid. As discussed above, continuing pre-training Plaid using the GSM8K-augmented dataset and performing reasoning with gradient-based conditioning is not a good choice for fine-tuning diffusion LMs on downstream tasks, because reasoning tasks require more specific guidance. An example of groundtruth and recovered text is shown below, where bold words in the query part are incorrectly recovered: ", "page_idx": 6}, {"type": "text", "text": "Groundtruth: Two trains leave San Rafael at the same time. They begin traveling westward, both traveling for 80 miles. The next day, they travel northwards, covering 150 miles. What\u2019s the distance covered by each train in the two days? $\\ll2\\!*\\!80\\!=\\!160\\!\\gg\\!$ $\\ll\\lvert50\\!*\\!2\\!=\\!300\\!\\gg\\!$ $\\ll300\\!+\\!160\\!=\\!460\\!\\gg$ $\\ll\\!460/2\\!\\!=\\!230\\!\\gg$ #### 230   \nRecovered Text: Three trains leave San Juan at the same time. They start traveling westward, both traveling for 80 miles. The next day, they travel southward, covering 150 miles. What\u2019s the distance covered by each train in the two days? $\\ll3\\!*\\!80\\!\\!=\\!180\\!\\gg$ $\\!\\ll\\!180\\!+\\!80\\!+\\!150\\!=\\!340\\!*$ $\\ll\\!340/30\\!\\!=\\!\\!12.5\\!\\!\\gg\\!\\!\\!$ #### 12.5 ", "page_idx": 6}, {"type": "text", "text": "We can see there are three recovered query tokens that exhibit minor differences due to soft gradient guidance, causing interference with the model\u2019s comprehension of the problem. The ablation of two sampling strategies proposed in $\\S3.3$ showcases their effectiveness. This provides evidence that better denoising models are trained using our training-time sampling strategies, allowing DoT models to self-correct more effectively during inference. Further analysis about self-correction is listed in $\\S4.6$ . In Figure 3, we further show the conditional ODE solver substantially speeds up the inference of continuous diffusion model Plaid, ensuring a decent performance with only 8 generation timesteps. ", "page_idx": 6}, {"type": "text", "text": "4.4 Reasonability-efficiency Trade-off ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The community has devoted substantial efforts to improve the reasoning capabilities of left-to-right language models, such as refining instructions [31, 67], finding better demonstrations [9, 55, 58], and designing elaborate decoding algorithm [43, 56, 57]. Non-autoregressive diffusion models naturally provide another simple way to enhance reasoning by allocating more timesteps during inference, albeit at the expense of efficiency. We show such efficiency trade-off in Figure 4(a), where we measure the reasoning steps as the average number of calling the model forward function for all the instances from the test set. For CoT and Implicit CoT baselines, we treat reasoning steps as the average number of output tokens for all the test instances5. ", "page_idx": 6}, {"type": "image", "img_path": "G0v0TxX01N/tmp/1cb926d8dfac37a4fd4e615ee676be2b68d1992999cb91eeb266642e6a5c62f7.jpg", "img_caption": ["Figure 4: (a) Accuracy over reasoning steps using various methods. We measure the reasoning steps as the average number of calling the model forward function for instances from the test set. DoT provides a flexible way to balance accuracy and efficiency through the reasoning steps. (b) Absolute accuracy improvement versus samples in self-consistency per instance on the GSM8K dataset with Plaid DoT. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Given a small budget of reasoning steps (e.g., 1 or 2) on simpler tasks such as $5\\!\\times\\!5$ , both DoT-Plaid and DoT-SEDD already have an accuracy of $100\\%$ , and no more reasoning steps are needed. For such cases of simple tasks, only a little computation cost is required for our method. For complex tasks such as GSM8K, we find DoT performance can continuously improve by allowing more reasoning steps, which indicates DoT can be efficient if we can sacrifice performance in certain scenarios. Specifically, DoT-SEDD-medium outperforms autoregressive CoT-GPT2-medium when we allocate 32 generation timesteps, and the performance continues improving when we increase the timesteps. In comparison, CoT and Implicit CoT with the autoregressive model are hard to be more efficient given their nature of token-by-token prediction. Overall, with DoT, we can flexibly control the trade-off between efficiency and performance for tasks with different difficulty levels. ", "page_idx": 7}, {"type": "text", "text": "4.5 Self-consistency in DoT ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Figure 4(b) shows the effectiveness of the self-consistency mechanism for Plaid DoT and its variant. We can see self-consistency improves both DoT and $\\boldsymbol{\\mathrm{DoT}}^{\\mathrm{MP}}$ , which is in line with the effectiveness of self-consistency for auto-regressive models [52]. From Table 1, SEDD DoT is also significantly improved by self-consistency. This beneftis from the diversity generation in DoT. We observe that DoT can generate diverse reasoning paths, such as $<\\!3{*3}\\!\\!=\\!9\\!>\\!<\\!9{*6}\\!0{=}540\\!>$ and $<\\!3{*}60{=}180{>}{<}180{*}3{=}540{>}$ for the same question, providing cross-validation when selecting the most \u201cconsistent\u201d answer. Note that different from autoregressive models, where diversity usually relies on decoding algorithms [8, 22], the natural advantage of the diffusion models is to generate different sentences with different random noises at each timestep. ", "page_idx": 7}, {"type": "text", "text": "4.6 Self-correction in DoT ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we provide several cases in Table 3 to show the self-correction ability of Plaid DoT, which acts as a distinct difference between diffusion models and autoregressive models. In the first case, we can see the model figures out all the correct thoughts together with only a single reasoning step (i.e., a single calling of the model forward function), and obtains the correct final answer in the second step. This mirrors how humans think in both fast and slow modes [26]. In the second case where the problem is slightly harder, the model cannot give concrete thoughts in the first step but can still produce the correct answer through the later \u201cslow\u201d thinking process. We can see the solution framework, roughly outlining how the task will be carried out, is established at the very beginning, and then the subsequent work is for refining and improving, which is also similar to how human performs a complex task. Interestingly, in DoT, the correct thoughts may not appear in a left-to-right paradigm as in the traditional chain-of-thought process. The third case serves as compelling evidence to illustrate this distinctive nature of diffusion-of-thought and how it diverges from the chain-of-thought approach. In step 4 the model has a wrong intermediate thought $<2{*}3{=}4>$ with the latter thoughts and final answer computed correctly first. In the next step, the error in the wrong intermediate thought is fixed, which suggests both prior and latter thoughts can help in the prediction of the current thought. Furthermore, from these three cases, we observed that the model tends to maintain its prediction after it considers the answer to be complete. This suggests we can further enhance the inference efficiency by incorporating mechanisms such as early exit [16], and easier tasks can get earlier exits as observed in Table 3. ", "page_idx": 7}, {"type": "table", "img_path": "G0v0TxX01N/tmp/79d001e8802265792f748f38bd01d1337d65b5a67a1c9acd300221bc8063076c.jpg", "table_caption": ["Table 3: Cases that show the predictions of Plaid DoT at each time-step $t$ with $T{=}8$ on the GSM8K test set. The incorrect thoughts are marked in bold red and we omit some correct predictions when $t<4$ . The difficulty level of the questions increases from left to right. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "5.1 Diffusion Models for Text ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Building upon advancements in diffusion models for image generation [21, 45], text continuous diffusion [15, 33] employs an embedding function to transform discrete text into the continuous space. Besides, discrete diffusion models [1, 23] directly introduce discrete noise to accommodate the discrete nature of texts, demonstrating significant potential [37, 65]. Numerous studies have shown that diffusion models can efficiently generate diverse texts [11, 14], and achieve competitive performance in various sequence-to-sequence NLP tasks, including machine translation [60, 61], summarization [62], code generation [44], and style transfer [24]. In this work, we explore diffusion model for mathematical reasoning tasks. ", "page_idx": 8}, {"type": "text", "text": "5.2 Pre-train and fine-tune Diffusion LMs ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The pre-training and fine-tuning paradigm, while a familiar concept in NLP before the era of prompting methods [36], remains relatively under-explored for diffusion language models. Prior efforts include initializing diffusion models with pre-trained masked language models such as BERT [20] and RoBERTa [66] and XLM-RoBERTa [59]. GENIE [35] adopts paragraph denoising to train encoder-decoder models, proving beneficial for summarization tasks. Plaid [18] and SEDD [37] are pioneers in pre-train diffusion language models from scratch, attaining comparative or better perplexity scores over GPT-2 [4]. To the best of our knowledge, we are the first to explore the fine-tuning of a pre-trained diffusion language model for reasoning tasks. ", "page_idx": 8}, {"type": "text", "text": "5.3 Reasoning Paradigms ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Large language models usually excel in performing system-1 [47] tasks that are processed quickly and intuitively by humans but struggle in system-2 tasks, which require deliberate thinking [4, 48, 54]. The chain-of-thought reasoning paradigm [31, 41, 54] has been widely employed to elicit reasoning abilities and can be further improved with various techniques. For instance, self-consistency [52] samples a diverse set of reasoning paths and selects the most consistent answer, while tree-ofthought [57] achieves different reasoning paths by tree search. Despite these advancements, errors introduced in intermediate CoT steps can lead to inaccurate answers [32], posing difficulties in self-correction [25]. Moreover, there are concerns about the inefficiency of CoT [7]. From the architecture perspective, we explore diffusion model as an alternative paradigm for reasoning. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and Limitation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose diffusion-of-thought (DoT), integrating CoT reasoning with continuous diffusion models. We thoroughly evaluate DoT on representative mathematical reasoning tasks in various aspects, including their flexible control of reasoning efficiency, self-correction capability, and the ability to generate diverse reasoning paths. Considering pre-trained diffusion models are still in their early stages, particularly in terms of model scales compared to the more extensively studied autoregressive language models, our study presents an initial exploration into the reasoning ability of current diffusion language models. A notable limitation of DoT is its requirement for additional training to achieve accurate reasoning. With more powerful pre-trained diffusion models, we anticipate DoT can attain comparative or better generalization capabilities of auto-regressive language models while removing the need for specialized training. Moreover, extending the standard Transformer to other variants [17] is also a viable direction to further improve inference efficiency. Besides, the diffusion training techniques employed in this work are general and applicable to other tasks beyond mathematical reasoning. Extending our training recipes of diffusion language models to further scaled setups such as multi-task instruction tuning and other modalities [19, 64], is an interesting avenue for future research. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 17981\u201317993, 2021.   \n[2] Andrea Banino, Jan Balaguer, and Charles Blundell. Pondernet: Learning to ponder. In 8th ICML Workshop on Automated Machine Learning (AutoML), 2021.   \n[3] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 1171\u20131179, 2015.   \n[4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.   \n[5] Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. ArXiv preprint, abs/2208.04202, 2022.   \n[6] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. ArXiv preprint, abs/2110.14168, 2021.   \n[7] Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. Implicit chain of thought reasoning via knowledge distillation. ArXiv preprint, abs/2311.01460, 2023.   \n[8] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proc. of ACL, pages 889\u2013898. Association for Computational Linguistics, 2018.   \n[9] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. In The Eleventh International Conference on Learning Representations, 2022.   \n[10] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.   \n[11] Zhujin Gao, Junliang Guo, Xu Tan, Yongxin Zhu, Fang Zhang, Jiang Bian, and Linli Xu. Difformer: Empowering diffusion model on embedding space for text generation. ArXiv preprint, abs/2212.09412, 2022.   \n[12] Daniel T. Gillespie. Approximate accelerated stochastic simulation of chemically reacting systems. Journal of Chemical Physics, 115:1716\u20131733, 2001. URL https://api.semanticscholar.org/CorpusID: 5109777.   \n[13] Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019.   \n[14] Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and Lingpeng Kong. DiffuSeq-v2: Bridging discrete and continuous text spaces for accelerated Seq2Seq diffusion models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9868\u20139875. Association for Computational Linguistics, 2023.   \n[15] Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and Lingpeng Kong. DiffuSeq: Sequence to sequence text generation with diffusion models. In International Conference on Learning Representations, ICLR, 2023.   \n[16] Alex Graves. Adaptive computation time for recurrent neural networks. ArXiv preprint, abs/1603.08983, 2016.   \n[17] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.   \n[18] Ishaan Gulrajani and Tatsunori B Hashimoto. Likelihood-based diffusion language models. ArXiv preprint, abs/2305.18619, 2023.   \n[19] William Harvey and Frank Wood. Visual chain-of-thought diffusion models. arXiv preprint arXiv:2303.16187, 2023.   \n[20] Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, and Xipeng Qiu. Diffusionbert: Improving generative masked language models with diffusion models. ArXiv preprint, abs/2211.15029, 2022.   \n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.   \n[22] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In Proc. of ICLR. OpenReview.net, 2020.   \n[23] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr\u00e9, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 12454\u201312465, 2021.   \n[24] Zachary Horvitz, Ajay Patel, Chris Callison-Burch, Zhou Yu, and Kathleen McKeown. Paraguide: Guided diffusion paraphrasers for plug-and-play textual style transfer. ArXiv preprint, abs/2308.15459, 2023.   \n[25] Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. ArXiv preprint, abs/2310.01798, 2023.   \n[26] Daniel Kahneman. Thinking, fast and slow. 2011.   \n[27] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. ArXiv preprint, abs/2001.08361, 2020.   \n[28] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 34:21696\u201321707, 2021.   \n[29] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, Proc. of ICLR, 2015.   \n[30] Philipp Koehn. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP, pages 388\u2013395. Association for Computational Linguistics, 2004.   \n[31] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. ArXiv preprint, abs/2205.11916, 2022.   \n[32] Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et al. Measuring faithfulness in chain-of-thought reasoning. ArXiv preprint, abs/2307.13702, 2023.   \n[33] Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B Hashimoto. Diffusion-lm improves controllable text generation. In Conference on Neural Information Processing Systems, NeurIPS, 2022.   \n[34] Chu-Cheng Lin, Aaron Jaech, Xin Li, Matthew R. Gormley, and Jason Eisner. Limitations of autoregressive models and their alternatives. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5147\u20135173. Association for Computational Linguistics, 2021.   \n[35] Zhenghao Lin, Yeyun Gong, Yelong Shen, Tong Wu, Zhihao Fan, Chen Lin, Nan Duan, and Weizhu Chen. Text generation with diffusion language models: A pre-training approach with continuous paragraph denoise. In International Conference on Machine Learning, pages 21051\u201321064. PMLR, 2023.   \n[36] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1\u201335, 2023.   \n[37] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion language modeling by estimating the ratios of the data distribution. ArXiv preprint, abs/2310.16834, 2023.   \n[38] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. In Conference on Neural Information Processing Systems, NeurIPS, 2022.   \n[39] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solve $^{++}$ : Fast solver for guided sampling of diffusion probabilistic models. ArXiv preprint, abs/2211.01095, 2022.   \n[40] Chenlin Meng, Kristy Choi, Jiaming Song, and Stefano Ermon. Concrete score matching: Generalized score matching for discrete data. Advances in Neural Information Processing Systems, 35:34532\u201334545, 2022.   \n[41] Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. ArXiv preprint, abs/2112.00114, 2021.   \n[42] OpenAI. Gpt-4 technical report. ArXiv preprint, abs/2303.08774, 2023.   \n[43] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. ArXiv preprint, abs/2303.11366, 2023.   \n[44] Mukul Singh, Jos\u00e9 Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, and Gust Verbruggen. CodeFusion: A pre-trained diffusion model for code generation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proc. of EMNLP, pages 11697\u201311708. Association for Computational Linguistics, 2023.   \n[45] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In Proc. of ICLR. OpenReview.net, 2021.   \n[46] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.   \n[47] Keith E. Stanovich and Richard F. West. Individual differences in reasoning: Implications for the rationality debate? Behavioral and Brain Sciences, 23:645 \u2013 665, 2000.   \n[48] Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed Huai hsin Chi, Denny Zhou, and Jason Wei. Challenging bigbench tasks and whether chain-of-thought can solve them. In Annual Meeting of the Association for Computational Linguistics, 2022.   \n[49] Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. Challenging BIG-bench tasks and whether chain-of-thought can solve them. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 13003\u201313051. Association for Computational Linguistics, 2023.   \n[50] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. ArXiv preprint, abs/2302.13971, 2023.   \n[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998\u20136008, 2017.   \n[52] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2022.   \n[53] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022.   \n[54] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv preprint, abs/2201.11903, 2022.   \n[55] Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. Self-adaptive in-context learning: An information compression perspective for in-context example selection and ordering. In Annual Meeting of the Association for Computational Linguistics, 2022.   \n[56] Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. Decomposition enhances reasoning via self-evaluation guided decoding. ArXiv preprint, abs/2305.00633, 2023.   \n[57] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Grifftihs, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. ArXiv preprint, abs/2305.10601, 2023.   \n[58] Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. Compositional exemplars for in-context learning. In International Conference on Machine Learning, 2023.   \n[59] Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, and Quanquan Gu. Diffusion language models can perform many tasks with scaling and instruction-finetuning. ArXiv preprint, abs/2308.12219, 2023.   \n[60] Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, and Mingxuan Wang. Dinoiser: Diffused conditional sequence learning by manipulating noises. ArXiv preprint, abs/2302.10025, 2023.   \n[61] Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, and Songfang Huang. Seqdiffuseq: Text diffusion with encoder-decoder transformers. ArXiv preprint, abs/2212.10325, 2022.   \n[62] Haopeng Zhang, Xiao Liu, and Jiawei Zhang. Diffusum: Generation enhanced extractive summarization with diffusion. ArXiv preprint, abs/2305.01735, 2023.   \n[63] Yizhe Zhang, Jiatao Gu, Zhuofeng Wu, Shuangfei Zhai, Joshua M. Susskind, and Navdeep Jaitly. PLANNER: Generating diversified paragraph via latent language diffusion model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[64] Zhuosheng Zhang, Aston Zhang, Mu Li, George Karypis, Alex Smola, et al. Multimodal chain-of-thought reasoning in language models. Transactions on Machine Learning Research, 2023.   \n[65] Lin Zheng, Jianbo Yuan, Lei Yu, and Lingpeng Kong. A reparameterized discrete diffusion model for text generation. ArXiv preprint, abs/2302.05737, 2023.   \n[66] Kun Zhou, Yifan Li, Wayne Xin Zhao, and Ji rong Wen. Diffusion-nat: Self-prompting discrete diffusion for non-autoregressive text generation. ArXiv preprint, abs/2305.04044, 2023.   \n[67] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. ArXiv preprint, abs/2211.01910, 2022.   \n[68] Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. In International Conference on Learning Representations, ICLR, 2024.   \n[69] Hao Zou, Zae Myung Kim, and Dongyeop Kang. A survey of diffusion models in natural language processing. ArXiv preprint, abs/2305.14671, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Derivations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Seq2Seq Modeling in DiffuSeq ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To implement the diffusion model in seq2seq generation, we inherit the design from DiffuSeq [14], which systematically defines the forward noising process and reverse denoising process on latent continuous space $\\mathbf{z}$ as two major components of the model. ", "page_idx": 14}, {"type": "text", "text": "Latent space configuration z. Following Li et al. [33], $\\mathbf{z}$ is constructed from an embedding function $\\operatorname{EMB}\\!\\left(\\mathbf{w}^{\\mathbf{z}}\\right)$ , which takes the discrete text $\\mathbf{w}^{z}$ as input. Particulatly, in Diffuseq [14], $\\mathbf{w}^{z}$ contains $\\mathbf{w}^{x}$ and $\\mathbf{w}^{y}$ where $\\mathbf{w}^{x}$ is the source sequence and $\\mathbf{w}^{y}$ is the target sequence. The relationship is defined as $\\mathbf{w}^{z}=\\mathbf{w}^{[x;y]}$ . They denote $\\mathbf{z}_{t}=\\mathbf{x}_{t}\\oplus\\mathbf{y}_{t}$ to simplify the wordings, where $\\mathbf{x}_{t}$ and $\\mathbf{y}_{t}$ represent parts of $\\mathbf{z}_{t}$ that belong to $\\mathbf{w}^{x}$ and $\\mathbf{w}^{y}$ , respectively. ", "page_idx": 14}, {"type": "text", "text": "Forward diffusion process $q(\\mathbf{z}_{t}|\\mathbf{z}_{t-1})$ and $q(\\mathbf{z}_{t}|\\mathbf{z}_{0})$ . The process of forward noising is to fractionally disrupt the content of input data $\\mathbf{z}_{\\mathrm{0}}$ , introduced as partial noising by Gong et al. [15]. It is achieved by only applying Gaussian noise to $\\mathbf{y}_{t}$ and preserving $\\mathbf{x}_{t}$ with a masking scheme, denoted as $\\mathbf{z}_{t}=[\\mathbf{x}_{t};\\mathbf{y}_{t}]$ with mask $[0;1]$ . ", "page_idx": 14}, {"type": "text", "text": "After the process of $f$ orward noising where $T$ -step forward random disturbance is applied, the $\\mathbf{z}_{\\mathrm{0}}$ is finally transformed into the partial Gaussian noise with $\\mathbf{y}_{T}\\sim\\mathcal{N}(0,\\mathbf{I})$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\nq(\\mathbf{z}_{t}|\\mathbf{z}_{t-1})=\\mathcal{N}(\\mathbf{z}_{t};\\sqrt{1-\\beta_{t}}\\mathbf{z}_{t-1},\\beta_{t}\\mathbf{I}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\nq(\\mathbf{z}_{1:T}|\\mathbf{z}_{0})=\\prod_{t=1}^{T}q(\\mathbf{z}_{t}|\\mathbf{z}_{t-1})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $t=1,2,...,T$ and $\\{\\beta_{t}\\in(0,1)\\}_{t=1}^{T}$ are the variance schedule. A reparameterization trick could be applied to the above process to attain a closed-form representation of sampling $\\mathbf{z}_{t}$ at any arbitrary time step $t$ . Let $\\alpha_{t}=1-\\beta_{t}$ and $\\begin{array}{r}{\\bar{\\alpha}_{t}=\\prod_{i=1}^{t}\\alpha_{i}}\\end{array}$ , the equation is reduced to: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{z}_{t}=\\sqrt{\\alpha_{t}}\\mathbf{z}_{t-1}+\\sqrt{1-\\alpha_{t}}\\epsilon_{t-1}=\\sqrt{\\alpha_{t}\\alpha_{t-1}}\\mathbf{z}_{t-2}+\\sqrt{1-\\alpha_{t}\\alpha_{t-1}}\\bar{\\epsilon}_{t-2}}\\\\ &{\\mathrm{~\\~\\}=\\ldots=\\sqrt{\\alpha_{t}}\\mathbf{z}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\epsilon_{t-1},\\epsilon_{t-2},\\cdot\\cdot\\cdot\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ and $\\epsilon$ merges all the Gaussians. In the end: ", "page_idx": 14}, {"type": "equation", "text": "$$\nq(\\mathbf{z}_{t}|\\mathbf{z}_{0})=\\mathcal{N}(\\mathbf{z}_{t};\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{z}_{0},(1-\\bar{\\alpha}_{t})\\mathbf{I})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A sqrt noise schedule is applied according to the Diffusion-LM [33], that is, $\\bar{\\alpha}_{t}=1-\\sqrt{t/T+s}$ with $s$ as a small constant at the start of the noise level. ", "page_idx": 14}, {"type": "text", "text": "Posterior $q\\big({\\bf z}_{t-1}\\big|{\\bf z}_{t},{\\bf z}_{0}\\big)$ . Derived by Bayes\u2019 rule, the posterior is given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\nq(\\mathbf{z}_{t-1}|\\mathbf{z}_{t},\\mathbf{z}_{0})=q(\\mathbf{z}_{t}|\\mathbf{z}_{t-1},\\mathbf{z}_{0})\\frac{q(\\mathbf{z}_{t-1}|\\mathbf{z}_{0})}{q(\\mathbf{z}_{t}|\\mathbf{z}_{0})}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Given the above relationship, the posterior is still in Gaussian form. After applying the Eq. (4) to it, the mean of $q\\big(\\mathbf{z}_{t-1}\\big|\\mathbf{z}_{t},\\mathbf{z}_{0}\\big)$ could be derived: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mu_{t}(\\mathbf{z}_{t},\\mathbf{z}_{0})=\\frac{\\sqrt{\\alpha_{t}}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_{t}}\\mathbf{z}_{t}+\\frac{\\sqrt{\\bar{\\alpha}_{t-1}}(1-\\alpha_{t})}{1-\\bar{\\alpha}_{t}}\\mathbf{z}_{0},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Backward generative process $p_{\\theta}(\\mathbf{z}_{0:T}|\\mathbf{z}_{T})$ . After the $f$ orward noising process is defined and the training is completed, the reverse denoising process then denoises $\\mathbf{z}_{t}$ , aiming to recover original $\\mathbf{z}_{\\mathrm{0}}$ with the trained Diffuseq model $\\mathbf{z}_{\\theta}(\\mathbf{z}_{t},t)$ . This process is defined as: ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{z}_{0:T})=p_{\\theta}(\\mathbf{z}_{T})\\prod_{t=1}^{T}p_{\\theta}\\big(\\mathbf{z}_{t-1}\\vert\\mathbf{z}_{t}\\big)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{z}_{t-1}|\\mathbf{z}_{t})=\\mathcal{N}(\\mathbf{z}_{t-1};\\mu_{\\theta}(\\mathbf{z}_{t},t),\\sigma_{\\theta}(\\mathbf{z}_{t},t)),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and the initial state $p_{\\theta}(\\mathbf{z}_{T})$ is defined as $\\mathcal{N}(0,\\mathbf{I})$ . ", "page_idx": 14}, {"type": "text", "text": "Training objective $\\mathcal{L}_{\\mathrm{VLB}}$ . Inherited from Diffuseq [14], the training objective is to recover the original $\\mathbf{z}_{\\mathrm{0}}$ by denoising $\\mathbf{z}_{t}$ as in Eq. (8). The learning process as Eq. (9) is modeled by Diffuseq: $\\mathbf{z}_{\\theta}(\\mathbf{z}_{t},t)$ , where the $\\mu_{\\theta}(\\cdot)$ and $\\sigma_{\\theta}(\\cdot)$ serve as the parameterization of the predicted mean and standard deviation of $q(\\mathbf{z}_{t-1}|\\mathbf{z}_{t})$ in the forward noising process respectively. The input $\\mathbf{x}_{t}$ serves as the condition during the reverse denoising process as the partial noising is adopted in the forward noising. ", "page_idx": 15}, {"type": "text", "text": "Typically, a transformer architecture is adopted to model $\\mathbf{z}_{\\theta}$ , which is capable of modeling the semantic relation between $\\mathbf{x}_{t}$ and $\\mathbf{y}_{t}$ instinctively. The variational lower bound $(\\mathcal{L}_{\\mathrm{VLB}})$ is computed as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{VLB}}(\\mathbf{w}^{z})=\\mathbb{E}_{q(\\mathbf{z}_{0}|\\mathbf{w}^{z})}\\Bigg[\\underbrace{\\log\\frac{q(\\mathbf{z}_{T}|\\mathbf{w}^{z})}{p_{\\theta}(\\mathbf{z}_{T})}}_{\\mathrm{Prior\\;loss}}+\\underbrace{\\mathcal{L}_{\\mathrm{VLB}}(\\mathbf{z}_{0})}_{\\mathrm{Diffusion\\;loss}}\\underbrace{-\\log p_{\\theta}(\\mathbf{w}^{z}|\\mathbf{z}_{0})}_{\\mathrm{Rounding\\;loss}}\\Bigg],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the diffusion loss is the same as the continuous diffusion loss in DDPM [21], which is given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{VLB}}(\\mathbf{z}_{0})=\\mathbb{E}_{q(\\mathbf{z}_{1:T}|\\mathbf{z}_{0})}\\left[\\underbrace{\\log\\frac{q(\\mathbf{z}_{T}|\\mathbf{z}_{0})}{p_{\\theta}(\\mathbf{z}_{T})}}_{\\mathcal{L}_{T}}+\\underbrace{\\sum_{t=2}^{T}\\log\\frac{q(\\mathbf{z}_{t-1}|\\mathbf{z}_{0},\\mathbf{z}_{t})}{p_{\\theta}(\\mathbf{z}_{t-1}|\\mathbf{z}_{t})}}_{\\mathcal{L}_{T-1}+\\cdots+\\mathcal{L}_{1}}-\\underbrace{\\log p_{\\theta}(\\mathbf{z}_{0}|\\mathbf{z}_{1})}_{\\mathcal{L}_{0}}\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here the prior loss and $\\mathcal{L}_{T}$ is considered as a constant when the noising schedule $q$ is fixed and $p_{\\theta}(\\mathbf{z}_{T})=\\bar{\\mathcal{N}}(0,\\mathbf{I})$ . ", "page_idx": 15}, {"type": "text", "text": "After reweighting each term (i.e., treating all the loss terms across time-steps equally) as in $\\mathrm{Ho}$ et al. [21] and using the Monte Carlo optimizer, the training objective can be further simplified as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{\\theta}\\ \\mathcal{L}_{\\nabla\\mathrm{LB}}(\\mathbf{w}^{z})\\rightarrow\\operatorname*{min}_{\\theta}\\mathbb{E}_{q(\\mathbf{z}_{0:T}|\\mathbf{w}^{z})}\\left[\\displaystyle\\sum_{t=2}^{T}||\\mathbf{z}_{0}-\\mathbf{z}_{\\theta}(\\mathbf{z}_{t},t)||^{2}+||\\mathrm{EuB}(\\mathbf{w}^{z})-\\mathbf{z}_{\\theta}(\\mathbf{z}_{1},1)||^{2}-\\log p_{\\theta}(\\mathbf{w}^{z})\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\rightarrow\\operatorname*{min}_{\\theta}\\left[\\displaystyle\\sum_{t=2}^{T}||\\mathbf{y}_{0}-\\tilde{\\mathbf{z}}_{\\theta}(\\mathbf{z}_{t},t)||^{2}+||\\mathrm{EuB}(\\mathbf{w}^{y})-\\tilde{\\mathbf{z}}_{\\theta}(\\mathbf{z}_{1},1)||^{2}+\\mathcal{R}(||\\mathbf{y}_{0}||^{2})\\right]}\\\\ &{\\displaystyle\\quad\\quad\\rightarrow\\operatorname*{min}_{\\theta}\\left[\\displaystyle\\sum_{t=1}^{T}||\\mathbf{y}_{0}-\\tilde{\\mathbf{z}}_{\\theta}(\\mathbf{z}_{t},t)||^{2}+\\mathcal{R}(||\\mathbf{y}_{0}||^{2})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\tilde{\\mathbf{z}}_{\\theta}(\\mathbf{z}_{t},t)$ is used to denote the fractions of recovered $\\mathbf{z}_{\\mathrm{0}}$ corresponding to $\\mathbf{y}_{0}$ . $\\mathcal{R}(||\\mathbf{y}_{0}||^{2}))$ is the regularization term which regularizes the embedding learning. The embedding function is shared between source and target sequences, contributing to the joint training process of two different feature spaces. ", "page_idx": 15}, {"type": "text", "text": "A.2 Pre-trained Plaid ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The Plaid model [18] mostly adopts the variational diffusion model (VDM) framework [28] and we illustrate its forward, reverse, and loss calculations in this section. When fine-tuning Plaid 1B, we use the VDM formulation and apply the same sequence-to-sequence modification as in DiffuSeq. This involves imposing partial noise on $\\mathbf{z}_{t}$ and keeping the source condition sentence anchored as un-noised. ", "page_idx": 15}, {"type": "text", "text": "Forward diffusion process $q(\\mathbf{z}_{t}|\\mathbf{z}_{0})$ and $q(\\mathbf{z}_{t}|\\mathbf{z}_{s})$ . The distribution of latent $\\mathbf{z}_{t}$ conditioned on $\\mathbf{z}_{\\mathrm{0}}$ is given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\nq(\\mathbf{z}_{t}|\\mathbf{z}_{0})=\\mathcal{N}(\\alpha_{t}\\mathbf{z}_{0},\\sigma_{t}^{2}\\mathbf{I}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "After reparameterization, we have ${\\bf z}_{0}=({\\bf z}_{s}-\\epsilon_{1}\\sigma_{s})/\\alpha_{s}$ and $\\mathbf{z}_{t}=(\\alpha_{t}/\\alpha_{s})\\mathbf{z}_{s}-(\\alpha_{t}\\sigma_{s}/\\alpha_{s})\\epsilon_{1}+\\sigma_{t}\\epsilon_{2}$ , where $\\epsilon_{1}\\sim\\mathcal{N}(0,\\bf{I})$ and $\\epsilon_{2}\\sim\\mathcal{N}(0,\\bf{I})$ . Then after merging two uniform Gaussians, the distribution of $\\mathbf{z}_{t}$ given $\\mathbf{z}_{s}$ , for any $0\\leq s<t\\leq1$ , is given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\nq(\\mathbf{z}_{t}|\\mathbf{z}_{s})=\\mathcal{N}\\left(\\alpha_{t|s}\\mathbf{z}_{s},\\sigma_{t|s}^{2}\\mathbf{I}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\alpha_{t|s}\\,=\\,\\alpha_{t}/\\alpha_{s}$ and $\\sigma_{t|s}^{2}\\,=\\,\\sigma_{t}^{2}\\,-\\,\\alpha_{t|s}^{2}\\sigma_{s}^{2}$ . The variance-preserving special case gives $\\alpha_{t}=$ $\\sqrt{1-\\sigma_{t}^{2}}$ . In VDM, the noise schedule $\\alpha_{t}$ and $\\sigma_{t}^{2}$ , which specify how much noise to add at each time in the diffusion process, are parameterized as a scalar-to-scalar neural network $\\eta$ that satisfies $\\boldsymbol{\\sigma}_{t}^{2}=\\mathrm{sigmoid}\\left(\\gamma_{\\eta}(t)\\right)$ and $\\alpha_{t}^{2}=\\mathrm{{\\dot{sigmoid}}}\\left(-\\gamma_{\\pmb{\\eta}}(t)\\right)$ . This is different from previous practices that use a predefined function, e.g., DDPM [21] set the forward process variances to constants increasing linearly from $\\beta_{1}=10^{-4}$ to $\\beta_{T}=0.02$ . ", "page_idx": 16}, {"type": "text", "text": "Posterior $q\\big(\\mathbf{z}_{s}\\big|\\mathbf{z}_{t},\\mathbf{z}_{0}\\big)$ . The joint distribution of latent variables $\\left(\\mathbf{z}_{s},\\mathbf{z}_{t},\\mathbf{z}_{u}\\right)$ at any subsequent timesteps $0\\leq s<t<u\\leq1$ is Markov: $q(\\mathbf{z}_{u}|\\mathbf{z}_{t},\\mathbf{z}_{s})=q(\\mathbf{z}_{u}|\\mathbf{z}_{t})$ . Given the distributions above, we can verify through the Bayes rule that $q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{z}_{0})$ , for any $0\\leq s<t\\leq1$ , is also Gaussian given by: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\ \\,q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{z}_{0})=\\mathcal{N}(\\mu_{Q}(\\mathbf{z}_{t},\\mathbf{z}_{0};s,t),\\sigma_{Q}^{2}(s,t)\\mathbf{I})}\\\\ &{\\mathrm{where}\\ \\ \\sigma_{Q}^{2}(s,t)=\\sigma_{t|s}^{2}\\sigma_{s}^{2}/\\sigma_{t}^{2}}\\\\ &{\\mathrm{and}\\ \\ \\mu_{Q}(\\mathbf{z}_{t},\\mathbf{z}_{0};s,t)=\\frac{\\alpha_{t|s}\\sigma_{s}^{2}}{\\sigma_{t}^{2}}\\mathbf{z}_{t}+\\frac{\\alpha_{s}\\sigma_{t|s}^{2}}{\\sigma_{t}^{2}}\\mathbf{z}_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Backward generative process $p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t})$ . In VDM, the reverse process or the generative process is also defined as a Gaussian that satisfies $p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t})\\,=\\,q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{\\bar{z}}_{0}\\,=\\,\\mathbf{z}_{\\theta}(\\mathbf{z}_{t};t)\\bar{\\mathbf{\\theta}})$ , i.e. the same as $q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{z}_{0})$ , but with the original data $\\mathbf{z}_{\\mathrm{0}}$ replaced by the output of the denoising model $\\mathbf{z}_{\\theta}(\\mathbf{z}_{t};t)$ . Therefore, based on Eq. (17), the mean of $p_{\\theta}\\big(\\mathbf{z}_{s}\\big|\\mathbf{z}_{t}\\big)$ is given by: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mu_{\\theta}(\\mathbf{z}_{t};s,t)=\\frac{\\alpha_{t|s}\\sigma_{s}^{2}}{\\sigma_{t}^{2}}\\mathbf{z}_{t}+\\frac{\\alpha_{s}\\sigma_{t|s}^{2}}{\\sigma_{t}^{2}}\\mathbf{z}_{\\theta}(\\mathbf{z}_{t};t),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and the variance is the same as Eq. (16). ", "page_idx": 16}, {"type": "text", "text": "Continuous diffusion loss term $\\mathcal{L}_{\\mathrm{VLB}}(\\mathbf{z}_{0})$ . The prior loss and rounding loss in Eq. (10) can be (stochastically and differentiably) estimated using standard techniques. We now derive an estimator for the diffusion loss in VDM. Different from Eq.(12) which simplifies the loss term by reweighting, VDM adopts the standard loss formulation. We begin with the derivations of diffusion loss for discrete-time diffusion with $t\\in\\{1,\\ldots,T\\}$ , which is given by: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{VLB}}(\\mathbf{z}_{0})=\\sum_{t=1}^{T}\\mathbb{E}_{q(\\mathbf{z}_{t}|\\mathbf{z}_{0})}D_{K L}[q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{z}_{0})||p(\\mathbf{z}_{s}|\\mathbf{z}_{t})],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and we derive the expression of $D_{K L}(q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{z}_{0})||p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t}))$ as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{D_{K L}(q(\\mathbf{z}_{s}|\\mathbf{z}_{s},\\mathbf{z}_{0}|)|p(\\mathbf{z}_{s}|\\mathbf{z}_{t}))=\\frac{1}{2\\sigma_{0}^{2}(s,t)}||q_{0}-\\mu_{0}||_{2}^{2}}\\\\ &{\\phantom{=}-\\frac{\\sigma_{0}^{2}}{2\\sigma_{1}^{2}\\sigma_{0}^{2}}\\frac{\\alpha_{s}^{2}q_{*}^{2}||\\mathbf{z}_{0}-\\mathbf{z}_{0}||_{2}}{\\sigma_{1}^{2}}}\\\\ &{\\phantom{=}-\\frac{1}{2\\sigma_{0}^{2}}\\frac{\\alpha_{s}^{2}q_{*}^{2}||\\mathbf{z}_{0}-\\mathbf{z}_{0}||_{2}}{\\sigma_{1}^{2}}||_{0}^{2}}\\\\ &{\\phantom{=}-\\frac{1}{2\\sigma_{0}^{2}}\\frac{\\alpha_{s}^{2}q_{*}^{2}||\\mathbf{z}_{0}-\\mathbf{z}_{0}||_{2}}{\\sigma_{1}^{2}}||_{0}^{2}}\\\\ &{\\phantom{=}-\\frac{1}{2\\sigma_{0}^{2}}\\frac{\\alpha_{s}^{2}(\\sigma_{0}^{2}-\\mathbf{z}_{*}^{2})||\\mathbf{z}_{0}-\\mathbf{z}_{0}(\\mathbf{z}_{*};t)||_{2}^{2}}{\\sigma_{0}^{2}}}\\\\ &{\\phantom{=}-\\frac{1}{2}\\frac{\\alpha_{s}^{2}\\sigma_{0}^{2}(\\sigma_{0}^{2}-\\mathbf{z}_{*}^{2})||\\mathbf{z}_{0}-\\mathbf{z}_{0}(\\mathbf{z}_{*};t)||_{2}^{2}}{\\sigma_{0}^{2}}}\\\\ &{\\phantom{=}=\\frac{1}{2}\\left(\\frac{\\alpha_{s}^{2}}{\\sigma_{0}^{2}}\\frac{\\alpha_{s}^{2}}{\\sigma_{0}^{2}}\\right)||\\mathbf{z}_{0}-\\mathbf{z}_{0}(\\mathbf{z}_{*};t)||_{2}^{2}}\\\\ &{\\phantom{=}-\\frac{1}{2}\\left(3\\mathbf{R}(s)-\\mathbf{S\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathrm{SNR}(t)=\\alpha_{t}^{2}/\\sigma_{t}^{2}$ and its physical meaning is signal-to-noise ratio. ", "page_idx": 16}, {"type": "text", "text": "After reparameterization of $\\mathbf{z}_{t}$ , the diffusion loss function becomes: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{VLB}}(\\mathbf{z}_{0})=\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{q(\\mathbf{z}_{t}|\\mathbf{z}_{0})}[D_{K L}\\big(q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{z}_{0})||p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t})\\big)]}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac{1}{2}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I})}[\\displaystyle\\sum_{t=1}^{T}\\big(\\mathrm{SNR}(s)-\\mathrm{SNR}(t)\\big)\\,||\\mathbf{z}_{0}-\\mathbf{z}_{\\theta}(\\mathbf{z}_{t};t)||_{2}^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In practice, we follow Plaid to use the continuous-time diffusion formulation, where $t\\in[0,1]$ , and we can express $\\mathcal{L}$ as a function of $\\tau$ with $\\tau\\rightarrow0$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{VLB}}(\\mathbf{z}_{0})=\\frac{1}{2}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I})}\\int_{0}^{1}\\left[\\frac{\\mathrm{SNR}(t-\\tau)-\\mathrm{SNR}(t)}{\\tau}||\\mathbf{z}_{0}-\\mathbf{z}_{\\theta}(\\mathbf{z}_{t};t)||_{2}^{2}\\right]d t,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and let $\\mathrm{SNR}^{\\prime}(t)$ denote the derivative of the SNR function, this then gives: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{VLB}}(\\mathbf{z}_{0})=-\\frac{1}{2}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I})}\\int_{0}^{1}\\mathrm{SNR}^{\\prime}(t)\\left\\|\\mathbf{z}_{0}-\\mathbf{z}_{\\theta}(\\mathbf{z}_{t};t)\\right\\|_{2}^{2}d t.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A.3 Pre-trained SEDD ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "SEDD [37] is a discrete diffusion language model built based on discrete score matching [40], which generalizes score matching [45, 46] to the discrete data. We now denote $x$ as a categorical random variable and the following derivation can be extended to a sequence of variable $\\mathbf{x}$ as well. ", "page_idx": 17}, {"type": "text", "text": "Concrete score. Instead of directly modeling $p_{\\theta}(x)$ to approximate original data distribution $q(x)$ , the core idea of discrete score matching is to learn a quantity known as the concrete score [40] through a neural network: ", "page_idx": 17}, {"type": "equation", "text": "$$\ns_{\\theta}(x)_{y}=\\frac{p_{\\theta}(y)}{p_{\\theta}(x)}=\\frac{e^{f_{\\theta}(y)}/Z}{e^{f_{\\theta}(x)}/Z}=\\frac{e^{f_{\\theta}(y)}}{e^{f_{\\theta}(x)}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which eliminates normalizing constant $Z$ as in the energy-based model. In particular, this quantity is the categorical equivalent of the famous score function $\\nabla_{x}\\log{p}$ in continuous space. Regarding the choice of $y$ , if we model the ratio for every possible $y$ , we would have $V$ items given $V$ as the dimension of $x$ , and $N^{V}$ items for $\\mathbf{x}$ given $N$ as the sequence length of $\\mathbf{x}$ , which is computationally intractable. So we sparsify and only model \"relevant\" ratios based on whether $y$ is \"close\" to $x$ . These relevant positions will be denoted as $y\\sim x$ , e.g., all sentences $y$ that differ from $x$ with Hamming distance 1. ", "page_idx": 17}, {"type": "text", "text": "Training objective. Lou et al. [37] define a learning objective named score entropy to learn the neural network, which is given by: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x\\sim q}\\left[\\sum_{y\\sim x}s_{\\theta}(x)_{y}-\\frac{q(y)}{q(x)}\\log s_{\\theta}(x)_{y}\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Taking a derivative w.r.t. $s$ and setting it to 0, we see that this occurs when $\\begin{array}{r}{s_{\\theta}(x)_{y}=\\frac{q(y)}{q(x)}}\\end{array}$ , which can be easily checked to be globally optimal as the function is convex as a function of $s$ . To handle the unknown term $\\frac{q(y)}{q(x)}$ , they further propose denoising score entropy motivated by denoising score matching [45] based on $\\begin{array}{r}{q(x_{t})=\\sum_{x_{0}}q_{t|0}(x_{t}|x_{0})q_{0}(x_{0})}\\end{array}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x_{0}\\sim q_{0},t\\sim U[0,T],x_{t}\\sim q_{t}|0}(x_{t}|x_{0})\\left[\\sum_{y\\sim x_{t}}s_{\\theta}(x_{t},t)_{y}-\\frac{q_{t}|0(y|x_{0})}{q_{t}|0(x_{t}|x_{0})}\\log s_{\\theta}(x_{t},t)_{y}\\right],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $q_{t|0}(\\cdot|x_{0})$ is a perturbation of a base density $q(\\cdot)$ by a transition kernel, and the transition ratio $\\frac{q_{t\\mid0}{\\left(y\\left|x_{0}\\right.\\right)}}{q_{t\\mid0}{\\left(x_{t}\\left|x_{0}\\right.\\right)}}$ is known by design. ", "page_idx": 17}, {"type": "text", "text": "Forward diffusion process. The transition $q_{t|0}(x_{t}|x_{0})$ is a vector that represents a categorical distribution, and can be defined by a forward diffusion process $q_{t|0}(x_{t}|x_{0})=\\exp(\\overline{{\\sigma}}(t)Q)_{x_{0}}$ , where $\\overline{{\\sigma}}(t)\\in\\mathbb{R}_{\\geq0}$ is the cumulative noise $\\textstyle\\int_{0}^{t}\\sigma(s)d s$ at timestep $t$ with a value close to 0 when $t$ is small and increasing when $t$ growing. Lou et al. [37] use two standard transition matrices with special structures to implement matrix $Q$ following prior work [1]: ", "page_idx": 18}, {"type": "equation", "text": "$$\nQ^{\\mathrm{uniform}}=\\left[\\begin{array}{c c c c}{1-V}&{1}&{\\cdot\\cdot\\cdot}&{1}\\\\ {1}&{1-V}&{\\cdot\\cdot\\cdot}&{1}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {1}&{1}&{\\cdot\\cdot\\cdot}&{1-V}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\nQ^{\\mathrm{absorb}}=\\left[\\begin{array}{c c c c c}{-1}&{0}&{\\cdots}&{0}&{0}\\\\ {0}&{-1}&{\\cdots}&{0}&{0}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}\\\\ {0}&{0}&{\\cdots}&{-1}&{0}\\\\ {1}&{1}&{\\cdots}&{1}&{0}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "One can view the above diffusion process by taking small $\\Delta t$ Euler steps and randomly sampling the resulting transitions: ", "page_idx": 18}, {"type": "equation", "text": "$$\nq_{t+\\Delta t|t}(x_{t+\\Delta t}=y|x_{t}=x)\\propto\\delta_{x y}+Q_{t}(y,x)\\Delta t+O(\\Delta t),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $Q_{t}=\\sigma(t)Q$ , and $O(\\Delta t)$ represents terms that tend to zero at a faster rate than $\\Delta t$ . ", "page_idx": 18}, {"type": "text", "text": "Backward generative process. To simulate the diffusion defined above, one can use the Euler strategy to derive the time reversal of the forward process: ", "page_idx": 18}, {"type": "equation", "text": "$$\nq_{t-\\Delta t|t}(x_{t-\\Delta t}=y\\mid x_{t}=x)\\propto\\delta_{x y}+R_{t}(y,x)\\Delta t+O(\\Delta t),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $R_{t}$ is the reverse transition rate matrix that can be derived using Bayes rule: $R_{t}(y,x)\\,=$ $\\frac{q_{t}(y)}{q_{t}(x)}Q_{t}(x,y)$ $R_{t}$ $t$ other tokens at timestep $t-\\Delta t$ . Let $p_{\\theta}(x_{t-\\Delta t}=y\\mid x_{t}=x)=q_{t-\\Delta t\\mid t}(x_{t-\\Delta t}=y\\mid x_{t}=x)$ , we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\np_{\\theta}(x_{t-\\Delta t}=y\\mid x_{t}=x)\\propto\\delta_{x y}+R_{t}^{\\theta}(y,x)\\Delta t+O(\\Delta t),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{R_{t}^{\\theta}(y,x)=\\sum_{x_{0}}q_{0|t}(x_{0}|x)\\frac{q_{t|0}(y|x_{0})}{q_{t|0}(x|x_{0})}Q_{t}(x,y)=s_{\\theta}(x,t){_y Q_{t}(x,y)}.}\\end{array}$ )qqtt||00((yx||xx00))Qt(x, y) = s\u03b8(x, t)yQt(x, y). For a sequence of random variables $\\mathbf{x}$ , this is inefficient because only one position is modified per step. A natural alternative has been to use $\\tau$ -leaping [12], which performs an Euler step at each position simultaneously. ", "page_idx": 18}, {"type": "text", "text": "A.4 Conditional ODE solver ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The sampling of continuous diffusion models can be implemented by solving the diffusion ODEs [45, 46]. Specifically, sampling by diffusion ODEs needs to discretize the following ODE [46] with $t$ changing from $T$ to $0$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{z}_{t}}{\\mathrm{d}t}=f(t)\\mathbf{z}_{t}+\\frac{g^{2}(t)}{2\\sigma_{t}}\\epsilon_{\\theta}(\\mathbf{z}_{t},t),\\quad\\mathbf{z}_{T}\\sim\\mathcal{N}(\\mathbf{0},\\tilde{\\sigma}^{2}\\mathbf{I}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The data prediction model $\\mathbf{z}_{\\theta}(\\mathbf{z}_{t},t)$ predicts the original data $\\mathbf{z}_{\\mathrm{0}}$ based on the noisy $\\mathbf{z}_{t}$ , and its relationship with $\\epsilon_{\\theta}(\\mathbf{z}_{t},t)$ is given by $\\mathbf{z}_{\\theta}(\\mathbf{z}_{t};t)\\;:=\\;(\\mathbf{z}_{t}\\,-\\,\\sigma_{t}\\epsilon_{\\theta}(\\mathbf{z}_{t},t))/\\alpha_{t}$ [28]. Therefore, the equivalent diffusion ODE w.r.t. the data prediction model $\\mathbf{z}_{\\theta}$ is: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{z}_{t}}{\\mathrm{d}t}=\\left(f(t)+\\frac{g^{2}(t)}{2\\sigma_{t}^{2}}\\right)\\mathbf{z}_{t}-\\frac{\\alpha_{t}g^{2}(t)}{2\\sigma_{t}^{2}}\\mathbf{z}_{\\theta}(\\mathbf{z}_{t},t),\\quad\\mathbf{z}_{T}\\sim\\mathcal{N}(\\mathbf{0},\\tilde{\\sigma}^{2}\\mathbf{I}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the coefficients $\\begin{array}{r}{f(t)=\\frac{\\mathrm{d}\\log\\alpha_{t}}{\\mathrm{d}t},g^{2}(t)=\\frac{\\mathrm{d}\\sigma_{t}^{2}}{\\mathrm{d}t}-2\\frac{\\mathrm{d}\\log\\alpha_{t}}{\\mathrm{d}t}\\sigma_{t}^{2}}\\end{array}$ [28]. ", "page_idx": 18}, {"type": "text", "text": "Given an initial value $\\mathbf{z}_{s}$ at time $s>0$ and denote $\\hat{\\mathbf{z}}_{\\theta}(\\hat{\\mathbf{z}}_{\\lambda},\\lambda):=\\mathbf{z}_{\\theta}(\\mathbf{z}_{t_{\\lambda}(\\lambda)},t_{\\lambda}(\\lambda))$ as the change-ofvariable form of $\\mathbf{z}_{\\theta}$ for $\\lambda$ , the solution $\\mathbf{z}_{t}$ at time $t\\in[0,s]$ of diffusion ODEs in Eq. (40) is: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{z}_{t}=\\frac{\\sigma_{t}}{\\sigma_{s}}\\mathbf{z}_{s}+\\sigma_{t}\\int_{\\lambda_{s}}^{\\lambda_{t}}e^{\\lambda}\\hat{\\mathbf{z}}_{\\theta}(\\hat{\\mathbf{z}}_{\\lambda},\\lambda)\\mathrm{d}\\lambda,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which can be proved by taking derivative w.r.t. $t$ in Eq. (41): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}\\mathbf{z}_{t}}{\\mathrm{d}t}=\\frac{\\mathrm{d}\\sigma_{t}}{\\mathrm{d}t}\\frac{\\mathbf{z}_{s}}{\\sigma_{s}}+\\frac{\\mathrm{d}\\sigma_{t}}{\\mathrm{d}t}\\int_{\\lambda_{s}}^{\\lambda_{t}}e^{\\lambda}\\hat{\\mathbf{z}}_{\\theta}(\\hat{\\mathbf{z}}_{\\lambda},\\lambda)\\mathrm{d}\\lambda+\\frac{\\mathrm{d}\\lambda_{t}}{\\mathrm{d}t}\\sigma_{t}e^{\\lambda_{t}}\\hat{\\mathbf{z}}_{\\theta}(\\hat{\\mathbf{z}}_{\\lambda_{t}},\\lambda_{t})}\\\\ {\\displaystyle\\qquad=\\frac{\\mathrm{d}\\sigma_{t}}{\\mathrm{d}t}\\frac{\\mathbf{z}_{t}}{\\sigma_{t}}+\\frac{\\mathrm{d}\\lambda_{t}}{\\mathrm{d}t}\\sigma_{t}e^{\\lambda_{t}}\\hat{\\mathbf{z}}_{\\theta}(\\hat{\\mathbf{z}}_{\\lambda_{t}},\\lambda_{t})}\\\\ {\\displaystyle\\qquad=\\left(f(t)+\\frac{g^{2}(t)}{2\\sigma_{t}^{2}}\\right)\\frac{\\mathbf{z}_{t}}{\\sigma_{t}}-\\frac{\\alpha_{t}g^{2}(t)}{2\\sigma_{t}^{2}}\\mathbf{z}_{\\theta}(\\mathbf{z}_{t},t),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and this gives us the exact formulation as in Eq. (41). ", "page_idx": 19}, {"type": "text", "text": "Based on Eq. (41), the aim of an ODE solver is to approximate the exact solution at time $t_{i}$ given the previous value $\\mathbf{z}_{t_{i-1}}$ at time $t_{i-1}$ . Denote $\\begin{array}{r}{{\\mathbf{z}}_{\\theta}^{(n)}(\\lambda):=\\frac{{\\mathrm{d}}^{n}\\hat{{\\mathbf{z}}}_{\\theta}({\\mathbf{z}}_{\\lambda},\\lambda)}{\\mathrm{d}\\lambda^{n}}}\\end{array}$ as the $n$ -th order total derivatives of $\\mathbf{z}_{\\theta}$ w.r.t. logSNR $\\lambda$ . Lu et al. [38, 39] show that by taking the $(k-1)$ -th Taylor expansion $\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!k\\geq1\\!\\!\\!\\!\\!\\!\\!$ at $\\lambda_{t_{i-1}}$ for $\\mathbf{z}_{\\theta}$ w.r.t. $\\lambda\\in[\\lambda_{t_{i-1}},\\lambda_{t_{i}}]$ and substitute it into Eq. (41) with $s=t_{i-1}$ and $t=t_{i}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{z}_{t_{i}}=\\frac{\\sigma_{t_{i}}}{\\sigma_{t_{i-1}}}\\mathbf{z}_{t_{i-1}}+\\sigma_{t_{i}}\\sum_{n=0}^{k-1}\\mathbf{z}_{\\theta}^{(n)}(\\hat{\\mathbf{z}}_{\\lambda_{t_{i-1}}},\\lambda_{t_{i-1}})\\int_{\\lambda_{t_{i-1}}}^{\\lambda_{t_{i}}}e^{\\lambda}\\frac{(\\lambda-\\lambda_{t_{i-1}})^{n}}{n!}\\mathrm{d}\\lambda+\\mathcal{O}(h_{i}^{k+1}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the integral $\\begin{array}{r}{\\int e^{\\lambda}\\frac{(\\lambda-\\lambda_{t_{i-1}})^{n}}{n!}\\mathrm{d}\\lambda}\\end{array}$ can be analytically computed by integral-by-parts. Therefore, to design a $k$ -th order ODE solver, we only need to estimate the $n$ -th order derivatives $\\mathbf{z}_{\\theta}^{(n)}(\\lambda_{t_{i-1}})$ for $n\\leq k-1$ after omitting the ${\\mathcal{O}}(h_{i}^{k+1})$ high-order error terms. For $k=1$ , Eq. (42) becomes (after omitting the ${\\mathcal{O}}(h_{i}^{k+1})$ terms) ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathsf{z}_{t_{i}}=\\frac{\\sigma_{t_{i}}}{\\sigma_{t_{i-1}}}\\mathbf{z}_{t_{i-1}}+\\sigma_{t_{i}}\\mathbf{z}_{\\theta}(\\mathbf{z}_{t_{i-1}},t_{i-1})\\int_{\\lambda_{t_{i-1}}}^{\\lambda_{t_{i}}}e^{\\lambda}\\mathrm{d}\\lambda=\\frac{\\sigma_{t_{i}}}{\\sigma_{t_{i-1}}}\\mathbf{z}_{t_{i-1}}-\\alpha_{t_{i}}(e^{-h_{i}}-1)\\mathbf{z}_{\\theta}(\\mathbf{z}_{t_{i-1}},t_{i-1}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $h_{i}:=\\lambda_{t_{i}}-\\lambda_{t_{i-1}}$ for $i=1,\\dots,T$ . ", "page_idx": 19}, {"type": "text", "text": "Since DoT is conditionally trained with partial nosing, we introduce a conditional form of Eq. (43) when adapting the above ODE solver into the inference stage. For $k=1$ , this is given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{y}_{t_{i}}=\\frac{\\sigma_{t_{i}}}{\\sigma_{t_{i-1}}}\\mathbf{y}_{t_{i-1}}-\\alpha_{t_{i}}(e^{-h_{i}}-1)\\tilde{\\mathbf{z}}_{\\theta}(\\mathbf{z}_{t_{i-1}},t_{i-1}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbf{z}_{t_{i-1}}=[\\mathbf{x};\\mathbf{y}_{t_{i}-1}]$ and $\\tilde{\\mathbf{z}}_{\\theta}(\\mathbf{z}_{t},t)$ is used to denote the fractions of recovered $\\mathbf{z}_{\\mathrm{0}}$ corresponding to y0. ", "page_idx": 19}, {"type": "text", "text": "B Experiment Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "B.1 Dataset Statistics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We list the statistics of our used datasets in Table 4. For the digit multiplication datasets and GSM8K dataset, we use processed datasets from Implict $\\mathrm{CoT^{6}}$ [7]. For boolean logic task, we construct the training and test dataset using the method from $\\mathrm{DyVal}^{7}$ [68]. All datasets contain 1000 test examples except GSM8K, which contains 1319 examples. ", "page_idx": 19}, {"type": "text", "text": "B.2 Details of Baselines ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "When fine-tuning GPT2, we train 40 epochs using the learning rate of 1e-4 for boolean logic and $5\\mathrm{e}{-4}$ for others. During inference, we use greedy decoding for single decoding. For self-consistency, following the original paper [52], we apply temperature sampling with $T=0.5$ and truncated at the top- ${\\cdot k}$ ( $k=40$ ) tokens with the highest probability for diverse generation. All GPT2-based models ", "page_idx": 19}, {"type": "text", "text": "Table 4: Training set size, average number of tokens in the input, intermediate, and output texts respectively when using Plaid tokenizer on the validation set and average number of rationales. ", "page_idx": 20}, {"type": "table", "img_path": "G0v0TxX01N/tmp/17b14dbe7e7962d00fe48ab88d65061ebe01f5c1ae25af16671acbf20b719147.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "use GPT2Tokenizer with vocabulary size of 50257. All datasets are trained using sequence length of 256 except boolean logic, which uses 384 length. ", "page_idx": 20}, {"type": "text", "text": "Note in Table 1, we compare Plaid DoT with the fine-tuned GPT2 small, given that the Plaid 1B [18] model exhibits similar perplexity to GPT2 small. This might put our Plaid DoT model at a disadvantage in terms of inference speed, as the parameters of Plaid 1B are nearly $10\\times$ greater than those of GPT2 small. ", "page_idx": 20}, {"type": "text", "text": "For Transformer-scratch baseline [51], we use 6 transformer encoder layers and 6 transformer decoder layers. We employ the tokenizer from bert-base-uncased with a vocabulary size of 30522. The learning rate is set to 1e-5, and we train for 60k steps with a batch size of 128. ", "page_idx": 20}, {"type": "text", "text": "For ChatGPT, we use OpenAI api8 with the following prompt in 5-shot. ", "page_idx": 20}, {"type": "table", "img_path": "G0v0TxX01N/tmp/93c487a5a33eae32e5aa103a20e9ff26115d887f31557bfe6e5e8fc815e0a6d8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Please note that the throughput of ChatGPT in Table 1 only measures the response speed of ChatGPT and does not represent the actual generation speed of the model. As a blackbox commercial product, ChatGPT may employ various optimization techniques to speedup generating responses to enhance user experiences. ", "page_idx": 20}, {"type": "text", "text": "B.3 DoT Implementation Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We conduct all the experiments on NVIDIA V100-32G GPUs, and we use 8 GPUs for training and sampling. We resort to half precision (fp16) instead of bfloat16 (bf16) as V100 GPU doesn\u2019t support bf16, and we don\u2019t observe any number explosion. By default, we train DoT from scratch on three datasets respectively, including the four-digit $(4\\times4)$ , five-digit $\\left(5\\times5\\right)$ multiplication datasets, and the GSM8k dataset. Additionally, we fine-tune the pre-trained model Plaid-1B on the GSM8K dataset with DoT to explore its effectiveness further. ", "page_idx": 20}, {"type": "text", "text": "For DoT trained from scratch. We use 12 layers of transformer and bert-base-uncased vocabulary. We preprocess the four-digit $(4\\times4)$ and five-digit $(5\\times5)$ multiplication datasets to prepare for the training process of the DoT multi-path variant, and sampling from it. The learning rate is $\\scriptstyle1\\ominus-4$ and we train for $60\\mathrm{k}$ steps with the batch size of 128 and max sequence length of 128. For digit multiplication in Table 1, we use sampling step $T=1$ to achieve high throughput while keeping the accuracy. For boolean logic dataset, we use $T=2$ . ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "For DoT fine-tuned from Plaid, we set the training steps of the DoT and multi-pass DoT to be $120\\mathbf{k}$ and $30\\mathrm{k}$ respectively, as we find more training steps will lead to performance degradation. The learning rate is set to $1\\!\\ e\\!-\\!4$ for boolean logic and 3e-4 for other datasets. The max sequence length is set to 384 for the boolean logic dataset and 256 for others. We use Adam optimizer [29]. During tokenization, we use Plaid\u2019s tokenizer and we treat all the digits as individual tokens. During training, we set $\\epsilon_{m i n}$ to be 0.95 as we find decreasing the probability of oracle demonstration hinders model training. We choose glancing sampling $\\gamma=0.01$ and self consistency $m=20$ . Following Gulrajani and Hashimoto [18], we also adopt self-conditioning [5] during training. During inference, we set the scoring temperature to 0.5 to sharpen the predicted noise distribution. We also use soft logits with a temperature of 0.5 to produce more diverse samples. By default, we use sampling step $T=64$ to ensure accuracy. Training DoT and DoT require 29h and 10h, respectively. For DoT trained from SEDD, we set the training steps of the DoT and multi-pass DoT to be $200\\mathbf{k}$ , with other parameters being the same as when training Plaid. For all the experiments, we have verified the statistical significance by running them multiple times. ", "page_idx": 21}, {"type": "text", "text": "B.4 Additional Results ", "text_level": 1, "page_idx": 21}, {"type": "table", "img_path": "G0v0TxX01N/tmp/2daf685479456b9ecaf2f946f1aec86eee0820ee1d02924fc5c3f2afe82dcd47.jpg", "table_caption": ["Table 6: Comparison to larger AR models. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Comparison to larger open language models. We compare our model with LoRA fine-tuning of AR LLMs on the same GSM-Aug dataset, which is listed in Table 6. Please note that the current diffusion pretrained model is much smaller than Llama 7B, so this comparison is not fair and we just list them for reference. We have validated that our DoT is better than the same scale autoregressive model GPT-2, which shares a similar architecture with Llama. We believe that further exploration of diffusion language models will lead to larger models that can compete with current LLMs, allowing DoT to achieve results more comparable to Llama. ", "page_idx": 21}, {"type": "table", "img_path": "G0v0TxX01N/tmp/1a8426490e6f7160d27a4ca3c860faf53e57c22759385ea6e45d56b7aea02172.jpg", "table_caption": ["Table 7: Comparison between DoT and no-DoT (Answer-only). "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Comparison with no-DoT finetune. We conduct the answer-only setting to further validate the effectiveness of DoT. The results in Table 7 reveal that fine-tuning diffusion models solely with answer data leads to inferior performance compared to DoT, mirroring the degradation of AR models in the absence of CoT. ", "page_idx": 21}, {"type": "text", "text": "Throughput Comparison. We have shown how T affects performance on grade school math in Figure 3, and here we also show how T affects throughput for Plaid $\\boldsymbol{\\mathrm{DoT^{MP}}}$ , as in Table 8. The relationship between throughput and T appears to be nearly linear. ", "page_idx": 21}, {"type": "text", "text": "Comparison of the reasoning paths between DoT and $\\mathbf{DoT^{MP}}$ . We observe that $\\mathrm{DoT^{MP}}$ outperforms DoT in correctness regarding the reasoning paths, while DoT slightly excels in diversity ", "page_idx": 21}, {"type": "text", "text": "Table 8: Throughput comparison when increasing the number of timesteps $T$ for Plaid $\\mathrm{DoT^{MP}}$ . ", "page_idx": 22}, {"type": "table", "img_path": "G0v0TxX01N/tmp/ec7d4b04927b2b5952d62fb91c764e19ce2a0c54b0de66f738d0305920ea7a41.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "as depicted in Figure 4(b). Below we show some examples where $\\mathrm{DoT^{MP}}$ can predict the correct reasoning path while DoT fails: ", "page_idx": 22}, {"type": "table", "img_path": "G0v0TxX01N/tmp/fdf73b4308dd39da469065bd163706a158d4a47a8ae4a124f40f7cd2aae1ab80.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Query: Skyler has 100 hats on his hand with the colors red, blue, and white. Half of the hats   \nare red, 3/5 of the remaining hats are blue, and the rest are white. How many white hats does   \nSkyler have?   \nDoT: $\\ll1/2\\ll100\\rightharpoonup50\\gg\\ll3/5\\ll50\\rightharpoonup30\\gg\\ll100\\rightharpoonup30\\{=70\\rightsquigarrow}$ #### 70   \nDoTMP : $\\ll\\!100/2\\!\\!=\\!50\\!\\gg$ \u00ab100-50=50\u00bb $\\ll\\!50\\!*\\!3/5\\!=\\!30\\!*\\!)$ $\\ll\\!50\\!\\!-\\!30\\!\\!=\\!\\!20\\!\\!\\gg\\!\\!\\!$ #### 20 ", "page_idx": 22}, {"type": "text", "text": "B.5 Other Attempts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For the ablation design for DoT fine-tuning in Table 2, we have tried to fine-tune a decoder-only autoregressive language model (i.e., GPT2 here), where we only change the base model from Plaid 1B to GPT2 large, remove the causal mask and keep all other diffusion training settings the same with the Plaid fine-tuning. In this setting, even though the model is formulated and trained in the diffusion manner, it still can not predict the right format of answers. This experiment may indicate that a pre-trained diffusion model is necessary for the further fine-tuning of downstream tasks. ", "page_idx": 22}, {"type": "text", "text": "Regarding datasets, we also try to mix up four-digit $(4\\times4)$ and five-digit $\\left(5\\times5\\right)$ multiplication datasets for training and testing, considering that the number of rationales is different in these two tasks. As for the result, the trained model learns when to conclude the computation and can attain $100\\%$ accuracy. ", "page_idx": 22}, {"type": "text", "text": "C Discussion about base models ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Our DoT approach is constrained by the pre-training and fine-tuning paradigm due to the not-strongenough base models. This lags behind the current trend of instruction-tuning LLMs and pursuing the generalization of LMs across various tasks. Nevertheless, considering the pre-trained diffusion models are still in their early stages and the lack of scaled pre-trained diffusion models, our study is a preliminary exploration to show the potential of diffusion models for reasoning tasks, and we believe that with more powerful pre-trained diffusion models and post-instruction tuning, DoT can attain the generalization capabilities of today\u2019s LLMs and yield further advantages. ", "page_idx": 22}, {"type": "text", "text": "D Boarder Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Our work contributes to the understanding of denoising generative models and enhances their generation capabilities within certain discrete text reasoning datasets. The proposed DoT with diffusion language models challenges autoregressive models with CoT, achieving competitive performance. While there is still a large gap with modern large autoregressive language models such as ChatGPT, we believe DoT can benefit more with future work on scaling diffusion language models. However, we acknowledge that deep generative models, as powerful tools for learning from unstructured data, can have detrimental societal impacts if misused. Specifically, these models can facilitate the spread of misinformation by reducing the resources required to create realistic fake content. Additionally, the generated samples from these models accurately reflect the statistics of their training datasets. Consequently, if these samples are interpreted as objective truth without considering the inherent biases present in the original data, they can perpetuate discrimination against minority groups. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Please see abstract and introduction. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We discuss the limitations in section 6. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not include theoretical results. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We provide the details of the dataset, training infrastructure, and implementations in Appendix B. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have uploaded the code to reproduce our results. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide the implementation details in Section 4.1 and Appendix B. We also provide the official code implementation in ... ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Please see Appendix B. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Please see Appendix B. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Please see Appendix D. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper poses no such risks ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Please see Section 4.1 and Appendix B. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 28}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The license detail is provided in the uploaded code. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]