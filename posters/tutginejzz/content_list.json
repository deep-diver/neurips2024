[{"type": "text", "text": "A Huber Loss Minimization Approach to Mean Estimation under User-level Differential Privacy ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Puning Zhao Lifeng Lai Li Shen Zhejiang Lab University of California, Davis Sun Yat-Sen University pnzhao@zhejianglab.com lflai@ucdavis.edu mathshenli@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Qingming Li Jiafei Wu Zhe Liu\u2217 Zhejiang University Zhejiang Lab Zhejiang Lab liqm@zju.edu.cn wujiafei@zhejianglab.com zhe.liu@zhejianglab.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Privacy protection of users\u2019 entire contribution of samples is important in distributed systems. The most effective approach is the two-stage scheme, which finds a small interval first and then gets a refined estimate by clipping samples into the interval. However, the clipping operation induces bias, which is serious if the sample distribution is heavy-tailed. Besides, users with large local sample sizes can make the sensitivity much larger, thus the method is not suitable for imbalanced users. Motivated by these challenges, we propose a Huber loss minimization approach to mean estimation under user-level differential privacy. The connecting points of Huber loss can be adaptively adjusted to deal with imbalanced users. Moreover, it avoids the clipping operation, thus significantly reducing the bias compared with the two-stage approach. We provide a theoretical analysis of our approach, which gives the noise strength needed for privacy protection, as well as the bound of mean squared error. The result shows that the new method is much less sensitive to the imbalance of user-wise sample sizes and the tail of sample distributions. Finally, we perform numerical experiments to validate our theoretical analysis. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Privacy is one of the major concerns in modern data analysis. Correspondingly, differential privacy (DP) [1] has emerged as a standard framework of privacy protection. Various statistical problems have been analyzed with additional DP requirements [2\u20134]. Among all these problems, mean estimation is a fundamental one [5\u20138], which is not only useful in its own right [9], but also serves as a building block of many other tasks relying on estimating gradients, such as private stochastic optimization [10\u201315] and machine learning [16\u201318]. Existing research on DP mean estimation focuses primarily on item-level cases, i.e. each user contributes only one sample. However, in many practical scenarios, especially in recommendation systems [19\u201321] and federated learning [22\u201326], a user has multiple samples. We hope to regard them as a whole for privacy protection. ", "page_idx": 0}, {"type": "text", "text": "In recent years, a flurry of works focus on user-level DP [27\u201330]. The most popular one is the Winsorized Mean Estimator (WME) proposed in [28], which takes a two-stage approach. In the first stage, WME identifies an interval, which is small but contains the ground truth $\\mu$ with high probability. In the second stage, WME clips user-wise averages to control the sensitivity and then calculates the final average with appropriate noise. This method can be extended to high dimensionality by Hadamard transform [31]. The convergence rate has been established in [28] under some ideal assumptions. Despite the merit of the two-stage approach from the theoretical perspective, this method may face challenges in many realistic settings. Firstly, [28] assumes that users are balanced, which means that users have the same number of items. Nevertheless, in federated learning applications, it is common for clients (each client is regarded as a user here) to possess different numbers of samples [32\u201334]. Secondly, this method is not suitable for heavy-tailed distributions, which is also common in reality [35\u201338]. For heavy-tailed distributions, the interval generated in the first stage needs to be large enough to prevent clipping bias, which results in large sensitivity. As a result, stronger additive noise is needed for privacy protection, which significantly increases the estimation error. These drawbacks hinder the practical application of user-level DP. We aim to propose new solutions to address these challenges. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Towards this goal, in this paper, we propose a new method, which estimates the mean using Huber loss minimizer [39], and then adds noise for privacy protection. A challenge is that to determine an appropriate noise strength, it is necessary to conduct a thorough analysis of the local sensitivity that considers all possible datasets. To overcome this challenge, we divide datasets into three types, including those with no outliers, a few outliers, and many outliers, and analyze these cases separately. Based on the sensitivity analysis, we then use the smooth sensitivity framework [40] to determine the noise strength carefully. ", "page_idx": 1}, {"type": "text", "text": "Our method has the following advantages. Firstly, our method adapts well to imbalanced datasets, since the threshold $T_{i}$ of Huber loss are selected adaptively according to the sample size per user, which leads to a better tradeoff between sensitivity and bias. Secondly, our method performs better for heavy-tailed distributions, since we control sensitivity by penalizing large distances using Huber loss, which yields a smaller bias than the clipping operation. Apart from solving these practical issues, it worths mentioning that our method solves robustness (to model poisoning attacks) and privacy issues simultaneously. In modern data analysis, it is common for a system to suffer from both poisoning and inference attacks at the same time [41\u201343]. Consequently, many recent works focus on unified methods for item-level DP and robustness to cope with both attacks simultaneously [44\u201347]. To the best of our knowledge, our method is the first attempt to unify robustness and DP at user-level. ", "page_idx": 1}, {"type": "text", "text": "The main contribution is summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose the Huber loss minimization approach, which finds the point with minimum Huber distance to all samples. Our method is convenient to implement and only requires linear time complexity.   \n\u2022 For the simplest case with balanced users, we provide a theoretical analysis, which shows that our method makes a slight improvement for bounded distributions and a significant improvement for heavy-tailed distributions over the two-stage approach.   \n\u2022 For imbalanced users, we design an adaptive strategy to select weights and connecting points in Huber loss, which makes our method much less sensitive to the imbalance of local sample sizes of users.   \n\u2022 We conduct experiments using both synthesized and real data, which also verify the effectiveness of the proposed method for imbalanced users and heavy-tailed distributions. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "User-level DP. [22] applies a brute-force clipping method [17] for user-level DP in federated learning. [27] made the first step towards optimal rates under user-level DP, which analyzed discrete distribution estimation problems. The popular method WME was proposed in [28], which uses the idea of twostage approaches [47\u201349]. The two-stage method for user-level DP has also been extended to stochastic optimization problems [50,51]. [52] analyzes mean estimation for boolean signals under user-level DP in heterogeneous settings. There are also some works focusing on black-box conversion from item-level DP to user-level counterparts. [29] analyzed general statistical problems, which shows that a class of algorithms for item-level DP problems having the pseudo-globally stable property can be converted into user-level DP algorithms. Following [29], [53] expanded such transformation for any item-level algorithms. [30] extends the works to smaller $m$ . It is discussed in [51] that these black-box methods have suboptimal dependence on $\\epsilon$ . [54\u201356] studies user-level DP under local model. ", "page_idx": 1}, {"type": "text", "text": "From robustness to DP. Robustness and DP have close relationships since they both require the outputs to be insensitive to minor changes in input samples. There are three types of methods for conversion from robust statistics to DP. The first one is propose-test-release (PTR), which was first proposed in [57], and was extended into high dimensional cases in [58]. The second choice is smooth sensitivity [40], which calculates the noise based on the \"smoothed\" local sensitivity. For example, [59] designed a method to protect trimmed mean with smooth sensitivity. The third solution is inverse sensitivity [5,60,61], which can achieve pure differential privacy (i.e. $\\delta=0$ ). All these methods require a detailed analysis of the sensitivity. For some recently proposed high dimensional estimators [62\u201364], the sensitivity is usually large and hard to analyze. As a common method for robust statistics [39], Huber loss minimization has been widely applied in robust regression [65,66], denoising [67] and robust federated learning [68]. Huber loss has also been used in DP [69,70] for linear regression problems. ", "page_idx": 2}, {"type": "text", "text": "Concurrent work. After the initial submission of this paper, we notice an independent work [71], which also studies mean estimation under user-level DP (which is called person-level DP in [71]). [71] considers directional bound, which requires that the moment is bounded in every direction. However, we consider non-directional bound, which bounds the $\\ell_{2}$ norm of a random vector. We refer to Section 1.3.1 in [71] for further discussion. ", "page_idx": 2}, {"type": "text", "text": "Our work is the first attempt to use the Huber loss minimization method in user-level DP. With an adaptive selection of weights and connecting points between quadratic and linear parts, our method achieves a significantly better performance for imbalanced users and heavy-tailed distributions. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce definitions and notations. To begin with, we recall some concepts of DP and introduce the notion of user-level DP. Denote $\\Omega$ as the space of all datasets, and $\\Theta$ as the space of possible outputs of an algorithm. ", "page_idx": 2}, {"type": "text", "text": "Definition 1. (Differential Privacy $(D P)\\,[I J)\\,L e t\\,\\epsilon,\\delta\\ge0.$ . A function $A:\\Omega\\to\\Theta$ is $(\\epsilon,\\delta)$ -DP if for any measurable subset $O\\subseteq\\Theta$ and any two adjacent datasets $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\nP(A(\\mathcal{D})\\in O)\\leq e^{\\epsilon}P(A(\\mathcal{D}^{\\prime})\\in O)+\\delta,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "in which $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ are adjacent if they differ only on a single sample. Moreover, $\\boldsymbol{\\mathcal{A}}$ is $\\epsilon$ -DP if (1) holds with $\\delta=0$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 1 is about item-level DP. In this work, we discuss the case where the dataset contains multiple users, i.e. $\\mathcal{D}=\\{D_{1},...,D_{n}\\}$ , with the $i$ -th user having $m_{i}$ samples. Considering that the sample sizes of users are usually much less sensitive [72], throughout this work, we assume that the local sample sizes $m_{i}$ are public information. Under this setting, user-level DP is defined as follows. ", "page_idx": 2}, {"type": "text", "text": "Definition 2. (User-level DP [28]) Two datasets $\\mathcal{D}$ , $\\mathcal{D}^{\\prime}$ are user-level adjacent if they differ in items belonging to only one user. In particular, if $\\mathcal{D}\\,=\\,\\{D_{1},.\\,.\\,.\\,,D_{n}\\}$ , $\\mathcal{D}^{\\prime}\\,=\\,\\{D_{1}^{\\prime},\\cdot\\cdot\\cdot,D_{n}^{\\prime}\\}$ , in which $|D_{i}|=|D_{i}^{\\prime}|=m_{i}$ for all $i,$ , and there is only one $i\\in[n]$ such that $D_{i}\\neq D_{i}^{\\prime},$ , then $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ are user-level adjacent. A function $\\boldsymbol{\\mathcal{A}}$ is user-level $(\\epsilon,\\delta)$ -DP if (1) is satisfied for any two user-level adjacent datasets $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ . ", "page_idx": 2}, {"type": "text", "text": "Since $m_{i}$ , $i=1,\\ldots,N$ are public information, in Definition 2, it is required that $|D_{i}|=|D_{i}^{\\prime}|$ , which means that two adjacent datasets need to have the same sample sizes for all users. We then state some concepts related to sensitivity, which describes the maximum change of the output after replacing a user with another one: ", "page_idx": 2}, {"type": "text", "text": "Definition 3. (Sensitivity) Define the local sensitivity of function $f$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\nL S_{f}({\\mathcal{D}})=\\operatorname*{sup}_{d_{H}({\\mathcal{D}},{\\mathcal{D}}^{\\prime})=1}\\left\\|f({\\mathcal{D}})-f({\\mathcal{D}}^{\\prime})\\right\\|,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "in which $\\begin{array}{r}{d_{H}(\\mathcal{D},\\mathcal{D}^{\\prime})=\\sum_{i=1}^{n}\\mathbf{1}(D_{i}\\neq D_{i}^{\\prime})}\\end{array}$ denotes the Hamming distance. The global sensitivity of $f$ is $G S_{f}=\\operatorname*{sup}_{\\mathscr D}L S_{f}(\\mathscr D)$ . ", "page_idx": 2}, {"type": "text", "text": "Adding noise proportional to the global sensitivity can be inefficient, especially for user-level problems. In this work, we use the smooth sensitivity framework [40]. ", "page_idx": 2}, {"type": "text", "text": "Definition 4. (Smooth sensitivity) $S_{f}$ is a $\\beta$ -smooth sensitivity of $f,\\;i f(l)$ for any $\\mathcal{D}$ , $S_{f}({\\mathcal{D}})\\geq$ $L S_{f}({\\mathcal{D}})$ ; (2) for any neighboring $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ , $S_{f}({\\mathcal{D}})\\le e^{\\beta}S_{f}({\\mathcal{D}}^{\\prime})$ . ", "page_idx": 3}, {"type": "text", "text": "The smooth sensitivity can be used to determine the scale of noise. In this work, the noise follows Gaussian distribution. It has been shown in [40] that if ${\\bf W}\\sim{\\mathcal{N}}(0,(S^{2}(D)/\\alpha^{2}){\\bf I})$ , then the final output $f(\\mathcal{D})+\\mathbf{W}$ is $(\\epsilon,\\delta)$ -DP for the following $(\\alpha,\\beta)$ pair: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\alpha=\\left\\{\\begin{array}{c c}{\\frac{\\epsilon}{\\sqrt{\\ln\\frac{1}{\\delta}}}}&{\\mathrm{if}\\quad d=1}\\\\ {\\frac{\\epsilon}{5\\sqrt{2\\ln\\frac{2}{\\delta}}}}&{\\mathrm{if}\\quad d>1,}\\end{array}\\right.\\beta=\\left\\{\\begin{array}{c c}{\\frac{\\epsilon}{2\\ln\\frac{1}{\\delta}}}&{\\mathrm{if}\\quad d=1}\\\\ {\\frac{\\epsilon}{4(d+\\ln\\frac{2}{\\delta})}}&{\\mathrm{if}\\quad d>1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Notations. Throughout this paper, $\\lVert\\cdot\\rVert$ denotes the $\\ell_{2}$ norm by default. $a\\lesssim b$ means that $a\\leq C b$ for some absolute constant $C$ , and $\\gtrsim$ is defined conversely. $a\\sim b$ if $a\\lesssim b$ and $b\\lesssim a$ . ", "page_idx": 3}, {"type": "text", "text": "4 The Proposed Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section introduces the algorithm structures. Details about parameter selection are discussed together with the theoretical analysis in Section 5 and 6, respectively. For a dataset $\\mathcal{D}=\\{D_{1},...,D_{n}\\}$ , in which $D_{i}\\subset\\mathbb{R}^{d}$ is the local dataset of the $i$ -th user, denote $m_{i}=|D_{i}|$ as the sample size of the $i$ -th user. Denote $N$ as the total number of samples, then $\\textstyle N=\\sum_{i=1}^{n}{\\dot{m}}_{i}$ . We calculate the user-wise mean first, i.e. $\\begin{array}{r}{\\mathbf{y}_{i}(\\mathcal{D})\\,=\\,(\\underline{{1}}/m_{i})\\sum_{\\mathbf{x}\\in D_{i}}\\mathbf{x}}\\end{array}$ . Consider that u sers can be unbalanced, we assign a weight $w_{i}$ for the $i$ -th user. The new proposed estimator (before adding noise) is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{0}(\\mathcal{D})=\\arg\\operatorname*{min}_{\\mathbf{s}}\\sum_{i=1}^{n}w_{i}\\phi_{i}\\big(\\mathbf{s},\\mathbf{y}_{i}(\\mathcal{D})\\big),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "in which $w_{i}$ is the normalized weight of user $i$ , i.e. $\\textstyle\\sum_{i=1}^{n}w_{i}=1$ . $\\phi_{i}$ is the Huber loss function: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi_{i}(\\mathbf{s},\\mathbf{y})=\\left\\{\\begin{array}{c l}{\\frac{1}{2}\\left\\|\\mathbf{s}-\\mathbf{y}\\right\\|^{2}}&{\\mathrm{if}\\quad\\left\\|\\mathbf{s}-\\mathbf{y}\\right\\|\\leq T_{i}}\\\\ {T_{i}\\left\\|\\mathbf{s}-\\mathbf{y}\\right\\|-\\frac{1}{2}T_{i}^{2}}&{\\mathrm{if}\\quad\\left\\|\\mathbf{s}-\\mathbf{y}\\right\\|>T_{i}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$T_{i}$ is the connecting point between quadratic and linear parts of Huber loss. For balanced users, $w_{i}$ and $T_{i}$ are the same for all users. For imbalanced users, $w_{i}$ and $T_{i}$ are set differently depending on the per-user sample sizes. The general guideline is that $w_{i}$ increases with $m_{i}$ , while $T_{i}$ decreases with $m_{i}$ . The final output needs to satisfy user-level $(\\epsilon,\\delta)$ -DP requirement. Hence, we set ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mu}(\\mathcal{D})=\\mathrm{Clip}(\\hat{\\mu}_{0}(\\mathcal{D}),R_{c})+\\mathbf{W},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "in which $\\mathrm{Clip}(\\mathbf{v},R_{c})=\\mathbf{v}\\operatorname*{min}\\left(1,R_{c}/\\left\\|\\mathbf{v}\\right\\|\\right)$ is the function that clips the result into $B_{d}(\\mathbf{0},R_{c})$ . The clipping operation is used to control the worst case sensitivity. W denotes the noise added to the estimated value. In this work, we use Gaussian noise $\\mathbf{W}\\sim\\dot{\\mathcal{N}}(0,\\sigma^{2}\\mathbf{I})$ . The clipping radius $R_{c}$ is determined by the knowledge of the range of $\\mu$ . Given a prior knowledge $\\|\\mu\\|\\leq R$ , then we can set $R_{c}=R$ . Actually, similar to [47], our analysis shows that $R_{c}$ can grow exponentially with $n$ without significantly compromising the accuracy. The noise parameter $\\sigma^{2}$ needs to be determined carefully through a detailed sensitivity analysis. ", "page_idx": 3}, {"type": "text", "text": "Now we comment on the implementation. As has been discussed in [68], minimizing multidimensional Huber loss can be implemented by a modification of an iterative Weiszfeld\u2019s algorithm [73, 74]. The overall worst-case time complexity is $O(n d/\\xi)$ , in which $\\xi$ is the desired precision. Moreover, for bounded support, with high probability, the algorithm requires only one iteration with time complexity $O(n d)$ . Details can be found in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "5 Analysis: Balanced Users ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, to gain some insights, we focus on the relatively simpler case and assume that all users have the same number of items, i.e. $m_{i}$ are equal for all $i$ . Therefore, throughout this section, we omit the subscript and use $m$ to denote the local sample size. Now $w_{i}=1/n$ for all $i$ . $T_{i}$ are also the same for all users, denoted as $T$ in this section. Let $\\mathbf{X}$ be a random vector, whose statistical mean $\\boldsymbol{\\mu}:=\\mathbb{E}[\\mathbf{X}]$ is unknown. Given a dataset $\\mathcal{D}=\\{D_{1},...,D_{n}\\}$ , the goal is to estimate $\\mu$ , while satisfying user-level $(\\epsilon,\\delta)$ -DP. We present the sensitivity analysis for the general dataset $\\mathcal{D}$ first, and then analyze the estimation error for a randomly generated dataset. To begin with, define ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nZ({\\mathcal{D}})=\\operatorname*{max}_{i\\in[n]}\\left\\|\\mathbf{y}_{i}({\\mathcal{D}})-{\\bar{\\mathbf{y}}}({\\mathcal{D}})\\right\\|,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "in which $\\begin{array}{r}{\\bar{\\mathbf{y}}(\\mathcal{D})=(1/n)\\sum_{i=1}^{n}\\mathbf{y}_{i}(\\mathcal{D})}\\end{array}$ is the overall average. A small $Z(\\mathcal{D})$ indicates that the userwise means $\\mathbf{y}_{i}$ are concentrated within a small region. If $Z(\\mathcal{D})$ is large, then there are some outliers. The sensitivity is bounded separately depending on whether the user-wise means are well concentrated. Throughout this section, we use $L S(D)$ to denote the local sensitivity of $\\mathrm{Clip}(\\hat{\\mu}_{0}(\\mathcal{D}),R_{c})$ , in which the subscript $f$ in (2) is omitted. ", "page_idx": 4}, {"type": "text", "text": "1) No outliers. We first bound the local sensitivity for the case with $Z(\\mathcal{D})<(1-2/n)T$ , in which $T$ represents $T_{i}$ in (5) for all $i$ . It requires that all $\\mathbf{y}_{i}$ \u2019s are not far away from their average. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1. If $Z(\\mathcal{D})<(1-2/n)T$ , then ", "page_idx": 4}, {"type": "equation", "text": "$$\nL S(\\mathcal{D})\\leq\\frac{T+Z(\\mathcal{D})}{n-1}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The proof of Lemma 1 is shown in Appendix B. Here we provide some intuition. From the definition (2), local sensitivity is the maximum change of $\\hat{\\mu}_{0}(\\mathcal{D})$ after replacing some $\\mathbf{y}_{i}$ with $\\mathbf{y}_{i}^{\\prime}$ . To achieve such maximum change, the optimal choice is to move $\\mathbf{y}_{i}$ sufficiently far away in the direction of $\\hat{\\mu}_{0}(\\mathcal{D})-\\mathbf{y}_{i}$ . The impact of $\\mathbf{y}_{i}$ on $\\hat{\\mu}_{0}(\\mathcal{D})$ is roughly $Z(\\mathcal{D})/(n-1)$ , while the impact of $\\mathbf{y}_{i}^{\\prime}$ is roughly $T/(n-1)$ . Since $\\mathbf{y}_{i}$ and $\\mathbf{y}_{i}^{\\prime}$ are at opposite direction with respect to $\\hat{\\mu}_{0}(\\mathcal{D})$ , the overall effect caused by replacing $\\mathbf{y}_{i}$ with $\\mathbf{y}_{i}^{\\prime}$ is upper bounded by $(T+Z(\\mathcal{D}))/(\\bar{n}-1)$ . ", "page_idx": 4}, {"type": "text", "text": "$2)\\,A$ few outliers. Now we consider a more complex case: $Z(\\mathcal{D})$ is large, and the dataset is not well concentrated, but the number of outliers is not too large. Formally, assume that there exists another dataset $\\mathcal{D}^{*}$ whose Hamming distance to $\\mathcal{D}$ is bounded by $k$ , and $\\mathcal{D}^{*}$ is well concentrated. Then we have the following lemma to bound the local sensitivity. ", "page_idx": 4}, {"type": "text", "text": "Lemma 2. For a dataset $\\mathcal{D}$ , if there exists a dataset $\\mathcal{D}^{*}$ such that $d_{H}(\\mathcal{D},\\mathcal{D}^{*})\\leq k_{\\cdot}$ , in which $d_{H}$ is the Hamming distance (see Definition $3$ ), and $Z(\\mathnormal{D}^{*})<(1-2(k+1)/n)\\,T$ , then $L S(\\mathcal{D})\\leq2T/(n\\!-\\!k)$ . ", "page_idx": 4}, {"type": "text", "text": "The proof of Lemma 2 is shown in Appendix C. The intuition is that since there exists a well concentrated dataset $\\mathcal{D}^{*}$ with $d_{H}(\\mathcal{D},\\mathcal{D}^{*})\\leq k,\\,^{\\prime}$ $\\mathcal{D}$ contains no more than $k$ outliers. At least $n-k$ other user-wise mean values fall in a small region. To achieve the maximum change of $\\hat{\\mu}_{0}(\\mathcal{D})$ , the optimal choice is to replace an outlier $\\mathbf{y}_{i}$ with $\\mathbf{y}_{i}^{\\prime}$ , such that $\\mathbf{y}_{i}-\\hat{\\mu}_{0}(\\mathcal{D})$ and $\\mathbf{y}_{i}^{\\prime}-\\hat{\\mu}_{0}(\\mathcal{D})$ have opposite directions. Each of them has an effect of roughly $T/(n-k)$ on $\\hat{\\mu}_{0}(\\mathcal{D})$ , thus the overall change is $2T/(n-k)$ . ", "page_idx": 4}, {"type": "text", "text": "3) Other cases. For all other cases, since $\\|\\mathrm{Clip}(\\hat{\\mu}_{0},R_{c})\\|\\leq R_{c}$ always hold, the local sensitivity can be bounded by $L S(D)\\leq2R_{c}$ . ", "page_idx": 4}, {"type": "text", "text": "From the analysis above, we now construct a valid smooth sensitivity. Define ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta(\\mathcal{D})=\\operatorname*{min}\\left\\{k|\\exists\\mathcal{D}^{*},d_{H}(\\mathcal{D},\\mathcal{D}^{*})\\leq k,Z(\\mathcal{D}^{*})<\\frac{1}{2}T\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$\\Delta(\\mathcal{D})$ can be viewed as the number of outliers. From (9), if $\\mathcal{D}$ is well concentrated, with $Z(D)<T/2$ , then $\\Delta(\\mathcal{D})=0$ . Now we define $G(\\mathcal{D},k)$ as follows. ", "page_idx": 4}, {"type": "text", "text": "Definition 5. (a) If $Z(\\mathcal{D})<(1-2/n)T$ , $k=0$ , then $G(\\mathcal{D},0)=(T+Z(\\mathcal{D}))/(n-1),$ ; ", "page_idx": 4}, {"type": "text", "text": "$(b)$ If conditions in (a) are not satisfied, and $\\Delta(\\mathcal{D})$ exists, $i f k\\le n/4-1-\\Delta(\\mathcal{D}),$ ", "page_idx": 4}, {"type": "equation", "text": "$$\nG(\\mathcal{D},k)=\\frac{2T}{n-k-\\Delta(\\mathcal{D})};\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "(c) If conditions in (a) and $(b)$ are not satisfied, then $G(D,k)=2R_{c}$ . ", "page_idx": 4}, {"type": "text", "text": "Based on Definition 5, the smooth sensitivity is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\nS(\\mathcal{D})=\\operatorname*{max}_{k}e^{-\\beta k}G(\\mathcal{D},k),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "in which $\\beta$ is determined in (3). Then we show that $S(\\mathcal{D})$ is a valid smooth sensitivity, and the privacy requirement is satisfied. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. With $\\sigma=S(\\mathcal{D})/\\alpha$ , in which $\\alpha$ is determined in (3), $\\mathbf{W}\\sim{\\mathcal{N}}(0,\\sigma^{2}\\mathbf{I})$ , the estimator $\\hat{\\mu}$ defined in (6) is $(\\epsilon,\\delta)$ -DP. ", "page_idx": 5}, {"type": "text", "text": "To prove Theorem 1, we need to show that $S$ is a valid smooth sensitivity, i.e. two conditions in Definition 4 are satisfied. The detailed proof is provided in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "In the analysis above, all results are derived for a general dataset $\\mathcal{D}$ . In the remainder of this section, we analyze the performance of estimator (6) for randomly generated samples. ", "page_idx": 5}, {"type": "text", "text": "5.1 Bounded Support ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Let $\\mathbf{X}$ be a random vector generated from distribution $P$ with an unknown statistical mean $\\boldsymbol{\\mu}:=\\mathbb{E}[\\mathbf{X}]$ . We make the following assumption: ", "page_idx": 5}, {"type": "text", "text": "Assumption 1. X is supported on $B_{d}(0,R)=\\{{\\bf u}|\\,\\|{\\bf u}\\|\\leq R\\}\\subset\\mathbb{R}^{d}.$ . ", "page_idx": 5}, {"type": "text", "text": "The mean squared error is analyzed in the following theorem. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Let $R_{c}\\;=\\;R_{v}$ , and $T\\,=\\,C_{T}R\\ln(m n^{3}(d+1))/\\sqrt{m}$ with $C_{T}\\,>\\,16\\sqrt{2/3}$ . If $n~>$ $(4/\\beta)\\ln(n R_{c}/T)$ , then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert{\\hat{\\mu}}(D)-\\mu\\right\\Vert^{2}\\right]\\lesssim{\\frac{R^{2}}{m n}}+{\\frac{d R^{2}}{m n^{2}\\epsilon^{2}}}\\ln(m n d)\\ln{\\frac{1}{\\delta}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proof of Theorem 2 is shown in Appendix E. With the selection rule of $T$ in Theorem 2, it is shown that with high probability, $\\Delta(\\mathcal{D})=0$ , indicating that $\\mathcal{D}$ is expected to be well concentrated around the population mean $\\mu$ . The smooth sensitivity $S(\\mathcal{D})$ can then be bounded. The first term in the right hand side of (12) is the non-private estimation error, i.e. the error of $\\hat{\\mu}_{0}(\\mathcal{D})$ , while the second term is the error caused by noise W. The condition $n>(4/\\beta)\\ln(n R_{c}/T)$ is necessary, since it ensures that $G(D,k)=2R_{c}$ (Definition 5 (c)) occurs only for sufficiently large $k$ , thus $e^{-\\beta k}$ is small, and does not affect the calculation of $S(\\mathcal{D})$ in (11). A lower bound on the number of users $n$ has also been imposed for the two-stage method [28]. ", "page_idx": 5}, {"type": "text", "text": "For the simplest case with bounded support and balanced users, the two-stage approach in [28] is already nearly optimal (Corollary 1 in [28]). Therefore, improvement in polynomial factors is impossible. Nevertheless, we still improve on the logarithm factor. The main purpose of Theorem 2 is to show that our improvement on heavy-tailed distributions and imbalanced users is not at the cost of hurting the performance under the simplest case with bounded distributions and balanced users. ", "page_idx": 5}, {"type": "text", "text": "5.2 Unbounded Support ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Now we analyze the heavy-tailed case. Instead of requiring $\\mathbf{X}\\in B_{d}(0,R)$ , we now assume that X has $p$ -th bounded moment. ", "page_idx": 5}, {"type": "text", "text": "Assumption 2. Suppose that $\\boldsymbol{\\mu}\\in B(\\mathbf{0},R)$ , and the $p$ -th $\\,p\\geq2,$ ) moment of $\\mathbf{X}-{\\boldsymbol{\\mu}}$ is bounded, i.e.   \n$\\mathbb{E}[\\|\\mathbf{X}\\|^{\\bar{p}}]\\leq M_{p}$ . ", "page_idx": 5}, {"type": "text", "text": "In Assumption 2, higher $p$ indicates a lighter tail and vice versa. We then show the convergence rate of mean squared error in Theorem 3. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3. Let $R_{c}=R$ , and ", "page_idx": 5}, {"type": "equation", "text": "$$\nT=C_{T}\\operatorname*{max}\\left\\{\\sqrt{\\frac{1}{m}\\ln\\frac{3(d+1)}{\\nu}},2(3m)^{\\frac{1}{p}-1}\\nu^{-\\frac{1}{p}}\\ln\\frac{3(d+1)}{\\nu}\\right\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $\\nu=\\sqrt{d}/(n\\epsilon)$ and $C_{T}>8M_{p}^{\\frac{1}{p}}$ . If $n>8(1+(1/\\beta)\\ln(n/2T))$ , then under Assumption 2, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|\\hat{\\mu}(D)-\\mu\\right\\|^{2}\\right]\\lesssim\\frac{1}{m n}+\\left[\\frac{d\\ln(n d)}{m n^{2}\\epsilon^{2}}+\\left(\\frac{d}{m^{2}n^{2}\\epsilon^{2}}\\right)^{1-\\frac{1}{p}}\\ln^{2}(n d)\\right]\\ln\\frac{1}{\\delta}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proof of Theorem 3 is shown in Appendix F. Here we provide an intuitive understanding. From the central limit theorem, each user-wise mean $\\mathbf{y}_{i}(\\mathcal{D})$ is the average of $m$ i.i.d variables, thus it has a Gaussian tail around the population $\\mu$ . However, since $\\mathbf{X}$ is only required to have $p$ -th bounded moment, the tail probability away from $\\mu$ is still polynomial. The formal statement of the tail bound is shown in Lemma 13 in the appendix. Then the threshold $T$ is designed based on the high probability upper bound of $Z(\\mathcal{D})$ to ensure that with high probability, $\\Delta(\\mathcal{D})$ is small. Regarding the result, we have the following remarks. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Remark 1. Here we comment on small and large m limits. If $m=1$ , the right hand side of (14) becomes $O(1/n\\!+\\!(d/n^{2}\\epsilon^{2})^{1-1/p})$ , which matches existing analysis on item-level $D P$ for heavy-tailed random variables $I^{49}J$ . For the opposite limit, with $m^{1-2/p}\\gtrsim n^{2/p}\\ln(n d)$ , then the convergence rate is the same as the case with bounded support, indicating that the tail of sample distribution does not affect the error more than a constant factor. ", "page_idx": 6}, {"type": "text", "text": "Remark 2. Now we compare (14) with the two-stage approach. Following the analysis in [28], it can be shown that the bound of mean squared error in $[28]$ is $\\tilde{O}((d/(n^{2}\\epsilon^{2}))(1/m+m^{4/p-2}n^{6/p}))$ (we refer to Appendix G for details). Therefore, we have achieved an improved rate in (14). ", "page_idx": 6}, {"type": "text", "text": "The theoretical results in this section are summarized as follows. If the support is bounded, our method has the same convergence rate as the existing method. For heavy-tailed distributions, our approach significantly reduces the error, since our method avoids the clipping process. In federated learning applications, it is common for gradients to have heavy-tailed distributions [35\u201338], thus our method has the potential of improving the performance of federated learning under DP requirements. Apart from heavy-tailed distributions, another common characteristic in reality is that users are usually imbalanced. We analyze it in the next section. ", "page_idx": 6}, {"type": "text", "text": "6 Analysis: Imbalanced Users ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Now we analyze the general case where $m_{i}$ , $i=1,\\hdots,n$ are different. Recall that for balanced users, we have defined $Z(\\mathcal{D})$ in (7) that finds the maximum distance from $\\mathbf{y}_{i}(\\mathcal{D})$ to their average $\\bar{\\mathbf{y}}(\\mathcal{D})$ . For imbalanced users, instead of taking the maximum, we define $Z_{i}$ separately for each $i$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\nZ_{i}(\\mathcal{D})=\\left|\\left|\\bar{\\mathbf{y}}(\\mathcal{D})-\\mathbf{y}_{i}(\\mathcal{D})\\right|\\right|,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "in which $\\begin{array}{r}{\\bar{\\mathbf{y}}(\\mathcal{D})=\\sum_{i=1}^{n}w_{i}\\mathbf{y}_{i}(\\mathcal{D})}\\end{array}$ is the average of samples all over the dataset. ", "page_idx": 6}, {"type": "text", "text": "From now on, without loss of generality, suppose that users are arranged in ascending order of $m_{i}$ , i.e. $m_{1}\\leq...\\leq m_{n}$ . Define ", "page_idx": 6}, {"type": "equation", "text": "$$\nh(\\mathcal{D},k)=\\frac{\\sum_{i=n-k+1}^{n}w_{i}(T_{i}+Z_{i}(\\mathcal{D}))}{\\sum_{i=1}^{n-k}w_{i}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Similar to the case with balanced users, we analyze the sensitivity for datasets with no outliers, a few outliers, and other cases separately. ", "page_idx": 6}, {"type": "text", "text": "1) No outliers. We show the following lemma. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3. If $h(\\mathcal D,1)\\leq\\operatorname*{min}_{i}(T_{i}-Z_{i}(\\mathcal D))$ , then $L S(\\mathcal{D})\\leq h(\\mathcal{D},1)$ . ", "page_idx": 6}, {"type": "text", "text": "The general idea of the proof is similar to Lemma 1. However, the details become more complex since now the samples are unbalanced. The detailed proof is shown in Appendix $_\\mathrm{H}$ . ", "page_idx": 6}, {"type": "text", "text": "2) A few outliers. Similar to Lemma 2, we find a neighboring dataset $\\mathcal{D}^{*}$ that is well concentrated and then bounds the local sensitivity. The formal statement is shown in the following lemma. ", "page_idx": 6}, {"type": "text", "text": "Lemma 4. For a dataset $\\mathcal{D}$ , if there exists another dataset $\\mathcal{D}^{*}$ such that $h(\\mathcal{D}^{*},k+1)<\\operatorname*{min}_{i}(T_{i}-$ $Z_{i}(\\mathcal{D}^{*}))$ , then $\\begin{array}{r}{L S(\\mathcal{D})\\leq2\\operatorname*{max}_{i}(w_{i}T_{i})/\\sum_{i=1}^{n-k-1}w_{i}}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "The proof of Lemma 4 is shown in Appendix I, which just follows the proof of Lemma 2. ", "page_idx": 6}, {"type": "text", "text": "3) Other cases. Finally, for all cases not satisfying the conditions in Lemma 3 and 4, we can just bound the local sensitivity with $2R_{c}$ , i.e. $L S(D)\\leq2R_{c}$ . ", "page_idx": 6}, {"type": "text", "text": "Similar to the case with balanced users, now we define ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Delta(\\mathcal{D})=\\operatorname*{min}\\{k|\\exists\\mathcal{D}^{*},d_{H}(\\mathcal{D},\\mathcal{D}^{*})=k,h(\\mathcal{D}^{*},k_{0})<\\operatorname*{min}_{i}(T_{i}-Z_{i}(\\mathcal{D}))\\},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "in which $k_{0}$ is any integer, and can be viewed as a design parameter. Correspondingly, the smooth sensitivity $G(\\mathcal{D},k)$ is defined as follows. ", "page_idx": 6}, {"type": "text", "text": "Definition 6. (a) If $h(\\mathcal D,1)\\leq\\operatorname*{min}_{i}(T_{i}-Z_{i}(\\mathcal D))$ , then $G(\\mathcal{D},0)=h(\\mathcal{D},1)$ ; ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "(b) If the conditions in (a) are not satisfied, and $\\Delta(\\mathcal{D})$ exists, then for all $k\\le k_{0}-\\Delta(D)-1$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\nG(\\mathcal{D},k)=\\frac{2\\underset{i\\in[n]}{\\mathrm{max}}w_{i}T_{i}}{\\sum_{i=1}^{n-\\Delta(\\mathcal{D})-k-1}w_{i}};\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "(c) If the conditions in both (a) and $(b)$ are not satisfied, then $G(D,k)=2R_{c}$ . ", "page_idx": 7}, {"type": "text", "text": "We still use the same settings of $\\alpha$ and $\\beta$ as in the case with balanced samples. With smooth sensitivity calculated using (11), the privacy requirement is satisfied: ", "page_idx": 7}, {"type": "text", "text": "Theorem 4. Let $\\mathbf{W}\\sim{\\mathcal{N}}(0,(S(\\mathcal{D})^{2}/\\alpha^{2})\\mathbf{I})$ , in which $S({\\mathcal{D}})=\\operatorname*{max}_{k}e^{-\\beta k}G({\\mathcal{D}},k),$ , then the estimator $\\hat{\\mu}$ is $(\\epsilon,\\delta){-}D P.$ ", "page_idx": 7}, {"type": "text", "text": "Now we analyze the convergence of the algorithm. We begin with Assumption 3. Intuitively, this assumption requires that the users can not be too unbalanced. At least half samples belong to users whose sample sizes are not very large. ", "page_idx": 7}, {"type": "text", "text": "Assumption 3. Suppose there exists a constant $\\gamma\\,\\geq\\,1$ . Let $k_{c}\\,=\\,\\operatorname*{min}\\{i|m_{i}\\,>\\,\\gamma N/n\\}$ , then $\\begin{array}{r}{\\sum_{i=k_{c}}^{n}\\hat{m}_{i}\\leq N/2}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "In Assumption 3, $\\gamma$ can be viewed as the degree of imbalance. For a better explanation, we provide the following examples: ", "page_idx": 7}, {"type": "text", "text": "\u2022 If users are balanced, then $\\gamma=1$ ; ", "page_idx": 7}, {"type": "text", "text": "\u2022 If the $i$ -th user has $k i$ samples (which means that the number of items belonging to each user is linear in its order), then for large $n,\\gamma$ is approximately $\\sqrt{2}$ . ", "page_idx": 7}, {"type": "text", "text": "In general, $\\gamma$ is large if users are highly imbalanced. Under Assumption 3, the convergence of mean squared error is shown in Theorem 5. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5. Let the weights of users in (4) be $\\begin{array}{r}{w_{i}=m_{i}{\\wedge}m_{c}/(\\sum_{j=1}^{n}m_{j}{\\wedge}m_{c})}\\end{array}$ , in which $m_{c}=\\gamma N/n$ . Moreover, let ", "page_idx": 7}, {"type": "equation", "text": "$$\nT_{i}=C_{T}\\sqrt{\\frac{R^{2}\\ln(N n^{2}(d+1))}{m_{i}\\wedge m_{c}}},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "in which $C_{T}>16\\sqrt{2/3}$ . In (17), $k_{0}=\\lfloor n/8\\gamma\\rfloor$ . ", "page_idx": 7}, {"type": "text", "text": "With the parameters above, if $\\dot{n}>8\\gamma(1+(1/2\\beta)\\ln(N n))$ , then under Assumption $^{\\,l}$ and 3, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert{\\hat{\\mu}}({\\mathcal{D}})-\\mu\\right\\Vert^{2}\\right]\\lesssim{\\frac{R^{2}}{N}}+{\\frac{d R^{2}\\gamma}{N n\\epsilon^{2}}}\\ln^{2}(N n d)\\ln{\\frac{1}{\\delta}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The proof of Theorem 5 is shown in Appendix J. If users are balanced, then $\\gamma=1$ , with $N=n m$ , (20) reduces to (12). From (20) and Assumption 3 it can be observed that our method is much less sensitive to a single large $m_{i}$ . As long as $m_{i}$ are not very large for most users, the convergence rate of mean squared error is not affected. There are two intuitive reasons. Firstly, $w_{i}$ is upper bounded by $m_{c}/(\\sum_{j=1}^{n}m_{j}\\wedge m_{c})$ , thus the worst-case sensitivity is controlled. Secondly, $T_{i}$ are set adaptively to achieve a good tradeoff between sensitivity and bias. With larger $m_{i}$ , smaller $T_{i}$ is used, and vice versa. We refer to Appendix $\\mathrm{G}$ for comparison with the two-stage approach. ", "page_idx": 7}, {"type": "text", "text": "7 Numerical Examples ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we show numerical experiments. We compare the performance of our new Huber loss minimization approach (denoted as HLM) versus the two-stage approach proposed in [28], called Winsorized Mean Estimator (denoted as WME). ", "page_idx": 7}, {"type": "text", "text": "7.1 Balanced Users ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Here all users have the same sizes of local datasets. In the following experiments, we fix $\\epsilon=1$ and $\\delta=10^{-5}$ . For a fair comparison, the parameter $T$ for our method as well as $\\tau$ in [28] are both tuned optimally for each case. Figure 1 shows the curve of mean squared error. Let the number of users $n$ be either $1,000$ and $10,000$ . In each curve, $n$ is fixed, while the number of samples per user $m$ varies from 1 to $1,000$ . The results are plotted in logarithm scale. Figure 1(a)-(c) show the results of uniform distribution in $[-1,1]$ , Gaussian distribution ${\\mathcal{N}}(0,1)$ , and the Lomax distribution, whose pdf is $f(x)=a/(1+x)^{\\stackrel{.}{a+1}}$ (we use $a=4$ in Figure 1(c)). Figure 1 (d)-(f) shows the corresponding experiments with dimensionality $d=3$ . Finally, Figure 1 (g) and (h) show the results using the IPUMS dataset [75] for total income and salary, respectively, which are typical examples of data following heavy-tailed distributions. ", "page_idx": 8}, {"type": "image", "img_path": "TutGINeJzZ/tmp/747845ea24f8dbcc448a2637d277d2dc0f14a23052dd7e18b10cece5e02a94c2.jpg", "img_caption": ["Figure 1: Convergence of mean squared error with balanced users. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 1 (a) and (b) show that for one-dimensional uniform and Gaussian distribution, the Huber loss minimization approach has nearly the same performance as the two-stage method. Our explanation is that uniform and Gaussian distributions are symmetric, with no tails or light tails, thus the clipping operation does not introduce additional bias. However, for heavy-tailed and skewed distribution, such as Lomax distribution, our new method has a significantly faster convergence rate than the two-stage method. These results agree with the theoretical analysis, which shows that our method reduces the clipping bias. With higher dimensionality, Figure 1(d)-(f) show that the advantage of the practical performance of our method becomes more obvious. ", "page_idx": 8}, {"type": "text", "text": "7.2 Imbalanced Users ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Now we show the performance with unbalanced users. For some $\\gamma\\geq1$ , let $s_{i}\\,=\\,\\lceil N(i/n)^{\\gamma}\\rceil$ for $i=0,\\dots,n$ , and $m_{i}=s_{i}-s_{i-1}$ for $i=1,\\hdots,n$ . It can be shown that Assumption 3 is satisfied with $\\gamma$ . Therefore, according\u221a to the analysis in Section 6, we let $\\begin{array}{r}{w_{i}=m_{i}\\wedge m_{c}/\\sum_{j}m_{j}\\wedge m_{c}}\\end{array}$ , with $m_{c}=\\gamma N/n$ , and $T_{i}=A/\\sqrt{m_{i}\\wedge m_{c}},$ , in which $A$ is tuned optimally for each case. The selection of $T_{i}$ may be slightly different from (19) in Theorem 5. In Theorem 5, $T_{i}$ is selected to minimize the theoretical upper bound. To ensure that the analysis is mathematically rigorous, the upper bound of estimation error is larger than the truth. Therefore, the optimal value of $T_{i}$ in practice is slightly different from that derived in theories. Note that such parameter tuning does not require additional privacy budget since in each experiment, $T_{i}$ are hyperparameters that is fixed before knowing the value of each sample. They are not determined adaptively based on the data. Figure 2 shows the growth curve of mean squared error with respect to $\\gamma$ . ", "page_idx": 8}, {"type": "text", "text": "From Figure 2, it can be observed that with the increase of $\\gamma$ , the two-stage method degrades, while the Huber loss minimization approach performs significantly more stable. ", "page_idx": 8}, {"type": "image", "img_path": "TutGINeJzZ/tmp/d130627cdfd899f7776a963f9e2912efe5c3a4253288856078b6571fb1c795bd.jpg", "img_caption": ["Figure 2: Growth of mean squared error with degree of imbalance $\\gamma$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have proposed a new approach to mean estimation under user-level DP based on Huber loss minimization. The sensitivity is bounded for all possible datasets. Based on the sensitivity analysis, we use the smooth sensitivity framework to determine the noise added to the result. We have also derived the bound on the mean squared error for various cases. The result shows that our method reduces the error for heavy-tailed distributions, and is more suitable to imbalanced users. It is promising to extend our approach to more learning problems, such as calculating average gradients in federated learning. ", "page_idx": 9}, {"type": "text", "text": "Limitations: The limitations of our work include: (1) There are some requirements on the minimum number of users $n$ . while entirely removing this condition is impossible, to make our method more practical, we expect that it can be somewhat weakened. (2) The case with local sample sizes $m_{i}$ also being private has not been analyzed. We will leave these two points as future works. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The work of L.Shen is supported by the STI 2030\u2014Major Projects (No. 2021ZD0201405). The work of Z.Liu is supported by the National Natural Science Foundation of China (No.62132008 and U22B2030), Natural Science Foundation of Jiangsu Province (BK20220075). The work of L.Lai is supported by the National Science Foundation under grants ECCS-20-00415 and CCF-22-32907. ", "page_idx": 9}, {"type": "text", "text": "We thank Prof.Gautam Kamath for his fruitful discussions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Dwork, C., F. McSherry, K. Nissim, et al. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pages 265\u2013284. Springer, 2006.   \n[2] Dwork, C., G. N. Rothblum, S. Vadhan. Boosting and differential privacy. In 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, pages 51\u201360. IEEE, 2010.   \n[3] Kasiviswanathan, S. P., H. K. Lee, K. Nissim, et al. What can we learn privately? SIAM Journal on Computing, 40(3):793\u2013826, 2011.   \n[4] Dwork, C., A. Roth, et al. The algorithmic foundations of differential privacy. Foundations and Trends\u00ae in Theoretical Computer Science, 9(3\u20134):211\u2013407, 2014.   \n[5] Huang, Z., Y. Liang, K. Yi. Instance-optimal mean estimation under differential privacy. Advances in Neural Information Processing Systems, 34:25993\u201326004, 2021.   \n[6] Asi, H., V. Feldman, K. Talwar. Optimal algorithms for mean estimation under local differential privacy. In International Conference on Machine Learning, pages 1046\u20131056. PMLR, 2022.   \n[7] Hopkins, S. B., G. Kamath, M. Majid. Efficient mean estimation with pure differential privacy via a sum-of-squares exponential mechanism. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 1406\u20131417. 2022.   \n[8] Nguy\u00ean, T. T., X. Xiao, Y. Yang, et al. Collecting and analyzing data from smart device users with local differential privacy. arXiv preprint arXiv:1606.05053, 2016.   \n[9] Sun, C., Y. Fu, J. Zhou, et al. Personalized privacy-preserving frequent itemset mining using randomized response. The Scientific World Journal, 2014, 2014.   \n[10] Chaudhuri, K., C. Monteleoni, A. D. Sarwate. Differentially private empirical risk minimization. Journal of Machine Learning Research, 12(3), 2011.   \n[11] Bassily, R., A. Smith, A. Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In 2014 IEEE 55th annual symposium on foundations of computer science, pages 464\u2013473. IEEE, 2014.   \n[12] Bassily, R., V. Feldman, K. Talwar, et al. Private stochastic convex optimization with optimal rates. Advances in neural information processing systems, 32, 2019.   \n[13] Feldman, V., T. Koren, K. Talwar. Private stochastic convex optimization: optimal rates in linear time. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, pages 439\u2013449. 2020.   \n[14] Asi, H., V. Feldman, T. Koren, et al. Private stochastic convex optimization: Optimal rates in l1 geometry. In International Conference on Machine Learning, pages 393\u2013403. PMLR, 2021.   \n[15] Cheu, A., M. Joseph, J. Mao, et al. Shuffle private stochastic convex optimization. In International Conference on Learning Representations. 2022.   \n[16] Shokri, R., V. Shmatikov. Privacy-preserving deep learning. In Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, pages 1310\u20131321. 2015.   \n[17] Abadi, M., A. Chu, I. Goodfellow, et al. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308\u2013318. 2016.   \n[18] McMahan, H. B., D. Ramage, K. Talwar, et al. Learning differentially private recurrent language models. arXiv preprint arXiv:1710.06963, 2017.   \n[19] Huang, R., H. Zhang, L. Melis, et al. Federated linear contextual bandits with user-level differential privacy. In International Conference on Machine Learning, pages 14060\u201314095. PMLR, 2023.   \n[20] Li, T., L. Song, C. Fragouli. Federated recommendation system via differential privacy. In 2020 IEEE international symposium on information theory (ISIT), pages 2592\u20132597. IEEE, 2020.   \n[21] McSherry, F., I. Mironov. Differentially private recommender systems: Building privacy into the netfilx prize contenders. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627\u2013636. 2009.   \n[22] Geyer, R. C., T. Klein, M. Nabi. Differentially private federated learning: A client level perspective. arXiv preprint arXiv:1712.07557, 2017.   \n[23] McMahan, H. B., G. Andrew, U. Erlingsson, et al. A general approach to adding differential privacy to iterative training procedures. arXiv preprint arXiv:1812.06210, 2018.   \n[24] Wei, K., J. Li, M. Ding, et al. Federated learning with differential privacy: Algorithms and performance analysis. IEEE transactions on information forensics and security, 15:3454\u20133469, 2020.   \n[25] Kairouz, P., H. B. McMahan, B. Avent, et al. Advances and open problems in federated learning. Foundations and Trends\u00ae in Machine Learning, 14(1\u20132):1\u2013210, 2021.   \n[26] Fu, J., Y. Hong, X. Ling, et al. Differentially private federated learning: A systematic review. arXiv preprint arXiv:2405.08299, 2024.   \n[27] Liu, Y., A. T. Suresh, F. X. X. Yu, et al. Learning discrete distributions: user vs item-level privacy. Advances in Neural Information Processing Systems, 33:20965\u201320976, 2020.   \n[28] Levy, D., Z. Sun, K. Amin, et al. Learning with user-level privacy. Advances in Neural Information Processing Systems, 34:12466\u201312479, 2021.   \n[29] Ghazi, B., R. Kumar, P. Manurangsi. User-level differentially private learning via correlated sampling. Advances in Neural Information Processing Systems, 34:20172\u201320184, 2021.   \n[30] Ghazi, B., P. Kamath, R. Kumar, et al. User-level differential privacy with few examples per user. Advances in Neural Information Processing Systems, 36, 2023.   \n[31] Yarlagadda, R. K., J. E. Hershey. Hadamard matrix analysis and synthesis: with applications to communications and signal/image processing, vol. 383. Springer Science & Business Media, 2012.   \n[32] Li, T., M. Sanjabi, A. Beirami, et al. Fair resource allocation in federated learning. In International Conference on Learning Representations. 2019.   \n[33] Duan, M., D. Liu, X. Chen, et al. Self-balancing federated learning with global imbalanced data in mobile systems. IEEE Transactions on Parallel and Distributed Systems, 32(1):59\u201371, 2020.   \n[34] Gong, B., T. Xing, Z. Liu, et al. Adaptive client clustering for efficient federated learning over non-iid and imbalanced data. IEEE Transactions on Big Data, 2022.   \n[35] Gurbuzbalaban, M., U. Simsekli, L. Zhu. The heavy-tail phenomenon in sgd. In International Conference on Machine Learning, pages 3964\u20133975. PMLR, 2021.   \n[36] Simsekli, U., L. Sagun, M. Gurbuzbalaban. A tail-index analysis of stochastic gradient noise in deep neural networks. In International Conference on Machine Learning, pages 5827\u20135837. PMLR, 2019.   \n[37] Zhang, J., S. P. Karimireddy, A. Veit, et al. Why are adaptive methods good for attention models? Advances in Neural Information Processing Systems, 33:15383\u201315393, 2020.   \n[38] Nguyen, T. D., T. H. Nguyen, A. Ene, et al. Improved convergence in high probability of clipped gradient methods with heavy tailed noise. Advances in Neural Information Processing Systems, 36:24191\u201324222, 2023.   \n[39] Huber, P. J. Robust statistics, vol. 523. John Wiley & Sons, 2004.   \n[40] Nissim, K., S. Raskhodnikova, A. Smith. Smooth sensitivity and sampling in private data analysis. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing, pages 75\u201384. 2007.   \n[41] Wang, Z., Y. Huang, M. Song, et al. Poisoning-assisted property inference attack against federated learning. IEEE Transactions on Dependable and Secure Computing, 2022.   \n[42] Lyu, W., S. Zheng, L. Pang, et al. Attention-enhancing backdoor attacks against bert-based models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10672\u201310690. 2023.   \n[43] Lyu, W., L. Pang, T. Ma, et al. Trojvlm: Backdoor attack against vision language models. European Conference on Computer Vision, 2024.   \n[44] Liu, X., W. Kong, S. Kakade, et al. Robust and differentially private mean estimation. Advances in neural information processing systems, 34:3887\u20133901, 2021.   \n[45] Liu, X., P. Jain, W. Kong, et al. Label robust and differentially private linear regression: Computational and statistical efficiency. Advances in Neural Information Processing Systems, 36, 2023.   \n[46] Qi, T., H. Wang, Y. Huang. Towards the robustness of differentially private federated learning. In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, pages 19911\u201319919. 2024.   \n[47] Li, M., T. B. Berrett, Y. Yu. On robustness and local differential privacy. The Annals of Statistics, 51(2):717\u2013737, 2023.   \n[48] Smith, A. Privacy-preserving statistical estimation with optimal convergence rates. In Proceedings of the forty-third annual ACM symposium on Theory of computing, pages 813\u2013822. 2011.   \n[49] Kamath, G., V. Singhal, J. Ullman. Private mean estimation of heavy-tailed distributions. In Conference on Learning Theory, pages 2204\u20132235. PMLR, 2020.   \n[50] Bassily, R., Z. Sun. User-level private stochastic convex optimization with optimal rates. In International Conference on Machine Learning, pages 1838\u20131851. PMLR, 2023.   \n[51] Liu, D., H. Asi. User-level differentially private stochastic convex optimization: Efficient algorithms with optimal rates. In International Conference on Artificial Intelligence and Statistics, pages 4240\u20134248. PMLR, 2024.   \n[52] Cummings, R., V. Feldman, A. McMillan, et al. Mean estimation with user-level privacy under data heterogeneity. Advances in Neural Information Processing Systems, 35:29139\u201329151, 2022.   \n[53] Bun, M., M. Gaboardi, M. Hopkins, et al. Stability is stable: Connections between replicability, privacy, and adaptive generalization. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing, pages 520\u2013527. 2023.   \n[54] Acharya, J., Y. Liu, Z. Sun. Discrete distribution estimation under user-level local differential privacy. In International Conference on Artificial Intelligence and Statistics, pages 8561\u20138585. PMLR, 2023.   \n[55] Zhao, P., L. Shen, R. Fan, et al. Learning with user-level local differential privacy. arXiv preprint arXiv:2405.17079, 2024.   \n[56] Ma, Y., K. Jia, H. Yang. Better locally private sparse estimation given multiple samples per user. arXiv preprint arXiv:2408.04313, 2024.   \n[57] Dwork, C., J. Lei. Differential privacy and robust statistics. In Proceedings of the forty-first annual ACM symposium on Theory of computing, pages 371\u2013380. 2009.   \n[58] Liu, X., W. Kong, S. Oh. Differential privacy and robust statistics in high dimensions. In Conference on Learning Theory, pages 1167\u20131246. PMLR, 2022.   \n[59] Bun, M., T. Steinke. Average-case averages: Private algorithms for smooth sensitivity and mean estimation. Advances in Neural Information Processing Systems, 32, 2019.   \n[60] Asi, H., J. Ullman, L. Zakynthinou. From robustness to privacy and back. In International Conference on Machine Learning, pages 1121\u20131146. PMLR, 2023.   \n[61] Asi, H., J. C. Duchi. Instance-optimality in differential privacy via approximate inverse sensitivity mechanisms. Advances in neural information processing systems, 33:14106\u201314117, 2020.   \n[62] Diakonikolas, I., G. Kamath, D. Kane, et al. Robust estimators in high-dimensions without the computational intractability. SIAM Journal on Computing, 48(2):742\u2013864, 2019.   \n[63] Diakonikolas, I., G. Kamath, D. M. Kane, et al. Being robust (in high dimensions) can be practical. In International Conference on Machine Learning, pages 999\u20131008. PMLR, 2017.   \n[64] Diakonikolas, I., D. M. Kane. Algorithmic high-dimensional robust statistics. Cambridge university press, 2023.   \n[65] Hall, P., M. Jones. Adaptive m-estimation in nonparametric regression. The annals of statistics, pages 1712\u20131728, 1990.   \n[66] Zhao, P., Z. Wan. Robust nonparametric regression under poisoning attack. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 17007\u201317015. 2024.   \n[67] Sardy, S., P. Tseng, A. Bruce. Robust wavelet denoising. IEEE transactions on signal processing, 49(6):1146\u20131152, 2001.   \n[68] Zhao, P., F. Yu, Z. Wan. A huber loss minimization approach to byzantine robust federated learning. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 21806\u201321814. 2024.   \n[69] Avella-Medina, M., C. Bradshaw, P.-L. Loh. Differentially private inference via noisy optimization. The Annals of Statistics, 51(5):2067\u20132092, 2023.   \n[70] Song, S., T. Steinke, O. Thakkar, et al. Evading the curse of dimensionality in unconstrained private glms. In International Conference on Artificial Intelligence and Statistics, pages 2638\u2013 2646. PMLR, 2021.   \n[71] Agarwal, S., G. Kamath, M. Majid, et al. Private mean estimation with person-level differential privacy. arXiv preprint arXiv:2405.20405, 2024.   \n[72] Wei, K., J. Li, M. Ding, et al. User-level privacy-preserving federated learning: Analysis and performance optimization. IEEE Transactions on Mobile Computing, 21(9):3388\u20133401, 2021.   \n[73] Weiszfeld, E., F. Plastria. On the point for which the sum of the distances to n given points is minimum. Annals of Operations Research, 167:7\u201341, 2009.   \n[74] Beck, A., S. Sabach. Weiszfeld\u2019s method: Old and new results. Journal of Optimization Theory and Applications, 164:1\u201340, 2015.   \n[75] Ruggles, S., S. Flood, M. Sobek, et al. IPUMS USA: Version 15.0 [dataset], 2024.   \n[76] Sun, L., J. Tian, G. Muhammad. Fedkc: Personalized federated learning with robustness against model poisoning attacks in the metaverse for consumer health. IEEE Transactions on Consumer Electronics, 2024.   \n[77] Sun, L., J. He. An extensible framework for ecg anomaly detection in wireless body sensor monitoring systems. International Journal of Sensor Networks, 29(2):101\u2013110, 2019.   \n[78] Zhang, Y., L. O. Wijeratne, S. Talebi, et al. Machine learning for light sensor calibration. Sensors, 21(18):6259, 2021.   \n[79] Lary, D. J., L. O. H. Wijeratne, G. K. Zewdie, et al. Machine learning, big data, and spatial tools: A combination to reveal complex facts that impact environmental health. In Geospatial Technology for Human Well-Being and Health, pages 219\u2013241. Springer, 2021.   \n[80] Tropp, J. A., et al. An introduction to matrix concentration inequalities. Foundations and Trends\u00ae in Machine Learning, 8(1-2):1\u2013230, 2015. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Comments About Algorithm Implementation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we describe the algorithm for Huber loss minimization. ", "page_idx": 14}, {"type": "text", "text": "In particular, we solve ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{c}=\\arg\\operatorname*{min}_{\\mathbf{s}}\\sum_{i=1}^{n}w_{i}\\phi_{i}(\\mathbf{s},\\mathbf{y}_{i}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "From (5), it can be shown that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{c}={\\frac{\\sum_{i=1}^{n}w_{i}\\operatorname*{min}\\left\\{1,{\\frac{T_{i}}{\\left\\|\\mathbf{c}-\\mathbf{y}_{i}\\right\\|}}\\right\\}\\mathbf{y}_{i}}{\\sum_{i=1}^{n}w_{i}\\operatorname*{min}\\left\\{1,{\\frac{T_{i}}{\\left\\|\\mathbf{c}-\\mathbf{y}_{i}\\right\\|}}\\right\\}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The algorithm can be designed from above equation. Suppose that the algorithm starts from ${\\bf c}_{0}$ . The update rule is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{c}_{k+1}=\\frac{\\sum_{i=1}^{n}w_{i}\\operatorname*{min}\\left\\{1,\\frac{T_{i}}{\\|\\mathbf{c}_{k}-\\mathbf{y}_{i}\\|}\\right\\}\\mathbf{y}_{i}}{\\sum_{i=1}^{n}w_{i}\\operatorname*{min}\\left\\{1,\\frac{T_{i}}{\\|\\mathbf{c}_{k}-\\mathbf{y}_{i}\\|}\\right\\}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "(23) is run iteratively until the norm of update $\\|\\mathbf{c}_{k+1}-\\mathbf{c}_{k}\\|$ between two iterations is less than $\\xi$ . ", "page_idx": 14}, {"type": "text", "text": "We provide a brief analysis on the computational complexity as follows. ", "page_idx": 14}, {"type": "text", "text": "Worst case. [74] has shown that the Weiszfeld\u2019s algorithm for calculating geometric median needs $O(1/\\xi)$ steps to achieve precision $\\xi$ . The proof can also be used to our algorithm (23). Moreover, from (23), each step requires $O(n d)$ time, in which $d$ is the dimensionality of $\\mathbf{y}_{i}$ , thus the overall time complexity is $\\bar{O}(n\\bar{d}/\\xi)$ . ", "page_idx": 14}, {"type": "text", "text": "Common case. From the analysis in Section 5 and 6, for bounded support, with high probability, $Z(D)\\leq T$ holds for balanced users, and $Z_{i}(\\mathcal{D})\\leq T_{i}$ holds for imbalanced users. In this case, we can just calculate the result within one step: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{c}=w_{i}\\mathbf{y}_{i}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore the time complexity is $O(n d)$ . ", "page_idx": 14}, {"type": "text", "text": "B Proof of Lemma 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Now the users are balanced. (4) and (5) becomes ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{0}(D)=\\underset{\\mathbf{s}}{\\arg\\operatorname*{min}}\\sum_{i=1}^{n}\\phi(\\mathbf{s},\\mathbf{y}_{i}(D)),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\phi(\\mathbf{s},\\mathbf{y})=\\left\\{\\begin{array}{c l}{\\frac{1}{2}\\left\\|\\mathbf{s}-\\mathbf{y}\\right\\|^{2}}&{\\mathrm{if}\\quad\\left\\|\\mathbf{s}-\\mathbf{y}\\right\\|\\leq T}\\\\ {T\\left\\|\\mathbf{s}-\\mathbf{y}\\right\\|-\\frac{1}{2}T^{2}}&{\\mathrm{if}\\quad\\left\\|\\mathbf{s}-\\mathbf{y}\\right\\|>T.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In this section, we prove a strengthened version of Lemma 1 for future usage. Define the modulus of continuity as ", "page_idx": 14}, {"type": "text", "text": "Definition 7. (Modulus of continuity) Define ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\omega(\\mathcal{D},k)=\\operatorname*{sup}_{\\mathcal{D}^{\\prime}:d_{H}(\\mathcal{D},\\mathcal{D}^{\\prime})\\leq k}\\|\\hat{\\mu}_{0}(\\mathcal{D})-\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime})\\|\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "From the definition, $\\omega(D,0)$ is just the local sensitivity $L S(D)$ . Then we show the following lemma. Lemma 5. If $Z(\\mathcal{D})<(1-2k/n)T$ , then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\omega(\\mathcal{D},k)\\leq\\frac{k(T+Z(\\mathcal{D}))}{n-k}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $k=0$ , then Lemma 5 reduces to Lemma 1. The remainder of this section shows the proof of Lemma 5. ", "page_idx": 15}, {"type": "text", "text": "According to (27), $\\omega(\\mathcal{D},k)$ is the supremum change after replacing all items belonging to $k$ users.   \nDenote $\\mathcal{D}^{\\prime}$ as a dataset such that $d\\bar{\\boldsymbol{H}}(\\mathcal{D},\\mathcal{D}^{\\prime})\\,\\leq\\,k$ . Let $I$ be the set of users such that $D_{i}\\,\\ne\\,D_{i}^{\\prime}$ .   \nThroughout this section, we denote $\\mathbf{y}_{i}=\\mathbf{y}_{i}(\\mathcal{D})$ and $\\mathbf{y}_{i}^{\\prime}=\\mathbf{y}_{i}(D^{\\prime})$ for simplicity. ", "page_idx": 15}, {"type": "text", "text": "From (25), ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime})=\\arg\\operatorname*{min}_{\\mathbf{s}}\\left[\\sum_{i\\in I}\\phi(\\mathbf{s},\\mathbf{y}_{i}^{\\prime})+\\sum_{i\\in[n]\\backslash I}\\phi(\\mathbf{s},\\mathbf{y}_{i})\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $\\nabla\\phi$ be the gradient of $\\phi$ with respect to the first argument. Define ", "page_idx": 15}, {"type": "equation", "text": "$$\ng(\\mathbf{s})=\\sum_{i\\in I}\\nabla\\phi(\\mathbf{s},\\mathbf{y}_{i}^{\\prime})+\\sum_{i\\in[n]\\backslash I}\\nabla\\phi(\\mathbf{s},\\mathbf{y}_{i}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{g(\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime}))=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{g(\\hat{\\mu}_{0}(D))}&{=}&{\\displaystyle\\sum_{i\\in I}\\nabla\\phi(\\hat{\\mu}_{0}(D),\\mathbf{y}_{i}^{\\prime})+\\displaystyle\\sum_{i=[n]\\backslash I}\\nabla\\phi(\\hat{\\mu}_{0}(D),\\mathbf{y}_{i})}\\\\ &{=}&{\\displaystyle\\sum_{i\\in I}\\nabla\\phi(\\hat{\\mu}_{0}(D),\\mathbf{y}_{i}^{\\prime})-\\displaystyle\\sum_{i\\in I}\\nabla\\phi(\\hat{\\mu}_{0}(D),\\mathbf{y}_{i})+\\sum_{i\\in[n]}\\nabla\\phi(\\hat{\\mu}_{0}(D),\\mathbf{y}_{i})}\\\\ &{=}&{\\displaystyle\\sum_{i\\in I}\\nabla\\phi(\\hat{\\mu}_{0}(D),\\mathbf{y}_{i}^{\\prime})-\\displaystyle\\sum_{i\\in I}\\nabla\\phi(\\hat{\\mu}_{0}(D),\\mathbf{y}_{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "in which the last step comes from (25). ", "page_idx": 15}, {"type": "text", "text": "Then we show the following lemma. ", "page_idx": 15}, {"type": "text", "text": "Lemma 6. If $Z(D)\\leq T$ , then $\\hat{\\mu}_{0}(\\mathcal{D})=\\bar{\\mathbf{y}}$ , in which $\\begin{array}{r}{\\bar{\\mathbf{y}}=\\left(1/n\\right)\\sum_{i=1}^{n}\\mathbf{y}_{i}}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. The proof has two steps. ", "page_idx": 15}, {"type": "text", "text": "(1) $\\bar{\\mathbf{y}}$ is a minimizer of $\\textstyle\\sum_{i=1}^{n}\\phi(\\mathbf{s},\\mathbf{y}_{i})$ . From (26), ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\nabla\\phi(\\mathbf{s},\\mathbf{y}_{i})}&{=}&{\\left\\{\\begin{array}{c c}{\\mathbf{s}-\\mathbf{y}_{i}}&{\\mathrm{if}\\quad\\left\\|\\mathbf{s}-\\mathbf{y}_{i}\\right\\|\\leq T}\\\\ {T\\frac{\\mathbf{s}-\\mathbf{y}_{i}}{\\left\\|\\mathbf{s}-\\mathbf{y}_{i}\\right\\|}}&{\\mathrm{if}\\quad\\left\\|\\mathbf{s}-\\mathbf{y}_{i}\\right\\|>T,}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "in which the second step holds because $\\|\\bar{\\mathbf y}-\\mathbf y_{i}\\|\\leq Z\\leq T$ . Moreover, $\\textstyle\\sum_{i=1}^{n}\\phi(\\mathbf{s},\\mathbf{y}_{i})$ is convex, thus $\\bar{\\bf y}$ is a minimizer. ", "page_idx": 15}, {"type": "text", "text": "(2) $\\bar{\\mathbf{y}}$ is the unique minimizer. The Hessian ", "text_level": 1, "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla^{2}\\sum_{i=1}^{n}\\phi(\\bar{\\mathbf{y}},\\mathbf{y}_{i})\\geq\\sum_{i=1}^{n}\\mathbf{1}\\left(\\|\\bar{\\mathbf{y}}-\\mathbf{y}_{i}\\|\\leq T\\right)=n.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore $\\textstyle\\sum_{i=1}^{n}\\phi(\\mathbf{s},\\mathbf{y}_{i})$ is strong convex around $\\bar{\\mathbf{y}}$ , and thus $\\bar{\\mathbf{y}}$ is unique. ", "page_idx": 15}, {"type": "text", "text": "From (30), ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\|g(\\hat{\\mu}_{0}(\\mathcal D))\\|}&{\\leq}&{\\displaystyle\\sum_{i=1}^{k}\\|\\nabla\\phi(\\bar{\\mathbf y},\\mathbf y_{i}^{\\prime})\\|+\\displaystyle\\sum_{i=1}^{k}\\|\\nabla\\phi(\\bar{\\mathbf y},\\mathbf y_{i})\\|}\\\\ &{\\leq}&{\\displaystyle k T+k\\displaystyle\\sum_{i=1}^{k}\\|\\bar{\\mathbf y}-\\mathbf y_{i}\\|}\\\\ &{\\leq}&{\\displaystyle k(T+Z(\\mathcal D)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover, for s satisfying $\\|\\mathbf{s}-\\bar{\\mathbf{y}}\\|\\leq T-Z$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\nabla g(\\mathbf{s})}&{=}&{\\displaystyle\\sum_{i=1}^{k}\\nabla^{2}\\phi(\\mathbf{s},\\mathbf{y}_{i}^{\\prime})+\\displaystyle\\sum_{i=k+1}^{n}\\nabla^{2}\\phi(\\mathbf{s},\\mathbf{y}_{i})}\\\\ &{\\displaystyle\\succeq~~\\displaystyle\\sum_{i=k+1}^{n}\\mathbf{1}(\\|\\mathbf{s}-\\mathbf{y}_{i}\\|\\leq T)}\\\\ &{\\displaystyle\\succeq~~(n-k)\\mathbf{I},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "in which the last step holds because ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{s}-\\mathbf{y}_{i}\\|\\leq\\|\\mathbf{s}-\\bar{\\mathbf{y}}\\|+\\|\\bar{\\mathbf{y}}-\\mathbf{y}_{i}\\|\\leq T-Z(\\mathcal{D})+Z(\\mathcal{D})=T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let $\\mathbf{L}$ be a path connecting $\\hat{\\mu}_{0}(\\mathcal{D})$ and $\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime})$ . Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\|g(\\hat{\\mu}_{0}(\\mathcal D))-g(\\hat{\\mu}_{0}(\\mathcal D^{\\prime}))\\|}&{=}&{\\displaystyle\\left\\|\\int_{\\mathbf{L}}\\nabla g(\\mathbf{s})\\cdot d\\mathbf{s}\\right\\|}\\\\ &{\\geq}&{(n-k)\\operatorname*{min}\\left\\{\\|\\hat{\\mu}_{0}(\\mathcal D)-\\hat{\\mu}_{0}(\\mathcal D^{\\prime})\\|\\,,T-Z(\\mathcal D)\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that from (31) and (35), ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|g(\\hat{\\mu}_{0}(\\mathcal{D}))-g(\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime}))\\|\\leq k(T+Z(\\mathcal{D})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From the condition $Z(\\mathcal{D})<(1-2k/n)T$ in Lemma 1, $(n-k)(T-Z(\\mathcal{D}))>k(T+Z(\\mathcal{D}))$ . Hence, (38) and (39) yields ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime})-\\hat{\\mu}_{0}(\\mathcal{D})\\|\\leq\\frac{k(T+Z(\\mathcal{D}))}{n-k}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "C Proof of Lemma 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Denote $\\mathcal{D}^{\\prime}$ as a dataset adjacent to $\\mathcal{D}$ at user-level. Then it remains to bound $\\left\\|\\hat{\\mu}_{0}(\\mathcal{D})-\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime})\\right\\|$ . Recall that in the statement of Lemma 2, we have required that there exists a dataset $\\mathcal{D}^{*}$ such that $Z(\\mathcal{D}^{*})<(1-2(k+1)/n)T$ , and $d_{H}(\\mathcal{D},\\mathcal{D}^{*})\\leq k$ . Denote $I$ as the set of users such that $\\mathcal{D}$ and $\\mathcal{D}^{*}$ have different values, while $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ differ at user $u$ . Now we discuss two cases. ", "page_idx": 16}, {"type": "text", "text": "Case 1. $u\\notin I$ . In other words, $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ differ at a user that is the same between $\\mathcal{D}$ and $\\mathcal{D}^{*}$ . In this case, $\\mathcal{D}^{\\prime}$ has $k+1$ users that are different from $\\mathcal{D}^{*}$ . Without loss of generality, suppose that $I=\\{1,\\ldots,k\\}$ , while $u=k+1.\\;D^{*},$ $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ can be written as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathcal{D}^{*}}&{=}&{\\{D_{1},\\ldots,D_{n}\\},}\\\\ {\\mathcal{D}}&{=}&{\\{D_{1}^{\\prime},\\ldots,D_{k}^{\\prime},D_{k+1},\\ldots,D_{n}\\},}\\\\ {\\mathcal{D}^{\\prime}}&{=}&{\\{D_{1}^{\\prime},\\ldots,D_{k+1}^{\\prime},D_{k+2},\\ldots,D_{n}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For convenience of expression, denote $\\mathbf{y}_{i}=\\mathbf{y}_{i}(\\mathcal{D})$ and $\\mathbf{y}_{i}^{\\prime}=\\mathbf{y}(\\mathcal{D}_{i}^{\\prime})$ . ", "page_idx": 16}, {"type": "text", "text": "In this section, define $g_{1}(\\mathbf{s})$ as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\ng_{1}(\\mathbf{s})=\\sum_{i=1}^{k+1}\\nabla\\phi(\\mathbf{s},\\mathbf{y}_{i}^{\\prime})+\\sum_{i=k+2}^{n}\\nabla\\phi(\\mathbf{s},\\mathbf{y}_{i}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "then ", "page_idx": 16}, {"type": "equation", "text": "$$\ng_{1}\\big(\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime})\\big)=0,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle g_{1}\\big(\\hat{\\mu}_{0}(\\mathcal{D})\\big)}&{=}&{\\displaystyle\\sum_{i=1}^{k+1}\\nabla\\phi\\big(\\hat{\\mu}_{0}(\\mathcal{D}),\\mathbf{y}_{i}^{\\prime}\\big)+\\sum_{i=k+2}^{n}\\nabla\\phi\\big(\\hat{\\mu}_{0}(\\mathcal{D}),\\mathbf{y}_{i}\\big)}\\\\ &{=}&{\\displaystyle\\sum_{i=1}^{k}\\nabla\\phi\\big(\\hat{\\mu}_{0}(\\mathcal{D}),\\mathbf{y}_{i}^{\\prime}\\big)+\\sum_{i=k+1}^{n}\\nabla\\phi\\big(\\hat{\\mu}_{0}(\\mathcal{D}),\\mathbf{y}_{i}\\big)}\\\\ &&{\\displaystyle+\\nabla\\phi\\big(\\hat{\\mu}_{0}(\\mathcal{D}),\\mathbf{y}_{k+1}^{\\prime}\\big)-\\nabla\\phi\\big(\\hat{\\mu}_{0}(\\mathcal{D}),\\mathbf{y}_{k+1}\\big)}\\\\ &{=}&{\\nabla\\phi\\big(\\hat{\\mu}_{0}(\\mathcal{D}),\\mathbf{y}_{k+1}^{\\prime}\\big)-\\nabla\\phi\\big(\\hat{\\mu}_{0}(\\mathcal{D}),\\mathbf{y}_{k+1}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|g_{1}(\\hat{\\mu}_{0}(\\mathcal{D}))\\|}&{\\leq\\;\\;\\left\\|\\nabla\\phi(\\hat{\\mu}_{0}(\\mathcal{D}),\\mathbf{y}_{k+1}^{\\prime})\\right\\|+\\|\\nabla\\phi(\\hat{\\mu}_{0}(\\mathcal{D}),\\mathbf{y}_{k+1})\\|}\\\\ &{\\overset{(a)}{\\leq}\\;\\;T+\\|\\hat{\\mu}_{0}(\\mathcal{D})-\\mathbf{y}_{k+1}\\|}\\\\ &{\\overset{(b)}{\\leq}\\;\\;T+\\|\\hat{\\mu}_{0}(\\mathcal{D})-\\hat{\\mu}_{0}(\\mathcal{D}^{*})\\|+\\|\\bar{\\mathbf{y}}(\\mathcal{D}^{*})-\\mathbf{y}_{k+1}\\|}\\\\ &{\\leq\\;\\;T+\\omega(\\mathcal{D}^{*},k)+Z(\\mathcal{D}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "(a) comes from (33). By taking gradient over s, we have $\\|\\nabla\\phi(\\mathbf{s},y)\\|\\leq\\operatorname*{min}\\{T,\\|\\mathbf{s}-\\mathbf{y}\\|\\}$ . (b) comes from Lemma 6, which states that $\\hat{\\mu}_{0}(\\mathcal{D})=\\bar{\\mathbf{y}}$ . ", "page_idx": 17}, {"type": "text", "text": "For all s satisfying $\\begin{array}{r}{\\|\\mathbf{s}-\\hat{\\mu}_{0}(\\mathcal{D})\\|\\leq T-Z(\\mathcal{D}^{*})-\\omega(\\mathcal{D}^{*},k),}\\end{array}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\nabla g_{1}(\\mathbf{s})}&{=}&{\\displaystyle\\sum_{i=1}^{k+1}\\nabla^{2}\\phi(\\mathbf{s},\\mathbf{y}_{i}^{\\prime})+\\displaystyle\\sum_{i=k+2}^{n}\\nabla^{2}\\phi(\\mathbf{s},\\mathbf{y}_{i})}\\\\ &{\\succeq}&{\\displaystyle\\sum_{i=k+2}^{n}\\mathbf{1}(\\|\\mathbf{s}-\\mathbf{y}_{i}\\|\\leq T)}\\\\ &{\\succeq}&{(n-k-1)\\mathbf{I},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "in which the last step holds because ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lVert\\mathbf{s}-\\mathbf{y}_{i}\\rVert}&{\\leq\\phantom{\\mathcal{Z}}\\lVert\\mathbf{s}-\\hat{\\mu}_{0}(\\mathcal{D})\\rVert+\\lVert\\hat{\\mu}_{0}(\\mathcal{D})-\\bar{\\mathbf{y}}\\rVert+\\lVert\\bar{\\mathbf{y}}-\\mathbf{y}_{i}\\rVert}\\\\ &{\\leq\\phantom{\\mathcal{Z}}T-Z(\\mathcal{D}^{*})-\\omega(\\mathcal{D}^{*},k)+\\omega(\\mathcal{D}^{*},k)+Z(\\mathcal{D}^{*})}\\\\ &{=\\phantom{\\mathcal{Z}}T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l l}{g_{1}(\\hat{\\mu}_{0}(\\mathcal{D}))-g_{1}(\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime}))\\|}&{=}&{\\displaystyle\\left\\|\\int_{\\mathbf{L}}\\nabla g_{1}(\\mathbf{s})d\\mathbf{s}\\right\\|}\\\\ &{\\geq}&{(n-k-1)\\operatorname*{min}\\left\\{\\|\\hat{\\mu}_{0}(\\mathcal{D})-\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime})\\|\\,,T-Z(\\mathcal{D}^{*})-\\omega(\\mathcal{D}^{*},k)\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "in which $\\mathbf{L}$ is the line connecting $\\hat{\\mu}_{0}(\\mathcal{D})$ and $\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime})$ . ", "page_idx": 17}, {"type": "text", "text": "From (45) and (47), ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lVert g_{1}\\big(\\hat{\\mu}_{0}(\\mathcal{D})\\big)-g_{1}\\big(\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime})\\big)\\rVert\\leq T+\\omega\\big(\\mathcal{D}^{*},k\\big)+Z\\big(\\mathcal{D}^{*}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "From the condition $Z(D^{*})<(1-2(k+1)/n)T$ in Lemma 2, ", "page_idx": 17}, {"type": "equation", "text": "$$\n(n-k-1)(T-Z(\\mathcal{D}^{*})-\\omega(\\mathcal{D}^{*},k))>T+\\omega(\\mathcal{D}^{*},k)+Z(\\mathcal{D}^{*}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|g_{1}(\\widehat{\\mu}_{0}(\\mathcal{D}))-g_{1}(\\widehat{\\mu}_{0}(\\mathcal{D}^{\\prime}))\\|<(n-k-1)(T-Z(\\mathcal{D}^{*})-\\omega(\\mathcal{D}^{*},k)).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "From (50) and (53), the second element in the minimum bracket in (50) will not take effect. Hence, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{\\mu}_{0}(\\mathcal{D})-\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime})\\|<T-Z(\\mathcal{D}^{*})-\\omega(\\mathcal{D}^{*},k).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "(54) will be useful in the analysis of the second case. From (50) and (53), we can also get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l l}{\\|\\hat{\\mu}_{0}(\\mathcal{D})-\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime})\\|}&{\\le}&{\\displaystyle\\frac{T+\\omega(\\mathcal{D}^{*},k)+Z(\\mathcal{D}^{*})}{n-k-1}}\\\\ &{\\le}&{\\displaystyle\\frac{n}{(n-k-1)(n-k)}(T+Z(\\mathcal{D}^{*})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "in which the last step uses the bound of $\\omega(\\mathcal{D}^{\\ast},k)$ in Lemma 1. Now we have bounded $\\left\\|\\hat{\\mu}_{0}(\\mathcal{D})-\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime})\\right\\|$ for the first case. ", "page_idx": 17}, {"type": "text", "text": "Case 2: $u\\in I$ . In other words, $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ differ in a user that is different between $\\mathcal{D}$ and $\\mathcal{D}^{*}$ . Without loss of generality, suppose that $I=\\{1,\\ldots,k\\}$ , and $u=k.\\;\\mathcal{D}^{*}$ , $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ can be written as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathcal{D}^{*}}&{=}&{\\{D_{1},\\ldots,D_{n}\\},}\\\\ {\\mathcal{D}}&{=}&{\\{D_{1}^{\\prime},\\ldots,D_{k}^{\\prime},D_{k+1},\\ldots,D_{n}\\},}\\\\ {\\mathcal{D}^{\\prime}}&{=}&{\\{D_{1}^{\\prime},\\ldots,D_{k-1}^{\\prime},D_{k}^{\\prime\\prime},D_{k+1},\\ldots,D_{n}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In order to bound $\\left\\|\\hat{\\mu}_{0}(D)-\\hat{\\mu}_{0}(D^{\\prime})\\right\\|$ , we construct a temporary dataset $\\mathcal{D}_{t e m p}$ as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{D}_{t e m p}=\\{D_{1}^{\\prime},...\\,,D_{k-1}^{\\prime},D_{k},...\\,,D_{n}\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Define ", "page_idx": 18}, {"type": "equation", "text": "$$\ng_{2}(\\mathbf{s})=\\sum_{i=1}^{k-1}\\nabla\\phi(\\mathbf{s},\\mathbf{y}_{i}^{\\prime})+\\nabla\\phi(\\mathbf{s},\\mathbf{y}_{k}^{\\prime\\prime})+\\sum_{i=k+1}^{n}\\nabla\\phi(\\mathbf{s},\\mathbf{y}_{i}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then $g_{2}\\big(\\hat{\\mu}_{0}\\big(\\mathcal{D}^{\\prime}\\big)\\big)=0$ , and ", "page_idx": 18}, {"type": "equation", "text": "$$\ng_{2}(\\hat{\\mu}_{0}(\\mathcal D))=\\nabla\\phi(\\hat{\\mu}_{0}(\\mathcal D),\\mathbf y_{k}^{\\prime\\prime})-\\nabla\\phi(\\hat{\\mu}_{0}(\\mathcal D),\\mathbf y_{k}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "From (33), ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|g_{2}(\\hat{\\mu}_{0}(\\mathcal{D}))-g_{2}(\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime}))\\|\\leq2T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now we bound $\\left\\|\\hat{\\mu}_{0}(\\mathcal{D})-\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime})\\right\\|$ . Corresponding to (54), we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{\\mu}_{0}(\\mathcal{D}_{t e m p})-\\hat{\\mu}_{0}(\\mathcal{D})\\|<T-Z(\\mathcal{D}^{*})-\\omega(\\mathcal{D}^{*},k-1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{\\mu}_{0}(\\mathcal{D}_{t e m p})-\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime})\\|<T-Z(\\mathcal{D}^{*})-\\omega(\\mathcal{D}^{*},k-1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Denote $\\mathbf{L}$ as the line connecting $\\hat{\\mu}_{0}(\\mathcal{D})$ and $\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime})$ . For all s \u2208 $\\begin{array}{r}{{\\mathbf{L}},\\|s-\\hat{\\mu}_{0}(\\mathcal{D}_{t e m p})\\|\\leq T\\!-\\!Z(\\mathcal{D}^{*})\\!-\\!}\\end{array}$ $\\omega(D^{*},k-1)$ . Corresponding to (48), for all $\\mathbf{s}\\in\\mathbf{L}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla g_{2}(\\mathbf{s})\\succeq(n-k)\\mathbf{I}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|g_{2}(\\hat{\\mu}_{0}(D))-g_{2}(\\hat{\\mu}_{0}(D^{\\prime}))\\|\\geq(n-k)\\,\\|\\hat{\\mu}_{0}(D)-\\hat{\\mu}_{0}(D^{\\prime})\\|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "From (62) and (66), ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\hat{\\mu}_{0}(\\mathcal{D})-\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime})\\|\\leq\\frac{2T}{n-k}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combine case 1 and 2, we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{L S(\\mathcal D)}&{\\leq}&{\\operatorname*{max}\\left\\{\\frac{n(T+Z(\\mathcal D^{*}))}{(n-k-1)(n-k)},\\frac{2T}{n-k}\\right\\}}\\\\ &{=}&{\\frac{2T}{n-k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The last step comes from the requirement that $Z(D^{*})<(1-2(k+1)/n)T$ . The proof is complete. ", "page_idx": 18}, {"type": "text", "text": "D Proof of Theorem 1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Two requirements on smooth sensitivity are shown in Definition 4, i.e. ", "page_idx": 18}, {"type": "text", "text": "(2) For any neighboring $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ , $S(\\mathcal{D})\\le e^{\\beta}S(\\mathcal{D}^{\\prime})$ . ", "page_idx": 18}, {"type": "text", "text": "It has been shown in [40] that based on (1) and (2), the final result $\\hat{\\mu}$ is $(\\epsilon,\\delta)$ -DP. In the remainder of this section, we show that both requirements (1) and (2) are satisfied. ", "page_idx": 18}, {"type": "text", "text": "For (1), from Lemma 1 and 2, $L S(\\mathcal{D})\\leq G(\\mathcal{D},0)$ if the corresponding conditions are satisfied. If these conditions are not satisfied, since $\\|\\mathrm{Clip}(\\hat{\\mu}_{0}\\overset{\\cdot}{(}D),R_{c})\\|\\leq\\overset{\\cdot}{R}_{c}$ always hold, the sensitivity can always be bounded by $2R_{c}$ . Hence $L S(\\mathcal{D})\\leq G(\\mathcal{D},0)$ holds for all $\\mathcal{D}$ . From (11), $G(\\mathcal{D},0)\\le\\bar{S}(\\mathcal{D})$ . Therefore the requirement (1) is satisfied. ", "page_idx": 18}, {"type": "text", "text": "For (2), it suffices to show that $G(\\mathcal{D},k)\\leq G(\\mathcal{D}^{\\prime},k+1)$ . From Lemma 2, if $\\Delta(\\mathcal{D}^{\\prime})$ exists, and $k+1\\leq n/4-1-\\Delta(\\mathcal{D}^{\\prime})$ , then ", "page_idx": 18}, {"type": "equation", "text": "$$\nG(\\mathcal{D}^{\\prime},k+1)=\\frac{2T}{n-k-1-\\Delta(\\mathcal{D}^{\\prime})}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "From the definition of $\\Delta$ in (9), since $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ are adjacent, $\\Delta(\\mathcal{D})\\le\\Delta(\\mathcal{D}^{\\prime})+1$ holds. Therefore, the condition $k+1\\leq n/4-1-\\Delta(\\mathcal{D}^{\\prime})$ yields $k\\le n/4-1-\\Delta(D)$ . Use Lemma 2 again, we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{G(\\mathcal D,k)}&{\\leq}&{\\displaystyle\\frac{2T}{n-k-\\Delta(\\mathcal D)}}\\\\ &&{\\leq}&{\\displaystyle\\frac{2T}{n-k-1-\\Delta(\\mathcal D^{\\prime})}}\\\\ &&{=}&{G(\\mathcal D^{\\prime},k+1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "If the conditions in Lemma 2 are not satisfied, i.e. $\\Delta(\\mathcal{D}^{\\prime})$ does not exists or $k\\!+\\!1>n/4\\!-\\!1\\!-\\!\\Delta(\\mathcal{D}^{\\prime})$ , then $G(\\mathcal{D}^{\\prime},k+1)=2R_{c}$ , thus $G(\\mathcal{D},k)\\leq G(\\mathcal{D}^{\\prime},k+1)$ holds. ", "page_idx": 19}, {"type": "text", "text": "Now we have shown that no matter whether the conditions in Lemma 2 holds, we always have $G(\\mathcal{D},k)\\leq G(\\mathcal{D}^{\\prime},k+1)$ . From (11), it can be easily shown that $S(\\mathcal{D})\\le e^{\\beta}S(\\mathcal{D}^{\\prime})$ . ", "page_idx": 19}, {"type": "text", "text": "E Proof of Theorem 2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we analyze the practical performance of the estimator for a random dataset. ", "page_idx": 19}, {"type": "text", "text": "Notation. Throughout this section, for convenience, we just denote $Z=Z(\\mathcal{D}),\\mathbf{Y}_{i}=\\mathbf{y}_{i}(\\mathcal{D})$ and $\\bar{\\mathbf Y}=\\bar{\\mathbf y}(\\mathcal D)$ for simplicity. We use capital letters here since $\\mathcal{D}$ is random, thus $\\mathbf{Y}_{i}$ and $\\bar{\\mathbf Y}$ are random variables. ", "page_idx": 19}, {"type": "text", "text": "From Lemma 11 in Section K, for $i=1,\\hdots,n$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{P}(\\|\\mathbf{Y}_{i}-\\mu\\|>t)\\leq(d+1)e^{-\\frac{3m t^{2}}{32R^{2}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Recall that $Z=\\operatorname*{max}_{i}\\left\\|\\mathbf{Y}_{i}-\\bar{\\mathbf{Y}}\\right\\|$ . If $\\|\\mathbf{Y}_{i}-\\mu\\|\\leq t/2$ for all $i$ , then $\\|\\bar{\\mathbf{Y}}-{\\boldsymbol{\\mu}}\\|\\leq t/2$ also holds, thus $Z\\leq t$ . Therefore ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\mathsf{P}({\\cal Z}>t)}&{\\le}&{\\displaystyle\\sum_{i=1}^{n}\\mathsf{P}\\left(\\left\\|\\mathbf{Y}_{i}-\\mu\\right\\|>\\frac{t}{2}\\right)}\\\\ &{\\le}&{n(d+1)e^{-\\frac{3m t^{2}}{128R^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Define ", "page_idx": 19}, {"type": "equation", "text": "$$\nZ_{0}=\\sqrt{\\frac{128R^{2}}{3m}\\ln(m n^{3}(d+1))}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname{P}\\left(Z>Z_{0}\\right)\\leq{\\frac{1}{m n^{2}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Recall that ", "page_idx": 19}, {"type": "equation", "text": "$$\nT=C_{T}\\frac{R}{\\sqrt{m}}\\ln(m n^{3}(d+1)),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with $C_{T}>16\\sqrt{2/3}$ . Then with probability at least $1-1/(m n^{2}),Z\\leq T/2$ holds, thus ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{S(\\mathcal D)}&{=}&{\\displaystyle\\operatorname*{max}_{k}e^{-\\beta k}G(\\mathcal D,k)}\\\\ &&{\\le}&{\\displaystyle\\operatorname*{max}\\left\\{\\operatorname*{max}_{k\\le n/4-1}e^{-\\beta k}\\frac{2T}{n-k},2R_{c}e^{-\\frac{1}{4}n\\beta}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that in the statement of Theorem 2, it is required that $n\\,>\\,(4/\\beta)\\ln(n R_{c}/T)$ . This yields $2R_{c}e^{-n\\beta/4}<2T/n$ , and $e^{-\\beta k}2T/(n-k)$ decreases with $k$ . Therefore ", "page_idx": 19}, {"type": "equation", "text": "$$\nS({\\mathcal{D}})\\leq{\\frac{2T}{n}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "It remains to bound the estimation error. The mean squared error of mean estimation can be decomposed in the following way: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\left\\|\\hat{\\mu}(\\mathcal{D})-\\mu\\right\\|^{2}\\right]}&{=\\phantom{\\mathbb{E}}\\Big[\\|\\mathrm{Clip}(\\hat{\\mu}_{0}(\\mathcal{D}),R)+\\mathbf{W}-\\mu\\|^{2}\\Big]}\\\\ &{\\stackrel{(a)}{=}\\phantom{\\mathbb{E}}\\Big[\\|\\mathrm{Clip}(\\hat{\\mu}_{0}(\\mathcal{D}),R)-\\mu\\|^{2}\\Big]+\\mathbb{E}[\\|\\mathbf{W}\\|^{2}]}\\\\ &{\\leq\\phantom{\\mathbb{E}}\\Big[\\|\\hat{\\mu}_{0}(\\mathcal{D})-\\mu\\|^{2}\\,\\mathbf{1}(Z\\leq Z_{0})\\Big]+\\mathbb{E}\\left[\\|\\mathbf{W}\\|^{2}\\,\\mathbf{1}(Z\\leq Z_{0})\\right]}\\\\ &{+\\mathbb{E}\\left[\\|\\mathrm{Clip}(\\hat{\\mu}_{0}(\\mathcal{D}),R)-\\mu\\|^{2}\\,\\mathbf{1}(Z>Z_{0})\\right]}\\\\ &{+\\mathbb{E}\\left[\\|\\mathbf{W}\\|^{2}\\,\\mathbf{1}(Z>Z_{0})\\right]}\\\\ {:=\\phantom{\\mathbb{E}}_{1}+I_{2}+I_{3}+I_{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "(a) holds because $\\mathbb{E}[\\mathbf{W}|D]=0$ for any $D$ , thus $\\hat{\\mu}_{0}(\\mathcal{D})-\\mu$ and $\\mathbf{W}$ are uncorrelated. Now we bound these four terms separately. ", "page_idx": 20}, {"type": "text", "text": "Bound of $I_{1}$ . From Lemma 6, for $Z\\,\\leq\\,R\\ln(n d)/{\\sqrt{m}}$ , $Z\\,<\\,T$ holds, thus $\\hat{\\mu}_{0}(\\mathcal{D})\\,=\\,\\bar{\\mathbf{Y}}$ . For convenience, denote $\\mathbf{Y}$ as an i.i.d copy of $\\mathbf{Y}_{i}$ $\\mathbf{\\Delta}_{i},i=1,\\ldots,n$ and $\\mathbf{X}$ as an i.i.d copy of $\\mathbf{X}_{i j}$ , $,i=1,\\ldots,n$ , $j=1,\\dots,m$ . Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{I_{1}}&{=\\mathrm{~\\mathbb{E}~}\\left[\\left\\|\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbf{Y}_{i}-\\boldsymbol{\\mu}\\right\\|^{2}\\right]^{-1}\\left(Z\\leq\\frac{R}{\\sqrt{m}}\\ln(n d)\\right)\\right]}\\\\ &{\\leq\\mathrm{~\\mathbb{E}~}\\left[\\left\\|\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbf{Y}_{i}-\\boldsymbol{\\mu}\\right\\|^{2}\\right]}\\\\ &{=\\mathrm{~\\mathbb{\\E}~}\\left[\\left\\|\\mathbf{Y}-\\boldsymbol{\\mu}\\right\\|^{2}\\right]}\\\\ &{=\\mathrm{~\\mathbb{\\E}~}\\frac{1}{m n}\\mathbb{E}\\left[\\left\\|\\mathbf{X}-\\boldsymbol{\\mu}\\right\\|^{2}\\right]}\\\\ &{\\leq\\mathrm{~\\mathbb{\\E}~}\\frac{R^{2}}{m n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Bound of $I_{2}$ . ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{I_{2}}}&{{\\stackrel{(a)}{\\leq}}}&{{\\displaystyle\\frac{d}{\\alpha^{2}}\\mathbb{E}[S^{2}(\\mathscr{D})\\mathbf{1}(Z\\leq Z_{0})]}}\\\\ {{}}&{{\\stackrel{(b)}{\\lesssim}}}&{{\\displaystyle\\frac{T^{2}d}{n^{2}\\alpha^{2}}}}\\\\ {{}}&{{\\stackrel{(c)}{\\sim}}}&{{\\displaystyle\\frac{d R^{2}}{m n^{2}\\epsilon^{2}}\\ln(m n^{3}d)\\ln\\frac{1}{\\delta}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "(a) holds because $\\mathbf{W}\\sim{\\mathcal{N}}(0,(\\lambda^{2}/\\alpha^{2})\\mathbf{I})$ . (b) comes from (77). (c) comes from (3). ", "page_idx": 20}, {"type": "text", "text": "Bound of $I_{3}$ . From (74), ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\begin{array}{r c l}{I_{3}}&{\\leq}&{4R^{2}\\mathbf{P}\\left(Z>Z_{0}\\right)}\\\\ &{\\leq}&{{\\frac{4R^{2}}{m n^{2}}}.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Bound of $I_{4}$ . ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{I_{4}}}&{{\\le}}&{{\\displaystyle\\frac{d}{\\alpha^{2}}\\mathbb{E}[S^{2}(\\mathscr{D}){\\bf1}(Z>Z_{0})]}}\\\\ {{}}&{{\\lesssim}}&{{\\displaystyle\\frac{d R^{2}}{\\alpha^{2}}\\mathrm{P}(Z>Z_{0})}}\\\\ {{}}&{{\\lesssim}}&{{\\displaystyle\\frac{d R^{2}}{m n^{2}\\epsilon^{2}}\\ln\\displaystyle\\frac{1}{\\delta}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now all four terms in (78) have been bounded. Therefore ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|{\\hat{\\mu}}({\\mathcal{D}})-\\mu\\right\\|^{2}\\right]\\lesssim{\\frac{R^{2}}{m n}}+{\\frac{d R^{2}}{m n^{2}\\epsilon^{2}}}\\ln(m n^{3}d)\\ln{\\frac{1}{\\delta}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The proof is complete. ", "page_idx": 21}, {"type": "text", "text": "F Proof of Theorem 3 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, following Appendix E, we still denote $\\mathbf{Y}_{i}=\\mathbf{y}_{i}(\\mathcal{D})$ and $\\bar{\\mathbf Y}=\\bar{\\mathbf y}(\\mathcal D)$ for simplicity. Define ", "page_idx": 21}, {"type": "equation", "text": "$$\nr_{0}=\\operatorname*{max}\\left\\{2M_{p}^{\\frac{1}{p}}\\sqrt{\\frac{1}{m}\\ln\\frac{3(d+1)}{\\nu}},4M_{p}^{\\frac{1}{p}}(3m)^{\\frac{1}{p}-1}\\nu^{-\\frac{1}{p}}\\ln\\frac{3(d+1)}{\\nu}\\right\\},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "in which $\\nu=\\sqrt{d}/(n\\epsilon)$ . From the statement of Theorem 3, $T>4r_{0}$ . From Lemma 13, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname{P}(\\left\\|\\mathbf{Y}_{i}-\\mu\\right\\|>r_{0})\\leq\\nu={\\frac{\\sqrt{d}}{n\\epsilon}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Define ", "page_idx": 21}, {"type": "equation", "text": "$$\nn_{o u t}=\\sum_{i=1}^{n}\\mathbf{1}(\\|\\mathbf{Y}_{i}-\\mu\\|>r_{0}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then $n_{o u t}$ follows a Binomial distribution with parameter $(n,p)$ with $p\\leq1/n.\\;\\mathbb{E}[n_{o u t}]\\leq1$ . This indicates that with the increase of $n$ , the number of outliers is still bounded by $O(1)$ with high probability. ", "page_idx": 21}, {"type": "text", "text": "Lemma 7. $\\Delta(D,k)\\leq n_{o u t}$ for $k<n/4-n_{o u t}-1.$ ", "page_idx": 21}, {"type": "text", "text": "Proof. From the dataset $\\mathcal{D}$ , we construct $\\mathcal{D}^{*}$ as follows. Let $D^{*}=\\{D_{1}^{*},\\ldots,D_{n}^{*}\\}$ such that $D_{i}^{*}=D_{i}$ if $\\|\\mathbf{Y}_{i}-\\mu\\|\\leq r_{0}$ , and $D_{i}^{*}$ is an arbitrary set with mean value $\\mu$ . In other words, denote $\\mathbf{Y}_{i}^{*}$ as the mean value in $D_{i}^{*}$ , then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{Y}_{i}^{*}=\\left\\{\\begin{array}{r l}{\\mathbf{Y}_{i}}&{\\mathrm{if}\\quad\\lVert\\mathbf{Y}_{i}-\\mu\\rVert\\leq r_{0}}\\\\ {\\mu}&{\\mathrm{if}\\quad\\lVert\\mathbf{Y}_{i}-\\mu\\rVert>r_{0}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then $d_{H}(\\mathcal{D},\\mathcal{D}^{*})=n_{o u t}$ . Let $\\begin{array}{r}{\\bar{\\mathbf{Y}}^{*}=\\left(1/n\\right)\\sum_{i=1}^{n}\\mathbf{Y}_{i}^{*}}\\end{array}$ . Note that ", "page_idx": 21}, {"type": "equation", "text": "$$\nZ(\\mathcal{D}^{*})=\\operatorname*{max}_{i}\\left\\|\\bar{\\mathbf{Y}}^{*}-\\mu\\right\\|+\\operatorname*{max}_{i}\\|\\mathbf{Y}_{i}^{*}-\\mu\\|\\leq2r_{0}<\\frac{1}{2}T.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "$G(\\mathcal{D},k)$ can be bounded using Definition 5. Since $R_{c}=R$ , for $k\\geq n/4-n_{o u t}-1$ , $G(\\mathcal \u1e0a D,k)\\leq2R$ . Therefore, for sufficiently large $n$ , if $n_{o u t}<n/8$ , then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\lambda}&{=}&{\\displaystyle\\operatorname*{max}_{k}e^{-\\beta k}G(\\mathcal{D},k)}\\\\ &{\\le}&{\\displaystyle\\operatorname*{max}_{k\\le n/4-n_{o u t}-1}\\frac{2T}{n-k-n_{o u t}},e^{-\\beta(n/4-n_{o u t}-1)}\\right\\}}\\\\ &{\\stackrel{(a)}{\\le}}&{\\displaystyle\\operatorname*{max}_{k<n/4-n_{o u t}-1}\\frac{2T}{n-k-n_{o u t}}}\\\\ &{=}&{\\displaystyle\\frac{8T}{3n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "(a) holds since Theorem 3 requires that $n>8(1+(1/\\beta)\\ln(n/2T))$ , thus the $e^{-\\beta(n/4-n_{o u t}-1)}\\leq$ $2T/n$ . ", "page_idx": 21}, {"type": "text", "text": "Then the mean squared error of $\\hat{\\mu}$ can be bounded by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{E}\\left[\\Vert\\hat{\\mu}(D)-\\mu\\Vert^{2}\\right]}&{=}&{\\mathbb{E}\\left[\\Vert\\mathrm{Clip}(\\hat{\\mu}_{0}(\\mathcal{D}),R)+\\mathbf{W}-\\mu\\Vert^{2}\\right]}\\\\ &{=}&{\\mathbb{E}\\left[\\Vert\\mathrm{Clip}(\\hat{\\mu}_{0}(\\mathcal{D}),R)-\\mu\\Vert^{2}\\right]+\\mathbb{E}[\\Vert\\mathbf{W}\\Vert^{2}]}\\\\ &{=}&{\\mathbb{E}\\left[\\Vert\\mathrm{Clip}(\\hat{\\mu}_{0}(\\mathcal{D}),R)-\\mu\\Vert^{2}\\,\\mathbf{1}(n_{o u t}<\\frac{1}{8}n)\\right]+\\mathbb{E}\\left[\\Vert\\mathbf{W}\\Vert^{2}\\,\\mathbf{1}(n_{o u t}<\\frac{1}{8}n)\\right]}\\\\ &{}&{+\\mathbb{E}\\left[\\Vert\\mathrm{Clip}(\\hat{\\mu}_{0}(\\mathcal{D}),R)-\\mu\\Vert^{2}\\,\\mathbf{1}(n_{o u t}\\ge\\frac{1}{8}n)\\right]+\\mathbb{E}\\left[\\Vert\\mathbf{W}\\Vert^{2}\\,\\mathbf{1}(n_{o u t}\\ge\\frac{1}{8}n)\\right]}\\\\ &{:=}&{I_{1}+I_{2}+I_{3}+I_{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "From Chernoff inequality, ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\bf P}(n_{o u t}>l)\\leq e^{-n\\nu}\\left(\\frac{e n\\nu}{l}\\right)^{l}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{P}\\left(n_{o u t}>\\frac{1}{8}n\\right)\\leq\\left(\\frac{8e\\sqrt{d}}{n\\epsilon}\\right)^{\\frac{n}{8}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which decays faster than any polynomial. Therefore, $I_{3}$ and $I_{4}$ can be neglected in asymptotic analysis. Now we bound $I_{1}$ and $I_{2}$ . ", "page_idx": 22}, {"type": "text", "text": "Bound of $I_{1}$ . Note that ", "text_level": 1, "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\hat{\\mu}_{0}(\\mathcal{D})-\\mu\\right\\|\\leq\\left\\|\\hat{\\mu}_{0}(\\mathcal{D})-\\hat{\\mu}_{0}(\\mathcal{D}^{*})\\right\\|+\\left\\|\\hat{\\mu}_{0}(\\mathcal{D}^{*}-\\mu)\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $Z({\\mathcal D}^{*})<T,\\hat{\\mu}_{0}({\\mathcal D}^{*})=\\bar{\\mathbf{Y}}^{*}$ , then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\left\\|\\hat{\\mu}_{0}(\\mathcal{D}^{*})-\\mu\\right\\|^{2}\\right]}&{=}&{\\mathbb{E}\\left[\\left\\|\\bar{\\mathbf{Y}}^{*}-\\mu\\right\\|^{2}\\right]}\\\\ &{\\leq}&{\\mathrm{tr}(\\mathrm{Var}[\\bar{\\mathbf{Y}}^{*}])+\\left\\|\\mathbb{E}[\\bar{\\mathbf{Y}}^{*}]-\\mu\\right\\|^{2}}\\\\ &{\\lesssim}&{\\displaystyle\\frac{1}{m n}+r_{0}\\nu,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "in which the last step uses Lemma 14. ", "page_idx": 22}, {"type": "text", "text": "From Lemma 1, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\|\\hat{{\\mu}}_{0}(\\mathcal D)-\\hat{{\\mu}}_{0}(\\mathcal D^{*})\\|}&{\\leq}&{\\omega(\\mathcal D^{*},n_{o u t})}\\\\ &{\\leq}&{\\displaystyle\\frac{n_{o u t}(T+Z(\\mathcal D^{*}))}{n-n_{o u t}}}\\\\ &{\\leq}&{\\displaystyle\\frac{\\frac{3}{2}T n_{o u t}}{n-n_{o u t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The expectation can be bounded by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\mathbb{E}\\left[\\left\\|\\hat{\\mu}_{0}(\\mathcal{D})-\\hat{\\mu}_{0}(\\mathcal{D}^{*})\\right\\|^{2}\\mathbf{1}\\left(n_{o u t}<\\frac{n}{8}\\right)\\right]}&{\\leq}&{\\displaystyle\\mathbb{E}\\left[\\left(\\frac{\\frac{3}{2}T n_{o u t}}{n-\\frac{n}{8}}\\right)^{2}\\right]}\\\\ &&{\\sim}&{\\displaystyle\\frac{T^{2}}{n^{2}}\\mathbb{E}[n_{o u t}^{2}]}\\\\ &&{\\sim}&{\\displaystyle\\frac{T^{2}}{n^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore ", "page_idx": 22}, {"type": "equation", "text": "$$\nI_{1}\\lesssim\\frac{T^{2}}{n^{2}}+\\frac{1}{m n}+\\nu^{2}r_{0}^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Bound of $I_{2}$ . ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert\\mathbf{W}\\Vert^{2}\\,\\mathbf{1}\\left(n_{o u t}<\\frac{n}{8}\\right)\\right]\\leq\\frac{d}{\\alpha^{2}}\\mathbb{E}\\left[\\lambda^{2}\\mathbf{1}(n_{o u t}<\\frac{n}{8})\\right]\\lesssim\\frac{d T^{2}}{n^{2}\\epsilon^{2}}\\ln\\frac{1}{\\delta},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "in which the last step uses (3) and (89). ", "page_idx": 23}, {"type": "text", "text": "Combine all terms, the final bound on the mean squared error is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\Vert\\hat{\\mu}(\\mathcal{D})-\\mu\\Vert^{2}\\right]}&{\\lesssim}&{\\displaystyle\\frac{1}{m n}+\\frac{d T^{2}}{n^{2}\\epsilon^{2}}\\ln\\frac{1}{\\delta}+r_{0}^{2}\\nu^{2}}\\\\ &{\\sim}&{\\displaystyle\\frac{1}{m n}+\\frac{d T^{2}}{n^{2}\\epsilon^{2}}\\ln\\frac{1}{\\delta}}\\\\ &{\\sim}&{\\displaystyle\\frac{1}{m n}+\\frac{d}{n^{2}\\epsilon^{2}}\\ln\\frac{1}{\\delta}\\left(\\frac{\\ln(n d)}{m}+m^{\\frac{2}{p}-2}n^{\\frac{2}{p}}\\epsilon^{\\frac{2}{p}}d^{-\\frac{1}{p}}\\ln^{2}(n d)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "in which the second step uses $T\\,>\\,4r_{0}$ (from (13) and (84)) and $\\nu\\,=\\,\\sqrt{d}/(n\\epsilon)$ . The proof is complete. ", "page_idx": 23}, {"type": "text", "text": "G Comment on Two-stage Approach ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section,we provide a brief comment on the two-stage approach WME in [28]. ", "page_idx": 23}, {"type": "text", "text": "1) For heavy-tailed distributions. We follow the steps of Appendix D.4 in [28]. ", "page_idx": 23}, {"type": "text", "text": "[28] defined a $(\\tau,\\gamma)$ concentration, which requires that there exists a point c, with probability $1-\\gamma$ , $\\|\\mathbf{Y}_{i}-\\mathbf{c}\\|\\leq\\tau$ for all $i$ , in which $\\mathbf{Y}_{i}$ is the $i$ -th user-wise average. We use capital letter here to denote that it is a random variable. ", "page_idx": 23}, {"type": "text", "text": "From Appendix D.4 in [28], $\\gamma\\sim1/(m n^{2}\\epsilon^{2})$ is used. From Lemma 13, let $\\nu=1/(m n^{3}\\epsilon^{2})$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\mathbf{Y}-\\mu\\|\\leq\\operatorname*{max}\\left\\{2M_{p}^{\\frac{1}{p}}\\sqrt{\\frac{1}{m}\\ln\\frac{3(d+1)}{\\nu}},4M_{p}^{\\frac{1}{p}}(3m)^{\\frac{1}{p}-1}\\nu^{-\\frac{1}{p}}\\ln\\frac{3(d+1)}{\\nu}\\right\\}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with probability at least $1-\\nu$ , in which $\\mathbf{Y}$ is i.i.d with $\\mathbf{Y}_{i}$ , $i=1,\\hdots,n$ . The value of $\\tau$ can be obtained by taking union bound for all $i$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\tau\\sim\\sqrt{\\frac{1}{m}\\ln(m n^{3}d)}+m^{\\frac{1}{p}-1}(m n^{3})^{\\frac{1}{p}}\\ln(m n^{3}d).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Follow other parts of Appendix D.4 in [28], we can get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Vert\\hat{\\mu}-\\mu\\Vert^{2}]\\lesssim\\frac{1}{m n}+\\frac{d\\ln^{2}(m n d)\\ln\\frac{1}{\\delta}}{n^{2}\\epsilon^{2}}\\left[\\frac{1}{m}+m^{\\frac{4}{p}-2}n^{\\frac{6}{p}}\\ln(n m d)\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Moreover, [28] has already shown the tightness of the mean squared error for the bounded case. For heavy-tailed distributions, following Appendix D.4 in [28], it can also be shown that the bound (102) is tight. ", "page_idx": 23}, {"type": "text", "text": "2) For imbalanced users. For simplicity, we focus on the one dimensional problem. With the two-stage approach, for users with different $m_{i}$ , the final estimate will be ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\hat{\\mu}=\\frac{1}{N}\\sum_{i=1}^{n}m_{i}\\Pi_{[a,b]}Y_{i},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "in which $\\pi_{[a,b]}$ means clipping on $[a,b]$ . The sensitivity is determined by the user with maximum $m_{i}$ Let $m_{\\operatorname*{max}}=\\operatorname*{max}_{i}m_{i}$ . Then the sensitivity if $m_{\\operatorname*{max}}(b-a)/n$ . ", "page_idx": 23}, {"type": "text", "text": "From [28], now suppose that $m_{i}$ are the same for all $i$ except one that is significantly larger. Then $\\tau\\,\\sim\\,R\\sqrt{n\\ln n/N}$ , $b-a=4\\tau$ , the sensitivity scales as $(R m_{\\operatorname*{max}}/N)\\sqrt{n\\ln n/N}$ . Denote $\\gamma_{0}\\,=$ $n m_{\\operatorname*{max}}/N$ as the ratio between maximum $m_{i}$ and average $m_{i}$ . Then the mean squared error induced by privacy mechanism is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}[W^{2}]\\gtrsim\\frac{R^{2}m_{\\mathrm{max}}^{2}}{N^{2}}\\frac{n\\ln N}{N}=\\frac{\\gamma_{0}^{2}R^{2}\\ln n}{N n\\epsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "H Proof of Lemma 3 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Following the proof of Theorem 2 in Appendix E, denote $\\mathbf{y}_{i}\\,=\\,\\mathbf{y}_{i}(\\mathcal{D})$ , $\\mathbf{y}_{i}^{\\prime}\\,=\\,\\mathbf{y}_{i}(D^{\\prime})$ and $\\bar{\\textbf{y}}=$ $(1/n)\\sum_{i=1}^{\\bar{n}}\\mathbf{y}_{i}$ . The proof begins with the following lemma. ", "page_idx": 24}, {"type": "text", "text": "Lemma 8. If $T_{i}>Z_{i}(\\mathcal{D})$ for all $i,$ , then $\\hat{\\mu}_{0}(\\mathcal{D})=\\bar{\\mathbf{y}}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. We just need to prove that $\\bar{\\mathbf{y}}$ is a minimizer of $\\textstyle\\sum_{i=1}^{n}w_{i}\\phi_{i}(\\mathbf{s},\\mathbf{y}_{i})$ . Since $Z_{i}<T_{i}$ for all $i$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\nabla\\phi_{i}({\\bar{\\mathbf{y}}},\\mathbf{y}_{i})=\\bar{\\mathbf{y}}-\\mathbf{y}_{i}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}w_{i}\\nabla\\phi_{i}(\\bar{\\bf y},{\\bf y}_{i})=\\sum_{i=1}^{n}w_{i}(\\bar{\\bf y}-{\\bf y}_{i})=0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now we derive the sensitivity. Without loss of generality, we assume that first $k$ users are replaced by others. Define ", "page_idx": 24}, {"type": "equation", "text": "$$\ng(\\mathbf{s})=\\sum_{i\\in I}w_{i}\\nabla\\phi(\\mathbf{s},\\mathbf{y}_{i}^{\\prime})+\\sum_{i\\in[n]\\backslash I}w_{i}\\nabla\\phi(\\mathbf{s},\\mathbf{y}_{i}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "then $g\\big(\\hat{\\mu}_{0}\\big(\\mathcal{D}^{\\prime}\\big)\\big)=0$ , and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{g(\\hat{\\mu}_{0}(\\mathcal{D}))}&{=}&{\\displaystyle\\sum_{i\\in I}w_{i}\\nabla\\phi(\\hat{\\mu}_{0}(\\mathcal{D}),\\mathbf{y}_{i}^{\\prime})+\\displaystyle\\sum_{i\\in[n]\\backslash I}w_{i}\\nabla\\phi(\\hat{\\mu}_{0}(\\mathcal{D}),\\mathbf{y}_{i})}\\\\ &{=}&{\\displaystyle\\sum_{i\\in I}w_{i}\\nabla\\phi(\\hat{\\mu}_{0}(\\mathcal{D}),\\mathbf{y}_{i}^{\\prime})-\\displaystyle\\sum_{i\\in I}w_{i}\\nabla\\phi(\\hat{\\mu}_{0}(\\mathcal{D}),\\mathbf{y}_{i})+\\displaystyle\\sum_{i=1}^{n}w_{i}\\nabla\\phi(\\hat{\\mu}_{0}(\\mathcal{D}),\\mathbf{y}_{i})}\\\\ &{=}&{\\displaystyle\\sum_{i\\in I}w_{i}\\nabla\\phi(\\hat{\\mu}_{0}(\\mathcal{D}),\\mathbf{y}_{i}^{\\prime})-\\displaystyle\\sum_{i\\in I}w_{i}\\nabla\\phi(\\hat{\\mu}_{0}(\\mathcal{D}),\\mathbf{y}_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Recall that $Z=\\mathrm{max}_{i}\\left\\|\\bar{\\mathbf{y}}-\\mathbf{y}_{i}\\right\\|$ . From Lemma 8, if $Z_{i}<T_{i}$ for all $i$ , then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{\\mu}_{0}(\\mathcal{D})-\\mathbf{y}_{i}\\|=\\|\\bar{\\mathbf{y}}-\\mathbf{y}_{i}\\|\\leq Z,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "thus ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|g(\\hat{\\mu}_{0}(\\mathcal{D}))\\right\\|\\leq\\sum_{i\\in I}w_{i}(T_{i}+Z_{i}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "From (107), ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\nabla g(\\mathbf{s})}&{=}&{\\displaystyle\\sum_{i\\in I}w_{i}\\nabla^{2}\\phi(\\mathbf{s},\\mathbf{y}_{i}^{\\prime})+\\displaystyle\\sum_{i\\in[n]\\backslash I}w_{i}\\nabla^{2}\\phi(\\mathbf{s},\\mathbf{y}_{i})}\\\\ &{\\displaystyle\\succeq}&{\\displaystyle\\sum_{i\\in[n]\\backslash I}w_{i}\\mathbf{1}(\\|\\mathbf{s}-\\mathbf{y}_{i}\\|\\leq T_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For all s satisfying $\\begin{array}{r}{\\|\\mathbf{s}-\\bar{\\mathbf{y}}\\|\\leq\\operatorname*{min}_{i}(T_{i}-Z_{i}),\\|\\mathbf{s}-\\mathbf{y}_{i}\\|\\leq T_{i}}\\end{array}$ always holds, thus ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\nabla g(\\mathbf{s})\\succeq\\left(\\sum_{i\\in[n]\\backslash I}w_{i}\\right)\\mathbf{I}=\\left(N-\\sum_{i\\in I}w_{i}\\right)\\mathbf{I}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Under condition $h(\\mathcal D,1)\\leq\\operatorname*{min}_{i}(T_{i}-Z_{i}(\\mathcal D))$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime})-\\hat{\\mu}_{0}(\\mathcal{D})\\|\\leq\\frac{\\sum_{i\\in I}w_{i}(T_{i}+Z_{i}(\\mathcal{D}))}{N-\\sum_{i\\in I}w_{i}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Recall that we have assumed $\\mathcal{D}^{\\prime}=\\{D_{1}^{\\prime},\\boldsymbol{\\cdot}\\boldsymbol{\\cdot}\\,,D_{k}^{\\prime},D_{k+1},\\dots,D_{n}\\}$ , i.e. first $k$ users are replaced by others. Actually, the replaced users can be arbitrarily selected from $n$ users. Therefore, a simple generalization yields: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\omega(\\mathcal{D},k)\\leq\\operatorname*{max}_{I\\subseteq[n],|I|=k}\\frac{\\sum_{i\\in I}w_{i}(T_{i}+Z_{i}(\\mathcal{D}))}{N-\\sum_{i\\in I}w_{i}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The proof is complete. ", "page_idx": 24}, {"type": "text", "text": "I Proof of Lemma 4 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Similar to the proof of Lemma 2 in Appendix $\\mathbf{C}$ , we analyze two cases. ", "page_idx": 25}, {"type": "text", "text": "For the first case, $u\\not\\in I$ , in which $u$ and $I$ have the same definition as in Appendix 5, without loss of generality, suppose $\\mathcal{D}$ and $\\mathcal{D}^{*}$ differ in the first $k$ users, while $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ differ in the $(k+1)$ -th user. Then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\|\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime})-\\hat{\\mu}_{0}(\\mathcal{D})\\|}&{\\leq}&{\\frac{m_{k+1}\\left(T_{k+1}+Z_{k+1}\\left(\\mathcal{D}^{*}\\right)+\\omega\\left(\\mathcal{D}^{*},k\\right)\\right)}{N-\\sum_{i=1}^{k+1}w_{i}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "From Lemma 3 and the condition $h(\\mathcal{D}^{*},k+1)<\\operatorname*{min}_{i}(T_{i}-Z_{i}(\\mathcal{D}^{*}))$ in Lemma 4, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\omega(D^{*},k)\\leq h(\\mathcal{D}^{*},k)<\\operatorname*{min}_{i}(T_{i}-Z_{i}(\\mathcal{D}^{*}))\\leq T_{k+1}-Z_{k+1}(\\mathcal{D}^{*}),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "thus ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\|\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime})-\\hat{\\mu}_{0}(\\mathcal{D})\\|}&{\\leq}&{\\frac{2m_{k+1}T_{k+1}}{N-\\sum_{i=1}^{k+1}w_{i}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For the second case, following steps in Appendix $\\mathbf{B}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\hat{\\mu}_{0}(\\mathcal{D}^{\\prime})-\\hat{\\mu}_{0}(\\mathcal{D})\\|\\leq\\frac{2m_{k}T_{k}}{N-\\sum_{i=1}^{k}w_{i}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Taking maximum, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\nL S(\\mathcal{D})\\leq\\frac{2\\underset{i\\in[n]}{\\mathrm{max}}w_{i}T_{i}}{N\\left(1-\\gamma(k+1)\\right)}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "J Proof of Theorem 5 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Similar to the proof of Theorem 2, denote $Z_{i}\\,=\\,Z_{i}({\\mathcal{D}})$ , $\\mathbf{Y}_{i}=\\mathbf{y}_{i}(\\mathcal{D})$ and $\\begin{array}{r}{\\bar{\\mathbf{Y}}=\\left(1/n\\right)\\sum_{i=1}^{n}\\mathbf{Y}_{i}}\\end{array}$ . Denote ", "page_idx": 25}, {"type": "equation", "text": "$$\nN_{c}=\\sum_{i=1}^{n}m_{i}\\wedge m_{c},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "then from the statement in Theorem 5, $w_{i}=m_{i}\\wedge m_{c}/N_{c}$ . From the statement of Theorem 5, recall that $m_{c}=\\gamma N/n$ . ", "page_idx": 25}, {"type": "text", "text": "The proof starts with the following lemma. ", "page_idx": 25}, {"type": "text", "text": "Lemma 9. With probability $1-(n+1)/(N n^{2})$ , for all $i,$ ", "page_idx": 25}, {"type": "equation", "text": "$$\nZ_{i}\\le\\sqrt{\\frac{32}{3}R^{2}\\ln(N n^{2}(d+1))}\\left(\\frac{1}{N_{c}}+\\frac{1}{\\sqrt{m_{i}}}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "From now on, denote $a_{i}$ as the right hand side of (121). ", "page_idx": 25}, {"type": "text", "text": "Proof. From Lemma 11, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{P}(\\left|\\left|\\bar{\\mathbf{Y}}-\\mu\\right|\\right|>t)\\leq(d+1)e^{-\\frac{3N t^{2}}{32R^{2}}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{P}\\left(\\left\\|\\mathbf{Y}_{i}-\\mu\\right\\|>t\\right)\\leq(d+1)e^{-\\frac{3m_{i}t^{2}}{32R^{2}}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Recall that $Z_{i}=\\left\\|\\bar{\\bf Y}-{\\bf Y}_{i}\\right\\|$ . Therefore ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\mathbb{P}\\left(\\cup_{i=1}^{n}\\{Z_{i}>a_{i}\\}\\right)}&{\\le}&{\\displaystyle\\mathbb{P}\\left(\\left\\|\\bar{\\mathbf{Y}}-\\mu\\right\\|>\\sqrt{\\frac{32R^{2}}{3N}\\ln(N n^{2}(d+1))}\\right)}\\\\ &&{\\displaystyle+\\sum_{i=1}^{n}\\mathbb{P}\\left(\\|\\mathbf{Y}_{i}-\\mu\\|>\\sqrt{\\frac{32R^{2}}{3m_{i}}\\ln(N n^{2}(d+1))}\\right)}\\\\ &{\\le}&{\\displaystyle\\frac{1}{N n^{2}}+n\\frac{1}{N n^{2}}}\\\\ &{=}&{\\displaystyle\\frac{n+1}{N n^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $Z_{i}\\leq a_{i}<T_{i}$ for all $i$ , from Lemma 8, $\\hat{\\mu}_{0}(\\mathcal{D})=\\bar{\\mathbf{Y}}$ . Moreover, we show that $\\Delta(\\mathcal{D})=0$ , in which $\\Delta$ is defined in (17). This needs the following lemma. ", "page_idx": 26}, {"type": "text", "text": "Lemma 10. For $k\\leq n/(8\\gamma)$ , if $Z_{i}\\leq a_{i}$ for all $i$ , in which $a_{i}$ is the right hand side of (121), then ", "page_idx": 26}, {"type": "equation", "text": "$$\nh({\\mathscr D},k)<\\operatorname*{min}_{i\\in[n]}(T_{i}-Z_{i}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Recall $T_{i}$ in (19), let ", "page_idx": 26}, {"type": "equation", "text": "$$\nA=C_{T}R\\sqrt{\\ln(N n^{2}(d+1))},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "then $T_{i}=A/\\sqrt{m_{i}\\wedge m_{c}}$ , thus ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{min}_{i}\\(T_{i}-Z_{i})}&{\\ge}&{\\operatorname*{min}_{i}\\left(\\frac{A}{\\sqrt{m_{i}\\wedge m_{c}}}-a_{i}\\right)}\\\\ &{\\ge}&{\\operatorname*{min}_{i}\\left(\\frac{A}{\\sqrt{m_{i}\\wedge m_{c}}}-2\\sqrt{\\frac{32R^{2}}{3m_{i}}}\\ln(N n^{2}(d+1))\\right)}\\\\ &{\\ge}&{\\operatorname*{min}_{i}\\left(\\frac{A}{\\sqrt{m_{i}\\wedge m_{c}}}-\\frac{A}{2\\sqrt{m_{i}}}\\right)}\\\\ {\\ge}&{\\frac{A}{2\\sqrt{m_{c}}}}\\\\ {=}&{\\frac{A}{2}\\sqrt{\\frac{n}{\\gamma N}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now we provide an upper bound of $h(\\mathcal{D},k)$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{h(\\mathcal{D},k)}&{\\overset{{(a)}}{\\leq}}&{\\frac{\\sum_{i=-k+1}^{n}w_{i}(\\mathcal{D}_{i}+{\\alpha}_{i})}{\\sum_{i=1}^{n-k}w_{i}}}\\\\ &{=}&{\\frac{\\sum_{i=n-k-1}^{n}\\left(m_{i}\\wedge m_{\\circ}\\right)\\left(T_{i}+{\\alpha}_{i}\\right)}{\\sum_{i=1}^{n-k}m_{i}\\wedge m_{\\circ}}}\\\\ &{\\leq}&{\\frac{\\sum_{i=n-k+1}^{n}\\left(m_{i}\\wedge m_{\\circ}\\right)\\,\\left(\\frac{A}{\\sqrt{m_{i}\\wedge m_{\\circ}}}+\\frac{A}{2\\sqrt{m_{i}\\wedge m_{\\circ}}}\\right)}{N_{C}-k m_{\\circ}}}\\\\ &{\\leq}&{\\frac{\\frac{3}{2}k A\\sqrt{m_{\\circ}}}{\\frac{k}{2}N-\\frac{k}{2}\\frac{m_{\\circ}}{2}}}\\\\ {\\overset{(b)}{\\leq}}&{\\frac{\\frac{3n}{2}A\\sqrt{\\frac{3n}{n}}}{\\frac{3}{4}N}}\\\\ &{=}&{\\frac{A}{2}\\sqrt{\\frac{n}{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "(a) comes from (16), and that $Z_{i}\\leq a_{i}$ for all $i$ . (b) uses the condition $k<n/(8\\gamma)$ . ", "page_idx": 26}, {"type": "text", "text": "Combine (127) and (128), (125) holds. The proof of Lemma 10 is complete. ", "page_idx": 26}, {"type": "text", "text": "Now it remains to bound of $G(\\mathcal{D},k)$ . From Lemma 10, if $Z_{i}\\leq a_{i}$ for all $i$ , then $\\Delta(\\mathcal{D})=0$ , since $h(\\mathcal{D}^{*},k_{0})<\\operatorname*{min}_{i}(T_{i}-Z_{i})$ . Then for all $k\\leq k_{0}-1$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{G(P,k)}&{\\stackrel{(a)}{\\leq}}&{\\displaystyle\\frac{2\\operatorname*{max}_{i}w_{1}T_{k}}{\\sum_{n=-1}^{n-1}w_{i}}}\\\\ &{=}&{\\displaystyle2\\frac{\\operatorname*{max}_{i}(m_{i}\\wedge m_{j})T_{k}}{\\sum_{n=-1}^{n-1}m_{i}\\wedge m_{n}}}\\\\ &{\\leq}&{\\displaystyle2\\frac{k\\wedge m_{k}}{N-1}}\\\\ &{\\stackrel{(b)}{\\leq}}&{\\displaystyle\\frac{2\\operatorname*{det}\\langle m_{n}\\rangle}{\\frac{2\\operatorname*{det}\\langle m_{1}\\rangle}{N-1\\wedge(k+1)^{\\frac{2N}{n}}}}}\\\\ &{=}&{\\displaystyle\\frac{4\\operatorname*{dam}_{N}}{N\\big(1-2\\gamma^{\\frac{k+1}{n}}\\big)}}\\\\ &{\\stackrel{(c)}{\\leq}}&{\\displaystyle\\frac{16\\operatorname{Ad}\\sqrt{m_{e}}}{3N\\sqrt{m_{e}}}}\\\\ &{=}&{\\displaystyle\\frac{164}{3N}\\sqrt{\\frac{m_{n}}{N}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "$h(\\mathcal D,1)\\leq\\operatorname*{min}_{i}(T_{i}-Z_{i}(\\mathcal D))$ 6 .h olNdso,t eo rt $2\\operatorname*{max}_{i}w_{i}T_{i}/(\\sum_{i=1}^{n-k-1}w_{i})$ $\\Delta(\\mathcal{D})~=~0$ $k\\,\\,\\leq\\,\\,k_{0}\\,-\\,1,\\,\\,G(\\mathscr{D},k)$ hiast $h(\\mathcal{D},1)$ eifr one is less than the latter, thus (a) holds. For (b), from Assumption 3, ", "page_idx": 27}, {"type": "equation", "text": "$$\nN_{c}=\\sum_{i=1}^{n}m_{i}\\wedge m_{c}\\geq N-\\sum_{k:m_{k}>\\gamma N/n}m_{k}\\geq N-{\\frac{N}{2}}={\\frac{N}{2}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "(c) holds because $\\gamma(k+1)/n\\leq\\gamma k_{0}/n\\leq1/8.$ ", "page_idx": 27}, {"type": "text", "text": "From (129), the smooth sensitivity can be bounded by ", "page_idx": 27}, {"type": "equation", "text": "$$\nS(\\mathcal D)\\leq\\operatorname*{max}\\left\\{{\\frac{16A}{3}}{\\sqrt{\\frac{\\gamma}{N n}}},2R e^{-\\beta k_{0}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Recall that $k_{0}\\,=\\,\\lfloor n/(8\\gamma)\\rfloor$ . In Theo\u221arem 5, it is required that $n>8\\gamma(1+(1/2\\beta)\\ln(N n))$ , thus $k_{0}\\geq\\ln(N n)/(2\\beta)$ , and $e^{-\\beta k_{0}}=1/\\sqrt{N n}$ . Therefore, the second term in (131) does not dominate. This result indicates that as long as $Z_{i}\\leq a_{i}$ for all $i$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\nS(D)\\leq{\\frac{16A}{3}}{\\sqrt{\\frac{\\gamma}{N n}}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now we bound the mean squared error. Denote $E$ as the event such that $Z_{i}\\leq a_{i}$ for all $i$ . Then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\left\\Vert\\hat{\\mu}(D)-\\mu\\right\\Vert^{2}\\right]}&{\\leq\\quad\\mathbb{E}\\left[\\left\\Vert\\hat{\\mu}_{0}(D)-\\mu\\right\\Vert^{2}\\mathbf{1}(E)\\right]+\\mathbb{E}\\left[\\left\\Vert W\\right\\Vert^{2}\\mathbf{1}(E)\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\left\\Vert\\mathrm{Clip}(\\hat{\\mu}_{0}(D),R)-\\mu\\right\\Vert^{2}\\mathbf{1}(E^{c})\\right]+\\mathbb{E}\\left[\\left\\Vert W\\right\\Vert^{2}\\mathbf{1}(E^{c})\\right]}\\\\ &{:=\\quad I_{1}+I_{2}+I_{3}+I_{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Bound of $I_{1}$ . ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{I_{1}}&{=\\;\\mathbb{E}\\left[\\left\\Vert\\mathbf{Y}-\\boldsymbol{\\mu}\\right\\Vert^{2}\\right]\\left(E_{1}\\right)}\\\\ &{\\leq\\;\\mathbb{E}\\left[\\left\\Vert\\mathbf{Y}-\\boldsymbol{\\mu}\\right\\Vert^{2}\\right]}\\\\ &{=\\;\\mathrm{trwh}\\left[\\sum_{i}w_{i}\\boldsymbol{\\Sigma}_{i}\\right]}\\\\ &{=\\;\\sum_{i}w_{i}^{2}\\frac{E_{i}^{2}}{m_{i}^{2}}}\\\\ &{=\\;\\frac{\\sum_{i}\\left(m_{i}\\cdot m_{i}\\right)^{2}\\frac{\\beta^{2}}{m_{i}\\cdot m_{i}}}{(\\sum_{i}m_{i}\\cdot\\Delta{m_{i}})^{2}}}\\\\ &{\\leq\\;\\frac{1}{\\sum_{i}\\left(m_{i}\\cdot m_{i}\\right)}}\\\\ &{=\\;\\frac{1}{N_{c}}}\\\\ &{\\leq\\;\\frac{2}{N_{c}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Bound of $I_{2}$ . ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l c l}{{I_{2}}}&{{=}}&{{\\displaystyle\\frac{\\mathbb{E}[S^{2}(\\mathcal{D}){\\bf1}(E)]}{\\alpha^{2}}d}}\\\\ {{}}&{{\\lesssim}}&{{\\displaystyle\\frac{d}{\\alpha^{2}}A^{2}\\frac{\\gamma}{N n}}}\\\\ {{}}&{{\\sim}}&{{\\displaystyle\\frac{d R^{2}\\gamma}{N n\\epsilon^{2}}\\ln(N n^{2}d)\\ln\\frac{1}{\\delta}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Bound of $I_{3}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{{I_{3}}}&{{\\le}}&{{4R^{2}{\\bf P}(E^{c})}}\\\\ {{}}&{{\\le}}&{{4\\displaystyle\\frac{n+1}{N n^{2}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Bound of $I_{4}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\nI_{4}\\lesssim\\frac{\\mathbb{E}[\\lambda^{2}{\\bf1}(E^{c})]}{\\alpha^{2}}d\\lesssim\\frac{d R^{2}}{\\epsilon^{2}}\\ln\\frac{1}{\\delta}\\frac{1}{N n}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "$I_{3}$ and $I_{4}$ converges to zero faster than any polynomial. Therefore ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{E}\\left[\\left\\|\\hat{\\mu}(D)-\\mu\\right\\|^{2}\\right]}&{\\lesssim}&{\\frac{R^{2}}{m n}+\\frac{d R^{2}\\gamma}{N\\epsilon^{2}}\\ln(N n^{2}d)\\ln\\frac{1}{\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "K Common Lemmas ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Lemma 11. (Concentration inequality of bounded random vector) Given a random vector X supported at $B_{d}(\\mathbf{0},R)$ , and $\\mathbb{E}[\\mathbf{X}]~=~\\mu.~~\\mathbf{X}_{1},\\ldots,\\mathbf{X}_{m}$ are m i.i.d copies of $\\mathbf{X}$ . Denote $\\bar{\\mathbf X}$ as the sample mean, i.e. $\\begin{array}{r}{\\bar{\\mathbf{X}}=\\left(1/\\bar{m}\\right)\\sum_{j=1}^{m}\\mathbf{X}_{j}}\\end{array}$ . Then ", "page_idx": 28}, {"type": "equation", "text": "$$\nP(\\|\\bar{\\mathbf{X}}-\\mu\\|>t)\\leq(d+1)e^{-\\frac{3m t^{2}}{32R^{2}}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. We use the following lemma. ", "page_idx": 28}, {"type": "text", "text": "Lemma 12. ( [80], Lemma 1.6.2) Let $\\mathbf{U}_{1},\\dotsc,\\mathbf{U}_{m}$ be independent centered random vectors with dimension d. Assume that each one is uniformly bounded, i.e. for $j=1,\\dots,m_{\\!}$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{U}_{j}]=0,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and with probability 1, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|\\mathbf{U}_{j}\\|\\leq L.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let $\\textstyle\\mathbf{Z}=\\sum_{j=1}^{m}\\mathbf{U}_{j}{}^{2}$ , and define ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\gamma(\\mathbf{Z})=\\left|\\sum_{j=1}^{m}\\mathbb{E}[\\mathbf{U}_{j}^{T}\\mathbf{U}_{j}]\\right|.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then for all $t>0$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\nP(\\|\\mathbf Z\\|>t)\\leq(d+1)\\exp\\left[-\\frac{t^{2}/2}{\\gamma(\\mathbf Z)+L t/3}\\right].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now we prove Lemma 11 based on Lemma 12. Let $\\mathbf{U}_{j}=\\mathbf{X}_{j}-\\boldsymbol{\\mu}$ . Since $\\|\\mathbf{X}_{j}\\|\\leq R,\\,\\|\\mu\\|\\leq R$ holds, $\\|\\mathbf{X}_{j}-\\boldsymbol{\\mu}\\|\\leq2R$ always holds, and $\\gamma(\\mathbf{Z})=4m R^{2}$ . Hence ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathrm{P}(\\|\\mathbf{Z}\\|>t)\\leq(d+1)\\exp\\left[-\\frac{t^{2}/2}{4m R^{2}+\\frac{2}{3}R t}\\right].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Hence ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{\\mathbf{P}(\\|\\bar{\\mathbf{X}}-{\\boldsymbol{\\mu}}\\|>t)}}&{{\\le}}&{{(d+1)\\exp\\left[-\\displaystyle\\frac{m^{2}t^{2}}{8m R^{2}+\\frac{4}{3}R t m}\\right]}}\\\\ {{}}&{{\\le}}&{{(d+1)\\exp\\left[-\\displaystyle\\frac{3m t^{2}}{32R^{2}}\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Lemma 13. (Concentrated inequality of unbounded random vector) Given a random vector $\\mathbf{X}$ , $\\mathbb{E}[\\mathbf{X}]=\\mu,$ , and E $[\\|\\mathbf{X}-\\boldsymbol{\\mu}\\|^{p}]\\leq M_{p}$ . $\\mathbf{X}_{1},\\hdots,\\mathbf{X}_{m}$ are m i.i.d copies of X. Denote $\\bar{\\mathbf X}$ as the sample mean. Then with probability at least $1-\\nu_{i}$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|\\bar{\\mathbf{X}}-\\mu\\right\\|\\leq\\operatorname*{max}\\left\\{2M_{p}^{\\frac{1}{p}}\\sqrt{\\frac{1}{m}\\ln\\frac{3(d+1)}{\\nu}},4M_{p}^{\\frac{1}{p}}(3m)^{\\frac{1}{p}-1}\\nu^{-\\frac{1}{p}}\\ln\\frac{3(d+1)}{\\nu}\\right\\}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. We still use Lemma 12. Pick $r>0$ , whose exact value will be determined later. Define ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbf{U}_{j}:=(\\mathbf{X}_{j}-\\mu)\\mathbf{1}(\\|\\mathbf{X}_{j}-\\mu\\|\\leq r),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbf{V}_{j}:=(\\mathbf{X}_{j}-\\mu)\\mathbf{1}(\\|\\mathbf{X}_{j}-\\mu\\|>r).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{X}}-\\mu=\\frac{1}{m}\\sum_{j=1}^{m}\\mathbf{U}_{j}+\\frac{1}{m}\\sum_{j=1}^{m}\\mathbf{V}_{j},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbf{P}\\left(\\left\\|{\\bar{\\mathbf{X}}}-\\mu\\right\\|>t\\right)\\leq\\mathbf{P}\\left(\\left\\|{\\frac{1}{m}}\\sum_{j=1}^{m}\\mathbf{U}_{j}\\right\\|>t\\right)+\\mathbf{P}\\left(\\left\\|{\\frac{1}{m}}\\sum_{j=1}^{m}\\mathbf{V}_{j}\\right\\|>0\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "2This definition of $\\mathbf{Z}$ is only used in this section. ", "page_idx": 29}, {"type": "text", "text": "Let $\\begin{array}{r}{{\\bf Z}=\\sum_{j=1}^{m}{\\bf U}_{j}}\\end{array}$ . Then from (142), ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\gamma(\\mathbf{Z})=\\mathbb{E}\\left[\\sum_{j=1}^{m}\\mathbf{U}_{j}^{T}\\mathbf{U}_{j}\\right]\\leq m\\mathbb{E}[\\left\\|\\mathbf{X}-\\boldsymbol{\\mu}\\right\\|^{2}]\\leq m M_{p}^{\\frac{2}{p}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Note that $\\|\\mathbf{U}_{j}\\|<r$ always holds. Therefore, from Lemma 12, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbf{P}\\left(\\left\\|\\mathbf{X}\\right\\|>t\\right)}&{\\le}&{(d+1)\\exp\\left[-\\frac{t^{2}/2}{m M_{p}^{\\frac{2}{p}}+\\frac{1}{3}r t}\\right]}\\\\ &{\\le}&{(d+1)\\exp\\left[-\\operatorname*{min}\\left\\{\\frac{t^{2}}{4m M_{p}^{\\frac{2}{p}}},\\frac{3t}{4r}\\right\\}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{\\displaystyle\\mathbb{P}\\left(\\left\\|\\frac{1}{m}\\sum_{j=1}^{m}\\mathbf{U}_{j}\\right\\|>t\\right)}&{=}&{\\displaystyle\\mathbb{P}(\\|\\mathbf{Z}\\|>m t)}\\\\ &{\\leq}&{\\displaystyle(d+1)\\exp\\left[-\\operatorname*{min}\\left\\{\\frac{m t^{2}}{4M_{p}^{\\frac{2}{p}}},\\frac{3m t}{4r}\\right\\}\\right]}\\\\ &{\\leq}&{\\displaystyle(d+1)e^{-\\frac{m t^{2}}{4r^{\\frac{2}{p}}}}+(d+1)e^{-\\frac{3m t}{4r}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now we have bounded the first term in (150). For the second term in (150), ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\mathbb{P}\\left(\\left\\|\\frac{1}{m}\\sum_{j=1}^{m}\\mathbf{V}_{j}\\right\\|>0\\right)}&{\\leq}&{\\displaystyle\\mathbb{P}\\left(\\cup_{j=1}^{m}\\{\\|\\mathbf{V}_{j}\\|>0\\}\\right)}\\\\ &{\\leq}&{m\\displaystyle\\mathbf{P}\\left(\\|\\mathbf{X}-\\boldsymbol{\\mu}\\|>r\\right)}\\\\ &{\\leq}&{m M_{p}r^{-p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, from (150), (153) and (154), ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{P}\\left(\\left\\|\\bar{\\mathbf{X}}-{\\boldsymbol{\\mu}}\\right\\|>t\\right)\\leq(d+1)e^{-\\frac{m t^{2}}{4M_{p}^{\\frac{2}{p}}}}+(d+1)e^{-\\frac{3m t}{4r}}+M_{p}m r^{-p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "To make the right hand side of (155) to be no more than $\\nu$ , we let each term to be no more than $\\nu/3$ . Note that now $r$ has not be determined. Therefore, we let the third term equals $\\nu/3$ first, thus ", "page_idx": 30}, {"type": "equation", "text": "$$\nr=\\left(\\frac{3M_{p}m}{\\nu}\\right)^{\\frac{1}{p}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then we let ", "page_idx": 30}, {"type": "equation", "text": "$$\nt=\\operatorname*{max}\\left\\{2M_{p}^{\\frac1p}\\sqrt{\\frac{1}{m}\\ln\\frac{3(d+1)}{\\nu}},4M_{p}^{\\frac1p}(3m)^{\\frac1p-1}\\nu^{-\\frac1p}\\ln\\frac{3(d+1)}{\\nu}\\right\\}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "With (156) and (157), all three terms in the right hand side of (155) will be no more than $\\nu/3$ . Therefore, with probability at least $1-\\nu$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left\\|\\bar{\\mathbf{X}}-\\mu\\right\\|\\leq\\operatorname*{max}\\left\\{2M_{p}^{\\frac{1}{p}}\\sqrt{\\frac{1}{m}\\ln\\frac{3(d+1)}{\\nu}},4M_{p}^{\\frac{1}{p}}(3m)^{\\frac{1}{p}-1}\\nu^{-\\frac{1}{p}}\\ln\\frac{3(d+1)}{\\nu}\\right\\}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The proof is complete. ", "page_idx": 30}, {"type": "text", "text": "Lemma 14. (Bias caused by outliers) Given a random vector X, $\\mathbb{E}[\\mathbf{X}]=\\mu,$ , and $\\mathbb{E}\\left[\\|\\mathbf{X}-\\boldsymbol{\\mu}\\|^{p}\\right]\\leq M_{p}$ . $\\mathbf{X}_{1},\\hdots,\\mathbf{X}_{m}$ are $m$ i.i.d copies of $\\mathbf{X}$ . Denote $\\bar{\\mathbf X}$ as the sample mean, and ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{X}}^{*}=\\left\\{\\begin{array}{l l}{\\bar{\\mathbf{X}}}&{i f\\quad\\left\\|\\bar{\\mathbf{X}}-\\boldsymbol{\\mu}\\right\\|\\leq r_{0}}\\\\ {\\mu}&{i f\\quad\\left\\|\\bar{\\mathbf{X}}-\\boldsymbol{\\mu}\\right\\|>r_{0},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "in which ", "page_idx": 31}, {"type": "equation", "text": "$$\nr_{0}=\\operatorname*{max}\\left\\{2M_{p}^{\\frac{1}{p}}\\sqrt{\\frac{1}{m}\\ln\\frac{3(d+1)}{\\nu}},4M_{p}^{\\frac{1}{p}}(3m)^{\\frac{1}{p}-1}\\nu^{-\\frac{1}{p}}\\ln\\frac{3(d+1)}{\\nu}\\right\\},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "then ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}[\\bar{\\mathbf{X}_{}^{*}}]-\\mu\\right\\|\\leq C_{p}r_{0}\\nu\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for some constant $C_{p}$ that depends only on $p$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. From (159), ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\bar{\\mathbf{X}}^{*}]=\\mathbb{E}[\\bar{\\mathbf{X}}\\mathbf{1}(\\left\\|\\bar{\\mathbf{X}}-\\boldsymbol{\\mu}\\right\\|\\leq r_{0})]+\\mu\\mathbf{P}(\\left\\|\\bar{\\mathbf{X}}-\\boldsymbol{\\mu}\\right\\|>r_{0}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Thus ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\bar{\\mathbf{X}}^{*}]-\\mu=\\mathbb{E}[(\\bar{\\mathbf{X}}-\\mu)\\mathbf{1}(\\left\\|\\bar{\\mathbf{X}}-\\mu\\right\\|\\leq r_{0})].\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Note that $\\mathbb{E}[\\bar{\\mathbf{X}}]=\\mathbb{E}[\\mathbf{X}]=\\mu$ . Hence ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\bar{\\mathbf{X}}^{*}]-\\mu=-\\mathbb{E}[(\\bar{\\mathbf{X}}-\\mu)\\mathbf{1}(\\left\\|\\bar{\\mathbf{X}}-\\mu\\right\\|>r_{0})].\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Therefore ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{\\displaystyle\\left\\|\\mathbb{E}[\\bar{\\mathbf{X}}^{*}]-\\mu\\right\\|}&{\\leq}&{\\displaystyle\\mathbb{E}[\\left\\|\\bar{\\mathbf{X}}-\\mu\\right\\|\\mathbf{1}(\\left\\|\\bar{\\mathbf{X}}-\\mu\\right\\|>r_{0})]}\\\\ &{=}&{\\displaystyle\\int_{0}^{\\infty}\\mathbf{P}(\\left\\|\\bar{\\mathbf{X}}-\\mu\\right\\|\\mathbf{1}(\\left\\|\\bar{\\mathbf{X}}-\\mu\\right\\|>r_{0})>t)d t}\\\\ &{=}&{r_{0}\\mathbf{P}(\\left\\|\\bar{\\mathbf{X}}-\\mu\\right\\|>r_{0})+\\displaystyle\\int_{r_{0}}^{\\infty}\\mathbf{P}(\\left\\|\\bar{\\mathbf{X}}-\\mu\\right\\|>t)d t}\\\\ &{=}&{r_{0}\\nu+\\displaystyle\\sum_{k=0}^{\\infty}\\int_{r_{k}}^{r_{k+1}}\\mathbf{P}\\left(\\left\\|\\bar{\\mathbf{X}}-\\mu\\right\\|>r_{k}\\right)d t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "in which the last step uses Lemma 13, and ", "page_idx": 31}, {"type": "equation", "text": "$$\nr_{k}=\\operatorname*{max}\\left\\{2M_{p}^{\\frac{1}{p}}\\sqrt{\\frac{1}{m}\\ln\\frac{3(d+1)}{2^{-k}\\nu}},4M_{p}^{\\frac{1}{p}}(3m)^{\\frac{1}{p}-1}(2^{-k}\\nu)^{-\\frac{1}{p}}\\ln\\frac{3(d+1)}{2^{-k}\\nu}\\right\\}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "From Lemma 1 $3,\\mathbf{P}(\\left\\|\\bar{\\mathbf{X}}-\\boldsymbol{\\mu}\\right\\|>r_{k})\\leq2^{-k}\\nu$ , thus ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\|\\mathbb{E}[\\bar{\\mathbf{X}}^{*}]-\\boldsymbol{\\mu}\\|}&{\\le}&{r_{0}\\nu+\\displaystyle\\sum_{k=0}^{\\infty}2^{-k}\\nu(r_{k+1}-r_{k})}\\\\ &{=}&{\\nu\\displaystyle\\sum_{k=1}^{\\infty}2^{-k}r_{k}}\\\\ &{=}&{r_{0}\\nu\\displaystyle\\sum_{k=1}^{\\infty}2^{-k}\\displaystyle\\frac{r_{k}}{r_{0}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "From (166) and (160), ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{r_{k}}{r_{0}}\\leq2^{\\frac{k}{p}}\\frac{\\ln\\frac{3(d+1)}{\\nu}+k\\ln2}{\\ln\\frac{3(d+1)}{\\nu}},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "thus ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\left\\|\\mathbb{E}[\\bar{\\mathbf{X}^{*}}]-\\mu\\right\\|}&{\\leq}&{r_{0}\\nu\\displaystyle\\left[\\displaystyle\\sum_{k=1}^{\\infty}2^{-k(1-1/p)}+\\displaystyle\\frac{\\ln2}{\\ln\\displaystyle\\frac{3(d+1)}{\\nu}}\\sum_{k=1}^{\\infty}k2^{-k(1-1/p)}\\right]}\\\\ &{\\leq}&{C_{p}r_{0}\\nu,}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for some constant $C_{p}$ that depends only on $p$ . The proof is complete. ", "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 32}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 32}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 32}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 32}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 32}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 32}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: The main contribution (i.e. proposing a new Huber loss minimization approach which is more suitable to realistic cases, and providing theoretical analysis) has been made clear in the abstract and introduction. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: It is explained at the end of conclusion section. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: Proofs are shown in the appendix, and intuition is provided in the paper. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: Experiment details are explained in the paper. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 33}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: Codes are provided. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 34}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Details are provided in the paper. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We provide error bars in figures. Since experiments are repeated 1000 times, the error is relatively low, thus the error bar may be too small to be visible for some cases. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The experiments need little computational resources. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 35}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: Our paper does not violate code of ethics. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper is foundational and theoretical research and not tied to particular applications. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper has no such risks. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: This paper uses IPUMS dataset. We cite them in the paper. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This paper does not release new assets ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]