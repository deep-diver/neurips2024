[{"figure_path": "0og7nmvDbe/figures/figures_1_1.jpg", "caption": "Figure 1: Entropy and Prediction. We mean ablate final-layer neurons across 4000 tokens and measure the variation in the entropy of the model's output Pmodel against average change of model's prediction (argmax Pmodel(x)). We identify a set of neurons whose effect depends on LayerNorm (red points; metric described in \u00a73.2), and which affect the model's confidence (quantified as entropy of Pmodel) with minimal impact on the prediction.", "description": "This figure shows the relationship between the change in entropy and the change in prediction after ablating (removing) individual neurons in the final layer of a language model.  Each point represents a neuron. The x-axis shows how much the model's prediction changes after the neuron is ablated, and the y-axis represents the change in the model's output entropy.  Red points highlight neurons whose effect is mediated by Layer Normalization, demonstrating that they influence confidence without significantly altering the model's prediction. This suggests the presence of specialized neurons influencing model uncertainty.", "section": "3 Entropy neurons"}, {"figure_path": "0og7nmvDbe/figures/figures_3_1.jpg", "caption": "Figure 2: Identifying and Analyzing Entropy Neurons. (a) Neurons in GPT-2 Small displayed by their weight norm and variance in logit attribution. Entropy neurons (red) have high norm and low logit variance. (b) Causal graph showing the total effect and direct effect (bypassing LayerNorm) of a neuron on the model's output. (c) Comparison of total and direct effects on model loss for entropy neurons and randomly selected neurons. (d) Singular values and cosine similarity between neuron output weights and singular vectors of Wu. (e) Entropy neurons (red) show significant LayerNorm-mediated effects and high projection onto the null space (p). (f) LN-mediated effect in LLaMA2 7B. p is computed with k = 40 \u2248 0.01 * dmodel. Color represents absolute change in entropy upon ablation (\u0394\u0397).", "description": "This figure analyzes entropy neurons, a type of neuron with high weight norm and low composition with the unembedding matrix.  Panel (a) shows the relationship between weight norm and logit variance, identifying entropy neurons. Panel (b) presents a causal graph illustrating the total and direct effects of a neuron on the model output, mediated by LayerNorm.  Panel (c) compares these effects for entropy neurons versus randomly selected neurons. Panel (d) shows the projection of neuron output weights onto the singular vectors of the unembedding matrix, indicating a high projection onto the null space for entropy neurons. Panel (e) demonstrates the significant LayerNorm-mediated effects of these neurons and their high projection onto the null space. Finally, panel (f) shows the same relationship in the LLaMA2 7B model.", "section": "3 Entropy neurons"}, {"figure_path": "0og7nmvDbe/figures/figures_5_1.jpg", "caption": "Figure 3: Token Frequency Neurons in Pythia 410M. (a) DKL(Pfreq||Pmodel) and Entropy are correlated negatively. (b) Scatter plot of neurons highlighting token frequency neurons (in green), with high effect on DKL(Pfreq||Pmodel), significantly mediated by the token frequency direction. (c) Box plots showing substantial difference in total vs. direct effect in token frequency neurons.", "description": "This figure analyzes token frequency neurons in the Pythia 410M language model. Panel (a) shows a negative correlation between the KL divergence from the token frequency distribution (DKL) and entropy, indicating that as model uncertainty increases (higher entropy), the model's output distribution becomes closer to the token frequency distribution. Panel (b) is a scatter plot showing the relationship between a neuron's effect on DKL and the mediation effect via the token frequency direction.  Token frequency neurons, highlighted in green, show a substantial mediation effect. Finally, panel (c) uses box plots to show that the total effect of token frequency neurons (on model loss) is significantly different from their direct effect, suggesting that these neurons impact confidence primarily through modulation of the token frequency distribution.", "section": "4 Token frequency neurons"}, {"figure_path": "0og7nmvDbe/figures/figures_6_1.jpg", "caption": "Figure 4: Examples of Neuron Activity in Language Models. (a) Change in loss after ablation of entropy neuron 11.2378 in GPT-2 Small. Color indicates reciprocal rank (RR) of the correct token prediction. (b) Activation of neuron 11.2378 on an example from the C4 Corpus. The neuron mitigates a loss spike at the token \u201cMes,\u201d after which the model predicts \u201cotherapy.\u201d (c) Change in entropy and KL divergence on correct tokens (RR = 1) post ablation of neuron 23.417 in Pythia 410M. The neuron increases entropy and aligns the model\u2019s output with the token frequency distribution.", "description": "This figure shows three subplots illustrating the activity of specific neurons in language models. Subplot (a) demonstrates the impact of ablating entropy neuron 11.2378 in GPT-2 Small on model loss, color-coded by the reciprocal rank of the correct prediction. Subplot (b) shows the activation pattern of the same neuron on a specific example from the C4 corpus, highlighting its role in mitigating a loss spike. Finally, subplot (c) displays the effect of ablating token frequency neuron 23.417 in Pythia 410M on entropy and KL divergence for correctly predicted tokens, indicating its influence on model confidence and alignment with the token frequency distribution.", "section": "Examples of neuron activity"}, {"figure_path": "0og7nmvDbe/figures/figures_7_1.jpg", "caption": "Figure 5: Entropy Neurons on Induction. (a) Activations, entropy, and loss across duplicated 200-token input sequences. (b) The effect of clip mean-ablation of specific entropy neurons. Neuron 11.2378 shows the most significant impact, with up to a 70% reduction in entropy. (c) BOS ablation of induction heads: Upon the ablation of three induction heads in GPT-2 Small, the activation of entropy neuron 11.2378 decreases substantially.", "description": "This figure shows the results of experiments on induction, which is the repetition of subsequences in the input.  Panel (a) displays activations, entropy, and loss across duplicated 200-token input sequences. Panel (b) demonstrates the impact of selectively turning off ('ablating') specific entropy neurons; particularly, neuron 11.2378 shows a significant reduction in entropy (up to 70%). Finally, panel (c) illustrates the effect of removing ('ablating') induction heads on the activation of neuron 11.2378, revealing a substantial decrease in its activation.", "section": "6 Case study: Induction"}, {"figure_path": "0og7nmvDbe/figures/figures_18_1.jpg", "caption": "Figure 6: Effect of Dropout on Wu. Comparison of singular values for the unembedding matrix between two versions of Pythia 160M\u2014one trained with dropout (red) and one without dropout (blue).", "description": "This figure shows the singular values of the unembedding matrix (Wu) for two versions of the Pythia 160M model. One model was trained with dropout, and the other was trained without dropout. The singular values represent the importance of different dimensions in the vocabulary space.  The sharp drop in singular values for both models indicates the presence of an effective null space in the unembedding matrix. The model trained with dropout has smaller singular values, especially in the lower dimensions, which is consistent with the hypothesis that dropout encourages the creation of a null space. This null space is where entropy neurons operate, and the low singular values suggest that the model is deliberately restricting the capacity of its representation by making certain dimensions of the residual stream have minimal effect on the final logits.", "section": "3 Entropy neurons"}, {"figure_path": "0og7nmvDbe/figures/figures_19_1.jpg", "caption": "Figure 7: Entropy Neurons Across Different Models. Relationship between the fraction p of neuron norm in the Wu null space and the LayerNorm-mediated effect on model output for neurons in the last layer of various models. (a) Change in loss after ablation of entropy neuron 31.3228 in LLaMA2 7B. Ablation decreases entropy and increases loss on incorrect predictions.", "description": "This figure shows the relationship between the fraction of a neuron's norm that lies in the null space of the unembedding matrix (p) and the LayerNorm-mediated effect on the model's output for neurons in the last layer of several different language models.  The color of each point represents the magnitude of the change in entropy after ablating the neuron.  The figure also shows a case study where ablating an entropy neuron in LLaMA2 7B decreases the entropy of the output distribution and increases the loss on incorrect predictions. This demonstrates that entropy neurons modulate confidence by impacting the LayerNorm, a mechanism that may be overlooked by other methods of analysis that focus solely on the logits.", "section": "3 Entropy neurons"}, {"figure_path": "0og7nmvDbe/figures/figures_20_1.jpg", "caption": "Figure 8: Token Frequency Neurons Across Different Models. Relationship between the token frequency-mediated effect and the average absolute change in the KL divergence from Pfreq for final-layer neurons in (a) GPT-2 Small and (b) Pythia 1B. Neurons are categorized as normal, entropy, or token frequency neurons.", "description": "This figure shows the relationship between the effect of token frequency neurons and the change in KL divergence from the unigram distribution for GPT-2 Small and Pythia 1B.  Each point represents a neuron, colored according to its type: normal, entropy, or token frequency. The x-axis represents the fraction of the neuron's total effect that is not mediated by the token frequency direction. The y-axis represents the average absolute change in KL divergence from the unigram distribution after ablating the neuron.  It illustrates how token frequency neurons modulate the model's output distribution's proximity to the unigram distribution, particularly in relation to entropy neurons and normal neurons.", "section": "4 Token frequency neurons"}, {"figure_path": "0og7nmvDbe/figures/figures_20_2.jpg", "caption": "Figure 5: Entropy Neurons on Induction. (a) Activations, entropy, and loss across duplicated 200-token input sequences. (b) The effect of clip mean-ablation of specific entropy neurons. Neuron 11.2378 shows the most significant impact, with up to a 70% reduction in entropy. (c) BOS ablation of induction heads: Upon the ablation of three induction heads in GPT-2 Small, the activation of entropy neuron 11.2378 decreases substantially.", "description": "This figure shows the results of experiments designed to investigate the role of entropy neurons in the phenomenon of induction (repeated subsequences in text). Panel (a) displays the activations, entropy, and loss in a model processing a repeated sequence.  Panel (b) demonstrates the effect of ablating (silencing) individual entropy neurons on the resulting entropy.  Neuron 11.2378 is highlighted as having a significant impact. Panel (c) explores the relationship between the activation of neuron 11.2378 and the activation of specific induction heads (parts of the model specialized for handling repeated sequences). Ablating induction heads reduces the activation of 11.2378, showing a connection between these components in the model's processing of induction.", "section": "Case study: Induction"}, {"figure_path": "0og7nmvDbe/figures/figures_21_1.jpg", "caption": "Figure 10: Token Frequency Neurons on Induction. (a) Activations, entropy, and loss across duplicated 200-token input sequences. (b) The effect of clip mean-ablation of specific token frequency neurons. (c) Scatter plot of loss changes per token post-ablation of neuron 23.417, colored by reciprocal rank (RR). Ablation tends to increase loss for low initial loss tokens and decrease it for high initial loss tokens.", "description": "This figure demonstrates the activity of token frequency neurons during induction tasks. Panel (a) shows the activations, entropy, and loss across duplicated input sequences. Panel (b) shows the effect of mean-ablating specific token frequency neurons, revealing their influence on entropy. Panel (c) provides a scatter plot illustrating the change in loss upon ablation of neuron 23.417, colored by reciprocal rank (RR). The results suggest that ablation tends to increase loss for tokens with initially low loss and decreases it for tokens with initially high loss.", "section": "H Additional Results on Induction"}, {"figure_path": "0og7nmvDbe/figures/figures_22_1.jpg", "caption": "Figure 1: Entropy and Prediction. We mean ablate final-layer neurons across 4000 tokens and measure the variation in the entropy of the model's output Pmodel against average change of model's prediction (argmax Pmodel(x)). We identify a set of neurons whose effect depends on LayerNorm (red points; metric described in \u00a73.2), and which affect the model's confidence (quantified as entropy of Pmodel) with minimal impact on the prediction.", "description": "This figure shows the relationship between the change in entropy and the change in prediction accuracy when ablating individual neurons in the final layer of a large language model.  The x-axis represents the change in prediction accuracy after ablating a neuron, while the y-axis represents the absolute change in entropy.  The red points highlight neurons whose effect is dependent on Layer Normalization (LN).  The figure demonstrates that there is a set of neurons that significantly impact the model's confidence (entropy), while having minimal direct effect on the model's predictions. This suggests that these neurons are involved in regulating the model's uncertainty.", "section": "3 Entropy neurons"}, {"figure_path": "0og7nmvDbe/figures/figures_23_1.jpg", "caption": "Figure 11: Neuron activation differences for synthetic and natural induction: (left) GPT2-Small final-layer neurons (right) LLaMa2 7B final-layer neurons. Among final-layer neurons, entropy neurons exhibit the largest activation change for both synthetic and natural induction settings. Entropy neurons with especially high fraction of neuron norm in the null space (p) and large change in activation are labelled.", "description": "This figure compares the change in activation of neurons in GPT-2 small and LLaMa2 7B models during synthetic and natural induction. It shows that entropy neurons exhibit the largest activation change in both cases.  Neurons with a high proportion of their norm projected onto the unembedding null space and a large change in activation are highlighted.", "section": "6 Case study: Induction"}, {"figure_path": "0og7nmvDbe/figures/figures_23_2.jpg", "caption": "Figure 2: Identifying and Analyzing Entropy Neurons. (a) Neurons in GPT-2 Small displayed by their weight norm and variance in logit attribution. Entropy neurons (red) have high norm and low logit variance. (b) Causal graph showing the total effect and direct effect (bypassing LayerNorm) of a neuron on the model's output. (c) Comparison of total and direct effects on model loss for entropy neurons and randomly selected neurons. (d) Singular values and cosine similarity between neuron output weights and singular vectors of Wu. (e) Entropy neurons (red) show significant LayerNorm-mediated effects and high projection onto the null space (p). (f) LN-mediated effect in LLAMA2 7B. p is computed with k = 40 \u2248 0.01 * dmodel. Color represents absolute change in entropy upon ablation (\u0394H).", "description": "This figure shows the analysis of entropy neurons in GPT-2 Small and LLaMA2 7B. Panel (a) displays neurons based on weight norm and logit variance, identifying entropy neurons. Panel (b) illustrates the causal mediation analysis showing the total and direct effects of a neuron on the model's output. Panel (c) compares the total and direct effects on model loss for entropy and randomly selected neurons. Panel (d) shows singular values and cosine similarity between neuron output weights and singular vectors of Wu (unembedding matrix). Panel (e) highlights entropy neurons' significant LayerNorm-mediated effects and projection onto the null space. Panel (f) shows the LayerNorm-mediated effect in LLaMA2 7B, relating to the fraction of the neuron's norm in the null space and the change in entropy upon ablation.", "section": "3 Entropy neurons"}, {"figure_path": "0og7nmvDbe/figures/figures_24_1.jpg", "caption": "Figure 7: Entropy Neurons Across Different Models. Relationship between the fraction p of neuron norm in the Wu null space and the LayerNorm-mediated effect on model output for neurons in the last layer of various models. (a) Change in loss after ablation of entropy neuron 31.3228 in LLaMA2 7B. Ablation decreases entropy and increases loss on incorrect predictions.", "description": "This figure examines the relationship between the fraction of a neuron's weight norm projected into the null space of the unembedding matrix and the effect of LayerNorm on the model's output. It shows that entropy neurons, characterized by a high weight norm and minimal direct effect on logits, significantly affect the model's output distribution through LayerNorm. The ablation of an entropy neuron in LLaMA2 7B decreases entropy and increases losses on incorrect predictions, highlighting their role in confidence calibration.", "section": "3 Entropy neurons"}]