[{"type": "text", "text": "DG-SLAM: Robust Dynamic Gaussian Splatting SLAM with Hybrid Pose Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yueming $\\mathbf{Xu^{1*}}$ Haochen Jiang1\u2217 Zhongyang Xiao2 Jianfeng Feng1 Li Zhang1\u2020 ", "page_idx": 0}, {"type": "text", "text": "Fudan University 2Autonomous Driving Division, NIO https://github.com/fudan-zvg/DG-SLAM ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Achieving robust and precise pose estimation in dynamic scenes is a significant research challenge in Visual Simultaneous Localization and Mapping (SLAM). Recent advancements integrating Gaussian Splatting into SLAM systems have proven effective in creating high-quality renderings using explicit 3D Gaussian models, significantly improving environmental reconstruction fidelity. However, these approaches depend on a static environment assumption and face challenges in dynamic environments due to inconsistent observations of geometry and photometry. To address this problem, we propose DG-SLAM, the first robust dynamic visual SLAM system grounded in 3D Gaussians, which provides precise camera pose estimation alongside high-fidelity reconstructions. Specifically, we propose effective strategies, including motion mask generation, adaptive Gaussian point management, and a hybrid camera tracking algorithm to improve the accuracy and robustness of pose estimation. Extensive experiments demonstrate that DG-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and novel-view synthesis in dynamic scenes, outperforming existing methods meanwhile preserving real-time rendering ability. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Visual simultaneous localization and mapping (SLAM), the task of reconstructing a 3D map within an unknown environment while simultaneously estimating the camera pose, is recognized as a critical component to achieving autonomous navigation in novel 3D environments for mobile robots [1]. It has been widely used in various forms in fields such as robotics, autonomous driving, and augmented/virtual reality (AR/VR). However, the majority of previous research [2, 3, 4, 5, 6, 7, 8, 9, 10] typically relies on the assumption of static environments, limiting the practical applicability of this technology in daily life. Consequently, how to achieve accurate and robust pose estimation in dynamic scenes remains an urgent problem to be addressed for mobile robots. ", "page_idx": 0}, {"type": "text", "text": "Recently, many researchers [6, 7, 8, 9, 10] have endeavored to replace the conventional explicit representations used in visual SLAM, such as Signed Distance Function (SDF), voxel grids [11], meshes [12], and surfel clouds [13], with the neural radiance field (NeRF) [14] approach for reconstructing the neural implicit map. This novel map representation is more continuous, efficient, and able to be optimized with differentiable rendering, which has the potential to benefit applications like navigation and reconstruction. However, these methods exhibit two primary issues: the scene\u2019s bounds are required to be predefined to initialize the neural voxel grid; and the implicit representation proves challenging for information fusion and editing. To address these problems, recent works like GS-SLAM [15], SplaTam [16], and Gaussian splatting SLAM [17] leverage the 3D-GS [18] to explicit represent the scene\u2019s map. This explicit geometric representation is also smooth, continuous, and differentiable. Moreover, a substantial array of Gaussians can be rendered with high efficiency through splatting rasterization techniques, achieving up to 300 frames per second (FPS) at a resolution of 1080p. However, all these above-mentioned neural SLAM methods do not perform well in dynamic scenes. The robustness of these systems significantly decreases, even leading to tracking failures, when dynamic objects appear in the environment. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To tackle these problems, we propose a novel 3D Gaussian-based visual SLAM that can reliably track camera motion in dynamic indoor environments. Due to the capability of 3D-GS to accomplish high-quality rendering in real-time, the SLAM system more readily converges to a global optimum during pose optimization, thereby achieving better and more stable pose optimization results. A cornerstone of our approach to achieving robust pose estimation lies in the innovative motion mask generation algorithm. This algorithm fliters out sampled pixels situated within invalid zones, thereby refining the estimation process. In addition to the constraint of depth residual, we employ a spatiotemporal consistency strategy within an observation window to generate depth warp masks. By incrementally fusing the depth warp mask and semantic mask, the motion mask will become more precise to reflect the true motion state of objects. To improve the accuracy and stability of pose estimation, we leverage DROID-SLAM [19] odometry (DROID-VO) to provide an initial pose estimate and devise a coarse-to-fine optimization algorithm built upon the initially estimated camera pose. This aims to minimize the disparity between pose estimation and the reconstructed map, employing photorealistic alignment optimization through Gaussian Splatting. Moreover, this hybrid pose optimization approach effectively ensures the accuracy and quality of the generated depth warp mask, thereby facilitating better performance in the next camera tracking stage. To obtain high-quality rendering results, we propose a novel adaptive Gaussian point addition and pruning method to keep the geometry clean and enable accurate and robust camera tracking. Capitalizing on the factor graph structure inherent in DROID-SLAM, our system is capable of executing dense Bundle Adjustment (DBA) upon completion of tracking to eliminate accumulated errors. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are summarized as follows: (i) To the best of our knowledge, this is the first robust dynamic Gaussian splatting SLAM with hybrid pose optimization, capable of achieving both real-time rendering and high-fidelity reconstruction performance. (ii) To mitigate the impact of dynamic objects during pose estimation, we propose an advanced motion mask generation strategy that integrates spatio-temporal consistent depth masks with semantic priors, thereby significantly enhancing the precision of motion object segmentation. (iii) We design a hybrid camera tracking strategy utilizing the coarse-to-fine pose optimization algorithm to improve the consistency and accuracy between the estimated pose and the reconstructed map. (iv) To better manage and expand the Gaussian map, we propose an adaptive Gaussian point addition and pruning strategy. It ensures geometric integrity and facilitates accurate camera tracking. (v) Extensive evaluations on two challenging dynamic datasets and one common static dataset demonstrate the state-of-the-art performance of our proposed SLAM system, particularly in real-world scenarios. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Visual SLAM with dynamic objects filter. Dynamic object filtering is pivotal for reconstructing static scenes and bolstering the robustness of pose estimation. Existing approaches fall into two main categories: the first relies on re-sampling and residual optimization strategies to remove outliers, as seen in works such as ORB-SLAM2 [2], ORB-SLAM3 [3], and Refusion [5]. These methods, however, are generally limited to addressing small-scale motions and often falter in the face of extensive, continuous object movements. The second group employs the additional prior knowledge, for example, semantic segmentation or object detection prior [20, 21, 22, 23, 24, 25, 26], to remove the dynamic objects. However, all these methods often exhibit a domain gap when applied in realworld environments, leading to the introduction of prediction errors. More recently, deep neural networks have been employed to build end-to-end visual odometry, which performs better in specific environments such as DROID-SLAM [19], DytanVO [27], and DeFlowSLAM [28]. However, these methods still require a substantial amount of training data and are unable to reconstruct the high-fidelity static map. ", "page_idx": 1}, {"type": "text", "text": "RGB-D SLAM with neural implicit representation. Neural implicit scene representations, also known as neural fields [29], have attracted considerable attention in the field of RGB-D SLAM for their impressive expressiveness and low memory footprint. Initial studies, including iMap [6] and DI-Fusion [30], explored the utilization of a single MLP and a feature grid to encode scene geometries within a latent space. However, they both share a critical issue: the problem of network forgetting, which is catastrophic for long-term localization and mapping. In response to this limitation, NICESLAM [7] introduces a hierarchical feature grid approach to enhance scene representation fidelity and implements a localized feature updating strategy to address the issue of network forgetting. While these advancements contribute to improved accuracy, they necessitate greater memory consumption and may impact the system\u2019s ability to operate in real-time. More recently, existing methods like Vox-Fusion [10], Co-SLAM [8], and ESLAM [9] explore sparse encoding or tri-plane representation strategy to improve the quality of scene reconstruction and the system\u2019s execution efficiency. PointSLAM [31] draws inspiration from the concept of Point-NeRF [32], utilizing neural points to encode spatial geometry and color features. It employs an explicit method to represent spatial maps, effectively improving the accuracy of localization and mapping. All these methods have demonstrated impressive results based on the strong assumptions of static scene conditions. The robustness of these systems significantly decreases when dynamic objects appear in the environment. Recently, Rodyn-SLAM [33] proposed utilizing optical flow and semantic segmentation prior to filter out dynamic objects, and employing a neural rendering method as the frontend. However, this approach is computationally intensive and limits the maximum accuracy achievable in pose estimation. ", "page_idx": 1}, {"type": "image", "img_path": "tGozvLTDY3/tmp/3e651929ca09a3012de29964bd06f6a0d8ea84ccbd584d57cee99cf6d65092e0.jpg", "img_caption": ["Figure 1: Overview of DG-SLAM. Given a series of RGB-D frames, we reconstruct the static high-fidelity 3D Gaussian map and optimize the camera pose represented with lie algebra $\\xi_{i}$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3D Gaussian splatting SLAM methods. Compared to the aforementioned NeRF-based SLAM methods, 3D Gaussian splatting (3D-GS) [18] has garnered widespread interest among researchers due to its advantages in high-fidelity and real-time rendering. Unlike previous implicit map representation methods, 3D-GS explicitly models scene maps by independent Gaussian spheres, naturally endowing it with geometric structure property. Some researchers [17, 15, 34, 16, 35] are exploring the replacement of implicit representations (NeRF) with 3D-GS in the mapping thread. However, these methods are currently constrained by the assumption of static environments, rendering them ineffective in dynamic scenes. It significantly restricts the practical application of Gaussian SLAM systems in real-life scenarios. Under the premise of ensuring high-fidelity reconstruction and realtime rendering, our system is designed to improve the accuracy and robustness of pose estimation under dynamic environments. ", "page_idx": 2}, {"type": "text", "text": "3 Approach ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a sequence of RGB-D frames $\\{I_{i},D_{i}\\}_{i=1}^{N},I_{t}\\,\\in\\,\\mathbb{R}^{3},D_{t}\\,\\in\\,\\mathbb{R}$ , our method (Fig. 1) aims to simultaneously recover camera poses $\\{\\xi_{i}\\}_{i=1}^{N},\\xi_{t}\\in\\mathbb{S}\\mathbb{E}(3)$ and reconstruct the static 3D scene map represented by 3D Gaussian sphere in dynamic environments. Similar to most modern SLAM systems [36, 37], our system comprises two distinct processes: the tracking process as the frontend and the mapping process as the backend. ", "page_idx": 2}, {"type": "text", "text": "3.1 3D Gaussian map representation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To obtain real-time rendering and high-fidelity reconstruction mapping, we represent the scene as a set of 3d Gaussian ellipsoid $\\mathcal{G}$ , which simultaneously possesses geometric and appearance properties. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{G}=\\left\\{\\mathcal{G}_{i}:(\\mu_{i},\\Sigma_{i},\\alpha_{i},\\mathbf{h}_{i})\\right|\\forall\\mathcal{G}_{i}\\in\\mathcal{G}\\right\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Each 3D Gaussian ellipsoid $\\mathcal{G}_{i}$ is composed of its center position $\\mu_{i}~\\in~\\mathbb{R}^{3}$ , covariance matrix $\\pmb{\\Sigma}_{i}\\in\\mathbb{R}^{3\\times3}$ , opacity $\\alpha_{i}\\in\\mathbb{R}$ , and spherical harmonics coefficients $\\mathbf{h}_{i}\\in\\mathbb{R}^{16}$ . ", "page_idx": 3}, {"type": "text", "text": "Color and depth splatting rendering. To obtain the rendering image of color and depth, we project the 3D Gaussian $\\left(\\mu_{w},\\Sigma_{w}\\right)$ in world coordinate to 2D Gaussian $(\\mu_{I},\\Sigma_{I})$ on the image plane: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{I}=\\pi\\left(\\pmb{T}_{w}^{c}\\cdot\\mu_{w}\\right),\\quad\\pmb{\\Sigma}_{I}=\\mathbf{J}\\mathbf{R}\\pmb{\\Sigma}_{w}\\mathbf{R}^{T}\\mathbf{J}^{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{R}$ is the viewing transformation and $\\mathbf{J}$ is the Jacobian of the affine approximation of the projective transformation. Following the alpha blending method in 3DGS [18], we accumulate the splatting Gaussian ellipsoid along the observation image pixel at the current estimation pose $\\xi_{i}$ to render the color and depth as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{C}=\\sum_{i\\in\\mathcal{G}}\\mathbf{c}_{i}f_{i}(\\mu_{I},\\Sigma_{I})\\prod_{j=1}^{i-1}\\left(1-f_{j}(\\mu_{I},\\Sigma_{I})\\right),\\quad\\hat{D}=\\sum_{i\\in\\mathcal{G}}\\mathbf{d}_{i}f_{i}(\\mu_{I},\\Sigma_{I})\\prod_{j=1}^{i-1}\\left(1-f_{j}(\\mu_{I},\\Sigma_{I})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $f_{i}(\\cdot)$ is the Gaussian distribution function weighted by opacity $\\alpha_{i}$ . $\\mathbf{c}_{i}$ represents the color of the projected Gaussian point computed by learnable spherical harmonics coefficients $\\mathbf{h}_{i}$ . Similarly, ${\\bf d}_{i}$ denotes the depth of Gaussian point $\\mathcal{G}_{i}$ , obtained by projecting to ${\\bf Z}$ -axis in the camera coordinate. ", "page_idx": 3}, {"type": "text", "text": "Accumulated opacity. We use accumulated opacity $\\hat{O}$ to represent the rendering reliability for each pixel and judge whether the Gaussian map is well-optimized. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{O}=\\sum_{i\\in\\mathcal{G}}f_{i}(\\mu_{I},\\Sigma_{I})\\prod_{j=1}^{i-1}\\left(1-f_{j}(\\mu_{I},\\Sigma_{I})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.2 Motion mask generation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For each input keyframe, we select its associated keyframe set $\\mathcal{D}$ within a sliding window. To reduce the computation and improve the accuracy of generating motion mask, we employ the depth warping operation solely on keyframes. To ensure the overlap between adjacent keyframes is not too small, we employ optical-flow distance to determine keyframe insertion. In regions with more intense motion, our goal is to insert as many keyframes as possible. ", "page_idx": 3}, {"type": "text", "text": "For the pixel $p$ in keyframe $i$ , we reproject it onto keyframe $j$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\pmb{p}}_{i\\rightarrow j}=f_{w a r p}\\left(\\xi_{j i},{\\pmb{p}}_{i},D_{i}({\\pmb{p}}_{i})\\right)={\\pmb K}T_{j i}\\left({\\pmb{K}}^{-1}D_{i}({\\pmb{p}}_{i}){\\pmb{p}}_{i}^{h o m o}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\kappa$ and $T_{j i}$ represent the intrinsic matrix and the transformation matrix between frame $i$ and frame $j$ , respectively. $\\pmb{p}_{i}^{h o m o}=(u,v,1)$ is the homogeneous coordinate of $\\textstyle p_{i}$ . ", "page_idx": 3}, {"type": "text", "text": "Given any associate keyframes $D_{i},D_{j}\\in\\mathcal{D}$ , we utilize warp function $f_{w a r p}$ to compute the residual of reprojection depth value. By setting a suitable threshold $e_{t h}$ , we derive the depth warp mask $\\widehat{\\mathcal{M}}_{j,i}^{w d}$ corresponding to dynamic objects as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathcal{M}}_{j,i}^{w d}:\\left\\{\\bigcap_{p_{i}\\in D_{i}}{\\mathbf1}(D_{j}(p_{i\\rightarrow j})-D_{i}(p_{i})<e_{t h})\\otimes I_{m\\times n}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $I_{m\\times n}$ represents a matrix of the same size as the image, filled with ones. The $\\otimes$ operation signifies that for each element in the matrix $I_{m\\times n}$ , we assess whether its warp depth meets a specified threshold and subsequently modify the corresponding value at that position. As illustrated in Fig. 1, to derive a more precise warp mask, we consider the spatial coherence of object motion within a sliding window of length $N$ and combine the multiple observation warp masks. Unlike ReFusion [5], our method can mitigate the potential impact of observation noise from a single warp mask. When object motion becomes significant, we only mask the foreground pixels where the depth residual is positive to avoid a large portion of pixels being masked as dynamic regions. Subsequently, we integrate the warp mask and semantic mask to derive the final motion mask ${\\widehat{\\mathcal{M}}}_{j}$ as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{M}}_{j}=\\widehat{\\mathcal{M}}_{j,i}^{w d}\\cap\\widehat{\\mathcal{M}}_{j,i-1}^{w d}\\cap\\widehat{\\mathcal{M}}_{j,i-2}^{w d}\\cdot\\cdot\\cdot\\cap\\widehat{\\mathcal{M}}_{j,i-N}^{w d}\\cup\\widehat{\\mathcal{M}}_{j}^{s g},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Thanks to our innovative approach to generating motion masks, the omission of dynamic objects by semantic priors can be effectively compensated. Furthermore, by leveraging a spatial consistency strategy, the inaccuracy of edge region recognition during depth warping can be significantly reduced. ", "page_idx": 3}, {"type": "text", "text": "3.3 Coarse-to-fine camera tracking. ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The constant speed motion model struggles to infer a reasonable initial optimized pose value in dynamic scenes. An inaccurate initial pose will lead to optimization getting trapped in local optima more easily and can affect the generation quality of the depth warp mask. To achieve more precise pose estimation in dynamic environments, we utilize the visual odometry (VO) component from DROID-SLAM [19] as the coarse pose estimation results in camera tracking. We also conduct a dense bundle adjustment in every interaction for a set of keyframes to optimize the corresponding pose $\\mathbf{G}$ and depth $\\mathbf{d}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{E}(\\mathbf{G},\\mathbf{d})=\\sum_{(i,j)\\in\\epsilon}\\left\\|\\mathbf{p}_{i j}^{*}-\\Pi_{C}\\left(G_{i j}\\circ\\Pi_{C}^{-1}\\left(\\mathbf{p}_{i},\\mathbf{d}_{i}\\right)\\right)\\right\\|_{\\Sigma_{i j}\\cdot\\widehat{\\mathcal{M}}_{j}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\Sigma_{i j}\\,=\\,\\mathrm{diag}\\left(w_{i j}\\right)$ , $w_{i j}$ represents the confidence weights. $G_{i j}$ denotes the relative transformation between the poses $G_{i}$ and $G_{j}$ . $\\mathbf{p}_{i}$ means a grid of pixel coordinates. Moreover, $\\mathbf{p}_{i j}^{*}$ is corrected correspondence as predicted by updated optical flow estimation. To overcome the influence of dynamic objects on bundle adjustment, we introduce suppression through the motion mask ${\\widehat{\\mathcal{M}}}_{j}$ applied to the weighted covariance matrix. Consequently, the weighted confidence associated  with dynamic objects is reduced to zero. ", "page_idx": 4}, {"type": "text", "text": "To further improve the accuracy of pose estimation and minimize inconsistencies between the estimated pose and the reconstructed map, we implement fine camera tracking by leveraging Gaussian splatting, based on the initial obtained pose. Due to obstructions by dynamic objects and the inadequate optimization of Gaussian points in the map, the rendered image may exhibit blurring or black floaters. Therefore, we employ the accumulated opacity, as outlined in 4, to denote the pixel rendering reliability. The reliable mask $\\widehat{\\mathcal{O}}_{i}$ for camera tracking is generated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathcal{O}}_{i}:\\left\\{\\bigcap_{[\\mu_{j},v_{j}]\\in I_{i}}{\\mathbf1}(\\hat{O}[\\mu_{j},v_{j}]>\\tau_{t r a c k})\\otimes I_{m\\times n}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mu_{j},v_{j}$ represents the pixel location. When the accumulated opacity at the pixel exceeds a given threshold $\\tau_{t r a c k}$ , we consider the Gaussian point associated with that pixel to be well-optimized. Consequently, using the rendered image at these pixels for pose estimation is deemed reliable. The overall loss function is finally formulated as the following minimization: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\xi_{i}^{*}=\\operatorname*{arg\\,min}_{\\xi_{i}}\\,\\lambda_{1}\\frac{1}{M}\\sum_{i=1}^{M}\\Big\\|\\left(\\hat{C}(\\mathcal{G},\\xi_{i})-C\\right)\\cdot\\widehat{M}_{i}\\cdot\\widehat{O}_{i}\\Big\\|_{2}^{2}+\\lambda_{2}\\frac{1}{N_{d}}\\sum_{\\mathbf{p}\\in N_{d}}\\Big\\|\\left(\\hat{D}(\\mathcal{G},\\xi_{i})-D\\right)\\cdot\\widehat{M}_{i}\\cdot\\hat{O}_{i}\\Big\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Where $\\xi_{i}$ denotes the camera pose requiring optimization. $C$ and $D$ denote the ground truth color and depth map, respectively. $M$ represents the number of sampled pixels in the current image. Note that only pixels with valid depth value $N_{d}$ are considered in optimization. ", "page_idx": 4}, {"type": "text", "text": "This hybrid pose optimization approach effectively ensures the accuracy and quality of the warp mask, thereby facilitating better performance in the next camera tracking stage. In short, this hybrid pose optimization strategy enables us to achieve more precise and robust pose estimation whether in dynamic or static environments. ", "page_idx": 4}, {"type": "text", "text": "3.4 SLAM system ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Map initialization. For the first frame, we do not conduct the tracking step and the camera pose is set to the identity matrix. To better initialization, we reduce the gradient-based dynamic radius to half so that can add more Gaussian points. For pixels located outside the motion mask, we sample and reproject them to the world coordinates. We initialize the Gaussian point color and center position $\\mu_{i}$ with the RGB value and reprojection coordinate of the pixel, respectively. The opacity $\\alpha_{i}$ is set as 0.1 and the scale vector $\\mathbf{S}_{i}$ is initialized based on the mean distance of the three nearest neighbor points. The rotation matrix $\\mathbf{R}_{i}$ is initialized as the identity matrix. ", "page_idx": 4}, {"type": "text", "text": "Map optimization. To optimize the scene Gaussian representation, we render depth and color in independent view as seen Eq. 3, comparing with the ground truth map: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{r g b}=\\frac{1}{M}\\sum_{i=1}^{M}\\Big\\|\\left(\\hat{C}-C\\right)\\cdot\\widehat{\\mathcal{M}}_{i}\\Big\\|_{2}^{2},\\quad\\mathcal{L}_{d e p t h}=\\frac{1}{N_{d}}\\sum_{\\mathbf{p}\\in N_{d}}\\Big\\|\\left(\\hat{D}-D\\right)\\cdot\\widehat{\\mathcal{M}}_{i}\\Big\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In contrast to existing methods, we introduce the motion mask ${\\widehat{\\mathcal{M}}}_{i}$ to remove sampled pixels within the dynamic region effectively. The final Gaussian map optimization is performed by minimizing the ", "page_idx": 4}, {"type": "image", "img_path": "tGozvLTDY3/tmp/d631081afbe876adec15e018c0246e007559d74273a86a242d12276e1e38ee37.jpg", "img_caption": ["Figure 2: Qualitative results of the motion mask generation. By fusing the semantic mask and depth warp mask, the final mask will be more precise. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "geometric depth loss and photometric color loss : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{m a p p i n g}=\\lambda_{1}\\mathcal{L}_{r g b}+\\lambda_{2}\\mathcal{L}_{s s i m}+\\lambda_{3}\\mathcal{L}_{d e p t h},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{L}_{s s i m}$ denotes the structural similarity loss between two images. Moreover, $\\lambda_{1},\\lambda_{2},\\lambda_{3}$ are weight factors for balance in the optimization process. ", "page_idx": 5}, {"type": "text", "text": "Adapative Gaussian point adding strategy. To guarantee the superior scene representation capability of 3D Gaussian, we also employ dynamic Gaussian point density, which is inspired by PointSLAM [31]. The insertion radius is determined based on color gradient, allowing for a denser allocation of points in areas with high texture while minimizing point addition in regions of low texture. This method efficiently manages memory consumption by adapting the point density according to the textural complexity of the scene. ", "page_idx": 5}, {"type": "text", "text": "To ensure the points we add are both necessary and effective, we adopt a two-stage adaptive point-add strategy. For new viewpoints without previous observation, we initially perform uniform sampling across the entire image to ensure that new observed areas can be covered by Gaussian points. Moreover, if the accumulated opacity $\\hat{O}$ , calculated by Eq. 4, falls below the threshold $o_{t h}$ , or the depth residual between the rendered pixels and the ground truth depth is excessively large, we then add 3D Gaussian points based on these under-fitting pixels. At last, these new Gaussian points will be initialized based on the point density. ", "page_idx": 5}, {"type": "text", "text": "Map point deleting strategy. Given that the added 3D Gaussian points have not been subjected to geometric consistency verification and may exhibit unreasonable representation values during optimization, this could lead to the generation of a low-quality dense map or the introduction of numerous artifacts in the rendering image. we implement the pruning operation as part of the Gaussian map optimization. To ensure the points we delete are both reasonable and accurate, we also adopt a two-stage point delete strategy. For all Gaussian points observed from the current viewpoint, we delete points based on three criteria: the opacity value, the maximum scale, and the ratio between the ellipsoid\u2019s major and minor axes, described as follow: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha_{i}<\\tau_{\\alpha}\\quad\\mathrm{or}\\quad\\operatorname*{max}(\\mathbf{S})>\\tau_{s1}\\quad\\mathrm{or}\\quad\\frac{\\operatorname*{max}(\\mathbf{S})}{\\operatorname*{min}(\\mathbf{S})}>\\tau_{s2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Moreover, due to potential inaccuracies along the edges of the current motion mask, adding these points could result in artifacts within the scene map. Thus, we project these points on keyframes in a small sliding window to recheck whether they can be observed by these keyframes. If the number of observations of a point is too low, we consider its addition to be insufficiently robust, and therefore, it will be removed from the current Gaussian map. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. Our methodology is evaluated using three publicly available challenging datasets: TUM RGB-D dataset [38], BONN RGB-D Dynamic dataset [5] and ScanNet [39]. These three datasets contain both challenging dynamic scenes and real static scenes. This selection facilitates a comprehensive assessment of our approach under varied conditions, demonstrating its applicability and robustness in real-world indoor environments. ", "page_idx": 5}, {"type": "image", "img_path": "tGozvLTDY3/tmp/497e02ad6eb81d471265650df89c999fe55fa8c6f8df7b714d545bc9fefada55.jpg", "img_caption": ["Figure 3: Visual comparison of the rendering image on the TUM and BONN datasets. Our results are more complete and accurate without the dynamic object floaters. "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "tGozvLTDY3/tmp/6e1f7e63a34971ff1c2f27085ca753739077ee0df72b5896da232c22c3ab36cc.jpg", "table_caption": ["Table 1: Reconstruction results on several dynamic scene sequences in the BONN dataset. Instances of tracking failures are denoted by \u201cX\". The most superior outcomes within the domain of RGB-D SLAMs are highlighted in bold for emphasis. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Metrics. For the evaluation of pose estimation, we utilize the Root Mean Square Error (RMSE) and Standard Deviation (STD) of Absolute Trajectory Error (ATE) [38]. Prior to assessment, the estimated trajectory is aligned with the ground truth trajectory via Horn\u2019s Procrustes method. [40], ensuring a coherent basis for evaluation. To evaluate the reconstruction quality of static maps in dynamic scenes, we employ three metrics(i) Accuracy (cm), (ii) Completion (cm), and (iii) Completion Ratio (percentage of points within a 5cm threshold), following NICE-SLAM [7]. Since the BONN dataset provides only the ground truth point cloud, we randomly sampled 200,000 points from the GT point cloud and the reconstructed mesh surface to calculate these metrics. ", "page_idx": 6}, {"type": "text", "text": "Implementation details. We run our DG-SLAM on an RTX 3090 Ti GPU at 2 FPS on BONN datasets, which takes roughly 9GB of memory. We set the loss weight $\\lambda_{1}=0.9$ , $\\lambda_{2}=0.2$ and $\\lambda_{3}\\,=\\,0.1$ to train our model. The number of iterations for the tracking and mapping processes has been set to 20 and 40, respectively. For the Gaussian points deleting, we set $\\tau_{\\alpha}~=~0.005$ , $\\tau_{S1}=0.4$ and $\\tau_{S2}=36$ to avoid the generation of abnormal Gaussian points. What\u2019s more, we utilize Oneformer [41] to generate prior semantic segmentation. For the depth wrap mask, we set the window size to 4 and the depth threshold to 0.6. We also adopt the keyframe selection strategy from DROID-VO [19] based on optical flow. ", "page_idx": 6}, {"type": "table", "img_path": "tGozvLTDY3/tmp/c6b458e7700b9155fdbc3174e528881bb2ffe9d680683ae906c6109d71d50656.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "tGozvLTDY3/tmp/2017997b6b9dfb7d837c0610c667a2ec1baf2d11bade8cd159a7439d4997b816.jpg", "table_caption": ["Table 2: Camera tracking results on several dynamic scene sequences in the TUM dataset. $\"*\"$ denotes the version reproduced by NICE-SLAM. \u201cX\" and \u201c-\" denote the tracking failures and absence of mention, respectively. The metric is Absolute Trajectory Error (ATE) and the unit is [cm]. ", "Table 3: Camera tracking results on several dynamic scene sequences in the BONN dataset. \u201c\u2217\" denotes the version reproduced by NICE-SLAM. \u201cX\" denotes the tracking failures. The metric is ATE and the unit is [cm]. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.1 Evaluation of generating motion mask ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluated our method on the balloon and move_no_box2 sequences of the BONN dataset to show the qualitative results of the generated motion mask. In these two sequences, in addition to the typical motion of pedestrians, there are movements of atypical objects accompanying the human, such as balloons and boxes. It might be missed if we rely solely on the semantic prior. As shown in Fig. 2, our generated methods notably improve the precision of motion mask segmentation, effectively reducing the inconsistencies along edge regions and detecting the true dynamic objects. ", "page_idx": 7}, {"type": "text", "text": "4.2 Evaluation of mapping performance ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To more effectively showcase the performance of our system in dynamic environments, we evaluate the reconstruction results from both qualitative and quantitative perspectives. Given dynamic scene datasets seldom offer static GT mesh or point cloud, we utilize the BONN dataset for our quantitative analytical experiments. We compare our DG-SLAM method against current state-of-the-art neuralbased SLAM methods, all of which are available as open-source projects. ", "page_idx": 7}, {"type": "text", "text": "As shown in Tab. 1, our method significantly surpasses contemporary approaches in terms of accuracy, completion, and completion ratio metrics, achieving state-of-the-art performance. Meanwhile, the reconstructed static map can be rendered with high fidelity, as shown in Fig. 3. This also indirectly demonstrates that our methods can generate a more accurate static map compared with other mainstream SLAM systems. ", "page_idx": 8}, {"type": "text", "text": "4.3 Evaluation of tracking performance ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To comprehensively assess the tracking performance of our DG-SLAM approach, we conduct comparative analyses within highly dynamic, slightly dynamic, and static environments. The comparison methods encompass classical SLAM methods such as ORB-SLAM3 [3], ReFusion [5],MIDfusion [43], and EM-fusion [44], and widely recognized NeRF-based SLAM systems such as NICE-SLAM [7], iMap [6], Vox-Fusion [10], ESLAM [9], Co-SLAM [8]. We also incorporate a comparison with the newly proposed dynamic neural RGB-D SLAM system, Rodyn-SLAM [33]. Furthermore, our evaluation extends to the latest advancements in 3D Gaussian-based SLAM, including SplaTAM [16] and GS-SLAM [17]. ", "page_idx": 8}, {"type": "text", "text": "Dynamic scenes. As shown in Tab. 2, we report the results on three highly dynamic sequences, two slightly dynamic sequences, and one static sequence from the TUM RGB-D dataset. Our system exhibits exceptional tracking performance, attributed to the implementation of the map point deleting strategy and the powerful coarse-to-fine camera tracking algorithm. Furthermore, the tracking capabilities of our system have also been rigorously evaluated on the intricate and demanding BONN RGB-D dataset, with outcomes presented in Tab. 3. In dynamic scenarios characterized by heightened complexity and challenge, our method has consistently demonstrated superior performance, underscoring its effectiveness and reliability in real-world navigation applications. Meanwhile, we also showcased the number of iterations of the tracking and mapping process, taking TUM as an example, as shown in Tab. 5. Compared to other methods, our method achieved superior results while maintaining competitive iterations and efficiency. ", "page_idx": 8}, {"type": "text", "text": "Static scenes. To better illustrate the robustness of our system, we also evaluate our methods with existing SLAM on common real-world static sequences from ScanNet [39]. As shown in Tab. 4, our DG-SLAM still achieves competitive performance within static scenes with fewer tracking and mapping iterations, despite our method being designed for dynamic scenes. Notably, the motion mask will become irrelevant in static scenes. Thus, it can sufficiently demonstrate the effectiveness of our proposed hybrid camera tracking strategy and adaptive Gaussian point management strategy. ", "page_idx": 8}, {"type": "table", "img_path": "tGozvLTDY3/tmp/ee2e19474eef9ceaa24348f815c3cac4503771ffdf5d8673672e48769829856c.jpg", "table_caption": [], "table_footnote": ["Table 4: Camera tracking results on ScanNet. The metric is ATE and the unit is [cm]. "], "page_idx": 8}, {"type": "table", "img_path": "tGozvLTDY3/tmp/1ca1c591a48e7d814fb79880fc692c0b3d352b74082578ac8c79b1afe190caf6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4 Ablation study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To evaluate the efficacy of the proposed algorithm in our system, we conducted ablation studies across seven representative sequences from the BONN dataset. Given that the semantic priors within the TUM dataset have covered major motion categories (humans) while motion categories in BONN contained undefined dynamic objects such as balloons and boxes, thus we opted to perform ablation studies on the BONN dataset. We calculated the average ATE and STD metrics to illustrate the impact of various components on the overall system performance. As the results presented in Tab. 6, the findings affirm the effectiveness of all proposed methods in improving camera tracking capabilities. Specifically, the strategies for adding and pruning points are equally crucial, significantly impacting the quality of the Gaussian map reconstruction and, in turn, affecting the tracking performance. The depth warp operation effectively removes floaters from multiple viewpoints, thereby noticeably enhancing the quality of the rendered images. One of the main contributions comes from the hybrid camera tracking strategy, which indirectly underscores the importance of eliminating inconsistencies between pose estimation and map reconstruction. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "4.5 Time consumption analysis ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "As shown in Tab. 7, we report time consumption (per frame) of the tracking and mapping without computing semantic segmentation. These results were achieved through an identical experimental setup that involved conducting 20 iterations for tracking and 40 iterations for mapping, all processed on an RTX 3090Ti GPU. Beneftiing from the rapid execution speed of Droid-VO and fast rendering of 3D Gaussian Splatting, our method exhibits a leading edge in terms of time consumption during the tracking process. Although our approach does not match the mapping speed of Co-SLAM, it achieves high-quality mapping and high-fidelity rendering with a competitive mapping running time. We also evaluate the inference time of our used semantic segmentation network, which required $163\\mathrm{ms}$ for every frame. It should be noted that our approach does not focus on the specific semantic segmentation network used, but rather on the fusion method itself. ", "page_idx": 9}, {"type": "table", "img_path": "tGozvLTDY3/tmp/6883740cd53748ff0c83d5fc15c3a5679b9b0d5c0b778018941d1062cd229abf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "tGozvLTDY3/tmp/cacc9dc6e0b7d85d9cd7ae982ceadacd4debae4c3c5e6b944e6194ea5aacf404.jpg", "table_caption": [], "table_footnote": ["Table 7: Run-time comparison on TUM f3/w_s. "], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have presented DG-SLAM, a robust dynamic Gaussian splatting SLAM with hybrid pose optimization under dynamic environments. Via motion mask filter strategy and coarse-to-fine camera tracking algorithm, our system significantly advances the accuracy and robustness of pose estimation within dynamic scenes. The proposed adaptive 3D Gaussians adding and pruning strategy effectively improves the quality of reconstructed maps and rendering images. We demonstrate its effectiveness in achieving state-of-the-art results in camera pose estimation, scene reconstruction, and novel-view synthesis in dynamic environments. While the tracking and reconstruction of largescale scenes is currently the biggest limitation of our system, we believe it will be addressed by a more flexible loop-closure optimization algorithm in future work. Moreover, the accuracy of pose estimation of our system is still influenced by the segmentation precision of the semantic prior. Therefore, efficiently perceiving moving objects within the dynamic scenes remains an unresolved issue that warrants further exploration. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by National Key R&D Program of China (No.2019YFA0709502, No.2018YFC1312904), National Natural Science Foundation of China (Grant No.62106050 and 62376060), Natural Science Foundation of Shanghai (Grant No.22ZR1407500), ZJ Lab, Shanghai Center for Brain Science and Brain-Inspired Technology, and NIO University Programme (NIO UP). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] John J Leonard and Hugh F Durrant-Whyte. Simultaneous map building and localization for an autonomous mobile robot. In IROS, 1991. 1   \n[2] Raul Mur-Artal and Juan D Tard\u00f3s. Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras. IEEE TRO, 2017. 1, 2   \n[3] Carlos Campos, Richard Elvira, Juan J G\u00f3mez Rodr\u00edguez, Jos\u00e9 MM Montiel, and Juan D Tard\u00f3s. Orb-slam3: An accurate open-source library for visual, visual\u2013inertial, and multimap slam. IEEE TRO, 2021. 1, 2, 8, 9   \n[4] Haochen Jiang, Rui Qian, Liang Du, Jian Pu, and Jianfeng Feng. Ul-slam: A universal monocular line-based slam via unifying structural and non-structural constraints. IEEE TASE, 2024. 1   \n[5] Emanuele Palazzolo, Jens Behley, Philipp Lottes, Philippe Giguere, and Cyrill Stachniss. Refusion: 3d reconstruction in dynamic environments for rgb-d cameras exploiting residuals. In IROS, 2019. 1, 2, 4, 6, 8, 9   \n[6] Edgar Sucar, Shikun Liu, Joseph Ortiz, and Andrew Davison. iMAP: Implicit mapping and positioning in real-time. In ICCV, 2021. 1, 3, 8, 9   \n[7] Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hujun Bao, Zhaopeng Cui, Martin R. Oswald, and Marc Pollefeys. Nice-slam: Neural implicit scalable encoding for slam. In CVPR, 2022. 1, 3, 7, 8, 9, 10   \n[8] Hengyi Wang, Jingwen Wang, and Lourdes Agapito. Co-slam: Joint coordinate and sparse parametric encodings for neural real-time slam. In CVPR, 2023. 1, 3, 7, 8, 9, 10   \n[9] Mohammad Mahdi Johari, Camilla Carta, and Fran\u00e7ois Fleuret. Eslam: Efficient dense slam system based on hybrid representation of signed distance fields. In CVPR, 2023. 1, 3, 7, 8, 9, 10   \n[10] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. Voxfusion: Dense tracking and mapping with voxel-based neural implicit representation. In ISMAR, 2022. 1, 3, 8, 9   \n[11] Richard A Newcombe, Shahram Izadi, Otmar Hilliges, David Molyneaux, David Kim, Andrew J Davison, Pushmeet Kohi, Jamie Shotton, Steve Hodges, and Andrew Fitzgibbon. Kinectfusion: Real-time dense surface mapping and tracking. In ISMAR, 2011. 1   \n[12] Thomas Sch\u00f6ps, Torsten Sattler, and Marc Pollefeys. Surfelmeshing: Online surfel-based mesh reconstruction. IEEE TPAMI, 2019. 1   \n[13] Thomas Schops, Torsten Sattler, and Marc Pollefeys. Bad slam: Bundle adjusted direct rgb-d slam. In CVPR, 2019. 1   \n[14] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Commun Acm, 2021. 1   \n[15] Chi Yan, Delin Qu, Dong Wang, Dan Xu, Zhigang Wang, Bin Zhao, and Xuelong Li. Gs-slam: Dense visual slam with 3d gaussian splatting. In CVPR, 2024. 2, 3   \n[16] Nikhil Keetha, Jay Karhade, Krishna Murthy Jatavallabhula, Gengshan Yang, Sebastian Scherer, Deva Ramanan, and Jonathon Luiten. Splatam: Splat, track & map 3d gaussians for dense rgb-d slam. In CVPR, 2024. 2, 3, 7, 8, 9   \n[17] Hidenobu Matsuki, Riku Murai, Paul H. J. Kelly, and Andrew J. Davison. Gaussian Splatting SLAM. In CVPR, 2024. 2, 3, 7, 8, 9   \n[18] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM TOG, 2023. 2, 3, 4   \n[19] Zachary Teed and Jia Deng. DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras. In NeurIPS, 2021. 2, 5, 7, 8   \n[20] Chao Yu, Zuxin Liu, Xin-Jun Liu, Fugui Xie, Yi Yang, Qi Wei, and Qiao Fei. Ds-slam: A semantic visual slam towards dynamic environments. In IROS, 2018. 2   \n[21] Berta Bescos, Jos\u00e9 M F\u00e1cil, Javier Civera, and Jos\u00e9 Neira. Dynaslam: Tracking, mapping, and inpainting in dynamic scenes. IEEE RAL, 2018. 2   \n[22] Linhui Xiao, Jinge Wang, Xiaosong Qiu, Zheng Rong, and Xudong Zou. Dynamic-slam: Semantic monocular visual localization and mapping based on deep learning in dynamic environment. IEEE RAS, 2019. 2   \n[23] Berta Bescos, Carlos Campos, Juan D Tard\u00f3s, and Jos\u00e9 Neira. Dynaslam ii: Tightly-coupled multi-object tracking and slam. IEEE RAL, 2021. 2   \n[24] Jun Zhang, Mina Henein, Robert Mahony, and Viorela Ila. Vdo-slam: a visual dynamic object-aware slam system. arXiv preprint, 2020. 2   \n[25] Jo\u00e3o Carlos Virgolino Soares, Marcelo Gattass, and Marco Antonio Meggiolaro. Crowd-slam: visual slam towards crowded environments using object detection. Journal of Intelligent & Robotic Systems, 2021. 2   \n[26] Mingrui Li, Jiaming He, Guangan Jiang, and Hongyu Wang. Ddn-slam: Real-time dense dynamic neural implicit slam with joint semantic encoding. arXiv preprin, 2024. 2   \n[27] Shihao Shen, Yilin Cai, Wenshan Wang, and Sebastian Scherer. Dytanvo: Joint refinement of visual odometry and motion segmentation in dynamic environments. In ICRA, 2023. 2   \n[28] Weicai Ye, Xingyuan Yu, Xinyue Lan, Yuhang Ming, Jinyu Li, Hujun Bao, Zhaopeng Cui, and Guofeng Zhang. Deflowslam: Self-supervised scene motion decomposition for dynamic dense slam. arXiv preprint, 2022. 2   \n[29] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Commun ACM, 2021. 2   \n[30] Jiahui Huang, Shi-Sheng Huang, Haoxuan Song, and Shi-Min Hu. Di-fusion: Online implicit 3d reconstruction with deep priors. In CVPR, 2021. 3   \n[31] Erik Sandstr\u00f6m, Yue Li, Luc Van Gool, and Martin R. Oswald. Point-slam: Dense neural point cloud-based slam. In ICCV, 2023. 3, 6, 9, 10   \n[32] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-based neural radiance fields. In CVPR, 2022. 3   \n[33] Haochen Jiang, Yueming Xu, Kejie Li, Jianfeng Feng, and Li Zhang. Rodyn-slam: Robust dynamic dense rgb-d slam with neural radiance fields. RAL, 2024. 3, 8, 9   \n[34] Vladimir Yugay, Yue Li, Theo Gevers, and Martin R Oswald. Gaussian-slam: Photo-realistic dense slam with gaussian splatting. arXiv preprint, 2023. 3   \n[35] Tianchen Deng, Yaohui Chen, Leyan Zhang, Jianfei Yang, Shenghai Yuan, Danwei Wang, and Weidong Chen. Compact 3d gaussian splatting for dense visual slam. arXiv preprint, 2024. 3   \n[36] Georg Klein and David Murray. Parallel tracking and mapping for small ar workspaces. In ISMAR, 2007. 3   \n[37] Richard A. Newcombe, Steven J. Lovegrove, and Andrew J. Davison. Dtam: Dense tracking and mapping in real-time. In ICCV, 2011. 3   \n[38] J\u00fcrgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. A benchmark for the evaluation of rgb-d slam systems. In IROS, 2012. 6, 7   \n[39] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, 2017. 6, 9   \n[40] Berthold KP Horn. Closed-form solution of absolute orientation using unit quaternions. Josa a, 1987. 7   \n[41] Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, and Humphrey Shi. OneFormer: One Transformer to Rule Universal Image Segmentation. In CVPR, 2023. 7   \n[42] Martin R\u00fcnz and Lourdes Agapito. Co-fusion: Real-time segmentation, tracking and fusion of multiple objects. In ICRA, 2017. 8   \n[43] Binbin Xu, Wenbin Li, Dimos Tzoumanikas, Michael Bloesch, Andrew Davison, and Stefan Leutenegger. Mid-fusion: Octree-based object-level multi-instance dynamic slam. In ICRA, 2019. 8, 9   \n[44] Michael Strecke and Jorg Stuckler. Em-fusion: Dynamic object-level slam with probabilistic data association. In ICCV, 2019. 8, 9 ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: We have clearly stated the contribution and scope of the paper in the abstract. Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Justification: We have discussed the limitations of the work in the conclusion part ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 13}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: We have provided the theoretical assumption and proof of this paper in the method part. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 14}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: We have disclosed all the information needed to reproduce the main experimental results of the paper in the experiment part. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 14}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 15}, {"type": "text", "text": "Answer: [No] ", "page_idx": 15}, {"type": "text", "text": "Justification: We have not released the code of this paper in the submission. Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 15}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: We have specified all the training and test details of this paper in the experiment part. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 15}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: We have reported appropriate information about the statistical significance of the experiments in the experiment part. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We have provided sufficient information on the computer resources of this method in the experiment part. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 16}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We have conducted research under the NeurIPS Code Of Ethics. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 16}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We have discussed the societal impacts of this paper in the introduction and method parts. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 17}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: This paper has not released data or models that have a high risk for misuse. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 17}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: All the assets used in the paper are properly licensed. Our paper cites the original paper that produced the code package or dataset. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: Our paper does not release new assets, including data and models. We also did not publish or submit the code. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 18}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: The research topic of this paper is robotic SLAM, not involving crowdsourcing nor research with human subjects. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 18}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: The research topic of this paper is robotic SLAM, not involving crowdsourcing nor research with human subjects. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 19}]