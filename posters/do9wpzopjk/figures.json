[{"figure_path": "DO9wPZOPjk/figures/figures_2_1.jpg", "caption": "Figure 1: Our method against notable BNNS on CIFAR10 using VGG-SMALL. The energy is analytically evaluated considering a hypothetical V100 equivalence with native 1-bit support; cf \u00a7 4 for details.", "description": "The figure compares the performance of the proposed BOLD method against other state-of-the-art binarized neural networks (BNNs) on the CIFAR10 dataset using the VGG-SMALL architecture.  It shows that BOLD achieves comparable accuracy while significantly reducing energy consumption during both training and inference, demonstrating its superior efficiency compared to existing BNN methods. The energy consumption is calculated analytically based on the hypothetical V100 architecture with native 1-bit support, with further details provided in section 4 of the paper.", "section": "4 Experiments"}, {"figure_path": "DO9wPZOPjk/figures/figures_5_1.jpg", "caption": "Figure 2: Illustration of backpropagation signals with a Boolean linear layer. Notice that the subsequent layer can be any FP/Boolean layers or activation functions.", "description": "This figure illustrates the backpropagation process in a network containing a Boolean linear layer.  It shows how signals flow between layers. The key is that the signals (\u03b4Loss/\u03b4x<sup>l+1</sup><sub>k,j</sub> and \u03b4Loss/\u03b4x<sup>l</sup><sub>k,i</sub>) can be either real-valued or Boolean, depending on the nature of the subsequent (l+1) layer.  The Boolean linear layer (layer l) uses Boolean weights (w<sup>l</sup><sub>i,j</sub> \u2208 B) and Boolean inputs (x<sup>l</sup><sub>k,i</sub> \u2208 B) to process the data. The figure highlights that the framework is flexible to handle different layer types and data types.", "section": "3.3 BackPropagation"}, {"figure_path": "DO9wPZOPjk/figures/figures_9_1.jpg", "caption": "Figure 3: An example of CITYSCAPES.", "description": "This figure shows a qualitative comparison of semantic segmentation results on the CITYSCAPES dataset.  The top row displays the input image and the ground truth segmentation mask. The bottom row shows the segmentation results from a full-precision model and the BOLD (Boolean Logic Deep Learning) model. The colors in the segmentation masks represent different classes in the CITYSCAPES dataset (e.g., road, building, person, vehicle).  The figure is intended to visually demonstrate the performance of the BOLD model compared to a full-precision baseline model. Note that, although the BOLD model uses significantly less precision, the qualitative results seem very close to the results from the full-precision model.", "section": "4 Experiments"}, {"figure_path": "DO9wPZOPjk/figures/figures_25_1.jpg", "caption": "Figure 1: Our method against notable BNNS on CIFAR10 using VGG-SMALL. The energy is analytically evaluated considering a hypothetical V100 equivalence with native 1-bit support; cf \u00a7 4 for details.", "description": "This figure compares the performance of the proposed BOLD method against other state-of-the-art binarized neural networks (BNNs) on the CIFAR-10 dataset using the VGG-Small architecture.  It highlights the significant energy savings achieved by BOLD while maintaining competitive accuracy. The energy consumption is analytically estimated, considering the hypothetical use of a V100 GPU with native 1-bit support.  Further details are provided in Section 4 of the paper.", "section": "Experiments"}, {"figure_path": "DO9wPZOPjk/figures/figures_27_1.jpg", "caption": "Figure 1: Our method against notable BNNS on CIFAR10 using VGG-SMALL. The energy is analytically evaluated considering a hypothetical V100 equivalence with native 1-bit support; cf \u00a7 4 for details.", "description": "This figure compares the performance of the proposed BOLD method against other state-of-the-art binarized neural networks (BNNs) on the CIFAR-10 dataset using the VGG-SMALL architecture.  It highlights the significant reduction in energy consumption achieved by BOLD during both training and inference while maintaining comparable accuracy to other methods.  The energy values are analytically estimated based on a hypothetical NVIDIA V100 GPU with native 1-bit support, and further details are available in section 4 of the paper.", "section": "Experiments"}, {"figure_path": "DO9wPZOPjk/figures/figures_28_1.jpg", "caption": "Figure 4: Empirical ratio of the mean to standard deviation of the backpropagation signal, experimented with CNN composed of BoolConv - BoolConv - BoolDense - RealDense layers and MNIST dataset.", "description": "This figure shows the empirical ratio of the mean to standard deviation of the backpropagation signal.  The experiment was conducted using a CNN composed of BoolConv, BoolConv, BoolDense, and RealDense layers with the MNIST dataset. The results are presented as a bar chart, showing the ratio for each layer type.", "section": "4 Experiments"}, {"figure_path": "DO9wPZOPjk/figures/figures_29_1.jpg", "caption": "Figure 5: Expected value of tanh derivative with integer values as input, for several output sizes m.", "description": "The figure shows the expected value of the derivative of the hyperbolic tangent function (tanh) with respect to its input, where the input is an integer. The plot shows how this expected value changes as the output size (m) of the layer varies. The expected value of the tanh derivative decreases as m increases, indicating a decreasing impact of the activation function's non-linearity as the layer size grows. This observation is important for understanding the backpropagation behavior in Boolean Logic Deep Learning (BOLD), where the activation function is a threshold function and this effect is considered in the scaling of the backpropagation signals.", "section": "C Training Regularization"}, {"figure_path": "DO9wPZOPjk/figures/figures_32_1.jpg", "caption": "Figure 6: Preliminary designs for the baseline architecture and the Boolean basic blocks. The dashed and red-shaded operations in the Boolean block II are introduced for downsampling blocks. For the convolution after reshaping, than the corresponding operation in Block I. This is exacerbated in upper layer convolutions, where the feature maps are deeper. Therefore, it makes sense to use Block I, as it is lighter and less prone to overfitting when the network capacity is expanded.", "description": "This figure shows two different designs for Boolean blocks in a neural network architecture. Block I is a simpler design that uses mostly Boolean operations, while Block II uses a combination of Boolean and real-valued operations. Block II also has additional shortcut and concatenation operations.  The figure also shows the layout of the standard RESNET18 architecture for comparison.", "section": "D.1.3 Ablation Study on Image Classification"}, {"figure_path": "DO9wPZOPjk/figures/figures_33_1.jpg", "caption": "Figure 7: Comparative graph of popular BNN techniques and our Boolean module. Notice how multiple full-precision operations like BN, PReLU, or Squeeze-and-Excitation are overly used on each BNN block.", "description": "This figure compares four different types of neural network blocks, including a Boolean block without batch normalization (BN), a BNEXT sub-block [36], a REACTNET [68] block, and a BINEAL-NET [77] block.  It highlights that the Boolean block uses only binary operations, while the others employ a mix of binary and full-precision (FP) operations, particularly Batch Normalization, PReLU activation function, and Squeeze-and-Excitation modules. The figure emphasizes the greater efficiency and simplicity of the Boolean block due to its reduced reliance on full-precision operations.  Different dataflows are also shown using arrows (binary, integer, real).", "section": "D.1.5 Basic Blocks SOTA BNNS for Classification"}, {"figure_path": "DO9wPZOPjk/figures/figures_34_1.jpg", "caption": "Figure 8: Small EDSR for single scale \u00d72 super-resolution and our Boolean version with Boolean residual blocks. In both architectures the channels dimensions are \u03ba = 256 and the shaded blocks are repeated 8x.", "description": "This figure compares the architecture of a standard EDSR model for super-resolution with the proposed Boolean EDSR model.  The standard EDSR uses convolutional layers with ReLU activation functions, while the Boolean EDSR replaces these with Boolean convolution layers and a Boolean activation function.  Both models use the same basic structure, including residual blocks and a final pixel shuffle layer, illustrating how the Boolean methods can be applied to existing network architectures.", "section": "D.2 Image Super-resolution"}, {"figure_path": "DO9wPZOPjk/figures/figures_34_2.jpg", "caption": "Figure 9: Ground-truth high resolution images and the output of our Boolean super-resolution methodology. First row: image \"013\" from BSD100, with PSNR: 35.54 dB. Second row: image \"014\" from Set14, with PSNR: 33.92 dB.", "description": "This figure shows the results of applying the proposed Boolean super-resolution method to two images: one from the BSD100 dataset (\"013\") and one from the Set14 dataset (\"014\").  The ground truth high-resolution images are shown next to their corresponding outputs from the Boolean super-resolution method.  The Peak Signal-to-Noise Ratio (PSNR) values are provided as a quantitative measure of the method's performance.", "section": "D.2 Image Super-resolution"}, {"figure_path": "DO9wPZOPjk/figures/figures_35_1.jpg", "caption": "Figure 10: Ground-truth high resolution target image (top) and the output of our Boolean super-resolution methodology (bottom). Image \"0810\" from the validation set of DIV2K, with PSNR: 34.90 dB", "description": "This figure compares a ground-truth high-resolution image with the output of the Boolean super-resolution model. The model is able to generate a visually similar image to the ground truth, with a PSNR of 34.90 dB. The enlarged crops allow for a detailed comparison of textures and details.", "section": "D.2 Image Super-resolution"}, {"figure_path": "DO9wPZOPjk/figures/figures_36_1.jpg", "caption": "Figure 11: Boolean segmentation architecture.", "description": "This figure illustrates the architecture of the Boolean semantic segmentation model. It shows the flow of data through different layers and operations, starting from input features and ending with the output segmentation map. The model utilizes a combination of Boolean convolutional layers, a multi-scale feature extraction block using dilated convolutions (ASPP), and a final classifier layer.", "section": "D.3 Semantic Segmentation"}, {"figure_path": "DO9wPZOPjk/figures/figures_36_2.jpg", "caption": "Figure 12: Boolean Atrous Spatial Pyramid Pooling (BOOL-ASPP) architecture. (a) 1 \u00d7 1 Conv branch. (b) 3 \u00d7 3 dilated Conv branch with dilation rate of d. (c) Naive global average pooling branch. (d) Global average pooling branch.", "description": "This figure shows the architecture of the Boolean Atrous Spatial Pyramid Pooling (BOOL-ASPP) module used in the semantic segmentation task.  It highlights the four main branches: a 1x1 convolution branch, three 3x3 dilated convolution branches with varying dilation rates, a naive global average pooling branch, and a modified global average pooling branch. The figure illustrates how these branches process Boolean inputs (X \u2208 B) and eventually produce a final output (X' \u2208 Z).  The use of Boolean convolutions (BConv) and Boolean activation functions (Sign(X)) are clearly marked, showing a core aspect of the BOLD approach. The modified GAP branch emphasizes a key difference between the BOOL-ASPP and the standard ASPP, showcasing the unique handling of inputs within BOLD.", "section": "D.3 Semantic Segmentation"}, {"figure_path": "DO9wPZOPjk/figures/figures_37_1.jpg", "caption": "Figure 1: Our method against notable BNNS on CIFAR10 using VGG-SMALL. The energy is analytically evaluated considering a hypothetical V100 equivalence with native 1-bit support; cf \u00a7 4 for details.", "description": "This figure compares the performance of the proposed BOLD method against other state-of-the-art binarized neural networks (BNNs) on the CIFAR-10 dataset using the VGG-SMALL architecture.  The key metric shown is the energy consumption relative to the full-precision baseline.  BOLD demonstrates significantly lower energy consumption while maintaining comparable accuracy.", "section": "Experiments"}, {"figure_path": "DO9wPZOPjk/figures/figures_38_1.jpg", "caption": "Figure 3: An example of CITYSCAPES.", "description": "This figure shows a qualitative comparison of the results obtained using different methods on the CITYSCAPES validation set.  Three image examples are shown, each with the input image, the ground truth segmentation, the segmentation produced by a full-precision model, and the segmentation result produced by the proposed BOLD (Boolean Logic Deep Learning) method.  The goal is to visually demonstrate the comparable performance of the BOLD method against the full precision baseline.", "section": "4 Experiments"}]