[{"type": "text", "text": "B\u2295LD: Boolean Logic Deep Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Van Minh Nguyen Cristian Ocampo Aymen Askri Louis Leconte Ba-Hien Tran ", "page_idx": 0}, {"type": "text", "text": "Mathematical and Algorithmic Sciences Laboratory Huawei Paris Research Center, France vanminh.nguyen@huawei.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Computational intensiveness of deep learning has motivated low-precision arithmetic designs. However, the current quantized/binarized training approaches are limited by: (1) significant performance loss due to arbitrary approximations of the latent weight gradient through its discretization/binarization function, and (2) training computational intensiveness due to the reliance on full-precision latent weights. This paper proposes a novel mathematical principle by introducing the notion of Boolean variation such that neurons made of Boolean weights and/or activations can be trained \u2014for the first time\u2014 natively in Boolean domain instead of latent-weight gradient descent and real arithmetic. We explore its convergence, conduct extensively experimental benchmarking, and provide consistent complexity evaluation by considering chip architecture, memory hierarchy, dataflow, and arithmetic precision. Our approach achieves baseline full-precision accuracy in ImageNet classification and surpasses state-of-the-art results in semantic segmentation, with notable performance in image super-resolution, and natural language understanding with transformer-based models. Moreover, it significantly reduces energy consumption during both training and inference. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep learning [59] has become the de facto solution to a wide range of tasks. However, running deep learning models for inference demands significant computational resources, yet it is just the tip of the iceberg. Training deep models is even much more intense. The extensive literature on this issue can be summarized into four approaches addressing different sources of complexity. These include: $(I)$ model compression, pruning [38, 20, 109] and network design [94, 44, 96] for large model dimensions; (2) arithmetic approximation [58, 94, 15] for intensive multiplication; (3) quantization techniques like post-training [106, 34], quantization-aware training [37, 114, 51], and quantized training to reduce precision [16, 93]; and (4) hardware design [18, 87, 105, 101, 35, 112] to overcome computing bottleneck by moving computation closer to or in memory. ", "page_idx": 0}, {"type": "text", "text": "Aside from hardware and dataflow design, deep learning designs have primarily focused on the number of compute operations (OPs), such as FLOPS or BOPS, as a complexity measure [33, 83] rather than the consumed energy or memory, and particularly in inference tasks. However, it has been demonstrated that OPs alone are inadequate and even detrimental as a measure of system complexity. Instead, energy consumption provides a more consistent and efficient metric for computing hardware [94, 95, 108, 92]. Data movement, especially, dominates energy consumption and is closely linked to system architecture, memory hierarchy, and dataflow [57, 89, 110, 19]. Therefore, efforts aimed solely at reducing OPs are inefficient. ", "page_idx": 0}, {"type": "text", "text": "Quantization-aware training, notably binarized neural networks (BNNs) [24, 47], have garnered significant investigation [see, e.g. 83, 34, and references therein]. BNNs typically binarize weights and activations, forming principal computation blocks in binary. They learn binary weights, $\\mathbf{w}_{\\mathtt{b i n}}$ , through full-precision (FP) latent weights, $\\mathbf{w_{fp}}$ , leading to no memory or computation savings during training. For example, a binarized linear layer is operated as $s=\\alpha\\cdot\\mathbf{w}_{\\mathtt{b i n}}^{\\top}\\mathbf{x}_{\\mathtt{b i n}}$ , where $s$ is the output and $\\alpha$ is a FP scaling factor, $\\mathbf{w}_{\\mathrm{bin}}=\\mathrm{sign}(\\mathbf{w}_{\\mathrm{fp}})$ , and $\\mathbf{x}_{\\mathrm{bin}}=\\mathrm{sign}(\\mathbf{x}_{\\mathrm{fp}})$ is the binarized inputs. The weights are updated via common gradient descent backpropagation, i.e. $\\mathbf{w}_{\\mathrm{bin}}=\\mathrm{sign}(\\mathbf{w}_{\\mathbf{fp}}-\\boldsymbol{\\eta}\\cdot\\mathbf{g}_{\\mathbf{w}_{\\mathbf{fp}}})$ with a learning rate $\\eta$ , and FP gradient signal $\\mathbf{g}_{\\mathbf{w}_{\\mathtt{f p}}}$ . Gradient approximation of binarized variables often employs a differentiable proxy of the binarization function sign, commonly the identity proxy. Various approaches treat BNN training as a constrained optimization problem [7, 2, 3, 65], exploring methods to derive binary weights from real-valued latent ones. BNNs commonly suffers notable accuracy drops due to reduced network capacity and the use of proxy FP optimizers [61] instead of operating directly in binary domain [83, 77, 36]. Recent works mitigate this by incorporating multiple FP components in the network, retaining only a few binary dataflows [68]. Thus, while binarization aids in reducing inference complexity, it increases network training complexity and memory usage. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In contrast to binarizing FP models like BNNs, designing native binary models not relying on FP latent weight has been explored. For example, Expectation Backpropagation [91], although operating on full-precision training, was proposed for this purpose. Statistical physics-inspired [8, 9] and Belief Propagation [10] algorithms utilize integer latent weights, mainly applied to single perceptrons, with unclear applicability to deep models. Evolutionary algorithms [75, 50] are also an alternative but encounter performance and scalability challenges. ", "page_idx": 1}, {"type": "text", "text": "Summary: No scalable and efficient algorithm currently exists for natively training deep models in binary. The challenge of significantly reducing the training complexity while maintaining high performance of deep learning models remains open. ", "page_idx": 1}, {"type": "text", "text": "Contributions. For the aforementioned challenge, we propose a novel framework \u2014 Boolean Logic Deep Learning $\\mathbf{\\Psi}(\\mathbf{B}\\oplus\\mathbf{LD})$ \u2014 which relies on Boolean notions to define models and training: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce the notion of variation to the Boolean logic and develop a new mathematical framework of function variation (see $\\S\\ 3.2\\rangle$ ). One of the noticeable properties is that Boolean variation has the chain rule (see Theorem 3.12) similar to the continuous gradient.   \n\u2022 Based on the proposed framework, we develop a novel Boolean backpropagation and optimization method allowing for a deep model to support native Boolean components operated solely with Boolean logic and trained directly in Boolean domain, eliminating the need for gradient descent and FP latent weights (see $\\S\\ 3.3\\$ ). This drastically cuts down memory footprint and energy consumption during both training and inference (see, e.g., Fig. 1).   \n\u2022 We provide a theoretical analysis of the convergence of our training algorithm (see Theorem 3.17).   \n\u2022 We conduct an extensive experimental campaign using modern network architectures such as convolutional neural networks (CNNs) and Transformers [100] on a wide range of challenging tasks including image classification, segmentation, super-resolution and natural language understanding (see $\\S\\ 4$ ). We rigorously evaluate analytically the complexity of B\u2295LD and BNNs. We demonstrate the superior performance of our method in terms of both accuracy and complexity compared to the state-of-the-art (see, e.g., Table 4, Table 5, Table 7). ", "page_idx": 1}, {"type": "text", "text": "2 Are Current Binarized Neural Networks Really Efficient? ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our work is closely related with the line of research on binarized neural networks (BNNs). The concept of BNNs traces back to early efforts to reduce the complexity of deep learning models. BINARYCONNECT [24] is one of the pioneering works that introduced the idea of binarizing FP weights during training, effectively reducing memory footprint and computational cost. Similarly, BINARYNET [47], XNOR-NET [86] extended this approach to binarize both weights and activations, further enhancing the efficiency of neural network inference. However, these early BNNs struggled with maintaining accuracy comparable to their full-precision counterparts. To address this issue, significant advances have been made on BNNs [see, e.g., 83, 85, and references therein], which can be categorized into three main aspects. ", "page_idx": 1}, {"type": "text", "text": "$\\circled{1}$ Binarization strategy. The binarization strategy aims to efficiently convert real-valued data such as activations and weights into binary form $\\{-1,1\\}$ . The sign function is commonly used for this purpose, sometimes with additional constraints such as clipping bounds in state-of-the-art (SOTA) methods like POKEBNN [117] or BNEXT [36]. REACTNET [68] introduces rsign as a more general alternative to the sign function, addressing potential shifts in the distribution of activations and weights. Another approach [99] explores the use of other two real values instead of strict binary to enhance the representational capability of BNNs. ", "page_idx": 1}, {"type": "table", "img_path": "DO9wPZOPjk/tmp/cef6a816c826676a87d918247ecb3c2da71c9e23dbe223097160a1754e643dc7.jpg", "table_caption": ["Table 1: A summary of SOTA BNNs compared to our method. The notation $\\pmb{x}$ indicates the nonexistence of a specific requirement (column) within a given method (row). The colors denoting the methods shall be used consistently throughout the paper. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "$\\circled{2}$ Optimization and training strategy. BNN optimization relies totally on latent-weight training, necessitating a differentiable proxy for the backpropagation. Moreover, latent-weight based training methods have to store binary and real parameters during training and often requires multiple sequential training stages, where one starts with training a FP model and only later enables binarization [36, 117, 107]. This further increases the training costs. Furthermore, knowledge distillation (KD) has emerged as a method to narrow the performance gap by transferring knowledge from a full-precision teacher model to a binary model [68, 107, 117, 60]. While a single teacher model can sufficiently improve the accuracy of the student model, recent advancements like multi-KD with multiple teachers, such as BNEXT [36], have achieved unprecedented performance. However, the KD approach often treats network binarization as an add-on to full-precision models. In addition, this training approach relies on specialized teachers for specific tasks, limiting adaptability on new data. Lastly, [103, 41] proposed some heuristics and improvements to the classic BNN latent-weight based optimizer. ", "page_idx": 2}, {"type": "text", "text": "$\\circled{3}$ Architecture design. Architecture design in BNNs commonly utilizes RESNET [69, 68, 11, 36] and MOBILENET [68, 36] layouts. These methodologies often rely on heavily modified basic blocks, including additional shortcuts, automatic channel scaling via squeezeand-excitation (SE) [117], block duplication with concatenation in the channel domain [68, 36]. Recent approaches introduce initial modules to enhance input adaptation for binary dataflow [107] or replace convolutions with lighter point-wise convolutions [66]. ", "page_idx": 2}, {"type": "text", "text": "Another related line of research involves logic gate networks (LGNs) [79]. In LGNs, each neuron functions as a binary logic gate and, consequently, has only two inputs. Unlike traditional neural networks, LGNs do not utilize weights; instead, they are parameterized by selecting a specific logic gate for each neuron, which can be learned. Compared to the standard neural networks or our proposed method, LGNs are sparse because each neuron receives only two inputs rather than multiple inputs. Recently, [5] expanded on LGNs by incorporating flexible and differentiable lookup tables. While these advancements show promises, adapting them to modern neural network architectures such as CNN or Transformers is challenging. Furthermore, these approaches have not been validated on large-scale datasets like IMAGENET or on tasks that require high precision, such as image segmentation or super-resolution, as demonstrated in our work. ", "page_idx": 2}, {"type": "image", "img_path": "DO9wPZOPjk/tmp/a78117b508fd7c188049ff4c28940099f4ba2c9774195a6e062876fa93faf4bb.jpg", "img_caption": ["Energy Consum. w.r.t. FP $(\\%)$ $(\\leftarrow)$ ", "Figure 1: Our method against notable BNNs on CIFAR10 using VGG-SMALL. The energy is analytically evaluated considering a hypothetical V100 equivalence with native 1-bit support; cf $\\S\\ 4$ for details. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Summary. Table 1 shows key characteristics of SOTA BNN methods. These methods will be considered in our experiments. Notice that all these techniques indeed have to involve operations on FP latent weights during training, whereas our proposed method works directly on native Boolean weights. In addition, most of BNN methods incorporate FP data and modules as mandatory components. As a result, existing BNNs consume much more training energy compared to our B\u2295LD method. An example is shown in Fig. 1, where we consider the VGG-SMALL architecture [24, 90] on CIFAR10 dataset. In $\\S\\ 4$ we will consider much larger datasets and networks on more challenging tasks. We can see that our method achieves $36\\times$ and more than $15\\times$ energy reduction compared to the FP baseline and BINARYNET, respectively, while yielding better accuracy than BNNs. Furthermore, BNNs are commonly tied to specialized network architecture and have to employ costly multi-stage or KD training. Meanwhile, our Boolean framework is completely orthogonal to these BNN methods. It is generic and applicable for a wide range of network architectures, and its training procedure purely relies on Boolean logic from scratch. Nevertheless, we stress that it is not obligatory to use all Boolean components in our proposed framework as it is flexible and can be extended to architectures comprised of a mix of Boolean and FP modules. This feature further improves the superior performance of our method as can be seen in Fig. 1, where we integrate batch normalization (BN) [49] into our Boolean model, and we will demonstrate extensively in our experiments. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Proposed Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Neuron Design ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Boolean Neuron. For the sake of simplicity, we consider a linear layer for presenting the design. Let $w_{0}$ , $\\left(w_{1},\\ldots,w_{m}\\right)$ , and $(x_{1},\\ldots,x_{m})$ be the bias, weights, and inputs of a neuron of input size $m\\geq1$ . In the core use case of our interest, these variables are all Boolean numbers. Let L be a logic gate such as AND, OR, XOR, XNOR. The neuron\u2019s pre-activation output is given as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\ns=w_{0}+\\sum_{i=1}^{m}\\operatorname{L}(w_{i},x_{i}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the summation is understood as the counting of TRUEs. ", "page_idx": 3}, {"type": "text", "text": "Mixed Boolean-Real Neuron. To allow for flexible use and co-existence of this Boolean design with real-valued parts of a deep model, two cases of mixed-type data are considered including Boolean weights with real-valued inputs, and real-valued weights with Boolean inputs. These two cases can be addressed by the following extension of Boolean logic to mixed-type data. To this end, we first introduce the essential notations and definitions. Specifically, we denote $\\mathbb{B}:=\\{\\mathrm{T},\\mathrm{F}\\}$ equipped with the Boolean logic. Here, T and $\\mathrm{F}$ indicate TRUE and FALSE, respectively. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (Three-valued logic). Define $\\mathbb{M}^{\\stackrel{\\mathrm{def}}{=}}\\mathbb{B}\\cup\\{0\\}$ with logic connectives defined according to those of Boolean logic as follows. First, the negation is: $\\neg\\mathrm{T}=\\mathrm{F}$ , $\\lnot\\mathrm{F}=\\mathrm{T}$ , and $\\neg0=0$ . Second, let L be a logic connective, denote by $\\mathrm{L}_{\\mathbb{M}}$ and $\\operatorname{L}_{\\mathbb{B}}$ when it is in M and in $\\mathbb{B}$ , respectively, then $\\mathrm{L}_{\\mathbb{M}}(a,b)=\\mathrm{L}_{\\mathbb{B}}(a,b)$ for $a,b\\in\\mathbb{B}$ and $\\boldsymbol{\\mathrm{L}}_{\\mathbb{M}}(a,b)=0$ otherwise. ", "page_idx": 3}, {"type": "text", "text": "Notation 3.2. Denote by $\\mathbb{L}$ a logic set (e.g., $\\mathbb{B}$ or $\\mathbb{M}$ ), $\\mathbb{R}$ the real set, $\\mathbb{Z}$ the set of integers, $\\mathbb{N}$ a numeric set (e.g., $\\mathbb{R}$ or $\\mathbb{Z}$ ), and $\\mathbb{D}$ a certain set of $\\mathbb{L}$ or $\\mathbb{N}$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 3.3. For $x\\,\\in\\,\\mathbb{N}$ , its logic value denoted by $x_{\\mathrm{{logic}}}$ $i s$ given as $x_{\\mathrm{{logic}}}\\,=\\,\\mathrm{{T}}\\,\\Leftrightarrow\\,x\\,>\\,0_{\\!\\!\\!{}}$ , $x_{\\mathrm{logic}}=\\mathrm{F}\\Leftrightarrow x<0$ , and $x_{\\mathrm{{logic}}}=0\\Leftrightarrow x=0$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 3.4. The magnitude of a variable $x$ , denoted $|x|$ , is defined as its usual absolute value if $x\\in\\mathbb{N}.$ . And for $x\\in\\mathbb{L}$ : $|x|=0$ if $x=0$ , and $|x|=1$ otherwise. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.5 (Mixed-type logic). For L a logic connective of $\\mathbb{L}$ and variables $a,\\ b,$ , operation $c=\\mathrm{L}(a,b)$ is defined such that $|c|=|a||b|$ and $c_{\\mathrm{logic}}=\\mathrm{L}(a_{\\mathrm{logic}},b_{\\mathrm{logic}})$ . ", "page_idx": 3}, {"type": "text", "text": "Using Definition 3.5, neuron formulation Eq. 1 directly applies to the mixed Boolean-real neurons. ", "page_idx": 3}, {"type": "text", "text": "Forward Activation. It is clear that there can only be one unique family of binary activation functions, which is the threshold function. Let $\\tau$ be a scalar, which can be fixed or learned, the forward Boolean activation is given as: $y\\,=\\,\\mathrm{T}$ if $s\\ \\geq\\ \\tau$ and $y\\,=\\,\\mathrm{F}$ otherwise where $s$ is the preactivation. The backpropagation throughout this activation will be described in Appendix C.3. ", "page_idx": 3}, {"type": "text", "text": "3.2 Mathematical Foundation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section we describe the mathematical foundation for our method to train Boolean weights directly in the Boolean domain without relying on FP latent weights. Due to the space limitation, essential notions necessary for presenting the main results are presented here while a comprehensive treatment is provided in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.6. Order relations $\\mathrm{<}\\,\\mathrm{^{\\prime}}\\,a n d\\,\\mathrm{^{\\prime}}\\mathrm{>}\\,^{\\prime}\\,i n\\,\\mathbb{B}$ are defined as follows: $\\mathrm{F}<\\mathrm{T}$ , and $\\mathrm{T}>\\mathrm{F}$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 3.7. For $a,b\\in\\mathbb{B}$ , the variation from a to $b,$ , denoted $\\delta(a\\rightarrow b)$ , is defined as: $\\delta(a\\rightarrow$ $b)\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\mathrm{T}\\ i f b>a,\\ {\\stackrel{\\mathrm{def}}{=}}\\ 0\\ i f b=a,a n d\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\mathrm{F}\\ i f b<a.$ . ", "page_idx": 4}, {"type": "text", "text": "Throughout the paper, ${\\mathcal{F}}(\\mathbb{S},\\mathbb{T})$ denotes the set of all functions from source $\\mathbb{S}$ to image $\\mathbb{T}$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 3.8. For $f\\in{\\mathcal{F}}(\\mathbb{B},\\mathbb{D})$ , $\\forall x\\in\\mathbb{B}$ , write $\\delta f(x\\rightarrow\\neg x):=\\delta(f(x)\\rightarrow f(\\neg x))$ . The variation of $f$ w.r.t. $x$ , denoted $f^{\\prime}(x)$ , is defined as: $f^{\\prime}(x)\\,{\\stackrel{\\mathrm{def}}{=}}\\,\\mathbf{xnor}(\\delta(x\\to\\lnot x),\\delta f(x\\to\\lnot x))$ . ", "page_idx": 4}, {"type": "text", "text": "Remark 3.9. The usual notation of continuous derivative $f^{\\prime}$ is intentionally adopted here for Boolean variation for convenience and notation unification. Its underlying meaning, i.e., continuous derivative or Boolean variation, can be understood directly from the context where function $f$ is defined. ", "page_idx": 4}, {"type": "text", "text": "Intuitively, the variation of $f$ w.r.t. $x$ is $\\mathrm{T}$ if $f$ varies in the same direction with $x$ . Example 3.10. Let $a\\in\\mathbb{B}$ , $f(x)=\\mathbf{xor}(x,a)$ for $x\\in\\mathbb{B}$ , the variation of $f$ w.r.t. $x$ can be derived by establishing a truth table (see Table 8 in Appendix A.1) from which we obtain $f^{\\prime}(x)=\\lnot a$ . ", "page_idx": 4}, {"type": "text", "text": "For $f\\in\\mathcal{F}(\\mathbb{Z},\\mathbb{N})$ , its derivative, also known in terms of finite differences, has been defined in the literature as $f^{\\prime}(x)=f(x+1)-f(x)$ , see e.g. [52]. With the logic variation as introduced above, we can make this definition more generic as follows. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.11. For $f\\in{\\mathcal{F}}(\\mathbb{Z},\\mathbb{D})$ , the variation of $f$ w.r.t. $x\\in\\mathbb{Z}$ is defined as $f^{\\prime}(x)\\,{\\stackrel{\\mathrm{def}}{=}}\\,\\delta f(x\\to$ $x+1)$ , where $\\delta f$ is in the sense of the variation defined in $\\mathbb{D}$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.12. The following properties hold: ", "page_idx": 4}, {"type": "text", "text": "1. For $f\\in{\\mathcal{F}}(\\mathbb{B},\\mathbb{B})\\colon(\\neg f)^{\\prime}(x)=\\neg f^{\\prime}(x),\\,\\forall x\\in\\mathbb{B}.$ 2. For $f\\in\\mathcal{F}(\\mathbb{B},\\mathbb{N}),\\,\\alpha\\in\\mathbb{N}\\!:(\\alpha f)^{\\prime}(x)=\\alpha f^{\\prime}(x),\\,\\forall x\\in\\mathbb{B}.$ . 3. For $f,g\\in\\mathcal{F}(\\mathbb{B},\\mathbb{N})$ : $(f+g)^{\\prime}(x)=f^{\\prime}(x)+g^{\\prime}(x),\\,\\forall x\\in\\mathbb{B}.$ $\\begin{array}{r}{\\4.\\ F o r\\,\\mathbb{B}\\xrightarrow{f}\\mathbb{B}\\xrightarrow{g}\\mathbb{D}\\colon(g\\circ f)^{\\prime}(x)=\\mathbf{xnor}(g^{\\prime}(f(x)),f^{\\prime}(x)),\\,\\forall x\\in\\mathbb{B}.}\\end{array}$ 5. For $\\mathbb{B}\\xrightarrow{f}\\mathbb{Z}\\xrightarrow{g}\\mathbb{D}$ , $x\\in\\mathbb{B},\\,i f\\,|f^{\\prime}(x)|\\leq1$ and $g^{\\prime}(f(x))=g^{\\prime}(f(x)-1).$ , then: $(g\\circ f)^{\\prime}(x)=\\operatorname{\\mathbf{xnor}}(g^{\\prime}(f(x)),f^{\\prime}(x)).$ ", "page_idx": 4}, {"type": "text", "text": "The proof is provided in Appendix A.1. These results are extended to the multivariate case in a straightforward manner. For instance, for multivariate Boolean functions it is as follows. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.13. For $\\mathbf{x}=(x_{1},\\ldots,x_{n})\\in\\mathbb{B}^{n}$ , denote $\\mathbf{x}_{\\rightarrow i}:=\\left(x_{1},\\dots,x_{i-1},\\neg x_{i},x_{i+1},\\dots,x_{n}\\right);$ for $n\\geq1$ and $1\\leq i\\leq n$ . For $f\\in{\\mathcal{F}}(\\mathbb{B}^{n},\\mathbb{B})$ , the (partial) variation of $f$ w.r.t. $x_{i}$ , denoted $f_{i}^{\\prime}(\\mathbf{x})$ or $\\delta f(\\mathbf{x})/\\delta x_{i}$ , is defined as: $f_{i}^{\\prime}({\\bf x})\\equiv\\delta f({\\bf x})/\\delta x_{i}\\,{\\stackrel{\\mathrm{def}}{=}}\\,{\\bf x n o r}(\\delta(x_{i}\\to-x_{i}),\\delta f({\\bf x}\\to{\\bf x}_{-i})).$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.14. Let $f\\in{\\mathcal{F}}(\\mathbb{B}^{n},\\mathbb{B})$ , $n\\geq1$ , and $g\\in{\\mathcal{F}}(\\mathbb{B},\\mathbb{B})$ . For $1\\leq i\\leq n$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n(g\\circ f)_{i}^{\\prime}(\\mathbf{x})=\\mathbf{xnor}(g^{\\prime}(f(\\mathbf{x})),f_{i}^{\\prime}(\\mathbf{x})),\\quad\\forall\\mathbf{x}\\in\\mathbb{B}^{n}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Example 3.15. From Example 3.10, we have $\\delta\\mathbf{xor}(x,a)/\\delta x=\\neg a$ for $a,x\\in\\mathbb{B}$ . Using Theorem 3.12- (1) we have: $\\delta\\mathbf{xnor}(x,a)/\\delta x=a$ since $\\mathbf{xnor}(x,a)=\\neg\\mathbf{xor}(x,a)$ . ", "page_idx": 4}, {"type": "text", "text": "Example 3.16. Apply Theorem 3.12-(3) to $s$ from Eq. 1: $\\delta s/\\delta w_{i}=\\delta\\mathrm{L}(w_{i},x_{i})/\\delta w_{i}$ and $\\delta s/\\delta x_{i}=$ $\\delta\\mathrm{L}(\\bar{w_{i}},x_{i})/\\delta x_{i}$ . Then, for $\\mathrm{L}=\\mathbf{xnor}$ as an example, we have: $\\delta s/\\delta w_{i}=x_{i}$ and $\\delta s/\\delta x_{i}=w_{i}$ . ", "page_idx": 4}, {"type": "text", "text": "3.3 BackPropagation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "With the notions introduced in $\\S\\ 3.2$ , we can write signals involved in the backpropagation process as shown in Fig. 2. Therein, layer $l$ is a Boolean layer of consideration. For the sake of presentation simplicity, layer $l$ is assumed a fully-connected layer, and: ", "page_idx": 4}, {"type": "equation", "text": "$$\nx_{k,j}^{l+1}=w_{0,j}^{l}+\\sum_{i=1}^{m}\\operatorname{L}\\bigl(x_{k,i}^{l},w_{i,j}^{l}\\bigr),\\quad1\\leq j\\leq n,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathrm{L}$ is the utilized Boolean logic, $k$ denotes sample index in the batch, $m$ and $n$ are the usual layer input and output sizes. Layer $l$ is connected to layer $l+1$ that can be an activation layer, a batch normalization, an arithmetic layer, or any others. The nature of $\\delta\\mathrm{Loss}/\\delta x_{k,j}^{l+1}$ depends on the property of layer $l+1$ . It can be the usual gradient if layer $l+1$ is a real-valued input layer, or a Boolean variation if layer $l+1$ is a Boolean-input layer. Given $\\delta\\mathrm{Loss}/\\delta x_{k,j}^{l+1}$ , layer $l$ needs to optimize its Boolean weights and compute signal $\\delta\\mathrm{Loss}/\\delta x_{k,i}^{l}$ for the upstream. Hereafter, we consider $\\mathrm{L}=\\mathbf{xnor}$ when showing concrete illustrations of the method. ", "page_idx": 4}, {"type": "image", "img_path": "DO9wPZOPjk/tmp/76591671827c4b318e4166127171abdec5e17cd545869d6121df9e3eb6260318.jpg", "img_caption": ["Figure 2: Illustration of backpropagation signals with a Boolean linear layer. Notice that the subsequent layer can be any FP/Boolean layers or activation functions. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Atomic Variation. First, using Theorem 3.12 and its extension to the multivariate case by Proposition 3.14 in the same manner as shown in Example 3.16, we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\delta x_{k,j}^{l+1}}{\\delta w_{i,j}^{l}}=\\frac{\\delta\\mathrm{L}(x_{k,i}^{l},w_{i,j}^{l})}{\\delta w_{i,j}^{l}}\\overset{\\mathrm{L}=\\mathrm{xnor}}{=}x_{k,i}^{l},\\quad\\frac{\\delta x_{k,j}^{l+1}}{\\delta x_{k,i}^{l}}=\\frac{\\delta\\mathrm{L}(x_{k,i}^{l},w_{i,j}^{l})}{\\delta x_{k,i}^{l}}\\overset{\\mathrm{L}=\\mathrm{xnor}}{=}w_{i,j}^{l}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Using the chain rules given by Theorem 3.12\u2013(4 & 5), we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{i,j,k}^{l}:=\\displaystyle\\frac{\\delta\\mathrm{Loss}}{\\delta w_{i,j}^{l}}\\vert_{k}=\\mathbf{xnor}(\\frac{\\delta\\mathrm{Loss}}{\\delta x_{k,j}^{l+1}},\\frac{\\delta x_{k,j}^{l+1}}{\\delta w_{i,j}^{l}})\\overset{\\mathrm{L=xnor}}{=}\\mathbf{xnor}(\\frac{\\delta\\mathrm{Loss}}{\\delta x_{k,j}^{l+1}},x_{k,i}^{l}),}\\\\ &{g_{k,i,j}^{l}:=\\displaystyle\\frac{\\delta\\mathrm{Loss}}{\\delta x_{k,i}^{l}}\\vert_{j}=\\mathbf{xnor}(\\frac{\\delta\\mathrm{Loss}}{\\delta x_{k,j}^{l+1}},\\frac{\\delta x_{k,j}^{l+1}}{\\delta x_{k,i}^{l}})\\overset{\\mathrm{L=xnor}}{=}\\mathbf{xnor}(\\frac{\\delta\\mathrm{Loss}}{\\delta x_{k,j}^{l+1}},w_{i,j}^{l}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Aggregation. Atomic variation $q_{i,j,k}^{l}$ is aggregated over batch dimension $k$ while $g_{k,i,j}^{l}$ is aggregated over output dimension $j$ . Let $\\mathbf{1}(\\cdot)$ be the indicator function. For $b\\in\\mathbb{B}$ and variable $x$ , define: $\\mathbf{\\bar{1}}(x=b)=1$ if $x_{\\mathrm{logic}}=b$ and $\\mathbf{1}(x=\\dot{b})=0$ otherwise. Atomic variations are aggregated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{i,j}^{l}:=\\displaystyle\\frac{\\delta\\mathrm{Loss}}{\\delta w_{i,j}^{l}}=\\sum_{k}\\mathbf{1}\\big(q_{i,j,k}^{l}=\\mathrm{T}\\big)|q_{i,j,k}^{l}|-\\displaystyle\\sum_{k}\\mathbf{1}\\big(q_{i,j,k}^{l}=\\mathrm{F}\\big)|q_{i,j,k}^{l}|,}\\\\ &{g_{k,i}^{l}:=\\displaystyle\\frac{\\delta\\mathrm{Loss}}{\\delta x_{k,i}^{l}}=\\sum_{j}\\mathbf{1}\\big(g_{k,i,j}^{l}=\\mathrm{T}\\big)|g_{k,i,j}^{l}|-\\displaystyle\\sum_{j}\\mathbf{1}\\big(g_{k,i,j}^{l}=\\mathrm{F}\\big)|g_{k,i,j}^{l}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Boolean Optimizer. With $q_{i,j}^{l}$ obtained in Eq. 7, the rule for optimizing $w_{i,j}^{l}$ subjected to making the loss decreased is simply given according to its definition as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\boxed{w_{i,j}^{l}=\\lnot w_{i,j}^{l}\\;\\mathrm{if}\\;\\mathbf{xnor}\\big(q_{i,j}^{l},w_{i,j}^{l}\\big)=\\mathrm{T}.}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Eq. 9 is the core optimization logic based on which more sophisticated forms of optimizer can be developed in the same manner as different methods such as Adam, Adaptive Adam, etc. have been developed from the basic gradient descent principle. For instance, the following is an optimizer that accumulates $q_{i,j}^{l}$ over training iterations. Denote by $q_{i,j}^{l,t}$ the optimization signal at iteration $t$ , and by $m_{i,j}^{l,t}$ its accumulator with $m_{i,j}^{l,0}:=0$ and: ", "page_idx": 5}, {"type": "equation", "text": "$$\nm_{i,j}^{l,t+1}=\\beta^{t}m_{i,j}^{l,t}+\\eta^{t}q_{i,j}^{l,t+1},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\eta^{t}$ is an accumulation factor that can be tuned as a hyper-parameter, and $\\beta^{t}$ is an autoregularizing factor that expresses the system\u2019s state at time $t$ . Its usage is linked to brain plasticity [31] and Hebbian theory [40] forcing weights to adapt to their neighborhood. For the chosen weight\u2019s neighborhood, for instance, neuron, layer, or network level, $\\beta^{t}$ is given as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\beta^{t}=\\frac{\\mathrm{Number\\;of\\;unchanged\\;weights\\;at}\\;t}{\\mathrm{Total\\;number\\;of\\;weights}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the experiments presented later, $\\beta^{t}$ is set to per-layer basis. Finally, the learning process is as described in Algorithm 1. We encourage the readers to check the detailed implementations, practical considerations, and example codes of our proposed method, available in Appendix B and Appendix C. ", "page_idx": 6}, {"type": "text", "text": "Convergence Analysis. The following result describes how the iterative logic optimization based on Eq. 9 minimizes a predefined loss $f$ , under the standard non-convex assumption. The technical assumptions and the proof are given in Appendix A.2. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.17. Under the specified assumptions, Boolean optimization logic converges at: ", "page_idx": 6}, {"type": "text", "text": "$\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\Big[\\left\\|\\nabla f\\left(\\boldsymbol{w}_{t}\\right)\\right\\|^{2}\\Big]\\leq\\frac{A^{*}}{T\\eta}{+}B^{*}\\eta{+}C^{*}\\eta^{2}{+}L r_{d},$ (12) where $A^{*}=2(f(w_{0})-f_{*})$ with $f_{*}$ being uniform lower bound assumed exists, $B^{*}=2L\\bar{\\sigma}^{2}$ , $C^{*}=$ $4L^{2}\\sigma^{2}\\frac{\\gamma}{(1\\!-\\!\\gamma)^{2}}$ 2(1\u2212\u03b3\u03b3)2 in which L is the assumed Lipschitz constant, and $r_{d}=d\\kappa/2$ .   \nThe bound in Theorem 3.17 contains four terms. The first is typical for a general non-convex target and expresses how initialization affects the convergence. The second and third terms depend on the fluctuation of the minibatch gradients. There is an \u201cerror bound\u201d of $2L d\\kappa$ independent of $T$ . This error bound is the cost of using discrete weights as part of the optimization algorithm. Previous work with quantized models also includes such error bounds [61, 62]. ", "page_idx": 6}, {"type": "table", "img_path": "DO9wPZOPjk/tmp/3f6f242807b9fbb4c4a7937e750a91df03f8bf42dcc46fec8b249fe7c2b893dc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Regularization. Exploding and vanishing gradients are two well-known issues when it comes to train deep neural networks. During Boolean Logic training, our preliminary experiments indicated that the backpropagation signal also experiences similar phenomenon, resulting in unstable training. Our idea is to scale the backpropagation signals so as to match their variance. Thanks to the Boolean structure, assumptions on the involved signals can be made so that the scaling factor can be analytically derived in closed-form without need of learning or batch statistics computation. Precisely, through linear layers of output size $m$ , the backpropagation signal is scaled with $\\sqrt{^{2}/m}$ , and for convolutional layers of output size $c_{\\mathrm{out}}$ , stride $v$ and kernel sizes $k_{x},k_{y}$ , the backpropagation signal is scaled with $2\\sqrt{2v/c_{\\mathrm{out}}k_{x}k_{y}}$ if maxpooling is applied, or $\\sqrt{2v/c_{\\mathrm{out}}k_{x}k_{y}}$ otherwise. The full detail is presented in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our B\u2295LD method achieves extreme compression by using both Boolean activations and weights. We rigorously evaluate its performance on challenging precision-demanding tasks, including image classification on CIFAR10 [55] and IMAGENET [56], as well as super-resolution on five popular datasets. Furthermore, recognizing the necessity of deploying efficient lightweight models for edge computing, we delve into three fine-tuning scenarios, showcasing the adaptability of our approach. Specifically, we investigate fine-tuning Boolean models for image classification on CIFAR10 and CIFAR100 using VGG-SMALL. For segmentation tasks, we study DEEPLABV3 [17] fine-tuned on CITYSCAPES [23] and PASCAL VOC 2012 [29] datasets. The backbone for such a model is our Boolean RESNET18 [39] network trained from scratch on IMAGENET. Finally, we consider an evaluation in the domain of natural language understanding, fine-tuning BERT [26], a transformerbased [100] language model, on the GLUE benchmark [102]. ", "page_idx": 6}, {"type": "text", "text": "Experimental Setup. To construct our B\u2295LD models, we introduce Boolean weights and activations and substitute full-precision (FP) arithmetic layers with Boolean equivalents. Throughout all benchmarks, we maintain the general network design of the chosen FP baseline, while excluding FP-specific components like ReLU, PReLU activations, or batch normalization (BN) [49], unless specified otherwise. Consistent with the common setup in existing literature [see, e.g., 86, 21, 11], only the first and last layers remain in FP and are optimized using an Adam optimizer [54]. Comprehensive experiment details for reproducibility are provided in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Complexity Evaluation. It has been demonstrated that relying solely on FLOPS and BOPS for complexity assessment is inadequate [94, 95, 108, 92]. In fact, these metrics fail to capture the actual load caused by propagating FP data through the BNNs. Instead, energy consumption serves as a crucial indicator of efficiency. Given the absence of native Boolean accelerators, we estimate analytically energy consumption by analyzing the arithmetic operations, data movements within storage/processing units, and the energy cost of each operation. This approach is implemented for the Nvidia GPU (Tesla V100) and Ascend [63] architectures. Further details are available in Appendix E. ", "page_idx": 7}, {"type": "text", "text": "4.1 Image Classification ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our $\\mathbf{B}\\oplus\\mathbf{LD}$ method is tested on two network configurations: small & compact and large & deep. In the former scenario, we utilize the VGG-SMALL [90] baseline trained on CIFAR10. Evaluation of our Boolean architecture is conducted both without BN [24], and with BN including activation from [69]. These designs achieve $90.29{\\pm}0.09\\%$ (estimated over six repetitions) and $92.37{\\pm}0.01\\%$ (estimated over five repetitions) accuracy, respectively (see Table 2). Notably, without BN, our results align closely with BINARYCONNECT [24], which employs 32-bit activations during both inference and training. Furthermore, BN brings the accuracy within 1 point of the FP baseline. Additional results are provided in the supplementary material for VGG-SMALL models ending with 1 FC layer. ", "page_idx": 7}, {"type": "text", "text": "Our method requires much less energy than the FP baseline. In particular, it consumes less than $5\\%$ of energy for our designs with and without BN respectively. These results highlight the remarkable energy efficiency of our B\u2295LD method in both inference and training, surpassing latent-weight based training methods [24, 86, 48] reliant on FP weights. Notably, despite a slight increase in energy consumption, utilizing BN yields superior accuracy. Even with BN, our approach maintains superior efficiency compared to alternative methods, further emphasizing the flexibility of our approach in training networks with a blend of Boolean and FP components. ", "page_idx": 7}, {"type": "text", "text": "In the large & deep case, we consider the RESNET18 baseline trained from scratch on IMAGENET. We compare our approach to methods employing the same baseline, larger architectures, and additional training strategies such as KD with a RESNET34 teacher or FP-based shortcuts [69]. Our method consistently achieves the highest accuracy across all categories, ranging from the standard model $(51.8\\%$ accuracy) to larger configurations $70.0\\%$ accuracy), as shown in Table 5. Additionally, our $\\mathbf{B}\\oplus\\mathbf{LD}$ method exhibits the smallest energy consumption in most categories, with a remarkable $24.45\\%$ for our large architecture with and without KD. Notably, our method outperforms the FP baseline when using $4\\times$ filter enlargement (base 256), providing significant energy reduction $(24.45\\%)$ . Furthermore, it surpasses the SOTA POKEBNN [117], utilizing RESNET50 as a teacher. ", "page_idx": 7}, {"type": "text", "text": "For completeness, we also implemented neural gradient quantization, utilizing INT4 quantization with a logarithmic round-to-nearest approach [21] and statistics-aware weight binning [22]. Our experiments on IMAGENET confirm that 4-bit quantization is sufficient to achieve standard FP performances, reaching $67.53\\%$ accuracy in 100 epochs (further details provided in Appendix D.1.4). ", "page_idx": 7}, {"type": "text", "text": "4.2 Image Super-resolution ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Next, we evaluate the efficacy of our $\\mathbf{B}\\oplus\\mathbf{LD}$ method to synthesize data. We use a compact EDSR network [64] as our baseline, referred to as SMALL EDSR, comprising eight residual blocks. Our $\\mathbf{B}\\oplus\\mathbf{LD}$ model employs Boolean residual blocks without BN. Results, presented in Table 3, based on the official implementation and benchmark2, reveal remarkable similarity to the FP reference at each scale. Particularly noteworthy are the prominent results achieved on SET14 and BSD100 datasets. Our method consistently delivers high PSNR for high-resolution images, such as DIV2K, and even higher for low-resolution ones, like SET5. However, akin to EDSR, our approach exhibits a moderate performance reduction at scale $4\\times$ . These findings highlight the capability of our method to perform adequately on detail-demanding tasks while exhibiting considerable robustness across image resolutions. ", "page_idx": 7}, {"type": "table", "img_path": "DO9wPZOPjk/tmp/c68a764bcf7781b25bb710d69de910799906d717c523b4f4a0311c9f9cf671ae.jpg", "table_caption": ["Table 2: Results with VGG-SMALL on CIFAR10. \u2018Cons.\u2019 is the energy consumption w.r.t. the FP baseline, evaluated on 1 training iteration. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "DO9wPZOPjk/tmp/f7774bd8298f6690b65eece1e3cf784cddf81664e9d761b19bf5a14f8387653a.jpg", "table_caption": ["Table 3: Super-resolution results measured in PSNR (dB) (\u2191), using the EDSR baseline [64]. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "DO9wPZOPjk/tmp/8913308c85873b74c8b08429b34456a80b437cc89d9fdd337dd35658a437e080.jpg", "table_caption": ["Table 4: Image segmentation results. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "DO9wPZOPjk/tmp/f3a8208888b08e1569ca2c8226bfc0a71e9bea55a5169b06345f614c8c67bf3e.jpg", "table_caption": ["Table 5: Results with RESNET18 baseline on IMAGENET. \u2018Base\u2019 is the mapping dimension of the first layer. Energy consumption is evaluated on 1 training iteration. \u2018Cons.\u2019 is the energy consumption w.r.t. the FP baseline. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "DO9wPZOPjk/tmp/4f70ddfd5916bb6e6c83acaaa962d376069ab17bed0301f515f0bf2c8fd6ef7d.jpg", "table_caption": ["Table 6: Results with VGG-SMALL baseline finetuned on CIFAR10 and CIFAR100. \u2018FT\u2019 means \u2018Fine-Tuning\u2019. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.3 Adaptability on New Data ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Image classification fine-tuning. We aim to assess the adaptability of our method to similar problems but different datasets, a common scenario for edge inference tasks. We employ the VGGSMALL architecture without BN under two training configurations. Firstly, the B\u2295LD model is trained from scratch with random initialization on CIFAR10 (REF. C) and CIFAR100 (REF. D). Secondly, we fine-tune the trained networks on CIFAR100 (REF. F) and CIFAR10 (REF. H), respectively. Notably, in Table 6, fine-tuning our trained model on CIFAR100 (REF. F) results in a model almost identical to the model trained entirely from scratch (REF. D). Additionally, a noteworthy result is observed with our model (REF. H), which achieves higher accuracy than the model trained from scratch (REF. C). ", "page_idx": 8}, {"type": "text", "text": "Image segmentation fine-tuning. Next, we expand the scope of the aforementioned fine-tuning experiment to encompass a larger network and a different task. The baseline is the DEEPLABV3 network for semantic segmentation. It consists of our Boolean RESNET18 (without BN) as the backbone, followed by the Boolean atrous pyramid pooling (ASPP) module [17] . We refrain from utilizing auxiliary loss or knowledge distillation techniques, as these methods introduce additional computational burdens, which are contrary to our objective of efficient on-device training. As demonstrated in Table 4, our method achieves a notable $67.4\\%$ mIoU on CITYSCAPES (see Fig. 3 for prediction examples). This result surpasses the SOTA, BINARY DAD-NET [30], and approaches the performance of the FP baseline. Likewise, on PASCAL VOC 2012, our methodology nears the performance of the FP baseline. Importantly, these improvements are attained without the intermediate use of FP parameters during training, highlighting the efficiency and effectiveness of our approach. This shows that our method not only preserves the inherent lightweight advantages of highly quantized neural networks but also significantly enhances performance in complex segmentation tasks. ", "page_idx": 8}, {"type": "text", "text": "BERT fine-tuning for NLU tasks. Finally, we consider fine-tuning BERT [26], a transformer-based model [100], on the GLUE benchmark [102]. We follow the standard experimental protocol as in [26, 6, 82]. Our model and the chosen baselines are employed with 1-bit bitwidth for both weights and activations. Our Boolean BERT model is inspired by BIT [67] for binarizing activations and incorporating KD during training, where the FP teacher guides the student in a layer-wise manner. We follow the experimental setup of BIT, including using the same method for binarizing activations and backpropagation for softmax and attention in the BERT model. As shown in Table 7, all methods suffer from performance drop compared to the FP model as extreme binarization of transformer-based model is not trivial. Nevertheless, our method yields results comparable to BIT [67], the SOTA method on this task, outperforming BINARYBERT [6] and BIBERT [82] on average. This is remarkable as our method natively uses Boolean weights during the training, whereas the baselines heavily rely on FP latent weights. These findings indicate potential for energy-efficient large language models (LLMs) using our method for both training and inference. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "image", "img_path": "DO9wPZOPjk/tmp/3033fbccfe133760059b7b27b1c021e18dfa69ef8148bb32374d53a671cd4148.jpg", "img_caption": ["Figure 3: An example of CITYSCAPES. "], "img_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "DO9wPZOPjk/tmp/1bb7e19acd7a02825bafa6680162c04e142b7c607b2f50e7b475a155de0cf545.jpg", "table_caption": ["Table 7: BERT models results. \u2020Source code [67]. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduced the notion of Boolean variation and developed a first framework of its calculus. This novel mathematical principle enabled the development of Boolean logic backpropagation and Boolean optimization replacing gradient backpropagation and gradient descent for binary deep learning. Deep models can be built with native Boolean weights and/or Boolean activations, and trained in Boolean natively by this principled exact Boolean optimization. That brings a key advantage to the existing popular quantized/binarized training approach that suffers from critical bottlenecks $-\\,(i)$ performance loss due to an arbitrary approximation of the latent weight gradient through its discretization/binarization function, $(i i)$ training computational intensiveness due to full-precision latent weights. We have extensively explored its capabilities, highlighting: (i) both training and inference are now possible in binary; $(i\\nu)$ deep training complexity can be drastically reduced to unprecedented levels. (iii) Boolean models can handle finer tasks beyond classification, contrary to common belief; $(i i)$ in some applications, suitablely enlarging Boolean model can recover FP performance while still gaining significant complexity reduction. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts. Our multidomain comprehensive examination confirms that it is possible to create high performing binary deep neural networks thanks to the proposed method. Our findings suggest the positive impacts in many domains, making deep learning more environmentally friendly, in particular reduce the complexity of huge models like LLMs, and enabling new applications like online, incremental, on-device training, and user-centric AI models. Given the prevalence of LLMs, our approach can facilitate faster predictions on more affordable devices, contributing to the democratization of deep learning. On the other hand, computing architectures have been so far pushed far from its native logic by high-precision arithmetic applications, e.g., 16-bit floating-point is currently a most popular AI computing architecture. Boolean logic deep learning would motivate new software optimization and hardware accelerator architectures in the direction of bringing them back to the native Boolean logic computing. The proposed mathematical notion and its calculus could also benefit other fields such as circuit theory, binary optimization, etc. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Due to current computing accelerators, such as GPUs, primarily designed for real arithmetic, our method could not be assessed on native Boolean accelerator. Nevertheless, its considerable potential may inspire the development of new logic circuits and architectures utilizing Boolean logic processing. It also remains as an open question the approximation capacity of Boolean neural networks. A mathematical result equivent to the existing universal approximation theory of real-valued neural networks would provide a solid guarantee. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work has been made possible through the invaluable support of various departments and colleagues within Huawei. Van Minh Nguyen would like to extend his sincere thanks to everyone involved, with special appreciation to Jean-Claude Belfiore and the former students who have participated in this project over the years: Valentin Abadie, Tom Huix, Tianyu Li, and Youssef Chaabouni. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] E. Agustsson and R. Timofte. NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, July 2017.   \n[2] T. Ajanthan, P. K. Dokania, R. Hartley, and P. H. Torr. Proximal Mean-field for Neural Network Quantization. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.   \n[3] T. Ajanthan, K. Gupta, P. Torr, R. Hartley, and P. Dokania. Mirror Descent View for Neural Network Quantization. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130, pages 2809\u20132817. PMLR, 13\u201315 Apr 2021.   \n[4] D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic. QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   \n[5] A. T. L. Bacellar, Z. Susskind, M. Breternitz Jr, E. John, L. K. John, P. M. V. Lima, and F. M. Fran\u00e7a. Differentiable weightless neural networks. In R. Salakhutdinov, Z. Kolter, K. Heller, A. Weller, N. Oliver, J. Scarlett, and F. Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 2277\u20132295. PMLR, 21\u201327 Jul 2024.   \n[6] H. Bai, W. Zhang, L. Hou, L. Shang, J. Jin, X. Jiang, Q. Liu, M. Lyu, and I. King. BinaryBERT: Pushing the Limit of BERT Quantization. In C. Zong, F. Xia, W. Li, and R. Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4334\u20134348. Association for Computational Linguistics, 2021.   \n[7] Y. Bai, Y.-X. Wang, and E. Liberty. ProxQuant: Quantized Neural Networks via Proximal Operators. In International Conference on Learning Representations, 2018.   \n[8] C. Baldassi. Generalization Learning in a Perceptron with Binary Synapses. Journal of Statistical Physics, 136(5):902\u2013916, Sep 2009.   \n[9] C. Baldassi and A. Braunstein. A Max-Sum Algorithm for Training Discrete Neural Networks. Journal of Statistical Mechanics: Theory and Experiment, 2015(8):P08008, 2015.   \n[10] C. Baldassi, A. Ingrosso, C. Lucibello, L. Saglietti, and R. Zecchina. Subdominant Dense Clusters Allow for Simple Learning and High Computational Performance in Neural Networks with Discrete Synapses. Physical Review Letters, 115(12):128101, 2015.   \n[11] J. Bethge, C. Bartz, H. Yang, Y. Chen, and C. Meinel. MeliusNet: An Improved Network Architecture for Binary Neural Networks. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 1439\u20131448, January 2021.   \n[12] M. Bevilacqua, A. Roumy, C. Guillemot, and M. line Alberi Morel. Low-Complexity Single-Image Super-Resolution based on Nonnegative Neighbor Embedding. In Proceedings of the British Machine Vision Conference, pages 135.1\u2013135.10. BMVA Press, 2012.   \n[13] S. Bianco, R. Cadene, L. Celona, and P. Napoletano. Benchmark Analysis of Representative Deep Neural Network Architectures. IEEE Access, 6:64270\u201364277, 2018.   \n[14] A. Canziani, A. Paszke, and E. Culurciello. An Analysis of Deep Neural Network Models for Practical Applications. arXiv preprint arXiv:1605.07678, 2016.   \n[15] H. Chen, Y. Wang, C. Xu, B. Shi, C. Xu, Q. Tian, and C. Xu. AdderNet: Do We Really Need Multiplications in Deep Learning? In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.   \n[16] J. Chen, Y. Gai, Z. Yao, M. W. Mahoney, and J. E. Gonzalez. A Statistical Framework for Low-bitwidth Training of Deep Neural Networks. In Advances in Neural Information Processing Systems, volume 33, pages 883\u2013894. Curran Associates, Inc., 2020.   \n[17] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam. Rethinking Atrous Convolution for Semantic Image Segmentation. arXiv preprint arXiv:1706.05587, 2017.   \n[18] Y.-H. Chen, J. Emer, and V. Sze. Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks. SIGARCH Computer Architecture News, 44(3):367\u2013379, jun 2016.   \n[19] Y.-H. Chen, T. Krishna, J. S. Emer, and V. Sze. Eyeriss: An Energy-efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks. IEEE Journal of Solid-state Circuits, 52(1):127\u2013138, 2016.   \n[20] Y. Cheng, D. Wang, P. Zhou, and T. Zhang. Model Compression and Acceleration for Deep Neural Networks: The Principles, Progress, and Challenges. IEEE Signal Processing Magazine, 35(1):126\u2013136, 2018.   \n[21] B. Chmiel, R. Banner, E. Hoffer, H. B. Yaacov, and D. Soudry. Logarithmic Unbiased Quantization: Simple 4-bit Training in Deep Learning. arXiv:2112.10769, 2021.   \n[22] J. Choi, P. I.-J. Chuang, Z. Wang, S. Venkataramani, V. Srinivasan, and K. Gopalakrishnan. Bridging the Accuracy Gap for 2-Bit Quantized Neural Networks (QNN). arXiv preprint arXiv:1807.06964, 2018.   \n[23] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The Cityscapes Dataset for Semantic Urban Scene Understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.   \n[24] M. Courbariaux, Y. Bengio, and J.-P. David. BinaryConnect: Training Deep Neural Networks with Binary Weights during Propagations. In Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.   \n[25] C. De Sa, M. Leszczynski, J. Zhang, A. Marzoev, C. R. Aberger, K. Olukotun, and C. R\u00e9. High-Accuracy Low-Precision Training. arXiv preprint arXiv:1803.03383, 2018.   \n[26] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186. Association for Computational Linguistics, 2019.   \n[27] R. Ding, T.-W. Chin, Z. Liu, and D. Marculescu. Regularizing Activation Distribution for Training Binarized Deep Networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.   \n[28] Z. Du, R. Fasthuber, T. Chen, P. Ienne, L. Li, T. Luo, X. Feng, Y. Chen, and O. Temam. ShiDianNao: Shifting Vision Processing Closer to the Sensor. In Proceedings of the 42nd Annual International Symposium on Computer Architecture, pages 92\u2013104, 2015.   \n[29] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. http://www.pascalnetwork.org/challenges/VOC/voc2012/workshop/index.html.   \n[30] A. Frickenstein, M.-R. Vemparala, J. Mayr, N.-S. Nagaraja, C. Unger, F. Tombari, and W. Stechele. Binary DAD-Net: Binarized Driveable Area Detection Network for Autonomous Driving. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 2295\u20132301. IEEE, 2020.   \n[31] E. Fuchs, G. Fl\u00fcgge, et al. Adult Neuroplasticity: More than 40 Years of Research. Neural plasticity, 2014, 2014.   \n[32] Y. Gao, Y. Liu, H. Zhang, Z. Li, Y. Zhu, H. Lin, and M. Yang. Estimating GPU Memory Consumption of Deep Learning Models. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pages 1342\u20131352, 2020.   \n[33] E. Garc\u00eda-Mart\u00edn, C. F. Rodrigues, G. Riley, and H. Grahn. Estimation of Energy Consumption in Machine Learning. Journal of Parallel and Distributed Computing, 134:75\u201388, 2019.   \n[34] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer. A Survey of Quantization Methods for Efficient Neural Network Inference. In Low-Power Computer Vision, pages 291\u2013326. Chapman and Hall/CRC, 2022.   \n[35] C. Grimm and N. Verma. Neural Network Training on In-Memory-Computing Hardware With Radix-4 Gradients. IEEE Transactions on Circuits and Systems I: Regular Papers I, 69(10):4056\u20134068, 2022.   \n[36] N. Guo, J. Bethge, C. Meinel, and H. Yang. Join the High Accuracy Club on ImageNet with A Binary Neural Network Ticket. arXiv preprint arXiv:2211.12933, 2022.   \n[37] S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan. Deep Learning with Limited Numerical Precision. In Proceedings of the 32nd International Conference on Machine Learning, volume 37, pages 1737\u20131746, Lille, France, 07\u201309 Jul 2015. PMLR.   \n[38] S. Han, H. Mao, and W. J. Dally. Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding. In International Conference on Learning Representations, 2015.   \n[39] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning for Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.   \n[40] D. O. Hebb. The Organization of Behavior: A Neuropsychological Theory. Psychology press, 2005.   \n[41] K. Helwegen, J. Widdicombe, L. Geiger, Z. Liu, K.-T. Cheng, and R. Nusselder. Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[42] M. Horowitz. 1.1 Computing\u2019s Energy Problem (and What We Can Do about It). In 2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC), pages 10\u201314, 2014.   \n[43] L. Hou, Q. Yao, and J. T. Kwok. Loss-aware Binarization of Deep Networks. In International Conference on Learning Representations, 2016.   \n[44] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. arXiv preprint arXiv:1704.04861, 2017.   \n[45] L. Hoyer, D. Dai, and L. Van Gool. DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9924\u20139935, 2022.   \n[46] J.-B. Huang, A. Singh, and N. Ahuja. Single Image Super-Resolution from Transformed Self-Exemplars. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.   \n[47] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio. Binarized Neural Networks. In Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016.   \n[48] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio. Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations. The Journal of Machine Learning Research, 18(1):6869\u20136898, 2017.   \n[49] S. Ioffe and C. Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In Proceedings of the 32nd International Conference on Machine Learning, volume 37, pages 448\u2013456, Lille, France, 07\u201309 Jul 2015. PMLR.   \n[50] R. Ito and T. Saito. Dynamic Binary Neural Networks and Evolutionary Learning. In The 2010 International Joint Conference on Neural Networks, pages 1\u20135. IEEE, 2010.   \n[51] Q. Jin, J. Ren, R. Zhuang, S. Hanumante, Z. Li, Z. Chen, Y. Wang, K. Yang, and S. Tulyakov. F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization. In International Conference on Learning Representations, 2021.   \n[52] C. Jordan. Calculus of Finite Differences. Chelsea Publishing Company, New York, 2nd edition, 1950.   \n[53] S. P. Karimireddy, Q. Rebjock, S. Stich, and M. Jaggi. Error Feedback Fixes SignSGD and other Gradient Compression Schemes. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pages 3252\u20133261. PMLR, 09\u201315 Jun 2019.   \n[54] D. P. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. In International Conference on Learning Representations, 2015.   \n[55] A. Krizhevsky and G. Hinton. Learning Multiple Layers of Features from Tiny Images. Master\u2019s thesis, Department of Computer Science, University of Toronto, 2009.   \n[56] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. In Advances in Neural Information Processing Systems, volume 25. Curran Associates, Inc., 2012.   \n[57] H. Kwon, P. Chatarasi, M. Pellauer, A. Parashar, V. Sarkar, and T. Krishna. Understanding Reuse, Performance, and Hardware Cost of DNN Dataflow: A Data-centric Approach. In Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture, pages 754\u2013768, 2019.   \n[58] I. Lavi, S. Avidan, Y. Singer, and Y. Hel-Or. Proximity Preserving Binary Code Using Signed Graph-Cut. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 4535\u20134544, April 2020.   \n[59] Y. LeCun, Y. Bengio, and G. Hinton. Deep Learning. Nature, 521(7553):436\u2013444, 2015.   \n[60] C. Lee, H. Kim, E. Park, and J.-J. Kim. INSTA-BNN: Binary Neural Network with INSTAnce-aware Threshold. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 17325\u201317334, October 2023.   \n[61] H. Li, S. De, Z. Xu, C. Studer, H. Samet, and T. Goldstein. Training Quantized Nets: A Deeper Understanding. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   \n[62] Z. Li and C. M. De Sa. Dimension-Free Bounds for Low-Precision Training. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[63] H. Liao, J. Tu, J. Xia, H. Liu, X. Zhou, H. Yuan, and Y. Hu. Ascend: a Scalable and Unified Architecture for Ubiquitous Deep Neural Network Computing $:$ Industry Track Paper. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA), pages 789\u2013801, 2021.   \n[64] B. Lim, S. Son, H. Kim, S. Nah, and K. Mu Lee. Enhanced Deep Residual Networks for Single Image Super-Resolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2017.   \n[65] M. Lin, R. Ji, Z. Xu, B. Zhang, Y. Wang, Y. Wu, F. Huang, and C.-W. Lin. Rotated Binary Neural Network. In Advances in Neural Information Processing Systems, volume 33, pages 7474\u20137485. Curran Associates, Inc., 2020.   \n[66] C. Liu, W. Ding, P. Chen, B. Zhuang, Y. Wang, Y. Zhao, B. Zhang, and Y. Han. RB-Net: Training Highly Accurate and Efficient Binary Neural Networks with Reshaped Point-wise Convolution and Balanced activation. IEEE Transactions on Circuits and Systems for Video Technology, 32(9):6414\u20136424, 2022.   \n[67] Z. Liu, B. Oguz, A. Pappu, L. Xiao, S. Yih, M. Li, R. Krishnamoorthi, and Y. Mehdad. BiT: Robustly Binarized Multi-distilled Transformer. In Advances in Neural Information Processing Systems, volume 35, pages 14303\u201314316. Curran Associates, Inc., 2022.   \n[68] Z. Liu, Z. Shen, M. Savvides, and K.-T. Cheng. ReActNet: Towards Precise Binary Neural Network with Generalized Activation Functions. In Proceedings of the European Conference on Computer Vision (ECCV), August 2020.   \n[69] Z. Liu, B. Wu, W. Luo, X. Yang, W. Liu, and K.-T. Cheng. Bi-Real Net: Enhancing the Performance of 1-bit CNNs with Improved Representational Capability and Advanced Training Algorithm. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018.   \n[70] J. Long, E. Shelhamer, and T. Darrell. Fully Convolutional Networks for Semantic Segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.   \n[71] I. Loshchilov and F. Hutter. Decoupled Weight Decay Regularization. In International Conference on Learning Representations, 2017.   \n[72] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A Database of Human Segmented Natural Images and its Application to Evaluating Segmentation Algorithms and Measuring Ecological Statistics. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), July 2001.   \n[73] B. Martinez, J. Yang, A. Bulat, and G. Tzimiropoulos. Training Binary Neural Networks with Real-tobinary Convolutions. In International Conference on Learning Representations, 2020.   \n[74] X. Mei, K. Zhao, C. Liu, and X. Chu. Benchmarking the Memory Hierarchy of Modern GPUs. In IFIP International Conference on Network and Parallel Computing, pages 144\u2013156. Springer, 2014.   \n[75] G. Morse and K. O. Stanley. Simple Evolutionary Optimization Can Rival Stochastic Gradient Descent in Neural Networks. In Proceedings of the Genetic and Evolutionary Computation Conference 2016, pages 477\u2013484, 2016.   \n[76] V. M. Nguyen. Boolean Variation and Boolean Logic BackPropagation. arXiv:2311.07427, May 2024.   \n[77] G. Nie, L. Xiao, M. Zhu, D. Chu, Y. Shen, P. Li, K. Yang, L. Du, and B. Chen. Binary Neural Networks as a General-propose Compute Paradigm for On-device Computer Vision. arXiv preprint arXiv:2202.03716, 2022.   \n[78] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[79] F. Petersen, C. Borgelt, H. Kuehne, and O. Deussen. Deep differentiable logic gate networks. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 2006\u20132018. Curran Associates, Inc., 2022.   \n[80] C. Philippenko and A. Dieuleveut. Bidirectional Compression in Heterogeneous Settings for Distributed or Federated Learning with Partial Participation: Tight Convergence Guarantees. arXiv preprint arXiv:2006.14591, 2020.   \n[81] B. T. Polyak. Introduction to Optimization. Translations Series in Mathematics and Engineering. Optimization Software Inc. Publication Division, New York, 1987.   \n[82] H. Qin, Y. Ding, M. Zhang, Q. Yan, A. Liu, Q. Dang, Z. Liu, and X. Liu. BiBERT: Accurate Fully Binarized BERT. In International Conference on Learning Representations, 2022.   \n[83] H. Qin, R. Gong, X. Liu, X. Bai, J. Song, and N. Sebe. Binary Neural Networks: A Survey. Pattern Recognition, 105:107281, 2020.   \n[84] H. Qin, R. Gong, X. Liu, M. Shen, Z. Wei, F. Yu, and J. Song. Forward and Backward Information Retention for Accurate Binary Neural Networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.   \n[85] H. Qin, M. Zhang, Y. Ding, A. Li, Z. Cai, Z. Liu, F. Yu, and X. Liu. BiBench: Benchmarking and Analyzing Network Binarization. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 28351\u201328388. PMLR, 23\u201329 Jul 2023.   \n[86] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks. In Proceedings of the European Conference on Computer Vision (ECCV), October 2016.   \n[87] A. Sebastian, M. Le Gallo, R. Khaddam-Aljameh, and E. Eleftheriou. Memory Devices and Applications for in-memory Computing. Nature Nanotechnology, 15(7):529\u2013544, 2020.   \n[88] Y. S. Shao and D. Brooks. Energy Characterization and Instruction-Level Energy Model of Intel\u00b4s Xeon Phi Processor. In International Symposium on Low Power Electronics and Design (ISLPED), pages 389\u2013394, 2013.   \n[89] J. Sim, S. Lee, and L.-S. Kim. An Energy-Efficient Deep Convolutional Neural Network Inference Processor With Enhanced Output Stationary Dataflow in 65-nm CMOS. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 28(1):87\u2013100, 2019.   \n[90] K. Simonyan and A. Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. In International Conference on Learning Representations, 2015.   \n[91] D. Soudry, I. Hubara, and R. Meir. Expectation Backpropagation: Parameter-Free Training of Multilayer Neural Networks with Continuous or Discrete Weights. In Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014.   \n[92] E. Strubell, A. Ganesh, and A. McCallum. Energy and Policy Considerations for Deep Learning in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3645\u20133650, Florence, Italy, Jul 2019. Association for Computational Linguistics.   \n[93] X. Sun, N. Wang, C.-Y. Chen, J. Ni, A. Agrawal, X. Cui, S. Venkataramani, K. El Maghraoui, V. V. Srinivasan, and K. Gopalakrishnan. Ultra-Low Precision 4-bit Training of Deep Neural Networks. In Advances in Neural Information Processing Systems, volume 33, pages 1796\u20131807. Curran Associates, Inc., 2020.   \n[94] V. Sze, Y.-H. Chen, T.-J. Yang, and J. S. Emer. Efficient Processing of Deep Neural Networks: A Tutorial and Survey. Proceedings of the IEEE, 105(12):2295\u20132329, 2017.   \n[95] V. Sze, Y.-H. Chen, T.-J. Yang, and J. S. Emer. How to Evaluate Deep Neural Network Processors: Tops/w (Alone) Considered Harmful. IEEE Solid-State Circuits Magazine, 12(3):28\u201341, 2020.   \n[96] M. Tan and Q. Le. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 6105\u20136114. PMLR, 09\u201315 Jun 2019.   \n[97] R. Timofte, E. Agustsson, L. Van Gool, M.-H. Yang, L. Zhang, B. Lim, et al. NTIRE 2017 Challenge on Single Image Super-Resolution: Methods and Results. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, July 2017.   \n[98] H. Touvron, A. Vedaldi, M. Douze, and H. Jegou. Fixing the Train-Test Resolution Discrepancy. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[99] Z. Tu, X. Chen, P. Ren, and Y. Wang. AdaBin: Improving Binary Neural Networks with Adaptive Binary Sets. In Proceedings of the European Conference on Computer Vision (ECCV), October 2022.   \n[100] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin. Attention is All you Need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   \n[101] N. Verma, H. Jia, H. Valavi, Y. Tang, M. Ozatay, L.-Y. Chen, B. Zhang, and P. Deaville. In-Memory Computing: Advances and Prospects. IEEE Solid-State Circuits Magazine, 11(3):43\u201355, 2019.   \n[102] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. In International Conference on Learning Representations, 2019.   \n[103] E. Wang, J. J. Davis, D. Moro, P. Zielinski, J. J. Lim, C. Coelho, S. Chatterjee, P. Y. Cheung, and G. A. Constantinides. Enabling Binary Neural Network Training on the Edge. In Proceedings of the 5th International Workshop on Embedded and Mobile Deep Learning, pages 37\u201338, 2021.   \n[104] J. Wangni, J. Wang, J. Liu, and T. Zhang. Gradient sparsification for communication-efficient distributed optimization. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \n[105] S. Williams, A. Waterman, and D. Patterson. Roofline: An Insightful Visual Performance Model for Multicore Architectures. Communications of the ACM, 52(4):65\u201376, apr 2009.   \n[106] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. SmoothQuant: Accurate and Efficient PostTraining Quantization for Large Language Models. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 38087\u201338099. PMLR, 23\u201329 Jul 2023.   \n[107] X. Xing, Y. Li, W. Li, W. Ding, Y. Jiang, Y. Wang, J. Shao, C. Liu, and X. Liu. Towards Accurate Binary Neural Networks via Modeling Contextual Dependencies. In Proceedings of the European Conference on Computer Vision (ECCV), October 2022.   \n[108] T.-J. Yang, Y.-H. Chen, J. Emer, and V. Sze. A Method to Estimate the Energy Consumption of Deep Neural Networks. In 2017 51st Asilomar Conference on Signals, Systems, and Computers, pages 1916\u20131920, October 2017.   \n[109] T.-J. Yang, Y.-H. Chen, and V. Sze. Designing Energy-Efficient Convolutional Neural Networks Using Energy-Aware Pruning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.   \n[110] X. Yang, M. Gao, Q. Liu, J. Setter, J. Pu, A. Nayak, S. Bell, K. Cao, H. Ha, P. Raina, et al. Interstellar: Using Halide\u2019s Scheduling Language to Analyze DNN Accelerators. In Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, pages 369\u2013383, 2020.   \n[111] Z. Yang, Y. Wang, K. Han, C. XU, C. Xu, D. Tao, and C. Xu. Searching for Low-Bit Weights in Quantized Neural Networks. In Advances in Neural Information Processing Systems, volume 33, pages 4091\u20134102. Curran Associates, Inc., 2020.   \n[112] S. Yu and P.-Y. Chen. Emerging Memory Technologies: Recent Trends and Prospects. IEEE Solid-State Circuits Magazine, 8(2):43\u201356, 2016.   \n[113] R. Zeyde, M. Elad, and M. Protter. On Single Image Scale-up using Sparse-Representations. In Curves and Surfaces: 7th International Conference, Avignon, France, June 24-30, 2010, Revised Selected Papers 7, pages 711\u2013730. Springer, 2012.   \n[114] D. Zhang, J. Yang, D. Ye, and G. Hua. LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018.   \n[115] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. Mixup: Beyond Empirical Risk Minimization. In International Conference on Learning Representations, 2018.   \n[116] R. Zhang, A. G. Wilson, and C. De Sa. Low-Precision Stochastic Gradient Langevin Dynamics. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162, pages 26624\u201326644. PMLR, 17\u201323 Jul 2022.   \n[117] Y. Zhang, Z. Zhang, and L. Lew. PokeBNN: A Binary Pursuit of Lightweight Accuracy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12475\u201312485, June 2022.   \n[118] Z. Zhang. Derivation of Backpropagation in Convolutional Neural Network (CNN). University of Tennessee, Knoxville, TN, 22:23, 2016.   \n[119] B. Zhuang, C. Shen, M. Tan, L. Liu, and I. Reid. Structured Binary Neural Networks for Accurate Image Classification and Semantic Segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A Details of the Boolean Variation Method 18 ", "page_idx": 17}, {"type": "text", "text": "A.1 Boolean Variation Calculus 18   \nA.2 Convergence Proof 22 ", "page_idx": 17}, {"type": "text", "text": "B Code Sample of Core Implementation 26 ", "page_idx": 17}, {"type": "text", "text": "C Training Regularization 28 ", "page_idx": 17}, {"type": "text", "text": "C.1 Assumptions and Notations 28   \nC.2 Backpropagation Scaling 29   \nC.3 BackPropagation Through Boolean Activation Function 31   \nD Experimental Details 31   \nD.1 Image Classification . 31   \nD.2 Image Super-resolution 34   \nD.3 Semantic Segmentation 34   \nD.4 Boolean BERT Fine-tuning 40   \nE Energy Estimation 40   \nE.1 Hardware Specification 40   \nE.2 Compute Energy 41   \nE.3 Memory Energy . . 41 ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "A Details of the Boolean Variation Method ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A.1 Boolean Variation Calculus ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "This section provides supplementary details of the Boolean Variation method, a sole development can be found in [76]. With reference to Example 3.10, Table 8 gives the variation truth table of $f(x)=\\mathbf{xor}(a,x)$ , for $a,x\\in\\mathbb{B}$ . ", "page_idx": 17}, {"type": "table", "img_path": "DO9wPZOPjk/tmp/07b564077959029f3be083ab40f8f194561f42c9313175ca6fa3c9cd2c61f036.jpg", "table_caption": ["Table 8: Variation truth table of $f(x)=\\mathbf{xor}(a,x),a,x\\in\\mathbb{B}.$ . "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Definition A.1 (Type conversion). Define: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname{p}\\colon\\mathbb{N}\\to\\mathbb{L}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\nx\\mapsto\\mathrm{p}(x)={\\left\\{\\begin{array}{l l}{\\mathrm{T},}&{i f\\,x>0,}\\\\ {0,}&{i f\\,x=0,}\\\\ {\\mathrm{F},}&{i f\\,x<0.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{e}\\colon\\mathbb{L}\\rightarrow\\mathbb{N}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\na\\mapsto\\mathrm{e}(a)=\\left\\{{+1,\\begin{array}{l l}{{i f a=\\mathrm{T},}}\\\\ {{0,}}\\\\ {{-1,}}\\end{array}}\\right.{i f a=\\mathrm{0},}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "p projects a numeric type in logic, and e embeds a logic type in numeric. The following properties are straightforward: ", "page_idx": 18}, {"type": "text", "text": "Proposition A.2. The following properties hold: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{l.\\ \\ \\forall x,y\\in\\mathbb{N}{\\colon}\\,\\mathbf{p}(x y)=\\mathbf{xnor}(\\mathrm{p}(x),\\mathrm{p}(y)).}\\\\ &{2.\\ \\forall a,b\\in\\mathbb{L}{\\colon}\\,\\mathrm{e}(\\mathbf{xnor}(a,b))=\\mathrm{e}(a)\\,\\mathrm{e}(b).}\\\\ &{3.\\ \\forall x,y\\in\\mathbb{N}{\\colon}\\,x=y\\Leftrightarrow|x|=|y|\\,a n d\\ \\mathrm{p}(x)=\\mathrm{p}(y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In particular, property Proposition A.2(2) implies that by the embedding map $\\mathrm{e}(\\cdot)$ , we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{(\\{\\mathrm{T},\\mathrm{F}\\},\\mathbf{xor})\\cong(\\{\\pm1\\},-\\times),}}\\\\ {{(\\{\\mathrm{T},\\mathrm{F}\\},\\mathbf{xnor})\\cong(\\{\\pm1\\},\\times),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\cong\\operatorname{and}\\times$ stand for isomorphic relation, and the real multiplication, resp. A consequence is that by $\\mathrm{e}(\\cdot)$ , a computing sequence of pointwise XOR/XNOR, counting, and majority vote is equivalent to a sequence of pointwise multiplications and accumulation performed on the embedded data. This property will be used in Appendices A.2 and $\\mathrm{C}$ for studying Boolean method using some results from BNNs literature and real analysis. ", "page_idx": 18}, {"type": "text", "text": "Proposition A.3. The following properties hold: ", "page_idx": 18}, {"type": "text", "text": "$I.\\,\\,\\,a\\in\\mathbb{L},\\,x\\in\\mathbb{N}\\colon\\mathbf{xnor}(a,x)=\\mathrm{e}(a)x.$   \n2. $x,y\\in\\mathbb{N}\\colon\\mathbf{xnor}(x,y)=x y.$ .   \n3. $x\\in\\{\\mathbb{L},\\mathbb{N}\\},\\,y,z\\in\\mathbb{N}\\colon\\mathbf{xnor}(x,y+z)=\\mathbf{xnor}(x,y)+\\mathbf{xnor}(x,z).$   \n4. $x\\in\\{\\mathbb{L},\\mathbb{N}\\},\\,y,\\lambda\\in\\mathbb{N}{\\cdot}\\,\\mathbf{xnor}(x,\\lambda y)=\\lambda\\mathbf{xnor}(x,y).$   \n5. $x\\in\\{\\mathbb{L},\\mathbb{N}\\},\\,y\\in\\mathbb{N}\\colon{\\mathbf{xor}}(x,y)=-{\\mathbf{xnor}}(x,y).$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. The proof follows definitions 3.5 and A.1. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Following Definition 3.1 we have $\\forall t\\ \\in\\ \\mathbb{M}$ , $\\mathbf{xnor}(\\mathrm{T},t)\\;=\\;t$ , $\\mathbf{xnor}(\\mathrm{F},t)\\;=\\;\\neg t.$ , and $\\mathbf{xnor}(0,t)\\,=\\,0$ . Put $v\\,=\\,{\\bf x n o r}(a,x)$ . We have $|v|\\,=\\,|x|$ and $\\mathrm{p}(v)\\,=\\,\\mathbf{xnor}(a,\\mathrm{p}(x))$ . Hence, $a~=~0~\\Rightarrow~\\mathrm{p}(v)~=~0~\\Rightarrow~\\dot{v}~=~0$ ; $a\\ =\\ \\mathrm{T}\\ \\Rightarrow\\ \\mathrm{p}(v)\\ =\\ \\mathrm{p}(x)\\ \\Rightarrow\\ v\\ =\\ x$ ; $a=\\operatorname{F}\\Rightarrow\\operatorname{p}(v)=\\lnot\\operatorname{p}(x)\\Rightarrow v=-x$ . Hence (1).   \n\u2022 The result is trivial if $x\\,=\\,0$ or $y\\,=\\,0$ . For $x,y\\ \\ne\\ 0$ , put $v\\,=\\,\\mathbf{xnor}(x,y)$ , we have $|v|\\,=\\,|x||y|$ and $\\mathrm{p}(v)\\,=\\,\\mathbf{xnor}(\\mathrm{p}(x),\\mathrm{p}(y))$ . According to Definition A.1, if $\\mathrm{sign}(x)\\,=$ $\\mathrm{sign}(y)$ , we have $\\mathrm{p}(v)\\,=\\,\\mathrm{T}\\,\\Rightarrow\\,v\\,=\\,|x||y|\\,=\\,x y$ . Otherwise, i.e., $\\mathrm{sign}(x)\\,=\\,-\\,\\mathrm{sign}(y)$ , $\\mathrm{p}(v)=\\mathrm{F}\\Rightarrow v=-|x||y|=x y$ . Hence (2).   \n\u2022 (3) and (4) follow (1) for $x\\in\\mathbb{L}$ and follow (2) for $x\\in\\mathbb{N}$ .   \n\u2022 For (5), write $u\\;=\\;\\mathbf{xor}(x,y)$ and $\\boldsymbol{v}\\;=\\;\\mathbf{xnor}(x,y)$ , we have $|u|\\;=\\;|v|$ and $\\mathrm{p}(u)\\,=$ $\\mathbf{xor}(\\mathrm{p}(x),\\mathrm{p}(y))\\,=\\,\\neg\\mathbf{xnor}(\\mathrm{p}(x),\\mathrm{p}(y))\\,=\\,\\neg\\,\\mathrm{p}(v)$ . Thus, $\\mathrm{sign}(u)\\,=\\,-\\,\\mathrm{sign}(v)\\,\\Rightarrow\\,u\\,=$ $-v$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Proposition A.4. For $f,g\\in\\mathcal{F}(\\mathbb{B},\\mathbb{B}),\\,\\forall x,y\\in\\mathbb{B}$ the following properties hold: ", "page_idx": 19}, {"type": "text", "text": "1. $\\delta f(x\\to y)=\\operatorname{xnor}(\\delta(x\\to y),f^{\\prime}(x)).$   \n2. $(\\neg f)^{\\prime}(x)=\\neg f^{\\prime}(x)$ .   \n3. $(g\\circ f)^{\\prime}(x)=\\operatorname{\\mathbf{xnor}}(g^{\\prime}(f(x)),f^{\\prime}(x)).$ ", "page_idx": 19}, {"type": "text", "text": "Proof. The proof is by definition: ", "page_idx": 19}, {"type": "text", "text": "1. $\\forall x,y\\in\\mathbb{B}$ , there are two cases. If $y=x$ , then the result is trivial. Otherwise, i.e., $y=\\lnot x$ , by definition we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{f^{\\prime}(x)=\\mathbf{xnor}(\\delta(x\\to\\lnot x),\\delta f(x\\to\\lnot x))}\\\\ &{\\Leftrightarrow}&{\\delta f(x\\to\\lnot x)=\\mathbf{xnor}(\\delta(x\\to\\lnot x),f^{\\prime}(x)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence the result. ", "page_idx": 19}, {"type": "text", "text": "2. $\\forall x,y\\in\\mathbb{B}$ , it is easy to verify by truth table that $\\delta(\\lnot f(x\\to y))=\\lnot\\delta f(x\\to y)$ . Hence, by definition, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\neg f)^{\\prime}(x)=\\mathbf{xnor}(\\delta(x\\to\\neg x),\\delta(\\neg f(x\\to\\neg x)))}\\\\ &{\\qquad\\qquad=\\mathbf{xnor}(\\delta(x\\to\\neg x),\\neg\\delta f(x\\to\\neg x))}\\\\ &{\\qquad\\qquad=\\neg\\mathbf{xnor}(\\delta(x\\to\\neg x),\\delta f(x\\to\\neg x))}\\\\ &{\\qquad\\qquad=\\neg f^{\\prime}(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "3. Using definition, property (i), and associativity of xnor, $\\forall x\\in\\mathbb{B}$ we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(g\\circ f)^{\\prime}(x)=\\mathbf{xnor}(\\delta(x\\to\\neg x),\\delta g(f(x)\\to f(\\neg x)))}\\\\ &{\\qquad\\qquad=\\mathbf{xnor}(\\delta(x\\to\\neg x),\\mathbf{xnor}(\\delta f(x\\to\\neg x),g^{\\prime}(f(x))))}\\\\ &{\\qquad\\qquad=\\mathbf{xnor}(g^{\\prime}(f(x)),\\mathbf{xnor}(\\delta(x\\to\\neg x),\\delta f(x\\to\\neg x)))}\\\\ &{\\qquad\\qquad=\\mathbf{xnor}(g^{\\prime}(f(x)),f^{\\prime}(x)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proposition A.5. For $f\\in\\mathcal{F}(\\mathbb{B},\\mathbb{N})$ , the following properties hold: ", "page_idx": 19}, {"type": "text", "text": "1 $\\mathbf{\\nabla}\\cdot\\mathbf{\\lambda}x,y\\in\\mathbb{B}\\colon\\delta f(x\\to y)=\\mathbf{xnor}(\\delta(x\\to y),f^{\\prime}(x)).$   \n2. $x\\in\\mathbb{B},\\,\\alpha\\in\\mathbb{N}\\colon(\\alpha f)^{\\prime}(x)=\\alpha f^{\\prime}(x).$ .   \n3. $x\\in\\mathbb{B}$ , $g\\in\\mathcal{F}(\\mathbb{B},\\mathbb{N})$ : $(f+g)^{\\prime}(x)=f^{\\prime}(x)+g^{\\prime}(x).$ ", "page_idx": 19}, {"type": "text", "text": "Proof. The proof is as follows: ", "page_idx": 19}, {"type": "text", "text": "1. For $x,y\\in\\mathbb{B}$ . Firstly, the result is trivial if $y=x$ . For $y\\ne x$ , i.e., $y=\\lnot x$ , by definition: ", "page_idx": 19}, {"type": "equation", "text": "$$\nf^{\\prime}(x)=\\mathbf{xnor}(\\delta(x\\rightarrow\\neg x),\\delta f(x\\rightarrow\\neg x)).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{p}(f^{\\prime}(x))=\\mathbf{xnor}(\\delta(x\\to\\neg x),\\mathrm{p}(\\delta f(x\\to\\neg x)))\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Leftrightarrow\\ }&{{}\\mathbf{p}(\\delta f(x\\to\\lnot x))=\\mathbf{xnor}(\\delta(x\\to\\lnot x),\\mathbf{p}(f^{\\prime}(x)))}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathrm{p}(\\cdot)$ is the logic projector Eq. 13. Thus, $\\delta f(x\\rightarrow\\neg x)=\\mathbf{xnor}(\\delta(x\\rightarrow\\neg x),f^{\\prime}(x)).$ Hence the result. ", "page_idx": 19}, {"type": "text", "text": "2. Firstly $\\forall x,y\\in\\mathbb{B}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\delta(\\alpha f(x\\rightarrow y))=\\alpha f(y)-\\alpha f(x)=\\alpha\\delta f(x\\rightarrow y).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, by definition, ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{(\\alpha f)^{\\prime}(x)=\\mathbf{xnor}(\\delta(x\\to\\neg x),\\delta(\\alpha f(x\\to\\neg x)))}\\\\ &{\\qquad\\qquad=\\mathbf{xnor}(\\delta(x\\to\\neg x),\\alpha\\delta f(x\\to\\neg x))}\\\\ &{\\qquad\\qquad=\\alpha\\,\\mathbf{xnor}(\\delta(x\\to\\neg x),\\delta f(x\\to\\neg x)),{\\mathrm{~due~to~Proposition~A.3(4)}}}\\\\ &{\\qquad\\qquad=\\alpha f^{\\prime}(x).}\\end{array}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "3. For $f,g\\in\\mathcal{F}(\\mathbb{B},\\mathbb{N})$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(f+g)^{\\prime}(x)=\\mathbf{xnor}(\\delta(x\\to\\neg x),\\delta(f+g)(x\\to\\neg x))}\\\\ &{\\qquad\\qquad=\\mathbf{xnor}(\\delta(x\\to\\neg x),\\delta f(x\\to\\neg x)+\\delta g(x\\to\\neg x))}\\\\ &{\\qquad\\qquad\\overset{(*)}{=}\\mathbf{xnor}(\\delta(x\\to\\neg x),\\delta f(x\\to\\neg x))+\\mathbf{xnor}(\\delta(x\\to\\neg x),\\delta g(x\\to\\neg x)),}\\\\ &{\\qquad\\qquad=f^{\\prime}(x)+g^{\\prime}(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $(*)$ is due to Proposition A.3(3). ", "page_idx": 20}, {"type": "text", "text": "Proposition A.6 (Composition rules). The following properties hold: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{I.~~F o r~\\mathbb{B}\\xrightarrow{f}\\mathbb{B}\\xrightarrow{g}\\mathbb{D}\\colon(g\\circ f)^{\\prime}(x)=\\mathbf{xnor}(g^{\\prime}(f(x)),f^{\\prime}(x)),\\forall x\\in\\mathbb{B}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n(g\\circ f)^{\\prime}(x)=\\mathbf{xnor}(g^{\\prime}(f(x)),f^{\\prime}(x))\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. The proof is as follows. ", "page_idx": 20}, {"type": "text", "text": ". The case of $\\mathbb{B}\\xrightarrow{f}\\mathbb{B}\\xrightarrow{g}\\mathbb{B}$ is obtained from Proposition A.4(3). For $\\mathbb{B}\\ {\\xrightarrow{f}}\\mathbb{B}\\ {\\xrightarrow{g}}\\ \\mathbb{N}$ , by using Proposition A.5(1), the proof is similar to that of Proposition A.4(3). ", "page_idx": 20}, {"type": "text", "text": "2. By definition, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n(g\\circ f)^{\\prime}(x)=\\mathbf{xnor}(\\delta(x\\to\\lnot x),\\delta g(f(x)\\to f(\\lnot x))).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using property (1) of Proposition A.5, we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f(\\neg x)=f(x)+\\delta f(x\\to\\neg x)}\\\\ {\\qquad\\qquad=f(x)+\\mathbf{xnor}(\\delta(x\\to\\neg x),f^{\\prime}(x)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Applying Eq. 18 back to Eq. 17, the result is trivial if $f^{\\prime}(x)=0$ . The remaining case is $|\\bar{f^{\\prime}}(x)|\\bar{=}\\,1$ for which we have $\\mathbf{xnor}(\\delta(x\\to\\neg x),f^{\\prime}(\\dot{x}))=\\pm1$ . First, for $\\mathbf{xnor}(\\delta(x\\rightarrow$ $\\neg x),f^{\\prime}(x))=1$ , we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta g(f(x)\\rightarrow f(\\neg x))=\\delta g(f(x)\\rightarrow f(x)+1)}\\\\ &{\\qquad\\qquad\\qquad=g^{\\prime}(f(x))}\\\\ &{\\qquad\\qquad\\qquad=\\mathbf{xnor}(g^{\\prime}(f(x)),1)}\\\\ &{\\qquad\\qquad\\qquad=\\mathbf{xnor}(g^{\\prime}(f(x)),\\mathbf{xnor}(\\delta(x\\rightarrow\\neg x),f^{\\prime}(x))).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Substitute Eq. 19 back to Eq. 17, we obtain: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(g\\circ f)^{\\prime}(x)=\\mathbf{xnor}(\\delta(x\\to\\neg x),\\delta g(f(x)\\to f(\\neg x)))}\\\\ &{\\qquad\\qquad=\\mathbf{xnor}(\\delta(x\\to\\neg x),\\mathbf{xnor}(g^{\\prime}(f(x)),\\mathbf{xnor}(\\delta(x\\to\\neg x),f^{\\prime}(x))))}\\\\ &{\\qquad\\qquad=\\mathbf{xnor}(g^{\\prime}(f(x)),f^{\\prime}(x)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where that last equality is by the associativity of xnor and that $\\mathbf{xnor}(x,x)=\\mathrm{T}$ for $x\\in\\mathbb{B}$ . Similarly, for $\\mathbf{xnor}(\\delta(x\\to\\neg x),f^{\\prime}(x))=\\dot{-}1$ , we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta g(f(x)\\rightarrow f(\\neg x))=\\delta g(f(x)\\rightarrow f(x)-1)}\\\\ &{\\qquad\\qquad\\qquad=-g^{\\prime}(f(x)-1)}\\\\ &{\\qquad\\qquad\\qquad=\\mathbf{xnor}(g^{\\prime}(f(x)-1),-1)}\\\\ &{\\qquad\\qquad\\qquad=\\mathbf{xnor}(g^{\\prime}(f(x)-1),\\mathbf{xnor}(\\delta(x\\rightarrow\\neg x),f^{\\prime}(x))).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Substitute Eq. 20 back to Eq. 17 and use the assumption that $g^{\\prime}(f(x))=g^{\\prime}(f(x)-1)$ , we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(g\\circ f)^{\\prime}(x)=\\mathbf{xnor}(\\delta(x\\to\\neg x),\\delta g(f(x)\\to f(\\neg x)))}\\\\ &{\\qquad\\qquad=\\mathbf{xnor}(\\delta(x\\to\\neg x),\\mathbf{xnor}(g^{\\prime}(f(x)-1),\\mathbf{xnor}(\\delta(x\\to\\neg x),f^{\\prime}(x))))}\\\\ &{\\qquad\\qquad=\\mathbf{xnor}(g^{\\prime}(f(x)),f^{\\prime}(x)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence the result. ", "page_idx": 20}, {"type": "text", "text": "The results of Theorem 3.12 are from Propositions A.4 to A.6. ", "page_idx": 20}, {"type": "text", "text": "A.2 Convergence Proof ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To the best of our knowledge, in the quantized neural network literature and in particular BNN, one can only prove the convergence up to an irreducible error floor [61]. This idea has been extended to SVRG [25], and recently to SGLD in [116], which is also up to an error limit. ", "page_idx": 21}, {"type": "text", "text": "In this section we provide complexity bounds for Boolean Logic in a smooth non-convex environment.   \nWe introduce an abstraction to model its optimization process and prove its convergence. ", "page_idx": 21}, {"type": "text", "text": "A.2.1 Continuous Abstraction ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Boolean optimizer is discrete, proving its convergence directly is a hard problem. The idea is to find a continuous equivalence so that some proof techniques existing from the BNN and quantized neural networks literature can be employed. We also abstract the logic optimization rules as compressor $Q_{0}(\\right),Q_{1}(\\right)$ , and define gradient accumulator $a_{t}$ as $a_{t+1}\\,=\\,a_{t}\\,+\\,\\varphi(q_{t})$ . When $\\eta$ is constant, we recover the definition (10) and obtain $m_{t}=\\eta a_{t}$ . Our analysis is based on the following standard non-convex assumptions on $f$ : ", "page_idx": 21}, {"type": "text", "text": "A. 1. Uniform Lower Bound: There exists $f_{*}\\in\\mathbb{R}$ s.t. $f(w)\\geq f_{*}$ , $\\forall w\\in\\mathbb{R}^{d}$ .   \nA. 2. Smooth Derivatives: The gradient $\\nabla f(w)$ is $L$ -Lipschitz continuous for some $L>0$ , i.e., $\\forall w,\\forall v\\in\\mathbb{R}^{d}$ $\\mathbb{R}^{d}\\!:\\,\\|\\nabla f(w)-\\nabla f(v)\\|\\leq L\\|w-v\\|$ .   \nA. 3. Bounded Variance: The variance of the stochastic gradients is bounded by some $\\sigma^{2}>0$ , i.e., $\\forall w\\in\\mathbb{R}^{d}\\!\\colon\\mathbb{E}\\Big[\\tilde{\\nabla}f(w)\\Big]=\\nabla f(w)\\,a n d\\,\\mathbb{E}\\Big[\\|\\tilde{\\nabla}f(w)\\|^{2}\\Big]\\leq\\sigma^{2}$ .   \nA. 4. Compressor: There exists $\\gamma<1\\;s.t.\\;\\forall w,\\forall v\\in\\mathbb{R}^{d}$ , $\\|Q_{1}(v,w)-v\\|^{2}\\leq\\gamma\\|v\\|^{2}$ .   \nA. 5. Bounded Accumulator: There exists $\\kappa\\in\\mathbb{R}_{+}^{*}$ s.t. $\\forall t$ and $\\forall i\\in[d]$ , we have $|a_{t}|_{i}\\leq\\kappa$ . A. 6. Stochastic Flipping Rule: For all $w\\in\\mathbb{R}^{d}$ , we have $\\mathbb{E}[Q_{0}(w)|w]=w$ . ", "page_idx": 21}, {"type": "text", "text": "In existing frameworks, quantity $\\tilde{\\nabla}f(\\cdot)$ denotes the stochastic gradient computed on a random minibatch of data. Boolean framework does not have the notion of gradient, it however has an optimization signal (i.e., $\\varphi({\\mathfrak{q}})$ or its accumulator $m_{t}$ (10)) that plays the same role as $\\tilde{\\nabla}f(\\cdot)$ . Therefore, these two notions, i.e., continuous gradient and Boolean optimization signal, can be encompassed into a generalized notion. That is the root to the following continuous relaxation in which $\\tilde{\\nabla}f(\\cdot)$ standards for the optimization signal computed on a random mini-batch of data. ", "page_idx": 21}, {"type": "text", "text": "Within this continuous relaxation framework, A. 6 expresses our assumption that the filpping rule is stochastic and unbiased. Note that this assumption is standard in the literature related to (stochastic) quantization, see e.g., [4, 81, 104]. ", "page_idx": 21}, {"type": "text", "text": "For reference, the original Boolean optimizer as formulated in $\\S\\ 3$ is summarized in Algorithm 2 in which $\\mathtt{f l i p}(w_{t},m_{t+1})$ flips weight and reset $(w_{t},m_{t+1})$ resets its accumulator when the flip condition is triggered. ", "page_idx": 21}, {"type": "table", "img_path": "DO9wPZOPjk/tmp/8bece99b0446f20929dce9ddffa3ca777abb3150d5fdc843dd79bdba661a035a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Algorithm 3 describes an equivalent formulation of Boolean optimizer. Therein, $Q_{0},Q_{1}$ are quantizers which are specified in the following. Note that EF-SIGNSGD (SIGNSGD with Error-Feedback) ", "page_idx": 21}, {"type": "text", "text": "algorithm from [53] is a particular case of this formulation with $Q_{0}(\\mathrm{)=Identity()}$ and $Q_{1}()=$ $\\mathrm{sign}()$ . For Boolean Logic abstraction, they are given by: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{Q_{1}(m_{t},w_{t})=w_{t}(\\mathtt{R e L u}(w_{t}m_{t}-1)+\\frac{1}{2}\\sin(w_{t}m_{t}-1)+\\frac{1}{2}),\\right.}\\\\ {\\left.Q_{0}(w_{t})=\\mathrm{sign}(w_{t}).\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The combination of $Q_{1}$ and $Q_{0}$ is crucial to take into account the reset property of the accumulator $m_{t}$ . Indeed in practice, $\\Delta_{t}\\;:=\\;Q_{1}(m_{t},w_{t})$ is always equal to 0 except when $|m_{t}|~>~1$ and $\\mathrm{sign}(m_{t})\\,=\\,\\mathrm{sign}(w_{t})$ (i.e., when the flipping rule is applied). As $w_{t}$ has only values in $\\{\\pm1\\}$ , $Q_{0}$ acts as identity function, except when $\\Delta_{t}$ is non-zero (i.e., when the flipping rule is applied). With the choices (21), we can identify ${\\sf f l i p}(w_{t},m_{t})\\,=\\,Q_{0}(w_{t}-Q_{1}(m_{t},\\bar{w_{t}}))$ . We do not have closed-form formula for $\\mathtt{r e s e t}(w_{t},m_{t+1})$ from Algorithm 2, but the residual errors $e_{t}$ play this role. Indeed, $e_{t+1}=m_{t}$ except when $\\Delta_{t}$ is non-zero (i.e., when the flipping rule is applied and $e_{t+1}$ is equal to 0). ", "page_idx": 22}, {"type": "text", "text": "The main difficulty in the analysis comes from the parameters quantization $Q_{0}(\\AA)$ . Indeed, we can follow the derivations in Appendix B.3 from [53] to bound the error term $\\mathbb{E}\\big[\\|e_{t}\\|^{2}\\big]$ , but we also have additional terms coming from the quantity: ", "page_idx": 22}, {"type": "equation", "text": "$$\nh_{t}=Q_{0}(w_{t}-Q_{1}(m_{t},w_{t}))-(w_{t}-Q_{1}(m_{t},w_{t})).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "As a consequence, assumptions 1 to 6 enable us to obtain $\\mathbb{E}[h_{t}]=0$ and to bound the variance of $h_{t}$ . Remark A.7. Assumptions 1 to 3 are standard. Assumptions 4 to 6 are non-classic but dedicated to Boolean Logic strategy. A. 4 is equivalent to assuming Boolean Logic optimization presents at least one filp at every iteration $t$ . A. 4 is classic in the literature of compressed SGD [53, 4, 80]. Moreover, A. 5 and A. 6 are not restrictive, but algorithmic choices. For example, rounding $Q_{0}$ function) can be stochastic based on the value of the accumulator $m_{t}$ . Similar to STE clipping strategy, the accumulator can be clipped to some pre-defined value $\\kappa$ before applying the flipping rule to verify A. 5. ", "page_idx": 22}, {"type": "text", "text": "Remark A.8. Our proof assumes that the step size $\\eta$ is constant over iterations. But in practice, we gently decrease the value of $\\eta$ at some time steps. Our proof can be adapted to this setting by defining a gradient accumulator $a_{t}$ such that $a_{t+1}=a_{t}+\\varphi(q_{t})$ . When $\\eta$ is constant we recover the definition (10) and we obtain $m_{t}=\\eta a_{t}$ . In the proposed algorithm, gradients are computed on binary weight $w_{t}$ and accumulated in $a_{t}$ . Then, one applies the flipping rule on the quantity $\\tilde{w}_{t}=\\eta a_{t}$ $\\tilde{w}_{t}=m_{t}$ when $\\eta$ is constant), and one (may) reset the accumulator $a_{t}$ . ", "page_idx": 22}, {"type": "text", "text": "We start by stating a key lemma which shows that the residual errors $e_{t}$ maintained in Algorithm 3 do not accumulate too much. ", "page_idx": 22}, {"type": "text", "text": "Lemma A.9. Under A. 3 and A. 4, the error can be bounded as $\\begin{array}{r}{\\mathbb{E}\\big[\\|e_{t}\\|^{2}\\big]\\leq\\frac{2\\gamma}{(1-\\gamma)^{2}}\\eta^{2}\\sigma^{2}}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. We start by using the definition of the error sequence: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|e_{t+1}\\|^{2}=\\|Q_{1}(m_{t},w_{t})-m_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Next we make use of A. 4: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|e_{t+1}\\|^{2}\\leq\\gamma\\|m_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We develop the accumulator update: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|e_{t+1}\\|^{2}\\leq\\gamma\\|e_{t}+\\eta\\tilde{\\nabla}f(w_{t})\\|^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We thus have a recurrence relation on the bound of $e_{t}$ . Using Young\u2019s inequality, we have that for any $\\beta>0$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Vert e_{t+1}\\Vert^{2}\\leq\\gamma(1+\\beta)\\Vert e_{t}\\Vert^{2}+\\gamma(1+\\frac{1}{\\beta})\\eta^{2}\\Vert\\tilde{\\nabla}f(w_{t})\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Rolling the recursion over and using A. 3 we obtain: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\|e_{t+1}\\|^{2}\\big]\\leq\\gamma(1+\\beta)\\mathbb{E}\\big[\\|e_{t}\\|^{2}\\big]+\\gamma(1+\\frac{1}{\\beta})\\eta^{2}\\mathbb{E}\\Big[\\|\\tilde{\\nabla}f(w_{t})\\|^{2}\\Big]}\\\\ &{\\qquad\\qquad\\leq\\gamma(1+\\beta)\\mathbb{E}\\big[\\|e_{t}\\|^{2}\\big]+\\gamma(1+\\frac{1}{\\beta})\\eta^{2}\\sigma^{2}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\sum_{r}^{t}(\\gamma(1+\\beta))^{r}\\gamma(1+\\frac{1}{\\beta})\\eta^{2}\\sigma^{2}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{\\gamma(1+\\frac{1}{\\beta})}{1-\\gamma(1+\\beta)}\\eta^{2}\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Take $\\begin{array}{r}{\\beta=\\frac{1-\\gamma}{2\\gamma}}\\end{array}$ and plug it in the above bounds gives: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\bigl[\\|e_{t+1}\\|^{2}\\bigr]\\leq\\frac{2\\gamma}{(1-\\gamma)^{2}}\\eta^{2}\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, the next Lemma allows us to bound the averaged norm-squared of the distance between the Boolean weight and $w_{t}-Q_{1}(m_{t},w_{t})$ . We make use of the previously defined quantity $h_{t}$ (22) and have: ", "page_idx": 23}, {"type": "text", "text": "Lemma A.10. Under assumptions A. 5 and A. 6: E \u2225ht\u22252 \u2264\u03b7d\u03ba. ", "page_idx": 23}, {"type": "text", "text": "Proof. Let consider a coordinate $i\\in[d].\\ Q_{0}|_{i}$ as $-1$ or $+1$ for value with some probability $p_{i,t}$ . For the ease of presentation, we will drop the subscript $i$ . Denote $u_{t}:=w_{t}-Q_{1}(m_{t},w_{t})$ . Hence, $h_{t}$ can take value $(1-u_{t})$ with some probability $p_{t}$ and $(-1-u_{t})$ with probability $1-p_{t}$ . Assumption A. 6 yields $2p_{t}-1=u_{t}$ . Therefore, we can compute the variance of $h_{t}$ as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\big[\\|h_{t}\\|^{2}\\big]=\\mathbb{E}\\Bigg[\\displaystyle\\sum_{i}^{d}1+\\big(w_{t}-Q_{1}(m_{t},w_{t})\\big)^{2}-2Q_{0}(w_{t}-Q_{1}(m_{t},w_{t})\\big(w_{t}-Q_{1}(m_{t},w_{t})\\Big)}\\\\ {\\displaystyle=\\sum_{i}^{d}((1-u_{t})^{2}p_{t}+(-1-u_{t})^{2}(1-p_{t}))}\\\\ {\\displaystyle=\\sum_{i}^{d}(1-u_{t}^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The definition of $u_{t}$ leads to ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1-u_{t}^{2}=1-(1+Q_{1}(m_{t},w_{t})^{2}-2w_{t}Q_{1}(m_{t},w_{t}))}\\\\ &{\\qquad\\qquad=Q_{1}(m_{t},w_{t})(2w_{t}-Q_{1}(m_{t},w_{t})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "When $m_{t}<1$ , we directly have $Q_{1}(m_{t},w_{t})(2w_{t}-Q_{1}(m_{t},w_{t}))=0\\leq\\eta\\kappa$ . When $m_{t}\\geq1$ , we apply the definition of $Q_{1}$ to obtain: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q_{1}(m_{t},w_{t})(2w_{t}-Q_{1}(m_{t},w_{t}))\\leq m_{t}(2-m_{t})}\\\\ {\\leq\\eta\\kappa.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, we can apply this result to every coordinate, and conclude that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\|h_{t}\\|^{2}\\big]\\leq\\eta d\\kappa.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "A.2.2 Proof of Theorem 3.17 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We now can proceed to the proof of Theorem 3.17. ", "page_idx": 23}, {"type": "text", "text": "Proof. Consider the virtual sequence $\\boldsymbol{x}_{t}=\\boldsymbol{w}_{t}-\\boldsymbol{e}_{t}$ . We have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{t+1}=Q_{0}(w_{t}-\\Delta_{t})-(m_{t}-\\Delta_{t})}\\\\ &{\\quad\\quad=(Q_{0}(w_{t}-\\Delta_{t})+\\Delta_{t}-e_{t})-\\eta\\tilde{\\nabla}f(w_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Considering the expectation with respect to the random variable $Q_{0}$ and the gradient noise, we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}[x_{t+1}|w_{t}]=x_{t}-\\eta\\nabla f(w_{t}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We consider $\\mathbb{E}_{t}[\\cdot]$ the expectation with respect to every random process know up to time $t$ . We apply the $L$ -smoothness assumption A. 2, and assumptions A. 3, A. 6 to obtain: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\natural_{t}[f(x_{t+1})-f(x_{t})]\\leq-\\eta\\langle\\nabla f(x_{t}),\\nabla f(w_{t})\\rangle+\\frac{L}{2}\\mathbb{E}_{t}\\Big[\\|(Q_{0}(w_{t}-\\Delta_{t})+\\Delta_{t})-\\eta\\tilde{\\nabla}f(w_{t})-w_{t}\\|^{2}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We now reuse $h_{t}$ from (22) and simplify the above: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}[f(x_{t+1})-f(x_{t})]\\leq-\\eta\\langle\\nabla f(x_{t}),\\nabla f(w_{t})\\rangle+\\frac{L}{2}\\mathbb{E}_{t}\\Big[\\|h_{t}-\\eta\\tilde{\\nabla}f(w_{t})\\|^{2}\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\leq-\\eta\\langle\\nabla f(x_{t})-\\nabla f(w_{t})+\\nabla f(w_{t}),\\nabla f(w_{t})\\rangle+\\frac{L}{2}\\mathbb{E}_{t}\\Big[\\|h_{t}-\\eta\\tilde{\\nabla}f(w_{t})\\|^{2}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Using Young\u2019s inequality, we have that for any $\\beta>0$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}[f(x_{t+1})-f(x_{t})]\\leq-\\eta\\langle\\nabla f(x_{t})-\\nabla f(w_{t})+\\nabla f(w_{t}),\\nabla f(w_{t})\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\frac{L}{2}(1+\\beta)\\mathbb{E}_{t}\\big[\\|h_{t}\\|^{2}\\big]+\\displaystyle\\frac{L}{2}\\eta^{2}(1+\\displaystyle\\frac{1}{\\beta})\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Making use again of smoothness and Young\u2019s inequality we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}[f(x_{t+1})-f(x_{t})]\\leq-\\eta\\|\\nabla f(w_{t})\\|^{2}-\\eta\\langle\\nabla f(x_{t})-\\nabla f(w_{t}),\\nabla f(w_{t})\\rangle}\\\\ &{\\phantom{\\leq}+\\frac{L}{2}(1+\\beta)\\mathbb{E}_{t}[\\|h_{t}\\|^{2}]+\\frac{L}{2}\\eta^{2}(1+\\frac{1}{\\beta})\\sigma^{2}}\\\\ &{\\phantom{\\leq}\\leq-\\eta\\|\\nabla f(w_{t})\\|^{2}+\\frac{\\eta\\rho}{2}\\|\\nabla f(w_{t})\\|^{2}+\\frac{\\eta}{2\\rho}\\|\\nabla f(x_{t})-\\nabla f(w_{t})\\|^{2}}\\\\ &{\\phantom{\\leq}+\\frac{L}{2}(1+\\beta)\\mathbb{E}_{t}[\\|h_{t}\\|^{2}]+\\frac{L}{2}\\eta^{2}(1+\\frac{1}{\\beta})\\sigma^{2}}\\\\ &{\\phantom{\\leq}\\leq-\\eta\\|\\nabla f(w_{t})\\|^{2}+\\frac{\\eta\\rho}{2}\\|\\nabla f(w_{t})\\|^{2}+\\frac{\\eta L^{2}}{2\\rho}\\underbrace{\\|x_{t}-w_{t}\\|^{2}}_{\\|e_{t}\\|^{2}}}\\\\ &{\\phantom{\\leq}+\\frac{L}{2}(1+\\beta)\\mathbb{E}_{t}\\big[\\|h_{t}\\|^{2}\\big]+\\frac{L}{2}\\eta^{2}(1+\\frac{1}{\\beta})\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Under the law of total expectation, we make use of Lemma A.9 and Lemma A.10 to obtain: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[f(x_{t+1})]-\\mathbb{E}[f(x_{t})]\\leq-\\eta(1-\\frac{\\rho}{2})\\mathbb{E}\\big[\\|\\nabla f(w_{t})\\|^{2}\\big]+\\frac{\\eta L^{2}}{2\\rho}\\frac{4\\gamma}{(1-\\gamma)^{2}}\\eta^{2}\\sigma^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\frac{L}{8}\\eta(1+\\beta)d\\kappa+\\frac{L}{2}\\eta^{2}(1+\\frac{1}{\\beta})\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Rearranging the terms and averaging over $t$ gives for $\\rho<2$ (we can choose for instance $\\rho=\\beta=1$ ): ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{T+1}\\sum_{t=0}^{T}\\|\\nabla f(w_{t})\\|^{2}\\leq\\frac{2(f(w_{0})-f_{*})}{\\eta(T+1)}+2L\\sigma^{2}\\eta+4L^{2}\\sigma^{2}\\frac{\\gamma}{(1-\\gamma)^{2}}\\eta^{2}+\\frac{L}{2}d\\kappa.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The bound in Theorem 3.17 contains 4 terms. The first term is standard for a general non-convex target and expresses how initialization affects convergence. The second and third terms depend on the fluctuation of the minibatch gradients. Another important aspect of the rate determined by Theorem 3.17 is its dependence on the quantization error. Note that there is an \u201cerror bound\u201d of $\\begin{array}{r}{\\frac{L}{2}d\\dot{\\kappa}}\\end{array}$ that remains independent of the number of update iterations. The error bound is the cost of using discrete weights as part of the optimization algorithm. Previous work with quantized models also includes error bounds [61, 62]. ", "page_idx": 24}, {"type": "text", "text": "B Code Sample of Core Implementation ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section we provide example codes in Python of a Boolean linear layer that employs xor logic kernel. This implementation is in particular based on PyTorch [78]. The class of Boolean linear layer is defined in Algorithm 4; and its backpropagation mechanism, overwritten from autograd, is shown in Algorithm 5. Here, we consider both cases of the received backpropagation signal $Z$ as described in Fig. 2, which are either Boolean (see Algorithm 6) or real-valued (see Algorithm 7). An example code of the Boolean optimizer is provided in Algorithm 8. ", "page_idx": 25}, {"type": "text", "text": "Notice that our custom XORLinear layer can be flexibly combined with any FP PyTorch modules to define a model. The parameters of this layer are optimized by the BooleanOptimizer, whereas those of FP layers are optimized by a common FP optimizer like Adam [54]. ", "page_idx": 25}, {"type": "text", "text": "Algorithm 4: Python code of XOR linear layer ", "text_level": 1, "page_idx": 25}, {"type": "table", "img_path": "DO9wPZOPjk/tmp/75f0338cc4f7174576fbfa1addc3b2748c613d53111d15c9e3b37454c7026a22.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Algorithm 5: Python code of the backpropagation logic of XOR linear layer ", "text_level": 1, "page_idx": 25}, {"type": "image", "img_path": "DO9wPZOPjk/tmp/4f4261b894ccf41897e7d913c1b7ae66e3f51602fe3c4015bb3aa409c314c9c6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Algorithm 6: Backpropagation logic with Boolean received backpropagation ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1 def backward_bool (ctx , Z):   \n2 \"\"\"   \n3 Variation of input:   \n4 - delta(xor(x,w))/delta(x) = neg w   \n5 delta(Loss)/delta(x) $=$ xnor(z,neg w) = xor(z,w)   \n6 Variation of weights:   \n7 delta(xor(x,w))/delta(w) $=$ neg x   \n8 delta(Loss)/delta(x) $=$ xnor(z,neg x) = xor(z,x)   \n9 Variation of bias:   \n10 bias $=$ xnor(bias ,True) $==>$ Variation of bias is driven in   \n1 the same basis as that of weight with xnor logic and input True.   \n2 Aggregation :   \n3 Count the number of TRUEs $=$ sum over the Boolean data   \n4 Aggr = TRUEs - FALSEs $=$ TRUEs (TOT TRUEs) $=$ 2TRUES - TOT   \n5 where TOT is the size of the aggregated dimension   \n16 1I 11   \n7 X, W, B $=$ ctx. saved_tensors   \n18   \n19 # Boolean variation of input   \n20 $\\mathtt{G\\_X}\\ =$ torch. logical_xor(Z[:,:, None], W[None ,: ,:])   \n21   \n22 # Aggregate over the out_features dimension   \n23 $\\texttt{G\\_X}=\\texttt{2}\\*\\texttt{G\\_X}$ .sum(dim $^{=1}$ ) - W.shape [0]   \n24   \n25 # Boolean variation of weights   \n26 G_W $=$ torch. logical_xor(Z[:,:, None], X[:,None ,:])   \n27   \n28 # Aggregate over the batch dimension   \n29 G_W = 2 \\* G_W.sum(dim $=\\!0$ ) - X.shape [0]   \n30   \n31 # Boolean variation of bias   \n32 if B is not None:   \n33 # Aggregate over the batch dimension   \n34 G_B = 2 \\* Z.sum( ${\\tt d i m}\\!=\\!0$ ) - Z.shape [0]   \n35   \n36 # Return   \n37 return G_X , G_W , G_B ", "page_idx": 26}, {"type": "text", "text": "Algorithm 7: Backpropagation logic with real received backpropagation ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "def backward_real (ctx , Z): X, W, B $=$ ctx. saved_tensors I 11 1I Boolean variation of input processed using torch avoiding loop: $->$ xor(Z: Real , W: Boolean) $\\mathrm{~\\,~\\ensuremath~{~\\mu~=~}~}\\mathrm{~\\,~\\ensuremath~{~-~}}\\mathrm{~Z~}$ \\* emb(W) $->$ emb(W): T->1, $\\begin{array}{r l}{\\mathbb{F}->-\\,1}&{{}=>}\\end{array}$ emb(W) $\\begin{array}{r l}{=}&{{}2\\left[\\!\\!\\sqrt{\\phantom{-}}-1\\right.}\\end{array}$ $=>$ delta(Loss)/delta(X) $=$ $\\mathsf{Z}*(1-2\\,\\mathsf{W})$ ) \"\"\" G_X = Z.mm(1 -2\\*W) \"\"\" Boolean variation of weights processed using torch avoiding loop: $->$ xor(Z: Real , X: Boolean) $\\begin{array}{r l r}{\\mathrm{~\\boldmath~\\omega~}}&{{}=}&{-\\,\\mathrm{Z}\\,\\mathrm{~\\boldmath~\\omega~}*}\\end{array}$ emb(X) $->$ emb(X): T->1, $\\begin{array}{r l}{\\mathbb{F}->-\\,1}&{{}=>}\\end{array}$ emb(X) $=$ 2X-1 $=>$ delta(Loss)/delta(W) $=$ Z^T $^*$ (1-2X) \"\"\" $\\begin{array}{r l r}{\\mathbb{G}_{\\mathbf{\\lambda}\\sim}\\mathbb{W}}&{{}=}&{\\mathbf{Z}\\cdot\\mathbf{t}\\left(\\mathbf{\\lambda}\\right)\\mathbf{\\lambda}\\cdot\\operatorname*{mm}\\left(\\mathbf{\\lambda}1-2\\ast\\mathbf{X}\\right)}\\end{array}$ 1 1I Boolean variation of bias 1 1I if B is not None: $\\textsf{G}\\_{\\textsf{B}}\\,=\\,\\textsf{Z}$ .sum(dim $=\\!0$ ) # Return return G_X , G_W , G_B ", "page_idx": 26}, {"type": "image", "img_path": "DO9wPZOPjk/tmp/5385df821d57761687095691b8324e7b8d65f015c1bba01fa4dcb1da2c4ed0bc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "C Training Regularization ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "C.1 Assumptions and Notations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We use capital letters to denote tensors. So, $W^{l}$ ${}^{l},X^{l},S^{l}$ , and $Z^{l}$ denote weight, input, pre-activation, and received backpropagation tensors of a layer $l$ . We also use Proposition A.2 and the discussion therein for studying Boolean logic in its embedding binary domain. It follows that in this section $\\mathbb{B}$ denotes the binary set $\\{\\mp1\\}$ . ", "page_idx": 27}, {"type": "text", "text": "Let $\\mathcal{N}(\\mu,\\sigma^{2})$ denote the Gaussian distribution of mean $\\mu$ and variance $\\sigma^{2}$ , and $\\mathbf{B}(p)$ denote Bernoulli distribution on $\\mathbb{B}$ with $p\\in[0,1]$ . Note that for $X\\sim\\mathbf{B}(p)$ , if $\\mathbb{E}[X]=0$ , then $\\mathbb{E}\\!\\left[X^{2}\\right]=1$ . ", "page_idx": 27}, {"type": "text", "text": "We assume that the backpropagation signal to each layer follows a Gaussian distribution. In addition, according to experimental evidence, cf. Fig. 4, we assume that their mean $\\mu$ can be neglected w.r.t. their variance $\\sigma^{\\dot{2}}$ ", "page_idx": 27}, {"type": "text", "text": "It follows that: ", "page_idx": 27}, {"type": "equation", "text": "$$\nZ^{l}\\sim\\mathcal{N}(\\mu,\\sigma^{2}),\\quad\\mathrm{with}\\quad\\mu\\ll\\sigma.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We also assume that signals are mutually independent, i.e., computations are going to be made on random vectors, matrices and tensors, and it is assumed that the coefficients of these vectors, matrices and tensors are mutually independent. We assume that forward signals, backpropagation signals, and weights (and biases) of the layers involved are independent. ", "page_idx": 27}, {"type": "text", "text": "Consider a Boolean linear layer of input size $n$ and output size $m$ , denote: ", "page_idx": 27}, {"type": "text", "text": "\u2022 Boolean weights: W l \u2208Bm\u00d7n;   \n\u2022 Boolean inputs: $X^{l}\\in\\mathbb{B}^{n}$ ;   \n\u2022 Pre-activation and Outputs: $S^{l}\\in[|-n,n|]^{m},X^{l+1}\\in\\mathbb{B}^{m}$ ;   \n\u2022 Downstream backpropagated signal: $Z^{l}\\in\\mathbb{R}^{m}$ ;   \n\u2022 Upstream backpropagated signal: $Z^{l-1}\\in\\mathbb{R}^{n}$ . ", "page_idx": 27}, {"type": "text", "text": "In the forward pass, we have: $S^{l}=\\mathbf{xor}(W^{l},X^{l})$ , and $X^{l+1}=\\mathbf{maj}(S^{l})$ . ", "page_idx": 27}, {"type": "text", "text": "With the following notations: ", "page_idx": 27}, {"type": "text", "text": "\u2022 $W^{l}=(W_{i j}^{l})_{i<m,j<n}\\sim({\\bf B}(p_{i j}))_{i<m,j<n}$ with $p_{i j}\\in[0,1]$ ;   \n\u2022 $X^{l}=(X_{i}^{l})_{i<n}\\sim(\\mathbf{B}(q_{i}))_{i<n}$ with $q_{i}\\in[0,1]$ ;   \n\u2022 $Z^{l}=(Z_{i}^{l})_{i<m}\\sim(N(\\mu_{i},\\sigma_{i}^{2}))_{i<m};$   \n\u2022 $\\tilde{Z}$ stands for the truncated (with derivative of tanh) version of $Z$ for sake of simplicity. ", "page_idx": 28}, {"type": "text", "text": "And assuming that $\\forall i,\\sigma_{i}=\\sigma,\\mu_{i}=\\mu$ and $\\mu\\ll\\sigma$ , we can derive scaling factors for linear layers in the next paragraph. Remark C.1. The scaling factor inside a convolutional layer behaves in a similar fashion except that the scalar dot product is replace by a full convolution with the 180-rotated kernel matrix. ", "page_idx": 28}, {"type": "text", "text": "C.2 Backpropagation Scaling ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We now compute the variance of the upstream backpropagated signal $Z^{l-1}$ with respect to the number of neurons and the variance of the downstream backpropagated signal: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Psi\\,j<n,\\mathrm{Var}(Z_{j}^{t-1})=\\displaystyle\\sum_{i\\ }^{m}\\mathrm{Var}(W_{i j}^{t}\\,\\hat{Z}_{i}^{t}),\\quad(W,Z\\mathrm{~are~self~mutually~independent})}\\\\ &{=\\displaystyle\\sum_{i\\ }^{m}\\mathrm{E}\\bigg[W_{i j}^{t}\\,\\hat{Z}_{i}^{t^{\\prime}}\\bigg]-\\mathbb{E}\\bigg[W_{i j}^{t}\\,\\hat{Z}_{i}^{t}\\bigg]^{2}}\\\\ &{=\\displaystyle\\sum_{i\\ }^{m}\\mathbb{E}\\bigg[W_{i j}^{t}\\,\\Big]\\mathbb{E}\\bigg[\\hat{Z}_{i}^{t^{\\prime}}\\bigg]-\\mathbb{E}\\big[W_{i j}^{t}\\big]^{2}\\mathbb{E}\\bigg[\\hat{Z}_{i}^{t}\\bigg]^{2},\\quad(W,Z\\mathrm{~are~independent})}\\\\ &{=\\displaystyle\\sum_{i\\ }^{m}\\mathbb{E}\\bigg[\\hat{Z}_{i}^{t^{\\prime}}\\bigg]-(2p_{i j}-1)^{2}\\mathbb{E}\\Big[\\hat{Z}_{i}^{t}\\Big]^{2}}\\\\ &{\\simeq m\\mathbb{E}\\bigg[\\hat{Z}_{i}^{t^{\\prime}}\\bigg],\\quad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad(\\mu\\ll\\sigma)}\\\\ &{=m\\mathbb{E}\\bigg[Z^{t^{\\prime}}\\bigg]\\mathbb{E}\\bigg[\\frac{\\partial\\mathrm{tanh}}{\\partial u}(u=\\alpha W^{t}\\cdot x^{t})^{2}\\bigg],\\quad\\quad\\mathrm{(indepondence~assump)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "image", "img_path": "DO9wPZOPjk/tmp/536ce4f4beb016a5eea66daf7bc48cb092ed2ab82e60b15b74becba91f1726ba.jpg", "img_caption": ["Figure 4: Empirical ratio of the mean to standard deviation of the backpropagation signal, experimented with CNN composed of BoolConv - BoolConv - BoolDense - RealDense layers and MNIST dataset. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "DO9wPZOPjk/tmp/0b48446a8e65d870f1181e8579bf814ed8a2c2a7378fd1e8589fe4f1dab1e8af.jpg", "img_caption": ["Figure 5: Expected value of tanh derivative with integer values as input, for several output sizes $m$ . "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "iLs ect ousm fpoucteusd  tohna tnhkes t teor me $\\mathbb{E}\\left[\\frac{\\partial\\operatorname{tanh}}{\\partial u}(u)^{2}\\right]$ h, e wehveernet $u\\in[\\vert-m,m\\vert]$ efso tr hmat  ewvee nh. aTvhe asbciallitayr $\\mathbb{P}(u=l)$ $u=l^{\\ast}$ $k+l$ level \u201c $\\mathbf{\\dot{\\rho}}+\\mathbf{1}^{\\bullet}$ and $k$ at level \u201c $\"1\"$ such that $2k+l=m$ . Hence, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{P}(u=l)=\\binom{m}{\\frac{m-l}{2}}\\frac{1}{2}^{\\frac{m-l}{2}}\\frac{1^{m-\\frac{m-l}{2}}}{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\bigg[\\frac{\\partial\\operatorname{tanh}}{\\partial u}(u)^{2}\\bigg]=\\sum_{u=-m}^{m}\\frac{\\partial\\operatorname{tanh}}{\\partial u}(u)^{2}p(u)}}\\\\ &{}&{\\qquad=2\\sum_{u=0}^{m}\\frac{\\partial\\operatorname{tanh}}{\\partial u}(u)^{2}p(u),\\qquad\\mathrm{(with~symmetry)}}\\\\ &{}&{\\qquad=\\frac{1}{2^{m-1}}\\sum_{u=0,u\\;e v e n}^{m}\\binom{m}{\\frac{m-l}{2}}(1-\\operatorname{tanh}^{2}(\\alpha u)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The latter can be easily pre-computed for a given value of output layer size $m$ , see Figure5. ", "page_idx": 29}, {"type": "text", "text": "The above figure suggests that for reasonable layer sizes $m$ , $\\begin{array}{r}{\\mathbb{E}\\!\\left[\\frac{\\partial\\operatorname{tanh}}{\\partial\\boldsymbol{u}}(\\boldsymbol{u}=\\alpha\\boldsymbol{W}^{l}\\cdot\\boldsymbol{x}^{l})^{2}\\right]\\simeq\\frac{1}{2}}\\end{array}$ . As a consequence we can make use of (30), and approximate the variance of the backpropagated signal as: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname{Var}(Z^{l-1})={\\frac{m}{2}}\\operatorname{Var}(Z^{l}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Remark C.2. The backpropagation inside a convolutional layer behaves in a similar fashion except that the tensor dot product is replace by a full convolution with the 180-rotated kernel matrix. For a given stride $v$ and kernel sizes $k_{x},k_{y}$ , the variance of the backpropagated signal is affected as follow: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname{Var}(Z^{l-1})={\\frac{m k_{x}k_{y}}{2v}}\\operatorname{Var}(Z^{l}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let denote by MP the maxpooling operator (we assume its size to be $(2,2)$ ). In the backward pass, one should not forget the impact of the $\\operatorname{tanh}^{\\prime}(\\alpha\\Delta)$ , and the MP operator so that: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathsf{Z}^{l-1}=\\mathrm{Conv}_{\\mathrm{full}}\\big(W_{\\mathrm{rotl}80}^{l},Z^{l}\\big)\\cdot\\frac{\\partial\\operatorname{tanh}}{\\partial u}\\big(u=\\mathrm{MP}[\\mathrm{Conv}(\\alpha W^{l},X^{l})]\\big)\\cdot\\frac{\\partial\\mathrm{MP}}{\\partial u}\\big(u=\\mathrm{Conv}(\\alpha W^{l},X^{l})\\big).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let us focus on the last term: $\\begin{array}{r}{\\frac{\\partial\\mathrm{MP}}{\\partial u}(u\\;=\\;\\mathrm{Conv}(\\alpha W^{l},X^{l}))\\;=\\;\\mathbf{1}\\big(u=\\operatorname*{max}(\\mathrm{Conv}(\\alpha W^{l},X^{l}))\\big).}\\end{array}$ Hence, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\bigg[\\frac{\\partial\\mathrm{MP}}{\\partial\\boldsymbol{u}}(\\boldsymbol{u}=\\mathrm{Conv}(\\alpha W^{l},\\boldsymbol{X}^{l}))^{2}\\bigg]=\\mathbb{E}\\bigg[\\frac{\\partial\\mathrm{MP}}{\\partial\\boldsymbol{u}}(\\boldsymbol{u}=\\mathrm{Conv}(\\alpha W^{l},\\boldsymbol{X}^{l}))\\bigg]}\\\\ {\\displaystyle=\\frac{1}{4}\\times1+0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "As a consequence, for a given stride $v$ and kernel sizes $k_{x},k_{y}$ , the variance of the backpropagated signal is affected as follow: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname{Var}(Z^{l-1})={\\frac{1}{4}}{\\frac{m k_{x}k_{y}}{2v}}\\operatorname{Var}(Z^{l}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "C.3 BackPropagation Through Boolean Activation Function ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Due to the binary activation, the effect on the loss function by an action on weight $w$ diminishes with the distance $\\Delta:=|s-\\tau|$ from threshold $\\tau$ to pre-activation $s$ to which $w$ contributes. Throughout the step activation function, the backpropagation signal can be optionally re-weighted by a function which is inversely proportional to $\\Delta$ , for instance, $\\operatorname{tanh}^{\\prime}(\\Delta)$ , $(\\dot{1}+\\Delta)^{-2}$ , $\\exp(-\\Delta)$ , or any other having this property. In our study, $\\operatorname{tanh}^{\\prime}(\\alpha\\Delta)$ turns out to be a good candidate in which $\\alpha$ is used to match the spreading in terms of standard deviation of this function and that of the pre-activation distribution. ", "page_idx": 30}, {"type": "text", "text": "We start by computing the variance of the pre-activation signal $S^{l}$ with respect to the number of neurons, without considering the influence of backward $\\mathrm{{tanh}^{\\overline{{\\prime}}}}$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Psi\\boldsymbol{j}<n,\\,\\mathrm{Var}(S_{j}^{t})=\\displaystyle\\sum_{i}^{m}\\mathrm{Var}(W_{i j}^{t}X_{i}^{t}),\\quad(W,X\\mathrm{~are~self~mutually~independent})}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\sum_{i}^{m}\\mathbb{E}\\Big[W_{i j}^{t}\\,X_{i}^{t}\\Big]^{2}-\\mathbb{E}\\big[W_{i j}^{t}X_{i}^{t}|^{2}\\big]}\\\\ &{\\qquad=\\displaystyle\\sum_{i}^{m}\\mathbb{E}\\Big[W_{i j}^{t}\\,\\Big]\\mathbb{E}\\Big[X_{i}^{t}\\Big]-\\mathbb{E}\\big[W_{i j}^{t}\\big]^{2}\\mathbb{E}\\big[X_{i}^{t}\\big]^{2},\\quad(W,X\\mathrm{~are~independent})}\\\\ &{\\qquad=\\displaystyle\\sum_{i}^{m}\\mathbb{E}\\Big[X_{i}^{t}\\Big]-(2p_{i j}-1)^{2}\\mathbb{E}\\big[X_{i}^{t}\\big]^{2}}\\\\ &{\\qquad\\simeq m\\mathbb{E}\\Big[X^{t}\\Big]^{2}\\Big],\\quad\\mathrm{~since~}\\mu\\ll\\sigma}\\\\ &{\\qquad=m.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This leads to ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\alpha=\\frac{\\pi}{2\\sqrt{3m}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "in order to have $\\begin{array}{r}{\\mathrm{Var}(\\alpha S)=\\frac{\\pi^{2}}{12}}\\end{array}$ , where $m$ is the range of the pre-activation, e.g., $m=c_{\\mathrm{in}}\\times k_{x}\\times k_{y}$ for a 2D convolution layer of filter dimensions $[c_{\\mathrm{in}},\\bar{k}_{x},k_{y},c_{\\mathrm{out}}]$ . ", "page_idx": 30}, {"type": "text", "text": "D Experimental Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "D.1 Image Classification ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "D.1.1 Training Setup ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The presented methodology and the architecture of the described Boolean neural networks (NNs) were implemented in PyTorch [78] and trained on 8 Nvidia Tesla V100 GPUs. The networks thought predominantly Boolean, also contain a fraction of FP parameters that were optimized using the Adam optimizer [54] with learning rate $10^{-3}$ . For learning the Boolean parameters we used the Boolean optimizer (see Algorithm 8). Training the Boolean networks for image classification was conducted with learning rates $\\eta=150$ and $\\eta=12$ (see Equation 10), for architectures with and without batch normalization, respectively. The hyper-parameters were chosen by grid search using the validation data. During the experiments, both optimizers used the cosine scheduler iterating over 300 epochs. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "We employ data augmentation techniques when training low bitwidth models which otherwise would overfit with standard techniques. In addition to techniques like random resize crop or random horizontal flip, we used RandAugment, lighting [68] and Mixup [115]. Following [98], we used different resolutions for the training and validation sets. For IMAGENET, the training images were $192\\!\\times\\!192$ px and $224\\!\\times\\!224$ px for validation images. The batch size was 300 for both sets and the cross-entropy loss was used during training. ", "page_idx": 31}, {"type": "text", "text": "D.1.2 CIFAR10 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "VGG-SMALL is found in the literature with different fully-connected fully-connected (FC) layers. Several works take inspiration from the classic work of [24], which uses 3 FC layers. Since other BNN methodologies only use a single FC layer, Table 9 presents the results with the modified VGG-SMALL. ", "page_idx": 31}, {"type": "table", "img_path": "DO9wPZOPjk/tmp/cffcbfbf31c59e7bed01b9306f44c038f1418293757567ba03eae9075cbf8c9c.jpg", "table_caption": ["Table 9: Top-1 accuracy for different binary methodologies using the modified VGG-SMALL (ending with 1 FC layer) on the CIFAR10 dataset. "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "D.1.3 Ablation Study on Image Classification ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The final block design for image classification was established after iterating over two models. The Boolean blocks examined were evaluated using the RESNET18 baseline architecture and adjusting the training settings to improve performance. Figure 6 presents the preliminary designs. ", "page_idx": 31}, {"type": "text", "text": "The Boolean Block I, Figure 6a, is similar to the original RESNET18 block in that BN operations are removed and ReLUs are replaced by the Boolean activation. This design always includes a convolution in the shortcut with spatial resolution being handled by the stride. Notice that for this block we add a Boolean activation after the Maxpool module in the baseline (also for the final baseline architecture). The Boolean Block II, Figure 6b, is composed by two stacked residual modules. For downsampling blocks we use the reshaping operation to reduce the spatial resolution and enlarge the channel dimensions both by a factor of 2. The shortcut is modified accordingly with different operations in order to guarantee similar spatial dimensions before the summation. ", "page_idx": 31}, {"type": "text", "text": "Table 10 summarizes the results obtained with the proposed designs on IMAGENET. During our experimentation, we validated the hypothesis that increasing network capacity on the convolutional layers yielded higher accuracy values. However, similar to FP CNNs, we confirmed there is a limit by which the hypothesis ceases to be true, leading to overfitting. Incorporating a more severe training strategy had a sustained positive impact. Even so, for larger configurations, the compromise between accuracy and size can be cumbersome. ", "page_idx": 31}, {"type": "text", "text": "Among the strategies to reduce overfitting during training we included: mixup data-augmentation [115], image illumination tweaking, rand-augment and smaller input resolution for training than for validation [98]. All combined, increased the accuracy by ${\\sim}3$ points (check results for Block $\\mathrm{II}+$ base channel 230 with and w/o additional data augmentation). ", "page_idx": 31}, {"type": "text", "text": "Compared to Block II, notice that the data streams in Block I are predominantly Boolean throughout the design. This is because it makes use of lightweight data types such as integer (after convolutions) and binary (after activations). In addition, it avoids the need of using a spatial transformation that may affect the data type and data distribution. In that regard, Block II requires 4 times more parameters for the convolution after reshaping, than the corresponding operation in Block I. This is exacerbated in upper layer convolutions, where the feature maps are deeper. Therefore, it makes sense to use Block I, as it is lighter and less prone to overfitting when the network capacity is expanded. ", "page_idx": 31}, {"type": "image", "img_path": "DO9wPZOPjk/tmp/a21705bd8ae4125e5021a4feae65ad9644c796978bf6406ad378f9f42119d49f.jpg", "img_caption": ["Figure 6: Preliminary designs for the baseline architecture and the Boolean basic blocks. The dashed and red-shaded operations in the Boolean block II are introduced for downsampling blocks. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "D.1.4 Neural Gradient Quantization ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "For completeness, we also implemented neural gradient quantization to quantize it by using INT4 quantization with logarithmic round-to-nearest approach [21] and statistics aware weight binning [22]. Statistics aware weight binning is a method that seeks for the optimal scaling factor, per layer, that minimizes the quantization error based on the statistical characteristics of neural gradients. It involves per layer additional computational computations, but stays negligible with respect to other (convolution) operations. On IMAGENET, we recover the findings from [21]: 4 bits quantization is enough to recover standard backpropagation performances. ", "page_idx": 32}, {"type": "text", "text": "D.1.5 Basic Blocks SOTA BNNs for Classification ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Recent BNN methodologies have proposed different mechanisms to improve performance. Most of them exploit full-precision operations to adjust datastreams within the network, like shift and scaling factors before binary activations [68] or channel scaling through Squeeze-and-Excitation modules [73, 36]. Figure 7 shows the basic blocks of three methodologies that perform particularly well in IMAGENET. Together with BN and regular activations, those techniques not only add an additional level of complexity but also lead to heavier use of computational resources and latency delays. ", "page_idx": 32}, {"type": "text", "text": "For comparison we also show the proposed block (Figure 7a) used in our experiments for Image Classification, Image Segmentation and Image Super-Resolution. Our block is compact in the sense that it only includes Boolean convolutions and Boolean activations, strategically placed to keep the input and output datastreams Boolean. ", "page_idx": 32}, {"type": "table", "img_path": "DO9wPZOPjk/tmp/dd041c00048b677647226ea267b16d16e0f6b547d3ec1f9c880c5f65dbc88556.jpg", "table_caption": ["Table 10: Evaluation of the proposed blocks in IMAGENET and their respective configurations during training. "], "table_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "DO9wPZOPjk/tmp/8cdd9beb24287dbeafa0adeaa4394fcc168ca00b230589c4d28226280858d445.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 7: Comparative graph of popular BNN techniques and our Boolean module. Notice how multiple full-precision operations like BN, PReLU, or Squeeze-and-Excitation are overly used on each BNN block. ", "page_idx": 33}, {"type": "text", "text": "D.2 Image Super-resolution ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "The seminal EDSR [64] method for super-resolution was used together with our Boolean methodology. In particular, the residual blocks are directly replaced by our Boolean basic block, see Figure 8. For all three tasks in super-resolution, (i.e. $\\times2,\\,\\times3,\\,\\times4)$ , training was carried out with small patches of $96\\!\\times\\!96$ px (40 of them extracted randomly from each single image in the DIV2K dataset) and validated with the original full-resolution images. The learning rate for real and boolean parameters were $10^{-4}$ and $\\eta=36$ , respectively. The networks were trained by minimizing the $L_{1}$ -norm between the ground-truth and the predicted upsampled image while using the Adam optimizer and Boolean optimizer (see Algorithm 8). In our experiments the batch size was 20. Some example images generated by our methodology are showed in Figures 9 and 10. ", "page_idx": 33}, {"type": "text", "text": "D.3 Semantic Segmentation ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "D.3.1 Network architecture ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Our Boolean architecture is based on DEEPLABV3 [17], which has shown great success in semantic segmentation. It is proven that using dilated or atrous convolutions, which preserve the large feature maps, instead of strided convolutions is prominent for this task. In our Boolean model with RESNET18 layout, we replace the strided convolutions in the last two RESNET18 layers with the non-strided version, and the dilated convolutions are employed to compensate for the reduced receptive field. Thus, the images are $8\\times$ downsampled instead of $32\\times$ , preserving small object features and allowing more information flow through the Boolean network. As shown in Figure 6a, in the Boolean basic block, a $3\\times3$ convolution instead of $1\\times1$ convolution is used to ensure the comparable dynamic range of pre-activations between the main path and the shortcut. Keeping these Boolean convolutional layers non-dilated naturally allows the backbone to extract multi-scale features without introducing additional computational cost. ", "page_idx": 33}, {"type": "text", "text": "The Atrous Spatial Pyramid Pooling (ASPP) consists of multiple dilated convolution layers with different dilation rates and global average pooling in parallel, which effectively captures multi-scale information. In the Boolean ASPP (BOOL-ASPP), we use one $1\\times1$ Boolean convolution and three $3\\times3$ Boolean dilated convolution with dilation rates of $\\{12,24,36\\}$ following by Boolean activation functions. The global average pooling (GAP) branch in ASPP captures image-level features, which is crucial for global image understanding as well as large object segmenting accuracy. However, in BOOL-ASPP, as shown in Figure 12c, the Boolean input $X$ leads to significant information loss before the global average pooling may cause performance degradation on large objects. Therefore, we keep the inputs integer for the GAP branch as demonstrated in Figure 12d. To prevent numerical instability, batch normalization is used in the GAP branch before each activation function. Using ", "page_idx": 33}, {"type": "image", "img_path": "DO9wPZOPjk/tmp/5610b4c4ea1e5936dddfb533a5acedef55f212afc6d99fd7763bb711c69027e9.jpg", "img_caption": ["Figure 8: Small EDSR for single scale $\\times2$ super-resolution and our Boolean version with Boolean residual blocks. In both architectures the channels dimensions are $\\kappa=256$ and the shaded blocks are repeated $8\\times$ . "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "DO9wPZOPjk/tmp/400469aa9113d19b97690abb77785743b1c3c403835ffffbe63234787929f1ad.jpg", "img_caption": ["Figure 9: Ground-truth high resolution images and the output of our Boolean super-resolution methodology. First row: image $\\bullet\\epsilon(\\,\\!013^{\\circ})$ from BSD100, with PSNR: 35.54 dB. Second row: image $\\bullet\\circ14^{\\circ}$ from Set14, with PSNR: 33.92 dB. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "image", "img_path": "DO9wPZOPjk/tmp/671ef62769f9db9545d2d255fc64a6f1542ae687dfd3d98c2c5ea05e79863b24.jpg", "img_caption": ["Figure 10: Ground-truth high resolution target image (top) and the output of our Boolean superresolution methodology (bottom). Image $\\phantom{-}^{\\bullet\\bullet}0810^{\\circ}$ from the validation set of DIV2K, with PSNR: 34.90 dB "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "BOOL-ASPP enhances the multi-scale feature extraction and avoids parameterized upsampling layers, e.g. transposed convolution. ", "page_idx": 35}, {"type": "text", "text": "D.3.2 Training setup ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "The model was trained on the CITYSCAPES dataset for 400 epochs with a batch size of 8. The AdamW optimizer [71] with an initial learning rate of $5\\times10^{-4}$ and the Boolean logic optimizer (see Algorithm 8) with a learning rate of $\\eta=12$ were used respectively for real and Boolean parameters. At the early training stage, parameters could easily be filpped due to the large backward signal; thus, to better benefit from the IMAGENET-pretrained backbone, we reduce the learning rate for Boolean parameters in the backbone to $\\eta=6$ . We employed the polynomial learning rate policy with $p=0.9$ for all parameters. The cross-entropy loss was used for optimization. We did not employ auxiliary loss or knowledge distillation as these training techniques require additional computational cost, which is not in line with our efficient on-device training objective. ", "page_idx": 35}, {"type": "image", "img_path": "DO9wPZOPjk/tmp/6e87893a167107d1defc6f5f0eae7c308cc624a112e3942b04eb17809b68c3ca.jpg", "img_caption": ["Figure 11: Boolean segmentation architecture. "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "DO9wPZOPjk/tmp/005d7f50d3dfa9307d0844101e6c2c0668de28ed333257ba938e24e9ef2f89d8.jpg", "img_caption": ["Figure 12: Boolean Atrous Spatial Pyramid Pooling (BOOL-ASPP) architecture. (a) $1\\times1$ Conv branch. (b) $3\\times3$ dilated Conv branch with dilation rate of $d$ . (c) Naive global average pooling branch. (d) Global average pooling branch. "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "D.3.3 Data sampling and augmentation ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We aim to reproduce closely full-precision model performance in the semantic segmentation task with Boolean architecture and Boolean logic training. Due to the nature of the Boolean network, the common regularization method, e.g., weight decay, is not applicable. Moreover, with more trainable parameters, the Boolean network can suffer from over-fitting. In particular, as shown in Table 11, the imbalanced dataset for semantic segmentation aggravates the situation. There is a significant performance gap for several classes which has low occurrence rate, including rider $(9.5\\%)$ , motor $(I I.2\\%)$ , bus $(9.5\\%)$ , truck $(6.9\\%)$ , train $(I7.0\\%)$ . We argue that the performance gap is due to the similarity between classes and the dataset\u2019s low occurrence rate, which is confirmed as shown in Figure 13. ", "page_idx": 36}, {"type": "text", "text": "Data augmentation and sampling are thus critical for Boolean model training. Regarding data augmentation, we employed multi-scale scaling with a random scaling factor ranging from 0.5 to 2. We adopted a random horizontal flip with probability $p=0.5$ and color jittering. In addition, we used rare class sampling (RCS) [45] to avoid the model over-fitting to frequent classes. For class $c$ , ", "page_idx": 36}, {"type": "text", "text": "Table 11: Class per image and performance gap occurrence rates in CITYSCAPES training set with naive BOOL-ASPP design. Class with low performance $\\mathrm{gap^{\\dagger}}$ and class with high performance $\\mathrm{gap^{*}}$ . ", "page_idx": 37}, {"type": "table", "img_path": "DO9wPZOPjk/tmp/b2ce1a736fea2b991b83549f72bcd5e90dcd609235d52ae11ecfd36ae2ea0130.jpg", "table_caption": [], "table_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "DO9wPZOPjk/tmp/f6c8e0ce5ebb3801704f3c3ff03c6978f00f609f25f425a9f0ae60f0ea7aef60.jpg", "img_caption": ["Figure 13: Class per image occurrence ratio and performance gap with naive BOOL-ASPP design. "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "the occurrence frequency in image $f_{c}$ is given by: ", "page_idx": 37}, {"type": "equation", "text": "$$\nf_{c}=\\frac{\\sum_{i=1}^{N}\\mathbf{1}(c\\in y_{i})}{N},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $N$ is the number of samples and $y_{i}$ is the set of classes existing in sample $i$ . The sampling probability of class $c$ is thus defined as: ", "page_idx": 37}, {"type": "equation", "text": "$$\np_{c}=\\frac{\\exp\\left(\\frac{1-f_{c}}{T}\\right)}{\\sum_{c^{\\prime}=1}^{K}\\exp\\left(\\frac{1-f_{c^{\\prime}}}{T}\\right)},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $K$ is the number of classes, and $T$ is a hyper-parameter for sampling rate balancing. In particular, for the CITYSCAPES dataset, we selected $T=0.5$ . ", "page_idx": 38}, {"type": "image", "img_path": "DO9wPZOPjk/tmp/eb1ead6f0c51af691e270716f7c86af88ce277e1c1368bd1c60f5d24d9236010.jpg", "img_caption": ["Figure 14: Qualitative comparison of Boolean model on CITYSCAPES validation set. "], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "D.3.4 Qualitative analysis on cityscapes validation set ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "The qualitative results of our Boolean network and the full-precision based are demonstrated in Figure 14. Despite the loss of model capacity, the proposed Boolean network trained with Boolean logic optimizer has comparable performance with large objects in the frequent classes, even in the complicated scene. ", "page_idx": 38}, {"type": "text", "text": "D.3.5 More experiments on semantic segmentation ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We evaluated the effectiveness of BOOL-ASPP by investigating the per-class performance gap to the full-precision model. As demonstrated in Table 12, a significant gap exists between the Boolean architecture with naive BOOL-ASPP design; i.e., using Boolean activations for ASPP module as illustrated in Figure 12c. However, the gap could be reduced by using BOOL-ASPP and RCS. In particular, the BOOL-ASPP improves the IoU of truck from $54.5\\%$ to $64.1\\%$ and bus from $65.5\\%$ to $\\bar{6}8.8\\%$ , bike from $68.8\\%$ to $69.1\\%$ and motor from $42.8\\%$ to $46.8\\%$ . This indicates that combining proposed BOOL-ASPP and RCS improves the model performance on low occurrence classes as well as similar classes with which are easy to be confused. ", "page_idx": 38}, {"type": "text", "text": "D.3.6 Validation on PASCAL VOC 2012 dataset ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We also evaluated our Boolean model on the 21-class PASCAL VOC 2012 dataset with augmented additional annotated data containing 10, 582, 1, 449, and 1, 456 images in training, validation, and test set, respectively. The same setting is used as in the experiments on the CITYSCAPES dataset, except the model was trained for 60 epochs. ", "page_idx": 38}, {"type": "text", "text": "As shown in Table 13, our model with fully Boolean logic training paradigm, i.e., without any additional intermediate latent weight, achieved comparable performance as the state-of-the-art latentweight-based method. Our Boolean model improved performance by incorporating multi-resolution feature extraction modules to $67.3\\%$ mIoU. ", "page_idx": 38}, {"type": "table", "img_path": "DO9wPZOPjk/tmp/9c627781026d03148b0e4a9a94098669a4a6929ab27a279031f3d9a8f9a94e5c.jpg", "table_caption": ["Table 12: Class-wise IoU performance on CITYSCAPES validation set. "], "table_footnote": [], "page_idx": 38}, {"type": "table", "img_path": "DO9wPZOPjk/tmp/310a8831a5af7f13ff699f9d592ebd8af3bcb9c1207dfd99496343381306b22f.jpg", "table_caption": ["Table 13: Performance on PASCAL VOC 2012 val set. "], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "D.4 Boolean BERT Fine-tuning ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "We conducted experiments on the BERT model [26] drawing on the experimental framework proposed in BIT [67]. For this experiment, our goal was to fine-tune the original pre-trained BERT model using Boolean precision. To validate our method we used the GLUE benchmark [102] with 8 datasets. For our experiments, we modified the baseline FP architecture using the proposed methodology Algorithm 8. That is, core operations within the transformer are substituted by native Boolean components, e.g. Boolean activations and Boolean linear layers. The FP parameters were optimized using the Adam optimizer with the learning rates proposed in [67]. Correspondingly for Boolean weights, we used our Boolean optimizer with learning rate $\\eta=100$ . ", "page_idx": 39}, {"type": "text", "text": "E Energy Estimation ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Energy consumption is a fundamental metric for measuring hardware complexity. However, it requires specific knowledge of computing systems and makes it hard to estimate. Few results are available, though experimental-based and limited to specific tested models [see, e.g., 32, 88, 74, 13, 14, 33]. Although experimental evaluation is precise, it requires considerable implementation efforts while not generalizing. In addition, most relevant works are only limited to inference and not training [19, 57, 110]. ", "page_idx": 39}, {"type": "text", "text": "E.1 Hardware Specification ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Ascend architecture. We intend to estimate the training energy consumption on Ascend chip architecture introduced in [63] and dedicated to DNN computing. The core design of Ascend is described in [63]. Essentially, it introduces a 3D (cube) computing unit, providing the bulk of high-intensity computation and increasing data reuse. On the other hand, it provides multiple levels of on-chip memory. In particular, memory L0, which is nearest to the computing cube, is tripled to boost further near-memory computing capability, namely L0-A dedicated to the left-hand-side (LHS) input data, L0-B dedicated to RHS input data, and L0-C for the output. For instance, in a convolution, L0-A, L0-B, and L0-C correspond to the input feature maps (IFMAPS), FILTERS, and output feature maps (OFMAPS), respectively. In addition, the output results going through L0-C can be processed by a Vector Unit for in-place operations such as normalization and activation. Table 14 shows energy efficiency and capacity of the memory hierarchy of a commercial Ascend architecture [63]. ", "page_idx": 39}, {"type": "table", "img_path": "DO9wPZOPjk/tmp/4e867013aee634b8d62a076a0af065b4c2537b48f7043b36d9e123bef78ed5ae.jpg", "table_caption": ["Table 14: Memory hierarchy and energy efficiency (EE) of an Ascend core [63] used in our evaluation. "], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "Nvidia architecture. We also have estimated the energy consumption for the Nvidia GPU (Tesla V100). Similarly, this architecture utilizes different memory levels with varying read and write energy characteristics. For instance, the L2 cache size is 6 MB per GPU. The L1 cache is $64\\;\\mathrm{KB}$ per Streaming Multiprocessor (SM). Each Tesla V100 GPU has 80 SMs, so the total L1 cache is 5120 KB (5 MB). However, specific details on the read and write energy consumption for each memory level of the Tesla V100 GPU are proprietary and not publicly disclosed by Nvidia. Thus, we show the normalized energy consumption relative to the computation of a MAC at the arithmetic logic unit (ALU). Table 15 shows the numbers for each storage level, which are extracted from a commercial $65\\;\\mathrm{nm}$ process [18]. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "Table 15: Normalized energy cost relative to the computation of one MAC operation at ALU. ", "page_idx": 40}, {"type": "table", "img_path": "DO9wPZOPjk/tmp/aaa7aba076e617268d076790e4497d97f04afa7746a36d3d88d779f8b7749671.jpg", "table_caption": [], "table_footnote": [], "page_idx": 40}, {"type": "text", "text": "E.2 Compute Energy ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Energy consumption is the sum of compute and memory energies. Compute energy is simply given by the number of arithmetic operations multiplied by their unit cost. The number of arithmetic operations is directly determined from the layer\u2019s parameters. For the Ascend architecture, their unit cost is obtained by considering the compute efficiency at 1.7 TOPS/W [63]. For Boolean logic operations, we follow the usual estimation that ADD INT- $n$ costs $(2n-1)$ logic operations where $n$ stands for bitwidth. ", "page_idx": 40}, {"type": "text", "text": "E.3 Memory Energy ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "On the other hand, memory energy is all consumed for moving data between their storage through memory levels and the computing unit during the entire lifetime of the process. Since energy consumed at each memory level is given by the number of data accesses to that level times peraccess energy cost, it consists in determining the number of accesses to each level of all data streams (i.e., LHS, RHS, Output). Besides taking into account the hardware architecture and memory hierarchy of chip, our approach to quantifying memory energy is based on existing methods [19, 94, 57, 110, 42, 108] for dataflow and energy evaluation. Given the layer parameters and memory hierarchy, it amounts to: ", "page_idx": 40}, {"type": "text", "text": "1. Tiling: determining the tiling strategy for allocating data streams on each memory level. ", "page_idx": 40}, {"type": "text", "text": "2. Movement: specifying how data streams are reused or kept stationary to determine their access numbers. ", "page_idx": 40}, {"type": "text", "text": "In the following, we present our method for the forward and backward passes by taking the example of a convolution layer, as convolutions are the main components of CNNs and the primary source of complexity due to their high data reuse. The parameters of 2D convolution layer are summarized in Table 16. Here, we denote IFMAPS, FILTERS, and OFMAPS by $I,F$ , and $O$ , respectively. ", "page_idx": 40}, {"type": "table", "img_path": "DO9wPZOPjk/tmp/3c7f866ad1952b9ef439ce01a4782605dc25f7064f0b508ea8b3439ff645826f.jpg", "table_caption": ["Table 16: Shape parameters of a convolution layer. "], "table_footnote": [], "page_idx": 40}, {"type": "text", "text": "E.3.1 Tiling ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Since the IFMAPS and FILTERS are usually too large to be stored in buffers, the tiling strategy is aimed at efficiently transferring them to the computing unit. Determining tiling parameters, which are summarized in Table 17, is an NP-Hard problem [110]. ", "page_idx": 40}, {"type": "text", "text": "Table 17: Tiling parameters of a convolution layer. ", "page_idx": 41}, {"type": "table", "img_path": "DO9wPZOPjk/tmp/b670bffb7e7f199ff392cfdecbaf5aa308751ce54df83472df07ffedf4d3c283.jpg", "table_caption": [], "table_footnote": [], "page_idx": 41}, {"type": "text", "text": "An iterative search over possibilities subject to memory capacity constraint provides tiling combinations of IFMAPS and FILTERS on each memory level. Different approaches can be used and Algorithm 9 shows an example that explores the best tiling parameters subjected to maximizing the buffer utilization and near compute stationary (i.e., as much reuse as possible to reduce the number of accesses to higher levels). Therein, the amount of data stored in level $i$ is calculated as: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{i}^{I}=N_{i}\\times C_{i}\\times H_{i}^{I}\\times W_{i}^{I}\\times b^{I},}\\\\ &{Q_{i}^{F}=M_{i}\\times C_{i}\\times H^{F}\\times W^{F}\\times b^{F},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $Q_{i}^{I}/Q_{i}^{F}$ and $b^{I}/b^{F}$ represent the memory and bitwidth of IFMAPS/FILTERS, respectively. ", "page_idx": 41}, {"type": "table", "img_path": "DO9wPZOPjk/tmp/25c89aa3035d2fbe57564c6ba7f0cb3882ce976f0cbc9c4b869224f3ad93780e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 41}, {"type": "text", "text": "E.3.2 Data movement ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "For data movement, at level L0, several data stationary strategies, called dataflows, have been proposed in the literature, notably weight, input, output, and row stationary [19]. Since Ascend chip provides tripled L0 buffers, partial sums can be directly stationary in the computing cube, hence equivalent to output stationary whose implementation is described in [28]. For the remaining levels, our question of interest is how to move IFMAPS block $[N_{i+1},C_{i+1},H_{i+1}^{I},W_{i+1}^{I}]$ and FILTERS block $[M_{i+1},C_{i+1},H^{F},W^{F}]$ from level $i+1$ to level $i$ efficiently. Considering that: ", "page_idx": 41}, {"type": "text", "text": "\u2022 IFMAPS are reused by the FILTERS over output channels, \u2022 FILTERS are reused over the IFMAPS spatial dimensions, \u2022 FILTERS are reused over the batch dimension, ", "page_idx": 41}, {"type": "text", "text": "the strategy that we follow is to keep FILTERS stationary on level $i$ and cycle through IFMAPS when fetching them from level $i+1$ as shown in Algorithm 10. Therein, FILTERS and IFMAPS are read block-by-block of their tiling sizes, i.e., FILTERS block $[M_{i},C_{i},H^{F},W^{F}]$ and IFMAPS block $[N_{i},C_{i},H_{i}^{\\tilde{I}},W_{i}^{I}]$ . Hence, the number of fliter accesses to level $i+1$ is 1 whereas the number of IFMAPS accesses to level $i+1$ equals the number of level- $^{\\,i}$ FILTERS blocks contained in level $i+1$ . Following this method, the number of accesses to memory levels of each data stream can be determined. Hence, denote by: ", "page_idx": 42}, {"type": "text", "text": "\u2022 $n_{i}^{d}$ : number of accesses to level $i$ of data $d$ , \u2022 $\\varepsilon_{i}$ : energy cost of accessing level $i$ , given as the inverse of energy efficiency from Table 14. ", "page_idx": 42}, {"type": "text", "text": "Following [19], the energy cost of moving data $d$ from DRAM (L3) into the cube is given as: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\xi^{d}=n_{3}^{d}\\varepsilon_{3}+n_{3}^{d}n_{2}^{d}\\varepsilon_{2}+n_{3}^{d}n_{2}^{d}n_{1}^{d}\\varepsilon_{1}+n_{3}^{d}n_{2}^{d}n_{1}^{d}n_{0}^{d}\\varepsilon_{0}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Regarding the output partial sums, the number of accumulations at each level is defined as the number of times each data goes in and out of its lower-cost levels during its lifetime. Its data movement energy is then given as: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}^{O}=(2n_{3}^{O}-1)\\varepsilon_{3}+2n_{3}^{O}(n_{2}^{O}-1)\\varepsilon_{2}+2n_{3}^{O}n_{2}^{O}(n_{1}^{O}-1)\\varepsilon_{1}+2n_{3}^{O}n_{2}^{O}n_{1}^{O}(n_{0}^{O}-1)\\varepsilon_{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where factor of 2 accounts for both reads and writes and the subtraction of 1 is because we have only one write in the beginning [19]. ", "page_idx": 42}, {"type": "text", "text": "Input: tiling parameters of IFMAPS and FILTERS at levels $i+1$ and $i$ .   \n1 repeat   \n2 read next FILTERS block of size $[M_{i},C_{i},H^{F},W^{F}]$ from levels $i+1$ to $i$ ;   \n3 repeat   \n4 read next IFMAPS block of size $[N_{i},C_{i},H_{i}^{I},W_{i}^{I}]$ from levels $i+1$ to $i$ ;   \n5 let the data loaded to $i$ be processed;   \n6 until all IFMAPS are read into level $i$ ; ", "page_idx": 42}, {"type": "text", "text": "7 until all FILTERS are read into level $i$ ; ", "page_idx": 42}, {"type": "text", "text": "E.3.3 Forward ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "In the forward pass, there are three types of input data reuse: ", "page_idx": 42}, {"type": "text", "text": "\u2022 For an $H^{I}\\,\\times\\,W^{I}$ IFMAP, there are $H^{O}\\times W^{O}$ convolutions performed with a single $H^{F}\\times W^{F}$ filter to generate a partial sum. The filter is reused $\\bar{H}^{O}\\times W^{O}$ times, and this type of reuse is defined as filter convolutional reuse. Also, each feature in the IFMAPS is reused $H^{F}\\times W^{F}$ times, and this is called feature convolutional reuse.   \n\u2022 Each IFMAP is further reused across $M$ fliters to generate $M$ output channels. This is called IFMAPS reuse.   \n\u2022 Each FILTER is further reused across the batch of $N$ IFMAPS. This type of reuse is called filter reuse. ", "page_idx": 42}, {"type": "text", "text": "From the obtained tiling parameters, the number of accesses that is used for (51) and (52) is determined by taking into account the data movement strategy as shown in Algorithm 10. As a result, Table 18 summarizes the number of accesses to memory levels for each data type in the forward pass. Therein, $\\alpha^{v}=H^{O}/H^{I}$ , $\\alpha^{h}=W^{O}/W^{I}$ , $H_{i}^{O}/W_{i}^{O}$ define the height/width of tiling OFMAPS in $\\mathrm{L}i$ buffers, $\\alpha_{i}^{v}=H_{i}^{O}/H_{i}^{I}$ , and $\\alpha_{i}^{h}=W_{i}^{O}/W_{i}^{I}$ for $i=2,1$ , and 0. ", "page_idx": 42}, {"type": "table", "img_path": "DO9wPZOPjk/tmp/bbbf4de904ac183ac13f2eb25b714014bf9ce01cad41954554c0e52239fe60a8.jpg", "table_caption": ["Table 18: Numbers of accesses at different memory levels of forward convolution. "], "table_footnote": [], "page_idx": 43}, {"type": "text", "text": "E.3.4 Backward ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "For the backward pass, given that ${\\partial\\mathrm{Loss}}/{\\partial O}$ is backpropagated from the downstream, it consists in computing $\\partial\\mathrm{Loss}/\\partial\\bar{F}$ and $\\partial\\mathrm{Loss}/\\partial I$ . Following the derivation of backpropagation in CNNs by [118], it is given that: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial\\mathrm{Loss}/\\partial F=\\mathrm{Conv}(I,\\partial\\mathrm{Loss}/\\partial O),}\\\\ &{\\partial\\mathrm{Loss}/\\partial I=\\mathrm{Conv}(\\mathrm{Rot}_{\\pi}(F),\\partial\\mathrm{Loss}/\\partial O),}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $\\mathrm{Rot}_{\\pi}(F)$ is the fliter rotated by 180-degree. As a result, the backward computation structure is also convolution operations, hence follows the same process as detailed above for the forward pass. For instance, Table 19 summarizes the number of accesses at each memory level in the backward pass when calculating the gradient $G^{I}=\\partial\\mathrm{Loss}/\\partial I$ . Therein, $C_{i}$ defines the number of tiling IFMAPS in $\\mathrm{L}i$ buffer, $\\bar{\\beta^{v}}=\\bar{H^{I}}/\\bar{H^{O}}$ , $\\beta^{h}=W^{I}/W^{O}$ , $\\beta_{i}^{v}=H_{i}^{I}/H_{i}^{O}$ , and $\\beta_{i}^{h}=W_{i}^{I}/W_{i}^{O}$ for $i=2,1$ , and 0. ", "page_idx": 43}, {"type": "table", "img_path": "DO9wPZOPjk/tmp/5fffea1821c6693663c77a2ed5535073a09805e8e76afc5d31c6bf87b3aad5e6.jpg", "table_caption": ["Table 19: Numbers of accesses at different memory levels for $\\partial\\mathrm{Loss}/\\partial I$ . "], "table_footnote": [], "page_idx": 43}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: The contributions claimed in the abstract and introduction are fully based on the method described in $\\S\\ 3$ and performance gains benchmarked in $\\S\\ 4$ . ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 44}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: We discuss limitations, in particular, of using our method in the current computing architectures in the paragraph \u201cLimitations\u201d in $\\S\\ S$ , and provide details about the convergence assumptions in Appendix A.2. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 44}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: The assumption of each theoretical result is explicty and formally stated. However, due to space constraint, all the proofs are postponed to Appendix A.2, and in particular the long list of convergence analysis assumptions are also reported in the appendix due to the space constraint. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 45}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: All the training details are described in $\\S\\ 4$ , in addition the utilized training regularization techniques are also fully described in Appendix C. Python code implementation is also provided in Appendix B. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 45}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 46}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We used publicly common datasets in our experiments. In Appendix we provide example codes and experimental details for reproduction purpose. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 46}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: All details can be found in $\\S\\,4$ , training regularization techniques in Appendix C and the code in Appendix B. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 46}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: Standard deviation are provided (when computationally doable) in $\\S\\ 4$ . Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 46}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 47}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We provide information on the computer resources in Appendix D and Appendix E. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 47}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: It has been and continues to be our first priority to be fully compliant with the scientific research spirit, and with NeurIPS Code of Ethics in particular. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 47}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes]   \nJustification: See $\\S\\ S$ .   \nGuidelines: \u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 48}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: Our work poses no such risks. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 48}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We used publicly common datasets and code package. We fully cited the original paper that produced those assets. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 48}, {"type": "text", "text": "", "page_idx": 49}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] Justification: NA Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 49}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 49}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 49}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA]   \nJustification: NA   \nGuidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 49}, {"type": "text", "text": "", "page_idx": 50}]