[{"type": "text", "text": "Optimal Multiclass U-Calibration Error and Beyond ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Haipeng Luo\u2217 Spandan Senapati\u2217 University of Southern California University of Southern California haipengl@usc.edu ssenapat@usc.edu ", "page_idx": 0}, {"type": "text", "text": "Vatsal Sharan\u2217 University of Southern California vsharan@usc.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the problem of online multiclass $U$ -calibration, where a forecaster aims to make sequential distributional predictions over $K$ classes with low $U.$ - calibration error, that is, low regret with respect to all bounded proper losses simultaneou\u221asly. Kleinberg et al. (2023) developed an algorithm with U-calibration error ${\\mathcal{O}}(K{\\sqrt{T}})$ after $T$ rounds and raised the open question of what the optimal bound\u221a is. We resolve this question by showing that the optimal U-calibration error is $\\Theta({\\sqrt{K T}})$ \u2014 we start with a simple observation that the Follow-the-PerturbedLeader algorithm of Daskalakis and Syrgkanis (2016) achieves this upper bound, followed by a matching lower bound constructed with a specific proper loss (which, as a side result, also proves the optimality of the algorithm of Daskalakis and Syrgkanis (2016) in the context of online learning against an adversary with finite choices). We also strengthen our results under natural assumptions on the loss functions, including $\\Theta(\\log T)$ U-calibration error for Lipschitz proper losses, ${\\mathcal{O}}(\\log T)$ U-calibration error for a certain class of decomposable proper losses, U-calibration error bounds for proper losses with a low covering number, and others. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the fundamental problem of making sequential probabilistic predictions over an outcome (e.g., predicting the probability of tomorrow\u2019s weather being sunny, cloudy, or rainy). Specifically, at each time $t=1,\\dots,T$ , a forecaster/learner predicts $p_{t}\\in\\Delta_{K}$ , where $\\Delta_{K}$ denotes the probability simplex over $K$ outcomes. At the same time, an adversary decides the true outcome, encoded by a one-hot vector $\\pmb{{y}}_{t}\\in\\mathcal{E}:=\\{\\pmb{{e}}_{1},\\dots,\\pmb{{e}}_{K}\\}$ , where $e_{i}$ denotes the $i$ -th standard basis vector of $\\mathbb{R}^{K}$ . The forecaster observes $\\scriptstyle\\pmb{y}_{t}$ at the end of time $t$ . ", "page_idx": 0}, {"type": "text", "text": "A popular approach to measure the performance of a forecaster is to measure her regret against the best fixed prediction in hindsight. Fixing some loss function $\\ell:\\Delta_{K}\\times\\mathcal{E}\\rightarrow\\mathbb{R}$ , the regret of the forecaster\u2019s predictions with respect to $\\ell$ is defined as $\\begin{array}{r}{\\mathrm{REG}_{\\ell}:=\\sum_{t=1}^{T}\\ell(p_{t},y_{t})-\\operatorname*{inf}_{p\\in\\Delta_{K}}\\overline{{\\sum_{t=1}^{T}\\ell(p,y_{t})}}}\\end{array}$ Perhaps the most common class of loss functions to eval uate a forecaster are proper loss functions. A loss function is proper if $\\mathbb{E}_{\\pmb{y}\\sim\\pmb{p}}[\\ell(\\pmb{p},\\pmb{y})]\\le\\mathbb{E}_{\\pmb{y}\\sim\\pmb{p}}[\\ell(\\pmb{p}^{\\prime},\\pmb{y})]$ for all $p,p^{\\prime}\\in\\Delta_{K}$ . Hence proper loss functions incentivize the forecaster to predict the true probability of the outcome (to the best of their knowledge). We will focus on proper loss functions in this work. ", "page_idx": 0}, {"type": "text", "text": "Note, however, that regret is measured with respect to a specific loss function $\\ell$ . It is unclear which proper loss one should minimize over for the specific application at hand \u2014 and there could even be multiple applications with different loss functions which use the forecasters\u2019s prediction. Could it be possible for a forecaster to simultaneously enjoy low regret with respect to all proper loss functions? This questions was raised in the interesting recent work of Kleinberg et al. (2023). They propose the notion of $U$ -calibration error $\\mathsf{U C a l}_{\\mathcal{L}}:=\\mathbb{E}\\left[\\operatorname*{sup}_{\\ell\\in\\mathcal{L}}\\mathsf{R E G}_{\\ell}\\right]$ (and a weaker version pseudo $U$ -calibration error $\\mathsf{P U C a l}_{\\mathcal{L}}:=\\operatorname*{sup}_{\\ell\\in\\mathcal{L}}\\mathbb{E}[\\mathbf{REG}_{\\ell}])$ for a family of proper loss functions $\\mathcal{L}$ . A forecaster with low U-calibration error thus enjoys good performance with respect to all loss functions in $\\mathcal{L}$ simultaneously. Unless explicitly mentioned, we shall let $\\mathcal{L}$ denote the set of all bounded (in $[-1,1])$ proper losses (as in Kleinberg et al. (2023)), and drop the subscript in $\\mathsf{U C a l}_{\\mathcal{L}}$ and $\\mathsf{P U C a l}_{\\mathcal{L}}$ for convenience. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The simplest way to get low U-calibration error is via the classical notion of low calibration error (Dawid, 1982), defined as $\\begin{array}{r}{\\mathsf{C a l}:=\\sum_{p\\in\\Delta_{K}}\\|\\sum_{t;p_{t}=p}(p-y_{t})\\|_{1}}\\end{array}$ . Intuitively, a forecaster with low calibration error guarantees that whenever she makes a prediction $\\pmb{p}$ , the empirical distribution of the true outcome is indeed close to $\\pmb{p}$ . Kleinberg et al. (2023) prove that $\\mathsf{P U C}\\bar{\\mathsf{a l}}\\leq\\mathsf{U C}\\mathsf{a l}=\\mathcal{O}(\\mathsf{C a l})$ and thus a well-calibrated forecaster must have small U-calibration error. However, getting low calibration error is difficult and faces known barriers: the best existing upper bound on Cal is $\\bar{\\mathcal{O}}(T^{\\frac{K}{K+1}})$ (Blum et al., 2008), and there is a $\\Omega(T^{0.528})$ lower bound (for $K=2$ ) (Qiao and Valiant, 2021). Therefore, a natural question to ask is if it is possible to side-step calibration and directly get low U-calibration error. Kleinberg et al. (2023) answer \u221athis in the affirmative, and show th\u221aat there exist simple and efficient algorithms with $\\mathsf{U C a l}={\\mathcal{O}}(\\sqrt{T})$ for $K=2$ and $\\mathsf{P U C a l}={\\mathcal O}(K\\sqrt{T})$ for general $K$ . This provides a strong decision-theoretic motivation for considering U-calibration error as opposed to calibration error; we refer the reader to Kleinberg et al. (2023) for further discussion. ", "page_idx": 1}, {"type": "text", "text": "Following up Kleinberg et al. (2023), this paper addresses the following question that was left open in their work: \u201cWhat is the minimax optimal multiclass $U.$ -calibration error?\u201d We give a complete answer to this question (regarding PUCal) by showing matching upper and lower bounds. Moreover, we identify several broad sub-classes of proper losses for which much smaller U-calibration error is possible. Concretely, our contributions are as follows. ", "page_idx": 1}, {"type": "text", "text": "1.1 Contributions and Technical Overview ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "First, we show that the minimax optimal value of PUCal is $\\Theta({\\sqrt{K T}})$ : ", "page_idx": 1}, {"type": "text", "text": "\u2022 In Section 3.1, we start by showing that a simple modification to the noise distribution of the Follow-the-Perturbed-Leader (FTPL) algorithm of Kleinberg et al. (2023) improves their $\\mathsf{P U C a l}=$ ${\\mathcal{O}}(K{\\sqrt{T}})$ bound to ${\\mathcal{O}}({\\sqrt{K T}})$ . In fact, our algorithm coincides with that of Daskalakis and Syrgkanis (2016) designed for an online learning setting with a fixed loss function and an adversary with only finite choices. The reason that it works for any proper losses simultaneously in our problem is because for any set of outcomes, the empirical risk minimizer with respect to any proper loss is always the average of the outcomes $\\left.c.f.\\right.$ property (1)). ", "page_idx": 1}, {"type": "text", "text": "\u2022 We then show in Section \u221a3.2 that there exists one particular proper loss $\\ell$ such t\u221ahat any algorithm has to suffer $\\mathrm{REG}_{\\ell}=\\Omega(\\sqrt{K T})$ in the worst case, hence implying $\\mathsf{P U C a l}=\\Omega(\\sqrt{K T})$ . While our proof follows a standard randomized argument, the novelty lies in the construction of the proper loss and the use of an anti-concentration inequality to bound the expected loss of the benchmark. We remark that, as a side result, our lower bound also implies the optimality of the FTPL algorithm of Daskalakis and Syrgkanis (2016) in their setting, which is unknown before to our knowledge. ", "page_idx": 1}, {"type": "text", "text": "While Kleinberg et al. (2023) only consider PUCal for general $K$ , we take a step forward and further study the stronger measure UCal (recall $\\mathsf{P U C a l}\\leq\\mathsf{U C a l})$ . We start by showing an upper bound on $\\mathsf{U C a l}_{\\mathcal{L}^{\\prime}}$ for the same FTPL algorithm and for any loss class ${\\mathcal{L}}^{\\prime}$ with a finite covering number. Then, we consider an even simpler algorithm, Follow-the-Leader (FTL), which is deterministic and makes UCal and PUCal trivially equal, and identify two broad classes of loss functions where FTL achieves logarithmic U-calibration error, an exponential improvement over the worst case: ", "page_idx": 1}, {"type": "text", "text": "\u2022 In Section 4.1, we show that for the class $\\mathcal{L}_{G}$ of $G$ -Lipschitz bounded proper losses (which includes standard losses such as the squared loss and spherical loss), FTL ensures $\\mathsf{P U C a l}_{\\mathcal{L}_{G}}=\\mathsf{U C a l}_{\\mathcal{L}_{G}}=$ ${\\mathcal{O}}(G\\log T)$ . ", "page_idx": 1}, {"type": "text", "text": "We further show that all algorithms must suffer $\\mathsf{P U C a l}_{\\mathcal L_{G}}=\\Omega(\\log T)$ . While we prove this lower bound using the standard squared loss that is known to admit $\\Theta(\\log T)$ regret in many online learning settings (e.g., Abernethy et al. (2008)), to our knowledge it has not been studied in our setting where the learner\u2019s decision set is a simplex and the adversary has finite choices. Indeed, our proof is also substantially different from Abernethy et al. (2008) and is one of the most technical contributions of our work. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 Next, in Section 4.2, we identify a class $\\mathcal{L}_{d e c}$ of losses that are decomposable over the $K$ outcomes and additionally satisfy some mild regularity conditions, and show that FTL again achieves $\\mathsf{P U C a l}=\\mathsf{U C a l}=\\mathcal O(\\log T)$ (ignoring other dependence). This class includes losses induced by a certain family of Tsallis entropy that are not Lipschitz. The key idea of our proof is to show that even though the loss might not be Lipschitz, its gradient grows at a controlled rate. \u2022 Given these positive results on FTL, one might wonder whether FTL is generally a good algorithm for any proper losses. We answer this question in the negative in Section 4.3 by showing that there exists a bounded proper loss such that the regret of FTL is $\\Omega(T)$ . This highlights the need of using FTPL if one cares about all proper losses (or at least losses not in $\\mathcal{L}_{G}$ or $\\mathcal{L}_{d e c.}$ ). ", "page_idx": 2}, {"type": "text", "text": "1.2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For calibration, Foster and Vohra (1998) proposed the first algorithm for the binary setting with an (expected) ${\\mathcal{O}}(T^{{\\frac{2}{3}}})$ calibration error (see also Blum and Mansour (2007) and Hart (2022) for a different proof of the result). In the multiclass setting, Blum et al. (2008) have shown an ${\\mathcal{O}}(T^{{\\frac{K}{K+1}}})$ calibration error. Several works have studied other variants of calibration error, such as the most recently proposed Distance to Calibration (B\u0142asiok et al., 2023; Qiao and Zheng, 2024; Arunachaleswaran et al., 2024); see the references therein for other earlier variants. ", "page_idx": 2}, {"type": "text", "text": "A recent research trend, initiated by Gopalan et al. (2022), has centered around the concept of simultaneous loss minimization, also known as omniprediction. Garg et al. (2024) study an online adversarial version of it, and U-calibration can be seen as a special non-contextual case of their setting with only proper losses considered. Their results, however, are not applicable here due to multiple reasons: for example, they consider only the binary case $[K\\,=\\,2]$ ), and their algorithm is either only designed for Lipschitz convex loss functions or computationally inefficient. We also note that omniprediction has been shown to have a surprising connection with multicalibration (H\u00e9bert-Johnson et al., 2018), a multi-group fairness notion, making it an increasingly important topic (Gopalan et al., 2022; B\u0142asiok et al., 2024; Gopalan et al., 2023a,b). ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation: We use lowercase bold alphabets to denote vectors. N, $\\mathbb{N}_{\\geq0}$ denote the set of positive, non-negative integers respectively. For any $m\\in\\mathbb{N}$ , $[m]$ denotes the index set $\\{1,\\ldots,m\\}$ . We use $\\Delta_{K}$ to denote the $(K-1)$ -dimensional simplex, i.e., $\\begin{array}{r}{\\Delta_{K}:=\\{p\\in\\mathbb{R}^{K}\\mid p_{i}\\geq0,\\sum_{i=1}^{K}p_{i}=1\\}}\\end{array}$ . The $i$ -th standard basis vector (dimension inferred from the context) is denoted by $e_{i}$ , and we use $\\mathcal{E}$ to represent the set $\\{e_{1},\\ldots,e_{K}\\}$ of all basis vectors of $\\mathbb{R}^{K}$ . By default, $\\left\\Vert\\cdot\\right\\Vert$ denotes the $\\ell_{2}$ norm. ", "page_idx": 2}, {"type": "text", "text": "Proper Losses: Throughout the paper, we consider the class of bounded proper losses ${\\mathcal{L}}\\mathrel{\\mathop:}=$ $\\{\\ell:\\Delta_{K}\\times\\mathcal{E}\\rightarrow[-1,1]\\ \\bar{|}\\ \\ell\\ \\mathrm{is\\proper}\\}$ or a subset of it. We emphasize that convexity (in the first argument) is never needed in our results. As mentioned, a loss $\\ell$ is proper if predicting the true distribution from which the outcome is sampled from gives the smallest loss in expectation, that is, $\\mathbb{E}_{\\pmb{y}\\sim\\pmb{p}}[\\ell(\\pmb{p},\\pmb{y})]\\le\\mathbb{E}_{\\pmb{y}\\sim\\pmb{p}}[\\ell(\\pmb{p}^{\\prime},\\pmb{y})]$ for all $p,\\bar{p^{\\prime}}\\in\\Delta_{K}$ . ", "page_idx": 2}, {"type": "text", "text": "For a proper loss $\\ell$ , we refer to $\\ell(p,y)$ as its bivariate form. The univariate form of $\\ell$ is defined as $\\ell(\\pmb{p}):=\\mathbb{E}_{\\pmb{y}\\sim\\pmb{p}}[\\ell(\\pmb{p},\\pmb{y})]$ . It turns out that a loss is proper only if its univariate form is concave. Moreover, one can construct a proper loss using a concave univariate form based on the following characterization lemma. ", "page_idx": 2}, {"type": "text", "text": "Lemma 1 (Theorem 2 of Gneiting and Raftery (2007)). A loss $\\ell:\\Delta_{K}\\times\\mathcal{E}\\rightarrow\\mathbb{R}$ is proper if and only if there exists a concave function $f$ such that $\\ell(\\pmb{p},\\pmb{y})=f(\\pmb{p})+\\langle\\pmb{g_{p}},\\pmb{y}-\\pmb{p}\\rangle$ for all $\\pmb{p}\\in\\Delta_{K},\\pmb{y}\\in\\mathcal{E}$ , where ${\\pmb g}_{p}$ denotes a subgradient of $f$ at $\\textbf{\\emph{p}}$ . Also, $f$ is the univariate form of $\\ell$ . ", "page_idx": 2}, {"type": "text", "text": "We provide several examples of proper losses below: ", "page_idx": 2}, {"type": "text", "text": "\u2022 The spherical loss is $\\ell(\\pmb{p},\\pmb{y})=-\\frac{\\langle\\pmb{p},\\pmb{y}\\rangle}{\\|\\pmb{p}\\|}$ , which is $\\sqrt{K}$ -Lipschitz (Proposition B.1) but non-convex in $\\pmb{p}$ . Its univariate form is $\\ell({\\pmb p})=-\\,\\|{\\pmb p}\\|$ .   \n\u2022 The squared loss (also known as the Brier score) is $\\begin{array}{r}{\\ell(p,y)\\;=\\;\\frac{1}{2}\\left\\|p-y\\right\\|^{2}}\\end{array}$ , which is clearly 2-Lipschitz and convex in $\\pmb{p}$ . Its univariate form is $\\ell(\\pmb{p})=1-\\|\\pmb{p}\\|^{2}$ .   \n\u2022 aGnedn seoramliez icnogn stthaen ts l, owssh,i cwhe i sc othnes iTdsear ltlihse  euntnriovparyi aatned f iosr cmo $\\begin{array}{r}{\\ell(\\pmb{p})=-\\tilde{c}_{K}\\sum_{i=1}^{K}p_{i}^{\\alpha}}\\end{array}$ fpoerr $\\alpha>1$ $\\tilde{c}_{K}>0$ $\\begin{array}{r}{\\ell(\\pmb{p},\\pmb{y})=\\tilde{c}_{K}(\\alpha-1)\\sum_{i=1}^{K}p_{i}^{\\alpha}-\\tilde{c}_{K}\\alpha\\sum_{i=1}^{K}p_{i}^{\\alpha-1}y_{i}}\\end{array}$ , which is not Lipschitz for $\\alpha\\in(1,2)$ .\\* ", "page_idx": 3}, {"type": "text", "text": "The following fact is critical for U-calibration: for any $\\textit{n}\\in\\mathbb{N}$ and a sequence of outcomes $y_{1},\\ldots,y_{n}\\in{\\mathcal{E}}$ , the mean forecaster is always the empirical risk minimizer for any proper loss $\\ell$ : (the proof is by definition and included in Appendix B for completeness): ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\frac{1}{n}}\\sum_{j=1}^{n}\\pmb{y}_{j}\\in\\operatorname*{argmin}_{p\\in\\Delta_{K}}{\\frac{1}{n}}\\sum_{j=1}^{n}\\ell(\\pmb{p},\\pmb{y}_{j}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Problem Setting: As mentioned, the problem we study follows the following protocol: at each time $t=1,\\dots,T$ , a forecaster predicts a distribution $p_{t}\\in\\Delta_{K}$ over $K$ possible outcomes, and at the same time, an adversary decides the true outcome encoded by a one-hot vector $\\pmb{y}_{t}\\in\\mathcal{E}$ , which is revealed to the forecaster at the end of time $t$ . ", "page_idx": 3}, {"type": "text", "text": "For a fixed proper loss function $\\ell_{*}$ , the regret of the forecaster is defined as: $\\begin{array}{r}{\\mathrm{REG}_{\\ell}:=\\sum_{t=1}^{T}\\ell(\\pmb{p}_{t},\\pmb{y}_{t})-}\\end{array}$ $\\begin{array}{r}{\\operatorname*{inf}_{p\\in\\Delta_{K}}\\sum_{t=1}^{T}\\ell(\\pmb{p},\\pmb{y}_{t})}\\end{array}$ , which, according to property (1), can be written as $\\textstyle\\sum_{t=1}^{T}\\ell(p_{t},{\\pmb y}_{t})\\,-$ $\\textstyle\\sum_{t=1}^{T}\\ell(\\beta,{\\boldsymbol y}_{t})$ where $\\begin{array}{r}{\\beta:=\\frac{1}{T}\\sum_{t=1}^{T}y_{t}}\\end{array}$ is simply the empirical average of all outcomes. ", "page_idx": 3}, {"type": "text", "text": "Our goal is to ensure low regret against a class of proper losses simultaneously. We define Ucalibration error as $\\mathsf{U C a l}_{\\mathcal{L}}~=~\\mathbb{E}\\left[\\mathrm{sup}_{\\ell\\in\\mathcal{L}}\\,\\mathsf{R E G}_{\\ell}\\right]$ and pseudo U-calibration error as $\\mathsf{P U C a l}_{\\mathscr{L}}~=$ $\\operatorname*{sup}_{\\ell\\in{\\mathcal{L}}}\\mathbb{E}[\\mathbf{R}\\mathrm{EG}_{\\ell}]$ for a family of loss functions $\\mathcal{L}$ . Unless explicitly mentioned, $\\mathcal{L}$ denotes the set of all bounded (in $[-1,1],$ ) proper losses and is dropped from the subscripts for convenience. ", "page_idx": 3}, {"type": "text", "text": "Oblivious Adversary versus Adaptive Adversary: As is standard in online learning, an oblivious adversary decides all outcomes $y_{1},\\dots,y_{T}$ ahead of the time with the knowledge of the forecaster\u2019s algorithm (but not her random seeds), while an adaptive adversary decides each $\\scriptstyle\\pmb{y}_{t}$ with the knowledge of past forecasts $\\pmb{p}_{1},\\dots,\\pmb{p}_{t-1}$ . Except for one result (upper bound on UCal for a class with lowcovering number), all our upper bounds hold for the stronger adaptive adversary, and all our lower bounds hold for the weaker oblivious adversary (which makes the lower bounds stronger). ", "page_idx": 3}, {"type": "text", "text": "3 Optimal U-calibration Error ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we prove that the minimax optimal pseudo U-calibration error is $\\Theta({\\sqrt{K T}})$ . ", "page_idx": 3}, {"type": "text", "text": "3.1 Algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As mentioned, our algorithm makes a simple change to the noise distribution of the FTPL algorithm of Kleinberg et al. (2023) and in fact coincides with the algorithm of Daskalakis and Syrgkanis (2016) designed for a different setting. To this end, we start by reviewing their setting and algorithm. Specifically, Daskalakis and Syrgkanis (2016) consider the following online learning problem: at each time $t\\in[T]$ , a learner chooses an action $\\mathbf{\\alpha}_{a_{t}}\\in\\mathcal{A}$ for some action set $\\boldsymbol{\\mathcal{A}}$ ; at the same time, an adversary selects an outcome $\\theta_{t}$ from a finite set $\\boldsymbol{\\Theta}:=\\{\\hat{\\pmb{\\theta}}_{1},\\dots,\\hat{\\pmb{\\theta}}_{K}\\}$ of size $K$ ; finally, the learner observes $\\theta_{t}$ and incurs loss $h(\\mathbf{a}_{t},\\pmb{\\theta}_{t})$ for some arbitrary loss function $h:\\mathcal{A}\\times\\Theta\\rightarrow[-1,1]$ that is fixed and known to the learner. Daskalakis and Syrgkanis (2016) propose the following FTPL algorithm: at each time $t$ , randomly generate a set of hallucinated outcomes, where the number of each possible outcome $\\widehat{\\pmb{\\theta}}_{i}$ for $i\\in[K]$ follows independently a geometric distribution with parameter $\\sqrt{K/T}$ , and then output the empirical risk minimizer using both the true outcomes and the hallucinated outcomes as the final action $\\mathbf{\\deltaa}_{t}$ . Formally, $\\begin{array}{r}{\\pmb{a}_{t}\\in\\operatorname{argmin}_{\\pmb{a}\\in\\mathcal{A}}\\sum_{i=1}^{K}Y_{t,i}h(\\pmb{a},\\hat{\\pmb{\\theta}}_{i})}\\end{array}$ where $Y_{t,i}=\\lvert\\{s<t\\mid\\pmb\\theta_{s}=\\hat{\\pmb\\theta}_{i}\\}\\rvert+m_{t,i}$ and $m_{t,i}$ is an i.i.d. sample of a geometric distribution with parameter $\\sqrt{K/T}$ . This simple algorithm enjoys the following regret guarantee. ", "page_idx": 3}, {"type": "table", "img_path": "7aFRgCC8Q7/tmp/5801620bf1fe456c8dbce9ec26f85403e18f815d9e2d16013b55442f8a64299c.jpg", "table_caption": ["Algorithm 1 FTPL with geometric noise for U-calibration "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Appendix F.3 of Daskalakis and Syrgkanis (2016)). The FTPL algorithm described above satisfies the following regret bound: E $\\begin{array}{r}{\\colon\\Bigl[\\sum_{t=1}^{T}h(\\mathbf{a}_{t},\\pmb{\\theta}_{t})-\\operatorname*{inf}_{\\mathbf{a}\\in\\mathcal{A}}\\sum_{t=1}^{T}h(\\mathbf{a},\\pmb{\\theta}_{t})\\Bigr]\\leq4\\sqrt{K T}}\\end{array}$ , where the expectation is taken over the randomness of both the algorithm and the adversary. ", "page_idx": 4}, {"type": "text", "text": "Now we are ready to discuss how to apply their algorithm to our multiclass U-calibration problem. Naturally, we take ${\\mathcal{A}}=\\Delta_{K}$ and $\\Theta=\\mathcal{E}$ . What $h$ should we use when we care about all proper losses? It turns out that this does not matter (an observation made by Kleinberg et al. (2023) already): according to property (1), the mean forecaster taking into account both the true outcomes and the hallucinated ones (that is, pt,i = Yt,i/ kK=1 is a solution of $\\begin{array}{r}{\\textstyle\\operatorname*{argmin}_{\\pmb{p}\\in\\Delta_{K}}\\sum_{i=1}^{K}Y_{t,i}h(\\pmb{p},\\pmb{e}_{i})}\\end{array}$ for any proper loss $h\\in{\\mathcal{L}}$ ! This immediately leads to the following result. ", "page_idx": 4}, {"type": "text", "text": "Corollary 1. Algorithm 1 ensures $\\mathsf{P U C a l}\\leq4\\sqrt{K T}$ against any adaptive adversary. ", "page_idx": 4}, {"type": "text", "text": "We remark that the only difference of Algorithm 1 compared to that of Kleinberg et al. (2023) i\u221as that $m_{t,i}$ is sampled from a geometric distribution instead of a uniform distribution in $\\{0,1,\\dots,\\lfloor\\sqrt{T}\\rfloor\\}$ . Using such noises that are skewed towards smaller values leads to better trade-off between the stability of the \u221aalgorithm \u221aand the expected noise range, which is the key to improve the regret bound from ${\\mathcal{O}}(K{\\sqrt{T}})$ to $\\mathcal{O}(\\sqrt{K T})$ . ", "page_idx": 4}, {"type": "text", "text": "3.2 Lower Bound ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now complement the upper bound of the previous section with a matching lower bound. Similar to the Multi-Armed Bandit problem, the regime of interest is $T=\\Omega(K)$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. There exists a proper loss $\\ell$ with range $[-1,1]$ such that the following holds: for any online algorithm ALG, there exists a c\u221ahoice of $y_{1},\\dots,y_{T}$ by an oblivious adversary such that the expected regret $\\mathbb{E}[\\mathbf{REG}_{\\ell}]$ of ALG is $\\Omega({\\sqrt{K T}})$ when $T\\geq12K$ . ", "page_idx": 4}, {"type": "text", "text": "We defer to the proof to Appendix C and highlight the key ideas and novelty here. First, the proper loss we use to prove the lower bound takes the following univariate form $\\begin{array}{r}{\\ell(\\pmb{p})=-\\frac{1}{2}\\sum_{i=1}^{K}\\left|p_{i}-\\frac{\\dag}{K}\\right|}\\end{array}$ which is in fact a direct generalization of the so-called \u201cV-shaped loss\u201d studied in Kleinberg et al. (2023) for the binary case. More specifically, they show that in the binary case, V-shaped losses are the \u201chardest\u201d in the sense that low regret with respect to all V-shaped losses directly implies low regret with respect to all proper losses (that is, low U-calibration error). On the other hand, they also prove that this is not true for the general multiclass case. Despite this fact, here, we show that V-shaped loss is still the \u201chardest\u201d\u221a in the multiclass case in a different sense: it is the hardest loss for any algorithm with $\\mathsf{P U C a l}={\\mathcal O}(\\sqrt{K T})$ . ", "page_idx": 4}, {"type": "text", "text": "With this loss function, we then follow a standard probabilistic argument and consider a randomized oblivious adversary that samples $y_{1},\\dots,y_{T}$ i.i.d. from the uniform distribution over $\\mathcal{E}$ . For such an adversary, we argue the following: (a) the expected loss incurred by ALG is non-negative, i.e., $\\mathbb{E}\\left[\\sum_{t=1}^{T}\\ell(\\pmb{p}_{t},\\pmb{y}_{t})\\right]\\stackrel{\\mathcal{-}}{\\geq}0$ , where the expectation is taken over $y_{1},\\dots,y_{T}$ and any internal randomness in ALG; (b) the expected loss incurred by the benchmark is bounded as $\\begin{array}{r}{\\mathbb{E}\\left[\\operatorname*{inf}_{\\pmb{p}\\in\\Delta_{K}}\\sum_{t=1}^{T}\\ell(\\pmb{p},\\pmb{y}_{t})\\right]\\leq}\\end{array}$ $-c\\sqrt{K T}$ for some universal positive constant $c$ , where \u221athe expectation is over $y_{1},\\dots,y_{T}$ . Together, this implies that the expected regret of ALG is at least $c\\sqrt{K T}$ in this randomized environment, which further implies that there must exist one particular sequence of $y_{1},\\dots,y_{T}$ such that the expected regret of ALG is at least $c\\sqrt{K T}$ , finishing the proof. We remark that our proof for (b) is novel and based on an anti-concentration inequality for Bernoulli random variables (Lemma A.4). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "We discuss some immediate implications of Theorem 2 below. First, it implies that in the online learning setting of Daskalakis and Syrgkanis (2016) where the adversary has only $K$ choices (formally defined in Section 3.1), without further assumptions on the loss function, their FTPL algorithm is minimax optimal. To our knowledge this is unknown before. ", "page_idx": 5}, {"type": "text", "text": "Secon\u221ad, since $\\mathsf{P U C a l}=\\operatorname*{sup}_{\\ell}\\mathbb{E}[\\mathsf{R E G}_{\\ell}]\\geq\\mathbb{E}[\\mathsf{R E G}_{\\ell^{\\prime}}]$ for any $\\ell^{\\prime}\\in\\mathcal{L}$ , Theorem 2 immediately implies a $\\Omega({\\sqrt{K T}})$ lower bound on the pseudo multiclass U-calibration error. In fact, since $\\mathsf{U C a l}\\geq\\mathsf{P U C a l}$ , the same lower bound holds for the actual U-calibration error. ", "page_idx": 5}, {"type": "text", "text": "Corollary 2. For any\u221a online forecasting algorithm, there exists an oblivious adversary such that $\\mathsf{U C a l}\\geq\\mathsf{P U C a l}=\\Omega(\\sqrt{K T})$ . ", "page_idx": 5}, {"type": "text", "text": "3.3 From PUCal to UCal ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now make an attempt to bound the U-calibration error UCal of Algorithm 1 for an oblivious adversary. Specifically, since the perturbations are sampled every round and the adversary is oblivious, using Hoeffding\u2019s inequality it is straightforward to show that for a fixed $\\ell$ and a fixed $\\boldsymbol{\\dot{\\delta}}\\in(0,1)$ , the regret of Algorithm 1 with respect to $\\ell$ satisfies $\\mathrm{REG}_{\\ell}\\leq4\\sqrt{K T}+\\sqrt{2T\\log\\left(1/\\delta\\right)}$ with probability at least $1-\\delta$ (see Hutter et al. (2005, Section 9) or Lemma D.1). Therefore, for a finite subset ${\\mathcal{L}}^{\\prime}$ of $\\mathcal{L}$ , taking a union bound over all $\\ell\\,\\in\\,{\\mathcal{L}}^{\\prime}$ gives $\\begin{array}{r}{\\operatorname*{sup}_{\\ell\\in\\mathcal{L}^{\\prime}}\\mathrm{REG}_{\\ell}\\leq4\\sqrt{K T}+\\sqrt{2T\\log\\left(|\\mathcal{L}^{\\prime}|/\\delta\\right)}}\\end{array}$ with probability at least $1-\\delta$ . Picking $\\delta=1/T$ and using the boundedness of losses, we obtain $\\mathsf{U C a l}_{\\mathcal{L}^{\\prime}}\\leq2+4\\sqrt{K T}+\\sqrt{2T\\log\\left(T\\left|\\mathcal{L}^{\\prime}\\right|\\right)}$ . In Appendix D, we generalize this simple argument to any infinite subset ${\\mathcal{L}}^{\\prime}$ of $\\mathcal{L}$ with a finite $\\epsilon$ -covering number $M(\\mathcal{L}^{\\prime},\\epsilon;\\|.\\|_{\\infty})$ and prove for any $\\epsilon>0$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathsf{U C a l}_{\\mathcal{L}^{\\prime}}\\leq2+4\\epsilon T+4\\sqrt{K T}+\\sqrt{2T\\log\\left(T\\cdot M(\\mathcal{L}^{\\prime},\\epsilon;\\left\\|\\cdot\\right\\|_{\\infty})\\right)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Using this bound, w\u221ae now giv\u221ae a concrete example of a simple parameterized family $\\mathcal{L}^{\\prime}\\subset\\mathcal{L}$ for which $\\mathsf{U C a l}_{\\mathcal L^{\\prime}}=\\mathcal O(\\sqrt{K T}+\\sqrt{T\\log T})$ . Consider the parameterized class ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}^{\\prime}=\\{\\alpha\\ell_{1}(\\pmb{p},\\pmb{y})+(1-\\alpha)\\ell_{2}(\\pmb{p},\\pmb{y})|\\alpha\\in[0,1]\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\ell_{1}(\\pmb{p},\\pmb{y}),\\ell_{2}(\\pmb{p},\\pmb{y})\\in\\mathcal{L}$ are two fixed bounded and proper losses. It is straightforward to verify that $\\ell_{\\alpha}(\\pmb{p},\\pmb{y}):=\\alpha\\ell_{1}(\\pmb{p},\\pmb{y})+(1-\\alpha)\\ell_{2}(\\pmb{p},\\pmb{y})\\in\\mathcal{L}$ , therefore $\\mathcal{L}^{\\prime}\\subset\\mathcal{L}$ . ", "page_idx": 5}, {"type": "text", "text": "To obtain an $\\epsilon\\in(0,1)$ cover for ${\\mathcal{L}}^{\\prime}$ , we consider the set $\\mathcal{C}:=\\{0,\\epsilon,\\ldots,1-\\epsilon,1\\}$ which partitions the interval $[0,1]$ to $\\frac{1}{\\epsilon}$ smaller intervals each of length $\\epsilon$ . For each $\\alpha\\in[0,1]$ , let $c_{\\alpha}\\in{\\mathcal{C}}$ denote the closest point to $\\alpha$ (break ties arbitrarily). Clearly, $|\\alpha-c_{\\alpha}|\\,\\le\\,\\epsilon$ . Next, consider the function $g_{\\alpha}(\\pmb{p},\\pmb{y}):=c_{\\alpha}\\ell_{1}(\\pmb{p},\\pmb{y})+(1-c_{\\alpha})\\ell_{2}(\\pmb{p},\\pmb{y})$ . The class $\\{g_{\\alpha}(\\pmb{p},\\pmb{y})|\\alpha\\in[0,1]\\}$ is clearly a $2\\epsilon$ cover of ${\\mathcal{L}}^{\\prime}$ with size $\\frac{1}{\\epsilon}$ . Thus, $\\begin{array}{r}{M(\\mathcal{L}^{\\prime},\\epsilon;\\|.\\|_{\\infty})=\\mathcal{O}(\\frac{1}{\\epsilon})}\\end{array}$ . It then follows from (2) that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathsf{U C a l}_{\\mathcal{L}^{\\prime}}=\\mathcal{O}\\left(\\epsilon T+\\sqrt{K T}+\\sqrt{T\\log\\left(\\frac{T}{\\epsilon}\\right)}\\right)=\\mathcal{O}\\left(\\sqrt{K T}+\\sqrt{T\\log T}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "on choosing $\\textstyle{\\epsilon={\\frac{1}{T}}}$ . On the other hand, in subsection 4.3 we shall argue that for this class with a specific example of $\\ell_{2}$ , FTL suffers linear U-calibration error (that is, $\\mathsf{U C a l}_{\\mathcal{L^{\\prime}}}=\\Omega(T),$ ). ", "page_idx": 5}, {"type": "text", "text": "4 Improved Bounds for Important Sub-Classes ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we show that it is possible to go beyond the $\\Theta({\\sqrt{K T}})$ U-calibration error for several broad sub-classes of $\\mathcal{L}$ that include important and common proper losses. These results are achieved by an extremely simple algorithm called Follow-the-Leader (FTL), which at time $t>1$ forecasts\\* ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\pmb p}_{t}=\\frac{1}{t-1}\\sum_{s=1}^{t-1}{\\pmb y}_{t}\\in\\underset{{\\pmb p}\\in\\Delta_{K}}{\\mathrm{argmin}}\\sum_{s=1}^{t-1}\\ell({\\pmb p},{\\pmb y}_{s}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "\\*The forecast at time $t=1$ can be arbitrary. ", "page_idx": 5}, {"type": "text", "text": "that is, the average of the past outcomes. For notational convenience, we define $\\begin{array}{r}{{{n}_{t}}=\\sum_{s=1}^{t}{{y}_{s}}}\\end{array}$ so that FTL predicts $\\begin{array}{r}{p_{t}=\\frac{n_{t-1}}{t-1}}\\end{array}$ , with ${\\boldsymbol{n}}_{t-1,i}$ being the count of outcome $i$ before time $t$ . ", "page_idx": 6}, {"type": "text", "text": "Importantly, since FTL is a deterministic algorithm, it\u2019s PUCal and UCal are always trivially the same. Moreover, there is also no distinction between an oblivious adversary and an adaptive adversary because of this deterministic nature. ", "page_idx": 6}, {"type": "text", "text": "4.1 Proper Lipschitz Losses ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we show that $\\Theta(\\log T)$ is the minimax optimal bound for PUCal and UCal for Lipschitz proper losses. Specifically, we consider the following class of $G$ -Lipschitz proper losses ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{G}:=\\left\\{\\ell\\in\\mathcal{L}\\mid\\lvert\\ell(p,y)-\\ell(p^{\\prime},y)\\rvert\\leq G\\left\\lVert p-p^{\\prime}\\right\\rVert,\\forall p,p^{\\prime}\\in\\Delta_{K},y\\in\\mathcal{E}\\right\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "As discussed in Section 2, the two common proper losses, squared loss and spherical loss, are both in ${\\mathcal{L}}_{G}$ for some $G$ . Note that the class of ${\\mathcal{L}}_{G}$ is rich since according to Lemma 1 it corresponds to the class of concave univariate forms that are Lipschitz and smooth (see Lemma B.2). We now show that FTL enjoys logarithmic U-calibration error with respect to ${\\mathcal{L}}_{G}$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. The regret of FTL for learning any $\\ell\\in{\\mathcal{L}}_{G}$ is at most $2+2G\\log T$ . Consequently, FTL ensures $\\mathsf{P U C a l}_{\\mathcal{L}_{G}}=\\mathsf{U C a l}_{\\mathcal{L}_{G}}=\\mathcal{O}(G\\log T)$ . ", "page_idx": 6}, {"type": "text", "text": "Proof. Using the standard Be-the-Leader lemma (see e.g., (Orabona, 2019, Lemma 1.2)) that says $\\begin{array}{r}{\\sum_{t=1}^{T}\\ell(\\pmb{p}_{t+1},\\pmb{y}_{t})\\leq\\operatorname*{inf}_{\\pmb{p}\\in\\Delta_{K}}\\sum_{t=1}^{T}\\ell(\\pmb{p},\\pmb{y}_{t})}\\end{array}$ , the regret of FTL can be bounded as ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\tt R E G}_{\\ell}\\leq2+\\sum_{t=2}^{T}\\ell({\\pmb{p}}_{t},{\\pmb{y}}_{t})-\\ell({\\pmb{p}}_{t+1},{\\pmb{y}}_{t})\\leq2+G\\sum_{t=2}^{T}\\|{\\pmb{p}}_{t}-{\\pmb{p}}_{t+1}\\|\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the second inequality is because $\\ell\\in{\\mathcal{L}}_{G}$ . Next, since $\\begin{array}{r}{p_{t}=\\frac{n_{t-1}}{t-1}}\\end{array}$ and $\\begin{array}{r}{p_{t+1}=\\frac{n_{t}}{t}}\\end{array}$ , we obtain ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathsf{R E G}_{\\ell}\\leq2+G\\sum_{t=2}^{T}\\left\\|\\frac{n_{t-1}}{t-1}-\\frac{n_{t}}{t}\\right\\|=2+G\\sum_{t=2}^{T}\\left\\|\\frac{n_{t-1}}{t(t-1)}-\\frac{y_{t}}{t}\\right\\|\\leq2+2G\\sum_{t=2}^{T}\\frac{1}{t},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the equality follows since $\\pmb{n}_{t}=\\pmb{n}_{t-1}+\\pmb{y}_{t}$ and the last inequality follows from the triangle inequality and $\\|n_{t-1}\\|\\leq\\|n_{t-1}\\|_{1}=t-1$ . Finally, since $\\begin{array}{r}{\\sum_{t=2}^{T}\\frac{1}{t}\\le\\int_{1}^{T}\\frac{1}{z}d z=\\log T}\\end{array}$ , we obtain $\\mathrm{REG}_{\\ell}\\le2+2G\\log T$ , which completes the proof. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "A closer look at the proof reveals that global Lipschitzness over the entire simplex $\\Delta_{K}$ is in fact not necessary. This is because, for example, in the term $\\ell(\\pmb{p}_{t},\\pmb{e}_{i})-\\ell(\\pmb{p}_{t+1},\\pmb{e}_{i})$ for some $i\\in[K]$ , by the definition of FTL the corresponding coordinates $p_{t,i}$ and $p_{t+1,i}$ are almost always at least $1/T$ , with only one exception which is when $t$ is the first time we have $\\pmb{y}_{t}=e_{i}$ and which we can ignore since the regret incurred is at most a constant. This means that having local Lipschitzness in a certain region is enough; see Lemma E.1 for details. Note that the loss induced by the Tsallis entropy (mentioned in Section 2) is exactly one such example where global Lipschitzness does not hold but local Lipschitzness does. We defer the concrete discussion of the regret bounds of FTL on this example to Section 4.2 (where yet another different analysis is introduced). ", "page_idx": 6}, {"type": "text", "text": "In the rest of this subsection, we argue that no algorithm can guarantee regret better than $\\Omega(\\log T)$ for one particular Lipschitz proper loss, making FTL minimax optimal for this class. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4. There exists a proper Lipschitz loss $\\ell$ such that: for any algorithm ALG, there exists $a$ choice of $y_{1},\\dots,y_{T}$ by an oblivious adversary such that the expected regret of ALG is $\\Omega(\\log T)$ . ", "page_idx": 6}, {"type": "text", "text": "The loss we use in this lower bound is simply the squared loss $\\ell(\\pmb{p},\\pmb{y})=\\|\\pmb{p}-\\pmb{y}\\|^{2}$ with $K=2$ . While squared loss is known to admit $\\Theta(\\log T)$ regret in other online learning problems such as that from Abernethy et al. (2008), as far as we know there is no study on our setting where the decision set is the simplex and the adversary has only finite choices. It turns out that this variation brings significant technical challenges, and our proof is substantially different from that of Abernethy et al. (2008). We defer the details to Appendix $\\mathrm{F}$ and discuss the key steps below. ", "page_idx": 6}, {"type": "text", "text": "Step 1: Since squared loss is convex in $\\textbf{\\emph{p}}$ , by standard arguments it suffices to consider deterministic algorithms only (see Lemma F.1). Moreover, for deterministic algorithms, there is no difference between an oblivious adversary and an adaptive adversary so that the minimax regret can be written as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{VAL}=\\operatorname*{inf}_{p_{1}\\in\\Delta_{K}}\\operatorname*{sup}_{y_{1}\\in\\mathcal{E}}\\cdot\\cdot\\operatorname*{inf}_{p_{T}\\in\\Delta_{K}}\\operatorname*{sup}_{y_{T}\\in\\mathcal{E}}\\left[\\sum_{t=1}^{T}\\ell(p_{t},y_{t})-\\operatorname*{inf}_{p\\in\\Delta_{K}}\\sum_{t=1}^{T}\\ell(p,y_{t})\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "To solve this, further define $\\psi_{n,r}$ recursively as $\\begin{array}{r}{\\mathcal{V}_{n,r}=\\operatorname*{inf}_{\\substack{p\\in\\Delta_{K}}}\\operatorname*{sup}_{y\\in\\mathcal{E}}\\mathcal{V}_{n+y,r-1}+\\ell(p,y)}\\end{array}$ with $\\begin{array}{r}{\\gamma_{n,0}=-\\operatorname*{inf}_{\\pmb{p}\\in\\Delta_{K}}\\sum_{i=1}^{K}n_{i}\\ell(\\pmb{p},\\pmb{e}_{i})}\\end{array}$ , so that VAL is simply $\\nu_{\\mathbf{0},T}$ . ", "page_idx": 7}, {"type": "text", "text": "Step 2: Using the minimax theorem, we further show that $\\begin{array}{r}{\\gamma_{n,r}=\\operatorname*{sup}_{\\pmb{q}\\in\\Delta_{K}}\\sum_{i=1}^{K}q_{i}\\gamma_{\\pmb{n}+e_{i},r-1}+}\\end{array}$ $\\ell(q)$ where the univariate form $\\ell(q)$ is $1-\\left\\|q\\right\\|^{2}$ (as mentioned in Section 2). Recall that we consider only the binary case $K=2$ , so it is straightforward to give an analytical form of the solution to the maximization over $\\pmb q\\in\\Delta_{K}$ . Specifically, writing $\\}_{{n,r}}=\\mathcal{V}_{(n_{1},n_{2}),r}$ as $\\nu_{n_{1},n_{2},r}$ to make notation concise, we show ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{V}_{n_{1},n_{2},r}=\\left\\{\\frac{\\mathcal{V}_{2}}{8}+\\frac{\\mathcal{V}_{1}+\\mathcal{V}_{2}}{2}+\\frac{1}{2}\\right.\\ \\mathrm{~if~}-2\\leq\\mathcal{V}_{1}-\\mathcal{V}_{2}\\leq2,}\\\\ {\\mathcal{V}_{1}\\leq\\frac{\\mathcal{V}_{1}}{2}+\\frac{\\mathcal{V}_{1}+\\mathcal{V}_{2}}{2}+\\frac{1}{2}\\ \\ \\mathrm{~if~}-2\\leq\\mathcal{V}_{1}-\\mathcal{V}_{2}\\leq2,}\\\\ {\\mathcal{V}_{1}\\leq\\mathcal{V}_{1}\\leq\\mathcal{V}_{2}>2,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\protect\\nu_{1}$ and $\\protect\\nu_{2}$ are shorthands for $\\mathcal{V}_{n_{1}+1,n_{2},r-1}$ and $\\mathcal{V}_{n_{1},n_{2}+1,r-1}$ respectively. Next, by an tihnedruecftoiroen $r$ e  iss haolww atyhsa te qfoura la tllo $n_{1},n_{2},r$ t.hat $-2\\leq\\mathcal{V}_{1}-\\mathcal{V}_{2}\\leq2$ (Lemma F.2), $\\nu_{n_{1},n_{2},r}$ $\\begin{array}{r}{\\frac{(\\mathcal{V}_{1}-\\mathcal{V}_{2})^{2}}{8}+\\frac{\\mathcal{V}_{1}+\\mathcal{V}_{2}}{2}+\\frac{1}{2}}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "Step 3: By an induction on $r$ again, we show that $\\nu_{n_{1},n_{2},r}$ exhibits a special structure of the form ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{V}_{n_{1},n_{2},r}=\\frac{(n_{1}-n_{2})^{2}}{2}\\cdot u_{r}-\\frac{2n_{1}n_{2}}{T}+v_{r},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\{u_{r}\\}_{r=0}^{T}$ and $\\{v_{r}\\}_{r=0}^{T}$ are recursively defined via $u_{r+1}\\,=\\,u_{r}\\,+\\,\\left(u_{r}+{\\frac{1}{T}}\\right)^{2}$ and $v_{r+1}\\,=$ ur + vr + r+1\u2212 with $u_{0}=v_{0}=0$ (Lemma F.3). Since $\\mathrm{VAL}=\\mathcal{V}_{0,0,T}=v_{T}$ , it remains to show $\\bar{v}_{T}=\\Omega(\\log\\dot{T})$ , which is done via two technical lemmas F.4 and F.5. ", "page_idx": 7}, {"type": "text", "text": "4.2 Decomposable Losses ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Next, we consider another sub-class of proper losses that are not necessarily Lipschitz. Instead, their univariate form is decomposable over the $K$ outcomes and additionally satisfies a mild regularity condition. Specifically, we define the following class ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{C}_{d e c}:=\\left\\{\\ell\\in\\mathcal{L}\\,\\bigg|\\,\\ell(p)\\propto\\sum_{i=1}^{K}\\ell_{i}(p_{i})\\mathrm{~where~each~}\\ell_{i}\\mathrm{~is~twice~continuously~differentiable~in~(0,1)}\\,\\right\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Both the squared loss and its generalization via Tsallis entropy discussed in Section 2 are clearly in this class $\\mathcal{L}_{d e c}$ , with the latter being non-Lipschitz when $\\alpha\\in(1,2)$ . The spherical loss, however, is not decomposable and thus not in $\\mathcal{L}_{d e c}$ . We now show that FTL achieves logarithmic regret against any $\\ell\\in{\\mathcal{L}}_{d e c}$ (see Appendix G for the full proof). ", "page_idx": 7}, {"type": "text", "text": "Theorem 5. The regret of FTL for learning any $\\ell\\in{\\mathcal{L}}_{d e c}$ is at most $2K+(K+1)\\beta_{\\ell}(1+\\log T)$ for some universal constant $\\beta_{\\ell}$ which only depends on $\\ell$ and $K$ . Consequently, FTL ensures $\\mathsf{P U C a l}_{\\mathcal{L}_{d e c}}=$ $\\begin{array}{r}{\\mathsf{U C a l}_{\\mathcal{L}_{d e c}}=\\mathcal{O}((\\operatorname*{sup}_{\\ell\\in\\mathcal{L}_{d e c}}\\beta_{\\ell})K\\log T)}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "Proof Sketch. We start by showing a certain controlled growth rate of the second derivative of the univariate form (see Appendix $_\\mathrm{H}$ for the proof). ", "page_idx": 7}, {"type": "text", "text": "Lemma 2. For a function $f$ that is concave, Lipschitz, and bounded over $[0,1]$ and twice continuously differentiable over $(0,1)$ , there exists a constant $c>0$ such that $\\begin{array}{r}{|f^{\\prime\\prime}(p)|\\leq c\\cdot\\operatorname*{max}\\left(\\frac{1}{p},\\frac{1}{1-p}\\right).}\\end{array}$ for all $p\\in(0,1)$ . ", "page_idx": 7}, {"type": "text", "text": "Note that according to Lemma 1, each $\\ell_{i}$ must be concave, Lipschitz, and bounded, for the induced loss to be proper and bounded. Therefore, using Lemma 2, there exists a constant $c_{i}>0$ such that $|\\ell_{i}^{\\prime\\prime}(p)|\\,\\leq\\,c_{i}\\operatorname*{max}\\left(\\textstyle{\\frac{1}{p}},\\textstyle{\\frac{1}{1-p}}\\right)$ p1,1\u22121p for each i. The rest of the proof in fact only relies on this property; in other words, the regret bound holds even if one replaces the twice continuous differentiability condition with this (weaker) property. ", "page_idx": 8}, {"type": "text", "text": "More specifically, for each $i\\in[K]$ , let $T_{i}:=\\{t_{i,1},\\ldots,t_{i,k_{i}}\\}\\subset[T]$ be the subset of rounds where the true outcome is $i$ (which could be empty). Then, using the Be-the-Leader lemma again and trivially bounding the regret by its maximum value for the (at most $K$ ) rounds when an outcome appears for the first time, we obtain ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathsf{R E G}_{\\ell}\\leq2K+\\sum_{i=1}^{K}\\sum_{t\\in\\mathcal{T}_{i}\\setminus\\{t_{i,1}\\}}\\underbrace{\\ell(p_{t},e_{i})-\\ell(p_{t+1},e_{i})}_{\\delta_{t,i}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "By using the characterization result in Lemma 1, we then express $\\ell(p_{t},e_{i})$ and $\\ell(\\pmb{p}_{t+1},\\pmb{e}_{i})$ in terms of the univariate forms $\\ell(\\pmb{p}_{t}),\\ell(\\pmb{p}_{t+1})$ , and their respective gradients $\\nabla\\ell(p_{t}),\\nabla\\ell(p_{t+1})$ . Next, using the concavity of $\\ell_{i}$ , the Mean Value Theorem, and Lemma 2, we argue that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\delta_{t,i}\\leq\\sum_{j=1}^{K}\\beta_{\\ell,j}\\cdot|p_{t+1,j}-p_{t,j}|\\cdot\\operatorname*{max}\\left(\\frac{1}{\\xi_{t,j}},\\frac{1}{1-\\xi_{t,j}}\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "for some $\\xi_{t}$ that is a convex combination of $\\scriptstyle{\\mathbf{\\mathit{p}}}_{t}$ and $\\pmb{p}_{t+1}$ , and constant $\\beta_{\\ell,i}\\,=\\,\\tilde{c}_{K}\\cdot c_{i}$ ( $\\tilde{c}_{K}$ is the scaling constant such that $\\begin{array}{r}{\\ell(\\pmb{p})=\\tilde{c}_{K}\\sum_{i=1}^{K}\\ell_{i}(p_{i}))}\\end{array}$ . To bound (4), we consider the terms $\\textstyle\\frac{|p_{t+1,j}-p_{t,j}|}{\\xi_{t,j}}$ and $\\begin{array}{r}{\\frac{|p_{t+1,j}-p_{t,j}|}{1-\\xi_{t,j}}}\\end{array}$ individually and find that they are always bounded by either $\\frac{1}{n_{t-1,i}}$ or $\\frac{1}{t-1}$ according to the update rule of FTL. Thus, we obtain ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathsf{R E G}_{\\ell}\\le2K+\\beta_{\\ell}(\\mathcal{S}_{1}+\\mathcal{S}_{2}),\\mathrm{~where~}\\mathcal{S}_{1}=\\sum_{i=1}^{K}\\sum_{t\\in\\mathcal{T}_{i}\\setminus\\{t_{i,1}\\}}\\frac{1}{n_{t-1,i}},\\mathcal{S}_{2}=\\sum_{i=1}^{K}\\sum_{t\\in\\mathcal{T}_{i}\\setminus\\{t_{i,1}\\}}\\frac{1}{t-1},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and $\\begin{array}{r}{\\beta_{\\ell}=\\sum_{i=1}^{K}\\beta_{\\ell,i}}\\end{array}$ . Finally, direct calculation shows $S_{1}\\,\\le\\,K(1+\\log T)$ and $S_{2}\\,\\leq\\,1+\\log T_{:}$ which finishes the proof. \u53e3 ", "page_idx": 8}, {"type": "text", "text": "To showcase the usefulness of this result, we go back to the Tsallis entropy example. ", "page_idx": 8}, {"type": "text", "text": "Corollary 3. For any loss $\\ell$ with univariate form $\\begin{array}{r}{\\ell(\\pmb{p})=-\\tilde{c}_{K}\\sum_{i=1}^{K}p_{i}^{\\alpha}}\\end{array}$ for $\\alpha\\in(1,2)$ (the constant $\\tilde{c}_{K}$ is such that the loss has range $[-1,1].$ ), FTL ensures $\\mathrm{REG}_{\\ell}=\\mathcal{O}(\\tilde{c}_{K}\\alpha(\\alpha-1)K^{2}\\log T)$ . ", "page_idx": 8}, {"type": "text", "text": "Proof. As mentioned, our proof of Theorem 5 only relies on Lemma 2, and it is straightforward to verify that for the loss considered here, one can take the constant $c$ in Lemma 2 to be $\\alpha(\\alpha-1)$ , and thus the regret of FTL is ${\\mathcal{O}}(K\\beta_{\\ell}\\log T)$ with $\\beta_{\\ell}=K\\widetilde{c}_{K}\\alpha(\\alpha-1)$ . \u53e3 ", "page_idx": 8}, {"type": "text", "text": "On the other hand, if one were to use the proof based on local Lipschitzness (mentioned in Section 4.1 and discussed in Appendix E), one would only obtain a regret bound of order $\\mathcal{O}(K+\\tilde{c}_{K}\\alpha(\\alpha-$ $1)T^{2-\\alpha}\\log T)$ , which is much worse (especially for small $\\alpha$ ). Finally, we remark that for $\\alpha\\geq2$ , the bivariate form is Lipschitz, and thus FTL also ensures logarithmic regret according to Theorem 3. ", "page_idx": 8}, {"type": "text", "text": "4.3 FTL Cannot Handle General Proper Losses ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Despite yielding improved regret for Lipschitz and other special classes of proper losses, unfortunately, FTL is not a good algorithm in general when dealing with proper losses, as shown below. ", "page_idx": 8}, {"type": "text", "text": "Theorem 6. There exists a proper loss $\\ell$ and a choice of $y_{1},\\dots,y_{T}$ by an oblivious adversary such that the regret $\\mathbf{REG}_{\\ell}$ of $F T L$ is $\\Omega(T)$ . ", "page_idx": 8}, {"type": "text", "text": "Proof. The loss we consider is in fa\u221act the same V-shaped loss used in the proof of Theorem 2 that shows all algorithms must suffer $\\Omega({\\sqrt{K T}})$ regret. Here, we show that FTL even suffers linear regret ", "page_idx": 8}, {"type": "text", "text": "for this loss. Specifically, it suffices to consider the binary case $K\\,=\\,2$ and the univariate form $\\begin{array}{r}{\\ell(\\pmb{p})=-\\frac{1}{2}\\left(\\left|p_{1}^{\\bot}-\\frac{1}{2}\\right|+\\left|\\tilde{p}_{2}-\\frac{1}{2}\\right|\\right)}\\end{array}$ . Using Lemma 1, we obtain the following bivariate form: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\ell(\\pmb{p},e_{1})=-\\frac{1}{2}\\mathrm{sign}\\left(p_{1}-\\frac{1}{2}\\right),\\quad\\ell(\\pmb{p},e_{2})=-\\frac{1}{2}\\mathrm{sign}\\left(p_{2}-\\frac{1}{2}\\right),\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where the sign function is defined as $\\mathrm{sign}(x)=1$ if $x\\,>\\,0$ ; $-1$ if $x<0;0$ if $x\\,=\\,0$ . Therefore, $\\ell(p,e_{1})$ is equal to $\\frac{1}{2}$ if $\\begin{array}{r}{p_{1}<\\frac{1}{2}}\\end{array}$ ; 0 if $\\begin{array}{r}{p_{1}=\\frac{1}{2}}\\end{array}$ ; $-\\,{\\frac{1}{2}}$ if $\\begin{array}{r}{p_{1}\\geq\\frac{1}{2}}\\end{array}$ . Similarly, $\\ell(p,e_{2})$ is equal to $-\\frac12$ if $p_{1}\\leq\\textstyle{\\frac{1}{2}}$ ; 0 if $\\begin{array}{r}{p_{1}=\\frac{1}{2}}\\end{array}$ ; $\\frac{1}{2}$ if $\\begin{array}{r}{p_{1}\\geq\\frac{1}{2}}\\end{array}$ . Let $T$ be even and ${\\pmb y}_{t}=e_{1}$ if $t$ is odd, and $e_{2}$ otherwise. For such a sequence $y_{1},\\dots,y_{T}$ , the benchmark selects $\\begin{array}{r}{\\beta=\\frac{1}{T}\\sum_{t=1}^{T}{y_{t}}=\\left[\\frac{1}{2},\\frac{1}{2}\\right]}\\end{array}$ and incurs 0 cost. On the other hand, FTL chooses pt = [ 12, 12] when t is odd, and pt = 2(tt\u22121),2(tt\u2212\u221221) otherwise. Thus, the regret of FTL is $\\begin{array}{r}{\\mathrm{REG}_{\\ell}=\\sum_{t=2}^{T}\\ell(\\pmb{p}_{t},\\pmb{e}_{2})=\\frac{T}{4}}\\end{array}$ . This completes the proof. \u53e3 ", "page_idx": 9}, {"type": "text", "text": "Consider the parametrized class in subsection 3.3, let $K=2$ , $\\ell_{2}(p,y)$ correspond to the ${\\mathrm{V}}.$ -shaped loss in Theorem 6, and consider any $\\ell_{1}(p,y)\\,\\in\\,\\mathcal{L}$ . It follows from Theorem 6 that $\\boldsymbol{\\mathrm{REG}_{\\ell_{\\alpha}}}\\,=$ $\\Omega(T)$ when $\\alpha=0$ , therefore $\\begin{array}{r}{\\mathsf{U C a l}_{\\mathcal{L}^{\\prime}}=\\operatorname*{sup}_{\\alpha\\in[0,1]}\\mathbf{REG}_{\\ell_{\\alpha}}=\\Omega(T)}\\end{array}$ , whereas Algorithm 1 ensures $\\mathsf{U C a l}_{\\mathcal{L}^{\\prime}}=\\mathcal{O}(\\sqrt{T\\log T})$ . ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Future Directions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we give complete answers to various questions regarding the minimax optimal bounds on multiclass U-calibration error, a notion of simultaneous loss minimization proposed by (Kleinberg et al., 2023) for the fundamental pr\u221aoblem of making online forecasts on unknown outcomes. We not only improve their $\\mathsf{P U C a l}={\\mathcal O}(K\\sqrt{T})$ upper bound and show that the minimax pseudo U-calibration error is $\\Theta({\\sqrt{K T}})$ , but also further show that logarithmic U-calibration error can be achieved by an extremely simple algorithm for several important classes of proper losses. ", "page_idx": 9}, {"type": "text", "text": "There are many interesting future directions, including 1) understanding the optimal bound on the actual U-calibration error UCal, 2) generalizing the results to losses that are not necessarily proper, and 3) studying the contextual case and developing more efficient algorithms with better bounds compared to those in the recent work of Garg et al. (2024). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank mathoverflow user mathworker21 for their help in formulating and proving Lemma F.5. Haipeng Luo was supported by NSF award IIS-1943607 and a Google Research Scholar Award, and Vatsal Sharan was supported by NSF CAREER Award CCF-2239265 and an Amazon Research Award. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of Amazon. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Abernethy, J., Bartlett, P. L., Rakhlin, A., and Tewari, A. (2008). Optimal strategies and minimax lower bounds for online convex games. In Proceedings of the 21st annual conference on learning theory, pages 414\u2013424. ", "page_idx": 9}, {"type": "text", "text": "Arunachalesw\u221aaran, E. R., Collina, N., Roth, A., and Shi, M. (2024). An elementary predictor obtaining $2\\sqrt{T}$ distance to calibration. arXiv preprint arXiv:2402.11410. ", "page_idx": 9}, {"type": "text", "text": "B\u0142asiok, J., Gopalan, P., Hu, L., Kalai, A. T., and Nakkiran, P. (2024). Loss Minimization Yields Multicalibration for Large Neural Networks. In Guruswami, V., editor, 15th Innovations in Theoretical Computer Science Conference (ITCS 2024), volume 287 of Leibniz International Proceedings in Informatics (LIPIcs), pages 17:1\u201317:21, Dagstuhl, Germany. Schloss Dagstuhl \u2013 Leibniz-Zentrum f\u00fcr Informatik. ", "page_idx": 9}, {"type": "text", "text": "B\u0142asiok, J., Gopalan, P., Hu, L., and Nakkiran, P. (2023). A unifying theory of distance from calibration. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing, pages 1727\u20131740. ", "page_idx": 9}, {"type": "text", "text": "Blum, A., Hajiaghayi, M., Ligett, K., and Roth, A. (2008). Regret minimization and the price of total anarchy. In Proceedings of the fortieth annual ACM symposium on Theory of computing, pages 373\u2013382. ", "page_idx": 10}, {"type": "text", "text": "Blum, A. and Mansour, Y. (2007). From external to internal regret. Journal of Machine Learning Research, 8(6). ", "page_idx": 10}, {"type": "text", "text": "Daskalakis, C. and Syrgkanis, V. (2016). Learning in auctions: Regret is hard, envy is easy. In 2016 ieee 57th annual symposium on foundations of computer science (focs), pages 219\u2013228. IEEE. ", "page_idx": 10}, {"type": "text", "text": "Dawid, A. P. (1982). The well-calibrated bayesian. Journal of the American Statistical Association, 77(379):605\u2013610. ", "page_idx": 10}, {"type": "text", "text": "Garg, S., Jung, C., Reingold, O., and Roth, A. (2024). Oracle efficient online multicalibration and omniprediction. In Proceedings of the 2024 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 2725\u20132792. SIAM. ", "page_idx": 10}, {"type": "text", "text": "Gneiting, T. and Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and estimation. Journal of the American statistical Association, 102(477):359\u2013378. ", "page_idx": 10}, {"type": "text", "text": "Gopalan, P., Hu, L., Kim, M. P., Reingold, O., and Wieder, U. (2023a). Loss Minimization Through the Lens Of Outcome Indistinguishability. In Tauman Kalai, Y., editor, 14th Innovations in Theoretical Computer Science Conference (ITCS 2023), volume 251 of Leibniz International Proceedings in Informatics (LIPIcs), pages 60:1\u201360:20, Dagstuhl, Germany. Schloss Dagstuhl \u2013 Leibniz-Zentrum f\u00fcr Informatik. ", "page_idx": 10}, {"type": "text", "text": "Gopalan, P., Kalai, A. T., Reingold, O., Sharan, V., and Wieder, U. (2022). Omnipredictors. In Braverman, M., editor, 13th Innovations in Theoretical Computer Science Conference (ITCS 2022), volume 215 of Leibniz International Proceedings in Informatics (LIPIcs), pages 79:1\u201379:21, Dagstuhl, Germany. Schloss Dagstuhl \u2013 Leibniz-Zentrum f\u00fcr Informatik. ", "page_idx": 10}, {"type": "text", "text": "Gopalan, P., Kim, M. P., and Reingold, O. (2023b). Swap agnostic learning, or characterizing omniprediction via multicalibration. In Thirty-seventh Conference on Neural Information Processing Systems. ", "page_idx": 10}, {"type": "text", "text": "Hart, S. (2022). Calibrated forecasts: The minimax proof. arXiv preprint arXiv:2209.05863. ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "H\u00e9bert-Johnson, U., Kim, M., Reingold, O., and Rothblum, G. (2018). Multicalibration: Calibration for the (computationally-identifiable) masses. In International Conference on Machine Learning, pages 1939\u20131948. PMLR. ", "page_idx": 10}, {"type": "text", "text": "Hutter, M., Poland, J., et al. (2005). Adaptive online prediction by following the perturbed leader. ", "page_idx": 10}, {"type": "text", "text": "Klein, P. and Young, N. (1999). On the number of iterations for dantzig-wolfe optimization and packing-covering approximation algorithms. In International Conference on Integer Programming and Combinatorial Optimization, pages 320\u2013327. Springer. ", "page_idx": 10}, {"type": "text", "text": "Kleinberg, B., Leme, R. P., Schneider, J., and Teng, Y. (2023). U-calibration: Forecasting for an unknown agent. In The Thirty Sixth Annual Conference on Learning Theory, pages 5143\u20135145. PMLR. ", "page_idx": 10}, {"type": "text", "text": "Orabona, F. (2019). A modern introduction to online learning. arXiv preprint arXiv:1912.13213. ", "page_idx": 10}, {"type": "text", "text": "Qiao, M. and Valiant, G. (2021). Stronger calibration lower bounds via sidestepping. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 456\u2013466. ", "page_idx": 10}, {"type": "text", "text": "Qiao, M. and Zheng, L. (2024). On the distance from calibration in sequential prediction. arXiv preprint arXiv:2402.07458. ", "page_idx": 10}, {"type": "text", "text": "Wainwright, M. J. (2019). High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge university press. ", "page_idx": 10}, {"type": "text", "text": "A Concentration and Anti-Concentration Inequalities ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Lemma A.1 (Markov\u2019s inequality). For a non-negative random variable $X$ , and any $a>0$ , we have $\\begin{array}{r}{\\mathbb{P}(X\\geq a)\\leq{\\frac{\\mathbb{E}[X]}{a}}}\\end{array}$ . ", "page_idx": 11}, {"type": "text", "text": "Lemma A.2 (Khintchine\u2019s inequality). Let $\\epsilon_{1},\\hdots,\\epsilon_{T}$ be i.i.d. Rademacher random variables, i.e., $\\mathbb{P}(\\epsilon_{i}=+1)=\\mathbb{P}(\\epsilon_{i}=-1)=\\frac{1}{2}$ for all $i\\in[T]$ . Then, ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left|\\sum_{i=1}^{T}\\epsilon_{i}x_{i}\\right|\\geq\\frac{1}{\\sqrt{2}}\\left(\\sum_{i=1}^{T}x_{i}^{2}\\right)^{\\frac{1}{2}}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "for any $x_{1},\\ldots,x_{T}\\in\\mathbb{R}$ . ", "page_idx": 11}, {"type": "text", "text": "Lemma A.3 (Hoeffding\u2019s inequality). Let $X_{1},\\ldots,X_{T}$ be independent random variables satisfying $a_{i}\\leq X_{i}\\leq b_{i}$ for all $i\\in[T]$ . Then, for any $\\epsilon>0$ , we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\displaystyle\\sum_{t=1}^{T}X_{T}-\\mathbb{E}[X_{t}]\\ge\\epsilon\\right)\\le\\exp\\left(-\\frac{2\\epsilon^{2}}{\\sum_{i=1}^{T}(b_{i}-a_{i})^{2}}\\right),a n d}\\\\ &{\\mathbb{P}\\left(\\displaystyle\\sum_{t=1}^{T}X_{T}-\\mathbb{E}[X_{t}]\\le-\\epsilon\\right)\\le\\exp\\left(-\\frac{2\\epsilon^{2}}{\\sum_{i=1}^{T}(b_{i}-a_{i})^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Lemma A.4 (Reverse Chernoff bounds). (Klein and Young, 1999, Lemma 5.2) Let $\\begin{array}{r}{\\bar{X}:=\\frac{1}{T}\\sum_{i=1}^{T}X_{i}}\\end{array}$ be the average of $T$ i.i.d. Bernoulli random variables $X_{1},\\ldots,X_{T}$ with $\\mathbb{E}[X_{i}]=p$ for all $i\\in[T]$ . If $\\epsilon\\in(0,\\frac{1}{2}],p\\in(0,\\frac{1}{2}]$ are such that $\\epsilon^{2}p T\\geq3$ , then ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(\\bar{X}\\leq(1-\\epsilon)p)\\geq\\exp(-9\\epsilon^{2}p T),\\quad\\mathbb{P}(\\bar{X}\\geq(1+\\epsilon)p)\\geq\\exp(-9\\epsilon^{2}p T).}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "B Deferred Proofs for Proper Losses ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Lemma B.1. For a proper loss $\\ell(p,y)$ , the following holds true for any $n\\in\\mathbb N$ and $y_{1},\\ldots,y_{n}\\in\\mathcal{E}$ : ", "page_idx": 11}, {"type": "equation", "text": "$$\n{\\frac{1}{n}}\\sum_{i=1}^{n}\\pmb{y}_{i}\\in\\operatorname{argmin}_{p\\in\\Delta_{K}}{\\frac{1}{n}}\\sum_{i=1}^{n}\\ell(\\pmb{p},\\pmb{y}).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Proof. Let $\\begin{array}{r}{\\beta:=\\frac{1}{n}\\sum_{i=1}^{n}y_{i}}\\end{array}$ . Since $\\ell$ is proper, $\\mathbb{E}_{\\pmb{y}\\sim\\pmb{\\beta}}[\\ell(\\pmb{\\beta},\\pmb{y})]\\leq\\mathbb{E}_{\\pmb{y}\\sim\\pmb{\\beta}}[\\ell(\\pmb{p}^{\\prime},\\pmb{y})]$ for all $\\pmb{p}^{\\prime}\\in\\Delta_{K}$ . Notably, for any $\\pmb{p}\\in\\Delta_{K}$ , we have ", "page_idx": 11}, {"type": "text", "text": "$\\sum_{i=1}^{K}\\beta_{i}\\ell(p,e_{i})=\\frac{1}{n}\\sum_{i=1}^{K}\\sum_{j=1}^{n}\\mathbf{1}[y_{j}=e_{i}]\\ell(p,e_{i})=\\frac{1}{n}\\sum_{j=1}^{n}\\sum_{i=1}^{K}\\mathbf{1}[y_{j}=e_{i}]\\ell(p,e_{i})=\\frac{1}{n}\\sum_{j=1}^{n}\\ell(p,y_{j}).$ Thus, $\\begin{array}{r}{\\frac{1}{n}\\sum_{j=1}^{n}\\ell(\\boldsymbol{\\beta},\\pmb{y}_{j})\\leq\\frac{1}{n}\\sum_{j=1}^{n}\\ell(\\pmb{p}^{\\prime},\\pmb{y}_{j})}\\end{array}$ for any $\\pmb{p}^{\\prime}\\in\\Delta_{K}$ . This completes the proof. ", "page_idx": 11}, {"type": "text", "text": "Lemma B.2. Let $\\ell(p)$ be a differentiable and concave, $\\alpha$ -Lipschitz function over $\\Delta_{K}$ such that $\\nabla\\ell(p)$ is $\\beta$ -Lipschitz over $\\Delta_{K}$ . Then, $\\ell(p,y)$ is proper and $\\left(2\\alpha+2\\beta\\right)$ -Lipschitz in $p\\in\\Delta_{K}$ for each $\\pmb{y}\\in\\mathcal{E}$ . ", "page_idx": 11}, {"type": "text", "text": "Proof. Since $\\ell(p)$ is concave, it follows from Lemma 1 that $\\ell({\\pmb p},{\\pmb y})$ is proper. To prove the second part, without any loss of generality, assume that $\\pmb{y}=e_{1}$ . For any $p,p^{\\prime}\\in\\Delta_{K}$ , we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell(p,e_{1})-\\ell(p^{\\prime},e_{1})=\\ell(p)+\\langle\\nabla\\ell(p),e_{1}-p\\rangle-(\\ell(p^{\\prime})+\\langle\\nabla\\ell(p^{\\prime}),e_{1}-p^{\\prime}\\rangle)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\alpha\\left\\|p-p^{\\prime}\\right\\|+\\nabla_{1}\\ell(p)-\\nabla_{1}\\ell(p^{\\prime})-(\\langle p,\\nabla\\ell(p)\\rangle-\\langle p^{\\prime},\\nabla\\ell(p^{\\prime})\\rangle)}\\\\ &{\\qquad\\qquad\\qquad\\leq(\\alpha+\\beta)\\left\\|p-p^{\\prime}\\right\\|+\\langle p^{\\prime}-p,\\nabla\\ell(p^{\\prime})\\rangle+\\langle p,\\nabla\\ell(p^{\\prime})-\\nabla\\ell(p)\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\leq(\\alpha+\\beta)\\left\\|p-p^{\\prime}\\right\\|+\\left\\|p^{\\prime}-p\\right\\|\\left\\|\\nabla\\ell(p^{\\prime})\\right\\|+\\left\\|p\\right\\|\\left\\|\\nabla\\ell(p^{\\prime})-\\nabla\\ell(p)\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq2(\\alpha+\\beta)\\left\\|p-p^{\\prime}\\right\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where the first equality follows from Lemma 1; the first inequality follows from the Lipschitzness of $\\ell$ ; the second inequality follows from the Lipschitzness of $\\bar{\\nabla}\\ell(\\pmb{p})$ ; the third inequality follows from the Cauchy-Schwartz inequality; the final inequality follows since $\\|\\pmb{p}\\|\\leq1$ and $\\nabla\\ell(\\pmb{p})$ is Lipschitz. This completes the proof. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "Proposition B.1. The spherical loss is $\\sqrt{K}$ -Lipschitz. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Proof. We shall show that $\\|\\nabla\\ell(p,y)\\|\\,\\leq\\,{\\sqrt{K}}$ for all $\\pmb{{y}}\\,\\in\\,\\mathcal{E},\\pmb{{p}}\\,\\in\\,\\Delta_{K}$ , where the gradient is taken with respect to the first argument. Without any loss of generality, assume $\\pmb{y}~=~e_{1}$ , thus $\\begin{array}{r}{\\ell(p,y)=-\\frac{p_{1}^{\\intercal}}{\\|p\\|}}\\end{array}$ . It is easy to obtain the following: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{\\partial\\ell(\\pmb{p},\\pmb{y})}{\\partial p_{1}}=-\\frac{\\left\\|\\pmb{p}\\right\\|^{2}-p_{1}^{2}}{\\left\\|\\pmb{p}\\right\\|^{3}},\\quad\\frac{\\partial\\ell(\\pmb{p},\\pmb{y})}{\\partial p_{i}}=\\frac{p_{1}p_{i}}{\\left\\|\\pmb{p}\\right\\|^{3}}\\ \\mathrm{~for~all~}i>2.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Thus, $\\begin{array}{r}{\\|\\nabla\\ell(p,e_{1})\\|\\,=\\,\\frac{1}{\\|p\\|^{3}}\\sqrt{(\\|p\\|^{2}-p_{1}^{2})^{2}+p_{1}^{2}\\sum_{i=2}^{K}p_{i}^{2}}\\,=\\,\\frac{1}{\\|p\\|^{2}}\\sqrt{(\\|p\\|^{2}-p_{1}^{2})}\\,\\le\\,\\frac{1}{\\|p\\|}\\,\\le\\,\\sqrt{K},}\\end{array}$ where the last inequality follows the Cauchy-Schwartz inequality. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "C Proof of Theorem 2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Theorem 2. There exists a proper loss $\\ell$ with range $[-1,1]$ such that the following holds: for any online algorithm ALG, there exists a c\u221ahoice of $y_{1},\\dots,y_{T}$ by an oblivious adversary such that the expected regret $\\mathbb{E}[\\mathbf{REG}_{\\ell}]$ of ALG is $\\Omega({\\sqrt{K T}})$ when $T\\geq12K$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. Consider the function defined as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\ell(\\pmb{p}):=-\\frac{1}{2}\\sum_{i=1}^{K}\\left|p_{i}-\\frac{1}{K}\\right|.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "It follows from Lemma 1 that the bivariate form of $\\ell$ is $\\ell(\\pmb{p},\\pmb{y})=\\ell(\\pmb{p})+\\langle\\pmb{g_{p}},\\pmb{p}-\\pmb{y}\\rangle$ , where ${\\pmb g}_{p}$ denotes a subgradient of $\\ell(p)$ at $\\textbf{\\emph{p}}$ . For the choice of $\\ell$ , $\\begin{array}{r}{g_{i}=-\\frac{1}{2}\\mathrm{sign}(p_{i}-\\frac{1}{K})}\\end{array}$ , where the sign function is defined as $\\mathrm{sign}(x)=1$ if $x>0$ ; $-1$ if $x<0$ ; 0 if $x=0$ . We first show that $\\ell(\\pmb{p},\\pmb{y})\\in[-1,1]$ . Without any loss of generality, assume $\\scriptstyle y\\;=\\;e_{1}$ . Therefore, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\ell(p,e_{1})=\\frac{1}{2}\\left[-\\sum_{i=1}^{K}\\left|p_{i}-\\frac{1}{K}\\right|-(1-p_{1})\\mathrm{sign}\\left(p_{1}-\\frac{1}{K}\\right)+\\sum_{i=2}^{K}p_{i}\\mathrm{sign}\\left(p_{i}-\\frac{1}{K}\\right)\\right]}\\\\ {\\displaystyle=\\frac{1}{2}\\left[-\\left|p_{1}-\\frac{1}{K}\\right|-(1-p_{1})\\mathrm{sign}\\left(p_{1}-\\frac{1}{K}\\right)+\\sum_{i=2}^{K}p_{i}\\mathrm{sign}\\left(p_{i}-\\frac{1}{K}\\right)-\\left|p_{i}-\\frac{1}{K}\\right|\\right]}\\\\ {\\displaystyle=\\frac{1}{2}\\left[-\\left(1-\\frac{1}{K}\\right)\\mathrm{sign}\\left(p_{1}-\\frac{1}{K}\\right)+\\frac{1}{K}\\sum_{i=2}^{K}\\mathrm{sign}\\left(p_{i}-\\frac{1}{K}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "It is then trivial to note that $\\pmb{p}=e_{1}$ corresponds to a minimum, with value $\\begin{array}{r}{\\ell(e_{1},e_{1})\\,=\\,-\\frac{K-1}{K}}\\end{array}$ ; $\\begin{array}{r}{p=[0,\\frac{1}{K-1},\\cdot\\cdot\\cdot,\\frac{1}{K-1}]}\\end{array}$ corresponds to a maximum, with value $\\textstyle{\\frac{K-1}{K}}$ ", "page_idx": 12}, {"type": "text", "text": "Next, we consider a randomized oblivious adversary which samples $y_{1},\\dots,y_{T}$ from the uniform distribution over $\\mathcal{E}$ . For such an adversary, we shall show that $\\mathbb{E}[\\mathbf{R}\\mathrm{EG}]=\\Omega(\\sqrt{K T})$ . We overload the notation and use $\\mathsf{A L G}_{1:t}$ to denote the internal randomness of the algorithm until time $t$ (inclusive). In particular, this notation succintly represents both deterministic and randomized algorithms $\\left({\\pmb{p}}_{t}\\right)$ could be sampled from a distribution $\\mathcal{D}_{t}$ ). Similarly, $\\mathsf{A L G}_{t}$ shall denote the randomness at time $t$ ", "page_idx": 12}, {"type": "text", "text": "With this notation, in the constructed randomized environment, the expected cost of ALG is ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\underset{t=1}{\\overset{T}{\\sum}}\\ell(p_{t},y_{t})\\right]=\\mathbb{E}_{\\mathrm{AL}_{1};\\tau_{1},y_{1},\\ldots,y_{T}}\\left[\\underset{t=1}{\\overset{T}{\\sum}}\\ell(p_{t},y_{t})\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\left.=\\underset{t=1}{\\overset{T}{\\sum}}\\mathbb{E}_{\\mathrm{AL}_{1};\\tau_{1},y_{1},\\ldots,y_{T}}[\\ell(p_{t},y_{t})]\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.=\\underset{t=1}{\\overset{T}{\\sum}}\\mathbb{E}_{\\mathrm{AL}_{1};\\tau_{1},\\ldots,y_{t-1},y_{t-1},\\ldots,y_{t-1}}\\mathbb{E}_{\\mathrm{AL}_{1};\\tau_{1}}[\\ell(p_{t},y_{t})|\\mathbb{A}\\mathrm{L}_{1:\\tau_{1}-1},y_{1},\\ldots,y_{t-1}]\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.\\underset{t=1}{\\overset{T}{\\sum}}\\mathbb{E}_{\\mathrm{AL}_{1};\\tau_{1},y_{1},\\ldots,y_{t-1}}\\mathbb{E}_{\\mathrm{AL}_{1};\\tau_{1}}[\\ell(p_{t},y_{t})|\\mathbb{A}\\mathrm{L}_{1:\\tau_{1}-1},y_{1},\\ldots,y_{t-1}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.=\\underset{t=1}{\\overset{T}{\\sum}}\\mathbb{E}_{\\mathrm{AL}_{1};\\tau_{1},y_{1},\\ldots,y_{t-1}}\\mathbb{E}_{\\mathrm{AL}_{1}}\\left[\\frac{1}{K}\\underset{t=1}{\\overset{S}{\\sum}}\\ell(p_{t},e_{t})\\right]\\geq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where in the first equality we have made the randomness explicit; the second equality follows from the linearity of expectations; the third equality follows from the law of iterated expectations; the fourth equality follows because $\\mathsf{A L G}_{t}$ is independent of $\\scriptstyle\\pmb{y}_{t}$ ( $\\mathbf{\\Psi}_{p_{t}}$ is chosen without knowing $\\scriptstyle\\pmb{y}_{t}$ ) and vice-versa $\\textbf{\\textit{y}}_{t}$ is sampled uniformly randomly from $\\mathcal{E}$ ); the fifth equality follows by expanding out the expectation; the first inequality follows since $\\begin{array}{r}{\\sum_{i=1}^{K}\\ell(\\pmb{p},\\pmb{e}_{i})\\geq0}\\end{array}$ for any $\\pmb{p}\\in\\Delta_{K}$ . This is because, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{K}\\ell(\\boldsymbol{p},\\boldsymbol{e}_{i})=K\\ell(\\boldsymbol{p})+\\sum_{i=1}^{K}\\left\\langle g_{\\boldsymbol{p}},\\boldsymbol{e}_{i}-\\boldsymbol{p}\\right\\rangle=K\\left(\\ell(\\boldsymbol{p})+\\left\\langle g_{\\boldsymbol{p}},\\frac{1}{K}\\cdot\\mathbf{1}_{K}-\\boldsymbol{p}\\right\\rangle\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where ${\\bf1}_{K}$ denotes the $K$ -dimensional vector of all ones. Next, since $\\ell(p)$ is concave over $\\Delta_{K}$ , the term above can be lower bounded by $\\begin{array}{r}{K\\ell\\left(\\frac{1}{K}\\cdot\\mathbf{1}_{K}\\right)=0}\\end{array}$ . ", "page_idx": 13}, {"type": "text", "text": "Next, the expected regret of ALG can be lower bounded in the following manner: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathrm{REG}_{\\ell}]=\\mathbb{E}_{\\mathsf{A L G}_{1:T},y_{1},\\ldots,y_{T}}\\left[\\displaystyle\\sum_{t=1}^{T}\\ell(p_{t},y_{t})-\\operatorname*{inf}_{p\\in\\Delta_{K}}\\sum_{t=1}^{T}\\ell(p,y_{t})\\right]}\\\\ &{\\phantom{\\sum_{t}^{t}}\\ge-\\mathbb{E}_{\\mathfrak{y}_{1},\\ldots,y_{T}}\\left[\\displaystyle\\operatorname*{inf}_{p\\in\\Delta_{K}}\\sum_{t=1}^{T}\\ell(p,y_{t})\\right]}\\\\ &{\\phantom{\\sum_{t}^{t}}=-\\mathbb{E}_{\\mathfrak{y}_{1},\\ldots,y_{T}}\\left[\\displaystyle\\sum_{t=1}^{T}\\ell\\left(\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}y_{t},y_{t}\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the first inequality follows since the expected cost of ALG is non-negative, and the benchmark is independent of ALG; the second equality follows from property 1. In the next steps, we deal with the expectation in (5). Sample $y_{1},\\dots,y_{T}$ from $\\mathcal{E}$ and let $n_{1},\\dots,n_{K}$ denote the counts of the $K$ basis vectors, i.e., $n_{i}=|\\{j\\in[T];{\\pmb y}_{j}={\\pmb e}_{i}\\}|$ . Clearly, $\\textstyle\\sum_{i=1}^{K}n_{i}=T$ . Let $\\pmb{n}=[n_{1},...,n_{K}]$ collect these counts. Then, $\\begin{array}{r}{{\\frac{1}{T}}\\sum_{t=1}^{T}\\pmb{y}_{t}=\\left[{\\frac{n_{1}}{T}},\\cdot\\cdot\\cdot,{\\frac{n_{K}}{T}}\\right]={\\frac{1}{T}}\\cdot\\pmb{n}}\\end{array}$ , and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}\\ell\\left(\\frac{1}{T}\\sum_{t=1}^{T}y_{t},y_{t}\\right)=\\displaystyle\\sum_{i=1}^{K}n_{i}\\ell\\left(\\frac{1}{T}\\cdot n,e_{i}\\right)}\\\\ {\\displaystyle=\\sum_{i=1}^{K}n_{i}\\left(\\ell\\left(\\frac{1}{T}\\cdot n\\right)+\\left\\langle g_{\\frac{1}{T}\\cdot n},e_{i}-\\frac{1}{T}\\cdot n\\right\\rangle\\right),}\\\\ {\\displaystyle=T\\ell\\left(\\frac{1}{T}\\cdot n\\right)=-\\frac{1}{2}\\sum_{i=1}^{K}\\left|n_{i}-\\frac{T}{K}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thus, the term in (5) equals $\\begin{array}{r}{\\frac{1}{2}\\cdot\\mathbb{E}_{n_{1},\\dots,n_{K}}\\left[\\sum_{i=1}^{K}\\left|n_{i}-\\frac{T}{K}\\right|\\right]}\\end{array}$ where $n_{1},\\dots,n_{K}$ are sampled from a multinomial distribution with event probability equal to $\\textstyle{\\frac{1}{K}}$ . Further, using the linearity of expectations we arrive at ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathsf{R E G}_{\\ell}]\\geq\\frac{1}{2}\\sum_{i=1}^{K}\\mathbb{E}_{n_{i}}\\left\\vert n_{i}-\\frac{T}{K}\\right\\vert=\\frac{K}{2}\\cdot\\mathbb{E}\\left[\\left\\vert\\sum_{t=1}^{T}X_{t}-\\mathbb{E}[X_{t}]\\right\\vert\\right],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $X_{t}$ is a Bernoulli random variable with mean $\\frac{1}{K}$ . Next, we bound $\\mathbb{E}\\left[\\left\\vert\\sum_{t=1}^{T}X_{t}-\\mathbb{E}[X_{t}]\\right\\vert\\right]$ using an anti-concentration bound on Bernoulli random variables. In particular, applying Lemma A.4, we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\sum_{t=1}^{T}X_{t}-\\mathbb{E}[X_{t}]\\right|\\geq a\\right)\\geq2\\exp\\left(-\\frac{9a^{2}K}{T}\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for any $\\boldsymbol{a}\\in\\left[\\sqrt{\\frac{3T}{K}},\\frac{T}{2K}\\right]$ . When $T\\geq12K$ , this interval is non-empty. From the Markov\u2019s inequality (Lemma A.1), we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left|\\sum_{t=1}^{T}X_{t}-\\mathbb{E}[X_{t}]\\right|\\right]\\geq a\\mathbb{P}\\left(\\left|\\sum_{t=1}^{T}X_{t}-\\mathbb{E}[X_{t}]\\right|\\geq a\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for any a > 0. Setting a = 3KT we arrive at $\\begin{array}{r}{\\mathbb{E}\\left[\\left|\\sum_{t=1}^{T}X_{t}-\\mathbb{E}[X_{t}]\\right|\\right]\\,\\geq\\,2\\sqrt{3}\\exp(-27)\\sqrt{\\frac{T}{K}}}\\end{array}$ . Thus, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\mathrm{REG}_{\\ell}]\\geq\\sqrt{3}\\exp(-27)\\sqrt{K T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which completes the proof. ", "page_idx": 14}, {"type": "text", "text": "Remark C.1. For $K=2$ , the use of Lemma A.4 can be sidestepped via the use of Khintchine\u2019s inequality. Indeed, in this case we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathrm{REG}_{\\ell}]\\geq\\mathbb{E}_{n}\\left|n-\\frac{T}{2}\\right|=\\frac{1}{2}\\cdot\\mathbb{E}_{\\epsilon_{1},...,\\epsilon_{T}}\\left|\\sum_{t=1}^{T}\\epsilon_{t}\\right|\\geq\\sqrt{\\frac{T}{8}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\epsilon_{1},\\mathbf{\\Omega}...\\,,\\epsilon_{T}$ are i.i.d. Rademacher random variables, and the last inequality follows from Khintchine\u2019s inequality (Lemma A.2). ", "page_idx": 14}, {"type": "text", "text": "D Bounding the (Actual) Multiclass U-Calibration Error ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we bound the U-calibration error $\\mathsf{U C a l}_{\\mathcal{L}^{\\prime}}$ for subclasses ${\\mathcal{L}}^{\\prime}$ of $\\mathcal{L}$ with a finite covering number. We begin with deriving a high probability bound on the regret of Algorithm 1. ", "page_idx": 14}, {"type": "text", "text": "Lemma D.1. Fix some $\\ell\\in{\\mathcal{L}}$ . Then, for any $\\delta\\in(0,1)$ , the regret of Algorithm $^{l}$ satisfies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{REG}_{\\ell}\\leq4\\sqrt{K T}+\\sqrt{2T\\log\\left(\\frac{1}{\\delta}\\right)},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with probability at least $1-\\delta$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Define the random variable $X_{t}\\,:=\\,\\ell(p_{t},y_{t})$ , thus $X_{t}\\,\\in\\,[-1,1]$ . Since the adversary is oblivious and $m_{t,1},\\ldots,m_{t,K}$ are sampled every round independently, $X_{1},\\ldots,X_{T}$ are independent. Applying Hoeffdings inequality (Lemma A.3), for any $\\epsilon>0$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sum_{t=1}^{T}\\ell(\\pmb{p}_{t},\\pmb{y}_{t})-\\mathbb{E}\\left[\\ell(\\pmb{p}_{t},\\pmb{y}_{t})\\right]\\ge\\epsilon\\right)\\le\\exp\\left(-\\frac{\\epsilon^{2}}{2T}\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which implies that $\\begin{array}{r}{\\mathbb{P}\\left(\\mathrm{REG}_{\\ell}-\\mathbb{E}[\\mathrm{REG}_{\\ell}]\\leq\\epsilon\\right)\\geq1-\\exp\\left(-\\frac{\\epsilon^{2}}{2T}\\right)}\\end{array}$ . Let $\\begin{array}{r}{\\delta:=\\exp\\left(-\\frac{\\epsilon^{2}}{2T}\\right)}\\end{array}$ . Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left({\\mathrm{REG}}_{\\ell}-\\mathbb{E}[{\\mathrm{REG}}_{\\ell}]\\leq{\\sqrt{2T\\log\\left({\\frac{1}{\\delta}}\\right)}}\\right)\\geq1-\\delta.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Finally, applying the result of Theorem 1 to bound $\\mathbb{E}[\\mathbf{REG}_{\\ell}]$ completes the proof. ", "page_idx": 14}, {"type": "text", "text": "Using this high probability bound, we first bound $\\mathsf{U C a l}_{\\mathcal{L}^{\\prime}}$ when ${\\mathcal{L}}^{\\prime}$ is a finite subset of $\\mathcal{L}$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma D.2. Fix a subset $\\mathcal{L}^{\\prime}\\subset\\mathcal{L}$ with $|{\\mathcal{L}}^{\\prime}|<\\infty$ . Algorithm $^{\\,l}$ ensures ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathsf{U C a l}_{\\mathcal{L}^{\\prime}}\\leq2+4\\sqrt{K T}+\\sqrt{2T\\log\\left(T\\left|\\mathcal{L}^{\\prime}\\right|\\right)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. For any $\\ell\\in{\\mathcal{L}}$ , let $\\mathscr{E}_{\\ell}$ denote the event that $\\begin{array}{r}{\\mathrm{REG}_{\\ell}\\leq4\\sqrt{K T}\\!+\\!\\sqrt{2T\\log\\left(\\frac{1}{\\delta}\\right)}}\\end{array}$ . From Lemma D.1, $\\mathbb{P}\\left(\\mathcal{E}_{\\ell}\\right)\\geq1-\\delta$ . Let $\\begin{array}{r}{S:=\\operatorname*{sup}_{\\ell\\in\\mathcal{L}^{\\prime}}\\operatorname{REG}_{\\ell}}\\end{array}$ . Thus, the probability that $\\boldsymbol{S}$ is bounded by the same quantity is $\\mathbb{P}\\left(\\cap_{\\ell\\in\\mathcal{L}^{\\prime}}\\mathcal{E}_{\\ell}\\right)$ , which can be bounded as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\cap_{\\ell\\in\\mathcal{L}^{\\prime}}\\mathcal{E}_{\\ell}\\right)=1-\\mathbb{P}\\left(\\cup_{\\ell\\in\\mathcal{L}^{\\prime}}\\mathcal{E}_{\\ell}^{\\prime}\\right)\\geq1-\\sum_{\\ell\\in\\mathcal{L}^{\\prime}}\\mathbb{P}\\left(\\mathcal{E}_{\\ell}^{\\prime}\\right)=\\sum_{\\ell\\in\\mathcal{L}^{\\prime}}\\mathbb{P}\\left(\\mathcal{E}_{\\ell}\\right)+1-|\\mathcal{L}^{\\prime}|\\geq1-|\\mathcal{L}^{\\prime}|\\,\\delta,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the first equality follows from De-Morgan\u2019s law; the first inequality follows from the union bound; the last inequality is because $\\mathbb{P}\\left(\\mathcal{E}_{\\ell}\\right)\\geq\\mathbf{\\bar{1}}-\\delta$ . Setting $\\begin{array}{r}{\\delta=\\frac{1^{\\mathbf{\\lambda}^{\\mathbf{\\lambda}}}}{T|\\mathcal{L}^{\\prime}|}}\\end{array}$ , we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\operatorname*{sup}_{\\ell\\in\\mathcal{L}^{\\prime}}\\mathrm{REG}_{\\ell}\\leq\\underbrace{4\\sqrt{K T}+\\sqrt{2T\\log\\left(T\\left|\\mathcal{L}^{\\prime}\\right|\\right)}}_{=:\\Delta}\\right)\\geq1-\\frac{1}{T}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that, $\\mathbb{E}\\left[S\\right]=\\mathbb{P}(A)\\mathbb{E}\\left[S|A\\right]+\\mathbb{P}(A^{\\prime})\\mathbb{E}\\left[S|A^{\\prime}\\right]$ , where $\\boldsymbol{\\mathcal{A}}$ denotes the event that ${\\mathcal{S}}\\leq{\\Delta}$ . Using the facts $\\mathbb{E}\\left[\\dot{S}|\\dot{A}\\right]\\leq\\dot{\\Delta},\\mathbb{P}(\\dot{A}^{\\prime})\\leq\\frac{1}{T}$ , and $\\mathbb{E}\\left[S|A^{\\prime}\\right]\\leq2T$ since $\\ell\\in[-1,1]$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[S\\right]\\leq2+\\Delta,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which completes the proof. ", "page_idx": 15}, {"type": "text", "text": "Before proceeding further, we first define the notion of cover and covering numbers. ", "page_idx": 15}, {"type": "text", "text": "Definition D.1 (Cover and Covering Number). The $\\epsilon$ -cover of $a$ function class $\\mathcal{F}$ defined over $a$ domain $\\mathcal{X}$ is a function class $\\mathcal{C}_{\\epsilon}$ such that, for any $f\\ \\in\\ {\\mathcal{F}}$ there exists $\\textit{g}\\in\\textit{C}_{\\epsilon}$ such that $\\begin{array}{r}{\\operatorname*{sup}_{\\pmb{x}\\in\\mathcal{X}}|f(\\pmb{x})-g(\\pmb{x})|~~\\leq~~\\epsilon.}\\end{array}$ . The covering number $M(\\mathcal{F},\\epsilon;\\|.\\|_{\\infty})$ is then defined as $M(\\mathcal{F},\\epsilon;\\left\\|.\\right\\|_{\\infty}):=\\operatorname*{min}\\left\\{\\left|\\mathcal{C}_{\\epsilon}\\right|;\\mathcal{C}_{\\epsilon}\\right.$ is an \u03f5-cover of ${\\mathcal F}\\}$ , i.e., the size of the minimal cover. ", "page_idx": 15}, {"type": "text", "text": "The $\\|.\\|_{\\infty}$ in the notation $M(\\mathcal{F},\\epsilon;\\|.\\|_{\\infty})$ is used to represent the fact that the \u201cdistance\u201d between two functions $f,g$ is measured with respect to the $\\|.\\|_{\\infty}^{\\bar{\\mathbf{\\alpha}}}$ norm, i.e., $\\operatorname*{sup}_{\\pmb{x}\\in\\mathcal{X}}|f(\\pmb{x})-g(\\pmb{x})|$ . Such a definition can be generalized to more general distance metrics/pseudo-metrics, but is not required for our purposes. Note that the cover $\\mathcal{C}_{\\epsilon}$ in Definiton D.1 is not necessarily a subset of $\\mathcal{F}$ . We refer to Wainwright (2019) for an exhaustive treatment of cover and covering numbers of different classes $\\mathcal{F}$ \u2019s. ", "page_idx": 15}, {"type": "text", "text": "Let $\\mathcal{C}_{\\epsilon}$ be a minimal $\\epsilon_{}$ -cover of $\\mathcal{F}$ . For each $g\\in{\\mathcal{C}}_{\\epsilon}$ , let $S_{g,\\epsilon}$ be the collection of functions $f\\in\\mathcal F$ such that $g$ is a representative of $f$ , i.e., ", "page_idx": 15}, {"type": "equation", "text": "$$\nS_{g,\\epsilon}:=\\left\\{f\\in\\mathcal{F}\\mid\\operatorname*{sup}_{{\\pmb x}\\in\\mathcal{X}}|f(\\pmb{x})-g(\\pmb{x})|\\leq\\epsilon\\right\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Clearly, $\\cup_{g\\in{\\mathcal{C}}_{\\epsilon}}S_{g,\\epsilon}={\\mathcal{F}}$ . For each $S_{g,\\epsilon}$ , fix a $f_{l e a d}\\in S_{g,\\epsilon}$ (chosen arbitrarily) as a \u201cleader\u201d. Let $\\mathcal{L}_{l e a d,\\mathcal{F}}$ denote the collection of these leaders. It is clear that $|\\mathcal{L}_{l e a d,\\mathcal{F}}|\\leq|\\mathcal{C}_{\\epsilon}|=M(\\mathcal{F},\\epsilon;\\|.\\|_{\\infty})$ . ", "page_idx": 15}, {"type": "text", "text": "In the following lemma, we generalize the result of Lemma D.2 to the case when ${\\mathcal{L}}^{\\prime}$ is a possibly infinite subset of $\\mathcal{L}$ . Our proof is based on applying the result of Lemma D.2 to the leader set of ${\\mathcal{L}}^{\\prime}$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma D.3. Fix a subset $\\mathcal{L}^{\\prime}\\subseteq\\mathcal{L}$ with a covering number $M(\\mathcal{L}^{\\prime},\\epsilon;\\|.\\|_{\\infty})$ . Then, the sequence of forecasts made by Algorithm $^{\\,l}$ satisfies for any $\\epsilon>0$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{U C a l}_{\\mathcal{L}^{\\prime}}\\leq2+4\\epsilon T+4\\sqrt{K T}+\\sqrt{2T\\log\\left(T\\cdot M(\\mathcal{L}^{\\prime},\\epsilon;\\left\\|\\cdot\\right\\|_{\\infty})\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Let $\\mathcal{C}_{\\epsilon}$ be an $\\epsilon$ -cover of ${\\mathcal{L}}^{\\prime}$ of size $M(\\mathcal{L}^{\\prime},\\epsilon;\\|.\\|_{\\infty})$ and $\\mathcal{L}_{l e a d}\\subset\\mathcal{L}^{\\prime}$ be the corresponding leader set. Applying Lemma D.2, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\natural\\left[\\operatorname*{sup}_{\\ell\\in{\\cal C}_{t e a d}}{\\mathrm{REG}}_{\\ell}\\right]\\leq2+4{\\sqrt{K T}}+{\\sqrt{2T\\log(T|{\\mathcal{L}}_{l e a d}|)}}\\leq2+4{\\sqrt{K T}}+{\\sqrt{2T\\log(T\\cdot M({\\mathcal{L}}^{\\prime},\\epsilon;\\|.\\|_{\\infty}))}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now, fix any $\\ell\\in{\\mathcal{L}}^{\\prime}$ and let $g\\in{\\mathcal{C}}_{\\epsilon}$ be the representative of $\\ell$ , and $\\ell^{\\prime}$ correspond to the leader of the partition $\\boldsymbol{S_{g,\\epsilon}}$ that $\\ell$ belongs to. By definition we have the following: ", "page_idx": 16}, {"type": "equation", "text": "$$\n|g(\\pmb{p},\\pmb{y})-\\ell(\\pmb{p},\\pmb{y})|\\leq\\epsilon,\\quad|g(\\pmb{p},\\pmb{y})-\\ell^{\\prime}(\\pmb{p},\\pmb{y})|\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for all $\\pmb{p}\\in\\Delta_{K},\\pmb{y}\\in\\mathcal{E}$ . Therefore, it follows from the triangle inequality that $|\\ell(p,y)-\\ell^{\\prime}(p,y)|\\leq$ $2\\epsilon$ . As usual, let $\\begin{array}{r}{\\beta=\\frac{1}{T}\\sum_{t=1}^{T}y_{t}}\\end{array}$ denote the empirical average of the outcomes. Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\ell(p_{t},y_{t})-\\ell(\\beta,y_{t})\\le\\ell^{\\prime}(p_{t},y_{t})-\\ell^{\\prime}(\\beta,y_{t})+4\\epsilon\\implies{\\mathrm{REG}}_{\\ell}\\le{\\mathrm{REG}}_{\\ell^{\\prime}}+4\\epsilon T.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Taking supremum with respect to $\\ell\\in{\\mathcal{L}}$ on both sides, followed by expectation, we obtain ", "page_idx": 16}, {"type": "text", "text": "$\\begin{array}{r}{\\mathbb{E}\\left[\\underset{\\ell\\in\\mathcal{L}}{\\operatorname*{sup}}\\,\\mathbb{R}\\mathrm{EG}_{\\ell}\\right]\\leq\\mathbb{E}\\left[\\underset{\\ell\\in\\mathcal{L}_{t\\mathrm{ead}}}{\\operatorname*{sup}}\\,\\mathbb{R}\\mathrm{EG}_{\\ell}\\right]+4\\epsilon T\\leq2+4\\epsilon T+4\\sqrt{K T}+\\sqrt{2T\\log\\left(T\\cdot M(\\mathcal{L}^{\\prime},\\epsilon;\\|.\\|_{\\infty})\\right)},}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "which finishes the proof. ", "page_idx": 16}, {"type": "text", "text": "E Regret of FTL for Locally Lipschitz Functions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma E.1. Suppose that for a loss function $\\ell$ , there exists a constant $G_{[\\frac{1}{T},1]}$ such that for each $i\\in[K],\\,\\ell(\\pmb{p},\\pmb{e}_{i})$ is locally Lipschitz in the sense that $|\\ell(\\pmb{p},\\pmb{e}_{i})-\\ell(\\pmb{p}^{\\prime},\\pmb{e}_{i})|\\overset{\\cdot}{\\leq}G_{[\\frac{1}{T},1]}\\,\\|\\pmb{p}-\\pmb{p}^{\\prime}\\|$ for all $p,p^{\\prime}\\in\\Delta_{K}$ such that $p_{i},p_{i}^{\\prime}\\in[\\frac{1}{T},1]$ . Then, the regret of FTL with respect to this loss is at most $2K+G_{[\\frac{1}{T},1]}(1+\\log T)$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Using the Be-the-Leader lemma, we know that the regret of FTL can be bounded as $\\mathrm{REG}\\leq$ $\\begin{array}{r}{2+\\sum_{i=1}^{K}\\sum_{t\\ge2;y_{t}=e_{i}}\\ell(p_{t},e_{i})-\\ell(p_{t+1},e_{i})}\\end{array}$ . Assume that $\\mathcal{E}_{m}\\subseteq\\mathcal{E}$ of size $m\\le K$ contains all the outcomes chosen by the adversary over $T$ rounds, i.e., $\\pmb{y}_{t}\\in\\mathcal{E}_{m}$ for all $t\\in[T]$ . For each $e_{i}\\in\\mathcal{E}_{m}$ , let $k_{i}$ denote the total number of time instants $t$ such that $\\pmb{y}_{t}=\\pmb{e}_{i}$ and let $\\mathcal{T}_{i}:=\\overline{{\\{t_{i,1},\\ldots,t_{i,k_{i}}\\}}}$ denote those time instants. Then, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{R E G}_{\\ell}\\leq2+\\displaystyle\\sum_{e_{i}\\in\\mathcal{E}_{m}}\\sum_{t\\geq2;y_{t}=e_{i}}\\ell(\\pmb{p}_{t},e_{i})-\\ell(\\pmb{p}_{t+1},e_{i})}\\\\ &{\\qquad\\leq2m+\\displaystyle\\sum_{e_{i}\\in\\mathcal{E}_{m}}\\sum_{t\\in\\mathcal{T}_{i}\\backslash\\{t_{i,1}\\}}\\ell(\\pmb{p}_{t},e_{i})-\\ell(\\pmb{p}_{t+1},e_{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality follows by bounding $\\ell(\\pmb{p}_{t},\\pmb{e}_{i})-\\ell(\\pmb{p}_{t+1},\\pmb{e}_{i})$ with 2 for all the $t$ \u2019s where an outcome appears for the first time. For each $e_{i}\\in\\mathcal{E}_{m}$ and for all $t>t_{i,1}$ , we have $\\begin{array}{r}{p_{t,i}=\\frac{n_{t-1,i}}{t-1}\\geq\\frac{1}{T}}\\end{array}$ Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{R E G}_{\\ell}\\leq2m+G_{[\\frac{1}{T},1]}\\displaystyle\\sum_{e_{i}\\in\\mathcal{E}_{m}}\\sum_{t\\in\\mathcal{T}_{i}\\setminus\\{t_{i,1}\\}}\\left\\|p_{t}-p_{t+1}\\right\\|}\\\\ &{\\qquad=2m+G_{[\\frac{1}{T},1]}\\displaystyle\\sum_{e_{i}\\in\\mathcal{E}_{m}}\\sum_{t\\in\\mathcal{T}_{i}\\setminus\\{t_{i,1}\\}}\\left\\|\\frac{n_{t-1}}{t-1}-\\frac{n_{t}}{t}\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proceeding similar to the proof of Theorem 3, we can show that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{R E G}_{\\ell}\\leq2m+G_{[\\frac{1}{T},1]}\\displaystyle\\sum_{e_{i}\\in\\mathcal E_{m}}\\sum_{t\\in\\mathcal T_{i}\\setminus\\{t_{i,1}\\}}\\frac{1}{t}=2m+G_{[\\frac{1}{T},1]}\\left(\\displaystyle\\sum_{t=1}^{T}\\frac{1}{t}-\\sum_{e_{i}\\in\\mathcal E_{m}}\\frac{1}{t_{i,1}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq2K+G_{[\\frac{1}{T},1]}(1+\\log T),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality follows by dropping the negative term, $m\\le K$ , and $\\begin{array}{r}{\\sum_{j=2}^{T}\\frac{1}{j}\\le\\int_{1}^{T}\\frac{1}{z}d z=}\\end{array}$ $\\log T$ . This completes the proof. ", "page_idx": 16}, {"type": "text", "text": "Example E.1. Consider for instance the loss whose univariate form is $\\begin{array}{r}{\\ell(\\pmb{p})=-\\tilde{c}_{K}\\sum_{i=1}^{K}p_{i}^{\\alpha},}\\end{array}$ i=1 pi\u03b1 , where $\\alpha\\in(1,2)$ , and $\\tilde{c}_{K}>0$ is a normalizing constant (which only depends on $K$ ) to ensure that $\\ell({\\pmb p},{\\pmb y})$ ", "page_idx": 16}, {"type": "text", "text": "is bounded in $[-1,1]$ . Clearly, $\\ell(p)$ is concave and thus the induced loss $\\ell({\\pmb p},{\\pmb y})$ is proper as per Lemma $^{l}$ . For the chosen $\\ell(p)$ , $\\ell(p,e_{1})$ is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\ell(p,e_{1})=\\ell(p)+(1-p_{1})\\nabla_{1}\\ell(p)-\\sum_{i=2}^{K}p_{i}\\nabla_{i}\\ell(p)=-\\tilde{c}_{K}\\left((1-\\alpha)\\sum_{i=1}^{K}p_{i}^{\\alpha}+\\alpha p_{1}^{\\alpha-1}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It is easy to verify that $\\nabla_{1}\\ell(p,e_{1})=-\\widetilde{c}_{K}\\cdot\\alpha(\\alpha-1)p_{i}^{\\alpha-2}(1-p_{1})$ and $\\nabla_{i}\\ell(\\pmb{p},e_{1})=-\\tilde{c}_{K}\\cdot\\alpha(1-$ $\\alpha)p_{i}^{\\alpha-1}$ for all $i>1$ . Thus, for a large $T$ , $G_{[\\frac{1}{T},1]}$ is of order $\\mathcal{O}(\\alpha(\\alpha-1)\\tilde{c}_{K}T^{2-\\alpha})$ . This yields $a$ regret bound $\\mathcal{O}(K+\\alpha(\\alpha-1)\\tilde{c}_{K}T^{2-\\alpha}\\log T)$ , which for $\\alpha\\in\\textstyle\\left({\\frac{3}{2}},2\\right)$ is better (with respect to $T$ ) than the $\\mathcal{O}(\\sqrt{K T})$ bound obtained in Theorem $^{l}$ . ", "page_idx": 17}, {"type": "text", "text": "F Proof of Theorem 4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Theorem 4. There exists a proper Lipschitz loss $\\ell$ such that: for any algorithm ALG, there exists a choice of $y_{1},\\dots,y_{T}$ by an oblivious adversary such that the expected regret of ALG is $\\Omega(\\log T)$ . ", "page_idx": 17}, {"type": "text", "text": "As mentioned, the loss we use in this lower bound construction is the squared loss $\\ell(p,y)\\,:=$ $\\left\\|p-y\\right\\|^{2}$ (we ignore the constant $1/2$ here for simplicity, which clearly does not affect the proof). It is clearly convex in $\\pmb{p}$ for any $\\textit{\\textbf{y}}$ . The univariate form of the loss is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\ell(p)=\\mathbb{E}_{y\\sim p}[\\ell(p,y)]=\\sum_{i=1}^{K}p_{i}\\left\\|p-e_{i}\\right\\|^{2}=\\left\\|p\\right\\|^{2}+1-2\\sum_{i=1}^{K}p_{i}\\left\\langle p,e_{i}\\right\\rangle=1-\\left\\|p\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We now follow the three steps outlined in Section 4.1. ", "page_idx": 17}, {"type": "text", "text": "Step 1: First, it is well-known that for convex losses, deterministic algorithms are as powerful as randomized algorithms. Formally, let $\\mathcal{A}_{r a n d}$ and $A_{d e t}$ be the class of randomized algorithms and deterministic algorithms respectively for the forecasting problem. Then the following holds: ", "page_idx": 17}, {"type": "text", "text": "Lemma F.1. For any loss $\\ell(p,y)$ that is convex in $p\\in\\Delta_{K}$ for any $\\pmb{y}\\in\\mathcal{E}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\mathsf{A L G}\\in\\mathcal{A}_{r a n d}}\\operatorname*{sup}_{y_{1},\\ldots,y_{T}\\in\\mathcal{E}}\\mathbb{E}\\left[\\mathrm{REG}_{\\ell}\\right]=\\operatorname*{inf}_{\\mathsf{A L G}\\in\\mathcal{A}_{d e t}}\\operatorname*{sup}_{y_{1},\\ldots,y_{T}\\in\\mathcal{E}}\\mathsf{R E G}_{\\ell}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. The direction $\"{\\le}\"$ is trivial since $\\mathcal{A}_{d e t}\\subseteq\\mathcal{A}_{r a n d}$ . For the other direction, it suffices to show that for any randomized algorithm $\\mathsf{A L G}\\in\\mathcal{A}_{r a n d}$ , one can construct a deterministic algorithm $\\mathsf{A L G}^{\\prime}\\in\\mathcal{A}_{d e t}$ such that, for any fixed sequence $y_{1},\\ldots,y_{T}\\in\\mathcal{E}$ , the expected regret of ALG is lower bounded by the regret of $\\mathsf{A L G^{\\prime}}$ . To do so, it suffices to let $\\mathsf{A L G^{\\prime}}$ output the expectation of the randomized output of ALG at each time $t$ . Since the loss if convex, by Jensen\u2019s inequality, the loss of $\\mathsf{A L G^{\\prime}}$ is at most the expect loss of ALG. This finishes the proof. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Since there is no difference between an oblivious adversary and an adaptive adversary for deterministic algorithms, $\\operatorname*{inf}_{\\mathsf{A L G}\\in A_{d e t}}\\operatorname*{sup}_{\\pmb{y}_{1},\\dots,\\pmb{y}_{T}\\in\\mathcal{E}}\\mathrm{REG}_{\\ell}$ can be written as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{VAL}=\\left\\langle\\left\\langle\\operatorname*{inf}_{p_{t}\\in\\Delta_{K}}\\operatorname*{sup}_{y_{t}\\in\\mathcal{E}}\\right\\rangle\\right\\rangle_{t=1}^{T}\\left[\\sum_{t=1}^{T}\\ell(p_{t},y_{t})-\\operatorname*{inf}_{p\\in\\Delta_{K}}\\sum_{t=1}^{T}\\ell(p,y_{t})\\right],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\left\\langle\\left\\langle\\operatorname*{inf}_{\\pmb{p}_{t}\\in\\Delta_{K}}\\operatorname*{sup}_{\\pmb{y}_{t}\\in\\mathcal{E}}\\right\\rangle\\right\\rangle_{t=1}^{T}$ is a shorthand for the iterated expression ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{p_{1}\\in\\Delta_{K}}\\operatorname*{sup}_{y_{1}\\in\\mathcal{E}}\\operatorname*{inf}_{p_{2}\\in\\Delta_{K}}\\operatorname*{sup}_{y_{2}\\in\\mathcal{E}}\\,\\cdot\\,\\cdot\\,\\cdot\\cdot\\cdot\\operatorname*{inf}_{p_{T-1}\\in\\Delta_{K}}\\operatorname*{sup}_{y_{T-1}\\in\\mathcal{E}}\\operatorname*{inf}_{p_{T}\\in\\Delta_{K}}\\operatorname*{sup}_{y_{T}\\in\\mathcal{E}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\pmb{n}\\in\\mathbb{N}_{>}^{K}$ be a vector such that $n_{i}$ represents the cumulative number of the outcome $i$ , and $r\\in\\{0,\\ldots,\\bar{T}\\}$ represent the number of remaining rounds. For any $\\mathbfit{\\Delta}$ and $r$ such that $\\left\\|n\\right\\|_{1}+r=T$ , define $\\psi_{n,r}$ recursively as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nu_{n,r}=\\operatorname*{inf}_{p\\in\\Delta_{K}}\\operatorname*{sup}_{y\\in\\mathcal{E}}\\nu_{n+y,r-1}+\\ell(p,y)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with $\\begin{array}{r}{\\gamma_{n,0}=-\\operatorname*{inf}_{\\pmb{p}\\in\\Delta_{K}}\\sum_{i=1}^{K}n_{i}\\ell(\\pmb{p},\\pmb{e}_{i})}\\end{array}$ . It is then clear that VAL is simply $\\nu_{0,T}$ . ", "page_idx": 17}, {"type": "text", "text": "Step 2: We proceed to rewrite and simplify $\\psi_{n,r}$ as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\dot{V}_{n}}&{=\\frac{\\mathrm{ind}}{n\\hbar\\omega}\\sin\\frac{\\mathrm{in}}{S}h_{n}+y_{-1}-i+\\ell(p,y)}\\\\ &{=\\frac{\\mathrm{in}}{\\mathrm{prad}\\;\\hbar\\omega}\\operatorname*{in}_{\\mathbf{g}\\sim\\pi}\\frac{\\mathbf{g}}{\\mathbf{g}_{\\ell}(\\mathbf{k}_{\\ell}\\times\\mathbf{r}_{n}-1)}+\\ell(p,y)}\\\\ &{=\\frac{\\mathrm{in}}{\\mathrm{exp}}\\frac{\\mathrm{in}}{S}h_{n}^{\\mathrm{tl}}\\mathbf{g}_{\\ell}}\\\\ &{=\\frac{\\mathrm{oup}}{\\mathrm{exp}}\\frac{\\mathrm{in}}{S}h_{n}^{\\mathrm{tl}}\\mathbf{g}_{\\ell}-\\left[\\dot{\\nabla}_{n}\\mathbf{g}_{\\ell},\\mathbf{r}_{n}-1+\\ell(p,y)\\right]}\\\\ &{=\\underbrace{\\mathrm{sup}}_{\\mathrm{~tot}}\\;\\frac{\\mathrm{in}}{S}\\bigg[\\;\\bigg\\langle\\frac{\\mathrm{in}}{S}h_{n}+\\ell_{n},\\mathbf{r}_{-1}+\\ell_{1}\\ell(p,\\mathbf{e}_{i})}\\\\ &{=\\underbrace{\\mathrm{sup}}_{\\mathrm{~tot}}\\;\\frac{\\mathrm{in}}{S}\\bigg[\\;\\bigg\\langle\\frac{\\mathrm{in}}{S}h_{n}+\\ell_{n},\\mathbf{r}_{-1}+\\ell_{i}\\ell(p)+\\langle\\nabla(\\ell(p),\\mathbf{e}_{i}-p)\\rangle}\\\\ &{=\\underbrace{\\mathrm{sup}}_{\\mathrm{~tot}}\\;\\frac{\\mathrm{in}}{S}\\bigg[\\;\\bigg\\langle\\frac{\\mathrm{in}}{S}h_{n}^{\\mathrm{tl}}\\bigg\\rangle\\bigg\\langle\\frac{\\mathrm{in}}{S}h_{n}+\\ell_{1}\\ell(p)+\\langle\\nabla(\\ell(p),\\mathbf{e}_{i}-p)\\rangle}\\\\ &{=\\underbrace{\\mathrm{sup}}_{\\mathrm{~tot}}\\;\\frac{\\mathrm{in}}{S}\\bigg[\\;\\bigg\\langle\\frac{\\mathrm{in}}{S}h_{n}^{\\mathrm{tl}}\\bigg\\rangle\\bigg\\langle\\frac{\\mathrm{in}}{S}h_{n}^{\\mathrm{tl}}\\bigg\\rangle+\\ell(p)+\\langle\\nabla(\\ell(p),\\mathbf{q}-p)}\\\\ &{=\\underbrace{\\mathrm{sup}}_{\\mathrm{~tot}}\\;\\bigg\\langle\\frac{\\mathrm{in}}{S}h_{n}^{\\mathrm{tl}}\\bigg\\\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the second equality follows since $\\mathbb{E}_{y\\sim q}\\left[\\mathcal{V}_{n+y,r-1}+\\ell(\\pmb{p},\\pmb{y})\\right]$ is a linear function in $\\pmb q$ , and the infimum/supremum of a linear function is attained at the boundary; the third equality follows from the minimax theorem as $\\mathbb{E}_{y\\sim q}\\left[\\mathcal{V}_{n+y,r-1}+\\ell(\\pmb{p},\\pmb{y})\\right]$ is convex in $\\pmb{p}$ and concave in $\\pmb q$ ; the final equality is because $\\ell({\\pmb p})+\\langle\\nabla\\ell(\\bar{\\pmb p}),\\bar{\\pmb q}-{\\pmb p}\\rangle\\^{\\prime}\\geq\\ell({\\pmb q})$ which follows from the concavity of the univariate form of $\\ell$ , and equality is attained at $\\pmb{p}=\\pmb{q}$ . ", "page_idx": 18}, {"type": "text", "text": "Throughout the subsequent discussion, we consider $K=2$ and use the concrete form of the squared loss. Writing $\\mathcal{V}_{(n_{1},n_{2}),r}$ as $\\nu_{n_{1},n_{2},r}$ for simplicity, we simplify the recurrence to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{V}_{n_{1},n_{2},r}=\\displaystyle\\operatorname*{sup}_{q\\in\\Delta_{2}}q_{1}\\mathcal{V}_{n_{1}+1,n_{2},r-1}+q_{2}\\mathcal{V}_{n_{1},n_{2}+1,r-1}+1-q_{1}^{2}-q_{2}^{2}}\\\\ &{\\qquad\\quad=\\displaystyle\\operatorname*{sup}_{q\\in[0,1]}\\mathcal{V}_{2}+(\\mathcal{V}_{1}-\\mathcal{V}_{2})q-2(q^{2}-q),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\rightarrow\\!\\!\\nu_{1},\\nu_{2}$ is a shorthand for $\\mathcal{V}_{n_{1}+1,n_{2},r-1},\\mathcal{V}_{n_{1},n_{2}+1,r-1}$ respectively. It is straightforward to show via the KKT conditions that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{q\\in[0,1]}\\mathcal{V}_{2}+(\\mathcal{V}_{1}-\\mathcal{V}_{2})q-2(q^{2}-q)=\\left\\{\\frac{\\mathcal{V}_{2}}{\\mathcal{V}_{1}}^{2}+\\frac{\\mathcal{V}_{1}+\\mathcal{V}_{2}}{2}+\\frac{1}{2}\\right.\\ \\stackrel{\\mathrm{if}\\ \\mathcal{V}_{1}-\\mathcal{V}_{2}<-2,}{\\mathrm{~if}\\ \\mathcal{V}_{2}-2\\leq\\mathcal{V}_{1}-\\mathcal{V}_{2}\\leq2,}\\ \\left.(\\mathcal{R})\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, $({\\mathcal{R}})$ (with $\\nu_{1},\\nu_{2}$ replaced by $\\mathcal{V}_{n_{1}+1,n_{2},r-1},\\mathcal{V}_{n_{1},n_{2}+1,r-1})$ represents the recurrence we wish to solve for to obtain $\\mathcal{V}_{0,0,T}$ . The base case of $({\\mathcal{R}})$ is the following: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{V}_{n,T-n,0}=-T\\ell\\left(\\left[\\frac{n}{T},1-\\frac{n}{T}\\right]\\right)=2T\\cdot\\frac{n}{T}\\cdot\\left(\\frac{n}{T}-1\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which holds for all $n$ such that $0\\leq n\\leq T$ . In the next lemma we show that $-2\\le\\mathcal{V}_{1}-\\mathcal{V}_{2}\\le2$ , therefore, it is sufficient to solve the recursion $({\\mathcal{R}})$ corresponding to this case only. ", "page_idx": 18}, {"type": "text", "text": "Lemma F.2. The recurrence $({\\mathcal{R}})$ is also equal to the following: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{V}_{n_{1},n_{2},r}=\\frac{(\\mathcal{V}_{n_{1}+1,n_{2},r-1}-\\mathcal{V}_{n_{1},n_{2}+1,r-1})^{2}}{8}+\\frac{\\mathcal{V}_{n_{1}+1,n_{2},r-1}+\\mathcal{V}_{n_{1},n_{2}+1,r-1}}{2}+\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. It suffices to show that the condition $|\\mathcal{V}_{n_{1}+1,n_{2},r-1}-\\mathcal{V}_{n_{1},n_{2}+1,r-1}|\\ \\leq\\ 2$ always holds. Rewriting $n_{2}+1$ as $n_{2}$ and $r-1$ as $r$ , this is equivalent to showing ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\mathcal{V}_{n_{1},n_{2},r}-\\mathcal{V}_{n_{1}+1,n_{2}-1,r}|\\leq2\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for all $n_{1},n_{2},r$ such that $n_{1}\\,+\\,n_{2}\\,+\\,r\\,=\\,T$ , and the arguments in (10) are well defined, i.e., $0\\leq n_{1}\\leq T-r-1$ , and $1\\leq n_{2}\\leq T-r$ . We prove this by an induction on $r$ . ", "page_idx": 18}, {"type": "text", "text": "Base Case: This corresponds to $r=0$ . For $0\\leq n\\leq T-1,|\\mathcal{V}_{n,T-n,0}-\\mathcal{V}_{n+1,T-n-1,0}|$ is equal to ", "page_idx": 19}, {"type": "equation", "text": "$$\n2T\\cdot\\left|{\\frac{n}{T}}\\cdot\\left({\\frac{n}{T}}-1\\right)-{\\frac{n+1}{T}}\\cdot\\left({\\frac{n+1}{T}}-1\\right)\\right|=2\\cdot\\left|1-{\\frac{2n+1}{T}}\\right|\\leq2\\cdot\\left(1-{\\frac{1}{T}}\\right)\\leq2,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which verifies the base case. ", "page_idx": 19}, {"type": "text", "text": "Induction Hypothesis: Fix a $k\\in\\{1,\\ldots,T\\}$ ; assume that (10) holds for $r=k-1$ . ", "page_idx": 19}, {"type": "text", "text": "Induction Step: We show that (10) holds for $r=k$ . It follows from $({\\mathcal{R}})$ and the induction hypothesis that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{V}_{n_{1},n_{2},k}=\\frac{1}{8}\\left(\\mathcal{V}_{n_{1}+1,n_{2},k-1}-\\mathcal{V}_{n_{1},n_{2}+1,k-1}\\right)^{2}+\\frac{\\mathcal{V}_{n_{1}+1,n_{2},k-1}+\\mathcal{V}_{n_{1},n_{2}+1,k-1}}{2}+\\frac{1}{2},}\\\\ {\\mathcal{V}_{n_{1}+1,n_{2}-1,k}=\\frac{1}{8}\\left(\\mathcal{V}_{n_{1}+2,n_{2}-1,k-1}-\\mathcal{V}_{n_{1}+1,n_{2},k-1}\\right)^{2}+\\frac{\\mathcal{V}_{n_{1}+2,n_{2}-1,k-1}+\\mathcal{V}_{n_{1}+1,n_{2},k-1}}{2}+\\frac{1}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $\\begin{array}{r l}{\\d_{\\lambda1}\\;:=\\;\\mathcal{V}_{n_{1}+2,n_{2}-1,k-1},\\d_{\\alpha2}\\;:=\\;\\mathcal{V}_{n_{1}+1,n_{2},k-1},\\d_{\\alpha3}}&{}\\end{array}$ := Vn1,n2+1,k\u22121, \u2206 := Vn1+1,n2\u22121,k \u2212 $\\nu_{n_{1},n_{2},k}$ . Subtracting the equations above and expressing in terms of the defined quantities, we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta=\\cfrac{\\alpha_{1}+\\alpha_{2}}{2}+\\cfrac{(\\alpha_{1}-\\alpha_{2})^{2}}{8}-\\cfrac{\\alpha_{2}+\\alpha_{3}}{2}-\\cfrac{(\\alpha_{2}-\\alpha_{3})^{2}}{8}}\\\\ &{\\quad=\\cfrac{\\alpha_{1}-\\alpha_{3}}{2}+\\cfrac{(\\alpha_{1}-\\alpha_{3})\\cdot\\big(\\alpha_{1}+\\alpha_{3}-2\\alpha_{2}\\big)}{8}}\\\\ &{\\quad=\\cfrac{x+y}{2}+\\cfrac{(x+y)\\cdot(x-y)}{8}}\\\\ &{\\quad=\\cfrac{(x+2)^{2}-(y-2)^{2}}{8},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we have defined $x:=\\alpha_{1}-\\alpha_{2},y:=\\alpha_{2}-\\alpha_{3}$ . It follows from the induction hypothesis that $|x|\\leq2,|y|\\leq2$ , therefore $|\\Delta|\\leq2$ . Summarizing, we have shown that $\\left|\\right\\rangle\\!_{n_{1}+1,n_{2}-1,k}-\\mathcal{V}_{n_{1},n_{2},k}\\right|\\leq$ 2 which completes the proof via induction. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Step 3: Since $n_{1}+n_{2}+r=T.$ , we may express $\\nu_{n_{1},n_{2},r}$ in terms of $n_{1},n_{2}$ , and $T$ . In the next lemma, we show that $\\nu_{n_{1},n_{2},r}$ exhibits a very special structure when expressed in this manner; this allows us to reduce $\\mathcal{V}_{0,0,T}$ to solving a one dimensional recurrence. ", "page_idx": 19}, {"type": "text", "text": "Lemma F.3. For all $n_{1},n_{2}$ such that $n_{1},n_{2}\\ge0,$ , and $n_{1}+n_{2}=T-r,$ , it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{V}_{n_{1},n_{2},r}=\\frac{(n_{1}-n_{2})^{2}}{2}\\cdot u_{r}-\\frac{2n_{1}n_{2}}{T}+v_{r},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\{u_{r}\\}_{r=0}^{T},\\{v_{r}\\}_{r=0}^{T}$ are sequences that depend only on $T$ , and are defined by the following recurrences: ", "page_idx": 19}, {"type": "equation", "text": "$$\nu_{r+1}=u_{r}+\\left(u_{r}+\\frac{1}{T}\\right)^{2},\\quad v_{r+1}=\\frac{u_{r}}{2}+v_{r}+\\frac{r+1}{T}-\\frac{1}{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for all $0\\leq r\\leq T-1$ , with $u_{0}=v_{0}=0$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Similar to Lemma F.2, the proof shall follow by an induction on $r$ . ", "page_idx": 19}, {"type": "text", "text": "Base Case: For $r=0$ , it follows from (9) that $\\begin{array}{r}{\\mathcal{V}_{n_{1},n_{2},0}=-\\frac{2n_{1}n_{2}}{T}}\\end{array}$ 2nT1n2, which is consistent with (11) and $u_{0}=v_{0}=0$ . ", "page_idx": 19}, {"type": "text", "text": "Induction Hypothesis: Fix a $k\\in\\{0,...\\,,T-1\\}$ . Assume that (11) holds for $r=k$ . ", "page_idx": 19}, {"type": "text", "text": "Induction Step: We show that (11) holds for $r=k+1$ . From Lemma F.2, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{V}_{n_{1},n_{2},k+1}=\\frac{(\\mathcal{V}_{n_{1}+1,n_{2},k}-\\mathcal{V}_{n_{1},n_{2}+1,k})^{2}}{8}+\\frac{\\mathcal{V}_{n_{1}+1,n_{2},k}+\\mathcal{V}_{n_{1},n_{2}+1,k}}{2}+\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "It follows from the induction hypothesis that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathcal{V}_{n_{1}+1,n_{2},k}=\\frac{(n_{1}+1-n_{2})^{2}}{2}\\cdot u_{k}-\\frac{2(n_{1}+1)\\cdot n_{2}}{T}+v_{k},}}\\\\ {{\\displaystyle\\mathcal{V}_{n_{1},n_{2}+1,k}=\\frac{(n_{1}-n_{2}-1)^{2}}{2}\\cdot u_{k}-\\frac{2n_{1}\\cdot(n_{2}+1)}{T}+v_{k}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Define $\\delta:=\\mathcal{V}_{n_{1}+1,n_{2},k}-\\mathcal{V}_{n_{1},n_{2}+1,k}$ and $\\sigma:=\\mathcal{V}_{n_{1}+1,n_{2},k}+\\mathcal{V}_{n_{1},n_{2}+1,k}$ . Subtracting the equations above, we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\delta=\\frac{(n_{1}+1-n_{2})^{2}-(n_{1}-n_{2}-1)^{2}}{2}\\cdot u_{k}+\\frac{2n_{1}\\cdot(n_{2}+1)-2(n_{1}+1)\\cdot n_{2}}{T}}}\\\\ {\\displaystyle{\\quad=2(n_{1}-n_{2})\\cdot\\bigg(u_{k}+\\frac{1}{T}\\bigg)\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Adding the equations above, we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\sigma=\\cfrac{(n_{1}+1-n_{2})^{2}+(n_{1}-n_{2}-1)^{2}}{2}\\cdot u_{k}-\\cfrac{2n_{1}\\cdot(n_{2}+1)+2(n_{1}+1)\\cdot n_{2}}{T}+2v_{k}}\\\\ {\\quad=\\left((n_{1}-n_{2})^{2}+1\\right)\\cdot u_{k}-\\cfrac{4n_{1}n_{2}}{T}-\\cfrac{2(n_{1}+n_{2})}{T}+2v_{k}}\\\\ {\\quad=(n_{1}-n_{2})^{2}\\cdot u_{k}-\\cfrac{4n_{1}n_{2}}{T}+u_{k}+2v_{k}+\\cfrac{2(r+1)}{T}-2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last equality follows since $n_{1}+n_{2}=T-k-1$ . Expressing $\\mathcal{V}_{n_{1},n_{2},k+1}$ in terms of $\\delta,\\sigma$ , we have $\\begin{array}{r}{\\mathcal{V}_{n_{1},n_{2},k+1}=\\frac{\\delta^{2}}{8}+\\frac{\\sigma}{2}+\\frac{1}{2}}\\end{array}$ . Substituting $\\delta,\\sigma$ , we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mathcal{V}_{n_{1},n_{2},k+1}=\\displaystyle\\frac{(n_{1}-n_{2})^{2}}{2}\\cdot\\left(u_{k}+\\left(u_{k}+\\frac{1}{T}\\right)^{2}\\right)-\\frac{2n_{1}n_{2}}{T}+\\frac{u_{k}}{2}+v_{k}+\\frac{(r+1)}{T}-\\frac{1}{2}}}\\\\ {{=\\displaystyle\\frac{(n_{1}-n_{2})^{2}}{2}\\cdot u_{k+1}-\\frac{2n_{1}n_{2}}{T}+v_{k+1},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which completes the induction step. The proof is hence complete by induction. ", "page_idx": 20}, {"type": "text", "text": "Since $\\mathrm{VAL}=\\mathcal{V}_{0,0,T}=v_{T}$ , it only remains to bound $v_{T}$ . Note that the recursion describing $v$ is coupled with $u$ . However, since we only want to bound $v_{T}$ , we can sum the recursion describing $v$ to obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{r=0}^{T-1}(v_{r+1}-v_{r})=\\frac{1}{2}\\cdot\\sum_{r=0}^{T-1}u_{r}+\\frac{1}{T}\\cdot\\sum_{r=0}^{T-1}(r+1)-\\frac{T}{2}=\\frac{1}{2}\\cdot\\left(\\sum_{r=0}^{T-1}u_{r}+1\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Moreover, since the summation with respect to $v$ telescopes and $v_{0}=0$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nv_{T}=\\frac{1}{2}\\cdot\\left(\\sum_{r=0}^{T-1}u_{r}+1\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, it remains to bound $\\textstyle\\sum_{r=0}^{T-1}u_{r}$ . Define $\\begin{array}{r}{a_{r}:=\\,u_{r}+\\frac{1}{T}}\\end{array}$ so that $\\begin{array}{r}{v_{T}\\,=\\,\\frac{1}{2}\\sum_{r=0}^{T-1}a_{r}}\\end{array}$ . The recurrence (12) describing $u$ reduces to $a_{r+1}=a_{r}+a_{r}^{2}$ for all $0\\leq r\\leq T-1$ , with $\\begin{array}{r}{a_{0}=\\frac{1}{T}}\\end{array}$ . In the next result, we obtain bounds on $a_{r}$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma F.4. For all $0\\leq r\\leq T-1$ , it holds that $\\begin{array}{r}{a_{r}\\leq\\frac{1}{T-r}}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Proof. As usual, the proof shall follow by an induction on $r$ . Since $\\begin{array}{r}{a_{0}=\\frac{1}{T}}\\end{array}$ , the base case is trivially satisfied. Fix a $k\\in\\{0,\\ldots,T-2\\}$ , and assume that $a_{k}\\leq\\frac{1}{T-k}$ . Since $\\bar{a_{k+1}}=a_{k}+a_{k}^{2}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\na_{k+1}\\leq{\\frac{1}{(T-k)^{2}}}+{\\frac{1}{T-k}}={\\frac{T-k+1}{(T-k)^{2}}}={\\frac{(T-k)^{2}-1}{(T-k)^{2}}}\\cdot{\\frac{1}{T-k-1}}\\leq{\\frac{1}{T-k-1}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This completes the induction step. ", "page_idx": 20}, {"type": "text", "text": "Lemma F.5. For all $0\\leq r\\leq T-1$ , it holds that $\\begin{array}{r}{a_{r}\\geq\\frac{1}{T-r+\\log T}}\\end{array}$ . Furthermore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{r=0}^{T-1}a_{r}\\geq\\log\\left({\\frac{T}{\\log T+1}}+1\\right)=\\Omega(\\log T).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. According to Lemma F.4, we can write ar asT \u2212r1+br for some non-negative sequence $\\{b_{r}\\}$ with $b_{0}=0$ . We next obtain the recurrence describing $\\{b_{r}\\}$ . In particular, since $a_{r+1}=a_{r}(a_{r}+1)$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{T-(r+1)+b_{r+1}}=\\frac{1}{T-r+b_{r}}\\cdot\\left(\\frac{1}{T-r+b_{r}}+1\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which on simplifying (by multiplying both sides with $(T-(r+1)+b_{r+1})(T-r+b_{r}))$ yields ", "page_idx": 21}, {"type": "equation", "text": "$$\nb_{r+1}=b_{r}+{\\frac{1+b_{r}-b_{r+1}}{T-r+b_{r}}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next, we shall show by an induction on $r$ that $\\begin{array}{r}{b_{r}\\leq\\sum_{i=0}^{r-1}\\frac{1}{T-i}}\\end{array}$ for all $0\\leq r\\leq T-1$ . Since $b_{0}=0$ the base case is trivially satisfied. Fix a $k\\in\\{0,...\\,,T-2\\}$ and assume that $\\begin{array}{r}{b_{k}\\leq\\sum_{i=0}^{k-1}\\frac{1}{T-i}}\\end{array}$ . We consider two cases depending on whether or not $b_{k+1}\\geq b_{k}$ . If , the induction step holds trivially. If $b_{k+1}\\geq b_{k}$ , it follows from the recurrence (14) that ", "page_idx": 21}, {"type": "equation", "text": "$$\nb_{k+1}\\leq b_{k}+{\\frac{1}{T-k+b_{k}}}\\leq b_{k}+{\\frac{1}{T-k}}\\leq\\sum_{i=0}^{k}{\\frac{1}{T-i}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which completes the induction step. Therefore, we have established that $\\begin{array}{r}{b_{r}\\leq\\sum_{i=0}^{r-1}\\frac{1}{T-i}}\\end{array}$ for all $0\\leq r\\leq T-1$ . It then follows that ", "page_idx": 21}, {"type": "equation", "text": "$$\nb_{r}\\leq\\sum_{i=0}^{T-2}{\\frac{1}{T-i}}\\leq\\int_{1}^{T}{\\frac{d z}{z}}=\\log T,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for all $0\\leq r\\leq T-1$ , which translates to $\\begin{array}{r}{a_{r}\\geq\\frac{1}{T-r+\\log T}}\\end{array}$ . This completes the proof of the first part of the lemma. With this lower bound on $a_{r}$ , we can lower bound $\\textstyle\\sum_{r=0}^{T-1}a_{r}$ as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{r=0}^{T-1}a_{r}\\geq\\sum_{r=0}^{T-1}{\\frac{1}{T-r+\\log T}}=\\sum_{i=1}^{T}{\\frac{1}{i+\\log T}}\\geq\\int_{1}^{T+1}{\\frac{d z}{z+\\log T}}=\\log\\left({\\frac{T}{\\log T+1}}+1\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which is $\\Omega(\\log T)$ for a large $T$ . This completes the proof. ", "page_idx": 21}, {"type": "text", "text": "To conclude, we have shown ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{VAL}=\\mathcal{V}_{0,0,T}=v_{T}=\\frac{1}{2}\\left(\\sum_{r=0}^{T-1}u_{r}+1\\right)=\\frac{1}{2}\\sum_{r=0}^{T-1}a_{r}=\\Omega(\\log T),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "proving Theorem 4. ", "page_idx": 21}, {"type": "text", "text": "G Proof of Theorem 5 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Theorem 5. The regret of FTL for learning any $\\ell\\in{\\mathcal{L}}_{d e c}$ is at most $2K+(K+1)\\beta_{\\ell}(1+\\log T)$ for some universal constant $\\beta_{\\ell}$ which only depends on $\\ell$ and $K$ . Consequently, FTL ensures $\\mathsf{P U C a l}_{\\mathcal{L}_{d e c}}=$ $\\begin{array}{r}{\\mathsf{U C a l}_{\\mathcal{L}_{d e c}}=\\mathcal{O}((\\operatorname*{sup}_{\\ell\\in\\mathcal{L}_{d e c}}\\beta_{\\ell})K\\log T)}\\end{array}$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. We work with the notation established in the proof of Lemma E.1. The regret of FTL can be bounded as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{REG}\\leq2m+\\sum_{e_{i}\\in\\mathcal{E}_{m}}\\sum_{t\\in\\mathcal{T}_{i}\\backslash\\{t_{i,1}\\}}\\underbrace{\\ell(p_{t},e_{i})-\\ell(p_{t+1},e_{i})}_{\\delta_{t,i}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In the subsequent steps, we shall bound $\\delta_{t,i}$ . We begin in the following manner: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{t,i}=\\ell(p_{t})+\\langle e_{i}-p_{t},\\nabla\\ell(p_{t})\\rangle-\\ell(p_{t+1})-\\langle e_{i}-p_{t+1},\\nabla\\ell(p_{t+1})\\rangle}\\\\ &{\\quad=\\ell(p_{t})-\\ell(p_{t+1})+[\\nabla\\ell(p_{t})]_{i}-[\\nabla\\ell(p_{t+1})]_{i}+\\langle p_{t+1},\\nabla\\ell(p_{t+1})\\rangle-\\langle p_{t},\\nabla\\ell(p_{t})\\rangle}\\\\ &{\\quad\\leq[\\nabla\\ell(p_{t})]_{i}-[\\nabla\\ell(p_{t+1})]_{i}+\\langle p_{t},\\nabla\\ell(p_{t+1})-\\nabla\\ell(p_{t})\\rangle\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the first equality follows from Lemma 1; the first inequality follows since $\\ell(\\pmb{p}_{t})\\leq\\ell(\\pmb{p}_{t+1})+$ $\\langle\\nabla\\ell(p_{t+1}),p_{t}-p_{t+1}\\rangle$ which follows from the concavity of $\\ell$ . Next, by the Mean Value Theorem, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla\\ell(p_{t+1})-\\nabla\\ell(p_{t})=\\nabla^{2}\\ell(p_{t}+v(p_{t+1}-p_{t}))\\cdot(p_{t+1}-p_{t})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for some $v\\in[0,1]$ . Note that $\\begin{array}{r}{p_{t,i}=\\frac{n_{t-1,i}}{t-1},p_{t+1,i}=\\frac{n_{t-1,i}+1}{t}}\\end{array}$ (since $\\pmb{y}_{t}=\\pmb{e}_{i}$ ), therefore $p_{t+1,i}\\ge$ $p_{t,i}$ . Let $\\pmb{\\xi}_{t}:=\\pmb{p}_{t}+v(\\pmb{p}_{t+1}-\\pmb{p}_{t})$ . Then, ", "page_idx": 22}, {"type": "equation", "text": "$$\n[\\nabla^{\\ell}({\\pmb p}_{t})]_{i}-[\\nabla^{\\ell}({\\pmb p}_{t+1})]_{i}=\\left\\langle[\\nabla^{2}\\ell({\\pmb\\xi}_{t})]_{i},{\\pmb p}_{t}-{\\pmb p}_{t+1}\\right\\rangle,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $[\\nabla^{2}\\ell(\\pmb{\\xi}_{t})]_{i}$ denotes the $i$ -th row of $\\left[\\nabla^{2}\\ell(\\pmb{\\xi}_{t})\\right]$ ; we arrive at ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{t,i}\\leq\\big\\langle[\\nabla^{2}\\ell(\\xi_{t})]_{i},p_{t}-p_{t+1}\\big\\rangle+\\big\\langle p_{t+1}-p_{t},\\nabla^{2}\\ell(\\xi_{t})p_{t}\\big\\rangle}\\\\ &{\\qquad=\\big|\\nabla_{i,i}^{2}\\ell(\\xi_{t})\\big|\\cdot\\big(p_{t+1,i}-p_{t,i}\\big)+\\displaystyle\\sum_{j=1}^{K}p_{t,j}\\cdot\\nabla_{j,j}^{2}\\ell(\\xi_{t})\\cdot\\big(p_{t+1,j}-p_{t,j}\\big)}\\\\ &{\\qquad\\leq\\big|\\nabla_{i,i}^{2}\\ell(\\xi_{t})\\big|\\cdot\\big(p_{t+1,i}-p_{t,i}\\big)+\\displaystyle\\sum_{j\\neq i}p_{t,j}\\cdot\\nabla_{j,j}^{2}\\ell(\\xi_{t})\\cdot\\big(p_{t+1,j}-p_{t,j}\\big)}\\\\ &{\\qquad\\leq\\displaystyle\\sum_{j=1}^{K}\\big|\\nabla_{j,j}^{2}\\ell(\\xi_{t})\\big|\\cdot\\big|\\big(p_{t,j}-p_{t+1,j}\\big)\\big|\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first equality follows since $p_{t+1,i}\\geq p_{t,i}$ and $\\nabla_{i,i}^{2}\\ell(\\pmb{\\xi}_{t})\\leq0$ ; the second inequality follows by dropping the term $p_{t,i}\\cdot\\nabla_{i,i}^{2}\\ell(\\pmb{\\xi}_{t})\\cdot(p_{t+1,i}-p_{t,i})$ which is non-positive; the final inequality is because, for $j\\neq i$ , we have $\\begin{array}{r}{p_{t+1,j}=\\frac{n_{t-1,j}}{t}}\\end{array}$ nt\u2212t1,j and pt,j = $\\begin{array}{r}{p_{t,j}=\\frac{n_{t-1,j}}{t-1}}\\end{array}$ , therefore $p_{t+1,j}\\leq p_{t,j}$ , and bounding $p_{t,j}\\leq1$ . ", "page_idx": 22}, {"type": "text", "text": "To proceed with the further bounding, we apply Lemma 2 and utilize the growth condition on the Hessian $\\begin{array}{r}{\\left|\\nabla_{j,j}^{2}\\ell(\\pmb{\\xi}_{t})\\right|\\leq\\tilde{c}_{K}\\cdot c_{j}\\cdot\\operatorname*{max}\\left(\\frac{1}{\\xi_{t,j}},\\frac{1}{1-\\xi_{t,j}}\\right)}\\end{array}$ for some constant $c_{j}>0$ and all $j\\in[K]$ (here $\\tilde{c}_{K}$ is the scaling constant such that $\\begin{array}{r}{\\ell(\\pmb{p})=\\tilde{c}_{K}\\sum_{i=1}^{K}\\ell_{i}(p_{i}))}\\end{array}$ . Let $\\beta_{\\ell,j}:=\\tilde{c}_{K}\\cdot c_{j}$ . Using the bound on $\\big|\\nabla_{j,j}^{2}\\ell(\\pmb{\\xi}_{t})\\big|$ to bound the term in (15), we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\delta_{t,i}\\leq\\sum_{j=1}^{K}\\beta_{\\ell,j}\\cdot\\operatorname*{max}\\left(\\frac{1}{\\xi_{t,j}},\\frac{1}{1-\\xi_{t,j}}\\right)\\cdot|p_{t,j}-p_{t+1,j}|\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Next, we shall bound the terms $\\begin{array}{r}{\\mathcal{T}_{1}:=\\frac{p_{t+1,i}-p_{t,i}}{\\xi_{t,i}},\\mathcal{T}_{2}:=\\frac{p_{t+1,i}-p_{t,i}}{1-\\xi_{t,i}},\\mathcal{T}_{3}:=\\frac{p_{t,j}-p_{t+1,j}}{\\xi_{t,j}}}\\end{array}$ , and $\\tau_{4}:=$ $\\frac{p_{t,j}-p_{t+1,j}}{1-\\xi_{t,j}}$ , where the index $j$ in $\\mathcal{T}_{3}$ and $\\tau_{4}$ runs over $j\\neq i$ . Note that, ", "page_idx": 22}, {"type": "equation", "text": "$$\nT_{1}=\\frac{p_{t+1,i}-p_{t,i}}{p_{t,i}+v(p_{t+1,i}-p_{t,i})}\\leq\\left(\\frac{p_{t+1,i}-p_{t,i}}{p_{t,i}}\\right)=\\left(\\frac{n_{t-1,i}+1}{n_{t-1,i}}\\cdot\\frac{t-1}{t}-1\\right)\\leq\\frac{1}{n_{t-1,i}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first equality follows from the definition of $\\xi_{t}$ ; the first inequality follows since $p_{t+1,i}\\geq p_{t,i}$ and $v\\in[0,1]$ . Similarly, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}_{2}=\\frac{p_{t+1,i}-p_{t,i}}{1-p_{t,i}-v\\left(p_{t+1,i}-p_{t,i}\\right)}\\leq\\frac{p_{t+1,i}-p_{t,i}}{1-p_{t+1,i}}}\\\\ &{\\quad=\\left(\\frac{n_{t-1,i}+1}{t}-\\frac{n_{t-1,i}}{t-1}\\right)\\cdot\\frac{1}{1-\\frac{n_{t-i,i}+1}{t}}}\\\\ &{\\quad=\\frac{t-1-n_{t-1,i}}{t\\cdot(t-1)}\\cdot\\frac{t}{t-1-n_{t-1,i}}=\\frac{1}{t-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first inequality follows since $p_{t+1,i}\\geq p_{t,i}$ . For $\\mathcal{T}_{3}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nT_{3}=\\frac{1-\\frac{p_{t+1,j}}{p_{t,j}}}{1+v\\left(\\frac{p_{t+1,j}}{p_{t,j}}-1\\right)}=\\frac{\\frac{1}{t}}{1-\\frac{v}{t}}\\leq\\frac{1}{t-1},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the inequality follows since $v\\in[0,1]$ . Finally, we bound $\\tau_{4}$ as ", "page_idx": 23}, {"type": "equation", "text": "$$\nT_{4}=\\frac{1-\\frac{p_{t+1,j}}{p_{t,j}}}{\\frac{1}{p_{t,j}}-\\left(1+v\\left(\\frac{p_{t+1,j}}{p_{t,j}}-1\\right)\\right)}=\\frac{\\frac{1}{t}}{\\frac{t-1}{n_{t-1,j}}-1+\\frac{v}{t}}\\leq\\frac{n_{t-1,j}}{t}\\cdot\\frac{1}{t-1-n_{t-1,j}}\\leq\\frac{1}{n_{t-1,i}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the final inequality is because $\\begin{array}{r}{\\sum_{j=1}^{K}n_{t-1,j}=t-1}\\end{array}$ . Collecting the bounds on $\\tau_{1},\\tau_{2},\\tau_{3}$ , and $\\tau_{4}$ , and substituting them back in (16), we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\delta_{t,i}\\leq\\sum_{j=1}^{K}\\beta_{\\ell,j}\\cdot\\operatorname*{max}\\left(\\frac{1}{n_{t-1,i}},\\frac{1}{t-1}\\right)\\leq\\sum_{j=1}^{K}\\beta_{\\ell,j}\\cdot\\left(\\frac{1}{n_{t-1,i}}+\\frac{1}{t-1}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Summing over all $t$ , we obtain $\\mathrm{REG}_{\\ell}\\le2m+\\beta_{\\ell}(S_{1}\\!+\\!S_{2})$ , where $\\begin{array}{r}{\\mathcal{S}_{1}:=\\sum_{e_{i}\\in\\mathcal{E}_{m}}\\sum_{t\\in\\mathcal{T}_{i}\\backslash\\{t_{i,1}\\}}\\frac{1}{n_{t-1,i}}}\\end{array}$ , $\\begin{array}{r}{\\mathcal{S}_{2}:=\\sum_{e_{i}\\in\\mathcal{E}_{m}}\\sum_{t\\in\\mathcal{T}_{i}\\backslash\\{t_{i,1}\\}}\\frac{1}{t-1}}\\end{array}$ , and $\\begin{array}{r}{\\beta_{\\ell}:=\\sum_{i=1}^{K}\\beta_{\\ell,i}.}\\end{array}$ . Note that the subscript in $\\beta_{\\ell}$ denotes that the constant is dependent on the loss $\\ell$ and only depends on $\\ell$ and $K$ . We bound $S_{1}$ as ", "page_idx": 23}, {"type": "equation", "text": "$$\nS_{1}=\\sum_{e_{i}\\in\\mathcal{E}_{m}}\\sum_{j=1}^{k_{i}-1}\\frac{1}{j}\\le m\\sum_{t=1}^{T}\\frac{1}{j}\\le m(1+\\log T)\\le K(1+\\log T).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next, note that $\\begin{array}{r}{S_{2}\\le\\sum_{t=1}^{T}\\frac{1}{t}\\le1+\\log T}\\end{array}$ . Thus, $S_{1}+S_{2}\\,\\le\\,(K+1)(1+\\log T)$ , which yields $\\mathrm{REG}_{\\ell}\\le2K+(K+1)\\mathring{\\beta}_{\\ell}(\\bar{1}+\\log T)$ . This completes the proof. ", "page_idx": 23}, {"type": "text", "text": "H Proof of Lemma 2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Lemma 2. For a function $f$ that is concave, Lipschitz, and bounded over $[0,1]$ and twice continuously differentiable over $(0,1)$ , there exists a constant $c>0$ such that $\\begin{array}{r}{|f^{\\prime\\prime}(p)|\\leq c\\cdot\\operatorname*{max}\\left(\\frac{1}{p},\\frac{1}{1-p}\\right)}\\end{array}$ for all $p\\in(0,1)$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. Since $f$ is twice differentiable, if $\\left|f^{\\prime\\prime}(p)\\right|$ does not approach infinity at the boundary of $[0,1]$ , then there is nothing to prove. In the rest of the proof, we assume that $\\left|{\\dot{f}}^{\\prime\\prime}(p)\\right|$ approaches infinity both when $p$ approaches 0 and when $p$ approaches 1 (the case when it only approaches infinty at one side is exactly the same). ", "page_idx": 23}, {"type": "text", "text": "Using a technical result from Lemma H.1, there exists an $\\epsilon_{0}\\in(0,1)$ such that for any $p\\in(0,\\epsilon_{0}]$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n|f^{\\prime\\prime}(p)|={\\frac{|f^{\\prime}(q)-f^{\\prime}(0)|}{q}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for some $q\\in[p,1]$ , which can be further bounded by ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\frac{|f^{\\prime}(q)-f^{\\prime}(0)|}{p}}\\leq{\\frac{|f^{\\prime}(q)|+|f^{\\prime}(0)|}{p}}\\leq2\\cdot{\\frac{\\operatorname*{sup}_{q\\in[0,1]}|f^{\\prime}(q)|}{p}}\\leq c_{1}\\operatorname*{max}\\left({\\frac{1}{p}},{\\frac{1}{1-p}}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with $c_{1}:=2\\operatorname*{sup}_{q\\in[0,1]}|f^{\\prime}(q)|$ (finite due to $f$ being Lipschitz). Similarly, there exists an $\\epsilon_{1}\\in(0,1)$ such that for any $p\\in[1-\\epsilon_{1},1)$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n|f^{\\prime\\prime}(p)|={\\frac{|f^{\\prime}(1)-f^{\\prime}(q)|}{1-q}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for some $q\\in[0,p]$ , which is further bounded by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{|f^{\\prime}(1)-f^{\\prime}(q)|}{1-p}\\leq\\frac{|f^{\\prime}(1)|+|f^{\\prime}(q)|}{1-p}\\leq c_{1}\\operatorname*{max}\\left(\\frac{1}{p},\\frac{1}{1-p}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Finally, for any $p\\in(\\epsilon_{0},1-\\epsilon_{1})$ , we trivially bound $\\left|f^{\\prime\\prime}(p)\\right|$ as ", "page_idx": 23}, {"type": "equation", "text": "$$\n|f^{\\prime\\prime}(p)|\\leq\\operatorname*{max}\\left(\\frac{1}{p},\\frac{1}{1-p}\\right)\\underbrace{\\operatorname*{sup}_{q\\in(\\epsilon_{0},1-\\epsilon_{1})}\\frac{|f^{\\prime\\prime}(q)|}{\\operatorname*{max}\\left(\\frac{1}{q},\\frac{1}{1-q}\\right)}}_{c_{2}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $c_{2}$ is finite since $f$ is twice continuously differentiable in $(0,1)$ . Setting $c=\\operatorname*{max}(c_{1},c_{2})$ finishes the proof. \u5382 ", "page_idx": 23}, {"type": "text", "text": "Lemma H.1. Let $f$ satisfy the conditions of Lemma 2 and additionally $\\mathrm{lim}_{p\\to0^{+}}\\left|f^{\\prime\\prime}(p)\\right|\\,=\\,\\infty$ and $\\mathrm{lim}_{p\\rightarrow1^{-}}\\left|f^{\\prime\\prime}(p)\\right|\\,=\\,\\infty$ . Then there exists $\\epsilon_{0}\\,\\in\\,(0,1)$ such that for any $p\\,\\in\\,(0,\\epsilon_{0}]$ , we have $f^{\\prime}(q)\\stackrel{..}{-}f^{\\prime}(0)\\,=\\,f^{\\prime\\prime}(p)q$ for some $q\\in[p,1]$ . Similarly, there exists $\\epsilon_{1}\\,\\in\\,(0,1)$ such that for any $p\\in[1-\\epsilon_{1},1)$ , we have $f^{\\prime}(1)-f^{\\prime}(q)=f^{\\prime\\prime}(p)(1-q)$ for some $q\\in[0,p]$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. For simplicity, we only prove the first part of the lemma since the proof of the second part follows the same argument. Since $f$ is twice continuously differentiable in $(0,1)$ and $\\bar{\\mathrm{lim}_{p\\rightarrow0^{+}}}\\,|f^{\\prime\\prime}(p)|=\\infty$ , there exists an $\\epsilon\\,\\in\\,(0,1)$ such that $\\left|f^{\\prime\\prime}\\right|$ is decreasing in $(0,\\epsilon]$ . Since $f$ is concave, this is equivalent to $f^{\\prime\\prime}$ being increasing in $(0,\\epsilon]$ . ", "page_idx": 24}, {"type": "text", "text": "Now, applying Mean Value Theorem, we know that there exists $\\epsilon_{0}\\in(0,\\epsilon]$ such that $f^{\\prime}(\\epsilon)-f^{\\prime}(0)=$ $f^{\\prime\\prime}(\\epsilon_{0})\\bar{\\epsilon}$ . It remains to prove that for any $p\\in(0,\\epsilon_{0}]$ , the function $g(q):=f^{\\prime}(q)-f^{\\prime}(0)-f^{\\prime\\prime}(p)q$ has a root in $[p,1]$ . This is true because ", "page_idx": 24}, {"type": "equation", "text": "$$\ng(p)=f^{\\prime}(p)-f^{\\prime}(0)-f^{\\prime\\prime}(p)p=(f^{\\prime\\prime}(\\xi)-f^{\\prime\\prime}(p))p\\le0,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the equality is by Mean Value Theorem again for some point $\\xi\\in[0,p]$ and the inequality holds since $f^{\\prime\\prime}$ is increasing in $(0,\\epsilon]$ . On the other hand, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\ng(\\epsilon)=f^{\\prime}(\\epsilon)-f^{\\prime}(0)-f^{\\prime\\prime}(p)\\epsilon=(f^{\\prime\\prime}(\\epsilon_{0})-f^{\\prime\\prime}(p))\\epsilon\\geq0,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the equality holds by the definition of $\\epsilon_{0}$ and the inequality holds since again $f^{\\prime\\prime}$ is increasing in $(0,\\epsilon]$ . Applying the Intermediate Value Theorem then shows that $g(q)$ has a root in $[p,\\epsilon]$ , which finishes the proof. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: All key contributions of the paper are summarized in the abstract and more details can be found in Section 1. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Refer to Sections 1 and 5. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. \u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: In every section, the assumptions are clearly specified, and each lemma/theorem has a proof which can be found in the appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not have any experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often ", "page_idx": 25}, {"type": "text", "text": "one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. ", "page_idx": 26}, {"type": "text", "text": "\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not have any experiments. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not have any experiments. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not have any experiments. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not have any experiments. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The authors have carefully reviewed the NeurIPS Code of Ethics and thereby confirm that this work agrees with it. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not pose any societal impacts. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not use any existing assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]