[{"type": "text", "text": "Neural Conditional Probability for Uncertainty Quantification ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vladimir R. Kostic1,2 Karim Lounici3 Gre\u00b4goire Pacreau3   \nGiacomo Turri1 Pietro Novelli1 Massimiliano Pontil1,4   \n1CSML, Istituto Italiano di Tecnologia 2University of Novi Sad   \n3CMAP-Ecole Polytechnique 4AI Centre, University College London ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We introduce Neural Conditional Probability (NCP), an operator-theoretic approach to learning conditional distributions with a focus on statistical inference tasks. NCP can be used to build conditional confidence regions and extract key statistics such as conditional quantiles, mean, and covariance. It offers streamlined learning via a single unconditional training phase, allowing efficient inference without the need for retraining even when conditioning changes. By leveraging the approximation capabilities of neural networks, NCP efficiently handles a wide variety of complex probability distributions. We provide theoretical guarantees that ensure both optimization consistency and statistical accuracy. In experiments, we show that NCP with a 2-hidden-layer network matches or outperforms leading methods. This demonstrates that a a minimalistic architecture with a theoretically grounded loss can achieve competitive results, even in the face of more complex architectures. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper studies the problem of estimating the conditional distribution associated with a pair of random variables, given a finite sample from their joint distribution. This problem is fundamental in machine learning, and instrumental for various purposes such as building prediction intervals, performing downstream analysis, visualizing data, and interpreting outcomes. This entails predicting the probability of an event given certain conditions or variables, which is a crucial task across various domains, ranging from finance (Markowitz, 1958) to medicine (Ray et al., 2017), to climate modeling (Harrington, 2017) and beyond. For instance, in finance, it is essential for risk assessment to estimate the probability of default given economic indicators. Similarly, in healthcare, predicting the likelihood of a disease, given patient symptoms, aids in diagnosis. In climate modeling, estimating the conditional probability of extreme weather events such as hurricanes or droughts, given specific climate indicators, helps in disaster preparedness and mitigation efforts. ", "page_idx": 0}, {"type": "text", "text": "According to Gao and Hastie (2022), there exist four main strategies to learn the conditional distribution. The first one relies on the Bayes formula for densities and proposes to apply non-parametric statistics to learn the joint and marginal densities separately. However, most of non-parametric techniques face a significant challenge known as the curse of dimensionality (Scott, 1991; Nagler and Czado, 2016). The second strategy, also known as Localization method, involves training a model unconditionally on reweighted samples, where weights are determined by their proximity to the desired conditioning point (Hall et al., 1999; Yu and Jones, 1998). These methods require retraining the model whenever the conditioning changes and may also suffer from the curse of dimensionality if the weighting strategy treats all covariates equally. The third strategy, known as Direct Learning of the conditional distribution involves finding the best linear approximation of the conditional density on a dictionary of base functions or a kernel space (Sugiyama et al., 2010; Li et al., 2007). The performance of these methods relies crucially on the selection of bases and kernels. Again for high-dimensional settings, approaches that assign equal importance to all covariates may be less effective. Finally, the fourth strategy, known as Conditional Training, involves training models to estimate a target variable conditioned on certain covariates. This is typically based on partitioning the covariates space $\\mathcal{X}$ into sets, followed by training models unconditionally within each partition (see Gao and Hastie, 2022; Winkler et al., 2020; Lu and Huang, 2020; Dhariwal and Nichol, 2021, and references therein). However, this strategy requires a large dataset to provide enough samples for each conditioning and is expensive as it requires training separate models for each conditioning input set, even though they stem from the same underlying joint distribution. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Contributions The principal contribution of this work is a different conditional probability approach that does not fall into any of the four aforementioned strategies. Rather than learning the conditional density directly, our method, called Neural Conditional Probability (NCP), aims to learn the conditional expectation operator $\\mathsf{E}_{Y\\mid X}$ associated to the random variables $X\\in\\mathcal{X}$ and $Y\\in\\mathcal{Y}$ based on data from their joint distribution. The operator is defined, for every measurable function $f:\\mathcal{y}\\to\\mathbb{R}$ , as ", "page_idx": 1}, {"type": "equation", "text": "$$\n[\\mathsf E_{Y|X}f](x):=\\mathbb{E}[f(Y)\\,|\\,X=x].\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "NCP is based on a principled loss, leveraging the connection between conditional expectation operators and deepCCA (Andrew et al., 2013) established in (Kostic et al., 2024), and can be used interchangeably to: ", "page_idx": 1}, {"type": "text", "text": "(a) retrieve the conditional density $p_{Y|X}$ with respect to marginal distributions of $X$ and $Y$ ;   \n(b) compute conditional statistics $\\mathbb{E}[f(Y)\\mid X]$ for arbitrary functions $f:\\mathcal{y}\\to\\mathbb{R}$ , including conditional mean, variance, moments, and the conditional cumulative distribution function, thereby providing access to all conditional quantiles simultaneously;   \n(c) estimate the conditional probabilities $\\mathbb{P}[Y\\,\\in\\,B\\,|\\,X\\,\\in\\,A]$ for arbitrary sets $B\\subset\\mathcal{V}$ and $A\\subset\\mathcal{X}$ with theoretical non-asymptotic guarantees on accuracy, allowing us to easily construct conditional confidence regions. ", "page_idx": 1}, {"type": "text", "text": "Notably, our approach extracts statistics directly from the trained operator without retraining or resampling, and it is supported by both optimization consistency and statistical guarantees. In addition our experiments show that our approach matches or exceeds the performance of leading methods, even when using a basic a 2-hidden-layer network. This demonstrates the effectiveness of a minimalistic architecture combined with a theoretically grounded loss function. ", "page_idx": 1}, {"type": "text", "text": "Paper organization In Section 2 we review related work. Section 3 introduces the operator theoretic approach to model conditional expectation, while Section 4 discusses its training pipeline. In Section 5, we derive learning guarantees for NCP. Finally, Section 6 presents numerical experiments. ", "page_idx": 1}, {"type": "text", "text": "2 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Non-parametric estimators are valuable for density and conditional density estimation as they don\u2019t rely on specific assumptions about the density being estimated. Kernel estimators, pioneered by Parzen (1962) and Rosenblatt (1956), are a widely used non-parametric density estimation method. Much effort has been dedicated to enhancing kernel estimation, focusing on aspects like bandwidth selection (Goldenshluger and Lepski, 2011), non-linear aggregation (Rigollet and Tsybakov, 2007), and computational efficiency (Langren\u00b4e and Warin, 2020), as well as extending it to conditional densities (Bertin et al., 2014). A comprehensive review of kernel estimators and their variants is provided in (Silverman, 2017). See also (Tsybakov, 2009) for a statistical analysis of their performance. However, most of non-parametric techniques face a significant challenge known as the curse of dimensionality (Scott, 1991; Nagler and Czado, 2016), meaning that the required sample size for accurate estimation grows exponentially with the dimensionality of the data (Silverman, 2017). Additionally, the computational complexity also increases exponentially with dimensionality (Langrene\u00b4 and Warin, 2020). ", "page_idx": 1}, {"type": "text", "text": "Examples of localization methods include the work by Hall et al. (1999) for conditional CDF estimation using local logistic regression and locally adjusted Nadaraya-Watson estimation, as well as conditional quantiles estimation via local pinball loss minimization in (Yu and Jones, 1998). Examples of direct learning of the conditional distribution include (Sugiyama et al., 2010) via decomposition on a dictionary of base functions. Similarly, Li et al. (2007) explores quantile regression in reproducing Hilbert kernel spaces. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Conditional training is a popular approach which was adopted in numerous works, as in the recent work by Gao and Hastie (2022) where a parametric exponential model for the conditional density $p_{\\theta}(y|x)$ is trained using the Lindsey method within each bin of a partition of the space $\\mathcal{X}$ . This strategy has also been implemented in several prominent classes of generative models, including Normalizing Flow (NF) and Diffusion Models (DM) (Tabak and Vanden-Eijnden, 2010; Dinh et al., 2014; Rezende and Mohamed, 2015a; Sohl-Dickstein et al., 2015). These models work by mapping a simple probability distribution into a more complex one. Conditional training approaches for NF and DM have been developed in many works including (e.g. Winkler et al., 2020; Lu and Huang, 2020; Dhariwal and Nichol, 2021). In efforts to lower the computational burden of conditional diffusion models, an alternative approach used heuristic approximations applied directly to unconditional diffusion models on computer vision related tasks (see e.g. Song et al., 2023; Zhang et al., 2023). However, the effectiveness of these heuristics in accurately mimicking the true conditional distributions remains uncertain. Another crucial aspect of these classes of generative models is that while the probability distribution is modelled explicitly, the computation of any relevant statistic, say $\\mathbb{E}[Y|X]$ is left as an implicit problem usually solved by sampling from $p_{\\theta}(y|x)$ and then approximating $\\bar{\\mathbb{E}}[Y|X]$ via simple Monte-Carlo integration. As expected, this approach quickly becomes problematic as the dimension of the output space $\\boldsymbol{\\wp}$ becomes large. ", "page_idx": 2}, {"type": "text", "text": "Conformal Prediction (CP) is a popular model-agnostic framework for uncertainty quantification (Vovk et al., 1999). Conditional Conformal Prediction (CCP) was later developed to handle conditional dependencies between variables, allowing in principle for more accurate and reliable predictions (see Lei and Wasserman, 2014; Romano et al., 2019; Chernozhukov et al., 2021; Gibbs et al., 2023, and the references cited therein). However, (CP) and (CCP) are not without limitations. The construction of these guaranteed prediction regions need to be recomputed from scratch for each value of the confidence level parameter and of the conditioning for (CCP). In addition, the produced confidence regions tend to be conservative. ", "page_idx": 2}, {"type": "text", "text": "3 Operator approach to probability modeling ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider a pair of random variables $X$ and $Y$ taking values in probability spaces $(\\mathcal{X},\\Sigma_{\\mathcal{X}},\\mu)$ and $(\\mathfrak{V},\\Sigma_{\\mathcal{V}},\\nu)$ , respectively, where $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ are state spaces, $\\Sigma_{\\mathcal{X}}$ and $\\Sigma_{\\mathcal{Y}}$ are sigma algebras, and $\\mu$ and $\\nu$ are probability measures. Let $\\rho$ be the joint probability measure of $(X,Y)$ from the product space $\\mathcal X\\times\\mathcal Y$ . We assume that $\\rho$ is absolutely continuous w.r.t. to the product measure of its marginals, that is $\\rho\\ll\\mu\\times\\nu$ , and denote the corresponding density by $p\\stackrel{=}{=}d\\rho/d(\\mu\\times\\nu)$ , so that $\\rho(d x,d y)=p(x,y)\\mu(d x)\\nu(d y)$ . ", "page_idx": 2}, {"type": "text", "text": "The principal goal of this paper is, given a dataset $D_{n}:=(x_{i},y_{i})_{i\\in[n]}$ of observations of $(X,Y)$ , to estimate the conditional probability measure ", "page_idx": 2}, {"type": "equation", "text": "$$\np(B\\,|\\,x):=\\mathbb{P}[Y\\in B\\,|\\,X=x],\\quad x\\in\\mathcal{X},\\,B\\in\\Sigma_{\\mathcal{Y}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Our approach is based on the simple fact that $p(B\\,|\\,x)\\,=\\,\\mathbb{E}[\\mathbb{1}_{B}(Y)\\,|\\,X\\,=\\,x]$ , where $\\mathbb{1}_{B}$ denotes the characteristic function of set $B$ . More broadly we address the above problem by studying the conditional expectation operator $\\mathsf{E}_{Y|X}\\colon L_{\\nu}^{2}(\\mathcal{Y})\\ \\dot{\\to}\\ L_{\\mu}^{2}(\\mathcal{X})$ , which is defined, for every $f\\in L_{\\nu}^{\\breve{2}}(\\mathcal{D})$ and $x\\in\\mathscr{X}$ , as ", "page_idx": 2}, {"type": "equation", "text": "$$\n[\\mathsf E_{Y|X}f](x):=\\mathbb{E}[f(Y)\\,|\\,X=x]=\\int_{y}f(y)p(d y\\,|\\,x)=\\int_{y}f(y)p(x,y)\\nu(d y),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $L_{\\mu}^{2}(\\mathcal{X})$ and $L_{\\nu}^{2}(\\mathcal{Y})$ denotes the Hilbert spaces of functions that are square integrable w.r.t. to $\\mu$ and $\\nu$ , respectively. One readily verifies that $\\|\\mathsf E_{Y|X}\\|=1$ and $\\mathsf E_{Y|X}\\mathbb{1}_{\\mathcal V}=\\mathbb{1}_{\\mathcal X}$ . ", "page_idx": 2}, {"type": "text", "text": "A prominent feature of the above operator is that its rank can reveal the independence of the random variables. That is, $X$ and $Y$ are independent random variables if and only if $\\mathsf{E}_{Y\\mid X}$ is a rank one operator, in which case we have that $\\mathsf E_{Y|X}=\\mathbb{1}_{X}\\otimes\\mathbb{1}_{\\mathcal V}$ . It is thus useful to consider the deflated operator $\\mathsf{D}_{Y|X}=\\mathsf{E}_{Y|X}-\\mathbb{1}_{X}\\otimes\\mathbb{1}_{\\mathcal{Y}}\\colon L_{\\nu}^{2}(\\mathcal{Y})\\to L_{\\mu}^{2}(\\mathcal{X})$ , for which we have that ", "page_idx": 2}, {"type": "equation", "text": "$$\n[\\mathsf{E}_{Y|X}f](x)=\\mathbb{E}[f(Y)]+[\\mathsf{D}_{Y|X}f](x),\\quad f\\in L_{\\nu}^{2}(\\mathcal{Y}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For dependent random variables, the deflated operator is nonzero. In many important situations, such as when the conditional probability distribution is a.e. absolutely continuous w.r.t. to the target ", "page_idx": 2}, {"type": "text", "text": "measure, that is $p(\\cdot\\,|\\,x)\\ll\\nu$ for $\\mu$ -a.e. $x\\in\\mathscr{X}$ , the operator $\\mathsf{E}_{Y\\mid X}$ is compact, and, hence, we can write the SVD of $\\mathsf{E}_{Y\\mid X}$ and $\\mathsf{D}_{Y|X}$ respectively as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{E}_{Y|X}=\\sum_{i=0}^{\\infty}\\sigma_{i}^{\\star}\\,u_{i}^{\\star}\\otimes v_{i}^{\\star},\\quad\\mathrm{~and~}\\quad\\mathsf{D}_{Y|X}=\\sum_{i=1}^{\\infty}\\sigma_{i}^{\\star}\\,u_{i}^{\\star}\\otimes v_{i}^{\\star},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the left $(u_{i}^{\\star})_{i\\in\\mathbb{N}}$ and right $(v_{i}^{\\star})_{i\\in\\mathbb{N}}$ singular functions form complete orthonormal systems of $L_{\\mu}^{2}(\\mathcal{X})$ and $L_{\\nu}^{2}(\\dot{\\mathcal{D}})$ , respectively. Notice that the only difference in the SVD of $\\mathsf{E}_{Y\\mid X}$ and $\\mathsf{D}_{Y|X}$ is the extra leading singular triplet $(\\sigma_{0}^{\\star},u_{0}^{\\star},v_{0}^{\\star})=(1,\\mathbb{1}_{\\mu},\\mathbb{1}_{\\nu})$ of $\\mathsf{E}_{Y\\mid X}$ . In terms of densities, the SVD of $\\mathsf{E}_{Y\\mid X}$ leads to the characterization ", "page_idx": 3}, {"type": "equation", "text": "$$\np(x,y)=\\sum_{i=0}^{\\infty}\\sigma_{i}^{\\star}\\,u_{i}^{\\star}(x)\\,v_{i}^{\\star}(y)=1+\\sum_{i=1}^{\\infty}\\sigma_{i}^{\\star}\\,u_{i}^{\\star}(x)\\,v_{i}^{\\star}(y).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The mild assumption that $\\mathsf{E}_{Y\\mid X}$ is a compact operator allows one to approximate it arbitrarily well with a (large enough) finite rank (empirical) operator. Choosing the operator norm as the measure of approximation error and appealing to the Eckart-Young-Mirsky Theorem (see Theorem 3 in Appendix B.1) one concludes that the best approximation is given by the truncated SVD, that is for every $d\\in\\mathbb N$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{D}_{Y|X}\\approx\\mathbb{I}[\\mathsf{D}_{Y|X}]_{d}:=\\sum_{i=1}^{d}\\sigma_{i}^{\\star}\\,u_{i}^{\\star}\\otimes v_{i}^{\\star},\\quad\\mathrm{~and~}\\quad\\mathbb{[D}_{Y|X}\\mathbb{\\]}_{d}\\in\\underset{\\mathrm{rank}(A)\\leq d}{\\arg\\operatorname*{min}}\\,\\|\\mathsf{D}_{Y|X}-A\\|,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the minimum is given by $\\sigma_{d}^{\\star}$ , and the minimizer is unique whenever $\\sigma_{d+1}^{\\star}<\\sigma_{d}^{\\star}$ . This leads to the approximation of the joint density w.r.t. marginals $\\begin{array}{r}{p(x,y)\\approx1+\\sum_{i=1}^{d}\\sigma_{i}^{\\star}\\,u_{i}^{\\star}(x)\\,v_{i}^{\\star}(y)}\\end{array}$ , so that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(Y)\\,|\\,X=x]\\approx\\mathbb{E}[f(Y)]+\\sum_{i=1}^{d}\\sigma_{i}^{\\star}\\,u_{i}^{\\star}(x)\\mathbb{E}[f(Y)\\,v_{i}^{\\star}(Y)],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which in particular, choosing $f=\\mathbb{1}_{B}$ , gives ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}[Y\\in B\\,|\\,X=x]\\approx\\mathbb{P}[Y\\in B]+\\sum_{i=1}^{d}\\sigma_{i}^{\\star}\\,u_{i}^{\\star}(x)\\,\\mathbb{E}[v_{i}^{\\star}(Y)\\,\\mathbb{1}_{B}(Y)].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Moreover, we have that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}[Y\\in B\\,|\\,X\\in A]=\\frac{\\langle\\mathbb{1}_{A},\\mathsf{E}_{Y|X}\\mathbb{1}_{B}\\rangle}{\\mathbb{P}[X\\in A]}\\approx\\mathbb{P}[Y\\in B]+\\sum_{i=1}^{d}\\sigma_{i}^{\\star}\\,\\frac{\\mathbb{E}[u_{i}^{\\star}(X)\\mathbb{1}_{A}(X)]}{\\mathbb{P}[X\\in A]}\\,\\mathbb{E}[v_{i}^{\\star}(Y)\\,\\mathbb{1}_{B}(Y)],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for which the approximation error is bounded in the following lemma. ", "page_idx": 3}, {"type": "text", "text": "Lemma 1 (Approximation bound). For any $A\\in\\Sigma_{\\mathcal{X}}$ such that $\\mathbb{P}[X\\in A]>0$ and any $B\\in\\Sigma{y}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathbb{P}[Y\\in B\\left|\\,X\\in A\\right|-\\mathbb{P}[Y\\in B]-\\frac{\\left\\langle\\mathbb{1}_{A},\\left\\|\\mathsf{D}_{Y\\mid X}\\right\\|_{d}\\mathbb{1}_{B}\\right\\rangle}{\\mathbb{P}[X\\in A]}\\right|\\leq\\sigma_{d+1}^{\\star}\\sqrt{\\frac{\\mathbb{P}[Y\\in B]}{\\mathbb{P}[X\\in A]}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Neural network model Inspired by the above observations, to build the NCP model, we will parameterize the truncated SVD of the conditional expectation operator and then learn it. Specifically, we introduce two parameterized embeddings $u^{\\theta}\\colon\\mathcal{X}\\stackrel{\\cdot}{\\rightarrow}\\mathbb{R}^{d}$ and $\\dot{\\boldsymbol{v}}^{\\theta}\\colon\\mathcal{V}\\rightarrow\\mathbb{R}^{d}$ , and the singular values parameterized by $\\overline{w}^{\\theta}\\in\\mathbb{R}^{d}$ , respectively given by ", "page_idx": 3}, {"type": "equation", "text": "$$\nu^{\\theta}(x){:=}[u_{1}^{\\theta}(x)\\;\\ldots\\;u_{d}^{\\theta}(x)]^{\\top},\\;v^{\\theta}(y){:=}[v_{1}^{\\theta}(y)\\;\\ldots\\;v_{d}^{\\theta}(y)]^{\\top},\\mathrm{~and~}\\sigma^{\\theta}{:=}[e^{-(w_{1}^{\\theta})^{2}},\\ldots,e^{-(w_{d}^{\\theta})^{2}}]^{\\top},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the parameter $\\theta$ takes values in a prescribed set $\\Theta$ . ", "page_idx": 3}, {"type": "text", "text": "We then aim to learn the joint density function $p(x,y)$ in the form ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{\\theta}(x,y):=1+\\sum_{i\\in[d]}\\sigma_{i}^{\\theta}u_{i}^{\\theta}(x)\\,v_{i}^{\\theta}(y)=1+\\langle\\sigma^{\\theta}\\odot u^{\\theta}(x),v^{\\theta}(y)\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\odot$ denotes element-wise product. To that end, we consider the loss $\\begin{array}{r}{\\mathcal{L}_{\\gamma}(\\theta):=\\mathcal{L}(\\theta)+\\gamma\\mathcal{R}(\\theta)}\\end{array}$ composed of two terms. The first term ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta):=\\mathbb{E}_{(X^{\\prime},Y^{\\prime})\\sim\\mu\\times\\nu}[[p_{\\theta}(X^{\\prime},Y^{\\prime})-1]^{2}]-2\\mathbb{E}_{(X,Y)\\sim\\rho}[p_{\\theta}(X,Y)]-1\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "essentially has been considered by HaoChen et al. (2022) in the specific context of augmentation graph in self-supervised deep learning, linked to kernel embeddings (Wang et al., 2022), and rediscovered and tested on DeepCCA tasks by Wells et al. (2024). Indeed, this loss can be written in terms of correlations between features. Namely, denoting the covariance and variance matrices by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Cov}[z,z^{\\prime}]:=\\mathbb{E}[(z-\\mathbb{E}[z])(z^{\\prime}-\\mathbb{E}[z^{\\prime}])^{\\top}]\\quad\\mathrm{~and~}\\quad\\mathrm{Var}[z]:=\\mathbb{E}[(z-\\mathbb{E}[z])(z-\\mathbb{E}[z])^{\\top}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and abbreviating $u^{\\theta}:=u^{\\theta}(X)$ and $v^{\\theta}:=v^{\\theta}(Y)$ for simplicity, we can write ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\mathcal{L}}(\\theta):=\\operatorname{tr}\\left(\\operatorname{Var}[{\\sqrt{\\sigma^{\\theta}}}\\odot u^{\\theta}]\\operatorname{Var}[{\\sqrt{\\sigma^{\\theta}}}\\odot v^{\\theta}]-2\\operatorname{Cov}[{\\sqrt{\\sigma^{\\theta}}}\\odot u^{\\theta},{\\sqrt{\\sigma^{\\theta}}}\\odot v^{\\theta}]\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "If $p{=}p_{\\theta}$ for some $\\theta{\\in}\\Theta$ , then the optimal loss is the $\\chi^{2}$ -divergence $\\scriptstyle{\\mathcal{L}}(\\theta)=D_{\\chi^{2}}(\\rho\\,|\\,\\mu\\times\\nu)=-\\sum_{i\\geq1}\\sigma_{i}^{\\star2}$ and, as we show below, ${\\mathcal{L}}(\\theta)$ measures how well $p_{\\theta}(x,y)-1$ approximates $\\textstyle\\sum_{i\\in[d]}\\sigma_{i}^{\\star}\\,u_{i}^{\\star}(x)\\,v_{i}^{\\star}(y)$ . However, in order to obtain a useful probability model, it is of paramount importance to align the metric in the latent spaces with the metrics in the data-spaces $L_{\\mu}^{2}(\\mathcal{X})$ and $\\bar{L}_{\\nu}^{2}(\\mathcal{Y})$ . For different reasons, a similar phenomenon has been observed in Kostic et al. (2024) where dynamical systems are learned via transfer operators. In our setting, this leads to the second term of the loss that measures how well features $u^{\\theta}$ and $v^{\\theta}$ span relevant subspaces in $L_{\\mu}^{2}(\\mathcal{X})$ and $L_{\\nu}^{2}(\\mathcal{Y})$ , respectively. Namely, aiming $\\mathbb{E}[u_{i}^{\\star}(X)u_{j}^{\\star}(X)]=\\mathbb{E}[v_{i}^{\\star}(Y)v_{j}^{\\star}(Y)]=\\mathbb{1}_{\\{i=j\\}}$ , $i,j\\in\\{0,1,\\ldots,d\\}$ leads to ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\theta){:=}\\|\\mathbb{E}[u^{\\theta}(X)u^{\\theta}(X)^{\\top}]{-}I\\|_{F}^{2}{+}\\|\\mathbb{E}[v^{\\theta}(Y)v^{\\theta}(Y)^{\\top}]{-}I\\|_{F}^{2}{+}2\\|\\mathbb{E}[u^{\\theta}(X)]\\|^{2}{+}2\\|\\mathbb{E}[v^{\\theta}(Y)\\|^{2}]~\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We now state our main result on the properties of the loss $\\mathcal{L}_{\\gamma}$ , which extends the result in Wells et al.   \n(2024) to infinite-dimensional operators and guarantees the uniqueness of the optimum due to $\\mathcal{R}$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Let $\\mathsf E_{Y|X}\\colon L_{\\nu}^{2}(\\mathcal V)\\to L_{\\mu}^{2}(\\mathcal X)$ be a compact operator and $\\begin{array}{r}{\\mathsf{D}_{Y|X}=\\sum_{i=1}^{\\infty}\\sigma_{i}^{\\star}u_{i}^{\\star}\\otimes v_{i}^{\\star}\\,l}\\end{array}$ be the SVD of its deflated version. If $u_{i}^{\\theta}\\in L_{\\mu}^{2}(\\mathcal{X})$ and $v_{i}^{\\theta}\\in L_{\\nu}^{2}(\\mathcal{D})$ , for all $\\theta\\in\\Theta$ and $i\\in[d]$ , then for every $\\theta\\in\\Theta$ , $\\begin{array}{r}{\\mathcal{L}_{\\gamma}(\\boldsymbol{\\theta})\\geq-\\sum_{i\\in[d]}\\sigma_{i}^{\\star2}}\\end{array}$ . Moreover, $i f\\gamma>0$ and $\\sigma_{d}^{\\star}>\\sigma_{d+1}^{\\star}$ , then the equality holds $i f$ and only $i f\\left(\\sigma_{i}^{\\theta},u_{i}^{\\theta},v_{i}^{\\theta}\\right)$ equals $(\\sigma_{i}^{\\star},u_{i}^{\\star},v_{i}^{\\star})$ $\\rho$ -a.e., up to unitary transform of singular spaces. ", "page_idx": 4}, {"type": "text", "text": "We provide the proof in Appendix B.3. In the following section, we show how to learn these canonical features from data and construct approximations of the conditional probability measure. ", "page_idx": 4}, {"type": "text", "text": "Comparison to previous methods NCP does not fall into any of the four categories defined by Gao and Hastie (2022), as it does not aim to learn conditional density of $Y|X$ directly. Instead, NCP focuses on learning the operator mapping $L_{\\nu}^{2}(\\mathcal{D})\\rightarrow L_{\\mu}^{2}(\\mathcal{X})$ , from which all relevant task-specific statistics can be derived without requiring retraining. This approach effectively integrates with deep representation learning to create a latent space adapted to $\\bar{p(y|x)}$ . As a result, NCP efficiently captures the intrinsic dimension of the data, which is supported by our theoretical guarantees that depend solely on the latent space dimension (Theorem 2). In contrast, strategies designed for learning density often encounter significant limitations, such as the curse of dimensionality, potential substantial misrepresentation errors when the pre-specified function dictionary misaligns with the true distribution $p(y|\\bar{x})$ , and high computational complexity due to the need for retraining. Experiments confirm NCP\u2019s capability to learn representations tailored to a wide range of data types\u2014including manifolds, graphs, and high-dimensional distributions\u2014without relying on predefined dictionaries. This flexibility allows NCP to outperform popular aforementioned methods. ", "page_idx": 4}, {"type": "text", "text": "4 Training the NCP inference method ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we discuss how to train the model. Given a training dataset $D_{n}=(x_{i},y_{i})_{i\\in[n]}$ and   \nnetworks $(u^{\\theta},v^{\\theta},\\sigma^{\\theta})$ , we consider the empirical loss $\\widehat{\\mathcal{L}}_{\\gamma}(\\theta):=\\widehat{\\mathcal{L}}(\\theta)+\\gamma\\widehat{\\mathcal{R}}(\\theta)$ , where we replaced   \n(8) and (9) by their empirical versions. In order to gu arantee th e unbiase d estimation, as we show   \nwithin the proof of Theorem 1, two terms of our loss can be written using two independent samples   \n$(X,Y)$ and $(X^{\\prime},Y^{\\prime})$ from $\\rho$ as   \n$\\mathcal{L}(\\theta){=}\\mathbb{E}[L(u^{\\theta}(X),u^{\\theta}(X^{\\prime}),v^{\\theta}(Y),v^{\\theta}(Y^{\\prime}),\\sigma^{\\theta})]$ and $\\mathcal{R}(\\theta){=}\\mathbb{E}[R(u^{\\theta}(X),u^{\\theta}(X^{\\prime}),v^{\\theta}(Y),v^{\\theta}(Y^{\\prime}))],$   \nwhere, the loss functionals $L$ and $R$ are defined for $u,u^{\\prime},v,v^{\\prime}\\in\\mathbb{R}^{d}$ and $s\\in[0,1]^{d}$ as $\\begin{array}{r l}&{L(u,u^{\\prime},v,v^{\\prime},s){:=}\\frac{1}{2}\\left(u^{\\top}\\operatorname{diag}(s)v^{\\prime}\\right)^{2}{+}\\frac{1}{2}\\left(u^{\\prime^{\\top}}\\operatorname{diag}(s)v\\right)^{2}{-}(u{-}u^{\\prime})^{\\top}\\operatorname{diag}(s)(v{-}v^{\\prime}),}\\\\ &{\\quad R(u,u^{\\prime},v,v^{\\prime}){:=}(u^{\\top}u^{\\prime})^{2}{-}(u{-}u^{\\prime})^{\\top}(u{-}u^{\\prime}){+}(v^{\\top}v^{\\prime})^{2}{-}(v{-}v^{\\prime})^{\\top}(v{-}v^{\\prime}){+}2d.}\\end{array}$ (10) (11) ", "page_idx": 4}, {"type": "text", "text": "Therefore, at every epoch we take two independent batches $\\mathcal{D}_{n}^{1}$ and $\\mathcal{D}_{n}^{2}$ of equal size from $\\mathcal{D}_{n}$ , leading to Algorithm 1. See Appendix A.1 for the full discussion, and Appendix A.2, where we also provide in Figure 4 an example of learning dynamics. ", "page_idx": 5}, {"type": "table", "img_path": "zXfhHJnMB2/tmp/f5286dc038902f653f2965f712c11651b4dcfb282a986a1bb19432bdc1cba1cc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Practical guidelines for training In the following, we briefly report a few aspects to be kept in mind when using the NCP in practice, referring the reader to Appendix A for further details. First, the computational complexity of unbiased estimation of the loss for a dataset of size $n$ is $O(n d)$ , allowing one to seamlessly use NCP in contemporary DL settings. Second, the size of latent dimension $d$ , as indicated by Theorem 1 relates to the problem\u2019s \u201ddifficulty\u201d in the sense of smoothness of joint density w.r.t. its marginals. Lastly, after the training, an additional post-processing may be applied to ensure the orthogonality of features $u^{\\theta}$ and $v^{\\theta}$ and improve statistical accuracy of the learned model. ", "page_idx": 5}, {"type": "text", "text": "Performing inference with the trained NCP model We now explain how to extract important statistical objects from the trained model $(\\widehat{u}^{\\theta},\\widehat{v}^{\\theta},\\sigma^{\\theta})$ . To this end, define the empirical operator ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}\\colon L_{\\nu}^{2}(\\mathcal{Y})\\!\\!\\to\\!\\!L_{\\mu}^{2}(\\mathcal{X})}&{[\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}f](x)\\!:=\\!\\!\\sum_{i\\in[d]}\\sigma_{i}^{\\theta}\\,\\widehat{u}_{i}^{\\theta}(x)\\,\\widehat{\\mathsf{E}}_{y}[\\widehat{v}_{i}^{\\theta}\\,f],\\quad f\\in L_{\\nu}^{2}(\\mathcal{Y}),\\,x\\in\\mathcal{X},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{\\widehat{\\mathsf{E}}_{y}[\\widehat{v}_{i}^{\\theta}\\;f]\\,:=\\,\\frac{1}{n}\\sum_{j\\in[n]}\\widehat{v}_{i}^{\\theta}(y_{j})f(y_{j})}\\end{array}$ . Then, without any retraining nor simulation, we can compute the following statisti cs: ", "page_idx": 5}, {"type": "text", "text": "$\\blacktriangleright$ Conditional Expectation: $\\begin{array}{r}{[\\widehat{\\mathsf{E}}_{Y|X}^{\\theta}f](x)\\!:=\\!\\widehat{\\mathsf{E}}_{y}f\\!+\\![\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}f](x),\\ f\\in L_{\\nu}^{2}(\\mathcal{Y}),x\\in\\mathcal{X}.}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "$\\blacktriangleright$ Conditional moments of order $\\alpha\\geq1$ : apply previous formula to $f(u)=u^{\\alpha}$ . ", "page_idx": 5}, {"type": "text", "text": "$\\blacktriangleright$ Conditional covariance: $\\widehat{\\mathrm{Cov}}^{\\theta}(Y|X):=\\widehat{\\mathsf{E}}_{Y|X}^{\\theta}[Y Y^{\\top}]-\\widehat{\\mathsf{E}}_{Y|X}^{\\theta}[Y]\\widehat{\\mathsf{E}}_{Y|X}^{\\theta}[Y^{\\top}].$ ", "page_idx": 5}, {"type": "text", "text": "$\\blacktriangleright$ Conditional probabilities: apply the above conditional expectation formula with $f(y)=\\mathbb{1}_{B}(y)$ , that is, $\\widehat{p}_{y}(B)=\\widehat{\\mathsf{E}}_{y}[\\mathbb{1}_{B}]$ and $\\begin{array}{r}{\\widehat{p}_{\\theta}(B\\,|\\,x)=\\widehat{p}_{y}(B)+\\sum_{i\\in[d]}\\sigma_{i}^{\\theta}\\,\\widehat{u}_{i}^{\\theta}(x)\\,\\widehat{\\mathsf{E}}_{y}[\\widehat{v}_{i}^{\\theta}\\mathbb{1}_{B}]}\\end{array}$ , $B\\in\\Sigma{y}$ , $x\\!\\in\\!\\mathcal{X}$ . Then, integrating over an arbitrary set $A\\in\\Sigma_{\\mathcal{X}}$ we get ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{p}_{\\theta}(B\\,|\\,A):=\\widehat{p}_{y}(B)+\\sum_{i\\in[d]}\\sigma_{i}^{\\theta}\\,\\frac{\\widehat{\\mathsf{E}}_{x}[\\widehat{u}_{i}^{\\theta}\\mathbb{1}_{A}]}{\\widehat{\\mathsf{E}}_{x}[\\mathbb{1}_{A}]}\\,\\widehat{\\mathsf{E}}_{y}[\\widehat{v}_{i}^{\\theta}\\mathbb{1}_{B}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$\\blacktriangleright$ Conditional quantiles: for scalar output $Y$ , the conditional CDF $\\widehat{F}_{Y\\mid X\\in A}(t)$ is obtained by taking $B=(-\\infty,t]$ , and in Algorithm 3 in Appendix $\\mathbf{C}$ we show how to extract quantiles from it. ", "page_idx": 5}, {"type": "text", "text": "5 Statistical guarantees ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We introduce some standard assumptions needed to state our theoretical learning guarantees. To that end, for any $A\\in\\Sigma_{\\mathcal{X}}$ and $B\\in\\Sigma{y}$ we define important constants, followed by the main assumption, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\varphi_{X}(A){:=}1\\vee\\sqrt{\\frac{1{-}\\mathbb{P}[X\\in A]}{\\mathbb{P}[X\\in A]}}\\quad\\mathrm{and}\\quad\\varphi_{Y}(B){:=}1\\vee\\sqrt{\\frac{1{-}\\mathbb{P}[Y\\in B]}{\\mathbb{P}[Y\\in B]}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Assumption 1. There exists finite absolute constants $c_{u},c_{v}>1$ such that for any $\\theta\\in\\Theta$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\underset{x\\sim\\mu}{\\mathrm{ess\\,sup}}\\|u^{\\theta}(x)\\|_{l_{\\infty}}\\leq c_{u},\\quad\\underset{y\\sim\\nu}{\\mathrm{ess\\,sup}}\\|v^{\\theta}(y)\\|_{l_{\\infty}}\\leq c_{v}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Next, we set $\\sigma_{\\theta}^{2}(X){\\mathrm{:=}}\\mathrm{Var}(\\|u^{\\theta}(X)-\\mathbb{E}[u^{\\theta}(X)]\\|_{l_{2}}),\\sigma_{\\theta}^{2}(Y){\\mathrm{:=}}\\mathrm{Var}(\\|v^{\\theta}(Y)-\\mathbb{E}[v^{\\theta}(Y)]\\|_{l_{2}})$ and ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\epsilon_{n}(\\delta)\\!:=\\!C\\!\\left((c_{u}\\vee c_{v})\\frac{d\\log(e\\delta^{-1})}{n}\\!+\\!(\\sigma_{\\theta}(X)\\vee\\sigma_{\\theta}(Y))\\sqrt{\\frac{\\log(e\\delta^{-1})}{n}}\\right),\\;\\bar{\\epsilon}_{n}(\\delta)\\!:=\\!2\\sqrt{2\\frac{\\log2\\delta^{-1}}{n}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for some large enough absolute constant $C>0$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 1. It follows easily from Assumption $^{\\,l}$ that $\\sigma_{\\theta}^{2}(X){\\leq}c_{u}^{2}d$ and $\\sigma_{\\theta}^{2}(Y){\\leq}c_{v}^{2}d$ and consequently $\\epsilon_{n}(\\delta)\\lesssim(c_{u}\\vee c_{v})[\\sqrt{d\\log(e\\delta^{-1})/n}\\vee(d\\log(e\\delta^{-1})/n)].$ . ", "page_idx": 6}, {"type": "text", "text": "Finally, for a given parameter $\\theta{\\in}\\Theta$ and $\\delta\\in(0,1)$ , let us denote ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\;\\mathcal{E}_{\\theta}:=\\operatorname*{max}\\{\\|\\|\\mathbb{D}_{Y|X}\\|_{d}\\!-\\!U_{\\theta}S_{\\theta}V_{\\theta}^{*}\\|,\\|U_{\\theta}^{*}U_{\\theta}\\!-\\!I\\|,\\|U_{\\theta}^{*}\\mathbb{1}_{X}\\|,\\|V_{\\theta}^{*}V_{\\theta}\\!-\\!I\\|,\\|V_{\\theta}^{*}\\mathbb{1}_{\\mathcal{Y}}\\|\\},}\\\\ &{\\quad\\quad\\psi_{n}(\\delta):=\\sigma_{d+1}^{\\star}+\\mathcal{E}_{\\theta}+2\\sqrt{1+\\mathcal{E}_{\\theta}}(\\mathcal{E}_{\\theta}+\\varepsilon_{n}(\\delta))+[\\varepsilon_{n}(\\delta)]^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In the following result, we prove that NCP model approximates well the conditional probability distribution w.h.p. whenever the empirical loss $\\widehat{\\mathcal{L}}_{\\gamma}(\\theta)$ is well minimized. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Let Assumption $^{\\,I}$ be satisfied, and in addition assume that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(X\\!\\in\\!A)\\bigwedge\\mathbb{P}(Y\\!\\in\\!B)\\!\\geq\\!\\overline{{\\epsilon}}_{n}(\\delta/3)\\quad a n d\\quad n\\!\\geq\\!(c_{u}\\vee\\!c_{v})^{2}d\\bigvee\\!8\\log(6\\delta^{-1})\\left[\\varphi_{X}(A)\\vee\\varphi_{Y}(B)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then for every $A\\in\\Sigma_{\\mathcal{X}}\\setminus\\{\\mathcal{X}\\}$ and $B\\in\\Sigma_{\\mathcal{V}}\\setminus\\{\\mathcal{V}\\}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left|\\frac{\\mathbb{P}[Y\\!\\in\\!B\\,|\\,X\\!\\in\\!A]}{\\mathbb{P}[Y\\!\\in\\!B]}-\\frac{\\widehat{p}_{\\theta}(B\\,|\\,A)}{\\widehat{p}_{y}(B)}\\right|\\leq\\frac{4\\psi_{n}(\\delta/3)+[1\\!+\\!\\psi_{n}(\\delta/3)]\\,[2\\varphi_{X}(A)\\!+\\!4\\varphi_{Y}(B)]\\,\\overline{{\\epsilon}}_{n}(\\delta/3)}{\\sqrt{\\mathbb{P}[X\\!\\in\\!A]\\mathbb{P}[Y\\!\\in\\!B]}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\bigg\\vert\\frac{\\mathbb{P}[Y\\!\\!\\in\\!\\!B\\,\\vert\\,X\\!\\!\\in\\!\\!A\\vert\\!-\\!\\!\\widehat{p}_{\\theta}(B\\,\\vert\\,A)]}{\\mathbb{P}[Y\\!\\!\\in\\!\\!B]}\\bigg\\vert\\!\\leq\\!\\!\\varphi_{Y}(B)\\overline{{\\epsilon}}_{n}(\\delta/3)\\!+\\!\\frac{2(1\\!+\\!\\psi_{n}(\\delta/3))\\varphi_{X}(A)\\overline{{\\epsilon}}_{n}(\\delta/3)\\!+\\!\\psi_{n}(\\delta/3)}{\\sqrt{\\mathbb{P}[X\\!\\!\\in\\!\\!A]\\mathbb{P}[Y\\!\\!\\in\\!B]}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "hold with probability at least $1-\\delta\\;w.r{t}.$ iid draw of the dataset $\\mathcal{D}_{n}=(x_{j},y_{j})_{j\\in[n]}\\,f r o m\\,\\rho.$ ", "page_idx": 6}, {"type": "text", "text": "Remark 2. In Appendix B.5, we prove a similar result under a less restrictive sub-Gaussian assumption on the singular functions $u^{\\boldsymbol{\\theta}}(\\boldsymbol{X})$ and $v^{\\theta}(Y)$ . ", "page_idx": 6}, {"type": "text", "text": "Discussion The rate $\\psi_{n}(\\delta)$ in (16) is pivotal for the efficacy of our method. If we appropriately choose the latent space dimension $d$ to ensure accurate approximation $(\\sigma_{d+1}^{\\star}\\ll1)$ ), achieve successful training $(\\mathcal{E}_{\\theta}\\ll1)$ , and secure a large enough sample size $(\\varepsilon_{n}(\\delta)\\ll1)$ , Theorem 2 provides assurance of accurate prediction of conditional probabilities. Indeed, (19) guarantees (up to a logarithmic factor) ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{P}[Y\\!\\in\\!B\\,|\\,X\\!\\in\\!A]\\!-\\!\\widehat{p}_{\\theta}(B\\,|\\,A)\\!=\\!O_{\\mathbb{P}}\\left(\\frac{1}{\\sqrt{n}}\\!+\\!\\sqrt{\\frac{\\mathbb{P}[Y\\!\\in\\!B]}{\\mathbb{P}[X\\!\\in\\!A]}}\\left(\\sigma_{d+1}^{\\star}\\!+\\!\\mathcal{E}_{\\theta}\\!+\\!\\sqrt{d/n}\\!+\\!\\varphi_{X}(A)/\\sqrt{n}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note the inclusion of the term $\\sqrt{\\mathbb{P}[X\\in A]}$ in the denominator of the last term on the right-hand side, along with $\\varphi_{X}(A)$ . This indicates a decrease in the accuracy of conditional probability estimates for rarely encountered event $A$ , aligning with intuition and with a known finite-sample impossibility result Lei and Wasserman (2014, Lemma 1) for conditional confidence regions when $A$ is reduced to any nonatomic point of the distribution (i.e. $A=\\{x\\}$ with $\\mathbb{P}[X=x]=0,$ ). For rare events, a larger sample size $n$ and a higher-dimensional latent space characterized by $d$ are necessary for accurate estimation of conditional probabilities. ", "page_idx": 6}, {"type": "text", "text": "We propose next a non-asymptotic estimation guarantee for the conditional CDF of $Y|X$ when $Y$ is a scalar output. This result ensures in particular that accurate estimation of the true quantiles is possible with our method. Fix $t\\ \\in\\ \\mathbb R$ and consider the set $B_{t}\\,=\\,(-\\infty,t]$ meaning that $\\mathbb{P}[Y\\!\\in\\!\\bar{B_{t}}|X\\!\\in\\!A]\\!=\\!F_{Y|X\\in{A}}(t)$ and $\\mathbb{P}[Y\\!\\in\\!B_{t}]\\!=\\!F_{Y}(t)$ . We define similarly for the NCP estimator of the conditional CDF ${\\widehat{F}}_{Y\\mid X\\in A}(t){=}{\\widehat{p}}_{\\theta}(B_{t}\\,|\\,A)$ . The result follows from applying (19) to the set $B_{t}$ . ", "page_idx": 6}, {"type": "text", "text": "Corollary 1. Let the Assumptions of Theorem 2 be satisfied. Then for any $t\\in\\mathbb{R}$ and $\\delta\\in(0,1)$ , $i t$ holds with probability at least $1-\\delta$ that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\widehat{F}_{Y|X\\in A}(t)-F_{Y|X\\in A}(t)|\\leq\\sqrt{F_{Y}(t)(1-F_{Y}(t))}\\bar{\\epsilon}_{n}(\\delta/3)}\\\\ &{\\qquad\\quad+\\,\\sqrt{\\frac{F_{Y}(t)}{\\mathbb{P}[X\\in A]}}\\left(\\sigma_{d+1}^{\\star}+2\\sqrt{2}\\mathcal{E}_{\\theta}+(2\\sqrt{2}+1)\\epsilon_{n}(\\delta/3)+4\\varphi_{X}(A)\\bar{\\epsilon}_{n}(\\delta/3)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "table", "img_path": "zXfhHJnMB2/tmp/4bf56a13c05d7ee0a073026e93afd0482c01fa958808c1d18c37cd017fed1960.jpg", "table_caption": ["Table 1: Mean and standard deviation of Kolmogorov-Smirnov distance of estimated CDF from the truth averaged over 10 repetitions with $n=10^{5}$ (best method in red, second best in bold black). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "An important application of Corollary 1 lies in uncertainty quantification when output $Y$ is a scalar. Indeed, for any $\\alpha\\in(0,1/2)$ , we can scan the empirical conditional CDF $\\widehat{F}_{Y\\mid X\\in A}$ for values $t_{\\alpha}<t_{\\alpha}^{\\prime}$ such that $\\widehat{F}_{Y|X\\in A}(t_{\\alpha}^{\\prime})\\,-\\,\\widehat{F}_{Y|X\\in A}(t_{\\alpha})\\,=\\,1\\,-\\,\\alpha$ and $t_{\\alpha}^{\\prime}-t_{\\alpha}$ is minimal. That way we define a non-asymptotic conditional confidence interval $\\widehat{B}_{\\alpha}:=(t_{\\alpha},t_{\\alpha}^{\\prime}]$ with approximate coverage $1-\\alpha$ . More precisely we deduce from Corollary 1 that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathbb{P}[Y\\in\\widehat{B}_{\\alpha}\\,|\\,X\\in A]-(1-\\alpha)|\\leq\\frac{1}{2}\\overline{{\\epsilon}}_{n}(\\delta/6)}\\\\ &{\\qquad\\qquad\\qquad+\\,\\sqrt{\\frac{1}{\\mathbb{P}[X\\in A]}}\\left(\\sigma_{d+1}^{\\star}+2\\sqrt{2}\\xi_{\\theta}+(2\\sqrt{2}+1)\\epsilon_{n}(\\delta/6)+4\\varphi_{X}(A)\\overline{{\\epsilon}}_{n}(\\delta/6)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Conditional density estimation We applied our NCP method to a benchmark of several conditional density models including those of Rothfuss et al. (2019); Gao and Hastie (2022). See Appendix C.1 for the complete description of the data models and the complete list of compared methods in Tab. 2 with references. We also plotted several conditional CDF along with our NCP estimators in Fig. 6. To assess the performance of each method, we use Kolmogorov-Smirnov (KS) distance between the estimated and the true conditional CDFs. We test each method on nineteen different conditional values uniformly sampled between the $5\\%$ - and $95\\%$ -percentile of $p(x)$ and computed the averaged performance over all the used conditioning values. In Tab. 1, we report mean performance (KS distance $\\pm$ std) computed over 10 repetitions, each with a different seed. NCP with whitening (NCP\u2013W) outperforms all other methods on 4 datasets, ties with FlexCode (FC) on 1 dataset, and ranks a close second on another one behind NF. These experiments underscore NCP\u2019s consistent performance. We also refer to Tab. 3 in App C.1 for an ablation study on post-treatments for NCP. ", "page_idx": 7}, {"type": "text", "text": "Confidence regions Our goal is to estimate conditional confidence intervals for two different data models (Laplace and Cauchy). We investigate the performance of our method in (21) and compare it to the popular conditional conformal prediction approach. We refer to App C.2 for a quick description of the principle underlying CCP. We trained an NCP model combined with an MLP architecture followed by whitening post-processing. See App C.2 for the full description. We obtained that way the NCP conditional CDE model that we used according to (21) to build the conditional $90\\%$ confidence intervals. We proceeded similarly to build another set of conditional confidence intervals based on NFs. Finally, we also implemented the CCP method of Gibbs et al. (2023). ", "page_idx": 7}, {"type": "text", "text": "In Fig. 1, the marginal is $X\\sim\\operatorname{Unif}([0,5])$ and $Y|X=x$ follows either a Laplace distribution (top) with location and scale parameters $(\\mu(x),b(x))=(x^{2},x)$ or a Cauchy distribution (bottom) with location and scale parameters $(x^{2},1+x)$ . In this experiment, we considered a favorable situation for the CCP method of Gibbs et al. (2023) by assuming prior knowledge that the true conditional location is a polynomial function (the truth is actually the square function). Every other parameter of the method was set as prescribed in their paper. ", "page_idx": 7}, {"type": "image", "img_path": "zXfhHJnMB2/tmp/f31b187233b1ffeb487111616bddb117491cceba336aa9e117290425f7e38774.jpg", "img_caption": ["Figure 1: Conditional mean (top only) and ${\\pmb{90\\%}}$ confidence interval for NCP, NFs and CCP. Top: Laplace distribution; Bottom: Cauchy distribution. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "In Fig. 1, observe first that the CCP regression achieves the best estimation of the conditional mean $m s e=3.6\\cdot10^{-3}$ against $m s e=3.8\\overset{\\cdot}{\\cdot}10^{-2}$ for NFs and $m s e=8.3\\cdot10^{-3}$ for NCP, as expected since the CCP regression model is well-specified in this example. However, the CCP confidence intervals are unreliable for most of the considered conditioning. We also notice instability for NF and CCP when conditioning in the neighborhood of $x=0$ , with the NF confidence region exploding at $x=0$ . We suspect this is due to the fact that the conditional distribution at $x=0$ is degenerate, hence violating the condition of existence of a diffeomorphism with the generating prior, a fundamental requirement for NFs models to work at all. Comparatively, NCP does not exhibit such instability around $x=0$ ; it only tends to overestimate the confidence region for conditioning close to $x=0$ . The Cauchy distribution is known to be more challenging due to its heavy tail and undefined moments. In Fig 1 (bottom), we notice that NF and CCP completely collapse. This is not a surprising outcome since CCP relies on estimation of the mean which is undefined in this case, creating instability in the constructed confidence regions, while NF attempts to build a diffeomorphism between a Gaussian prior and the final Cauchy distribution. We suspect the conservative confidence region produced by NF might originate from the successive Jacobians involved in the NF mapping taking large values. In comparison, our NCP method still returns some reasonable results. Although the NCP coverage might appear underestimated for larger $x$ , actual mean coverages computed on a test set of 200 samples are $88\\%$ for NCP, $99\\%$ for NF and $79\\%$ for CCP. Tab. 5 in Appendix C.2 provides a comparison study on real data for learning a confidence region with NCP, NF and a split conformal predictor featuring a Random Forest regressor (RFSCP). ", "page_idx": 8}, {"type": "image", "img_path": "zXfhHJnMB2/tmp/e2aecaf61ec3cc14417113cb595f32007f97945959f818e5353df4fba66dfd29.jpg", "img_caption": ["Figure 2: Protein folding dynamics. Pairwise Euclidean distances between Chignolin atoms exhibit increased variance during folded metastable states (between $87{-}88\\mu\\mathrm{s}$ and around $89.5\\mu\\mathrm{s}$ ). Ground truth is depicted in blue, predicted mean in orange, and the grey lines indicate the estimated $10\\%$ lower and upper quantiles. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "High-dimensional synthetic experiment We simulated the following $d$ -distribution for different values of $d\\,\\in\\,\\{100,500,1000\\}$ . Let $\\Bar{x}\\,=\\,(\\Bar{x}_{1},\\Bar{x}_{2},0,\\dots,0)^{\\intercal}\\,\\in\\,\\mathbb{R}^{d}$ where $\\boldsymbol{x}^{\\prime}\\,=\\,\\left(\\bar{x}_{1},\\bar{x}_{2}\\right)$ admits uniform distribution on the 2-dimensional unit sphere. We pick a random mapping $A\\in{\\mathcal{O}}_{d}$ and we set $X=A{\\bar{x}}$ and the angle $\\theta(X)=\\arcsin(\\bar{x}_{2})$ . Next we consider two conditional distribution models for $Y|X$ (Gaussian and discrete) described in Figure 3. NCP performs similarly to NF in the Gaussian case and outperforms NF for discrete distribution. Figure 7 in Appendix C.3 demonstrates that NCP scales effectively with increasing dimensionality $d$ . As the dimension rises from $d=100$ to $d\\,=\\,1000$ , the computation time increases by only $20\\%$ , while maintaining strong statistical performance throughout. ", "page_idx": 9}, {"type": "image", "img_path": "zXfhHJnMB2/tmp/9d372251b2a0b2b7d07f5e9a6a8c2ce7c04dd2b1f0da6e0366879d79a97801de.jpg", "img_caption": ["Figure 3: High-dimensional synthetic experiment. We consider two models for $Y|X$ with $d=100$ . Left: $Y|{\\bar{X}}\\sim N(\\theta{\\bar{(}}X),\\sin(\\theta(X)/2)$ . Right: $Y\\in\\{1,2,3,4,5\\}$ admits discrete distribution depending on $\\theta(X)$ : $Y|X\\sim P_{1}$ if $\\theta(X)\\in[0,\\pi/2)$ , $P_{2}$ if $\\theta(X)\\in[\\pi/2,\\pi)$ , $P_{3}$ if $\\theta(X)\\in[\\pi,3\\pi/2)$ , $P_{4}$ if $\\theta(X)\\in[3\\pi/2,2\\pi)$ . We take $P_{1}=(1/5,1/5,1/5,1/5,1/5)$ , $P_{2}=(1/2,1/2,0,0,0)$ , $P_{3}=(0,0,1,0,0)$ , $P_{4}=(0,0,0,1/2,1/2)$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "High-dimensional experiment in molecular dynamics We investigate protein folding dynamics and predict conditional transition probabilities between metastable states. Figure 2 shows how, by integrating our NCP approach with a graph neural network (GNN), we achieve accurate state forecasting and strong uncertainty quantification, enabling efficient tracking of transitions. For further context and a full model description, see App C.3. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduced NCP, a novel neural operator approach to learn the conditional probability distribution from complex and highly nonlinear data. NCP offers a number of benefits. Notably, it streamlines the training process by requiring just one unconditional training phase to learn the joint distribution $p(x,y)$ . Subsequently, it allows us to efficiently derive conditional probabilities and other relevant statistics from the trained model analytically, without any additional conditional training steps or Monte Carlo sampling. Additionally, our method is backed by theoretical non-asymptotic guarantees ensuring the soundness of our training method and the accuracy of the obtained conditional statistics. Our experiments on learning conditional densities and confidence regions demonstrate our approach\u2019s superiority or equivalence to leading methods, even using a simple Multi-Layer Perceptron (MLP) with two hidden layers and GELU activations. This highlights the effectiveness of a minimalistic architecture coupled with a theoretically grounded loss function. While complex architectures often dominate advanced machine learning, our results show that simplicity can achieve competitive results without compromising performance. Our numerical experiments suggest that, while our approach works well across different datasets and models, the price we pay for this generality appears to be the need for a relatively large sample size $(n\\gtrsim10^{4})$ to start outperforming other methods. Hence, a future direction is to study how to incorporate prior knowledge into our method to make it more data-efficient. Future works will also investigate the performance of NCP for multi-dimensional time series, causality and more general sensitivity analysis in uncertainty quantification. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We acknowledge financial support from EU Project ELIAS under grant agreement No. 101120237, by NextGenerationEU and MUR PNRR project PE0000013 CUP J53C22003010006 \u201cFuture Artificial Intelligence Research (FAIR)\u201d and by NextGenerationEU and MUR PNRR project RAISE \u201cRobotics and AI for Socio-economic Empowerment\u201d (ECS00000035). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Ambrogioni, L., Gu\u00a8\u00b8clu\u00a8, U., van Gerven, M. A. J., and Maris, E. (2017). The kernel mixture network: A nonparametric method for conditional density estimation of continuous random variables. ", "page_idx": 10}, {"type": "text", "text": "Andrew, G., Arora, R., Bilmes, J., and Livescu, K. (2013). Deep canonical correlation analysis. In International Conference on Machine Learning, pages 1247\u20131255.   \nBercu, B., Delyon, B., and Rio, E. (2015). Concentration Inequalities for Sums and Martingales. SpringerBriefs in Mathematics. Springer.   \nBertin, K., Lacour, C., and Rivoirard, V. (2014). Adaptive pointwise estimation of conditional density function. arXiv:1312.7402.   \nBishop, C. M. (1994). Mixture density networks.   \nCaponnetto, A. and De Vito, E. (2007). Optimal rates for the regularized least-squares algorithm. Foundations of Computational Mathematics, 7(3):331\u2013368.   \nChanussot, L., Das, A., Goyal, S., Lavril, T., Shuaibi, M., Riviere, M., Tran, K., Heras-Domingo, J., Ho, C., Hu, W., Palizhati, A., Sriram, A., Wood, B., Yoon, J., Parikh, D., Zitnick, C. L., and Ulissi, Z. (2021). Open catalyst 2020 (OC20) dataset and community challenges. ACS Catalysis, 11(10):6059\u20136072.   \nChernozhukov, V., Wu\u00a8thrich, K., and Zhu, Y. (2021). Distributional conformal prediction. Proceedings of the National Academy of Sciences, 118(48):e2107794118.   \nDevroye, L., Gy\u00a8orf,i L., and Lugosi, G. (1996). A Probabilistic Theory of Pattern Recognition. Springer New York.   \nDhariwal, P. and Nichol, A. (2021). Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems, volume 34, pages 8780\u20138794.   \nDinh, L., Krueger, D., and Bengio, Y. (2014). Nice: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516.   \nDurkan, C., Bekasov, A., Murray, I., and Papamakarios, G. (2019). Neural spline flows. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alche\u00b4-Buc, F., Fox, E., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.   \nFreeman, P. E., Izbicki, R., and Lee, A. B. (2017). A unified framework for constructing, tuning and assessing photometric redshift density estimates in a selection bias setting. Monthly Notices of the Royal Astronomical Society, 468(4):4556\u20134565.   \nGao, Z. and Hastie, T. (2022). Lincde: conditional density estimation via lindsey\u2019s method. Journal of Machine Learning Research, 23(52):1\u201355.   \nGibbs, I., Cherian, J. J., and Cande\\`s, E. J. (2023). Conformal prediction with conditional guarantees. arXiv:2305.12616.   \nGoldenshluger, A. and Lepski, O. (2011). Bandwidth selection in kernel density estimation: Oracle inequalities and adaptive minimax optimality. Annals of Statistics, 39(3):1608\u20131632.   \nHall, P., Wolff, R. C., and Yao, Q. (1999). Methods for estimating a conditional distribution function. Journal of the American Statistical Association, 94(445):154\u2013163.   \nHaoChen, J. Z., Wei, C., Gaidon, A., and Ma, T. (2022). Provable guarantees for self-supervised deep learning with spectral contrastive loss. In Advances in Neural Information Processing Systems, volume 34, pages 5000\u20135011.   \nHarrington, L. J. (2017). Investigating differences between event-as-class and probability densitybased attribution statements with emerging climate change. Climatic Change, 141:641\u2013654.   \nHo, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851.   \nIzbicki, R. and Lee, A. B. (2017). Converting high-dimensional regression to high-dimensional conditional density estimation. Electronic Journal of Statistics, 11(2):2800 \u2013 2831.   \nIzbicki, R., Lee, A. B., and Freeman, P. E. (2017). Photo- $z$ estimation: An example of nonparametric conditional density estimation under selection bias. The Annals of Applied Statistics, 11(2):698 \u2013 724.   \nKoltchinskii, V. and Lounici, K. (2017). Concentration inequalities and moment bounds for sample covariance operators. Bernoulli, pages 110\u2013133.   \nKostic, V., Novelli, P., Grazzi, R., Lounici, K., and Pontil, M. (2024). Learning invariant representations of time-homogeneous stochastic dynamical systems. In International Conference on Learning Representations (ICLR).   \nLangrene\u00b4, N. and Warin, X. (2020). Fast multivariate empirical cumulative distribution function with connection to kernel density estimation. arXiv:2005.03246.   \nLei, J. and Wasserman, L. (2014). Distribution-free prediction bands for non-parametric regression. Journal of the Royal Statistical Society Series B: Statistical Methodology, 76(1):71\u201396.   \nLi, Q. and Racine, J. S. (2006). Nonparametric Econometrics: Theory and Practice, volume 1 of Economics Books. Princeton University Press.   \nLi, Y., Liu, Y., and Zhu, J. (2007). Quantile regression in reproducing kernel Hilbert spaces. Journal of the American Statistical Association, 102(477):255\u2013268.   \nLu, Y. and Huang, B. (2020). Structured output learning with conditional generative flows. Proceedings of the AAAI Conference on Artificial Intelligence, 34(04):5005\u20135012.   \nMarkowitz, H. M. (1958). Portfolio selection: Efficient diversification of investments. Yale University Press, 23.   \nMendil, M., Mossina, L., and Vigouroux, D. (2023). Puncc: a python library for predictive uncertainty calibration and conformalization. In Conformal and Probabilistic Prediction with Applications, pages 582\u2013601. PMLR.   \nMinsker, S. (2017). On some extensions of bernstein\u2019s inequality for self-adjoint operators. Statistics & Probability Letters, 127:111\u2013119.   \nNagler, T. and Czado, C. (2016). Evading the curse of dimensionality in nonparametric density estimation with simplified vine copulas. Journal of Multivariate Analysis, 151:69\u201389.   \nPapamakarios, G., Pavlakou, T., and Murray, I. (2017). Masked autoregressive flow for density estimation. Advances in neural information processing systems, 30.   \nParzen, E. (1962). On Estimation of a Probability Density Function and Mode. The Annals of Mathematical Statistics, 33(3):1065\u20131076.   \nPospisil, T. and Lee, A. B. (2018). Rfcde: Random forests for conditional density estimation.   \nRay, E. L., Sakrejda, K., Lauer, S. A., Johansson, M. A., and Reich, N. G. (2017). Infectious disease prediction with kernel conditional density estimation. Statistical Medicine, 36(30):4908\u20134929.   \nRezende, D. and Mohamed, S. (2015a). Variational inference with normalizing flows. In Proceedings of the 32nd International Conference on Machine Learning, volume 37, pages 1530\u20131538.   \nRezende, D. and Mohamed, S. (2015b). Variational inference with normalizing flows. In International conference on machine learning, pages 1530\u20131538. PMLR.   \nRigollet, P. and Tsybakov, A. (2007). Linear and convex aggregation of density estimators. Mathematical Methods od Statistics, 16:260\u2013280.   \nRomano, Y., Patterson, E., and Candes, E. (2019). Conformalized quantile regression. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alche\u00b4-Buc, F., Fox, E., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.   \nRonneberger, O., Fischer, P., and Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention\u2013MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234\u2013241. Springer.   \nRosenblatt, M. (1956). Remarks on some nonparametric estimates of a density function. The Annals of Mathematical Statistics, 27(3):832\u2013837.   \nRothfuss, J., Ferreira, F., Walther, S., and Ulrich, M. (2019). Conditional density estimation with neural networks: Best practices and benchmarks. arXiv preprint arXiv:1903.00954.   \nSch\u00a8utt, K. T., Hessmann, S. S. P., Gebauer, N. W. A., Lederer, J., and Gastegger, M. (2023). SchNetPack 2.0: A neural network toolbox for atomistic machine learning. The Journal of Chemical Physics, 158(14):144801.   \nSch\u00a8utt, K. T., Kessel, P., Gastegger, M., Nicoli, K. A., Tkatchenko, A., and M\u00a8uller, K.-R. (2019). SchNetPack: A Deep Learning Toolbox For Atomistic Systems. Journal of Chemical Theory and Computation, 15(1):448\u2013455.   \nScott, D. W. (1991). Feasibility of multivariate density estimates. Biometrika, 78(1):197\u2013205.   \nSilverman, B. W. (1986). Density estimation for statistics and data analysis, volume 26 of Monographs on Statistics & Applied Probability. Chapman and Hall.   \nSilverman, B. W. (2017). Density Estimation for Statistics and Data Analysis. Routledge, New York.   \nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. (2015). Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256\u20132265. PMLR.   \nSong, J., Vahdat, A., Mardani, M., and Kautz, J. (2023). Pseudoinverse-guided diffusion models for inverse problems. In International Conference on Learning Representations.   \nStimper, V., Liu, D., Campbell, A., Berenz, V., Ryll, L., Scho\u00a8lkopf, B., and Herna\u00b4ndez-Lobato, J. M. (2023). normflows: A pytorch package for normalizing flows. Journal of Open Source Software, 8(86):5361.   \nSugiyama, M., Takeuchi, I., Suzuki, T., Kanamori, T., Hachiya, H., and Okanohara, D. (2010). Conditional density estimation via least-squares density ratio estimation. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 781\u2013788.   \nTabak, E. G. and Vanden-Eijnden, E. (2010). Density estimation by dual ascent of the log-likelihood. Communications in Mathematical Sciences, 8(1):217\u2013233.   \nTsybakov, A. B. (2009). Introduction to Nonparametric Estimation. Springer Series in Statistics.   \nVershynin, R. (2011). Introduction to the non-asymptotic analysis of random matrices. arXiv:1011.3027.   \nVovk, V., Gammerman, A., and Saunders, C. (1999). Machine-learning applications of algorithmic randomness. In International Conference on Machine Learning, pages 444\u2013453.   \nWang, Z., Luo, Y., Li, Y., Zhu, J., and Sch\u00a8olkopf, B. (2022). Spectral representation learning for conditional moment models. arXiv preprint arXiv:2210.16525.   \nWells, L., Thurimella, K., and Bacallado, S. (2024). Regularised canonical correlation analysis: graphical lasso, biplots and beyond. arXiv preprint arXiv:2403.02979.   \nWinkler, C., Worrall, D., Hoogeboom, E., and Welling, M. (2020). Learning likelihoods with conditional normalizing flows. arXiv:1912.00042.   \nYu, K. and Jones, M. (1998). Local linear quantile regression. Journal of the American Statistical Association, 93(441):228\u2013237.   \nZhang, G., Ji, J., Zhang, Y., Yu, M., Jaakkola, T. S., and Chang, S. (2023). Towards coherent image inpainting using denoising diffusion implicit models. arXiv:2304.03322. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Supplemental material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The appendix is organized as follows: ", "page_idx": 13}, {"type": "text", "text": "\u2022 Appendix A provides additional details on the post-processing for NCP. \u2022 Appendix B contains the proofs of the theoretical results and additional statistical results. \u2022 In Appendix C, comprehensive details are presented regarding the experiment benchmark utilized to evaluate the performances of NCP. ", "page_idx": 13}, {"type": "text", "text": "A Details on training and algorithms ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Practical guidelines for training NCP ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "\u2022 It is better to choose a larger $d$ rather than a smaller one. Typically for the problems we considered in Section 6, we used $d\\in\\{100,500\\}$ . ", "page_idx": 13}, {"type": "text", "text": "\u2022 The regularization parameter $\\gamma$ was found to yield the best results for $\\gamma\\in\\{10^{-2},10^{-3}\\}$ . ", "page_idx": 13}, {"type": "text", "text": "\u2022 To ensure the positivity of the singular values, we transform the vector $w^{\\theta}$ with the Gaussian function $x\\mapsto\\exp(-\\dot{x}^{2})$ to recover $\\sigma^{\\theta}$ during any call of the forward method. The vector $w^{\\theta}$ is initialized at random with parameters following a normal distribution of mean 0 and standard deviation $^1\\!/\\!d$ .   \n\u2022 With the ReLU function, we observe instabilities in the loss function during training, whereas Tanh struggles to converge. In contrast, the use of GELU solves both problems.   \n\u2022 We can compute some statistical objects as a sanity check for the convergence of NCP training. For instance, we can ensure that the computed conditional CDFsatisfies all the conditions to be a valid CDF.   \n\u2022 After training, an additional post-processing may be applied to ensure the orthogonality of operators $u^{\\dot{\\theta}}$ and $v^{\\theta}$ . This whitening step is described in Alg 2 in App A.3. It leads to an improvement of statistical accuracy of the trained NCP model. See the ablation study in Tab. 3. ", "page_idx": 13}, {"type": "text", "text": "A.2 Learning dynamics with NCP ", "text_level": 1, "page_idx": 13}, {"type": "image", "img_path": "zXfhHJnMB2/tmp/838598aa7a3e579546a8254d1717f457008cb18dacd482b1c2e7d469d564ae8f.jpg", "img_caption": ["Figure 4: Learning dynamic for the Laplace experiment in Section 6. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.3 Whitening post-processing ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We describe in Algorithm 2 the whitening post-processing procedure that we apply after training. ", "page_idx": 13}, {"type": "text", "text": "Require: new data $(X_{\\mathrm{new}},Y_{\\mathrm{new}})$ ; trained $u^{\\theta}$ , $\\sigma^{\\theta}$ and $v^{\\theta}$ Evaluate $u_{X}=u^{\\theta}(X_{\\operatorname{train}})$ and $v_{Y}=v^{\\theta}(Y_{\\mathrm{train}})$ Centering: $u_{X}\\leftarrow\\bar{u}_{X}-\\hat{\\mathsf{E}}(u^{\\theta}(X_{\\mathrm{train}}))$ and $v_{Y}\\leftarrow v_{Y}-\\hat{\\mathsf{E}}(v^{\\theta}(Y_{\\mathrm{train}}))$ $u_{X}\\leftarrow u_{X}\\mathrm{diag}(\\sigma^{\\theta})^{\\frac{1}{2}}$ and $v_{Y}\\leftarrow v_{Y}\\mathrm{diag}(\\sigma^{\\theta})^{\\frac{1}{2}}$ Compute covariance matrices : $\\begin{array}{r l}&{C_{X}\\leftarrow u_{x}^{\\top}u_{x}/n}\\\\ &{C_{Y}\\leftarrow v_{Y}^{\\top}v_{Y}/n}\\\\ &{C_{X Y}\\leftarrow u_{x}^{\\top}v_{Y}/n}\\\\ &{U,V,\\sigma^{\\mathrm{new}}\\leftarrow\\mathrm{SVD}\\left(C_{X}^{-1/2}C_{X Y}C_{Y}^{-1/2}\\right)}\\\\ &{\\mathbf{if}\\:(X_{\\mathrm{new}},Y_{\\mathrm{new}})\\:\\mathrm{is\\;different\\;than}\\:(X_{\\mathrm{train}},Y_{\\mathrm{train}})\\:\\mathbf{then}}\\\\ &{\\quad u_{X}\\leftarrow\\Big(u^{\\theta}(X_{\\mathrm{new}})-\\widehat{\\mathsf{E}}(u^{\\theta}(X_{\\mathrm{train}})\\Big)\\:\\mathrm{diag}(\\sigma^{\\theta})^{\\frac{1}{2}}}\\\\ &{v_{Y}\\leftarrow\\Big(v^{\\theta}(X_{\\mathrm{new}})-\\widehat{\\mathsf{E}}(v^{\\theta}(Y_{\\mathrm{train}})\\Big)\\:\\mathrm{diag}(\\sigma^{\\theta})^{\\frac{1}{2}}}\\\\ &{\\quad v_{X}\\leftarrow\\Big(v^{\\theta}(X_{\\mathrm{new}})-\\widehat{\\mathsf{E}}(v^{\\theta}(Y_{\\mathrm{train}})\\Big)\\:\\mathrm{diag}(\\sigma^{\\theta})^{\\frac{1}{2}}}\\\\ &{\\quad v_{X}\\leftarrow\\Big(v^{\\theta}(X_{\\mathrm{new}})-\\widehat{\\mathsf{E}}(v^{\\theta}(Y_{\\mathrm{train}})\\Big)\\:\\mathrm{diag}(\\sigma^{\\theta})^{\\frac{1}{2}}}\\\\ &{\\quad v_{X}\\leftarrow\\Big(v^{\\theta}(X_{\\mathrm{new}})-\\widehat{\\mathsf{E}}(v^{\\theta}(X_{\\mathrm{train}})\\Big)\\:\\mathrm{diag}(\\sigma^{\\theta})^{\\frac{1}{2}}}\\\\ &{\\quad v_{Y}\\leftarrow\\Big(v^{\\theta}(X_{\\mathrm{new}})-\\widehat{\\mathsf{E}}(v^{\\theta}(X_{\\mathrm{train}})\\Big)\\:\\mathrm{diag}(\\sigma^{\\theta})^{\\frac{1}{2}}}\\\\ &{\\quad v_{X}\\leftarrow\\end{array}$ end if Final whitening: r $\\begin{array}{r l}&{u_{X}^{\\mathrm{new}}\\leftarrow u_{X}\\breve{C}_{X}^{-1/2}U}\\\\ &{v_{Y}^{\\mathrm{new}}\\leftarrow v_{Y}\\breve{C}_{Y}^{-1/2}V}\\\\ &{\\mathfrak{s t u r n}\\ u_{X}^{\\mathrm{new}},\\sigma^{\\mathrm{new}},v_{Y}^{\\mathrm{new}}}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "B Proofs of theoretical results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 A reminder on Hilbert spaces and compact operators ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Definition 1. Given a vector space $\\mathcal{H}$ , we say it is a Hilbert space if there exists an inner product $\\langle\\cdot,\\cdot\\rangle$ such that: ", "page_idx": 14}, {"type": "text", "text": "An important example of an infinite-dimensional Hilbert space is $L_{\\mu}^{2}(\\mathbb R)$ , the space of squareintegrable functions w.r.t probability measure $\\mu$ on $\\mathbb{R}$ with the inner product defined as $\\langle f,g\\rangle\\,=$ $\\textstyle\\int_{\\mathbb{R}}f(x){\\overline{{g(x)}}}\\,\\mu(d x)$ . ", "page_idx": 14}, {"type": "text", "text": "Definition 2 (Bounded Operators). Let $\\mathcal{H}_{1}$ and $\\mathcal{H}_{2}$ be Hilbert spaces. A linear operator $T:\\mathcal{H}_{1}\\to$ $\\mathcal{H}_{2}$ is called bounded if there exists a constant $C\\geq0$ such that for all $x\\,\\in\\,\\mathcal{H}_{1}$ , the following inequality holds: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|T x\\|_{\\mathcal{H}_{2}}\\leq C\\|x\\|_{\\mathcal{H}_{1}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The smallest such constant $C$ is called the operator norm of $T$ , denoted by $\\lVert T\\rVert$ , and is given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|T\\|=\\operatorname*{sup}_{x\\neq0}{\\frac{\\|T x\\|_{\\mathcal{H}_{2}}}{\\|x\\|_{\\mathcal{H}_{1}}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Bounded operators are continuous and play a key role in functional analysis. ", "page_idx": 14}, {"type": "text", "text": "Definition 3 (Compact Operators). Let $\\mathcal{H}_{1}$ and $\\mathcal{H}_{2}$ be Hilbert spaces. A bounded linear operator $T:\\mathcal{H}_{1}\\to\\mathcal{H}_{2}$ is called compact if for any bounded sequence $\\{x_{n}\\}\\subset\\mathcal{H}_{1}$ , there exists a subsequence $\\{x_{n_{k}}\\}$ such that $T x_{n_{k}}$ converges in $\\mathcal{H}_{2}$ . ", "page_idx": 14}, {"type": "text", "text": "Compact operators can be viewed as infinite-dimensional analogues of matrices with finite rank in finite-dimensional spaces. ", "page_idx": 14}, {"type": "text", "text": "A key result in the theory of compact operators is the existence of a singular value decomposition $(S V D)$ for compact operators. The following is the statement of the Eckart-Young-Mirsky theorem: ", "page_idx": 14}, {"type": "text", "text": "Theorem 3 (Eckart-Young-Mirsky). Let $T:\\mathcal{H}_{1}\\to\\mathcal{H}_{2}$ be a compact operator between Hilbert spaces. Then $T$ can be decomposed as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nT=\\sum_{i=1}^{\\infty}\\sigma_{i}\\langle\\cdot,u_{i}\\rangle v_{i},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\{u_{i}\\}\\subset\\mathcal{H}_{1}$ and $\\{v_{i}\\}\\subset\\mathcal{H}_{2}$ are orthonormal sets, and $\\sigma_{i}$ are the singular values of T, which satisfy $\\sigma_{1}\\geq\\sigma_{2}\\geq\\cdots\\geq0$ . ", "page_idx": 15}, {"type": "text", "text": "Moreover, for any rank- $k$ operator $\\begin{array}{r}{T_{k}=\\sum_{i=1}^{k}\\sigma_{i}\\langle\\cdot,u_{i}\\rangle v_{i},}\\end{array}$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|T-T_{k}\\|=\\operatorname*{min}_{r a n k(S)\\leq k}\\|T-S\\|,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\|\\cdot\\|$ is the operator norm induced by the Hilbert spaces. ", "page_idx": 15}, {"type": "text", "text": "B.2 Proof of Lemma 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Lemma $^{\\,l}$ . It follows from (3) and (4) that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}[Y\\in B\\,|\\,X\\in A]-\\mathbb{P}[Y\\in B]-\\frac{\\langle\\mathbb{1}_{A},\\|\\mathsf{D}_{Y|X}\\|_{d}\\mathbb{1}_{B}\\rangle}{\\mathbb{P}[X\\in A]}=\\frac{\\langle\\mathbb{1}_{A},\\big(\\mathsf{D}_{Y|X}-\\mathbb{\\|}\\mathsf{D}_{Y|X}\\mathbb{1}_{d}\\big)\\mathbb{1}_{B}\\rangle}{\\mathbb{P}[X\\in A]}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next, by definition of the operator norm, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big|\\langle\\mathbb{1}_{A},\\big(\\mathsf{D}_{Y|X}-\\mathbb{1}[\\mathsf{D}_{Y|X}]\\big|d\\rangle\\mathbb{1}_{B}\\big)\\big|\\leq\\|\\mathsf{D}_{Y|X}-\\mathbb{1}[\\mathsf{D}_{Y|X}]_{d}\\|_{L_{\\nu}^{2}(\\mathcal{Y})\\to L_{\\mu}^{2}(\\mathcal{X})}\\|\\mathbb{1}_{A}\\|_{L_{\\mu}^{2}(\\mathcal{X})}\\|\\mathbb{1}_{B}\\|_{L_{\\nu}^{2}(\\mathcal{Y})}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\|\\mathsf{D}_{Y|X}-\\mathbb{1}[\\mathsf{D}_{Y|X}]_{d}\\|_{L_{\\nu}^{2}(\\mathcal{Y})\\to L_{\\mu}^{2}(\\mathcal{X})}\\sqrt{\\mathbb{P}[X\\in A]}\\sqrt{\\mathbb{P}[Y\\in B]},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the operator norm $\\|\\mathsf{D}_{Y|X}-\\mathbb{\\left[D}_{Y|X}\\right]_{d}\\|_{L_{\\nu}^{2}(\\mathcal{Y})\\to L_{\\mu}^{2}(\\mathcal{X})}$ is upper bounded by $\\sigma_{d+1}^{\\star}$ by definition of the SVD of $\\mathsf{D}_{Y|X}$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "B.3 Proof of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Theorem $^{\\,l}$ . In the following, to simplify notation, whenever dependency on the parameters is not crucial, recalling that $(X,Y)$ and $(\\bar{X^{\\prime}},\\bar{Y^{\\prime}})$ are two iid samples from the joint distribution $\\rho$ , we will denote the vector-valued random variables in the latent (embedding) space as $u:=u^{\\theta}(X)$ , $u^{\\prime}:=u^{\\theta}(X^{\\prime})$ , $v:=v^{\\theta}(Y)$ and $v^{\\prime}:=v^{\\theta}(Y^{\\prime})$ , as well as $s=\\sigma^{\\theta}$ and $S:=S_{\\theta}$ . Then, we can write the training loss simply as $\\mathbb{E}\\left[L_{\\gamma}(u,u^{\\prime},v,v^{\\prime},S)\\right]$ . ", "page_idx": 15}, {"type": "text", "text": "First, let us prove that $\\mathcal{L}_{0}(\\boldsymbol{\\theta})=\\|U_{\\boldsymbol{\\theta}}S_{\\boldsymbol{\\theta}}V_{\\boldsymbol{\\theta}}^{*}\\|_{\\mathrm{HS}}^{2}-2\\operatorname{tr}(S_{\\boldsymbol{\\theta}}U_{\\boldsymbol{\\theta}}^{*}\\mathsf{D}_{Y|X}V_{\\boldsymbol{\\theta}})$ . Indeed, we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\nU_{\\theta}^{*}\\mathsf{D}_{Y|X}V_{\\theta}=U_{\\theta}^{*}\\mathsf{E}_{Y|X}V_{\\theta}-U_{\\theta}^{*}\\mathbb{1}_{X}\\otimes(V_{\\theta}^{*}\\mathbb{1}_{Y})=\\mathbb{E}[u^{\\theta}(X)\\mathbb{E}[v^{\\theta}(Y)^{\\top}\\mid X]]-(\\mathbb{E}[u^{\\theta}(X)])(\\mathbb{E}[v^{\\theta}(Y)])^{\\top}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "that is $U_{\\theta}^{*}\\mathsf{D}_{Y|X}V_{\\theta}=\\mathbb{E}[u v^{\\top}]-\\mathbb{E}[u]]\\mathbb{E}[v]^{\\top}$ is simply centered cross-covariance in the embedding space. Recalling that $U_{\\theta}^{*}U_{\\theta}=\\mathbb{E}[u u^{\\top}]$ and $V_{\\theta}^{*}V_{\\theta}=\\mathbb{E}[v v^{\\top}]$ are covariance matrices in the embedding space, we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{0}(\\theta)=-2\\,\\mathrm{tr}\\,\\mathbb{E}[(S^{1/2}u)(S^{1/2}v)^{\\top}]+2\\,\\mathrm{tr}(\\mathbb{E}[S^{1/2}u]\\,\\mathbb{E}[S^{1/2}v]^{\\top})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\mathrm{tr}(\\mathbb{E}[(S^{1/2}u)(S^{1/2}u)^{\\top}]\\mathbb{E}[(S^{1/2}v)(S^{1/2}v)^{\\top}])}\\\\ &{\\qquad=-2\\mathbb{E}[u^{\\top}S v]+2(\\mathbb{E}[u])^{\\top}S(\\mathbb{E}[v])+\\mathrm{tr}(\\mathbb{E}[(S^{1/2}u)(S^{1/2}u)^{\\top}]\\mathbb{E}[(S^{1/2}v)(S^{1/2}v)^{\\top}])}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which, by taking $(X,Y)$ and $(X^{\\prime},Y^{\\prime})$ to be iid random variables drawn from $\\rho$ , gives that ${\\mathcal{L}}_{0}(\\theta)$ can be written as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[-u S v-u^{\\prime}S v^{\\prime}+u^{\\prime}S v+u S v^{\\prime}+\\frac{1}{2}\\operatorname{tr}\\left(S^{1/2}u u^{\\top}S v^{\\prime}v^{\\top}S^{1/2}+S^{1/2}u^{\\prime}u^{\\top\\top}S v v^{\\top}S^{1/2}\\right)\\right]}\\\\ &{=\\mathbb{E}\\left[\\frac{1}{2}\\left(u^{\\top}S v^{\\prime}\\right)^{2}+\\frac{1}{2}\\left(u^{\\prime\\top}S v\\right)^{2}-(u-u^{\\prime})S(v-v^{\\prime})\\right]}\\\\ &{=\\mathbb{E}\\left[L_{0}(u,u^{\\prime},v,v^{\\prime},s)\\right]=\\mathbb{E}\\left[L_{0}(u^{\\theta}(X),u^{\\theta}(X^{\\prime}),v^{\\theta}(Y),v^{\\theta}(Y^{\\prime}),\\sigma^{\\theta})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which implies that $\\mathcal{L}_{0}(\\boldsymbol{\\theta})=\\|U_{\\boldsymbol{\\theta}}S_{\\boldsymbol{\\theta}}V_{\\boldsymbol{\\theta}}^{*}\\|_{\\mathrm{HS}}^{2}-2\\operatorname{tr}(S_{\\boldsymbol{\\theta}}U_{\\boldsymbol{\\theta}}^{*}\\mathsf{D}_{Y|X}V_{\\boldsymbol{\\theta}})$ . Moreover, to show that ${\\mathcal{L}}_{\\gamma}(\\theta)=$ $\\mathcal{L}_{0}(\\theta)+\\gamma\\,\\mathcal{R}(\\theta)$ . It suffices to note that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Vert U_{\\theta}^{*}U_{\\theta}-I\\Vert_{F}^{2}=\\operatorname{tr}((U_{\\theta}^{*}U_{\\theta}-I)^{2})=\\operatorname{tr}((U_{\\theta}^{*}U_{\\theta})^{2}-2U_{\\theta}^{*}U_{\\theta}+I)=}\\\\ &{\\qquad\\qquad\\qquad=\\operatorname{tr}(\\mathbb{E}[u u^{\\top}]\\mathbb{E}[u^{\\prime}u^{\\top}]-\\mathbb{E}[u u^{\\top}]-\\mathbb{E}[u^{\\prime}u^{\\top}]+I)}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}[\\operatorname{tr}(u u^{\\top}u^{\\prime}u^{\\top}-u u^{\\top}-u^{\\prime}u^{\\top}+I)]=\\mathbb{E}\\left[(u^{\\top}u^{\\prime})^{2}-\\|u\\|^{2}-\\|u^{\\prime}\\|^{2}\\right]+d,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "as well as that $\\|U_{\\theta}^{*}\\mathbb{1}_{\\mu}\\|^{2}=\\|\\mathbb{E}u\\|^{2}=(\\mathbb{E}u)^{\\top}(\\mathbb{E}u)=\\mathbb{E}u^{\\top}u^{\\prime}$ , and apply the analogous reasoning for random variable $Y\\sim\\nu$ . ", "page_idx": 16}, {"type": "text", "text": "Now, given $r>d+1$ , let us denote $\\begin{array}{r}{D_{r}:=\\sum_{i\\in[r]}\\sigma_{i}^{\\star}u_{i}^{\\star}\\otimes v_{i}^{\\star}}\\end{array}$ and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}_{0}^{r}(\\theta):=\\left\\|D_{r}-U_{\\theta}S_{\\theta}V_{\\theta}\\right\\|_{\\mathrm{HS}}^{2}-\\left\\|D_{r}\\right\\|_{\\mathrm{HS}}^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, applying the Eckhart-Young-Mirsky theorem, we obtain that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{0}^{r}(\\theta)\\geq\\sum_{i=d+1}^{r}\\!\\sigma_{i}^{\\star2}-\\sum_{i\\in[r]}\\!\\sigma_{i}^{\\star2}=-\\!\\sum_{i\\in[d]}\\!\\sigma_{i}^{\\star2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with equality holding whenever $(\\sigma_{i}^{\\theta},u_{i}^{\\theta},v_{i}^{\\theta})=(\\sigma_{i}^{\\star},u_{i}^{\\star},v_{i}^{\\star})$ , $\\rho$ -almost everywhere. ", "page_idx": 16}, {"type": "text", "text": "To prove that the same holds for ${\\mathcal{L}}_{0}(\\theta)$ , observe that after expanding the HS norm via trace in (22), we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{0}^{r}(\\theta)=-2\\,\\mathrm{tr}\\left(S_{\\theta}^{1/2}U_{\\theta}^{*}D_{r}V_{\\theta}S_{\\theta}^{1/2}\\right)+\\|U_{\\theta}S_{\\theta}V_{\\theta}^{*}\\|_{\\mathrm{HS}}^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and, consequently, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{0}^{r}(\\theta)=\\Vert U_{\\theta}S_{\\theta}V_{\\theta}^{*}\\Vert_{\\mathrm{HS}}^{2}-2\\operatorname{tr}(S_{\\theta}^{1/2}U_{\\theta}^{*}D_{r}V_{\\theta}S_{\\theta}^{1/2})=\\mathcal{L}_{0}(\\theta)+2\\operatorname{tr}(S_{\\theta}U_{\\theta}^{*}(\\mathsf{D}_{Y|X}-D_{r})V_{\\theta}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, using Cauchy-Schwartz inequality, we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathcal{L}_{0}^{r}(\\theta)-\\mathcal{L}_{0}(\\theta)|\\leq|\\mathrm{tr}(S_{\\theta}U_{\\theta}^{*}(\\mathsf{D}_{Y|X}\\!-\\!D_{r})V_{\\theta})|\\leq\\|S_{\\theta}\\|\\|U_{\\theta}^{*}\\|_{\\mathrm{HS}}\\|\\mathsf{D}_{Y|X}-\\big[\\mathsf{D}_{Y|X}\\big]r\\|\\|V_{\\theta}^{*}\\|_{\\mathrm{HS}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and, therefor $\\mathrm{e},\\,|\\mathcal{L}_{0}^{r}(\\theta)\\,-\\,\\mathcal{L}_{0}(\\theta)|\\,\\le\\,\\sigma_{r+1}^{\\star}\\sqrt{\\mathrm{tr}(U_{\\theta}^{*}U_{\\theta})\\,\\mathrm{tr}(V_{\\theta}^{*}V_{\\theta})}\\,\\le\\,M d\\sigma_{r+1}^{\\star}.$ where the constant is given by $\\begin{array}{r}{M\\,:=\\,\\mathrm{max}_{i\\in[d]}\\{\\|u_{i}^{\\theta}\\|_{L_{\\mu}^{2}(\\mathcal{X})},\\|v_{i}^{\\theta}\\|_{L_{\\nu}^{2}(\\mathcal{Y})}\\}\\,<\\,\\infty}\\end{array}$ . So, $\\begin{array}{r l}{\\mathcal{L}_{0}^{r}(\\theta)\\,-\\,M d\\sigma_{r+1}^{\\star}\\,\\le\\,\\mathcal{L}_{0}(\\theta)\\,\\le\\,}&{{}}\\end{array}$ $\\mathcal{L}_{0}^{r}(\\theta)+M d\\sigma_{r+1}^{\\star}$ , and, since $r>d+1$ was arbitrary, we can take $r$ arbitrary large to obtain $\\sigma_{r}^{\\star}\\to0$ and conclude that $\\begin{array}{r}{{\\mathcal L}_{0}(\\theta)\\ge-\\sum_{i\\in[d]}\\sigma_{i}^{\\star2}}\\end{array}$ , with equality holding when $(\\sigma_{i}^{\\theta},u_{i}^{\\theta},v_{i}^{\\theta})=(\\sigma_{i}^{\\star},u_{i}^{\\star},v_{i}^{\\star})$ , $\\rho$ -almost everywhere, since then $U_{\\theta}^{*}\\dot{\\mathsf{D}}_{Y|X}V_{\\theta}=U_{\\theta}^{*}U_{\\theta}S_{\\theta}=S_{\\theta}=\\mathrm{diag}(\\sigma_{1},\\dots,\\sigma_{d})$ . ", "page_idx": 16}, {"type": "text", "text": "Finally, we prove that $\\gamma>0$ and $\\sigma_{d}^{\\star}>\\sigma_{d+1}^{\\star}$ assure uniqueness of the global optimum. First, if the global minimum is achieved $\\sigma_{d}^{\\star}>\\sigma_{d+1}^{\\star}$ allows one to use uniqueness result in the Eckhart-YoungMirsky theorem that states that $\\begin{array}{r}{\\sum_{i\\in[d]}\\sigma_{i}^{\\star}\\,u_{i}^{\\star}\\otimes\\widehat{v}_{i}^{\\theta}=\\sum_{i\\in[d]}\\sigma_{i}^{\\theta}\\,\\widehat{u}_{i}^{\\theta}\\otimes\\widehat{v}_{i}^{\\theta}}\\end{array}$ . But since, $\\gamma>0$ implies that $\\mathcal{R}(\\theta)=0$ , i.e. $(u_{i}^{\\theta})_{i\\in[d]}\\subset L_{\\mu}^{2}(\\dot{\\mathcal{X}})$ and $(v_{i}^{\\theta})_{i\\in[d]}\\subset L_{\\nu}^{2}(\\mathcal{P})$ a re two orthonormal systems in the corresponding orthogonal complements of constant functions, using the uniqueness of SVD, the proof is completed. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "B.4 Proof of Theorem 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 2. Let us denote the operators arising from centered and empirically centered features as $\\overline{{{U}}}_{\\theta},\\widehat{U}_{\\theta}\\colon\\mathbb{R}^{d}\\rightarrow L_{\\mu}^{2}(\\mathcal{X})$ and $\\bar{V}_{\\theta},\\bar{V}_{\\theta}\\colon\\mathbb R^{d}\\to L_{\\nu}^{2}(\\mathcal{Y})$ by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\overline{{U}}_{\\theta}z:=z^{\\top}(u^{\\theta}-\\mathbb{E}[u^{\\theta}(X)])\\mathbb{1}_{X},\\ \\overline{{V}}_{\\theta}z:=z^{\\top}(v^{\\theta}-\\mathbb{E}[v^{\\theta}(Y)])\\mathbb{1}_{Y}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "respectively, for $z\\in\\mathbb{R}^{d}$ . ", "page_idx": 16}, {"type": "text", "text": "We first bound the error of the conditional expectation model as $\\|\\mathsf{D}_{Y|X}-\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}\\|$ as follows. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathsf{D}_{Y|X}-\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}\\|=\\|\\mathsf{D}_{Y|X}\\pm\\|\\mathsf{D}_{Y|X}\\|_{d}\\pm U_{\\theta}^{*}S_{\\theta}V_{\\theta}\\pm\\overline{{U}}_{\\theta}^{*}S_{\\theta}\\overline{{V}}_{\\theta}-\\widehat{U}_{\\theta}^{*}S_{\\theta}\\widehat{V}_{\\theta}\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\sigma_{d+1}^{\\star}+\\mathcal{E}_{\\theta}+\\|U_{\\theta}^{*}S_{\\theta}V_{\\theta}-\\overline{{U}}_{\\theta}^{*}S_{\\theta}\\overline{{V}}_{\\theta}\\|+\\|\\overline{{U}}_{\\theta}^{*}S_{\\theta}\\overline{{V}}_{\\theta}-\\widehat{U}_{\\theta}^{*}S_{\\theta}\\widehat{V}_{\\theta}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next, using that $\\|S_{\\theta}\\|\\le1$ and that centered covariances are bounded by uncentered ones, i.e. $\\overline{{U}}_{\\theta}^{*}\\overline{{U}}_{\\theta}\\preceq U_{\\theta}^{*}U_{\\theta}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|U_{\\theta}^{*}S_{\\theta}V_{\\theta}-\\overline{{U}}_{\\theta}^{*}S_{\\theta}\\overline{{V}}_{\\theta}\\|=\\|U_{\\theta}^{*}S_{\\theta}V_{\\theta}\\pm\\overline{{U}}_{\\theta}^{*}S_{\\theta}V_{\\theta}-\\overline{{U}}_{\\theta}^{*}S_{\\theta}\\overline{{V}}_{\\theta}\\|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\|U_{\\theta}-\\overline{{U}}_{\\theta}\\|\\|V_{\\theta}\\|+\\|\\overline{{U}}_{\\theta}\\|\\|V_{\\theta}-\\overline{{V}}_{\\theta}\\|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\|U_{\\theta}^{*}\\1_{\\mathcal{N}}\\|\\|V_{\\theta}^{*}V_{\\theta}\\|^{1/2}+\\|V_{\\theta}^{*}\\1_{\\mathcal{N}}\\|\\|U_{\\theta}^{*}U_{\\theta}\\|^{1/2}\\leq2\\mathcal{E}_{\\theta}\\sqrt{1+\\mathcal{E}_{\\theta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In a similar way, we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\overline{{U}}_{\\theta}^{*}S_{\\theta}\\overline{{V}}_{\\theta}-\\widehat{U}_{\\theta}^{*}S_{\\theta}\\widehat{V}_{\\theta}\\|=\\|\\overline{{U}}_{\\theta}^{*}S_{\\theta}\\overline{{V}}_{\\theta}\\pm\\widehat{U}_{\\theta}^{*}S_{\\theta}\\overline{{V}}_{\\theta}-\\widehat{U}_{\\theta}^{*}S_{\\theta}\\widehat{V}_{\\theta}\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|\\overline{{U}}_{\\theta}-\\widehat{U}_{\\theta}\\|\\|\\overline{{V}}_{\\theta}\\|+\\|\\widehat{U}_{\\theta}\\|\\|\\overline{{V}}_{\\theta}-\\widehat{V}_{\\theta}\\|}\\\\ &{\\leq\\|\\overline{{U}}_{\\theta}-\\widehat{U}_{\\theta}\\|\\|\\overline{{V}}_{\\theta}\\|+\\|\\overline{{V}}_{\\theta}-\\widehat{V}_{\\theta}\\|\\|\\overline{{U}}_{\\theta}\\|+\\|\\overline{{U}}_{\\theta}-\\widehat{U}_{\\theta}\\|\\|\\overline{{V}}_{\\theta}-\\widehat{V}_{\\theta}\\|}\\\\ &{\\leq\\sqrt{1+\\mathcal{E}_{\\theta}}\\big(\\|\\widehat{\\mathbf{E}}_{x}[u^{\\theta}]-\\mathbb{E}[u^{\\theta}(X)]\\|+\\|\\widehat{\\mathbf{E}}_{y}[v^{\\theta}]-\\mathbb{E}[v^{\\theta}(Y)]\\|\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\|\\widehat{\\mathbf{E}}_{x}[u^{\\theta}]-\\mathbb{E}[u^{\\theta}(X)]\\|\\,\\|\\widehat{\\mathbf{E}}_{y}[v^{\\theta}]-\\mathbb{E}[v^{\\theta}(Y)]\\|}\\\\ &{\\leq2\\sqrt{1+\\mathcal{E}_{\\theta}}\\varepsilon_{n}(\\delta)+[\\varepsilon_{n}(\\delta)]^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\begin{array}{r}{\\|\\widehat{\\mathsf{E}}_{x}[u^{\\theta}]-\\mathbb{E}[u^{\\theta}(X)]\\|\\leq\\varepsilon_{n}(\\delta)}\\end{array}$ and $\\begin{array}{r}{\\|\\widehat{\\mathsf{E}}_{y}[v^{\\theta}]-\\mathbb{E}[v^{\\theta}(Y)]\\|\\le\\varepsilon_{n}(\\delta)}\\end{array}$ hold w.p.a.l. $1-\\delta$ in view of Lemma 2. ", "page_idx": 17}, {"type": "text", "text": "To summarize, it holds w.p.a.l. $1-\\delta$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathsf{D}_{Y|X}-\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}\\|\\le\\sigma_{d+1}^{\\star}+\\mathcal{E}_{\\theta}+2\\sqrt{1+\\mathcal{E}_{\\theta}}(\\mathcal{E}_{\\theta}+\\varepsilon_{n}(\\delta))+[\\varepsilon_{n}(\\delta)]^{2}=:\\psi_{n}(\\delta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By definition in (4) and (13), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}[Y\\in B\\,|\\,X\\in A]-\\widehat{p}_{\\theta}(B\\,|\\,A)=\\mathbb{E}[\\mathbb{1}_{B}(Y)]-\\widehat{\\mathsf{E}}_{y}[\\mathbb{1}_{B}]+\\frac{\\langle\\mathbb{1}_{A},\\mathsf{D}_{Y\\mid X}\\mathbb{1}_{B}\\rangle}{\\mathbb{E}[\\mathbb{1}_{A}(X)]}-\\frac{\\langle\\mathbb{1}_{A},\\widehat{\\mathsf{D}}_{Y\\mid X}^{\\theta}\\mathbb{1}_{B}\\rangle}{\\widehat{\\mathsf{E}}_{x}[\\mathbb{1}_{A}]},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\langle\\mathbb{1}_{A},\\mathsf{D}_{Y|X}\\mathbb{1}_{B}\\rangle}{\\mathbb{E}[\\mathbb{1}_{A}(X)]}=\\frac{\\langle\\mathbb{1}_{A},\\big(\\mathsf{D}_{Y|X}-\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}\\big)\\mathbb{1}_{B}\\rangle}{\\mathbb{E}[\\mathbb{1}_{A}(X)]}+\\frac{\\langle\\mathbb{1}_{A},\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}\\mathbb{1}_{B}\\rangle}{\\widehat{\\mathsf{E}}_{x}[\\mathbb{1}_{A}]}\\frac{\\widehat{\\mathsf{E}}_{x}[\\mathbb{1}_{A}]}{\\mathbb{E}[\\mathbb{1}_{A}(X)]}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note also that $\\|\\mathbb{1}_{A}(X)\\|_{L_{\\mu}^{2}(X)}={\\sqrt{\\mathbb{E}[\\mathbb{1}_{A}(X)]}}={\\sqrt{\\mathbb{P}[X\\in A]}}.$ , \u22251B(Y )\u2225L2\u03bd(Y) = E[1B(Y )] = $\\sqrt{\\mathbb{P}[Y\\in B]}$ , for any $A\\in\\Sigma_{\\mathcal{X}}$ and $B\\in\\Sigma_{\\mathcal{V}}$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\big|\\langle\\mathbb{1}_{A},\\big(\\mathsf{D}_{Y|X}-\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}\\big)\\mathbb{1}_{B}\\rangle\\big|\\leq\\|\\mathsf{D}_{Y|X}-\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}\\|\\|\\mathbb{1}_{A}(X)\\|_{L_{\\mu}^{2}(\\mathcal{X})}\\|\\mathbb{1}_{B}(Y)\\|_{L_{\\nu}^{2}(\\mathcal{Y})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combining the previous observations, we get ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{P}[Y\\in B\\,|\\,X\\in A]-\\widehat{p}_{\\theta}(B\\,|\\,A)|\\leq\\left(\\frac{|\\widehat{\\mathbb{E}}_{y}[\\mathbb{1}_{B}]-\\mathbb{E}[\\mathbb{1}_{B}(Y)]|}{\\mathbb{E}[\\mathbb{1}_{B}(Y)]}+\\frac{\\|\\mathsf{D}_{Y|X}-\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}\\|}{\\sqrt{\\mathbb{E}[\\mathbf{1}_{A}(X)]\\mathbb{E}[\\mathbf{1}_{B}(Y)]}}\\right)\\mathbb{E}[\\mathbb{1}_{B}(Y)]\\,}\\\\ &{}&{+\\,\\frac{|\\langle\\mathbb{1}_{A},\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}\\mathbb{1}_{B}\\rangle|}{\\widehat{\\mathbb{E}}_{x}[\\mathbb{1}_{A}]}\\frac{|\\widehat{\\mathbb{E}}_{x}[\\mathbb{1}_{A}]-\\mathbb{E}[\\mathbb{1}_{A}(X)]|}{\\mathbb{E}[\\mathbb{1}_{A}(X)]},\\qquad\\quad(24)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\left|\\langle\\mathbb{1}_{A},\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}\\mathbb{1}_{B}\\rangle\\right|}{\\widehat{\\mathsf{E}}_{x}[\\mathbb{1}_{A}]}\\leq\\frac{\\mathbb{E}[\\mathbb{1}_{A}(X)]}{\\widehat{\\mathsf{E}}_{x}[\\mathbb{1}_{A}]}\\left(\\frac{\\left|\\langle\\mathbb{1}_{A},\\mathsf{D}_{Y|X}\\mathbb{1}_{B}\\rangle\\right|}{\\mathbb{E}[\\mathbb{1}_{A}(X)]}+\\frac{\\left|\\langle\\mathbb{1}_{A},\\left(\\mathsf{D}_{Y|X}-\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}\\right)\\mathbb{1}_{B}\\rangle\\right|}{\\mathbb{E}[\\mathbb{1}_{A}(X)]}\\right)}\\\\ &{\\phantom{\\leq\\frac{|\\langle\\mathbb{1}_{A}(X)|}{\\widehat{\\mathsf{E}}_{x}[\\mathbb{1}_{A}]}}\\leq\\frac{\\mathbb{E}[\\mathbb{1}_{A}(X)]}{\\widehat{\\mathsf{E}}_{x}[\\mathbb{1}_{A}]}\\left(\\|\\mathsf{D}_{Y|X}\\|+\\|\\mathsf{D}_{Y|X}-\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}\\|\\right)\\sqrt{\\frac{\\mathbb{E}[\\mathbb{1}_{B}(Y)]}{\\mathbb{E}[\\mathbb{1}_{A}(X)]}}}\\\\ &{\\phantom{\\leq\\frac{\\mathbb{E}[\\mathbb{1}_{A}(X)]}{\\widehat{\\mathsf{E}}_{x}[\\mathbb{1}_{A}]}}\\sqrt{\\frac{\\mathbb{E}[\\mathbb{1}_{B}(Y)]}{\\mathbb{E}[\\mathbb{1}_{A}(X)]}}\\left(1+\\|\\mathsf{D}_{Y|X}-\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}\\|\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we have used that $\\|\\mathsf{D}_{Y|X}\\|\\leq1$ . ", "page_idx": 17}, {"type": "text", "text": "Similarly, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\Gamma}{R^{5}}\\frac{6}{\\gamma}\\Bigg\\Vert\\sum_{k^{\\prime}\\in\\mathcal{A}_{j}}\\frac{\\tilde{\\mathcal{G}}_{y}(B_{j}|\\mathcal{A})}{\\sqrt{\\pi}}}\\\\ &{=\\frac{\\left(\\ensuremath{\\mathrm{L}}_{0}(\\ensuremath{\\mathrm{L}}_{0})\\right)\\gamma\\exp\\left(-\\ensuremath{\\mathrm{L}}_{0}(\\ensuremath{\\mathrm{L}}_{0})\\right)}{\\left(\\ensuremath{\\mathrm{L}}_{0}(\\ensuremath{\\mathrm{L}}_{0})\\right)\\left(\\ensuremath{\\mathrm{L}}_{0}(\\ensuremath{\\mathrm{L}}_{0})\\right)}}\\\\ &{=\\frac{\\left(\\ensuremath{\\mathrm{L}}_{0}(\\ensuremath{\\mathrm{L}}_{0})\\right)\\gamma\\exp\\left(-\\ensuremath{\\mathrm{R}}_{0}(B_{j}|\\mathcal{A})\\right)}{\\left(\\ensuremath{\\mathrm{L}}_{0}(\\ensuremath{\\mathrm{L}}_{0})\\right)\\left(\\ensuremath{\\mathrm{L}}_{0}(\\ensuremath{\\mathrm{L}}_{0})\\right)}}\\\\ &{\\ \\ \\ \\ \\ +\\left(\\ensuremath{\\mathrm{L}}_{0}(\\ensuremath{\\mathrm{R}}_{0})\\right)\\left(\\frac{1}{\\ensuremath{\\mathrm{L}}_{0}(\\ensuremath{\\mathrm{L}}_{0})\\ensuremath{\\mathrm{L}}_{0}(\\ensuremath{\\mathrm{L}}_{0})}-\\frac{1}{\\ensuremath{\\mathrm{L}}_{0}(\\ensuremath{\\mathrm{L}}_{0})\\ensuremath{\\mathrm{L}}_{0}(\\ensuremath{\\mathrm{L}}_{0})}\\right)}\\\\ &{=\\frac{\\left(\\ensuremath{\\mathrm{L}}_{0}(\\ensuremath{\\mathrm{R}}_{0})\\right)\\gamma\\exp\\left(-\\ensuremath{\\mathrm{R}}_{0}(B_{j}|\\mathcal{A})\\right)}{\\ensuremath{\\mathrm{L}}_{0}^{2}\\left(\\ensuremath{\\mathrm{L}}_{0}(\\ensuremath{\\mathrm{L}}_{0})\\right)\\left(\\ensuremath{\\mathrm{L}}_{0}(\\ensuremath{\\mathrm{L}}_{0})\\right)}}\\\\ &{\\ \\ \\ \\ \\ +\\frac{\\left(\\ensuremath{\\mathrm{L}}_{0}(\\ensuremath{\\mathrm{R}}_{0})\\right)\\gamma\\exp\\left(-\\ensuremath{\\mathrm{R}}_{0}(B_{j}|\\mathcal{A})\\right)}{\\ensuremath{\\mathrm{L}}_{0}^{3}\\left(\\ensuremath{\\mathrm{L}}_{0}(\\ensuremath{\\mathrm{L}}_{0})\\right)}}\\\\ &{=\\frac{\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next Lemmas 3 and 4 combined with (17) and elementary algebra give w.p.a.l. $1-2\\delta$ that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Big|\\frac{\\widehat{\\mathsf{E}}_{x}[\\mathbb{1}_{A}]-\\mathbb{E}[\\mathbb{1}_{A}(X)]}{\\widehat{\\mathsf{E}}_{x}[\\mathbb{1}_{A}]}\\Big|\\leq2\\varphi_{X}(A)\\overline{{\\epsilon}}_{n}(\\delta),\\quad\\Big|\\frac{\\widehat{\\mathsf{E}}_{y}[\\mathbb{1}_{B}]-\\mathbb{E}[\\mathbb{1}_{B}(Y)]}{\\widehat{\\mathsf{E}}_{x}[\\mathbb{1}_{B}]}\\Big|\\leq2\\varphi_{Y}(B)\\overline{{\\epsilon}}_{n}(\\delta),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{E}[\\mathbb{1}_{A}(X)]}{\\widehat{\\mathsf{E}}_{x}[\\mathbb{1}_{A}]}\\vee\\frac{\\mathbb{E}[\\mathbb{1}_{B}(Y)]}{\\widehat{\\mathsf{E}}_{y}[\\mathbb{1}_{B}]}\\le2,\\quad\\Big|\\frac{\\mathbb{E}[\\mathbb{1}_{A}(X)](\\widehat{\\mathsf{E}}_{y}[\\mathbb{1}_{B}]-\\mathbb{E}[\\mathbb{1}_{B}(Y)])}{\\widehat{\\mathsf{E}}_{x}[\\mathbb{1}_{A}]\\widehat{\\mathsf{E}}_{y}[\\mathbb{1}_{B}]}\\Big|\\le4\\varphi_{Y}(B)\\bar{\\epsilon}_{n}(\\delta).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It also holds on the same probability event as above that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\langle\\mathbb{1}_{A},(\\mathsf{D}_{Y|X}-\\widehat{\\mathsf{D}}_{Y|X}^{\\theta})\\mathbb{1}_{B}\\rangle}{\\mathsf{E}_{x}[\\mathbb{1}_{A}]\\hat{\\mathsf{E}}_{y}[\\mathbb{1}_{B}]}\\bigg|\\!\\leq\\!\\frac{\\|\\mathsf{D}_{Y|X}-\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}\\|}{\\sqrt{\\mathbb{E}[\\mathbb{1}_{A}(X)]\\mathbb{E}[\\mathbb{1}_{B}(Y)]}}\\frac{\\mathbb{E}[\\mathbb{1}_{A}(X)]}{\\widehat{\\mathsf{E}}_{x}[\\mathbb{1}_{A}]}\\frac{\\mathbb{E}[\\mathbb{1}_{B}(Y)]}{\\widehat{\\mathsf{E}}_{y}[\\mathbb{1}_{B}]}\\!\\leq\\!4\\frac{\\|\\mathsf{D}_{Y|X}-\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}\\|}{\\sqrt{\\mathbb{E}[\\mathbb{1}_{A}(X)]\\mathbb{E}[\\mathbb{1}_{B}(Y)]}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining Lemma 2 and (23), we get with probability at least $1-\\delta$ that $\\|\\mathsf{D}_{Y|X}-\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}\\|\\leq\\psi_{n}(\\delta)$ ", "page_idx": 18}, {"type": "text", "text": "By a union bound combining the last two displays with (23), (26), (24) and (25), we get with probability at least $1-3\\delta$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Big|\\frac{\\mathbb{P}[Y\\in B\\,|\\,X\\in A]}{\\mathbb{P}[Y\\in B]}-\\frac{\\widehat{p}_{\\theta}(B\\,|\\,A)}{\\widehat{p}_{y}(B)}\\Big|\\leq\\frac{4\\psi_{n}(\\delta)+\\left[1+\\psi_{n}(\\delta)\\right]\\left[2\\varphi_{X}(A)+4\\varphi_{Y}(B)\\right]\\overline{{\\epsilon}}_{n}(\\delta)}{\\sqrt{\\mathbb{E}[\\mathbb{I}_{A}(X)]\\mathbb{E}[\\mathbb{I}_{B}(Y)]}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left|\\frac{\\mathbb{P}[Y\\in B\\,|\\,X\\in A]-\\widehat{p}_{\\theta}(B\\,|\\,A)}{\\mathbb{P}[Y\\in B]}\\right|\\leq\\varphi_{Y}(B)\\overline{{\\epsilon}}_{n}(\\delta)+\\frac{2(1+\\psi_{n}(\\delta))\\varphi_{X}(A)\\overline{{\\epsilon}}_{n}(\\delta)+\\psi_{n}(\\delta)}{\\sqrt{\\mathbb{E}[\\mathbb{1}_{A}(X)]\\mathbb{E}[\\mathbb{1}_{B}(Y)]}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Replacing $\\delta$ by $\\delta/3$ , we get the result w.p.a.l. $1-\\delta$ . ", "page_idx": 18}, {"type": "text", "text": "The following result will be useful to investigate the theoretical properties of the NCP method in the iid setting. ", "page_idx": 18}, {"type": "text", "text": "Lemma 2. Let Assumption $^{\\,l}$ be satisfied. Then there exists an absolute constant $C>0$ such that, for any $\\delta\\in(0,1)$ , it holds w.p.a.l. $1-\\delta$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\widehat{\\mathsf{E}}_{x}[u^{\\theta}]-\\mathbb{E}[u^{\\theta}(X)]\\|\\le C\\left(c_{u}\\frac{d\\log(e\\delta^{-1})}{n}+\\sigma_{\\theta}(X)\\sqrt{\\frac{\\log(e\\delta^{-1})}{n}}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similarly, w.p.a.l. $1-\\delta$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\widehat{\\mathsf{E}}_{y}[v^{\\theta}]-\\mathbb{E}[v^{\\theta}(Y)]\\|\\le C\\left(c_{v}\\frac{d\\log(e\\delta^{-1})}{n}+\\sigma_{\\theta}(Y)\\sqrt{\\frac{\\log(e\\delta^{-1})}{n}}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma 2. We note that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widehat{\\mathsf{E}}_{x}[u^{\\theta}]-\\mathbb{E}[u^{\\theta}(X)]=\\frac{1}{n}\\sum_{i=1}^{n}Z_{i}\\quad\\mathrm{with}\\quad Z_{i}=u^{\\theta}(X_{i})-\\mathbb{E}u^{\\theta}(X_{i}),\\;\\forall i\\in[n].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We note that $\\|Z_{i}\\|\\leq2c_{u}\\,d=:U$ and $\\mathrm{Var}(Z_{i})=\\mathrm{Var}(\\|u^{\\theta}(X_{i})-\\mathbb{E}[u^{\\theta}(X_{i})]\\|)=\\sigma_{\\theta}^{2}(X)$ for any $i\\in[n]$ . We apply Minsker (2017, Corollary 4.1) to get for any $\\begin{array}{r}{t\\geq\\frac{1}{6}(U+\\sqrt{U^{2}+36n\\sigma_{\\theta}^{2}(X)})}\\end{array}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\|\\sum_{i=1}^{n}Z_{i}\\|>t\\right]\\leq28\\exp\\left(-\\frac{t^{2}/2}{n\\sigma_{\\theta}^{2}(X)+t U/3}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Replacing $t$ by $n t$ and some elementary algebra give for any $\\begin{array}{r}{t\\geq\\frac{1}{6}\\left(\\frac{U}{n}+\\sqrt{\\frac{U^{2}}{n^{2}}+36\\frac{\\sigma_{\\theta}^{2}(X)}{n}}\\right)=:\\overline{{c}},}\\end{array}$ w.p.a.l. $1-28\\exp\\left(-t\\right)$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\frac{1}{n}\\sum_{i=1}^{n}Z_{i}\\|\\leq\\frac{4U}{3}\\frac{t}{n}+2\\sigma_{\\theta}(X)\\sqrt{\\frac{t}{n}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Replacing $t$ by $t+\\overline{{c}}$ , we get for any $t\\geq0$ , w.p.a.l. $1-28\\exp{(-t+\\overline{{c}})}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\frac{1}{n}\\sum_{i=1}^{n}Z_{i}\\|\\leq\\frac{4U}{3}\\frac{t+\\overline{{c}}}{n}+2\\sigma_{\\theta}(X)\\sqrt{\\frac{t+\\overline{{c}}}{n}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Up to a rescaling of the constants, there exists a numerical constant $C>0$ such that for any $\\delta\\in(0,1)$ , w.p.a.l. $1-\\delta$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\frac{1}{n}\\sum_{i=1}^{n}Z_{i}\\|\\leq C\\left(\\frac{U}{n}\\overline{{c}}+\\sigma_{\\theta}(X)\\sqrt{\\frac{\\overline{{c}}}{n}}+U\\frac{t}{n}+\\sigma_{\\theta}(X)\\sqrt{\\frac{t}{n}}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Elementary computations give the following bound, that is, there exists a numerical constant $C>0$ such that for any $t>0$ , w.p.a.l. $1-\\exp(-t)$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\frac{1}{n}\\sum_{i=1}^{n}Z_{i}\\|\\leq C\\left(\\frac{c_{u}\\,d}{n}\\vee\\frac{c_{u}^{2}\\,d^{2}}{n^{2}}\\vee\\frac{\\sigma_{\\theta}^{3/2}(X)}{n^{3/4}}\\vee\\frac{\\sigma_{\\theta}^{2}(X)}{n}+c_{u}\\frac{d t}{n}+\\sigma_{\\theta}(X)\\sqrt{\\frac{t}{n}}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Under Assumption 1 and the condition $\\begin{array}{r}{\\frac{c_{u}^{2}\\,d}{n}\\leq1}\\end{array}$ , it also holds that ${\\frac{\\sigma_{\\theta}^{2}(X)}{n}}\\leq1\\,$ since $\\sigma_{\\theta}^{2}(X)\\leq c_{u}^{2}d$ Consequently, the bound simplifies and we obtain for any $t>1$ , w.p.a.l. $1-\\exp(-t)$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\frac{1}{n}\\sum_{i=1}^{n}Z_{i}\\|\\leq C\\left(c_{u}\\frac{d t}{n}\\vee\\sigma_{\\theta}(X)\\sqrt{\\frac{t}{n}}\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $C>0$ is possibly a different absolute constant from the previous bound. Taking $t=\\log e\\delta^{-1}$ for any $\\delta\\in(0,1)$ gives the first result. We proceed similarly to get the second result. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Control on empirical probabilities We derive now a concentration result for empirical probabilities. ", "page_idx": 20}, {"type": "text", "text": "Lemma 3. For any $A\\in\\Sigma_{\\mathcal{X}}$ and any $\\delta\\in(0,1)$ , it holds w.p.a.l. $1-\\delta$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\widehat{\\mathsf{E}}_{x}[\\mathbb{1}_{A}]-\\mathbb{E}[\\mathbb{1}_{A}(X)]|\\leq2\\frac{\\log2\\delta^{-1}}{n}+\\sqrt{\\mathbb{P}[X\\in A](1-\\mathbb{P}[X\\in A])}\\sqrt{2\\frac{\\log2\\delta^{-1}}{n}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Assume in addition that $\\mathbb{P}(X\\in A)\\geq2{\\sqrt{2{\\frac{\\log2\\delta^{-1}}{n}}}}$ . Then it holds w.p.a.l. $1-\\delta$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{|\\widehat{\\mathsf{E}}_{x}[\\mathbb{1}_{A}]-\\mathbb{E}[\\mathbb{1}_{A}(X)]|}{\\mathbb{E}[\\mathbb{1}_{A}(X)]}\\le\\sqrt{2\\frac{\\log2\\delta^{-1}}{n}}\\sqrt{1\\vee\\frac{1-\\mathbb{P}[X\\in A]}{\\mathbb{P}[X\\in A]}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. We note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\widehat{\\mathsf{E}}_{x}[\\mathbb{1}_{A}(X)]-\\mathbb{E}[\\mathbb{1}_{A}(X)]=\\frac{1}{n}\\sum_{i=1}^{n}Z_{i}\\quad\\mathrm{with}\\quad Z_{i}=\\mathbb{1}_{A}(X_{i})-\\mathbb{E}[\\mathbb{1}_{A}(X_{i})],\\;\\forall i\\in[n].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We note that $|Z_{i}|\\le2$ and $\\operatorname{Var}(Z_{i})=\\mathbb{P}[X\\in A](1-\\mathbb{P}[X\\in A])$ . Then Bercu et al. (2015, Theorem 2.9) gives w.p.a.l. $1-2\\delta$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\widehat{\\mathsf{E}}_{x}[\\mathbb{1}_{A}]-\\mathbb{E}[\\mathbb{1}_{A}(X)]|\\leq2\\frac{\\log\\delta^{-1}}{n}+\\sqrt{\\mathbb{P}[X\\in A](1-\\mathbb{P}[X\\in A])}\\sqrt{2\\frac{\\log\\delta^{-1}}{n}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Dividing by $\\mathbb{E}[\\mathbb{1}_{A}(X)]$ gives w.p.a.l. $1-2\\delta$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{|\\widehat{\\mathsf{E}}_{x}[\\mathbb{1}_{A}]-\\mathbb{E}[\\mathbb{1}_{A}(X)]|}{\\mathbb{E}[\\mathbb{1}_{A}(X)]}\\leq2\\sqrt{2\\frac{\\log\\delta^{-1}}{n}}\\sqrt{\\frac{[2\\log(\\delta^{-1})/n]\\vee(1-\\mathbb{P}[X\\in A])}{\\mathbb{P}[X\\in A]}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Replacing $\\delta$ by $\\delta/2$ gives the result for $X$ . The result for $Y$ follows from a similar reasoning. ", "page_idx": 20}, {"type": "text", "text": "The same proof argument gives an identical result for $Y$ ", "page_idx": 20}, {"type": "text", "text": "Lemma 4. For any $B\\in\\Sigma{y}$ and any $\\delta\\in(0,1)$ , it holds w.p.a.l. $1-\\delta$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\widehat{\\mathsf{E}}_{y}[\\mathbb{1}_{B}]-\\mathbb{E}[\\mathbb{1}_{B}(Y)]|\\leq2\\frac{\\log2\\delta^{-1}}{n}+\\sqrt{\\mathbb{P}[Y\\in B](1-\\mathbb{P}[Y\\in B])}\\sqrt{2\\frac{\\log2\\delta^{-1}}{n}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Assume in addition that $\\mathbb{P}(Y\\in B)\\ge2\\sqrt{2\\frac{\\log2\\delta^{-1}}{n}}$ . Then it holds w.p.a.l. $1-\\delta$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{|\\widehat{\\mathsf{E}}_{y}[\\mathbb{1}_{B}]-\\mathbb{E}[\\mathbb{1}_{B}(Y)]|}{\\mathbb{E}[\\mathbb{1}_{B}(Y)]}\\le\\sqrt{2\\frac{\\log2\\delta^{-1}}{n}}\\sqrt{1\\vee\\frac{1-\\mathbb{P}[Y\\in B]}{\\mathbb{P}[Y\\in B]}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "B.5 Sub-Gaussian case ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Sub-Gaussian setting. We derive another concentration result under a less restricted sub-Gaussian condition on functions $u^{\\theta}$ and $v^{\\theta}$ . This result relies on Pinelis and Sakhanenko\u2019s inequality for random variables in a separable Hilbert space, see (Caponnetto and De Vito, 2007, Proposition 2). ", "page_idx": 20}, {"type": "text", "text": "Let $\\psi_{2}(x)=e^{x^{2}}-1,x\\geq0$ . We define the $\\psi_{2}$ -Orlicz norm of a random variable $\\eta$ as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\eta\\|_{\\psi_{2}}:=\\operatorname*{inf}\\left\\{C>0\\,:\\,\\mathbb{E}\\left[\\psi_{2}\\left({\\frac{|\\eta|}{C}}\\right)\\right]\\leq1\\right\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We recall the definition of a sub-Gaussian random vector. ", "page_idx": 20}, {"type": "text", "text": "Definition 4 (Sub-Gaussian random vector). $A$ centered random vector $X\\,\\in\\,\\mathbb{R}^{\\underline{{d}}}$ will be called sub-Gaussian iff, for all $u\\in\\mathbb{R}^{\\underline{{d}}}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\langle X,u\\rangle\\|_{\\psi_{2}}\\lesssim\\|\\langle X,u\\rangle\\|_{L_{2}(\\mathbb{P})}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proposition 1. Caponnetto and De Vito (2007, Proposition 2) Let $A_{i}$ , $i\\in[n]$ be i.i.d copies of $a$ random variable $A$ in a separable Hilbert space with norm $\\left\\Vert\\cdot\\right\\Vert$ . If there exist constants $L>0$ and $\\sigma>0$ such that for every $m\\geq2$ , $\\begin{array}{r}{\\mathbb{E}\\|A\\|^{m}\\leq\\frac{1}{2}m!L^{m-2}\\sigma^{2}}\\end{array}$ , then with probability at least $1-\\delta$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|{\\frac{1}{n}}\\sum_{i\\in[n]}A_{i}-\\mathbb{E}A\\right\\|\\leq{\\frac{4{\\sqrt{2}}}{\\sqrt{n}}}{\\sqrt{\\sigma^{2}+{\\frac{L^{2}}{n}}}}\\log{\\frac{2}{\\delta}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma 5 ((Sub-Gaussian random variable) Lemma 5.5. in Vershynin (2011)). Let $Z$ be a random variable. Then, the following assertions are equivalent with parameters $K_{i}>0$ differing from each other by at most an absolute constant factor. ", "page_idx": 21}, {"type": "text", "text": "1. Tails: $P\\{|Z|>t\\}\\le\\exp(1-t^{2}/K_{1}^{2})$ for all $t\\geq0,$ ;   \n2. Moments: $(\\mathbb{E}|Z|^{p})^{1/p}\\le K_{2}\\sqrt{p}$ for all $p\\geq1$ ;   \n3. Super-exponential moment: E $\\exp(Z^{2}/K_{3}^{2})\\le2.$ ", "page_idx": 21}, {"type": "text", "text": "A random variable $Z$ satisfying any of the above assertions is called a sub-Gaussian random variable.   \nWe will denote by $K_{3}$ the sub-Gaussian norm. ", "page_idx": 21}, {"type": "text", "text": "Consequently, a sub-Gaussian random variable satisfies the following equivalence of moments property. There exists an absolute constant $c>0$ such that for any $m\\geq2$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bigl(\\mathbb{E}|Z|^{m}\\bigr)^{1/m}\\leq c K_{3}\\sqrt{m}\\bigl(\\mathbb{E}|Z|^{2}\\bigr)^{1/2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma 6. Assume that $\\|u^{\\theta}(X)-\\mathbb{E}[u^{\\theta}(X)]\\|$ and $\\|v^{\\theta}(Y)-\\mathbb{E}[v^{\\theta}(Y)]\\|$ are sub-Gaussian with sub-Gaussian norm $K$ . We set $\\sigma_{\\theta}^{2}(X):=\\operatorname{Var}(\\|u^{\\theta}(X)-\\mathbb{E}[u^{\\theta}(X)]\\|)$ , ${\\widehat{\\sigma_{\\theta}^{2}(Y)}}:=\\operatorname{Var}(\\|v^{\\theta}(Y)\\,-$ $\\mathbb{E}[v^{\\theta}(Y)]\\Vert)$ . Then there exists an absolute constant $C>0$ such that for any $\\delta\\,\\in\\,(0,1)$ , it holds $w.p.a.l.\\mathrm{~}1-\\delta$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Vert\\widehat{\\mathsf{E}}_{x}[u^{\\theta}]-\\mathbb{E}[u^{\\theta}(X)]\\Vert\\leq\\frac{C}{\\sqrt{n}}\\sqrt{\\sigma_{\\theta}^{2}(X)+\\frac{K^{2}}{n}}\\log(2\\delta^{-1}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Similarly, w.p.a.l. $1-\\delta$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Vert\\widehat{\\mathsf{E}}_{y}[v^{\\theta}]-\\mathbb{E}[v^{\\theta}(Y)]\\Vert\\leq\\frac{C}{\\sqrt{n}}\\sqrt{\\sigma_{\\theta}^{2}(Y)+\\frac{K^{2}}{n}}\\log(2\\delta^{-1})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Set $Z:=\\|u^{\\theta}(X)-\\mathbb{E}u^{\\theta}(X)\\|$ and we recall that $\\sigma_{\\theta}^{2}(X):=\\operatorname{Var}(\\|u^{\\theta}(X)-\\mathbb{E}[u^{\\theta}(X)]\\|)$ . We check that the moment condition, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}Z^{m}\\leq\\frac{1}{2}m!L^{m-2}\\sigma_{\\theta}^{2}(X)^{2},\\quad\\forall m\\geq2,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for some constant $L>0$ to be specified. ", "page_idx": 21}, {"type": "text", "text": "The condition is obviously satisfied for $m=2$ . Next for any $m\\geq3$ , the Cauchy-Schwarz inequality and the equivalence of moment property give ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}Z^{m}\\leq\\left(\\mathbb{E}Z^{2(m-2)}\\right)^{1/2}\\left(\\mathbb{E}Z^{4}\\right)^{1/2}\\leq4K_{3}^{2}\\sigma_{\\theta}^{2}(X)^{2}\\left(\\mathbb{E}Z^{2(m-2)}\\right)^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next, by homogeneity, rescaling $Z$ to $Z/K_{1}$ we can assume that $K_{1}=1$ in Lemma 5. We recall that if $Z$ is in addition non-negative random variable, then for every integer $p\\geq1$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}Z^{p}=\\int_{0}^{\\infty}\\mathbb{P}\\{Z\\ge t\\}\\,p t^{p-1}\\,d t\\le\\int_{0}^{\\infty}e^{1-t^{2}}p t^{p-1}\\,d t=\\big(\\frac{e p}{2}\\big)\\Gamma\\big(\\frac{p}{2}\\big).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "With $p=2(m-2)$ , we get that $\\mathbb{E}Z^{p}\\leq e(m-2)\\Gamma\\big(m-2\\big)=e(m-2)!=e m!/2$ . Using again Lemma 5, we can take $L=c K$ for some large enough absolute constant $c>0$ . Then Proposition 1 gives the result. ", "page_idx": 21}, {"type": "text", "text": "B.6 Estimation of conditional expectation and Conditional covariance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We now derive guarantees for the estimation of the conditional expectation and the conditional covariance for vector-valued output $Y\\in\\mathbb{R}^{d_{y}}$ . ", "page_idx": 22}, {"type": "text", "text": "We start with a general result for arbitrary vector-valued functions of $Y$ . We consider a vector-valued function $\\underline{{h}}=(\\bar{h}_{1},\\dots,h_{\\underline{{d}}})$ where $h_{j}\\in L_{\\nu}^{2}(\\mathcal{V})$ for any $j\\,\\in\\,[\\underline{d}]$ . We introduce the space of square integrable vector-valued functions $[L_{\\nu}^{2}(\\mathcal{V},\\mathbb{R}^{d})]$ equipped with the norm ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\underline{{h}}\\|=\\sqrt{\\sum_{j\\in[d]}\\|h_{j}\\|_{L_{\\nu}^{2}(\\mathcal{V})}^{2}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Next we can define the conditional expectation of $\\underline{{h}}(Y)=\\big(h_{1}(Y^{(1)}),\\dots h_{\\underline{{d}}}(Y^{(d_{y})})\\big)^{\\top}$ conditionally on $X\\in A$ as follows ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\underline{{h}}(Y)\\,|\\,X\\in A]=\\bigg(\\mathbb{E}[h_{1}(Y)]+\\frac{\\langle\\mathbb{1}_{A},\\mathsf{D}_{Y|X}h_{1}\\rangle}{\\mathbb{P}(X\\in A)},\\dots,\\mathbb{E}[h_{\\underline{{d}}}(Y)]+\\frac{\\langle\\mathbb{1}_{A},\\mathsf{D}_{Y|X}h_{\\underline{{d}}}\\rangle}{\\mathbb{P}(X\\in A)}\\bigg)^{\\top}}\\\\ &{\\qquad\\qquad=\\mathbb{E}[\\underline{{h}}(Y)]+\\frac{\\langle\\mathbb{1}_{A},\\big[\\mathbb{1}_{\\underline{{d}}}\\otimes\\mathsf{D}_{Y|X}\\big]\\underline{{h}}\\rangle}{\\mathbb{P}(X\\in A)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We define similarly its empirical version as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathsf{E}}^{\\theta}[\\![\\!h(Y)\\!]\\,X\\in A]=\\left(\\widehat{\\mathsf{E}}_{y}[h_{1}]+\\frac{\\langle\\!\\ 1_{A},\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}h_{1}\\rangle}{\\widehat{\\mathsf{E}}_{x}[\\![\\!1_{A}]\\!]},\\dots,\\widehat{\\mathsf{E}}_{y}[h_{\\underline{{d}}}]+\\frac{\\langle\\!\\ 1_{A},\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}h_{\\underline{{d}}}\\!\\rangle}{\\widehat{\\mathsf{E}}_{x}[\\![\\!1_{A}]\\!]}\\right)^{\\top}}\\\\ &{\\qquad\\qquad=\\widehat{\\mathsf{E}}_{y}[\\!]\\!+\\frac{\\langle\\!\\mathbb{1}_{A},[\\![\\!1_{\\underline{{d}}}\\otimes\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}]\\!]\\!,\\!\\bar{h}\\rangle}{\\widehat{\\mathsf{E}}_{x}[\\![\\!1_{A}]\\!]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Assuming that $\\underline{{h}}(Y)$ is sub-Gaussian, we set ", "page_idx": 22}, {"type": "equation", "text": "$$\nK:=\\|\\|_{\\underline{{h}}}(Y)-\\mathbb{E}[\\underline{{h}}(Y)]\\|\\|_{\\psi_{2}},\\quad\\sigma^{2}(\\underline{{h}}(Y)):=\\mathrm{Var}(\\|\\underline{{h}}(Y)-\\mathbb{E}[\\underline{{h}}(Y)]\\|).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Define ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underline{{\\psi}}_{n}(\\delta):=\\displaystyle\\frac{1}{\\sqrt{n}}\\sqrt{\\sigma^{2}(\\underline{{h}}(Y))+\\frac{K^{2}}{n}}\\log(3\\delta^{-1})}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\frac{\\|\\underline{{h}}\\|}{\\sqrt{\\mathbb{P}(X\\in A)}}\\bigg(\\psi_{n}(\\delta/3)+2(1+\\psi_{n}(\\delta/3))\\varphi_{X}(A)\\overline{{\\epsilon}}_{n}(\\delta/3)\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Theorem 4. Let the assumptions of Theorem 2 be satisfied. Assume in addition that ${\\underline{{h}}}(Y)$ is sub-Gaussian. Then we have w.p.a.l. $1-\\delta$ that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\widehat{\\mathsf{E}}^{\\theta}[\\underline{{h}}(Y)\\,|\\,X\\in A]-\\mathbb{E}[\\underline{{h}}(Y)\\,|\\,X\\in A]\\|\\lesssim_{\\underline{{\\psi}}_{n}}(\\delta).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\underline{{h}}(Y)\\,|\\,X\\in A]-\\widehat{\\mathbb{E}}^{\\theta}[\\underline{{h}}(Y)|X\\in A]\\|}\\\\ &{\\qquad\\quad\\leq\\|\\mathbb{E}[\\underline{{h}}(Y)]-\\widehat{\\mathbb{E}}_{y}[\\underline{{h}}]\\|+\\frac{\\|\\mathsf{D}_{Y|X}-\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}\\|}{\\sqrt{\\mathbb{P}(X\\in A)}}\\|\\underline{{h}}\\|+|\\langle\\mathfrak{I}_{A},[\\underline{{\\mathbb{1}}}_{d}\\otimes\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}]\\underline{{h}}\\rangle|\\bigg|\\frac{1}{\\widehat{\\mathsf{E}}_{x}[\\underline{{\\mathbb{1}}}_{A}]}-\\frac{1}{\\mathbb{P}(X\\in A)}\\bigg|}\\\\ &{\\qquad\\quad\\leq\\|\\mathbb{E}[\\underline{{h}}(Y)]-\\widehat{\\mathsf{E}}_{y}[\\underline{{h}}]\\|+\\frac{\\|\\mathsf{D}_{Y|X}-\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}\\|}{\\sqrt{\\mathbb{P}(X\\in A)}}\\|\\underline{{h}}\\|}\\\\ &{\\qquad\\qquad+\\,\\sqrt{\\mathbb{P}(X\\in A)}(\\|\\mathsf{D}_{Y|X}\\|+\\|\\mathsf{D}_{Y|X}-\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}\\|)\\|\\underline{{h}}\\|\\bigg|\\frac{\\mathbb{P}(X\\in A)-\\widehat{\\mathsf{E}}_{x}[\\underline{{\\mathbb{1}}}_{A}]}{\\widehat{\\mathsf{E}}_{x}[\\underline{{\\mathbb{1}}}_{A}]\\mathbb{P}(X\\in A)}\\bigg|.\\qquad\\mathrm{(32)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Recall that $\\|\\mathsf{D}_{Y|X}\\|\\leq1$ , (23) and Lemma 3. Hence, a union bound we get with w.p.a.l. $1-2\\delta$ that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbb{E}[{h}(Y)\\,\\vert\\,X\\!\\in\\!A]\\!-\\!\\widehat{\\mathbb{E}}^{\\theta}[{h}(Y)\\vert X\\!\\in\\!A]\\|}\\\\ &{\\qquad\\qquad\\le\\|\\mathbb{E}[{h}(Y)]\\!-\\!\\widehat{\\mathbb{E}}_{y}[{h}]\\|\\!+\\!\\frac{\\|{h}\\|}{\\sqrt{\\mathbb{P}(X\\!\\in\\!A)}}\\biggl(\\psi_{n}(\\delta)\\!+\\!2(1\\!+\\!\\psi_{n}(\\delta))\\varphi_{X}(A)\\overline{{\\epsilon}}_{n}(\\delta)\\biggr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We now handle the first term $\\|\\mathbb{E}[{\\underline{{h}}}(Y)]-\\widehat{\\mathsf{E}}_{y}[{\\underline{{h}}}]\\|$ . We recall that a similar quantity was already studied in Lemma 6. We can just replace $u^{\\theta}(X)$ by $\\underline{{h}}(Y)\\in\\mathbb{R}^{d}$ to get the result since we assumed that $\\underline{{h}}(Y)$ is sub-Gaussian. Hence there exists an absolute constant $C>0$ such that w.p.a.l. $1-\\delta$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n||\\mathbb{E}[\\underline{{h}}(Y)]-\\widehat{\\mathbb{E}}_{y}[\\underline{{h}}]||\\leq\\frac{C}{\\sqrt{n}}\\sqrt{\\sigma^{2}(\\underline{{h}}(Y))+\\frac{K^{2}}{n}}\\log(2\\delta^{-1}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Actually, we can handle the conditional expectation $\\mathbb{E}[Y\\,|\\,X\\in A]$ in a more direct way. Set ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\underline{{\\epsilon}}_{n}(\\delta):=\\sqrt{\\frac{\\log(\\delta^{-1}d_{y})}{n}}\\,\\bigvee\\frac{\\log(\\delta^{-1}d_{y})}{n}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Corollary 2. Let the Assumptions of Theorem 2 be satisfied. Assume in addition that $Y$ is $a$ sub-Gaussian vector. Then for any $\\delta\\in(0,1)$ , it holds with probability at least $1-\\delta$ that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbb{E}[Y\\,|\\,X\\in A]-\\widehat{\\mathbf{E}}^{\\theta}[Y|X\\in A]\\|\\lesssim\\sqrt{\\operatorname{tr}(\\operatorname{Cov}(Y))}\\underline{{\\epsilon}}_{n}(\\delta/3)}\\\\ &{\\qquad\\qquad\\qquad+\\,\\frac{\\|h\\|}{\\sqrt{\\mathbb{P}(X\\in A)}}\\bigg(\\psi_{n}(\\delta/3)+2(1+\\psi_{n}(\\delta/3))\\varphi_{X}(A)\\overline{{\\epsilon}}_{n}(\\delta/3)\\bigg)=:\\psi_{n}^{(1)}(\\delta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. The proof of this result is identical to that of Theorem 4 up to (33). Now if we specify $\\underline{{h}}(Y)=Y\\in\\mathbb{R}^{d_{y}}$ . Then, applying Bernstein\u2019s inequality on each of the $d_{y}$ components of $\\mathbb{E}[Y]-\\overline{{Y}}_{n}^{\\,\\cdot}$ and a union bound, we get w.p.a.l. $1-\\delta$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\mathbb{E}[Y]-\\overline{{Y}}_{n}\\|\\lesssim\\sqrt{\\mathrm{tr}(\\mathrm{Cov}(Y))}\\sqrt{\\frac{\\log(\\delta^{-1}d_{y})}{n}}+\\operatorname*{max}_{j\\in[d_{y}]}\\|Y^{(j)}\\|_{\\psi_{2}}\\frac{\\log(\\delta^{-1}d_{y})}{n}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Using again Definition 4, we obtain $\\begin{array}{r}{\\operatorname*{max}_{j\\in[d_{y}]}\\|Y^{(j)}\\|_{\\psi_{2}}\\lesssim\\sqrt{\\|\\mathrm{Cov}(Y)\\|}\\leq\\sqrt{\\mathrm{tr}(\\mathrm{Cov}(Y))}.}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "It follows from the last two displays, w.p.a.l. $1-\\delta$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\lVert\\mathbb{E}[Y]-\\overline{{Y}}_{n}\\rVert\\lesssim\\sqrt{\\mathrm{tr}(\\mathrm{Cov}(Y))}\\underline{{\\epsilon}}_{n}(\\delta).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "A union bound combining the previous display with (33) gives the first result. ", "page_idx": 23}, {"type": "text", "text": "We focus now on the conditional covariance estimation problem. We first define the conditional covariance as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{Cov}(Y|X\\in A)=\\mathrm{Cov}(Y)+\\langle\\mathbb{1}_{A},[(\\mathbb{1}_{d_{y}}\\otimes\\mathbb{1}_{d_{y}})\\otimes\\mathsf{D}_{Y|X}]\\underline{{h}}\\otimes\\underline{{h}}\\rangle/\\mathbb{P}[X\\in A]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that $\\langle\\mathbb{1}_{A},\\left[\\left(\\mathbb{1}_{d_{y}}\\otimes\\mathbb{1}_{d_{y}}\\right)\\otimes\\mathsf{D}_{Y|X}\\right]\\!\\underline{{h}}\\otimes\\underline{{h}}\\right\\rangle=\\left(\\langle\\mathbb{1}_{A},\\mathsf{D}_{Y|X}h_{j}h_{k}\\rangle\\right)_{j,k\\in[d_{y}]}$ is a $d_{y}\\times d_{y}$ matrix. We obtain a similar decomposition for the estimator ${\\widehat{\\operatorname{Cov}}}^{\\theta}(Y|X\\in A)$ of the conditional covariance $\\operatorname{Cov}(Y|X\\in A)$ by replacing $\\mathsf{D}_{Y|X}$ by $\\widehat{\\mathsf{D}}_{Y\\mid X}^{\\theta}$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathrm{Cov}}^{\\theta}(Y|X\\in A):=\\widehat{\\mathrm{Cov}}(Y)+\\langle\\mathbb{1}_{A},\\left[(\\mathbb{1}_{d_{y}}\\otimes\\mathbb{1}_{d_{y}})\\otimes\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}\\right]\\!\\!\\underline{{h}}\\otimes\\underline{{h}}\\rangle/\\widehat{\\mathsf{E}}_{x}[\\mathbb{1}_{A}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad-\\left\\langle\\mathbb{1}_{A},\\left[\\mathbb{1}_{d_{y}}\\otimes\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}\\right]\\!\\!\\underline{{h}}\\right\\rangle\\otimes\\left\\langle\\mathbb{1}_{A},\\left[\\mathbb{1}_{d_{y}}\\otimes\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}\\right]\\!\\!\\underline{{h}}\\right\\rangle/(\\widehat{\\mathsf{E}}_{x}[\\mathbb{1}_{A}])^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We define the effective of covariance matrix $\\operatorname{Cov}(Y)$ as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{r}(\\mathrm{Cov}(Y)):=\\frac{\\mathrm{tr}(\\mathrm{Cov}(Y))}{\\|\\mathrm{Cov}(Y)\\|}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We set for any $\\delta\\in(0,1)$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\epsilon_{n}^{(2)}(\\delta):=\\|\\mathrm{Cov}(Y)\\|\\left(\\sqrt{\\frac{\\mathbf{r}(\\mathrm{Cov}(Y))}{n}}+\\frac{\\mathbf{r}(\\mathrm{Cov}(Y))}{n}+\\sqrt{\\frac{\\log(\\delta^{-1})}{n}}+\\frac{\\log(\\delta^{-1})}{n}\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi_{n}^{(2)}(\\delta)=\\epsilon_{n}^{(2)}(\\delta)+\\left[\\psi_{n}(\\delta/4)+2(1+\\psi_{n}(\\delta/4))\\varphi_{X}(A)\\overline{{\\epsilon_{n}}}(\\delta/4)\\right]\\frac{(\\mathbb{E}[\\|Y\\|^{2}])^{2}}{\\sqrt{\\mathbb{P}[X\\in A]}}}\\\\ &{\\quad\\quad\\quad\\quad\\ +\\ \\psi_{n}^{(1)}(\\delta/4)\\big[2\\|\\mathbb{E}[Y\\,|\\,X\\in A]\\|+\\psi_{n}^{(1)}(\\delta/4)\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Corollary 3. Let the assumptions of Corollary 2 be satisfied. Then for any $\\delta\\in(0,1)$ , it holds with probability at least $1-\\delta$ that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|\\widehat{\\mathrm{Cov}}^{\\theta}(Y|X\\in A)-\\mathrm{Cov}(Y|X\\in A)\\|\\lesssim\\psi_{n}^{(2)}(\\delta).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. We use again the function $\\underline{{h}}(Y)=Y$ . We note in view of (36)-(37) that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat\\mathrm{Cov}^{\\theta}(Y|X\\in A)-\\mathrm{Cov}(Y|X\\in A)\\|\\leq\\|\\widehat\\mathrm{Cov}(Y)-\\mathrm{Cov}(Y)\\|}\\\\ &{\\qquad+\\,\\|\\langle\\mathbb{1}_{A},\\left[(\\mathbb{1}_{d_{y}}\\otimes\\mathbb{1}_{d_{y}})\\otimes\\left(\\frac{\\mathsf{D}_{Y|X}}{\\mathbb{P}[X\\in A]}-\\frac{\\widehat\\mathsf{D}_{Y|X}^{\\theta}}{\\widehat\\mathsf{E}_{x}[\\mathbb{1}_{A}]}\\right)\\right]\\underline{{h}}\\otimes\\underline{{h}}\\rangle\\|}\\\\ &{\\qquad\\,+\\|\\mathbb{E}[\\underline{{h}}(Y)\\,|\\,X\\!\\in\\!A]\\otimes\\mathbb{E}[\\!\\!\\left[{h}(Y)\\,|\\,X\\in A\\!\\right]\\!-\\!\\widehat\\mathsf{E}^{\\theta}[\\!\\!\\left[{h}(Y)\\,|\\,X\\!\\in\\!A\\!\\right]\\otimes\\widehat\\mathsf{E}^{\\theta}[\\!\\!\\left[h(Y)\\,|\\,X\\!\\in\\!A\\!\\right]\\!\\right]\\!,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Next, we note that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\langle\\mathbb{1}_{A},\\left[(\\mathbb{1}_{d_{y}}\\otimes\\mathbb{1}_{d_{y}})\\otimes\\left(\\frac{\\mathsf{D}_{Y|X}}{\\widetilde{\\mathbb{P}}[X\\in A]}-\\frac{\\widehat{\\mathbb{D}}_{Y|X}^{\\theta}}{\\widetilde{\\mathbb{E}}_{x}[\\mathbb{1}_{A}]}\\right)\\right]\\mathbb{h}\\otimes\\mathbb{h}\\rangle\\|}\\\\ &{\\quad\\le\\|\\langle\\mathbb{1}_{A},\\left[(\\mathbb{1}_{d_{y}}\\otimes\\mathbb{1}_{d_{y}})\\otimes\\left(\\frac{\\mathsf{D}_{Y|X}}{\\widetilde{\\mathbb{P}}[X\\in A]}-\\frac{\\widehat{\\mathbb{D}}_{Y|X}^{\\theta}}{\\widetilde{\\mathbb{E}}_{x}[\\mathbb{1}_{A}]}\\right)\\right]\\mathbb{h}\\otimes\\mathbb{h})\\|_{H S}}\\\\ &{\\quad\\quad\\le\\sqrt{\\mathbb{P}[X\\in A]}\\|\\frac{\\mathsf{D}_{Y|X}}{\\widetilde{\\mathbb{P}}[X\\in A]}-\\frac{\\widehat{\\mathbb{D}}_{Y|X}^{\\theta}}{\\widetilde{\\mathbb{E}}_{x}[\\mathbb{1}_{A}]}\\|\\sum_{j,k\\in[d_{y}]}\\|Y_{j}Y_{k}\\|_{L_{y}^{2}(y)}}\\\\ &{\\quad\\quad\\lesssim\\sqrt{\\mathbb{P}[X\\in A]}\\left(\\|\\frac{\\mathsf{D}_{Y|X}-\\widehat{\\mathbb{D}}_{Y|X}^{\\theta}}{\\widetilde{\\mathbb{P}}[X\\in A]}\\|+\\|\\widehat{\\mathbb{D}}_{Y|X}^{\\theta}\\|\\left(\\frac{1}{\\widehat{\\mathbb{P}}[X\\in A]}-\\frac{1}{\\widehat{\\mathbb{E}}_{x}[\\mathbb{1}_{A}]}\\right)\\right)\\sum_{j,k\\in[d_{y}]}\\|Y_{j}Y_{k}\\|_{L_{y}^{2}(y)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Remind that $Y$ is a sub-Gaussian vector. Using the equivalence of moments property of sub-Gaussian vector, we get that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|Y_{j}Y_{k}\\|_{L_{\\nu}^{2}(\\mathcal{Y})}\\leq\\sqrt{\\mathbb{E}[Y_{j}^{4}]\\mathbb{E}[Y_{k}^{4}]}\\lesssim\\mathbb{E}[Y_{j}^{2}]\\mathbb{E}[Y_{k}^{2}],\\quad\\forall j,k\\in[d_{y}].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By a union bound combining the last two displays with (23) and Lemma 3, we get w.p.a.l. $1-2\\delta$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\langle\\mathbb{1}_{A},\\left[(\\mathbb{1}_{d_{y}}\\otimes\\mathbb{1}_{d_{y}})\\otimes\\left(\\frac{\\mathsf{D}_{Y|X}}{\\mathbb{P}[X\\in A]}-\\frac{\\widehat{\\mathsf{D}}_{Y|X}^{\\theta}}{\\widehat{\\mathsf{E}}_{x}[\\mathbb{1}_{A}]}\\right)\\right]\\underline{{h}}\\otimes\\underline{{h}}\\right\\rangle\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq[\\psi_{n}(\\delta)+2(1+\\psi_{n}(\\delta))\\varphi_{X}(A)\\bar{\\epsilon}_{n}(\\delta)]\\,\\frac{(\\mathbb{E}[\\|Y\\|^{2}])^{2}}{\\sqrt{\\mathbb{P}[X\\in A]}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Next, we set $u=\\mathbb{E}[\\underline{{h}}(Y)\\,|\\,X\\in A]$ and $\\hat{u}=\\widehat{\\mathsf{E}}^{\\theta}[\\underline{{h}}(Y)\\,|\\,X\\in A]$ . Then we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|u\\otimes u-\\hat{u}\\otimes\\hat{u}\\|\\leq\\|u-\\hat{u}\\|(\\|u\\|+\\|\\hat{u}\\|)\\leq\\|u-\\hat{u}\\|(2\\|u\\|+\\|\\hat{u}-u\\|).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We apply next Corollary 2 to get w.p.a.l. $1-\\delta$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|u\\otimes u-\\hat{u}\\otimes\\hat{u}\\|\\leq\\psi_{n}^{(1)}(\\delta)\\big[2\\|\\mathbb{E}[Y\\,|\\,X\\in A]\\|+\\psi_{n}^{(1)}(\\delta)\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Next Koltchinskii and Lounici (2017, Theorem 4) guarantees that w.p.a.l $1-\\delta$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|\\widehat{\\mathrm{Cov}}(Y)-\\mathrm{Cov}(Y)\\|\\lesssim\\epsilon_{n}^{(2)}(\\delta),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\epsilon_{n}^{(2)}(\\delta)$ is defined in (38). ", "page_idx": 24}, {"type": "text", "text": "A union bound combining (40), (41), (42) and (43) gives the result. ", "page_idx": 24}, {"type": "text", "text": "C Numerical Experiments ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Experiments were conducted on a high-performance computing cluster equipped with an Intel(R) Xeon(R) Silver 4210 CPU $@$ 2.20GHz Sky Lake CPU, 377GB RAM, and an NVIDIA Tesla V100 16Gb GPU. Code is available at https://github.com/pietronvll/NCP. ", "page_idx": 25}, {"type": "text", "text": "C.1 Conditional Density Estimation ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "To evaluate our method\u2019s ability to estimate conditional densities, we tested NCP on six different data models (described in the following paragraph) and compared its performance with ten other methods (detailed in Tab. 2). We assessed the methods\u2019 performance using the KS distance between the estimated conditional CDF and the true CDF. Additionally, we explored how the performance of each method scales with the number of training samples, ranging from $10^{2}$ to $10^{5}$ , with a validation set of $10^{3}$ samples. We tested each method on nineteen different conditional values uniformly sampled between the $5\\%$ - and $95\\%$ -percentile of $p(x)$ . Conditional CDFs were estimated on a grid of 1000 points uniformly distributed over the support of $Y$ . The KS distance between each pair of CDFs was averaged over all the conditioning values. In Tab. 5, we present the mean performance (KS distance $\\pm$ standard deviation), computed over 10 repetitions, each with a different random seed. ", "page_idx": 25}, {"type": "text", "text": "Synthetic data models. We included the following synthetic datasets from Rothfuss et al. (2019) and Gao and Hastie (2022) into our benchmark: ", "page_idx": 25}, {"type": "text", "text": "\u2022 LinearGaussian, a simple univariate linear density model defined as $\\textstyle Y=X+{\\mathcal{N}}(0,0.1)$ where $X\\sim\\operatorname{Unif}(-1,1)$ .   \n\u2022 EconDensity, an economically inspired heteroscedastic density model with a quadratic dependence on the conditional variable defined as $Y=X^{2}+\\epsilon_{Y},\\dot{\\epsilon}_{Y}\\sim\\mathcal{N}(0,1+\\dot{X})$ where $\\bar{X^{\\sim}}|\\mathcal{N}(0,1)|$ .   \n\u2022 ArmaJump, a first-order autoregressive model with a jump component exhibiting negative skewness and excess kurtosis, defined as ", "page_idx": 25}, {"type": "equation", "text": "$$\nx_{t}=\\left[c(1-\\alpha)+\\alpha x_{t-1}\\right]+(1-z_{t})\\epsilon_{t}+z_{t}\\left[-3c+2\\epsilon_{t}\\right],\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\epsilon_{t}\\sim\\mathcal{N}(0,0.05)$ and $z_{t}\\,\\sim\\,B(1,p)$ denote a Gaussian shock and a Bernoulli distributed jump indicator with probability $p$ , respectively. The parameters were left at their default value. ", "page_idx": 25}, {"type": "text", "text": "\u2022 GaussianMixture, a bivariate Gaussian mixture model with 5 kernels where the goal is to estimate the conditional density of one variable given the other. The mixture model is defined as $\\begin{array}{r}{p(X,Y)\\,=\\,\\sum_{k=1}^{5}\\overset{\\cdot}{\\pi}_{k}{\\mathcal N}\\left(\\mu_{k},\\Sigma_{k}\\right)}\\end{array}$ where $\\pi_{k},\\,\\mu_{k}$ , and $\\Sigma_{k}$ are the mixing coefficient, mean vector, and covariance matrix of the $k$ -th distribution. All the parameters were randomly initialized.   \n\u2022 SkewNormal, a univariate skew normal distribution defined as $Y=2\\phi(X)\\psi(\\alpha X)$ where $\\phi(\\cdot)$ and $\\psi(\\cdot)$ are the standard normal probability and cumulative density functions, and $\\alpha$ is a parameter regulating the skewness. The parameters were left at their default value.   \n\u2022 Locally Gaussian or Gaussian mixture distribution (LGGMD) (Gao and Hastie, 2022), a regression dataset where the target $y$ depends on the three first dimensions of $x$ , with seventeen irrelevant features added to $x$ . The features of $x$ are all uniformly distributed between $-1$ and 1. The first dimension of $x$ gives the mean of $Y|X$ , the second is whether the data is Gaussian or a mixture of two Gaussians, and the third gives its asymmetry. More specifically: ", "page_idx": 25}, {"type": "equation", "text": "$$\nY|X\\sim\\left\\{\\begin{array}{l l}{0.5\\mathcal{N}(0.25X^{(1)}-0.5,0.25(0.25X^{(3)}+0.5)^{2})}\\\\ {\\quad+0.5\\mathcal{N}(0.25X^{(1)}+0.5,0.25(0.25X^{(3)}-0.5)^{2})\\;\\mathrm{if}\\;X^{(2)}\\le0.2}\\\\ {0\\!\\mathcal{N}(0.25X^{(1)}-0.5,0.3)\\;\\mathrm{if}\\;X^{(2)}>0.2}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "To sample data from EconDensity, ArmaJump, GaussianMixture, and SkewNormal, we used the library Conditional Density Estimation (Rothfuss et al., 2019) available at https://github. com/freelunchtheorem/Conditional_Density_Estimation. ", "page_idx": 25}, {"type": "text", "text": "Training NCP. We trained an NCP model with $u^{\\theta}$ and $v^{\\theta}$ as multi-layer perceptrons (MLPs), each having two hidden layers of 64 units using GELU activation function in between. The vector $\\sigma^{\\theta}$ has a size of $d=100$ , and $\\gamma$ is set to $10^{-3}$ . Optimization was performed over $10^{4}$ epochs using the Adam optimizer with a learning rate of $10^{-3}$ . Early stopping was applied based on the validation set with patience of 1000 epochs. To ensure the positiveness of the singular values, we transform the vector $\\dot{\\sigma}^{\\theta}$ with the Gaussian function $x\\mapsto\\exp(-x^{2})$ during any call of the forward method. Whitening was applied at the end of training. ", "page_idx": 26}, {"type": "text", "text": "Compared methods. We compared our NCP network with ten different CDE methods. See Tab. 2 for the exhaustive list of models including a brief summary and key hyperparameters. ", "page_idx": 26}, {"type": "text", "text": "In particular, the methods were set up as follows: ", "page_idx": 26}, {"type": "text", "text": "\u2022 NF was characterized by a $1D$ Gaussian base distribution and two Masked Affine Autoregressive flows (Papamakarios et al., 2017) followed by a LU Linear permutation flow. To match the NCP architecture, each flow was defined by two hidden layers with 64 units each. The training procedure was the same as for the NCP model. The model was implemented using the library normflows (Stimper et al., 2023).   \n\u2022 DDPM was characterized by a U-Net (Ronneberger et al., 2015), a noise schedule starting from $10^{-}4$ to 0.02 and 400 steps of diffusion as implemented in https://github.com/ TeaPearce/Conditional_Diffusion_MNIST.   \n\u2022 CKDE\u2019s kernels bandwidth was estimated according to Silverman\u2019s rule (Silverman, 1986).   \n\u2022 MDN\u2019s architecture was defined by two hidden layers with 64 units each and 20 Gaussians kernels.   \n\u2022 KMN\u2019s architecture was defined by two hidden layers with 64 units each, 50 Gaussians kernels, and kernels bandwidth was estimated according to Silverman\u2019s rule (Silverman, 1986).   \n\u2022 LSCDE was defined by 500 components which bandwidths were set to 0.5 and kernels center found via a k-means procedure.   \n\u2022 NNKDE\u2019s number of neighbors was set using the heuristics $k={\\sqrt{n}}$ (Devroye et al., 1996). Kernels bandwidth was estimated according to Silverman\u2019s rule (Silverman, 1986). We used the implementation available at https://github.com/lee-group-cmu/NNKCDE.   \n\u2022 RFCDE was characterized by a Random Forest with 1000 trees and 31 cosine basis functions. The training was performed using the rfcde library available at https://github.com/ lee-group-cmu/RFCDE.   \n\u2022 FC was trained using a Random Forest with 1000 trees as a regression method and had 31 cosine basis functions. The training was performed using the flexcode library available at https://github.com/lee-group-cmu/FlexCode.   \n\u2022 LinCDE was trained with 1000 LinCDE trees using the LinCDE.boost R function from https://github.com/ZijunGao/LinCDE. ", "page_idx": 26}, {"type": "text", "text": "CKDE, MDN, KMN, and LSCDE hyperparameters were set according to Rothfuss et al. (2019) and were trained using the library Conditional Density Estimation available at https://github.com/ freelunchtheorem/Conditional_Density_Estimation. All methods involving the training of a neural network were assigned the same number of epochs given to NCP. All other method parameters were set as prescribed in their paper. ", "page_idx": 26}, {"type": "text", "text": "Results. See Tab. 4 for the comparison of performances for $n=10^{4}$ . See also Fig. 5. We also carried out an ablation study on centering and whitening post-treatment for NCP in Tab. 3 ", "page_idx": 26}, {"type": "text", "text": "C.2 Confidence Regions ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The objective of this next experiment is to estimate a confidence interval at coverage level $90\\%$ for two distribution models with different properties (Laplace and Cauchy) and one real dataset in order to showcase the versatility of our NCP approach. ", "page_idx": 26}, {"type": "table", "img_path": "zXfhHJnMB2/tmp/0d6fe2d1121d6de265005cbf3a68cef41ae933cd98e7c82036e51f045c0ea544.jpg", "table_caption": ["Table 2: Compared methods for the CDE problem. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table 3: Ablation study on post-treatment for NCP. We report the mean and std of KS distance of estimated CDF from the truth averaged over 10 repetitions with $n=10^{5}$ (best method in bold red). NCP\u2013C and NCP\u2013W refer to our method with centering and whitening post-treatment, respectively. ", "page_idx": 27}, {"type": "table", "img_path": "zXfhHJnMB2/tmp/dc8a57e0e0683e7ea0e95a65b3599fcae998af0ff93cd5809b62e5f8b7d153ef.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table 4: Mean and standard deviation of KS distance of estimated CDF from the truth averaged over 10 repetitions with sample size of $10^{4}$ (best method in bold red, second best in bold black). NCP\u2013C and NCP\u2013W refer to our method with centering and whitening post-treatment, respectively. ", "page_idx": 27}, {"type": "table", "img_path": "zXfhHJnMB2/tmp/150d63706e01bb4bdd14ad49788453344492b594d4aab7a3f7856769055286a7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "zXfhHJnMB2/tmp/e1194e845c8cbdfce6774d011926c5d4a897338daa255cb58239d15af4efd09c.jpg", "img_caption": ["Figure 5: Performances for CDE on synthetic datasets w.r.t sample size $n$ . Performance metric is Kolmogorov-Smirnov (KS) distance to truth. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Compared methods. We compared our NCP procedure for building conditional confidence intervals to the state-of-the-art conditional conformal prediction method in Gibbs et al. (2023). We also developed another method based on Normalizing Flows\u2019 estimation of the conditional CDE and we added it to the benchmark. ", "page_idx": 28}, {"type": "text", "text": "Experiment for Laplace and Cauchy distributions. We generate a dataset where the $X$ variable follows a uniform distribution on interval [0, 5] and $Y|X=x$ follows either a Laplace distribution with location and scale parameters $(\\mu(x),\\dot{b(x)})^{\\prime}\\!=(x^{2},\\dot{x})$ or a Cauchy distribution with location and scale parameters $(\\mu(x),\\^{-}\\!b(x))=(x^{2},1+x)$ . We create a train set of 50000 samples, a validation set of 1000 samples and a test set of 1000 samples. ", "page_idx": 28}, {"type": "text", "text": "For the Laplace distribution, we train an NCP where $u^{\\theta}$ and $v^{\\theta}$ are multi-layer perceptrons with two hidden layers of 128 cells, $\\sigma^{\\theta}$ is a vector of size $d=500$ and $\\gamma=10^{-2}$ . Between each layer, we use the GELU activation function. We optimize over 5000 epochs using the Adam optimizer with a learning rate of $10^{-3}$ . We apply early stopping with regard to the validation set with a patience of 100 epochs. Whitening is applied at the end of training. To fit the Cauchy distribution, we increase the depth of the MLPs to 5 and the width to 258. ", "page_idx": 28}, {"type": "text", "text": "We compare this NCP network with two state-of-the-art methods. The first is a normalizing flow with base distribution a $1D$ Gaussian and two Autoregressive Rational Quadratic spline flows (Durkan et al., 2019) followed by a LU Linear permutation flow. All flows come from the library normflows (Stimper et al., 2023). The spline flows have each two blocks of 128 hidden units to match the NCP architecture. The normalizing flow is allowed the same number of epochs as ours with the same optimizer. The second model is the conditional conformal predictor from Gibbs et al. (2023). This model needs a regressor as an input. We consider a situation favorable to Gibbs et al. (2023) as we assume as prior knowledge that the true conditional expectation is a polynomial function (the truth is actually the quadratic function in this example). Therefore we chose a linear regression with polynomial features as in Gibbs et al. (2023) as this regressor should fti the data without any problem. For all other choices of parameters, we follow the prescriptions of Gibbs et al. (2023). For the sake of fairness, we note that the validation set used for early stopping in NF and NCP was also used as a calibration set for the CCP method. ", "page_idx": 28}, {"type": "image", "img_path": "zXfhHJnMB2/tmp/3183a53c8b437e6be6712417dc53dea7706d1503a5f123695991fcc1bfb29d9b.jpg", "img_caption": ["Figure 6: Estimated conditional PDFs (left) and CDFs (right) for each synthetic dataset for 3 different conditioning points. Dotted lines represent the true distributions, while solid lines represent the estimates from NCP. The average KS distance over 5 repetitions is also reported on the right plots. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "By design, the Conditional Conformal Predictor (CCP) gives the confidence interval directly. However NCP and NF output the conditional distribution. To find the smallest confidence interval with desired coverage, we apply the linear search algorithm described in Algorithm 3 on the discretized conditional CDFs provided by NCP and NF. The results are provided in Fig. 1. First, observe that although the linear regression achieves the best estimation of the conditional mean, as should be expected since the model is well-specified in this case, the confidence intervals, however, are unreliable for most of the considered conditioning. We also notice instability for NF and CCP for conditioning in the neighborhood of $x=0$ with NF confidence region exploding at $x=0$ . We expect this behavior is due to the fact that the conditional distribution at $x=0$ is degenerate. Comparatively, NCP does not exhibit such instability around $x=0$ . It only tends to overestimate the produced confidence region for conditioning close to $x=0$ . ", "page_idx": 30}, {"type": "text", "text": "Algorithm 3 Confidence interval search given a CDF ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Require: $Y$ a vector of values, $F_{Y}$ a vector of realisations of the CDF at points $Y$ , $\\alpha\\in[0,1]$ a confidence level Initialize $\\ensuremath{t_{\\mathrm{low}}}=0$ and $t_{\\mathrm{high}}=1$ Initialize $t_{\\mathrm{low}}^{*}=0$ and $t_{\\mathrm{high}}^{*}=-1$ Initialize $s*=\\infty$ while Center and scale $X_{\\mathrm{train}}$ and $Y_{\\mathrm{train}}$ do if $F_{Y}[t_{\\mathrm{high}}]-F_{Y}[t_{\\mathrm{low}}]\\geq\\alpha$ then $\\mathrm{size}=\\mathrm{\\bar{\\boldsymbol{Y}}}[t_{\\mathrm{high}}]-\\boldsymbol{Y}[t_{\\mathrm{low}}]$ if size $<s*$ then $t_{\\mathrm{low}}^{*}=t_{\\mathrm{low}},\\;\\;t_{\\mathrm{high}}^{*}=t_{\\mathrm{high}},\\;\\;s*=\\mathrm{size}$ end if $t_{\\mathrm{low}}=t_{\\mathrm{low}}+1$ else if $t_{\\mathrm{high}}=\\mathrm{len}(Y)-1$ then break else $t_{\\mathrm{high}}=t_{\\mathrm{high}}+1$ end if end while Return $Y[t_{\\mathrm{low}}],Y[t_{\\mathrm{high}}]$ ", "page_idx": 30}, {"type": "text", "text": "Experiment on real data. We also evaluate the performance of NCP in estimating confidence intervals using the Student Performance dataset available at https://www.kaggle.com/datasets/ nikhil7280/student-performance-multiple-linear-regression/data. This dataset comprises 10000 records, each defined by five predictors: hours studied, previous scores, extracurricular activities, sleep hours, and sample question papers practiced, with a performance index as the target variable. In this experiment, the NCP\u2019s $u^{\\theta}$ and $v^{\\theta}$ are defined by MLPs with two hidden layers, each containing 32 units and using GELU activation functions, $\\sigma^{\\theta}$ is a vector of size $d=50$ and $\\gamma=10^{-2}$ . Optimization was performed over 50000 epochs using the Adam optimizer with a learning rate of $10^{-3}$ . We compare NCP with a normalizing flow defined as above in which spline flows have each two blocks of 32 hidden units to match NCP architecture. The normalizing flow is trained for the same number of epochs as our model, using the same optimizer. We further compare NCP with a split conformal predictor featuring a Random Forest regressor (RFSCP) with 100 estimators. We used the implementation of the library puncc (Mendil et al., 2023). For NCP and the normalizing flow, early stopping is based on the validation set, while for RFSCP, the validation set serves as the calibration set. We performed 10 repetitions, randomly splitting the dataset into a training set of 8000 samples and validation and test sets of 1000 samples each. We report the results of the estimated confidence interval at a coverage level of $90\\%$ in Tab. 5. The methods provide fairly good coverage. NF did not respect the $90\\%$ coverage condition. Only NCP and RFSCP both respect the coverage condition but the width of the confidence intervals for RFSCP are larger than for NCP. ", "page_idx": 30}, {"type": "text", "text": "Discussion on Conformal Prediction. Conformal prediction (CP) is a popular model-agnostic framework for uncertainty quantification approach Vovk et al. (1999). CP assigns nonconformity ", "page_idx": 30}, {"type": "text", "text": "Table 5: Mean and standard deviation of $90\\%$ prediction interval (PI) coverages and interval widths, averaged over 10 repetitions for the Student Performance dataset from Kaggle.NCP\u2013C and NCP\u2013W refer to our method with centering and whitening post-treatment, respectively. ", "page_idx": 31}, {"type": "table", "img_path": "zXfhHJnMB2/tmp/4fc18f78a0996ca4aaa641ea45ed1e129ee1eff72ed599511d5b0d1273230f24.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "scores to new data points. These scores reflect how well each point aligns with the model\u2019s predictions. CP then uses these scores to construct a prediction region that guarantees the true outcome will fall within it with a user-specified confidence parameter. However, CP is not without limitations. The construction of these guaranteed prediction regions can be computationally expensive especially for large datasets, and need to be recomputed from scratch for each value of the confidence level parameter. In addition, the produced CP confidence regions tend to be conservative. Another limitation of regular CP is that predictions are made based on the entire input space without considering potential dependencies between variables. Conditional conformal prediction (CCP) was later developed to handle conditional dependencies between variables, allowing in principle for more accurate and reliable predictions Gibbs et al. (2023). CCP suffers from the typical limitations of regular CP and the theoretical guarantees. ", "page_idx": 31}, {"type": "text", "text": "C.3 High-dimensional Experiments ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Experiment on high-dimensional synthetic data. In Fig. 3, we trained NCP for $d=100$ using the same MLP architecture and the same NF with autoregressive flow as in our initial experiments based on $n\\,=\\,10^{5}$ samples $\\{(X_{i},Y_{i})\\}_{i=1}^{n}$ with values in $\\mathbb R^{d}\\times\\mathcal D$ . We plot the conditional CDF for several conditioning w.r.t. $\\theta(x)$ on 10 repetitions. NCP paired with a small MLP architecture performs comparably to the NF model for Gaussian distributions. For discrete distributions, the NCP demonstrates superior performance compared to the NF model. ", "page_idx": 31}, {"type": "text", "text": "We repeated the experiment in Fig. 3 for $d\\ \\in\\ \\{100,200,500,1000\\}$ and recorded the average Kolmogorov-Smirnov (KS) distance of the NCP conditional distribution to the truth, computation time and their standard deviations over 10 repetitions. ", "page_idx": 31}, {"type": "image", "img_path": "zXfhHJnMB2/tmp/9f53d366e939d075dbf2396e8db38f3e6fd7ad631000a3ef2853cd050821e323.jpg", "img_caption": ["Figure 7: Left: we observe only $\\approx20\\%$ increase in compute time going from $d=10^{2}$ to $d=10^{3}$ . Right: average KS distance to the truth and standard deviation over 10 repetitions. "], "img_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "zXfhHJnMB2/tmp/31692247b1e2090d738f7351e63d3f9b7d89a7865ca39e65e077199613e85fa9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "High-dimensional experiment in molecular dynamics: Chignolin folding. We investigated the dynamics of Chignolin folding, using a molecular dynamics simulation lasting $106\\mu s$ and sampled every $200p s$ , resulting in 524, 743 data points. Our analysis focuses on 39 heavy atoms (nodes) with a cutoff radius of 5 Angstroms. To predict the conditional transition probability between metastable states, we integrate our NCP approach with a graph neural network (GNN) model. GNNs, as demonstrated by Chanussot et al. (2021), represent the state-of-the-art in modeling atomistic systems, adeptly incorporating the roto-translational and permutational symmetries inherent in physical systems. In particular, we employed a SchNet model Sch\u00a8utt et al. (2019, 2023) with three interaction blocks. Each block features a 64-dimensional latent atomic environment, and the inter-atomic distances for message passing are expanded over 20 radial basis functions. After the final interaction block, each latent atomic environment is processed through a linear layer and then aggregated by averaging. The model underwent training for 100 epochs using an Adam optimizer with a learning rate of $10^{-3}$ . We employed a batch size of 256 and set $\\gamma$ to $10^{-3}$ . In Fig. 2, we show how our NCP approach enables the tracking transitions between metastable states, demonstrating accurate forecasting and strong uncertainty quantification. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: see abstract, Introduction and section Related works. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: see discussion under main results in Section Theoretical guarantees and conclusion. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Yes each statement clearly state all required assumptions and all proofs are provided in Appendix B. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper presents the pseudo-code of our method, links to the datasets and methods used to reproduce our results ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: While the paper is predominantly theoretical, we have presented experiments which illustrate our theory. Data and code can be made available upon request during the rebuttal and will be made readily available should the paper be accepted. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: Appendix C provides all details on the architecture used for our method in order to reproduce the experiment. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Whenever appropriate, we provided standard deviations for the performance of the compared methods computed over several repetitions. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \u201cYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 35}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We provided the information at the beginning of Appendix C. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: This is a theoretical paper. Experiments were carried out on synthetic data or publicly available data. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Paper of theoretical nature. There are no particular concerns. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 36}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: there is no such risk. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: all existing methods used in our experimental study were properly cited in the main paper and/or Appendix C. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA]   \nJustification: This is a theoretical work. Guidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] Justification: see above. ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: see above. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}]