[{"type": "text", "text": "Randomized Sparse Matrix Compression for Large-Scale Constrained Optimization in Cancer Radiotherapy ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shima Adeli1 Mojtaba Tefagh1,2 $^*$ Gourav Jhanwar3 Ma 1Sharif University of Technology 2University of Edinburgh 3Memorial Sloan Kettering Cancer Center ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Radiation therapy, treating over half of all cancer patients, involves using specialized machines to direct high-energy beams at tumors, aiming to damage cancer cells while minimizing harm to nearby healthy tissues. Customizing the shape and intensity of radiation beams for each patient leads to solving large-scale constrained optimization problems that need to be solved within tight clinical time-frame. At the core of these challenges is a large matrix that is commonly sparsified for computational efficiency by neglecting small elements. Such a crude approximation can degrade the quality of treatment, potentially causing unnecessary radiation exposure to healthy tissues\u2014this may lead to significant radiation-induced side effects\u2014or delivering inadequate radiation to the tumor, which is crucial for effective tumor treatment. In this work, we demonstrate, for the first time, that randomized sketch tools can effectively sparsify this matrix without sacrificing treatment quality. We also develop a novel randomized sketch method with desirable theoretical guarantees that outperforms existing techniques in practical application. Beyond developing a novel randomized sketch method, this work emphasizes the potential of harnessing scientific computing tools, crucial in today\u2019s big data analysis, to tackle computationally intensive challenges in healthcare. The application of these tools could have a profound impact on the lives of numerous cancer patients. Code and sample data available at https://github.com/PortPy-Project/CompressRTP ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In 2020, an estimated 18.1 million new cancer cases and about 9.9 million cancer-related deaths were reported globally [22]. Radiation therapy (RT) is integral to cancer treatment, utilized in approximately half of all cases, either alone or in combination with other treatments like surgery or chemotherapy [22]. RT involves using specialized machines to direct high-energy radiation beams at tumors, with the primary goal of destroying cancer cells while minimizing damage to healthy tissues. This process requires precise optimization of machine parameters, such as beam shapes and angles, tailored to each patient\u2019s unique anatomy. It involves solving large-scale, constrained, non-linear optimization problems swiftly within clinical time constraints [22, 19]. The urgency of this task is heightened in modern online adaptive radiotherapy techniques, where rapid solution is essential since patients remain immobilized on the treatment couch during preparation [12]. Delays not only compromise patient comfort but can also affect treatment outcomes, as any rapid anatomical changes (e.g., bladder fliling in prostate cancer) can render treatment plans based on initial anatomy sub-optimal. Thus, quickly solving these optimization problems is crucial. ", "page_idx": 0}, {"type": "text", "text": "We briefly describe the mathematical modeling of radiotherapy treatment (see [22, 19] for more details). The patient\u2019s body is discretized into small three-dimensional voxels (indexed by $i=$ $1,\\ldots,m)$ , and each radiation beam is discretized into small two-dimensional beamlets (indexed by $j\\,=\\,1,\\ldots,n)$ . The radiation dose delivered to each voxel $i$ from each beamlet $j$ with unit intensity is precalculated and represented by $a_{i j}$ , forming a matrix $A$ \u2014 commonly referred to as the dose influence matrix. This matrix is typically large, containing about 100,000 to 500,000 rows corresponding to the patient\u2019s voxels, and 1,000 to 20,000 columns representing the beamlets of the radiotherapy machine [18]. Our objective is to optimize the intensities of the beamlets, denoted by $x$ , in order to achieve a desired radiation dose, $A x$ , that is delivered to the patient\u2019s body. For the tumor voxels, we aim to achieve a radiation dose that approximates the dose prescribed by a physician while for healthy voxels we aim to minimize the radiation dose as much as possible. The optimization problem can be described in the following general form: ", "page_idx": 1}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{{\\mathrm{minimize}}}&{f_{0}(A x)+f_{1}(x)}\\\\ {{\\mathrm{subject~to}}}&{g(A x)\\leq0}\\\\ &{h(x)\\leq0,}\\end{array}}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where, $f_{0}(A x)$ measures the quality and \u2018goodness\u2019 of the radiation dose $A x$ , $f_{1}(x)$ assesses the quality of the beamlet intensities. The functions $g$ and $h$ represent constraints on the dose received by the voxels and the intensities of the beamlets, respectively. Various formulations for these functions have been suggested in existing research [17]; however, the following quadratic optimization problem has arguably been the most commonly used formulation [22, 19]: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{s\\in\\bar{S}}\\sum_{i\\in I_{s}}(w_{+}^{s}\\operatorname*{max}(A_{i}^{s}x-d^{s},0)^{2}+w_{-}^{s}\\operatorname*{max}(d^{s}-A_{i}^{s}x,0)^{2})+||P x||_{2}^{2}}\\\\ &{A_{i}^{s}x\\leq d_{M a x}^{s},\\forall s\\in\\bar{S},i\\in I_{s}}\\\\ &{M e a n(A^{s}x)\\leq d_{M e a n}^{s},\\forall s\\in\\bar{S}}\\\\ &{x\\geq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where, $\\bar{S}$ represents the set of structures (i.e., organs, tumors), including tumor and healthy structures, and $I_{s}$ represents the set of voxels belonging to structure $s$ . The first term in the objective function is a two-sided quadratic function penalizing the radiation overdose/underdose with the penalty weight $w_{+}^{s}/w_{-}^{s}$ , where the radiation overdose/underdose is defined as a delivered radiation dose, $A^{s}x$ , exceeding/below the prescribed dose, $d^{s}$ , for each structure $s\\in\\bar{S}$ . For tumor-affiliated structures, the prescribed dose is given by a radiation oncologist. For healthy structures, the prescribed dose is zero, $d^{s}\\,=\\,0$ , and there is no underdose penalty, $w_{-}^{s}\\;=\\;0$ . The second term in the objective function, $f_{1}(x)=||P x||_{2}^{2}$ , aims to penalize variations in intensities across neighboring beamlets to promote smoothness in beamlet intensities for enhanced radiation delivery (each row of matrix $P$ has a value of 1 and $^{-1}$ for two neighboring beamlets). The first/second set of constraints impose maximum/mean dose constraints to satisfy the maximum/mean dose limits defined by a clinical protocol for each structure $s\\in{\\bar{S}}$ , and the last constraint is a physical non-negativity constraint on the beamlet intensities. The overdose/underdose penalty weights $w_{+}^{s}/w_{-}^{s}$ need to be adjusted for each patient and various techniques have been developed to automate this process (see [22] and references therein). ", "page_idx": 1}, {"type": "text", "text": "We will use the second formulation (Eq. 2) for our experiments in this study. However, regardless of the specific functions chosen in Problem 1 and the technique used to adjust the problem hyperparameters, the size and structure of matrix $A$ are crucial in determining the computational intensity of the problem. Considering that these large-scale, non-linear, constrained optimization problems are often solved using the interior-point method with cubic computational complexity, the computational time can increase significantly with the size and density of non-zero elements in matrix $A$ . Therefore, matrix sketching could be a compelling choice to improve the computational complexity of these problems. There has been a body of research employing matrix sketching, often using a transformation matrix $S$ , resulting in $S A$ with a reduced number of rows, to improve the computational efficiency of the least-squares optimization problems commonly arising in machine learning applications [14, 16, 21]. However, these sketching techniques cannot be used where matrix $A$ is also involved in the constraints, as is the case in radiotherapy applications. Reducing the number of rows would prevent direct access to $A x$ , which is crucial for evaluating $g(A x)\\leq0$ in Problem 1. Thus, we explore the potential of using matrix sparsification, a specific form of matrix sketching, to substitute the dose influence matrix $A$ with a sparse matrix $S$ to improve the computational efficiency of our radiotherapy constrained optimization problems. In fact, a very simple form of matrix sparsification is currently being used in practice, where all small elements of the matrix below a predefined threshold, typically less than $0.01\\times\\operatorname*{max}(A)$ , are discarded [10]. This method, which we will refer to as the \"naive\" approach, is clearly not the most optimal solution and may adversely affect the quality of the treatment. The primary concern with this approach is that the radiation dose calculated and optimized in Problem 1 using the modified matrix $A$ may not accurately reflect the actual dose received by the patient. This discrepancy arises from the inherent inaccuracies introduced by the truncation of matrix $A$ , which could potentially lead to sub-optimal treatment outcomes. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In this study, we demonstrate that using matrix sparsification techniques, primarily developed by the machine learning community, we can enjoy the computational efficiency of working with sparse matrices and still being able to solve constrained optimization problems within the clinical timeframe, without significantly compromising the integrity of the original problem that could potentially degrade the treatment quality. Matrix sparsification techniques carefully sample and scale the elements of the original dense matrix $A$ to create a sparse sketch matrix $S$ that minimizes $||A-S||_{2}$ . Prior research predominantly utilized matrix sparsification for applications such as low-rank approximation and principal component analysis (PCA) [1, 2]. This study uniquely demonstrates the utility of matrix sparsification for efficiently addressing large-scale, constrained, nonlinear optimization challenges within constrained timeframes. We demonstrate that applying a randomized sketch to the influence matrix $A$ in radiotherapy optimization significantly outperforms the current naive sparsification approach. To the best of our knowledge, this is the first application of matrix sparsification with a publicly available benchmark dataset, encouraging further research in this direction. Furthermore, we have developed a novel randomized sketching algorithm that exhibits superior performance compared to existing techniques and is supported by theoretical guarantees for its efficacy. As formally stated in Theorem 3.6, Lemma 3.7, and Theorem 3.9, the proposed algorithm ensures a minimal impact on the constraints, objective function and the optimal points of the original optimization Problem 2. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In matrix sparsification, we typically aim for an unbiased approximation of a matrix $A$ by another matrix $S$ such that $\\left\\|A-S\\right\\|_{2}\\leq\\tau$ for a given $\\tau>0$ , while minimizing the number of nonzero entries in $S$ [1, 2, 3, 7, 8, 15]. The $\\ell_{2}$ norm of the difference between $A$ and $S$ serves as a measure of this error, a choice that has been justified in the literature [2] for various applications. The sparsification techniques proposed in the literature fall into two categories. The first involves randomly scaling each matrix entry independently. Specifically, for an entry $a_{i j}$ it is scaled to $a_{i j}/p_{i j}$ with probability $p_{i j}$ , or is set to zero otherwise [1, 3, 8, 15]. This process increases the magnitude of certain matrix entries and zeros out others, resulting in a sparse matrix. Importantly, the resulting matrix serves as an unbiased estimator of the original matrix, with its entries acting as independent random variables. The probability typically follows the formula $p_{i j}=c f(a_{i j})$ , where $c>0$ is a universal constant. This method, first introduced by Achlioptas and McSherry [1], involves scaling each entry $a_{i j}$ to $a_{i j}/p_{i j}$ with probability $p_{i j}=c a_{i j}^{2}$ ; otherwise, the entry is set to zero. This technique, referred to as $\\ell_{2}$ scaling, laid a foundational basis in the domain of matrix sparsification. However, Achlioptas and McSherry [1]\u2019s approach faced limitations in theoretical guarantees and required a significant number of non-zero entries to achieve a satisfactory $\\ell_{2}$ norm bound. Additionally, scaling small entries could disproportionately inflate values in the resulting matrix. Building upon this, Arora et al. [3] introduced a variation focused on deterministically retaining the largest entries in the matrix while randomly scaling the smaller ones. During the scaling phase, each matrix entry is scaled to $a_{i j}/p$ with a probability $p=c|a_{i j}|$ , or set to zero otherwise. This method is fast in practice as it requires only a single pass over all the non-zero entries and Arora et al. [3] demonstrated that their method outperforms Achlioptas and McSherry [1] approach. However, this approach substitutes all small entries, which are not reduced to zero, with a constant ( $\\stackrel{\\cdot}{c}$ or $-c)$ . This could significantly affect its performance, especially when the matrix undergoes extensive sparsification. ", "page_idx": 2}, {"type": "text", "text": "The second category of sparsification techniques, introduced later, involves independently sampling from the entries of $A$ using a probability distribution $p$ . Each sample generates a matrix filled with zeroes, except for the sampled entry. Subsequently, $s$ samples are collected, and their average forms a sparse approximation. Contrary to the first approach, the entries of the resulting matrix $S$ are not independent. Instead, $S$ is formed by summing independent random matrices. To ensure that the resulting matrix is an unbiased estimator of $A$ , the sampled matrix\u2019s entry should be $a_{i j}/p_{i j}$ , where $p_{i j}$ is derived from the probability distribution $p$ . In this context, the choice of probability distribution $p$ is critical. Drineas and Zouzias [7] introduced a technique where $p_{i j}$ is proportional to $a_{i j}^{2}$ , termed $\\ell_{2}$ sampling, and used Bernstein inequality [20] to calculate the number of samples needed to achieve a desired accuracy. Later, Achlioptas et al. [2] introduced a near-optimal probability distribution under specific conditions. Additionally, Braverman et al. [4] developed a near-optimal method specifically designed for numerically sparse matrices (this method belongs to the first category). While the theoretical bounds of this method closely align with those of Achlioptas et al. [2], the approach by Braverman et al. [4] is applicable to all matrices. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "The primary issue with the second category of techniques is the increase in the required number of non-zero elements in the sparse matrix, which grows in proportion to the inverse square of the error, $1/\\tau^{2}$ , unlike the $1/\\tau$ growth rate seen in the first category. This is especially problematic given that the interior point methods typically used to solve Problem 1 have cubic computational complexity, which results in a cubic increase in processing time with the size of the input data. However, the second category may be preferable in applications when data is only accessible in a streaming manner. The first category necessitates access to the complete dataset. In this study, we assume that the matrix $A$ is precomputed\u2014a time-intensive process that often takes as long as solving the optimization problems themselves. Theoretically, these matrices could be computed on a row-by-row basis [6], allowing the sparsification algorithm to operate concurrently, provided it can handle streaming data. Our proposed hybrid approach can, in principle, be used in a streaming manner [2]. ", "page_idx": 3}, {"type": "text", "text": "Notations. Let $a_{i j}$ denote the element in the $i$ -the row and $j$ -th column of matrix $A$ , and let $\\mathrm{nnz}(A)$ denote the number of non-zero entries in $A$ . For matrix $A$ , we consider the entry-wise $\\ell_{1}$ norm, defined as $\\begin{array}{r}{\\|A\\|_{1}=\\sum_{i=1}^{m}\\sum_{j=1}^{n}|a_{i j}|}\\end{array}$ , and the spectral norm defined as $\\left\\|A\\right\\|_{2}=\\operatorname*{max}_{\\left\\|x\\right\\|_{2}=1}\\left\\|A x\\right\\|_{2}$ . Additionally, we introduce $A_{(i)}\\in\\mathbb{R}^{m\\times n}$ , a matrix with all zero entries except for the $i$ -th row, which remains identical to the $i$ -th ro\u221aw of $A$ . Finally, for a vector $a$ , we define its numerical sparsity as $\\operatorname{ns}(a)=\\operatorname*{min}\\{k\\geq0:\\|a\\|_{1}\\leq{\\sqrt{k}}\\,\\|a\\|_{2}\\}$ and let $\\operatorname{ns}(A)$ denote the maximum numerical sparsity of its rows and columns. ", "page_idx": 3}, {"type": "text", "text": "3 Algorithm Description ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this study, we introduce an algorithm that matches the theoretical requirements of the first category for the number of non-zero elements (i.e., the growth of the non-zero elements proportional to $1/\\tau)$ , while outperforming existing techniques in practical experiments. Our findings reveal that this approach markedly enhances the accuracy of sparse sketches for large matrices that appear in the context of cancer radiotherapy optimization. This method can be seen as a hybrid, combining advantages from both categories. We deterministically retain the largest entries, akin to the strategy suggested by Arora et al. [3]. This method is particularly relevant for our application, as the matrices appearing in radiotherapy exhibit a distribution closely resembling an exponential curve. Consequently, the number of elements exceeding any given threshold remains significantly small relative to the total matrix size. This phenomenon occurs because radiation delivered from each individual beamlet (corresponding to a matrix column) directly deposits radiation to a limited number of voxels (corresponding to matrix rows), resulting in a few large matrix entries. However, radiation also scatters, delivering smaller doses to additional voxels throughout the body, leading to small values across all voxels. In contrast to the approach by Arora et al. [3], which involves substituting all minor non-zeroed-out entries with $\\pm c$ , our strategy seeks to counterbalance the effects of sparsification on a row-by-row basis by redistributing the sum of all minor elements within that row. This method is designed to preserve the integrity of each row, taking into consideration the diverse distribution patterns across the matrix\u2019s rows. Specifically, in our application, rows corresponding to tumor voxels often exhibit higher values due to their position in the direct path (\"cross-fire\") of radiation, receiving more substantial doses. Applying a uniform value for all substituted entities, in this context, results in a more pronounced approximation for tumor voxels, potentially compromising the accuracy of the final dose delivered to the tumor, as our results will demonstrate. After isolating the larger elements, we apply $\\ell_{1}$ sampling to the remaining entries in each row independently. This process generates a random matrix, essentially the aggregate of several random matrices, which more closely aligns with the practices of the second category. However, unlike conventional sampling methods (e.g., [7]), our technique automatically identifies the optimal number of samples for each row, consistently less than the total number of columns, eliminating the necessity for tuning sampling parameters. ", "page_idx": 3}, {"type": "text", "text": "Our method is nearly as fast as that proposed by Arora et al. [3] but provides superior accuracy, particularly beneficial when high levels of sparsification needed. This is crucial in our application, where approximately $96{-}98\\%$ sparsification is required to solve the large-scale non-linear constraint optimization problems within a feasible clinical timeframe. Our algorithm, described in Algorithm 1, is named the Randomized Minor-value Rectification (RMR), as it preserves the larger values while rectifying a random selection of smaller values to offset those that have been zeroed out. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Randomized Minor-value Rectification (RMR) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "input $A\\in\\mathbb{R}^{m\\times n}$ : The matrix to be sparsified, $\\epsilon$ : The threshold for sparsification   \noutput $S$ : The sparsified matrix $S=$ deep copy of $A$ for each row $i$ in $\\{1,2,\\ldots,m\\}$ do $T_{i}=\\left\\{j\\mid0<|\\dot{a}_{i j}|\\leq\\epsilon\\right\\}$ set $s_{i j}=0$ for all $j\\in T_{i}$ $\\begin{array}{r}{\\Sigma_{i}=\\sum_{j\\in T_{i}}|a_{i j}|}\\end{array}$ $k_{i}=\\lceil\\Sigma_{i}/\\epsilon\\rceil$ for $t=1,2,\\ldots,k_{i}$ do randomly select $j\\in\\mathbf{\\nabla}T_{i}$ (with probability proportional to $|a_{i j}|)$ and update $s_{i j}~\\gets~s_{i j}~+$ $\\Sigma_{i}/k_{i}\\times\\mathrm{{sign}}(a_{i j})$ end for end for ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.1. For any matrix $A\\in\\mathbb{R}^{m\\times n}$ the number of samples taken by the RMR algorithm is bounded above by $\\mathrm{nnz}(A)$ . ", "page_idx": 4}, {"type": "text", "text": "The $\\mathrm{nnz}(A)$ upper bound in the above lemma, which also applies to the algorithm proposed by Arora et al. [3], may not seem interesting at first; however, the number of samples could be much larger than $\\mathrm{nnz}(A)$ for other algorithms, as each entry of the matrix could be sampled multiple times. The following theorems provide the theoretical guarantees of the RMR algorithm (proof in Appendix A). ", "page_idx": 4}, {"type": "text", "text": "In the remainder of this section, we assume that the sparse matrix $S$ is obtained by applying Algorithm 1 to $A$ . Instead of solving the original optimization problem 2 directly, we substitute the dense matrix $A$ with the sparse matrix $S$ , thereby formulating an approximated optimization problem that we refer to as the surrogate problem. Let $x_{A}$ and $x_{S}$ denote arbitrary optimal points for the original and surrogate optimization problems, respectively. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2 (Absolute $\\ell_{2}$ -norm error). Given a matrix $A\\in\\mathbb{R}^{m\\times n}$ , it can be shown that: ", "page_idx": 4}, {"type": "text", "text": "(a) The number of nonzero entries of $S$ is less than $m+\\|A\\|_{1}/\\epsilon$ . ", "page_idx": 4}, {"type": "text", "text": "(b) For any given $\\delta,\\tau>0,$ , by setting ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\epsilon=\\frac{\\tau\\left(\\sqrt{1+\\frac{9\\operatorname*{max}(m,n-1)}{\\log\\frac{m+n}{\\delta}}}-1\\right)}{3\\sqrt{2}\\operatorname*{max}(m,n-1)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "it follows that $\\mathbb{P}\\left\\{\\|A-S\\|_{2}\\geq\\tau\\right\\}\\leq\\delta$ . ", "page_idx": 4}, {"type": "text", "text": "Corollary 3.3 (Absolute $\\ell_{2}$ -norm error). In Algorithm $^{\\,l}$ , by setting ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\epsilon=\\frac{\\tau}{4\\sqrt{\\operatorname*{max}(m,n-1)\\log(m+n)}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "the resultant matrix $S$ will contain no more than $m\\,+\\,4\\sqrt{\\operatorname*{max}(m,n-1)\\log(m+n)}\\,\\|A\\|_{1}\\,/\\tau$ nonzero entries, and we have $\\mathbb{P}\\left\\{\\|A-S\\|_{2}\\geq\\tau\\right\\}\\leq1/(m+\\stackrel{\\cdot}{n})$ . ", "page_idx": 4}, {"type": "text", "text": "Theorems 3.6 and 3.9 establish theoretical guarantees and provide bounds on the discrepancies between the original and surrogate optimization problems with respect to their constraints and objective functions, respectively. In essence, applying the RMR algorithm to sparsify the matrix and subsequently solving the surrogate problem yields a near-optimal solution to the original problem. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.4 (Row-wise error). For every vector $x\\in\\mathbb{R}^{n}$ , the probability that the abso\u221alute value of the $i$ -th entry of the vector $(A-S)x$ exceeds $c\\epsilon\\left\\|x\\right\\|_{2}$ is less than $2\\exp(-c^{2}/(4+2{\\sqrt{2}}c/3))$ ), i.e., ${\\ensuremath{\\mathbb P}}\\left\\{|((A-S)x)_{i}|\\ge c\\epsilon\\,\\|x\\|_{2}\\right\\}\\le2\\exp(-c^{2}/(4+2\\sqrt{2}c/3))$ . ", "page_idx": 4}, {"type": "text", "text": "Note that the upper bound obtained from Lemma (3.4) is significantly tighter than the bound derived directly from Corollary (3.3). To understand this, consider the following inequality: ", "page_idx": 5}, {"type": "equation", "text": "$$\n|((A-S)x)_{i}|\\leq\\|(A-S)x\\|_{2}\\leq\\|A-S\\|_{2}\\|x\\|_{2}\\leq4\\sqrt{\\operatorname*{max}(m,n-1)\\log(m+n)}\\epsilon\\|x\\|_{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with probability at least $1-1/(m+n)$ , as stated in (3.3). However, if we set $c=5\\log(m+n)$ , we achieve the same failure probability, but the upper bound on the error becomes $5\\log(m+n)\\epsilon\\|x\\|_{2},$ which is of a better order. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.5 (Feasibility gap). Let $C\\in\\mathbb{R}^{k\\times m}$ be a normalized matrix where the $l_{1}$ -norm of each row is less than or equal to one, and assume that for an arbitrary $x\\in\\mathbb{R}^{n}$ , there exists $u\\in\\dot{\\mathbb{R}}^{k}$ for which we have $C S x\\leq u$ . Then, with a probability of 0.95, the maximum violation of any constraint in $C A x\\leq u$ is bounded above by $(19+5\\log m)\\epsilon\\|x\\|_{2}$ . Conversely, i $f C A x\\leq u,$ , then the maximum violation of any constraint in $C S x\\leq u$ is bounded above by $(19+5\\log m)\\epsilon\\|x\\|_{2}$ with a probability of 0.95. ", "page_idx": 5}, {"type": "text", "text": "The feasible region of the optimization problem (2) satisfies the assumptions of Lemma 3.5, leading to the following theorem. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.6 (Feasibility gap for Problem 2). An optimal point of the original problem, $x_{A}$ , violates each constraint of the surrogate problem by no more than $(19+5\\log m)\\epsilon\\|x\\|_{2}$ with a probability of at least 0.95. Conversely, an optimal point of the surrogate problem, $x_{S}$ , violates each constraint of the original problem by no more than $(19+5\\log m)\\epsilon\\|x\\|_{2}$ with the same probability. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.7 (Objective function discrepancy). The absolute discrepancy between the objective functions of the original and surrogate problems does not exceed $\\begin{array}{r}{e\\left(f_{0}(\\bar{A x^{}})+m(\\bar{1}+e)\\sum_{s\\in\\bar{S}}\\bigcap_{\\epsilon\\neq}+w_{-}^{s}\\right)}\\end{array}$ with $a$ probability of at least 0.95, where $e=(19+5\\log m)\\epsilon\\|x\\|_{2}$ . ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.8. Suppose that for an $x\\in\\mathbb{R}^{n}$ satisfying the convex constraints $h(x)\\leq0,$ , the maximum violation of any convex constraint $g_{i}\\leq0$ in the optimization problem (1) is bounded above by $e\\geq0$ , i.e., $g_{i}(A x)\\leq e$ . Additionally, assume there exists an interior point of the feasible set $\\tilde{x}\\in\\mathbb{R}^{n}$ that is strictly feasible by a margin of at least $s>0$ for each constraint $g_{i}\\leq0$ , i.e., $g_{i}(A\\tilde{x})\\leq-s$ . Then, there exists a feasible point $\\hat{x}\\in\\mathbb{R}^{n}$ such that: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|x-{\\hat{x}}\\|\\leq{\\frac{e}{s}}(\\|x\\|+\\|{\\tilde{x}}\\|)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 3.9 (Sub-optimality gap for Problem 2). An optimal point of the surrogate problem, $x_{S}$ , is $a$ near-optimal solution to the original problem with a probability of at least 0.95, and the sub-optimality gap of $O(e)$ , where ", "page_idx": 5}, {"type": "equation", "text": "$$\ne=(19+5\\log m)\\,\\epsilon\\,\\operatorname*{max}\\left(\\|x_{A}\\|_{2},\\|x_{S}\\|_{2}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In other words, we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\n[f_{0}(A x_{S})+f_{1}(x_{S})]-[f_{0}(A x_{A})+f_{1}(x_{A})]=O(e).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark 3.10. Note that the proof of Theorem (3.9) can be generalized to any matrix approximation scheme with a bounded error norm for the general convex optimization problem (1), given the key assumptions that the objective function is Lipschitz continuous and that there exists a strictly feasible point with a lower bounded slackness for all approximated constraints. These assumptions are crucial. If the objective function is not Lipschitz continuous, the error in matrix approximation can significantly alter the value of the objective function, thereby drastically affecting the optimal point. Additionally, if the assumption of Lemma (3.8) is not satisfied and no interior point exists for the approximated constraints, the feasible set could be reduced to a single point in extreme cases. Even with an arbitrarily small error in approximating the constraints, the feasible set might vanish, rendering the approximated problem infeasible. ", "page_idx": 5}, {"type": "text", "text": "Now we conduct a comparative analysis of the performance of the RMR algorithm against the algorithms proposed by Arora et al. [3], Drineas and Zouzias [7], Achlioptas et al. [2], and Braverman et al. [4] denoted as \u201cAHK06\u201d, \u201cDZ11\u201d, \u201cAKL13\u201d, and \u201cBKKS21\u201d respectively. Table 1 presents the sparsity of the resulting matrix $S$ for each considered method, adhering to the constraint $\\|\\mathbf{\\bar{{A}}}-S\\|_{2}\\leq$ $\\tau$ . The computational time required to solve the constrained optimization Problem 2 is strongly correlated with the sparsity of the matrix, as also verified by our computational experiments (see Figure 1). Therefore, we use sparsity as a proxy for computational efficiency. Although the values in Table 1 are not directly comparable, for DZ11, AKL13, and BKKS21 the number of non-zero elements scales inversely with the square of the error rate (i.e., $\\scriptstyle{\\frac{1}{\\tau^{2}}}\\,\\!{\\mathrm{,}}$ ), while for RMR and AHK06, it is determined by the inverse of the error rate (i.e., $\\frac{1}{\\tau}$ ). As also confirmed by our computational experiments (Figure 1), this makes RMR and AHK06 algorithms particularly beneficial at lower error rates, which is needed for our application. Furthermore, due to the dependence of the BKKS21 on the numerical sparsity of the matrix, and considering that the matrices we tested exhibit low numerical sparsity, this algorithm also performs well across various metrics. In terms of the runtime and computational complexity of the algorithms themselves, it mainly depends on the number of required samplings. For DZ11 and AKL13, the number of required samplings also depends on the error rate, while for AHK06, BKKS21, and RMR, the dependency is solely on $\\mathrm{nnz}(A)$ . This also gives AHK06, BKKS21, and RMR an edge in runtime, particularly for small error rates, as confirmed by our experiments in Figure 1. ", "page_idx": 5}, {"type": "table", "img_path": "ItzD2Cnu9y/tmp/b1864a0b58aecc995d16fbf14aea2f2ab845076f2163c4afc5c076087a2dcc9e.jpg", "table_caption": ["Table 1: Comparison of Theoretical Guarantees Across Various Algorithms. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our objective is to illustrate that employing a randomized sparse sketch of the large dose influence matrix in radiotherapy markedly surpasses the existing \u201cnaive\" method used in practice, which simply sparsifies the matrix by neglecting all minor elements. Additionally, we demonstrate the superior performance of our RMR algorithm over the randomized sketch techniques introduced by Arora et al. [3] (\u201cAHK06\u201d), Drineas and Zouzias [7] (\u201cDZ11\u201d), Achlioptas et al. [2] (\u201cAKL13\u201d) and Braverman et al. [4] (\u201cBKKS21\u201d). ", "page_idx": 6}, {"type": "text", "text": "Dataset. Our analysis utilized real-world data recently made publicly available through the opensource package PortPy [10]. We conducted experiments on data from 10 randomly selected lung patients, with detailed information provided in the Appendix (Table 2). The dose influence matrices in PortPy were derived from an FDA-approved commercial treatment planning system, Varian EclipseTM, using its Application Programming Interface (API). Further detailed information about the data can be found on PortPy\u2019s GitHub page. Furthermore, we conducted experiments using data from five prostate patients, which are not publicly available. The results of these experiments are included in the Appendix. ", "page_idx": 6}, {"type": "text", "text": "Experiment Settings. Due to the inherent randomness of all the algorithms, except the naive one, each experiment was repeated 5 times to assess and report variations and the algorithms\u2019 robustness. Each algorithm has a hyper-parameter threshold that determines the sparsity of the output matrix. To ensure a fair comparison, we ran each algorithm with various threshold values and compared the results based on different levels of sparsity in the output matrix (e.g., runtime of algorithm 1 vs. algorithm 2 for sparsity levels x, y). The experiments were conducted on a dual CPU system (Intel(R) Xeon(R) 6248 2.5GHz) running Windows 10 with 128 GB of RAM. For each patient, the optimization problem 2 was solved using the penalty weights, $w_{+}^{s}/w_{-}^{s}$ , recommended by the PortPy package [10], along with the maximum and mean dose constraints presented in Table 3 in Appendix. All optimization problems were modeled and solved using CVXPY [5] and MOSEK [13]. To report the optimality gap, we needed to solve the original optimization problems using the original matrix $A$ . However, we encountered limited memory errors on the PC. Consequently, we solved the original optimization problems on a powerful high-performance computing (HPC) system with approximately ", "page_idx": 6}, {"type": "text", "text": "320 GB of memory, which was used exclusively to obtain the optimal solution $x_{A}$ for optimality gap comparisons. ", "page_idx": 7}, {"type": "text", "text": "Code availability. Our code, which includes implementations of all models and experiment configurations, is available at https://github.com/PortPy-Project/CompressRTP. ", "page_idx": 7}, {"type": "text", "text": "4.1 Evaluation Metrics ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The algorithms are compared using the following metrics: ", "page_idx": 7}, {"type": "text", "text": "Relative $\\ell_{2}$ -Norm Error. Defined as $\\left\\|A-S\\right\\|_{2}/\\left\\|A\\right\\|_{2}$ , this metric, widely adopted in existing literature, indicates how closely the sparse matrix approximates the original matrix for a given level of sparsity with respect to the spectral norm. ", "page_idx": 7}, {"type": "text", "text": "Relative Optimally Gap. This metric quantifies the sub-optimality resulting from solving the approximated problem by measuring the relative difference between the optimal objective value of the original problem (solved on HPC) and the objective value of the original problem using the optimal solution of the approximated problem. It is defined as ( $\\underbrace{\\cdot(f_{0}(A x_{A})\\!+\\!f_{1}(\\overline{{{x}}}_{A}))\\!-\\!(f_{0}(A x_{S})\\!+\\!f_{1}(\\overline{{{x}}}_{S}))}_{f_{\\sim}(\\textit{A m},\\textit{V o},\\textit{N})})$ $f_{0}(A x_{A})\\!+\\!f_{1}(x_{A})$ ", "page_idx": 7}, {"type": "text", "text": "Feasibility Gap. This metric measures how much the constraints of the original optimization problem (i.e., $g(A x)\\leq0)$ are violated when solving the approximated problem. It is defined as $||\\operatorname*{max}(g(A x_{S}),0)||_{2}$ . ", "page_idx": 7}, {"type": "text", "text": "Relative Dose Discrepancy. This metric, which is particularly relevant in the context of radiotherapy applications, is defined a s \u2225Ax\u2225SA\u2212xSS\u2225x2S\u22252and quantifies the discrepancy between the radiation dose computed using the sparse optimization problem, $S x_{S}$ , and the actual dose received by the patient, AxS. ", "page_idx": 7}, {"type": "text", "text": "Runtime. This includes the runtime of the sparsification algorithms and the runtime of the constrained optimization problems. It is worth noting that, in practice, the constrained optimization problem needs to be solved multiple times for each patient, depending on the technique used to adjust the hyper-parameter penalty weight. ", "page_idx": 7}, {"type": "text", "text": "Discrepancies in Dose Volume Histograms (DVH). DVH plots are two-dimensional graphs specific to radiotherapy applications and are reported in the Appendix (Figures 7 and 8). These plots, extensively used by clinicians, show how much radiation is delivered to different volumes of organs (e.g., 10 Gray is delivered to at least $30\\%$ of the esophagus). Discrepancies between these plots for $A x_{S}$ and $S x_{S}$ are illustrated for different algorithms. ", "page_idx": 7}, {"type": "text", "text": "4.2 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Figure 1 provides detailed comparisons with respect to the different levels of sparsity in the output sparse matrix for one patient. Additional comparisons for the remaining nine patients can be found in the Appendix (Figures 4 to 6). The Naive algorithm is excluded in some of the plots due to its poor performance and significant deviation from other methods (impeding a meaningful comparison of others). The incomplete range coverage in the plots for DZ11 is due to this algorithm exceeding our predefined maximum time limit of 10 minutes for some of its threshold values. The standard deviation band is plotted for all metrics (over 5 runs), however, due to the robust performance of the algorithms, the bands are not visible except in the feasibility gap plot. ", "page_idx": 7}, {"type": "text", "text": "As demonstrated in Figure 1 (top-left plot), all randomized algorithms surpass the current naive approach in balancing accuracy and sparsity. Among these, AHK06, BKKS21, and RMR stand out, exhibiting superior performance. While AHK06 and BKKS21 marginally surpasses RMR in terms of the $\\ell_{2}$ norm, RMR significantly exceeds AHK06 and BKKS21 in terms of feasibility and optimality gaps as well as the dose discrepancy error. This is further supported by reduced discrepancies among DVH plots reported in the Appendix (Figures 7 and 8). ", "page_idx": 7}, {"type": "text", "text": "In terms of computational efficiency, the naive approach unsurprisingly leads in speed due to its simple implementation, with AHK06 following closely behind. However, the relatively longer runtime ", "page_idx": 7}, {"type": "image", "img_path": "ItzD2Cnu9y/tmp/1614f869eb13b42b6522d559bb4938f977fc820fd936b59364b747505d2966d0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 1: (One patient, various sparsification levels) The performance of different algorithms in terms of: relative $\\ell_{2}$ -norm error, relative optimality gap, feasibility gap, relative dose discrepancy, algorithm runtime, and optimization runtime, for a lung patient. ", "page_idx": 8}, {"type": "image", "img_path": "ItzD2Cnu9y/tmp/7e0255386960309fdffbe8c65adc20a1fde1df553f8d93248f99246f3c704c55.jpg", "img_caption": ["Figure 2: (Ten patients, one sparsification level) The performance of different algorithms in terms of: relative $\\ell_{2}$ -norm error, relative optimality gap, feasibility gap, and algorithm runtime, for ten lung patients. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "of the RMR algorithm should not be seen as a significant limitation. The primary computational challenge lies in solving the optimization problem 2, which often needs to be solved multiple times for hyper-parameter tuning. For example, to achieve a relative optimality gap of $35\\%$ for patient 1, the RMR algorithm necessitates a sparse matrix with $98.90\\%$ relative sparsification (approximately 2,850,000 non-zero elements), in contrast to the $95.05\\%$ sparsification (about 12,750,000 non-zero elements) required by AHK06. Consequently, solving the optimization problem 2 with RMR\u2019s sparse sketch is considerably faster, taking 57 seconds, compared to 200 seconds for the sparse sketch produced by AHK06. The bottom-right plot illustrates the strong correlation between the number of non-zero elements in the matrix and the computational time of the constrained optimization problem, indicating that relative sparsification serves as an excellent proxy for the computational time of the optimization problems. ", "page_idx": 8}, {"type": "text", "text": "Figure 2 offers a high-level comparison across all patients at a fixed relative sparsity level of $98\\%$ , which can also be interpreted as a fixed computational time for the constrained optimization problem. Confirming the results of Figure 1 for more patients, this figure demonstrates the superior performance of AHK06 in terms of the $\\ell_{2}$ norm error and algorithm runtime, while highlighting the significant advantage of the RMR algorithm in reducing optimality and feasibility gaps. ", "page_idx": 8}, {"type": "image", "img_path": "ItzD2Cnu9y/tmp/bfe6c463b0a8ffe5028aa8280f57882ddce5548e18a56b44acf639bd979d6381.jpg", "img_caption": ["Figure 3: Radiation dose maps: Naive (left), RMR (middle) and dose labels (right) "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 3 presents a qualitative comparison between treatment plans generated using the naive approach (left) and RMR (middle), based on the radiation dose map commonly used by clinicians. The dose map visually represents the distribution of radiation, with color-coding overlaid on a medical image. Ideally, the high-dose regions (shown in red) should conform closely to the tumor\u2019s shape, with minimal radiation spillover into surrounding healthy tissues. While clinical expertise is required for a detailed interpretation, it is clear that using the RMR sparse matrix, as opposed to the naive sparse matrix, results in reduced radiation exposure to the right lung (visible on the left side of the figure). ", "page_idx": 9}, {"type": "text", "text": "4.3 Limitations and Broader Impacts ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We acknowledge that this study primarily focused on leveraging advanced matrix sparsification techniques to accurately and efficiently solve the computationally intensive optimization problems in cancer radiotherapy. While we have presented dose maps and DVH figures to illustrate potential improvements in treatment plan quality, our analysis remains limited. Future studies are necessary to comprehensively evaluate the clinical benefits of our techniques. Additionally, this study was reviewed by machine learning experts who focused on the technical aspects rather than the clinical implications of the study. Another limitation of this study is that the theoretical bounds for feasibility and optimality gaps are provided for the quadratic optimization problem 2, not the general problem 1. It can be proven that, by assuming Lipschitz continuity for the functions $f_{0}$ and $g$ , a small $||A-S||_{2}$ ensures small feasibility and optimality gaps. However, the bounds will depend on the Lipschitz constants of $f_{0}$ and $g$ . ", "page_idx": 9}, {"type": "text", "text": "Although our work primarily focused on Intensity Modulated Radiation Therapy (IMRT) with photon radiation, a widely used treatment modality, our approach could have a much broader impact in the field of radiotherapy. This is because all treatment modalities eventually boil down to solving constrained optimization problems with large and dense matrices. As technology advances and new digital machines with greater flexibility, such as those allowing couch movement, become available, the resultant optimization problems are becoming larger, making efficient approximation techniques increasingly essential to solve these problems within clinical timeframes [9, 11]. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduced a novel and high-impact application of matrix sparsification alongside an innovative algorithm that combines desirable theoretical guarantees with superior experimental performance. Our algorithm creates a highly sparse, randomized sketch of the original dose influence matrix used in radiotherapy optimization, preserving essential information. This enables solving the large-scale radiotherapy optimization problems within clinically viable timeframes, ensuring minimal discrepancy between the optimized radiation dose and the actual dose received by patients. It effectively tackles the \"garbage-in-garbage-out\" problem prevalent in current radiotherapy optimization caused by reliance on inaccurately sparsified matrices from the current \u201cnaive\" approaches. This method holds significant promise for enhancing the quality of radiotherapy treatments. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Gourav Jhanwar and Masoud Zarepisheh gratefully acknowledge funding from the MSK Cancer Center Support Grant/Core Grant from the NIH (P30 CA008748). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Dimitris Achlioptas and Frank McSherry. Fast computation of low rank matrix approximations. In Proceedings of the Thirty-Third Annual ACM Symposium on Theory of Computing, STOC \u201901, page 611\u2013618, New York, NY, USA, 2001. Association for Computing Machinery.   \n[2] Dimitris Achlioptas, Zohar S Karnin, and Edo Liberty. Near-optimal entrywise sampling for data matrices. In Advances in Neural Information Processing Systems, volume 26, pages 1565\u20131573. Curran Associates, Inc., 2013. [3] Sanjeev Arora, Elad Hazan, and Satyen Kale. A fast random sampling algorithm for sparsifying matrices. In Proceedings of the 9th International Conference on Approximation Algorithms for Combinatorial Optimization Problems, and 10th International Conference on Randomization and Computation, page 272\u2013279. Springer-Verlag, 2006. [4] Vladimir Braverman, Robert Krauthgamer, Aditya R. Krishnan, and Shay Sapir. Near-optimal entrywise sampling of numerically sparse matrices. In Mikhail Belkin and Samory Kpotufe, editors, Proceedings of Thirty Fourth Conference on Learning Theory, volume 134 of Proceedings of Machine Learning Research, pages 759\u2013773. PMLR, 15\u201319 Aug 2021.   \n[5] Steven Diamond and Stephen Boyd. Cvxpy: A python-embedded modeling language for convex optimization. Journal of Machine Learning Research, 17(83):1\u20135, 2016.   \n[6] Felix C Difliippo. Forward and adjoint methods for radiotherapy planning. Medical Physics, 25 (9):1702\u20131710, 1998.   \n[7] Petros Drineas and Anastasios Zouzias. A note on element-wise matrix sparsification via a matrix-valued bernstein inequality. Inf. Process. Lett., 111(8):385\u2013389, 2011.   \n[8] Alex Gittens and Joel A Tropp. Error bounds for random matrix approximation schemes. arXiv preprint arXiv:0911.4108, 2009.   \n[9] Charles Huang, Yong Yang, and Lei Xing. Fully automated noncoplanar radiation therapy treatment planning. Medical Physics, 48(11):7439\u20137449, 2021.   \n[10] Gourav Jhanwar, Mojtaba Tefagh, Vicki T Taasti, Sadegh R Alam, Seppo Tuomaala, Saad Nadeem, and Masoud Zarepisheh. Portpy: An open-source python package for planning and optimization in radiation therapy including benchmark data and algorithms. AAPM 65th Annual Meeting & Exhibition, 2023.   \n[11] Angelia Landers, Daniel O\u2019Connor, Dan Ruan, and Ke Sheng. Automated $4\\pi$ radiotherapy treatment planning with evolving knowledge-base. Medical physics, 46(9):3833\u20133843, 2019.   \n[12] H. Liu, D. Schaal, H. Curry, R. Clark, A. Magliari, P. Kupelian, D. Khuntia, and S. Beriwal. Review of cone beam computed tomography based online adaptive radiotherapy: current trend and future direction. Radiation oncology (London, England), 18(1), 2023.   \n[13] ApS MOSEK. Mosek optimizer api for python. Software Package, Ver, 9, 2020.   \n[14] Riley Murray, James Demmel, Michael W Mahoney, N Benjamin Erichson, Maksim Melnichenko, Osman Asif Malik, Laura Grigori, Piotr Luszczek, Micha\u0142 Derezin\u00b4ski, Miles E Lopes, et al. Randomized numerical linear algebra: A perspective on the field with an eye to software. arXiv preprint arXiv:2302.11474, 2023.   \n[15] NH Nguyen, Petros Drineas, and TD Tran. Matrix sparsification via the khintchine inequality. 2009.   \n[16] Mert Pilanci and Martin J Wainwright. Iterative hessian sketch: Fast and accurate solution approximation for constrained least-squares. Journal of Machine Learning Research, 17(53): 1\u201338, 2016.   \n[17] H Edwin Romeijn, James F Dempsey, and Jonathan G Li. A unifying framework for multicriteria fluence map optimization models. Physics in Medicine & Biology, 49(10):1991, 2004.   \n[18] H. Edwin Romeijn, Ravindra K. Ahuja, James F. Dempsey, and Arvind Kumar. A column generation approach to radiation therapy treatment planning using aperture modulation. SIAM Journal on Optimization, 15(3):838\u2013862, 2005.   \n[19] David M. Shepard, Michael C. Ferris, Gustavo H. Olivera, and T. Rockwell Mackie. Optimizing the delivery of radiation therapy to cancer patients. SIAM Review, 41(4):721\u2013744, 1999.   \n[20] Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics, 12(4):389\u2013434, 2011.   \n[21] David P Woodruff et al. Sketching as a tool for numerical linear algebra. Foundations and Trends\u00ae in Theoretical Computer Science, 10(1\u20132):1\u2013157, 2014.   \n[22] M. Zarepisheh, L. Hong, Y. Zhou, Q. Huang, J. Yang, G. Jhanwar, H. D. Pham, P. Dursun, P. Zhang, M. A. Hunt, G. S. Mageras, J. T. Yang, Y. Yamada, and J. O. Deasy. Automated and clinically optimal treatment planning for cancer radiotherapy. INFORMS journal on applied analytics, 52(1):69\u201389, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Proof of Lemma 3.1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Proof. The proof proceeds through straightforward analysis. Consider that for each row $i$ of matrix $A$ , the inequality $\\begin{array}{r}{\\dot{\\Sigma}_{i}=\\sum_{j\\in T_{i}}|\\dot{a_{i j}}|\\leq\\epsilon|\\bar{T}_{i}|}\\end{array}$ holds. Consequently, the number of samples $k_{i}$ for row $i$ satisfies $k_{i}\\leq|T_{i}|$ . Furthermore, $|T_{i}|$ is at most the count of non-zero entries in the $i$ -th row of $A$ . Summing over all rows, the total number of samples is indeed less than or equal to $\\mathrm{nnz}(A)$ . \u518f\u53e3 ", "page_idx": 12}, {"type": "text", "text": "A.2 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We employ the following theorem and lemmas to prove Theorem 3.2. The initial theorem establishes a probabilistic bound for the $\\ell_{2}$ norm of the aggregate of random matrices, serving as an essential instrument in various contexts. This theorem has also been utilized by Achlioptas et al. [2] and Drineas and Zouzias [7] to establish their theoretical guarantees. ", "page_idx": 12}, {"type": "text", "text": "Theorem A.1 (Matrix Bernstein [20]). Suppose that $X_{1},X_{2},\\ldots X_{t}$ are independent, zero mean, random matrices with dimensions $m\\times n$ . If for each $1\\leq i\\leq t,$ , $\\|X_{i}\\|_{2}\\leq R$ almost surely and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left(\\left\\|\\sum_{i=1}^{t}\\mathbb{E}\\left[X_{i}X_{i}^{\\top}\\right]\\right\\|_{2},\\left\\|\\sum_{i=1}^{t}\\mathbb{E}\\left[X_{i}^{\\top}X_{i}\\right]\\right\\|_{2}\\right)\\leq\\sigma^{2},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "then for all $\\tau\\geq0$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left\\{\\left\\Vert\\sum_{i=1}^{t}X_{i}\\right\\Vert_{2}\\geq\\tau\\right\\}\\leq(m+n)\\exp\\left(\\frac{-\\frac{\\tau^{2}}{2}}{\\sigma^{2}+\\frac{R\\tau}{3}}\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Lemma A.2. Suppose that $v\\ \\ =\\ \\ [v_{1}\\quad v_{2}\\quad\\cdot\\cdot\\cdot\\quad v_{n}]^{\\top}$ is a vector. Then the matrix $M\\;=\\;$ $\\|v\\|_{1}\\,\\mathrm{diag}(|v_{1}|,|v_{2}|,\\ldots,|v_{n}|)-v v^{\\top}$ is a positive semi-definite matrix. ", "page_idx": 12}, {"type": "text", "text": "Proof. It is sufficient to prove that for every vector $x=[x_{1}\\quad x_{2}\\quad\\cdot\\cdot\\cdot\\quad x_{n}]^{\\top},x^{\\top}M x\\geq0.$ . Notice that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\boldsymbol{x}^{\\top}\\boldsymbol{M}\\boldsymbol{x}=\\left\\|\\boldsymbol{v}\\right\\|_{1}\\sum_{i=1}^{n}\\left|v_{i}|x_{i}^{2}-\\left(x^{\\top}\\boldsymbol{v}\\right)^{2}=\\left(\\sum_{i=1}^{n}|v_{i}|\\right)\\left(\\sum_{i=1}^{n}|v_{i}|x_{i}^{2}\\right)-\\left(\\sum_{i=1}^{n}v_{i}x_{i}\\right)^{2}\\geq0.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The last inequality is a result of Cauchy\u2013Schwarz inequality because we can set $a_{i}=\\sqrt{|v_{i}|}$ for $v_{i}\\geq0$ , and $a_{i}=-\\sqrt{|v_{i}|}$ for $v_{i}\\leq0$ and $b_{i}=\\sqrt{|v_{i}|}x_{i}$ , in the below Cauchy-Schwarz inequality: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{n}a_{i}b_{i}\\right)^{2}\\leq\\left(\\sum_{i=1}^{n}a_{i}^{2}\\right)\\left(\\sum_{i=1}^{n}b_{i}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Lemma A.3. Let $\\alpha_{1},\\alpha_{2},\\ldots,\\alpha_{m}$ be positive real numbers, $v_{1},v_{2},\\ldots,v_{m}\\in\\mathbb{R}^{n}$ and $M\\in\\mathbb{R}^{n\\times n}$ . If $\\begin{array}{r}{M^{\\prime}=M-\\sum_{i=1}^{m}\\alpha_{i}v_{i}v_{i}^{\\top}}\\end{array}$ is a positive semi-definite matrix then $\\left\\|M^{\\prime}\\right\\|_{2}\\leq\\left\\|M\\right\\|_{2}$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. Since $M^{\\prime}$ is a positive semi-definite matrix, $\\|M^{\\prime}\\|_{2}=\\operatorname*{sup}_{\\|x\\|_{2}=1}x^{\\top}M^{\\prime}x$ . Note that ", "page_idx": 12}, {"type": "equation", "text": "$$\nx^{\\top}M^{\\prime}x=x^{\\top}M x-\\sum_{i=1}^{m}\\alpha_{i}\\left(x^{\\top}v_{i}\\right)^{2}\\leq\\left|x^{\\top}M x\\right|\\leq\\left\\|M\\right\\|_{2},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "since $M$ is a symmetric matrix. Hence $\\left\\|M^{\\prime}\\right\\|_{2}\\leq\\left\\|M\\right\\|_{2}$ . ", "page_idx": 12}, {"type": "text", "text": "Now, we offer a proof for Theorem 3.2. ", "page_idx": 12}, {"type": "text", "text": "Proof. Let $\\begin{array}{r}{M_{i}=\\sum_{j=1}^{n}|a_{i j}|}\\end{array}$ and define $E_{i}=\\{|a_{i j}|\\;|\\;1\\leq j\\leq n,|a_{i j}|>\\epsilon\\}$ . As per Algorithm 1, the sum of the elements in $E_{i}$ equals $M_{i}-\\Sigma_{i}$ . Consequently, the cardinality of $E_{i}$ , denoted by $|E_{i}|$ , is at most $(M_{i}-\\Sigma_{i})/\\epsilon$ . Additionally, the number of samples is $\\lceil\\Sigma_{i}/\\epsilon\\rceil$ . Therefore, the number of nonzero entries in the $i$ -th row is less than $M_{i}/\\epsilon+1$ . Consequently, the total number of non-zero (nnz) entries in $S$ is less than $\\begin{array}{r}{m+\\sum_{i=1}^{m}M_{i}/\\epsilon=m+\\|A\\|_{1}\\,\\bar{/}\\epsilon}\\end{array}$ . ", "page_idx": 13}, {"type": "text", "text": "Assume that in the $i$ -th row and during the $t^{\\th}$ -th iteration of the redistribution step in Algorithm 1, index $j$ is selected. Let $S_{i t}$ represent an $m\\times n$ matrix characterized by a single non-zero entry, $\\mathrm{sign}(a_{i j})\\cdot\\Sigma_{i}$ , located in the $i$ -th row and $j$ -th column. Now, let us zero out all entries of $A$ that are greater than $\\epsilon$ in absolute value and call the new matrix A\u02c6, and define $r_{i}^{\\top}$ the $i^{\\th}$ -th row of $\\hat{A}$ . It becomes apparent that $\\Sigma_{i}=\\lVert\\boldsymbol{r}_{i}\\rVert_{1}$ . Consequently, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nS=A-\\hat{A}+\\sum_{i=1}^{m}\\frac{1}{k_{i}}\\sum_{t=1}^{k_{i}}S_{i t}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then ", "page_idx": 13}, {"type": "equation", "text": "$$\n4-S=\\sum_{i=1}^{m}\\left(\\hat{A}_{(i)}-\\frac{1}{k_{i}}\\sum_{t=1}^{k_{i}}S_{i t}\\right)=\\sum_{i=1}^{m}\\left(\\frac{1}{k_{i}}\\sum_{t=1}^{k_{i}}\\hat{A}_{(i)}-\\frac{1}{k_{i}}\\sum_{t=1}^{k_{i}}S_{i t}\\right)=\\sum_{i=1}^{m}\\sum_{t=1}^{k_{i}}\\frac{1}{k_{i}}\\left(\\hat{A}_{(i)}-S_{i t}\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and given that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[S_{i t}\\right]_{i j}=\\frac{\\left\\vert\\hat{a}_{i j}\\right\\vert}{\\sum_{i}}\\mathrm{sign}(\\hat{a}_{i j})\\Sigma_{i}=\\hat{a}_{i j},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "we have $\\mathbb{E}\\left[S_{i t}\\right]=\\hat{A}_{(i)}$ . Now denote $(\\hat{A}_{(i)}-S_{i t})/k_{i}$ by $X_{i t}$ . Since $X_{i t}$ has just one nonzero row, $X_{i t}X_{i t}^{\\top}$ has just one nonzero entry which is on its diagonal. This entry is equal to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{1}{k_{i}^{2}}\\left((a_{i j}-\\Sigma_{i}\\mathrm{sign}(a_{i j}))^{2}+\\sum_{l\\neq j}^{n}a_{i l}^{2}\\right)=\\frac{1}{k_{i}^{2}}\\left(\\|r_{i}\\|_{2}^{2}-2|a_{i j}|\\Sigma_{i}+\\Sigma_{i}^{2}\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for some $1\\le j\\le n$ . Therefore $\\|X_{i t}\\|_{2}=\\sqrt{(\\|r_{i}\\|_{2}^{2}-2|a_{i j}|\\Sigma_{i}+\\Sigma_{i}^{2})/k_{i}^{2}}$ . Also ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{1}{k_{i}^{2}}\\left(\\|r_{i}\\|_{2}^{2}-2|a_{i j}|\\Sigma_{i}+\\Sigma_{i}^{2}\\right)}\\le\\sqrt{\\frac{\\epsilon^{2}}{\\Sigma_{i}^{2}}\\left(2\\Sigma_{i}^{2}\\right)}=\\sqrt{2}\\epsilon,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and due to Theorem A.1 one can let $R=\\sqrt{2}\\epsilon$ . ", "page_idx": 13}, {"type": "text", "text": "Note that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{m}\\sum_{t=1}^{k_{i}}\\mathbb{E}\\left[X_{i t}^{\\top}X_{i t}\\right]=\\sum_{i=1}^{m}k_{i}\\mathbb{E}\\left[X_{i1}^{\\top}X_{i1}\\right],\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "so we just need to calculate $k_{i}\\mathbb{E}\\left[X_{i1}^{\\top}X_{i1}\\right]$ , which is equal to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ddots\\mathbb{E}\\left[\\frac{1}{k_{i}^{2}}\\left(\\hat{A}_{(i)}^{\\top}-S_{i1}^{\\top}\\right)\\left(\\hat{A}_{(i)}-S_{i1}\\right)\\right]=\\frac{1}{k_{i}}\\left(\\hat{A}_{(i)}^{\\top}\\hat{A}_{(i)}-\\hat{A}_{(i)}^{\\top}\\mathbb{E}\\left[S_{i1}\\right]-\\mathbb{E}\\left[S_{i1}^{\\top}\\right]\\hat{A}_{(i)}+\\mathbb{E}\\left[S_{i1}^{\\top}S_{i1}\\right]\\right)}\\\\ &{\\phantom{\\ddots}=\\frac{1}{k_{i}}\\left(-\\hat{A}_{(i)}^{\\top}\\hat{A}_{(i)}+\\mathbb{E}\\left[S_{i1}^{\\top}S_{i1}\\right]\\right)}\\\\ &{\\phantom{\\ddots}=\\frac{1}{k_{i}}\\left(-r_{i}r_{i}^{\\top}+\\mathbb{E}\\left[S_{i1}^{\\top}S_{i1}\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Recall that $S_{i1}$ has just one nonzero entry, therefore $S_{i1}^{\\top}S_{i1}$ is a diagonal matrix. Also ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[S_{i1}^{\\top}S_{i1}\\right]_{j j}=\\frac{|\\hat{a}_{i j}|}{\\sum_{i}}\\Sigma_{i}^{2}=|\\hat{a}_{i j}|\\Sigma_{i}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "From Lemma A.2, one can deduce that $\\begin{array}{r}{\\sum_{i=1}^{m}\\sum_{t=1}^{k_{i}}\\mathbb{E}\\left[X_{i t}^{\\top}X_{i t}\\right]}\\end{array}$ is a positive semi-definite matrix and from Lemma A.3, its spectral norm is less than or equal to the spectral norm of $\\begin{array}{r}{\\sum_{i=1}^{m}\\mathbb{E}\\left[S_{i1}^{\\top}S_{i1}\\right]/k_{i}}\\end{array}$ , which is a diagonal matrix and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{m}\\frac{1}{k_{i}}\\mathbb{E}\\left[S_{i1}^{\\top}S_{i1}\\right]\\right)_{j j}=\\sum_{i=1}^{m}\\frac{\\lvert\\hat{a}_{i j}\\rvert\\Sigma_{i}}{k_{i}}\\leq\\sum_{i=1}^{m}\\lvert\\hat{a}_{i j}\\rvert\\epsilon\\leq m\\epsilon^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\sum_{i=1}^{m}\\sum_{t=1}^{k_{i}}\\mathbb{E}\\left[X_{i t}^{\\top}X_{i t}\\right]\\right\\|_{2}\\leq m\\epsilon^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "According to Theorem A.1, there should also be an upper bound for $\\begin{array}{r}{\\left\\|\\sum_{i=1}^{m}\\sum_{t=1}^{k_{i}}\\mathbb{E}\\left[X_{i t}X_{i t}^{\\top}\\right]\\right\\|_{2}}\\end{array}$ . Similarly, ", "page_idx": 14}, {"type": "equation", "text": "$$\nk_{i}\\mathbb{E}\\left[X_{i1}X_{i1}^{\\top}\\right]=\\frac{1}{k_{i}}\\left(-\\hat{A}_{(i)}\\hat{A}_{(i)}^{\\top}+\\mathbb{E}\\left[S_{i1}S_{i1}^{\\top}\\right]\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The resulting matrix consists of just a nonzero entry, the $i i-$ th entry of which is equal to $(-\\|r_{i}\\|_{2}^{2}+$ $\\Sigma_{i}^{~2})/k_{i}\\geq0$ . Therefore $\\begin{array}{r}{\\sum_{i=1}^{m}\\sum_{t=1}^{k_{i}}\\mathbb{E}\\left[X_{i t}X_{i t}^{\\top}\\right]}\\end{array}$ is a diagonal matrix. Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\displaystyle\\sum_{i=1}^{m}\\sum_{t=1}^{k_{i}}\\mathbb{E}\\left[X_{i t}X_{i t}^{\\top}\\right]\\right|_{2}=\\displaystyle\\operatorname*{max}_{1\\leq i\\leq n}\\left(\\displaystyle\\frac{1}{k_{i}}\\left(-\\|r_{i}\\|_{2}^{2}+\\Sigma_{i}^{2}\\right)\\right)}\\\\ &{\\leq\\displaystyle\\operatorname*{max}_{1\\leq i\\leq n}\\left(\\displaystyle\\frac{\\epsilon}{\\Sigma_{i}}\\left(-\\displaystyle\\frac{\\Sigma_{i}^{2}}{n}+\\Sigma_{i}^{2}\\right)\\right)}\\\\ &{=\\displaystyle\\operatorname*{max}_{1\\leq i\\leq n}\\left(\\displaystyle\\frac{(n-1)\\epsilon}{n}\\Sigma_{i}\\right)}\\\\ &{\\leq(n-1)\\epsilon^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "So one can let $\\sigma^{2}=\\operatorname*{max}(m,n-1)\\epsilon^{2}$ . ", "page_idx": 14}, {"type": "text", "text": "Building on Theorem A.1, the derivation of Theorem 3.2 becomes a straightforward computation. We need to have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\delta=(m+n)\\exp\\left(\\frac{-\\frac{\\tau^{2}}{2}}{\\operatorname*{max}(m,n-1)\\epsilon^{2}+\\frac{\\sqrt{2}\\epsilon\\tau}{3}}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\log\\left(\\frac{m+n}{\\delta}\\right)\\operatorname*{max}(m,n-1)\\epsilon^{2}+\\log\\left(\\frac{m+n}{\\delta}\\right)\\frac{\\sqrt{2}\\tau}{3}\\epsilon-\\frac{\\tau^{2}}{2}=0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The above equation has just one non-negative root. Therefore ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\epsilon\\!=\\!\\frac{\\sqrt{\\log^{2}\\left(\\frac{m+n}{\\delta}\\right)\\frac{2\\tau^{2}}{9}+2\\log\\left(\\frac{m+n}{\\delta}\\right)\\operatorname*{max}(m,n-1)\\tau^{2}-\\log\\left(\\frac{m+n}{\\delta}\\right)\\frac{\\sqrt{2}\\tau^{2}}{3}}}{2\\log\\left(\\frac{m+n}{\\delta}\\right)\\operatorname*{max}(m,n-1)}}\\\\ &{\\quad\\!=\\!\\frac{\\tau\\left(\\sqrt{1+\\frac{9\\operatorname*{max}(m,n-1)}{\\log\\left(\\frac{m+n}{\\delta}\\right)}}-1\\right)}{3\\sqrt{2}\\operatorname*{max}(m,n-1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.3 Proof of Corollary 3.3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. Notice that $\\sqrt{1+c}-1\\geq\\sqrt{c}/2$ for every $c\\ge16/9$ . In Theorem 3.2 let $\\delta=1/(m+n)$ . Then we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{r\\left(\\sqrt{1+\\frac{9\\operatorname*{max}(m,n-1)}{\\log\\left(\\frac{m+n}{\\delta}\\right)}}-1\\right)}{3\\sqrt{2}\\operatorname*{max}(m,n-1)}=\\frac{\\tau\\left(\\sqrt{1+\\frac{9\\operatorname*{max}(m,n-1)}{2\\log(m+n)}}-1\\right)}{3\\sqrt{2}\\operatorname*{max}(m,n-1)}\\geq\\frac{\\frac{\\tau}{2}\\sqrt{\\frac{9\\operatorname*{max}(m,n-1)}{2\\log(m+n)}}}{3\\sqrt{2}\\operatorname*{max}(m,n-1)}}\\\\ {=\\frac{\\tau}{4\\sqrt{\\operatorname*{max}(m,n-1)\\log(m+n)}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "since $9\\operatorname*{max}(m,n-1)/(2\\log(m+n))\\ \\geq\\ 16/9$ . Therefore based on Theorem 3.2 the result follows. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "A.4 Proof of Lemma 3.4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We apply the Bernstein inequality to establish the result. ", "page_idx": 14}, {"type": "text", "text": "Theorem A.4 (Bernstein inequality). Let $X_{1},\\ldots,X_{n}$ be independent zero-mean random variables. Suppose that $|X_{i}|\\leq M$ almost surely, for all $i$ . Then, for all positive $\\tau$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left\\{\\left|\\sum_{i=1}^{n}X_{i}\\right|\\geq\\tau\\right\\}\\leq2\\exp\\left(\\frac{-\\frac{\\tau^{2}}{2}}{\\sum_{i=1}^{n}\\mathbb{E}\\left[X_{i}^{2}\\right]+\\frac{M\\tau}{3}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now we present a proof for Lemma 3.4. ", "page_idx": 15}, {"type": "text", "text": "Proof. Analogous to the proof of Theorem 3.2, we define the $X_{i t}$ matrices. Let $y_{t}^{\\top}$ be the $i$ -th row of $X_{i t}$ . From the proof of Theorem 3.2, the $i$ -th row of $A-S$ is given by $\\textstyle\\sum_{t=1}^{k_{i}}y_{t}^{\\top}$ and $\\mathbb{E}\\left[y_{t}^{\\top}\\right]=0$ . Then, for each $x\\in\\mathbb{R}^{n}$ , we have $\\mathbb{E}\\left[y_{t}^{\\top}x\\right]=0$ and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}\\left[\\left(y_{t}^{\\top}x\\right)^{2}\\right]=\\mathbb{E}\\left[x^{\\top}y_{t}y_{t}^{\\top}x\\right]=\\mathbb{E}\\left[\\mathrm{tr}\\left(x^{\\top}y_{t}y_{t}^{\\top}x\\right)\\right]=\\mathbb{E}\\left[\\mathrm{tr}\\left(x x^{\\top}y_{t}y_{t}^{\\top}\\right)\\right]}\\\\ &{}&{=\\mathrm{tr}\\left(\\mathbb{E}\\left[x x^{\\top}y_{t}y_{t}^{\\top}\\right]\\right)}\\\\ &{}&{=\\mathrm{tr}\\left(x x^{\\top}\\mathbb{E}\\left[y_{t}y_{t}^{\\top}\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Based on the proof of Theorem 3.2, we know that $\\mathbb{E}\\left[y_{t}y_{t}^{\\top}\\right]\\,=\\,(-r_{i}r_{i}^{\\top}+Y)/k_{i}^{2}$ , where $Y$ is a diagonal matrix and the $j j$ -th entry of it equals $|\\hat{a}_{i j}|\\Sigma_{i}$ . Hence, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(y_{t}^{\\top}x\\right)^{2}\\right]=\\cfrac{1}{k_{i}^{2}}\\mathrm{~tr~}\\!\\left(-x x^{\\top}r_{i}r_{i}^{\\top}+x x^{\\top}Y\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\cfrac{1}{k_{i}^{2}}\\mathrm{~tr~}\\!\\left(-x^{\\top}r_{i}r_{i}^{\\top}x+x^{\\top}Y x\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\cfrac{1}{k_{i}^{2}}\\left(-(r_{i}^{\\top}x)^{2}+\\displaystyle\\sum_{j=1}^{n}|\\hat{a}_{i j}|\\Sigma_{i}x_{j}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\xi\\left[\\displaystyle\\sum_{i=1}^{k_{i}}\\left(y_{i}^{\\tau}x_{i}^{\\tau}\\right)^{2}\\right]=\\displaystyle\\frac{1}{k_{i}}\\left(-(r_{i}^{\\tau}x)^{2}+\\displaystyle\\sum_{j=1}^{n}\\left|\\hat{u}_{i j}\\big|\\big|\\mathcal{Z}_{i}x_{j}^{2}\\right)}\\\\ {=\\displaystyle\\frac{1}{k_{i}}\\left(-\\displaystyle\\sum_{j=1}^{n}\\hat{u}_{i j}^{2}x_{j}^{2}-2\\displaystyle\\sum_{1\\leq i<j\\leq n}\\hat{u}_{i j}\\hat{u}_{i j}x_{j}x_{i}+\\displaystyle\\sum_{j=1}^{n}\\hat{u}_{i j}^{2}x_{j}^{2}+\\displaystyle\\sum_{1\\leq i<k_{i}}\\left|\\hat{u}_{i j}\\hat{u}_{i i}\\right|\\alpha_{j}^{2}+x_{i}^{2}\\right)}\\\\ {=\\displaystyle\\frac{1}{k_{i}}\\displaystyle\\sum_{1\\leq i\\leq j\\leq n}|\\hat{u}_{i j}\\hat{u}_{i i}|\\alpha_{j}-\\mathrm{sign}(\\hat{u}_{i j}\\hat{u}_{i i})x_{i})^{2}}\\\\ {\\le\\displaystyle\\frac{\\epsilon}{\\displaystyle\\sum_{1\\leq i\\leq j\\leq n}}\\displaystyle\\sum_{1\\leq i\\leq j\\leq n}|\\hat{u}_{i j}\\hat{u}_{i i}|2(x_{j}^{2}+x_{i}^{2})}\\\\ {=\\displaystyle\\frac{2\\epsilon}{\\displaystyle\\sum_{1\\leq i,j\\leq n}|\\hat{u}_{i j}|\\alpha_{j}^{2}}\\displaystyle\\sum_{l\\neq j}^{n}|\\hat{u}_{i i}|}\\\\ {\\le2e^{2}\\displaystyle\\|\\alpha_{1}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Also, $|y_{t}^{\\top}x|\\leq\\|y_{t}\\|_{2}\\,\\|x\\|_{2}$ and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|y_{t}\\|_{2}=\\sqrt{\\frac{1}{k_{i}^{2}}\\left((a_{i j}-\\Sigma_{i}\\mathrm{sign}(a_{i j}))^{2}+\\displaystyle\\sum_{l\\neq j}^{n}a_{i l}^{2}\\right)}=\\sqrt{\\frac{1}{k_{i}^{2}}\\left(\\|r_{i}\\|_{2}^{2}-2|a_{i j}|\\Sigma_{i}+\\Sigma_{i}^{2}\\right)}}\\\\ {\\leq\\sqrt{\\frac{\\epsilon^{2}}{\\Sigma_{i}^{2}}(2\\Sigma_{i}^{2})}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\\\ {=\\sqrt{2}\\epsilon.~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now according to Theorem A.4 we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb P\\left\\{\\left|\\sum_{t=1}^{k_{i}}y_{t}^{\\top}x\\right|\\geq\\tau\\right\\}\\leq2\\exp\\left(\\frac{-\\frac{\\tau^{2}}{2}}{2\\epsilon^{2}\\left\\|x\\right\\|_{2}^{2}+\\frac{\\sqrt2\\epsilon\\left\\|x\\right\\|_{2}\\tau}{3}}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Setting $\\tau=c\\epsilon\\left\\|\\boldsymbol{x}\\right\\|_{2}$ results in ", "page_idx": 16}, {"type": "equation", "text": "$$\n2\\exp\\left(\\frac{-\\frac{\\tau^{2}}{2}}{2\\epsilon^{2}\\left\\|x\\right\\|_{2}^{2}+\\frac{\\sqrt{2}\\epsilon\\left\\|x\\right\\|_{2}\\tau}{3}}\\right)=2\\exp\\left(\\frac{-c^{2}}{4+\\frac{2\\sqrt{2}c}{3}}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "thereby completing the proof. ", "page_idx": 16}, {"type": "text", "text": "A.5 Proof of Lemma 3.5 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. Let $d_{A}$ and $d_{S}$ denote $A x$ and $S x$ , respectively. In Lemma 3.4 if we let $c=19+5\\log m$ , then we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\exp\\left(\\displaystyle\\frac{-c^{2}}{4+\\frac{2\\sqrt{2}c}{3}}\\right)\\le2\\exp\\left(\\displaystyle\\frac{-c^{2}}{4c+\\frac{2\\sqrt{2}c}{3}}\\right)=2\\exp\\left(\\displaystyle\\frac{-c}{4+\\frac{2\\sqrt{2}}{3}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=2\\exp\\left(\\displaystyle\\frac{-19-5\\log m}{4+\\frac{2\\sqrt{2}}{3}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad<\\displaystyle\\frac{0.05}{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|(d_{A}-d_{S})_{i}|\\ge(19+5\\log m)\\epsilon\\|x\\|_{2}\\right)<\\frac{0.05}{m}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Applying the union bound, with probability $\\textstyle1-m\\cdot{\\frac{0.05}{m}}=0.95$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|d_{A}-d_{S}\\|_{\\infty}<(19+5\\log m)\\epsilon\\|x\\|_{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now, if $c^{T}$ is any arbitrary row of $C$ , we have $\\|c\\|_{1}\\leq1$ . Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n|c^{T}(d_{A}-d_{S})|\\leq\\sum_{i=1}^{m}|c_{i}||(d_{A}-d_{S})_{i}|\\leq\\sum_{i=1}^{m}|c_{i}|\\|d_{A}-d_{S}\\|_{\\infty}\\leq\\|d_{A}-d_{S}\\|_{\\infty},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which proves the result. ", "page_idx": 16}, {"type": "text", "text": "A.6 Proof of Lemma 3.7 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. Consider ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{0}(A x)=\\sum_{s\\in\\bar{S}}\\sum_{i\\in I_{s}}\\left(w_{+}^{s}\\operatorname*{max}(A_{i}^{s}x-d^{s},0)^{2}+w_{-}^{s}\\operatorname*{max}(d^{s}-A_{i}^{s}x,0)^{2}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which is the term dependent on $A x$ in the objective function of the optimization problem (2). Note that the remaining part of the objective function is independent of $A$ and hence is unaffected by the approximation. ", "page_idx": 16}, {"type": "text", "text": "Let $e=(19+5\\log m)\\epsilon\\|x\\|_{2}$ . For all $s\\in{\\bar{S}}$ and $i\\in I_{s}$ , we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}(S_{i}^{s}x-d^{s},0)\\geq\\operatorname*{max}(A_{i}^{s}x-e-d^{s},0)\\geq\\operatorname*{max}(A_{i}^{s}x-d^{s},0)-e.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similarly, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}(S_{i}^{s}x-d^{s},0)\\leq\\operatorname*{max}(A_{i}^{s}x+e-d^{s},0)\\leq\\operatorname*{max}(A_{i}^{s}x-d^{s},0)+e.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n|\\operatorname*{max}(A_{i}^{s}x-d^{s},0)-\\operatorname*{max}(S_{i}^{s}x-d^{s},0)|\\leq e,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and, ", "page_idx": 16}, {"type": "equation", "text": "$$\n|\\operatorname*{max}(A_{i}^{s}x-d^{s},0)+\\operatorname*{max}(S_{i}^{s}x-d^{s},0)|\\leq2\\operatorname*{max}(A_{i}^{s}x-d^{s},0)+e.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Altogether, we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n|\\operatorname*{max}(A_{i}^{s}x-d^{s},0)^{2}-\\operatorname*{max}(S_{i}^{s}x-d^{s},0)^{2}|\\leq e(2\\operatorname*{max}(A_{i}^{s}x-d^{s},0)+e).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By a similar argument, we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n|\\operatorname*{max}(d^{s}-A_{i}^{s}x,0)^{2}-\\operatorname*{max}(d^{s}-S_{i}^{s}x,0)^{2}|\\leq e(2\\operatorname*{max}(d^{s}-A_{i}^{s}x,0)+e).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In conclusion, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f_{0}(A x)-f_{0}(S x)|\\leq\\displaystyle\\sum_{s\\in\\bar{S},i\\in I_{s}}e\\left(w_{+}^{s}(2\\operatorname*{max}(A_{i}^{s}x-d^{s},0)+e)+w_{-}^{s}(2\\operatorname*{max}(d^{s}-A_{i}^{s}x,0)+e)\\right)}\\\\ &{\\leq\\displaystyle\\sum_{s\\in\\bar{S},i\\in I_{s}}e\\left(w_{+}^{s}(\\operatorname*{max}(A_{i}^{s}x-d^{s},0)^{2}+1+e)+w_{-}^{s}(\\operatorname*{max}(d^{s}-A_{i}^{s}x,0)^{2}+1+\\varepsilon)\\right)}\\\\ &{\\leq e\\left(f_{0}(A x)+\\displaystyle\\sum_{s\\in\\bar{S},i\\in I_{s}}(w_{+}^{s}+w_{-}^{s})(1+e)\\right)}\\\\ &{\\leq e\\bigl(f_{0}(A x)+m(1+e)\\displaystyle\\sum_{s\\in\\bar{S}}(w_{+}^{s}+w_{-}^{s})\\bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, in summary, for any arbitrary $x\\in\\mathbb{R}^{n}$ , we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n|f_{0}(A x)-f_{0}(S x)|\\leq e\\big(f_{0}(A x)+m(1+e)\\sum_{s\\in\\bar{S}}(w_{+}^{s}+w_{-}^{s})\\big),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with probability at least 0.95. ", "page_idx": 17}, {"type": "text", "text": "A.7 Proof of Lemma 3.8 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. It suffices to consider: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{x}=\\frac{e}{s}\\tilde{x}+\\left(1-\\frac{e}{s}\\right)x.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For each convex constraint $g_{i}\\leq0$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{i}(A\\hat{x})=g_{i}\\left(A\\left(\\frac{e}{s}\\tilde{x}+(1-\\frac{e}{s})x\\right)\\right)}\\\\ &{\\qquad\\leq\\frac{e}{s}g_{i}(A\\tilde{x})+\\left(1-\\frac{e}{s}\\right)g_{i}(A x)}\\\\ &{\\qquad\\leq\\frac{e}{s}(-s)+\\left(1-\\frac{e}{s}\\right)e}\\\\ &{\\qquad\\leq-\\frac{e^{2}}{s}}\\\\ &{\\qquad<0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Additionally, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|x-\\hat{x}\\|=\\left\\|x-\\left(\\frac{e}{s}\\tilde{x}+\\left(1-\\frac{e}{s}\\right)x\\right)\\right\\|}\\\\ &{\\qquad\\quad=\\left\\|\\frac{e}{s}(x-\\tilde{x})\\right\\|}\\\\ &{\\qquad\\quad=\\frac{e}{s}\\|x-\\tilde{x}\\|}\\\\ &{\\qquad\\quad\\leq\\frac{e}{s}(\\|x\\|+\\|\\tilde{x}\\|).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This completes the proof. ", "page_idx": 17}, {"type": "text", "text": "A.8 Proof of Theorem 3.9 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. Let $s=\\mathrm{min}(\\mathrm{min}_{s}(d_{\\mathrm{Max}}^{s}),\\mathrm{min}_{s}(d_{\\mathrm{Mean}}^{s}))$ . Then, $\\tilde{x}=0$ satisfies the assumptions of Lemma (3.8) for the approximated optimization problem. Therefore, with a probability of 0.95, there exists a ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{0}(A x_{S})+f_{1}(x_{S})\\leq f_{0}(S x_{S})+f_{1}(x_{S})+e\\big(f_{0}(A x_{S})+m(1+e)\\displaystyle\\sum_{s\\in\\mathcal{S}}(w_{+}^{s}+w_{-}^{s})\\big)}\\\\ &{\\qquad\\qquad\\leq f_{0}(S\\hat{x}_{A})+f_{1}(\\hat{x}_{A})+e\\big(f_{0}(A x_{S})+m(1+e)\\displaystyle\\sum_{s\\in\\mathcal{S}}(w_{+}^{s}+w_{-}^{s})\\big)}\\\\ &{\\qquad\\qquad\\leq f_{0}(A\\hat{x}_{A})+f_{1}(\\hat{x}_{A})+e\\big(f_{0}(A x_{S})+m(1+e)\\displaystyle\\sum_{s\\in\\mathcal{S}}(w_{+}^{s}+w_{-}^{s})\\big)}\\\\ &{\\qquad\\qquad+\\,e\\big(f_{0}(A\\hat{x}_{A})+m(1+e)\\displaystyle\\sum_{s\\in\\mathcal{S}}(w_{+}^{s}+w_{-}^{s})\\big)}\\\\ &{\\qquad\\qquad\\leq f_{0}\\left(A(x_{A}+O(e))\\right)+f_{1}(x_{A}+O(e))+O(e)}\\\\ &{\\qquad\\qquad\\leq f_{0}\\left(A x_{A}\\right)+f_{1}(x_{A})+O(e)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The first and third lines follow from Lemma (3.7), the second line follows from the optimality of $x_{S}$ for the approximated optimization problem, and the forth line follows from $\\textstyle{\\frac{e}{s}}\\|x_{A}\\|=O(e)$ . Finally, the last line follows from the Lipschitz continuity of the objective function over the feasible set which is bounded. Altogether, the sub-optimality of the solution to the approximated optimization problem is $O(e)$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B Additional Experimental Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Dataset and problem formulation. Table 2 presents data for ten randomly selected lung patients, detailing the number of rows (i.e., patient\u2019s voxels), columns (i.e., machine\u2019s beamlets), and the count of non-zero elements. Each patient received a prescribed radiation dose of 60 Gray. The treatment plans involved 9 manually selected radiation beams, chosen by an expert clinical physicist and tailored to each patient\u2019s anatomy. Table 3 presents the maximum and mean dose constraints used in the optimization problem 2. All the data have been downloaded from PortPy. ", "page_idx": 18}, {"type": "table", "img_path": "ItzD2Cnu9y/tmp/8f6f83f1043252e26563a0f679056d1e7551f008334f93c861800683cca26a46.jpg", "table_caption": ["Table 2: Data size summary for the 10 patients in the study. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Optimization Runtime for Full and RMR Sparse Matrices. Table 4 provides a comparison between the optimization runtime when using the full matrix and when employing the RMR sparse matrix with $98\\%$ sparsity. The optimization using the full matrix was performed on a high-performance computing (HPC) system with approximately $320\\;\\mathrm{GB}$ of memory, while the RMR sparse matrix optimization was conducted on a system with $128\\;\\mathrm{GB}$ of RAM. Despite the more advanced hardware for the full matrix, the results demonstrate significant improvements in optimization time with the RMR sparse matrix, especially in higher-dimensional cases. Additionally, the execution time of the RMR algorithm is reported, indicating the overhead introduced by the sparsification process. ", "page_idx": 18}, {"type": "text", "text": "Table 3: Recommended clinical dose bounds for lung cancer patients. ", "page_idx": 19}, {"type": "table", "img_path": "ItzD2Cnu9y/tmp/93f7ea9c539eb1f3d6874fe516ad2ab5280de0b309c503b1c9b4af21b3fd6db5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Nevertheless, the total computational cost remains substantially lower compared to the full matrix approach. The primary computational bottleneck in this application stems from the optimization problem, where the computational time increases cubically with respect to the size of the input matrix. In contrast, the computational time for the RMR algorithm grows linearly. ", "page_idx": 19}, {"type": "table", "img_path": "ItzD2Cnu9y/tmp/85215c7d152b6ab786a97a0794b14cb397b5346d9d59442165a2340503012e77.jpg", "table_caption": ["Table 4: Comparison of Optimization Runtime Using the Full Matrix vs. the RMR Sparse Matrix $98\\%$ Sparsity) and RMR Algorithm Runtime "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Ten patients, fixed sparsification level. Table 7 presents the performance of different algorithms in terms of relative $\\ell_{2}$ -norm error, algorithm runtime, feasibility gap, and relative optimality gap for ten lung patients with a fixed sparsification level of $98\\%$ . This table provides a detailed expansion of the results shown in Figure 2. Better scores are highlighted in bold for easier visual comparison. Standard deviations are provided for all algorithms, except the naive algorithm, which is deterministic. As can be readily seen, the AHK06 algorithm excels in terms of $\\ell_{2}$ -norm error, while the RMR algorithm consistently outperforms in terms of feasibility and optimality gaps. The naive algorithm is the fastest due to its simple implementation. ", "page_idx": 19}, {"type": "text", "text": "Nine patients, various sparsification levels. Figures 4 to 6 expands the comparisons made for a single patient in Figure 1 to nine additional patients. These figures confirm that the results observed for one patient are consistent across other patients. ", "page_idx": 19}, {"type": "text", "text": "Dose Volume Histogram (DVH) comparisons. A DVH is a two-dimensional plot used in radiotherapy to depict the distribution of radiation dose within the tumor and the surrounding normal tissues (Figures 7 and 8). Each structure has its own curve on the DVH, where the horizontal axis represents the dose, and the vertical axis indicates the percentage of the volume receiving at least that dose. For instance, in the top-left figure for the lungs (i.e., left and right lungs), the solid line intersects the point (20,10), signifying that $10\\%$ of the lungs receives at least a 10 Gy radiation dose. By illustrating the volume of tissue exposed to various dose levels, DVHs help clinicians assess the uniformity and adequacy of dose coverage to the tumor and ensure that surrounding healthy tissues are spared as much as possible. This visualization is crucial for comparing and optimizing treatment plans, enabling effective tumor control while minimizing adverse effects on normal tissues. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Figures 7 and 8 showcase three DVH plots: the naive approach (left), representing current practice; the AHK06 algorithm (middle), representing the most competitive existing approach; and the proposed RMR algorithm (right). In each plot, dashed lines illustrate the approximated radiation dose, $S x_{S}$ , used in the optimization problem 2, while solid lines depict the actual radiation dose, $A x_{S}$ . While clinical expertise is necessary to fully interpret these curves, the gap between the solid and dashed lines indicates the discrepancies resulting from using the approximated matrix $S$ in the optimization problem 2, with a smaller gap being preferred. It is readily apparent that the gap is much smaller, especially for the tumor, when using the RMR algorithm, where the solid and dashed lines are closely aligned and often overlap, making the dashed lines nearly invisible. ", "page_idx": 20}, {"type": "text", "text": "Prostate Patients. Previously, we focused exclusively on lung patients; we now examine the results for five different prostate patients. As shown in Figures 9 and 10, the results are largely consistent with those for lung patients, with only a few minor differences. The proposed RMR method demonstrates a shorter runtime than AHK06 across all five patients. At high sparsity levels, such as $99\\%$ , however, AKL13 outperforms RMR in terms of the relative $\\ell_{2}$ -norm metric. Additionally, DZ11 exhibits a higher feasibility gap at lower sparsity levels, indicating that additional samples do not reduce the feasibility gap for this method as expected. Table 5 reveals that the dose influence matrices for prostate patients are considerably denser than those for lung patients, accounting for some observed differences in results. Finally, Figure 11 presents the DVH plots for these five patients, with three DVH plots for each patient: the left plot represents the naive approach, the middle plot shows the AHK06 algorithm, and the right plot displays the RMR algorithm. These findings are consistent with those observed for lung patients, except in the naive approach, which appears to outperform AHK06. This suggests that prior sparsification methods may not consistently outperform the naive approach within this application. ", "page_idx": 20}, {"type": "table", "img_path": "ItzD2Cnu9y/tmp/d03902bc54284d187ccbfc846a304e121dfd4339a1e63421204ed0bab91e7e0d.jpg", "table_caption": ["Table 5: Data size summary for the 5 prostate patients. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "ItzD2Cnu9y/tmp/a284ea29090b3aa95f77175360da7c3118be38907a90d9fc834962b3302cbb9f.jpg", "table_caption": ["Table 6: Recommended clinical dose bounds for prostate cancer patients. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "ItzD2Cnu9y/tmp/ef90cd3b19dba35bad45bb720e8a4863f74d770eca5435ce988b23357797c9f8.jpg", "table_caption": ["Table 7: The performance of different algorithms in terms of relative $\\ell_{2}$ -norm error, algorithm runtime, feasibility gap, and relative optimality gap for ten lung patients with a fixed sparsification level of $98\\%$ . "], "table_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "ItzD2Cnu9y/tmp/3c1455d6a355111899683ffbdee490a88c1fdd6081036e1f7802157317b80bd5.jpg", "img_caption": ["Figure 4: (Various sparsification levels, Lung patients 2-4) The performance of different algorithms in terms of: relative $\\ell_{2}$ -norm error, relative optimality gap, feasibility gap, relative dose discrepancy, algorithm runtime, and optimization runtime, for patients 2 (first two rows), 3 (second two rows), and 4 (third two rows). "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "ItzD2Cnu9y/tmp/d9d50d332b8df808c03f2c636ad08ad3ba11de2e05eb66de2a8bcea21fd6884a.jpg", "img_caption": ["Figure 5: (Various sparsification levels, Lung patients 5-7) The performance of different algorithms in terms of: relative $\\ell_{2}$ -norm error, relative optimality gap, feasibility gap, relative dose discrepancy, algorithm runtime, and optimization runtime, for patients 5 (first two rows), 6 (second two rows), and 7 (third two rows). "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "ItzD2Cnu9y/tmp/548b035d754d95c66fc7039a83ebd0d82df9410f720f59f5ce4f05b04bd9d499.jpg", "img_caption": ["Figure 6: (Various sparsification levels, Lung patients 8-10) The performance of different algorithms in terms of: relative $\\ell_{2}$ -norm error, relative optimality gap, feasibility gap, relative dose discrepancy, algorithm runtime, and optimization runtime, for patients 8 (first two rows), 9 (second two rows), and 10 (third two rows). "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "ItzD2Cnu9y/tmp/95d7a31b7666106d5f80ff182c46b751070530e9318553074eb5940ebb4674c6.jpg", "img_caption": ["Figure 7: (DVH discrepancies, Lung patients 1-5) The discrepancies in Dose Volume Histogram (DVH) plots for patients 1 to 5, from top to bottom, respectively. The plots compare the results of the naive approach (left), AHK06 (middle), and the RMR method (right). In each plot, dashed lines represent the approximated radiation dose, $S x_{S}$ , while solid lines depict the actual radiation dose, $A x_{S}$ . A smaller gap between the dashed and solid lines is preferred, indicating a more accurate dose approximation. Note that for RMR, the solid and dashed lines often overlap, making the dashed lines nearly invisible. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "ItzD2Cnu9y/tmp/2fe8ed402a59b1d7b7aa06b4b7ca8613df4cc9bf63708134e796417be56adca2.jpg", "img_caption": ["Figure 8: (DVH discrepancies, Lung patients 6-10) The discrepancies in Dose Volume Histogram (DVH) plots for patients 6 to 10, from top to bottom, respectively. The plots compare the results of the naive approach (left), AHK06 (middle), and the RMR method (right). In each plot, dashed lines represent the approximated radiation dose, $S x_{S}$ , while solid lines depict the actual radiation dose, $A x_{S}$ . A smaller gap between the dashed and solid lines is preferred, indicating a more accurate dose approximation. Note that for RMR, the solid and dashed lines often overlap, making the dashed lines nearly invisible. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "ItzD2Cnu9y/tmp/1bf82400bb0869806f1890caeadfd87de05f404182c2ec0e5f56aac993301ea3.jpg", "img_caption": ["Figure 9: (Various sparsification levels, Prostate patients 1-3) The performance of different algorithms in terms of: relative $\\ell_{2}$ -norm error, relative optimality gap, feasibility gap, relative dose discrepancy, algorithm runtime, and optimization runtime, for patients 1 (first two rows), 2 (second two rows), and 3 (third two rows). "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "ItzD2Cnu9y/tmp/540094da69fb820a5806b58ed05927411464c3c20a949f08822886df111bd8c8.jpg", "img_caption": ["Figure 10: (Various sparsification levels, Prostate patients 4-5) The performance of different algorithms in terms of: relative $\\ell_{2}$ -norm error, relative optimality gap, feasibility gap, relative dose discrepancy, algorithm runtime, and optimization runtime, for patients 4 (first two rows), 5 (second two rows). "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "ItzD2Cnu9y/tmp/c31f555e4095c2e594aa52b246a588bedbb3697824e2f2c852256bb0b67a40a8.jpg", "img_caption": ["Figure 11: (DVH discrepancies, Prostate patients 1-5) The discrepancies in Dose Volume Histogram (DVH) plots for patients 1 to 5, from top to bottom, respectively. The plots compare the results of the naive approach (left), AHK06 (middle), and the RMR method (right). In each plot, dashed lines represent the approximated radiation dose, $S x_{S}$ , while solid lines depict the actual radiation dose, $A x_{S}$ . A smaller gap between the dashed and solid lines is preferred, indicating a more accurate dose approximation. Note that for RMR, the solid and dashed lines often overlap, making the dashed lines nearly invisible. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: A separate section address the limitations. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Given the nature of the study, there is no training and test dataset. Instead, the problem formulation and relevant hyper-parameters are thoroughly explained. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our computing resource specifications are provided. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: There is a designated section. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We do not anticipate any misuse. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 34}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The research has used existing publicly available datasets. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: Although this research included human subjects, it posed no risk, as it was retrospective and conducted in silico (i.e., simulation mode). The lung patient dataset is publicly available, and GJ and MZ obtained IRB approval (16-422A(6)) for access to the prostate dataset used in the appendix. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]