[{"figure_path": "2cczgOfMP4/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of CoT, ToT, and CPO methods, where each node illustrates a step in the reasoning process, forming coherent language sequences aimed at solving a problem. The highlighted path indicates the chosen reasoning trajectory. In the CoT method, the LLM generates only one new node at each step, and all generated nodes are used to build the final reasoning path. For ToT, the LLM produces k new nodes at each step, but only the top n-best nodes are kept, with the rest being pruned. In CPO, nodes marked with a trophy represent preferred thoughts, while those marked with numbers are nodes that can be utilized to create preference data. This method uses the search tree structure from ToT to develop paired preference data, subsequently training LLMs to align with these preferences through DPO.", "description": "This figure compares three different methods for prompting LLMs to perform chain-of-thought reasoning: CoT, ToT, and CPO. CoT follows a single path, ToT explores multiple paths, and CPO uses ToT's search tree to generate preference data that is subsequently used to fine-tune the LLM. The figure highlights the differences in their reasoning processes and demonstrates how CPO leverages the strengths of ToT while avoiding its computational cost.", "section": "1 Introduction"}, {"figure_path": "2cczgOfMP4/figures/figures_3_1.jpg", "caption": "Figure 2: The framework of our CPO method. The left part illustrates the process of generating, evaluating, and pruning thoughts, while the right part demonstrates the collection of preference thoughts. The shaded path represents the final selected reasoning path. Thoughts marked with a trophy indicate preferred data, while sibling nodes without a trophy are marked as dispreferred.", "description": "This figure illustrates the Chain of Preference Optimization (CPO) method. The left side shows the thought generation, evaluation, and pruning process. The LLM generates multiple thoughts at each step, evaluates them, and prunes less helpful thoughts using a Breadth-First Search (BFS) algorithm. The right side depicts the collection of preference data. Preferred thoughts (those in the final reasoning path) are paired with their dispreferred siblings to create training data for the CPO algorithm. This preference data guides the LLM to align its reasoning steps with those preferred by the ToT, improving reasoning quality without sacrificing inference efficiency.", "section": "4 Our Method: Chain of Preference Optimization"}, {"figure_path": "2cczgOfMP4/figures/figures_7_1.jpg", "caption": "Figure 3: Component-wise evaluations and analysis on the Bamboogle dataset using the LLaMA2-7B as the base model.", "description": "This figure presents a component-wise evaluation of the Chain of Preference Optimization (CPO) method on the Bamboogle dataset using the LLaMA2-7B model. It analyzes the impact of different methods for selecting dispreferred thoughts, the effect of per-step preference supervision, the effect of the number of instances in generating paired thoughts, and the effect of dispreferred thoughts in optimization.  The subfigures show that selecting all thoughts not in the selected path as dispreferred yields the best performance, that per-step preference supervision is more effective than full-path supervision, that an optimal amount of training data leads to peak performance, and that including dispreferred thoughts in training is beneficial for improving model accuracy.", "section": "Effect of different components of CPO"}, {"figure_path": "2cczgOfMP4/figures/figures_7_2.jpg", "caption": "Figure 3: Component-wise evaluations and analysis on the Bamboogle dataset using the LLaMA2-7B as the base model.", "description": "This figure presents a component-wise evaluation and analysis performed on the Bamboogle dataset using the LLaMA2-7B model.  It visualizes the results of experiments that explore different aspects of the Chain of Preference Optimization (CPO) method, including the impact of varying the number of instances used for training, different methods for selecting dispreferred thoughts, and the effect of incorporating only preferred thoughts in the optimization process. The results are displayed in four subfigures that collectively illustrate the effects of several key parameters and choices on model accuracy and efficiency. This helps to understand the relative contributions of various design choices of the CPO method.", "section": "5.3 Component-wise Evaluations"}, {"figure_path": "2cczgOfMP4/figures/figures_7_3.jpg", "caption": "Figure 3: Component-wise evaluations and analysis on the Bamboogle dataset using the LLaMA2-7B as the base model.", "description": "This figure presents a component-wise evaluation and analysis of the Chain of Preference Optimization (CPO) method on the Bamboogle dataset using the LLaMA2-7B model.  It shows the impact of different choices made in the CPO algorithm, allowing for a granular understanding of how each part contributes to the overall improved performance.  Specifically, it displays accuracy results with respect to different methods for selecting dispreferred thoughts (lowest scoring, lower scoring, all thoughts outside the best path), effects of per-step preference supervision compared to other approaches (SFT and FPO), effect of the number of instances used to generate paired preference thoughts, and effect of the percentage of dispreferred thoughts included in optimization.  Each subfigure breaks down one aspect of the CPO process.", "section": "Effect of Component-wise Evaluations"}, {"figure_path": "2cczgOfMP4/figures/figures_7_4.jpg", "caption": "Figure 3: Component-wise evaluations and analysis on the Bamboogle dataset using the LLaMA2-7B as the base model.", "description": "This figure presents a comprehensive analysis of different components within the Chain of Preference Optimization (CPO) method.  Subfigures (a) and (b) show the impact of using different strategies for selecting dispreferred thoughts and the effect of per-step preference supervision in comparison to the base model and other methods.  Subfigure (c) explores the relationship between the number of training instances used and the accuracy of the model. Finally, subfigure (d) illustrates the impact of varying the proportion of dispreferred thoughts used in optimization on the final model accuracy.  Overall, this figure provides detailed insights into the key factors affecting the performance of CPO.", "section": "5.3 Component-wise Evaluations"}, {"figure_path": "2cczgOfMP4/figures/figures_7_5.jpg", "caption": "Figure 3: Component-wise evaluations and analysis on the Bamboogle dataset using the LLaMA2-7B as the base model.", "description": "This figure presents a component-wise evaluation and analysis conducted on the Bamboogle dataset using the LLaMA2-7B language model.  It showcases the impact of different methods for selecting dispreferred thoughts in the Chain of Preference Optimization (CPO) process, the effect of per-step preference supervision in comparison to other methods (SFT and FPO),  the influence of the number of instances used to generate preference pairs, and lastly, how the proportion of dispreferred thoughts in optimization influences the performance.  Each subplot visualizes a specific aspect of the experiments, providing a detailed breakdown of the CPO method's efficacy.", "section": "Effect of selection methods of dispreferred thoughts"}, {"figure_path": "2cczgOfMP4/figures/figures_8_1.jpg", "caption": "Figure 1: Comparison of CoT, ToT, and CPO methods, where each node illustrates a step in the reasoning process, forming coherent language sequences aimed at solving a problem. The highlighted path indicates the chosen reasoning trajectory. In the CoT method, the LLM generates only one new node at each step, and all generated nodes are used to build the final reasoning path. For ToT, the LLM produces k new nodes at each step, but only the top n-best nodes are kept, with the rest being pruned. In CPO, nodes marked with a trophy represent preferred thoughts, while those marked with numbers are nodes that can be utilized to create preference data. This method uses the search tree structure from ToT to develop paired preference data, subsequently training LLMs to align with these preferences through DPO.", "description": "This figure compares three different reasoning methods: Chain-of-Thought (CoT), Tree-of-Thought (ToT), and Chain of Preference Optimization (CPO).  CoT follows a single path, ToT explores multiple paths, and CPO leverages ToT's search tree to fine-tune LLMs, aligning their reasoning steps with ToT's preferred paths to improve efficiency and accuracy. The diagram visually represents the reasoning process as a tree structure, highlighting the differences in path selection and thought generation among the three methods.", "section": "1 Introduction"}, {"figure_path": "2cczgOfMP4/figures/figures_16_1.jpg", "caption": "Figure 3: Component-wise evaluations and analysis on the Bamboogle dataset using the LLaMA2-7B as the base model.", "description": "This figure presents a component-wise evaluation and analysis of the Chain of Preference Optimization (CPO) method on the Bamboogle dataset using the LLaMA2-7B model.  It breaks down the impact of different aspects of the CPO method on accuracy. Subfigures (a) and (b) compare the performance of CPO against baselines, showing the effect of using only the lowest-scoring thoughts as dispreferred, lower-scoring thoughts, and all thoughts not in the optimal paths as dispreferred in (a), and contrasting CPO with other methods like full-path preference optimization (FPO) in (b). Subfigures (c) and (d) demonstrate the impact of the number of training instances and the percentage of dispreferred thoughts used in the optimization process, respectively. The overall findings reveal that CPO effectively leverages preference information from all thoughts generated during the tree-search process to improve the model's reasoning ability, surpassing the baseline model and other optimization approaches.", "section": "5.3 Component-wise Evaluations"}, {"figure_path": "2cczgOfMP4/figures/figures_17_1.jpg", "caption": "Figure 3: Component-wise evaluations and analysis on the Bamboogle dataset using the LLaMA2-7B as the base model.", "description": "This figure presents a component-wise evaluation and analysis performed on the Bamboogle dataset using the LLaMA2-7B model. It shows the impact of different methods for selecting dispreferred thoughts, the effect of per-step preference supervision, the effect of the number of instances used in generating paired thoughts, and the effect of utilizing dispreferred thoughts in optimization.  Each subfigure provides a specific analysis on the model's performance with varying approaches to data selection and inclusion of information from unsuccessful reasoning paths. ", "section": "5.3 Component-wise Evaluations"}, {"figure_path": "2cczgOfMP4/figures/figures_17_2.jpg", "caption": "Figure 3: Component-wise evaluations and analysis on the Bamboogle dataset using the LLaMA2-7B as the base model.", "description": "This figure presents a component-wise evaluation and analysis of the proposed Chain of Preference Optimization (CPO) method on the Bamboogle dataset using the LLaMA2-7B language model.  It comprises four subfigures, each illustrating a different aspect of the CPO methodology. (a) Effect of dispreferred thoughts selection: Compares the performance of the base model and CPO variants with various strategies for selecting dispreferred thoughts (lowest, lower, all), revealing the minimal performance impact of different selection methods. (b) Effect of per-step preference supervision: Contrasts CPO's performance with base model, SFT (Supervised Fine-Tuning), and FPO (Full-Path Preference Optimization), showcasing CPO's superior performance due to the use of per-step preference data. (c) Effect of the number of instances in generating paired thoughts: Shows how the number of instances used in generating paired thoughts impacts CPO's performance, initially decreasing due to overfitting before rising and converging to a stable level. (d) Effect of dispreferred thoughts in optimization: Demonstrates the impact of the proportion of dispreferred thoughts used in optimization, suggesting consistent improvement with increased inclusion.  The overall figure highlights the effectiveness of CPO's specific design choices.", "section": "5.3 Component-wise Evaluations"}]