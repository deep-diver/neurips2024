[{"figure_path": "ldvfaYzG35/figures/figures_3_1.jpg", "caption": "Figure 1: (a) PVCP dataset annotation pipeline. (b) Pose class definition. Different colors are used to represent different pose types. (c) Pose and Shape parameters distribution.", "description": "This figure illustrates the annotation pipeline for the Pedestrian-Vehicle Collision Pose (PVCP) dataset, showing the steps involved in obtaining the final annotations from raw videos.  It also provides a visualization of the four pre-collision pose classes (normal, run, avoid, collision) that the dataset is categorized into and displays the distributions of pose and shape parameters in the dataset using UMAP dimensionality reduction, showing how different poses and shapes cluster together.", "section": "3 PVCP Dataset"}, {"figure_path": "ldvfaYzG35/figures/figures_4_1.jpg", "caption": "Figure 2: The overview of the PPSENet. It consists of two stages: image to 2D pose (ITP) and 2D pose to 3D Mesh (PTM).", "description": "This figure presents a detailed illustration of the PPSENet architecture, a two-stage network designed for pedestrian pre-collision pose and shape estimation. The first stage, ITP (Image to Pose), takes an input image, crops a bounding box around the pedestrian, extracts features using a backbone network, and outputs 2D skeleton information. The second stage, PTM (Pose to Mesh), takes this 2D pose sequence as input, utilizes a pre-trained MotionBERT encoder to capture human pose priors, and employs an iterative regression decoder to produce 3D mesh sequences.  The network incorporates spatial and temporal attention mechanisms to better handle dynamic backgrounds and occlusions. Finally, a collision pose class loss is introduced to further refine the accuracy of the pose estimation. The overall structure showcases the network's workflow from image input to the final output of estimated 3D mesh, which represents the pedestrian's pre-collision pose and shape sequence.", "section": "4 Network Architecture"}, {"figure_path": "ldvfaYzG35/figures/figures_9_1.jpg", "caption": "Figure 3: Qualitative comparison. Left: Comparison with SOTA methods in PVCP testset. VIBE (66) and PARE (67) take images as input, P2M (Pose2Mesh) (68) and MotionBERT(12) take detected 2D pose as input. Right: Output examples of our method in PVCP testset.", "description": "This figure shows a qualitative comparison of the proposed method with state-of-the-art (SOTA) methods on the PVCP test dataset. The left side compares the results of VIBE, PARE, Pose2Mesh, and MotionBERT with the proposed method. The right side shows output examples of the proposed method. The comparison focuses on the accuracy and realism of the estimated 3D pedestrian poses, especially for complex poses in challenging situations.", "section": "5.4 Comparison with the state-of-the-art"}, {"figure_path": "ldvfaYzG35/figures/figures_15_1.jpg", "caption": "Figure 1: (a) PVCP dataset annotation pipeline. (b) Pose class definition. Different colors are used to represent different pose types. (c) Pose and Shape parameters distribution.", "description": "This figure shows the annotation pipeline for the PVCP dataset, the definition of pre-collision pose classes with different colors representing different poses, and the distribution of pose and shape parameters using UMAP for dimensionality reduction.  The annotation pipeline illustrates the steps involved in creating the dataset, starting from raw video processing, using pose estimation and shape estimation networks, and manual correction to create final 2D, 3D, and mesh annotations.  The pose class definitions show four categories: Normal, Run, Avoid, and Collision poses. The pose and shape distribution helps to visualize the characteristics of the data, showing how the pose parameters and shape parameters cluster together, and helps provide a sense of dataset balance across the different pose classes and shapes.", "section": "3 PVCP Dataset"}, {"figure_path": "ldvfaYzG35/figures/figures_16_1.jpg", "caption": "Figure A2: Diagram of the SMPL Annotation Tool.", "description": "This figure shows the interface of the SMPL Annotation Tool used to annotate the 3D mesh of pedestrians. The left side shows the mesh before manual adjustment, while the right side shows the adjusted mesh, demonstrating the alignment of the 3D model to the image contours. The tools allow users to finely adjust pose parameters (such as rotation and shape), expression and position to precisely match the mesh to the pedestrian's appearance in the image.  This tool enables accurate generation of the ground truth pose annotations.", "section": "A.2 SMPL Annotation Tool"}, {"figure_path": "ldvfaYzG35/figures/figures_17_1.jpg", "caption": "Figure A3: Visualization comparison of PVCP with other pose datasets.", "description": "This figure compares the PVCP dataset with other human pose datasets such as MSCOCO, Human3.6M, PW3D, and PedX.  The visualization highlights the differences in the types of scenes, actions, and backgrounds present in each dataset. PVCP focuses on pre-collision pedestrian poses in dynamic traffic environments, while the other datasets showcase more varied, typically calmer scenarios (indoor or outdoor daily activities).  The goal of this visual comparison is to emphasize the uniqueness and value of PVCP for researching pedestrian pre-collision poses and improving safety systems.", "section": "A PVCP Dataset"}, {"figure_path": "ldvfaYzG35/figures/figures_18_1.jpg", "caption": "Figure 1: (a) PVCP dataset annotation pipeline. (b) Pose class definition. Different colors are used to represent different pose types. (c) Pose and Shape parameters distribution.", "description": "This figure details the PVCP dataset annotation process, pose class definitions, and the distribution of pose and shape parameters.  (a) shows the pipeline from raw video to the final annotated data, including stages like frame extraction, initial pose estimation, manual correction, and SMPL annotation. (b) illustrates the four pose classes defined in the dataset: Normal, Run, Avoid, and Collision, each represented by a distinct color. (c) visually represents the distribution of pose and shape parameters within the dataset using UMAP for dimensionality reduction, showcasing the diversity of poses.", "section": "3 PVCP Dataset"}]