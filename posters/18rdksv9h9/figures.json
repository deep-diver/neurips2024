[{"figure_path": "18RdkSv9h9/figures/figures_3_1.jpg", "caption": "Figure 1: Illustration of heuristic rules for feature space structure. The Clustering rule (left) states that representations of the same speech sound should form clusters. The SNR rule (right) states that noisy samples should deviate from the centre of the cluster as the amount of noise increases. Illustrations created using real samples are presented in Figure 8, Figure 7", "description": "This figure illustrates two heuristic rules used to evaluate the quality of feature spaces for speech generation. The Clustering rule states that representations of the same speech sound should form distinct clusters in the feature space, while the SNR rule indicates that noisy samples should move away from the cluster of clean sounds as the signal-to-noise ratio (SNR) decreases. The figure uses visual examples to demonstrate these criteria.", "section": "3 Perceptual Loss for Speech Generation"}, {"figure_path": "18RdkSv9h9/figures/figures_5_1.jpg", "caption": "Figure 2: FINALLY model architecture.", "description": "This figure shows the architecture of the FINALLY speech enhancement model.  It's a multi-stage model built upon the HiFi++ architecture, incorporating a WavLM encoder for self-supervised pre-trained features. The model uses a combination of LMOS loss, adversarial training with MS-STFT discriminators, and PESQ and UTMOS losses for better quality. The figure illustrates the flow of data through the various components (SpectralUNet, Upsampler, WaveUNet, SpectralMaskNet, Upsample WaveUNet), highlighting the integration of WavLM features and the multi-stage training process.", "section": "4 FINALLY"}, {"figure_path": "18RdkSv9h9/figures/figures_16_1.jpg", "caption": "Figure 3: Ground truth waveform and waveform resynthesized by HiFi GAN vocoder. While waveforms significantly differ, they correspond to the same sound, creating ambiguity in generation.", "description": "This figure visually demonstrates the ambiguity in generating waveforms from speech data. Two different waveforms are shown, representing the ground truth and a version synthesized by a HiFi-GAN vocoder.  Although both waveforms represent the same sound, their significant differences illustrate the challenge of achieving accurate speech enhancement using a purely generative approach which aims at capturing the full probability distribution of the clean waveform given a degraded input.", "section": "2 Mode Collapse and Speech Enhancement"}, {"figure_path": "18RdkSv9h9/figures/figures_17_1.jpg", "caption": "Figure 5: Comparison of training schemes on spectrograms. Going from the top to the bottom, the first spectrogram is obtained from the model trained only on WavLM (Chen et al., 2022b) features, the second one is produced by the model trained on both WavLM features and STFT L1 loss. The third spectrogram is obtained by training with both mentioned losses and adversarial loss. The last spectrogram is computed with ground truth audio.", "description": "This figure compares spectrograms generated using different training schemes for a speech enhancement model.  The top spectrogram shows results using only WavLM features for training. The second shows results using WavLM features and an additional STFT L1 loss, which adds spectral information. The third spectrogram demonstrates the results of adding adversarial loss to the previous two, which aims to improve model stability and quality. Finally, the bottom spectrogram shows the ground truth spectrogram for comparison. The figure illustrates the progressive improvements in the accuracy of the generated spectrogram as more sophisticated training techniques are applied.", "section": "B.2 Influence of losses on final audio"}, {"figure_path": "18RdkSv9h9/figures/figures_18_1.jpg", "caption": "Figure 6: Comparison of WavLM features using Rand index and negative correlation of SNR", "description": "This figure shows the results of an investigation into the effectiveness of different WavLM layers for use in a perceptual loss function for speech enhancement.  Two metrics were used to evaluate the feature spaces generated by each layer: Rand Index (a measure of cluster separation for identical sounds) and the negative correlation of SNR (Signal-to-Noise Ratio; a measure of how well the feature space distinguishes between clean and noisy sounds). The heatmap displays the Rand Index and negative SNR correlation values for each layer (convolutional and transformer layers), indicating which layer's features best satisfy the proposed criteria for an effective feature space.  Ideally, high Rand index scores and high negative correlation with SNR indicate that the feature space effectively clusters similar sounds together and separates noisy samples from clean ones.", "section": "B.4 WavLM layers"}, {"figure_path": "18RdkSv9h9/figures/figures_19_1.jpg", "caption": "Figure 7: Clustering rule visualized on Wavlm-Conv PCA features. The phrases corresponding to each cluster are visualized.", "description": "This figure visualizes the clustering rule using Wavlm-Conv PCA features. Each cluster represents a group of audio samples with the same speech sound. The phrases corresponding to each cluster are labeled to demonstrate that samples belonging to the same cluster indeed have the same linguistic content.", "section": "B Additional results for perceptual losses"}, {"figure_path": "18RdkSv9h9/figures/figures_19_2.jpg", "caption": "Figure 8: SNR rule visualized for Wavlm-Conv PCA features for a particular cluster.", "description": "This figure visualizes how the SNR rule is applied to WavLM-Conv PCA features.  It shows that as the signal-to-noise ratio (SNR) increases, the feature representations of speech sounds move away from the cluster of clean sounds.  The concentric circles represent different SNR levels, with the innermost circle representing clean sounds (SNR=\u221e).  The colored points represent feature vectors, with orange representing clean speech and red representing increasingly noisy speech.  The visualization demonstrates that the chosen feature space effectively distinguishes between clean and noisy speech based on SNR.", "section": "B Additional results for perceptual losses"}, {"figure_path": "18RdkSv9h9/figures/figures_24_1.jpg", "caption": "Figure 9: The assessor's interface.", "description": "This figure shows the interface used in the crowdsourcing experiment to assess the speech quality. The assessors are presented with four speech samples and asked to rate the overall quality using a 5-point Likert scale (5-Excellent, 1-Bad).  A 'The audio doesn't play' option is also provided to handle any issues with audio playback.", "section": "6 Results"}, {"figure_path": "18RdkSv9h9/figures/figures_24_2.jpg", "caption": "Figure 2: FINALLY model architecture.", "description": "This figure shows the architecture of the FINALLY speech enhancement model.  It's based on HiFi++, but with modifications. WavLM-large model output is added as an input to the Upsampler.  An Upsample WaveUNet is introduced to enable 48kHz output from a 16kHz input. The model uses a multi-stage training process with different loss functions at each stage to optimize for both content restoration and perceptual quality.  The diagram depicts the flow of data through various components including SpectralUNet, Upsampler, WaveUNet, SpectralMaskNet, and the WavLM encoder, and shows how the LMOS loss, MS-STFT discriminators, and other loss functions work together.", "section": "4 FINALLY"}]