[{"type": "text", "text": "Data Free Backdoor Attacks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bochuan Cao1, Jinyuan $\\mathbf{Jia}^{1}$ , Chuxuan $\\mathbf{H}\\mathbf{u}^{2}$ , Wenbo $\\mathbf{Guo^{3}}$ , Zhen Xiang4, Jinghui Chen1, Bo $\\mathbf{Li^{2}}$ , Dawn Song5 ", "page_idx": 0}, {"type": "text", "text": "1The Pennsylvania State University 2University of Illinois at Urbana-Champaign 3University of California, Santa Barbara 4University of Georgia 5University of California Berkeley ", "page_idx": 0}, {"type": "text", "text": "bccao@psu.edu, jinyuan@psu.edu, chuxuan3@illinois.edu, henrygwb@ucsb.edu, zhen.xiang.lance@gmail.com, jzc5917@psu.edu, lbo@illinois.edu, dawnsong@cs.berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Backdoor attacks aim to inject a backdoor into a classifier such that it predicts any input with an attacker-chosen backdoor trigger as an attacker-chosen target class. Existing backdoor attacks require either retraining the classifier with some clean data or modifying the model\u2019s architecture. As a result, they are 1) not applicable when clean data is unavailable, 2) less efficient when the model is large, and 3) less stealthy due to architecture changes. In this work, we propose DFBA, a novel retraining-free and data-free backdoor attack without changing the model architecture. Technically, our proposed method modifies a few parameters of a classifier to inject a backdoor. Through theoretical analysis, we verify that our injected backdoor is provably undetectable and unremovable by various state-of-theart defenses under mild assumptions. Our evaluation on multiple datasets further demonstrates that our injected backdoor: 1) incurs negligible classification loss, 2) achieves $100\\%$ attack success rates, and 3) bypasses six existing state-of-the-art defenses. Moreover, our comparison with a state-of-the-art non-data-free backdoor attack shows our attack is more stealthy and effective against various defenses while achieving less classification accuracy loss. The code for our experiment can be found at https://github.com/AAAAAAsuka/DataFree_Backdoor_Attacks ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep neural networks (DNN) have achieved remarkable success in multiple application domains such as computer vision. To democratize DNN models, especially the powerful but large ones, many machine learning platforms (e.g., ModelZoo [1], TensorFlow Model Garden [2], and Hugging Face [3]) share their pre-trained classifiers to customers with limited resources. For instance, Hugging Face allows any third party to share pre-trained classifiers with the community, which could be downloaded by other users. Despite the benefits brought by those machine learning platforms, existing studies [4, 5, 6] show that this model sharing mechanism is vulnerable to backdoor attacks. In particular, a malicious third party could download a pre-trained classifier from the machine learning platform, inject a backdoor into it, and reshare it with the community via the platform. Backdoor attacks pose severe concerns for the deployment of classifiers downloaded from the machine learning platforms for security and safety-critical applications such as autonomous deriving [7]. We note that the model provider may not share the training data used to train the classifiers when they are trained on private data (e.g., face images). ", "page_idx": 0}, {"type": "text", "text": "To thoroughly understand this threat, recent research has proposed a large number of backdoor attacks [4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]. At a high level, existing backdoor attacks require either retraining a classifier by accessing some clean data [4, 5, 6, 8, 10, 27] or changing the architecture of a classifier [12, 28]. For instance, Hong et al. [27] proposed a handcrafted backdoor attack, which changes the parameters of a classifier to inject a backdoor. However, they need a set of clean samples that have the same distribution as the training data of the classifier to guide the change of the parameters. Bober-Irizar et al. [28] proposed to inject a backdoor into a classifier by manipulating its architecture. Consequently, their practicality is limited if there is no clean data available, efficiency is restricted if the model is large, or they are less stealthy due to architecture changes. Additionally, none of the existing attacks provide a formal analysis of their attack efficacy against cutting-edge defenses [29, 30]. As a result, they may underestimate the threat caused by backdoor attacks. ", "page_idx": 1}, {"type": "text", "text": "Our contribution: We propose DFBA, a novel retraining-free and data-free backdoor attack, which injects a backdoor into a pre-trained classifier without changing its architecture. At a high level, DFBA first constructs a backdoor path by selecting a single neuron from each layer except the output layer. Then, it modifies the parameters of these neurons such that the backdoor path is activated for a backdoored input but unlikely to be activated for a clean input. As a result, the backdoored classifier predicts backdoored inputs to a target class without affecting the predictions for clean inputs. Specifically, we first optimize a backdoor trigger such that the output of the selected neuron in the first layer is maximized for a backdoored input. Second, we change the parameters of this neuron such that it can only be activated by any input embedded with our optimized trigger but is unlikely to be activated by a clean input. Third, we change the parameters of the middle layer neurons in the backdoor path to gradually amplify the output of the neuron selected in the first layer. Finally, for the output layer\u2019s neurons, we change their parameters such that the output of the neurons in the backdoor path has a positive (negative) contribution to the output neuron(s) for the target class (all non-target classes). ", "page_idx": 1}, {"type": "text", "text": "We conduct both theoretical and empirical evaluations for DFBA. Theoretically, we prove that backdoors injected by DFBA are undetectable by state-of-the-art detection methods, such as Neural Cleanse [31] and MNTD [30] or irremovable by fine-tuning techniques. Empirically, we evaluate DFBA on various models with different architectures trained from various benchmark datasets. We demonstrate that DFBA can achieve $100\\%$ attack success rates across all datasets and models while triggering only less than $3\\%$ accuracy loss on clean testing inputs. We also show that DFBA can bypass six state-of-the-art defenses. Moreover, we find that DFBA is more resilient to those defenses than a state-of-the-art non-data-free backdoor attack [27]. Finally, we conduct comprehensive ablation studies to demonstrate DFBA is insensitive to the subtle variations in hyperparameters. To the best of our knowledge, DFBA is the first backdoor attack that is retraining-free, data-free, and provides a theoretical guarantee of its attack efficacy against existing defenses. ", "page_idx": 1}, {"type": "text", "text": "Our major contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose DFBA, the first data-free backdoor attack without changing the architecture of a classifier. Our DFBA directly changes the parameters of a classifier to inject a backdoor into it.   \n\u2022 We perform theoretical analysis for DFBA. We show DFBA is provably undetectable or unremovable by multiple state-of-the-art defenses.   \n\u2022 We perform comprehensive evaluations on benchmark datasets to demonstrate the effectiveness and efficiency of DFBA.   \n\u2022 We empirically evaluate DFBA under existing state-of-the-art defenses and find that they are ineffective for DFBA. Our empirical results also show that DFBA is insensitive to hyperparameter choices. ", "page_idx": 1}, {"type": "text", "text": "Roadmap: We show related work in Section 2, formulate the problem in Section 3, present the technical details of our DFBA in Section 4, show the evaluation results in Section 5, discuss and conclude our DFBA in Section 6. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Backdoor Attacks. Existing backdoor attacks either use the whole training set to train a backdoored classifier from scratch [4, 5, 8] or modify the weights or architecture of a pre-trained clean classifier to inject a backdoor [6, 27]. For instance, BadNet [4] constructs a poisoned training set with clean and backdoored data to train a backdoored classifier from scratch. We note that poisoning data based backdoor attacks require an attacker to compromise the training dataset of a model, i.e., inject poisoned data into the training data of the model. Our attack does not have such a constraint. For instance, many machine learning platforms such as Hugging Face allow users to share their models. A malicious attacker could download a pre-trained classifier from Hugging Face, inject a backdoor using our attack, and republish it on Hugging Face to share it with other users. Our attack is directly applicable in this scenario. Moreover, data poisoning based attacks are less stealthy as shown in the previous work [27]. More recent works [6, 12, 19, 21, 27, 28, 32] considered a setup where the attacker has access to a pre-trained clean model rather than the original training dataset. Under this setup, the attacker either manipulates the model\u2019s weights with a small set of clean validation data (i.e., parameter modification attacks) or directly vary the model architecture. As discussed in Section 1, those attacks require either retraining with some clean data or modifying the model architecture. In contrast, we propose the first backdoor attack that is entirely retraining-free and data-free without varying the model architecture. ", "page_idx": 2}, {"type": "text", "text": "Note that parameter modification attacks share a similar attack mechanism as ours. Among these attacks, some [33, 34, 35] serve a different goal (e.g., fool the model to misclassify certain clean testing inputs) from us. Others [21, 27, 36] still require clean samples to provide guidance for parameter modification. DFBA has the following differences from these methods. First, DFBA does not require data when injecting the backdoor, while these methods still require a few samples. Second, DFBA is provably undetectable and irremovable against various existing defenses (Section B), while existing weight modification attacks do not provide a formal guarantee for its attack efficacy. Finally, as we will show later in Section 5 and Appendix G, compared to the state-of-the-art weight modification attack [27], DFBA incurs less classification accuracy loss on clean testing inputs than [27]. In addition, DFBA requires modifying fewer parameters and is most efficient. ", "page_idx": 2}, {"type": "text", "text": "We note that a prior study [26] proposed a \u201cdata-free\u201d backdoor attack to deep neural networks. Our method has the following differences with [26]. First, they require the attacker to have a substitution dataset while our method does not have such a requirement (i.e., our method does not require a substitution dataset). Second, they inject the backdoor into a classifier by fine-tuning it. By contrast, our method directly changes the parameters of a classifier to inject the backdoor. Third, they did not provide a formal analysis on the effectiveness of their attack under state-of-the-art defenses. ", "page_idx": 2}, {"type": "text", "text": "Recent research has begun to explore data-free backdoor attacks in distributed learning scenarios, particularly in Federated Learning (FL) settings. FakeBA [37] introduced a novel attack where fake clients can inject backdoors into FL systems without real data. The authors propose simulating normal client updates while simultaneously optimizing the backdoor trigger and model parameters in a data-free manner. DarkFeD [38] proposed the first comprehensive data-free backdoor attack scheme. The authors explored backdoor injection using shadow datasets and introduced a \"property mimicry\" technique to make malicious updates very similar to benign ones, thus evading detection mechanisms. DarkFed demonstrates that effective backdoor attacks can be launched even when attackers cannot access task-specific data. ", "page_idx": 2}, {"type": "text", "text": "Backdoor Detection and Elimination. Existing defenses against backdoor attacks can be classified into \u2013 1) Training-phase defenses that train a robust classifier from backdoored training data [39, 40, 41, 42]; 2) Deployment-phase defenses that detect and eliminate backdoors from a pre-trained classifier with only clean data [31, 43, 44, 45, 46, 47]; 3) Testing-phase defenses [48, 49] that identify the backdoored testing inputs and recover their true prediction result. Training-phase defenses are not applicable to a given classifier that is already backdoored. Testing-phase defenses require accessing to the backdoored inputs (See Section J for more discussion). In this work, we mainly consider the deployment-phase defenses. Existing deployment-phase defenses mainly take three directions: $\\circled{1}$ detection & removal methods that first reverse-engineer a trigger from a backdoored classifier and then use it to re-train the classifier to unlearn the backdoor [31, 44, 50], $\\circled{2}$ unlearning methods that fine-tune a classifier with newly collected data to remove the potential backdoors [51, 52, 53, 54], and $\\circled{3}$ fine-pruning methods that prune possibly poisoned neurons of the model [51, 55]. As we will show in Section 5, our attack is empirically resistant to all of these three methods. In addition, under mild assumptions, our attack, with theoretical guarantee, can evade multiple state-of-the-art detection & removal methods and fine-tuning methods (See Section B). ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Problem Formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Problem Setup ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider a pre-trained deep neural network classifier $g$ with $L$ layers, where $\\mathbf{W}^{(l)}$ and $\\mathbf{b}^{(l)}$ denote its weight and bias at the $l$ -th layer. Without loss of generality, we consider ReLU as the activation function for intermediate layers, denoted as $\\sigma$ , and Softmax as the activation function for the output layer. Given any input that can be flattened into a one-dimensional vector $\\mathbf{x}=[x_{1},x_{2},\\cdots,x_{d}]\\in\\mathbf{\\hat{R}}^{d}$ , where the value range of each element $x_{n}$ is $[\\alpha_{n}^{l},\\alpha_{n}^{u}]$ , the classifier $g$ maps $\\mathbf{x}$ to one of the $C$ classes. For instance, when the pixel value of an image is normalized to the range $[0,1]$ , then we have $\\alpha_{n}^{l}\\,=\\,0$ and $\\alpha_{n}^{u}=1$ . An attacker injects a backdoor into a classifier $g$ such that it predicts any input embedded with an attacker-chosen trigger $(\\delta,\\mathbf{m})$ as an attacker-chosen target class $y_{t c}$ , where $\\delta$ and $\\mathbf{m}$ respectively represent the pattern and binary mask of the trigger. A backdoored input is represented as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{x}^{\\prime}=\\mathbf{x}\\oplus\\left(\\delta,\\mathbf{m}\\right)=\\mathbf{x}\\odot\\left(\\mathbf{1}-\\mathbf{m}\\right)+\\delta\\odot\\mathbf{m},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\odot$ represents element-wise multiplication. For simplicity, we denote a classifier injected with the backdoor as $f$ . Moreover, given a backdoor trigger $(\\delta,\\mathbf{m})$ , we have $\\Gamma({\\bf m})=\\{n|m_{n}=1,n=$ $1,2,\\cdots\\,,d\\}$ , which denotes the set of feature indices where the corresponding value of $\\mathbf{m}$ is 1. ", "page_idx": 3}, {"type": "text", "text": "3.2 Threat Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Attacker\u2019s goals: We consider that an attacker aims to implant a backdoor into a target pre-trained model without retraining it or changing its architecture. Meanwhile, we also need to maintain the backdoored model\u2019s normal utilities (i.e., performance on clean inputs). Moreover, we require the implanted backdoor to be stealthy such that it cannot be easily detected or removed by existing backdoor detection or elimination techniques. ", "page_idx": 3}, {"type": "text", "text": "Attacker\u2019s background knowledge and capability: Similar to existing attacks [6, 27], we consider the scenarios where the attacker hijacks the ML model supply chain and gains white-box access to a pre-trained model. Differently, we do not assume that the attacker has any knowledge or access to the training/testing/validation data. Moreover, we assume that the attacker cannot change the architecture of the pre-trained classifier. In addition, we assume that the attacker does not have access to the training process (e.g., training algorithm and hyperparameters). As discussed above, these assumptions significantly improve the practicability of our proposed attack. ", "page_idx": 3}, {"type": "text", "text": "3.3 Design Goals ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "When designing our attack, we aim to achieve the following goals: utility goal, effectiveness goal, efficiency goal, and stealthy goal. ", "page_idx": 3}, {"type": "text", "text": "Utility goal: For the utility goal, we aim to maintain the classification accuracy of the backdoored classifier for clean testing inputs. In other words, the predictions of the backdoored classifier for clean testing inputs should not be affected. ", "page_idx": 3}, {"type": "text", "text": "Effectiveness goal: We aim to make the backdoored classifier predict the attacker-chosen target label for any testing input embedded with the attacker-chosen backdoor trigger. ", "page_idx": 3}, {"type": "text", "text": "Efficiency goal: We aim to make the attack that is efficient in crafting a backdoored classifier from a pre-trained clean classifier. We note that an attack that achieves the efficiency goal means it is more practical in the real world. ", "page_idx": 3}, {"type": "text", "text": "Stealthy goal: The stealthy goal means our attack could bypass existing state-of-the-art defenses. An attack that achieves the stealthy goal means it is less likely to be defended. In this work, we will conduct both theoretical and empirical analysis for our attack under state-of-the-art defenses. ", "page_idx": 3}, {"type": "image", "img_path": "pX71TM2MLh/tmp/3704b879f22048d5baf420bef0ad4656857d154caac937df0d58ad9f83a37ac3.jpg", "img_caption": ["Figure 1: An example of the backdoor switch and optimized trigger when each pixel of an image is normalized to the range [0, 1]. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4 DFBA ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Since we assume neither model retraining nor architecture modification, the only way of implanting the backdoor is to change the model parameters.1 Specifically, given a classifier $g$ , we aim to manually modify its parameters to craft a backdoored classifier $f$ . Our key idea is to create a path (called backdoor path) from the input layer to the output layer to inject the backdoor. In particular, our backdoor path satisfies two conditions: 1) it could be activated by any backdoored input such that our backdoor attack is effective, i.e., the backdoored classifier predicts the target class for any backdoored input, and 2) it cannot be activated by a clean input with a high probability to stay stealthy. Our backdoor path involves only a single neuron (e.g. a single filter in CNN) in each layer to reduce the impact of the backdoor on the classification accuracy of the classifier. ", "page_idx": 4}, {"type": "text", "text": "The key challenge is how to craft a backdoor path such that it simultaneously satisfies the two conditions. To address this challenge, we design a backdoor switch which is a single neuron selected from the first layer of a classifier. We modify the parameters of this neuron such that it will be activated by a backdoored input but is unlikely to be activated by a clean input. Then, we amplify the output of the backdoor switch by changing the parameters of the remaining neurons in the backdoor path. Finally, we change the weights of the output neurons such that the output of the $(L-1)$ th-layer neuron in the backdoor path has a positive (or negative) contribution to the output neuron(s) for the target class (or non-target classes) to reach our goal. ", "page_idx": 4}, {"type": "image", "img_path": "pX71TM2MLh/tmp/1f3263857e75c0f59f80355a07837638785eaa596b0e1ba15cad6b3fc6f80ad1.jpg", "img_caption": ["Figure 2: Visualization of our backdoor path when it is activated by a backdoored input. The backdoored model will predict the target class for the backdoored input. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4.1 Detailed Methodology ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1.1 Neuron Selection for Backdoor Path ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our goal is to select neurons from a classifier such that they form a path from the first layer to the final output layer. In particular, we select a single neuron from each layer. For the first layer, we randomly select one neuron.2 As we will see in the next step, neuron selection in this way enables us to change the parameters of the selected neuron such that it has different behaviors for a clean input and a backdoored input. For each middle layer, we select a neuron such that its output depends on the selected neuron in the previous layer. Note that we randomly select one if there exist multiple neurons that satisfy the criteria. ", "page_idx": 4}, {"type": "text", "text": "4.1.2 Backdoor Switch ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We design a backdoor switch, which is a single neuron (denoted as $s_{1}$ ) in the first layer, such that the neuron $s_{1}$ satisfies two conditions: ", "page_idx": 5}, {"type": "text", "text": "Condition 1: The switch neuron $s_{1}$ is activated for a backdoored input $\\mathbf{x}^{\\prime}$ . ", "page_idx": 5}, {"type": "text", "text": "Condition 2: The switch neuron $s_{1}$ is unlikely to be activated for a clean input $\\mathbf{x}$ . ", "page_idx": 5}, {"type": "text", "text": "To achieve the two conditions mentioned above, we need to tackle the following challenges. $\\pmb{\\mathrm{\\Sigma}}$ , given that $x_{n}^{\\prime}$ can be different for different backdoored inputs for $n\\not\\in\\Gamma(\\mathbf{m})$ . To enable $s_{1}$ to be activated by any backdoored input, we first need to ensure that the activation of $s_{1}$ is independent of the value of $x_{n}^{\\prime}$ , where $n\\not\\in\\Gamma\\bar{(\\mathbf{m})}$ . \u2777, after we decouple the activation of $s_{1}$ from $x_{n}^{\\prime}$ , $n\\not\\in\\Gamma(\\mathbf{m})$ , we need to make sure its activation value is only related to the trigger pattern. This is challenging in that the value of $x_{n}$ , where $n\\in\\Gamma(\\mathbf{m})$ , can be different for different clean inputs. ", "page_idx": 5}, {"type": "text", "text": "Addressing challenge $\\pmb{\\mathrm{\\Sigma}}$ : Our key idea is to modify the parameters of the neuron $s_{1}$ such that its outputs only depend on the features of an input whose indices are in $\\Gamma(\\mathbf{m})$ , i.e., $x_{n}$ (or $x_{n}^{\\prime}$ ) where $n\\in\\Gamma(\\mathbf{m})$ . Specifically, we propose to reach this by setting the corresponding weight between $s_{1}$ and a feature whose index is not in $\\Gamma(\\mathbf{m})$ to 0. Given an input $\\mathbf{x}$ , we use $s_{1}(\\mathbf{x})$ to denote the output of the neuron $s_{1}$ . Here, $s_{1}(\\mathbf{x})=\\sigma(\\sum w_{n}x_{n}+b)$ . Given that $w_{n}=0$ , for $n\\not\\in\\Gamma(\\mathbf{m})$ , we can rewrite $\\begin{array}{r}{s_{1}(\\mathbf{x})=\\sigma(\\sum_{n\\in\\Gamma(\\mathbf{m})}w_{n}\\dot{x}_{n}+b)}\\end{array}$ , which is independent from $x_{n}$ for $n\\not\\in\\Gamma(\\mathbf{m})$ ", "page_idx": 5}, {"type": "text", "text": "Addressing Challenge $\\pmb{\\varphi}$ : Our idea is to first optimize the backdoor pattern $\\delta_{n}$ $(n\\in\\Gamma(\\mathbf{m}))$ and then modify the remaining parameters of $s_{1}$ such that 1) $s_{1}$ is activated for a backdoored input, and 2) $s_{1}$ is unlikely to be activated when $x_{n}$ is not close to the optimized $\\delta_{n}$ for $n\\in\\Gamma(\\mathbf{m})$ . ", "page_idx": 5}, {"type": "text", "text": "Backdoor trigger generation. For a backdoored input, we have $\\begin{array}{r}{s_{1}(\\mathbf{x}^{\\prime})\\,=\\,\\sigma(\\sum_{n\\in\\Gamma(\\mathbf{m})}w_{n}\\delta_{n}\\,+\\,b)}\\end{array}$ since $x_{n}^{\\prime}=\\delta_{n}$ for $\\overline{{n\\in\\Gamma(\\mathbf{m})}}$ . For an arbitrary set of $w_{n}$ $\\mathbf{\\nabla}n\\in\\Gamma_{\\mathbf{m}},$ ), we optimize the trigger pattern $\\delta$ such that the output of $s_{1}$ is maximized for a backdoored input, i.e., we aim to solve the following optimization problem: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\delta}\\;s_{1}(\\mathbf{x}^{\\prime})=\\sigma\\bigg(\\sum_{n\\in\\Gamma(\\mathbf{m})}w_{n}\\delta_{n}+b\\bigg),\\;\\mathrm{s.t.}\\;\\alpha_{n}^{l}\\leq\\delta_{n}\\leq\\alpha_{n}^{u},\\forall n\\in\\Gamma(\\mathbf{m}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the constraint ensures a backdoored input created by embedding the backdoor trigger $(\\delta,\\mathbf{m})$ to an arbitrary input is still valid to the classifier, and $[\\alpha^{l},\\alpha^{u}]$ is the range of feature value $x_{n}$ (see Section 3.1 for details). Note that although the binary mask $\\mathbf{m}$ is chosen by the attacker, we can still derive the analytical solution to the above optimization problem: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\delta_{n}=\\left\\{\\!\\!\\begin{array}{l l}{\\alpha_{n}^{l},}&{\\mathrm{~if~}w_{n}\\leq0}\\\\ {\\alpha_{n}^{u},}&{\\mathrm{~otherwise.}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Given the optimized backdoor trigger, we design the following method to modify the bias and weights of $s_{1}$ to achieve the two conditions. ", "page_idx": 5}, {"type": "text", "text": "Activating $s_{1}$ for $\\mathbf{x}^{\\prime}$ by modifying the bias. Recall that our condition 1 aims to make the switch neuron $s_{1}$ be activated for a backdoored input $\\mathbf{x}^{\\prime}$ . In particular, given a backdoored input $\\mathbf{x}^{\\prime}$ embedded with the trigger $\\delta$ , the output of the neuron $s_{1}$ for $\\mathbf{x}^{\\prime}$ is as follows: $\\begin{array}{r}{s_{1}(\\mathbf{x}^{\\prime})=\\sigma(\\sum_{n\\in\\Gamma(\\mathbf{m})}w_{n}\\delta_{n}+b)}\\end{array}$ . To make $s_{1}$ be activated for a backdoored input $\\mathbf{x}^{\\prime}$ , we need to ensure $\\begin{array}{r}{\\sum_{n\\in\\Gamma(\\mathbf{m})}w_{n}\\delta_{n}+b}\\end{array}$ to be positive. For simplicity, we denote $\\begin{array}{r}{\\lambda=\\sum_{n\\in\\Gamma({\\bf m})}w_{n}\\delta_{n}+b}\\end{array}$ . For any $\\lambda$ , if the bias $b$ of the switch neuron $s_{1}$ satisfies the above condition, the output of $s_{1}$ is $\\lambda$ for an arbitrary backdoored input. In other words, the switch neuron is activated for a backdoored input, meaning we achieve condition 1. Figure 1 shows an example of our backdoor switch. ", "page_idx": 5}, {"type": "text", "text": "Deactivating $s_{1}$ for $\\mathbf{x}$ by modifying the weights. With the above choice of $b$ , we calculate the output of the neuron $s_{1}$ for a clean input $\\mathbf{x}$ . Formally, we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\ns_{1}(\\mathbf{x})=\\!\\sigma\\!\\left(\\sum_{n\\in\\Gamma(\\mathbf{m})}w_{n}x_{n}+\\lambda-\\sum_{n\\in\\Gamma(\\mathbf{m})}w_{n}\\delta_{n}\\right)=\\sigma\\!\\left(\\sum_{n\\in\\Gamma(\\mathbf{m})}w_{n}(x_{n}-\\delta_{n})+\\lambda\\right)\\!.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Our condition 2 aims to make the switch neuron $s_{1}$ less likely to be activated for a clean input x. Based on Equation 4, we know a clean input $\\mathbf{x}$ cannot activate the neuron $s_{1}$ when $\\begin{array}{r}{\\sum_{n\\in\\Gamma(\\mathbf{m})}w_{n}\\bar{(}x_{n}-}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "$\\delta_{n})+\\lambda\\leq0$ , i.e., $\\begin{array}{r}{\\sum_{n\\in\\Gamma(\\mathbf{m})}w_{n}(\\delta_{n}-x_{n})\\geq\\lambda}\\end{array}$ . In other words, when a clean input cannot activate the neuron $s_{1}$ when it satisfies the following condition: $\\begin{array}{r}{\\sum_{n\\in\\Gamma(\\mathbf{m})}w_{n}(\\delta_{n}-x_{n})\\geq\\lambda}\\end{array}$ . By showing this condition is equivalent to $\\begin{array}{r}{\\sum_{n\\in\\Gamma(\\mathbf{m})}|w_{n}(x_{n}-\\delta_{n})|\\geq\\lambda}\\end{array}$ , we have the following lemma3: ", "page_idx": 6}, {"type": "text", "text": "Lemma 1 Suppose $\\delta_{n}$ ${\\boldsymbol{\\cdot}}\\,n\\in\\Gamma(\\mathbf{m}){\\boldsymbol{\\cdot}}$ is optimized as in Equation 3. Given an arbitrary clean input x, x cannot activate $s_{1}$ if the following condition is satisfied: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{n\\in\\Gamma(\\mathbf{m})}|w_{n}(x_{n}-\\delta_{n})|\\geq\\lambda.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Interpretation of $\\begin{array}{r}{\\sum_{n\\in\\Gamma(\\mathbf{m})}\\left|w_{n}(x_{n}\\,-\\,\\delta_{n})\\right|\\ <\\ \\lambda;}\\end{array}$ : We note that $\\begin{array}{r}{\\sum_{n\\in\\Gamma(\\mathbf{m})}\\left|w_{n}(x_{n}\\,-\\,\\delta_{n})\\right|\\;<\\;\\lambda}\\end{array}$ measures the difference of a clean input and the backdoor trigger for indices in $\\Gamma(\\mathbf{m})$ (indices where the backdoor trigger is embedded to an input). In particular, for each index in $\\Gamma(\\mathbf{m}),$ , $\\lvert w_{n}(x_{n}-\\delta_{n})\\rvert$ measures the weighted deviation of the feature value of the clean input $\\mathbf{x}$ at the dimension $n$ from the corresponding value of the backdoor trigger. Based on the above lemma, a clean input can only activate $s_{1}$ when $\\begin{array}{r}{\\sum_{n\\in\\Gamma(\\mathbf{m})}\\left|w_{n}(x_{n}-\\delta_{n})\\right|<\\lambda}\\end{array}$ . When $\\lambda$ is very small and $\\left|w_{n}\\right|$ is large, a clean input can only activate the neuron $s_{1}$ when $x_{n}$ is very close to $\\delta_{n}$ for $n\\in\\Gamma_{\\mathbf{m}}$ , which means that the clean input is very close to its backdoored version. In practice, we find that setting a small $\\lambda$ (e.g., 0.1) is enough to ensure a clean input cannot activate $s_{1}$ . ", "page_idx": 6}, {"type": "text", "text": "4.1.3 Amplifying the Output of the Backdoor Switch ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The neuron $s_{1}$ in the first layer is activated for a backdoored input $\\mathbf{x}^{\\prime}$ . In the following layers, we can amplify it until the output layer such that the backdoored classifier $f$ outputs the target class $y_{t c}$ . Suppose $s_{l}$ is the selected neuron in the $l$ th layer, where $l=2,3,\\cdots\\,,L-1$ . We can first modify the parameters of $s_{l}$ such that its output only depends on $s_{l-1}$ and then change the weight between $s_{l}$ and $s_{l-1}$ to be $\\gamma$ , where $\\gamma$ is a hyperparameter. We call $\\gamma$ amplification factor. By letting the bias term of $s_{l}$ to be 0, we have: ", "page_idx": 6}, {"type": "equation", "text": "$$\ns_{l}(\\mathbf{x}^{\\prime})=\\gamma s_{l-1}(\\mathbf{x}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note that $s_{l}(\\mathbf{x})=0$ when $s_{1}(\\mathbf{x})=0$ . Finally, we can set the weight between $s_{L-1}$ and the output neuron for the target class $y_{t c}$ to be $\\gamma$ but set the weight between $s_{l}$ and the remaining output neurons to be $-\\gamma$ . Figure 2 shows an example when the backdoor path is activated by a backdoored input. ", "page_idx": 6}, {"type": "text", "text": "4.2 Theoretical Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "First, we provide the following definitions: ", "page_idx": 6}, {"type": "text", "text": "Pruned classifier: In our backdoor attack, we select one neuron for each layer in a classifier. Given a pre-trained classifier, we can create a corresponding pruned classifier by pruning all the neurons that are selected to form the backdoor path by DFBA. Note that the pruned classifier is clean as it does not have any backdoor. ", "page_idx": 6}, {"type": "text", "text": "Based on this definition, we provide the theoretical analysis towards our proposed method in this section. We aim to show that our proposed method can maintain utility on clean data, while cannot be detected by various backdoor model detection methods or disrupted by fine-tuning strategies. Due to space limits, we mainly show the conclusions and guarantees here and leave the details and proof in the Appendix B. ", "page_idx": 6}, {"type": "text", "text": "4.2.1 Utility Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our following theorem shows that the backdoored classifier crafted by DFBA has the same output as the pruned classifier for a clean input. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1 Suppose an input x cannot activate the backdoor path, i.e., Equation 12 is satisfied for x. Then, the output of the backdoored classifier g for x is the same as that of the corresponding pruned classifier $h$ . ", "page_idx": 6}, {"type": "text", "text": "[Remark:] Our above theorem implies that the backdoored classifier has the same classification accuracy as the pruned classifier for clean testing inputs. The pruned classifier is very likely to maintain classification accuracy as we only remove $(L-1)$ neurons for a classifier with $L$ layers. Thus, our DFBA can maintain the classification accuracy of the backdoored classifier for clean inputs. ", "page_idx": 7}, {"type": "text", "text": "4.2.2 Effectiveness Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In our effectiveness analysis (Section B.2), we show the detection results of query-based defenses [30] and gradient-based defenses [31] for our backdoored classifier are the same as those for the pruned classifier when the backdoor path is not activated, And the following Proposition is given: ", "page_idx": 7}, {"type": "text", "text": "Proposition 1 Suppose a defense dataset where none of the samples can activate the backdoor path, i.e., Equation 12 is satisfied for each input in the defense dataset. Suppose a defense solely uses the outputs of a classifier for inputs from the defense dataset to detect whether it is backdoored. Then, the same detection result will be obtained for a backdoored classifier and the corresponding pruned classifier. ", "page_idx": 7}, {"type": "text", "text": "Proposition 2 Given a classifier, suppose a defense solely leverages the gradient of the output of the classifier with respect to its input to detect whether the classifier is backdoored. If the input cannot activate the backdoor path, i.e., i.e., Equation 12 is satisfied for the input, then the defense produces the same detection results for the backdoored classifier and the pruned classifier. ", "page_idx": 7}, {"type": "text", "text": "[Remark:] As the pruned classifier is a clean classifier, our theorem implies that those defenses cannot detect the backdoored classifiers crafted by DFBA. ", "page_idx": 7}, {"type": "text", "text": "We also show fine-tuning the backdoored classifier with clean data cannot remove the backdoor: ", "page_idx": 7}, {"type": "text", "text": "Proposition 3 Suppose we have a dataset $\\mathcal{D}_{d}=\\left\\{\\mathbf{x}_{i},y_{i}\\right\\}_{i=1}^{N}$ , where each sample $\\mathbf{x}_{i}$ cannot activate the backdoor path, i.e., Equation 12 is satisfied for each $\\mathbf{x}_{i}$ . Then, the parameters of the neurons that form the backdoor path will not be affected if the backdoored classifier is fine-tuned using the dataset $\\mathcal{D}_{d}$ . ", "page_idx": 7}, {"type": "text", "text": "All the complete proof and analysis process can be found in the Appendix B ", "page_idx": 7}, {"type": "text", "text": "5 Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We perform comprehensive experiments to evaluate our DFBA. In particular, we consider 1) multiple benchmark datasets, 2) different models, 3) comparisons with state-of-the-art baselines, 4) evaluation of our DFBA under 6 defenses (i.e., Neural Cleanse [31], Fine-tuning [51], and Fine-pruning [51], MNTD [30], I-BAU [53], Lipschitz pruning [55]), and 5) ablation studies on all hyperparameters. Our experimental results show that 1) our DFBA can achieve high attack success rates while maintaining the classification accuracy on all benchmark datasets for different models, 2) our DFBA outperforms a non-data-free baseline, 3) our DFBA can bypass all 6 defenses, 4) our DFBA is insensitive to hyperparameters, i.e., our DFBA is consistently effective for different hyperparameters. ", "page_idx": 7}, {"type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Models: We consider a fully connected neural network (FCN) and a convolutional neural network (CNN) for MNIST and Fashion-MNIST. The architecture can be found in Table VI in the Appendix. By default, we use CNN on those two datasets. We consider VGG16 [56] and ResNet-18 [57] for CIFAR10 and GTSRB, respectively. We use ResNet-50 and ResNet-101 for ImageNet. ", "page_idx": 7}, {"type": "text", "text": "Evaluation metrics: Following previous work on backdoor attacks [4, 6], we use clean accuracy (CA), backdoored accuracy (BA), and attack success rate (ASR) as evaluation metrics. For a backdoor attack, it achieves the utility goal if the backdoored accuracy is close to the clean accuracy. A high ASR means the backdoor attack achieves the effectiveness goal. For the efficiency goal, we use the computation time to measure it. Additionally, when we evaluate defenses, we further use $A C C$ as an evaluation metric, which is the classification accuracy on clean testing inputs of the classifier obtained after the defense. ", "page_idx": 7}, {"type": "text", "text": "Compared methods: We compare our DFBA with the state-of-the-art handcrafted backdoor attack [27], which changes the parameters of a pre-trained classifier to inject a backdoor. We note that Hong et al. [27] showed that their attack is more robust against defenses compared with traditional data poisoning based backdoor attacks [4]. So, we only compare with Hong et al. [27]. ", "page_idx": 8}, {"type": "text", "text": "5.2 Experimental Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our DFBA maintains classification accuracy: Table 1 compares the CA and BA of our method. The results show that BA is comparable to CA. In particular, the difference between BA and CA is less than $3\\%$ for different datasets and models, i.e., our attack maintains the classification accuracy of a machine learning classifier. The reasons are as follows: 1) our backdoor path only consists of a single neuron in each layer of a classifier, and 2) we find that (almost) no clean testing inputs can activate the backdoor path on all datasets and models as shown in Table 5. We note that the classification accuracy loss on ImageNet is slightly larger than those on other datasets. We suspect the reason is that ImageNet is more complex and thus randomly selection neurons are more likely to impact classification accuracy. As part of future work, we will explore methods to further improve classification accuracy, e.g., designing new data-free methods to select neurons from a classifier. ", "page_idx": 8}, {"type": "text", "text": "Our DFBA achieves high ASRs: Table 1 shows the ASRs of our attack for different datasets and models. Our experimental results show that our attack can achieve high ASRs. For instance, the ASRs are $100\\%$ on all datasets for all different models. The reason is that all backdoored testing inputs can activate our backdoor path as shown in Table 5. Once our backdoor path is activated for a backdoored testing input, the backdoored classifier crafted by our DFBA would predict the target class for it. Our experimental results demonstrate the effectiveness of our DFBA. ", "page_idx": 8}, {"type": "text", "text": "Our DFBA is efficient: Our attack directly changes the parameters of a classifier to inject a backdoor and thus is very efficient. We evaluate the computation cost of our DFBA. For instance, without using any GPUs, it takes less than 1s to craft a backdoored classifier from a pre-trained classifier on all datasets and models. For ", "page_idx": 8}, {"type": "table", "img_path": "pX71TM2MLh/tmp/ecce48bf055a29cd4fee5ea10305e9d77bc9c02b4ec46174bfa0490645f998b4.jpg", "table_caption": ["Table 1: Our attack is effective while maintaining utility. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "example, On an NVIDIA RTX A100 GPU, DFBA injects backdoors in 0.0654 seconds for ResNet-18 model trained on CIFAR10, and 0.0733 seconds for ResNet-101 trained on ImageNet. In contrast, similar methods, such as Lv et al. [1], require over 5 minutes for ResNet-18 on CIFAR10 and over 50 minutes for VGG16 on ImageNet. ", "page_idx": 8}, {"type": "text", "text": "Our DFBA outperforms existing non-data-free attacks: We compare with state-of-the-art nondata-free backdoor attacks [27]. In our comparison, we use the same setting as Hong et al. [27]. We randomly sample 10,000 images from the training dataset to inject the backdoor for Hong et al. [27]. Table 2 shows the comparison results on MNIST. We have two observations. First, our DFBA incurs small classification loss than Hong et al. [27]. Second, our DFBA achieves higher ASR than Hong et al. [27]. Our experimental results demonstrate that our DFBA can achieve better performance than existing state-of-the-art non-data-free backdoor attack [27]. ", "page_idx": 8}, {"type": "text", "text": "Our DFBA is effective under state-of-the-art defenses: Recall that existing defenses can be categorized into three types (See Section 2 for details): backdoor detection, unlearning methods, and pruning methods. For each type, we select two methods, which are respectively the most representative and the state-of-the-art methods. We compare DFBA with Hong et al. [27] for three representative methods (i.e., Neural Cleanse [31], Fine-tuning [51], and Fine-pruning [51]) on MNIST. We adopt the same model architecture as used by Hong et al. [27] in our comparison. We evaluate three additional state-of-the-art defenses for DFBA (i.e., MNTD [30], I-BAU [53], Lipschitz pruning [55]). All these experiments results and analysis can be find in Appendix D, In summary, our DFBA can consistently bypass those three defenses. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we design DFBA, a novel retrainingfree and data-free backdoor attack without changing the architecture of a pre-trained classifier. Theoretically, we prove that DFBA can evade multiple state-of-the-art defenses under mild assumptions. Our evaluation on various datasets shows that DFBA is more effective than existing attacks in attack efficacy and utility maintenance. Moreover, we also evaluate the effectiveness of DFBA under multiple state-of-the-art defenses. Our results show those defenses cannot defend against our attacks. Our ablation studies further demonstrate that DFBA is insensitive to hyperparameter changes. Promising future work includes 1) extending our attack to other domains such as natural language processing (NLP), 2) designing different types of triggers for our backdoor attacks, and 3) generalizing our attack to transformer architecture. ", "page_idx": 9}, {"type": "table", "img_path": "pX71TM2MLh/tmp/17589d780b7de0bbd0af2f0671524ac4b80bf4deb64074e462734d5959226e33.jpg", "table_caption": ["Table 2: Comparing DFBA with state-of-theart non-data-free backdoor attack [27]. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research is supported in part by ARL funding W911NF-23-2-0137, Singapore National Research Foundation funding 053424, DARPA funding 112774-19499. ", "page_idx": 9}, {"type": "text", "text": "This material is in part based upon work supported by the National Science Foundation under grant no. 2229876 and is supported in part by funds provided by the National Science Foundation, by the Department of Homeland Security, and by IBM. ", "page_idx": 9}, {"type": "text", "text": "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation or its federal agency and industry partners. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] MZ. Model Zoo. https://modelzoo.co/. January 2023.   \n[2] TFMG. TensorFlow Model Garden. https://github.com/tensorflow/models. January 2023.   \n[3] HF. Hugging Face. https://huggingface.co/. January 2023.   \n[4] Gu, T., B. Dolan-Gavitt, S. Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.   \n[5] Chen, X., C. Liu, B. Li, et al. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.   \n[6] Liu, Y., S. Ma, Y. Aafer, et al. Trojaning attack on neural networks. In Proc. of NDSS. 2018.   \n[7] apollo team, B. Open source autonomous driving. https://github.com/ApolloAuto/ apollo, 2017. Online; accessed 11 October 2023.   \n[8] Turner, A., D. Tsipras, A. Madry. Clean-label backdoor attacks. arxiv preprint arXiv:2206.04881, 2018.   \n[9] Saha, A., A. Subramanya, H. Pirsiavash. Hidden trigger backdoor attacks. In Proceedings of the AAAI conference on artificial intelligence, vol. 34, pages 11957\u201311965. 2020.   \n[10] Yao, Y., H. Li, H. Zheng, et al. Latent backdoor attacks on deep neural networks. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, pages 2041\u2013 2055. 2019.   \n[11] Liu, Y., X. Ma, J. Bailey, et al. Reflection backdoor: A natural backdoor attack on deep neural networks. In European Conference on Computer Vision, pages 182\u2013199. Springer, 2020.   \n[12] Tang, R., M. Du, N. Liu, et al. An embarrassingly simple approach for trojan attack in deep neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 218\u2013228. 2020.   \n[13] Li, Y., T. Zhai, B. Wu, et al. Rethinking the trigger of backdoor attack. arXiv preprint arXiv:2004.04692, 2020.   \n[14] Nguyen, A., A. Tran. Wanet\u2013imperceptible warping-based backdoor attack. arXiv preprint arXiv:2102.10369, 2021.   \n[15] Doan, K., Y. Lao, W. Zhao, et al. Lira: Learnable, imperceptible and robust backdoor attacks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11966\u2013 11976. 2021.   \n[16] Li, S., M. Xue, B. Z. H. Zhao, et al. Invisible backdoor attacks on deep neural networks via steganography and regularization. IEEE Transactions on Dependable and Secure Computing, 18(5):2088\u20132105, 2020.   \n[17] Nguyen, T. A., A. Tran. Input-aware dynamic backdoor attack. Advances in Neural Information Processing Systems, 33:3454\u20133464, 2020.   \n[18] Bagdasaryan, E., V. Shmatikov. Blind backdoors in deep learning models. In 30th USENIX Security Symposium (USENIX Security 21), pages 1505\u20131521. 2021.   \n[19] Bai, J., B. Wu, Y. Zhang, et al. Targeted attack against deep neural networks via filpping limited weight bits. arXiv preprint arXiv:2102.10496, 2021.   \n[20] Li, Y., Y. Li, B. Wu, et al. Invisible backdoor attack with sample-specific triggers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16463\u201316472. 2021.   \n[21] Rakin, A. S., Z. He, D. Fan. Tbt: Targeted neural network attack with bit trojan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13198\u201313207. 2020.   \n[22] Doan, K., Y. Lao, P. Li. Backdoor attack with imperceptible input and latent modification. Advances in Neural Information Processing Systems, 34:18944\u201318957, 2021.   \n[23] Wenger, E., J. Passananti, A. N. Bhagoji, et al. Backdoor attacks against deep learning systems in the physical world. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6206\u20136215. 2021.   \n[24] Salem, A., R. Wen, M. Backes, et al. Dynamic backdoor attacks against machine learning models. In 2022 IEEE 7th European Symposium on Security and Privacy (EuroS&P), pages 703\u2013718. IEEE, 2022.   \n[25] Doan, K. D., Y. Lao, P. Li. Marksman backdoor: Backdoor attacks with arbitrary target class. arXiv preprint arXiv:2210.09194, 2022.   \n[26] Lv, P., C. Yue, R. Liang, et al. A data-free backdoor injection approach in neural networks. In 32nd USENIX Security Symposium (USENIX Security 23), pages 2671\u20132688. 2023.   \n[27] Hong, S., N. Carlini, A. Kurakin. Handcrafted backdoors in deep neural networks. In NeurIPS. 2022.   \n[28] Bober-Irizar, M., I. Shumailov, Y. Zhao, et al. Architectural backdoors in neural networks, 2023.   \n[29] NC. Code of Neural Cleanse. https://github.com/bolunwang/backdoor. January 2023.   \n[30] Xu, X., Q. Wang, H. Li, et al. Detecting ai trojans using meta neural analysis. In 2021 IEEE Symposium on Security and Privacy $(S P)$ , pages 103\u2013120. IEEE, 2021.   \n[31] Wang, B., Y. Yao, S. Shan, et al. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In Proceedings of the IEEE Symposium on Security and Privacy (IEEE S&P). 2019.   \n[32] Goldwasser, S., M. P. Kim, V. Vaikuntanathan, et al. Planting undetectable backdoors in machine learning models. arXiv preprint arXiv:2204.06974, 2022.   \n[33] Rakin, A. S., Z. He, D. Fan. Bit-flip attack: Crushing neural network with progressive bit search. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1211\u20131220. 2019.   \n[34] Hong, S., P. Frigo, Y. Kaya, et al. Terminal brain damage: Exposing the graceless degradation in deep neural networks under hardware fault attacks. In USENIX Security Symposium, pages 497\u2013514. 2019.   \n[35] Rakin, A. S., Z. He, J. Li, et al. T-bfa: Targeted bit-flip adversarial weight attack. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11):7928\u20137939, 2021.   \n[36] Chen, H., C. Fu, J. Zhao, et al. Proflip: Targeted trojan attack with progressive bit flips. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7718\u20137727. 2021.   \n[37] Fang, P., B. Cao, J. Jia, et al. Backdoor attack for federated learning with fake clients.   \n[38] Li, M., W. Wan, Y. Ning, et al. Darkfed: A data-free backdoor attack in federated learning. arXiv preprint arXiv:2405.03299, 2024.   \n[39] Steinhardt, J., P. W. Koh, P. Liang. Certified defenses for data poisoning attacks. In Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS). 2017.   \n[40] Tran, B., J. Li, A. Madry. Spectral signatures in backdoor attacks. In Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS). 2018.   \n[41] Du, M., R. Jia, D. Song. Robust anomaly detection and backdoor attack detection via differential privacy. arXiv preprint arXiv:1911.07116, 2019.   \n[42] Weber, M., X. Xu, B. Karla\u0161, et al. Rab: Provable robustness against backdoor attacks. arXiv preprint arXiv:2003.08904, 2020.   \n[43] Guo, W., L. Wang, Y. Xu, et al. Towards inspecting and eliminating trojan backdoors in deep neural networks. In 2020 IEEE International Conference on Data Mining (ICDM), pages 162\u2013171. IEEE, 2020.   \n[44] Liu, Y., W.-C. Lee, G. Tao, et al. Abs: Scanning neural networks for back-doors by artificial brain stimulation. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, pages 1265\u20131282. 2019.   \n[45] Gao, Y., C. Xu, D. Wang, et al. Strip: A defence against trojan attacks on deep neural networks. In Proc. of ACSAC. 2019.   \n[46] Chou, E., F. Tramer, G. Pellegrino. Sentinet: Detecting localized universal attacks against deep learning systems. In Proc. of IEEE Security and Privacy Workshops (SPW). 2020.   \n[47] Ma, W., D. Wang, R. Sun, et al. The\" beatrix\u201dresurrections: Robust backdoor detection via gram matrices. arXiv preprint arXiv:2209.11715, 2022.   \n[48] Xiang, C., A. N. Bhagoji, V. Sehwag, et al. {PatchGuard}: A provably robust defense against adversarial patches via small receptive fields and masking. In 30th USENIX Security Symposium (USENIX Security 21), pages 2237\u20132254. 2021.   \n[49] Xiang, C., S. Mahloujifar, P. Mittal. {PatchCleanser}: Certifiably robust defense against adversarial patches for any image classifier. In 31st USENIX Security Symposium (USENIX Security 22), pages 2065\u20132082. 2022.   \n[50] Wang, Z., K. Mei, H. Ding, et al. Rethinking the reverse-engineering of trojan triggers. arXiv preprint arXiv:2210.15127, 2022.   \n[51] Liu, K., B. Dolan-Gavitt, S. Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In Proceedings of Research in Attacks, Intrusions, and Defenses (RAID). 2018.   \n[52] Wu, D., Y. Wang. Adversarial neuron pruning purifies backdoored deep models. Advances in Neural Information Processing Systems, 34:16913\u201316925, 2021.   \n[53] Zeng, Y., S. Chen, W. Park, et al. Adversarial unlearning of backdoors via implicit hypergradient. In International Conference on Learning Representations. 2022.   \n[54] Chai, S., J. Chen. One-shot neural backdoor erasing via adversarial weight masking. In A. H. Oh, A. Agarwal, D. Belgrave, K. Cho, eds., Advances in Neural Information Processing Systems. 2022.   \n[55] Zheng, R., R. Tang, J. Li, et al. Data-free backdoor removal based on channel lipschitzness. In European Conference on Computer Vision, pages 175\u2013191. Springer, 2022.   \n[56] Simonyan, K., A. Zisserman. Very deep convolutional networks for large-scale image recognition, 2014.   \n[57] He, K., X. Zhang, S. Ren, et al. Deep residual learning for image recognition, 2015.   \n[58] Carlini, N., A. Terzis. Poisoning and backdooring contrastive learning. arXiv preprint arXiv:2106.09667, 2021.   \n[59] Yan, Z., G. Li, Y. TIan, et al. Dehib: Deep hidden backdoor attack on semi-supervised learning via adversarial perturbation. In Proc of AAAI. 2021.   \n[60] Jia, J., Y. Liu, N. Z. Gong. Badencoder: Backdoor attacks to pre-trained encoders in selfsupervised learning. In 2022 IEEE Symposium on Security and Privacy $(S P)$ , pages 2043\u20132059. IEEE, 2022.   \n[61] Saha, A., A. Tejankar, S. A. Koohpayegani, et al. Backdoor attacks on self-supervised learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13337\u201313346. 2022.   \n[62] Bagdasaryan, E., A. Veit, Y. Hua, et al. How to backdoor federated learning. In Proc. of AISTAT. 2020.   \n[63] Wang, H., K. Sreenivasan, S. Rajput, et al. Attack of the tails: Yes, you really can backdoor federated learning. In Proc. of NeurIPS. 2020.   \n[64] Xie, C., K. Huang, P.-Y. Chen, et al. Dba: Distributed backdoor attacks against federated learning. In Proc. of ICLR. 2019.   \n[65] Dai, J., C. Chen, Y. Li. A backdoor attack against lstm-based text classification systems. IEEE Access, 7:138872\u2013138878, 2019.   \n[66] Chen, X., A. Salem, M. Backes, et al. Badnl: Backdoor attacks against nlp models. In ICML 2021 Workshop on Adversarial Machine Learning. 2021.   \n[67] Xi, Z., R. Pang, S. Ji, et al. Graph backdoor. In 30th USENIX Security Symposium (USENIX Security 21), pages 1523\u20131540. 2021.   \n[68] Zhang, Z., J. Jia, B. Wang, et al. Backdoor attacks to graph neural networks. In Proceedings of the 26th ACM Symposium on Access Control Models and Technologies, pages 15\u201326. 2021.   \n[69] Wang, L., Z. Javed, X. Wu, et al. Backdoorl: Backdoor attack against competitive reinforcement learning. arXiv preprint arXiv:2105.00579, 2021.   \n[70] Kiourti, P., K. Wardega, S. Jha, et al. Trojdrl: Trojan attacks on deep reinforcement learning agents. arXiv preprint arXiv:1903.06638, 2019.   \n[71] Moosavi-Dezfooli, S.-M., A. Fawzi, O. Fawzi, et al. Universal adversarial perturbations. In Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.   \n[72] Madry, A., A. Makelov, L. Schmidt, et al. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Proof of Lemma 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "[Proof of Lemma 1] A clean input $\\mathbf{x}$ cannot activate the neuron $s_{1}$ when $\\begin{array}{r}{\\sum_{n\\in\\Gamma(\\mathbf{m})}w_{n}(x_{n}-\\delta_{n})+\\lambda\\leq}\\end{array}$ 0, i.e., $\\begin{array}{r}{\\sum_{n\\in\\Gamma(\\mathbf{m})}w_{n}(\\delta_{n}-x_{n})\\geq\\lambda}\\end{array}$ . We prove this condition is equivalent to $\\begin{array}{r}{\\sum_{n\\in\\Gamma(\\mathbf{m})}|w_{n}(x_{n}-\\right.}\\end{array}$ $\\delta_{n})|\\,\\geq\\,\\lambda$ . Suppose $w_{n}\\leq0$ , then we know $\\delta_{n}=\\alpha_{n}^{l}$ based on Equation 3. Since $x_{n}\\in[\\alpha_{n}^{l},\\alpha_{n}^{u}]$ , we know $x_{n}\\,\\geq\\,\\delta_{n}$ . Therefore, we have $w_{n}(\\delta_{n}-x_{n})\\geq0$ , i.e., $w_{n}(\\delta_{n}-x_{n})\\,=\\,|w_{n}(\\delta_{n}\\stackrel{{\\scriptscriptstyle...}}{-}x_{n})|$ . Similarly, we can show that $w_{n}(\\delta_{n}-x_{n})\\,=\\,|w_{n}(\\delta_{n}-x_{n})|$ when $w_{n}\\,>\\,0$ . Therefore, we have $\\begin{array}{r}{\\sum_{n\\in\\Gamma(\\mathbf{m})}w_{n}(\\delta_{n}-x_{n})=\\sum_{n\\in\\Gamma(\\mathbf{m})}|w_{n}(x_{n}\\cdot\\delta_{n})|}\\end{array}$ , i.e., the condition that a clean input cannot activate $s_{1}$ is as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{n\\in\\Gamma(\\mathbf{m})}|w_{n}(x_{n}-\\delta_{n})|\\geq\\lambda,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\lambda$ is a hyperparameter. ", "page_idx": 14}, {"type": "text", "text": "B Theoretical Analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "From Lemma 1, we know that a clean input $\\mathbf{x}$ cannot activate the backdoor path if and only if the following equation is satisfied: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{n\\in\\Gamma(\\mathbf{m})}|w_{n}(x_{n}-\\delta_{n})|\\geq\\lambda.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In other words, $\\mathbf{x}$ can only activate the backdoor path if we have $\\begin{array}{r}{\\sum_{n\\in\\Gamma(\\mathbf{m})}\\left|w_{n}(x_{n}-\\delta_{n})\\right|\\,<\\,\\lambda}\\end{array}$ Suppose that the input $\\mathbf{x}$ is sampled from a certain distribution. We use $p$ to denote the probability that an input $\\mathbf{x}$ can activate our injected backdoor in a classifier. Then, we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\np=\\mathrm{Pr}(\\sum_{n\\in\\Gamma(\\mathbf{m})}|w_{n}(x_{n}-\\delta_{n})|<\\lambda).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This probability is very small when $\\lambda$ is small and $w_{n}$ is large. We have the following example when each entry of $\\mathbf{x}$ follows uniform distribution. ", "page_idx": 14}, {"type": "text", "text": "Example 1 Suppose $x_{n}$ $(n=1,2,\\cdots\\,,d)$ follows a uniform distribution over $[0,1]$ . Moreover, we assume $x_{n}$ to be i.i.d.. When $|w_{n}|\\geq\\alpha$ for $n\\in\\Gamma(\\mathbf{m}).$ , we have $p\\leq{\\frac{(2\\lambda)^{e}}{\\alpha^{e}e!}}$ (\u03b12e\u03bbe)! , where e is the number of elements in $\\Gamma(\\mathbf{m})$ . ", "page_idx": 14}, {"type": "text", "text": "When $|w_{n}|\\geq\\alpha$ , we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{p}=\\mathrm{Pr}(\\displaystyle\\sum_{n\\in\\Gamma(\\mathbf{m})}\\left\\vert w_{n}(x_{n}-\\delta_{n})\\right\\vert<\\lambda)}\\\\ {\\ \\ \\leq\\mathrm{Pr}(\\displaystyle\\sum_{n\\in\\Gamma(\\mathbf{m})}\\left\\vert x_{n}-\\delta_{n}\\right\\vert<\\lambda/\\alpha).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Moreover, since $x_{n}$ follows a uniform distribution between 0 and 1, the probability $p$ is no larger than the volume of an $\\ell_{1}$ -ball with radius $\\lambda/\\alpha$ in the space $\\mathbb{R}^{e}$ , where $e$ is the number of elements in $\\Gamma(\\mathbf{m})$ . The volume can be computed as (\u03b12e\u03bbe)! . Thus, we have p \u2264 $\\begin{array}{r}{p\\leq\\frac{(2\\lambda)^{e}}{\\alpha^{e}e!}}\\end{array}$ . We have the following remarks from our above example: ", "page_idx": 14}, {"type": "text", "text": "\u2022 As a concrete example, we have $p\\leq3.13\\times10^{-9}$ when $\\lambda=1$ , $\\alpha=1$ , and $e=16$ for a 4 \u00d7 4 trigger. ", "page_idx": 14}, {"type": "text", "text": "\u2022 In practice, x may follow a different distribution. We empirically find that almost all testing examples cannot activate the backdoor path when $\\lambda$ is small (e.g., 0.1) on various benchmark datasets, indicating that it is hard in general for regular data to activate the backdoor path. As we will show, this enables us to perform theoretical analysis on the utility and effectiveness of the backdoored classifier by DFBA. ", "page_idx": 14}, {"type": "text", "text": "B.1 Utility Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Given an input $\\mathbf{x}=[x_{1},x_{2},\\cdot\\cdot\\cdot\\,,x_{d}]$ and a backdoored classifier $g$ crafted by our attack. Based on Lemma 1, we know the input $\\mathbf{x}$ cannot activate the backdoor path if the following condition is satisfied: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{n\\in\\Gamma(\\mathbf{m})}|w_{n}(x_{n}-\\delta_{n})|\\geq\\lambda,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $x_{n}$ is the feature value of $\\mathbf{x}$ at the $n$ th dimension, $w_{n}$ is the weight between the first neuron in the backdoor path of the backdoored classifier $g$ and $x_{n}$ , $\\Gamma(\\mathbf{m})$ is a set of indices of the location of the backdoor trigger, and $\\delta_{n}$ $(n\\in\\Gamma(\\mathbf{m}))$ is the value of the backdoor pattern. The above equation means a clean input $\\mathbf{x}$ cannot activate the backdoor path when the weighted sum of its deviation from the backdoor trigger is no smaller than $\\lambda$ (a hyper-parameter). ", "page_idx": 15}, {"type": "text", "text": "If $\\mathbf{x}$ cannot activate the backdoor path, the outputs of the neurons in the backdoor path are 0. Thus, the output of the backdoored classifier does not change if those neurons are pruned. As a result, the prediction of the backdoored classifier for $\\mathbf{x}$ is the same as that of the pruned classifier. ", "page_idx": 15}, {"type": "text", "text": "B.2 Attack Efficacy Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We will theoretically analyze the performance of our DFBA under various backdoor defenses. ", "page_idx": 15}, {"type": "text", "text": "B.2.1 Undetectable Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We consider two types of defenses: query-based defenses [30] and gradient-based defenses [31]. ", "page_idx": 15}, {"type": "text", "text": "[Proof of Proposition 1] Based on Theorem 1, the output of the backdoored classifier is the same as the pruned classifier if an input cannot activate the backdoor path. Thus, the output for any input from the defense dataset will be the same for the two classifiers, which leads to the same detection result. ", "page_idx": 15}, {"type": "text", "text": "[Proof of Proposition 2] When inputs cannot activate backdoor path, gradients of outputs of the backdoored classifier and pruned classifier with respect to their inputs are the same. Thus, detection results are same. ", "page_idx": 15}, {"type": "text", "text": "Pruning-based defenses [51, 55]: We note that a defender can prune the neurons whose outputs on clean data are small or Lipschitz constant is large to mitigate our attack [51, 55]. As we will empirically show in Section 5, our DFBA can be adapted to evade those defenses. Moreover, we empirically find that our adaptive attack designed for pruning-based defenses can also evade other defenses such as Neural Cleanse and MNTD (see Section 5 for details). Therefore, we can use our adaptive attack in practice if we don\u2019t have any information on the defense. ", "page_idx": 15}, {"type": "text", "text": "B.2.2 Unremovable Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Twhide eglyo aul soefd  btaoc rkedmooorv er etmheo vbaalc iks dtoo orre imn oav ec ltahses ibfaiecr.k dSouopr pions ae  cwlaes shiafvieer .a  Fdoart ianssett $\\mathcal{D}_{d}=\\{\\mathbf{x}^{i},y^{i}\\}_{i=1}^{\\overline{{N}}}$ Given a classifier $f$ , fine-tuning aims to train it such that it has high classification accuracy on $\\mathcal{D}_{d}$ . Formally, we have the following optimization problem: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{f^{\\prime}}\\frac{1}{|\\mathcal{D}_{d}|}\\sum_{(\\mathbf{x},y)\\in\\mathcal{D}_{d}}\\ell(f^{\\prime}(\\mathbf{x}),y),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\ell$ is the loss function, e.g., cross-entropy loss, and $f^{\\prime}$ is initialized with $f$ . We can use SGD to solve the optimization problem. However, fine-tuning is ineffective against our DFBA. Formally, we have: ", "page_idx": 15}, {"type": "text", "text": "[Proof of Proposition 3] Given that 1) each training input cannot activate the backdoor path, and 2) the output of the neurons in the backdoor path is independent of the neurons that are not in the backdoor path, the gradient of loss function with respect to parameters of the neurons in the backdoor path is 0. Thus, the parameters of those neurons do not change. ", "page_idx": 15}, {"type": "text", "text": "Table 3: The neural network architectures for MNIST and FashionMNIST. ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "pX71TM2MLh/tmp/b169670fa3e24ef6ee78fe2f4bb6d570aaf220700d208d8a2ec256b44c78e167.jpg", "table_caption": ["(a) FCN "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "pX71TM2MLh/tmp/6f800cd095768e97a98650ec1086565e5089ded1097c707866f03ea0381d5197.jpg", "table_caption": ["(b) CNN "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C More Details of Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Datasets: We consider the following benchmark datasets: MNIST, Fashion-MNIST, CIFAR10, GTSRB, and ImageNet. ", "page_idx": 16}, {"type": "text", "text": "\u2022 MNIST: MNIST dataset is used for digit classification. In particular, the dataset contains 60,000 training images and 10,000 testing images, where the size of each image is $28\\times28$ . Moreover, each image belongs to one of the 10 classes.   \n\u2022 Fashion-MNIST: Fashion-MNIST is a dataset of Zalando\u2019s article images. Specifically, the dataset contains 60,000 training images and 10,000 testing images. Each image is a 28 $\\times\\ 28$ grayscale image and has a label from 10 classes.   \n\u2022 CIFAR10: This dataset is used for object recognition. The dataset consists of $60{,}000\\;32\\;\\times$ $32\\times3$ colour images, each of which belongs to one of the 10 classes. The dataset is divided into 50,000 training images and 10,000 testing images.   \n\u2022 GTSRB: This dataset is used for traffic sign recognition. The dataset contains 26,640 training images and 12,630 testing images, where each image belongs to one of 43 classes. The size of each image is $32\\times32\\times3$ .   \n\u2022 ImageNet: The ImageNet dataset is used for object recognition. There are 1,281,167 training images and 50,000 testing images in the dataset, where each image has a label from 1,000 classes. The size of each image is $224\\times224\\times3$ . ", "page_idx": 16}, {"type": "text", "text": "Table 4 summarizes the statistics of those datasets. Unless otherwise mentioned, we use MNIST dataset in our evaluation. ", "page_idx": 16}, {"type": "text", "text": "Parameter settings: We conducted all experiments on an NVIDIA A100 GPU, and the random seed for all experiments was set to 0. Our attack has the following parameters: threshold $\\lambda$ , amplification factor $\\gamma$ , and trigger size. Unless otherwise mentioned, we adopt the following default parameters: we set $\\lambda=0.1$ . Moreover, we set $\\gamma$ to satisfy $\\lambda\\gamma^{L-1}=100$ , where $L$ is the total number of layers of a neural network. In Figure 7, we conduct an ablation study on $\\lambda$ and $\\gamma$ . We find that $\\lambda$ and $\\gamma$ could influence the utility of a classifier and attack effectiveness. When $\\lambda$ is small, our method would not influence utility. When $\\gamma$ is large, our attack could consistently achieve a high attack success rate. Thus, in practice, we could set a small $\\lambda$ and a large $\\gamma$ . ", "page_idx": 16}, {"type": "text", "text": "We set the size of the backdoor trigger (in the bottom right corner) to $4\\times4$ and the target class to 0 for all datasets. In our ablation studies, we will study their impact on our attack. In particular, we set all other parameters to their default values when studying the impact of one parameter. Note that our trigger pattern is calculated via solving the optimization problem in Equation 2, whose solution can be found in Equation 3. Figure 8 (in Appendix K) visualizes the trigger pattern. ", "page_idx": 16}, {"type": "table", "img_path": "pX71TM2MLh/tmp/504e07b664974e080f0117a6932179877ae1853fcf3f006d9610ff345330a15b.jpg", "table_caption": ["Table 4: Dataset statistics. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "pX71TM2MLh/tmp/d39a6948aa83184ec308f098ef445a75a1de4f7a5399e65eb07168c693e85e23.jpg", "table_caption": ["Table 5: Number of clean testing inputs and backdoored testing inputs that can activate our backdoor path. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "D Effectiveness of DFBA Under State-of-the-art Defenses ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Our DFBA cannot be detected by Neural Cleanse [31]: Neural Cleanse (NC) leverages a clean dataset to reverse engineer backdoor triggers and use them to detect whether a classifier is backdoored. In our experiments, we use the training dataset to reverse engineer triggers. We adopt the publicly available code [29] in our implementation. We train 5 clean classifiers and then respectively craft 5 backdoored classifiers using DFBA and Hong et al. [27]. We report the detection rate which is the fraction of backdoored classifiers that are correctly identified by NC for each method. The detection rate of NC for DFBA is 0. In contrast, NC can achieve $100\\%$ detection rate for Hong et al. [27] based on the results in Figure 9 in Hong et al. [27] (our setting is the same as Hong et al. [27]). Therefore, our DFBA is more stealthy than Hong et al. [27] under NC. The reason why NeuralCleanse does not work is as follows. NeuralCleanse uses a validation dataset to reverse engineer a trigger such that a classifier is very likely to predict the target class when the trigger is added to inputs in the validation dataset. However, our backdoor path is very hard to be activated by non-backdoored inputs (as shown in Table 5). In other words, our backdoor path is not activated when NeuralCleanse reverse engineers the trigger, which makes NeuralCleanse ineffective. ", "page_idx": 17}, {"type": "image", "img_path": "pX71TM2MLh/tmp/b825a79dd033abbf46060d0e4e934759306583e808a56c87ca9248bd63671123.jpg", "img_caption": ["Figure 3: Comparing DFBA with Hong et al. [27] under fine-tuning. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "pX71TM2MLh/tmp/ea1bbdecc810863584b0c2c3e9124ea09ac6fb952f59d0977cc66eb0f42c9730.jpg", "img_caption": ["Figure 4: Comparing DFBA with Hong et al. [27] under pruning [51]. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "pX71TM2MLh/tmp/d3b699baf8d2e163f661e22e56feb87881207da34f0d9f5c68c647580ad28efd.jpg", "img_caption": ["Figure 5: Comparing our DFBA with Hong et al. [27] under fine-tuning after pruning neurons on MNIST. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Our DFBA is resilient to fine-tuning: Given a backdoored model, a defender can use clean data to fine-tune it to remove the backdoor. To consider a strong defense, we use the entire training dataset of MNIST to fine-tune the backdoored classifier, where the learning rate is 0.01. Figure 3 shows the experimental results of Hong et al. [27] and DFBA. We find that the ASR of DFBA remains high when fine-tuning the backdoored classifier for different epochs. In contrast, the ASR of Hong et al. [27] decreases as the number of fine-tuning epochs increases. ", "page_idx": 18}, {"type": "text", "text": "Our DFBA is resilient to fine-pruning: Liu et al. [51] proposed to prune neurons whose outputs are small on a clean dataset in a middle layer of a classifier to remove the backdoor. Our DFBA can be adapted to evade this defense. Suppose we have a clean validation dataset, Liu et al. [51] proposed to remove neurons whose outputs are small in a certain middle layer (e.g., the last fully connected layer in a fully connected neural network). Our DFBA can be adapted to evade this attack. Our idea is to let both clean and backdoored inputs activate our backdoor path. As we optimize the backdoor trigger, the outputs of neurons on backdoored inputs are much larger than those on clean inputs. Thus, our adapted backdoor attack is effective while maintaining classification accuracy on clean inputs. In particular, we randomly sample from a zero-mean Gaussian distribution $\\mathcal{N}(0,\\sigma^{\\tilde{2}})$ as parameters that are related to features whose indices in $\\Gamma(\\mathbf{m})$ for the selected neuron in the first layer, where $\\sigma$ is the standard deviation of Gaussian noise. Moreover, we don\u2019t change the bias of the selected neuron in the first layer such that both clean inputs and backdoored inputs can activate the backdoor path. In our experiments, we set $\\sigma=4$ , 000 and $\\gamma=1$ . Note that we set $\\gamma=1$ because the output of the neuron selected from the first layer is already very large for a backdoored input. ", "page_idx": 18}, {"type": "table", "img_path": "pX71TM2MLh/tmp/1e5e6a492c91553d1f23f9781415b7f8c21a783c11e845ad4bf878caec1b81ac.jpg", "table_caption": ["Table 6: Our attack is effective under I-BAU [53]. "], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "pX71TM2MLh/tmp/70f38699db43f8c14b5fb730e3cfca92ef352f35e4340db270592df7523b284b.jpg", "img_caption": ["Figure 6: The ACC and ASR of our attack under Lipchitz Pruning on MNIST. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "We prune neurons whose outputs are small on the training dataset. Figure 4 shows results for DFBA and Hong et al. [27]. We find that DFBA can consistently achieve high ASR when we prune different fractions of neurons. In contrast, the ASR of Hong et al. [27] decreases as more neurons are pruned. We further fine-tune the pruned model (we prune neurons until the ACC drop is up to $5\\%$ ) using the training dataset. Figure 5 shows the results. We find that DFBA can still achieve high ASR after fine-tuning. ", "page_idx": 19}, {"type": "text", "text": "MNTD [30] cannot detect DFBA: MNTD trains a meta classifier to predict whether a classifier is backdoored or not. Roughly speaking, the idea is to train a set of clean models and backdoored models. Specifically, given a set of inputs (called query set) and a model, MNTD uses the output of the model on the query set as its feature. Then, a meta-classifier is trained to distinguish clean models and backdoored models based on their features. Note that they also optimize the query set to boost the performance. ", "page_idx": 19}, {"type": "text", "text": "We evaluate the performance of MNTD for DFBA on MNIST. We use the publicly available code of MNTD in our experiments4. We respectively train 5 clean classifiers using different seeds and then craft 5 backdoored classifiers using DFBA for FCN and CNN. We use the detection rate as the evaluation metric, which measures the fraction of backdoored classifiers that are correctly identified by MNTD. We find the detection rate of MNTD is 0 for both FCN and CNN, i.e., MNTD is ineffective for DFBA. Our empirical results are consistent with our theorem (Proposition 1). ", "page_idx": 19}, {"type": "text", "text": "I-BAU [53] cannot remove DFBA\u2019s backdoor: Zeng et al. [53] proposed I-BAU, which aims to unlearn the backdoor in a classifier. I-BAU formulates the backdoor unlearn as a minimax optimization problem. In the inner optimization problem, I-BAU aims to find a trigger such that the classifier has a high classification loss when the trigger is added to clean inputs. In the outer optimization problem, I-BAU aims to re-train the classifier such that it has high classification accuracy on clean inputs added with the optimized trigger. ", "page_idx": 19}, {"type": "image", "img_path": "pX71TM2MLh/tmp/e1078cb1d5f04f4ac11c7c6b974a17c41965c753feb52190c8cb4f3e58ee76b4.jpg", "img_caption": ["Figure 7: Impact of $\\lambda,\\gamma_{:}$ , and trigger size on DFBA. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "We apply I-BAU to unlearn the backdoor injected by DFBA on MNIST. We use the publicly available code in our implementation5. Table 6 shows our experimental results. We find that the ASR is still very high after applying I-BAU to unlearn the backdoor in the classifier injected by DFBA. Our results demonstrate that I-BAU cannot effectively remove the backdoor. ", "page_idx": 20}, {"type": "text", "text": "DFBA can be adapted to evade Lipschitz Pruning [55]: Zheng et al. [55] proposed to leverage Lipschitz constant to prune neurons in a classifier to remove the backdoor. In particular, for the $k$ th convolutional layer, Zheng et al. [55] proposed to compute a Lipschitz constant for each convolution fliter. Then, Zheng et al. [55] compute the mean (denoted as $\\mu_{k}$ ) and standard deviation (denoted as $\\sigma_{k}$ ) of those Lipschitz constants. The convolution filters whose Lipschitz constants are larger than $\\mu_{k}+u\\sigma_{k}$ are pruned, where $u$ is a hyperparameter. The method can be extended to a fully connected layer by computing a Lipschitz constant for each neuron. ", "page_idx": 20}, {"type": "text", "text": "Our DFBA can be adapted to evade [55]. In particular, we set $\\gamma$ to be a small value for the neurons selected in the middle layers such that its Lipschitz constant is smaller than $\\mu_{k}$ . To ensure the effectiveness of backdoor attacks, our idea is to change the parameter of the neurons in the output layer. In particular, we can set the weight between $s_{L-1}$ and the output neuron for the target class $y_{t c}$ to be a larger number but set the weight between $s_{l}$ and the remaining output neurons to be a small number. Note that the neurons in the output layer are not pruned in [55]. Figure 6 shows our experimental results. We find that our DFBA can consistently achieve high ASR for different $u$ , which demonstrates the effectiveness of our backdoor attacks. We note that the ACC is low for small $u$ because more neurons are pruned by Lipschitz Pruning [55] when $u$ is smaller. ", "page_idx": 20}, {"type": "text", "text": "Effectiveness of our adaptive attacks tailored to pruning-based defenses for other defenses: Our attack requires the attacker to know the defense information to have a formal guarantee of the attack efficacy under those defenses. When the attacker does not have such information, the attacker can use our adaptive attack designed for pruning-based defenses in practice. We performed evaluations under our default setting to validate this. We find that our adaptive attack designed for fine-pruning can also evade Neural Cleanse, fine-tuning, MNTD, I-BAU, and Lipschitz pruning. In particular, the detection rate of both Neural Cleanse and MNTD for backdoored classifiers crafted by DFBA is $0\\%$ (we apply the detection on five backdoored classifiers and report the detection accuracy as the fraction of backdoored classifiers that are detected by each method), which means they cannot detect backdoored classifiers. The attack success rate (ASR) is still $100\\%$ after we fine-tune the backdoored classifier for 50 epochs (or use I-BAU to unlearn the backdoor or use Lipschitz pruning to prune neurons to remove the backdoor). Our results demonstrate that our adaptive attack can be used when the information on the defense is unavailable. ", "page_idx": 20}, {"type": "text", "text": "E Ablation Studies ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We perform ablation studies to study the impact of hyperparameters of our DFBA. In particular, our DFBA has the following hyperparameters: threshold $\\lambda$ , amplification factor $\\gamma$ , and trigger size. When we study the impact of each hyperparameter, we set the remaining hyperparameters to their default values. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Impact of $\\lambda$ : Figure 7a shows the impact of $\\lambda$ on MNIST. We have the following observations. First, our attack consistently achieves high ASR. The reason is that the backdoor path crafted by DFBA is always activated for backdoored inputs when $\\lambda>0$ . Second, DFBA achieves high BA when $\\lambda$ is very small, i.e., DFBA can maintain the classification accuracy of the backdoored classifier for clean testing inputs when $\\lambda$ is small. Third, BA decreases when $\\lambda$ is larger than a threshold. This is because the backdoored path can also be activated by clean inputs when $\\lambda$ is large. As a result, those clean inputs are predicted as the target class which results in the classification loss. Thus, we can set $\\lambda$ to be a small value in practice, e.g., 0.1. ", "page_idx": 21}, {"type": "text", "text": "Impact of $\\gamma$ : Figure 7b shows the impact of $\\gamma$ on MNIST. The ASR of DFBA first increases as $\\gamma$ increases and then becomes stable. The reason is that the output of neurons in the backdoor path is larger for a backdoored input when $\\gamma$ is larger. As a result, the backdoored input is more likely to be predicted as the target class. Thus, we can set $\\gamma$ to be a large value in practice. ", "page_idx": 21}, {"type": "text", "text": "Impact of trigger size: Figure 7c shows the impact of trigger sizes on MNIST. We find that our backdoor attack can consistently achieve high ASR and BA for backdoor triggers with different sizes. For instance, our attack could still achieve a $100\\%$ ASR when the size of the trigger is $2\\times2$ . ", "page_idx": 21}, {"type": "text", "text": "Impact of trigger location: We note that our attack is also effective even if the trigger position changes for convolutional neural networks. The reason is that a convolutional filter is applied in different locations of an image to perform convolution operation. Thus, the output of the convolution filter would be large when the trigger is present and thus activate our back path, making our attack successful. We also validate this by experiments. For instance, we find that our attack could still achieve a $100\\%$ ASR when we change the location of the trigger under the default setting. ", "page_idx": 21}, {"type": "text", "text": "F Neuron Selection for a CNN ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For a convolutional neural network, a convolution fliter generates a channel for an input. In particular, each value in the channel represents the output of one neuron, where all neurons whose outputs are in the same channel share the same parameters. We randomly select one neuron whose output value depends on the features with indices in $\\Gamma(\\mathbf{m})$ . We note that, as neurons in the same channel share the parameters, they would be affected if we change the parameters of one neuron. We consider this when we design our DFBA. As a result, our DFBA can maintain the classification accuracy of the classifier for normal testing inputs as shown in our experimental results. ", "page_idx": 21}, {"type": "text", "text": "G Comparing with Hong et al. [27] on CIFAR10 Dataset ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We also compare our attack with Hong et al. on CIFAR10 dataset, where the classifier is CNN. We compare DFBA with Hong et al. for fine-tuning and fine-pruning. Our comparison results are as follows. After fine-tuning, the ASRs of our DFBA and Hong et al. are $100\\%$ and $88\\%$ , respectively. After fine-pruning, the ASRs of our DFBA and Hong et al. are $100\\%$ and $84\\%$ , respectively. Our results demonstrate that our attack is more effective than Hong et al.. Our observations on CIFAR10 are consistent with those on MNIST. ", "page_idx": 21}, {"type": "text", "text": "H Evaluation of Neural Cleanse, MNTD, I-BAU, and Lipschitz pruning against Our Attack on CIFAR10 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We also evaluate other defenses on CIFAR10, including Neural Cleanse, MNTD, I-BAU, and Lipschitz pruning against our attack. The detection accuracy (we apply the detection on five backdoored classifiers and report the detection accuracy as the fraction of backdoored classifiers that are detected by each method) of Neural Cleanse and MNTD is $0\\%$ for our DFBA. Our DFBA can still achieve a $100\\%$ ASR after we apply Lipschitz pruning to the backdoored classifier. We find that I-BAU could indeed reduce the ASR of our method to $10\\%$ . But it also significantly jeopardized the model\u2019s classification accuracy on the clean data (from $80.15\\%$ to $18.59\\%$ ). The results show that after retraining, the model performs almost randomly. We tried different hyperparameters for I-BAU and consistently have this observation. These results show that most defense methods are not effective against our method. Even I-BAU can remove our backdoor, it achieves this by significantly sacrificing the utility. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "I Potential Adaptive Defenses ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We designed two adaptive defense methods tailored for DFBA. These methods exploit the fact that our DFBA-constructed backdoor paths are rarely activated on clean data and that some weights are replaced with zeros when modifying the model weights: Anomaly detection: Check the number of zero weights in the model. Activation detection: Remove neurons in the first layer that always have zero activation values on clean datasets. ", "page_idx": 22}, {"type": "text", "text": "To counter these adaptive defenses, we replaced zero weights with small random values. We used Gaussian noise with $\\sigma=0.001$ . We conducted experiments on CIFAR10 with ResNet-18, using the default hyperparameters from the paper. Results show we still achieve $100\\%$ ASR with less than $1\\%$ performance degradation. ", "page_idx": 22}, {"type": "text", "text": "This setup eliminates zero weights, rendering anomaly detection ineffective. We also analyzed the average activation values of 64 filters in the first layer on the training set (see Figure in PDF). Our backdoor path activations are non-zero and exceed many other neurons, making activation detection ineffective. We tested fine-pruning and Neural-Cleanse (Anomaly Index $=1.138\\$ ) under this setting. Both defenses failed to detect the backdoor. We didn\u2019t adopt this setting in the paper as it compromises our theoretical guarantees. Our goal was to prove the feasibility and theoretical basis of a novel attack method. Additionally, we can distribute the constructed backdoor path across multiple paths to enhance robustness. We plan to discuss these potential methods in the next version. ", "page_idx": 22}, {"type": "text", "text": "Another interesting idea is to use the GeLU activation function instead of ReLU. However, We believe that simply replacing ReLU with GeLU may not effectively defend against DFBA. We\u2019ll discuss this in two scenarios: when the value before the activation function in the model\u2019s first layer is positive or negative. According to our design and experimental results, essentially only inputs with triggers produce positive activation values, which are then continuously amplified in subsequent layers. In this part, GeLU would behave similarly to ReLU. For cases where the value before the activation function is negative (i.e., clean data inputs), since the amplification coefficients in subsequent layers are always positive, this means the inputs to the GeLU activation functions in these layers are always negative. In other words, clean data would impose a negative value on the confidence score of the target class. The minimum possible output from GeLU only being approximately 0.17, and in most cases this negative value is close to 0. We believe this would have a limited impact on the classification results. ", "page_idx": 22}, {"type": "text", "text": "On the other hand, directly replacing ReLU activation functions with GeLU in a trained model might affect the model\u2019s utility. Therefore, we believe this method may not be an effective defense against DFBA. ", "page_idx": 22}, {"type": "text", "text": "J Discussion and Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Generalization of DFBA: In this work, we mainly focus on supervised image classification. Recent research has generalized backdoor attacks to broader learning paradigms and application domains, such as weak-supervised learning [58, 59, 60, 61], federated learning [62, 63, 64], natural language processing [65, 66], graph neural networks [67, 68], and deep reinforcement learning [69, 70]. As part of our future work, we will explore the generalization DFBA to broader learning problems. We will also investigate the extension of DFBA to other models (e.g., RNN and Transformer). ", "page_idx": 22}, {"type": "text", "text": "Potential countermeasures: In Appendix B, we prove DFBA is undetectable and unremovable by certain deployment-phase defenses. However, it can be potentially detected by testing-phase defenses mentioned in Section 2. For example, we will show that a state-of-the-art testing-phase defense [49] can prevent our backdoor when the trigger size is small but it is less effective when the trigger size is large. ", "page_idx": 22}, {"type": "text", "text": "PatchCleanser [49] is a state-of-the-art provably defense against backdoor attacks to classifiers. Roughly speaking, given a classifier, PatchCleanser can turn it into a provably robust classifier whose predicted label for a testing input is unaffected by the backdoor trigger, once the size of the backdoor trigger is bounded. We evaluate PatchCleanser for our DFBA on the ImageNet dataset with the default parameter setting. We conducted three sets of experiments. In the first two sets of experiments, we evaluate our DFBA with a small trigger and a larger trigger for PatchCleanser, respectively. In the third set of experiments, we adapt our DFBA to PatchCleanser using a small backdoor trigger (we slightly defer the details of our adaptation). PatchCleanser uses a patch to occlude an image in different locations and leverages the inconsistency of the predicted labels of the given classifier for different occluded images to make decisions. Following Xiang et al. [49], we use $1\\%$ pixels of an image as the patch for PatchCleanser. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "We have the following observations from our experimental results. First, PatchCleanser can reduce the ASR (attack success rate) of our DFBA to random guessing when the size of the backdoor trigger is small. The reason is that PatchCleanser has a formal robustness guarantee when the size of the backdoor trigger is small. Second, we find that our DFBA can achieve a $100\\%$ ASR when the trigger size is no smaller than $31\\times31$ (the trigger occupies around $\\begin{array}{r}{1.9\\%\\approx\\frac{31\\cdot31}{224\\cdot224}}\\end{array}$ 23214\u00b7\u00b732124 pixels of an image). Our experimental results demonstrate that our DFBA is effective under PatchCleanser with a large trigger. Third, we find that we can adapt our DFBA to evade PatchCleanser. In particular, we place a small trigger $\\left(4\\times4\\right)$ in two different locations of an image (e.g., upper left corner and bottom right corner). Note that we still use a single backdoor path for DFBA. Our adapted version of DFBA can evade PatchCleanser because PatchCleanser leverages the inconsistency of the predicted labels for different occluded images to make decisions. As the trigger is placed in different locations, different occluded images are consistently predicted as the target class for a backdoored input since the patch used by PatchCleanser can only occlude a single trigger. As a result, PatchCleanser is ineffective for our adapted DFBA. We confirm this by evaluating our adapted version of DFBA on the ImageNet dataset and find it can achieve a $100\\%$ ASR under PatchCleanser. ", "page_idx": 23}, {"type": "text", "text": "Universal adversarial examples: Given a classifier, many existing studies [71] showed that an attacker could craft a universal adversarial perturbation such that the classifier predicts a target class for any input added with the perturbation. The key difference is that our method could make a classifier predict the target label with a very small trigger, e.g., our method could achieve $100\\%$ Attack Success Rate (ASR) with a $2\\times2$ trigger as shown in Figure 7c. Under the same setting, the ASR for the universal adversarial perturbation (we use Projected Gradient Descent (PGD) [72] to optimize it) is $9.84\\%$ . In other words, our method is more effective. ", "page_idx": 23}, {"type": "text", "text": "Limitations: Our DFBA has the following limitations. First, we mainly consider the patch trigger in this work. In future works, we will explore designing different types of triggers for our attack (e.g., watermark). Second, to achieve a strong theoretical guarantee, we need to relax our assumption and assume the knowledge of the defenses. Our future work will investigate how to relax this assumption. ", "page_idx": 23}, {"type": "text", "text": "K Trigger Image ", "text_level": 1, "page_idx": 23}, {"type": "image", "img_path": "pX71TM2MLh/tmp/f70101e7a542032caa2bda0681607104b4479ecd4ee5c56dca606de3528127f0.jpg", "img_caption": ["Figure 8: Visualization of triggers optimized on different datasets/models "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 24}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 24}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 24}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 24}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 24}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 24}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We explain our contributions and scope in the introduction. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We discuss the limitations of our paper in the Appendix J ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We give the derivation of our method in Section 4, and the theoretical guarantees and their proofs in Appendix B. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We describe in detail how our algorithm is implemented in Section 4, and our experimental setup is described in detail in Section 5 and Appendix C. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We have made all experimental code publicly available. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 26}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our experimental setup is described in detail in Section 5 and Appendix C. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: We did not report the error bar in some of experiments. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We included the computing devices and computational time used in the paper. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The research conducted in this paper conformed with the NeurIPS Code of Ethics in every respect. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We discuss the potential impacts of our paper in the Appendix J ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper poses no such risks. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We specified the sources of all assets used and complied with all licenses and terms. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]