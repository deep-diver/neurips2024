[{"type": "text", "text": "Learning Partitions from Context ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Simon Buchholz Department for Empirical Inference Max Planck Institute for Intelligent Systems T\u00fcbingen AI Center T\u00fcbingen, Gemany sbuchholz@tue.mpg.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we study the problem of learning the structure of a discrete set of $N$ tokens based on their interactions with other tokens. We focus on a setting where the tokens can be partitioned into a small number of classes, and there exists a real-valued function $f$ defined on certain sets of tokens. This function, which captures the interactions between tokens, depends only on the class memberships of its arguments. The goal is to recover the class memberships of all tokens from a finite number of samples of $f$ . We begin by analyzing this problem from both complexity-theoretic and information-theoretic viewpoints. We prove that it is NP-complete in general, and for random instances, we show that on the order of $N\\ln(N)$ samples, implying very sparse interactions, suffice to identify the partition. We then investigate the conditions under which gradient flow dynamics of token embeddings can reveal the class structure, finding that this is achievable in certain settings when given on the order of $N^{2}\\ln^{2}(N)$ samples. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Modern machine learning systems are able to learn extremely complicated relations from data. They often rely on learned embeddings of discrete tokens in a continuous space. This is notably true for Large Language Models (LLMs) [9, 17, 25, 23] which encode their input by converting text into a sequence of discrete tokens that are embedded in a high dimensional embedding space and those embeddings are fed into, e.g., a transformer architecture [28] which allows to predict the next token. But also in the other domains, e.g., in vision, discrete embeddings are frequently used as a component of deep learning architectures [27, 10] as this enables capturing complex concepts that are often discrete. ", "page_idx": 0}, {"type": "text", "text": "It was observed that after training these word embeddings exhibit many interesting structures. The most prominent example probably is the observation that the difference of the Word2Vec embedding vectors of the nouns \u2019king\u2019 and \u2019queen\u2019 approximately equals the difference of the embeddings of \u2019man\u2019 and \u2019woman\u2019 [21, 22]. Similarly it was found that using word similarity as an inductive bias to structure latent spaces helps downstream performance [4]. Thus a properly structured latent space seems to be an important ingredient to capture the intricate correlations in complex data. ", "page_idx": 0}, {"type": "text", "text": "A proper theoretical understanding of such complex models currently remains an elusive goal. However, there have been various attempts to understand various components of deep learning models. Many works investigated the the behaviour of feedforward-networks in particular focusing on shallow networks [2, 8] and asymptotic regimes [15, 29]. More recently several works investigated transformer architectures (often focusing on the one layer case with linearized attention mechanism) [16, 1, 30, 12]. On a technical level [11] is closely related as they study the large depth limit of transformers through the lens of particle systems (however in their case time corresponds to depth, while in our case it corresponds to training time). A feature shared by many of those works is that little structure is assumed on the input, i.e., fixed token embeddings are assumed and often those are even assumed to be isotropic Gaussian. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Here we instead focus on the dynamics of the token embeddings and study how these can recover structure present in the data. There is no ground truth target for the embeddings for large scale models used in deep learning and their embeddings need to capture a variety of nuanced correlations and relations that are hard to formalize. Therefore we focus on a simplified problem that nevertheless shares important features with more complex real world settings. One general heuristic is that the embeddings contain information about the similarity of tokens. We focus here on the strongest form of similarity namely equality. Indeed, our central assumption is that tokens can be clustered in a small number of groups such that tokens within a cluster behave exactly the same, i.e., they interact in the same manner with other tokens. ", "page_idx": 1}, {"type": "text", "text": "Then the central questions is what assumptions allow us to recover a hidden structure. The crucial feature of this setting is that we only get information about a token by its interaction with other tokens about which we also only learn through their interaction behaviour. Moreover, those interactions are typically sparse, i.e., we observe only a small subset of all possible interactions. Note that this setting resembles observations made in the context of collective behaviour where a global structure emerges from local interactions [13, 24, 3]. A related question was investigated in [6] where they study associative memory and also want to identify a hidden structure, however, they learn the class memberships directly through the interaction with a class embedding (and they train the interaction instead of the embedding). In [19] the dynamics of word embeddings and transformer layers was investigated when the data follows a topic model. This work shares the crucial feature that membership of a word in a certain topic is only transmitted through the co-occurrence with other words from the same topic. In contrast to their work we here do not focus on learning class membership from token frequencies and in fact consider uniform token distribution. Instead we view this problem as a logical inference problem: Given a set of facts about a set of tokens can the hidden structure of the tokens discovered. ", "page_idx": 1}, {"type": "text", "text": "We summarize the main contributions of the paper as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce a learning problem that shares important features with learning through the interaction behaviour that is crucial for LLMs and complex systems. \u2022 We analyse this problem from a complexity-theoretic viewpoint where we show that it is in general hard, and from an information-theoretic viewpoint where we show, roughly, that for $N$ different tokens in the alphabet order $N\\ln(N)$ samples are sufficient to identify the latent structure. \u2022 We then carefully investigate the gradient dynamics of token embeddings finding local recovery of the cluster structure and global recovery for tensor-product functions on the tokens if we have more than $N^{2}\\ln(N)$ samples for an alphabet with $N$ tokens. ", "page_idx": 1}, {"type": "text", "text": "Notation. We write $[N]=\\{1,\\dots,N\\}$ for the set of the first $N$ integers. The cardinality of a finite set $A$ is denoted by $|A|$ and we also denote the standard Euclidean norm of any vector $v\\in\\mathbb{R}^{d}$ by $|v|$ . The expressions $\\lambda_{\\operatorname*{max}}(A)$ and $\\lambda_{\\mathrm{min}}(A)$ denote the largest and smallest eigenvalue of a symmetric matrix $A$ respectively. We denote the uniform distribution over a set $A$ by $\\mathcal{U}(A)$ . For two subsets $A,B\\subset\\mathbb{R}^{d}$ we denote by $A+B=\\{a+b:\\,a\\in A,b\\in B\\}$ their Minkowski sum. We denote the permutation group on $k$ elements by ${\\mathfrak{S}}_{k}$ . An overview of the used variable names can be found in Appendix A. ", "page_idx": 1}, {"type": "text", "text": "2 Setting and Motivation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we illustrate our problem with an example and define the setup more formally. Consider the set of all animals. Those can be grouped into classes such as mammals, birds, or reptiles (in fact there is a rich hierarchical structure which we ignore here). Those groups were conceived by findings sets of animals that share many properties. Once these groups are found, we can predict unobserved properties by first identifying the cluster to which an animal belongs and then predict that the property is shared with animals in the same cluster. Note that this is a specific instance of a general problem in scientific inference where we want to uncover a hidden grouping of similar entities from sparse observations about these entities. ", "page_idx": 1}, {"type": "image", "img_path": "pRSgf5VdD0/tmp/1d9102531641112b841cefdaec0585383826b7c7e901e0fba5a09acf97f82869.jpg", "img_caption": ["Figure 1: Illustration of the setting for $I=3$ different groups clustered in 3, 2, and 3 subgroups respectively. Samples consist of one element of each group, the dashed lines indicate samples $(1,3,1)$ and $(3,7,6)$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Here our main motivation, however, stems from the analysis of large language models where a similar problem arises implicitly during training. They are trained by next token prediction, so we do not expect them to learn structure by deductive reasoning such as cows are mammals, and mammals have lungs, so cows have lungs. Instead, their learning signal is whether a token can be replaced by another token for a given context. Thus, it is a natural question whether gradient descent-based training on token embeddings can uncover a hidden cluster structure of the data. Note that if the hidden structure is recovered, then generalization to unseen prompts is possible. ", "page_idx": 2}, {"type": "text", "text": "We now introduce our formal setup that captures key aspects of the discussion. We consider $I$ sets of $N_{1},N_{2},\\ldots,N_{I}$ tokens or entities (such as words). For simplicity we identify these with tokens from the set $[N_{i}]$ . For each of the sets $[N_{i}]$ there is a partition $\\mathcal{P}_{i}$ in $K_{i}$ classes which we can identify with the set $[K_{i}]$ . Then we can encode the partitions through maps $\\Pi_{i}:[N_{i}]\\rightarrow[K_{i}]$ so that the partition is given by ${\\mathcal{P}}_{i}=(\\Pi_{i}^{-1}(k_{i}))_{k_{i}\\in[K_{i}]}$ , i.e., $\\Pi_{i}$ encode the class membership. We consider the map ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Pi=\\Pi_{1}\\otimes\\ldots\\otimes\\Pi_{I},\\,\\mathrm{i.e.,}\\quad\\Pi(n_{1},\\ldots,n_{I})=(\\Pi_{1}(n_{1}),\\ldots,\\Pi_{I}(n_{I})).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This structure is illustrated in Figure 1 Now we assume that there is a function $g:\\left[K_{1}\\right]\\times\\left[K_{2}\\right]\\times$ $\\cdots[K_{I}]\\rightarrow\\mathbb{R}$ which depends only on the classes. The map $f\\,=\\,g\\circ\\Pi$ extends this map to the tokens such that it only depends on the group a token belongs to. In the case where $g$ (and thus $f$ ) maps to $\\{0,1\\}$ this can be interpreted as truth assignments, i.e., a statement consisting of a sequence $(n_{1},\\ldots,n_{I})$ is true if $f(n_{1},\\,.\\;.\\;.\\;,n_{I})=1$ and false otherwise and this case is the main motivation of our work. More generally, $f$ could output the index of a suitable next token where in the 0, 1 case 0 could correspond to a negation while 1 to an end of sentence token. Our goal is to learn the partitions $\\mathcal{P}_{i}$ or, equivalently, $\\Pi$ up to a permutation and thereby identify the hidden structure. ", "page_idx": 2}, {"type": "text", "text": "We assume that we are given data in the form of samples $(n^{s},f(n^{s})$ where $\\pmb{n}^{s}\\,=\\,(n_{1}^{s},.~.~.~,n_{I}^{s})$ and $n_{i}^{s}\\in[N_{i}]$ . In other words, we try to learn the underlying structure from the interactions of a token with the other tokens which is the same for every element of the partition. To simplify the notation and statements we assume in the following that $N_{i}=N$ and $K_{i}=K$ for some $N$ , $K$ and all $1\\leq i\\leq I$ . Our main interest concerns the case where $N$ is large, i.e., there are many entities and $K$ and $I$ are small, i.e., the number of groups is small. ", "page_idx": 2}, {"type": "text", "text": "Let us summarize several features that this model shares with real world problem such as learning suitable embeddings in latent space. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Hierarchical structures, i.e., groups of objects that share certain features, as discussed here are abundant in language and science.   \n\u2022 We only receive an indirect learning signal for the value of $\\Pi_{i}(n)$ through its interaction with other tokens.   \n\u2022 Interactions can be very complex, i.e., here the output depends on the interaction of $I$ different tokens and ignoring parts of the context makes learning infeasible. ", "page_idx": 2}, {"type": "text", "text": "On the other hand, many important features are abstracted away, e.g.: ", "page_idx": 2}, {"type": "text", "text": "\u2022 Here we assume that tokens from the same element of the partition interact in exactly the same way with other tokens while in reality there are many different partitions of the tokens depending on the broader context (e.g., we can group species by habitat, color, or, size each ", "page_idx": 2}, {"type": "text", "text": "resulting in different partitions) or there are exceptions, e.g., mammals generally do not lay eggs but the platypus does.   \n\u2022 Many more complex notions of similarity or further properties of embeddings such as a vector space structure are not covered. Also there can be many uninformative features.   \n\u2022 We do not consider noisy data or errors in this work which is crucial for real world applications. ", "page_idx": 3}, {"type": "text", "text": "3 Complexity-Theoretic and Information-Theoretic Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now study this learning problem in different settings. Let us first briefly discuss complexitytheoretic and information-theoretic properties of the learning problem to understand the general boundaries of this learning task. We first study the information-theoretic viewpoint, i.e., the question how many samples are necessary to identify $\\Pi$ (and potentially $g$ ). We focus on the case where we sample $\\Pi$ and the data-samples uniformly at random. To learn an unstructured map $\\boldsymbol{\\left[N\\right]^{I}}\\,\\rightarrow$ $\\mathbb{R}$ we generally need of the order $N^{I}\\ln(N^{I})$ independent samples ( $N^{I}$ when sampling without replacement). ", "page_idx": 3}, {"type": "text", "text": "For the structured setting we show that if $\\Pi$ is drawn uniformly at random then generally order $K^{I}N\\ln(N)$ samples are sufficient to learn $f$ and the partition induced by $\\Pi$ . In other words, for every token $n_{i}$ and each of the $K^{I}$ classes $\\Pi^{-1}(k)$ we need of the order of $\\mathrm{ln}(N)$ samples $\\mathbfit{\\Delta}$ such that $\\boldsymbol{\\Pi}(\\boldsymbol{n})=\\boldsymbol{k}$ and ${\\pmb n}_{i}=n_{i}$ . In particular, for $N\\gg K^{I}$ any token will interact only with $\\bar{K}^{I}\\ln(N)\\ll N$ other tokens, i.e., a very sparse subset of the other tokens. ", "page_idx": 3}, {"type": "text", "text": "We require the following necessary condition for identifiability: For every $k_{i}~\\neq~k_{i}^{\\prime}$ there are $k_{1},\\ldots,k_{i-1},k_{i+1},\\ldots,k_{I}\\in[K]$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\ng(k_{1},\\ldots,k_{i-1},k_{i},k_{i+1},\\ldots,k_{I})\\neq g(k_{1},\\ldots,k_{i-1},k_{i}^{\\prime},k_{i+1},\\ldots,k_{I}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that if this condition is indeed necessary because if it is not satisfied then it is not possible to distinguish $\\Pi_{i}^{-1}(k_{i})$ and $\\Pi_{i}^{-1}(k_{i}^{\\prime})$ . Clearly, we can generally only identify $\\Pi$ up to a permutation symmetry, i.e., we can only find $\\tilde{\\Pi}$ such that there are permutations $\\pi_{i}\\in\\mathfrak{S}_{K}$ such that $\\Pi_{i}=\\pi_{i}\\tilde{\\Pi}_{i}$ . We have the following result. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. Assume that $g:[K]^{I}\\to\\mathbb{R}$ is a function satisfying the assumption (2). Assume we randomly sample maps $\\Pi_{i}$ such that $\\Pi_{i}(n_{i})=k_{i}$ with probability $K^{-1}$ for all $i$ , $n_{i}$ , and $k_{i}$ and such that $(\\Pi_{i}(n_{i}))_{i\\in I,n_{i}\\in[N]}$ are independent. Assume we are given $S$ samples $(\\boldsymbol n,g\\circ\\Pi(\\boldsymbol n))$ where $\\pmb{n}\\sim\\mathcal{U}([N]^{I})$ . Then there is a constant $N_{0}(I,K,\\eta)$ such that with probability at least $1-2e^{-\\eta}$ for $N\\ge N_{0}(I,K,\\eta)$ and ", "page_idx": 3}, {"type": "equation", "text": "$$\nS\\ge2^{2I+3}I K^{I}N\\ln(N)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "we can recover $\\Pi$ and $g$ up to permutations of $[K]$ . ", "page_idx": 3}, {"type": "text", "text": "This result is a special case of Theorem 6 in Appendix $\\mathbf{B}$ which shows similar bounds for arbitrary maps $\\Pi$ that are not necessarily random. In the more general setting there are additional dependencies on the size of the preimages $\\bar{\\Pi}_{i}^{-1}(k)$ . Note that this dependency cannot be avoided because if there is a $^k$ such that $|\\Pi^{-1}(k)|\\,=\\,1$ and $g(k)\\,=\\,1$ and $g(\\pmb{k}^{\\prime})\\,=\\,0$ for $k\\neq k^{\\prime}$ then order $N^{I}$ samples are necessary to find $\\mathbfit{\\Delta}$ such that $\\boldsymbol{\\Pi}(n)=k$ and thus $\\Pi$ . The general proof idea is to bound the probability that any fixed $\\Pi^{\\prime}\\neq\\Pi$ is compatible with the dataset. It turns out that it is possible to bound this probability in terms of the partitions distance of the partitions induced by $\\Pi$ and $\\Pi^{\\prime}$ . Then we are left with bounding the number of partitions and we conclude with the union bound. We now show that this this bound is essentially tight. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2. Let $g:[K]^{I}\\to\\mathbb{R}$ be a function such that $g(1,k_{2},\\ldots,k_{I})=g(2,k_{2},\\ldots,k_{I})$ for all $k_{2},\\ldots,k_{I}\\,\\in\\,[K]$ except when $k_{2}\\,=\\,k_{3}\\,=\\,.\\,.\\,.\\,=\\,k_{I}\\,=\\,1$ . Assume that $N$ is divisible by $K$ and that $|\\Pi_{i}^{-1}(k)|=N/K$ for all $i\\in[I]$ and $k\\in[K]$ . Given $3\\leq S\\leq N K^{I-1}\\ln(N/K)/4$ samples $(\\pmb{n}^{s},g\\circ\\Pi(\\pmb{n}^{s}))$ where $\\pmb{n}{\\sim}\\mathcal{U}([N]^{I})$ i.i.d. Then the function $\\Pi$ is identifiable with probability at most 2e\u2212 N/K. ", "page_idx": 3}, {"type": "text", "text": "The proof of this result can be found in Appendix B. Next, we emphasize that while typically a rather small number of samples is sufficient to learn $\\Pi$ it can generally be very hard to do this in practice. More concretely we show that for $I\\geq3$ even deciding whether there is a map $\\Pi=\\Pi_{1}\\otimes...\\otimes\\Pi_{I}:$ ", "page_idx": 3}, {"type": "text", "text": "$[N]^{I}\\to[K]^{I}$ such that $f=g\\circ\\Pi$ given access to samples of the form $(n^{s},t^{s})$ is NP-complete. We show that this is true even if $g$ is known, $I=3$ and $K=2$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 3. Consider the map $g:\\{0,1\\}^{3}\\rightarrow\\{0,1\\}$ given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{g(k_{1},k_{2},k_{3})=\\mathbf{1}_{k_{1}+k_{2}+k_{3}=2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then it is an $N\\!P$ -complete problem to decide given samples of the form $(n_{1}^{s},n_{2}^{s},n_{3}^{s},t^{s})\\in[N]^{3}\\!\\times\\!\\{0,1\\}$ whether there is a map $\\Pi=\\Pi_{1}\\otimes\\Pi_{2}\\otimes\\Pi_{3}$ with $\\Pi_{i}:[N]\\rightarrow\\{0,1\\}$ such that $t^{s}=g\\circ\\Pi(n_{1}^{s},n_{2}^{s},n_{3}^{s})$ for all samples. ", "page_idx": 4}, {"type": "text", "text": "The proof of this result can be found in Appendix C. Now that we established under what the conditions $\\Pi$ can in principle be learned, and clarified that this might be hard in general, we next discuss how we can find $\\Pi$ in practice. First, we remark that Theorem 3 rules out the existence of any general fast algorithms to learn $\\Pi$ . Given the combinatorial nature and the hardness of the problem it is natural to reformulate the task as a constraint satisfaction problem which can then be solved using standard SAT solvers (see, e.g., [14] for a review). Indeed, we can introduce Boolean variables $t_{k n}^{i^{-}}$ for $i\\,\\in\\,[I]$ , $k\\,\\in\\,[K]$ , and $n\\,\\in\\,[N]$ which encode whether $\\Pi_{i}(n)\\,=\\,k$ and $r_{k v}$ for every $k\\in[K]^{I}$ and $v\\in\\operatorname{Im}(f)$ in the (finite) image of $f$ that encode whether $g(\\pmb{k})=v$ . It is then relatively straightforward to then express the conditions for the map $\\Pi$ as a constraint satisfaction problem which is satisfiable if and only if there are maps $\\Pi$ and $g$ such that $t^{s}=g(\\Pi(n^{s}))$ holds for all samples. We outline the construction in more detail in Appendix C. We leave the task of developing and studying efficient algorithms for the considered problem for future work because the main motivation of this paper is rather to understand how the complex statistical patterns can be extracted using simple gradient based algorithms. This will be investigated in the next section. ", "page_idx": 4}, {"type": "text", "text": "4 Analysis of Gradient Descent Dynamics ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section we investigate under what conditions the clustering induced by $\\Pi$ can be learned using gradient descent on token embeddings. Our main finding is that for uniformly random $\\Pi$ and $S$ sufficiently large gradient descent can be used to uncover or at least preserve the cluster structure of the embeddings. This shows that while the general problem is NP hard typical random instances with sufficiently many samples can be solved with straightforward algorithms quickly. This is in spirit similar to the results found in, e.g., [5]. Let us start by introducing the setting. ", "page_idx": 4}, {"type": "text", "text": "Setting. We assume that we have token embeddings for each of the $I$ sets $[N_{1}]$ to $\\left[N_{I}\\right]$ , i.e., we assume that there are vectors $\\boldsymbol{v}(i,n)\\in\\mathbb{R}^{D}$ for some $D$ and all $i\\in[I]$ , $n\\in[N]$ . Based on these embeddings we assume that we are given a function $\\hat{f}:\\mathbb{R}^{I D}\\rightarrow\\mathbb{R}$ that transforms the embeddings into a prediction. We will abuse notation and write for $n\\in[N]^{I}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{f}(\\pmb{n})=\\hat{f}(v(1,\\pmb{n}_{1}),\\dots,v(I,\\pmb{n}_{I})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "i.e., we will suppress the map from tokens to embeddings in the notation. ", "page_idx": 4}, {"type": "text", "text": "Now we consider gradient descent for the embedding vectors using the least square loss on training samples, i.e., the loss of a sample $\\boldsymbol{n},t=f(\\boldsymbol{n}))$ is $({\\hat{f}}(n)-f(n))^{2}$ . ", "page_idx": 4}, {"type": "text", "text": "We assume that we are given a dataset $\\mathcal{D}=\\{\\pmb{n}^{1},\\pmb{\\dots},\\pmb{n}^{S}\\}\\sim\\mathcal{U}([N]^{I})^{S}$ . Then the empirical loss reads (the division by 2 is convenient for the gradient evaluation later) ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{L}}((v(i,n))_{i\\in[I],n\\in[N]})=\\frac{1}{2}\\sum_{s=1}^{S}(\\hat{f}(n^{s})-f(n^{s}))^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We also define shorthands for certain concatenations of embeddings. For a sample $n\\in[N]^{I}$ we denote the collection of embeddings by $v(n)=(v(1,\\pmb{n}_{1}),\\dots,v(I,\\mathbf{\\bar{n}}_{I}))$ . Using the convention (5) we can then write ${\\hat{f}}(\\pmb{v}(\\pmb{n}))={\\hat{f}}(\\pmb{n})$ . ", "page_idx": 4}, {"type": "text", "text": "Moreover, we define $\\bar{\\pmb{v}}(i)\\in\\mathbb{R}^{D N}$ as the concatenation of the vectors $v(i,1),\\ldots,v(i,n).$ , i.e., the combined embedding for the $i$ -th slot and $\\bar{\\pmb{v}}\\in\\mathbb{R}^{D N I}$ as the concatenation of the vectors $\\bar{\\pmb v}(i)$ for $1\\leq i\\leq I$ , i.e., all token embeddings concatenated. We consider the regularized loss given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{R}}^{\\lambda}(\\bar{\\boldsymbol{v}})=\\frac{N}{S}\\hat{\\mathcal{L}}(\\bar{\\boldsymbol{v}})+\\frac{\\lambda}{2}|\\bar{\\boldsymbol{v}}|^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that the scaling by $N/S$ (instead of usual $1/S)$ is natural because every token embedding $v(i,n)$ occurs in approximately $S/N$ of the samples and so the scaling ensures that the gradient of $\\hat{\\mathcal{R}}^{\\lambda}$ with respect to the token embedding $v(i,n)$ is of order one. Now, we consider the continuous gradient descent of the loss with respect to the embeddings, i.e., we consider (omitting the time variable from the notation) ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\dot{v}(i,n)=-\\frac{\\mathrm{d}}{\\mathrm{d}v(i,n)}\\hat{\\mathcal{R}}^{\\lambda}(\\bar{v})=-\\frac{N}{S}\\frac{\\mathrm{d}}{\\mathrm{d}v(i,n)}\\hat{\\mathcal{L}}(\\bar{v})-\\lambda v(i,n).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This introduces a time dynamics on the token embeddings. We indicate the time dependence by $v(i,n,t)$ but we drop $t$ if not necessary. Our main goal is to understand which conditions ensure that the token embeddings $v(i,n,t)$ and $\\bar{v(i,n^{\\prime},t)}$ converge to each other if $\\Pi_{i}(n)=\\Pi_{i}(n^{\\prime})$ as $t\\to\\infty$ . To investigate this we define the center of the class embeddings by ", "page_idx": 5}, {"type": "equation", "text": "$$\nw(i,k,t)=\\frac{1}{|\\Pi_{i}^{-1}(k)|}\\sum_{n\\in\\Pi_{i}^{-1}(k)}v(i,n,t)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and we consider the deviations from the class centres given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\delta(i,n,t)=v(i,n,t)-w(i,\\Pi_{i}(n),t).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Thus the vectors $\\delta(i,n,t)$ capture whether we recover the cluster structure, in particular if all norms $|\\delta(i,n,t)|$ are small then we essentially recovered the hidden structure. Therefore we define ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\delta_{\\operatorname*{max}}(t)=\\operatorname*{max}_{i\\in[I]}\\operatorname*{max}_{n\\in[N]}|\\delta(i,n,t)|.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Similarly, to the notation introduced before we consider for $\\pmb{k}~\\in~[K]^{I}$ the vector $w(k)~=$ $(w(1,k_{1}),\\dots,w(I,\\pmb{k}_{I}))$ . As in (5) we abuse notation and write ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{g}(\\pmb{k})=\\hat{f}(\\pmb{w}(\\pmb{k}))=\\hat{f}(\\pmb{w}(1,\\pmb{k}_{1}),\\pmb{\\cdot}\\pmb{\\cdot}.\\ .\\ ,\\pmb{w}(I,\\pmb{k}_{I})).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Similar to $\\bar{\\pmb v}(i)$ and $\\bar{\\pmb v}$ we introduce $\\bar{\\pmb w}(i)$ as the concatenation of $(w(k,i))_{k\\in[K]}$ and $\\bar{\\pmb w}$ as the concatenation of $\\bar{\\pmb w}(i)$ . ", "page_idx": 5}, {"type": "text", "text": "Assumptions. Our first result for the gradient dynamics states that clusters are stable under the dynamics if the initial loss is sufficiently small. More precisely this means that we assume that $v(i,n)$ and $v(i,n^{\\prime})$ are close initially whenever $\\Pi_{i}(n)=\\bar{\\Pi}_{i}(n^{\\prime})$ , i.e., $\\delta_{\\mathrm{max}}(0)$ is small. In addition, we assume that $|\\hat{g}(\\pmb{k})-g(\\pmb{k})|$ is small. To capture this we define ", "page_idx": 5}, {"type": "equation", "text": "$$\nr_{\\operatorname*{max}}(t)=\\operatorname*{max}_{k\\in[K]^{I}}|\\widehat{g}(\\pmb{w}(\\pmb{k},t))-g(\\pmb{k})|.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then the result shows that $\\delta_{\\mathrm{max}}$ stays small for all times if $S\\gtrsim N^{2}$ under mild additional assumptions. In other words if we start from the correctly learned substructures and $\\hat{g}(\\pmb{k})\\approx g(\\pmb{k})$ for all $\\pmb{k}\\in[K]^{I}$ then this remains true for all times. Note that while we phrase smallness as an assumption on the mean embeddings $w(i,k)$ this is generally a consequence of $\\delta_{\\mathrm{max}}$ small and a small empirical loss $\\hat{\\mathcal{L}}(\\bar{\\boldsymbol{v}})$ . Let us now state the required assumptions. ", "page_idx": 5}, {"type": "text", "text": "Assumption 1. We assume that the map $\\Pi:[N]^{I}\\to[K]^{I}$ is approximately balanced which means that for all $i\\in[I],\\,k\\in[K]$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{N}{2K}\\leq\\Pi_{i}^{-1}(k)\\leq\\frac{2N}{K}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This assumption ensures that clusters are of approximately equal size. We have already seen in Section 3 that different cluster sizes increase the sample complexity of learning $\\Pi$ . ", "page_idx": 5}, {"type": "text", "text": "Assumption 2. We assume that there is a convex set $\\Omega\\subset\\mathbb{R}^{D}$ and a constant $M^{\\prime}\\geq1$ such that the following bound holds ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\substack{v\\in\\Omega^{I}\\,\\,i_{1},i_{2},i_{3}\\in[D I]}}\\operatorname*{max}\\left(|\\hat{f}(v)|,|\\partial_{i_{1}}\\hat{f}(v)|,|\\partial_{i_{1}}\\hat{f}(v)|,|\\partial_{i_{1}}\\partial_{i_{2}}\\partial_{i_{3}}\\hat{f}(v)|\\right)\\leq M^{\\prime}=\\frac{\\sqrt{M}}{4}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here it is convenient to introduce $M=16M^{\\prime2}$ so that later certain errors in a Taylor approximation are bounded by $M$ . We also assume that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{k\\in[K]^{I}}|g(k)|\\leq M^{\\prime}=\\frac{\\sqrt{M}}{4}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This is a rather mild assumption. For $C^{3}$ functions $\\hat{f}$ and $\\Omega$ bounded this is always true. The next assumption entails a rigidity of approximate minimizers of the loss. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3. We assume that for all embeddings $w(i,k)\\in\\mathbb{R}^{D}$ for $i\\in[I]$ , $k\\in[K]$ that satisfy ", "page_idx": 6}, {"type": "equation", "text": "$$\nr_{\\operatorname*{max}}=\\operatorname*{max}_{k\\in[K]^{I}}|\\hat{f}(\\pmb{w}(\\pmb{k}))-g(\\pmb{k})|\\le1\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "the bound ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\omega_{0}=\\operatorname*{min}_{k,i}\\lambda_{\\operatorname*{min}}\\left(\\sum_{k\\in[K]^{I},k_{i}=k}\\nabla_{w(i,k)}\\hat{f}({\\pmb w}({\\pmb k}))\\otimes\\nabla_{w(i,k)}\\hat{f}({\\pmb w}({\\pmb k}))\\right)>0\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "holds for some positive constant $\\omega_{0}$ . Of course the bound $r_{\\operatorname*{max}}\\leq1$ could be replaced by any other constant. ", "page_idx": 6}, {"type": "text", "text": "Note that this condition can only hold if $D\\le K^{I-1}$ , i.e., the latent space dimension cannot be too large. The high level intuition of this assumption is essentially that (at least if $\\begin{array}{r}{\\sum_{k}(\\hat{f}(\\pmb{w}(\\pmb{k}))-g(\\pmb{k}))^{2}}\\end{array}$ is small) there is no direction $\\boldsymbol{v}\\in\\mathbb{R}^{D}$ such that $v\\cdot\\nabla_{w(i,k)}\\hat{f}({\\pmb w}({\\pmb k}))\\approx0$ for all $^k$ such that $k_{i}=k$ i.e., we cannot move one single embedding without changing the output $\\hat{f}({\\pmb w}({\\pmb k}))$ for at least one $^k$ If this condition does not hold then we cannot guarantee that $\\begin{array}{r}{\\sum_{k}(\\hat{f}(\\pmb{w}(\\pmb{k}))-f(\\pmb{k}))^{2}}\\end{array}$ is minimized for a unique $w(i,k)$ (for all other embeddings fixed). This gen erally prevents concentration of $v(i,n)$ . Note that this condition does not ensure that there is a unique minimizer $\\bar{\\pmb w}$ , in particular there could still be a rotationally invariant family of embeddings $w(i,k)$ such that $\\hat{f}({\\pmb w}({\\pmb k}))\\,=\\,g({\\pmb k})$ for all $\\pmb{k}\\in[K]^{I}$ . Finally, we need a further mild assumption that ensures that mean token embeddings $w(i,k)$ stay bounded in some set if the loss is small. This can be achieved, e.g., if $\\hat{f}\\,\\rightarrow\\,\\infty$ if $|w(k)|\\rightarrow\\infty$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 4. We assume that for all collections of mean embeddings $w(i,k)\\in\\mathbb{R}^{D}$ for $i\\in[I]$ , $k\\in[K]$ that satisfy ", "page_idx": 6}, {"type": "equation", "text": "$$\nr_{\\operatorname*{max}}=\\operatorname*{max}_{k\\in[K]^{I}}|\\hat{f}(\\pmb{w}(\\pmb{k}))-g(\\pmb{k})|\\le1\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "there is a convex set $\\Omega_{0}\\subset\\mathbb{R}^{d}$ such that $w(i,k)\\in\\Omega_{0}$ for all $i\\in[I],\\,k\\in[K]$ . Again, the right hand side of the bound $r_{\\operatorname*{max}}\\leq1$ could be replaced by any other constant. ", "page_idx": 6}, {"type": "text", "text": "Results. The first stability theorem can then be stated as follows. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4. Let $\\Pi:[N]^{I}\\to[K]^{I}$ be approximately balanced as stated in Assumption $^{\\,l}$ Assume that the functions $g:[K]^{I}\\to\\mathbb{R}$ and $\\hat{f}:\\mathbb{R}^{I D}\\,\\rightarrow\\,\\mathbb{R}$ satisfy Assumption 3 for some $\\omega_{0}\\,>\\,0$ and Assumption $^{4}$ for some convex set $\\Omega_{0}$ . Assume that Assumption 2 holds for some $M$ and the set $\\Omega=\\Omega_{0}+B_{2}(0)$ . Then there are constants $c_{1},C_{2},C_{3},C_{4}>0$ depending on $I,\\,M,\\,D$ , and $\\omega_{0}$ such that for all initial embeddings $v(i,n,t=0)\\in\\mathbb{R}^{D}$ for $i\\in[I]$ and $n\\in[N]$ satisfying ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\delta_{\\mathrm{max}}(0)\\leq C_{2}K^{-3I/2},\\quad r_{\\mathrm{max}}(0)\\leq C_{3}K^{-3I/2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and sample size ", "page_idx": 6}, {"type": "equation", "text": "$$\nS\\ge c_{1}\\operatorname*{max}\\left(K^{3I}N^{2}\\ln^{2}(N),N\\ln(N)K^{9I/2}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "the following holds with probability at least $1-S^{-1}$ over the randomness of the dataset. When considering the gradient dynamics of the embeddings given by (8) the bound ", "page_idx": 6}, {"type": "equation", "text": "$$\nR=\\operatorname*{lim}_{t\\to\\infty}{r_{\\operatorname*{max}}(t)}\\leq1\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "holds and moreover ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to\\infty}\\delta_{\\operatorname*{max}}(t)\\leq C_{4}K^{3I/2}\\sqrt{\\ln(S)\\frac{N}{S}}R.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In particular $\\delta_{\\mathrm{max}}(t)\\rightarrow0$ if $r_{\\operatorname*{max}}(t)\\to0,$ , i.e., all token embeddings for one fixed class converge to the same point. ", "page_idx": 6}, {"type": "text", "text": "This results shows that for order $N^{2}\\ln(N)$ samples and initialization sufficiently close to a global minimum the cluster structure remains stable. We do not provide conditions that ensure $r_{\\mathrm{max}}(\\bar{t})\\rightarrow0$ which guarantees convergence to 0 loss and perfect recovery of the clusters. ", "page_idx": 7}, {"type": "text", "text": "Next we note that we cannot expect the clustering to be stable in general even if $\\delta_{\\mathrm{max}}(0)$ is arbitrarily small if initialization is not close to a minimum. This is true even in the simplest case where $I=K=1$ , i.e, we consider gradient descent for a single function value. Then gradient descent does not necessarily converge to a global minimum and close by points do not necessarily stay close because gradient descent is not well posed. Let us clarify this by an example. ", "page_idx": 7}, {"type": "image", "img_path": "pRSgf5VdD0/tmp/95207741e42250e22092f5d39237377f5a23c719c079f8ce76c213ff6e492ee6.jpg", "img_caption": ["Figure 2: Simulation of the setting in Theorem 5 with $N\\,=\\,1000$ , $K\\,=\\,I\\,=\\,3$ , $D\\,=\\,2.$ , $\\lambda=0$ , $S=100.000$ . (left) trajectories of 50 randomly sampled tokens from 6 different classes. (right) Average distance of token embeddings within a class for different classes (colored) and average distance between all pairs of embeddings (black). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Example 1. Assume $I=K=D=1,$ , $N>1$ , ${\\hat{f}}(x)=-x^{2}+x^{3}$ and $f(n)=-2$ . Consider the dataset $\\mathcal{D}=\\{1,\\ldots,n\\}$ . Assume that $v(1,n,t=0)\\sim N(0,\\sigma^{2})$ for any $\\sigma^{2}>0$ . Then the gradient dynamics introduced in (8) reads ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\dot{v}(1,n,t)=\\hat{f}^{\\prime}(v(1,n,t))(\\hat{f}(v(1,n,t))-f(n))\\approx4v(1,n,t)\\quad\\mathrm{if}\\ |v(1,n,t)|\\mathrm{\\is\\small}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We find that $v(1,n,t)\\to2/3$ (which is a local minimum of $\\hat{f}$ as $t\\to\\infty$ if $v(1,n,t=0)>0.$ On the other hand $v(1,n,t)\\to-1$ if $v(1,n,t=0)<0$ . So in this case $\\delta_{\\mathrm{max}}(0)=O(\\sigma^{2}\\sqrt{\\ln(N)})$ but $\\delta_{\\mathrm{max}}(t)\\rightarrow5/3$ as $t\\to\\infty$ . Slight modifications show that also $\\delta_{\\mathrm{max}}\\rightarrow\\infty$ is possible. ", "page_idx": 7}, {"type": "text", "text": "The previous example shows that without additional assumption we cannot expect to recover the structure of the data. Therefore we impose additional restrictions on the function $\\hat{f}$ . As apparent from Example 1 and also from the bound in Lemma 1 in Appendix E, it is the curvature of the function $\\hat{f}$ that can push token embeddings of the same class apart. We therefore consider the function class of slow-wise linear functions defined as follows. ", "page_idx": 7}, {"type": "text", "text": "Definition 1. We call $\\hat{f}$ slot-wise linear if for every $\\pmb{v}=(v(1),\\dots,v(I))\\in\\mathbb{R}^{D\\times I}$ and any $i\\in[I]$ , $\\alpha,\\beta\\in[D]$ the relation ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}v_{\\alpha}(i)}\\frac{\\mathrm{d}}{\\mathrm{d}v_{\\beta}(i)}\\hat{f}(\\pmb{v})=0\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "holds. ", "page_idx": 7}, {"type": "text", "text": "Let us denote for $\\boldsymbol{v}\\in\\mathbb{R}^{D}$ by $\\tilde{v}\\,\\in\\,\\mathbb{R}^{D+1}$ the vector $v$ where we append a 1. The most general slot-wise linear function is then of the form ", "page_idx": 7}, {"type": "equation", "text": "$$\nf(v)=T(\\tilde{v}(1)\\otimes\\tilde{v}(2)\\otimes...\\otimes\\tilde{v}(I))\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $T:\\mathbb{R}^{(D+1)^{I}}\\rightarrow\\mathbb{R}$ is a linear map. Note that this class covers linearized attention where the embeddings $v(i,n)$ are split in three separate parts that are used to form key, query, and value vectors. For this function class we can show stronger clustering results. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5. Let $\\Pi$ be approximately balanced, i.e., assume that Assumption 1 holds. Let $\\hat{f}:\\mathbb{R}^{I D}\\rightarrow$ $\\mathbb{R}$ be a slot-wise linear. Assume that $v(i,n,t)$ follow the gradient dynamics (8) and that $v(i,n,t)\\in\\Omega$ for all $i\\in[I]$ , $n\\,\\in\\,[N]$ and $t>0$ for some convex set $\\Omega$ . Assume that Assumption 2 holds with constant $M$ for the set $\\Omega$ . Let $C_{1}$ be the constant from Lemma $^{\\,l}$ (which depends on on $I,\\,D,\\,M,$ ). Assume that at initialization ", "page_idx": 8}, {"type": "equation", "text": "$$\n|v(i,n,t=0)|\\leq\\frac{\\lambda}{8C_{1}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Then there are constant $C_{2},C_{3}\\ge0$ depending on $M,\\,I,\\,D$ , such that for ", "page_idx": 8}, {"type": "equation", "text": "$$\nS\\ge\\operatorname*{max}\\left(C_{2}\\frac{N^{2}K^{I-1}}{\\lambda^{2}}\\ln^{2}(N/\\lambda),C_{3}\\frac{N K^{I-1}}{\\lambda^{4}}\\ln(N/\\lambda)\\right)\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "the bound ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\delta_{\\mathrm{max}}(t)\\leq\\operatorname*{max}\\left(\\delta_{\\mathrm{max}}(0)e^{-\\lambda t/8},\\frac{4C_{1}K^{(I-1)/2}}{\\lambda}\\sqrt{\\ln(S)\\frac{N}{S}}\\right)\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "holds for all $t\\geq0$ . ", "page_idx": 8}, {"type": "text", "text": "The high level summary of this result is that for order $N^{2}\\ln(N)$ samples the clusters can be recovered up to an error of order $\\lambda^{-1}\\sqrt{\\ln(S)N/S}$ . Note that the a-priori assumption that the gradient flow is restricted to the set $\\Omega$ might appear difficult to guarantee in practice. However, we conjecture that the results extend to gradient dynamics clamped at the boundary. Moreover, in Lemma 2 we prove that the mean embeddings $w(i,k,t)$ stay within a ball of radius $R=O(\\sqrt{\\lambda}^{-1})$ for all times. This allows us to prove Theorem 8 which does not require any a-priori bounds on the evolution of $v(i,n,t)$ but comes at the price that the constants $C_{1}$ , $C_{2}$ , and $C_{3}$ depend on $\\lambda$ so we cannot infer the explicit $\\lambda$ dependence. Let us make an important remark. ", "page_idx": 8}, {"type": "text", "text": "Remark 1. While we state our results for a fixed function $\\hat{f}$ this function could in principle be time dependent, e.g., $\\hat{f}$ could be given by a neural network and we consider gradient descent not only on the token embeddings but also on the network parameters. The only requirement is that the assumptions hold uniformly for all times $t$ . In particular, for Theorem 5 we only need to ensure that the derivatives of $\\hat{f}$ stay uniformly bounded in time. This can, e.g., be guaranteed by clipping the parameters of the slot-wise linear map $\\hat{f}$ . ", "page_idx": 8}, {"type": "text", "text": "Our results so far show that we can recover the right cluster structure in the sense that the embeddings from the same group cluster. However, this leaves open the question whether there is any non-trivial dynamics at all, i.e., all embeddings could cluster at the same point. This is in general not the case as can be seen from Corollary 1 which states that in the setting of Theorem 5 and for large times the dynamics of the cluster-means follow the equation ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\dot{w}(i,k)=-\\sum_{k\\in[K]^{I},k_{i}=k}\\frac{\\alpha_{k,i}}{2}\\nabla_{w(i,k_{i})}(\\hat{g}(k)-g(k))^{2}-\\lambda w(i,k)+O\\left(\\sqrt{\\ln(S)\\frac{N}{S}}\\right)\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $(2K)^{-I}\\,\\le\\,\\alpha_{{\\pmb k},i}\\,\\le\\,(2/K)^{I}$ are positive numbers. This shows that $w(i,k)$ follow generally a non-trivial dynamic (and this also justifies the scaling as this expression is of order 1). So in the generic case the cluster structure will be revealed if the numbers of samples is sufficiently large, however, there is no general guarantee that the clusters are well separated. As an illustration of this result we refer to Figure 2 where the clustering of the embeddings becomes apparent. ", "page_idx": 8}, {"type": "text", "text": "Proof idea and overview. Let use here give a quick summary of the main steps of the proof and where to find them. The first important ingredient is an expansion of the loss gradient. We Taylor expand the loss of a sample ${\\pmb v}({\\pmb n})$ around the point ${\\pmb w}(\\Pi({\\pmb n}))$ to second order with remainder terms, the relevant calculations can be found in Appendix D (see Proposition 1 for the outcome). A second ingredient are concentration bounds for certain datapoint-statistics random variables and random matrices. Those are derived in Section G with the necessary results collected in Theorem 9. Combining the Taylor expansion with the concentration result we can extract the dominant terms of the expansion (see Appendix E). Moreover, we obtain such an expansion for $w(i,k)$ (see Corollary 1) and thus the displacements $\\dot{\\delta}(i,n)$ (see Corollary 2). This expansion can then be used to control $\\partial_{t}|\\delta(i,n)|^{2}$ (see Lemma 1) which is sufficient to control $\\delta_{\\mathrm{max}}$ . ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Discussion of assumptions Let us contemplate the differences and similarities to training token embeddings in neural networks. ", "page_idx": 9}, {"type": "text", "text": "\u2022 For the first main result Theorem 4 we make minimal assumptions on $\\hat{f}$ so this could in principle be a neural network applied to the token embeddings. The second result, Theorem 5, is more restrictive but covers subclasses of linearized attention. \u2022 An important feature of the results is that $\\hat{f}$ itself could be time dependent (see Remark 1). ", "page_idx": 9}, {"type": "text", "text": "Differences to standard training of neural networks are: ", "page_idx": 9}, {"type": "text", "text": "\u2022 We use continuous time gradient descent instead of stochastic gradient descent. This is a frequently used modification and in suitable limits those converge (see, e.g., [20]).   \n\u2022 We use mean squared error while sequence modelling usually relies on cross entropy loss. This simplification is frequently used in theoretical analysis but it is expected that results generally extend to the non-convex cross entropy loss.   \n\u2022 A more crucial difference is that the embedding space dimension in practice is usually chosen large to provide large representation capacity. Here $D$ has to be rather small to allow a unique optimal solution of the token embeddings that allows us to recover the cluster structure. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper we considered a learning problem where we try to recover a partition of tokens from their interaction with other tokens. This can be seen as a toy problem for next token prediction in LLMs but also more broadly as a problem in scientific inference. We studied this problem from different perspectives, namely from an information-theoretic, complexity-theoretic, and gradient descent based viewpoint. We found that order $N\\ln(N)$ samples are sufficient to recover the partition for $N$ tokens while we showed that $N^{2}\\ln(N)$ samples are sufficient for gradient based methods. There are several natural open follow up questions. First, there are some open questions regarding the tightness of our analysis of the gradient descent. In particular it is a natural question whether already $\\Omega(N\\ln(N))$ samples are sufficient to control $\\delta_{\\mathrm{max}}$ which is the information-theoretic threshold and would be similar to the optimal results for matrix completion [18, 7]). Another interesting question for future research is whether Theorem 5 holds for standard initialization schemes for the token embeddings. Secondly, it is of interest to relax the notion of clustering of embeddings to more general notions that still allow to recover some structure but are also applicable to high dimensional latent spaces and potentially to multiple partitions and relations on the same tokens (e.g., tokens belonging to different clusters). Thirdly, it is a natural question whether this work can be connected more closely to empirical findings. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[2] Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Machine Learning Research, 18(19):1\u201353, 2017.   \n[3] William Bialek, Andrea Cavagna, Irene Giardina, Thierry Mora, Edmondo Silvestri, Massimiliano Viale, and Aleksandra M. Walczak. Statistical mechanics for natural flocks of birds. Proceedings of the National Academy of Sciences, 109(13):4786\u20134791, 2012.   \n[4] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tom\u00e1s Mikolov. Enriching word vectors with subword information. Trans. Assoc. Comput. Linguistics, 5:135\u2013146, 2017.   \n[5] Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a ConvNet with Gaussian inputs. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 605\u2013614. PMLR, 06\u201311 Aug 2017.   \n[6] Vivien Cabannes, Berfin Simsek, and Alberto Bietti. Learning associative memories with gradient descent. CoRR, abs/2402.18724, 2024.   \n[7] Emmanuel J. Candes and Terence Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Transactions on Information Theory, 56(5):2053\u20132080, 2010.   \n[8] L\u00e9na\u00efc Chizat and Francis Bach. On the global convergence of gradient descent for overparameterized models using optimal transport. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. [9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171\u20134186. Association for Computational Linguistics, 2019.   \n[10] Patrick Esser, Robin Rombach, and Bj\u00f6rn Ommer. Taming transformers for high-resolution image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 12873\u201312883. Computer Vision Foundation / IEEE, 2021.   \n[11] Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, and Philippe Rigollet. A mathematical perspective on transformers, 2024.   \n[12] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 5484\u20135495. Association for Computational Linguistics, 2021.   \n[13] Irene Giardina. Collective behavior in animal groups: theoretical models and empirical studies. HFSP J, 2(4):205\u2013219, Aug 2008.   \n[14] Carla P. Gomes, Henry Kautz, Ashish Sabharwal, and Bart Selman. Chapter 2 satisfiability solvers. In Frank van Harmelen, Vladimir Lifschitz, and Bruce Porter, editors, Handbook of Knowledge Representation, volume 3 of Foundations of Artificial Intelligence, pages 89\u2013134. Elsevier, 2008.   \n[15] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \n[16] Samy Jelassi, Michael Sander, and Yuanzhi Li. Vision transformers provably learn spatial structure. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 37822\u201337836. Curran Associates, Inc., 2022.   \n[17] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7b, 2023.   \n[18] Raghunandan H. Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few entries. IEEE Transactions on Information Theory, 56(6):2980\u20132998, 2010.   \n[19] Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure: Towards a mechanistic understanding. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 19689\u201319729. PMLR, 23\u201329 Jul 2023.   \n[20] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit. CoRR, abs/1902.06015, 2019.   \n[21] Tom\u00e1s Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. In Yoshua Bengio and Yann LeCun, editors, 1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings, 2013.   \n[22] Tom\u00e1s Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. In Lucy Vanderwende, Hal Daum\u00e9 III, and Katrin Kirchhoff, editors, Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, June 9-14, 2013, Westin Peachtree Plaza Hotel, Atlanta, Georgia, USA, pages 746\u2013751. The Association for Computational Linguistics, 2013.   \n[23] OpenAI. GPT-4 technical report, 2023.   \n[24] P. Romanczuk, M. B\u00e4r, W. Ebeling, B. Lindner, and L. Schimansky-Geier. Active brownian particles. The European Physical Journal Special Topics, 202(1):1\u2013162, 2012.   \n[25] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u00e9lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023.   \n[26] Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics, 12(4):389\u2013434, 2012.   \n[27] A\u00e4ron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 6306\u20136315, 2017.   \n[28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[29] Greg Yang. Wide feedforward or recurrent neural networks of any architecture are gaussian processes. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[30] Ruiqi Zhang, Spencer Frei, and Peter L. Bartlett. Trained transformers learn linear models in-context. Journal of Machine Learning Research, 25(49):1\u201355, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "This supplementary material is structured as follows. We first review the notation used in the paper in Appendix A. Then we provide the proofs for the information-theoretic results in Section 3 in Appendix $\\mathbf{B}$ and the proof of Theorem 3 and a reduction to a constraint satisfaction problem in Appendix C. The proofs of the main results concerning the gradient flow rely on a careful Taylor expansion of the loss gradient. This expansion can be found in Appendix D and bounds for this expansion are derived in Appendix E. Based on these bounds we can prove our main results in Appendix F. An important ingredient in bounding the Taylor expansion are concentration results for the dataset statistics that can be found in Appendix G. Finally, we review some results on random matrices in Appendix H which are necessary for the concentration bounds and we review the definition of Kronecker products in Appendix I. ", "page_idx": 12}, {"type": "text", "text": "A Overview of Notation used ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Let us here collect important notation used throughout the paper and the proofs. ", "page_idx": 12}, {"type": "text", "text": "General notation. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "\u2022 Numbers up to $n$ : $[n]=\\{1,\\ldots,n\\}$ \u2022 Eigenvalues of a matrix: $\\lambda_{i}(A)$ \u2022 Largest eigenvalue: $\\lambda_{\\operatorname*{max}}(A)$ \u2022 Operator norm of a matrix: $\\|A\\|$ ", "page_idx": 12}, {"type": "text", "text": "Notation used in the learning problem. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "\u2022 Number of slots: $I$   \n\u2022 Number of tokens for each slot: $N$   \n\u2022 Number of classes: $K$   \n\u2022 Number of samples: $S$   \n\u2022 Map defining the partition in sub-classes: $\\Pi=\\Pi_{1}\\otimes.\\ldots\\otimes\\Pi_{I}:[N]^{I}\\to[K]^{I}$   \n\u2022 Function on classes: $g:[K]^{N}\\to\\mathbb{R}$   \n\u2022 Induced function on tokens: $f:[N]^{I}\\to\\mathbb{R}$ , $f=g\\circ\\Pi$ ", "page_idx": 12}, {"type": "text", "text": "Notation used in the gradient descent analysis. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "\u2022 Dimension of latent space: $D$   \n\u2022 Embedding for token $n$ of slot $i$ : $v(i,n)$   \n\u2022 Token embeddings for a sample $\\mathbfit{\\Delta}$ $:v(n)=(v(1,\\pmb{n}_{1}),\\pmb{\\ldots},v(I,\\pmb{n}_{I}))$   \n\u2022 Token embeddings of a slot $i$ : $\\bar{\\pmb{v}}(i)=(\\boldsymbol{v}(i,n))_{n\\in[N]}$   \n\u2022 All token embeddings: v\u00af = (v\u00af(i))i\u2208[I]   \n\u2022 Mean of cluster embeddings: $w(i,k)$   \n\u2022 Sample version: ${\\pmb w}({\\pmb k})=(w(1,{\\pmb k}_{1}),\\dots,w(I,{\\pmb k}_{I}))$   \n\u2022 Mean token embeddings for slot $i$ : $\\bar{\\pmb{w}}(i)=(\\boldsymbol{w}(i,\\boldsymbol{k}))_{k\\in[K]}$   \n\u2022 All mean embeddings: w\u00af = ( w\u00af(i))i\u2208[I]   \n\u2022 Displacements from cluster center: $\\delta(i,n)=v(i,n)-w(i,\\Pi_{i}(n))$   \n\u2022 Displacements of a sample: $\\pmb{\\delta}(\\pmb{n})=(\\delta(1,\\pmb{n}_{1}),\\pmb{\\ldots},\\delta(\\pmb{I},\\pmb{n}_{I}))$   \n\u2022 Displacements of all tokens for slot $i$ : $\\bar{\\pmb\\delta}(i)=(\\delta(i,n))_{n\\in[N]}$   \n\u2022 Function on token embeddings: ${\\hat{f}}:\\mathbb{R}^{D I}\\rightarrow\\mathbb{R}$ , identified with $\\hat{f}:[N]^{I}\\,\\rightarrow\\,\\mathbb{R},\\,\\hat{f}(\\pmb{n})\\,=$ $\\hat{f}({\\pmb v}({\\pmb n}))$   \n\u2022 Function on classes: $\\hat{g}:[K]^{I}\\rightarrow\\mathbb{R}$ , $\\hat{g}(\\pmb{k})=\\hat{f}(\\pmb{w}(\\pmb{k}))$   \n\u2022 Regularization: $\\lambda$ ", "page_idx": 12}, {"type": "text", "text": "B Proofs for Information-Theoretic Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section we provide the proofs and additional results for Section 3. Let us first start to state the general information-theoretic bound that handles the case where $|\\Pi_{i}^{-1}(k)|$ might be of arbitrary size. ", "page_idx": 13}, {"type": "text", "text": "Theorem 6. Let $g:[K]^{I}\\rightarrow\\mathbb{R}$ satisfy (2) and there is a projection $\\Pi=\\Pi_{1}\\otimes...\\,.\\otimes\\Pi_{I}:[N]^{I}\\to[K]^{I}$ . Assume there is $L>0$ such that for every $\\pmb{k}\\in[K]^{I}$ we have $\\Pi^{-1}(k)\\geq N^{I}/L.$ . Moreover assume for all $i\\in[I]$ and $k\\,\\in\\,[K]$ the bound $|\\Pi_{i}^{-1}(k)|\\geq M$ , i.e., every class has at least $M$ members. Assume we are given $S$ samples $\\{(\\pmb{n}^{s},g\\circ\\Pi(\\pmb{n}^{s})):s\\in[S]\\}$ where $\\pmb{n}\\sim\\mathcal{U}([N]^{I})$ . If ", "page_idx": 13}, {"type": "equation", "text": "$$\nS\\geq2\\cdot\\operatorname*{max}\\left(\\frac{K^{I+1}L}{M}\\operatorname*{max}\\left(\\ln(K)I N,\\eta\\right),2^{I}L N\\operatorname*{max}(2\\ln(N K)I,\\eta)\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for some $\\eta\\geq2$ then with probability at least $1-6e^{-\\eta}$ we can recover $\\Pi$ and g up to permutations of $[K]$ . In other words, for any map $\\Pi^{\\prime}=\\Pi_{1}^{\\prime}\\otimes\\ldots\\otimes\\Pi_{I}^{\\prime}$ and $g^{\\prime}$ such that $g\\circ\\Pi({\\pmb n}^{s})=g^{\\prime}\\circ\\Pi^{\\prime}({\\pmb n}^{s})$ . There are permutations $\\pi_{i}\\in\\mathfrak{S}_{K}$ such that $\\Pi_{i}=\\pi_{i}\\circ\\Pi_{i}^{\\prime}$ and the corresponding relation holds for $g$ $g^{\\prime}$ . ", "page_idx": 13}, {"type": "text", "text": "A few remarks to explain this result are in order. ", "page_idx": 13}, {"type": "text", "text": "Remark 2. The scaling of $S$ might appear slightly complicated so let us comment on this. We are mostly interested in the case where $N$ large and $K$ stays bounded. In addition, we are primarily interested in the regime where $L\\leq C K^{I}$ bounded and $M\\ge c N/K$ (which holds for random $\\Pi$ ), i.e., the sampling probability of each class $\\bar{k}\\in[K]^{I}$ is of similar size. In this case the first term in the condition (31) for $S$ stays constant as $N\\rightarrow\\infty$ . The second term dominates and we see that we need only $O(N\\ln(N))$ samples to identify $\\Pi$ and $g$ . This is the setting studied in Theorem 1 in the main text. The result is essentially tight in this limit (up to the dependence on $I$ ) as stated in Theorem 2. Note that as remarked in the main text the dependence on $L$ cannot be avoided. Indeed, consider the extreme case where $\\Pi_{i}(n)=1$ for $n=1$ and $\\Pi_{i}(n)=2$ for $n>1$ and $g(k_{1},\\dots,k_{I})=1$ iff $k_{1}=...=k_{I}=0$ and $g(k_{1},\\dots,k_{I})=1$ otherwise. Then we have $L=N^{I}$ and we also need order $N^{I}$ samples to sample the point $(0,0,\\ldots,0)\\in[N]^{I}$ which is necessary to identify $\\Pi$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. Let us first introduce some notation. We denote the set of samples ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\mathcal{D}}=\\{\\pmb{n}^{1},\\dots,\\pmb{n}^{S}\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Consider two partition $\\mathcal{P},\\mathcal{Q}$ of a set $[N]$ . We denote by $\\mathcal{P}^{A}$ for $A\\subset[N]$ the restriction of a partition to a subset (i.e., $\\{P\\cap A:P\\in{\\mathcal{P}}\\},$ . The partition distance is defined by ", "page_idx": 13}, {"type": "equation", "text": "$$\nD(\\mathcal{P}_{1},\\mathcal{P}_{2})=\\operatorname*{min}\\{|A^{c}|:\\mathcal{P}_{1}^{A}=\\mathcal{P}_{2}^{A},A\\subset[N]\\},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "i.e., the minimal number of elements that need to be removed such that the partitions agree. ", "page_idx": 13}, {"type": "text", "text": "We call $\\Pi^{\\prime}$ compatible with the datapoints $\\mathcal{D}$ if there is a $g^{\\prime}$ such that $g^{\\prime}\\circ\\Pi^{\\prime}=g\\circ\\Pi$ on the the data. Now the general strategy is to consider any other candidate map $\\Pi^{\\prime}$ and upper bound the probability that is compatible, i.e., for some function $g^{\\prime}$ the functions $f^{\\prime}=g^{\\prime}\\Pi$ and $f$ agree on the data. We will then conclude by applying the union bound over maps $\\Pi^{\\prime}$ . Thus we need to prove an upper bound on the number of partitions with partition distance at most $\\Delta$ and a lower bound on the error probability for a given $\\Delta$ . Let us start with the latter. Denote by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{P}_{i}=\\{\\Pi_{i}^{-1}(k):k\\in[K]\\}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "the partition generated by $\\Pi_{i}$ . Consider any other map $\\Pi^{\\prime}=\\Pi_{1}^{\\prime}\\otimes.\\ldots\\otimes\\Pi_{I}^{\\prime}:[N]^{I}\\rightarrow[K]^{I}$ with corresponding partitions $\\mathcal{P}_{i}^{\\prime}$ . We define for any such $\\Pi^{\\prime}$ the quantity ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Delta(\\Pi^{\\prime})=\\operatorname*{max}_{i\\in[I]}D(\\mathcal{P}_{i},\\mathcal{P}_{i}^{\\prime})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Assume now that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Delta=\\Delta(\\Pi^{\\prime})=D({\\mathcal P}_{1},{\\mathcal P}_{1}^{\\prime})\\leq\\frac{M}{2},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and in particular $D(\\mathcal{P}_{i},\\mathcal{P}_{i}^{\\prime})\\leq\\Delta$ . Let $\\mathcal{P}_{1}=\\{P_{1},\\ldots,P_{K}\\}$ . Let $A$ be a set of maximal size such that $\\mathcal{P}_{1}^{A}=\\mathcal{P}_{1}^{\\prime A}$ . After relabeling we can assume that $P_{k}\\cap A=P_{k}^{\\prime}\\cap A$ where $P_{k}^{\\prime}\\in\\mathcal{P}_{1}^{\\prime}$ and by composing a permutation we can also assume that $P_{k}^{\\prime}=\\Pi_{1}^{\\prime-1}(k)$ . Moreover, ", "page_idx": 13}, {"type": "equation", "text": "$$\n|P_{k}^{\\prime}\\cap A|=|P_{k}\\cap A|\\geq|P_{k}|-|A^{c}|\\geq|P_{k}|-|P_{k}|/2=|P_{k}|/2.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The same applies to all partitions $\\mathcal{P}_{i}$ and $\\mathcal{P}_{i}^{\\prime}$ . Next we claim that if $f=g\\circ\\Pi$ and $f^{\\prime}=g^{\\prime}\\circ\\Pi^{\\prime}$ agree on all samples then with probability $K^{I}e^{-\\frac{S}{2{\\cal I}_{L}}}$ over the randomness of the samples $g=g^{\\prime}$ . Define $E_{\\pmb{k}}:=\\Pi^{\\prime\\,\\hat{-}1}(\\pmb{k})\\cap\\Pi^{-1}(\\pmb{k})$ . Using (37) and the assumption on $\\Pi$ we have that for $k\\in[K]^{N}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n|E_{k}|:=|\\Pi^{\\prime-1}({\\pmb k})\\cap\\Pi^{-1}({\\pmb k})|\\geq2^{-I}|\\Pi^{-1}({\\pmb k})|\\geq\\frac{N^{I}}{2^{I}L}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The probability that none of the $S$ samples is in $E_{k}$ can then be bounded as follows ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{D}\\cap E_{k}=\\emptyset)=\\left(1-\\frac{|E_{k}|}{N^{I}}\\right)^{S}\\leq\\left(1-\\frac{1}{2^{I}L}\\right)^{S}\\leq e^{-\\frac{S}{2^{I}L}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Applying the union bound over $^k$ we find that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{D}}(\\Pi^{\\prime}\\operatorname{is\\consistent}\\operatorname{with}\\mathcal{D}\\operatorname{\\,for\\,some}g^{\\prime}\\neq g)\\leq K^{I}e^{-\\frac{S}{2^{I}L}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now we bound the probability that $g^{\\prime}=g$ is compatible, i.e., $g\\circ\\Pi(\\pmb{n}^{s})=g\\circ\\Pi^{\\prime}(\\pmb{n}^{s})$ for all $s\\in[S]$ . Let now $n_{1}\\in[N]$ . Denote by $E_{n_{1}}\\subset[N]^{I}$ the set of all vectors $\\pmb{n}=(n_{1},n_{2},.~.~.~,n_{I})$ such that ", "page_idx": 14}, {"type": "equation", "text": "$$\ng\\circ\\Pi(n)\\neq g\\circ\\Pi^{\\prime}(n)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We now lower bound the size of $E_{n_{1}}$ under the assumption that there is $k\\in[K]$ such that $n_{1}\\in P_{k}^{\\prime}\\cap A^{c}$ (where $A$ is as above a set such that $\\mathcal{P}_{1}^{A}=\\mathcal{P}_{1}^{\\prime A}\\,$ ). Then $\\Pi_{1}^{\\prime}(n_{1})=k$ and (by minimality of $A^{c}$ ) we find $n_{1}\\notin P_{k}$ and thus $\\Pi_{1}n_{1}={\\bar{k}}\\neq{\\bar{k}}$ for some $\\bar{k}$ . By assumption, we can find $k_{2},\\dots,k_{I}$ such that $g(k;k_{2},\\ldots,k_{I})\\neq g(\\bar{k},k_{2},\\ldots,k_{I})$ . Consider now any vector $\\pmb{n}=(n_{1},n_{2},.~.~.~,n_{I})$ such that $n_{i}\\in\\Pi_{i}^{-1}(k_{i})\\cap\\Pi_{i}^{\\prime-1}(k_{i})$ . Then, for any such $\\mathbfit{\\Delta}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\ng\\circ\\Pi(n)=g(\\bar{k},k_{2},\\ldots,k_{I})\\neq g(k,k_{2},\\ldots,k_{I})=g\\circ\\Pi^{\\prime}(n)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "we conclude that all such $\\mathbfit{\\Delta}$ are in $E_{n_{1}}$ By assumption we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left|[N]\\times\\prod_{i=2}^{I}(\\Pi_{i}^{-1}(k_{i})\\cap\\Pi_{i}^{\\prime-1}(k_{i}))\\right|\\geq\\frac{N^{I}}{2^{I}L}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This implies that ", "page_idx": 14}, {"type": "equation", "text": "$$\n|E_{n_{1}}|\\geq\\frac{N^{I-1}}{2^{I}L}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and therefore ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}(E_{n_{1}})\\geq{\\frac{1}{2^{I}L N}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The sets $E_{n_{1}}$ are disjoint so we conclude that $E=\\cup_{n_{1}\\in[N]}E_{n_{1}}$ satisfies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}(E)=\\sum_{n_{1}\\in[N]}\\mathbb{P}(E_{n_{1}})\\geq|A^{c}|\\frac{1}{2^{I}L N}=\\frac{\\Delta}{2^{I}L N}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now we can upper bound the probability that $\\mathcal{D}$ is compatible with $\\Pi^{\\prime}$ and $g$ by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{D}}(\\mathcal{D}\\mathrm{~is~compatible~with~I',~}g)\\le\\mathbb{P}_{\\mathcal{D}}(E\\cap\\mathcal{D}=\\emptyset)\\le\\left(1-\\frac{\\Delta}{2^{I}L N}\\right)^{S}\\le e^{-\\frac{S\\Delta}{2^{I}L N}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Clearly the same reasoning applies to any other index instead of 1. ", "page_idx": 14}, {"type": "text", "text": "The next step is to upper bound the number of such candidate partitions. The number of partitions such that $D(\\bar{\\mathcal{P}}_{1},\\bar{\\mathcal{P}}_{1}^{\\prime})\\bar{\\leq}\\;\\Delta$ can be bounded by $(N K)^{\\Delta}$ , i.e., we $\\Delta$ times select one of $N$ tokens and assign it to another (or the same) class. This bound can be applied to all indices $1\\leq i\\leq I$ and by the union bound, we find that ", "page_idx": 14}, {"type": "equation", "text": "$$\n|\\{(\\mathcal{P}_{1}^{\\prime},\\ldots,\\mathcal{P}_{I}^{\\prime}):\\,\\operatorname*{max}D(\\mathcal{P}_{i}^{\\prime},\\mathcal{P}_{i})\\leq\\Delta\\}|\\leq(N K)^{I\\Delta}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus we can bound the the probability that any of the maps $\\Pi^{\\prime}$ (there are more such maps than partitions because there is a label assigned to each class but consistency with the data depends only on the underlying permutation) with $\\bar{0^{\\prime}}<\\Delta(\\Pi^{\\prime})\\le M/2$ is consistent with the data is bounded by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{D}}\\big(\\exists\\Pi^{\\prime}\\mathrm{~with~}\\Delta(\\Pi^{\\prime})\\leq M/2\\mathrm{~consistent~with~}\\mathcal{D}\\big)\\leq I\\sum_{\\Delta=1}^{M/2}(N K)^{I\\Delta}\\left(e^{-\\frac{S\\Delta}{2I_{L N}}}+K^{I}e^{-\\frac{S}{2I_{L}}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here the first term corresponds to the upper bound for the probability that (after permutation) $g=g^{\\prime}$ is compatible with the data and the second term bounds the probability that any other function is compatible with the data and $I$ accounts for the fact that any $\\bar{i^{\\in}}\\bar{[I]}$ can be arg maxi $D(\\mathcal{P}_{i},\\mathcal{P}_{i}^{\\prime})$ . ", "page_idx": 15}, {"type": "text", "text": "We now consider the remaining case that there is an index $i\\in[I]$ such that $D(\\mathcal{P}_{i},\\mathcal{P}_{i}^{\\prime})\\geq M/2$ where we can assume w.l.o.g. that $i=1$ . As above let $\\mathcal{P}_{1}=\\{P_{1},\\dotsc,\\dot{P}_{K}\\}$ and $\\mathcal{P}_{1}^{\\prime}=\\{P_{1}^{\\prime},\\ldots,P_{K}^{\\prime}\\}$ such that $P_{k}\\cap A=P_{k}^{\\prime}\\cap A$ and $A$ is a set of maximal size. We now claim that we can find an indices $k\\ne\\bar{k}$ such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n|P_{k}^{\\prime}\\cap P_{k}|\\geq{\\frac{M}{2K}}\\quad{\\mathrm{and}}\\quad|P_{k}^{\\prime}\\cap P_{\\bar{k}}|\\geq{\\frac{M}{2K^{2}}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Indeed, first assume that there is an index $k$ such that $|P_{k}^{\\prime}\\cap P_{k}|\\le M/K$ . Since $|P_{k}^{\\prime}|\\geq M$ there is another index $\\bar{k}$ such that $|P_{\\bar{k}}^{\\prime}\\cap P_{k}|\\ge M/K$ . By maximality of $A$ we have $P_{k}\\cap A=P_{k}^{\\prime}\\cap A=P_{k}\\cap P_{k}^{\\prime}$ and moreover ", "page_idx": 15}, {"type": "equation", "text": "$$\n|P_{k}^{\\prime}\\cap P_{k}|+|P^{\\prime}{\\bar{k}}\\cap P_{\\bar{k}}|\\geq|P^{\\prime}{\\bar{k}}\\cap P_{k}|+|P^{\\prime}{}_{k}\\cap P_{\\bar{k}}|\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "because otherwise exchanging $P_{k}^{\\prime}$ and $P_{\\bar{k}}^{\\prime}$ would allow to pick a larger $A$ . By our assumption we conclude from here that ", "page_idx": 15}, {"type": "equation", "text": "$$\n|P_{\\bar{k}}\\cap P_{\\bar{k}}^{\\prime}|\\geq|P_{k}\\cap P_{\\bar{k}}^{\\prime}|+|P_{\\bar{k}}\\cap P_{k}^{\\prime}|-|P_{k}\\cap P_{k}^{\\prime}|\\geq\\frac{M}{K}-\\frac{M}{2K}=\\frac{M}{2K}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus in this case $|P_{\\bar{k}}^{\\prime}\\cap P_{\\bar{k}}|\\ge M/(2K)$ and $|P_{\\bar{k}}^{\\prime}\\cap P_{k}|\\ge M/K$ and (50) holds. Assume now to the contrary that there is no index $k$ such that $|P_{k}^{\\prime}\\cap P_{k}|<M/K$ . The assumption $|A^{c}|\\geq M/2$ implies that there is $k$ such that $|A^{c}\\cap P_{k}^{\\prime}|\\ge M/(2\\ddot{K})$ and by minimality of $A^{c}$ we find $A^{c}\\cap P_{k}^{\\prime}\\cap P_{k}=\\emptyset$ and thus there is $\\bar{k}\\neq k$ such that $|P_{k}^{\\prime}\\cap P_{\\bar{k}}|=|A^{c}\\cap P_{k}^{\\prime}\\cap P_{\\bar{k}}|\\ge M/(2K^{2})$ . This finishes the proof of (50). ", "page_idx": 15}, {"type": "text", "text": "We now fix $k$ and $\\bar{k}$ such that $|P_{k}\\cap P_{k}^{\\prime}|\\,\\geq\\,M/(2K^{2})$ and $|P_{\\bar{k}}\\cap P_{k}^{\\prime}|\\;\\geq\\;M/(2K^{2})$ . Then, by assumption, we can find $k_{2},\\ldots,k_{I}\\in[K]$ such that $\\pmb{k}=(k,k_{2},\\ldots,k_{I})$ and $\\bar{\\pmb{k}}=(\\bar{k},k_{2},\\ldots,k_{I})$ satisfy $g(\\pmb{k})\\neq g(\\bar{\\pmb{k}})$ . ", "page_idx": 15}, {"type": "text", "text": "Moreover $|\\Pi^{-1}(k)|\\ \\geq\\ N^{I}/L$ . Now for every $k_{i}$ there is $k_{i}^{\\prime}$ such that $|\\Pi_{i}^{-1}(k_{i})\\cap\\Pi_{i}^{\\prime-1}(k_{i}^{\\prime})|\\ \\geq$ $K^{-1}|\\Pi_{i}^{-1}(k_{i})|$ . Thus there is $\\pmb{k}^{\\prime}\\in[K]^{I}$ such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\prod_{i=2}^{I}\\left|\\Pi_{i}^{\\prime-1}(k_{i}^{\\prime})\\cap\\Pi_{i}^{-1}(k_{i})\\right|\\geq K^{-I+1}\\left|\\prod_{i=2}^{I}\\Pi_{i}^{-1}(k_{i})\\right|\\geq\\frac{N^{I}}{K^{I-1}L}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Define $A_{i}=\\Pi_{i}^{\\prime-1}(k_{i}^{\\prime})\\cap\\Pi_{i}^{-1}(k_{i})$ and $A_{1}=P_{k}\\cap P_{k}^{\\prime}$ and $\\bar{A}_{1}=P_{\\bar{k}}\\cap P_{k}^{\\prime}$ . Define $A=A_{1}\\times A_{2}\\times$ $\\dots\\times A_{I}$ and $\\bar{\\pmb{A}}=\\bar{\\pmb{A}}_{1}\\times\\pmb{A}_{2}\\times\\ldots\\times\\pmb{A}_{I}$ . Note that by (50) ", "page_idx": 15}, {"type": "equation", "text": "$$\n|{\\pmb A}|,|\\bar{\\pmb A}|\\geq N^{I}\\frac{M}{2K^{I+1}L}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Clearly $\\Pi^{\\prime}$ is constant (and equal to $(k,k_{2}^{\\prime},\\ldots,k_{I}^{\\prime}))$ on $A\\cap{\\bar{A}}$ and therefore also $g^{\\prime}\\circ\\Pi^{\\prime}$ is constant for any $g^{\\prime}$ . On the other hand $\\Pi(A)=\\{k\\}$ is constant and $\\Pi({\\bar{\\boldsymbol{A}}})={\\bar{\\boldsymbol{k}}}$ is constant but ", "page_idx": 15}, {"type": "equation", "text": "$$\ng\\circ\\Pi(n)=g(\\pmb{k})\\neq g(\\bar{\\pmb{k}})=g\\circ\\Pi(\\bar{\\pmb{n}})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for $\\bar{n}\\in{\\bf A}$ and $\\bar{n}\\in\\bar{A}$ . Now $\\Pi^{\\prime}$ can only be consistent with the data (for any $g^{\\prime}$ ) if there is no sample in $\\pmb{A}$ or no sample in $\\bar{A}$ , i.e., and thus ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{\\mathcal{D}}\\big((\\Pi^{\\prime},g^{\\prime})\\mathrm{~consistent~with~}\\mathcal{D}\\mathrm{~for~a~}g^{\\prime}\\big)\\le\\mathbb{P}_{\\mathcal{D}}\\big(\\mathcal{D}\\cap A=\\emptyset\\mathrm{~or~}\\mathcal{D}\\cap\\bar{A}=\\emptyset\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\le\\mathbb{P}_{\\mathcal{D}}\\big(\\mathcal{D}\\cap A=\\emptyset\\big)+\\mathbb{P}_{\\mathcal{D}}\\big(\\mathcal{D}\\cap\\bar{A}=\\emptyset\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we used the union bound in the last step. Then we find using (54) and (56) ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{D}}\\left((\\Pi^{\\prime},g^{\\prime})\\ \\mathrm{consistent}\\ \\mathrm{with}\\ \\mathcal{D}\\ \\mathrm{for}\\ \\mathrm{a}\\ g^{\\prime}\\right)\\le2\\left(1-\\frac{M}{2K^{I+1}L}\\right)^{S}\\le2e^{-\\frac{S M}{2K^{I+1}L}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally we can bound the number of partitions by the number of maps $\\Pi_{i}:[N]\\rightarrow[K]$ which implies that ", "page_idx": 16}, {"type": "equation", "text": "$$\n|\\{\\mathcal P_{1},\\ldots,\\mathcal P_{I}:\\mathcal P_{i}\\mathrm{~partition~of~}[N]\\mathrm{~in~at~most~}K\\mathrm{~classes}\\}|\\le(K^{N})^{I}=K^{N I}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "So we get the upper bound ", "page_idx": 16}, {"type": "text", "text": "$\\begin{array}{r}{\\mathbb{P}_{\\mathcal{D}}\\big(\\exists\\,\\Pi^{\\prime}\\mathrm{~such~that~}\\Delta(\\Pi^{\\prime})\\geq M/2\\mathrm{~and~}\\Pi^{\\prime}\\mathrm{~compatible~with~}\\mathcal{D}\\big)\\leq2I K^{N I}e^{-\\frac{M S}{K^{I+1}L}}.}\\end{array}$ Combining (49) and (59) and using the union bound we find ", "page_idx": 16}, {"type": "text", "text": "$\\mathbb{P}_{\\mathcal{D}}((\\Pi,g)$ not identifiable up to permutations) $\\leq\\mathbb{P}_{D}(\\,\\exists\\,\\Pi^{\\prime}$ with $\\Delta(\\Pi^{\\prime})>0$ is compatible with $\\mathcal{D}$ ) ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\leq2K^{N I}I e^{-\\frac{M S}{K^{I+1}L}}+I\\sum_{\\Delta=1}^{M/2}(N K)^{I\\Delta}\\left(e^{-\\frac{S\\Delta}{2^{I}L N}}+K^{I}e^{-\\frac{S}{2^{I}L}}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that clearly the assumptions imply $M\\leq N/K<N$ and thus $e^{-S/(2^{I}L)}\\leq e^{-S\\Delta/(2^{I}L N)}$ for $\\Delta\\le M/2$ . We then find (using the simple bound $I K^{I}\\le(2K)^{I}\\le(N K)^{I\\Delta}$ that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{\\Delta=1}^{M/2}(N K)^{I\\Delta}\\left(e^{-\\frac{S\\Delta}{2^{I}L N}}+K^{I}e^{-\\frac{S}{2^{I}L}}\\right)\\leq2\\sum_{\\Delta=1}^{M/2}e^{\\left(2\\ln(N K)I-\\frac{S}{2^{I}L N}\\right)\\Delta}\\leq4e^{-\\eta}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for ", "page_idx": 16}, {"type": "equation", "text": "$$\nS\\geq\\operatorname*{max}(4\\ln(N K)I2^{I}L N,2^{I+1}\\eta L N)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and $\\eta>2$ where we bound the geometric sum by twice its largest term. The first summand in (60) can be bounded by ", "page_idx": 16}, {"type": "equation", "text": "$$\n2I K^{N I}e^{-\\frac{M S}{K^{I+1}L}}=2e^{2\\ln(K)N I-\\frac{M S}{K^{I+1}L}}\\leq2e^{-\\eta}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for ", "page_idx": 16}, {"type": "equation", "text": "$$\nS\\geq\\operatorname*{max}\\left(\\frac{4\\ln(K)I K^{I+1}L N}{M},\\frac{2\\eta K^{I+1}L}{M}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The proof of Theorem 1 is now a direct consequence of the previous result because when assuming that a uniformly random map $\\Pi$ is chosen we can estimate the quantities $M$ and $L$ in the previous theorem with high probability. ", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem $^{\\,l}$ . We observe that $|\\pi_{i}^{-1}(k_{i})|\\sim\\mathrm{Bin}(N,K^{-1})$ . Applying a Chernoff bound on the tail of the binomial variable we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|\\pi_{i}^{-1}(k_{i})|<\\frac{N}{K}(1-\\frac{1}{2})\\right)\\le e^{-\\frac{1}{2^{2}}\\frac{N/K}{2}}=e^{-\\frac{N}{8K}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By the union bound we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\operatorname*{min}_{i\\in[I],k\\in[K]}|\\pi_{i}^{-1}(k)|<N/(2K)\\right)\\le K^{I}e^{-\\frac{N}{8K}}=e^{\\ln(K)I-\\frac{N}{8K}}\\le e^{-\\eta}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "if $N\\geq N_{0}(\\eta,K,I)$ . Note that $M\\geq N/(2K)$ implies that $L\\le(N/M)^{I}\\le(2K)^{I}$ . Assuming that $\\Pi$ is such that the bounds for $M$ and $L$ hold we can apply Theorem 6 and find that for $N\\geq N_{0}$ sufficiently large (depending on $I,K,\\eta)$ and ", "page_idx": 16}, {"type": "equation", "text": "$$\nS\\ge2^{2I+3}I K^{I}N\\ln(N)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "(we bounded $\\ln(N K)\\le2\\ln(N))$ the maps $\\Pi$ and $g$ are identifiable with probability at least $1-e^{-\\eta}$ . Here we used that as $N\\rightarrow\\infty$ the term indicated above is dominating in (31). Now the union bound over the bad events ends the proof. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 2. As before, we note ${\\mathcal{D}}=\\{\\pmb{n}^{1},\\dots,\\pmb{n}^{S}\\}.$ . Assume there is $n_{1}$ such that $\\Pi_{1}(n_{1})=$ 1 and such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\mathcal{D}}\\cap\\{n_{1}\\}\\times\\Pi_{2}^{-1}(1)\\times\\ldots\\times\\Pi_{I}^{-1}(1)=\\emptyset.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then $\\Pi^{\\prime}\\neq\\Pi$ is compatible with $\\mathcal{D}$ where $\\Pi_{i}^{\\prime}=\\Pi_{i}$ for $i\\geq2$ and $\\Pi_{1}(n)=\\Pi_{1}^{\\prime}(n)$ for $n\\neq n_{1}$ and $\\Pi_{1}^{\\prime}(n_{1})=2\\neq1=\\Pi_{1}(n_{1})$ (by assumption on $g$ ). Let us denote ", "page_idx": 17}, {"type": "equation", "text": "$$\nA_{n_{1}}=\\{n_{1}\\}\\times\\Pi_{2}^{-1}(1)\\times\\ldots\\times\\Pi_{I}^{-1}(1)\\subset[N]^{I}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\pmb{n}^{s}\\in A_{n_{1}})=\\frac{1}{N K^{I-1}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To estimate the probability of the event $\\bigcup_{n_{1}}\\left\\{A_{n_{1}}\\cap{\\mathcal{D}}=\\emptyset\\right\\}$ we use Poissonization to make the events independent. Consider datasets $\\tilde{\\mathcal{D}}$ whose distribution is generated by first sampling $\\bar{S}\\sim\\mathrm{Poi}(2S)$ and then conditional on $\\bar{S}$ sample a dataset $\\tilde{\\mathcal{D}}$ as before, i.e., $n^{s}\\sim\\mathcal{U}([N]^{I})$ for $s\\in[\\bar{S}]$ . Then we find that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left|A_{n_{1}}\\cap\\tilde{\\mathcal{D}}\\right|\\sim\\mathrm{Poi}\\left(\\frac{2S}{N K^{I-1}}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and those events are independent for $n_{1}\\neq n_{1}^{\\prime}$ . Thus we find that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\tilde{\\mathcal{D}}}(|A_{n_{1}}\\cap\\tilde{\\mathcal{D}}|=0)=e^{-\\frac{2S}{N K^{I-1}}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By independence we now find that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{P}_{\\tilde{\\mathcal{D}}}\\left(\\bigcap_{n_{1}\\in\\Pi_{1}^{-1}(1)}\\{A_{n_{1}}\\cap\\tilde{\\mathcal{D}}\\neq\\emptyset\\}\\right)=\\prod_{n_{1}\\in\\Pi_{1}^{-1}(1)}\\mathbb{P}_{\\tilde{\\mathcal{D}}}\\left(A_{n_{1}}\\cap\\tilde{\\mathcal{D}}\\neq\\emptyset\\right)}}\\\\ &{}&{=\\left(1-e^{-\\frac{2S}{N K^{I-1}}}\\right)^{N/K}}\\\\ &{}&{\\leq\\left(1-e^{-\\frac{1}{2}\\ln(N/K)}\\right)^{N/K}=\\left(1-\\frac{\\sqrt{K}}{\\sqrt{N}}\\right)^{N/K}\\leq e^{-\\sqrt{N/K}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we used the upper bound for $S$ and $(1-x)\\leq e^{-x}$ . Assume $D_{s}\\sim\\mathcal{U}([N]^{I})^{s}$ and define ", "page_idx": 17}, {"type": "equation", "text": "$$\np_{s}=\\mathbb{P}_{\\mathcal{D}_{s}}\\left(\\bigcap_{n_{1}\\in\\Pi_{1}^{-1}(1)}\\{A_{n_{1}}\\cap\\mathcal{D}_{s}\\neq\\emptyset\\}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that $p_{S}$ is an upper bound on the probability that $\\Pi$ is identifiable as explained above. We have shown that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}(p_{\\mathrm{Poi}(2S)})\\leq e^{-\\sqrt{N/K}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Clearly $p_{s}$ is decreasing in $s$ . This implies that ", "page_idx": 17}, {"type": "equation", "text": "$$\ne^{-\\sqrt{N/K}}\\geq\\mathbb{E}(p_{\\mathrm{Poi}(2S)})\\geq p_{S}\\cdot\\mathbb{P}(\\mathrm{Poi}(2S)\\geq S).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A Chernoff bound for the Poisson distribution reads ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathrm{Poi}(\\lambda)\\leq x)\\leq\\frac{(e\\lambda)^{x}e^{-\\lambda}}{x^{x}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which implies with $\\lambda=2S$ and $x=S$ that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathrm{Poi}(2S)\\leq S)\\leq\\frac{(2S e)^{S}e^{-2S}}{S^{S}}=\\left(\\frac{2}{e}\\right)^{S}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We thus conclude that ", "page_idx": 17}, {"type": "equation", "text": "$$\np_{S}\\leq2e^{-\\sqrt{N/K}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for $S\\geq3$ (implying $1-(2/e)^{S}\\geq1/2)$ ). ", "page_idx": 17}, {"type": "text", "text": "C Proofs of Complexity-Theoretic Analysis and Constraint Satisfaction Reduction ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we provide the proof of Theorem 3 and a general reduction to a constraint satisfaction problem. We start with the proof of Theorem 3. An important property we will use frequently in the proof is that $T$ has the property that for and values $x_{1}$ and $x_{2}$ there is $x_{3}(x_{1},x_{2})$ such that $T(x_{1},x_{2},x_{3}(x_{1},x_{2}))=0$ . Indeed $x_{3}(x_{1},x_{2})=0$ except for $x_{1}=x_{2}=1$ where $x_{3}(1,1)=0$ has this property. ", "page_idx": 18}, {"type": "text", "text": "Proof. We reduce the problem to 3SAT. We consider an arbitrary formula ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\phi(x)=\\textstyle\\bigwedge C_{i}(x)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "on $\\ell$ variables with $m$ clauses $C_{i}$ . We denote $f=g\\circ\\Pi$ . To improve readability we consider a symbol set $\\boldsymbol{S}$ instead of $[N]$ which is used for all 3 slots. We proceed in three steps. First we show that we can write down a set of equations that ensures that two symbols $s_{0},s_{1}\\in{\\mathcal{S}}$ satisfy $\\Pi_{i}(s_{j})=j$ for all $i$ (if $f=g\\circ\\Pi_{,}$ ). Suppose the following relations hold ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(s_{0},s_{0},s_{1})=f(s_{0},s_{1},s_{0})=f(s_{1},s_{0},s_{0})=0,}\\\\ &{f(s_{0},s_{1},s_{1})=f(s_{1},s_{0},s_{1})=f(s_{1},s_{1},s_{0})=1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then it is easy to conclude that $\\Pi_{i}(s_{j})\\,=\\,j$ holds. Indeed suppose that $\\Pi_{1}(s_{1})\\,=\\,0$ . Then the lower part of the previous display combined with the definition of $g$ imply $\\dot{\\Pi}_{2}(s_{0})\\mathrm{~=~}\\Pi_{3}(s_{1})\\mathrm{~=~}$ $\\Pi_{2}(s_{1}\\big)=\\Pi_{3}(s_{0})\\bar{=}\\,1$ . But this is a contradiction to $f(s_{1},s_{0},s_{0})=0$ . Thus $\\Pi_{1}(s_{1})=1$ and the same reasoning implies $\\Pi_{i}(s_{1})=1$ . Then the second equation in (81) directly implies $\\Pi_{i}(s_{0})=0$ . ", "page_idx": 18}, {"type": "text", "text": "We now consider a symbol set ", "page_idx": 18}, {"type": "equation", "text": "$$\nS=\\{s_{0},s_{1}\\}\\cup\\mathcal{X}\\cup\\bar{\\mathcal{X}}\\cup\\mathcal{C}\\cup\\bar{\\mathcal{C}}\\cup\\mathcal{T}\\cup\\mathcal{T}^{\\prime}\\cup\\mathcal{F}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{X}=\\{X_{1},\\ldots,X_{\\ell}\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "will encode the variables in the formula $\\phi$ and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{C}=\\{C_{1,1},C_{1,2},C_{2,1},C_{2,2},...\\,,C_{m,1},C_{m,2}\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "are auxiliary variables for the clauses. In addition we need further auxiliary variables ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\mathcal{X}}=\\{\\bar{X}_{1},\\ldots,\\bar{X}_{\\ell}\\},}\\\\ &{\\bar{\\mathcal{C}}=\\{\\bar{C}_{1,1},\\bar{C}_{1,2},\\bar{C}_{2,1},\\bar{C}_{2,2},\\ldots,\\bar{C}_{m,1},\\bar{C}_{m,2}\\},\\quad\\mathrm{and}}\\\\ &{\\mathcal{O}=\\{o_{1},\\ldots,o_{m}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We now add a set of equations that will ensure that for some value $x_{i}\\in\\{0,1\\}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\Pi_{1}(X_{i})=\\Pi_{2}(X_{i})=\\Pi_{3}(X_{i})=x_{i}}}\\\\ {{\\Pi_{1}(\\bar{X}_{i})=\\Pi_{2}(\\bar{X}_{i})=\\Pi_{3}(\\bar{X}_{i})=1-x_{i}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This can be achieved by adding the following relations for all $i$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(X_{i},\\bar{X}_{i},s_{1})=f(\\bar{X}_{i},X_{i},s_{1})=f(s_{1},\\bar{X}_{i},X_{i})=1,}\\\\ &{f(s_{1},X_{i},\\bar{X}_{i})=f(X_{i},s_{1},\\bar{X}_{i})=f(\\bar{X}_{i},s_{1},X_{i})=1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that the these relations indeed imply that $\\Pi_{a}(X_{i})\\neq\\Pi_{b}(\\bar{X}_{i})$ for $a\\neq b$ , $a,b\\in\\{1,2,3\\}$ . This implies then that (86) holds. We add the similar relations for $C_{i,k}$ which again ensure that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Pi_{j}(C_{i,k})=\\Pi_{j^{\\prime}}(C_{i,k})\\neq\\Pi_{j}(\\bar{C}_{i,k}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now we encode the clauses of the formula. Consider a clause $C_{l}$ of the form $x_{i_{1}}\\vee x_{i_{2}}\\vee x_{i_{3}}$ . Then we add the relations ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\bar{X}_{i_{1}},\\bar{X}_{i_{2}},C_{l,1})=0}\\\\ &{f(\\bar{X}_{i_{2}},\\bar{X}_{i_{3}},C_{l,2})=0}\\\\ &{\\quad f(\\bar{C}_{l,1},\\bar{C}_{l,2},o_{l})=1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For a given choice of $\\Pi$ we set $x_{i}=\\Pi_{1}(X_{i})$ . We now claim that the relations in the last display can hold if and only if $x_{i_{1}}\\vee x_{i_{2}}\\vee x_{i_{3}}$ evaluates to true. Suppose the formula evaluates to false. Then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Pi_{j}(X_{i_{1}})=\\Pi_{j}(X_{i_{2}})=\\Pi_{j}(X_{i_{3}})=0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and therefore ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Pi_{j}(\\bar{X}_{i_{1}})=\\Pi_{j}(\\bar{X}_{i_{2}})=\\Pi_{j}(\\bar{X}_{i_{3}})=1.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then relation (89) implies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Pi_{3}(C_{l,i})=1\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for $i=1,2$ . Using (88) we find that $\\Pi_{1}(\\bar{C}_{l,1})=\\Pi_{2}(\\bar{C}_{l,2})=0$ which implies ", "page_idx": 19}, {"type": "equation", "text": "$$\nf(\\bar{C}_{l,1},\\bar{C}_{l,2},o_{l})=f(0,0,o_{l})=0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for any $\\Pi_{3}{\\left(o_{l}\\right)}$ . Therefore the equations cannot all hold. ", "page_idx": 19}, {"type": "text", "text": "Suppose to the contrary that the formula evaluates to true. Let us first assume that $\\Pi_{1}(X_{i_{1}})=1$ . Then $\\Pi_{1}(\\bar{X}_{i_{1}})=0$ and we can set $\\Pi_{3}(C_{l,1})=0$ which ensures that the first equation is satisfied (for any $\\Pi_{2}(\\bar{X}_{i_{2}}))$ . By definition of $g$ we can also find a value $c_{l,2}$ such that for $c_{l,2}=\\Pi_{3}(C_{l,2})$ the relation $f(\\bar{X}_{i_{2}},\\bar{X}_{i_{3}},C_{l,2})=f(x_{i_{2}},x_{i_{3}},c_{l,2})=0$ holds. Now $\\Pi_{1}(\\bar{C}_{l,1})=1-\\Pi_{3}(C_{l,1})=1$ ensures that for any value of $\\bar{c}_{l,2}=\\Pi_{2}(\\bar{C}_{l,2})$ we can choose $\\Pi_{3}{\\left(o_{l}\\right)}$ such that the relation $f(\\bar{C}_{l,1},\\bar{C}_{l,2},o_{l})=$ $f(1,\\bar{c}_{l,2},o_{l})=1$ holds. The same reasoning applies for $\\dot{\\Pi_{1}}(X_{i_{3}})=1$ and a similar argument applies if $\\Pi_{1}(X_{i_{2}})=1$ . For clauses containing negations the same construction works except that ${\\bar{X}}_{i_{j}}$ has to be replaced by $X_{i_{j}}$ for the negated variables in equation (89). ", "page_idx": 19}, {"type": "text", "text": "Putting everything together we have shown that for a given formula $\\phi$ there is a set of relations of the form $f(n^{s})=t^{s}$ where ${\\pmb n}^{s}\\,\\in{\\cal S}^{3}$ given by (81), (87) (plus similar equations for $C_{l,i})$ ), and (89) which has the following properties. For an assignment $x=(x_{1},\\ldots,x_{n})\\in\\{0,1\\}^{n}$ such that $\\phi(x)=1$ evaluates to true there is a map $\\Pi$ such that for all $s$ the relations $t^{s}=f(n^{s})$ hold and where $\\Pi_{1}(X_{i})=x_{i}$ . On the other hand, if for some $\\Pi$ all the relations $t^{s}=f(n^{s})$ hold, then the Boolean variables $x_{i}=\\Pi(X_{i})$ will satisfy the formula $\\phi$ . This ends the proof. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "We now show how the problem of finding $\\Pi$ and $g$ can be generally expressed as a constraint satisfaction problem. Recall that we want to find (if they exist) for a given $K$ a projection map $\\Pi:[N]^{I}\\to\\mathbf{\\dot{[}K]^{\\!I}}$ and $g:K^{I}\\to\\mathbb{R}$ such that for all given samples $(n^{s},t^{s})$ the relation $t^{s}=g\\circ\\Pi(n^{s})$ holds. The general strategy is to introduce Boolean variables encoding the maps $\\Pi$ and $g$ and then express all conditions as suitable constraints for these variables. We first introduce Boolean variables $t_{k n}^{i}$ for $i\\in[I],\\,k\\in[K]$ , and $n\\in[N]$ which are 1 if token $n$ is in cluster $k$ , i.e., $\\Pi_{i}(n)=k$ and 0 otherwise. Then the expression ", "page_idx": 19}, {"type": "equation", "text": "$$\nt_{1n}^{i}\\vee t_{2n}^{i}\\vee\\ldots\\vee t_{k n}^{i}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for $i\\in[I]$ and $n\\in[N]$ is true if and only if $n$ is assigned to to at least one cluster. In addition, we consider variables $r_{k v}$ for every $\\pmb{k}\\in[K]^{\\check{I}}$ and $v\\in\\operatorname{Im}(f)$ in the (finite) image of $f$ . These variables shall encode whether $g(\\pmb{k})=v$ is is true or not. Then the constraints ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\neg r_{k v}\\vee\\neg r_{k v^{\\prime}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for $v\\neq v^{\\prime}$ and all $^k$ ensure that each cluster is assigned at most one value $v$ . Finally, we add for every datapoint $(n^{s},f(n^{s}))$ and every $\\pmb{k}$ the constraint ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\neg t_{k_{1}n_{1}^{s}}^{1}\\lor\\ldots.\\ldots\\lor\\neg t_{k_{I}n_{I}^{s}}^{I}\\lor r_{k f(n^{s})}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This ensures that if $\\Pi(n^{s})=k$ then this cluster must be assigned value $f(n^{s})$ . ", "page_idx": 19}, {"type": "text", "text": "We then consider the constraint satisfaction problem consisting of the $\\Lambda$ of all the conditions in (94), (95), and (96). Then any satisfying assignment gives rise to a map $\\Pi$ and $g$ such that $g\\circ\\Pi(n^{2})=t^{s}$ . Indeed, we set $g(\\pmb{k})=\\dot{v}$ if $s_{k v}=1$ if such a $v$ exists and arbitrarily otherwise. Note that for every satisfying assignment and every $^k$ there is at most one such $v\\mathrm{~so~}g$ is well defined. Moreover, we set $\\Pi_{i}(n)=k$ for any $k$ such that $t_{n k}^{i}=1$ and at least one such $k$ exists. On the other hand, we can easily construct a satisfying assignment given $\\Pi$ and $g$ so that there are no solutions $\\Pi$ and $g$ if the constraint satisfaction problem has no solution. ", "page_idx": 19}, {"type": "text", "text": "Note that we did not ensure that each token is assigned only to a single cluster but this can be achieved in postprocessing or by adding additional constraints (such as $\\neg t_{k_{1}n}^{i}\\vee\\neg t_{k_{1}n}^{i})$ . ", "page_idx": 19}, {"type": "text", "text": "D Taylor Expansion of the Loss Gradient ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The goal of this and the following section is to lay the groundwork for the proof of the main results. ", "page_idx": 20}, {"type": "text", "text": "The general strategy of our proofs is to Taylor expand the loss of each term around the mean token embedding $w(i,\\Pi_{i}({\\pmb n}_{i}))$ to first order plus a remainder term. We can then extract the dominating terms of these expansions using concentration results for the datapoint statistics. It turns out that the linearized dynamics has favourable properties while the remainder terms can be bounded. In this section we derive the expansion of the loss gradient while the required bounds, in particular Theorem 7 and Lemma 1, are derived in the next Section, Appendix E (they rely on concentration results which are deferred to Appendix G). ", "page_idx": 20}, {"type": "text", "text": "As pointed out above, the goal of this section is to Taylor expand the sample loss for one sample where we expand the loss around the point ${\\pmb w}(\\Pi({\\pmb n}))$ . Let us first introduce some notation. Recall that we denoted by $\\pmb{v}(\\pmb{n})=(v(1,\\pmb{n}_{1}),\\pmb{\\ldots},v(I,\\pmb{\\dot{n}}_{I}))$ and by $\\bar{\\pmb v}(i)$ the concatenation of the embeddings $(v(i,n))_{n\\in[N]}$ . We define similarly ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\pmb{\\delta}(\\pmb{n})=(\\delta(1,\\pmb{n}_{1}),\\pmb{\\ldots},\\delta(\\pmb{I},\\pmb{n}_{I}))\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and we denote by $\\pmb{\\bar{\\delta}}(i)\\in\\mathbb{R}^{D N}$ the concatenation of $\\delta(i,1),\\dots,\\delta(i,n)$ , i.e., ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\bar{\\pmb\\delta}(i)=(\\delta(i,n))_{n\\in[N]}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and $\\overline{{\\pmb\\delta}}\\in\\mathbb{R}^{I D N}$ as the concatenation of $\\bar{\\pmb\\delta}(1),\\dots,\\bar{\\pmb\\delta}(I)$ . ", "page_idx": 20}, {"type": "text", "text": "Consider a data-point $\\mathbfit{\\Delta}$ with ${\\boldsymbol{\\Pi}}({\\boldsymbol{n}})={\\boldsymbol{k}}$ . We now consider the derivative of the mean squared error of this term, i.e., ", "page_idx": 20}, {"type": "equation", "text": "$$\nL(\\pmb{n})=\\frac{1}{2}(\\hat{f}(\\pmb{n})-f(\\pmb{n}))^{2}=\\frac{1}{2}(\\hat{f}(\\pmb{n})-g(\\pmb{k}))^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We use greek-indices for the latent space dimension. Fix an $i\\in[I]$ as the slot with respect to which we take the derivative. We then find for $\\alpha\\in[D]$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathrm{d}}{\\mathrm{d}v_{\\alpha}(i,n_{i})}\\frac{1}{2}\\left(\\hat{f}(v(n))-f(k)\\right)^{2}=\\frac{\\mathrm{d}}{\\mathrm{d}v_{\\alpha}(i,n_{i})}\\frac{1}{2}\\left(\\hat{f}(v(n))-f(k)\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\left(\\frac{\\mathrm{d}}{\\mathrm{d}v_{\\alpha}(i,n_{i})}\\hat{f}(v(n))\\right)\\cdot\\left(\\hat{f}(v(n))-f(k)\\right)=:h(v(n)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here we introduced the shorthand $h(v(n))$ for this function which also depends on $i,k$ , and $\\alpha$ . Now we estimate this function by Taylor expanding it around the point ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\pmb{w}(\\pmb{k})=(w(1,\\pmb{k}_{1}),\\dots,w(I,\\pmb{k}_{I}))).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then we find the following expansion to second order ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h({\\pmb v}({\\pmb n}))=h({\\pmb w}({\\pmb k}))+\\displaystyle\\sum_{j\\in[I]}\\sum_{\\beta\\in[D]}\\frac{\\mathrm{d}}{\\mathrm{d}w_{\\beta}(j,{\\pmb k}_{j})}h({\\pmb w}({\\pmb k}))\\delta_{\\beta}(j,{\\pmb n}_{j})}\\\\ &{\\qquad\\qquad+\\displaystyle\\sum_{j_{1},j_{2}\\in[I]}\\sum_{\\beta_{1}\\beta_{2}\\in[D]}R_{(j_{1},\\beta_{1}),(j_{2},\\beta_{2})}^{n,i,\\alpha}({\\pmb w}({\\pmb k}))\\delta_{\\beta_{1}}(j_{1},{\\pmb n}_{j_{1}})\\delta_{\\beta_{2}}(j_{2},{\\pmb n}_{j_{2}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $R_{(j_{1},\\beta_{1}),(j_{2},\\beta_{2})}$ denotes the remainder terms. We remark that if we assume that $v(i,n)\\in\\Omega$ for some convex set $\\Omega$ and all $i\\in[I],n\\in[N]$ then (by convexity of $\\Omega$ ) also $w(i,\\pmb{k}_{i})\\in\\Omega$ and ", "page_idx": 20}, {"type": "equation", "text": "$$\n|R_{(j_{1},\\beta_{1}),(j_{2},\\beta_{2})}^{n,i,\\alpha}|\\leq\\operatorname*{max}_{{v\\in\\Omega}^{I}}\\frac{1}{2}\\left|\\frac{\\mathrm{d}}{\\mathrm{d}v_{\\beta_{1}}(j_{1},{n}_{j_{1}})}\\frac{\\mathrm{d}}{\\mathrm{d}v_{\\beta_{2}}(j_{2},{n}_{j_{2}})}h(\\boldsymbol{v})\\right|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let us now (abusing notation again, it will be clear from context which function we refer to) write ", "page_idx": 20}, {"type": "equation", "text": "$$\nL(\\pmb{k})=\\frac{1}{2}(\\hat{f}(\\pmb{w}(\\pmb{k}))-g(\\pmb{k}))^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We introduce some more quantities to get a concise representation of the Taylor expansion. We define the matrices $D_{i,j}L(\\pmb{k})\\in\\dot{\\mathbb{R}}^{D\\times D}$ containing the derivatives of a function $L$ with respect to $w(j_{1},n_{j_{1}})$ and $w(j_{2},n_{j_{2}})$ , i.e., we consider ", "page_idx": 21}, {"type": "equation", "text": "$$\n(D_{j_{1},j_{2}}L(k))_{\\alpha,\\beta}=\\frac{\\mathrm{d}}{\\mathrm{d}w_{\\alpha}(j_{1},k_{j_{1}})}\\frac{\\mathrm{d}}{\\mathrm{d}w_{\\beta}(j_{2},k_{j_{2}})}L(k).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Similarly we define the vector $D_{j}L(\\pmb{k})\\in\\mathbb{R}^{D}$ by ", "page_idx": 21}, {"type": "equation", "text": "$$\n(D_{j}L(\\pmb{k}))_{\\alpha}=\\frac{\\mathrm{d}}{\\mathrm{d}w_{\\alpha}(j,\\pmb{k}_{j})}L(\\pmb{k}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the diagonal entries $D_{j,j}$ we need a more fine grained decomposition. Note that by the product rule we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\mathrm{d}}{\\mathrm{d}w_{\\alpha}(j,k_{j})}\\frac{\\mathrm{d}}{\\mathrm{d}w_{\\beta}(j,k_{j})}L(k)=\\left(\\frac{\\mathrm{d}}{\\mathrm{d}w_{\\alpha}(j,k_{j})}\\frac{\\mathrm{d}}{\\mathrm{d}w_{\\beta}(j,k_{j})}\\hat{f}(w(k))\\right)\\cdot\\left(\\hat{f}(w(k))-g(k)\\right)}\\\\ {+\\left(\\frac{\\mathrm{d}}{\\mathrm{d}w_{\\alpha}(i,k_{i})}\\hat{f}(w(k)\\right)\\left(\\frac{\\mathrm{d}}{\\mathrm{d}w_{\\beta}(i,k_{i})}\\hat{f}(w(k))\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using the notations we introduced we can therefore write (recall $\\hat{g}(\\pmb{k})=\\hat{f}(\\pmb{w}(\\pmb{k})))$ ", "page_idx": 21}, {"type": "equation", "text": "$$\nD_{j,j}L(\\pmb{k}))=\\left(D_{j,j}\\hat{g}(\\pmb{k})\\right)\\left(\\hat{g}(\\pmb{k})-g(\\pmb{k})\\right)+D_{j}\\hat{g}(\\pmb{k})\\otimes D_{j}\\hat{g}(\\pmb{k}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Collecting finally all remainder terms as ", "page_idx": 21}, {"type": "equation", "text": "$$\nR_{\\alpha}(i,n)=\\sum_{j_{1},j_{2}\\in[I]}\\sum_{\\beta_{1}\\beta_{2}\\in[D]}R_{(j_{1},\\beta_{1}),(j_{2},\\beta_{2})}^{n,i,\\alpha}(w(k))\\delta_{\\beta_{1}}(j_{1},n_{j_{1}})\\delta_{\\beta_{2}}(j_{2},n_{j_{2}})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "we can thus summarize the Taylor expansion result as follows ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{D_{i}L(n)=D_{i}L(k)+\\left((D_{i,i}\\hat{g}(k))\\left(\\hat{g}(k)-g(k)\\right)+D_{i}\\hat{g}(k)\\otimes D_{i}\\hat{g}(k)\\right)\\delta(i,n_{i})}}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~~+\\sum_{j\\ne i}(D_{i,j}L(k))\\delta(j,n_{j})+R(i,n)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Based on the expansion (110) we now want to get an expression for the gradient of the total loss on the entire dataset. We decompose the dataset as follows ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{D}^{k,n,i}=\\{n^{s}\\in\\mathcal{D}|\\,\\Pi(n^{s})=k,\\,n_{i}^{s}=n\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that if $\\Pi_{i}(n)\\neq k_{i}$ then $\\mathcal{D}^{k,n,i}=\\emptyset$ . We also define similarly ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\mathcal{D}}^{n,i}=\\{{\\pmb n}^{s}\\in{\\mathcal{D}}|\\,{\\pmb n}_{i}^{s}=n\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We find the following expansion ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}v(i,n)}\\hat{\\mathcal{L}}(\\bar{v})=\\frac{\\mathrm{d}}{\\mathrm{d}v(i,n)}\\sum_{s=1}^{S}L({\\pmb n}^{s})=\\displaystyle\\sum_{k\\in[K]^{l}}\\sum_{n\\in\\mathcal{D}^{k,n,i}}\\frac{\\mathrm{d}}{\\mathrm{d}v(i,n)}L({\\pmb n})}\\\\ {\\displaystyle=\\sum_{k\\in[K]^{l}}\\sum_{n\\in\\mathcal{D}^{k,n,i}}D_{i}L(k)+\\left((D_{i,i}\\hat{g}(k))\\left(\\hat{g}(k)-g(k)\\right)+D_{i}\\hat{g}(k)\\otimes D_{i}\\hat{g}(k)\\right)\\delta(i,{\\pmb n}_{i})}\\\\ {\\displaystyle\\qquad+\\sum_{k\\in[K]^{l}}\\sum_{n\\in\\mathcal{D}^{k,n,i}}\\sum_{j\\neq i}(D_{i,j}L(k))\\delta(j,{\\pmb n}_{j})+\\sum_{{\\pmb n}\\in\\mathcal{D}^{n,i}}R(i,{\\pmb n})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We now rewrite or bound the four summands. First we relate those expressions to the datapoint statistics matrices. We define ", "page_idx": 21}, {"type": "equation", "text": "$$\nB_{n}^{k,i}=|{\\mathcal{D}}^{k,n,i}|=|\\{n\\in{\\mathcal{D}}:\\Pi(n)=k,\\,n_{i}=n\\}|.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\nA_{n,n^{\\prime}}^{k,i,j}=|\\{n\\in\\mathcal{D}^{k,n,i}|n_{j}=n^{\\prime}\\}|=|\\{n\\in\\mathcal{D}:\\,\\Pi(n)=k,\\,n_{i}=n,\\,n_{j}=n^{\\prime}\\}|.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then the first term equals ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{{\\pmb k}\\in[K]^{I}}\\sum_{{\\pmb n}\\in{\\mathcal{D}}^{{\\pmb k},{\\boldsymbol n},i}}D_{i}L({\\pmb k})=\\sum_{{\\pmb k}\\in[K]^{I}}B_{n}^{{\\pmb k},i}D_{i}L({\\pmb k}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The second term is similarly given by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{k\\in[K]^{I}}\\sum_{n\\in\\mathbb{D}^{k,n,i}}\\left(\\left(D_{i,i}\\hat{g}(k)\\right)\\left(\\hat{g}(k)-g(k)\\right)+D_{i}\\hat{g}(k)\\otimes D_{i}\\hat{g}(k)\\right)\\delta(i,n_{i})}}\\\\ &{=\\displaystyle\\sum_{k\\in[K]^{I}}B_{n}^{k,i}\\Big(\\left(D_{i,i}\\hat{g}(k)\\right)\\left(\\hat{g}(k)-g(k)\\right)+D_{i}\\hat{g}(k)\\otimes D_{i}\\hat{g}(k)\\Big)\\delta(i,n)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The third term is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{k\\in[K]^{I}}\\sum_{n\\in\\mathcal{D}^{k,n,i}}\\sum_{j\\neq i}(D_{i,j}L(k))\\delta(j,n_{j})=\\sum_{k\\in[K]^{I}}\\sum_{j\\neq i}\\sum_{n^{\\prime}\\in[N]}A_{n,n^{\\prime}}^{k,i,j}(D_{i,j}L(k))\\delta(j,n^{\\prime})}}\\\\ &{=\\displaystyle\\sum_{k\\in[K]^{I}}\\sum_{j\\neq i}\\left(\\left(A^{k,i,j}\\otimes D_{i,j}L(k)\\right)\\bar{\\delta}(j)\\right)_{n D:(n+1)D}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\bar{\\pmb\\delta}$ was introduced in (98). Let us summarize those findings as a proposition. ", "page_idx": 22}, {"type": "text", "text": "Proposition 1. Assume that the bound (15) holds for some $\\Omega$ and $v(i,n)\\in\\Omega$ for all $i\\in[I],\\,n\\in[N]$ . Then the following expansion holds ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\mathrm{d}}{\\mathrm{d}v(i,n)}\\hat{L}(\\bar{v})=\\sum_{k\\in[K]^{I}}B_{n}^{k,i}D_{i}L(k)}}\\\\ &{+\\sum_{k\\in[K]^{I}}B_{n}^{k,i}\\left((D_{i,i}\\hat{g}(k))\\left(\\hat{g}(k)-g(k)\\right)+D_{i}\\hat{g}(k)\\otimes D_{i}\\hat{g}(k)\\right)\\delta(i,n)}\\\\ &{+\\sum_{k\\in[K]^{I}}\\sum_{j\\neq i}\\left(\\left(A^{k,i,j}\\otimes D_{i,j}L(k)\\right)\\bar{\\delta}(j)\\right)_{n D;(n+1)D}}\\\\ &{+\\sum_{n\\in\\mathbb{D}^{n,i}}R(i,n)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. The result follows from the calculations above. ", "page_idx": 22}, {"type": "text", "text": "E Bounds for the Loss Gradient ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The goal of this section is to apply the concentration bounds that will be derived in Appendix G to the expansion from the previous section which allows us to extract the (with high probability) dominating terms. Recall that we defined $\\delta_{\\mathrm{max}}=\\mathrm{max}_{i\\in[I],n\\in[N]}\\,|\\delta(i,n)|$ in (11) as the maximal deviation norm and $r_{\\mathrm{max}}=\\mathrm{max}_{k\\in[K]^{I}}\\,|\\hat{g}(\\pmb{k})-g(\\pmb{k})|$ in (13) as the maximal residual. Finally we introduce the notation ", "page_idx": 22}, {"type": "equation", "text": "$$\np(\\pmb{k},i)=\\frac{|\\Pi^{-1}(\\pmb{k})|}{N^{I}|\\Pi_{i}^{-1}(\\pmb{k}_{i})|}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that $B_{n}^{k,i}\\sim\\mathrm{Bin}(S,p(\\pmb{k},i))$ if $\\Pi_{i}(n)=k_{i}$ and $B_{n}^{k,i}=0$ otherwise. In particular, $\\mathbb{E}B_{n}^{k,i}=$ $S\\cdot p({\\pmb k},i)$ . Note that if $\\Pi$ is approximately balanced if the following bounds hold ", "page_idx": 22}, {"type": "equation", "text": "$$\nN p(\\pmb{k},i)=N\\frac{|\\Pi^{-1}(\\pmb{k})|}{N^{I}|\\Pi_{i}^{-1}(\\pmb{k}_{i})|}=\\prod_{j\\neq i}\\frac{|\\Pi_{j}^{-1}(\\pmb{k}_{j})|}{N}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This implies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left(\\frac{1}{2K}\\right)^{I}\\leq N p(\\pmb{k},i)\\leq\\left(\\frac{2}{K}\\right)^{I}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Theorem 7. Assume that the bounds in Theorem 9 hold and assume $\\hat{f}$ satisfies Assumption 2 for some set $\\Omega$ and $v(i,n)\\in\\Omega$ for $i\\in[I]$ and $[N]$ . Then we obtain the following expansion for the loss gradient ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{N}{S}\\frac{\\mathrm{d}}{\\mathrm{d}v(i,n)}\\mathcal{\\hat{L}}(\\bar{v})}\\\\ &{\\quad=\\underset{k\\in[K]^{I}:k_{i}=\\Pi_{i}(n)}{\\sum\\sum}\\,N p(k,i)D_{i}L(k)}\\\\ &{\\quad\\quad+\\underset{k\\in[K]^{I}:k_{i}=\\Pi_{i}(n)}{\\sum}\\,N p(k,i)\\left((D_{i,i}\\hat{g}(k))\\left(\\hat{g}(k)-g(k)\\right)+D_{i}\\hat{g}(k)\\otimes D_{i}\\hat{g}(k)\\right)\\delta(i,n)}\\\\ &{\\quad\\quad+\\,E^{F}(i,n)+E^{S,1}(i,n)+E^{S,2}(i,n)+E^{I}(i,n)+E^{T}(i,n).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here the error terms are bounded by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{|E_{\\alpha}^{F}(i,n)|\\leq4M\\operatorname*{min}(1,r_{\\operatorname*{max}})\\left(2K\\right)^{(I-1)/2}\\sqrt{\\ln(S)\\frac{N}{S}},}&\\\\ &{|E_{\\alpha}^{S,1}(i,n)|\\leq4D M\\operatorname*{min}(1,r_{\\operatorname*{max}})\\left(2K\\right)^{(I-1)/2}\\sqrt{\\ln(S)\\frac{N}{S}}\\delta_{\\operatorname*{max}},}&\\\\ &{|E_{\\alpha}^{S,2}(i,n)|\\leq4D M\\left(2K\\right)^{(I-1)/2}\\sqrt{\\ln(S)\\frac{N}{S}}\\delta_{\\operatorname*{max}},}&\\\\ &{|E^{I}(i,n)|\\leq6D M\\left(2K\\right)^{(I-1)/2}\\ln(S)\\sqrt{\\frac{N^{2}}{S}}\\delta_{\\operatorname*{max}},}&\\\\ &{|E_{\\alpha}^{T}(i,n)|\\leq M I^{2}D\\delta_{\\operatorname*{max}}^{2}\\left(1+4\\sqrt{\\frac{\\ln(S)N}{S}}\\right).}&\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. The proof is a bit technical and essentially combines the assumptions and the concentration results in a straigtforward fashion. First we remark that for $n\\in\\Pi_{i}^{-1}(k)$ we have as stated before that ", "page_idx": 23}, {"type": "equation", "text": "$$\nE_{\\mathcal{D}}B_{n}^{k,i}={\\boldsymbol{S}}\\cdot{\\boldsymbol{p}}({\\boldsymbol{k}},i)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and $B_{n}^{k,i}=0$ otherwise. Then Proposition 1 implies that (123) holds with the definitions of the error terms given below. We now define and bound the error terms in the decomposition. Note that we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n|(D_{j}L(k))_{\\alpha}|=\\left|{\\frac{\\mathrm{d}}{\\mathrm{d}v_{\\alpha}(j,k_{j})}}L(k)\\right|=\\left|{\\frac{\\mathrm{d}}{\\mathrm{d}v_{\\alpha}(j,k_{j})}}\\hat{g}(k)(\\hat{g}(k)-g(k))\\right|\\leq M\\operatorname*{min}(1,r_{\\mathrm{max}}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we either bound both terms using Assumption 2 or the second term by $r_{\\mathrm{max}}$ and the first one by Assumption 2 together with $M^{\\prime}\\leq M$ . This then implies that the error term capturing the fluctuations of the occupation statistics ", "page_idx": 23}, {"type": "equation", "text": "$$\nE^{F}(i,n)=\\frac{N}{S}\\sum_{k\\in[K]^{I}}\\left(\\pmb{B}_{n}^{k,i}-\\mathbb{E}_{\\mathcal{D}}\\pmb{B}_{n}^{k,i}\\right)D_{i}L(\\pmb{k})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "can be bounded by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|E_{\\alpha}^{F}(i,n)\\right|\\leq4M\\operatorname*{min}(1,r_{\\operatorname*{max}})\\left(2K\\right)^{(I-1)/2}\\sqrt{\\ln(S)\\frac{N}{S}}\\leq C(I,K,M)\\operatorname*{min}(1,r_{\\operatorname*{max}})\\sqrt{\\ln(S)\\frac{N}{S}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": ")e. d Nthexatt , thweer ec oanres $K^{I-1}$ e nsoenlf- viantneirsahcitnigo nt eerrmrosr  itne rthmes sum (if $\\Pi_{i}(n)\\neq k_{i}$ we have $B_{n}^{k,i}=$ $\\mathbb{E}B_{n}^{k,i}=0_{.}^{!}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle E^{S,1}(i,n)=\\frac{N}{S}\\sum_{k\\in[K]^{I}}\\left(B_{n}^{k,i}-\\mathbb{E}B_{n}^{k,i}\\right)(\\hat{g}(k)-g(k))\\left(D_{i,i}\\hat{g}(k)\\right)\\delta(i,n),}}\\\\ {{\\displaystyle E^{S,2}(i,n)=\\frac{N}{S}\\sum_{k\\in[K]^{I}}\\left(B_{n}^{k,i}-\\mathbb{E}B_{n}^{k,i}\\right)(D_{i}\\hat{g}(k)\\otimes D_{i}\\hat{g}(k))\\delta(i,n).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For $E^{S,1}$ we get (similar to above) and using that the operator norm of a $D\\times D$ matrix $A$ is bounded by $D\\operatorname*{max}\\left|a_{i j}\\right|)$ the bound ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|E_{\\alpha}^{S,1}(i,n)|\\leq4D M\\operatorname*{min}(1,r_{\\operatorname*{max}})\\left(2K\\right)^{(I-1)/2}\\sqrt{\\ln(S)\\frac{N}{S}}\\delta_{\\operatorname*{max}}}\\\\ &{\\qquad\\qquad\\leq C(I,K,M,D)\\operatorname*{min}(1,r_{\\operatorname*{max}})\\delta_{\\operatorname*{max}}\\sqrt{\\ln(S)\\frac{N}{S}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The error term $E^{S,2}$ could also be absorbed in other terms later on but since it is not dominant we just bound it. We can obtain similarly to our treatment of $E^{S,1}$ the bound ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle|E_{\\alpha}^{S,2}(i,n)|\\leq4D M\\left(2K\\right)^{(I-1)/2}\\sqrt{\\ln(S)\\frac{N}{S}}\\delta_{\\operatorname*{max}}}\\\\ {\\displaystyle\\leq C(I,K,M,D)\\delta_{\\operatorname*{max}}\\sqrt{\\ln(S)\\frac{N}{S}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Next, we consider the interaction error term ", "page_idx": 24}, {"type": "equation", "text": "$$\nE^{I}(i,n)=\\frac{N}{S}\\sum_{\\pmb{k}\\in[K]^{I}}\\sum_{j\\neq i}\\left(\\left(\\pmb{A}^{k,i,j}\\otimes D_{i,j}L(\\pmb{k})\\right)\\bar{\\delta}(j)\\right)_{n D:(n+1)D}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We note that $\\mathbb{E}(A_{n_{1},n_{2}}^{k,i,j}=c$ for some constant $c$ if $\\Pi_{i}(n_{1})=k_{i}$ and $\\Pi_{j}(n_{2})=k_{j}$ and 0 otherwise. This implies that if $\\Pi_{i}(n)=k_{i}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left(\\left(\\mathbb{E}A^{k,i,j}\\otimes D_{i,j}L(k)\\right)\\bar{\\delta}(j)\\right)_{n D:(n+1)D}=\\sum_{n^{\\prime}\\in\\Pi_{j}^{-1}(k_{j})}L(k)\\delta(j,n^{\\prime})=0\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "here we used ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{n\\in|\\Pi_{i}^{-1}(k)|}\\delta(i,n)=0\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which follows from the definition (10) in the last step. If $\\Pi_{i}(n)\\neq k_{i}$ then this expression is clearly zero as the corresponding row of $A^{k,i,j}$ is zero. Thus we find that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|E^{I}(i,n)|=\\Bigg|\\displaystyle\\frac{N}{S}\\sum_{k\\in[K]^{I}}\\sum_{j\\neq i}\\left(\\left((A^{k,i,j}-\\mathbb{E}A^{k,i,j})\\otimes D_{i,j}L(k)\\right)\\bar{\\delta}(j)\\right)_{n D:(n+1)D}\\Bigg|}\\\\ &{\\qquad\\qquad\\leq K^{I-1}\\displaystyle\\frac{N}{S}\\operatorname*{max}_{k,i,j}\\lVert A^{k,i,j}-\\mathbb{E}A^{k,i,j}\\rVert\\lVert D_{i,j}L(k)\\rVert\\cdot\\operatorname*{max}_{j}|\\bar{\\delta}(j)|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here we used the subm\u221aultiplicativity of the operator norm of Kronecker-products stated in (280). Now we use $\\vert\\bar{\\delta}(j)\\vert\\,\\le\\,\\sqrt{N}\\delta_{\\mathrm{max}}$ , the concentration bound (251), and bound $\\lVert D_{i,j}L(\\pmb{k})\\rVert\\leq D M$ similar as before to find ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|E^{I}(i,n)\\right|=6D M\\left(2K\\right)^{(I-1)/2}\\ln(S)\\sqrt{\\frac{N^{2}}{S}}\\delta_{\\mathrm{max}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Finally, we bound the Taylor expansion remainder term ", "page_idx": 24}, {"type": "equation", "text": "$$\nE^{T}(i,n)={\\frac{N}{S}}\\sum_{{\\pmb{n}}\\in{\\mathcal{D}}^{n,i}}R(i,{\\pmb n}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Recall that ", "page_idx": 24}, {"type": "equation", "text": "$$\nR_{\\alpha}(i,n)=\\sum_{j_{1},j_{2}\\in[I]}\\sum_{\\beta_{1}\\beta_{2}\\in[D]}R_{(j_{1},\\beta_{1}),(j_{2},\\beta_{2})}^{n,i,\\alpha}(w(k))\\delta_{\\beta_{1}}(j_{1},n_{j_{1}})\\delta_{\\beta_{2}}(j_{2},n_{j_{2}})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n|R_{(j_{1},\\beta_{1}),(j_{2},\\beta_{2})}^{n,i,\\alpha}|\\leq\\operatorname*{max}_{\\pmb{v}\\in\\Omega}\\frac{1}{2}\\left|\\frac{\\mathrm{d}}{\\mathrm{d}\\boldsymbol{v}_{\\beta_{1}}(j_{1},\\boldsymbol{n}_{j_{1}})}\\frac{\\mathrm{d}}{\\mathrm{d}\\boldsymbol{v}_{\\beta_{2}}(j_{2},\\boldsymbol{n}_{j_{2}})}h(\\pmb{v})\\right|\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $h=h^{n,i,\\alpha}$ was introduced in (100). ", "page_idx": 25}, {"type": "text", "text": "Assuming the bounds (15) and (16) we find ", "page_idx": 25}, {"type": "equation", "text": "$$\n|R_{(j_{1},\\beta_{1}),(j_{2},\\beta_{2})}^{n,i,\\alpha}|\\leq\\operatorname*{max}_{v\\in\\Omega}\\frac{1}{2}\\left|\\frac{\\mathrm{d}}{\\mathrm{d}v_{\\beta_{1}}(j_{1},n_{j_{1}})}\\frac{\\mathrm{d}}{\\mathrm{d}v_{\\beta_{2}}(j_{2},n_{j_{2}})}h(v)\\right|\\leq4\\frac{\\sqrt{M}}{4}\\cdot\\left(\\frac{\\sqrt{M}}{4}+\\frac{\\sqrt{M}}{4}\\right)\\leq M.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Indeed, the derivatives of $h$ can be decomposed by the product rule in 4 terms which each can bounded using (15) and (16). Then we can bound the remainder term as follows ", "page_idx": 25}, {"type": "equation", "text": "$$\n|R_{\\alpha}(i,n)|\\leq M\\sum_{j_{1},j_{2}\\in[I]}\\sum_{\\beta_{1}\\beta_{2}\\in[D]}|\\delta_{\\beta_{1}}(j_{1},n_{j_{1}})\\delta_{\\beta_{2}}(j_{2},n_{j_{2}})|\\leq M I D\\sum_{j\\in[I]}|\\delta(j,n_{j})|^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus we obtain using (252) ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|E_{\\alpha}^{T}(i,n)\\right|=\\frac{N}{S}\\left|\\sum_{n\\in\\mathbb{Z}^{n,i}}R_{\\alpha}(i,n)\\right|\\leq B_{n}^{i}M I^{2}D\\delta_{\\operatorname*{max}}^{2}\\leq M I^{2}D\\delta_{\\operatorname*{max}}^{2}\\left(1+4\\sqrt{\\frac{\\ln(S)N}{S}}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "If we consider the gradient descent dynamics we obtain the following expansion for the time derivative $\\dot{w}(i,n)$ . Let us from now on absorb the dependence on $I$ and $D$ in generic constants $C$ . ", "page_idx": 25}, {"type": "text", "text": "Corollary 1. Under the same assumptions as in Theorem 7 we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\dot{w}(i,k)=-\\sum_{k\\in[K]^{I},k_{i}=k}N p({\\pmb k},i)D_{i}L({\\pmb k})-\\lambda w(i,k)+{\\cal E}^{w}(i,k)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $E^{w}$ can be bounded by ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|E_{\\alpha}^{w}(i,k)\\right|\\leq C\\operatorname*{min}(1,r_{\\operatorname*{max}})K^{(I-1)/2}\\sqrt{\\ln(S)\\frac{N}{S}}+C K^{(I-1)/2}\\ln(S)\\sqrt{\\frac{N^{2}}{S}}\\delta_{\\operatorname*{max}}+C\\delta_{\\operatorname*{max}}^{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. The result follows from Theorem 7 and the definition of $w(i,n)$ . We also use the relation $\\begin{array}{r}{\\sum_{n\\in|\\Pi_{i}^{-1}(k)|}\\delta(i,n)=0}\\end{array}$ (already stated in (139)) to conclude that terms involving $\\delta(i,n)$ cancel. The condition $S\\ge N\\ln(N)$ allows us to bound $\\ln(S)N/S\\le2$ to control $E^{T}$ . Note that here we use that $E^{S,1}$ and $E^{S,2}$ can be bounded by $E^{I}$ . \u53e3 ", "page_idx": 25}, {"type": "text", "text": "We therefore get the following bound for $\\dot{\\delta}(i,n)$ . ", "page_idx": 25}, {"type": "text", "text": "Corollary 2. Under the same assumptions as in Theorem 7 we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{\\delta}(i,n)=-\\displaystyle\\sum_{k\\in[K]^{l},k_{i}=n}N p(k,i)\\left(\\left(D_{i,i}\\hat{g}(k)\\right)(\\hat{g}(k)-g(k))+D_{i}\\hat{g}(k)\\otimes D_{i}\\hat{g}(g)\\right)\\delta(i,n)}\\\\ &{\\qquad\\qquad-\\lambda\\dot{\\delta}(i,n)+E^{\\delta}(i,n).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $E^{\\delta}$ can be bounded by ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\vert E^{\\delta}(i,n)\\vert\\leq C\\operatorname*{min}(1,r_{\\mathrm{max}})K^{(I-1)/2}\\sqrt{\\ln(S)\\frac{N}{S}}+C K^{(I-1)/2}\\ln(S)\\sqrt{\\frac{N^{2}}{S}}\\delta_{\\mathrm{max}}+C\\delta_{\\mathrm{max}}^{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. This follows from Theorem 7, Corollary 2, and the relation ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\dot{\\delta}(i,n)=\\dot{\\upsilon}(i,n)-\\dot{w}(i,\\Pi_{i}(n)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now we are in the position to control the time evolution of $\\vert\\delta(i,n)\\vert^{2}$ . We obtain the following relation Lemma 1. Under the same assumptions as in Theorem 7 the following bound holds ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{1}{2}|\\delta(i,n)|^{2}\\leq\\left(-\\lambda-\\frac{\\omega}{(2K)^{I}}+2^{I}\\operatorname*{sup}_{v_{1},\\ldots,v_{I}\\in\\Omega}\\operatorname*{max}_{i}\\|D_{i,i}^{2}\\hat{f}(v_{1},\\ldots,v_{n})\\|r_{\\mathrm{max}}\\right)|\\delta(i,n)|^{2}}\\\\ {\\displaystyle+\\,C_{1}\\operatorname*{min}(1,r_{\\mathrm{max}})K^{(I-1)/2}\\sqrt{\\ln(S)\\frac{N}{S}}\\delta_{\\mathrm{max}}}\\\\ {\\displaystyle+\\,C_{1}K^{(I-1)/2}\\ln(S)\\sqrt{\\frac{N^{2}}{S}}\\delta_{\\mathrm{max}}^{2}+C_{1}\\delta_{\\mathrm{max}}^{3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $C_{1}$ is a constant depending on $D,\\,I,$ , and $M$ , and $\\omega$ is defined by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\omega=\\operatorname*{min}_{k,i}\\lambda_{\\operatorname*{min}}\\left(\\sum_{k\\in[K]^{I},k_{i}=k}D_{i}\\hat{g}(\\pmb{k})\\otimes D_{i}\\hat{g}(\\pmb{k})\\right)\\geq0.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. We have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{1}{2}|\\delta(i,n)|^{2}=\\delta(i,n)\\dot{\\delta}(i,n)}\\\\ &{\\qquad\\qquad\\qquad=-\\displaystyle\\sum_{k\\in[K]^{I},k_{i}=n}N p(k,i)(\\hat{g}(k)-g(k))\\delta(i,n)\\cdot(D_{i,i}\\hat{g}(k))\\,\\delta(i,n)}\\\\ &{\\qquad\\qquad\\qquad-\\displaystyle\\sum_{k\\in[K]^{I},k_{i}=n}N p(k,i)\\delta(i,n)\\cdot D_{i}\\hat{g}(k)\\otimes D_{i}\\hat{g}(k)\\delta(i,n)}\\\\ &{\\qquad\\qquad\\qquad-\\delta(i,n)E^{\\delta}(i,n)-\\lambda|\\delta(i,n)|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Recall that by (122) we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left(\\frac{1}{2K}\\right)^{I}\\leq N p(\\pmb{k},i)\\leq\\left(\\frac{2}{K}\\right)^{I}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We consider $\\omega$ as defined in the statement of the lemma where $\\omega\\ge0$ follows because we sum positive semi-definite rank one matrices. Then we find using the lower bound on the spectrum ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta(i,n)\\cdot\\left(\\sum_{k\\in[K]^{I},k_{i}=n}D_{i}\\hat{g}(k)\\otimes D_{i}\\hat{g}(k)\\right)\\delta(i,n)}\\\\ &{\\qquad\\qquad\\geq|\\delta(i,n)|^{2}\\lambda_{\\operatorname*{min}}\\left(\\displaystyle\\sum_{k\\in[K]^{I},k_{i}=n}D_{i}\\hat{g}(k)\\otimes D_{i}\\hat{g}(k)\\right)\\geq\\omega|\\delta(i,n)|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus we find ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{1}{2}|\\delta(i,n)|^{2}\\leq-\\lambda|\\delta(i,n)|^{2}-\\frac{\\omega}{(2K)^{I}}|\\delta(i,n)|^{2}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad+2^{I}\\operatorname*{sup}_{v_{1},\\ldots,v_{I}\\in\\Omega}\\operatorname*{max}_{i}\\bigl\\|D_{i,i}^{2}\\hat{f}(v_{1},\\ldots,v_{n})\\bigr\\|r_{\\mathrm{max}}\\bigl|\\delta(i,n)\\bigr|^{2}-\\delta(i,n)E^{\\delta}(i,n)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we used for the second to last contribution that we sum over $K^{I-1}$ terms which is cancelled by the $(2/K)^{I}$ factor. We finally bound the error term by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\delta(i,n)\\cdot E^{\\delta}(i,n)|}\\\\ &{\\leq C\\sqrt{D}\\operatorname*{min}(1,r_{\\operatorname*{max}})K^{(I-1)/2}\\sqrt{\\ln(S)\\frac{N}{S}}\\delta_{\\operatorname*{max}}+C K^{(I-1)/2}\\sqrt{\\ln(S)\\frac{N^{2}}{S}}\\delta_{\\operatorname*{max}}^{2}+C\\delta_{\\operatorname*{max}}^{3}}\\\\ &{\\leq C\\operatorname*{min}(1,r_{\\operatorname*{max}})K^{(I-1)/2}\\sqrt{\\ln(S)\\frac{N}{S}}\\delta_{\\operatorname*{max}}+C K^{(I-1)/2}\\sqrt{\\ln(S)\\frac{N^{2}}{S}}\\delta_{\\operatorname*{max}}^{2}+C\\delta_{\\operatorname*{max}}^{3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This ends the proof. ", "page_idx": 26}, {"type": "text", "text": "F Proofs of the Main Results ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this Section we prove our main results, Theorem 4 and Theorem 5. In addition we state and prove Theorem 8 which is a variant of Theorem 5 without a-priori bound on the embeddings. Essentially the proof strategy for both results is to rely on the groundwork from the previous two sections, in particular on Lemma 1. Indeed, we first verify that the conditions of Lemma 1 hold for all times and then application of this Lemma allows us control the evolution of $\\delta_{\\mathrm{max}}$ for all times. ", "page_idx": 27}, {"type": "text", "text": "Proof of Theorem 4. The proof proceeds in three steps. First we use the monotonicity of the loss to deduce that $r_{\\operatorname*{max}}(t)$ can be bounded in terms of $r_{\\operatorname*{max}}(0)$ and $\\delta_{\\mathrm{max}}(t)$ and $\\delta_{\\mathrm{max}}(0)$ . Then we show that choosing the variables as in the statement of the theorem allows us to bound all the terms in Lemma 1 in Appendix E. Then we apply Lemma 1 and deduce the decay of the maximum of the displacements $\\bar{|\\delta(i,n,t)|}$ . Here we need to check carefully that the assumptions of the lemma are satisfied for all $t$ . ", "page_idx": 27}, {"type": "text", "text": "First we assume the conclusions on the concentration properties from Theorem 9 in Appendix G hold which occurs with probability at least $1-S^{-1}$ over the randomness of the training data. ", "page_idx": 27}, {"type": "text", "text": "Let us consider any time $T\\,>\\,0$ and we assume that $v(i,n,t)\\in\\Omega$ for all $i\\,\\in\\,[I]$ , $n\\,\\in\\,[N]$ and $0\\le\\tau\\le T$ . This allows us in particular to apply the Assumption 2 up to time $T$ for embeddings $v(i,n)$ . ", "page_idx": 27}, {"type": "text", "text": "We now implement the first step where we bound $r_{\\operatorname*{max}}(t)$ for all $0\\leq t\\leq T$ in terms of $r_{\\mathrm{max}}(0)$ , $\\delta_{\\mathrm{max}}(t)$ , and $\\delta_{\\mathrm{max}}(t)$ . The idea is to upper bound the initial sample loss $\\hat{\\mathcal{L}}(t=0)$ and lower bound the loss $\\hat{\\mathcal{L}}(t>0)$ . First we note that by a first order Taylor expansion and using Assumption 2 ", "page_idx": 27}, {"type": "equation", "text": "$$\n|\\hat{f}(v(n))-\\hat{f}(\\pmb{w}(\\Pi(n)))|\\leq\\sum_{i\\in I}M|\\delta(i,\\pmb{n}_{i})|\\leq M I\\delta_{\\operatorname*{max}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This implies that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\hat{\\mathcal{L}}(\\bar{v})=\\frac{1}{2}\\sum_{n\\in\\mathcal{D}}(\\hat{f}(n)-g(\\Pi(n)))^{2}}\\\\ {\\displaystyle\\leq\\frac{1}{2}\\sum_{n\\in\\mathcal{D}}\\Bigg(|\\hat{f}(\\Pi(n))-g(\\Pi(n))|+\\displaystyle\\sum_{i\\in I}M|\\delta(i,n_{i})|\\Bigg)^{2}\\leq\\frac{1}{2}S(r_{\\operatorname*{max}}+I M\\delta_{\\operatorname*{max}})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Next, we derive a lower bound on the loss. Let $^k$ be such that $|\\hat{f}(\\pmb{k})-f(\\pmb{k})|=r_{\\operatorname*{max}}$ . By assumption ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left|\\Pi^{-1}(\\pmb{k})\\right|\\geq\\left(\\frac{N}{2K}\\right)^{I}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and using concentration bounds (e.g., by summing (253) over $n$ and using the lower bound for $S$ ) we find that ", "page_idx": 27}, {"type": "equation", "text": "$$\n|\\mathcal{D}^{k}|=|\\{{\\pmb n}\\in\\mathcal{D}:\\,\\Pi({\\pmb n})={\\pmb k}\\}|\\geq\\frac{1}{2}\\frac{S}{(2K)^{I}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus we can lower bound the loss using the same Taylor expansion as before by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{L}}(\\bar{v}(t))\\geq\\frac{1}{4}\\frac{S}{(2K)^{I}}\\left(\\operatorname*{max}(0,r_{\\operatorname*{max}}(t)-M I\\delta_{\\operatorname*{max}}(t))\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since gradient descent does not increase the loss the bounds (161) and (164) together imply for $t\\geq0$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac12S(r_{\\mathrm{max}}(0)+I M\\delta_{\\mathrm{max}}(0))^{2}\\geq\\hat{\\mathcal{L}}(\\bar{v}(0)))\\geq\\hat{\\mathcal{L}}(\\bar{v}(t))}\\\\ {\\displaystyle\\geq\\frac14\\frac{S}{(2K)^{I}}\\left(\\operatorname*{max}(0,r_{\\mathrm{max}}(t)-M I\\delta_{\\mathrm{max}}(t))\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This implies ", "page_idx": 27}, {"type": "equation", "text": "$$\nr_{\\mathrm{max}}(t)\\leq2(2K)^{I/2}(r_{\\mathrm{max}}(0)+I M\\delta_{\\mathrm{max}}(0))+I M\\delta_{\\mathrm{max}}(t).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then we get the bound ", "page_idx": 28}, {"type": "equation", "text": "$$\nr_{\\operatorname*{max}}(t)\\leq4(2K)^{I/2}(r_{\\operatorname*{max}}(0)+I M\\operatorname*{max}(\\delta_{\\operatorname*{max}}(0),\\delta_{\\operatorname*{max}}(t))).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Now we choose $C_{2}$ and $C_{3}$ such that the initialization condition (20) becomes ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\delta_{\\operatorname*{max}}(0)\\leq\\operatorname*{min}\\bigg(\\frac{\\omega_{0}}{8C_{1}(2K)^{I}},\\frac{\\omega_{0}}{64I M^{2}D(2K)^{3I/2}2^{I}},\\frac{1}{8(2K)^{I/2}I M},1\\bigg)=:\\bar{\\delta}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and ", "page_idx": 28}, {"type": "equation", "text": "$$\nr_{\\mathrm{max}}(0)\\le\\operatorname*{min}\\left(\\frac{\\omega_{0}}{64M D(2K)^{3I/2}2^{I}},\\frac{1}{8(2K)^{I/2}}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Assume in addition that for all times $0\\leq t\\leq T$ the bound $\\delta_{\\mathrm{max}}(t)\\leq\\bar{\\delta}$ holds. Using (167) we then find that for $0\\leq t\\leq T$ ", "page_idx": 28}, {"type": "equation", "text": "$$\nr_{\\mathrm{max}}(t)\\le4(2K)^{I/2}\\left(\\frac{1}{64M D(2K)^{3I/2}2^{I}}+I M\\frac{\\omega_{0}}{64I M^{2}D(2K)^{3I/2}2^{I}}\\right)\\le\\frac{\\omega_{0}}{8M D(2K)^{I}2^{I}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and similarly ", "page_idx": 28}, {"type": "equation", "text": "$$\nr_{\\operatorname*{max}}(t)\\leq4(2K)^{I/2}\\left(\\frac{1}{8(2K)^{I/2}}+I M\\frac{1}{8(2K)^{I/2}I M}\\right)\\leq1\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and therefore ", "page_idx": 28}, {"type": "equation", "text": "$$\nr_{\\mathrm{max}}(t)\\leq\\mathrm{min}\\left(\\frac{\\omega_{0}}{8M D(2K)^{I}2^{I}},1\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The next step is to bound the various error terms appearing in Lemma 1. Using the last display we can bound for all $i\\in[I]$ and $n\\in[N]$ using Assumption 2 for $t\\in[0,T]$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{2}^{I}\\underset{v_{1},\\ldots,v_{I}\\in\\Omega}{\\operatorname*{sup}}\\underset{i}{\\operatorname*{max}}\\|D_{i,i}^{2}\\hat{f}(v_{1},\\ldots,v_{n})\\|r_{\\operatorname*{max}}(t)|\\delta(i,n,t)|^{2}\\le2^{I}M D\\frac{\\omega_{0}}{8M D(2K)^{I}2^{I}}|\\delta(i,n,t)|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le\\frac{\\omega_{0}}{8(2K)^{I}}|\\delta(i,n,t)|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Moreover, we can estimate for $0\\leq t\\leq T$ ", "page_idx": 28}, {"type": "equation", "text": "$$\nC_{1}\\delta_{\\mathrm{max}}^{3}(t)\\leq C_{1}\\delta_{\\mathrm{max}}^{2}(t)\\frac{\\omega_{0}}{8C_{1}(2K)^{I}}\\leq\\frac{\\omega_{0}}{8(2K)^{I}}\\delta_{\\mathrm{max}}^{2}(t).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Now we observe that for $x=a\\ln^{2}(a)$ and $a\\geq1$ we find (using $\\ln(a)\\leq a)$ the bound ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\frac{\\ln(x)}{\\sqrt{x}}}\\leq{\\frac{\\ln(a^{3})}{\\sqrt{a}\\ln(a)}}={\\frac{3}{\\sqrt{a}}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence we consider ", "page_idx": 28}, {"type": "equation", "text": "$$\nS\\ge S_{0}=\\frac{24^{2}C_{1}^{2}(2K)^{3I}N^{2}}{\\omega_{0}^{2}}\\ln^{2}\\left(\\frac{24^{2}C_{1}^{2}(2K)^{3I}N^{2}}{\\omega_{0}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Note that tracking only the dependence on $K$ and $N$ (and using $K\\leq N$ ) this condition reads ", "page_idx": 28}, {"type": "equation", "text": "$$\nS\\geq C(2K)^{3I}N^{2}\\ln^{2}(N).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then we find using (175) combinded with the observation that $\\ln(S)/\\sqrt{S}$ is decreasing for $S\\geq e^{2}$ for $S\\geq S_{0}$ the bound ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{1}K^{(I-1)/2}\\ln(S)\\sqrt{\\frac{N^{2}}{S}}\\leq C_{1}K^{(I-1)/2}\\ln(S_{0})\\sqrt{\\frac{N^{2}}{S_{0}}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\cfrac{3C_{1}K^{I/2}N}{24C_{1}(2K)^{3I/2}N\\omega_{0}^{-1}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\cfrac{1}{8}\\frac{\\omega_{0}}{(2K)^{I}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We note that similarly for $x=a\\ln(a)$ we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\frac{\\ln(x)}{x}}\\leq{\\frac{2\\ln(a)}{a\\ln(a)}}={\\frac{2}{a}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus we consider similarly ", "page_idx": 29}, {"type": "equation", "text": "$$\nS\\geq S_{1}=\\frac{2\\cdot16^{2}N(2K)^{3I}C_{1}^{2}}{\\omega_{0}^{2}\\bar{\\delta}}\\ln\\left(\\frac{2\\cdot16^{2}N(2K)^{3I}C_{1}^{2}}{\\omega_{0}^{2}\\bar{\\delta}}\\right)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\bar{\\delta}$ was introduced in (168). Note that tracking only $N$ and $K$ this bound becomes ", "page_idx": 29}, {"type": "equation", "text": "$$\nS\\geq C N\\ln(N)K^{9I/2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then, similar to before, (using monotonicity of $\\mathrm{ln}(S)/S$ for $S\\geq e$ ) we find for $S\\geq S_{1}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{1}K^{(I-1)/2}\\operatorname*{max}(1,r_{\\operatorname*{max}})\\sqrt{\\ln(S)\\frac{N}{S}}\\leq C_{1}K^{I/2}\\operatorname*{max}(1,r_{\\operatorname*{max}})\\sqrt{\\ln(S_{1})\\frac{N}{S_{1}}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{1}{16}\\frac{\\omega_{0}}{(2K)^{I}}\\bar{\\delta}\\operatorname*{max}(1,r_{\\operatorname*{max}}(t)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "After all these preliminary estimates we can now finish the proof. ", "page_idx": 29}, {"type": "text", "text": "Let us define $T\\in\\mathbb{R}_{0}^{+}\\cup\\{\\infty\\}$ to be the maximal time such that $\\boldsymbol{v}(i,n,t)\\in\\Omega$ for all $i\\in[I],n\\in[N]$ and $0\\leq t\\leq T$ and the bound (168) holds for $\\delta_{\\mathrm{max}}(t)$ where we for convenience recall ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\delta_{\\operatorname*{max}}(t)\\leq\\bar{\\delta}=\\operatorname*{min}\\left(\\frac{\\omega_{0}}{8C_{1}(2K)^{I}},\\frac{\\omega_{0}}{64I M^{2}D(2K)^{3I/2}2^{I}},\\frac{1}{8(2K)^{I/2}I M},1\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We want to show that $T\\,=\\,\\infty$ . We assume that $T\\,<\\,\\infty$ and prove by contradiction that this does not hold. By (172) we know that for $t\\,\\leq\\,T$ the bound $r_{\\operatorname*{max}}(t)\\,\\leq\\,1$ holds and therefore Assumption 4 implies that $w(i,k,t=0)\\in\\Omega_{0}$ and since moreover $\\delta_{\\mathrm{max}}(t)\\leq1$ for $t\\leq T$ we find that $v(i,n,t=0)\\in\\Omega_{0}+B_{1}(0)\\subset\\subset\\Omega$ . By continuity of $v(i,n,t)$ in $t$ we conclude that there is $\\varepsilon>0$ such that $v(i,n,t)\\in\\Omega$ for all $0\\leq t\\leq T+\\varepsilon,\\,i\\in[I]$ , and $n\\,\\in\\,[N]$ . Thus we can apply Lemma 1 for $t=T$ and then find using Assumption 3 and the bounds (173), (174), (178), and (182) ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{1}{2}|\\delta(i,n,t)|^{2}\\leq\\left(-\\lambda-\\frac{\\omega_{0}}{(2K)^{I}}+2^{I}\\operatorname*{sup}_{v_{I}\\in\\Omega}\\operatorname*{max}_{i}\\lVert D_{i,i}^{2}\\hat{f}(v_{1},\\ldots,v_{n})\\rVert r_{\\operatorname*{max}}\\right)|\\delta(i,n,t)|^{2}}\\\\ &{\\;\\;+\\;C_{1}\\operatorname*{max}(1,r_{\\operatorname*{max}})K^{(I-1)/2}\\sqrt{\\ln(S)\\frac{N}{S}}\\delta_{\\operatorname*{max}}(t)+C_{1}K^{(I-1)/2}\\ln(S)\\sqrt{\\frac{N^{2}}{S}}\\delta_{\\operatorname*{max}}^{2}+C_{1}\\delta_{\\operatorname*{max}}(t)^{3}}\\\\ &{\\;\\leq\\frac{\\omega_{0}}{(2K)^{I}}\\left(-|\\delta(i,n,t)|^{2}+\\frac{1}{8}|\\delta(i,n,t)|^{2}+\\frac{1}{16}\\operatorname*{max}(1,r_{\\operatorname*{max}}(t))\\bar{\\delta}+\\frac{1}{8}\\delta_{\\operatorname*{max}}(t)^{2}+\\frac{1}{8}\\delta_{\\operatorname*{max}}(t)^{2}\\right)}\\\\ &{\\;\\leq\\frac{\\omega_{0}}{(2K)^{I}}\\left(-|\\delta(i,n,t)|^{2}+\\frac{3}{8}\\delta_{\\operatorname*{max}}(t)^{2}+\\frac{1}{16}\\operatorname*{max}(1,r_{\\operatorname*{max}}(t))\\bar{\\delta}.\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "If $\\lvert\\delta(i,n,t)\\rvert\\ge3/4\\delta_{\\mathrm{max}}$ we find $|\\delta(i,n,t)|^{2}\\geq\\delta_{\\mathrm{max}}(t)^{2}/2$ and we conclude that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{1}{2}|\\delta(i,n,t)|^{2}\\leq\\frac{\\omega_{0}}{(2K)^{I}}\\delta_{\\operatorname*{max}}(t)(-\\frac{1}{2}\\delta_{\\operatorname*{max}}(t)+\\frac{3}{8}\\delta_{\\operatorname*{max}}(t)+\\frac{1}{16}\\operatorname*{max}(1,r_{\\operatorname*{max}}(t))\\bar{\\delta}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\leq\\frac{1}{8}\\frac{\\omega_{0}}{(2K)^{I}}\\delta_{\\operatorname*{max}}(t)(-\\delta_{\\operatorname*{max}}(t)+\\bar{\\delta}/2).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now, if $\\delta_{\\mathrm{max}}(T)\\,\\leq\\,\\bar{\\delta}/2$ we conclude by continuity that there is $\\varepsilon>0$ such that $\\delta_{\\mathrm{max}}(t)<\\bar{\\delta}$ for $t\\in[T,T+\\varepsilon]$ . On the other hand if $\\delta_{\\mathrm{max}}(t)>\\bar{\\delta}$ we conclude that $\\begin{array}{r}{\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{1}{2}|\\delta(i,n,T)|^{2}<0}\\end{array}$ for all $i,n$ such that $|\\delta(i,n,t)|\\ge3/4\\delta_{\\mathrm{max}}$ . This in particular implies that $\\delta_{\\mathrm{max}}(\\tilde{t})\\,\\bar{<}\\,\\delta_{\\mathrm{max}}(T)$ for $t\\in[T,T\\!+\\!\\varepsilon]$ and some $\\varepsilon>0$ ( $\\delta_{\\mathrm{max}}$ is non-increasing at $T$ ). This is a contradiction and we conclude that $T=\\infty$ . ", "page_idx": 29}, {"type": "text", "text": "Finally we prove the decay of $\\delta_{\\mathrm{max}}$ . Assume that for $t\\geq T$ the bound $\\operatorname*{min}(1,r_{\\operatorname*{max}}(t))\\leq R$ holds. Note that the function $\\delta_{\\mathrm{max}}(t)$ is not necessarily differentiable but its left and right derivative exist (as the maximum of finitely many differentiable functions). Then we obtain, similar to (184), for the right derivative the bound ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t^{+}}\\frac{1}{2}\\delta_{\\operatorname*{max}}(t)^{2}\\leq-\\frac{1}{8}\\frac{\\omega_{0}}{(2K)^{I}}\\delta_{\\operatorname*{max}}(t)^{2}+C_{1}R K^{(I-1)/2}\\sqrt{\\ln(S)\\frac{N}{S}}\\delta_{\\operatorname*{max}}(t).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\delta_{\\mathrm{max}}(t)>\\frac{C_{1}16(2K)^{I}K^{(I-1)/2}R}{\\omega_{0}}\\sqrt{\\ln(S)\\frac{N}{S}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "the previous bound simplifies to ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t^{+}}\\frac{1}{2}\\delta_{\\operatorname*{max}}(t)^{2}\\leq-\\frac{1}{16}\\frac{\\omega_{0}}{(2K)^{I}}\\delta_{\\operatorname*{max}}(t)^{2}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which implies that for $t\\geq T$ by Gronwall\u2019s Lemma ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\delta_{\\mathrm{max}}(t)^{2}\\leq\\exp\\left(-\\frac{1}{16}\\frac{\\omega_{0}}{(2K)^{I}}(t-T)\\right)\\delta_{\\mathrm{max}}(T)^{2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus in finite time we achieve ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\delta_{\\operatorname*{max}}(t)\\leq2\\frac{C_{1}16(2K)^{I}K^{(I-1)/2}R}{\\omega_{0}}\\sqrt{\\ln(S)\\frac{N}{S}}\\leq C K^{3I/2}\\sqrt{\\ln(S)\\frac{N}{S}}R.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This ends the proof. Note that we also get an exponential rate of convergence. ", "page_idx": 30}, {"type": "text", "text": "Remark 3. The exponent $9/2$ for the lower bound of $S$ in (181) is not tight because we could use that $r_{\\mathrm{max}}$ is small but this provides only a small improvement that does not justify the additional technicalities. ", "page_idx": 30}, {"type": "text", "text": "Before proving and stating Theorem 8 let us first prove that the weight decay $\\lambda>0$ allows us to derive a-priori bounds on the time evolution as stated in the following lemma. ", "page_idx": 30}, {"type": "text", "text": "Lemma 2. Let \u03a0 be approximately balanced, i.e., assume that Assumption 1 holds. Let $M_{1}>0$ be such that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\substack{v(1),\\ldots,v(I)\\in B_{1}(0)\\subset\\mathbb{R}^{D}}}|\\hat{f}(v(1),\\ldots,v(I))|\\leq\\frac{\\sqrt{M_{1}}}{2},\\quad\\operatorname*{max}_{k}|g(k)|\\leq\\frac{\\sqrt{M_{1}}}{2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Let $0<\\lambda\\leq1$ be a fixed number and assume that $v(i,n,t)$ follow the gradient dynamics (8) and are initialized such that ", "page_idx": 30}, {"type": "equation", "text": "$$\n|v(i,n,t=0)|\\leq1\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and assume that the dynamics exists for all times. Then for all $i\\in[I]$ , $k\\in[K]$ , and all times ", "page_idx": 30}, {"type": "equation", "text": "$$\n|w(i,k,t)|\\leq R^{\\prime}=\\sqrt{\\frac{4K M_{1}}{\\lambda}+2I K}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. Note that by assumption ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{N}{S}\\hat{\\mathcal{L}}(\\bar{v}(t=0))=\\frac{N}{S}\\sum_{n\\in D}\\frac{1}{2}(\\hat{f}(n)-f(n))^{2}\\leq\\frac{N}{S}S\\left(\\frac{2\\sqrt{M_{1}}}{2}\\right)^{2}=N M_{1}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then we conclude that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N\\left(M_{1}+I\\frac{\\lambda}{2}\\right)\\geq\\hat{\\mathcal{R}}^{\\lambda}(\\bar{\\pmb{v}}(0))\\geq\\hat{\\mathcal{R}}^{\\lambda}(\\bar{\\pmb{v}}(t))\\geq\\frac{\\lambda}{2}\\operatorname*{max}_{i\\in[I]}\\operatorname*{max}_{h\\in[K]}\\sum_{n\\in\\Pi_{i}^{-1}(k)}\\vert v(i,n,t)\\vert^{2}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\frac{\\lambda}{2}\\operatorname*{min}_{i\\in[I],k\\in[K]}\\vert\\Pi_{i}^{-1}(k)\\vert\\operatorname*{max}_{i\\in[I],k\\in[K]}\\vert w(i,k,t)\\vert^{2}\\geq\\frac{N\\lambda}{4K}\\operatorname*{max}_{i\\in[I],k\\in[K]}\\vert w(i,k,t)\\vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus we conclude that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[I],k\\in[K]}|w(i,k,t)|^{2}\\leq\\frac{4K M_{1}}{\\lambda}+2I K.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This ends the proof. ", "page_idx": 30}, {"type": "text", "text": "Now we state a refined version of Theorem 5 which does not require an a-priori bound but instead proves boundedness of the embeddings $v(i,n,t)$ for all times using Lemma 2. ", "page_idx": 31}, {"type": "text", "text": "Theorem 8. Let $\\Pi$ be approximately balanced, i.e., assume that Assumption $^{\\,l}$ holds. Assume that the function $\\hat{f}:\\mathbb{R}^{I D}\\rightarrow\\mathbb{R}$ is slot-wise linear. Assume that there is $M_{1}>0$ such that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\substack{v(1),\\ldots,v(I)\\in B_{1}(0)\\subset\\mathbb{R}^{D}}}|\\hat{f}(v(1),\\ldots,v(I))|\\leq\\frac{\\sqrt{M_{1}}}{2},\\quad\\operatorname*{max}_{k}|g(k)|\\leq\\frac{\\sqrt{M_{1}}}{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Let $0<\\lambda\\leq1$ be a fixed number and assume that for ", "page_idx": 31}, {"type": "equation", "text": "$$\nR=\\sqrt{\\frac{4K M_{1}}{\\lambda}+2I K}+3\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Assumption 2 holds with $\\Omega=B_{R}(0)$ and some $M>0$ . Note that $M$ depends on $R$ and thus on $K$ and $\\lambda$ . Let $C_{1}$ be the constant from Lemma $^{\\,\\,\\,I}$ . Assume that $v(i,n,t)$ follow the gradient dynamics (8) and are initialized such that ", "page_idx": 31}, {"type": "equation", "text": "$$\n|v(i,n,t=0)|\\leq\\frac{\\lambda}{8C_{1}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then there are constants $C_{2},C_{3}\\ge0$ depending on $M,\\,I,\\,D,\\,\\lambda,$ , and $C_{1}$ such that for ", "page_idx": 31}, {"type": "equation", "text": "$$\nS\\ge\\operatorname*{max}\\left(C_{2}\\frac{N^{2}K^{I-1}}{\\lambda^{2}}\\ln^{2}(N/\\lambda),C_{3}\\frac{N K^{I-1}}{\\lambda^{4}}\\ln(N/\\lambda)\\right)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "the bound ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\delta_{\\operatorname*{max}}(t)\\leq\\operatorname*{max}\\left(\\delta_{\\operatorname*{max}}(0)e^{-\\lambda t/8},\\frac{4C_{1}K^{(I-1)/2}}{\\lambda}\\sqrt{\\ln(S)\\frac{N}{S}}\\right)}\\\\ &{}&{\\leq\\operatorname*{max}\\left(\\delta_{\\operatorname*{max}}(0)e^{-\\lambda t/8},C_{I,D,\\lambda,K}\\sqrt{\\ln(S)\\frac{N}{S}}\\right)\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "holds for all $t\\geq0$ . ", "page_idx": 31}, {"type": "text", "text": "Remark 4. We emphasize again that since $M$ might depend on $\\lambda$ we obtain no explicit rate in $\\lambda$ .   \nHowever, the $S$ and $N$ dependence is the same as in Theorem 5. ", "page_idx": 31}, {"type": "text", "text": "Proof. The general strategy of the proof is similar to the proof of Theorem 4 but the proof is slightly simpler as it is easier to obtain a-priori estimates on the evolution of $v(i,n,t)$ . We assume that the conclusion of Theorem 9 holds which occurs with probability at least $1-S^{-1}$ over the randomness of $\\mathcal{D}$ . ", "page_idx": 31}, {"type": "text", "text": "Now let $T\\geq0$ be the largest time such that for all $0\\leq t\\leq T$ and all $i\\in[I]$ and $n\\in[N]$ we have $v(i,n,t)\\in\\Omega$ and the bound ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\delta_{\\mathrm{max}}(t)\\leq\\frac{\\lambda}{4C_{1}}\\leq1\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "(we assume w.l.o.g. that $C_{1}\\geq1$ ) holds for $0\\leq t\\leq T$ . By assumption those relations hold at $t=0$ (note that $\\delta_{\\mathrm{max}}(t)\\leq2\\,\\mathrm{max}_{i,n}\\,|v(i,n)|)$ . We can bound for $t=T$ using the a-priori estimate (193) from Lemma 2 and find ", "page_idx": 31}, {"type": "equation", "text": "$$\n|v(i,n,T)|\\leq|w(i,\\Pi_{i}(k),T)|+\\delta_{\\operatorname*{max}}(T)\\leq\\sqrt{\\frac{4K M_{1}}{\\lambda}+2I K}+2\\leq R-1.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Thus, by continuity, there is $\\varepsilon>0$ such that $v(i,n,t)\\in B_{R}(0)$ for some $\\varepsilon>0$ and all $t\\in[0,T+\\varepsilon]$ . Now we can apply Lemma 1 for the interval $[T,T+\\varepsilon]$ . We note that by slot-wise linearity we have $D_{i,i}{\\hat{f}}=0$ and $\\omega\\ge0$ . Therefore we get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{1}{2}|\\delta(i,n)|^{2}\\leq-\\lambda|\\delta(i,n)|^{2}+C_{1}K^{(I-1)/2}\\sqrt{\\ln(S)\\frac{N}{S}}\\delta_{\\operatorname*{max}}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad+\\,C_{1}K^{(I-1)/2}\\ln(S)\\sqrt{\\frac{N^{2}}{S}}\\delta_{\\operatorname*{max}}^{2}+C_{1}\\delta_{\\operatorname*{max}}^{3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now (with the same reasoning as in the proof of Theorem 4) there is a constant $C_{2}$ depending on $D$ , $I$ , and $M$ such that for ", "page_idx": 32}, {"type": "equation", "text": "$$\nS\\geq S_{0}=\\frac{(12)^{2}C_{1}^{2}N^{2}K^{I-1}}{\\lambda^{2}}\\ln^{2}\\left(\\frac{(12)^{2}C_{1}^{2}N^{2}K^{I-1}}{\\lambda^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "the bound ", "page_idx": 32}, {"type": "equation", "text": "$$\nC_{1}K^{(I-1)/2}\\ln(S)\\sqrt{\\frac{N^{2}}{S}}\\le\\frac{\\lambda}{4}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "holds. Clearly we get for a suitable constant $C_{2}$ ", "page_idx": 32}, {"type": "equation", "text": "$$\nS_{0}\\le C_{2}\\frac{N^{2}K^{I-1}}{\\lambda^{2}}\\ln^{2}(N/\\lambda).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Similarly, we get for ", "page_idx": 32}, {"type": "equation", "text": "$$\nS\\geq S_{1}=\\frac{2\\cdot16^{2}C_{1}^{4}N K^{I-1}}{\\lambda^{4}}\\ln\\left(\\frac{2\\cdot16^{2}C_{1}^{4}N K^{I-1}}{\\lambda^{4}}\\right)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "the bound ", "page_idx": 32}, {"type": "equation", "text": "$$\nC_{1}K^{(I-1)/2}\\sqrt{\\ln(S)\\frac{N}{S}}\\le\\frac{\\lambda^{2}}{16C_{1}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "holds. And we find ", "page_idx": 32}, {"type": "equation", "text": "$$\nS_{1}\\le C_{3}\\frac{N K^{I-1}}{\\lambda^{4}}\\ln(N/\\lambda).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now we can continue to bound (204) for $S$ satisfying (205) and (208) and using (202) as follows ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{1}{2}\\vert\\delta(i,n,T)\\vert^{2}\\leq-\\lambda\\vert\\delta(i,n)\\vert^{2}+\\frac{\\lambda^{2}}{16C_{1}}\\delta_{\\mathrm{max}}+\\frac{\\lambda}{4}\\delta_{\\mathrm{max}}^{2}+\\frac{\\lambda}{4}\\delta_{\\mathrm{max}}^{2}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now, if $\\delta_{\\mathrm{max}}(T)\\,\\leq\\,\\lambda/(8C_{1})$ then there is $\\varepsilon\\ >\\ 0$ such that $\\delta_{\\mathrm{max}}(t)\\,\\leq\\,\\lambda/(4C_{1})$ holds for $t\\ \\in$ $[0,T+\\varepsilon]$ . Thus we assume that $\\delta_{\\operatorname*{max}}(T)\\geq\\lambda/(8C_{1}).$ . Let $i\\in[I]$ , $n\\in[N]$ be any index such that $\\bar{|\\delta(i,n,T)|}=\\delta_{\\operatorname*{max}}(T)$ . Then we conclude that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{1}{2}|\\delta(i,n,T)|^{2}\\leq-\\frac{\\lambda}{2}\\delta_{\\operatorname*{max}}^{2}+\\frac{\\lambda^{2}}{16C_{1}}\\delta_{\\operatorname*{max}}\\leq-\\frac{\\lambda}{2}\\frac{\\lambda}{8C_{1}}\\delta_{\\operatorname*{max}}+\\frac{\\lambda^{2}}{16C_{1}}\\delta_{\\operatorname*{max}}=0.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This implies that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t^{+}}\\frac{1}{2}\\delta_{\\mathrm{max}}(T)^{2}\\leq0\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "from which we conclude that there is $\\varepsilon>0$ such that $\\delta_{\\mathrm{max}}(t)\\leq\\lambda/(4C_{1})$ holds for $0\\leq t\\leq T+\\varepsilon$ In either case we get a contradiction and thus $T=\\infty$ . In particular we can apply Lemma 1 for all times $t\\geq0$ . Suppose now that for some $t$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\delta_{\\mathrm{max}}(t)\\geq\\frac{4C_{1}K^{(I-1)/2}}{\\lambda}\\sqrt{\\ln(S)\\frac{N}{S}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "then we conclude from Lemma 1 that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathrm{d}}{\\mathrm{d}t}\\displaystyle\\frac{1}{2}\\delta_{\\operatorname*{max}}^{2}(t)\\leq-\\displaystyle\\frac{\\lambda}{2}\\delta_{\\operatorname*{max}}(t)^{2}+C_{1}K^{(I-1)/2}\\sqrt{\\ln(S)\\displaystyle\\frac{N}{S}}\\delta_{\\operatorname*{max}}(t)}\\\\ &{\\qquad\\qquad\\qquad\\leq-\\displaystyle\\frac{\\lambda}{2}\\delta_{\\operatorname*{max}}^{2}(t)+-\\displaystyle\\frac{\\lambda}{4}\\delta_{\\operatorname*{max}}^{2}(t)\\leq\\displaystyle\\frac{\\lambda}{4}\\delta_{\\operatorname*{max}}^{2}(t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Thus we conclude from Gronwall\u2019s inequality that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\delta_{\\operatorname*{max}}(t)^{2}\\leq\\operatorname*{max}\\left(\\delta_{\\operatorname*{max}}(0)^{2}e^{-\\lambda t/4},\\left(\\frac{4C_{1}K^{(I-1)/2}}{\\lambda}\\sqrt{\\ln(S)\\frac{N}{S}}\\right)^{2}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This ends the proof. ", "page_idx": 32}, {"type": "text", "text": "Proof of Theorem 5. The proof is the same as the proof of Theorem 8 above with the only exception that we do not need to show that $v(i,n,t)\\in\\Omega$ but this holds by assumption which makes the proof strictly simpler. \u53e3 ", "page_idx": 32}, {"type": "text", "text": "G Concentration of Datapoint Statistics Matrices ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this section we prove high probability concentration bounds for certain matrices capturing the statistics of the dataset. We start with the matrices $\\pmb{A}^{i j}\\in\\mathbb{R}^{N\\times N}$ with entries ", "page_idx": 33}, {"type": "equation", "text": "$$\nA_{n_{1},n_{2}}^{i j}=|\\{n\\in\\mathcal{D}:n_{i}=n_{1},n_{j}=n_{2}\\}|,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "the matrix counting the appearance of a pair of tokens in slot $i$ and $j$ . Then we have the following result. ", "page_idx": 33}, {"type": "text", "text": "Lemma 3. Let $\\pmb{A}^{i,j}\\in\\mathbb{R}^{N\\times N}$ be as defined above. Assume that $S\\geq N$ . Then the following bound holds for $\\eta>0$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{D}}\\left(\\|A^{i,j}-\\mathbb{E}(A^{i,j})\\|\\ge(1+\\eta)\\ln(S)\\sqrt{\\frac{S}{N}}\\right)\\le S^{-\\eta}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This implies ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{D}}\\left(\\|A^{i,j}\\|\\ge\\frac{S}{N}+(1+\\eta)\\ln(S)\\sqrt{\\frac{S}{N}}\\right)\\le S^{-\\eta}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. As in the proof of Theorem 2, we use Poissonization, i.e., we consider dataset $\\tilde{\\mathcal{D}}$ generated by first sample $\\bar{S}\\sim\\mathrm{Poi}(S)$ and then $\\mathcal{D}_{\\bar{S}}\\sim\\mathcal{U}([N]^{I})^{\\bar{S}}$ . A uniform sample $n\\sim\\mathcal{U}([N]^{I})$ satisfies ${\\pmb n}_{i}=n_{1}$ and $n_{j}=n_{2}$ with probability ", "page_idx": 33}, {"type": "equation", "text": "$$\np=\\frac{1}{N^{2}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Therefore the distribution of $A^{i,j}$ is ", "page_idx": 33}, {"type": "equation", "text": "$$\nA_{n_{1},n_{2}}^{i,j}=\\mathrm{Poi}\\left(\\frac{S}{N^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and the entries are independent. We can now apply Lemma 7 where we use $\\eta\\ln(S)$ as $\\eta$ in the statement of the lemma and find using $S\\geq N$ that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\tilde{\\mathcal{D}}}\\left(\\|A^{i,j}-\\mathbb{E}(A^{i,j})\\|\\ge(1+\\eta)\\ln(S)\\sqrt{\\frac{S}{N}}\\right)\\le2N S^{-\\eta}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Let us define ", "page_idx": 33}, {"type": "equation", "text": "$$\nr(S,N,\\eta)=(1+\\eta)\\ln(S)\\sqrt{\\frac{S}{N}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Note that $\\mathbb{P}({\\bar{S}}=S)=S^{S}e^{-S}/S!\\geq1/(3{\\sqrt{S}})$ . This implies ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{\\mathcal{D}}\\left(\\|A^{i,j}-\\mathbb{E}(A^{i,j})\\|\\ge r(S,N,\\eta)\\right)}\\\\ &{\\quad\\quad=\\mathbb{P}_{\\tilde{\\mathcal{D}}}\\left(\\|A^{k,i,j}-\\mathbb{E}(A^{i,j})\\|\\ge r(S,N,\\eta)|\\bar{S}=S\\right)}\\\\ &{\\quad\\le\\frac{\\mathbb{P}_{\\mathcal{\\tilde{D}}}\\left(\\|A^{i,j}-\\mathbb{E}(A^{i,j})\\|\\ge r(S,N,\\eta)\\mathrm{~and~}\\bar{S}=S\\right)}{\\mathbb{P}_{\\mathcal{\\tilde{D}}}(\\bar{S}=S)}}\\\\ &{\\quad\\le2N S^{-\\eta}\\frac{1}{3\\sqrt{S}}\\le S^{-\\eta+3/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Here we used $S\\geq N$ in the last step. The relation (219) follows because $\\|\\mathbb{E}A^{i,j}\\|=N S/N^{2}=$ $S/N$ . ", "page_idx": 33}, {"type": "text", "text": "Next we prove a similar but slightly more involved result for the matrices that capture a similar statistics as $A^{i,j}$ but which in addition only consider $\\mathbfit{\\Delta}$ such that $\\boldsymbol{\\Pi}(n)\\,=\\,k$ . We thus consider any $\\pmb{k}~\\in~[K]^{I}$ and any indices $i,j\\ \\in\\ I$ which we consider fixed for now. Consider the sets ", "page_idx": 33}, {"type": "text", "text": "$\\mathcal{N}(i,k)=\\Pi_{i}^{-1}(\\pmb{k}_{i}),\\mathcal{N}(j,\\pmb{k})=\\Pi_{j}^{-1}(\\pmb{k}_{j})\\subset[N]$ . For a given dataset $\\boldsymbol{D}=\\{\\pmb{n}^{1},\\dots,\\pmb{n}^{S}\\}$ we define the matrices $A^{k,i,j}\\in\\mathbb{R}^{\\mathcal{N}(i,k)\\times\\mathcal{N}(j,\\pmb{k})}$ by ", "page_idx": 34}, {"type": "equation", "text": "$$\nA_{n_{1},n_{2}}^{k,i,j}=\\left|\\{{\\pmb n}\\in\\mathcal{D}:\\,\\Pi({\\pmb n})=k,\\,{\\pmb n}_{i}=n_{1},\\,{\\pmb n}_{j}=n_{2}\\}\\right|\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "for $n_{1}\\in\\mathcal{N}(i,\\pmb{k})$ , $n_{2}\\in\\mathcal{N}(j,\\pmb{k})$ . Thus, the entries are the number of datapoints that are mapped to $\\pmb{k}$ by $\\Pi$ and whose entries $i$ and $j$ are equal to $n_{1}$ and $n_{2}$ . Note that this agrees with the definition in (115) given in Section D. Applying concentration bounds for random matrices derived in Appendix $\\mathrm{H}$ below we obtain the following result. ", "page_idx": 34}, {"type": "text", "text": "Lemma 4. Let $A^{k,i,j}\\in\\mathbb{R}^{N(i,k)\\times N(j,\\pmb{k})}$ be as defined above. Assume that for all $i\\in[I]$ and $k\\in[K]$ the bound $N/(2K)\\le|\\Pi_{i}^{-1}(k)|\\le2N/K$ holds and let $S\\geq N$ . Then the following bound holds for $\\eta>0$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{D}}\\left(\\|A^{k,i,j}-\\mathbb{E}(A^{k,i,j})\\|\\ge(1+\\eta)\\ln(S)\\operatorname*{max}\\left(1,\\left(\\frac{2}{K}\\right)^{(I-1)/2}\\sqrt{\\frac{S}{N}}\\right)\\right)\\le3S^{-\\eta+3/2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Note that for $K\\ge2$ we get for $S\\geq N$ the bound ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{D}}\\left(\\|A^{k,i,j}-\\mathbb{E}(A^{k,i,j})\\|\\ge(1+\\eta)\\ln(S)\\sqrt{\\frac{S}{N}}\\right)\\le3S^{-\\eta+3/2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. The proof is essentially the same as the proof of Lemma 3 with some additional notational complications. Again we consider a dataset $\\tilde{\\mathcal{D}}$ generated by first sampling $\\bar{S}\\sim\\mathrm{Poi}(S)$ and then $D_{\\bar{S}}\\sim\\mathcal{U}([N]^{I})^{\\bar{S}}$ . Note that a uniform sample $n\\sim\\mathcal{U}([N]^{I})$ satisfies $\\boldsymbol{\\Pi}(n)\\,=\\,k$ , ${\\pmb n}_{i}\\,=\\,n_{1}$ , and $n_{j}=n_{2}$ for $\\bar{n_{1}}\\in\\mathcal{N}(i,\\pmb{k})$ and $n_{2}\\in\\mathcal{N}(j,\\pmb{k})$ with probability ", "page_idx": 34}, {"type": "equation", "text": "$$\np(\\pmb{k},i,j)=\\frac{1}{N^{I}}\\frac{|\\Pi^{-1}(\\pmb{k})|}{|\\Pi_{i}^{-1}(\\pmb{k}_{i})|\\cdot|\\Pi_{j}^{-1}(\\pmb{k}_{j})|}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and thus the distribution of $A^{k,i,j}$ is ", "page_idx": 34}, {"type": "equation", "text": "$$\nA_{n_{1},n_{2}}^{k,i,j}=\\mathrm{Poi}\\left(\\frac{S}{N^{I}}\\frac{|\\Pi^{-1}(k)|}{|\\Pi_{i}^{-1}(k_{i})|\\cdot|\\Pi_{j}^{-1}(k_{j})|}\\right)=:\\mathrm{Poi}(\\lambda(k,i,j))\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "for all $n_{1}\\in\\mathcal{N}(i,\\pmb{k})$ and $n_{2}\\in\\mathcal{N}(j,\\pmb{k})$ and the entries are independent. Note that the assumption $N/(2K)\\le|\\Pi_{i}^{-1}(k)|\\le2N/K$ implies that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{|\\Pi^{-1}(\\pmb{k})|}{|\\Pi_{i}^{-1}(\\pmb{k}_{i})|\\cdot|\\Pi_{j}^{-1}(\\pmb{k}_{j})|}=\\prod_{l\\in[K]\\backslash\\{i,j\\}}|\\Pi_{l}^{-1}(\\pmb{k}_{l})|\\left\\{\\geq\\frac{2^{l-2}N^{I-2}}{K^{I-2}},\\right.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which implies ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{1}{(2K)^{I-2}}\\frac{S}{N^{2}}\\leq\\lambda(\\pmb{k},i,j)\\leq\\left(\\frac{2}{K}\\right)^{I-2}\\frac{S}{N^{2}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Now we apply Lemma 7 where we use $\\eta\\ln(S)$ as $\\eta$ in the statement of the lemma and find that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\tilde{\\mathcal{D}}}\\left(\\|A^{k,i,j}-\\mathbb{E}(A^{k,i,j})\\|\\ge(1+\\eta)\\ln(S)\\mathrm{max}\\left(1,\\left(\\frac{2}{K}\\right)^{(I-1)/2}\\sqrt{\\frac{S}{N}}\\right)\\right)\\le\\frac{8N}{K}S^{-\\eta}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Let us set ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{r(S,N,K,I,\\eta)=(1+\\eta)\\ln(S)\\mathrm{max}\\left(1,\\left(\\frac{2}{K}\\right)^{(I-1)/2}\\sqrt{\\frac{S}{N}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Note that $\\mathbb{P}({\\bar{S}}=S)=S^{S}e^{-S}/S!\\geq1/(3{\\sqrt{S}})$ . This implies ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{\\mathcal{D}}\\left(\\|A^{k,i,j}-\\mathbb{E}(A^{k,i,j})\\|\\ge r(S,N,K,I,\\eta)\\right)}\\\\ &{\\quad=\\mathbb{P}_{\\mathcal{\\tilde{D}}}\\left(\\|A^{k,i,j}-\\mathbb{E}(A^{k,i,j})\\|\\ge r(S,N,K,I,\\eta)|\\bar{S}=S\\right)}\\\\ &{\\quad\\le\\frac{\\mathbb{P}_{\\mathcal{\\tilde{D}}}\\left(\\|A^{k,i,j}-\\mathbb{E}(A^{k,i,j})\\|\\ge r(S,N,K,I,\\eta)\\mathrm{~and~}\\bar{S}=S\\right)}{\\mathbb{P}_{\\mathcal{\\tilde{D}}}\\left(\\bar{S}=S\\right)}}\\\\ &{\\quad\\le\\frac{8N}{K}S^{-\\eta}\\frac{1}{3\\sqrt{S}}\\le3S^{-\\eta+3/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We also consider the matrix $\\pmb{A}^{k,i,j}\\in\\mathbb{R}^{I N\\times I N}$ which we obtain by embedding $A^{k,i,j}$ suitable, i.e, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{(i_{1},n_{1}),(j_{1},n_{2})}^{k,i,j}=\\left\\{\\!\\!\\begin{array}{l l}{A_{n_{1},n_{2}}^{k,i,j}}&{\\!\\mathrm{if}\\ i_{1}=i,j_{1}=j\\mathrm{~and~}n_{1}\\in\\mathcal{N}(i,k),n_{2}\\mathcal{N}(j,k)}\\\\ {0}&{\\mathrm{otherwise}.}\\end{array}\\!\\!\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Since $A^{k,i,j}$ is a submatrix of $A^{k,i,j}$ we find ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|\\pmb{A}^{k,i,j}-\\mathbb{E}\\pmb{A}^{k,i,j}\\|=\\|\\pmb{A}^{k,i,j}-\\mathbb{E}\\pmb{A}^{k,i,j}\\|\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "so that Lemma 4 applies to Ak,i,j. ", "page_idx": 35}, {"type": "text", "text": "We also need simpler concentration statement for the frequency of datapoints $n^{s}$ such that ${\\pmb n}_{i}=n$ . We define the vectors $B^{i}\\in\\mathbb{R}^{N}$ by ", "page_idx": 35}, {"type": "equation", "text": "$$\nB_{n}^{i}=|\\{n\\in\\mathcal{D}:\\,n_{i}=n\\}|.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We need a simple upper bound on the vectors $B^{i}$ . ", "page_idx": 35}, {"type": "text", "text": "Lemma 5. The following bound holds for $\\eta>0$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(B_{n}^{i}\\geq\\frac{S}{N}+\\operatorname*{max}(2\\sqrt{\\eta\\ln(S)S N^{-1}},4/3\\eta\\ln(S))\\right)\\leq S^{-\\eta}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. Note that $B_{n}^{i}\\sim\\mathrm{Bin}(S,N^{-1})$ . The variance of a $\\operatorname{Ber}(p)$ variable is $p(1\\!-\\!p)\\leq p$ . Bernstein\u2019s one sided inequality then reads ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\mathrm{Bin}(S,N^{-1})-S N^{-1}\\geq t\\Big)\\leq\\exp\\left(-\\frac{t^{2}}{2S N^{-1}+2t/3}\\right)\\leq\\exp\\left(-\\operatorname*{min}\\left(\\frac{t^{2}}{4S N^{-1}},3t/4\\right)\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Now we apply this bound with $t=\\operatorname*{max}(2\\sqrt{\\eta\\ln(S)S N^{-1}},4/3\\eta\\ln(S))$ which implies ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\Big(\\mathrm{Bin}(S,N^{-1})-S N^{-1}\\geq\\operatorname*{max}(2\\sqrt{\\eta\\ln(S)S N^{-1}},4/3\\eta\\ln(S))\\Big)\\leq S^{-\\eta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "As before for the $A$ matrices we also need an extension of the previous result to a setting where we in addition require ${\\boldsymbol{\\Pi}}({\\boldsymbol{n}})={\\boldsymbol{k}}$ for a given $\\pmb{k}$ and $n\\in[N]$ . Recall that $\\mathcal{N}(i,\\pmb{k})=\\Pi_{i}^{-1}(\\pmb{k}_{i})$ . Then we consider the vector $B^{\\pmb{k},i}\\in\\mathbb{R}^{\\mathcal{N}(i,\\pmb{k})}$ given by ", "page_idx": 35}, {"type": "equation", "text": "$$\nB_{n}^{\\mathbf{k},i}=|\\{\\pmb{n}\\in\\mathcal{D}:\\Pi(\\pmb{n})=\\pmb{k},\\,\\pmb{n}_{i}=\\boldsymbol{n}\\}|.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Again this agrees with the definition of $B$ given in (114). The following Lemma holds. ", "page_idx": 35}, {"type": "text", "text": "Lemma 6. Let $B^{\\pmb{k},i}\\in\\mathbb{R}^{\\mathcal{N}(i,\\pmb{k})}$ be as defined above. Assume that for all $i\\in[I]$ and $k\\,\\in\\,[K]$ the bound $|\\Pi_{i}^{-1}(k)|\\le2N/K$ holds. Then the following bound holds for $\\eta>0$ and $n\\in\\mathcal{N}(i,\\pmb{k})$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|B_{n}^{k,i}-\\mathbb{E}(B_{n}^{k,i})|\\geq\\operatorname*{max}\\left(2\\left(\\frac{2}{K}\\right)^{(I-1)/2}\\sqrt{\\eta\\ln(S)\\frac{S}{N}},\\frac{4}{3}\\eta\\ln(S)\\right)\\right)\\leq2S^{-\\eta}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Note that viewing $I$ and $K$ as constant we get for $S\\geq N\\cdot\\ln(N)$ the bound ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{D}}\\left(|B_{n}^{k,i}-\\mathbb{E}(B_{n}^{k,i})|\\ge C_{I,K}\\sqrt{\\eta\\ln(S)\\frac{S}{N}}\\right)\\le2S^{-\\eta}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. The proof is along the lines of Lemma 5 but slightly more technical. Note that the entries of $B^{k,i}$ are distributed according to $\\mathrm{Bin}(S,p(\\pmb{k},i))$ where ", "page_idx": 36}, {"type": "equation", "text": "$$\np(\\pmb{k},i)=\\frac{|\\Pi^{-1}(\\pmb{k})|}{N^{I}|\\Pi_{i}^{-1}(\\pmb{k}_{i})|}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Note that by assumption ", "page_idx": 36}, {"type": "equation", "text": "$$\np(\\pmb{k},i)\\leq\\left(\\frac{2N}{K}\\right)^{I-1}N^{-I}=\\frac{1}{N}\\left(\\frac{2}{K}\\right)^{I-1}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Since the variance of a $\\mathrm{Ber}(p)$ variable is $p(1-p)\\leq p$ Bernstein\u2019s inequality then implies ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(|\\mathrm{Bin}(S,p(\\pmb{k},i))-S\\cdot p(\\pmb{k},i)|\\geq t)\\leq2\\exp\\left(-\\frac{t^{2}}{2S p(\\pmb{k},i)+2t/3}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq2\\exp\\left(-\\operatorname*{min}\\left(\\frac{t^{2}}{4S p(\\pmb{k},i)},3t/4\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Now we apply this bound with $t=\\operatorname*{max}(2{\\sqrt{\\eta\\ln(S)S p(k,i)}},4/3\\eta\\ln(S))$ which implies ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\vert\\mathrm{Bin}(S,p(k,i))-S\\cdot p(k,i)\\vert\\ge\\operatorname*{max}(2\\sqrt{\\eta\\ln(S)S p(k,i)},4/3\\eta\\ln(S))\\Big)\\le2S^{-\\eta}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Applying (245) we find ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\mathrm{Bin}(S,p(k,i))-S\\cdot p(k,i)\\right|\\geq\\operatorname*{max}\\left(2\\left(\\frac{2}{K}\\right)^{(I-1)/2}\\sqrt{\\eta\\ln(S)\\frac{S}{N}},\\frac{4}{3}\\eta\\ln(S)\\right)\\right)\\leq2S^{-\\eta}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "As before we also consider the vector $B^{k,i}\\in\\mathbb{R}^{[N]}$ by ", "page_idx": 36}, {"type": "equation", "text": "$$\nB_{n}^{k,i}={\\binom{B_{n}^{k,i}\\ \\ \\ \\mathrm{if}\\ n\\in{\\mathcal{N}}(i,k)}{0\\ \\ \\ \\mathrm{otherwise}}}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "For our analysis we want to summarize all those concentration bounds in one results. To simplify the statement we only consider our main regime of interest. ", "page_idx": 36}, {"type": "text", "text": "Theorem 9. Assume that the bound $N/(2K)\\,\\le\\,|\\Pi_{i}^{-1}(k)|\\,\\le\\,2N/K$ holds for all $i\\;\\in\\;[I]$ and $k\\,\\in\\,[K]$ . Assume that $S\\ge I^{2}K^{I}\\ln(N)N$ . Then with probability at least $1-S^{-1}$ the following bounds hold simultaneously for all $\\pmb{k}\\in[K]^{I},\\,i,j\\in[I]$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad\\left\\lVert A^{i,j}-\\mathbb{E}(A^{i,j})\\right\\rVert\\le3\\ln(S)\\sqrt{\\frac{S}{N}},}\\\\ &{\\quad\\quad\\left\\lVert A^{k,i,j}-\\mathbb{E}(A^{k,i,j})\\right\\rVert\\le6\\ln(S)\\left(\\frac{2}{K}\\right)^{(I-1)/2}\\sqrt{\\frac{S}{N}},}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\mathbf{B}_{n}^{i}\\le\\frac{S}{N}+4\\sqrt{\\frac{\\ln(S)S}{N}},}\\\\ &{\\quad\\quad\\quad\\quad\\left|B_{n}^{k,i}-\\mathbb{E}(B_{n}^{k,i})\\right|\\le4\\left(\\frac{2}{K}\\right)^{(I-1)/2}\\sqrt{\\ln(S)\\frac{S}{N}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. Applying Lemma 3 with $\\eta=2$ we obtain that the first bound holds with probability at least $1-\\bar{2}S^{-\\bar{2}}$ (here we used $S\\geq N;$ ). Similarly, we obtain for $S\\geq(K/2)^{I-1}N$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left(1,\\left({\\frac{2}{K}}\\right)^{(I-1)/2}{\\sqrt{\\frac{S}{N}}}\\right)=\\left({\\frac{2}{K}}\\right)^{(I-1)/2}{\\sqrt{\\frac{S}{N}}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Setting $\\eta=5$ in Lemma 4 we get that the second bound holds (by the union bound) with probability at least $\\mathrm{i}-I^{2}K^{I}S^{-3}$ for all $i,j\\in[I]$ and $\\pmb{k}\\in[K]^{I}$ . For $S\\geq\\dot{\\alpha}N\\ln(N)$ and $N\\geq\\alpha$ we find ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{S}{\\ln(S)}\\geq\\frac{\\alpha N\\ln(N)}{\\ln(\\alpha\\ln(N)N)}\\geq\\frac{\\alpha N\\ln(N)}{3\\ln(N)}\\geq\\frac{\\alpha}{3}N\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "which implies for $\\eta=2$ and $S\\geq3N\\ln(N)$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left(2{\\sqrt{\\eta\\ln(S){\\frac{S}{N}}}},4/3\\eta\\ln(S)\\right)\\leq4{\\sqrt{\\ln(S){\\frac{S}{N}}}}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and thus applying Lemma 5 with $\\eta=2$ implies that the third bound holds with probability at least $1-S^{-2}$ . Finally we find for $S\\geq3(K/2)^{L-1}N\\ln(N)$ and $\\eta=3$ the bound ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left(2\\left(\\frac{2}{K}\\right)^{(I-1)/2}\\sqrt{\\eta\\ln(S)\\frac{S}{N}},\\frac{4}{3}\\eta\\ln(S)\\right)\\le4\\left(\\frac{2}{K}\\right)^{(I-1)/2}\\sqrt{\\ln(S)\\frac{S}{N}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Lemma 6 implies that the last bound holds with probability at least $1-K^{I}I S^{-3}$ . All bounds simultaneously then hold with probability at least ", "page_idx": 37}, {"type": "equation", "text": "$$\n1-2S^{-2}-I^{2}K^{I}S^{-3}-S^{-2}-K^{I}I S^{-3}\\geq1-(2+1+1+1)S^{-2}\\geq1-S^{-1}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "H Spectral bounds for Random Matrices ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "We need concentration bounds for random matrices whose entries follow a Poisson distribution. Lemma 7 below should be folklore but we did not find an exact reference so we provide a proof based on standard concentration results for random matrices. Let us first state the general result. ", "page_idx": 37}, {"type": "text", "text": "Theorem 10 (Corollary 3.7 in [26]). Let $X_{k}$ be a sequence of independent random symmetric matrices with dimension $d$ . We assume that there is a function $\\bar{g^{\\ast}}\\left(0,\\bar{\\infty}\\right)\\rightarrow\\left[0,\\infty\\right]$ and symmetric matrices $A_{k}$ such that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}e^{\\theta X_{k}}\\leq e^{g(\\theta)A_{k}}\\quad f o r\\,\\theta>0.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Define ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\rho:=\\lambda_{\\operatorname*{max}}(\\sum_{k}A_{k}).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Then for all $t\\in\\mathbb R$ the following bound holds ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\lambda_{\\operatorname*{max}}\\left(\\sum_{k}X_{k}\\right)\\geq t\\right)\\leq d\\cdot\\operatorname*{inf}_{\\theta>0}e^{-\\theta t+g(\\theta)\\rho}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We now apply the previous concentration bound to our specific setting of interest. ", "page_idx": 37}, {"type": "text", "text": "Lemma 7. Consider a random matrix $A\\in\\mathbb{R}^{d_{1}\\times d_{2}}$ whose entries are independent random variables with $A_{i j}\\sim\\mathrm{Poi}(\\lambda)$ for some $\\lambda>0$ . Then the following bound holds for all $\\eta\\geq0$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\|A-\\mathbb{E}(A)\\|\\ge(\\eta+1)\\operatorname*{max}\\left(\\sqrt{\\operatorname*{max}(d_{1},d_{2})\\lambda},1\\right)\\right)\\le2(d_{1}+d_{2})e^{-\\eta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. To bound the norm of this non-symmetric matrix we use the usual approach to consider ", "page_idx": 37}, {"type": "equation", "text": "$$\nQ=\\left(\\!\\!\\begin{array}{c c}{{\\mathbf{0}_{d_{1}\\times d_{1}}}}&{{A-\\mathbb{E}(A)}}\\\\ {{A^{\\top}-\\mathbb{E}(A)^{\\top}}}&{{\\mathbf{0}_{d_{1}\\times d_{1}}}}\\end{array}\\!\\!\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Consider the index set ", "page_idx": 37}, {"type": "equation", "text": "$$\nI=\\{(i,j):i\\in[d_{1}],\\,j\\in\\{d_{1}+1,\\dots,d_{2}+d_{2}\\}\\}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "then we can write ", "page_idx": 38}, {"type": "equation", "text": "$$\nQ\\stackrel{\\mathcal{D}}{=}\\sum_{(i,j)\\in I}(N_{i,j}-\\lambda)(E_{i,j}+E_{j,i})\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $E_{i,j}$ is the matrix whose entry $(i,j)$ is 1 and all other entries vanish and $N_{i,j}\\sim\\mathrm{Poi}(\\lambda)$ . Let $X_{i j}=(\\check{N_{i,j}}-\\lambda)(E_{i,j}+E_{j,i})$ . Note that by induction one directly finds ", "page_idx": 38}, {"type": "equation", "text": "$$\n(E_{i,j}+E_{j,i})^{k}={\\left\\{\\begin{array}{l l}{E_{i i}+E_{j j}\\quad{\\mathrm{if~}}k{\\mathrm{~is~even}}}\\\\ {E_{i,j}+E_{j,i}\\quad{\\mathrm{if~}}k{\\mathrm{~is~odd}}.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In any case we get $\\pm(E_{i,j}+E_{j,i})^{k}\\le E_{i,i}+E_{j,j}$ in the sense of symmetric matrices and we set $A_{i j}=E_{i,i}+E_{j,j}$ . Now we obtain for any $\\theta\\in\\mathbb{R}$ the relation ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}e^{\\theta X_{i j}}=A_{i j}\\displaystyle\\sum_{k=0}^{\\infty}\\frac{\\theta^{2k}\\mathbb{E}(\\mathrm{Poi}(\\lambda)-\\lambda)^{2k}}{(2k)!}+(E_{i,j}+E_{j,i})\\displaystyle\\sum_{k=0}^{\\infty}\\frac{\\theta^{2k+1}\\mathbb{E}(\\mathrm{Poi}(\\lambda)-\\lambda)^{2k+1}}{(2k+1)!}}\\\\ &{\\qquad\\quad\\leq A_{i j}\\left(\\displaystyle\\sum_{k=0}^{\\infty}\\frac{\\theta^{2k}\\mathbb{E}(\\mathrm{Poi}(\\lambda)-\\lambda)^{2k}}{(2k)!}+\\displaystyle\\left|\\sum_{k=0}^{\\infty}\\frac{\\theta^{2k+1}\\mathbb{E}(\\mathrm{Poi}(\\lambda)-\\lambda)^{2k+1}}{(2k+1)!}\\right|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Note that the first summand is invariant under $\\theta\\rightarrow-\\theta$ while the term in absolute values changes its sign. This implies that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left(\\sum_{k=0}^{\\infty}\\frac{\\theta^{2k}\\mathbb{E}(\\mathrm{Poi}(\\lambda)-\\lambda)^{2k}}{(2k)!}+\\left|\\sum_{k=0}^{\\infty}\\frac{\\theta^{2k+1}\\mathbb{E}(\\mathrm{Poi}(\\lambda)-\\lambda)^{2k+1}}{(2k+1)!}\\right|\\right)}}\\\\ &{=\\operatorname*{max}\\left(\\sum_{k=0}^{\\infty}\\frac{\\theta^{k}\\mathbb{E}(\\mathrm{Poi}(\\lambda)-\\lambda)^{k}}{k!},\\sum_{k=0}^{\\infty}\\frac{\\theta^{k}\\mathbb{E}(\\mathrm{Poi}(\\lambda)-\\lambda)^{k}}{k!}\\right)}\\\\ &{=\\operatorname*{max}\\left(\\mathbb{E}e^{\\theta(\\mathrm{Poi}(\\lambda)-\\lambda)},\\mathbb{E}e^{-\\theta(\\mathrm{Poi}(\\lambda)-\\lambda)}\\right)}\\\\ &{=\\operatorname*{max}\\left(e^{\\lambda(e^{\\theta}-1)-\\lambda\\theta},e^{\\lambda(e^{-\\theta}-1)+\\lambda\\theta}\\right)=e^{\\lambda(e^{|\\theta|}-1)-\\lambda|\\theta|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Here we used in the last step that $\\lambda\\geq0$ and then for $\\theta\\ge0$ ", "page_idx": 38}, {"type": "equation", "text": "$$\ne^{\\theta}-e^{-\\theta}=2\\sinh(\\theta)\\geq2\\theta.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Thus we infer from (267) and (268) that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}e^{\\theta X_{i j}}\\leq e^{\\lambda(e^{|\\theta|}-1)-\\lambda|\\theta|}A_{i j}=e^{\\left(\\lambda(e^{|\\theta|}-1)-\\lambda|\\theta|\\right)A_{i j}}:=e^{g_{\\lambda}(|\\theta|)A_{i j}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "using that $A_{i j}^{k}=A_{i j}$ in the second step. Note that here ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sum_{(i,j)]i n I}A_{i j}=\\sum_{(i,j)]i n I}E_{i,i}+E_{j,j}=\\sum_{i=1}^{d_{1}}d_{2}E_{i,i}+\\sum_{i=d_{1}+1}^{d_{2}+d_{2}}d_{1}E_{i,i}\\quad\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which implies ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\rho=\\lambda\\left(\\sum_{(i,j)]i n I}A_{i j}\\right)=\\operatorname*{max}(d_{1},d_{2}).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Thus we can apply Theorem 10 and get ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\lambda_{\\operatorname*{max}}\\left(\\displaystyle\\sum_{(i,j)\\in I}X_{i j}\\right)\\geq t\\right)\\leq(d_{1}+d+2)\\cdot\\operatorname*{inf}_{\\theta>0}e^{-\\theta t+g(\\theta)\\rho}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq(d_{1}+d_{2})\\cdot\\operatorname*{inf}_{\\theta>0}e^{-\\theta t+\\lambda\\operatorname*{max}(d_{1},d_{2})(e^{\\theta}-1-\\theta)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "If $\\operatorname*{max}(d_{1},d_{2})\\lambda\\leq1$ we apply this bound with $t=\\eta+1$ and $\\theta=1$ and find ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\lambda_{\\operatorname*{max}}\\left(\\displaystyle\\sum_{(i,j)\\in I}X_{i j}\\right)\\geq(1+\\eta)\\right)\\leq(d_{1}+d+2)\\cdot\\exp\\left(-(1+\\eta)+\\lambda\\operatorname*{max}(d_{1},d_{2})(e^{1}-2)\\right)}\\\\ {\\leq(d_{1}+d+2)e^{-\\eta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "If $\\operatorname*{max}(d_{1},d_{2})\\lambda\\geq1$ we set $\\theta=\\sqrt{\\operatorname*{max}(d_{1},d_{2})\\lambda}^{-1}\\leq1$ and $t=(\\eta+1)\\sqrt{\\operatorname*{max}(d_{1},d_{2})\\lambda}$ and find using that for $0\\leq\\theta\\leq1$ the bound $e^{\\theta}-1-\\theta\\leq\\theta^{2}$ holds ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\lambda_{\\operatorname*{max}}\\left(\\sum_{(i,j)\\in I}X_{i j}\\right)\\geq(\\eta+1)\\sqrt{\\operatorname*{max}(d_{1},d_{2})\\lambda}\\right)\\leq\\exp\\left(-\\theta t+\\lambda\\operatorname*{max}(d_{1},d_{2})\\theta^{2}\\right)}\\\\ &{\\quad\\leq(d_{1}+d+2)\\cdot\\exp\\left(-(1+\\eta)\\frac{\\sqrt{\\operatorname*{max}(d_{1},d_{2})\\lambda}}{\\sqrt{\\operatorname*{max}(d_{1},d_{2})\\lambda}}+\\lambda\\operatorname*{max}(d_{1},d_{2})\\sqrt{\\operatorname*{max}(d_{1},d_{2})\\lambda}^{-2}\\right)}\\\\ &{\\quad=(d_{1}+d+2)e^{-\\eta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Thus we can combine both cases and obtain the relation ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\lambda_{\\operatorname*{max}}\\left(Q\\right)\\geq(\\eta+1)\\operatorname*{max}\\left(\\sqrt{\\operatorname*{max}(d_{1},d_{2})\\lambda},1\\right)\\right)\\leq(d_{1}+d_{2})e^{-\\eta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We observe that by (270) ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}e^{\\theta(-X_{i j})}=\\mathbb{E}e^{-\\theta X_{i j}}\\leq e^{g_{\\lambda}(|\\theta|)A_{i j}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Thus the same reasoning shows that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\lambda_{\\operatorname*{min}}\\left(Q\\right)\\leq-(\\eta+1)\\operatorname*{max}\\left(\\sqrt{\\operatorname*{max}(d_{1},d_{2})\\lambda},1\\right)\\right)\\leq(d_{1}+d_{2})e^{-\\eta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and since $\\|A-\\mathbb{E}A\\|=\\|Q\\|=\\operatorname*{max}|\\lambda_{i}(Q)|$ the claim follows by applying the union bound over (276) and (278). \u53e3 ", "page_idx": 39}, {"type": "text", "text": "I Kronecker Products ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Let us here state some basic properties of the Kronecker product of matrices. For two matrices $A\\in\\mathbb{R}^{n\\times m}$ and $\\b{B}\\in\\mathbb{R}^{p\\times q}$ the Kronecker product is defined by ", "page_idx": 39}, {"type": "equation", "text": "$$\nA\\otimes B=\\left(\\!\\!{\\begin{array}{c c c}{a_{11}B}&{\\cdots}&{a_{1m}B}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {a_{n1}B}&{\\cdots}&{a_{n m}B}\\end{array}}\\!\\!\\right)\\in\\mathbb{R}^{n p\\times m q}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We will need the property that the operator norm satisfies ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\|A\\otimes B\\|\\leq\\|A\\|\\cdot\\|B\\|.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Note that we use the notation $v\\otimes w\\;=\\;v w^{\\top}$ to denote the outer product. Formally we have $v w^{\\top}=v\\otimes w^{\\top}$ but it is convenient and common to drop the transposed here. ", "page_idx": 39}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: See main results and Section 3. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 40}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: See discussions in Section 2 and Section 4 and Section 5. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 40}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: All assumptions are stated and complete proofs can be found in the sumpplementary material. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 41}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 41}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 42}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 42}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 43}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 43}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 43}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 44}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 44}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 44}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 44}, {"type": "text", "text": "13. New Assets ", "page_idx": 44}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 45}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 45}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 45}]