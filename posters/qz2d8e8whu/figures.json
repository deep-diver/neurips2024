[{"figure_path": "QZ2d8E8Whu/figures/figures_0_1.jpg", "caption": "Figure 1: An example of DBZ bug", "description": "This figure shows a Java program with a potential divide-by-zero (DBZ) bug.  The variable `x` at line 9 is identified as a source, potentially receiving a zero value from user input. Variables `b` at line 4 and `y` at lines 11 and 14 are sinks because they are used as divisors. The program demonstrates how a zero value from the source could lead to a DBZ error at line 14 if the flow of the variable `x` is not carefully handled. The safe and buggy instances show the context of the vulnerability.", "section": "1 Introduction"}, {"figure_path": "QZ2d8E8Whu/figures/figures_1_1.jpg", "caption": "Figure 2: Two different paradigms of dataflow analysis", "description": "This figure compares classical dataflow analysis with the proposed LLM-powered approach.  Classical dataflow analysis (left) relies on compilation and requires customization by experts for specific analysis needs.  The intermediate representation (IR) generated by the compiler is used in the analysis, which limits its applicability to incomplete programs. The LLM-powered method (right) is compilation-free. It directly analyzes the source code and can be customized by the user through prompts, making it more flexible and applicable to real-world scenarios.  The LLM synthesizes code that outsources tasks to external tools, leading to more reliable and robust analysis. ", "section": "Introduction"}, {"figure_path": "QZ2d8E8Whu/figures/figures_2_1.jpg", "caption": "Figure 3: An example of CFG", "description": "This figure shows a partial CFG (Control Flow Graph) of the Java program example in Figure 1.  The CFG visually represents the flow of control within the program, showing the sequence of statements executed and the conditions that govern those transitions.  It highlights aspects like branch conditions (e.g., `Math.abs(b) > 1`), function calls (shown with dashed boxes representing argument passing and return value), and assignments. The dashed boxes represent the data flowing from arguments to parameters and from the function's return value to the program's output value. This illustrative CFG helps to understand how program values propagate, which is fundamental to dataflow analysis.", "section": "2 Preliminaries and Problem Formulation"}, {"figure_path": "QZ2d8E8Whu/figures/figures_3_1.jpg", "caption": "Figure 4: The workflow of LLMDFA consists of three phases", "description": "The workflow of LLMDFA is divided into three phases: Source/Sink Extraction, Dataflow Summarization, and Path Feasibility Validation. In the first phase, LLMs synthesize scripts leveraging parsing libraries to extract sources and sinks from the program's Abstract Syntax Tree (AST). The second phase employs few-shot chain-of-thought prompting to summarize dataflow facts within individual functions. The third phase leverages LLMs to synthesize Python scripts that invoke Z3 solver to validate the feasibility of discovered dataflow paths.  Each phase helps mitigate the hallucinations of LLMs, improving the accuracy and reliability of the dataflow analysis.", "section": "3 Method"}, {"figure_path": "QZ2d8E8Whu/figures/figures_4_1.jpg", "caption": "Figure 5: A summary discovered via CoT", "description": "This figure illustrates how the Chain-of-Thought (CoT) prompting method helps LLMs reason step by step to discover dataflow facts.  It depicts a query asking whether there is a dataflow between variable x at line 9 and variable z at line 13. The LLM's response indicates a positive dataflow, showing the intermediate variables x@l12 and z@l12 that form the path between the initial variables. This demonstrates the CoT's effectiveness in summarizing and explaining the dataflow path.", "section": "3.2 Phase II: Dataflow Summarization"}, {"figure_path": "QZ2d8E8Whu/figures/figures_4_2.jpg", "caption": "Figure 6: A script invoking Z3 solver", "description": "This figure shows a short Python script that uses the Z3 solver to check the satisfiability of a constraint.  The constraint, `Abs(b) > 1` and `b == 0`, is clearly unsatisfiable;  however, the script demonstrates how LLMDFA uses external tools to validate path feasibility and avoid hallucinations.  LLMs generate this type of script to outsource complex reasoning tasks to more reliable tools.", "section": "3.3 Phase III: Path Feasibility Validation"}, {"figure_path": "QZ2d8E8Whu/figures/figures_6_1.jpg", "caption": "Figure 7: The comparison of LLMDFA, CodeFuseQuery, Pinpoint, and LLM-based end-to-end analysis. LLMDFA and LLM-based end-to-end analysis are powered with gpt-3.5", "description": "This figure compares the performance of LLMDFA with three other methods for detecting three types of bugs (DBZ, XSS, and OSCI) in terms of precision, recall, and F1 score.  LLMDFA and an end-to-end LLM-based approach both utilize gpt-3.5.  The other two methods are CodeFuseQuery and Pinpoint, representing more traditional static analysis techniques. The chart visually demonstrates LLMDFA's superiority in terms of overall performance across all three bug types.", "section": "4.3 Comparison with Baselines"}, {"figure_path": "QZ2d8E8Whu/figures/figures_7_1.jpg", "caption": "Figure 8: The comparison of LLMDFA and ablations using gpt-3.5", "description": "This figure compares the performance of LLMDFA with three ablation studies: NoSynExt (no synthesized extractors), NoCoT (no chain-of-thought prompting), and NoSynVal (no synthesized validation scripts).  The results are shown separately for DBZ, XSS, and OSCI bug detection, illustrating the impact of each component on precision, recall, and F1 score.  The radar chart format facilitates a visual comparison of the different methods across these three metrics.", "section": "4.4 Ablation Studies"}, {"figure_path": "QZ2d8E8Whu/figures/figures_15_1.jpg", "caption": "Figure 5: A summary discovered via CoT", "description": "This figure shows an example of how Chain-of-Thought (CoT) prompting is used in LLMDFA to summarize dataflow facts.  The prompt asks if there is a dataflow from variable x at line 9 to variable z at line 13. The LLM's response, facilitated by CoT, breaks down the dataflow path into intermediate steps (x@l9 \u2192 x@l12 and z@l12 \u2192 z@l13), eventually confirming the dataflow fact x@l9 \u2192 z@l13.  This demonstrates how CoT helps LLMs align with program semantics and reduce hallucinations.", "section": "3.2 Phase II: Dataflow Summarization"}, {"figure_path": "QZ2d8E8Whu/figures/figures_15_2.jpg", "caption": "Figure 4: The workflow of LLMDFA consists of three phases", "description": "The workflow of LLMDFA is decomposed into three phases: Source/Sink Extraction, Dataflow Summarization, and Path Feasibility Validation.  In the first phase, LLMs synthesize scripts that use parsing libraries to extract sources and sinks from the program's Abstract Syntax Tree (AST).  The second phase uses few-shot chain-of-thought prompting to summarize dataflow facts in individual functions. Finally, the third phase leverages LLMs to generate Python scripts that utilize Z3 solver to validate the feasibility of program paths, mitigating hallucinations and ensuring reliable analysis results.", "section": "3 Method"}, {"figure_path": "QZ2d8E8Whu/figures/figures_15_3.jpg", "caption": "Figure 12: An example program with sources/sinks (a) and a sink extractor (b) for the DBZ detection", "description": "This figure shows an example program with identified sources and sinks for divide-by-zero bug detection. Part (a) displays the Java code snippet illustrating various source variables (x1, x2, x3, x4) which can potentially lead to zero values, along with a sink variable (x) used as a divisor. Part (b) presents the Python code for a sink extractor.  The sink extractor, automatically generated by LLMs, traverses the Abstract Syntax Tree (AST) of the program and identifies sinks (variables used in division or modulo operations). This demonstrates how LLMs are utilized in LLMDFA for automated source/sink extraction.", "section": "3.1 Phase I: Source/Sink Extraction"}, {"figure_path": "QZ2d8E8Whu/figures/figures_17_1.jpg", "caption": "Figure 13: The numbers of fixes in path feasibility validation using different LLMs", "description": "This figure shows the number of times the Python scripts synthesized by LLMDFA for path feasibility validation needed to be fixed before successfully encoding the path conditions correctly.  The results are broken down by the specific bug type (DBZ, XSS, OSCI) and the LLM used (gpt-3.5, gpt-4, gemini-1.0, claude-3). It illustrates the number of paths where no fixes were needed (#Fix = 0), one fix (#Fix = 1), two fixes (#Fix = 2), three fixes (#Fix = 3), or where the system ultimately fell back to using the LLM directly (#Using LLM).", "section": "4.3 Comparison with Baselines"}, {"figure_path": "QZ2d8E8Whu/figures/figures_18_1.jpg", "caption": "Figure 14: The precision, recall, and F1 score of LLMDFA and LLM-based end-to-end analysis in the DBZ, XSS, and OSCI detection. From left to right in each line from left to right, the sub-figures depict the statistics of performance in the DBZ, XSS, and OSCI detection, respectively.", "description": "This figure presents a comparison of the performance of LLMDFA and a baseline LLM-based end-to-end analysis method across three different bug types: Divide-by-Zero (DBZ), Cross-Site Scripting (XSS), and OS Command Injection (OSCI).  The comparison is shown using radar charts that visualize the precision, recall, and F1-score for each method and bug type. Each chart shows the relative strengths and weaknesses of each approach in terms of identifying these specific types of bugs. The figure allows for a visual assessment of how LLMDFA performs compared to the baseline, highlighting its superior performance in several instances.", "section": "A.3.3 Performance of LLMDFA and LLM-based End-to-End Analysis Using Different LLMs"}, {"figure_path": "QZ2d8E8Whu/figures/figures_18_2.jpg", "caption": "Figure 7: The comparison of LLMDFA, CodeFuseQuery, Pinpoint, and LLM-based end-to-end analysis. LLMDFA and LLM-based end-to-end analysis are powered with gpt-3.5", "description": "This figure compares the performance of LLMDFA against three other methods for detecting bugs: CodeFuseQuery, Pinpoint, and an LLM-based end-to-end approach.  It shows precision, recall, and F1 score for three types of bugs (DBZ, XSS, and OSCI).  LLMDFA demonstrates higher overall performance across all metrics, indicating that it's better at accurately identifying bugs and avoiding false positives or negatives.", "section": "4.3 Comparison with Baselines"}, {"figure_path": "QZ2d8E8Whu/figures/figures_18_3.jpg", "caption": "Figure 14: The precision, recall, and F1 score of LLMDFA and LLM-based end-to-end analysis in the DBZ, XSS, and OSCI detection. From left to right in each line from left to right, the sub-figures depict the statistics of performance in the DBZ, XSS, and OSCI detection, respectively.", "description": "This figure compares the performance of LLMDFA and a baseline LLM-based end-to-end approach across three types of bugs (DBZ, XSS, and OSCI).  It uses radar charts to visualize the precision, recall, and F1 score for each method and bug type.  The results show LLMDFA generally outperforming the baseline approach in terms of precision, recall, and F1 score for all bug types. Note that each small radar chart represents a different bug type.", "section": "A.3.3 Performance of LLMDFA and LLM-based End-to-End Analysis Using Different LLMs"}, {"figure_path": "QZ2d8E8Whu/figures/figures_18_4.jpg", "caption": "Figure 14: The precision, recall, and F1 score of LLMDFA and LLM-based end-to-end analysis in the DBZ, XSS, and OSCI detection. From left to right in each line from left to right, the sub-figures depict the statistics of performance in the DBZ, XSS, and OSCI detection, respectively.", "description": "This figure compares the performance of LLMDFA and a baseline LLM-based end-to-end approach across three different bug types: Divide-by-Zero (DBZ), Cross-Site Scripting (XSS), and OS Command Injection (OSCI).  The radar charts show precision, recall, and F1-score for each method and bug type.  It allows for a visual comparison of the two approaches' effectiveness in detecting these types of vulnerabilities.", "section": "A.3.3 Performance of LLMDFA and LLM-based End-to-End Analysis Using Different LLMs"}, {"figure_path": "QZ2d8E8Whu/figures/figures_19_1.jpg", "caption": "Figure 7: The comparison of LLMDFA, CodeFuseQuery, Pinpoint, and LLM-based end-to-end analysis. LLMDFA and LLM-based end-to-end analysis are powered with gpt-3.5", "description": "This figure compares the performance of LLMDFA with three other methods for detecting three types of bugs: Divide-by-Zero (DBZ), Cross-Site Scripting (XSS), and OS Command Injection (OSCI).  The three other methods are CodeFuseQuery (a classical dataflow analyzer), Pinpoint (another classical dataflow analyzer), and an LLM-based end-to-end approach. The comparison is done in terms of precision, recall, and F1-score for each bug type and each method. The figure clearly shows that LLMDFA significantly outperforms the other three methods in almost all cases.", "section": "4.3 Comparison with Baselines"}, {"figure_path": "QZ2d8E8Whu/figures/figures_19_2.jpg", "caption": "Figure 8: The comparison of LLMDFA and ablations using gpt-3.5", "description": "This figure compares the performance of LLMDFA with three ablation studies (NoSynExt, NoCoT, and NoSynVal) using the gpt-3.5 language model.  Each ablation represents a modification to the LLMDFA approach, removing a key component (source/sink extraction, few-shot chain-of-thought prompting, or path feasibility validation). The radar charts visualize the precision, recall, and F1 score for each method across three types of bugs: DBZ, XSS, and OSCI. This illustrates the impact of each component on the overall performance of the dataflow analysis.", "section": "4.4 Ablation Studies"}, {"figure_path": "QZ2d8E8Whu/figures/figures_19_3.jpg", "caption": "Figure 8: The comparison of LLMDFA and ablations using gpt-3.5", "description": "This figure compares the performance of LLMDFA and its three ablation models (NoSynExt, NoCoT, NoSynVal) across three different bug types (DBZ, XSS, OSCI).  Each ablation removes a key component of LLMDFA's design to isolate its impact on overall performance.  The results, presented as radar charts, show precision, recall, and F1-score for each model and bug type.  This allows for a quantitative assessment of the contributions of each component of the LLMDFA framework. LLMDFA consistently outperforms each ablation, demonstrating the effectiveness of its multi-faceted approach in mitigating the hallucinations inherent in LLM-based dataflow analysis.", "section": "4.4 Ablation Studies"}, {"figure_path": "QZ2d8E8Whu/figures/figures_19_4.jpg", "caption": "Figure 7: The comparison of LLMDFA, CodeFuseQuery, Pinpoint, and LLM-based end-to-end analysis. LLMDFA and LLM-based end-to-end analysis are powered with gpt-3.5", "description": "This figure compares the performance of LLMDFA with three other methods: CodeFuseQuery, Pinpoint, and an LLM-based end-to-end approach.  It shows precision, recall, and F1 scores for each method across three types of bugs (DBZ, XSS, and OSCI).  The comparison highlights LLMDFA's superior performance, particularly its improved F1 score, indicating a better balance of precision and recall compared to the other techniques.", "section": "4.3 Comparison with Baselines"}, {"figure_path": "QZ2d8E8Whu/figures/figures_22_1.jpg", "caption": "Figure 4: The workflow of LLMDFA consists of three phases", "description": "The workflow of LLMDFA is presented as a flowchart, highlighting its three main phases: Source/Sink Extraction, Dataflow Summarization, and Path Feasibility Validation. Each phase utilizes LLMs and external tools to mitigate hallucinations and improve the reliability of dataflow analysis.  The Source/Sink Extraction phase leverages LLMs to generate scripts that use parsing libraries to identify sources and sinks. The Dataflow Summarization phase employs few-shot chain-of-thought prompting to summarize dataflow facts within individual functions.  Finally, the Path Feasibility Validation phase utilizes LLMs to synthesize scripts that use SMT solvers to check the feasibility of identified dataflow paths. This decomposition of the problem into smaller, more manageable subtasks is a key aspect of LLMDFA's approach to minimizing inaccuracies from large language model outputs.", "section": "3 Method"}, {"figure_path": "QZ2d8E8Whu/figures/figures_23_1.jpg", "caption": "Figure 4: The workflow of LLMDFA consists of three phases", "description": "The figure illustrates the three-phase workflow of LLMDFA. Phase I, Source/Sink Extraction, uses LLMs to synthesize scripts leveraging parsing libraries to extract sources and sinks from the program's CFG. Phase II, Dataflow Summarization, employs few-shot chain-of-thought prompting to align LLMs with program semantics and summarize dataflow facts. Phase III, Path Feasibility Validation, utilizes LLMs to synthesize Python scripts invoking Z3 solvers to validate the feasibility of the dataflow facts.  This multi-step approach aims to mitigate the hallucinations inherent in using LLMs for complex reasoning tasks.", "section": "3 Method"}, {"figure_path": "QZ2d8E8Whu/figures/figures_23_2.jpg", "caption": "Figure 4: The workflow of LLMDFA consists of three phases", "description": "The figure shows a detailed workflow of LLMDFA, which is divided into three phases: Source/Sink Extraction, Dataflow Summarization, and Path Feasibility Validation.  Each phase utilizes LLMs in conjunction with external tools to mitigate hallucinations and improve accuracy.  The Source/Sink Extraction phase leverages LLMs to generate scripts that use parsing libraries to identify sources and sinks in the code.  The Dataflow Summarization phase employs few-shot chain-of-thought prompting to summarize dataflow facts within functions. Finally, the Path Feasibility Validation phase uses LLMs to generate scripts that use Z3 solvers to validate the feasibility of identified dataflow paths.", "section": "3 Method"}]