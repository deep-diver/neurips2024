[{"type": "text", "text": "Optimal Multi-Fidelity Best-Arm Identification ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Riccardo Poiani\u2217 DEIB, Politecnico di Milano, Milan, Italy riccardo.poiani@polimi.it ", "page_idx": 0}, {"type": "text", "text": "R\u00e9my Degenne Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189-CRIStAL, F-59000 Lille, France remy.degenne@inria.fr ", "page_idx": 0}, {"type": "text", "text": "Emilie Kaufmann Univ. Lille, CNRS, Inria, Centrale Lille, UMR 9189-CRIStAL, F-59000 Lille, France emilie.kaufmann@univ-lille.fr ", "page_idx": 0}, {"type": "text", "text": "Alberto Maria Metelli DEIB, Politecnico di Milano, Milan, Italy albertomaria.metelli@polimi.it ", "page_idx": 0}, {"type": "text", "text": "Marcello Restelli DEIB, Politecnico di Milano, Milan, Italy marcello.restelli@polimi.it ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In bandit best-arm identification, an algorithm is tasked with finding the arm with highest mean reward with a specified accuracy as fast as possible. We study multifidelity best-arm identification, in which the algorithm can choose to sample an arm at a lower fidelity (less accurate mean estimate) for a lower cost. Several methods have been proposed for tackling this problem, but their optimality remain elusive, notably due to loose lower bounds on the total cost needed to identify the best arm. Our first contribution is a tight, instance-dependent lower bound on the cost complexity. The study of the optimization problem featured in the lower bound provides new insights to devise computationally efficient algorithms, and leads us to propose a gradient-based approach with asymptotically optimal cost complexity. We demonstrate the beneftis of the new algorithm compared to existing methods in experiments. Our theoretical and empirical findings also shed light on an intriguing concept of optimal fidelity for each arm. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In multi-armed bandits [20], an algorithm chooses at each step one arm among $K>1$ possibilities. It then observes a reward, sampled from a probability distribution on $\\mathbb{R}$ corresponding to the arm. Several goals are possible for the algorithm, and we focus on the best arm identification task (BAI) in which we aim to identify the arm with the largest mean, using as few samples as possible. This is a well-studied problem [6, 1, 12, 10, 8] with potential applications to, e.g. A/B/n testing [27] or hyper-parameter optimization [11]. ", "page_idx": 0}, {"type": "text", "text": "In some applications, like physics, parameter studies, or hyper-parameter optimization, getting a sample from the arm distribution might be expensive since it requires evaluating or training a complex model and is computationally demanding. However, it is often the case that cheaper, less accurate sampling methods are available, for instance, by using a coarser model in the physics study example. ", "page_idx": 0}, {"type": "text", "text": "The multi-fidelity bandit framework takes such scenarios into account. When choosing an arm, the algorithm also chooses a fidelity, with a trade-off: a higher fidelity gives a more precise observation but has a higher cost. We assume that the algorithm knows both the cost and the maximal bias of the observations from each fidelity. This is also how the knowledge about the fidelity was modeled in prior work [see, e.g., 16, 15, 25, 31]. The goal is then to find the best arm (i.e., the arm with the highest mean at the highest fidelity) with high probability and minimal cost. ", "page_idx": 1}, {"type": "text", "text": "Specifically, the bandit algorithm interacts with the multi-fidelity environment and gathers information to find which arm has the highest mean when pulled at the highest fidelity. In the fixed confidence setting, we want to ensure that the algorithm returns a correct answer with probably at least $1-\\delta$ for a given parameter $\\delta\\in(0,1)$ . A good algorithm should do that at a minimum cost, and thus, the appropriate quality metric for evaluating an algorithm\u2019s performance is the sum of costs paid until it stops, i.e., the cost complexity. Previous work on the multi-fidelity BAI problem [25, 31] provided lower bounds on the cost complexity as well as algorithms with cost upper bounds. Those lower and upper bounds do not match, and the proposed methods require additional prior information [31], or their guarantees are restricted to problems satisfying additional hypotheses [25]. We lift all those requirements and provide an improved lower bound and an algorithm with a matching upper bound. ", "page_idx": 1}, {"type": "text", "text": "Contributions and organization of the paper After presenting additional related works, in Section 2, we define fixed-confidence best arm identification in multi-fidelity bandits in more mathematical detail and introduce the notations used throughout the paper. Then, Section 3 contains our first contribution: a tight instance-dependent lower bound on the cost complexity of any algorithm expressed with the maximum of a complex function over all possible cost allocations. We also highlight features of that lower bound, like the existence of an optimal fidelity for each arm, which should be chosen exclusively. In Section 4, we propose a computationally efficient procedure for computing gradients of the function featured in the lower bound and describe a gradient-based algorithm whose cost complexity is asymptotically matching the lower bound. Finally, in Section 5, we present the results of numerical experiments which demonstrate the good empirical performance of our new algorithm compared to prior work. ", "page_idx": 1}, {"type": "text", "text": "Additional related works The multi-fidelity setting has mostly been studied in the context of Bayesian optimization [9, 24, 17, 26, 14, 21] and black-box function optimization with different structural assumptions [28, 29, 7, 23]. The goal there is to find the minimum of a function by successive queries of that function or of cheaper approximations. The metric for success in these works is most often the simple regret, that is, the difference between the best value found and the true minimum, although other goals were considered like the cumulative regret [16, 15]. Furthermore, we notice that best arm identification with costs has recently been studied in [13] for BAI with only one fidelity. The authors introduce a variant of the Track-and-Stop algorithm [8] and prove its asymptotic optimality. However, we will not be able to adapt this study to the multi-fidelity case because, as we shall see, it requires solving a complex optimization problem for which we have no efficient solution. Finally, our work is related to the vast strand of BAI studies that proposes tight lower bound with asymptotically optimal algorithms [e.g., 8, 4, 22]; nevertheless, as we discuss throughout the text, these studies cannot be directly applied to the multi-fidelity BAI problem. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we provide essential background and notation that is used throughout the rest of the paper. A table that summarizes the notation is available in Appendix A. ", "page_idx": 1}, {"type": "text", "text": "A multi-fidelity bandit model with $K$ arms and $M$ fidelities is a set of $K\\times M$ probability distributions $\\pmb{\\nu}=(\\nu_{a,m})_{a\\in[K],m\\in[M]}$ where $\\nu_{a,m}$ has mean of $\\mu_{a,m}$ . For each arm $a\\in[K]$ , $\\mu_{a,m}$ represents the mean value of an observation of arm $a$ using fidelity $m$ , and let $\\pmb{\\mu}=(\\mu_{a,m})_{a\\in[K],m\\in[M]}$ . An observation at fidelity $m$ is assigned a (known) cost $\\lambda_{m}\\geq0$ with $\\lambda_{1}<\\lambda_{2}<\\cdot\\cdot<\\dot{\\lambda_{M}}$ . The goal is to identify the arm that has the largest mean at the highest fidelity $M$ , $a_{\\star}(\\pmb\\mu):=\\operatorname{argmax}_{a\\in[K]}\\mu_{a,M}$ (sometimes denoted by $\\star$ in the sequel to ease notation) with a small total sampling cost, by exploring the arms at different fidelities and using some prior knowledge about their precision. Specifically, we assume that there are some (known) values $\\xi_{1}>\\xi_{2}>\\cdot\\cdot\\cdot>\\xi_{M}=0$ such that, for all arm $a\\in[K]$ , the vector $\\mu_{a}:=(\\mu_{a,m})_{m\\in[M]}$ satisfies ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\forall m\\in[M],\\ \\ |\\mu_{a,m}-\\mu_{a,M}|\\leq\\xi_{m}\\ .\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "We write $\\mu_{a}\\in\\mathrm{MF}$ to indicate that arm $a$ satisfies these multi-fidelity constraints, with these particular parameters $\\xi_{m}$ (although they are not shown in the notation). In this paper, we consider arms that belong to a canonical exponential family [2]. This includes, e.g. arms that have Bernoulli distributions or Gaussian distributions with known variances. Such models are known to be characterized by their means and we refer to such an exponential multi-fidelity bandit model $\\pmb{\\nu}$ using the means of its arms $\\pmb{\\mu}$ , which belongs to the set $\\overset{\\cdot}{\\mathcal{M}_{\\mathrm{MF}}}:=\\{\\pmb{\\mu}\\in\\Theta^{K\\times M}:\\dot{\\forall}a\\in[K],\\mu_{a}\\in\\mathrm{MF}\\}$ , where $\\Theta\\subseteq\\mathbb{R}$ is the interval of possible means. ", "page_idx": 2}, {"type": "text", "text": "At each interaction round $t=1,2,\\dots$ , the agent selects an arm $A_{t}$ and a fidelity $M_{t}$ , observes a sample $X_{t}\\sim\\nu_{A_{t},M_{t}}$ and pays a cost $\\lambda_{M_{t}}$ . Letting $\\mathcal{F}_{t}\\,=\\,\\sigma\\bigl(A_{1},M_{1},X_{1},\\ldots,A_{t},M_{t},X_{t}\\bigr)$ be the sigma field generated by the observations up to time $t$ , a fixed-confidence identification algorithm takes as input a risk parameter $\\delta\\in(0,1)$ and is defined by the following ingredients: (i) a sampling rule $(A_{t},M_{t})_{t}$ , where $(A_{t},M_{t})$ is $\\mathcal{F}_{t-1}$ -measurable, (ii) a stopping rule $\\tau_{\\delta}$ , which is a stopping time w.r.t. $\\mathcal{F}_{t}$ , and (iii) a decision rule $\\hat{a}_{\\tau_{\\delta}}\\in[K]$ , which is $\\mathcal{F}_{\\tau_{\\delta}}$ -measurable. We want to build strategies that ensure $\\mathbb{P}_{\\mu}$ $\\overset{\\vartriangle}{\\mu}(\\hat{a}_{\\tau_{\\delta}}\\neq a_{\\star}(\\pmb{\\mu}))\\leq\\delta$ for all $\\pmb{\\mu}\\in\\mathcal{M}_{\\mathrm{MF}}$ with a unique optimal arm. Such a strategy is called $\\delta$ -correct. Among $\\delta$ -correct strategies, we are looking for strategies that minimize the expected identification cost (i.e., cost complexity) defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu}[c_{\\tau_{\\delta}}]:=\\sum_{a\\in[K]}\\sum_{m\\in[M]}\\lambda_{m}\\mathbb{E}_{\\mu}[N_{a,m}(\\tau_{\\delta})]=\\sum_{a\\in[K]}\\sum_{m\\in[M]}\\mathbb{E}_{\\mu}[C_{a,m}(\\tau_{\\delta})],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $N_{a,m}(t)$ denotes the number of pulls of arm $a$ at fidelity $m$ up to time $t$ and $C_{a,m}(t)\\;=\\;$ $\\lambda_{m}N_{a,m}(t)$ denotes the cost associated to these pulls. In the sequel, we will provide cost complexity guarantees for multi-fidelity instances $\\pmb{\\mu}$ that belong to the set $\\mathcal{M}_{\\mathrm{MF}}^{*}$ of multi-fidelity instances with a unique optimal arm, i.e., for which $|a_{\\star}(\\mu)|\\,=\\,1$ . We remark that for $M=1$ and $\\lambda_{m}\\,=1$ we recover the best arm identification problem in a classical bandit model, for which the cost complexity coincides with the sample complexity, $\\mathbb{E}_{\\mu}[\\tau_{\\delta}]$ . ", "page_idx": 2}, {"type": "text", "text": "Additional notation Given an integer $\\textit{n}\\in\\mathbb{N}$ , we denote by $\\Delta_{n}$ the $n$ -dimensional simplex. Furthermore, given $x,y\\,\\in\\,(0,1)$ , we define $\\mathrm{kl}(x,y)\\,=\\,x\\log(\\dot{x}/y)+(1-x)\\log((1-x)/\\Bar{(1-x)})$ $y)$ ). Given $(p,q)\\in\\Theta^{2}$ , we denote by $d(p,q)$ the Kullback-Leibler (KL) divergence between the distribution in the exponential family with mean $p$ and that with mean $q$ . We also write $d^{-}(x,y)=$ $d(x,y)\\mathbf{1}\\left\\{x\\geq y\\right\\}$ and $d^{+}(x,y)=d(x,y){\\mathbf1}\\,\\{x\\leq y\\}$ . Finally, we denote by $v(p)$ the variance of the distribution with mean p. ", "page_idx": 2}, {"type": "text", "text": "3 On the cost complexity of multi-fidelity best-arm identification ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we discuss the statistical complexity of identifying the best-arm in MF-BAI problems.   \nFormal proofs of the claims of this section are presented in Appendix B. ", "page_idx": 2}, {"type": "text", "text": "3.1 Lower bound on the cost complexity ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We present an instance-dependent lower bound on the expected cost-complexity. The lower bound uses the solution to an optimization problem, where the functions optimized quantify the trade-off between the information gained by pulling an arm at some fidelity and the cost of that fidelity. Since those functions also appear in our algorithm, we will now introduce notation for them. For all $\\omega\\in\\Delta_{K\\times M}$ and $\\pmb{\\mu}\\in\\dot{\\Theta}^{T}\\!\\times\\!M$ , we define ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{i,j}(\\boldsymbol{\\omega},\\boldsymbol{\\mu}):=\\operatorname*{inf}_{\\boldsymbol{\\theta}_{i}\\in\\mathrm{MF},\\ \\boldsymbol{\\theta}_{j}\\in\\mathrm{MF}}\\displaystyle\\sum_{\\boldsymbol{a}\\in\\{i,j\\}}\\sum_{m\\in[M]}\\omega_{a,m}\\frac{d(\\mu_{a,m},\\theta_{a,m})}{\\lambda_{m}}\\,,}\\\\ &{\\quad F(\\boldsymbol{\\omega},\\boldsymbol{\\mu}):=\\operatorname*{max}_{i\\in[K]}\\operatorname*{in}_{j\\neq i}f_{i,j}(\\boldsymbol{\\omega},\\boldsymbol{\\mu})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The quantity $f_{i,j}(\\omega,\\pmb{\\mu})$ is the dissimilarity according to a KL weighted by the costs between $\\pmb{\\mu}$ and the closest $\\pmb{\\theta}\\in\\Theta^{K\\times M}$ such that arms $i$ and $j$ satisfy the multi-fidelity constraints and $\\theta_{k}=\\mu_{k}$ for $k\\notin\\{i,j\\}$ , with arm $j$ better than arm $i$ . If $\\pmb{\\mu}\\in\\mathcal{M}_{\\mathrm{MF}}$ then that closest $\\theta$ is also in $\\mathcal{M}_{\\mathrm{MF}}$ but otherwise it might not be the case: if an arm $k\\notin\\{i,j\\}$ is not in MF for $\\pmb{\\mu}$ , then it is equally not in MF for $\\pmb{\\theta}$ . For $\\pmb{\\mu}\\in\\mathcal{M}_{\\mathrm{MF}}$ the maximum in the definition of $F$ is realized at the best arm $\\star,$ , as $\\operatorname*{min}_{a\\neq i}f_{i,a}(\\omega,\\mu)$ is zero for $i\\neq\\star$ . That is, $\\begin{array}{r}{F(\\omega,\\pmb{\\mu})=\\operatorname*{min}_{j\\neq\\star}f_{\\star,j}(\\omega,\\pmb{\\mu})}\\end{array}$ . We define $F$ with a maximum over $i$ and not with that last expression because we want to define it for all points in $\\Theta^{K\\times M}$ , even the points which are not in $\\mathcal{M}_{\\mathrm{MF}}$ . For those points, we could imagine different notions of best arm, for example, arg $\\operatorname*{max}_{k}\\mu_{k,M}$ , but the right one for our algorithm is the arm for which we have the most evidence (weighted by cost) to say that all other arms are not better. That arm is the argmax in our definition of $F$ . Given these definitions, we now introduce our new lower bound. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1. Let $\\delta\\ \\in\\ (0,1)$ . For any $\\delta$ -correct strategy, and any multi-fidelity bandit model $\\pmb{\\mu}\\in\\mathcal{M}_{\\mathrm{MF}}^{*}$ , it holds that: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\pmb{\\mu}}[c_{\\tau_{\\delta}}]\\geq C^{\\ast}(\\pmb{\\mu})\\log\\left(\\frac{1}{2.4\\,\\delta}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$\\begin{array}{r}{C^{*}(\\mu)^{-1}:=\\operatorname*{sup}_{\\omega\\in\\Delta_{K\\times M}}F(\\omega,\\mu)=\\operatorname*{sup}_{\\omega\\in\\Delta_{K\\times M}}\\operatorname*{min}_{a\\neq\\star}f_{\\star,a}(\\omega,\\mu)\\,.}\\end{array}$ ", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The quantity $C^{*}({\\pmb{\\mu}})$ describes the statistical complexity of an MF problem $\\pmb{\\mu}$ as the typical max-min game that appears in lower bounds for BAI problems [see, e.g., 8, 3]. Specifically, first, the max-player chooses a vector $\\omega\\in\\Delta_{K\\times M}$ , and then the min-player chooses a bandit model $\\pmb{\\theta}\\in\\mathcal{M}_{\\mathrm{MF}}$ in which the optimal arm is different, with the goal of minimizing the function $F(\\omega,\\pmb{\\mu})$ . Following the methods from previous work, the objective value for $\\omega$ and $\\pmb{\\theta}$ should be $\\begin{array}{r}{\\sum_{a\\in[K]}\\sum_{m\\in[M]}\\omega_{a,m}\\frac{d(\\mu_{a,m},\\theta_{a,m})}{\\lambda_{m}}}\\end{array}$ , featuring a sum over all arms and fidelities. However in the definition of $f_{i,a}(\\omega,\\pmb{\\mu})$ we restrict $\\pmb{\\theta}$ to be different from $\\pmb{\\mu}$ on only two arms. We can prove that if $\\pmb{\\mu}\\in\\mathcal{M}_{\\mathrm{MF}}$ , this gives the same objective value at the minimizing $\\pmb{\\theta}$ as the full sum. The difference will be important in our algorithm, which will compute that minimizer for points $\\hat{\\pmb{\\mu}}$ that do not belong to $\\mathcal{M}_{\\mathrm{MF}}$ . ", "page_idx": 3}, {"type": "text", "text": "A difference with standard BAI settings is that in Equation (1) each $\\omega\\in\\Delta_{K\\times M}$ should be interpreted as a vector of cost proportions that the max-player is investing (in expectation) in each arm-fidelity pair to identify the optimal arm \u00b5\u22c6,M.2 We can interpret the oracle weights \u03c9\u2217\u2208argmax\u2206K\u00d7M F as the optimal cost proportions that the agent should follow in order to identify $\\mu_{\\star,M}$ while minimizing the identification cost. To clarify the difference and the relationship between cost and pull proportions we notice that, given a cost proportion $\\omega$ , it is always possible to compute the pull proportions $\\pi(\\omega)\\in\\Delta_{K\\times M}$ that the agent should play in order to incur the costs proportions specified by $\\omega$ , and vice versa. More specifically, these relationships are described for each arm-fidelity pair by the following equations for every $a\\in[K]$ and $m\\in[\\bar{M}]$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi_{a,m}(\\omega)=\\frac{\\omega_{a,m}}{\\lambda_{m}}\\frac{1}{\\sum_{i\\in[K]}\\sum_{j\\in[M]}\\frac{\\omega_{i,j}}{\\lambda_{j}}}\\qquad\\omega_{a,m}(\\pi)=\\frac{\\lambda_{m}\\pi_{a,m}}{\\sum_{i\\in[K]}\\sum_{j\\in[M]}\\lambda_{j}\\pi_{i,j}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As a direct consequence, it is possible to rewrite $C^{*}(\\mu)^{-1}$ as a function of $\\pi$ , the pull proportions. Doing so reveals that the minimizer $\\pmb{\\theta}$ in $f_{\\star,j}$ does not depend on the costs: it is also the minimizer of $\\begin{array}{r}{\\sum_{a\\in\\{i,j\\},m\\in[M]}\\pi_{a,m}d(\\mu_{a,m},\\theta_{a,m})}\\end{array}$ . While the agent optimizes the cost proportions $\\omega$ to get the best possible information/cost ratio, the min-player minimizes only the information available to the algorithm to tell $\\pmb{\\mu}$ and $\\pmb{\\theta}$ apart. Finally, we notice that $F(\\omega,\\mu)$ is concave in $\\omega^{3}$ but $F(\\omega(\\pi),\\mu)$ is not concave in $\\pi$ . As we shall see in Section 4, this difference will play a crucial role in constructing an asymptotically optimal algorithm. ", "page_idx": 3}, {"type": "text", "text": "The formulation of the lower bound as a game where one player maximizes an information/cost ratio while the other player minimizes information makes our result close to lower bounds for regret minimization like the one of [5], where the (unknown) gap of an arm plays the role of the cost. ", "page_idx": 3}, {"type": "text", "text": "Comparison to previous work The only known lower bound for the multi-fidelity BAI problem is the one presented in [25]. That same bound was then shown in [31]. The bound from those previous works is looser than Theorem 3.1. For example, in a two-arms bandit with a single fidelity (denoted by $M$ ) and Gaussian rewards with variance 1, the bound from previous work is $\\lambda_{M}(\\dot{\\mu_{1,M}}-\\mu_{2,M})^{\\dot{-}2}\\log(1/2.4\\delta)$ , while our lower bound is $8\\lambda_{M}(\\mu_{1,M}-\\mu_{2,\\hat{M}}\\stackrel{\\cdot}{)}^{-2}\\log(1/2.4\\delta)$ Furthermore, on particular instances with 2 arms and 2 fidelity, we can prove that our lower bound improves by a factor $\\lambda_{M}/\\lambda_{1}$ , which can be arbitrarily large (See Appendix B.2). More generally, the proof of the previous lower bounds exhibits a particular point in the alternative, which makes it always looser than our bound which features an infimum over all points. Theorem 3.1 is also optimal in the regime $\\delta\\rightarrow0$ since it is matched by the algorithm we introduce in the next section. ", "page_idx": 3}, {"type": "text", "text": "3.2 Sparsity of the oracle weights: a tight concept of optimal fidelity ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We conclude our study of the lower bound by further analyzing the optimal allocation $\\omega^{*}$ . Unlike in the standard best arm identification problem, we did not find an efficient algorithm to compute it, which prevents us from using a Track-and-Stop-like approach [8]. Nevertheless, we will explain in the next section how to efficiently compute the $f_{i,j}$ functions and their gradient. These computations are crucial for our algorithm but also allow us to prove our next result about the possible sparsity of $\\omega^{*}$ . For each arm $a\\in[K]$ , it is not difficult to show that there must exist some fidelity $m\\in[M]$ for which $\\omega_{a,m}^{*}>0$ (Lemma B.2). However, as the following result highlights, in most cases, only one fidelity per arm has non-zero weight. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2. Let $\\textstyle\\Delta_{K\\times M}^{*}(\\pmb{\\mu}):=\\operatorname*{argmax}_{\\pmb{\\omega}\\in\\Delta_{K\\times M}}F(\\pmb{\\omega},\\pmb{\\mu})$ and ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widetilde{M}_{\\mathrm{MF}}:=\\left\\{\\mu\\in\\mathcal{M}_{\\mathrm{MF}}^{*}:\\exists i\\in[K],\\exists m_{1},m_{2}\\in[M]^{2},\\exists\\omega^{*}\\in\\Delta_{K\\times M}^{*}(\\mu):\\omega_{i,m_{1}}^{*}>0,\\omega_{i,m_{2}}^{*}>0\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The set $\\widetilde{\\mathcal{M}}_{\\mathrm{MF}}$ is a subset of $\\mathbb{R}^{K\\times M}$ whose Lebesgue measure is zero. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2 implies that in almost all multi-fidelity bandits, for any $\\omega^{\\ast}\\in\\Delta_{K\\times M}^{\\ast}(\\pmb{\\mu})$ and each arm $a\\in[K]$ , there exists a single fidelity $m_{a}^{*}\\in[M]$ for which $\\omega_{a,m_{a}^{*}}^{*}>0$ holds. However, we note that this result does not offer an easy way to compute these optimal arm-dependent fidelities.4 Nevertheless, as we shall see in the next section, our algorithm does not actually require identifying these optimal fidelity levels to enjoy optimality guarantees. ", "page_idx": 4}, {"type": "text", "text": "Finally, we remark that existing MF-BAI works [25, 31] already proposed notions of optimal, armdependent fidelity that the agent should employ to identify the optimal arm $\\star$ . Nevertheless, as we verify in Appendix B.5, these concepts do not comply with the concept of optimal fidelity that arises from the tight lower bound of Theorem 3.1. In other words, there exist bandit models $\\pmb{\\mu}$ in which following these alternative concepts of optimal fidelity leads to sub-optimal performance. ", "page_idx": 4}, {"type": "text", "text": "4 The multi-fidelity sub-gradient ascent algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We present our solution for solving MF-BAI problems, an algorithm called Multi-Fidelity SubGradient Ascent (MF-GRAD). Its pseudocode can be found in Algorithm 1. All proofs for this section are presented in Appendix C. ", "page_idx": 4}, {"type": "text", "text": "A reader familiar with the literature on BAI algorithms inspired from lower bounds like Theorem 3.1 may have the natural idea of simply using the Track-and-Stop algorithm [8] or the related game-based algorithm of [4]. Those algorithms can\u2019t be directly applied here, first because of the costs: we want to bound the cost complexity, not the stopping time, and adapting those methods to costs is not trivial. Furthermore, Track-and-Stop (even in the cost-aware variant of [13]) would require the computation of the optimal cost proportions at ${\\hat{\\pmb{\\mu}}}(t)$ , which is a max-min problem for which we don\u2019t have an efficient algorithm. Our solution is inspired by the gradient ascent algorithm of [22], which requires computing gradients of $F$ (hence only a minimization problem and not a max-min). The same innovations required to extend this method to the multi-fidelity case could likely allow us to adapt the algorithm of [4], or the exploration part of the regret-minimizing algorithm of [5]. ", "page_idx": 4}, {"type": "text", "text": "Let us introduce some auxiliary notation. Let $\\overline{{\\omega}}\\in\\Delta_{K\\times M}$ be the uniform vector $\\left(\\frac{1}{K M},\\cdot\\cdot\\cdot,\\frac{1}{K M}\\right)$ . For all t \u2208N, we define Clipt(x) = min{xa,m, G t} a\u2208[K],m\u2208[M] for an arbitrary constant $G>0$ . We also define $\\begin{array}{r}{\\alpha_{t}=\\frac{1}{\\sqrt{t}}}\\end{array}$ and $\\textstyle\\gamma_{t}={\\frac{1}{4{\\sqrt{t}}}}$ . Finally, for all $t\\in\\mathbb{N}$ , we denote by $\\b{C}(t)\\in\\mathbb{R}^{K M}$ the vector whose $(a,m)$ -th dimension is given by $C_{a,m}(t)$ . We now present Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "Sampling rule After a first initialization phase in which the algorithm pulls each arm at each fidelity once (Line 1), the agent starts its sub-gradient ascent routine. More specifically at each iteration $t\\in\\mathbb{N}$ , the agent first computes the vector $\\tilde{\\omega}(t+1)$ using the Exponential Weights algorithm on the sequence of gain functions $\\begin{array}{r}{\\{g_{s}\\}_{s=1}^{t}:=\\{\\mathrm{Clip}_{s}\\left(\\left(\\sum_{a,m}\\lambda_{m}\\tilde{\\pi}_{a,m}(s)\\right)\\nabla F(\\tilde{\\omega}(s),\\hat{\\mu}(s))\\right)\\}_{s=1}^{t},}\\end{array}$ where $\\tilde{\\pmb{\\pi}}(t):=\\pmb{\\pi}(\\tilde{\\pmb{\\omega}}(t))$ represents the pull-proportions induced by $\\tilde{\\omega}(t)$ and $\\nabla F(\\tilde{\\omega}(s),\\hat{\\pmb\\mu}(\\acute{s}))$ denotes a sub-gradient of $\\dot{F}(\\omega,\\mu)$ w.r.t $\\omega$ (Line 3). Neglecting for a moment the clipping function and the term ", "page_idx": 4}, {"type": "text", "text": "1: Initialization. Pull each arm at each fidelity once, and set $\\tilde{\\omega}(t)=\\bar{\\omega}$ for all $t\\in\\{1,\\ldots,K M\\}$   \n2: Sampling Rule for $t\\geq K M$   \n3: Sub-gradient Ascent ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{\\omega}(t+1)\\in\\operatornamewithlimits{a r g m a x}_{\\omega\\in\\Delta_{K\\times M}}\\alpha_{t+1}\\sum_{s=K M}^{t}\\omega\\cdot\\mathrm{Clip}_{s}\\left(\\left(\\sum_{a,m}\\lambda_{m}\\tilde{\\pi}_{a,m}(s)\\right)\\nabla F(\\tilde{\\omega}(s),\\hat{\\mu}(s))\\right)-\\mathrm{kl}(\\omega,\\overline{{\\omega}})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4: From Costs to Pulls ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{\\pi}_{a,m}(t+1)=\\frac{\\tilde{\\omega}_{a,m}(t+1)}{\\lambda_{m}}\\frac{1}{\\sum_{i\\in[K]}\\sum_{j\\in[M]}\\frac{\\tilde{\\omega}_{i,j}(t+1)}{\\lambda_{j}}}\\quad\\forall a\\in[K],m\\in[M]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{S:\\;F o r c e d\\;E x p l o r a t i o n}\\qquad\\pi^{\\prime}(t+1)=(1-\\gamma_{t})\\widetilde{\\pi}(t+1)+\\gamma_{t}\\overline{{\\omega}}}\\\\ &{\\mathsf{e:\\;C u m u l a t i v e\\;T r a c k i n g}\\qquad(A_{t+1},M_{t+1})\\in\\arg\\!\\operatorname*{max}_{(a,m)\\in[K]\\times[M]}\\sum_{s=1}^{t}\\pi_{a,m}^{\\prime}(s)-N_{a,m}(t)}\\\\ &{\\mathsf{7:\\;S t o p p i n g\\;R u l e\\;}\\qquad\\tau_{\\delta}=\\operatorname*{inf}\\big\\{t\\ge K M:\\operatorname*{max}_{i\\in[K]}\\operatorname*{min}_{j\\ne i}f_{i,j}(C(t),\\hat{\\mu}(t))\\ge\\beta_{t,\\delta}\\big\\}}\\\\ &{\\mathsf{B:\\;D e c i s i o n\\;R u l e\\;}\\qquad\\hat{a}_{\\tau_{\\delta}}\\in\\arg\\!\\operatorname*{max}_{i\\in[K]}\\operatorname*{min}_{j\\ne i}f_{i,j}(C(t),\\hat{\\mu}(t))}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$\\begin{array}{r}{\\tilde{c}(s):=\\left(\\sum_{a,m}\\lambda_{m}\\tilde{\\pi}_{a,m}(s)\\right)}\\end{array}$ (these terms are present mainly for technical reasons), this step can be interpreted, from an intuitive perspective, as finding a sequence of weights $\\{\\tilde{\\omega}(t)\\}_{t}$ that minimizes the regret on the sequence of empirical losses $F(\\omega^{\\ast},\\hat{\\pmb{\\mu}}(s))-F(\\tilde{\\omega}(s),\\hat{\\pmb{\\mu}}(s))$ . 5 At this point, once $\\tilde{\\omega}(t+1)$ is computed, Algorithm 1 will convert these cost proportions into pull proportions while adding some forced exploration (Line 4-5), and then, it applies a standard cumulative tracking procedure [8] in the pull-proportion space so to ensure that $\\begin{array}{r}{N_{a,m}(t)\\approx\\sum_{s=1}^{t}\\pi_{a,m}^{\\prime}(s)}\\end{array}$ (Line 6). ", "page_idx": 5}, {"type": "text", "text": "Stopping and decision rule Finally, the algorithm applies a generalized likelihood ratio (GLR) test to decide when to stop (Line 7). For $\\bar{i},j\\in[\\bar{K}]$ , $f_{i,j}(\\bar{C}(t),\\hat{\\pmb\\mu}(\\bar{t}))$ can be interpreted a GLR statistics for comparing two classes: $\\Theta^{K M}$ versus $\\{\\pmb\\theta\\mid\\theta_{i}\\in\\mathrm{MF}$ , $\\theta_{j}\\in{\\bf{M F}}$ , $\\theta_{j,M}\\geq\\theta_{i,M}\\}$ . If that GLR is large enough (if it exceeds a threshold $\\beta_{t,\\delta}.$ ), we can reject the hypothesis that $\\pmb{\\mu}$ belongs to the second class. If there is an arm $i$ for which we can reject the alternative class for all $j\\neq i$ , we have rejected all $\\pmb{\\theta}\\in\\mathcal{M}_{\\mathrm{MF}}$ where $i$ is not the best arm and we can safely stop and return the answer $\\hat{a}_{\\tau_{\\delta}}=i$ . Since each $f_{i,j}$ is expressed as a sum of only two arms and $M$ fidelities, it is possible to show that choosing $\\beta_{t,\\delta}\\approx\\operatorname{log}(K/\\delta)+2M\\log\\left(\\log(t)+1\\right)$ (see its exact expression in (31)) guarantees the correctness of the test, namely that $\\mathbb{P}_{\\mu}(\\hat{a}_{\\tau_{\\delta}}\\neq\\star)\\leq\\delta$ holds (Proposition C.13). ", "page_idx": 5}, {"type": "text", "text": "4.1 Theoretical guarantees ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "At this point, we are ready to state the main theoretical result on the performance of our algorithm. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1. For any multi-fidelity bandit model $\\pmb{\\mu}\\in\\mathcal{M}_{\\mathrm{MF}}$ , Algorithm $^{\\,l}$ using the threshold $\\beta_{t,\\delta}$ given in (31) is $\\delta$ -correct and satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\delta\\rightarrow0}\\operatorname*{sup}_{\\log(1/\\delta)}\\leq C^{\\ast}(\\pmb{\\mu}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "As we can see from Theorem 4.1, Algorithm 1 is asymptotically optimal, meaning that it matches the lower bound we presented in Theorem 3.1 for the asymptotic regime of $\\delta\\rightarrow0$ . ", "page_idx": 5}, {"type": "text", "text": "Comparison with existing MF-BAI algorithms We conclude this section by comparing our results with the literature [25, 31]. First, [25] and [31] rely on additional assumptions that play a crucial role both for the algorithm design and the resulting theoretical guarantees. In [25], the authors enforce an additional and intricate structural assumption on the relationships between $\\lambda$ \u2019s and $\\xi$ \u2019s (see Assumption 1 in [25]). In [31], instead, the authors assume additional knowledge expressed as an upper bound on $\\mu_{\\star,M}$ and a lower bound on $\\mathrm{argmax}_{i\\neq\\star}\\,\\mu_{i,M}$ . For both works, whenever these assumptions are not satisfied (i.e., $\\lambda$ \u2019s and $\\xi$ \u2019s do not respect Assumption 1 in [25], and the knowledge on $\\mu_{\\star,M}$ , $\\mathrm{argmax}_{i\\neq\\star}\\,\\mu_{i,M}$ is imprecise/not available), the theoretical guarantees offered by existing algorithms are arbitrarily sub-optimal. On the other hand, our algorithm requires no additional assumptions and is the only one that matches exactly the cost complexity lower bound. Indeed, neither the cost upper bound of [25] nor the one of [31] matches the lower bound of Theorem 3.1, even when their additional hypotheses are satisfied. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.2 Computing the gradient of $F(\\omega,\\mu)$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Algorithm 1 requires computing a sub-gradient of $F(\\omega,\\mu)$ . Notably, we remark that this is needed for a generic $\\pmb{\\mu}\\in\\Theta^{K M}$ , as ${\\hat{\\pmb{\\mu}}}(t)$ might violate the fidelity constraints due to inaccurate estimations or degenerate cases in which the multi-fidelity constraints are attained with equality. In this section, we provide an efficient algorithm for the computation of the sub-gradient that arises from a more in-depth study of the function $F(\\omega,\\mu)$ . To this end, we begin by presenting some intermediate characterization of the functions $f_{i,j}(\\omega,\\pmb{\\mu})$ that define $F(\\omega,\\mu)$ . ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.2. Consider ${\\pmb{\\mu}}\\in\\Theta^{K M}$ and $\\omega\\in\\Delta_{K\\times M}$ . Define for $k\\in[K]$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\psi_{k}^{*}:=\\underset{\\psi\\in\\mathbb{R}}{\\mathrm{argmin}}\\,\\sum_{m=1}^{M}\\omega_{k,m}\\frac{d^{-}(\\mu_{k,m},\\psi+\\xi_{m})+d^{+}(\\mu_{k,m},\\psi-\\xi_{m})}{\\lambda_{m}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then, the following holds: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{i,j}(\\omega,\\mu)=\\displaystyle\\sum_{k\\in\\{i,j\\}}\\displaystyle\\sum_{m=1}^{M}\\omega_{k,m}\\frac{d^{-}(\\mu_{k,m},\\psi_{k}^{*}+\\xi_{m})+d^{+}(\\mu_{k,m},\\psi_{k}^{*}-\\xi_{m})}{\\lambda_{m}}\\quad i f\\psi_{j}^{*}>\\psi_{i}^{*}}\\\\ &{f_{i,j}(\\omega,\\mu)=\\displaystyle\\operatorname*{inf}_{\\eta\\in\\mathbb{R}}\\displaystyle\\sum_{k\\in\\{i,j\\}}\\sum_{m=1}^{M}\\omega_{k,m}\\frac{d^{-}(\\mu_{k,m},\\eta+\\xi_{m})+d^{+}(\\mu_{k,m},\\eta-\\xi_{m})}{\\lambda_{m}}\\quad o t h e r w i s e.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We further introduce $\\eta_{i,j}^{*}$ as the minimizer in the expression in (7) 6. When $\\mu\\in\\mathcal{M}_{\\mathrm{MF}}$ , we can show that $\\psi_{k}^{*}=\\mu_{k,M}$ for all $\\hat{k}$ and due to the multi-fidelity constraints the expression in (6) is always equal to zero. Hence in both cases $f_{i,j}(\\omega,\\pmb{\\mu})$ is equal to the expression in (7), which can be rewritten ", "page_idx": 6}, {"type": "equation", "text": "$$\nf_{i,j}(\\omega,\\mu)=\\mathbf{1}\\left(\\mu_{i,M}\\geq\\mu_{j,m}\\right)\\sum_{k\\in\\{i,j\\}}\\sum_{m=1}^{M}\\omega_{k,m}\\frac{d^{-}(\\mu_{k,m},\\eta_{i,j}^{*}+\\xi_{m})+d^{+}(\\mu_{k,m},\\eta_{i,j}^{*}-\\xi_{m})}{\\lambda_{m}}\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This quantity can be interpreted as the transportation cost for making $\\mu_{j,M}$ larger than $\\mu_{i,M}$ . When $\\mu_{i}\\notin\\mathrm{MF}$ or $\\mu_{j}\\notin\\mathrm{MF}$ , if $\\psi_{j}^{*}>\\psi_{i}^{*}$ , $f_{i,j}(\\omega,\\pmb{\\mu})$ is equal to the expression (6) that can be interpreted as a transportation cost with an alternative in which $i$ and $j$ satisfy the multi-fidelity constraints. ", "page_idx": 6}, {"type": "text", "text": "Using this preliminary result, we provide a precise expression for the sub-gradient of $F(\\omega,\\mu)$ ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.3. Consider ${\\pmb{\\mu}}\\in\\Theta^{K M}$ and $\\omega\\in\\Delta_{K\\times M}$ such that $F(\\omega,\\mu)>0$ holds. Let $(i,a)\\in[K]^{2}$ be a pair of arms that attains the max-min value in Equation (2). Then a sub-gradient $\\nabla F(\\omega,\\pmb{\\mu})$ of $F(\\omega,\\mu)$ w.r.t. to $\\omega$ is given by one of the two following expressions: for $j\\in\\{a,i\\}$ and $m\\in[M]$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla F(\\omega,\\mu)_{j,m}=\\frac{d^{+}(\\mu_{j,m},\\eta_{i,a}^{*}-\\xi_{m})+d^{-}(\\mu_{j,m},\\eta_{i,a}^{*}+\\xi_{m})}{\\lambda_{m}}\\ \\ \\mathrm{if}\\ \\psi_{i}^{*}\\geq\\psi_{a}^{*}\\,,}\\\\ &{\\nabla F(\\omega,\\mu)_{j,m}=\\frac{d^{+}(\\mu_{j,m},\\psi_{j}^{*}-\\xi_{m})+d^{-}(\\mu_{j,m},\\psi_{j}^{*}+\\xi_{m})}{\\lambda_{m}}\\ \\ \\mathrm{otherwise}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "That sub-gradient $\\nabla F(\\omega,\\pmb{\\mu})$ is $0$ in all the remaining $K M-2M$ dimensions. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.3 shows how to compute a sub-gradient of $F(\\omega,\\pmb{\\mu})$ under the mild assumption that $F(\\omega,\\mu)>0$ .7 More specifically, it is sufficient to consider the pair $(i,a)$ that attains the max-min value in Equation (2), and then test whether $\\psi_{i}^{*}\\geq\\psi_{a}^{*}$ holds to choose which expression to use among Equations (8) and (9). An interesting interpretation of the sub-gradient expression is that, whenever $\\psi_{i}^{*}\\geq\\psi_{a}^{*}$ , the sub-gradient is pointing toward the direction of the space that aims at increasing the information to discriminate the eventual optimality of arm $a$ against $i$ . On the other hand, whenever $\\psi_{a}^{*}>\\psi_{i}^{*}$ holds, the sub-gradient points towards the direction of minimizing errors in the multi-fidelity constraints for arm $i$ and arm $a$ (if any). ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Computing the sub-gradient efficiently To conclude, we notice that to compute a sub-gradient, it is required to compute $\\psi_{k}^{*}$ for all arm $k$ and $\\eta_{i,j}^{*}$ for all pairs of arms such that $\\psi_{i}^{*}\\geq\\psi_{j}^{*}$ . Using their definitions, this will require solving $\\mathcal{O}(K^{2})$ one-dimensional optimization problems of functions that involve $\\mathcal{O}(M)$ variables, which leads to a computational complexity which is roughly $O(K^{2}M n)$ , where $n$ is the number of iterations of the convex solver. In the following, we show that it is possible to exploit the structure of the $f_{i,j}$ \u2019s to obtain an algorithm whose total complexity is $O(K^{2}\\bar{M}^{2})$ and that does not suffer from any approximation error due to the optimization procedure. Specifically, we now present a result that shows how to compute $\\eta_{i,j}^{*}$ . A similar result holds also for $\\psi_{k}^{*}$ and is deferred to Appendix $\\mathbf{C}$ . ", "page_idx": 7}, {"type": "text", "text": "Lemma 4.4. Consider $\\pmb{\\mu}\\,\\in\\,\\Theta^{K M}$ and $\\omega\\;\\in\\;\\Delta_{K\\times M}$ such that $f_{i,j}(\\omega,\\mu)\\;>\\;0$ . Suppose that $\\psi_{i,}^{*}\\geq\\psi_{j}^{*}$ holds. Then, there exists a unique minimizer $\\eta_{i,j}^{*}(\\omega)$ of Equation (7) which is the unique solution of the following equation of $\\eta$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\eta=\\frac{\\sum_{a\\in\\{i,j\\}}\\sum_{m}\\frac{\\omega_{a,m}}{\\lambda_{m}}\\left(\\overline{{k}}_{a,m}(\\eta)\\frac{\\mu_{a,m}+\\xi_{m}}{v(\\eta-\\xi_{m})}+\\underline{{k}}_{a,m}(\\eta)\\frac{\\mu_{a,m}-\\xi_{m}}{v(\\eta+\\xi_{m})}\\right)}{\\sum_{a\\in\\{i,j\\}}\\sum_{m}\\frac{\\omega_{a,m}}{\\lambda_{m}}\\left(\\overline{{k}}_{a,m}(\\eta)\\frac{1}{v(\\eta-\\xi_{m})}+\\underline{{k}}_{a,m}(\\eta)\\frac{1}{v(\\eta+\\xi_{m})}\\right)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\overline{{k}}_{a,m}(x)=\\mathbf{1}\\{x\\geq\\mu_{a,m}+\\xi_{m}\\}$ and $\\underline{{k}}_{a,m}(x)=\\mathbf{1}\\{x\\leq\\mu_{i,m}-\\xi_{m}\\}.$ . ", "page_idx": 7}, {"type": "text", "text": "From Lemma 4.4, to compute $\\eta_{i,j}^{*}$ it is sufficient to find the unique solution to the fixed point equation given in (10). To do this efficiently, we observe that the right hand side of Equation (10) depends on $\\eta$ only for the presence of the indicator functions $\\overline{{k}}_{a,m}(\\eta)$ and $\\underline{{k}}_{a,m}(\\eta)$ , which can only take a finite number of values. Hence, it is sufficient to evaluate the right-hand side at an arbitrary point within a given interval where the values of the indicator functions do not change. If the resulting value is within the considered interval, then this value is our fixed point. Since there are at most $\\mathcal{O}(M)$ candidate fixed points, this procedure takes at most $\\mathcal{O}(M^{2})$ steps. ", "page_idx": 7}, {"type": "text", "text": "Computational complexity remark It follows that the per-iteration computational complexity of Algorithm 1 is $O\\left(K^{\\bar{2}}M^{2}\\right)$ . The computationally efficient technique explained above indeed applies not only to the sampling rule but also to the stopping and the decision rules.8 ", "page_idx": 7}, {"type": "text", "text": "5 Numerical experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conclude this work by presenting numerical simulations whose goal is to show the empirical benefits of our approach. We compare MF-GRAD against IISE [25], and the gradient approach of [22] that simply does BAI using samples collected at fidelity $M$ . We will refer to this additional baseline as GRAD. In the following, we avoided the comparison with the multi-fidelity algorithms in [31] as we ran into issues when doing experiments. We elaborate more on this point in Appendix D.6, where we provide numerical evidence of the fact those algorithms might fail at stopping, together with an argument that shows a mistake in the proofs of [31]. ", "page_idx": 7}, {"type": "text", "text": "Given this setup, first, we test all methods on a $4\\times5$ multi-fidelity bandit with Gaussian arms that have been randomly generated, using a risk parameter $\\delta=0.01$ . Due to space constraints and for the sake of exposition, we refer the reader to Appendix D.1 for the value of $\\pmb{\\mu}$ , $\\xi$ \u2019s and $\\lambda$ \u2019s and details on the stopping rules calibration. We report the empirical distribution of the resulting cost complexities in Figure 1. As one can verify, MF-GRAD obtains the most competitive performance. Experiments on additional $4\\times5$ bandits that are reported in Appendix D.3 provide a similar conclusion. ", "page_idx": 7}, {"type": "text", "text": "Furthermore, to illustrate the sub-optimality of IISE and GRAD from an intuitive perspective, we test our algorithm on a simple $5\\times2$ instance that allows to easily understand why existing methods underperform MF-GRAD. Specifically, we consider $\\mu_{i}=[0.4,0.5]$ for all $i\\in$ [4], $\\mu_{5}=[0.5,0.6]$ , $\\lambda\\,=\\,[0.5,5]$ , $\\xi\\,=\\,[0.1,0]$ and we report the cost complexity of the three algorithms in Figure 2. In this case, we can prove that the optimal fidelity is sparse on fidelity $m=1$ for $i\\in$ [4], and on fidelity $m\\,=\\,2$ for arm 5. Furthermore, thanks to the symmetry of the problem, it is possible to show that $\\omega_{i}^{*}=[0.09621,0]$ for all $i\\in[4]$ , and $\\omega_{5}^{*}=[0,0.61516]$ (see Appendix D.1). As one can see, IISE obtains the worst performance in this domain. The reason is that the concept of optimal fidelity on which IISE relies is sub-optimal (i.e., according to the design principle of IISE, the optimal fidelity is $m=2$ for all arms), and the algorithm, in practice, will discard sub-optimal arms using samples that have been collected only at fidelity $m=2$ . Nevertheless, this will only happen after a first period in which IISE tries to exploit (unsuccessfully) data at fidelity $m=1$ . GRAD, on the other hand, obtains sub-optimal performances since although most of the budget should be spent on fidelity 2 (as $\\omega^{*}{}_{5,2}=0.61516)$ , it never pulls the cheapest (and optimal) fidelity for arms $i\\in$ [4]. Finally, MF-GRAD, on the other hand, obtains the most competitive performance since, as learning progresses, its empirical cost proportions eventually approach the one prescribed by $\\omega^{*}$ . To verify this behavior, we removed the stopping rule from MF-GRAD, and let the algorithm run for $10^{5}$ iterations. In Figure 3, we report the entire evolution of the cost proportions during learning. As one can appreciate, at the end of this process, the empirical cost proportions of MF-GRAD are approaching the one described by $\\omega^{*}$ . 9. Finally, we also refer the reader to Appendix $\\mathrm{D}$ for additional results (e.g., additional domains, smaller regimes of $\\delta$ ) and further insights. ", "page_idx": 7}, {"type": "image", "img_path": "gKMTM1i8Ew/tmp/43c90437422c48e87c05394eb40670df5f57a9abc5e674b5247538ee7d26deba.jpg", "img_caption": ["Figure 1: Empirical cost complexity for 1000 runs Figure 2: Empirical cost complexity for 1000 runs times with $\\delta=0.01$ on the $4\\times5$ multi-fidelity bandit. times with $\\delta=0.01$ on the $5\\times2$ multi-fidelity bandit. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "gKMTM1i8Ew/tmp/152c07bbab7f6dd21ea7ae6ce4864d7417da7552f132953a0316ade4e7c83227.jpg", "img_caption": ["Figure 3: Empirical cost proportions of MF-GRAD for 100000 iterations on the $5\\times2$ bandit model. Results are average over 100 runs and shaded area report $95\\%$ confidence intervals. Empirical cost proportions of a certain arm are plotted with the same color. Cost proportions at fidelity 1 and 2 are visualized with a circle and a squared respectively. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "For fixed-confidence best arm identification in multi-fidelity bandits, we presented a lower bound on the cost complexity and an algorithm with a matching upper bound in the regime of high confidence. The algorithm uses features of the lower bound optimization problem in order to compute its updates efficiently. Unlike prior work, it does not require any assumption or prior knowledge on the bandit instance. Our work also confirmed the existence in most cases of an \u201coptimal fidelity\u201d to explore each arm in the asymptotic regime, and revealed that the intuitive such notions proposed in prior work were inaccurate. Yet, our algorithm does not need to identify these optimal fidelities in order to be asymptotically optimal. ", "page_idx": 9}, {"type": "text", "text": "This raises the following question: could the performance of the algorithm be enhanced by exploiting the sparsity pattern? We conjecture that estimating the optimal fidelities accurately may actually be harder than identifying the best arm. However, leveraging some sufficient conditions for $w_{a,m}^{*}=0$ (such as the ones given in Proposition B.6) to eliminate some fidelities and reduce the support of the forced exploration component of the algorithm seems a promising idea. A limitation of our current analysis is that it only provides asymptotic guarantees in the high confidence regime, although our experiments reveal good performance for moderate values of $\\delta$ . In future work, we will seek a better understanding of the moderate confidence regime [30]. To this end, we may leverage some proof techniques from other works using online optimization that obtain finite-time bounds [4, 5]. On the lower bound side, while $C^{*}({\\pmb{\\mu}})$ essentially scales with $K$ due to the sparsity pattern, an interesting open question is whether there is a worse case ${\\mathcal{O}}(K M)$ scaling in the moderate confidence regime, indicating that all fidelities do need to be explored at least a constant amount of times. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was done while Riccardo Poiani was visiting the Scool team of Inria Lille. He acknowledges the funding of a MOB-LIL-EX grant from the University of Lille. R\u00e9my Degenne and Emilie Kaufmann acknowledge the funding of the French National Research Agency under the project FATE (ANR22-CE23-0016-01) and the PEPR IA FOUNDRY project (ANR-23-PEIA-0003). Alberto Maria Metelli and Marcello Restelli acknowledge the funding of the European Union \u2013 Next Generation EU within the project NRPP M4C2, Investment 1.,3 DD. 341 - 15 march 2022 \u2013 FAIR \u2013 Future Artificial Intelligence Research \u2013 Spoke 4 - PE00000013 - D53C22002380006. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] J-Y. Audibert, S. Bubeck, and R. Munos. Best Arm Identification in Multi-armed Bandits. In Proceedings of the 23rd Conference on Learning Theory, 2010. [2] Olivier Capp\u00e9, Aur\u00e9lien Garivier, Odalric-Ambrym Maillard, R\u00e9mi Munos, and Gilles Stoltz. Kullback-leibler upper confidence bounds for optimal sequential allocation. The Annals of Statistics, pages 1516\u20131541, 2013. [3] R\u00e9my Degenne and Wouter M Koolen. Pure exploration with multiple correct answers. Advances in Neural Information Processing Systems, 32, 2019. [4] R\u00e9my Degenne, Wouter M Koolen, and Pierre M\u00e9nard. Non-asymptotic pure exploration by solving games. Advances in Neural Information Processing Systems, 32, 2019. [5] R\u00e9my Degenne, Han Shao, and Wouter Koolen. Structure adaptive algorithms for stochastic bandits. In International Conference on Machine Learning, pages 2443\u20132452. PMLR, 2020. [6] E. Even-Dar, S. Mannor, and Y. Mansour. Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems. Journal of Machine Learning Research, 7:1079\u20131105, 2006. [7] C\u00f4me Fiegel, Victor Gabillon, and Michal Valko. Adaptive multi-fidelity optimization with fast learning rates. In International Conference on Artificial Intelligence and Statistics, pages 3493\u20133502. PMLR, 2020. [8] Aur\u00e9lien Garivier and Emilie Kaufmann. Optimal best arm identification with fixed confidence. In Conference on Learning Theory, pages 998\u20131027. PMLR, 2016. [9] Deng Huang, Theodore T Allen, William I Notz, and R Allen Miller. Sequential kriging optimization using multiple-fidelity evaluations. Structural and Multidisciplinary Optimization, 32:369\u2013382, 2006.   \n[10] K. Jamieson, M. Malloy, R. Nowak, and S. Bubeck. lil\u2019UCB: an Optimal Exploration Algorithm for Multi-Armed Bandits. In Proceedings of the 27th Conference on Learning Theory, 2014.   \n[11] Kevin G. Jamieson and Ameet Talwalkar. Non-stochastic best arm identification and hyperparameter optimization. In AISTATS, 2016.   \n[12] Shivaram Kalyanakrishnan, Ambuj Tewari, Peter Auer, and Peter Stone. Pac subset selection in stochastic multi-armed bandits. In ICML, volume 12, pages 655\u2013662, 2012.   \n[13] Kellen Kanarios, Qining Zhang, and Lei Ying. Cost aware best arm identification. arXiv preprint arXiv:2402.16710, 2024.   \n[14] Kirthevasan Kandasamy, Gautam Dasarathy, Junier Oliva, Jeff Schneider, and Barnabas Poczos. Multi-fidelity gaussian process bandit optimisation. Journal of Artificial Intelligence Research, 66:151\u2013196, 2019.   \n[15] Kirthevasan Kandasamy, Gautam Dasarathy, Junier B Oliva, Jeff Schneider, and Barnab\u00e1s P\u00f3czos. Gaussian process bandit optimisation with multi-fidelity evaluations. Advances in neural information processing systems, 29, 2016.   \n[16] Kirthevasan Kandasamy, Gautam Dasarathy, Barnabas Poczos, and Jeff Schneider. The multifidelity multi-armed bandit. Advances in neural information processing systems, 29, 2016.   \n[17] Kirthevasan Kandasamy, Gautam Dasarathy, Jeff Schneider, and Barnab\u00e1s P\u00f3czos. Multi-fidelity bayesian optimisation with continuous approximations. In International conference on machine learning, pages 1799\u20131808. PMLR, 2017.   \n[18] Emilie Kaufmann, Olivier Capp\u00e9, and Aur\u00e9lien Garivier. On the complexity of best arm identification in multi-armed bandit models. Journal of Machine Learning Research, 17:1\u201342, 2016.   \n[19] Emilie Kaufmann and Wouter M Koolen. Mixture martingales revisited with applications to sequential tests and confidence intervals. Journal of Machine Learning Research, 22(246):1\u201344, 2021.   \n[20] Tor Lattimore and Csaba Szepesvari. Bandit Algorithms. Cambridge University Press, 2019.   \n[21] Shibo Li, Wei Xing, Robert Kirby, and Shandian Zhe. Multi-fidelity bayesian optimization via deep neural networks. Advances in Neural Information Processing Systems, 33:8521\u20138531, 2020.   \n[22] Pierre M\u00e9nard. Gradient ascent for active exploration in bandit problems. arXiv preprint arXiv:1905.08165, 2019.   \n[23] Quan Nguyen, Arghavan Modiri, and Roman Garnett. Nonmyopic multifidelity acitve search. In International Conference on Machine Learning, pages 8109\u20138118. PMLR, 2021.   \n[24] Victor Picheny, David Ginsbourger, Yann Richet, and Gregory Caplin. Quantile-based optimization of noisy computer experiments with tunable precision. Technometrics, 55(1):2\u201313, 2013.   \n[25] Riccardo Poiani, Alberto Maria Metelli, and Marcello Restelli. Multi-fidelity best-arm identification. Advances in Neural Information Processing Systems, 35:17857\u201317870, 2022.   \n[26] Matthias Poloczek, Jialei Wang, and Peter Frazier. Multi-information source optimization. Advances in neural information processing systems, 30, 2017.   \n[27] Yoan Russac, Christina Katsimerou, Dennis Bohle, Olivier Capp\u00e9, Aur\u00e9lien Garivier, and Wouter M Koolen. A/b/n testing with control in the presence of subpopulations. Advances in Neural Information Processing Systems, 34, 2021.   \n[28] Rajat Sen, Kirthevasan Kandasamy, and Sanjay Shakkottai. Multi-fidelity black-box optimization with hierarchical partitions. In International conference on machine learning, pages 4538\u20134547. PMLR, 2018.   \n[29] Rajat Sen, Kirthevasan Kandasamy, and Sanjay Shakkottai. Noisy blackbox optimization using multi-fidelity queries: A tree search approach. In The 22nd international conference on artificial intelligence and statistics, pages 2096\u20132105. PMLR, 2019.   \n[30] Max Simchowitz, Kevin G. Jamieson, and Benjamin Recht. The simulator: Understanding adaptive sampling in the moderate-confidence regime. In International Conference On Learning Theory (COLT), 2017.   \n[31] Xuchuang Wang, Qingyun Wu, Wei Chen, and John Lui. Multi-fidelity multi-armed bandits revisited. Advances in Neural Information Processing Systems, 36, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Table of Symbols ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Table 1 reports a summary on the main symbols and the notation used throughout the paper. ", "page_idx": 12}, {"type": "table", "img_path": "gKMTM1i8Ew/tmp/0ba525c5d6322ce9fb0f221b8b4dbd67129727e62d9968674dd8446ee94fcac8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "B Cost complexity lower bound: proofs and derivations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "B.1 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Theorem 3.1. Let $\\delta\\ \\in\\ (0,1)$ . For any $\\delta$ -correct strategy, and any multi-fidelity bandit model $\\pmb{\\mu}\\in\\mathcal{M}_{\\mathrm{MF}}^{*}$ , it holds that: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\pmb{\\mu}}[c_{\\tau_{\\delta}}]\\geq C^{\\ast}(\\pmb{\\mu})\\log\\left(\\frac{1}{2.4\\,\\delta}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$\\begin{array}{r}{C^{*}(\\mu)^{-1}:=\\operatorname*{sup}_{\\omega\\in\\Delta_{K\\times M}}F(\\omega,\\mu)=\\operatorname*{sup}_{\\omega\\in\\Delta_{K\\times M}}\\operatorname*{min}_{a\\neq\\star}f_{\\star,a}(\\omega,\\mu)\\,.}\\end{array}$ ", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. Consider $\\delta\\in(0,1)$ , a multi-fidelity bandit model $\\pmb{\\mu}$ and an alternative instance $\\pmb\\theta\\in\\operatorname{Alt}(\\pmb\\mu)$ where $\\begin{array}{r}{\\mathrm{Alt}(\\mu)=\\bigcup_{i\\neq\\star}\\left\\{\\pmb{\\theta}\\in\\mathcal{M}_{\\sf M F}:\\theta_{a,M}>\\theta_{\\star,M}\\right\\}}\\end{array}$ . Then, by applying Lemma 1 in [18], we can ", "page_idx": 12}, {"type": "text", "text": "directly connect the expected number of draws of each arm to the KL divergence of the two multifidelity bandit models. More specifically, we have that: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{a\\in[K]}\\sum_{m\\in[M]}\\mathbb{E}_{\\pmb{\\mu}}[N_{a,m}(\\tau_{\\delta})]d(\\mu_{a,m},\\theta_{a,m})\\geq\\mathbf{kl}(\\delta,1-\\delta).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, similarly to Theorem 1 in [8], we now proceed by applying Equation (11) with all the alternative models $\\pmb\\theta\\in\\operatorname{Alt}(\\pmb\\mu)$ . Specifically, we have that: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{M}(\\delta,1-\\delta)\\leq}&{\\underset{\\theta\\in\\mathbb{R}_{2}}{\\operatorname*{inf}}\\;\\underset{\\theta\\in\\mathbb{R}_{2}}{\\sum}\\underset{\\theta\\in\\mathbb{R}_{3}}{\\sum}\\;\\underset{\\theta\\in\\mathbb{R}_{3}}{\\sum}\\;\\underset{\\theta\\in\\mathbb{R}_{3}}{\\sum}[\\mathcal{R}_{a_{i}}[N_{a_{i}},m_{i}(\\pi_{3},\\theta_{a_{i},m_{i}})}\\\\ &{\\phantom{\\frac{(1)}{\\theta^{2}}}=\\mathbb{E}_{\\mu}|\\mathcal{C}_{a_{i}}]\\underset{\\theta\\in\\mathbb{R}_{3}}{\\operatorname*{inf}}[\\sum_{i=1}^{N}\\sum_{\\theta\\in[i,i]}]\\frac{\\mathbb{E}_{\\theta\\in[i,1]}[\\lambda_{m_{i},m_{i}}(\\pi_{3})]}{\\mathbb{E}_{\\theta\\in[i,1]}}\\frac{d\\left(\\mu_{a_{i},m_{i}},\\theta_{a_{i},m}\\right)}{\\lambda_{m_{i}}}}\\\\ &{\\leq\\mathbb{E}_{\\mu}|\\mathcal{C}_{a_{i}}\\biggr|_{s}\\underset{\\theta\\in\\mathbb{R}_{3}}{\\operatorname*{sup}}\\;\\underset{\\theta\\in\\mathbb{R}_{3}}{\\operatorname*{sup}}\\;\\underset{\\theta\\in[k]}{\\sum}\\sum_{i=1}^{N}\\sum_{\\theta\\in[i,n]}\\frac{d\\left(\\mu_{a_{i},m_{i}},\\theta_{a_{i},m}\\right)}{\\lambda_{m_{i}}}}\\\\ &{=\\mathbb{E}_{\\mu}|\\mathcal{C}_{a_{i}}\\biggr|_{s}\\underset{\\theta\\in\\mathbb{R}_{3}}{\\operatorname*{sup}}\\;\\underset{\\theta\\in\\mathbb{R}_{3}}{\\operatorname*{max}}\\;\\underset{\\theta\\in\\mathbb{R}_{3}}{\\operatorname*{minf}}\\;\\underset{\\theta\\in\\mathbb{R}_{3}}{\\sum}\\sum_{i=1}^{N}\\sum_{m_{i}}\\frac{d\\left(\\mu_{i},m_{i},\\theta_{i},m\\right)}{\\lambda_{m_{i}}}}\\\\ &{\\overset{(a)}{=}\\mathbb{E}_{\\mu}|\\mathcal{C}_{a_{i}}|\\underset{\\theta\\in\\mathbb{R}_{3}}{\\operatorname*{sup}}\\;\\underset{\\theta\\in\\mathbb{R}_{3}}{\\operatorname*\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where in $(a)$ we use that as $\\pmb{\\mu}\\in\\mathcal{M}_{\\mathrm{MF}}$ , the minimum in $\\pmb{\\theta}$ does not change any arm $i\\not\\in\\{\\star,a\\}$ . Finally, we lower bound $\\mathrm{kl}(\\delta,1-\\delta)$ with $\\log\\left({\\frac{1}{2.4\\,\\delta}}\\right)$ , thus concluding the proof. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "B.2 Comparison with existing lower bound ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide a comparison with the existing lower bound for the MF-BAI setting. In the following, we restrict our attention to bandits with Gaussian arms with variance $1/2$ . We assume for simplicity of the exposition that $\\star=1$ and that $\\mu_{1,M}>\\mu_{2,M}\\geq\\dots\\geq\\mu_{K,M}$ . Given this setup, we begin by recalling Theorem 1 in [25]. ", "page_idx": 13}, {"type": "text", "text": "Theorem B.1 (Theorem 1 in [25]). Consider any multi-fidelity bandit model $\\pmb{\\mu}$ with Gaussian arms with variance $1/2$ . Then, for any $\\delta$ -correct algorithm and $\\delta~\\leq~0.15$ it holds that $\\mathbb{E}_{\\mu}[c_{\\tau_{\\delta}}]/\\log\\left((2.4\\delta)^{-1}\\right)$ is lower bounded by: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\stackrel{m\\in[M]}{\\mu_{1,m}>\\mu_{2,M}+\\xi_{m}}}\\frac{\\lambda_{m}}{(\\mu_{1,m}-(\\mu_{2,M}+\\xi_{m}))^{2}}+\\sum_{i=1}^{K}\\operatorname*{min}_{\\stackrel{m\\in[M]}{\\mu_{1,M}-\\xi_{m}>\\mu_{i,m}}}\\frac{\\lambda_{m}}{(\\mu_{i,m}-(\\mu_{1,M}-\\xi_{m}))^{2}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "At this point, focus, for simplicity on the following $2\\times2$ bandit model (but a trivial generalization holds for $K\\times M$ bandits). We consider $\\begin{array}{r}{\\mu_{1,m}=\\mu_{1,M}+\\frac{\\xi_{m}}{2}}\\end{array}$ and $\\begin{array}{r}{\\mu_{2,m}=\\mu_{2,M}-\\frac{\\xi_{m}}{2}}\\end{array}$ . Furthermore, suppose that $\\mu_{1,M}=-\\mu_{2,M}$ . Then, let $\\Delta:=\\mu_{1,M}-\\mu_{2,M}$ . Suppose that $\\Delta=\\xi_{m}$ , which yields \u00b51,M = \u03be2m and \u00b52,M = \u2212 2m , and that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\Delta}{\\Delta-\\frac{\\xi_{m}}{2}}\\leq\\sqrt{\\frac{\\lambda_{M}}{\\lambda_{m}}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since $\\Delta=\\xi_{m}$ , this condition actually simplifies to $\\lambda_{M}\\geq4\\lambda_{m}$ . ", "page_idx": 13}, {"type": "text", "text": "Under these conditions it is possible to verify that the lower bound of [25] is given by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu}[c_{\\tau_{\\delta}}]\\geq\\frac{2\\lambda_{m}}{\\left(\\Delta-\\frac{\\xi_{m}}{2}\\right)^{2}}\\log\\left(\\frac{1}{2.4\\delta}\\right)=\\frac{8\\lambda_{m}}{\\Delta^{2}}\\log\\left(\\frac{1}{2.4\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "At this point, consider, instead, the result that we presented in Theorem 3.1 and consider a generic weight proportion $\\omega$ . From Corollary C.2, we know that: ", "page_idx": 14}, {"type": "equation", "text": "$$\nF(\\omega,\\mu)=f_{1,2}(\\omega,\\mu)=\\operatorname*{inf}_{\\eta\\in[\\mu_{2,M},\\mu_{1,M}]}\\sum_{m\\in[M]}\\omega_{1,m}\\frac{d^{-}(\\mu_{1,m},\\eta+\\xi_{m})}{\\lambda_{m}}+\\omega_{2,m}\\frac{d^{+}(\\mu_{2,m},\\eta-\\xi_{m})}{\\lambda_{m}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, let $\\omega^{*,M}$ be the optimal weights restricted on the portion of the simplex in which $\\omega_{1,m}\\,=$ $\\omega_{2,m}~=~0$ . Then, let $\\eta^{*,M}$ be the optimal solution of Equation (12) when considering $\\omega^{*,M}$ . Using the symmetry of the KL divergence for Gaussian distributions, it holds that $\\eta^{*,M}=0$ and $\\omega_{1,2}^{*,M}=\\omega_{2,2}^{*,M}=0.\\dot{5}.$ . Then, for any $\\omega$ it holds that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{F(\\omega,\\mu)\\le\\sum_{m\\in[M]}\\omega_{1,m}\\frac{d^{-}\\left(\\mu_{1,m},\\eta^{*,M}+\\xi_{m}\\right)}{\\lambda_{m}}+\\omega_{2,m}\\frac{d^{+}\\left(\\mu_{2,m},\\eta^{*,M}-\\xi_{m}\\right)}{\\lambda_{m}}}\\quad}&{}\\\\ &{=\\omega_{1,M}\\frac{d\\left(\\mu_{1,M},\\eta^{*,M}\\right)}{\\lambda_{M}}+\\omega_{2,M}\\frac{d\\left(\\mu_{2,M},\\eta^{*,M}\\right)}{\\lambda_{M}}}\\\\ &{\\le F(\\omega^{*,M},\\mu),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where in the second step, we have used the fact that $\\begin{array}{r}{d(\\mu_{2,m},\\eta^{*,M}-\\xi_{m})=d(\\mu_{2,M}-\\frac{\\xi_{m}}{2},-\\xi_{m})=0}\\end{array}$ and $\\begin{array}{r}{d(\\mu_{1,m},\\eta^{*,M}+\\xi_{m})=d(\\mu_{1,M}+\\frac{\\xi_{m}}{2},\\xi_{m})=0}\\end{array}$ . In other words, we have shown that in this example the optimal allocation is sparse and on fidelity $M$ . To conclude, we have that: ", "page_idx": 14}, {"type": "equation", "text": "$$\nF(\\omega^{*,M},\\mu)=0.5*\\frac{d(\\frac{\\xi_{m}}{2},0)}{\\lambda_{M}}+0.5*\\frac{d(-\\frac{\\xi_{m}}{2},0)}{\\lambda_{M}}=\\frac{d(\\frac{\\xi_{m}}{2},0)}{\\lambda_{M}}=\\frac{(\\Delta/2)^{2}}{\\lambda_{M}}=\\frac{\\Delta^{2}}{4\\lambda_{M}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which leads to: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pmb{\\mu}}[c_{\\tau_{\\delta}}]\\geq\\frac{4\\lambda_{M}}{\\Delta^{2}}\\log\\left((2.4\\delta)^{-1}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Under the assumptions on the problem, $\\frac{4\\lambda_{M}}{\\Delta^{2}}$ is always larger than $\\frac{8\\lambda_{m}}{\\Delta^{2}}$ . This result says that the ratio among the lower bounds can be of order $\\overline{{\\lambda}}_{M}/\\lambda_{m}$ , which is arbitrarily large. ", "page_idx": 14}, {"type": "text", "text": "B.3 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we provide a formal proof on the sparsity of the optimal oracle allocation $\\omega^{*}$ . The proofs given in this section rely on results that are explained in Appendix C.1. ", "page_idx": 14}, {"type": "text", "text": "At this point, in order to prove Theorem 3.2, we first introduce some intermediate results that will be used in the proving the theorem. Specifically, we begin by showing that, for each arm $a$ , there always exists a fidelity $m$ such that $\\omega_{a,m}^{*}>0$ holds. ", "page_idx": 14}, {"type": "text", "text": "Lemma B.2. Consider $\\pmb{\\mu}\\in\\mathcal{M}_{\\mathrm{MF}}$ and $\\omega^{\\ast}\\;\\in\\;\\Delta_{K\\times M}^{\\ast}(\\pmb{\\mu})$ . Then, for all $a~\\in~[K]$ , there exists $m\\in[M]$ such that $\\omega_{a,m}^{*}>0$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. We split the proof into two cases. First we consider $a\\ne\\star$ , and proceed by contradiction. Consider $\\omega^{\\ast}\\in\\Delta_{K\\times M}^{\\ast}(\\pmb{\\mu})$ , and suppose there exists $a\\ne\\star$ such that $\\omega_{\\star,m}^{*}=0$ for all $m\\in[M]$ . In this case, however, we have that: ", "page_idx": 14}, {"type": "equation", "text": "$$\nF(\\omega^{*},\\mu)\\leq f_{\\star,a}(\\omega^{*},\\mu)=\\operatorname*{inf}_{\\substack{\\theta_{a}\\in\\operatorname{MF},\\theta_{\\star}\\in\\operatorname{MF}:\\,\\displaystyle\\sum_{m=1}^{M}\\omega_{\\star,m}^{*}\\frac{d(\\mu_{\\star,m},\\theta_{\\star,m})}{\\lambda_{m}}=0,}}=0,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where, in the first step we have used the definition of $F(\\omega^{*},\\mu)$ , in the second one the fact that $\\omega_{a,m}^{*}\\,=\\,0$ for all $m\\ \\in\\ [M]$ , and in the last one we selected $\\theta_{\\star,m}\\,=\\,\\mu_{\\star,m}$ for all $m\\ \\in\\ [M]$ . Nevertheless, from Lemma C.6, we know that, whenever $\\omega_{i,M}\\,>\\,0$ holds for all $i\\,\\in\\,[K]$ , then $F(\\omega,\\mu)>0$ holds as well. Therefore, $\\omega^{*}\\notin\\Delta_{K\\times M}^{*}$ . ", "page_idx": 14}, {"type": "text", "text": "The proof for the case in which $i=\\star$ follows identical reasoning. ", "page_idx": 14}, {"type": "text", "text": "We then continue by proving that, at any optimal allocation $\\omega^{*}$ , all the transportation costs $f_{a}(\\omega^{*},\\mu)$ are equal. ", "page_idx": 15}, {"type": "text", "text": "Lemma B.3. Consider $\\mu\\in\\mathcal{M}_{\\mathrm{MF}}$ and $\\omega^{\\ast}\\in\\Delta_{K\\times M}^{\\ast}(\\pmb{\\mu})$ . Then, for all $a,b$ such that $a\\ne\\star$ and $b\\ne\\star$ the following holds: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{\\star,a}(\\omega^{*},\\pmb{\\mu})=f_{\\star,b}(\\omega^{*},\\pmb{\\mu}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. We introduce the following notation: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{A}=\\left\\{a\\in[K]\\setminus\\{\\star\\}:a\\in\\underset{b\\not=\\star}{\\operatorname{argmin}}\\,f_{b}(\\omega^{*},\\mu)\\right\\}}\\\\ &{\\mathcal{B}=([K]\\setminus\\{\\star\\})\\setminus\\mathcal{A}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "At this point, we proceed by contradiction. Suppose that $B\\neq\\emptyset$ . Then, for some sufficiently small $\\epsilon>0$ , we define $\\tilde{\\omega}\\in\\Delta_{K\\times M}$ in the following way. For all $a\\in A$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\tilde{\\omega}_{a,M}=\\omega_{a,M}^{*}+\\epsilon/|A|}&{{}}\\\\ {\\tilde{\\omega}_{a,m}=\\omega_{a,m}^{*}}&{{}\\forall m<M.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For all $b\\in\\mathcal{B}$ , instead: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\omega}_{b,m_{b}}=\\omega_{b,m_{b}}^{*}-\\epsilon/|\\boldsymbol{\\mathcal{B}}|}\\\\ &{\\tilde{\\omega}_{b,m}=\\omega_{b,m}^{*}\\quad\\forall m\\neq m_{b},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $m_{b}\\in[M]$ is any fidelity such that $\\omega_{b,m_{b}}^{*}>0$ (which exists by Lemma B.2). ", "page_idx": 15}, {"type": "text", "text": "Given this definition of $\\tilde{\\omega}$ , it is easy to see that $f_{\\star,a}(\\tilde{\\omega},\\pmb{\\mu})\\;>\\;f_{\\star,a}(\\pmb{\\omega}^{\\ast},\\pmb{\\mu})$ for all $a\\,\\in\\,{\\mathcal{A}}$ . This is a direct consequence of the fact that $f_{\\star,a}(\\cdot,\\pmb{\\mu})$ is a strictly increasing function of $\\omega_{a,M}$ , which is apparent from its expression from its expression for $\\pmb{\\mu}\\in\\mathcal{M}_{\\mathrm{MF}}$ given in Corollary C.2 and the computation of its gradient (Lemma C.3). Moreover, due to similar arguments, it also holds that $f_{\\star,b}(\\tilde{\\omega},\\pmb{\\mu})\\leq f_{\\star,b}(\\bar{\\omega}^{\\ast},\\pmb{\\mu})$ for all $b\\in\\mathcal{B}$ . Using the continuity of the functions $f$ , for $\\varepsilon$ small enough we further have $f_{\\star,a}(\\tilde{\\omega},\\pmb{\\mu})<f_{\\star,b}(\\tilde{\\omega},\\pmb{\\mu})$ for all $a\\in A$ and $b\\in\\mathfrak{B}$ . This leads to $\\begin{array}{r}{\\operatorname*{min}_{a\\neq\\star}f_{\\star,a}(\\omega^{*},\\mu)<}\\end{array}$ $\\mathrm{min}_{a\\neq\\star}\\,f_{\\star,a}(\\tilde{\\omega},\\pmb{\\mu})$ which contradicts the optimality of $\\omega^{*}$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "We now continue by providing necessary conditions that characterize some key properties of the oracle weights $\\omega$ , which follows from the expression of the gradient of $f_{\\star,a}(\\omega,\\mu)$ with respect to $\\omega$ for $\\pmb{\\mu}\\in\\mathcal{M}_{\\mathrm{MF}}$ (Lemma C.3). ", "page_idx": 15}, {"type": "text", "text": "Lemma B.4. Consider $\\pmb{\\mu}\\in\\mathcal{M}$ and $\\omega^{\\ast}\\in\\Delta_{K\\times M}(\\pmb{\\mu})$ . Then, for all $a\\ne\\star$ the following conditions holds: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{d^{+}(\\mu_{a,m_{1}},\\eta_{\\star,a}^{*}(\\omega^{*})-\\xi_{m_{1}})}{\\lambda_{m_{1}}}=\\frac{d^{+}(\\mu_{a,m_{2}},\\eta_{\\star,a}^{*}(\\omega^{*})-\\xi_{m_{2}})}{\\lambda_{m_{2}}}\\,\\forall m_{1},m_{2}:\\omega_{a,m_{1}}^{*},\\omega_{a,m_{2}}^{*}>0\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. We begin by recalling the definition of $C^{*}(\\pmb{\\mu})^{-1}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nC^{*}(\\pmb{\\mu})^{-1}=\\operatorname*{sup}_{\\pmb{\\omega}\\in\\Delta_{K\\times M}}\\operatorname*{min}_{a\\neq\\star}f_{\\star,a}(\\pmb{\\omega},\\pmb{\\mu}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which, is a concave optimization problem with a non-empty feasible region. Therefore, we can apply the KKT conditions to study the properties of each local optimal point $\\omega^{*}$ for which the sub-derivatives exist, i.e., from Theorem 4.3 and Corollary C.2, the ones for which the following condition hold:10 ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{a\\neq\\star}f_{\\star,a}(\\omega)>0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "At this point, fix any arm $a$ that attains the minimum in Equation (14). Then, from the KKT conditions, we obtain the following system of inequalities: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\!-\\!\\frac{\\partial}{\\omega_{a,m}}f_{\\star,a}(\\omega,\\mu)+c-b_{a,m}=0}&{\\forall m\\in[M]}\\\\ {\\!-\\!\\frac{\\partial}{\\omega_{\\star,m}}f_{\\star,a}(\\omega,\\mu)=0}&{\\forall m\\in[M]}\\\\ {\\!b_{i,m}\\omega_{i,m}^{*}=0}&{\\forall i\\in\\{\\star,a\\},\\forall m\\in[M]}\\\\ {\\!b_{i,m}\\geq0}&{\\forall i\\in[K],\\forall m\\in[M]}\\\\ {\\!\\omega_{i,m}^{*}\\geq0}&{\\forall i\\in[K],\\forall m\\in[M]}\\\\ {\\!\\sum_{i,m}\\omega_{i,m}^{*}=1}&\\end{array}\\right.\\,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "At this point, suppose that $\\omega_{i_{1},m_{1}}>0$ , $\\omega_{i_{2},m_{2}}>0$ for some $i_{1},i_{2}\\in\\{\\star,a\\}$ and some $m_{1},m_{2}\\in[M]$ , then $b_{i_{1},m_{1}}=0$ , $b_{i_{2},m_{2}}=0$ . As a consequence, by applying Lemma C.3 and the fact that $\\pmb{\\mu}\\in\\mathcal{M}_{\\mathrm{MF}}$ (i.e., see Corollary C.2), the following equations holds: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{d^{+}(\\mu_{a,m_{1}},\\eta_{\\star,a}^{*}(\\omega^{*})-\\xi_{m_{1}})}{\\lambda_{m_{1}}}=\\frac{d^{+}(\\mu_{a,m_{2}},\\eta_{\\star,a}^{*}(\\omega^{*})-\\xi_{m_{2}})}{\\lambda_{m_{2}}}\\,\\forall m_{1},m_{2}:\\omega_{a,m_{1}}^{*},\\omega_{a,m_{2}}^{*}>0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, to conclude the proof, it is sufficient to iterate these arguments for all $a\\ne\\star$ . Indeed, from Lemma B.3, we know that all sub-optimal arms will attain the minimum in Equation (14) at a global optimum $\\omega^{*}$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "At this point we are ready to prove our main result. ", "page_idx": 16}, {"type": "text", "text": "Theorem 3.2. Let $\\textstyle\\Delta_{K\\times M}^{*}(\\pmb{\\mu}):=\\operatorname*{argmax}_{\\pmb{\\omega}\\in\\Delta_{K\\times M}}F(\\pmb{\\omega},\\pmb{\\mu})$ and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\widetilde{M}_{\\mathrm{MF}}:=\\left\\{\\mu\\in\\mathcal{M}_{\\mathrm{MF}}^{*}:\\exists i\\in[K],\\exists m_{1},m_{2}\\in[M]^{2},\\exists\\omega^{*}\\in\\Delta_{K\\times M}^{*}(\\mu):\\omega_{i,m_{1}}^{*}>0,\\omega_{i,m_{2}}^{*}>0\\right\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The set $\\widetilde{\\mathcal{M}}_{\\mathrm{MF}}$ is a subset of $\\mathbb{R}^{K\\times M}$ whose Lebesgue measure is zero. ", "page_idx": 16}, {"type": "text", "text": "Proof. Let us introduce some additional notation. Consider a subset of arm-fidelity pairs $\\chi\\subseteq$ $[K]\\times[M]$ , and define $\\mathcal{G}(\\mathcal{X})\\subseteq\\mathcal{M}$ as the subset of multi-fidelity bandit models $\\pmb{\\mu}$ for which there exists $\\bar{\\omega}^{*}\\,\\,\\bar{\\in}\\,\\Delta_{K\\times M}^{*}(\\pmb{\\mu})$ such that, for all $(i,m)\\in\\mathcal{X}$ , $\\omega_{i,m}^{*}>0$ holds. ", "page_idx": 16}, {"type": "text", "text": "Then, fix an arm $i\\;\\;\\neq\\;\\;\\star.$ , and any three fidelity $m_{1},m_{2},m_{3}\\mathrm{\\boldmath~\\in~}\\ [M]$ , and consider $\\textbf{\\em{\\mu}}\\in$ $\\mathcal{G}(\\{(i,m_{1}),(i,m_{2}),(\\star;m_{3})\\})$ ).11 Then, from Lemma B.4, we know that the following condition holds: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{d^{+}(\\mu_{i,m_{1}},\\eta_{\\star,i}^{*}(\\omega^{*})-\\xi_{m_{1}})}{\\lambda_{m_{1}}}=\\frac{d^{-}(\\mu_{\\star,m_{3}},\\eta_{\\star,i}^{*}(\\omega^{*})+\\xi_{m_{3}})}{\\lambda_{m_{3}}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This, in turn, implies that $\\eta_{\\star,i}^{*}(\\omega^{*})$ is uniquely identified as a function of $\\mu_{i,m_{1}},\\,\\mu_{\\star,m_{3}},\\,\\xi_{m_{1}},\\,\\xi_{m_{3}},$ $\\lambda_{m_{1}}$ and $\\lambda_{m_{3}}$ . Indeed, $d^{+}(\\mu_{i,m_{1}},\\eta_{\\star,i}^{*}(\\omega^{*})-\\xi_{m_{1}})$ is a strictly increasing function of $\\eta_{\\star,i}^{*}(\\omega^{*})$ , while $d^{-}(\\mu_{\\star,m_{3}},\\eta_{\\star,i}^{\\ast}(\\omega^{\\ast})+\\xi_{m_{3}})$ is a strictly decreasing function of $\\eta_{\\star,i}^{*}(\\omega^{*})$ . Let $c_{1}=\\eta_{\\star,i}^{*}(\\omega^{*})$ , and let d+(\u00b5i,m1,\u03b7\u03bb\u22c6\u2217m,i1(\u03c9\u2217)\u2212\u03bem1). At this point, since \u03c9i\u2217,m2 > 0 holds by definition, we also know, from Lemma B.4, that the following condition has to be satisfied: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{d^{+}(\\mu_{i,m_{2}},c_{1}-\\xi_{m_{2}})}{\\lambda_{m_{2}}}=c_{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, the value of $\\mu_{i,m_{2}}$ is uniquely identified as a function of $\\mu_{i,m_{1}},\\,\\mu_{\\star,m_{3}},\\xi_{m_{1}},\\,\\xi_{m_{3}},\\,\\lambda_{m_{1}}$ and $\\lambda_{m_{3}}$ . That function is measurable (it\u2019s a combination of $d^{+}$ , $d^{-}$ and their inverses), hence $\\pmb{\\mu}$ lies on the graph of a measurable function, and such a graph has Lebesgue measure 0. Therefore, ", "page_idx": 16}, {"type": "text", "text": "$\\mathcal{G}(\\{(i,m_{1}),(i,m_{2}),(\\star,m_{3})\\})$ has measure 0. At this point, thanks to Lemma B.2, we know that, for arm $\\star$ , there always exists at least a fidelity $m_{3}$ such that $\\omega_{\\star,m_{3}}^{*}>0$ . We thus have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{G}((i,m_{1}),(i,m_{2}))\\subseteq\\bigcup_{m_{3}\\in[M]}\\mathcal{G}((i,m_{1}),(i,m_{2}),(\\star,m_{3}))\\:.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\mathcal{G}((i,m_{1}),(i,m_{2}))$ is contained in a set which is a countable union of null measure sets, it has null measure. ", "page_idx": 17}, {"type": "text", "text": "To conclude the proof, we notice that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{M}}_{\\mathrm{MF}}\\subseteq\\bigcup_{i=1}^{K}\\bigcup_{m_{1},m_{2}\\in[M]^{2}}\\mathcal{G}(\\{(i,m_{1}),(i,m_{2})\\}):=\\mathcal{V}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The proof follows from the fact that (i) $\\boldsymbol{\\wp}$ is a countable union of set of null measure (and, consequently, has null measure), and (ii) $\\widetilde{\\mathcal{M}}_{\\mathrm{MF}}\\subseteq\\mathcal{Y}$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "B.4 Additional results on the sparsity of the oracle weights ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we present additional results on the sparsity of the oracle weights. Specifically: ", "page_idx": 17}, {"type": "text", "text": "(i) We identify a specific class of multi-fidelity bandit models in which the optimal allocation is sparse. In particular, within this class of MF-bandit models, the optimal allocation have non-zero values only at the cheapest fidelity.   \n(ii) We then provide sufficient conditions to determine whether some fidelity have zero weights at any optimal weight vector $\\omega^{*}$ ", "page_idx": 17}, {"type": "text", "text": "We now proceed by constructing the class of multi-fidelity bandits that we mentioned in point (i) above. In this construction, we will consider Gaussian multi-fidelity bandits with variance $\\frac{\\mathfrak{j}}{2}$ . Then, for any number of arms $K$ and fidelity $M$ , we will denote with ${\\mathcal{A}}_{K M}$ , the set of Gaussian multifidelity bandits that satisfy the following construction. We start by building the means of the arms at the highest fidelity $M$ . Specifically, we consider a generic $\\mu_{\\star,m}>0$ , and let $\\mu_{a,M}=-\\mu_{\\star,m}$ for all $a\\ne\\star$ . Then, for each fidelity $m<M$ , and any values of $\\lambda_{m}$ and $\\xi_{m}$ , we let $\\mu_{i,m}=\\mu_{i,M}-\\xi_{m}$ for all $i\\ne\\star$ , and $\\mu_{\\star,m}=\\mu_{\\star,M}+\\xi_{m}$ . Finally, to simplify some computations, we set $\\sigma^{2}$ of each Gaussian distribution to 12. ", "page_idx": 17}, {"type": "text", "text": "Proposition B.5. For all $\\pmb{\\mu}\\in\\mathcal{A}_{K M}$ , and any $\\omega^{*}\\in\\Delta_{K\\times M}^{*}$ , it holds that, for all $a\\in[K]$ and all $m>1$ , $\\omega_{a,m}^{*}=0$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. To prove the result, starting from Corollary C.2, it is sufficient to notice that, for all $\\pmb{\\mu}\\in\\mathcal{A}_{K M}$ and all $a\\neq\\star,f_{\\star,a}(\\omega,\\mu)$ can be rewritten as: ", "page_idx": 17}, {"type": "equation", "text": "$$\nf_{\\star,a}(\\omega,\\pmb{\\mu})=\\operatorname*{inf}_{\\eta\\in[\\mu_{a,M},\\mu_{\\star,m}]}\\sum_{i\\in\\{\\star,a\\}}\\sum_{m=1}^{M}\\omega_{i,m}\\frac{d(\\mu_{i,M},\\eta)}{\\lambda_{m}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Specifically, Equation (16) follows directly from the symmetric property of $\\mathrm{KL}$ divergence for Gaussian distributions, and by the construction of $\\pmb{\\mu}$ . The proof then continue by contradiction. Suppose there exists $\\omega^{*}$ such that there exists $(i,m)$ (with $m>1$ ) such that $\\omega_{i,m}^{*}>0$ . By defining $\\tilde{\\omega}$ as the vector which is equal to $\\omega^{*}$ except in the components $(i,m)$ and $(a,1)$ for all $a\\in[K]$ . More specifically, for a sufficiently small $\\epsilon>0$ , we define $\\Tilde{\\omega}_{i,m}=\\omega_{i,m}^{*}-\\epsilon$ and $\\tilde{\\omega}_{a,1}=\\omega_{a,1}^{*}+\\epsilon/(K)$ for all $a\\,\\in\\,[K]$ . Then, it is easy to see that $f_{\\star,a}(\\tilde{\\omega},\\pmb{\\mu})\\,>\\,f_{\\star,a}(\\pmb{\\omega}^{\\ast},\\pmb{\\mu})$ holds for all $a\\ne\\star$ , thus contradicting the optimality of $\\omega^{*}$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Finally, we now provide sufficient conditions to determine whether some fidelity have zero weights at any optimal weight vector $\\omega^{*}$ ", "page_idx": 17}, {"type": "text", "text": "Proposition B.6. Fix $a\\ne\\star.$ . Then, if $\\mu_{a,m}+\\xi_{m}\\ge\\mu_{\\star,m}$ , then it holds that $\\omega_{a,m}^{*}=0.$ . Furthermore, $i f\\mu_{\\star,m}-\\xi_{m}\\leq\\mu_{j,M}$ for all $j\\neq*$ , then it holds that $\\omega_{\\star,m}^{*}=0$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Consider $a\\ne\\star.$ , and let us analyze $f_{\\star,a}(\\omega,\\mu)$ for any $\\omega\\in\\Delta_{K\\times M}$ . More specifically, we recall from Corollary C.2, that the only term in which $\\omega_{a,m}$ plays a role is the following one: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\omega_{a,m}\\frac{d(\\mu_{a,m},\\eta_{\\star,a}^{*}(\\omega)-\\xi_{m})}{\\lambda_{m}}{\\bf1}\\left\\{\\eta_{\\star,a}^{*}(\\omega)\\geq\\mu_{a,m}+\\xi_{m}\\right\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Nevertheless, since $\\eta_{\\star,a}^{\\ast}(\\omega)\\leq\\mu_{\\star,m}\\leq\\mu_{a,m}+\\xi_{m}$ , we have that Equation (17) is always equal to 0 for all $\\omega\\in\\Delta_{K\\times M}$ . To prove the result we now proceed by contradiction. Suppose that $\\omega^{*}$ is such that $\\omega_{a,m}^{*}>0$ . Then, consider $\\tilde{\\omega}$ as a vector which is equal to $\\omega^{*}$ except in the components $(a,m)$ and $(i,M)$ for all $i\\neq\\star$ . More specifically, for a sufficiently small $\\epsilon>0$ , we define $\\tilde{\\omega}_{a,m}=\\omega_{a,m}^{*}-\\epsilon$ and $\\tilde{\\omega}_{i,M}=\\omega_{i,M}^{*}+\\epsilon/(K-1)$ for all $i\\neq\\star$ . At this point, by noticing that $f_{\\star,i}(\\omega,\\mu)$ is strictly increasing in $\\omega_{i,M}$ (i.e., due to Theorem 4.3 and the fact that $\\pmb{\\mu}\\in\\mathcal{M}_{\\mathrm{MF}})$ , and since $f_{\\star,a}(\\omega,\\mu)$ is not affected by the value of $\\omega_{a,m}$ (i.e., Equation (17)), we have that $f_{\\star,i}(\\tilde{\\omega},\\pmb{\\mu})>f_{\\star,i}(\\pmb{\\omega}^{\\ast},\\pmb{\\mu})$ for all $i\\neq*$ , thus contradicting the optimality of $\\omega^{*}$ . ", "page_idx": 18}, {"type": "text", "text": "To show that if $\\mu_{\\star,m}-\\xi_{m}\\,\\leq\\,\\mu_{j,M}$ for all $j\\neq*.$ , then it holds that $\\omega_{\\star,m}^{*}=0$ , it is possible to follow identifical reasonings. The only difference is that the term $\\omega_{\\star,m}$ plays a role in each of the $(K-1)$ -equations defining $F(\\omega,\\pmb{\\mu})$ , namely: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\omega_{\\star,m}\\frac{d(\\mu_{\\star,m},\\eta_{\\star,a}^{\\star}(\\omega)+\\xi_{m})}{\\lambda_{m}}{\\bf1}\\left\\{\\eta_{\\star,a}^{\\ast}(\\omega)\\leq\\mu_{1,m}-\\xi_{m}\\right\\}\\quad\\forall a\\not=\\star.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Nevertheless, Equation (18) is equal to 0 for all $a\\ne\\star$ since $\\eta_{\\star,a}^{\\ast}(\\omega)\\geq\\mu_{i,m}-\\xi_{m}\\geq\\mu_{a,M}$ holds for all $\\omega$ and all $a\\ne\\star$ . The proof then follows by an identical construction of an alternative weight vector $\\tilde{\\omega}$ which increases the objective function. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B.5 Sub-optimality of \"optimal\" fidelity of previous works ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we discuss how the concept of \"optimal\" fidelity of previous works (i.e., [25] and [31]) fails to satisfy the notion of optimal fidelity that arises from the tighter lower bound that we presented in Section 3. In this section, we consider as example $2\\times2$ multi-fidelity bandit models with Gaussian distributions. To ease the notation, we will consider \u00b51,M > \u00b52,M. ", "page_idx": 18}, {"type": "text", "text": "B.5.1 Case 1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We notice that [25] provided the two concepts of optimal fidelity. The first one is from their Theorem 1. This same concept was then considered later in [31]. A fidelity $m$ is optimal for a certain arm $a\\in[K]$ if it satisfies the following condition: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{m_{a}^{*}\\in\\underset{m\\in[M]}{\\operatorname{argmax}}\\frac{\\mu_{1,M}-\\left(\\mu_{a,m}+\\xi_{m}\\right)}{\\sqrt{\\lambda_{m}}}\\quad\\mathrm{if}\\;a\\neq1}\\\\ &{m_{a}^{*}\\in\\underset{m\\in[M]}{\\operatorname{argmax}}\\frac{\\left(\\mu_{a,m}-\\xi_{m}\\right)-\\mu_{2,M}}{\\sqrt{\\lambda_{m}}}\\quad\\mathrm{if}\\;a=1}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, consider the following $2\\times2$ example of multi-fidelity BAI problem. Let $\\xi_{1}=0.1$ , $\\mu_{1,M}=0.6$ , $\\mu_{1,m}=0.65,\\mu_{2,M}=0.5,\\mu_{2,m}=0.45$ (where we use the notation $M=2$ for the maximal fidelity and $m=1$ ). Suppose, furthermore, that all distributions are Gaussian. In this case, from Equation (19)-(20), we have that $m_{1}^{*}=1$ and $m_{2}^{*}=1$ whenever the following conditions are satisfied: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\mu_{1,M}-\\left(\\mu_{2,m}+\\xi_{m}\\right)}{\\sqrt{\\lambda_{m}}}>\\frac{\\mu_{1,M}-\\mu_{2,M}}{\\sqrt{\\lambda_{M}}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Plugging in the numerical values, we obtain in both cases ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\frac{0.05}{\\sqrt{\\lambda_{m}}}}>{\\frac{0.1}{\\sqrt{\\lambda_{M}}}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "thus showing that, according to [31], the optimal fidelity for both arms is $m=1$ whenever $\\begin{array}{r}{\\sqrt{\\frac{\\lambda_{M}}{\\lambda_{m}}}>}\\end{array}$ $\\frac{0.1}{0.05}$ ", "page_idx": 18}, {"type": "text", "text": "At this point, consider the expression of $F(\\omega,\\pmb{\\mu})=f_{1,2}(\\omega,\\pmb{\\mu})$ in this particular example. Then, it is possible to show that, for any $\\omega\\in\\Delta_{2\\times2}$ such that $\\omega_{1,M}=\\omega_{2,M}=0$ , then, $f_{1,2}(\\bar{\\omega},\\mu)=0$ . Specifically, we have that $F(\\omega,\\pmb{\\mu})$ is given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\substack{\\leqslant[\\mu_{2,M},\\mu_{1,M}]}}\\omega_{1,m}\\frac{d(\\mu_{1,m},\\eta+\\xi_{m})\\mathbf{1}\\big\\{\\eta\\le\\mu_{1,m}-\\xi_{m}\\big\\}}{\\lambda_{m}}+\\omega_{a,m}\\frac{d(\\mu_{2,m},\\eta-\\xi_{m})\\mathbf{1}\\big\\{\\eta\\ge\\mu_{a,m}+\\xi_{m}\\big\\}}{\\lambda_{m}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In turn, this is equal to: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\eta\\in[0.5,0.6]}\\omega_{1,m}\\frac{d(0.55,\\eta)\\mathbf{1}\\{\\eta\\le0.55\\}}{\\lambda_{m}}+\\omega_{a,m}\\frac{d(0.55,\\eta)\\mathbf{1}\\{\\eta\\ge0.55\\}}{\\lambda_{m}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which is always 0 for $\\eta=0.55$ . ", "page_idx": 19}, {"type": "text", "text": "On the other hand, Lemma C.6, shows that any strategy that gives positive value to weights at fidelity $M=2$ obtains $F(\\omega,\\mu)>0$ . ", "page_idx": 19}, {"type": "text", "text": "B.5.2 Case 2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Furthermore, [25] provided also the following concept of optimal fidelity which only holds for sub-optimal arms (see Definition 1 in [25]). A fidelity $m$ such that $\\mu_{1,M}-\\mu_{2,M}>4\\xi_{m}$ holds is said to be optimal for arm $a\\ne1$ if the following holds: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\lambda_{m}}{(\\mu_{1,M}-\\mu_{a,M}-4\\xi_{m})^{2}}\\leq\\operatorname*{min}_{\\bar{m}>m}\\frac{\\lambda_{\\bar{m}}}{(\\mu_{1,M}-\\mu_{a,M}-4\\xi_{m})^{2}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "At this point, consider the following classes of multi-fidelity bandit models: $\\mu_{2,m}=\\mu_{2,M}-\\xi_{m}$ , $\\mu_{1,m}=\\mu_{1,M}+\\xi_{m},\\mu_{1,M}-\\mu_{2,M}\\leq4\\xi_{m}.$ . In this case, from Equation (20) it follows that the optimal fidelity for arm 2 is always $M$ . Nevertheless, since $\\mu_{2,m}=\\mu_{2,M}-\\xi_{m}$ , $\\mu_{1,m}=\\mu_{1,M}+\\xi_{m}$ , we know from Proposition B.5 $\\omega_{1,M}=\\omega_{2,M}=0$ . ", "page_idx": 19}, {"type": "text", "text": "C Algorithm analysis ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C.1 Gradient computation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We start by analyzing a salient feature of $f_{i,j}(\\omega,\\pmb{\\mu})$ that holds for any ${\\pmb{\\mu}}\\in\\Theta^{K M}$ . ", "page_idx": 19}, {"type": "text", "text": "Lemma C.1. Consider $\\pmb{\\mu}\\in\\Theta^{K M}$ . Fix any $\\omega\\in\\Delta_{K\\times M}$ and $i,j\\in[K]$ . Let $\\theta^{*}$ be the solution of the following optimization problem: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\theta^{*}\\in\\operatorname{argmin}_{\\theta_{i}\\in\\operatorname{MF},\\theta_{j}\\in\\operatorname{MF}\\colon}\\sum_{m\\in[M]}\\omega_{j,m}\\frac{d(\\mu_{j,m},\\theta_{j,m})}{\\lambda_{m}}+\\sum_{m\\in[M]}\\omega_{i,m}\\frac{d(\\mu_{i,m},\\theta_{i,m})}{\\lambda_{m}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Furthermore, define for $k\\in\\{i,j\\}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{M}}_{k}(\\omega,\\mu,\\theta^{*}):=\\left\\{m\\in[M-1]:\\theta_{k,M}^{*}>\\mu_{k,m}+\\xi_{m}\\right\\}}\\\\ &{\\underline{{M}}_{k}(\\omega,\\mu,\\theta^{*}):=\\left\\{m\\in[M-1]:\\theta_{k,M}^{*}<\\mu_{k,m}-\\xi_{m}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, for $k\\in\\{i,j\\}$ we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\theta_{k,m}^{*}=\\mu_{k,m}}&&{\\qquad\\forall m\\in[M]\\setminus\\left(\\overline{{M}}_{k}(\\omega,\\mu,\\theta^{*})\\cup\\underline{{M}}_{k}(\\omega,\\mu,\\theta^{*})\\right)}\\\\ &{\\theta_{k,m}^{*}=\\theta_{k,M}^{*}-\\xi_{m}}&&{\\qquad\\forall m\\in\\overline{{M}}_{k}(\\omega,\\mu,\\theta^{*})}\\\\ &{\\theta_{k,m}^{*}=\\theta_{k,M}^{*}+\\xi_{m}}&&{\\qquad\\forall m\\in\\underline{{M}}_{k}(\\omega,\\mu,\\theta^{*})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In particular, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{f_{i,j}(\\pmb{w},\\pmb{\\mu})}&{=}&{\\displaystyle\\sum_{k\\in\\{i,j\\}}\\sum_{m\\in[M]}\\omega_{k,m}\\frac{d^{-}\\big(\\mu_{k,m},\\pmb{\\theta}_{k,M}^{*}+\\xi_{m}\\big)+d^{+}\\big(\\mu_{k,m},\\pmb{\\theta}_{k,M}^{*}-\\xi_{m}\\big)}{\\lambda_{m}}}\\\\ &{=}&{\\displaystyle\\operatorname*{min}_{\\pmb{\\theta}_{j,M}\\geq\\pmb{\\theta}_{i,M}}\\sum_{k\\in\\{i,j\\}}\\sum_{m\\in[M]}\\omega_{k,m}\\frac{d^{-}\\big(\\mu_{k,m},\\pmb{\\theta}_{k,M}+\\xi_{m}\\big)+d^{+}\\big(\\mu_{k,m},\\pmb{\\theta}_{k,M}-\\xi_{m}\\big)}{\\lambda_{m}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. We begin by proving Equation (22). To this end, it is sufficient to notice that, given a fixed $\\theta_{k,M}^{*}$ , it is possible to set $\\theta_{k,m}^{*}:=\\mu_{k,m}$ , whenever the following condition is satisfied: ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\theta_{k,M}^{*}-\\mu_{k,m}|\\leq\\xi_{m}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The condition is Equation (25) is equivalent to requiring $\\begin{array}{r l r l}{m}&{{}}&{\\in}&{{}\\quad[M]\\quad\\backslash}\\end{array}$ $\\left(\\overline{{\\mathcal{M}}}_{k}(\\omega,\\pmb{\\mu},\\pmb{\\theta}^{*})\\cup\\underline{{\\mathcal{M}}}_{k}(\\omega,\\pmb{\\mu},\\pmb{\\theta}^{*})\\right)$ , which concludes the first part of the proof. ", "page_idx": 20}, {"type": "text", "text": "We continue by proving Equation (23). Consider $m\\in\\overline{{M}}_{k}(\\omega,\\mu,\\theta^{*})$ , that is $\\theta_{k,m}^{*}>\\mu_{k,m}+\\xi_{m}$ . From this condition, it directly follows that $\\theta_{k,m}^{*}>\\mu_{k,m}$ ; therefore, since $d(\\mu_{k,m},x)$ is increasing in $x$ , it follows that, in order to attain the argmin, we need to pick the smallest value of $\\theta_{k,m}$ that satisfies the multi-fidelity constraint $|\\theta_{k,M}^{*}-\\theta_{k,m}|$ , that is $\\theta_{k,M}^{\\ast}-\\xi_{m}$ . ", "page_idx": 20}, {"type": "text", "text": "The proof of Equation (23) follows is almost identical to the one of Equation (24); it is sufficient to replace the definition of $\\overline{{M}}_{k}(\\omega,\\mu,\\theta^{*})$ with $\\underline{{M}}_{k}(\\omega,\\mu,\\theta^{*})$ . \u53e3 ", "page_idx": 20}, {"type": "text", "text": "At this point, we continue by analyzing in more detail the function $f_{i,j}(\\omega,\\pmb{\\mu})$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma 4.2. Consider ${\\pmb{\\mu}}\\in\\Theta^{K M}$ and $\\omega\\in\\Delta_{K\\times M}$ . Define for $k\\in[K]$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\psi_{k}^{*}:=\\underset{\\psi\\in\\mathbb{R}}{\\mathrm{argmin}}\\sum_{m=1}^{M}\\omega_{k,m}\\frac{d^{-}(\\mu_{k,m},\\psi+\\xi_{m})+d^{+}(\\mu_{k,m},\\psi-\\xi_{m})}{\\lambda_{m}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, the following holds: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{i,j}(\\omega,\\mu)=\\displaystyle\\sum_{k\\in\\{i,j\\}}\\displaystyle\\sum_{m=1}^{M}\\omega_{k,m}\\frac{d^{-}(\\mu_{k,m},\\psi_{k}^{*}+\\xi_{m})+d^{+}(\\mu_{k,m},\\psi_{k}^{*}-\\xi_{m})}{\\lambda_{m}}\\quad i f\\,\\psi_{j}^{*}>\\psi_{i}^{*}}\\\\ &{f_{i,j}(\\omega,\\mu)=\\displaystyle\\operatorname*{inf}_{\\eta\\in\\mathbb{R}}\\displaystyle\\sum_{k\\in\\{i,j\\}}\\sum_{m=1}^{M}\\omega_{k,m}\\frac{d^{-}(\\mu_{k,m},\\eta+\\xi_{m})+d^{+}(\\mu_{k,m},\\eta-\\xi_{m})}{\\lambda_{m}}\\quad o t h e r w i s e.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. The proof follows by analyzing the definition of $f_{i,j}(\\omega,\\pmb{\\mu})$ . Consider $\\theta_{i}^{*},\\theta_{j}^{*}$ that attaines the minimum in Equation (1). Then, there are two possibilities: either $\\theta_{i,M}^{*}=\\theta_{j,M}^{*}$ or $\\theta_{j,M}^{*}>\\theta_{i,M}^{*}$ . ", "page_idx": 20}, {"type": "text", "text": "Suppose that $\\theta_{j,M}^{*}>\\theta_{i,M}^{*}$ , then we notice that the optimization problem in $f_{i,j}(\\omega,\\pmb{\\mu})$ is a 2D-convex optimization problem in the variables $\\theta_{j,M},\\theta_{i,M}$ (thanks to Lemma C.1). Therefore, since the minimum of the constrained problem is such that $\\theta_{j,M}>\\theta_{i,M}$ , than, by the convexity of the problem, this is also a minimum for the unconstrained problem, thus leading to: ", "page_idx": 20}, {"type": "equation", "text": "$$\nf_{i,j}(\\omega,\\mu)=\\operatorname*{inf}_{\\theta_{i}\\in\\mathrm{MF},\\theta_{j}\\in\\mathrm{MF}}\\sum_{k\\in\\{i,j\\}}\\sum_{m\\in[M]}\\omega_{k,m}\\frac{d(\\mu_{k,m},\\theta_{k,m})}{\\lambda_{m}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "At this point, we notice that the constraints in the previous optimization problem are only intra-arm. Therefore, we can rewrite $f_{i,j}(\\omega,\\pmb{\\mu})$ as: ", "page_idx": 20}, {"type": "equation", "text": "$$\nf_{i,j}(\\boldsymbol{\\omega},\\boldsymbol{\\mu})=\\sum_{k\\in\\{i,j\\}}\\operatorname*{inf}_{\\boldsymbol{\\theta}_{k}\\in\\mathrm{MF}}\\sum_{m\\in[M]}\\omega_{k,m}\\frac{d(\\mu_{k,m},\\boldsymbol{\\theta}_{k,m})}{\\lambda_{m}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Furthermore, applying the same reasoning as in the proof of Lemma C.1, we can further rewrite $f_{i,j}(\\omega,\\pmb{\\mu})$ as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{i,j}(\\omega,\\mu)=\\displaystyle\\sum_{k\\in\\{i,j\\}}\\operatorname*{inf}_{\\psi\\in\\mathbb{R}}\\displaystyle\\sum_{m\\in[M]}\\omega_{k,m}\\frac{d^{+}(\\mu_{k,m},\\psi-\\xi_{m})+d^{-}(\\mu_{k,m},\\psi+\\xi_{m})}{\\lambda_{m}}}\\\\ &{\\qquad\\quad=\\displaystyle\\sum_{k\\in\\{i,j\\}}\\sum_{m\\in[M]}\\omega_{k,m}\\frac{d^{-}(\\mu_{k,m},\\psi_{k}^{*}+\\xi_{m})+d^{+}(\\mu_{k,m},\\psi_{k}^{*}-\\xi_{m})}{\\lambda_{m}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "At this point, we notice that due to Lemma C.1 we know that $\\theta_{j,M}^{*}\\,=\\,\\psi_{j}^{*}$ and $\\theta_{i,M}^{*}=\\psi_{i}^{*}$ , thus concluding the first part of the proof. ", "page_idx": 20}, {"type": "text", "text": "Consider now the case in which $\\theta_{j,M}^{*}\\;=\\;\\theta_{i,M}^{*}$ holds. Then, applying Lemma C.1, and using $\\theta_{i,M}^{*}=\\theta_{j,M}^{*}$ , we can rewrite $f_{i,j}(\\omega,\\pmb{\\mu})$ as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\nf_{i,j}(\\omega,\\mu)=\\operatorname*{inf}_{\\eta\\in\\mathbb{R}}\\sum_{\\substack{k\\in\\{i,j\\}}}\\sum_{m=1}^{M}\\frac{\\omega_{k,m}}{\\lambda_{m}}\\left(d^{+}(\\mu_{k,m},\\eta-\\xi_{m})+d^{-}(\\mu_{k,m},\\eta+\\xi_{m})\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "thus concluding the proof. ", "page_idx": 21}, {"type": "text", "text": "Given this result, we recall that the definitions of ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi_{i}^{*}=\\underset{\\psi\\in\\mathbb{R}}{\\mathrm{argmin}}\\,\\displaystyle\\sum_{m\\in[M]}\\omega_{i,m}\\frac{d^{-}(\\mu_{i,m},\\psi+\\xi_{m})+d^{+}(\\mu_{i,m},\\psi-\\xi_{m})}{\\lambda_{m}}}\\\\ &{\\eta_{i,j}^{*}=\\underset{\\eta\\in\\mathbb{R}}{\\mathrm{argmin}}\\,\\displaystyle\\sum_{k\\in\\{i,j\\}}\\sum_{m\\in[M]}\\omega_{k,m}\\frac{d^{-}(\\mu_{k,m},\\eta+\\xi_{m})+d^{+}(\\mu_{k,m},\\eta-\\xi_{m})}{\\lambda_{m}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Corollary C.2. Consider $\\pmb{\\mu}\\in\\mathcal{M}_{\\mathrm{MF}}$ , and $a\\in[K]$ such that $a\\ne\\star.$ . Then it holds that: ", "page_idx": 21}, {"type": "equation", "text": "$$\nf_{\\star,a}(\\omega,\\mu)=\\qquad\\operatorname*{inf}_{\\eta\\in[\\mu_{a,M},\\mu_{\\star,M}]}\\sum_{i\\in\\{\\star,a\\}}\\sum_{m=1}^{M}\\omega_{i,m}\\frac{d^{-}(\\mu_{i,m},\\eta+\\xi_{m})+d^{+}(\\mu_{i,m},\\eta-\\xi_{m})}{\\lambda_{m}}\\omega_{i,m}(\\omega+\\mu_{m,m})\\omega_{m}(\\omega+\\mu_{m,m})\\omega_{m}(\\omega+\\eta)}\\\\ {=\\qquad\\operatorname*{inf}_{\\eta\\in[\\mu_{a,M},\\mu_{\\star,M}]}\\sum_{m=1}^{M}\\omega_{\\star,m}\\frac{d^{-}(\\mu_{\\star,m},\\eta+\\xi_{m})}{\\lambda_{m}}+\\omega_{a,m}\\frac{d^{+}(\\mu_{a,m},\\eta-\\xi_{m})}{\\lambda_{m}}\\omega_{m}(\\omega+\\mu_{m,m})\\omega_{m}(\\omega+\\eta)\\omega_{m}(\\omega+\\eta)\\omega_{m}(\\omega+\\eta),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. At this point, we notice that whenever $\\pmb{\\mu}\\in\\mathcal{M}_{\\mathrm{MF}}$ it holds that $f_{\\star,a}$ can always be expressed as Equation (7). This is direct by the condition on $\\psi$ \u2019s in Lemma 4.2. Furthermore, it also holds at $\\eta_{\\star,a}^{*}$ that $d^{-}(\\mu_{a,m},\\eta_{\\star,a}^{\\ast}+\\xi_{m})\\stackrel{\\cdot}{=}0$ , and $d^{+}(\\mu_{\\star,m},\\eta_{\\star,a}^{\\ast}-\\xi_{m})=0$ . This is a consequence of the fact that $\\eta_{\\star,a}^{*}\\in[\\mu_{a,M},\\mu_{\\star,M}]$ for all weights $\\omega$ . Indeed, $\\eta_{\\star,a}^{*}\\in[\\mu_{a,M},\\mu_{\\star,M}]$ holds due to monotonicity property of the KL divergence. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "We now analyze in more detail Equations (26) and (27). In particular, we begin by focusing on Equation (26). Taking the gradient in Equation (27) w.r.t. the optimization variable $\\eta$ , and setting it equal to 0, we obtain that any optimal point $\\eta_{i,j}^{*}(\\omega)$ needs to satisfy the following equation: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta\\left(\\underset{a\\in\\{i,j\\}}{\\sum}\\,\\displaystyle\\sum_{m=1}^{M}\\frac{\\omega_{a,m}}{\\lambda_{m}}\\left(\\overline{{k}}_{a,m}\\frac{1}{v(\\eta-\\xi_{m})}+\\underline{{k}}_{a,m}\\frac{1}{v(\\eta+\\xi_{m})}\\right)\\right)=}\\\\ &{\\displaystyle\\sum_{a\\in\\{i,j\\}}\\sum_{m=1}^{M}\\frac{\\omega_{a,m}}{\\lambda_{m}}\\left(\\overline{{k}}_{a,m}\\frac{\\mu_{a,m}+\\xi_{m}}{v(\\eta-\\xi_{m})}+\\underline{{k}}_{a,m}\\frac{\\mu_{a,m}-\\xi_{m}}{v(\\eta+\\xi_{m})}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we recall that $\\overline{{k}}_{a,m}(\\eta)$ and $\\underline{{k}}_{a,m}(\\eta)$ are given by: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{k}}_{a,m}(\\eta)=\\mathbf{1}\\left\\{\\eta\\geq\\mu_{a,m}+\\xi_{m}\\right\\}}\\\\ &{\\underline{{k}}_{a,m}(\\eta)=\\mathbf{1}\\left\\{\\eta\\leq\\mu_{a,m}-\\xi_{m}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Given this intermediate result, we now investigate in more depth the solution of Equation (28). ", "page_idx": 21}, {"type": "text", "text": "Lemma 4.4. Consider $\\pmb{\\mu}\\,\\in\\,\\Theta^{K M}$ and $\\omega\\;\\in\\;\\Delta_{K\\times M}$ such that $f_{i,j}(\\omega,\\mu)\\;>\\;0$ . Suppose that $\\psi_{i,\\,}^{*}\\geq\\psi_{j}^{*}$ holds. Then, there exists a unique minimizer $\\eta_{i,j}^{*}(\\omega)$ of Equation (7) which is the unique solution of the following equation of $\\eta$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\eta=\\frac{\\sum_{a\\in\\{i,j\\}}\\sum_{m}\\frac{\\omega_{a,m}}{\\lambda_{m}}\\left(\\overline{{k}}_{a,m}(\\eta)\\frac{\\mu_{a,m}+\\xi_{m}}{v(\\eta-\\xi_{m})}+\\underline{{k}}_{a,m}(\\eta)\\frac{\\mu_{a,m}-\\xi_{m}}{v(\\eta+\\xi_{m})}\\right)}{\\sum_{a\\in\\{i,j\\}}\\sum_{m}\\frac{\\omega_{a,m}}{\\lambda_{m}}\\left(\\overline{{k}}_{a,m}(\\eta)\\frac{1}{v(\\eta-\\xi_{m})}+\\underline{{k}}_{a,m}(\\eta)\\frac{1}{v(\\eta+\\xi_{m})}\\right)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\overline{{k}}_{a,m}(x)=\\mathbf{1}\\{x\\geq\\mu_{a,m}+\\xi_{m}\\}$ and $\\underline{{k}}_{a,m}(x)=\\mathbf{1}\\{x\\leq\\mu_{i,m}-\\xi_{m}\\}$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Let us analyze: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f_{i,j}(\\omega,\\mu)=\\displaystyle\\operatorname*{inf}_{\\eta\\in\\mathbb{R}}\\sum_{\\substack{a\\in\\{i,j\\}\\,m\\in M}}\\omega_{a,m}\\frac{d^{-}(\\mu_{a,m},\\eta+\\xi_{m})}{\\lambda_{m}}+\\omega_{a,m}\\frac{d^{+}(\\mu_{a,m},\\eta-\\xi_{m})}{\\lambda_{m}}}\\\\ {\\quad:=\\displaystyle\\operatorname*{inf}_{\\eta\\in\\mathbb{R}}g_{i,j}(\\omega,\\mu,\\eta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "At this point, we proceed by contradiction. Suppose that there exists $x_{1},x_{2}\\quad\\in$ $\\mathrm{argmin}_{\\eta\\in\\mathbb{R}}\\,g_{i,j}(\\omega,\\pmb{\\mu},\\eta)$ such that $x_{1}\\neq\\;x_{2}$ . From the convexity of $g_{i,j}(\\omega,\\mu,\\eta)$ w.r.t. $\\eta$ , we know that any $x\\in[x_{1},x_{2}]$ belongs to the argmin set as well. Furthermore, for all $x\\in[x_{1},x_{2}]$ , since $f_{i,j}(\\omega,\\mu)>0$ , at least one of the following condition is satisfied: ", "page_idx": 22}, {"type": "text", "text": "Therefore, from Equation (28), we obtain that all $x\\,\\in\\,[x_{1},x_{2}]$ are fixed points of the following Equation: ", "page_idx": 22}, {"type": "equation", "text": "$$\nx=\\frac{\\sum_{a\\in\\{i,j\\}}\\sum_{m=1}^{M}\\frac{\\omega_{a,m}}{\\lambda_{m}}\\left(\\overline{{k}}_{a,m}(x)\\frac{\\mu_{a,m}+\\xi_{m}}{v(x-\\xi_{m})}+\\underline{{k}}_{a,m}(x)\\frac{\\mu_{a,m}-\\xi_{m}}{v(x+\\xi_{m})}\\right)}{\\left(\\sum_{a\\in\\{i,j\\}}\\sum_{m=1}^{M}\\frac{\\omega_{a,m}}{\\lambda_{m}}\\left(\\overline{{k}}_{a,m}(x)\\frac{1}{v(x-\\xi_{m})}+\\underline{{k}}_{a,m}(x)\\frac{1}{v(x+\\xi_{m})}\\right)\\right)},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "At this point, we notice that for any couple of different $\\widetilde{x}_{1},\\widetilde{x}_{2}$ that satisfies Equation (29), there exists at least one arm $a\\,\\in\\,\\{i,j\\}$ and one fidelity $m\\,<\\,M$ such that at least one of the following two conditions hold: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bullet\\;\\overline{{k}}_{a,m}(\\tilde{x}_{1})\\neq\\overline{{k}}_{a,m}(\\tilde{x}_{2})}\\\\ {\\bullet\\;\\underline{{k}}_{a,m}(\\tilde{x}_{2})\\neq\\underline{{k}}_{a,m}(\\tilde{x}_{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This however, is possible only for a finite number of points, while the interval $[x_{1},x_{2}]$ contains infinitely many optimal points. Therefore, there exists a unique solution $\\eta_{i,j}^{*}(\\omega)~~\\in$ $\\mathrm{argmin}_{\\eta\\in\\mathbb{R}}\\,g_{i,j}(\\omega,\\pmb{\\mu},\\eta)$ , and, furthermore, it is a solution of Equation (29), thus concluding the proof. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Given this result, we continue by providing a result on how to compute the derivative of $f_{i,j}(\\omega,\\pmb{\\mu})$ whenever $f_{i,j}(\\omega,\\pmb{\\mu})$ is given by Equation (7). ", "page_idx": 22}, {"type": "text", "text": "Lemma C.3. Consider ${\\pmb{\\mu}}\\in\\Theta^{K M}$ and $\\omega\\in\\Delta_{K\\times M}$ such that $f_{i,j}(\\omega,\\mu)>0.$ . Furthermore, suppose that $f_{i,j}(\\omega,\\pmb{\\mu})$ is given by Equation (7). Then, for all $a\\in\\{i,j\\}$ and all $m\\in[M]$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\partial f_{i,j}(\\boldsymbol{\\omega},\\mu)}{\\partial\\omega_{a,m}}=\\frac{d^{+}(\\mu_{a,m},\\eta_{i,j}^{*}+\\xi_{m})+d^{-}(\\mu_{i,m},\\eta_{i,j}^{*}-\\xi_{m})}{\\lambda_{m}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. First of all, we notice that, since $f_{i,j}(\\omega,\\mu)>0$ holds, and since $f_{i,j}(\\omega,\\pmb{\\mu})$ is expressed as in Equation (7), then, thanks to Lemma 4.4, we know that $\\eta_{i,j}^{*}(\\omega)$ is the unique optimum of the Equation (7). In the rest of this proof, we will explicit the relationship between $f_{i,j}$ and $\\eta_{i,j}^{*}(\\omega)$ by writing $f_{i,j}(\\omega,\\pmb{\\mu},\\eta_{i,j}^{*}(\\omega))$ . At this point, fix $a\\in\\{i,j\\}$ and $m\\in[M]$ . Then, it is easy to verify from Equation (10) that both the right and left derivative of $\\eta_{i,j}^{*}(\\omega)$ w.r.t. $\\omega_{a,m}$ exists. Suppose for a moment that they are equal, then we have that $\\frac{\\partial\\eta_{i,j}^{*}(\\omega)}{\\partial\\omega_{a,m}}$ exists and it continuous. Therefore, we obtain ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial}{\\partial\\omega_{a,m}}f_{i,j}(\\omega,\\mu,\\eta_{i,j}^{*}(\\omega))=\\frac{\\partial f_{i,j}}{\\partial\\omega_{a,m}}(\\omega,\\mu,\\eta_{i,j}^{*}(\\omega))+\\frac{\\partial f_{i,j}}{\\partial\\eta_{i,j}^{*}(\\omega)}(\\omega,\\mu,\\eta_{i,j}^{*}(\\omega))\\frac{\\partial\\eta_{i,j}^{*}(\\omega)}{\\partial\\omega_{a,m}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\frac{\\partial f_{i,j}}{\\partial\\omega_{a,m}}(\\omega,\\mu,\\eta_{i,j}^{*}(\\omega))}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\frac{d^{+}(\\mu_{a,m},\\eta_{i,j}^{*}+\\xi_{m})+d^{-}(\\mu_{a,m},\\eta_{i,j}^{*}-\\xi_{m})}{\\lambda_{m}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where in the second step we have used that \u2202\u03b7\u2202i\u2217,fji,(j\u03c9)(\u03c9, \u00b5, \u03b7i\u2217,j(\u03c9)) = 0 since \u03b7i\u2217,j(\u03c9) is a minimizer of Equation (7). ", "page_idx": 23}, {"type": "text", "text": "Similarly, whenever, the right and the left derivatives of $\\eta_{i,j}^{*}(\\omega)$ are different12, we can follow similar arguments, but analyzing left and right derivatives, and we will obtain an identical result. Indeed, this does not introduce discontinuity issue in the derivatives of $f_{i,j}$ thanks to the fact that $\\eta_{i,j}^{*}(\\omega)$ is a minimizer of Equation (7). \u53e3 ", "page_idx": 23}, {"type": "text", "text": "At this point, it remains to analyze in more detail the case in which we have that $f_{i,j}(\\omega)$ is expressed as in Equation (6). ", "page_idx": 23}, {"type": "text", "text": "Lemma C.4. Consider ${\\pmb{\\mu}}\\in\\Theta^{K M}$ and $\\omega\\in\\Delta_{K\\times M}$ such that $f_{i,j}(\\omega,\\mu)>0$ holds. Furthermore, suppose that $\\psi_{j}^{*}>\\psi_{i}^{*}$ . Then, for each $a\\in\\{i,j\\}$ , there exists a unique minimizer $\\psi_{a}^{*}$ of Equation (6) which is the unique solution of the following equation of $\\psi$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\psi=\\frac{\\sum_{m=1}^{M}\\frac{\\omega_{a,m}}{\\lambda_{m}}\\left(\\overline{{k}}_{a,m}(\\psi)\\frac{\\mu_{a,m}+\\xi_{m}}{v(\\psi-\\xi_{m})}+\\underline{{k}}_{a,m}(\\psi)\\frac{\\mu_{a,m}-\\xi_{m}}{v(\\psi+\\xi_{m})}\\right)}{\\left(\\sum_{m=1}^{M}\\frac{\\omega_{a,m}}{\\lambda_{m}}\\left(\\overline{{k}}_{a,m}(\\psi)\\frac{1}{v(\\psi-\\xi_{m})}+\\underline{{k}}_{a,m}(\\psi)\\frac{1}{v(\\psi+\\xi_{m})}\\right)\\right)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. The proof follows by noticing that, for each $a\\in\\{i,j\\}$ , the optimization problem in Equation (6) is an unconstrained convex optimization problem in $\\psi$ . Taking the derivative and setting it equal to 0 yields the desired result. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "At this point, we proceed by showing how to compute the partial derivatives of $f_{i,j}(\\omega)$ whenever it is expressed as in Equation (6). ", "page_idx": 23}, {"type": "text", "text": "Lemma C.5. Consider ${\\pmb{\\mu}}\\in\\Theta^{K M}$ and $\\omega\\in\\Delta_{K\\times M}$ such that $f_{i,j}(\\omega,\\mu)>0$ . Furthermore, suppose that $\\psi_{j}^{*}>\\psi_{i}^{*}$ . Then, for all $m\\in[M]$ it holds that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\partial f_{i,j}(\\omega,\\mu)}{\\partial\\omega_{a,m}}=\\frac{d^{+}(\\mu_{a,m},\\psi_{a}^{*}+\\xi_{m})+d^{-}(\\mu_{a,m},\\psi_{a}^{*}-\\xi_{m})}{\\lambda_{m}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. The proof is a straightforward adaptation of the proof of Lemma C.3. ", "page_idx": 23}, {"type": "text", "text": "Finally, we are now ready to prove our result on the sub-gradient of $F(\\omega,\\mu)$ . ", "page_idx": 23}, {"type": "text", "text": "Theorem 4.3. Consider ${\\pmb{\\mu}}\\in\\Theta^{K M}$ and $\\omega\\in\\Delta_{K\\times M}$ such that $F(\\omega,\\mu)>0$ holds. Let $(i,a)\\in[K]^{2}$ be a pair of arms that attains the max-min value in Equation (2). Then a sub-gradient $\\nabla F(\\omega,\\pmb{\\mu})$ of $F(\\omega,\\mu)$ w.r.t. to $\\omega$ is given by one of the two following expressions: for $j\\in\\{a,i\\}$ and $m\\in[M]$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla F(\\omega,\\mu)_{j,m}=\\frac{d^{+}\\left(\\mu_{j,m},\\eta_{i,a}^{*}-\\xi_{m}\\right)+d^{-}\\left(\\mu_{j,m},\\eta_{i,a}^{*}+\\xi_{m}\\right)}{\\lambda_{m}}\\quad\\mathrm{if~}\\psi_{i}^{*}\\geq\\psi_{a}^{*}\\,,}\\\\ &{\\nabla F(\\omega,\\mu)_{j,m}=\\frac{d^{+}\\left(\\mu_{j,m},\\psi_{j}^{*}-\\xi_{m}\\right)+d^{-}\\left(\\mu_{j,m},\\psi_{j}^{*}+\\xi_{m}\\right)}{\\lambda_{m}}\\quad\\mathrm{otherwise}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "That sub-gradient $\\nabla F(\\omega,\\pmb{\\mu})$ is $0$ in all the remaining $K M-2M$ dimensions. ", "page_idx": 23}, {"type": "text", "text": "Proof. The proof follows by the definition of $F(\\omega,\\pmb{\\mu})$ together with Lemma 4.2, Lemma C.3, and Lemma C.5. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "We now show a sufficient condition for $F(\\omega,\\mu)>0$ to hold when ${\\pmb{\\mu}}\\in\\Theta^{K M}$ . ", "page_idx": 24}, {"type": "text", "text": "Lemma C.6. Consider $\\pmb{\\mu}\\,\\in\\,\\Theta^{K M}$ such that there exists $\\star$ for which $\\mu_{\\star,M}\\ >\\ \\operatorname*{max}_{a\\neq\\star}\\mu_{a,M}$ . Furthermore, consider $\\omega\\in\\Delta_{K\\times M}$ such that $\\omega_{i,M}>0$ holds for all $i\\in[K]$ . Then, we have that $F(\\omega,\\mu)>0$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. From the definition of $F$ , and the definition of $\\star$ , we have that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(\\omega,\\mu)\\geq\\underset{a\\neq\\star\\,\\,\\theta_{a}\\in\\mathrm{MF},\\,\\theta_{*}\\in\\mathrm{MF}}{\\operatorname*{inf}}\\!\\!\\!\\!\\operatorname*{im}_{j\\in\\{\\star,\\alpha\\}}\\sum_{\\substack{m\\in[M]}}\\omega_{j,m}\\frac{d\\left(\\mu_{j,m},\\theta_{j,m}\\right)}{\\lambda_{m}}}\\\\ &{\\qquad\\qquad\\geq\\underset{a\\neq\\star}{\\operatorname*{min}}\\,\\underset{y\\geq x}{\\operatorname*{inf}}\\,\\omega_{\\star,M}\\frac{d\\left(\\mu_{\\star,M},x\\right)}{\\lambda_{M}}+\\omega_{a,M}\\frac{d\\left(\\mu_{a,M},y\\right)}{\\lambda_{M}}}\\\\ &{\\qquad=\\underset{a\\neq\\star\\,\\eta\\in\\mathrm{[}\\mu_{a,M},\\mu_{\\star,M}]}{\\operatorname*{min}}\\omega_{\\star,M}\\frac{d\\left(\\mu_{\\star,M},\\eta\\right)}{\\lambda_{M}}+\\omega_{a,M}\\frac{d\\left(\\mu_{a,M},\\eta\\right)}{\\lambda_{M}}}\\\\ &{\\qquad>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where in the last step we have used the fact that $\\mu_{\\star,M}>\\mu_{i,M}$ for all $i\\neq\\star$ , together with $\\omega_{i,M}>0$ for all $i\\in[K]$ . \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Furthermore, we show that the sequence of weights generated by Algorithm 1 satisfy $\\omega_{i,M}>0$ for all $i\\in[K]$ ", "page_idx": 24}, {"type": "text", "text": "Lemma C.7. The sequence of weights $\\{\\tilde{\\omega}(t)\\}_{t}$ satisfy $\\omega_{i,M}(t)>0$ for all $i\\in[K]$ and for all $t$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. We begin by recalling the definition of $\\tilde{\\omega}(t)$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{\\omega}(t+1)\\in\\operatornamewithlimits{a r g m a x}_{\\omega\\in\\Delta_{K\\times M}}\\alpha_{t+1}\\sum_{s=K M}^{t}\\omega\\cdot\\mathrm{Clip}_{s}\\left(\\nabla F(\\tilde{\\omega}(s),\\hat{\\mu}(s))-\\mathrm{kl}(\\omega,\\overline{{\\omega}})\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "From this definition, thanks to the property of $\\boldsymbol{\\mathrm{kl}}$ , we have that $\\tilde{\\omega}_{a,m}(t)>0$ for all $a\\in[K]$ , and all $m\\in[M]$ . \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Remark C.8. It follows by combining Lemma C.6 and Lemma C.7, that $F(\\omega(t),\\hat{\\pmb\\mu}(t))=0$ might happen only when there are multiple best arms at fidelity $M$ . Whenever this condition is encountered, it is possible to project the bandit model ${\\hat{\\pmb{\\mu}}}(t)$ to have a unique optimal arm (e.g., by adding a small $\\epsilon>0$ to one of the optimal arms). When looking at the proof of Theorem 4.1, we can see that this does not impact its theoretical guarantees as (i) on the good event $\\mathcal{E}_{T}$ this does not happen, and, Lemma C.17 holds unchanged. ", "page_idx": 24}, {"type": "text", "text": "C.2 Smoothness of $F(\\omega,\\mu)$ ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Lemma C.9. For any set $S\\subseteq\\Theta^{K M}$ and any subset of arms $A\\subseteq[K]$ , the function $(\\omega,\\pmb{\\mu})\\mapsto$ $\\begin{array}{r}{\\operatorname*{inf}_{\\theta\\in S}\\sum_{a\\in A,m\\in[M]}\\omega_{a,m}\\frac{d\\left(\\mu_{a,m},\\theta_{a,m}\\right)}{\\lambda_{m}}}\\end{array}$ is jointly continuous on $\\Delta_{K\\times M}\\times\\Theta^{K M}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. We apply (a trivial generalization of) Lemma 27 of [3]. The lemma in that paper is stated for a set of alternative models, but the proof actually works for any set $S$ . Likewise, it is stated for the case of $\\lambda_{m}=1$ , but since it works for an arbitrary Bregman divergence $d$ it applies to a rescaled version as well. To deal with the restriction to a subset of arms $A$ instead of all arms, we can view the function as a function of $\\left(\\omega_{A\\times\\left[M\\right]},\\mu_{A}\\right)$ , where we restrict the vectors to the arms in $A$ , and continuity of the original function is equivalent to continuity of the restricted version. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Lemma C.10. The function $(\\omega,\\mu)\\mapsto F(\\omega,\\mu)$ is jointly continuous on $\\Delta_{K\\times M}\\times\\Theta^{K M}$ ", "page_idx": 24}, {"type": "text", "text": "Proof. By definition, $\\begin{array}{r l r}{F(\\omega,\\mu)}&{{}\\,\\,=\\,}&{\\,\\,\\operatorname*{max}_{i}\\operatorname*{min}_{a\\neq i}f_{i,a}(\\omega,\\mu)}\\end{array}$ with $\\begin{array}{r l r}{f_{i,a}(\\omega,\\mu)}&{{}}&{=}\\end{array}$ $\\begin{array}{r}{\\operatorname*{inf}_{\\theta\\in S_{i,a}}\\sum_{a\\in A_{i,a},m\\in[M]}\\omega_{a,m}\\frac{d(\\mu_{a,m},\\theta_{a,m})}{\\lambda_{m}}}\\end{array}$ md(\u00b5a,\u03bbmm,\u03b8a,m) for Si,a = {\u03b8 \u2208 MMF | \u03b8a,M \u2265 \u03b8i,M} and $A_{i,a}=\\{i,a\\}$ . Since a minimum of finitely many continuous functions is continuous and likewise for the maximum, it suffices to show that each $f_{i,a}$ is jointly continuous. This is true by Lemma C.9. ", "page_idx": 24}, {"type": "text", "text": "Proof. The set $\\Delta_{K\\times M}\\times C$ is compact and $F$ is continuous, hence it is uniformly continuous on that set. ", "page_idx": 25}, {"type": "text", "text": "Lemma C.12. Let $\\pmb{\\mu}\\in\\Theta^{K M}$ . For all $\\varepsilon>0$ , there exists $\\kappa_{\\epsilon}>0$ such that for all $\\omega\\in\\triangle_{K\\times M}$ and all \u00b5\u2032 \u2208\u0398KM ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\left|\\mu-\\mu^{\\prime}\\right|\\right|_{\\infty}\\leq\\kappa_{\\epsilon}\\implies\\left|F(\\omega,\\mu^{\\prime})-F(\\omega,\\mu)\\right|\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Take any compact ball $B(\\pmb{\\mu},\\kappa)$ for the norm $\\|\\cdot\\|_{\\infty}$ with $\\kappa>0$ centered at $\\pmb{\\mu}$ . Then $F$ is uniformly continuous on $\\Delta_{K\\times M}\\times B(\\mu,\\kappa)$ by Corollary C.11. This means that for any $\\varepsilon>0$ , there exists $\\kappa_{\\varepsilon}^{\\prime}>0$ such that for all $(\\omega^{\\prime},\\mu^{\\prime})\\in\\Delta_{K\\times M}\\times\\mathcal{B}(\\pmb{\\mu},\\kappa)$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\omega-\\omega^{\\prime}\\|_{\\infty}\\leq\\kappa_{\\varepsilon}^{\\prime}\\;\\wedge\\;\\|\\pmb{\\mu}-\\pmb{\\mu}^{\\prime}\\|_{\\infty}\\leq\\kappa_{\\varepsilon}^{\\prime}\\implies|F(\\omega^{\\prime},\\pmb{\\mu}^{\\prime})-F(\\omega,\\pmb{\\mu})|\\leq\\varepsilon\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We can take $\\kappa_{\\varepsilon}=\\operatorname*{min}\\{\\kappa,\\kappa_{\\varepsilon}^{\\prime}\\}$ to remove the condition $\\mu^{\\prime}\\in\\mathcal{B}(\\mu,\\kappa)$ . The result of the Lemma is this for the special case $\\omega^{\\prime}=\\omega$ . \u53e3 ", "page_idx": 25}, {"type": "text", "text": "C.3 Correctness ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In the following, we propose an analysis on the correctness which is based on the concentration results provided in [22]. We notice that these results are based on Gaussian distributions. Nevertheless, at the cost of a more involved notation, it is possible to extend all the results of this work for canonical exponential families using, e.g., Theorem 7 in [19]. ", "page_idx": 25}, {"type": "text", "text": "At this point, let us consider the following value of $\\beta_{t,\\delta}$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\beta_{t,\\delta}=\\log\\left(\\frac{K}{\\delta}\\right)+2M\\log\\left(4\\log\\left(\\frac{K}{\\delta}\\right)+1\\right)+12M\\log\\left(\\log(t)+3\\right)+2M\\tilde{C},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\tilde{C}$ is a universal constant (see Proposition 1 in [22]). Then, we can show the following result.   \nProposition C.13. Let $\\delta>0$ , then it holds that $\\mathbb{P}_{\\mu}(\\hat{a}_{\\tau_{\\delta}}\\neq\\ast)\\leq\\delta$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. With probabilistic arguments we have that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{\\mu}\\big(\\hat{a}_{\\tau_{5}}\\neq\\star\\big)\\leq\\mathbb{P}_{\\mu}\\left(\\exists t\\geq K M,\\,\\exists i\\neq\\star,\\,\\operatorname*{min}_{j\\neq i}(c(t),\\hat{\\mu}(t))\\geq\\beta_{t,\\delta}\\right)}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\sum_{i\\neq k}\\mathbb{P}_{\\mu}\\left(\\exists t\\geq K M,\\,\\operatorname*{min}_{j\\neq i}f_{i,j}(C(t),\\hat{\\mu}(t))\\geq\\beta_{t,\\delta}\\right)}\\\\ &{\\qquad\\leq\\displaystyle\\sum_{i\\neq k}\\mathbb{P}_{\\mu}\\left(\\exists t\\geq K M,\\,f_{i,\\star}(C(t),\\hat{\\mu}(t))\\geq\\beta_{t,\\delta}\\right)}\\\\ &{\\qquad\\leq\\displaystyle\\sum_{i\\neq k}\\mathbb{P}_{\\mu}\\left(\\exists t\\geq K M,\\,\\sum_{k\\in\\{i,\\star\\}}\\sum_{m\\in[M]}N_{k,m}d(\\hat{\\mu}_{a,m}(t),\\mu_{a,m})\\geq\\beta_{t,\\delta}\\right)}\\\\ &{\\qquad<\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where in fourth step we have used the definition of $f_{i,\\star}$ , and in the last one Proposition 1 in [22] together with a union bound on $K$ . ", "page_idx": 25}, {"type": "text", "text": "C.4 Auxiliary lemmas ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "This section contains auxiliary lemmas that will be used in the analysis of Algorithm 1. ", "page_idx": 25}, {"type": "text", "text": "Lemma C.14. For all $a\\in[K]$ , $m\\in[M]$ , and for all $t\\geq1$ , it holds that $\\begin{array}{r}{N_{a,m}(t)\\ge\\frac{\\sqrt{t}}{4K M}{-}\\mathrm{ln}(K M)}\\end{array}$ . Furthermore, it holds that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\bigg\\vert\\bigg\\vert\\sum_{s=0}^{t}\\tilde{\\pi}(s)-N(t)\\bigg\\vert\\bigg\\vert_{\\infty}\\leq2\\ln(K M)\\sqrt{t}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. This lemma is a simple combination of Lemma 3 in [22] with the tracking result of [5]. Using algebraic manipulations, we have that: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{N_{\\alpha,m}(t)\\geq\\sum_{s=1}^{t}\\pi_{\\alpha,m}^{\\prime}(s)-\\left|N_{\\alpha,m}(t)-\\sum_{s=1}^{t}\\pi_{\\alpha,m}^{\\prime}(s)\\right|}}\\\\ &{}&{\\geq\\sum_{s=1}^{t}\\pi_{\\alpha,m}^{\\prime}(s)-\\ln(K M)}\\\\ &{}&{\\geq\\displaystyle\\sum_{s=1}^{t}\\frac{\\gamma_{s}}{K M}-\\ln(K M)}\\\\ &{}&{=\\displaystyle\\frac{1}{K M}\\sum_{s=1}^{t}\\frac{1}{4\\sqrt{s}}-\\ln(K M)}\\\\ &{}&{\\geq\\displaystyle\\frac{\\sqrt{t}}{4K M}-\\ln(K M),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where, in the second step we have used Theorem 6 in [5], together with the fact that $\\ln(K M)\\ge$ $\\ln(4)\\geq1$ . ", "page_idx": 26}, {"type": "text", "text": "For the second part of the proof, we have that: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\Big|\\displaystyle\\sum_{s=1}^{t}\\tilde{\\pi}_{a,m}(s)-N_{a,m}(t)\\Big|\\leq\\Big|\\displaystyle\\sum_{s=1}^{t}\\pi_{a,m}^{\\prime}(s)-N_{a,m}(t)\\Big|+2\\displaystyle\\sum_{s=1}^{t}\\gamma_{s}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\ln(K M)+\\sqrt{t}}\\\\ &{\\qquad\\qquad\\qquad\\quad\\leq2\\ln(K M)\\sqrt{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma C.15. Consider $\\epsilon>0$ and $B\\in\\mathbb{R}$ such that $C^{*}(\\pmb{\\mu})^{-1}-B-\\epsilon>0$ . Then, there exists $a$ constant $C_{\\epsilon}$ such that, for ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{a,m}C_{a,m}(T)\\geq\\operatorname*{max}\\left\\{\\lambda_{M}C_{\\epsilon},\\frac{\\log\\left(\\frac{K}{\\delta}\\right)+2M\\log\\left(4\\log\\left(\\frac{K}{\\delta}\\right)+1\\right)}{C^{*}(\\mu)^{-1}-B-\\epsilon}\\right\\}:=C_{0}(\\epsilon,\\delta),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "it holds that: ", "page_idx": 26}, {"type": "equation", "text": "$$\nC^{*}(\\pmb{\\mu})^{-1}-B\\geq\\frac{\\beta_{T,\\delta}}{\\sum_{a,m}C_{a,m}(T)}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Let $C_{\\epsilon}$ be a constant that depends on $\\epsilon$ such that, for $T\\geq C_{\\epsilon}$ it holds that: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{12M\\log(\\log(T)+3)+2M\\tilde{C}}{\\lambda_{\\operatorname*{min}}}\\leq\\epsilon T.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then, for $\\begin{array}{r}{\\sum_{a,m}C_{a,m}(T)\\geq C_{0}(\\epsilon,\\delta)}\\end{array}$ , we have that: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\beta_{T,\\delta}}{\\sum_{a,m}C_{a,m}(T)}=\\frac{\\log\\left(\\frac{K}{\\delta}\\right)+2M\\log\\left(4\\log\\left(\\frac{K}{\\delta}\\right)+1\\right)+12M\\log(\\log(T)+3)+2M\\tilde{C}}{\\sum_{a,m}C_{a,m}(T)}}\\\\ &{\\phantom{\\frac{\\beta_{T,\\delta}}{\\sum_{a,m}C_{a,m}(T)}}\\leq\\frac{\\log\\left(\\frac{K}{\\delta}\\right)+2M\\log\\left(4\\log\\left(\\frac{K}{\\delta}\\right)+1\\right)}{\\sum_{a,m}C_{a,m}(T)}+\\epsilon}\\\\ &{\\phantom{\\frac{\\beta_{T,\\delta}}{\\sum_{a,m}C_{a,m}(T)}}\\leq C^{*}(\\mu)^{-1}-B,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 26}, {"type": "text", "text": "C.5 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Before diving into the proof of Theorem 4.1, we introduce some additional notation. We denote with $B_{\\infty}(x,\\kappa)$ the ball of radius $\\kappa$ centered at $x$ . Then, for all $T$ , and $\\epsilon>0$ , we introduce the following event: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\epsilon}(T)=\\bigcap_{t\\geq h(T)}^{T}\\left\\{\\hat{\\pmb{\\mu}}(t)\\in\\mathcal{B}_{\\infty}(\\pmb{\\mu},\\kappa_{\\epsilon})\\right\\},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $h(T)\\approx T^{1/4}$ . At this point, we present our result. Furthermore, we denote with $\\omega(t)$ the vector of empirical cost proportions, namely, for all (a, m), \u03c9a,m(t) =  i\u2208[K]C a,jm\u2208[(Mt)] Ci,j(t). ", "page_idx": 27}, {"type": "text", "text": "First of all, we introduce an initial result that controls the expectation of the stopping cost. ", "page_idx": 27}, {"type": "text", "text": "Lemma C.16. Consider $B$ such that $C^{*}(\\pmb{\\mu})^{-1}-B-\\epsilon>0,$ , and suppose that there exists a constant $T_{\\epsilon}$ such that, for all $T\\geq T_{\\epsilon}$ , it holds that $F(\\omega(T),\\hat{\\pmb{\\mu}}(T))\\,\\geq\\,F(\\pmb{\\omega}^{*}(\\pmb{\\mu}))\\,-\\,B$ on the good event $\\mathcal{E}_{\\epsilon}(T)$ . Then, it holds that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu}[c_{\\tau_{\\delta}}]\\leq\\lambda_{M}T_{\\epsilon}+C_{0}(\\epsilon,\\delta)+1+\\sum_{t=0}^{+\\infty}\\mathbb{P}_{\\mu}(\\mathcal{E}(T)^{c}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Using probabilistic arguments, we have that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mu}[c_{\\tau_{\\delta}}]=\\int_{0}^{+\\infty}\\mathbb{P}_{\\mu}(c_{\\tau_{\\delta}}>x)d x}\\\\ &{\\le\\lambda_{M}T_{\\epsilon}+C_{0}(\\epsilon,\\delta)+\\int_{\\lambda_{M}T_{\\epsilon}+C_{0}(\\epsilon,\\delta)}\\mathbb{P}_{\\mu}(c_{\\tau_{\\delta}}>x,c_{\\tau_{\\delta}}\\ge C_{0}(\\epsilon,\\delta))d x}\\\\ &{\\le\\lambda_{M}T_{\\epsilon}+C_{0}(\\epsilon,\\delta)+\\int_{\\lambda_{M}T_{\\epsilon}+C_{0}(\\epsilon,\\delta)}\\mathbb{P}_{\\mu}\\left(\\tau_{\\delta}>\\frac{x}{\\lambda_{M}},c_{\\tau_{\\delta}}\\ge C_{0}(\\epsilon,\\delta)\\right)d x}\\\\ &{\\le\\lambda_{M}T_{\\epsilon}+C_{0}(\\epsilon,\\delta)+1+\\underset{T=1}{\\overset{\\sum_{\\infty}^{\\infty}}{\\sum_{\\tau=1}^{\\infty}}}\\mathbb{P}_{\\mu}(\\tau_{\\delta}>T,c_{\\tau_{\\delta}}\\ge C_{0}(\\epsilon,\\delta))}\\\\ &{\\le\\lambda_{M}T_{\\epsilon}+C_{0}(\\epsilon,\\delta)+1+\\underset{T=1}{\\overset{\\sum_{\\infty}^{\\infty}}{\\sum_{\\tau=1}^{\\infty}}}\\mathbb{P}_{\\mu}(\\xi(T)^{\\epsilon})}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where (i) in the second inequality, we have upper bounded $c_{\\tau_{\\delta}}\\leq\\lambda_{M}\\tau_{\\delta}$ , (ii) in the third inequality, we have used the fact that $\\tau_{\\delta}$ is an integer variable, and (iii) in the last inequality we have used that for all $T\\geq T_{\\epsilon}$ such that $\\begin{array}{r}{\\sum_{a,m}C_{a,m}^{\\,\\texttt{-}}(T)\\,\\geq\\,C_{0}(\\epsilon,\\delta)}\\end{array}$ , then we have that $\\mathcal{E}(\\bar{T})\\,\\subseteq\\,\\{\\tau_{\\delta}\\,<\\,T\\}$ . Indeed, combining Lemma C.15 with the definition $T_{\\epsilon}$ , we obtain that for all $T\\geq T_{\\epsilon}$ such that $\\begin{array}{r}{\\sum_{a,m}C_{a,m}(T)\\stackrel{}{\\geq}C_{0}(\\epsilon,\\delta)}\\end{array}$ , then we have that $\\mathcal{E}(T)\\subseteq\\{\\tau_{\\delta}<T\\}$ . This last step is direct by noticing that $\\begin{array}{r}{F(\\omega(T),\\hat{\\pmb\\mu}(T))\\ge\\frac{\\beta_{T,\\delta}}{\\sum_{a,m}C_{a,m}(T)}}\\end{array}$ implies stopping. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Lemma C.16 shows how to upper-bound the expected cost complexity. Notice that this result requires different arguments w.r.t. the usual ones that appears while controlling the expected sample complexity (see, e.g., [8]). ", "page_idx": 27}, {"type": "text", "text": "Then, we report a basic property of the sub-gradient ascent routing that is employed in our algorithm. Before doing that, we recall that, on the good-event $\\mathcal{E}_{T}$ , it holds that there exists a constant $L$ , that depends on $\\pmb{\\mu}$ , such that the empirical sub-gradients are uniformly-bounded by $L$ . ", "page_idx": 27}, {"type": "text", "text": "Lemma C.17. Let $\\begin{array}{r}{\\tilde{c}(t)=\\sum_{a,m}\\lambda_{m}\\tilde{\\pi}_{a,m}(s)}\\end{array}$ . Define $C_{r}:=\\log(K M)+K M G+4(L\\lambda_{M})^{2}+2G^{2}$ , and consider the sequence of weights $\\{\\tilde{\\omega}(t)\\}_{t}$ generated by Algorithm $^{\\,l}$ . Then, on the good event $\\mathcal{E}_{\\epsilon}(T)$ it holds that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{t=h(t)}^{T}\\tilde{c}(t)\\nabla F(\\omega^{*},\\hat{\\mu}(t))\\cdot(\\omega^{*}-\\tilde{\\omega}(t))\\leq C_{r}\\sqrt{T}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. The proof is identical to the one of Proposition 3 in [22]. The only difference is that, in our case, we need to multiply the scale of the sub-gradient $L$ by $\\lambda_{M}$ , to the additional presence of $\\tilde{c}(t)$ in the sequence of gains that we use in our sub-gradient ascent algorithm. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Finally, we show that there exists an additional problem dependent constant $C_{\\mu}$ that will be useful in performing some upper-bound reasoning in the proof of the final result. ", "page_idx": 28}, {"type": "text", "text": "Lemma C.18. Consider the following quantity: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\operatorname*{min}_{a\\neq\\star}\\operatorname*{inf}_{\\theta_{a},\\theta_{\\star}\\in\\mathrm{MF}}\\sum_{s=1}^{T}\\sum_{i\\in\\{\\star,a\\}}\\sum_{m}\\tilde{\\pi}_{i,m}(s)d(\\mu_{i,m},\\theta_{i,m})}{\\sum_{a,m}C_{a,m}(T)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "There exists a problem dependent constant $C_{\\mu}$ such that the previous equation can be upper bounded by: ", "page_idx": 28}, {"type": "equation", "text": "$$\nF(\\omega(T),\\pmb{\\mu})+\\frac{4\\ln(K M)M\\lambda_{M}C_{\\pmb{\\mu}}}{\\lambda_{\\operatorname*{min}}\\sqrt{T}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Let us begin by analyzing $F(\\omega(T),\\mu)$ . Fix $\\bar{a}$ such that $F(\\omega(T),\\pmb{\\mu})=f_{\\star,\\bar{a}}(\\omega(T),\\pmb{\\mu})$ . Moreover, consider $\\begin{array}{r}{\\theta_{\\star},\\theta_{\\bar{a}}\\in\\operatorname*{argmin}\\sum_{i\\in\\{\\star,\\bar{a}\\}}\\sum_{m}\\omega_{a,m}(T)\\frac{d(\\mu_{a,m},\\theta_{a,m})}{\\lambda_{m}}}\\end{array}$ . Then, consider the following difference: ", "page_idx": 28}, {"type": "equation", "text": "$$\nH:=\\frac{\\operatorname*{min}_{a\\neq\\star}\\operatorname*{inf}_{\\theta_{a},\\theta_{\\star}\\in\\mathrm{MF}}\\sum_{s=1}^{T}\\sum_{i\\in\\{\\star,a\\}}\\sum_{m}\\tilde{\\pi}_{i,m}(s)d(\\mu_{i,m},\\theta_{i,m})}{\\sum_{a,m}C_{a,m}(T)}-F(\\omega(T),\\mu).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then, the previous Equation can be upper bounded by: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H\\leq\\frac{\\sum_{i\\in\\{\\star,\\bar{a}\\}}\\sum_{m}\\left(\\sum_{s=1}^{T}\\widetilde{\\pi}_{i,m}(s)-N_{i,m}(T)\\right){d(\\mu_{i,m},\\theta_{i,m}^{\\ast})}}{\\sum_{a,m}C_{a,m}(T)}}\\\\ &{\\quad\\leq2\\ln(K M)\\sqrt{T}\\frac{\\sum_{i\\in\\{\\star,\\bar{a}\\}}\\sum_{m}d\\left(\\mu_{i,m},\\theta_{i,m}^{\\ast}\\right)}{\\sum_{a,m}C_{a,m}(T)}}\\\\ &{\\quad\\leq\\frac{4\\ln(K M)M\\sqrt{T}C_{\\mu}}{\\sum_{a,m}C_{a,m}(T)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where in the first step, we have used the definition of $\\theta_{\\bar{a}}^{\\ast},\\theta_{\\star}^{\\ast}$ and the definition of $\\omega(T)$ , in the second one, we have used Lemma C.14, and in the last one the facts that, thanks to definition $\\theta_{\\bar{a}}^{*},\\theta_{\\star}^{*}$ , there exists some problem dependent constant $C_{\\mu}$ such that $d(\\mu_{i,m},\\theta_{i,m}^{*})$ is bounded. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Theorem 4.1. For any multi-fidelity bandit model $\\pmb{\\mu}\\in\\mathcal{M}_{\\mathrm{MF}}$ , Algorithm 1 using the threshold $\\beta_{t,\\delta}$ given in (31) is $\\delta$ -correct and satisfies ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\delta\\rightarrow0}\\operatorname*{sup}_{\\log(1/\\delta)}\\leq C^{\\ast}(\\pmb{\\mu}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. The proof of the $\\delta$ -correctness is from Proposition C.13. ", "page_idx": 28}, {"type": "text", "text": "To prove the optimality, we first proceed by upper bounding the following quantity on the good event $\\mathcal{E}_{\\epsilon}(T)$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\nF(\\omega^{*},\\mu)-F(\\omega(T),\\hat{\\mu}(T)).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Define, for brevity, $\\widetilde T:=T-h(T)+1$ and $\\begin{array}{r}{\\tilde{c}(s):=\\sum_{a,m}\\lambda_{m}\\tilde{\\pi}_{a,m}(s)}\\end{array}$ . Then, we start by analyzing $F(\\omega^{*},\\mu)$ . On $\\mathcal{E}_{\\epsilon}(T)$ we have that: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(\\omega^{*},\\mu)=\\displaystyle\\frac{\\sum_{a,m}^{L}C_{a,m}(T)}{\\sum_{a,m}C_{a,m}(T)}F(\\omega^{*},\\mu)-\\frac{\\sum_{a,m}^{L}\\tilde{C}(s)}{\\sum_{a,m}C_{a,m}(T)}F(\\omega^{*},\\mu)+\\frac{\\sum_{c=1}^{L}\\tilde{C}(s)}{\\sum_{a,m}C_{a,m}(T)}F(\\omega^{*},\\mu)}\\\\ &{\\quad\\quad\\quad\\leq\\displaystyle\\frac{\\sum_{s=1}^{T}\\tilde{C}(s)}{\\sum_{a,m}C_{a,m}(T)}F(\\omega^{*},\\mu)+\\frac{F(\\omega^{*},\\mu)}{\\sum_{a,m}C_{a,m}(T)}2\\mathrm{ln}(K M)\\sqrt{T}}\\\\ &{\\quad\\quad\\quad\\leq\\displaystyle\\frac{\\sum_{s=1}^{T}\\tilde{C}(s)}{\\sum_{a,m}C_{a,m}(T)}F(\\omega^{*},\\mu)+\\frac{2\\ln(K M)F(\\omega^{*},\\mu)}{\\lambda_{\\operatorname*{min}}\\sqrt{T}}}\\\\ &{\\quad\\quad\\quad\\leq\\displaystyle\\sum_{a,m}^{T}\\!\\!\\!\\!(T)^{\\tilde{c}(s)}\\ F(\\omega^{*},\\mu)+\\frac{h(T)}{\\lambda_{\\operatorname*{min}}T}F(\\omega^{*},\\mu)+\\frac{2\\ln(K M)F(\\omega^{*},\\mu)}{\\lambda_{\\operatorname*{min}}\\sqrt{T}}}\\\\ &{\\quad\\quad\\quad\\leq\\displaystyle\\frac{\\sum_{a,m}^{T}C_{a,m}(T)F(\\omega^{*},\\mu(t)))}{\\sum_{a,m}C_{a,m}(T)}+\\frac{\\lambda_{M}\\tilde{T}^{\\tilde{c}}}{\\lambda_{\\operatorname*{min}}T}F(\\omega^{*},\\mu)+\\frac{2\\ln(K M)F(\\omega^{*},\\mu)}{\\lambda_{\\operatorname*{min}}\\sqrt{T}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where in the first inequality we have used Lemma C.14, while in the last step we have used Lemma C.12 together with the event $\\mathcal{E}_{\\epsilon}(T)$ . At this point, we focus our analysis on $\\frac{\\sum_{s=h(T)}^{T}\\tilde{c}(s)F(\\pmb{\\omega}^{*},\\hat{\\pmb{\\mu}}(t))}{\\sum_{a,m}C_{a,m}(T)}$ Define, for brevity, $g_{s}=\\widetilde{c}(s)\\nabla F(\\widetilde{\\pmb{\\omega}}(s),\\hat{\\pmb{\\mu}}(s))$ ; then, we have that: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\sum_{s=h(T)}^{T}\\tilde{c}(s)F(\\omega^{*},\\hat{\\mu}(s)))}{\\sum_{a,m}C_{a,m}(T)}=\\frac{\\sum_{s=h(T)}^{T}\\tilde{c}(s)\\,(F(\\omega^{*},\\hat{\\mu}))\\pm F(\\tilde{\\omega}(s),\\hat{\\mu}(s))))}{\\sum_{a,m}C_{a,m}(T)}}\\\\ &{\\leq\\frac{\\sum_{s=h(T)}^{T}\\tilde{c}(s)F(\\tilde{\\omega}(s),\\hat{\\mu}(s)))}{\\sum_{a,m}C_{a,m}(T)}+\\frac{\\sum_{s=h(T)}^{T}g_{s}\\cdot\\left(\\omega^{*}-\\tilde{\\omega}(s)\\right)}{\\sum_{a,m}C_{a,m}(T)}}\\\\ &{\\leq\\frac{\\sum_{s=h(T)}^{T}\\tilde{c}(s)F(\\tilde{\\omega}(s),\\hat{\\mu}(s)))}{\\sum_{a,m}C_{a,m}(T)}+\\frac{C_{r}}{\\lambda_{\\mathrm{min}}\\sqrt{T}}}\\\\ &{\\leq\\frac{\\sum_{s=h(T)}^{T}\\tilde{c}(s)F(\\tilde{\\omega}(s),\\mu)}{\\sum_{a,m}C_{a,m}(T)}+\\frac{C_{r}}{\\lambda_{\\mathrm{min}}\\sqrt{T}}+\\frac{\\lambda_{\\mathrm{M}}\\widetilde{T}\\epsilon}{\\lambda_{\\mathrm{min}}T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where in the first inequality we have used the concavity of $F$ , in the second one we have used Lemma C.17, and in the last one Lemma C.12 and the definition of $\\mathcal{E}_{\\epsilon}(T)$ . ", "page_idx": 29}, {"type": "text", "text": "Finally, we have that: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\sum_{s=1}^{T}\\tilde{c}(s)F(\\tilde{\\omega}(s),\\mu)}{\\sum_{a,m}C_{a,m}(T)}=\\frac{\\sum_{s=1}^{T}\\operatorname*{min}_{a\\neq s}\\operatorname*{inf}_{\\theta_{a},\\theta_{\\star}\\in\\mathbb{M}\\Gamma}\\sum_{i\\in\\{\\star,a\\}}\\sum_{m}\\tilde{\\pi}_{i,m}(s)d(\\mu_{i,m},\\theta_{i,m})}{\\sum_{a,m}C_{a,m}(T)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\operatorname*{min}_{a\\neq s}\\operatorname*{inf}_{\\theta_{a},\\theta_{\\star}\\in\\mathbb{M}\\Gamma}\\sum_{s=1}^{T}\\sum_{i\\in\\{\\star,a\\}}\\sum_{m}\\tilde{\\pi}_{i,m}(s)d(\\mu_{i,m},\\theta_{i,m})}{\\sum_{a,m}C_{a,m}(T)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq F(\\omega(T),\\mu)+\\frac{4\\ln(K M M)M C_{\\mu}}{\\lambda_{\\operatorname*{min}}\\sqrt{T}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq F(\\omega(T),\\hat{\\mu}(T))+\\frac{4\\ln(K M M)M C_{\\mu}}{\\lambda_{\\operatorname*{min}}\\sqrt{T}}+\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where (i) in the first equality we used the definition of $\\tilde{c}(s),\\tilde{\\omega}(s)$ and $F$ , in the second one we used Lemma C.18, and in the last one Lemma C.12. ", "page_idx": 29}, {"type": "text", "text": "Given this analysis, let us define: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{T}:=\\frac{2\\lambda_{M}\\widetilde{T}\\epsilon}{\\lambda_{\\operatorname*{min}}T}+\\frac{h(T)}{\\lambda_{\\operatorname*{min}}T}F(\\omega^{*},\\mu)+\\frac{2\\ln(K M)F(\\omega^{*},\\mu)}{\\lambda_{\\operatorname*{min}}\\sqrt{T}}+}\\\\ &{\\qquad\\qquad+\\,\\frac{C_{r}}{\\lambda_{\\operatorname*{min}}\\sqrt{T}}+\\frac{4\\ln(K M)M C_{\\mu}}{\\lambda_{\\operatorname*{min}}\\sqrt{T}}+\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Consider $T$ such that: ", "page_idx": 30}, {"type": "equation", "text": "$$\nT\\geq\\operatorname*{max}\\left\\{\\left(\\frac{2\\ln(K M)F(\\omega^{*},\\mu)}{\\lambda_{\\operatorname*{min}}\\epsilon}\\right)^{2},\\left(\\frac{C_{r}}{\\lambda_{\\operatorname*{min}}\\epsilon}\\right)^{2},\\left(\\frac{4\\ln(K M)M C_{\\mu}}{\\lambda_{\\operatorname*{min}}\\epsilon}\\right)^{2}\\right\\}:=T_{\\epsilon}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then, it holds that: ", "page_idx": 30}, {"type": "equation", "text": "$$\nB_{T}\\leq\\frac{2\\lambda_{M}\\epsilon}{\\lambda_{\\mathrm{min}}}+5\\epsilon=\\bar{B}_{\\epsilon},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and, consequently, we have that ", "page_idx": 30}, {"type": "equation", "text": "$$\nF(\\omega(T),\\hat{\\pmb{\\mu}}(T))\\geq F(\\pmb{\\omega}^{*},\\pmb{\\mu})-\\bar{B}_{\\epsilon}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Combining this result with Lemma C.15 and C.16, we obtain: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}[c_{\\tau_{\\delta}}]\\le\\lambda_{M}T_{\\epsilon}+C_{0}(\\delta,\\epsilon)+1+\\sum_{t=0}^{+\\infty}\\mathbb{P}_{\\mu}\\big(\\mathcal{E}_{\\epsilon}(t)^{c}\\big).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By Lemma C.14 and Lemma 19 in [8], we obtain that: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\delta\\to0}\\operatorname*{sup}_{\\log(1/\\delta)}\\leq\\frac{1}{C^{*}(\\pmb{\\mu})^{-1}-\\bar{B}_{\\epsilon}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Letting $\\epsilon\\rightarrow0$ concludes the proof. ", "page_idx": 30}, {"type": "text", "text": "D Experiment details and additional results ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this section, we provide experimental details and additional results. For the experiments we relied on a server with 100 Intel(R) Xeon(R) Gold 6238R CPU $\\textcircled{a}2.20\\mathrm{GHz}$ cpus and 256GB of RAM. The time to obtain all the empirical results is less than a day. ", "page_idx": 30}, {"type": "text", "text": "This section is structured as follows. ", "page_idx": 30}, {"type": "text", "text": "\u2022 First, we provide an additional details and results on the experiment presented in Section 5 (Section D.1 and Section D.2).   \n\u2022 Secondly, we provide results on additional $4\\times5$ multi-fidelity bandits (Section D.3).   \n\u2022 Then, we analyze a typical trick that is used to improve the performance of gradient-based methods, that is using a constant rate against using the learning rate that the theory prescribes. (Section D.4).   \n\u2022 We then present results using very small value of $\\delta$ w.r.t. to the one that has been considered in the main text (Section D.5). In particular, we verify that the performance difference amplifies.   \n\u2022 Finally, we discuss the approaches of [31]. ", "page_idx": 30}, {"type": "text", "text": "D.1 Further details on the experiments presented in Section 5 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "First instance We begin by providing further details on the $4\\times5$ multi-fidelity bandit model that we used in Figure 1 and Figure 2. First, Table 2 reports the $4\\times5$ bandit model of Figure 1. ", "page_idx": 30}, {"type": "table", "img_path": "gKMTM1i8Ew/tmp/f7215632d94b8a83c1d6f2c7bc2285ade7522333c56f1c71564881ee8aff7b56.jpg", "table_caption": ["Table 2: Multi-fidelity bandit model presented in Figure 1. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "All arms, both for the bandit model of Figure 1 and 2 are Gaussian distributions with variance $\\sigma^{2}=0.1$ . The bandit model in Figure 1 has been generated according to a procedure that has been used to generate MF instances in [25] (see their Appendix D.1). Specifically, first, two $M$ -dimensional vectors are specified, which we refer to as $\\textbf{\\em a}$ and $^{b}$ . Specifically, $\\textbf{\\em a}$ and $^{b}$ are such that $a_{m}\\geq a_{m+1}$ and $b_{m}\\geq b_{m+1}$ for all $m\\in[M-1]$ . Then, we first sample the means of the arm at fidelity $M^{13}$ , and once this is done we sample $\\begin{array}{r}{\\bar{\\mu_{i,m}}\\in\\left[\\mu_{i,M}-a_{m}-\\frac{b}{2},\\mu_{i,M}+a_{m}+\\frac{b}{2}\\right]}\\end{array}$ . Then, $\\xi$ is computed as $\\begin{array}{r}{\\xi_{m}=a_{m}+\\frac{b_{m}}{2}}\\end{array}$ . In this sampling procedure, we have used $\\pmb{a}=[0.075,0.06,0.04,0.02,0]$ and $\\pmb{b}=[0.05,0.04,\\bar{0}.02,0.01,0]$ . ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "Second instance We now recall the $5\\times2$ example of Section 5: ", "page_idx": 31}, {"type": "table", "img_path": "gKMTM1i8Ew/tmp/2d8a4e86cd4d05d2d071d7887e842a6a6e90d6ec7f4ac1a1df65ef7f45c0a078.jpg", "table_caption": ["Table 3: Multi-fidelity bandit model presented in Figure 2. "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "We prove that, in this instance, the oracle weights are given by $\\omega_{i}^{*}=[0.09621,0]$ for all $i\\in[4]$ , and $\\omega_{5}^{*}=[0,0.61516]$ (this number have been rounded to the fourth decimal precision). In order to prove this, we first notice that in the considered domain the optimal fidelity for $i\\in[4]$ is $m=1$ . This is direct from the fact that $\\mu_{i,m}=\\mu_{i,M}-\\xi_{m}$ (see, e.g., Proposition B.5). Furthermore, we recall the expression $f_{5,i}$ , for any $i\\in$ [4]: ", "page_idx": 31}, {"type": "equation", "text": "$$\nf_{5,i}(\\omega,\\mu)=\\operatorname*{inf}_{\\eta\\in[\\mu_{i,M},\\mu_{5,M}]}\\sum_{m\\in[M]}\\omega_{5,m}\\frac{d^{-}(\\mu_{5,m},\\eta+\\xi_{m})}{\\lambda_{m}}+\\omega_{i,m}\\frac{d^{+}(\\mu_{i,m},\\eta-\\xi_{m})}{\\lambda_{m}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then, since $\\eta_{5,i}^{*}\\in[\\mu_{i,M},\\mu_{5,M}]$ , $\\mu_{i,M}=\\mu_{5,m}=\\mu_{5,M}-\\xi_{m}$ , we have that the optimal fidelity for arm 5 is $m=2$ . At this point, consider the oracle weights $\\omega^{*}$ . We notice that, due to the symmetry of the problem, $\\omega_{i,1}^{*}$ is equal for all $i\\in$ [4]. Then, we can rewrite $f_{5,i}(\\omega^{*},\\mu)$ as a function of a single variable, that is: ", "page_idx": 31}, {"type": "equation", "text": "$$\nf_{5,i}(\\omega,\\mu)=\\operatorname*{inf}_{\\eta\\in[\\mu_{i,M},\\mu_{5,M}]}(1-4\\omega_{i,1})\\frac{d^{-}(\\mu_{5,2},\\eta)}{\\lambda_{M}}+\\omega_{i,1}\\frac{d^{+}(\\mu_{i,1},\\eta-\\xi_{m})}{\\lambda_{m}},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and, consequently, we obtain that $C^{*}(\\pmb{\\mu})^{-1}$ can be expressed as a convex optimization of a single variable, that is $\\omega_{i,1}$ . Taking the derivative of $F(\\omega,\\mu)$ w.r.t. $\\omega_{i,1}$ we obtain that the following equality should be satisfied at the optimum: ", "page_idx": 31}, {"type": "equation", "text": "$$\n4\\frac{d(\\mu_{1,M},\\eta_{5,i}^{*})}{\\lambda_{M}}=\\frac{d(\\mu_{i,1},\\eta_{5,i}^{*}-1)}{\\lambda_{m}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Solving for $\\eta_{5,i}^{*}$ gives a unique solution in the range [0.5, 0.6], which is 0.539. Then, using Lemma C.3 and solving for $\\omega_{i,1}$ , we obtain $\\omega_{i,1}=0.09621$ , and consequently, $\\omega_{5,2}=0.61516$ . ", "page_idx": 31}, {"type": "text", "text": "Thresholds To conclude, we comment on the thresholds $\\beta_{t,\\delta}$ used by the algorithms. For the stopping rule in MF-GRAD we used $\\beta_{t,\\delta}=\\log(K/\\delta)+M\\log(\\log(t)+1)$ , which is a simplification of its theoretical value (31) that retains the same scaling in $K$ and $M$ (up to constants). In GRAD, we used $\\beta_{t,\\delta}=\\log(K/\\delta)+\\log(\\log(t)+1)$ , which is a similar simplification of the usual threshold for BAI, which instead of concentrating a sum of $2M\\,\\mathrm{KL}$ terms (see the proof of Proposition C.13) only requires to concentrate a sum over $2\\;\\mathrm{KL}$ terms. Finally, in the confidence intervals that are used in IISE we have used the confidence bonuses 2\u03c32(log(KM/\u03b4)+log(log(t)))14, which compared to their original form is replacing some crude union bound over $t$ with a stylized version of the threshold that would follows from using tight time-uniform concentration. These choices were adopted consistently in all the experiments presented in this appendix, and they ensured the $\\delta$ -correctness requirement in all cases. ", "page_idx": 31}, {"type": "text", "text": "D.2 Empirical cost proportions of MF-GRAD ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section, we analyze the behavior of MF-GRAD by analyzing the evolution of the empirical cost proportions. Specifically, we repeat on the $4\\times5$ bandit model of Table 2, the same experiment that we presented in Section 5 for the $5\\times2$ bandit model. Figure 4 reports the result. Interestingly, we highlight how the sparsity pattern emerges also in this domain. ", "page_idx": 32}, {"type": "text", "text": "D.3 Results on additional domains ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section, we present results on additional $4\\times5$ multi-fidelity bandit models. Specifically, we generate another random instance according to the procedure of [25], and we report the model in Table 4. Furthermore, we created an additional bandit model where means of some arms are slightly increasing on displaying a stationary trend over fidelity and we report the model in Table 5. In both cases, we considered Gaussian distribution with $\\sigma^{2}=\\bar{0}.1$ . Empirical results of MF-GRAD, IISE and GRAD for $\\delta=0.01$ can be found in Figure 5 and 6 respectively. As we can appreciate, MF-GRAD maintains the most competitive performance across both domains. ", "page_idx": 32}, {"type": "table", "img_path": "gKMTM1i8Ew/tmp/f8f22c2c88bfd2a4271f31453ba1b30625b9ea26b8753f1bdecc3e3a58159c8e.jpg", "table_caption": ["Table 4: Additional random multi-fidelity bandit model. "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "gKMTM1i8Ew/tmp/b384937b77b85189674e3a84235dc97a0a314194f72c9e824e8a07ad96d045fc.jpg", "table_caption": ["Table 5: Additional multi-fidelity bandit model. "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "D.4 Improving performance with constant learning rate ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "As reported in [22], using constant learning rate can improve the identification performance in standard BAI settings. In the following, we analyze the performance difference of MF-GRAD that uses the theoretical learning rate, and MF-GRAD that uses a constant learning rate of $\\alpha=0.25$ . We will refer this second version as MF-GRAD-CONST. Figure 7 and 8 reports the performance of the algorithms in the two bandit models of Section 5. As we can see, in both cases, MF-GRAD-CONST outperforms MF-GRAD. ", "page_idx": 32}, {"type": "text", "text": "We further investigate this behavior by showing the evolution of the empirical cost proportions of MF-GRAD-CONST during the learning process. Figure 9 and 10 reports the evolution of the empirical costs, over 100000 iterations. Comparing the results with Figure 4 and 9 we can appreciate as MF-GRAD-CONST move away from the initial cost proportions way sooner than MF-GRAD, which explains its superior performance in the moderate regime of $\\delta$ . ", "page_idx": 32}, {"type": "text", "text": "D.5 Smaller value of $\\delta$ ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Finally, we repeat the experiments that we presented in the previous section using smaller values of $\\delta$ . Specifically, we consider $\\delta=10^{-10}$ . Figure 11 reports the performance of the $4\\times5$ bandit model of Section 5, Figure 12 reports the performance of the $5\\times2$ bandit model of Section 5, Figure 13 and 14 reports the performance of the additional bandit models presented in Appendix D.3. As one can notice the performance gap between MF-GRAD and the considered baseline increases. ", "page_idx": 32}, {"type": "image", "img_path": "gKMTM1i8Ew/tmp/7c38cff6e8ffcae14d758e6a402e7cf6f872d95d68ee7a2d69eb84e59253ac24.jpg", "img_caption": ["Figure 4: Empirical cost proportions of MF-GRAD for 100000 iterations on the $5\\times2$ bandit model of Section 5. Results are average over 100 runs and shaded area report $95\\%$ confidence intervals. Empirical cost proportions of each arm are plotted with the same color. Cost proportions at fidelity 1, 2, 3, 4 and 5 are visualized with circle, squared, cross, triangle, and diamond respectively. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "gKMTM1i8Ew/tmp/d072530871b3056a082af21b9e5893cb1575828b650ef6e432c0263d045bfcfa.jpg", "img_caption": ["Figure 5: Empirical cost complexity for 1000 runs Figure 6: Empirical cost complexity for 1000 runs times with $\\delta\\:=\\:0.01$ on the multi-fidelity bandit of times with $\\delta\\:=\\:0.01$ on the multi-fidelity bandit of Table 4. Table 5. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "gKMTM1i8Ew/tmp/61dd3fa65fdc305c423757f9356280b9f7d9d35899e64df49e6df8b2d2890a14.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "gKMTM1i8Ew/tmp/92d9ab81623518d991f8f952b8c8be6584ca472cdd1148129ff7ca60b9c65b38.jpg", "img_caption": ["Figure 7: Empirical cost complexity for 1000 runs times with $\\delta=0.01$ on the $4\\times5$ multi-fidelity bandit of Section 5. "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "gKMTM1i8Ew/tmp/2f815e2ab8cb914ffa9f22f7322c1d255bf2a19ab059446f4a4c063cc34a596d.jpg", "img_caption": ["Figure 8: Empirical cost complexity for 1000 runs times with $\\delta=0.01$ on the $5\\times2$ multi-fidelity bandit of Section 5. "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "gKMTM1i8Ew/tmp/956c9df6afa6982b39c05f8076eee34560254178054d783b83245d10972d706e.jpg", "img_caption": ["Figure 9: Empirical cost proportions of MF-GRAD-CONST for 100000 iterations on the $4\\times5$ bandit model of Section 5. Results are average over 100 runs and shaded area report $95\\%$ confidence intervals. Empirical cost proportions of each arm are plotted with the same color. Cost proportions at fidelity 1, 2, 3, 4 and 5 are visualized with circle, squared, cross, triangle, and diamond respectively. "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "gKMTM1i8Ew/tmp/c0057d7e49ff3e37ae61476edc49273b2799272e49b3441d1b22110161f2f232.jpg", "img_caption": ["Figure 10: Empirical cost proportions of MF-GRAD-CONST for 100000 iterations on the $5\\times2$ bandit model of Section 5. Results are average over 100 runs and shaded area report $95\\%$ confidence intervals. Empirical cost proportions of each arm are plotted with the same color. Cost proportions at fidelity 1 and 2 are visualized with circle and squared respectively. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "gKMTM1i8Ew/tmp/877d7447370887001bdf8487e22971fe893f16a9e5e77d8e5c77de5ed503f782.jpg", "img_caption": ["Figure 11: Empirical cost complexity for 1000 runs Figure 12: Empirical cost complexity for 1000 runs times with $\\delta=\\mathrm{i}0^{-10}$ on the $4\\times5$ multi-fidelity bandit times with $\\delta=\\mathrm{i}0^{-10}$ on the $5\\times2$ multi-fidelity bandit of Table 2. of Table 3. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "gKMTM1i8Ew/tmp/4c48af3f24677109277b9d8c06b80c749d4425e1061ebd8a09479ba6a9a5f14c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "gKMTM1i8Ew/tmp/26821cb5ed89f947debd4e25ef2c73ee22b0ea9094a5862b3ea3672ff8a5fc4e.jpg", "img_caption": ["Figure 13: Empirical cost complexity for 1000 runs Figure 14: Empirical cost complexity for 1000 runs times with $\\delta=\\mathrm{{10^{-10}}}$ on the multi-fidelity bandit of times with $\\delta=\\mathrm{{10^{-10}}}$ on the multi-fidelity bandit of Table 4. Table 5. "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "D.6 On LUCB-ExploreA and LUCB-ExploreB ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In this section, we present in detail the main issue behind the algorithms presented in [31], i.e., LUCBExploreA and LUCBExploreB, that is the fact that these algorithm might fail at stopping in some specific multi-fidelity bandit models. First, we provide numerical evidence of this phenomena by running both methods in a specific instance (Section D.6.1). Then, in Section D.6.2, we point out an error in the analysis of [31] that highlights how both algorithms fails at stopping when considering instances such as the one that has been considered in Section D.6.2. ", "page_idx": 36}, {"type": "text", "text": "D.6.1 Experimental issues ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "When experimenting with the algorithms proposed in [31], namely LUCBExploreA and LUCBExploreB, we have faced stopping issues. Specifically, both algorithms were not terminating in any reasonable number of steps on some specific instances. We now report an illustrative example of such scenarios. Consider the following Gaussian multi-fidelity bandit model: $\\mu_{1}=[0.64,0.6]$ , $\\mu_{2}=[0.46,0.5]$ , $\\lambda=[0.1,5]$ , $\\xi=[0.1,0]$ and $\\sigma^{2}=1$ . In this scenario, the well-known LUCB algorithm [12] which only uses samples at fidelity $M$ , stops soon (iteration $\\approx100\\mathbf{k})$ ) paying a total cost of roughly $500\\mathbf{k}$ . When running LUCBExploreA and LUCBExploreB, instead, we faced termination issues. We let both algorithms run for a maximum number of $10^{8}$ samples (reaching a total cost which is approximately $10^{7}$ ), and the stopping criterion was never met for LUCBExploreA, while $70\\%$ of LUCBExploreB runs did not stop. LUCBExploreB explores more fidelities at the beginning, and that initial exploration can be enough to trigger the stopping test on some runs, but many continue until we artificially stop the experiment. Figure 15 reports the results of this experiment. ", "page_idx": 36}, {"type": "text", "text": "As a final remark, we notice that both LUCBExploreA and LUCBExploreB require additional knowledge in order to run, that is an upper bound on $\\mu_{1,M}$ and a lower bound on $\\mu_{2,M}$ (assuming arms to being ordered according to $\\mu_{1,M}\\,>\\,\\mu_{2,M}\\,\\geq\\,\\cdot\\,\\cdot\\,\\geq\\,\\mu_{K,M})$ . The result presented in this section have been presented running their algorithms in the most favorable scenario, that is the situation in which the agent has perfect knowledge on the values $\\mu_{1,M}$ and $\\mu_{2,M}$ . ", "page_idx": 36}, {"type": "text", "text": "D.6.2 Theoretical issues ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "The general idea of the LUCBExplore algorithms of [31] is to identify for each arm the \u201coptimal fidelity\u201d and pull the arm at that fidelity. In Appendix B.5, we described how that \u201coptimal fidelity\u201d can differ from the fidelity selected by our lower bound. Since our lower bound can be matched by an algorithm and thus describes the actual cost complexity of the problem, it betters represent the notion of optimal fidelity. We will thus call the fidelity used by the LUCBExplore algorithms target fidelity instead. The two variants ExploreA and ExploreB differ in the mechanism used to look for the target fidelity. ", "page_idx": 36}, {"type": "image", "img_path": "gKMTM1i8Ew/tmp/614721b96fe1d10acf0ea692c961053bbf190ef5ec16220db278dec8d26ba4b8.jpg", "img_caption": ["Figure 15: Visualization of the non-stopping behavior of LUCBExploreA and LUCBExploreB. "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "We first show that even if their algorithm used an oracle for the fidelity exploration mechanism that returns the target fidelity for all arms, it would still not be able to stop on some examples. We then highlight an issue with the proof of [31]. ", "page_idx": 37}, {"type": "text", "text": "Failure to stop with an oracle Consider the bandit instance from Appendix B.5. Recall that this is a $2\\times2$ example of multi-fidelity BAI problem with $\\xi_{1}=0.1$ , $\\xi_{2}=0.0$ , $\\mu_{1,M}=0.6$ , $\\mu_{1,m}=0.65$ , $\\mu_{2,M}\\,=\\,0.5$ , $\\mu_{2,m}\\,=\\,0.45$ (we write $M=2$ and $m\\,=\\,1$ ). All distributions are Gaussian with variance 1. We choose $\\lambda_{M}>4\\lambda_{m}$ , which means that the target fidelity for that problem are $m_{1}^{*}=1$ and $m_{2}^{*}=1$ (see details in Appendix B.5). LUCBExplore with an oracle that always selects that fidelity is the following algorithm: ", "page_idx": 37}, {"type": "text", "text": "\u2022 Initialization: $\\hat{\\mu}_{k,m}(t)=0$ $\\mathrm{~\\boldmath~\\Omega~}(t)=0,N_{k,m}(t)=0,U C B_{k}(t)=1,L C B_{k}(t)=0$ for all arms $k$ and fidelity $m.\\ \\ell_{t}=1,u_{t}=2$ .   \n\u2022 While $L C B_{\\ell_{t}}(t)\\leq U C B_{u_{t}}(t)$ \u2013 $\\ell_{t}=\\arg\\operatorname*{max}_{k}U C B_{k}(t),u_{t}=\\arg\\operatorname*{max}_{k\\in[k]\\backslash\\{\\ell_{t}\\}}U C B_{k}(t)$ \u2013 Pull arms $\\ell_{t}$ and $u_{t}$ at their target fidelity. ", "page_idx": 37}, {"type": "text", "text": "\u2022 Output $\\ell_{t}$ ", "page_idx": 37}, {"type": "text", "text": "The indices are ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L C B_{k}(t)=\\underset{m}{\\mathrm{max}}\\left(\\hat{\\mu}_{k,m}(t)-\\xi_{m}-\\beta(N_{k,m}(t),t,\\delta)\\right)}\\\\ &{U C B_{k}(t)=\\underset{m}{\\mathrm{min}}\\left(\\hat{\\mu}_{k,m}(t)+\\xi_{m}+\\beta(N_{k,m}(t),t,\\delta)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\beta(n,t,\\delta)=\\sqrt{\\log(L t^{4}/\\delta)/n}$ for some constant $L>0$ . ", "page_idx": 37}, {"type": "text", "text": "In the two-arms example here, the algorithm simplifies greatly: it always pulls both arms alternatively, always at fidelity $m=1$ . It stops when the LCB of one arm surpasses the LCB of the other. ", "page_idx": 37}, {"type": "text", "text": "We show that it can\u2019t stop and return the best arm 1, unless a confidence interval is not valid, which happens with small probability. If $\\hat{\\mu}_{1,1}(t)\\leq\\mu_{1,1}+\\beta(t/2,t,\\delta)$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L C B_{1}(t)=\\operatorname*{max}\\{0,\\hat{\\mu}_{1,1}(t)-\\xi_{1}-\\beta(t/2,t,\\delta)\\}}\\\\ &{\\qquad\\qquad\\leq\\mu_{1,1}-\\xi_{2}}\\\\ &{\\qquad\\qquad=0.55\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "On the other hand, if $\\operatorname*{min}\\{1,\\hat{\\mu}_{2,1}(t)\\geq\\mu_{2,1}-\\beta(t/2,t,\\delta),$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U C B_{2}(t)=\\operatorname*{min}\\{1,\\hat{\\mu}_{2,1}(t)+\\xi_{1}+\\beta(t/2,t,\\delta)\\}}\\\\ &{\\qquad\\qquad\\geq\\mu_{2,1}+\\xi_{1}}\\\\ &{\\qquad\\qquad=0.55\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We get that we always have $L C B_{1}(t)\\leq U C B_{2}(t)$ , unless one of the two concentration inequalities on the empirical means are not true. The confidence width $\\beta$ is designed to make those inequalities true for all $t\\in\\mathbb{N}$ with probability close to 1. We can similarly get that $L C B_{2}(t)\\leq U C B_{1}(t)$ (which is expected since 2 is a worse arm) unless some concentration inequality is false. ", "page_idx": 38}, {"type": "text", "text": "We obtain that this algorithm with an oracle selection for the target fidelity cannot stop fast: the only way it can stop is if unlikely deviations occur. ", "page_idx": 38}, {"type": "text", "text": "Issue with the proof There is an issue with the proof of the cost complexity upper bound of [31]. The issue is in the first 3 steps of their appendix E.2, pages 17 and 18. They identify a threshold $c$ (with value 0.55 in our example of the last paragraph) and prove the following. ", "page_idx": 38}, {"type": "text", "text": "\u2022 Step 1: if the algorithm does not terminate and confidence intervals hold, then either both $L C B_{\\ell_{t}}(t)\\le c$ and $U C B_{\\ell_{t}}(t)\\geq c$ or both $L C B_{u_{t}}(t)\\le c$ and $U C B_{u_{t}}(t)\\geq c$ .   \n\u2022 Step 2: confidence intervals are likely to hold.   \n\u2022 Step 3: if a sub-optimal arm $k$ satisfies $L C B_{k}(t)\\leq c$ and $U C B_{k}(t)\\geq c$ , then its target fidelity cannot be pulled much. ", "page_idx": 38}, {"type": "text", "text": "They conclude that for all arms, the number of pulls at the target fidelity is upper bounded, with large probability. ", "page_idx": 38}, {"type": "text", "text": "Let\u2019s see the issue with that proof, on the same example as in the last paragraph. ", "page_idx": 38}, {"type": "text", "text": "In the example above with the oracle choice for the target fidelity, we saw that if confidence intervals hold and $\\ell_{t}=1$ (which is the most likely), then $L C\\bar{B}_{\\ell_{t}}(t)\\leq\\bar{c}$ and $U C B_{\\ell_{t}}(t)\\geq c$ . That is, step 1 gives a condition on arm 1 only (and nothing on arm 2). But then we get nothing from step 3, since arm 1 is not a sub-optimal arm. ", "page_idx": 38}, {"type": "text", "text": "We only get an upper bound on the number of pulls for sub-optimal arms if we can say that they satisfy $L\\bar{C}B_{k}(t)\\bar{\\leq}\\;c$ and $U C B_{k}(t)\\geq c$ at some point, but it might not be the case. Indeed, when the algorithm does not terminate, steps 1 and 2 together give that with large probability either both $L C B_{\\ell_{t}}^{-}(t)\\leq c$ and $U C B_{\\ell_{t}}(t)\\geq c$ or both $L C B_{u_{t}}\\bar{(t)}\\leq\\bar{c}$ and $U C B_{u_{t}}(t)\\bar{\\geq c}$ . It is possible that we always have this property for $\\ell_{t}=1$ (the optimal arm), and that we can never apply step 3. ", "page_idx": 38}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Claims and introduction reflects the contribution and content of the paper. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 39}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: We discuss limitations and future direction of improvements in Section 6 ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 39}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Each statement presented in the main text is supported with formal proofs presented in the appendix. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 40}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Our algorithm is described in mathematical rigor so that it can be reproduced.   \nFurthermore, codebase and detailed instructions on how to reproduce the result is provided. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 40}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We provide the codebase with instructions on how to reproduce the results. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 41}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: Experiments have been detailed in Appendix D. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 41}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: All results are average of 1000/100 runs. Error bars are reported in all cases (e.g., depending on the experiment, via boxplots and $95\\%$ confidence intervals). ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: Computational resources employed in this study are reported in Appendix D.   \nTime taken to re-run all the experiments is also reported. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 42}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper aligns with the guidelines (e.g., does not involve human participants nor datasets) and anonymity of the submission is preserved. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 42}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: This paper is a foundational research work whose goal is to advance theoretical aspects of sequential-decision making. We do not any direct path to negative applications. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 42}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 43}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: This paper does not involve such assets. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 43}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper does not rely on existing assets. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We provide the codebase together with instruction on how to reproduce the experiments. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 44}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: No crowdsourcing experiments and research with human subjects were involved in this work. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 44}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: No study participants were involved in this work. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. ", "page_idx": 44}, {"type": "text", "text": "\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 45}]