[{"figure_path": "gvlOQC6oP1/figures/figures_0_1.jpg", "caption": "Figure 1: Some generated images (top) from diffusion models replicates the contents of existing images (bottom). The existing (matched) images are from LAION-Aesthetics [1]. The diffusion models include both commercial and open-source ones.", "description": "This figure shows a comparison of images generated by six different diffusion models (Midjourney, New Bing, DALL-E 2, SDXL, DeepFloyd IF, and GLIDE) with their corresponding source images from the LAION-Aesthetics dataset. The top row displays the generated images, while the bottom row shows the original images from which they appear to be copied.  The figure highlights the issue of diffusion models potentially replicating existing content, which is a key challenge addressed in the paper.", "section": "Introduction"}, {"figure_path": "gvlOQC6oP1/figures/figures_1_1.jpg", "caption": "Figure 2: The comparison between current ICD with the ICDiff. The current ICD focuses on detecting edited copies generated by transformations like horizontal flips, random rotations, and random crops. In contrast, the ICDiff aims to detect replication generated by diffusion models, such as Stable Diffusion [2]. (Source of the original image: Lawsuit from Getty Images.)", "description": "This figure compares the capabilities of current Image Copy Detection (ICD) methods and the proposed ICDiff method.  The left shows a standard ICD method detecting an image modified by simple transformations (flip, rotation, crop). The right shows the proposed ICDiff method successfully detecting an image replicated by a diffusion model (Stable Diffusion).  The original image is sourced from a Getty Images lawsuit, highlighting a real-world application of this challenge.", "section": "1 Introduction"}, {"figure_path": "gvlOQC6oP1/figures/figures_3_1.jpg", "caption": "Figure 1: Some generated images (top) from diffusion models replicates the contents of existing images (bottom). The existing (matched) images are from LAION-Aesthetics [1]. The diffusion models include both commercial and open-source ones.", "description": "This figure shows a comparison of images generated by six different diffusion models (Midjourney, New Bing, DALL-E 2, SDXL, DeepFloyd IF, and GLIDE) with their corresponding source images from the LAION-Aesthetics dataset. The purpose is to visually demonstrate the issue of content replication in diffusion-generated images, which is the main focus of the paper.", "section": "Image Copy Detection for Diffusion Models"}, {"figure_path": "gvlOQC6oP1/figures/figures_4_1.jpg", "caption": "Figure 4: The demonstration of the proposed PDF-Embedding. Initially, PDF-Embedding converts manually-labeled replication levels into probability density functions (PDFs). To learn from these PDFs, we use a set of vectors as the representation of an image.", "description": "This figure illustrates the PDF-Embedding method.  The method first converts the manually labeled replication levels (0-5) into probability density functions (PDFs). These PDFs represent the probability distribution of neighboring replication levels.  Then, a Vision Transformer (ViT) is used to transform each image into a set of vectors. Finally, the KL divergence between the predicted and ground truth PDFs is used as the loss function during training.  The goal is to learn a set of representative vectors for each image that reflects the probability distribution of its replication level.", "section": "4 Method"}, {"figure_path": "gvlOQC6oP1/figures/figures_7_1.jpg", "caption": "Figure 5: The comparison of different PDFs: Gaussian (left), linear (middle), and exponential (right). \"A\" is the amplitude in each PDF function (Eqn. 3 to Eqn. 5).", "description": "This figure compares the performance of three different probability density functions (PDFs) \u2013 Gaussian, linear, and exponential \u2013 used in the PDF-Embedding method.  The x-axis represents the amplitude (A) parameter of each PDF, and the y-axis shows the PCC (Pearson Correlation Coefficient) and RD (Relative Deviation) values.  The plots illustrate how the choice of PDF impacts the performance metrics.  The optimal choice seems to be the exponential PDF with an amplitude (A) around 1.5-1.8, which balances the PCC and RD.", "section": "5.3 The Effectiveness of PDF-Embedding"}, {"figure_path": "gvlOQC6oP1/figures/figures_9_1.jpg", "caption": "Figure 6: Left: Examples of diffusion-based replication fetched by our PDF-Embedding. The accompanying percentages indicate the replication ratio of each model. Right: Examples filtered by SSCD [15] in [10]. Compared to them, our results are more diverse: For example, the \"Groot\" generated by SDXL includes the whole body, whereas the original one features only the face; and the \"Moai statues\" created by DeepFloyd IF are positioned differently compared to the original image.", "description": "The figure shows examples of images generated by different diffusion models that replicate existing images.  The left side displays images generated by six different diffusion models, showing varying degrees of replication (as indicated by percentages).  The right side offers a comparison to results from a previous method (SSCD), highlighting that the new method (PDF-Embedding) finds more diverse and subtle cases of replication.", "section": "Simulated Evaluation of Diffusion Models"}, {"figure_path": "gvlOQC6oP1/figures/figures_16_1.jpg", "caption": "Figure 7: The cosine similarity heatmap of the learned vectors.", "description": "The figure shows a heatmap visualizing the cosine similarity between the learned vectors representing different replication levels.  The heatmap reveals the relationships between these vectors in the embedding space, demonstrating that vectors representing similar replication levels exhibit higher cosine similarity, while vectors representing dissimilar levels have lower similarity.  This illustrates the effectiveness of the PDF-Embedding method in capturing the relationships between different replication levels.", "section": "4.2 Representing an Image as a Set of Vectors"}, {"figure_path": "gvlOQC6oP1/figures/figures_18_1.jpg", "caption": "Figure 1: Some generated images (top) from diffusion models replicates the contents of existing images (bottom). The existing (matched) images are from LAION-Aesthetics [1]. The diffusion models include both commercial and open-source ones.", "description": "This figure shows examples of images generated by six different diffusion models (Midjourney, New Bing, DALL-E 2, SDXL, DeepFloyd IF, and GLIDE).  The top row displays the generated images, while the bottom row shows the corresponding original images from the LAION-Aesthetics dataset, which the generated images replicate to varying degrees. This highlights the challenge of detecting image replication originating from diffusion models, as addressed in the paper.", "section": "Introduction"}, {"figure_path": "gvlOQC6oP1/figures/figures_19_1.jpg", "caption": "Figure 1: Some generated images (top) from diffusion models replicates the contents of existing images (bottom). The existing (matched) images are from LAION-Aesthetics [1]. The diffusion models include both commercial and open-source ones.", "description": "This figure shows a comparison between images generated by various diffusion models (Midjourney, New Bing, DALL-E 2, SDXL, DeepFloyd IF, and GLIDE) and their corresponding original images from the LAION-Aesthetics dataset.  The top row displays the generated images, while the bottom row shows the original images that the generated images appear to replicate. This visually demonstrates the issue of content replication in diffusion models, which is the main focus of the paper.", "section": "Introduction"}, {"figure_path": "gvlOQC6oP1/figures/figures_20_1.jpg", "caption": "Figure 3: The demonstration of the manual-labeled D-Rep dataset. The percentages on the left show the proportion of images with a particular level.", "description": "This figure shows the manual-labeled D-Rep dataset, which contains 40,000 image-replica pairs. Each pair consists of an image from LAION-Aesthetics V2 and its replica generated by Stable Diffusion V1.5. The image pairs are manually annotated with six replication levels, ranging from 0 (no replication) to 5 (total replication). The percentages on the left of the figure represent the proportion of images at each replication level in the dataset.", "section": "3.1 D-Rep Dataset"}, {"figure_path": "gvlOQC6oP1/figures/figures_21_1.jpg", "caption": "Figure 1: Some generated images (top) from diffusion models replicates the contents of existing images (bottom). The existing (matched) images are from LAION-Aesthetics [1]. The diffusion models include both commercial and open-source ones.", "description": "This figure shows a comparison of images generated by several different diffusion models (Midjourney, New Bing, DALL-E 2, SDXL, DeepFloyd IF, and GLIDE) with their corresponding source images from the LAION-Aesthetics dataset.  The top row displays the generated images, and the bottom row shows the original images that appear to have been replicated by the models. The figure highlights the challenge of detecting image replication in diffusion model outputs.", "section": "Introduction"}, {"figure_path": "gvlOQC6oP1/figures/figures_22_1.jpg", "caption": "Figure 1: Some generated images (top) from diffusion models replicates the contents of existing images (bottom). The existing (matched) images are from LAION-Aesthetics [1]. The diffusion models include both commercial and open-source ones.", "description": "This figure shows a comparison of images generated by several different diffusion models (Midjourney, New Bing, DALL-E 2, Stable Diffusion XL, DeepFloyd IF, and GLIDE) with their corresponding source images from the LAION-Aesthetics dataset.  The purpose is to visually demonstrate the phenomenon of diffusion models replicating content from existing images, highlighting the challenge addressed by the paper.", "section": "Image Copy Detection for Diffusion Models"}, {"figure_path": "gvlOQC6oP1/figures/figures_23_1.jpg", "caption": "Figure 1: Some generated images (top) from diffusion models replicates the contents of existing images (bottom). The existing (matched) images are from LAION-Aesthetics [1]. The diffusion models include both commercial and open-source ones.", "description": "This figure shows a comparison of images generated by various diffusion models (Midjourney, New Bing, DALL-E 2, Stable Diffusion XL, DeepFloyd IF, and GLIDE) with their corresponding original images from the LAION-Aesthetics dataset.  The top row displays the generated images, while the bottom row shows the original images they appear to replicate. This visualization highlights the challenge of detecting image copy in diffusion model outputs, as the generated images are often subtle variations of existing works.", "section": "Introduction"}, {"figure_path": "gvlOQC6oP1/figures/figures_24_1.jpg", "caption": "Figure 1: Some generated images (top) from diffusion models replicates the contents of existing images (bottom). The existing (matched) images are from LAION-Aesthetics [1]. The diffusion models include both commercial and open-source ones.", "description": "This figure shows a comparison of images generated by different diffusion models (Midjourney, New Bing, DALL-E 2, SDXL, DeepFloyd IF, and GLIDE) with their corresponding original images from the LAION-Aesthetics dataset.  The top row displays the generated images, while the bottom row shows the original images they seem to replicate.  The purpose is to illustrate the phenomenon of content replication in images generated by diffusion models, highlighting a key challenge the paper addresses.", "section": "Introduction"}, {"figure_path": "gvlOQC6oP1/figures/figures_25_1.jpg", "caption": "Figure 15: The distributions converted from replication levels. We use Gaussian, linear, and exponential functions as the representative demonstrations.", "description": "This figure visualizes the probability density functions (PDFs) derived from different replication levels using three different functions: Gaussian, linear, and exponential. Each function is shown for various normalized levels (p\u00b9), representing the transformation of replication levels into probability distributions. The different shapes of the curves illustrate how the rate at which each function deviates from its peak value changes based on the function.  The Gaussian function shows a slow rate of change, the linear function has a constant rate, and the exponential function exhibits a rapid rate of change. This visualization helps illustrate the choice of PDF functions in their proposed PDF-Embedding method.", "section": "4 Method"}, {"figure_path": "gvlOQC6oP1/figures/figures_26_1.jpg", "caption": "Figure 3: The demonstration of the manual-labeled D-Rep dataset. The percentages on the left show the proportion of images with a particular level.", "description": "This figure shows the distribution of replication levels in the manually labeled D-Rep dataset.  The dataset consists of 40,000 image-replica pairs, each manually annotated with a replication level from 0 (no replication) to 5 (total replication). The figure visually represents the percentage of images belonging to each of these six replication levels.", "section": "3.1 D-Rep Dataset"}, {"figure_path": "gvlOQC6oP1/figures/figures_27_1.jpg", "caption": "Figure 1: Some generated images (top) from diffusion models replicates the contents of existing images (bottom). The existing (matched) images are from LAION-Aesthetics [1]. The diffusion models include both commercial and open-source ones.", "description": "This figure shows a comparison of images generated by six different diffusion models (Midjourney, New Bing, DALL-E 2, SDXL, DeepFloyd IF, and GLIDE) with their corresponding original images from the LAION-Aesthetics dataset.  The top row displays the generated images, and the bottom row shows the original images that the generated images appear to replicate. This visual demonstrates the challenge of detecting image replication when dealing with diffusion models, as the generated images are often subtly different from, but still clearly based on, the original images. The models used represent a mix of both commercial and open-source options.", "section": "Introduction"}, {"figure_path": "gvlOQC6oP1/figures/figures_28_1.jpg", "caption": "Figure 1: Some generated images (top) from diffusion models replicates the contents of existing images (bottom). The existing (matched) images are from LAION-Aesthetics [1]. The diffusion models include both commercial and open-source ones.", "description": "This figure shows a comparison of images generated by six different diffusion models (Midjourney, New Bing, DALL-E 2, SDXL, DeepFloyd IF, and GLIDE) with their corresponding source images from the LAION-Aesthetics dataset. The top row displays the generated images, while the bottom row shows the original images from LAION-Aesthetics that they appear to replicate.  This visually demonstrates the potential for diffusion models to reproduce existing content.", "section": "Introduction"}, {"figure_path": "gvlOQC6oP1/figures/figures_29_1.jpg", "caption": "Figure 1: Some generated images (top) from diffusion models replicates the contents of existing images (bottom). The existing (matched) images are from LAION-Aesthetics [1]. The diffusion models include both commercial and open-source ones.", "description": "This figure shows a comparison of images generated by six different diffusion models (Midjourney, New Bing, DALL-E 2, SDXL, DeepFloyd IF, and GLIDE) with their corresponding original images from the LAION-Aesthetics dataset. The purpose of this figure is to illustrate the phenomenon of content replication in images generated by diffusion models, highlighting that some generated images may closely replicate content from existing images.", "section": "Introduction"}, {"figure_path": "gvlOQC6oP1/figures/figures_30_1.jpg", "caption": "Figure 1: Some generated images (top) from diffusion models replicates the contents of existing images (bottom). The existing (matched) images are from LAION-Aesthetics [1]. The diffusion models include both commercial and open-source ones.", "description": "This figure shows a comparison between images generated by six different diffusion models (Midjourney, New Bing, DALL-E 2, SDXL, DeepFloyd IF, and GLIDE) and their corresponding source images from the LAION-Aesthetics dataset. The top row displays the generated images, while the bottom row shows the original images they seem to replicate. This visual comparison highlights the potential for diffusion models to reproduce existing content, raising concerns about originality and copyright.", "section": "Image Copy Detection for Diffusion Models"}, {"figure_path": "gvlOQC6oP1/figures/figures_31_1.jpg", "caption": "Figure 1: Some generated images (top) from diffusion models replicates the contents of existing images (bottom). The existing (matched) images are from LAION-Aesthetics [1]. The diffusion models include both commercial and open-source ones.", "description": "This figure shows a comparison between images generated by six different diffusion models (Midjourney, New Bing, DALL-E 2, SDXL, DeepFloyd IF, and GLIDE) and their corresponding original images from the LAION-Aesthetics dataset. The top row displays the generated images, while the bottom row shows the original images that they seem to replicate.  The figure highlights the issue of diffusion models potentially replicating existing content, which is a core motivation for the paper's work on Image Copy Detection for Diffusion Models.", "section": "Image Copy Detection for Diffusion Models"}, {"figure_path": "gvlOQC6oP1/figures/figures_32_1.jpg", "caption": "Figure 1: Some generated images (top) from diffusion models replicates the contents of existing images (bottom). The existing (matched) images are from LAION-Aesthetics [1]. The diffusion models include both commercial and open-source ones.", "description": "This figure shows a comparison between images generated by six different diffusion models (Midjourney, New Bing, DALL-E 2, Stable Diffusion XL, DeepFloyd IF, and GLIDE) and their corresponding original images from the LAION-Aesthetics dataset.  The top row displays the generated images, and the bottom row shows the original images that they seem to replicate. This visual demonstrates the issue of content replication in diffusion model outputs, highlighting that generated images might not be entirely original.", "section": "Introduction"}, {"figure_path": "gvlOQC6oP1/figures/figures_33_1.jpg", "caption": "Figure 1: Some generated images (top) from diffusion models replicates the contents of existing images (bottom). The existing (matched) images are from LAION-Aesthetics [1]. The diffusion models include both commercial and open-source ones.", "description": "This figure shows a comparison of images generated by six different diffusion models (Midjourney, New Bing, DALL-E 2, SDXL, DeepFloyd IF, and GLIDE) with their corresponding original images from the LAION-Aesthetics dataset. The top row displays the generated images, while the bottom row shows the original images that they seem to replicate.  This visually demonstrates the challenge of detecting image replication in the output of diffusion models.", "section": "Introduction"}]