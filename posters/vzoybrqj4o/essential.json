{"importance": "This paper is crucial for researchers in computer vision and 360-degree image processing.  It **directly addresses the critical shortage of labeled data** in 360-degree depth estimation, a significant hurdle for the field's advancement. The proposed method opens **new avenues for leveraging unlabeled data and knowledge distillation**, benefiting various applications in virtual reality, autonomous navigation, and immersive media.", "summary": "Depth Anywhere enhances 360-degree monocular depth estimation by cleverly using perspective models to label unlabeled 360-degree data, significantly improving accuracy.", "takeaways": ["A novel training technique effectively leverages unlabeled 360-degree data through knowledge distillation from perspective models.", "The proposed method significantly improves depth estimation accuracy, particularly in zero-shot scenarios.", "An online data augmentation strategy bridges knowledge gaps between different camera projections, leading to robust performance."], "tldr": "Accurately estimating depth from 360-degree images is challenging due to the lack of suitable datasets and the differences in camera projections compared to traditional perspective images. Existing methods either fail to handle the unique distortions of 360-degree images or underperform due to limited training data. This necessitates the development of novel approaches that can effectively utilize available data and handle the inherent challenges of 360-degree imagery. \nThe proposed \"Depth Anywhere\" framework tackles this problem by employing a two-stage training pipeline. The first stage generates masks for invalid image regions (like sky or watermarks), while the second stage uses a semi-supervised learning approach. It leverages state-of-the-art perspective depth estimation models as teachers to generate pseudo labels for unlabeled 360-degree images via a six-face cube projection.  This allows the efficient use of extensive unlabeled 360-degree data for training a robust 360-degree depth estimation model. The results demonstrate significant improvements in accuracy, particularly in zero-shot scenarios, showcasing the approach's effectiveness and generalizability across various datasets and models.", "affiliation": "Stanford University", "categories": {"main_category": "Computer Vision", "sub_category": "3D Vision"}, "podcast_path": "VzoyBrqJ4O/podcast.wav"}