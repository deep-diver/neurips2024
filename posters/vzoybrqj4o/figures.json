[{"figure_path": "VzoyBrqJ4O/figures/figures_1_1.jpg", "caption": "Figure 1: Our proposed training pipeline improves existing 360 monocular depth estimators. This figure demonstrated the improvement of our proposed training pipeline tested on the Stanford2D3D [2] dataset in a zero-shot setting.", "description": "This figure shows a qualitative comparison of depth estimation results on the Stanford2D3D dataset using different methods. The top row displays the RGB images, ground truth depth maps, and error maps generated by the BiFuse++ model. The middle row shows the depth maps produced by the BiFuse++ model and our proposed method. The bottom row displays the point clouds generated from the ground truth and estimated depth maps. The comparison illustrates the improved depth estimation accuracy achieved by incorporating our training pipeline, particularly in challenging areas.", "section": "1 Introduction"}, {"figure_path": "VzoyBrqJ4O/figures/figures_3_1.jpg", "caption": "Figure 2: Training Pipeline. Our proposed training pipeline involves joint training on both labeled 360 data with ground truth and unlabeled 360 data. (a) For labeled data, we train our 360 depth model with the loss between depth prediction and ground truth. (b) For unlabeled data, we propose to distill knowledge from a pre-trained perspective-view monocular depth estimator. In this paper, we use Depth Anything [59] to generate pseudo ground truth for training. However, more advanced techniques could be applied. These perspective-view monocular depth estimators fail to produce reasonable equirectangular depth as a domain gap exists. Therefore, we distill knowledge by inferring six perspective cube faces and passing them through perspective-view monocular depth estimators. To ensure stable and effective training, we propose generating a valid pixel mask with Segment Anything [20] while calculating loss. (c) Furthermore, we augment random rotation on RGB before passing it into Depth Anything, as well as on predictions from the 360 depth model.", "description": "This figure illustrates the training pipeline of the proposed approach.  It consists of two main stages: supervised training with labeled data and distillation with unlabeled data. The labeled data is used to train a 360-degree depth model using ground truth depth values. The unlabeled data is processed using a perspective depth estimation model (Depth Anything) to create pseudo labels. This process leverages a six-face cube projection technique to align the perspective and 360-degree views.  To improve robustness, invalid regions (e.g., sky, watermarks) are masked out using Segment Anything before computing the loss on pseudo labels.  Finally, random rotation is added as data augmentation to bridge the knowledge gap across different camera projections.", "section": "3 Methods"}, {"figure_path": "VzoyBrqJ4O/figures/figures_4_1.jpg", "caption": "Figure 3: Valid Pixel Masking. We used Grounded-Segment-Anything [36] to mask out invalid pixels based on two text prompts: \"sky\" and \"watermark.\" These regions lack depth sensor ground truth labels in all previous datasets. Unlike Depth Anything [59], which sets sky regions as 0 disparity, we follow ground truth training to ignore these regions during training for two reasons: (1) segmentation may misclassify and set other regions as zero, leading to noisy labeling, and (2) watermarks are post-processing regions that lack geometrical meaning.", "description": "This figure shows examples of how the Grounded-Segment-Anything model is used to identify and mask out invalid regions (sky and watermarks) in unlabeled 360-degree images.  These invalid regions are masked because they lack ground truth depth labels and can interfere with training.  The approach uses text prompts (\"sky\" and \"watermark\") to guide the segmentation process.  The figure highlights the importance of this masking step for improving the accuracy of 360-degree depth estimation.", "section": "3 Methods"}, {"figure_path": "VzoyBrqJ4O/figures/figures_5_1.jpg", "caption": "Figure 4: Qualitative visualization of a model trained directly on pseudo equirectangular data without scale alignment. We propose calculating the loss with pseudo ground truth on cube faces due to scale misalignment between the six faces during the cube-to-equirectangular projection. We showcase the results of a model trained on pseudo equirectangular data without scale alignment as a simple baseline to demonstrate the importance of calculating loss separately on each of the six faces. The images are presented from top to bottom as follows: (a) RGB images. (b) Pseudo cube ground truth projected directly to equirectangular. (c) Prediction trained with row 2. (d) Pseudo cube ground truth with rotation projected directly to equirectangular. (e) Prediction trained with row 4. (f) Our model's predictions are trained on cube faces separately with rotation.", "description": "This figure shows a comparison of depth estimation results using different methods.  The top row shows the input RGB images. Rows 2 and 3 show the pseudo ground truth and model predictions without random rotation, demonstrating the cube artifacts resulting from the scale misalignment between cube faces. Rows 4 and 5 show the results with random rotation applied before feeding images to the perspective model, resulting in better depth estimation. The last row shows our model's predictions, highlighting the effectiveness of our approach in addressing the scale misalignment problem.", "section": "3 Methods"}, {"figure_path": "VzoyBrqJ4O/figures/figures_6_1.jpg", "caption": "Figure 1: Our proposed training pipeline improves existing 360 monocular depth estimators. This figure demonstrated the improvement of our proposed training pipeline tested on the Stanford2D3D [2] dataset in a zero-shot setting.", "description": "This figure shows the results of applying the proposed training pipeline to improve existing 360 monocular depth estimators. The top row displays the input RGB images from the Stanford2D3D dataset. The second row shows the ground truth depth maps for these images. The third row displays the depth maps generated by the BiFuse++ model, and the bottom row displays the depth maps generated by the BiFuse++ model after applying the proposed training pipeline. The figure demonstrates that the proposed training pipeline significantly improves the accuracy of depth estimation.", "section": "1 Introduction"}, {"figure_path": "VzoyBrqJ4O/figures/figures_8_1.jpg", "caption": "Figure 1: Our proposed training pipeline improves existing 360 monocular depth estimators. This figure demonstrated the improvement of our proposed training pipeline tested on the Stanford2D3D [2] dataset in a zero-shot setting.", "description": "This figure shows the results of applying the proposed training pipeline to improve existing 360 monocular depth estimation models.  The left side displays the input RGB image, ground truth depth map, and the depth predictions of a baseline model (BiFuse++) and its improved version after applying the training pipeline (BiFuse++(p)). The right side displays an error map illustrating the difference between the predicted depth and the ground truth depth. The results demonstrate significant improvements in depth estimation accuracy achieved by the proposed method, particularly in challenging zero-shot scenarios where models are trained on one dataset and tested on another.", "section": "1 Introduction"}, {"figure_path": "VzoyBrqJ4O/figures/figures_9_1.jpg", "caption": "Figure 1: Our proposed training pipeline improves existing 360 monocular depth estimators. This figure demonstrated the improvement of our proposed training pipeline tested on the Stanford2D3D [2] dataset in a zero-shot setting.", "description": "This figure shows the improvement achieved by the proposed training pipeline. The pipeline is tested on the Stanford2D3D dataset in a zero-shot setting. The figure shows the input RGB image, the ground truth depth map, the depth map predicted by the BiFuse++ model, and the error map for BiFuse++.  The pipeline is shown to significantly improve depth estimation.", "section": "1 Introduction"}, {"figure_path": "VzoyBrqJ4O/figures/figures_9_2.jpg", "caption": "Figure 1: Our proposed training pipeline improves existing 360 monocular depth estimators. This figure demonstrated the improvement of our proposed training pipeline tested on the Stanford2D3D [2] dataset in a zero-shot setting.", "description": "This figure shows the improvement achieved by the proposed training pipeline.  The pipeline uses a semi-supervised learning approach, combining labeled and unlabeled data.  The example shown is a zero-shot setting where the model is tested on the Stanford2D3D dataset, demonstrating successful knowledge transfer and improved depth estimation accuracy. The images display input RGB images, ground truth depth maps, and the depth predictions from BiFuse++ both with and without the authors' proposed training pipeline improvements.  The difference in accuracy is clearly visible, highlighting the effectiveness of their method.", "section": "1 Introduction"}, {"figure_path": "VzoyBrqJ4O/figures/figures_16_1.jpg", "caption": "Figure 1: Our proposed training pipeline improves existing 360 monocular depth estimators. This figure demonstrated the improvement of our proposed training pipeline tested on the Stanford2D3D [2] dataset in a zero-shot setting.", "description": "This figure shows the improvement achieved by applying the proposed training pipeline to enhance existing 360 monocular depth estimation models.  The results are demonstrated using the Stanford2D3D dataset in a zero-shot setting.  It visualizes the input RGB image, ground truth depth map, depth estimations from two baseline models (BiFuse++, UniFuse), and the improved depth maps obtained after using the proposed technique. The differences (error maps) between the ground truth and different model predictions are also displayed, highlighting the effectiveness of the proposed method.", "section": "1 Introduction"}, {"figure_path": "VzoyBrqJ4O/figures/figures_16_2.jpg", "caption": "Figure 1: Our proposed training pipeline improves existing 360 monocular depth estimators. This figure demonstrated the improvement of our proposed training pipeline tested on the Stanford2D3D [2] dataset in a zero-shot setting.", "description": "This figure shows a comparison between ground truth depth maps, depth maps estimated using BiFuse++, and depth maps estimated using the proposed method on the Stanford2D3D dataset. The proposed method significantly improves the accuracy of depth estimation, particularly in challenging areas such as the edges of objects and areas with significant shadows. The improved accuracy is particularly notable in the zero-shot setting, where the model is tested on a dataset that it was not trained on.", "section": "1 Introduction"}, {"figure_path": "VzoyBrqJ4O/figures/figures_17_1.jpg", "caption": "Figure 8: Generalization ability in the wild with point cloud visualization. We showcase zero-shot qualitative results in the point cloud using a combination of images we captured and randomly sourced from the internet to assess the model's generalization ability.", "description": "This figure demonstrates the generalization capability of the model by visualizing point clouds generated from both images captured by the authors and images randomly sourced from the internet.  The zero-shot performance on unseen data is showcased to highlight the model's ability to generalize to diverse scenes and viewpoints.", "section": "4.4 Qualtative Results in the Wild"}, {"figure_path": "VzoyBrqJ4O/figures/figures_18_1.jpg", "caption": "Figure 8: Generalization ability in the wild with point cloud visualization. We showcase zero-shot qualitative results in the point cloud using a combination of images we captured and randomly sourced from the internet to assess the model's generalization ability.", "description": "This figure demonstrates the model's ability to generalize to unseen data by visualizing point clouds generated from 360-degree images captured in various real-world environments.  The results showcase the model's performance in estimating depth and generating a 3D representation from various scenarios, illustrating its robustness and generalizability beyond the datasets used during training.", "section": "4.4 Qualtative Results in the Wild"}]