[{"figure_path": "VzoyBrqJ4O/tables/tables_3_1.jpg", "caption": "Table 1: 360 monocular depth estimation lacks a large amount of training data. The number of images in 360-degree monocular depth estimation datasets alongside perspective depth datasets from the Depth Anything methodology.", "description": "This table compares the amount of labeled and unlabeled data available for training 360-degree monocular depth estimation models versus perspective depth estimation models.  It highlights the significant disparity in data availability, with perspective datasets having substantially more labeled and unlabeled data. This difference in data size underscores the challenge of training effective 360-degree depth estimation models and motivates the use of techniques like perspective distillation to leverage the abundance of perspective data.", "section": "3.1 Unleashing the Power of Unlabeled 360 data"}, {"figure_path": "VzoyBrqJ4O/tables/tables_7_1.jpg", "caption": "Table 2: Matterport3D Benchmark. The upper section lists 360 methods trained with metric depths in meters using BerHu loss. All numbers are sourced from their respective papers. The lower section includes selected methods retrained with relative depth (disparity) using affine-invariant loss.", "description": "This table presents a comparison of different 360-degree depth estimation methods' performance on the Matterport3D benchmark dataset.  The upper part shows results using the BerHu loss function for metric depth (in meters), while the lower part shows results obtained by retraining selected models with the Affine-Invariant loss on relative depth (disparity).  The metrics used for evaluation are Absolute Relative Error (Abs Rel), \u03b41, \u03b42, and \u03b43, representing the percentage of pixels within a certain range of the ground truth depth.", "section": "4.2 Benchmarks Evaluation"}, {"figure_path": "VzoyBrqJ4O/tables/tables_8_1.jpg", "caption": "Table 3: Zero-shot Evaluation on Stanford2D3D. We perform zero-shot evaluations with models trained on other datasets. Following the original training settings, we train the 360 models [48, 16, 45, 64] on the entire Matterport3D dataset and then test on Stanford3D's test set.", "description": "This table presents the results of a zero-shot evaluation on the Stanford2D3D dataset.  Models were trained on the Matterport3D dataset and then evaluated on the Stanford2D3D dataset without further training.  The table shows various metrics (Abs Rel, \u03b41, \u03b42, \u03b43) to compare the performance of different methods, including baselines and the proposed method with and without additional pseudo-labeled data from other datasets (ST-all (p), SP-all (p)).  It demonstrates the effectiveness of the proposed training technique in generalizing to unseen data.", "section": "4.3 Zero-Shot Evaluation"}, {"figure_path": "VzoyBrqJ4O/tables/tables_8_2.jpg", "caption": "Table 3: Zero-shot Evaluation on Stanford2D3D. We perform zero-shot evaluations with models trained on other datasets. Following the original training settings, we train the 360 models [48, 16, 45, 64] on the entire Matterport3D dataset and then test on Stanford3D's test set.", "description": "This table presents the results of zero-shot evaluation on the Stanford2D3D dataset.  Models were initially trained on the full Matterport3D dataset and then tested on Stanford2D3D without further training.  The table compares the performance of several 360-degree depth estimation methods (UniFuse, BiFuse++, HoHoNet, EGFormer) and shows improvements when using the proposed training approach with pseudo-labels generated from Depth Anything. Different loss functions (BerHu and Affine-Inv) and the use of additional datasets (Structured3D and SpatialAudioGen) for pseudo-label generation are also analyzed. The metrics used for evaluation include Absolute Relative error (AbsRel), \u03b41, \u03b42, and \u03b43.", "section": "4.3 Zero-Shot Evaluation"}, {"figure_path": "VzoyBrqJ4O/tables/tables_9_1.jpg", "caption": "Table 5: Metric depth fine-tuning. We fine-tune our model trained with Matterport3D [6] ground truth label and Structured3D [65] pseudo label on relative depth with Stanford2D3D [2]'s training set metric depths with a single epoch.", "description": "This table shows the results of fine-tuning a model pre-trained on relative depth using the Matterport3D and Structured3D datasets. The fine-tuning is performed on the Stanford2D3D dataset for a single epoch using metric depth.  The table presents several evaluation metrics, such as Mean Absolute Error (MAE), Absolute Relative Error (Abs Rel), Root Mean Squared Error (RMSE), Root Mean Squared Logarithmic Error (RMSElog), and \u03b4i metrics (\u03b41, \u03b42, \u03b43), to assess the model's performance after fine-tuning.", "section": "4.5 Fine-Tuned to Metric Depth Estimation"}, {"figure_path": "VzoyBrqJ4O/tables/tables_17_1.jpg", "caption": "Table 1: 360 monocular depth estimation lacks a large amount of training data. The number of images in 360-degree monocular depth estimation datasets alongside perspective depth datasets from the Depth Anything methodology.", "description": "This table compares the amount of labeled and unlabeled data available in popular 360-degree depth estimation datasets with that of perspective depth datasets used in the Depth Anything model.  It highlights the significant disparity in data availability, with 360-degree datasets having considerably less labeled and unlabeled data than perspective datasets, thus motivating the use of the proposed method to leverage unlabeled data.", "section": "3.1 Unleashing the Power of Unlabeled 360 data"}, {"figure_path": "VzoyBrqJ4O/tables/tables_17_2.jpg", "caption": "Table 7: Ratios of GT and Pseudo label during training. We conduct additional experiments with varying ratios, which shows our method is robust across different ratios starting from 1:1.", "description": "This table presents an ablation study on the ratio of ground truth to pseudo labels used during training.  It shows the results of experiments conducted with ratios of 1:1, 1:2, and 1:4, evaluating the performance using Absolute Relative Error (AbsRel) and three other metrics (\u03b41, \u03b42, \u03b43). The results demonstrate the robustness of the proposed method across different ratios, with consistent performance starting from a 1:1 ratio.", "section": "4.3 Zero-Shot Evaluation"}, {"figure_path": "VzoyBrqJ4O/tables/tables_18_1.jpg", "caption": "Table 8: Comparison between Tangent Image and Cube Projection. We compare two of the most commonly used perspective camera projections for panorama images. As shown in the table, both projections yield similar quantitative results. However, we select the cube projection for knowledge distillation due to its broader field of view coverage.", "description": "This table compares the performance of two different projection methods (cube and tangent image) used in the proposed 360-degree depth estimation framework.  The results demonstrate that both methods achieve similar accuracy, but the cube projection is preferred due to its wider field of view, which is beneficial for knowledge distillation during training.", "section": "4 Experiments"}]