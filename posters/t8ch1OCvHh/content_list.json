[{"type": "text", "text": "Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Modern large language models (LLMs) have established state-of-the-art perfor  \n2 mance through architectural improvements, but still require significant computa  \n3 tional cost for inference. In an effort to reduce the inference cost, post-training   \n4 quantization (PTQ) has become a popular approach, quantizing weights and acti  \n5 vations to lower precision, such as INT8. In this paper, we reveal the challenges   \n6 of activation quantization in GLU variants [40], which are widely used in feed  \n7 forward network (FFN) of modern LLMs, such as LLaMA family. The problem is   \n8 that severe local quantization errors, caused by excessive magnitudes of activation   \n9 in GLU variants, significantly degrade the performance of the quantized LLM. We   \n10 denote these activations as activation spikes. Our further observations provide a   \n11 systematic pattern of activation spikes: 1) The activation spikes occur in the FFN of   \n12 specific layers, particularly in the early and late layers, 2) The activation spikes are   \n13 dedicated to a couple of tokens, rather than being shared across a sequence. Based   \n14 on our observations, we propose two empirical methods, Quantization-free Module   \n15 (QFeM) and Quantization-free Prefix (QFeP), to isolate the activation spikes during   \n6 quantization. Our extensive experiments validate the effectiveness of the proposed   \n17 methods for the activation quantization, especially with coarse-grained scheme, of   \n18 latest LLMs with GLU variants, including LLaMA-2/3, Mistral, Mixtral, SOLAR,   \n19 and Gemma. In particular, our methods enhance the current alleviation techniques   \n20 (e.g., SmoothQuant) that fail to control the activation spikes.1 ", "page_idx": 0}, {"type": "text", "text": "21 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "22 Large language models (LLMs) have become a key paradigm in natural language processing, acceler  \n23 ating the release of variations within the community [49, 58]. Furthermore, latest LLMs establish   \n24 state-of-the-art performance by training with increased scale, as well as by adopting architectural   \n25 improvements such as GLU [40], RoPE [41], GQA [2], and MoE [21]. Especially, GLU (Gated   \n26 Linear Unit) variants (e.g., SwiGLU, GeGLU) has been adopted in the most of modern LLM archi  \n27 tectures (e.g., LLaMA family [46]), due to training efficiency [31, 40]. Although LLMs broaden   \n28 foundational capabilities in natural language tasks and potential for various applications, billions of   \n29 parameters in the large models impose considerable computational costs on end users in practice. To   \n30 reduce GPU memory requirements and accelerate inference speed, post-training quantization (PTQ)   \n31 offers an affordable solution by quantizing weights and activations into a lower precision (e.g., INT8)   \n32 without a need for expensive retraining steps [17, 19, 30]. However, recent studies have revealed that   \n33 large magnitude values at certain coordinates exist in the activations of LLMs, which are often called   \n34 outliers, posing a key challenge in activation quantization [1, 12, 50, 51]. Another line of works   \n35 attempts to explain the role of outlier values in the attention mechanism [9, 42]. Nevertheless, current   \n36 research on the impact of evolving LLM architectures on the outliers remains insufficient.   \n37 In this paper, we present our discovery that the GLU architecture in the feed-forward network (FFN)   \n38 generates excessively large activation values, which are responsible for significant local quantization   \n39 errors. Specifically, we observe that these problematic activation values occur in specific linear   \n40 layers and are dedicated to a couple of tokens, which will be discussed in Section 3. To distinguish   \n41 the excessive GLU activations from the outliers, we refer to them as activation spikes. In light of   \n42 our observations, we propose two empirical methods to mitigate the impact of activation spikes   \n43 on quantization: Quantization-free Module (QFeM) and Quantization-free Prefix (QFeP). QFeM   \n44 aims to partially exclude quantization for linear layers (or modules) where large quantization errors   \n45 occur, instead of quantizing the entire linear modules in the LLM. By scoring the extent of scale   \n46 disparity, QFeM selects linear modules to exclude. On the other hand, QFeP identifies the prefix that   \n47 triggers activation spikes and preserves its context as a key-value (KV) cache, thereby preventing the   \n48 recurrence of activation spikes in subsequent tokens. It is noteworthy that both QFeM and QFeP rely   \n49 on calibration results to capture activation spikes in advance, without any modifications to the target   \n50 LLM. This indicates that our methods can be integrated into any existing quantization methods.   \n51 In our comprehensive experiments, we demonstrate that recently released LLMs incorporating   \n52 GLU variants struggle with activation spikes when applying activation quantization. Consequently,   \n53 the proposed methods, QFeM and QFeP, substantially enhance the performance of the primitive   \n54 quantization method, the round-to-nearest (RTN) method. Furthermore, we observe that current   \n55 outlier alleviation methods [50, 51] are exposed to the activation spikes and benefti from our proposed   \n56 methods. Compared to the strong baseline of fine-grained activation quantization [55], our methods   \n57 show competitive performance, achieving reduced latency and memory footprint. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "58 In summary, the contributions of our work are as follows: ", "page_idx": 1}, {"type": "text", "text": "59 \u2022 We find that the GLU architecture in modern LLMs systematically generates excessive activation   \n60 values, which are responsible for significant performance degradation in activation quantization.   \n61 \u2022 Based on our observations, we propose two empirical methods, QFeM and QFeP, which effectively   \n62 exclude the activation spikes during quantization, with negligible computational overhead and   \n63 compatibility with any existing quantization techniques.   \n64 \u2022 Our extensive experimental results validate the detrimental impact of the activation spikes on activa  \n65 tion quantization, while our proposed methods consistently enhance the quantization performance. ", "page_idx": 1}, {"type": "text", "text": "66 2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "67 Outlier Values in LLMs. Previously, outlier values have been observed in the transformer-based   \n68 language models such as BERT [14] and early GPT [36] models through numerous studies [8, 24,   \n69 27, 35, 45]. Since the advent of LLMs [10, 57] rooted in the GPT, recent studies by [1, 12, 51] have   \n70 tackled the existence of outlier values in LLMs. According to them, these outliers exhibit a large   \n71 magnitude of values at the shared dimensions of hidden states across tokens. More recently, [9, 42]   \n72 explain that the outliers attribute to the vertical pattern in the attention mechanism [25, 52], which   \n73 influences the performance of LLMs. In particular, [42] claims a different type of outlier existing in   \n74 the hidden states of specific tokens. However, prior studies merely focus on the superficial hidden   \n75 states between the decoder layers. Our work provides a module-level investigation where quantization   \n76 is applied practically, focusing on different LLM architectures.   \n77 Post-training Quantization for LLMs. Post-training quantization (PTQ) refers to the quantization   \n78 of a neural network model to low precision, such as INT8, without additional parameter updates [17,   \n79 19]. Especially for LLMs, this approach cost-effectively achieves inference with low memory usage   \n80 and faster inference latency by quantizing the weights and activations used in matrix multiplication   \n81 (e.g., linear layer). However, because of the challenges in activation quantization of LLMs, many   \n82 recent works are mainly focused on the weight-only quantization [11, 13, 15, 23, 26, 39, 54].   \n83 Otherwise, the activation quantization faces inherent outliers, which hinder accurate quantization   \n84 by reducing representation resolution. To address this challenge, [12] proposes a mixed-precision   \n85 quantization method where the outlier dimensions are computed in high precision. [50, 51] approach   \n86 migration of scale from activation to weights to alleviate the scale of outlier activations. Along this   \n87 line of research, we propose to enhance the activation quantization based on our observations. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "t8ch1OCvHh/tmp/7d938688281cbba55252a406754ac577a1526a4e49bfc628e8bd26ab93929a83.jpg", "img_caption": ["Figure 1: Calibration results on GLU-implemented and non GLU-implemented LLMs. We present the maximum magnitudes of input activations for each linear modules and layer-wise hidden states. For more results on different LLMs, see Appendix A.2, A.3. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "88 3 Activation Spikes: Excessive Magnitude of GLU Activations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "89 For clarity, \"hidden states\" refer to the output tensor of a transformer layer (or block), while \"input   \n90 activations\" or \"activations\" denote the input tensor of a linear layer (or module) in the remain of this   \n91 paper. Recent work [42] has investigated a novel type of outlier existing in the hidden states across   \n92 modern LLMs. Although these outliers of hidden states play a crucial role in the attention mechanism   \n93 [9, 42, 52], their relationship with input activations for quantization has not been fully explored.   \n94 Importantly, because recent LLMs adopt Pre-LN [4, 53], which normalizes hidden states before self  \n95 attention and feed-forward network (FFN) blocks, the scale of hidden states does not reflect the scale   \n96 of input activations within the transformer block. Therefore, we focus on the input activations fed into   \n97 each linear module within the transformer block to connect to activation quantization. Specifically, we   \n98 examine the four linear (projection) layers: query (parallel to key and value), out, up (parallel to   \n99 gate), and down modules. For detailed illustration of Pre-LN transformer, please see Appendix D.1. ", "page_idx": 2}, {"type": "text", "text": "100 3.1 Existence of Activation Spikes in GLU Variants ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "101 To analyze the input activations, we employ a calibration method, which is used to estimate the   \n102 quantization factors such as scale and zero-point. For the calibration data, we use 512 samples   \n103 randomly collected from the C4 [37] training dataset. Afterwards, we feed each sample into the LLM   \n104 and monitor each hidden state and input activation through the decoder layers. To estimate the scale   \n105 factor, we use absolute maximum value. The tested LLMs are listed in Appendix A.1.   \n106 GLU-implemented LLMs exhibit activation spikes at specific layers. In Figure 1a, we display   \n107 the calibrated scale factors for the LLMs that implement GLU variants (e.g., SwiGLU, GeGLU).   \n108 Across models, we observe a shared pattern of scale from the results. Within the early and late   \n109 layers, the down modules in the FFN show noticeable magnitudes of input activations. Note that   \n110 these input activations are derived from the Hadamard Product within GLU. Thus, the GLU variants   \n111 generate activation spikes at the specific layers. Interestingly, we notice a high correlation between the   \n112 emergence of activation spikes and intermediate hidden states of large scale. This indicates that the   \n113 FFN contributes to amplifying the hidden states via the addition operation in the residual connection   \n114 [18]. Once the magnitude of the hidden states is exploded, it persists through layers until encounter   \n115 the activation spikes at late layers.   \n116 Non GLU-implemented LLMs show modest scale distribution. Figure 1b illustrates the cali  \n117 bration results for LLMs with the original feed-forward implementation in Transformer [48]. We   \n118 observe that the LLMs continue to generate the large-scale hidden states, regardless of the GLU   \n119 implementation. This corresponds to the observations in [42]. More importantly, our module-level   \n120 results elaborate that the scale of hidden states is not transferable to the input activations of inner   \n121 linear modules. Instead, we reveal that GLU variants are associated with the hidden states and   \n122 generate activation spikes. This clarifies the quantization challenge of the GLU-implemented LLMs   \n123 concentrated in the early and late layers. Because excessive scales of activation spikes have the   \n124 potential to hinder the accurate quantization, we conduct an in-depth analysis to better understand   \n125 these activation spikes in the following sections. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "t8ch1OCvHh/tmp/e80ef7410b5be0b907e3ffd287130c43d705f087ab2df83aa081334ff9ce7971.jpg", "img_caption": ["Figure 2: Token-wise scales in a specific layer with an activation spike. When quantizing the input activations using a per-tensor scale, the scale of the activation spike dominates the scales of the other tokens. For more examples, see Appendix D.2. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "126 3.2 Token-level Scale Analysis within Activation Spikes ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "127 In the previous section, we observed the excessive scale of the input activations derived from GLU   \n128 activation. When quantizing the input activations, the variance of input activation scales for each   \n129 token affects the quantization performance [55]. To delve into the disparity between token-wise   \n130 scales in the activation spikes, we unroll them through the sequence of tokens. Figure 2 illustrates   \n131 the individual input activation scales where the activation spike appears. Given a token sequence,   \n132 the large magnitudes of input activations are observed in a couple of tokens, such as the BOS token,   \n133 newline $(\\setminus\\mathbf{n})$ , and apostrophe ('). These specific tokens coincide with the observations of [42], which   \n134 suggests that such tokens exhibit massive values in the hidden states. Thus, the activation spike is   \n135 associated with the process of assigning a special role to these tokens in later transformer layers.   \n136 However, the excessive scale of specific token hinders the estimation of scale factor for the other   \n137 tokens, such as in per-tensor quantization. Additionally, the largest scale is dedicated to the first   \n138 instance of the specified token, while the following usage exhibits a modest scale. This phenomenon   \n139 makes the quantization more complicated, as the activation spikes dynamically occur depending on   \n140 the current input sequence. ", "page_idx": 3}, {"type": "text", "text": "141 3.3 Effect of Quantization on Activation Spikes ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "142 We explore the impact of local quantization errors caused by activation spikes on LLM outputs. To   \n143 identify the layers where activation spikes occur, we utilize a ratio between the maximum and median   \n144 values of the token-wise input activation scales, instead of using the maximum scale value alone.   \n145 The max-median ratio for linear layer m can be formulated as r(m) = memdaixa(nS((Sm()m))) , where S(m)   \n146 represents the token-wise input activation scales incoming to module $m\\in M$ . This max-median   \n147 ratio captures the extent to which maximum scale dominate the other token scales. For comparison,   \n148 we choose the activation quantization targets as the top-4, middle-4, and bottom-4 modules, based on   \n149 the max-median ratio in descending order. Then, we evaluate the perplexity and mean-squared error   \n150 (MSE) using the calibration dataset. Here, the MSE is calculated for the last hidden states between   \n151 the original (FP16) and partially quantized LLM. As shown in Table 1, quantization on the top-4 rated   \n152 modules solely degrades the LLM performance by significant margins, while the other cases exhibit   \n153 negligible performance changes. We consider these quantization-sensitive input activations (inter alia   \n154 activation spikes) to be the quantization bottleneck, which, in this paper, refers to the quantization   \n155 error caused by outliers.   \n156 Furthermore, the activation spikes are conditioned on the specific context of the input sequence as   \n157 discussed in Section 3.2. Altogether, such dynamic bottlenecks must be handled with caution to   \n158 enhance the quantization performance of LLMs. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "table", "img_path": "t8ch1OCvHh/tmp/fa54256a6fd59cbb5a92f26fe2b7446d32e84ecd9e020cc1fb23e7cc4642b3e0.jpg", "table_caption": ["Table 1: Perplexity and MSE of partial activation quantization of LLMs "], "table_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "t8ch1OCvHh/tmp/b9ecb4f5fae104fc298bab45fce754e30febddc81cdf2050f9dd8a242a19f65a.jpg", "img_caption": ["Figure 3: Overview of QFeM and QFeP. (Left): QFeM excludes the modules whose $r^{(m)}$ is larger than the hyperparameter $\\alpha$ from quantization. (Right): QFeP computes in advance the prefix of activation spikes and utilizes solely their KV cache during the quantization phase, effectively preventing further activation spikes in subsequent sequences. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "159 4 Mitigating Quantization Quality Degradation Based on the Observation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "160 To address the quantization bottleneck, our approach is based on the deterministic occurrence patterns   \n161 of activation spikes. First, we utilize the observation that bottlenecks occur at a few specific layers.   \n162 This implies that naive full quantization of LLMs is affected by these bottlenecks. Second, we exploit   \n163 the phenomenon that the activation spike is derived from the first occurrence of specific tokens. Thus,   \n164 the planned occurrence prevents recurrence in the subsequent and possibly future tokens. In the   \n165 following sections, we propose two methods inspired the above insights. ", "page_idx": 4}, {"type": "text", "text": "166 4.1 Quantization-free Module (QFeM) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "167 In the full quantization of LLM, all linear layers within the LLM are quantized. Among these   \n168 linear layers, we propose omitting the quantization of input activations for linear layers where   \n169 significant quantization errors are caused by activation spikes. To be noted, increasing the number of   \n170 unquantized modules exhibits a trade-off between the inference latency and the model performance.   \n171 Thus, determining which module should be quantized (or left unquantized) is crucial to retain the   \n172 efficacy of quantization. Here, we use the max-median ratio $r^{(m)}$ and define a set of unquantized   \n173 modules, denoted as $M_{\\mathrm{unq}}$ , where the ratio $r^{(m)}$ of each linear layer is larger than threshold $\\alpha$ . For   \n174 instance, all linear layers in $M$ are quantized if $\\alpha=\\infty$ . For clarity, we treat sibling linear layers,   \n175 such as query-key-value, as a single linear layer. To control the impact of activation quantization only,   \n176 we leave the weight parameters in unquantized linear layers as INT8 and dequantize them into FP16   \n177 during matrix multiplication with the incoming activations, operating as weight-only quantization.   \n178 Optimizing the threshold $\\alpha$ . To calculate the activation   \n179 scale ratio for each linear layer, we first gather token-wise   \n180 input activation scales from the calibration examples dis  \n181 cussed in Section 3.1. Exceptionally, for FFN experts in   \n182 the mixture of experts (MoE) architectures like the Mix  \n183 tral model [21], calibration is performed separately. After   \n184 determining these ratios, we use binary search to set the   \n185 threshold value $\\alpha$ , balancing inference latency and perfor  \n186 mance degradation. As a metric, we assess performance   \n187 through perplexity measured on the same calibration ex  \n188 amples. For example, the relationship between threshold   \n189 value $\\alpha$ and its impact on performance is depicted in Fig  \n190 ure 4, demonstrating how full quantization can degrade   \n191 performance. Rather than fully quantizing, we identify an   \n192 optimal threshold by finding the intersection of two performance curves; in Figure 4, this threshold is   \n193 approximately 16. Details on the QFeM implementation are provided in Table 2.   \n195 Orthogonal to the QFeM, we propose Quantization-free Prefix (QFeP) that mitigates the quantization   \n196 errors by precomputing the prefix (or short prompt) corresponding to activation spikes. This method   \n197 is based on the observations presented in Section 3.2, which indicate that significant quantization   \n198 errors result from the overestimated scale factor of the first instance within the restricted token   \n199 set. Inspired by this occurrence pattern of activation spikes, we aim to construct a prefix which   \n200 stabilizes the quantization scale factor of the tokens that come after the prefix. In other words,   \n201 once the prefix is fixed at the beginning, the activation spikes consistently occur within the prefix.   \n202 Afterward, we employ key-value (KV) caching mechanism to process the activation spikes in advance.   \n203 In practice, KV cache is utilized to optimize the decoding speed of causal language models by storing   \n204 precomputed key and value states of the previous tokens [32, 34]. This approach provides a bypass   \n205 of the quantization including activation spikes, while preserving the context of prefix through the   \n206 KV cache. The KV cache for the prefix is precomputed once through the offline inference of LLM   \n207 without quantization. Then, this KV cache is exploited in the quantization phases, such as calibration   \n208 or dynamic quantization, even for quantized inference. The process of QFeP is illustrated in Figure 3.   \n209 Prefix Search. To form a prefix of explicit activation spike, we first identify candidate token that   \n210 represent the activation spike at the linear layer with the highest max-median ratio $r^{(m)}$ . For instance,   \n211 the candidate token can be apostrophe $(\\,\"\\,)$ token for LLaMA-2-70B model, as highlighted in red in   \n212 Figure 2. Once the candidate token is identified, we search the middle context token for between   \n213 the BOS token and the candidate token in the prefix. This middle context provides dummy context,   \n214 which is required to activate the candidate token. To find the middle context, we design a template   \n215 $[B,T_{1},C_{1},\\bar{T}_{2},C_{2}]$ where $B,T_{i}$ , and $C_{i}$ denote the BOS token, context token, and candidate token in   \n216 the vocabulary $V$ , respectively. Then, we select the context token $T$ where $C_{1}$ triggers an activation   \n217 spikes, while later instance of the same token $C_{2}$ does not. When the context token for the activation   \n218 spikes is varied, we choose the token that maximizes the activation scale ratio between the $C_{1}$ and   \n219 $C_{2}$ . Finally, we prepare the KV cache for searched prefix of $[B,T,C]$ . Note that the latter sequence   \n220 in the template can be replaced with sequences from dataset instead of repetition.   \n221 Implementation Details. During the prefix   \n222 search phase, we exploit the calibration dataset   \n223 used in Section 3.1. For the candidate tokens, we   \n224 consider the tokens with the top three largest in  \n225 put activation magnitudes. Then, we search for   \n226 the middle context token among top 200 most fre  \n227 quent tokens in the calibration dataset, which is   \n228 the subset of the vocabulary $V$ . Finally, with the   \n229 search result, we prepare the KV cache for the   \n230 target model in FP16 precision. Exceptionally, for   \n231 the Mixtral [21] model, we use the scale of output   \n232 hidden states instead of input activations, as the   \n233 tokens are divided sparsely in a mixture of experts   \n234 architecture. Table 2 presents the searched prefix. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "t8ch1OCvHh/tmp/bb786cce584927a7352a66f65a62bbcab74f1868f79561a3abad6ac07b9cafd8.jpg", "img_caption": ["Figure 4: Trade-off between perplexity (stands for performance) and $\\vert M_{u n q}\\vert$ (stands for latency) according to the threshold $\\alpha$ for LLaMA-2-13B model. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "t8ch1OCvHh/tmp/1b658cc35ce8225266e09fa600ec088a078d63f9fa6f36d18819e29907e17e4e.jpg", "table_caption": ["Table 2: Specifications for $\\mathrm{QFeM}$ and QFeP used in experiments. $|M|$ denotes the total number of linear layers in the LLM, and $\\vert M_{u n q}\\vert$ represents the number of unquantized layers for $\\mathrm{QFeM}$ . "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "235 5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "236 5.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "237 Models. Our proposed methods, QFeM and QFeP, aim to mitigate the quantization bottleneck,   \n238 which is discussed in Section 3.3, caused by the activation spikes, especially in the GLU variants. To   \n239 validate the efficiency proposed methods, we tested publicly released LLMs that were implemented   \n240 with GLU, according to their paper and source code. We recognize recent LLMs, including LLAMA  \n241 2-{7B, 13B, 70B} [47], LLaMA-3-{7B, 70B}, Mistral-7B [20], Mixtral-8x7B [21], SOLAR-10.7B   \n242 [22], and Gemma-7B [43], utilize the GLU architecture. The LLMs with original FFN are not   \n243 covered, as they suffer from the existing outliers rather than activation spikes. All models are sourced   \n244 from the huggingface-hub2 repository.   \n245 Quantization. In the experiments, we quantize both the input activations and the weights of linear   \n246 layers for INT8 matrix multiplication operations. Note that in Table 2, $|M|$ denotes the total number   \n247 of linear modules targeted for quantization. In these linear layers, we opt for dynamic per-tensor   \n248 quantization as the quantization scheme of input activations, and per-channel quantization for weights,   \n249 respectively. Regarding both input activations and weights, we symmetrically quantize the range   \n250 using the absolute maximum value as the scale estimation function. For comparison, we use FP16   \n251 and per-token activation quantization [55] as baselines. We refer the reader to Appendix B for Batch   \n252 Matrix-Multiplication (BMM) quantization, which involves quantizing tensors in the self-attention.   \n253 Evaluations. We evaluate the quantized LLMs with two metrics: zero-shot evaluation accuracy   \n254 and perplexity. For zero-shot evaluation, we use the four datasets: PIQA [7], LAMBADA [33],   \n255 HellaSwag [56], and WinoGrande [38]. We utilize the lm-evaluation-harness library [16] to evaluate   \n256 zero-shot tasks. To measure perplexity, we use the WikiText-2 [28] dataset. In all cases, we use the   \n257 [BOS] token as the starting token for each input sequence by default. ", "page_idx": 5}, {"type": "table", "img_path": "t8ch1OCvHh/tmp/3e030f8385b458a0bd3ed7dc270fea90ef3676ff4f7b14643a9cdf1b51541044.jpg", "table_caption": ["Table 3: Perplexity and zero-shot evaluation for the quantization on LLaMA-2 models. FP16 denotes the original model precision, and W8A8 denotes the model quantized to INT8 for both weights and activations. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "258 5.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "259 LLaMA-2 Models. We report the evaluation results of quantization on LLaMA-2 models in Table 3.   \n260 Compared to FP16 precision, quantizing both weights and activations (W8A8) degrades the overall   \n261 performance. The results demonstrate that our proposed methods resolve the activation spikes   \n262 and, surprisingly, restore the performance of the W8A8 close to that of FP16. For example, the   \n263 LLaMA-2 7B model achieves less than a $1\\%$ performance drop from FP16. It is worth noting that the   \n264 proposed QFeM and QFeP improve at comparable levels. This indicates that the activation spikes   \n265 present a direct cause of the significant decrease in quantization performance. Because the proposed   \n266 methods are orthogonal, the performance slightly increases when incorporating both $\\mathrm{QFeM}$ and QFeP   \n267 compared to applying them individually.   \n268 Other GLU-implemented LLMs. For other LLMs that incorporate GLU, we investigated the   \n269 effectiveness of our methods in mitigating the quantization bottleneck. As can be seen in Figure 5,   \n270 our methods consistently remedy the performance drop caused by activation spikes. Noticeably,   \n271 the Mixtral model demonstrates robustness towards the performance degradation. This indicates   \n272 that the mixture of experts architecture, which divides the MLP experts by tokens, helps to alleviate   \n273 the impact of the activation spikes. Meanwhile, addressing the activation spikes is not a sufficient   \n274 complement for the Gemma model compared to other models. We attribute this to the choice of   \n275 activation function among GLU variants; specifically, Gemma uses GeGLU, while other models   \n276 employ SwiGLU. ", "page_idx": 6}, {"type": "table", "img_path": "t8ch1OCvHh/tmp/0cd4a2f2e926530e0097e44b8ee7edc62c85b2a8dda67c7be3f3dd5e24c7bad7.jpg", "table_caption": ["Table 4: Evaluation of outlier alleviation methods with QFeM and QFeP. We report perplexity on WikiText-2 and averaged accuracy of four zero-shot tasks. The same quantization scheme for used on both SQ and OSP. Per-tensor weight quantization results are provided in Appendix C.1. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "277 5.3 Combining Outlier Alleviation Methods ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "278 While our method focuses on the activation spikes, the inherent outlier values in the input activations   \n279 remain. Here, we combine the prior outlier alleviation methods, such as SmoothQuant (SQ) [51]   \n280 and OutlierSuppressionPlus (OSP) [50], to further improve the quantization error. In practice, our   \n281 methods are utilized during the scale calibration phase of alleviation methods to mitigate the impact   \n282 of activation spikes on scale migration between activations and weights. Table 4 demonstrates the   \n283 evaluation results of applying the outlier alleviation methods solely and combining them with our   \n284 methods. We find that there are cases where the alleviation method fails to recover the performance   \n285 when quantizing the activations with per-tensor scheme.3 This indicates that alleviating the outlier   \n286 scales, including the activation spikes, is challenging. With the QFeM, the activation spikes are   \n287 excluded, and the accurate alleviation is enabled. In addition, the QFeP also benefits from the SQ   \n288 method, as seen in the case of LLaMA-2 70B. Exceptionally, the OSP successfully addresses the   \n289 activation spikes in the 13B and 70B cases. ", "page_idx": 7}, {"type": "text", "text": "290 5.4 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "291 For the QFeP, we designed a length-three prefix for   \n292 the KV cache, including the BOS token, context to  \n293 ken, and extra token for activation spike. Because the   \n294 KV cache consumes the capacity of the pretrained se  \n295 quence position, it raises a question about the length   \n296 of the prefix. Therefore, we conduct ablation study   \n297 for different prefixes for the KV cache. For the pre", "page_idx": 7}, {"type": "image", "img_path": "t8ch1OCvHh/tmp/1200ef392c196f881d5bc0a080914006ee2f28d25c9c3e94b0679a8d13d26e5b.jpg", "img_caption": ["Figure 6: Prefix ablation. Y-axis represents averaged accuracy of four zero-shot tasks. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "fixes, we prepare random, BOS only, and both QFeP without and with the context token. We illustrate the results of ablation study in Figure 6. In all cases, the random prefix showcases the lowest performance. While the KV cache with the BOS token demonstrates inconsistent performance, our QFeP ", "page_idx": 7}, {"type": "image", "img_path": "t8ch1OCvHh/tmp/79f3ea75d5171324ae4a9e09d0a04575c0b9a4d9315e09632866341c9478ee34.jpg", "img_caption": ["tization schemes: dynamic per-token (AQ1), dynamic per-tensor (AQ2), and static per-tensor (AQ3). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "301 consistently shows significant improvement. Importantly, the results imply that the sufficient prefix   \n302 for the models exhibits differences. However, we emphasize that our KV design for QFeP shows   \n303 improvements by large margins across all models. ", "page_idx": 8}, {"type": "text", "text": "304 5.5 Computational Cost Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "305 The proposed methods require additional resources to evict the activation spikes. Therefore, we ana  \n306 lyze the computational costs of the methods and compare them in various schemes. For comparison,   \n307 we evaluate different activation quantization schemes: dynamic per-token, dynamic per-tensor, and   \n308 static per-tensor, denoted as AQ1, AQ2, and AQ3, respectively. This distinction establishes strong   \n309 baselines and demonstrates the potential of the methods. To calibrate the static scales, we estimate   \n310 the absolute maximum value using the calibration dataset, which is used in Section 3.1.   \n311 Inference Latency. For each setting, we present the accuracy of the zero-shot tasks and inference   \n312 latency of the fixed token sequence, as shown in Figure 7. While the fine-grained scheme (AQ1) shows   \n313 a negligible accuracy drop, the counterparts (AQ2, AQ3) degrade with the quantization bottleneck.   \n314 However, by applying our methods, the coarse-grained schemes achieve a competitive performance   \n315 gain. For example, the combination of AQ2 and QFeM demonstrates the performance close to   \n316 the AQ1 but with faster latency. The results signify that addressing the quantization bottleneck   \n317 is important to accelerate the inference latency with coarser granularity. Specifically, the naive   \n318 static quantization (AQ3), the fastest scheme, exhibits a significant decline. We hope that our work   \n319 contributes to the future works, which address the remaining challenges in static quantization.   \n320 Memory Footprint. In Table 5, we record the maximum memory footprint of our methods. For   \n321 QFeP, the additional memory is consistently required for the preserved KV cache. However, this   \n322 memory overhead is much smaller than that used in the fine-grained quantization (AQ1), as QFeM   \n323 utilizes only three tokens for the cache. Contrary to QFeP, QFeM shows inconsistent memory   \n324 utilization. For example, the 7B model with QFeM exhibits memory usage similar to AQ2, while the   \n325 70B model with QFeM incur additional consumption for a sequence length of 1K. This is attributed to   \n326 the use of W8A16 for the unquantization modules in QFeM. To tailor the memory usage or inference   \n327 speed, an alternative strategy can be utilized for QFeM, such as applying fine-grained activation   \n328 quantization to the unquantization modules instead of using W8A16. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "329 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "330 We explore the quantization challenge of GLU activations for modern LLMs. We find that the GLU   \n331 variants generates excessive activation scales, which cause significant quantization bottlenecks at   \n332 the specific layers. Based on the systematic generation pattern of the activation spikes, we propose   \n333 methods that address the spikes in a layer-wise (QFeM) and token-wise manner (QFeP). In the   \n334 experiments, we confirm that the proposed methods effectively resolve the quantization bottlenecks   \n335 and result in a large performance gain. We expect that our work sheds light on the potential challenges   \n336 in future studies regarding quantization and facilitates the development of efficient LLM systems. ", "page_idx": 8}, {"type": "text", "text": "337 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "338 [1] Arash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh, Zhen Stephen Gou, Phil   \n339 Blunsom, Ahmet \u00dcst\u00fcn, and Sara Hooker. Intriguing properties of quantization at scale.   \n340 Advances in Neural Information Processing Systems, 36:34278\u201334294, 2023.   \n341 [2] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr\u00f3n, and   \n342 Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head   \n343 checkpoints. arXiv preprint arXiv:2305.13245, 2023.   \n344 [3] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxan  \n345 dra Cojocaru, M\u00e9rouane Debbah, \u00c9tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin   \n346 Malartic, et al. The falcon series of open language models. arXiv preprint arXiv:2311.16867,   \n347 2023.   \n348 [4] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling.   \n349 In International Conference on Learning Representations, 2018.   \n350 [5] Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth   \n351 Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al. Stable $\\ln21.6\\,\\mathrm{b}$   \n352 technical report. arXiv preprint arXiv:2402.17834, 2024.   \n353 [6] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien,   \n354 Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward   \n355 Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In   \n356 International Conference on Machine Learning, pages 2397\u20132430. PMLR, 2023.   \n357 [7] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys  \n358 ical commonsense in natural language. In Proceedings of the AAAI conference on artificial   \n359 intelligence, volume 34, pages 7432\u20137439, 2020.   \n360 [8] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming   \n361 the challenges of efficient transformer quantization. In Marie-Francine Moens, Xuanjing   \n362 Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on   \n363 Empirical Methods in Natural Language Processing, pages 7947\u20137969, Online and Punta Cana,   \n364 Dominican Republic, November 2021. Association for Computational Linguistics.   \n365 [9] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Quantizable transformers:   \n366 Removing outliers by helping attention heads do nothing. Advances in Neural Information   \n367 Processing Systems, 36, 2024.   \n368 [10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,   \n369 Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are   \n370 few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n371 [11] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M De Sa. Quip: 2-bit quantiza  \n372 tion of large language models with guarantees. Advances in Neural Information Processing   \n373 Systems, 36, 2024.   \n374 [12] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix   \n375 multiplication for transformers at scale. Advances in Neural Information Processing Systems,   \n376 35:30318\u201330332, 2022.   \n377 [13] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh   \n378 Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized   \n379 representation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078,   \n380 2023.   \n381 [14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of   \n382 deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,   \n383 2018.   \n384 [15] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training   \n385 quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.   \n386 [16] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPof,i Charles   \n387 Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac\u2019h, Haonan Li, Kyle McDonell, Niklas   \n388 Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron,   \n389 Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework   \n390 for few-shot language model evaluation, 12 2023.   \n391 [17] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer.   \n392 A survey of quantization methods for efficient neural network inference. In Low-Power Com  \n393 puter Vision, pages 291\u2013326. Chapman and Hall/CRC, 2022.   \n394 [18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual   \n395 networks. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The   \n396 Netherlands, October 11\u201314, 2016, Proceedings, Part IV 14, pages 630\u2013645. Springer, 2016.   \n397 [19] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard,   \n398 Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for   \n399 efficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer   \n400 vision and pattern recognition, pages 2704\u20132713, 2018.   \n401 [20] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh   \n402 Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile   \n403 Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.   \n404 [21] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris   \n405 Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand,   \n406 et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.   \n407 [22] Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeon  \n408 woo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, et al. Solar 10.7 b: Scaling large language   \n409 models with simple yet effective depth up-scaling. arXiv preprint arXiv:2312.15166, 2023.   \n410 [23] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W   \n411 Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint   \n412 arXiv:2306.07629, 2023.   \n413 [24] Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, and Anna Rumshisky. BERT busters: Out  \n414 lier dimensions that disrupt transformers. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto   \n415 Navigli, editors, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,   \n416 pages 3392\u20133405, Online, August 2021. Association for Computational Linguistics.   \n417 [25] Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the dark   \n418 secrets of BERT. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings   \n419 of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th   \n420 International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages   \n421 4365\u20134374, Hong Kong, China, November 2019. Association for Computational Linguistics.   \n422 [26] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq:   \n423 Activation-aware weight quantization for llm compression and acceleration. arXiv preprint   \n424 arXiv:2306.00978, 2023.   \n425 [27] Ziyang Luo, Artur Kulmizev, and Xiaoxi Mao. Positional artefacts propagate through masked   \n426 language model embeddings. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, edi  \n427 tors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics   \n428 and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long   \n429 Papers), pages 5312\u20135327, Online, August 2021. Association for Computational Linguistics.   \n430 [28] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture   \n431 models. arXiv preprint arXiv:1609.07843, 2016.   \n432 [29] Javaheripi Mojan and Bubeck S\u00e9bastien. Phi-2: The surprising power of small language models,   \n433 2023.   \n434 [30] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart Van Baalen,   \n435 and Tijmen Blankevoort. A white paper on neural network quantization. arXiv preprint   \n436 arXiv:2106.08295, 2021.   \n437 [31] Sharan Narang, Hyung Won Chung, Yi Tay, Liam Fedus, Thibault Fevry, Michael Matena,   \n438 Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi Zhou, Wei Li, Nan Ding,   \n439 Jake Marcus, Adam Roberts, and Colin Raffel. Do transformer modifications transfer across   \n440 implementations and applications? In Marie-Francine Moens, Xuanjing Huang, Lucia Specia,   \n441 and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in   \n442 Natural Language Processing, pages 5758\u20135773, Online and Punta Cana, Dominican Republic,   \n443 November 2021. Association for Computational Linguistics.   \n444 [32] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,   \n445 and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint   \n446 arXiv:1904.01038, 2019.   \n447 [33] Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi,   \n448 Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. The LAMBADA   \n449 dataset: Word prediction requiring a broad discourse context. In Katrin Erk and Noah A. Smith,   \n450 editors, Proceedings of the 54th Annual Meeting of the Association for Computational Linguis  \n451 tics (Volume 1: Long Papers), pages 1525\u20131534, Berlin, Germany, August 2016. Association   \n452 for Computational Linguistics.   \n453 [34] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan   \n454 Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference.   \n455 Proceedings of Machine Learning and Systems, 5, 2023.   \n456 [35] Giovanni Puccetti, Anna Rogers, Aleksandr Drozd, and Felice Dell\u2019Orletta. Outlier dimensions   \n457 that disrupt transformers are driven by frequency. In Yoav Goldberg, Zornitsa Kozareva, and Yue   \n458 Zhang, editors, Findings of the Association for Computational Linguistics: EMNLP 2022, pages   \n459 1286\u20131304, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational   \n460 Linguistics.   \n461 [36] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.   \n462 Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n463 [37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,   \n464 Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified   \n465 text-to-text transformer. Journal of machine learning research, 21(140):1\u201367, 2020.   \n466 [38] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An   \n467 adversarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641, 2019.   \n468 [39] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng   \n469 Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantiza  \n470 tion for large language models. arXiv preprint arXiv:2308.13137, 2023.   \n471 [40] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.   \n472 [41] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:   \n473 Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.   \n474 [42] Mingjie Sun, Xinlei Chen, J Zico Kolter, and Zhuang Liu. Massive activations in large language   \n475 models. arXiv preprint arXiv:2402.17762, 2024.   \n476 [43] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya   \n477 Pathak, Laurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open   \n478 models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.   \n479 [44] MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially   \n480 usable llms, 2023. Accessed: 2023-05-05.   \n481 [45] William Timkey and Marten van Schijndel. All bark and no bite: Rogue dimensions in trans  \n482 former language models obscure representational quality. In Marie-Francine Moens, Xuanjing   \n483 Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on   \n484 Empirical Methods in Natural Language Processing, pages 4527\u20134546, Online and Punta Cana,   \n485 Dominican Republic, November 2021. Association for Computational Linguistics.   \n486 [46] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo  \n487 th\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open   \n488 and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n489 [47] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,   \n490 Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open   \n491 foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n492 [48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,   \n493 \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information   \n494 processing systems, 30, 2017.   \n495 [49] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani   \n496 Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large   \n497 language models. arXiv preprint arXiv:2206.07682, 2022.   \n498 [50] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo,   \n499 and Xianglong Liu. Outlier suppression+: Accurate quantization of large language models   \n500 by equivalent and effective shifting and scaling. In Houda Bouamor, Juan Pino, and Kalika   \n501 Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language   \n502 Processing, pages 1648\u20131665, Singapore, December 2023. Association for Computational   \n503 Linguistics.   \n504 [51] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.   \n505 SmoothQuant: Accurate and efficient post-training quantization for large language models. In   \n506 Proceedings of the 40th International Conference on Machine Learning, 2023.   \n507 [52] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming   \n508 language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.   \n509 [53] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,   \n510 Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture.   \n511 In International Conference on Machine Learning, pages 10524\u201310533. PMLR, 2020.   \n512 [54] Zhewei Yao, Cheng Li, Xiaoxia Wu, Stephen Youn, and Yuxiong He. A comprehensive study   \n513 on post-training quantization for large language models. arXiv preprint arXiv:2303.08302,   \n514 2023.   \n515 [55] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong   \n516 He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers.   \n517 Advances in Neural Information Processing Systems, 35:27168\u201327183, 2022.   \n518 [56] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a   \n519 machine really finish your sentence? In Anna Korhonen, David Traum, and Llu\u00eds M\u00e0rquez, edi  \n520 tors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,   \n521 pages 4791\u20134800, Florence, Italy, July 2019. Association for Computational Linguistics.   \n522 [57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,   \n523 Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained   \n524 transformer language models. arXiv preprint arXiv:2205.01068, 2022.   \n525 [58] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,   \n526 Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv   \n527 preprint arXiv:2303.18223, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "528 A Additional Calibration Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "529 In this section, we provide details of LLMs when performing calibration, which is the step during   \n530 quantization where the FP16 ranges are computed (Appendix A.1), and additional calibration results   \n531 (Appendix A.2, A.3). ", "page_idx": 13}, {"type": "text", "text": "532 A.1 Detailed Specification of LLMs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "533 In Section 3.1, we have performed the calibration method on various LLMs. We observe the   \n534 calibration results by categorizing based on the presence of GLU in the LLMs. Table 6 shows the   \n535 detailed structures of the LLMs. We refer notations for feed-forward implementiation from [40]. In   \n536 the case of GLU-implemented LLMs, which is LLaMA-2, LLaMA-3, Mistral, Mixtral, SOLAR,   \n537 StableLM-2, and Gemma, most models have SwiGLU for FFN activation, while only Gemma has   \n538 GeGLU. On the other hand, in non GLU-implemented LLMs, most of them utilize GeLU for FFN   \n539 activation, with the exception of OPT, which uses ReLU. ", "page_idx": 13}, {"type": "text", "text": "Table 6: Architecture specification of LLMs. We categorize them into two groups depending on whether GLU is implemented in the FFN. All LLMs in the table use Pre-LN for the LayerNorm position. ", "page_idx": 13}, {"type": "table", "img_path": "t8ch1OCvHh/tmp/c3e340ef937f3198389d7aa582ce32f91e84601999db9638d9cca0d87e9b9970.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "540 A.2 Other Calibration Results on GLU-implementation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "541 Figure 8, 9 show the calibration result examples for various GLU-implemented LLMs that are not   \n542 shown in the models in Figure 1a. In most GLU-implemented LLMs, we observe that the input   \n543 activations have large values near the first and last layers. Unlike the typical GLU-implemented LLM   \n544 architecture, Mixtral is composed of 8 feed-forward blocks in the single FFN, containing multiple   \n545 gate linear units [21]. According to this structure, we can observe that one of the gates spikes in value   \n546 in Figure 8. ", "page_idx": 13}, {"type": "image", "img_path": "t8ch1OCvHh/tmp/390c66e2f060fb69fc55b008bdcd2ae381013ebd795e019687a2116722b8afe1.jpg", "img_caption": ["Figure 8: Calibration results on GLU-implemented LLMs (Mixtral- $\\mathbf{8x7B}$ ). "], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "t8ch1OCvHh/tmp/c4647af225dd7c253398a645bc896bc31f67945ef8b255cc29139b5beb7e298b.jpg", "img_caption": ["Figure 9: Calibration results on GLU-implemented LLMs. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "t8ch1OCvHh/tmp/5cd28b98c3fd7e1cdfd49a31610a9fb26b9cc01a16b95ecd0ce870bdc205baf8.jpg", "img_caption": ["Figure 10: Calibration results on Non GLU-implemented LLMs. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "547 A.3 Other Calibration Results on Non GLU-implementation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "548 Figure 10 shows the calibration result examples for various non GLU-implemented LLMs that were   \n549 not shown in the models in Figure 1b. There are no activation spikes on non GLU-implemented   \n550 LLMs. ", "page_idx": 15}, {"type": "text", "text": "551 B BMM Quantization ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "552 To achieve faster inference latency, BMM operations in the self-attention also can be computed as   \n553 INT8 operation [51]. This requires a quantization on the query, key, and value states including the   \n554 cached context. Because activation spikes produce a large magnitude of latent values, it is important   \n555 to confirm the extent of quantization errors from KV quantization. This confirmation is necessary to   \n556 gain advantages from BMM quantization. In Table 7, we examine the impact of BMM quantization on   \n557 the W8A8 and QFeM. Regardless of the BMM quantization, the QFeM method consistently improves   \n558 the quantization bottleneck. For example, the 13B and 70B models maintain their performance,   \n559 while the 7B model shows a slight decrease. However, this decrease appears to be due to inherent   \n560 quantization errors rather than a quantization bottleneck from activation spikes. As a result, we   \n561 confirm that our QFeM method effectively improves the overall performance even in the BMM   \n562 quantization scenario. ", "page_idx": 15}, {"type": "table", "img_path": "t8ch1OCvHh/tmp/84d73d2b2a4295c7b2b0cf73aece7efb6896a768e424c4decbac663cd19a6a66.jpg", "table_caption": ["Table 7: BMM quantization results. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "563 C Supplementary Experiment Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "564 C.1 Additional Results for Combining Outlier Alleviation Methods ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "565 In Table 8, we provide additional results for Section 5.3 with coarse-grained quantization (i.e.,   \n566 per-tensor quantization) scheme for weight quantization. Compared to the results obtained with per  \n567 channel weight quantization in Table 4, these results elucidate the negative impact of activation spikes   \n568 on the performance of outlier alleviation methods. Furthermore, this suggests that the performance of   \n569 OSP method resort to the weight quantization scheme. Nevertheless, the proposed methods, QFeM   \n570 and QFeP, consistently improve the effectiveness of outlier alleviation methods by mitigating the   \n571 impact of activation spikes. ", "page_idx": 15}, {"type": "text", "text": "Table 8: Evaluation of outlier alleviation methods with QFeM and QFeP. We report perplexity on WikiText-2 and averaged accuracy of four zero-shot tasks. Compared to Table 4, per-tensor weight quantization and dynamic per-tensor activation quantization are used. ", "page_idx": 15}, {"type": "table", "img_path": "t8ch1OCvHh/tmp/bd7156fbd90b97f8874df757953ca99c3e7f73d0295f82bf67bf2b523a7d8217.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "572 D Miscellaneous ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "573 D.1 Transformer Architecture. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "574 In Figure 11, we illustrate the Pre-LN transformer architecture and each sub-modules. We highlight   \n575 with the same color the linear modules that accept identical input activations. Note that the hidden   \n576 states are normalized before forwarding into the query and up linear modules. ", "page_idx": 16}, {"type": "image", "img_path": "t8ch1OCvHh/tmp/193c10f8fc40dcef1cf6c35a928db0052b692fd2a1241e5d92e7b8acd8e9b577.jpg", "img_caption": ["Figure 11: An illustration of Pre-LN transformer block and its sub-modules. Two feed-forward implementation, GLU and Non-GLU, are visualized in (c) and (d) respectively. In feed-forward network, $\\sigma$ denotes non-linear activation function, such as GeLU. We highlight the linear modules where input activations are quantized. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "577 D.2 Additional Results for Token-level Scale Analysis ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "578 We provide additional results for token-level scale analysis (Section 3.2). In Figure 12 and Figure 13,   \n579 the token for the activation spikes behind the BOS token does not exhibit the excessive activation   \n580 scale. ", "page_idx": 16}, {"type": "image", "img_path": "t8ch1OCvHh/tmp/89be7ec625514b40b7486e6586bd45f3784cf6b025feca1a3dbfb5e6ad64cd82.jpg", "img_caption": ["Figure 12: Token-wise scales analysis for LLaMA-2-7B. The newline token behind the BOS token does not exhibit the activation spikes. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "t8ch1OCvHh/tmp/5a3859c5a22eee7765254995f37ab34ff702cc47594fad9bbe60dee426698154.jpg", "img_caption": ["Figure 13: Token-wise scales from the unrolled activation spike of LLaMA-2-70B. The newline token behind the BOS token does not exhibit the activation spikes. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "581 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "3 Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Justification: We clarify our research scope and contributions in abstract and introduction. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "597 2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [No] ", "page_idx": 17}, {"type": "text", "text": "Justification: The limitation of our work is that our methods are based on the observations without theoretical validation. However, our extensive experimental results validate the effectiveness of our methods. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "30 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA]   \nJustification: We propose empirical methods based on our observation, rather than theoretical analysis.   \n\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "647 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We precisely describe the process of the proposed methods in their respective subsections. The models (LLMs) and datasets used in the experiments are publicly accessible. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "687 5. Open access to data and code   \n688 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n689 tions to faithfully reproduce the main experimental results, as described in supplemental   \n690 material?   \n691 Answer: [Yes]   \n692 Justification: We provide accessible URL in the abstract.   \n693 Guidelines:   \n694 \u2022 The answer NA means that paper does not include experiments requiring code.   \n695 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n696 public/guides/CodeSubmissionPolicy) for more details.   \n697 \u2022 While we encourage the release of code and data, we understand that this might not be   \n698 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n699 including code, unless this is central to the contribution (e.g., for a new open-source   \n700 benchmark).   \n701 \u2022 The instructions should contain the exact command and environment needed to run to   \n702 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n703 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n704 \u2022 The authors should provide instructions on data access and preparation, including how   \n705 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n706 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n707 proposed method and baselines. If only a subset of experiments are reproducible, they   \n708 should state which ones are omitted from the script and why.   \n709 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n710 versions (if applicable).   \n711 \u2022 Providing as much information as possible in supplemental material (appended to the   \n712 paper) is recommended, but including URLs to data and code is permitted.   \n713 6. Experimental Setting/Details   \n714 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n715 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n716 results?   \n717 Answer: [Yes]   \n718 Justification: We provide the hyperparameter settings in Table 2.   \n719 Guidelines:   \n720 \u2022 The answer NA means that the paper does not include experiments.   \n721 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n722 that is necessary to appreciate the results and make sense of them.   \n723 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n724 material.   \n725 7. Experiment Statistical Significance   \n726 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n727 information about the statistical significance of the experiments?   \n728 Answer: [No]   \n729 Justification: The proposed methods rely on the sample size of the calibration dataset.   \n730 Nevertheless, we are convinced that the sample size used in the experiments is sufficient for   \n731 achieving reliable and consistent calibration results.   \n732 Guidelines:   \n733 \u2022 The answer NA means that the paper does not include experiments.   \n734 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n735 dence intervals, or statistical significance tests, at least for the experiments that support   \n736 the main claims of the paper.   \n737 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n738 example, train/test split, initialization, random drawing of some parameter, or overall   \n739 run with given experimental conditions).   \n740 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n741 call to a library function, bootstrap, etc.)   \n742 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n743 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n744 of the mean.   \n745 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n746 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n747 of Normality of errors is not verified.   \n748 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n749 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n750 error rates).   \n751 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n752 they were calculated and reference the corresponding figures or tables in the text.   \n753 8. Experiments Compute Resources   \n754 Question: For each experiment, does the paper provide sufficient information on the com  \n755 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n756 the experiments?   \n757 Answer: [Yes]   \n758 Justification: We provide a computational cost analysis in Section 5.5.   \n759 Guidelines:   \n760 \u2022 The answer NA means that the paper does not include experiments.   \n761 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n762 or cloud provider, including relevant memory and storage.   \n763 \u2022 The paper should provide the amount of compute required for each of the individual   \n764 experimental runs as well as estimate the total compute.   \n765 \u2022 The paper should disclose whether the full research project required more compute   \n766 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n767 didn\u2019t make it into the paper).   \n768 9. Code Of Ethics   \n769 Question: Does the research conducted in the paper conform, in every respect, with the   \n770 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n771 Answer: [Yes]   \n772 Justification: We have reviewed the code of ethics.   \n773 Guidelines:   \n774 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n775 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n776 deviation from the Code of Ethics.   \n777 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n778 eration due to laws or regulations in their jurisdiction).   \n779 10. Broader Impacts   \n780 Question: Does the paper discuss both potential positive societal impacts and negative   \n781 societal impacts of the work performed?   \n782 Answer: [NA]   \n783 Justification:   \n784 Guidelines:   \n785 \u2022 The answer NA means that there is no societal impact of the work performed.   \n786 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n787 impact or why the paper does not address societal impact.   \n788 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n789 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n790 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n791 groups), privacy considerations, and security considerations.   \n792 \u2022 The conference expects that many papers will be foundational research and not tied   \n793 to particular applications, let alone deployments. However, if there is a direct path to   \n794 any negative applications, the authors should point it out. For example, it is legitimate   \n795 to point out that an improvement in the quality of generative models could be used to   \n796 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n797 that a generic algorithm for optimizing neural networks could enable people to train   \n798 models that generate Deepfakes faster.   \n799 \u2022 The authors should consider possible harms that could arise when the technology is   \n800 being used as intended and functioning correctly, harms that could arise when the   \n801 technology is being used as intended but gives incorrect results, and harms following   \n802 from (intentional or unintentional) misuse of the technology.   \n803 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n804 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n805 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n806 feedback over time, improving the efficiency and accessibility of ML).   \n807 11. Safeguards   \n808 Question: Does the paper describe safeguards that have been put in place for responsible   \n809 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n810 image generators, or scraped datasets)?   \n811 Answer: [NA]   \n812 Justification:   \n813 Guidelines:   \n814 \u2022 The answer NA means that the paper poses no such risks.   \n815 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n816 necessary safeguards to allow for controlled use of the model, for example by requiring   \n817 that users adhere to usage guidelines or restrictions to access the model or implementing   \n818 safety filters.   \n819 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n820 should describe how they avoided releasing unsafe images.   \n821 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n822 not require this, but we encourage authors to take this into account and make a best   \n823 faith effort.   \n824 12. Licenses for existing assets   \n825 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n826 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n827 properly respected?   \n828 Answer: [Yes]   \n829 Justification: We cite the models and dataset used in the experiments (see Section 5.1).   \n830 Guidelines:   \n831 \u2022 The answer NA means that the paper does not use existing assets.   \n832 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n833 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n834 URL.   \n835 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n836 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n837 service of that source should be provided.   \n838 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n839 package should be provided. For popular datasets, paperswithcode.com/datasets   \n840 has curated licenses for some datasets. Their licensing guide can help determine the   \n841 license of a dataset. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. \u2022 If this information is not available online, the authors are encouraged to reach out to ", "page_idx": 22}, {"type": "text", "text": "845 the asset\u2019s creators.   \n846 13. New Assets   \n847 Question: Are new assets introduced in the paper well documented and is the documentation   \n848 provided alongside the assets?   \n49 Answer: [NA]   \n850 Justification:   \n851 Guidelines:   \n852 \u2022 The answer NA means that the paper does not release new assets.   \n53 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n854 submissions via structured templates. This includes details about training, license,   \n55 limitations, etc.   \n856 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n857 asset is used.   \n858 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n859 create an anonymized URL or include an anonymized zip file.   \n860 14. Crowdsourcing and Research with Human Subjects   \n861 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n862 include the full text of instructions given to participants and screenshots, if applicable, as   \n863 well as details about compensation (if any)?   \n864 Answer: [NA]   \n865 Justification:   \n866 Guidelines:   \n867 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n868 human subjects.   \n869 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n870 tion of the paper involves human subjects, then as much detail as possible should be   \n871 included in the main paper.   \n872 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n873 or other labor should be paid at least the minimum wage in the country of the data   \n874 collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]