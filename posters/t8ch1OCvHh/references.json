{"references": [{"fullname_first_author": "Arash Ahmadian", "paper_title": "Intriguing properties of quantization at scale", "publication_date": "2023-12-01", "reason": "This paper reveals the challenges of activation quantization in LLMs, which is the core problem addressed in the target paper."}, {"fullname_first_author": "Joshua Ainslie", "paper_title": "GQA: Training generalized multi-query transformer models from multi-head checkpoints", "publication_date": "2023-05-16", "reason": "This paper introduces GQA, an architectural improvement in LLMs that is relevant to the target paper's discussion of modern LLM architectures."}, {"fullname_first_author": "Alexei Baevski", "paper_title": "Adaptive input representations for neural language modeling", "publication_date": "2018-01-01", "reason": "This paper discusses adaptive input representations, which are related to the target paper's focus on activation quantization in LLMs."}, {"fullname_first_author": "Stella Biderman", "paper_title": "Pythia: A suite for analyzing large language models across training and scaling", "publication_date": "2023-01-01", "reason": "This paper provides a framework for analyzing LLMs, which is relevant to the target paper's experimental analysis of various LLMs."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-01-01", "reason": "This paper establishes the foundation of LLMs and their capabilities as few-shot learners, providing context for the target paper's work on LLM quantization."}]}