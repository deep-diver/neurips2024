[{"type": "text", "text": "On Theoretical Limits of Learning with Label Differential Privacy ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Label differential privacy (DP) is designed for learning problems with private labels   \n2 and public features. Although various methods have been proposed for learning   \n3 under label DP, the theoretical limits remain unknown. The main challenge is to   \n4 take infimum over all possible learners with arbitrary model complexity. In this   \n5 paper, we investigate the fundamental limits of learning with label DP under both   \n6 central and local models. To overcome the challenge above, we derive new lower   \n7 bounds on testing errors that are adaptive to the model complexity. Our analyses   \n8 indicate that $\\epsilon$ -local label DP only enlarges the sample complexity with respect to   \n9 \u03f5, without affecting the convergence rate over the sample size $N$ , except the case   \n10 with heavy-tailed label. Under the central model, the performance loss due to the   \n11 privacy mechanism is further weakened, such that the additional sample complexity   \n12 becomes negligible. Overall, our analysis validates the promise of learning under   \n13 the label DP from a theoretical perspective and shows that the learning performance   \n4 can be significantly improved by weakening the DP definition to only labels. ", "page_idx": 0}, {"type": "text", "text": "15 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "16 Many modern machine learning tasks require sensitive training samples that need to be protected   \n17 from leakage [1]. As a standard approach for privacy protection, differential privacy (DP) [2] has   \n18 been extensively studied [3\u20139]. However, the learning performances under original DP definition   \n19 are usually far from satisfactory [10\u201313]. Therefore, researchers attempt to design weakened DP   \n20 requirements, under which the performances can be significantly improved, while still securing   \n2 sensitive information. Under such background, label DP has emerged in recent years [14], which   \n22 regards features as public, while only labels are sensitive and need to be protected. Such setting is   \n23 realistic in many applications, such as computational advertising [15], recommendation systems [16]   \n24 and medical diagnosis [17]. These tasks usually use some basic demographic information as features,   \n25 which can be far less sensitive.   \n26 Despite various approaches for learning with label DP [14, 18\u201321], the fundamental limits are   \n27 still unknown. An interesting question is: By weakening the DP definitions to only labels, how   \n28 much accuracy improvement is possible? From an information-theoretic perspective [22], the   \n29 underlying limits of statistical problems are characterized by the minimax lower bound, which takes   \n30 the supremum over all possible distributions from a general class, and infimum over all learners.   \n31 Deriving minimax lower bounds for learning under the label DP is challenging in two aspects. Firstly,   \n32 under label DP, each sample has both public (i.e. the feature) and private (i.e. the label) components.   \n33 Directly applying the methods for original DP [23\u201327] treats all components as private, and thus does   \n34 not yield tight results. Secondly, the classical packing method [47] is only suitable for fixed model   \n35 structures with fixed dimensionality. However, to establish lower bounds, one needs to take infimum   \n36 over all possible learners with arbitrary model complexity.   \n37 In this paper, we investigate the theoretical limits of classification and regression problems under label   \n38 DP. Our analysis involves both central and local models. For each problem, we derive the information  \n39 theoretic minimax lower bound of the risk function over a wide class of distributions satisfying the   \n40 $\\beta$ -H\u00f6lder smoothness and the $\\gamma.$ -Tsybakov margin assumption [28] (see Assumption 1 for details).   \n41 The general idea is to convert the problem to multiple hypothesis testing. To overcome the challenges   \n42 above, we provide a bound of Kullback-Leibler divergence over joint distributions of private and   \n43 public random variables, which is tighter than the bound between fully private variables. Moreover,   \n44 under the central model, instead of using the packing method, we develop a new lower bound on the   \n45 minimum testing error for each pair of hypotheses based on the group privacy property [4], which   \n46 is suitable for arbitrary model complexity. After deriving minimax lower bounds, we also propose   \n47 algorithms with matching upper bounds to validate the tightness of our results.   \n48 The results are shown in Table 1, in which the third row refers to the bounds under the original local   \n49 DP definition, while the fourth row lists the non-private baselines. To the best of our knowledge,   \n50 minimax rates under central DP have not been established, and are thus not listed here. The main   \n51 findings are summarized as follows.   \n52 \u2022 Under $\\epsilon$ -local label DP, for classification and regression with bounded label noise, the   \n53 sample complexity is larger by a factor of ${\\cal O}(1/\\epsilon^{2})$ . However, the convergence rate remains   \n54 unaffected, which is in clear contrast with the original DP, under which the convergence rate   \n55 is slower.   \n56 \u2022 Under $\\epsilon$ -local label DP constraint, for regression with heavy-tailed label noise, the conver  \n57 gence rate of risk over $N$ becomes slower, indicating that heavy-tailed labels increase the   \n58 difficulty of privacy protection.   \n59 \u2022 Under $\\epsilon$ -central label DP constraint, the performance loss caused by the privacy mechanism   \n60 becomes further weakened. The risk only increases by a term that decays faster than the   \n61 non-private rate, indicating that the additional sample complexity caused by the privacy   \n62 mechanism becomes negligible with large $N$ .   \n63 In general, our analysis provides a theoretical perspective of understanding label DP. The result   \n64 shows that by weakening the DP definition to protecting labels only, the learning performances can   \n65 be significantly improved. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "table", "img_path": "oQc7TOCk5G/tmp/97b3f4cb07ba0f3cc71f94991893f05401ee9c83f43d120997f6badf779085d9.jpg", "table_caption": [], "table_footnote": ["Table 1: Minimax rate of convergence under label differential privacy. $d$ is the dimension of features. "], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "66 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "67 Label DP. Under the local model, labels are randomized before training. The simplest method is   \n68 randomized response [30]. An important improvement is proposed in [14], called RRWithPrior,   \n69 which incorporates prior distribution. [19] proposes ALIBI, which further improves randomized   \n70 response by generating soft labels through Bayesian inference. There are also several methods for   \n71 regression under label DP [18,31]. Under central label DP, [20] proposes a clustering approach. [19]   \n72 proposes private aggregation of teacher ensembles (PATE), which is then further improved in [21].   \n73 Minimax analysis for public data. Minimax theory provides a rigorous framework for the best   \n74 possible performance of an algorithm given some assumptions. Classical methods include Le   \n75 Cam [32], Fano [33] and Assouad [34]. Using these methods, minimax lower bounds have been   \n76 widely established for both classification and regression problems [28, 29, 35\u201341]. If the feature   \n77 vector has bounded support, then the minimax rate of classification and regression are $O(N^{-\\frac{\\beta(\\gamma+1)}{2\\beta+d}})$   \n78 and $O(N^{-\\frac{2\\beta}{2\\beta+d}})$ , respectively.   \n79 Minimax analysis for private data. Under the local model, [42] finds the relation between label DP   \n80 and stochastic query. [23] and [24] develop the variants of Le Cam, Fano, and Assouad\u2019s method   \n81 under local DP. Lower bounds are then established for various statistical problems, such as mean   \n82 estimation [43\u201346], classification [26] and regression [27]. Under central model, for pure DP, the   \n83 standard approach is the packing method [47], which is then used in hypothesis testing [48], mean   \n84 estimation [49,50], and learning of distributions [51\u201353]. There are also several works on approximate   \n85 DP, such as [54,55].   \n86 This work studies the theoretical limits of label DP, under which each sample is a mixture of public   \n87 feature and private labels, thus existing methods can not be directly applied here. Under the central   \n88 model, the minimax analysis becomes more challenging, since the packing method is only suitable   \n89 for fixed model structures (i.e. the dimensionality of model output is fixed), while we need to find the   \n90 minimum possible error over all possible learners with arbitrary output dimensions. As a result, the   \n91 lower bounds of general classification and regression problems have not been established even under   \n92 the original DP definition. To overcome such challenge, we develop a new approach to bound the   \n93 error of hypothesis testing (see Lemma 1 in Appendix D). ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "94 3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "95 In this section, we show some necessary definitions, background information, and notations. ", "page_idx": 2}, {"type": "text", "text": "96 3.1 Label DP ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "97 To begin with, we review the definition of DP. Suppose the dataset consists of $N$ samples $\\left({{\\bf{x}}_{i}},{y_{i}}\\right)$ ,   \n98 $i=1,\\ldots,N$ , in which $\\mathbf{x}_{i}\\in\\mathcal{X}$ is the feature vector, while $y_{i}\\in\\mathcal{D}\\subset\\mathbb{R}^{d}$ is the label.   \n99 Definition 1. (Differential Privacy $\\left(D P\\right)\\left(2J\\right)L e t\\left\\epsilon\\geq0.$ A randomized function $A:(\\mathcal{X},\\mathcal{Y})^{N}\\to\\Theta$   \n100 is $\\epsilon{-}D P$ if for any two adjacent datasets $D,D^{\\prime}\\in(\\mathcal{X},\\mathcal{Y})^{N}$ and any $S\\subseteq\\Theta$ , ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nP(A(D)\\in S)\\leq e^{\\epsilon}P(A(D^{\\prime})\\in S),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "101 in which $D$ and $D^{\\prime}$ are adjacent if they differ only on a single sample, including both the feature   \n102 vector and the label.   \n103 In machine learning tasks, the output of $\\boldsymbol{\\mathcal{A}}$ is the model parameters, while the input is the training   \n104 dataset. Definition 1 requires that both features and labels are privatized. Consider that in some   \n105 applications, the features may be much less sensitive, the notion of label DP is defined as follows.   \n106 Definition 2. (Central label $D P$ ) A randomized function $\\boldsymbol{\\mathcal{A}}$ is $\\epsilon$ -label DP if for any two datasets $D$   \n107 and $D^{\\prime}$ that differ on the label of only one training sample and any $S\\subseteq\\Theta$ , (1) holds.   \n108 Compared with Definition 1, Definition 2 only requires the output to be insensitive to the replacement   \n109 of a label. Therefore label DP is a weaker requirement. Correspondingly, the local label DP is defined   \n110 as follows. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "111 Definition 3. (Local label DP) A randomized function $M:(\\mathcal{X},\\mathcal{Y})\\rightarrow\\mathcal{Z}$ is $\\epsilon$ -local label DP if ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{y,y^{\\prime}\\in\\mathcal{Y}S\\subseteq\\mathcal{Z}}\\ln\\frac{P(M(\\mathbf{x},y)\\in S)}{P(M(\\mathbf{x},y^{\\prime})\\in S)}\\le\\epsilon.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "112 Definition 3 requires that each label is privatized locally before running any machine learning   \n113 algorithms. It is straightforward to show that local label DP ensures central label DP. To be more   \n114 precise, we have the following proposition.   \n115 Proposition 1. Let $\\mathbf{z}_{i}=M(\\mathbf{x}_{i},y_{i})$ for $i=1,\\ldots,N$ . If $\\boldsymbol{\\mathcal{A}}$ is a function of $(\\mathbf{x}_{i},\\mathbf{z}_{i}),\\,i=1,\\ldots,N,$ ,   \n116 then $\\boldsymbol{\\mathcal{A}}$ is $\\epsilon$ -label $D P.$ ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "117 3.2 Risk of Classification and Regression ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "118 In supervised learning problems, given $N$ samples $(\\mathbf{X}_{i},Y_{i})$ , $i=1,\\ldots,N$ drawn from a common   \n119 distribution, the task is to learn a function $g:\\mathcal{X}\\to\\mathcal{Y}$ . For a loss function $l:\\mathcal{Y}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}$ , the goal   \n120 is to minimize the risk function, which is defined as the expectation of loss function between the   \n121 predicted value and the ground truth: ", "page_idx": 2}, {"type": "equation", "text": "$$\nR=\\mathbb{E}[l(\\hat{Y},Y)].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "122 The minimum risk among all function $g$ is called Bayes risk, i.e. $\\begin{array}{r}{R^{*}=\\operatorname*{min}_{g}\\mathbb{E}[l(g(\\mathbf{X},Y))]}\\end{array}$ . In   \n123 practice, the sample distribution is unknown, and we need to learn $g$ from samples. Therefore, the   \n124 risk of any practical classifiers is larger than Bayes risk. The gap $R-R^{*}$ is called excess risk, and we   \n125 hope that $R-R^{*}$ to be as small as possible. Now we discuss classification and regression problems   \n126 separately.   \n127 1) Classification. For classification problems, the size of $\\boldsymbol{\\wp}$ is finite. For convenience, we denote   \n128 $\\mathcal{V}=[K]$ , in which $[K]:=\\{1,\\ldots,K\\}$ . In this paper, we use $0-1$ loss, i.e. $l(\\hat{Y},Y)={\\bf1}(\\hat{Y}\\neq Y)$ ,   \n129 then $R={\\mathsf{P}}({\\hat{Y}}\\neq Y)$ . Define $K$ functions $\\eta_{1},\\dots,\\eta_{K}$ as the conditional class probabilities: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\eta_{k}(\\mathbf{x})=\\mathbf{P}(Y=k|\\mathbf{X}=\\mathbf{x}),k=1,\\ldots,K.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "130 Under this setting, the Bayes optimal classifier and the corresponding Bayes risk is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{c^{*}({\\bf x})}&{=}&{\\arg\\operatorname*{max}\\eta_{j}({\\bf x}),}\\\\ &{}&{j\\in[K]\\quad}\\\\ {R_{c l s}^{*}}&{=}&{{\\mathrm{P}}(c^{*}({\\bf X})\\ne Y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "131 2) Regression. Now we consider the case with $\\boldsymbol{\\wp}$ having infinite size. We use $\\ell_{2}$ loss in this paper, i.e.   \n132 $l(\\hat{Y},Y)=(\\hat{Y}-Y)^{2}$ . Then the Bayes risk is ", "page_idx": 3}, {"type": "equation", "text": "$$\nR_{r e g}^{*}=\\mathbb{E}[(Y-\\eta(\\mathbf{X}))^{2}].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "133 Then the following proposition gives a bound of the excess risk for classification and regression   \n134 problems. ", "page_idx": 3}, {"type": "text", "text": "135 Proposition 2. For any classifier $c:\\mathcal{X}\\to[K]$ , the excess risk of classification is bounded by ", "page_idx": 3}, {"type": "equation", "text": "$$\nR_{c l s}-R_{c l s}^{*}=\\int(\\eta^{*}(\\mathbf{x})-\\mathbb{E}[\\eta_{c(\\mathbf{x})}(\\mathbf{x})])f(\\mathbf{x})d\\mathbf{x}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "136 For any regression estimate $\\hat{\\eta}:\\mathcal{X}\\to\\mathcal{Y}_{:}$ , the excess risk of regression is bounded by ", "page_idx": 3}, {"type": "equation", "text": "$$\nR_{r e g}-R_{r e g}^{*}=\\mathbb{E}[(\\hat{\\eta}(\\mathbf{X})-\\eta(\\mathbf{X}))^{2}].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "137 The proof of Proposition 2 is shown in Appendix A. Finally, we state some basic assumptions that   \n138 will be used throughout this paper. ", "page_idx": 3}, {"type": "text", "text": "139 Assumption 1. There exists some constants $L,\\,\\beta,\\,C_{T},\\,\\gamma,\\,c,\\,D$ and $\\theta\\in(0,1]$ such that ", "page_idx": 3}, {"type": "text", "text": "141 $(b)$ For any $t>0$ , $P\\left(0<\\eta^{\\ast}(\\mathbf{X})-\\eta_{s}(\\mathbf{X})<t\\right)\\leq C_{T}t^{\\gamma}$ , in which $\\eta_{s}(\\mathbf{x})$ is the second largest one   \n142 among $\\{\\eta_{1}({\\bf x}),\\dots,\\eta_{K}({\\bf x})\\}$ ;   \n143 (c) The feature vector $\\mathbf{X}$ has a probability density function (pdf) $f$ which is bounded from below, i.e.   \n144 f(x) \u2265c;   \n145 $(d)$ For all $r<D$ , $V_{r}(\\mathbf{x})\\geq\\theta v_{d}r^{d}$ , in which $V_{r}(\\mathbf{x})$ is the volume (Lebesgue measure) of $B(\\mathbf{x},r)\\cap\\mathcal{X}$ ,   \n146 $v_{d}$ is the volume of a unit ball.   \n147 Assumption 1 (a) requires that all $\\eta_{j}$ are H\u00f6lder continuous. This condition is common in literatures   \n148 about nonparametric statistics [28]. (b) is generalized from the Tsybakov noise assumption for binary   \n149 classification, which is commonly used in many existing works in the field of both nonparametric   \n150 classification [29,37,40,41] and differential privacy [26,27]. If $K=2$ , then $\\eta^{*}$ and $\\eta_{s}$ refer to the   \n151 larger and smaller class conditional probability, respectively. An intuitive understanding of (b) is that   \n152 in the majority of the support, the maximum value among $\\{\\eta_{1}({\\bf x}),\\dots,\\eta_{K}({\\bf x})\\}$ should have some   \n153 gap to the second largest one. With sufficiently large sample size and model complexity, assumption   \n154 (b) ensures that for test samples within the majority of the support $\\mathcal{X}$ , the algorithm is highly likely to   \n155 correctly identify the class with the maximum conditional probability. Therefore, in (b), we only care   \n156 about $\\eta^{*}(\\mathbf{x})$ and $\\eta_{s}(\\mathbf{x})$ , while other classes with small conditional probabilities can be ignored. (c)   \n157 is usually called \"strong density assumption\" in existing works [39,40], which is quite strong. It is   \n158 possible to relax this assumption so that the theoretical analysis becomes suitable for general cases.   \n159 However, we do not focus on such generalization in this paper. Assumption (d) prevents the corner of   \n160 the support $\\mathcal{X}$ from being too sharp. In the remainder of this section, denote $\\mathcal{F}_{c l s}$ as the set of all   \n161 pairs $(f,\\eta)$ satisfying Assumption 1. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "162 4 Classification ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "163 In this section, we derive the upper and lower bounds of learning under central and local label DP,   \n164 respectively. ", "page_idx": 4}, {"type": "text", "text": "165 4.1 Local Label DP ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "166 1) Lower bound. The following theorem shows the minimax lower bound, which characterizes the   \n167 theoretical limit.   \n168 Theorem 1. Denote $\\mathcal{M}_{\\epsilon}$ as the set of all privacy mechanisms satisfying $\\epsilon$ -local label DP (Definition   \n169 3). Then ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{Y}}\\operatorname*{inf}_{M\\in\\mathcal{M}_{\\epsilon}}\\operatorname*{sup}_{(f,\\eta)\\in\\mathcal{F}_{c l s}}(R_{c l s}-R_{c l s}^{*})\\gtrsim\\big[N\\left(\\epsilon^{2}\\wedge1\\right)\\big]^{-\\frac{\\beta(\\gamma+1)}{2\\beta+d}}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "170 Proof. (Outline) It suffices to derive (10) with $K=2$ . We convert the problem into multiple binary   \n171 hypothesis testing problems. In particular, we divide the support into $G$ bins. For some of them, we   \n172 construct two opposite hypotheses such that they are statistically not distinguishable. Our proof uses   \n173 some techniques in local DP [24] and some classical minimax theory [28]. The detailed proof is   \n174 shown in Appendix B. \u53e3   \n175 In Theorem 1, (10) takes supremum over all joint distributions of $(\\mathbf{X},Y)$ , and infimum over all   \n176 classifiers and privacy mechanisms satisfying $\\epsilon$ -local label DP.   \n177 2) Upper bound. We then show that the bound (10) is achievable. Let the privacy mechanism $M(\\mathbf{x},y)$   \n178 outputs a $K$ dimensional vector, with each component being either 0 or 1, such that ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{P}(M(\\mathbf{x},y)(j)=1)=\\left\\{\\begin{array}{l l}{\\frac{e^{\\frac{\\epsilon}{2}}}{e^{\\frac{\\epsilon}{2}}+1}}&{\\mathrm{if}\\quad y=j}\\\\ {\\frac{1}{e^{\\frac{\\epsilon}{2}}+1}}&{\\mathrm{if}\\quad y\\neq j,}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "179 and $\\operatorname{P}(M(\\mathbf{x},y)(j)=0)=1-\\operatorname{P}(M(\\mathbf{x},y)(j)=1)$ , in which $M(\\mathbf{x},y)(j)$ is the $j$ -th component of   \n180 $M(\\mathbf{x},y)$ . For $N$ random training samples $(\\mathbf{X}_{i},Y_{i})$ , let $\\mathbf{Z}_{i}=M(\\mathbf{X}_{i},Y_{i})$ , and correspondingly, $Z_{i}(j)$   \n181 is the $j$ -th component of $\\mathbf{Z}_{i}$ .   \n182 Divide the support $\\mathcal{X}$ into $G$ bins, named $B_{1},\\ldots,B_{G}$ , such that the length of each bin is $h$   \n183 $B_{1},\\ldots,B_{G}$ are disjoint, and these bins form a covering of $\\mathcal{X}$ , i.e. $\\mathcal{X}\\,\\subset\\,\\bar{\\cup}_{l=1}^{G}B_{l}$ . Then calcu  \n184 late ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nS_{l j}=\\sum_{i:\\mathbf{X}_{i}\\in B_{l}}Z_{i}(j),l=1,\\ldots,G,j=1,\\ldots,K,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "185 The classification within the $l$ -th bin is ", "page_idx": 4}, {"type": "equation", "text": "$$\nc_{l}=\\arg\\operatorname*{max}_{j}S_{l j},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "186 such that the the prediction given $\\mathbf{x}$ is $c(\\mathbf{x})=c_{l}$ for all $\\mathbf{x}\\in B_{l}$ . The next theorem shows the privacy   \n187 guarantee, as well as the bound of the excess risk.   \n188 Theorem 2. The privacy mechanism $M$ is $\\epsilon$ -local label DP. Moreover, under Assumption $^{\\,l}$ , with   \n189 $h\\sim\\left(N(\\epsilon^{2}\\wedge1)/\\ln K\\right)^{-\\frac{1}{2\\beta+d}}$ , the excess risk of the classifier described above can be upper bounded   \n190 as follows: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{c l s}-R_{c l s}^{*}\\lesssim\\left(\\frac{N(\\epsilon^{2}\\wedge1)}{\\ln K}\\right)^{-\\frac{\\beta(\\gamma+1)}{2\\beta+d}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "191 Proof. (Outline) For privacy guarantee, we need to show that (11) is $\\epsilon$ -local label DP: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l l}{\\frac{\\mathrm{P}(M(\\mathbf{x},y)=\\mathbf{z})}{\\mathrm{P}(M(\\mathbf{x},y^{\\prime})=\\mathbf{z})}}&{=}&{\\Pi_{j=1}^{K}\\frac{\\mathrm{P}(M(\\mathbf{x},y)(j)=\\mathbf{z}(j))}{\\mathrm{P}(M(\\mathbf{x},y^{\\prime})(j)=\\mathbf{z}(j))}}\\\\ &{=}&{\\frac{\\mathrm{P}(M(\\mathbf{x},y)(y)=\\mathbf{z}(y))}{\\mathrm{P}(M(\\mathbf{x},y^{\\prime})(y)=\\mathbf{z}(y))}\\frac{\\mathrm{P}(M(\\mathbf{x},y)(y^{\\prime})=\\mathbf{z}(y^{\\prime}))}{\\mathrm{P}(M(\\mathbf{x},y^{\\prime})(y^{\\prime})=\\mathbf{z}(y^{\\prime}))}}\\\\ &{\\leq}&{e^{\\frac{\\epsilon}{2}}e^{\\frac{\\epsilon}{2}}=e^{\\epsilon}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "192 According to Definition 3, $M$ is $\\epsilon$ -local label DP. For the performance guarantee (14), according to   \n193 Proposition 2, we need to bound $\\eta^{*}(\\mathbf{x})-\\mathbb{E}[\\eta_{c(\\mathbf{x})}(\\mathbf{x})]$ for each $\\mathbf{x}$ . If $\\eta^{*}(\\bar{\\mathbf{x})}-\\eta_{s}(\\mathbf{x})$ is large, then with   \n194 high probability, $c(\\mathbf{x})=c^{*}(\\mathbf{x})$ , and then $\\eta^{*}(\\mathbf{x})=\\eta_{c(\\mathbf{x})}(\\mathbf{x})$ . Thus we mainly consider the case with   \n195 small $\\eta^{*}(\\mathbf{x})-\\eta_{s}(\\mathbf{x})$ . The details of proof are shown in Appendix C. \u53e3   \n196 The lower bound (10) and the upper bound (14) match up to a logarithm factor, indicating that the   \n197 results are tight. Now we comment on the results.   \n198 Remark 1. 1) Comparison with non-private bound. The classical minimax lower bound for non  \n199 private classification problem is N \u2212 2\u03b2+d . Therefore, the lower bound (10) reaches the non-private   \n200 bound with $\\epsilon\\gtrsim1$ . With small $\\epsilon$ , $N$ training samples with privatized labels roughly equals $N\\epsilon^{2}$   \n201 non-privatized samples in terms of performance.   \n202 2) Comparison with local DP that protects both features and labels. In this case, the optimal   \n203 excess risk is $(N\\epsilon^{2})^{-\\beta(\\gamma+1)/(2\\beta+2d)}\\stackrel{\\star}{\\vee}N^{-\\beta(\\gamma+1)/(\\tilde{2}\\beta+d)}$ , which is worse than the right hand side of   \n204 (10). Such result indicates that compared with classical $D P,$ label DP incurs significantly weaker   \n205 performance loss.   \n206 3) Comparison with other baseline methods. If we use the randomized response method instead   \n207 of the privacy mechanism (11), then the performance decreases sharply with the number of classes   \n208 $K$ . Several methods have been proposed to improve the randomized response method, such as   \n209 RRWithPrior [14] and ALIBI [19]. However, these methods are not guaranteed in theory. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "210 4.2 Central Label DP ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "211 1) Lower bound. The following theorem shows the minimax lower bound under the central label DP.   \n212 Theorem 3. Denote $A_{\\epsilon}$ as the set of all learning algorithms satisfying $\\epsilon$ -label $D P$ (Definition 2).   \n213 Then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{A\\in\\mathcal{A}_{\\epsilon}}\\operatorname*{sup}_{(f,\\eta)\\in\\mathcal{F}_{c l s}}(R_{c l s}-R_{c l s}^{*})\\gtrsim N^{-\\frac{\\beta(\\gamma+1)}{2\\beta+d}}+(\\epsilon N)^{-\\frac{\\beta(\\gamma+1)}{\\beta+d}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "214 Proof. (Outline) Lower bounds under central DP are usually constructed by packing method [47],   \n215 which works for fixed output dimensions. However, to achieve a desirable bias and variance tradeoff,   \n216 the model complexity needs to increase with $N$ . In our proof, we still divide the support into $G$ bins   \n217 and construct two hypotheses for each bin, but we develop a new tool (see Lemma 1) to give a lower   \n218 bound of the minimum error of hypothesis testing. We then use the group privacy property [4] to get   \n219 the overall lower bound. The details can be found in Appendix D. \u53e3   \n220 2) Upper bound. Now we show that (16) is achievable. Similar to the local label DP problem, now   \n221 divide the support into $G$ bins, such that the length of each bin is $h$ . Now the classification within the   \n222 $l$ -th bin follows a exponential mechanism [56]: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{P}(c_{l}=j|\\mathbf{X}_{1:N},Y_{1:N})=\\frac{e^{\\epsilon n_{l j}/2}}{\\sum_{k=1}^{K}e^{\\epsilon n_{l k}/2}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "223 in which $\\begin{array}{r}{n_{l j}\\,=\\,\\sum_{i=1}^{N}\\mathbf{1}(\\mathbf{X}_{i}\\,\\in\\,B_{l},Y_{i}\\,=\\,j)}\\end{array}$ . Then let $c(\\mathbf{x})\\,=\\,c_{l}$ for $\\textbf{x}\\in\\~B_{l}$ . The excess risk is   \n224 bounded in the n ext theorem.   \n225 Theorem 4. The privacy mechanism (17) is $\\epsilon$ -label DP. Moreover, under Assumption $^{\\,l}$ , if h scales as   \n226 $h\\sim(\\ln K/\\epsilon N)^{\\frac{1}{\\beta+d}}+(\\ln K/N)^{\\frac{1}{2\\beta+d}}$ , then the excess risk can be bounded as follows: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nR-R^{*}\\lesssim\\left({\\frac{N}{\\ln K}}\\right)^{-{\\frac{\\beta(\\gamma+1)}{2\\beta+d}}}+\\left({\\frac{\\epsilon N}{\\ln K}}\\right)^{-{\\frac{\\beta(\\gamma+1)}{\\beta+d}}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "227 Proof. (Outline) The privacy guarantee of the exponential mechanism has been analyzed in [4].   \n228 Following these existing analyses, it can be shown that (17) is $\\epsilon$ -label DP. It remains to show (18).   \n229 Note that if $\\eta^{*}(\\mathbf{x})-\\eta_{s}(\\mathbf{x})$ is large, then the difference between the largest and the second largest   \n230 one from $\\{n_{l j}|j=1,\\ldots,K\\}$ will also be large. From (17), the following inequality holds with high   \n231 probability: $\\begin{array}{r}{\\dot{c_{l}}=\\arg\\operatorname*{max}_{j}\\dot{n}_{l j}=\\arg\\operatorname*{max}_{j}\\eta_{j}(\\mathbf{x})=c^{*}(\\mathbf{x})}\\end{array}$ , which means that the classifier makes   \n232 optimal prediction. Hence we mainly consider the case with small $\\eta^{*}(\\mathbf{x})-\\eta_{s}(\\mathbf{x})$ . The details of the   \n233 proof can be found in Appendix E. \u53e3   \n234 The upper and lower bounds match up to logarithmic factors. In (18), the first term is just the   \n235 non-private convergence rate, while the second term $(\\epsilon N)^{-\\frac{\\beta(\\gamma+1)}{\\beta+d}}$ can be regarded as the additional   \n236 risk caused by the privacy mechanism. It decays faster with $N$ compared with the first term, thus the   \n237 additional performance loss caused by the privacy mechanism becomes negligible as $N$ increases.   \n238 This result is crucially different from the local model, under which the privacy mechanism always   \n239 induces higher sample complexity by a factor of $O(1/(\\epsilon^{2}\\wedge1))$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "240 5 Regression with Bounded Noise ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "241 Now we analyze the theoretical limits of regression problems under local and central label DP.   \n242 Throughout this section, we assume that the label is restricted within a bounded interval. ", "page_idx": 6}, {"type": "text", "text": "243 Assumption 2. Given any $\\mathbf{x}\\in\\mathcal{X}$ , $P(|Y|<T|\\mathbf{X}=\\mathbf{x})=1.$ . ", "page_idx": 6}, {"type": "text", "text": "244 Assumption 1 remains the same here. In the remainder of this section, denote $\\mathcal{F}_{r e g1}$ as the set of   \n245 $(f,\\eta)$ that satisfies Assumption 1 and 2. ", "page_idx": 6}, {"type": "text", "text": "246 5.1 Local Label DP ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "247 1) Lower bound. Theorem 5 shows the minimax lower bound. ", "page_idx": 6}, {"type": "text", "text": "248 Theorem 5. Denote $\\mathcal{M}_{\\epsilon}$ as the set of all privacy mechanisms satisfying $\\epsilon$ -label DP. Then ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{\\eta}}\\operatorname*{inf}_{M\\in\\mathcal{M}_{\\epsilon}}\\operatorname*{sup}_{(f,\\eta)\\in\\mathcal{F}_{r e g1}}(R_{r e g}-R_{r e g}^{*})\\gtrsim(N(\\epsilon^{2}\\wedge1))^{-\\frac{2\\beta}{d+2\\beta}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "249 The proof of Theorem 5 is similar to that of Theorem 1, except for some details in hypotheses   \n250 construction and the final bound of excess risk. The details are shown in Appendix F.   \n251 2) Upper bound. The privacy mechanism is $Z=Y+W$ , in which $W\\sim\\mathrm{Lap}(2T/\\epsilon)$ . Then the   \n252 privacy mechanism satisfies $\\epsilon$ -label DP. In this case, the real regression function $\\eta(\\mathbf{x})$ can be estimated   \n253 using the nearest neighbor approach. Let ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{\\eta}(\\mathbf{x})=\\frac{1}{k}\\sum_{i\\in\\mathcal{N}_{k}(\\mathbf{x})}Z_{i},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "254 in which $\\mathcal{N}_{k}({\\bf x})$ is the set of $k$ nearest neighbors of $\\mathbf{x}$ among $\\mathbf{X}_{1},\\ldots,\\mathbf{X}_{N}$ . ", "page_idx": 6}, {"type": "text", "text": "255 Theorem 6. The method described above is $\\epsilon$ -local label DP. Moreover, with $k\\sim N^{\\frac{2\\beta}{d+2\\beta}}(\\epsilon{\\textstyle\\wedge}1)^{-\\frac{2d}{d+2\\beta}}$   \n256 then under Assumption $^{\\,I}$ and 2, ", "page_idx": 6}, {"type": "equation", "text": "$$\nR_{r e g}-R_{r e g}^{*}\\lesssim(N(\\epsilon^{2}\\wedge1))^{-\\frac{2\\beta}{d+2\\beta}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "257 Proof. (Outline) Since $|Y|<T,W\\sim\\mathrm{Lap}(2T/\\epsilon).$ , it is obvious that $Z=Y+W$ is $\\epsilon$ -local label   \n258 DP. For the performance (21), the bias can be bounded by the $k$ nearest neighbor distances based on   \n259 Assumption 1(a). The variance of $\\hat{\\eta}({\\bf x})$ scales inversely with $k$ . An appropriate $k$ can be selected to   \n260 achieve a good tradeoff between bias and variance. The details are shown in Appendix G. \u53e3   \n261 From standard minimax analysis on regression problems, the non-private convergence rate is   \n262 $N^{-2\\beta/(d+2\\beta)}$ . From Theorem 5 and 6, the privatization process makes sample complexity larger by   \n263 a ${\\cal O}(1/\\epsilon^{2})$ factor. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "264 5.2 Central Label DP ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "265 1) Lower bound. The following theorem shows the minimax lower bound. ", "page_idx": 6}, {"type": "text", "text": "266 Theorem 7. Let $A_{\\epsilon}$ be the set of all algorithms satisfying $\\epsilon$ -central DP. Then ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{A\\in\\mathcal A_{\\epsilon}(f,\\eta)\\in\\mathcal F_{r e g1}}(R_{r e g}-R_{r e g}^{*})\\gtrsim N^{-\\frac{2\\beta}{2\\beta+d}}+(\\epsilon N)^{-\\frac{2\\beta}{d+\\beta}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "267 2) Upper bound. For each bin $B_{l}$ , let $\\begin{array}{r}{n_{l}=\\sum_{i=1}^{N}\\mathbf{1}(\\mathbf{X}_{i}\\in B_{l})}\\end{array}$ be the number of samples in $B_{l}$ . If   \n268 ${n}_{l}>0$ , then ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\hat{\\eta}_{l}=\\frac{1}{n_{l}}\\sum_{i=1}^{N}\\mathbf{1}(\\mathbf{X}_{i}\\in B_{l})Y_{i}+W_{l},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "269 in which $W_{l}\\,\\sim\\,\\mathrm{Lap}(2/(n_{l}\\epsilon))$ . If $n_{l}\\,=\\,0$ , i.e. no sample falls in $B_{l}$ , then just let $\\hat{\\eta}_{l}=0$ . For all   \n270 $\\mathbf{x}\\in B_{l}$ , let $\\hat{\\eta}(\\mathbf{x})=\\hat{\\eta}_{l}$ . The excess risk can be bounded with the following theorem.   \n271 Theorem 8. (23) is $\\epsilon$ -label DP. Moreover, under Assumption $^{\\,I}$ and 2, if h scales as $h\\sim N^{-\\frac{1}{2\\beta+d}}+$   \n272 $(\\epsilon N)^{-\\frac{1}{d+\\beta}}$ , then the excess risk is bounded by ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\nR-R^{*}\\lesssim N^{-\\frac{2\\beta}{2\\beta+d}}+(\\epsilon N)^{-\\frac{2\\beta}{d+\\beta}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "273 The upper and lower bounds match, indicating that the results are tight. Again, the second term in   \n274 (24) converges faster than the first one with respect to $N$ , the performance loss caused by privacy   \n275 constraints becomes negligible as $N$ increases. ", "page_idx": 7}, {"type": "text", "text": "276 6 Regression with Heavy-tailed Noise ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "277 In this section, we consider the case such that the noise has tails. We make the following assumption.   \n278 Assumption 3. For all $\\mathbf{x}\\in\\mathcal{X}$ , $\\mathbb{E}[|Y|^{p}|\\mathbf{X}=\\mathbf{x}]\\leq M_{p}$ for some $p\\geq2$ .   \n279 Instead of requiring $|Y|<T$ for some $T$ , now we only assume that the $p$ -th order moment is bounded.   \n280 For non-private cases, given fixed noise variance, the tail does not affect the mean squared error of   \n281 regression. As a result, as long as $p\\geq2$ , the convergence rate of regression risk is the same as the   \n282 case with bounded noise. However, the label DP requires the output to be insensitive to the worst   \n283 case replacement of labels, which can be harder if the noise has tails. To achieve $\\epsilon$ -DP, the clipping   \n284 radius decreases with $\\epsilon$ , thus the noise strength needs to grow faster than $O(1/\\epsilon)$ . As a result, the   \n285 convergence rate becomes slower than the non-private case. In the remainder of this section, denote   \n286 $\\mathcal{F}_{r e g2}$ as the set of $(f,\\eta)$ that satisfies Assumption 1 and 3. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "287 6.1 Local Label DP ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "288 1) Lower bound. In earlier sections about classification and regression with bounded noise, the impact   \n289 of privacy mechanisms is only a polynomial factor on $\\epsilon$ , while the convergence rate of excess risk   \n290 with respect to $N$ is not changed. However, this rule no longer holds when the noise has heavy tails.   \n291 Theorem 9. Denote $\\mathcal{M}_{\\epsilon}$ as the set of all privacy mechanisms satisfying $\\epsilon$ -label DP. Then for small \u03f5, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{\\eta}}\\operatorname*{inf}_{M\\in\\mathcal{M}_{\\epsilon}({f},\\eta)\\in\\mathcal{F}}(R_{r e g}-R_{r e g}^{*})\\gtrsim(N(e^{\\epsilon}-1)^{2})^{-\\frac{2\\beta(p-1)}{2p\\beta+d(p-1)}}+N^{-\\frac{2\\beta}{2\\beta+d}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "292 2) Upper bound. Since now the noise has unbounded distribution, without preprocessing, the   \n293 sensitivity is unbounded, thus simply adding noise to $Y$ can no longer protect the privacy. Therefore,   \n294 a solution is to clip $Y$ into $[-T,T]$ , and add noise proportional to $T/\\epsilon$ to achieve $\\epsilon$ -local label DP.   \n295 Such truncation will inevitably introduce some bias. To achieve a tradeoff between clipping bias and   \n296 sensitivity, the value of $T$ needs to be tuned carefully. Based on such intuition, the method is precisely   \n297 stated as follows. Let $Z_{i}=Y_{T i}\\!+\\!W_{i}$ , in which $Y_{T i}$ is the truncation of $Y_{i}$ , i.e. $Y_{T i}=(Y_{i}{\\wedge}T){\\vee}(-T),$ ,   \n298 and $W\\sim\\mathrm{Lap}(2T/\\epsilon)$ . The result is shown in the next theorem.   \n299 Theorem 10. The method above is $\\epsilon$ -local label DP. Moreover, with $k\\sim(N\\epsilon^{2})^{\\frac{2p\\beta}{2p\\beta+d(p-1)}}\\vee N^{\\frac{2\\beta}{2\\beta+d}},$   \n300 and $T\\sim(k\\epsilon^{2})^{\\frac{1}{2p}}$ , the risk is bounded by ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R_{r e g}-R_{r e g}^{*}\\lesssim(N\\epsilon^{2})^{-\\frac{2\\beta(p-1)}{2p\\beta+d(p-1)}}+N^{-\\frac{2\\beta}{2\\beta+d}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "301 Proof. (Outline) It can be shown that the clipping bias scales as $T^{2(1-p)}$ . To meet the $\\epsilon$ -label DP, an   \n302 additional error that scales as $T/\\epsilon$ is needed. By averaging over $k$ nearest neighbors, the variance   \n303 caused by noise $W$ scales with ${\\dot{T}}^{2}/(k\\epsilon^{2})$ . From standard analysis on nearest neighbor methods [29],   \n304 the non-private mean squared error scales as $1/k+(k/N)^{2\\beta/d}$ . Put all these terms together, Theorem   \n305 10 can be proved. Details can be found in Appendix $\\mathbf{K}$ . \u53e3   \n306 With the limit of $p\\rightarrow\\infty$ , the problem reduces to the case with bounded noise, and the growth rate of   \n307 $k$ and the convergence rate of risk are the same as those in Theorem 6. For finite $p$ , $2\\beta(\\bar{p}-1)/(2p\\beta+$   \n308 $d(p-1))<2\\beta\\bar{/}(2\\beta+d)$ , thus the convergence rate becomes slower due to the privacy mechanism. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "309 6.2 Central Label DP ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "310 1) Lower bound. The minimax lower bound is shown in Theorem 11. ", "page_idx": 8}, {"type": "text", "text": "311 Theorem 11. The minimax lower bound is ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{A\\in{\\mathcal{A}}_{\\epsilon}}\\operatorname*{sup}_{(f,\\eta)\\in{\\mathcal{F}}_{r e g2}}(R_{r e g}-R_{r e g}^{*})\\gtrsim N^{-\\frac{2\\beta}{2\\beta+d}}+(\\epsilon N)^{-\\frac{2\\beta(p-1)}{p\\beta+d(p-1)}}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "312 2) Upper bound. Now we derive the upper bound. To restrict the sensitivity, instead of estimating   \n313 with (23) directly, now we calculate an average of clipped label values: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\hat{\\eta}_{l}=\\frac{1}{n_{l}}\\sum_{i=1}^{N}\\mathbf{1}({\\mathbf{X}}_{i}\\in B_{l})\\operatorname{Clip}(Y_{i},T)+W_{l},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "314 in which $W_{l}\\sim\\mathrm{Lap}(2T/(n_{l}\\epsilon))$ . Then for all $\\mathbf{x}\\in B_{l}$ , let $\\hat{\\eta}(\\mathbf{x})=\\hat{\\eta}_{l}$ . The following theorem bounds   \n315 the excess risk.   \n316 Theorem 12. (28) is $\\epsilon$ -label DP. Moreover, under Assumption $^{\\,l}$ and 3, if h and $T$ scales as $h\\sim$   \n317 $N^{-\\frac{1}{2\\beta+d}}+(\\epsilon N)^{-\\frac{1}{p\\beta+d(p-1)}}$ , and $T\\sim(\\epsilon N h^{d})^{1/p}$ , then the excess risk can be bounded by ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\nR_{r e g}-R_{r e g}^{*}\\lesssim N^{-\\frac{2\\beta}{2\\beta+d}}+(\\epsilon N)^{-\\frac{2\\beta(p-1)}{p\\beta+d(p-1)}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "318 The proof of Theorem 11 and 12 follow that of Theorem 7 and 8. The details are shown in Appendix   \n319 L and $\\mathbf{M}$ respectively. With $p=2$ , the right hand side of (29) becomes $(\\epsilon\\wedge1)^{-\\frac{2\\beta}{2\\beta+d}}$ , indicating that   \n320 the privacy constraint blows up the sample complexity by a constant factor. With larger $p$ , the second   \n321 term in (29) becomes negligible compared with the first one.   \n322 The theoretical analyses in this section are summarized as follows. In general, with fixed noise   \n323 variance, if the label noise is heavy-tailed, while the non-private convergence rates remain unaffected,   \n324 the additional risk caused by privacy mechanisms becomes significantly higher, indicating the   \n325 difficulty of privacy protection for heavy-tailed distributions. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "326 7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "327 In this paper, we have derived the minimax lower bounds of learning under label DP for both central   \n328 and local models. Furthermore, we propose methods whose upper bounds match these lower bounds.   \n329 The results indicate the theoretical limits of learning under the label DP. From these results, it is   \n330 discovered that under local label DP constraints, the sample complexity blows up by a factor of at least   \n331 ${\\cal O}(1/\\epsilon^{2})$ . Under central label DP requirements, the additional error caused by privacy mechanisms   \n332 is significantly smaller. Finally, it is shown that for regression problem with heavy-tailed label   \n333 distribution, the additional risk induced by privacy requirement becomes inevitably higher.   \n334 Limitations: The limitations of our work include the following aspects. Some assumptions can   \n335 be weakened. For example, current analysis assumes that feature distributions have bounded sup  \n336 ports, which may be extended to the unbounded case. One can let the bin splitting and nearest   \n337 neighbor method be adaptive in the tails of features, such as [41]. Moreover, the bounds derived in   \n338 this paper require that samples increase exponentially with dimensionality. However, in practice,   \n339 the performance of learning under the label DP can be quite well even in high dimensions. The   \n340 discrepancy can be explained by the fact that the minimax lower bound considers the worst-case   \n341 distribution over a wide range of distributions. However, in most realistic cases, the distributions   \n342 satisfy significantly better properties. A better modeling is to assume that these samples lie on a low   \n343 dimensional manifold [57,58]. In this case, it is possible to achieve a much better convergence rate.   \n344 Finally, it is not sure whether approximate DP (i.e. $(\\epsilon,\\delta)$ -DP) can improve the convergence rates. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "345 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "346 [1] Rao, B., J. Zhang, D. Wu, et al. Privacy inference attack and defense in centralized and federated   \n347 learning: A comprehensive survey. IEEE Transactions on Artificial Intelligence, 2024.   \n348 [2] Dwork, C., F. McSherry, K. Nissim, et al. Calibrating noise to sensitivity in private data analysis.   \n349 In Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York,   \n350 NY, USA, March 4-7, 2006. Proceedings 3, pages 265\u2013284. Springer, 2006.   \n351 [3] Abadi, M., A. Chu, I. Goodfellow, et al. Deep learning with differential privacy. In Proceedings   \n352 of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pages   \n353 308\u2013318. 2016.   \n354 [4] Dwork, C., A. Roth, et al. The algorithmic foundations of differential privacy. Foundations and   \n355 Trends\u00ae in Theoretical Computer Science, 9(3\u20134):211\u2013407, 2014.   \n356 [5] Bassily, R., A. Smith, A. Thakurta. Private empirical risk minimization: Efficient algorithms   \n357 and tight error bounds. In 2014 IEEE 55th Annual Symposium on Foundations of Computer   \n358 Science, pages 464\u2013473. IEEE, 2014.   \n359 [6] Bassily, R., V. Feldman, K. Talwar, et al. Private stochastic convex optimization with optimal   \n360 rates. Advances in Neural Information Processing Systems, 32, 2019.   \n361 [7] Wang, D., H. Xiao, S. Devadas, et al. On differentially private stochastic convex optimization   \n362 with heavy-tailed data. In International Conference on Machine Learning, pages 10081\u201310091.   \n363 PMLR, 2020.   \n364 [8] Asi, H., V. Feldman, T. Koren, et al. Private stochastic convex optimization: Optimal rates in l1   \n365 geometry. In International Conference on Machine Learning, pages 393\u2013403. PMLR, 2021.   \n366 [9] Das, R., S. Kale, Z. Xu, et al. Beyond uniform lipschitz condition in differentially private   \n367 optimization. In International Conference on Machine Learning, pages 7066\u20137101. PMLR,   \n368 2023.   \n369 [10] Tramer, F., D. Boneh. Differentially private learning needs better features (or much more data).   \n370 In International Conference on Learning Representations. 2021.   \n371 [11] Bu, Z., J. Mao, S. Xu. Scalable and efficient training of large convolutional neural networks   \n372 with differential privacy. Advances in Neural Information Processing Systems, 35:38305\u201338318,   \n373 2022.   \n374 [12] De, S., L. Berrada, J. Hayes, et al. Unlocking high-accuracy differentially private image   \n375 classification through scale. arXiv preprint arXiv:2204.13650, 2022.   \n376 [13] Wei, J., E. Bao, X. Xiao, et al. Dpis: An enhanced mechanism for differentially private sgd with   \n377 importance sampling. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and   \n378 Communications Security, pages 2885\u20132899. 2022.   \n37 9 [14] Ghazi, B., N. Golowich, R. Kumar, et al. Deep learning with label differential privacy. Advances   \n380 in Neural Information Processing Systems, 34:27131\u201327145, 2021.   \n381 [15] McMahan, H. B., G. Holt, D. Sculley, et al. Ad click prediction: a view from the trenches. In   \n382 Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and   \n383 data mining, pages 1222\u20131230. 2013.   \n384 [16] McSherry, F., I. Mironov. Differentially private recommender systems: Building privacy into   \n385 the netfilx prize contenders. In Proceedings of the 15th ACM SIGKDD international conference   \n386 on Knowledge discovery and data mining, pages 627\u2013636. 2009.   \n387 [17] Bussone, A., B. Kasadha, S. Stumpf, et al. Trust, identity, privacy, and security considerations   \n388 for designing a peer data sharing platform between people living with hiv. Proceedings of the   \n389 ACM on Human-Computer Interaction, 4(CSCW2):1\u201327, 2020.   \n390 [18] Ghazi, B., P. Kamath, R. Kumar, et al. Regression with label differential privacy. In The   \n391 Eleventh International Conference on Learning Representations. 2022.   \n392 [19] Malek Esmaeili, M., I. Mironov, K. Prasad, et al. Antipodes of label differential privacy: Pate   \n393 and alibi. Advances in Neural Information Processing Systems, 34:6934\u20136945, 2021.   \n394 [20] Esfandiari, H., V. Mirrokni, U. Syed, et al. Label differential privacy via clustering. In   \n395 International Conference on Artificial Intelligence and Statistics, pages 7055\u20137075. PMLR,   \n396 2022.   \n397 [21] Tang, X., M. Nasr, S. Mahloujifar, et al. Machine learning with differentially private labels:   \n398 Mechanisms and frameworks. Proceedings on Privacy Enhancing Technologies, 2022.   \n399 [22] Cover, T. M. Elements of information theory. John Wiley & Sons, 1999.   \n400 [23] Duchi, J. C., M. I. Jordan, M. J. Wainwright. Local privacy and statistical minimax rates. In   \n401 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, pages 429\u2013438.   \n402 IEEE, 2013.   \n403 [24] \u2014. Minimax optimal procedures for locally private estimation. Journal of the American   \n404 Statistical Association, 113(521):182\u2013201, 2018.   \n405 [25] Gopi, S., G. Kamath, J. Kulkarni, et al. Locally private hypothesis selection. In Conference on   \n406 Learning Theory, pages 1785\u20131816. PMLR, 2020.   \n407 [26] Berrett, T., C. Butucea. Classification under local differential privacy. arXiv preprint   \n408 arXiv:1912.04629, 2019.   \n409 [27] Berrett, T. B., L. Gy\u00f6rf,i H. Walk. Strongly universally consistent nonparametric regression and   \n410 classification with privatised data. Electronic Journal of Statistics, 15:2430\u20132453, 2021.   \n411 [28] Tsybakov, A. B. Introduction to Nonparametric Estimation. 2009.   \n412 [29] Audibert, J.-Y., A. B. Tsybakov. Fast learning rates for plug-in classifiers. Annals of Statistics,   \n413 2007.   \n414 [30] Warner, S. L. Randomized response: A survey technique for eliminating evasive answer bias.   \n415 Journal of the American Statistical Association, 60(309):63\u201369, 1965.   \n416 [31] Badanidiyuru Varadaraja, A., B. Ghazi, P. Kamath, et al. Optimal unbiased randomizers for   \n417 regression with label differential privacy. Advances in Neural Information Processing Systems,   \n418 36, 2023.   \n419 [32] LeCam, L. Convergence of estimates under dimensionality restrictions. The Annals of Statistics,   \n420 pages 38\u201353, 1973.   \n421 [33] Verd\u00fa, S., et al. Generalizing the fano inequality. IEEE Transactions on Information Theory,   \n422 40(4):1247\u20131251, 1994.   \n423 [34] Assouad, P. Deux remarques sur l\u2019estimation. Comptes rendus des s\u00e9ances de l\u2019Acad\u00e9mie des   \n424 sciences. S\u00e9rie 1, Math\u00e9matique, 296(23):1021\u20131024, 1983.   \n425 [35] Yang, Y. Minimax nonparametric classification. i. rates of convergence. IEEE Transactions on   \n426 Information Theory, 45(7):2271\u20132284, 1999.   \n427 [36] \u2014. Minimax nonparametric classification. ii. model selection for adaptation. IEEE Transactions   \n428 on Information Theory, 45(7):2285\u20132292, 1999.   \n429 [37] Chaudhuri, K., S. Dasgupta. Rates of convergence for nearest neighbor classification. Advances   \n430 in Neural Information Processing Systems, 27, 2014.   \n431 [38] Yang, Y., S. T. Tokdar. Minimax-optimal nonparametric regression in high dimensions. The   \n432 Annals of Statistics, pages 652\u2013674, 2015.   \n433 [39] D\u00f6ring, M., L. Gy\u00f6rf,i H. Walk. Rate of convergence of $k$ -nearest-neighbor classification rule.   \n434 Journal of Machine Learning Research, 18(227):1\u201316, 2018.   \n435 [40] Gadat, S., T. Klein, C. Marteau. Classification in general finite dimensional spaces with the   \n436 k-nearest neighbor rule. Annals of Statistics, 2016.   \n437 [41] Zhao, P., L. Lai. Minimax rate optimal adaptive nearest neighbor classification and regression.   \n438 IEEE Transactions on Information Theory, 67(5):3155\u20133182, 2021.   \n439 [42] Kasiviswanathan, S. P., H. K. Lee, K. Nissim, et al. What can we learn privately? SIAM Journal   \n440 on Computing, 40(3):793\u2013826, 2011.   \n441 [43] Li, M., T. B. Berrett, Y. Yu. On robustness and local differential privacy. The Annals of Statistics,   \n442 51(2):717\u2013737, 2023.   \n443 [44] Feldman, V., T. Koren, K. Talwar. Private stochastic convex optimization: optimal rates in linear   \n444 time. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing,   \n445 pages 439\u2013449. 2020.   \n446 [45] Duchi, J., R. Rogers. Lower bounds for locally private estimation via communication complexity.   \n447 In Conference on Learning Theory, pages 1161\u20131191. PMLR, 2019.   \n448 [46] Huang, Z., Y. Liang, K. Yi. Instance-optimal mean estimation under differential privacy.   \n449 Advances in Neural Information Processing Systems, 34:25993\u201326004, 2021.   \n450 [47] Hardt, M., K. Talwar. On the geometry of differential privacy. In Proceedings of the forty-second   \n451 ACM symposium on Theory of computing, pages 705\u2013714. 2010.   \n452 [48] Bun, M., G. Kamath, T. Steinke, et al. Private hypothesis selection. Advances in Neural   \n453 Information Processing Systems, 32, 2019.   \n454 [49] Narayanan, S. Better and simpler lower bounds for differentially private statistical estimation.   \n455 arXiv preprint arXiv:2310.06289, 2023.   \n456 [50] Kamath, G., V. Singhal, J. Ullman. Private mean estimation of heavy-tailed distributions. In   \n457 Conference on Learning Theory, pages 2204\u20132235. PMLR, 2020.   \n458 [51] Kamath, G., J. Li, V. Singhal, et al. Privately learning high-dimensional distributions. In   \n459 Conference on Learning Theory, pages 1853\u20131902. PMLR, 2019.   \n460 [52] Alabi, D., P. K. Kothari, P. Tankala, et al. Privately estimating a gaussian: Efficient, robust, and   \n461 optimal. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing, pages   \n462 483\u2013496. 2023.   \n463 [53] Arbas, J., H. Ashtiani, C. Liaw. Polynomial time and private learning of unbounded gaussian   \n464 mixture models. In International Conference on Machine Learning, pages 1018\u20131040. 2023.   \n465 [54] Bun, M., J. Ullman, S. Vadhan. Fingerprinting codes and the price of approximate differential   \n466 privacy. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing,   \n467 pages 1\u201310. 2014.   \n468 [55] Kamath, G., A. Mouzakis, V. Singhal. New lower bounds for private estimation and a generalized   \n469 fingerprinting lemma. Advances in neural information processing systems, 35:24405\u201324418,   \n470 2022.   \n471 [56] McSherry, F., K. Talwar. Mechanism design via differential privacy. In 48th Annual IEEE   \n472 Symposium on Foundations of Computer Science (FOCS\u201907), pages 94\u2013103. IEEE, 2007.   \n473 [57] Kpotufe, S. k-nn regression adapts to local intrinsic dimension. Advances in neural information   \n474 processing systems, 24, 2011.   \n475 [58] Carter, K. M., R. Raich, A. O. Hero III. On local intrinsic dimension estimation and its   \n476 applications. IEEE Transactions on Signal Processing, 58(2):650\u2013663, 2009. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "477 A Proof of Proposition 2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "478 From (5) and (6), the Bayes risk is ", "page_idx": 12}, {"type": "equation", "text": "$$\nR_{c l s}^{*}=\\mathrm{P}(Y\\neq c^{*}(\\mathbf{X}))=\\int\\mathrm{P}(Y\\neq c^{*}(\\mathbf{x})|\\mathbf{X}=\\mathbf{x})f(\\mathbf{x})d\\mathbf{x}=\\int(1-\\eta^{*}(\\mathbf{x}))f(\\mathbf{x})d\\mathbf{x}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "479 The risk of classifier $c$ is ", "page_idx": 12}, {"type": "equation", "text": "$$\nR_{c l s}=\\mathrm{P}(Y\\neq c(\\mathbf{X}))=\\mathbb{E}\\left[\\int\\left(1-\\eta_{c(\\mathbf{x})}(\\mathbf{x})\\right)f(\\mathbf{x})d\\mathbf{x}\\right].\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "480 From (31) and (6), ", "page_idx": 12}, {"type": "equation", "text": "$$\nR_{c l s}-R_{c l s}^{*}=\\int(\\eta^{*}(\\mathbf{x})-\\mathbb{E}[\\eta_{c(\\mathbf{x})}(\\mathbf{x})])f(\\mathbf{x})d\\mathbf{x}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "481 The proof is complete. ", "page_idx": 12}, {"type": "text", "text": "482 B Proof of Theorem 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "483 In this section, we prove the minimax lower bound of multi-class classification. The problem with $K$   \n484 classes with $K>2$ is inherently harder than that with $K=2$ . Therefore, we just need to prove the   \n485 lower bound for binary classification, in which $\\mathcal{D}=\\{1,2\\}$ . Let ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\eta(\\mathbf{x})=\\eta_{2}(\\mathbf{x})-\\eta_{1}(\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "486 Since $\\eta_{1}(\\mathbf{x})+\\eta_{2}(\\mathbf{x})=1$ always holds, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\eta_{1}(\\mathbf{x})=\\frac{1-\\eta(\\mathbf{x})}{2},\\eta_{2}(\\mathbf{x})=\\frac{1+\\eta(\\mathbf{x})}{2}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "487 Therefore, $\\eta(\\mathbf{x})$ captures the conditional distribution of $Y$ given $\\mathbf{x}$ . ", "page_idx": 12}, {"type": "text", "text": "488 Find $G$ disjoint cubes $B_{1},\\ldots,B_{G}\\subset\\mathcal{X}$ , such that the length of each cube is $h$ . Denote $\\mathbf{c}_{1},\\ldots,\\mathbf{c}_{G}$   \n489 as the centers of these cubes. Let $\\phi(\\mathbf{u})$ be some function supported at $[-1/2,1/2]^{d}$ , such that ", "page_idx": 12}, {"type": "equation", "text": "$$\n0\\leq\\phi(\\mathbf{u})\\leq1.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "490 Let $f(\\mathbf{x})=c$ over $\\mathbf{x}\\in\\mathcal{X}$ . For $\\mathbf{v}\\in\\mathcal{V}:=\\{-1,1\\}^{m}$ , let ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\eta_{\\bf v}({\\bf x})=\\sum_{k=1}^{m}v_{k}\\phi\\left(\\frac{{\\bf x}-{\\bf c}_{k}}{h}\\right)h^{\\beta}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "491 It can be proved that if for some constant $C_{M}$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\nm\\leq C_{M}h^{\\gamma\\beta-d},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "492 then for any $\\eta=\\eta_{\\mathbf{v}},\\eta_{1}$ and $\\eta_{2}$ satisfies Assumption 1(b). Denote ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\hat{v}_{k}=\\underset{s\\in\\{-1,1\\}}{\\arg\\operatorname*{max}}\\,\\int_{B_{k}}\\phi\\left(\\frac{\\mathbf x-\\mathbf c_{k}}{h}\\right)\\mathbf1(\\mathrm{sign}(\\hat{\\eta}(\\mathbf x))=s)f(\\mathbf x)d\\mathbf x.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "493 Then the excess risk is bounded by ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{R-R^{*}}&{=}&{\\displaystyle\\int|\\eta_{\\mathbf{v}}(\\mathbf{x})|\\mathrm{P}(\\mathrm{sign}(\\hat{\\eta}(\\mathbf{x}))\\neq\\mathrm{sign}(\\eta_{\\mathbf{v}}(\\mathbf{x})))f(\\mathbf{x})d\\mathbf{x}}\\\\ &{\\displaystyle\\ge}&{\\displaystyle\\sum_{k=1}^{m}\\int_{B_{k}}|\\eta_{\\mathbf{v}}(\\mathbf{x})|\\mathrm{P}(\\mathrm{sign}(\\hat{\\eta}(\\mathbf{x}))\\neq\\mathrm{sign}(\\eta_{\\mathbf{v}}(\\mathbf{x})))f(\\mathbf{x})d\\mathbf{x}}\\\\ &{\\displaystyle=}&{\\displaystyle\\sum_{k=1}^{m}h^{\\beta}\\int_{B_{k}}\\phi\\left(\\frac{\\mathbf{x}-\\mathbf{c}_{k}}{h}\\right)\\mathrm{P}(\\mathrm{sign}(\\hat{\\eta}(\\mathbf{x})))f(\\mathbf{x})d\\mathbf{x}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "494 If $\\hat{v}_{k}\\neq v_{k}$ , then from (38), ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\int_{B_{k}}\\phi\\left(\\frac{\\mathbf{x}-\\mathbf{c}_{k}}{h}\\right)\\mathbf{1}(\\mathrm{sign}(\\hat{\\eta}(\\mathbf{x})))f(\\mathbf{x})d\\mathbf{x}\\geq\\int_{B_{k}}\\phi\\left(\\frac{\\mathbf{x}-\\mathbf{c}_{k}}{h}\\right)\\mathbf{1}(\\mathrm{sign}(\\hat{\\eta}(\\mathbf{x}))=v_{k})f(\\mathbf{x})d\\mathbf{x}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "495 Therefore ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\int_{B_{k}}\\phi\\left(\\frac{\\mathbf{x}-\\mathbf{c}_{k}}{h}\\right)\\mathbf{1}(\\mathrm{sign}(\\hat{\\eta}(\\mathbf{x}))\\,\\neq\\,v_{k})f(\\mathbf{x})d\\mathbf{x}\\geq\\frac{1}{2}\\int_{B_{k}}\\phi\\left(\\frac{\\mathbf{x}-\\mathbf{c}_{k}}{h}\\right)f(\\mathbf{x})d\\mathbf{x}\\geq\\frac{1}{2}c h^{d}\\left\\|\\phi\\right\\|_{1}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "496 Hence ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{\\displaystyle R-R^{*}}}&{{\\ge}}&{{\\displaystyle\\frac{1}{2}c h^{\\beta+d}\\left\\|\\phi\\right\\|_{1}\\sum_{k=1}^{m}\\mathbf{P}(\\hat{v}_{k}\\ne v_{k})}}\\\\ {{\\displaystyle}}&{{=}}&{{\\displaystyle\\frac{1}{2}c h^{\\beta+d}\\left\\|\\phi\\right\\|_{1}\\mathbb{E}[\\rho_{H}(\\hat{\\mathbf{v}},\\mathbf{v})],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "497 in which $\\rho_{H}$ denotes the Hamming distance. Then ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{Y}}\\operatorname*{inf}_{M\\in\\mathcal{M}_{\\epsilon}(f,\\eta)\\in\\mathcal{P}}(R-R^{*})\\ge\\frac{1}{2}h^{\\beta+d}\\operatorname*{l}\\|\\phi\\|_{1}\\operatorname*{inf}_{\\hat{\\mathbf{v}}}\\operatorname*{inf}_{M\\in\\mathcal{M}_{\\epsilon}\\:\\mathbf{v}\\in\\mathcal{V}}\\!\\!\\mathbb{E}[\\rho_{H}(\\hat{\\mathbf{v}},\\mathbf{v})].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "498 Define ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\delta=\\operatorname*{sup}_{M\\in\\mathcal{M}_{\\epsilon}{\\mathbf v},{\\mathbf v}^{\\prime}:\\rho_{H}({\\mathbf v},{\\mathbf v}^{\\prime})=1}D_{K L}(P_{(X,Z)_{1:N}|{\\mathbf v}}||P_{(X,Z)_{1:N}|{\\mathbf v}^{\\prime}}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "499 in which $P_{\\left(X,Z\\right)_{1:N}|\\mathbf{v}}$ denotes the distribution of $({\\bf X}_{1},Z_{1}),\\ldots,({\\bf X}_{N},Z_{N})$ with $\\eta\\:=\\:\\eta_{\\mathbf{v}}$ . $D_{K L}$   \n500 denotes the Kullback-Leibler divergence. Then from [28], Theorem 2.12(iv), ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{infinf}_{\\hat{\\mathbf{v}}~M}\\operatorname*{max}_{\\mathbf{v}\\in\\mathcal{V}}[\\rho_{H}(\\hat{\\mathbf{v}},\\mathbf{v})]\\ge\\frac{m}{2}\\left(\\frac12e^{-\\delta},1-\\sqrt{\\frac{\\delta}{2}}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "501 It remains to bound $\\delta$ . Without loss of generality, suppose $v_{1}\\neq v_{1}^{\\prime}$ , and $v_{i}=v_{i}^{\\prime}$ for $i\\neq1$ . Then ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{D_{K L}(P_{(X,Z)_{1,N}|\\mathbf v}||P_{(X,Z)_{1,N}|\\mathbf v^{\\prime}})}&{\\stackrel{(a)}=}&{N D_{K L}(P_{X,Z|\\mathbf v}||P_{X,Z|\\mathbf v^{\\prime}})}\\\\ &{\\stackrel{(b)}=}&{N\\int_{B_{1}}f(\\mathbf x)D_{K L}(P_{Z|\\mathbf X=\\mathbf v,\\mathbf v}||P_{Z|\\mathbf X=\\mathbf v,\\mathbf v^{\\prime}})d\\mathbf x}\\\\ &{\\stackrel{(c)}\\le}&{N\\int_{B_{1}}f(\\mathbf x)(e^{\\epsilon}-1)^{2}\\mathbb{T}\\mathbb{V}^{2}(P_{Z|\\mathbf X=\\mathbf v,\\mathbf v^{\\prime}}P_{Z|\\mathbf X=\\mathbf v,\\mathbf v^{\\prime}})d\\mathbf x}\\\\ {=}&{\\,N\\int_{B_{1}}f(\\mathbf x)(e^{\\epsilon}-1)^{2}\\eta_{v}^{2}(\\mathbf x)d\\mathbf x}\\\\ {=}&{\\,N(e^{\\epsilon}-1)^{2}\\int_{B_{1}}f(\\mathbf x)\\phi^{2}\\left(\\frac{\\mathbf x-\\mathbf c_{1}}{h}\\right)h^{2\\beta}d\\mathbf x}\\\\ &{\\stackrel{(d)}=}&{N(e^{\\epsilon}-1)^{2}h^{2\\beta+d}\\|\\phi\\|_{2}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "502 In (a), $P_{X,Z|\\mathbf{v}}$ denotes the distribution of a single sample with privatized label $(X,Z)$ , with $\\eta=\\eta_{\\mathbf{v}}$ .   \n503 In (b), $P_{Z|\\mathbf{X}=\\mathbf{x},\\mathbf{v}}$ denotes the conditional distribution of $Z$ given ${\\bf X}={\\bf x}$ , with $\\eta=\\eta_{\\mathbf{v}}$ . (c) uses [24],   \n504 Theorem 1. In (d), $\\begin{array}{r}{\\|\\phi\\|_{2}^{2}=\\int\\phi^{2}(\\mathbf{u})d\\mathbf{u}}\\end{array}$ , which is a constant. Moreover, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle)_{K L}(P_{X,Z|\\mathbf{v}}||P_{X,Z|\\mathbf{v}^{\\prime}})}&{\\overset{(a)}{\\leq}\\quad{\\cal D}_{K L}(P_{X,Y|\\mathbf{v}}||P_{X,Y|\\mathbf{v}^{\\prime}})}\\\\ &{=\\displaystyle}&{\\int_{B_{1}}f(\\mathbf{x})\\left[\\mathbf{P}(Y=1|\\mathbf{v})\\ln\\frac{\\mathbf{P}(Y=1|\\mathbf{v})}{\\mathbf{P}(Y=1|\\mathbf{v}^{\\prime})}+\\mathbf{P}(Y=-1|\\mathbf{v})\\ln\\frac{\\mathbf{P}(Y=-1|\\mathbf{v})}{\\mathbf{P}(Y=-1|\\mathbf{v})}\\right]\\mathrm{d}\\mathbf{x}}\\\\ &{=\\displaystyle}&{\\int_{B_{1}}f(\\mathbf{x})\\left[\\frac{1+\\eta_{\\mathbf{v}}(\\mathbf{x})}{2}\\ln\\frac{1+\\eta_{\\mathbf{v}}(\\mathbf{x})}{1-\\eta_{\\mathbf{v}}(\\mathbf{x})}+\\frac{1-\\eta_{\\mathbf{v}}(\\mathbf{x})}{2}\\ln\\frac{1-\\eta_{\\mathbf{v}}(\\mathbf{x})}{1+\\eta_{\\mathbf{v}}(\\mathbf{x})}\\right]d\\mathbf{x}}\\\\ &{\\overset{(b)}{\\leq}\\quad3\\int_{B_{1}}f(\\mathbf{x})\\eta_{\\mathbf{v}}^{2}(\\mathbf{x})d\\mathbf{x}}\\\\ &{\\leq\\quad3\\,h^{2\\beta+d}\\left\\Vert\\phi\\right\\Vert_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "505 For (a), note that $Z$ is generated from $Y$ . From data processing inequality, (a) holds. For (b), without   \n506 loss of generality, suppose that $v_{1}=1$ , thus $\\eta_{\\mathbf{v}}(\\mathbf{x})\\geq0$ in $B_{1}$ . Then $\\ln(1+\\eta_{\\mathbf{v}}(\\mathbf{x}))\\leq\\eta_{\\mathbf{v}}(\\mathbf{x})$ . From   \n507 (35) and (36), $|\\bar{\\eta_{\\mathrm{v}}}(\\mathbf{x})|\\leq1/2$ . Therefore, $-\\ln(1-\\eta_{\\mathbf{v}}(\\mathbf{x}))\\le2\\eta_{\\mathbf{v}}(\\mathbf{x})$ . Therefore (b) holds. ", "page_idx": 13}, {"type": "text", "text": "508 From (46) and (47), ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\delta\\leq N\\left[(e^{\\epsilon}-1)^{2}\\wedge3\\right]h^{2\\beta+d}\\left\\|\\phi\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "509 Let ", "page_idx": 14}, {"type": "equation", "text": "$$\nh\\sim\\left(N\\left(\\epsilon^{2}\\wedge1\\right)\\right)^{-\\frac{1}{2\\beta+d}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "510 Then $\\delta\\lesssim1$ . From (45), with $m\\sim h^{\\gamma\\beta-d}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{\\mathbf{v}}}\\operatorname*{inf}_{M\\in\\mathcal{M}_{\\epsilon}}\\operatorname*{max}_{\\mathbf{v}\\in\\mathcal{V}}\\mathbb{E}[\\rho_{H}(\\hat{\\mathbf{v}},\\mathbf{v})]\\gtrsim h^{\\gamma\\beta-d}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "511 Hence ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{Y}}\\operatorname*{inf}_{M\\in\\mathcal{M}_{\\epsilon}(f,\\eta)\\in\\mathcal{P}}(R-R^{*})\\gtrsim h^{\\beta+d}h^{\\gamma\\beta-d}\\sim h^{\\beta(\\gamma+1)}\\sim\\big[N\\left(\\epsilon^{2}\\wedge1\\right)\\big]^{-\\frac{\\beta(\\gamma+1)}{2\\beta+d}}\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "512 The proof is complete. ", "page_idx": 14}, {"type": "text", "text": "513 C Proof of Theorem 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "514 Denote ", "page_idx": 14}, {"type": "equation", "text": "$$\nn_{l}=\\sum_{i=1}^{N}\\mathbf{1}(\\mathbf{X}_{i}\\in B_{l}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "515 and for $\\mathbf{Z}=M(\\mathbf{X},Y)$ , let ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\widetilde{\\eta}_{j}(\\mathbf{x})}&{:=}&{\\mathbb{E}[\\mathbf{Z}(j)|\\mathbf{X}=\\mathbf{x}]}\\\\ &{=}&{\\frac{e^{\\frac{\\epsilon}{2}}}{e^{\\frac{\\epsilon}{2}}+1}\\eta_{j}(\\mathbf{x})+\\frac{1}{e^{\\frac{\\epsilon}{2}}+1}(1-\\eta_{j}(\\mathbf{x}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "516 as the number of training samples whose feature vectors fall in $B_{l}$ , and ", "page_idx": 14}, {"type": "equation", "text": "$$\nv_{l j}:=\\frac{1}{n_{l}}\\sum_{i:\\mathbf{X}_{i}\\in B_{l}}\\tilde{\\eta}_{j}(\\mathbf{X}_{i}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "517 Recall (12) that defines $S_{l j}$ . From Hoeffding\u2019s inequality, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{P}\\left(|S_{l j}-n_{l}v_{l j}|>t|\\mathbf{X}_{1:N}\\right)\\leq2\\exp\\left[-\\frac{2t^{2}}{n_{l}}\\right],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "518 in which $\\mathbf{X}_{1:N}$ denotes $\\mathbf{X}_{1},\\ldots,\\mathbf{X}_{N}$ . ", "page_idx": 14}, {"type": "text", "text": "519 Define ", "page_idx": 14}, {"type": "equation", "text": "$$\nv_{l}^{*}:=\\operatorname*{max}_{j}v_{l j},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "520 and ", "page_idx": 14}, {"type": "equation", "text": "$$\nc_{l}^{*}:=\\arg\\operatorname*{max}_{j}v_{l j}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "521 Now we bound $\\mathbf{P}(v_{l}^{*}-v_{l c_{l}}>t)$ , in which $c_{l}$ is defined in (13). $c_{l}$ can be viewed as the prediction at   \n522 the $l$ -th bin. We would like to show that the even if the prediction is wrong, the value (i.e. conditional   \n523 probability) of the predicted class is close to the ground truth. $v_{l}^{*}-v_{l c_{l}}>t$ only if $\\exists j,v_{l}^{*}-v_{l j}>t$ ,   \n524 and $S_{l j}>S_{l c_{l}^{*}}$ . Therefore either $S_{l j}-n_{l}v_{l j}>t/2$ or $S_{l c_{l}^{*}}-\\dot{n}_{l}v_{l}^{*}>t/2$ holds. Hence ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{P}\\left(v_{l}^{*}-v_{l c_{l}}\\geq t\\right)\\leq\\mathbf{P}\\left(\\exists j,|S_{l j}-n_{l}v_{l j}|\\geq\\frac{1}{2}n_{l}t\\right)\\leq2K\\exp\\left(-\\frac{1}{2}n_{l}t^{2}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "525 Define ", "page_idx": 14}, {"type": "equation", "text": "$$\nt_{0}=\\sqrt{\\frac{2\\ln(2K)}{n_{l}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "526 Then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{v_{l}^{*}-\\mathbb{E}[v_{l c_{l}}|\\mathbf{X}_{1:N}]}&{=}&{\\displaystyle\\int_{0}^{1}\\mathbf{P}(v_{l}^{*}-v_{l c_{l}}>t)d t}\\\\ &{\\le}&{t_{0}+\\displaystyle\\int_{t_{0}}^{\\infty}2K\\exp\\left(-\\frac{1}{2}n t^{2}\\right)d t}\\\\ &{\\overset{(a)}{\\le}}&{t_{0}+2\\sqrt{\\frac{2\\pi}{n_{l}}}K\\exp\\left(-\\frac{1}{2}n t_{0}^{2}\\right)}\\\\ &{=}&{\\displaystyle\\sqrt{\\frac{2\\ln(2K)}{n_{l}}}+\\sqrt{\\frac{2\\pi}{n_{l}}}}\\\\ &{\\le}&{3\\sqrt{\\frac{\\ln(2K)}{n_{l}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "527 In (a), we use the inequality ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\int_{t}^{\\infty}e^{-{\\frac{u^{2}}{2\\sigma^{2}}}}d u\\leq{\\sqrt{2\\pi}}\\sigma e^{-{\\frac{t^{2}}{2\\sigma^{2}}}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "528 Now we bound the excess risk. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{R-R^{*}}&{=}&{\\displaystyle\\int\\left(\\eta^{*}(\\mathbf{x})-\\mathbb{E}[\\eta_{c(\\mathbf{x})}(\\mathbf{x})]\\right)f(\\mathbf{x})d\\mathbf{x}}\\\\ &{=}&{\\displaystyle\\sum_{l=1}^{G}\\int_{B_{l}}\\left(\\eta^{*}(\\mathbf{x})-\\mathbb{E}[\\eta_{c(\\mathbf{x})}(\\mathbf{x})]\\right)f(\\mathbf{x})d\\mathbf{x}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "529 We need to bound $\\begin{array}{r}{\\int_{B_{l}}\\left(\\eta^{*}(\\mathbf{x})-\\mathbb{E}[\\eta_{c(\\mathbf{x})}(\\mathbf{x})]\\right)f(\\mathbf{x})d\\mathbf{x}}\\end{array}$ for each $l$ . From Assumption 1(a), for any   \n530 $\\mathbf{x},\\mathbf{x}^{\\prime}\\in B_{l}$ , the distance is bounded by $\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\|\\leq\\sqrt{d}L$ . Thus ", "page_idx": 15}, {"type": "equation", "text": "$$\n|\\eta_{j}(\\mathbf{x})-\\eta_{j}(\\mathbf{x}^{\\prime})|\\leq L_{d}h^{\\beta},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "531 in which $L_{d}$ is defined as $L_{d}:=L\\sqrt{d}$ . From (63) and (53), ", "page_idx": 15}, {"type": "equation", "text": "$$\n|\\tilde{\\eta}_{j}({\\bf x})-\\tilde{\\eta}_{j}({\\bf x}^{\\prime})|\\leq\\frac{e^{\\frac{\\epsilon}{2}}-1}{e^{\\frac{\\epsilon}{2}}+1}L_{d}h^{\\beta}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "532 Define ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{\\eta}^{*}(\\mathbf{x})=\\operatorname*{max}_{j}\\tilde{\\eta}_{j}(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "533 then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\eta^{*}(\\mathbf{x})-\\mathbb{E}[\\eta_{c_{l}}(\\mathbf{x})\\vert\\mathbf{X}_{1:N}]}&{\\leq}&{\\frac{e^{\\frac{\\epsilon}{2}}+1}{e^{\\frac{\\epsilon}{2}}-1}\\left(\\tilde{\\eta}^{*}(\\mathbf{x})-\\mathbb{E}[\\tilde{\\eta}_{c_{l}}(\\mathbf{x})\\vert\\mathbf{X}_{1:N}]\\right)}\\\\ &{\\leq}&{\\frac{e^{\\frac{\\epsilon}{2}}+1}{e^{\\frac{\\epsilon}{2}}-1}\\left(v_{l}^{*}-\\mathbb{E}[v_{l c_{l}}\\vert\\mathbf{X}_{1:N}]\\right)+2L_{d}h^{\\beta}}\\\\ &{\\leq}&{3\\frac{e^{\\frac{\\epsilon}{2}}+1}{e^{\\frac{\\epsilon}{2}}-1}\\sqrt{\\frac{2\\ln(2K)}{n_{l}}}+2L_{d}h^{\\beta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "534 Take integration over cube $B_{l}$ , we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{B_{l}}\\big(\\eta^{*}(\\mathbf{x})-\\mathbb{E}[\\eta_{c l}(\\mathbf{x})]\\big)\\,f(\\mathbf{x})d\\mathbf{x}}\\\\ {\\le}&{\\displaystyle\\mathbf{P}\\left(n_{l}<\\frac{1}{2}N p(B_{l})\\right)\\int_{B_{l}}\\bigg(\\eta^{*}(\\mathbf{x})-\\mathbb{E}[\\eta_{c l}(\\mathbf{x})|n_{l}<\\frac{1}{N}p(B_{l})]\\bigg)\\,f(\\mathbf{x})d\\mathbf{x}}\\\\ &{\\displaystyle+\\int_{B_{l}}\\bigg(\\eta^{*}(\\mathbf{x})-\\mathbb{E}[\\eta_{c l}(\\mathbf{x})|n_{l}\\ge\\frac{1}{N}p(B_{l})]\\bigg)\\,f(\\mathbf{x})d\\mathbf{x}}\\\\ {\\le}&{p(B_{l})e^{-\\frac{1}{2}(1-\\ln2)N p(B_{l})}+\\left[3\\frac{e^{\\frac{\\varepsilon}{2}}+1}{e^{\\frac{\\varepsilon}{2}}-1}\\sqrt{\\frac{2\\ln(2K)}{N p(B_{l})}}+2L^{d}h^{\\beta}\\right]p(B_{l}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "535 in which $p(B_{l})=\\mathbf{P}(\\mathbf{X}\\in B_{l})$ is the probability mass of $B_{l}$ . Moreover, define ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Delta_{l}=\\operatorname*{inf}_{{\\bf x}\\in B_{l}}\\left(\\eta^{*}({\\bf x})-\\eta_{s}({\\bf x})\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "536 and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{\\Delta}_{l}=\\operatorname*{inf}_{\\mathbf{x}\\in B_{l}}\\left(\\tilde{\\eta}^{*}(\\mathbf{x})-\\tilde{\\eta}_{s}(\\mathbf{x})\\right)=\\frac{e^{\\frac{\\epsilon}{2}}-1}{e^{\\frac{\\epsilon}{2}}+1}\\Delta_{l},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "537 in which the $\\tilde{\\eta}_{s}$ is the second largest value of $\\tilde{\\eta}_{j}$ among $j=1,\\dots,K$ , which follows the definition   \n538 of $\\eta_{s}$ . ", "page_idx": 16}, {"type": "text", "text": "539 If $\\Delta_{l}>0$ , then $c^{*}(\\mathbf{x})$ is the same over $B_{l}$ . Then either $v_{l}^{*}-v_{l c_{l}}=0$ or $v_{l}^{*}-v_{l c_{l}}\\geq\\Delta_{l}$ holds. Hence ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\eta}^{*}(\\mathbf x)-\\mathbb{E}[\\widetilde{\\eta}_{G}(\\mathbf x)|\\mathbf X_{1:N}]}\\\\ {=}&{\\int_{0}^{1}\\mathbf{P}(\\bar{\\eta}^{*}(\\mathbf x)-\\bar{\\eta}_{G}(\\mathbf x)>t|\\mathbf X_{1:N})\\,d t}\\\\ {\\leq}&{\\int_{0}^{1}\\mathbf{P}\\left(v_{l}^{*}-v_{l c_{l}}>t-2L_{a}h\\vartheta_{e}^{\\frac{\\alpha}{2}}\\!+\\!1|\\mathbf X_{1:N}\\right)d t}\\\\ {\\leq}&{\\int_{0}^{\\bar{\\Delta}_{l}+2L_{a}h^{\\beta}}\\mathbf{P}(v_{l}^{*}-v_{l c_{l}}\\geq\\Delta_{l})d t+\\displaystyle\\int_{\\bar{\\Delta}_{l}+2L_{a}h^{\\beta}}^{\\infty}2K\\exp\\left[-\\frac{1}{2}n_{l}(t-2L_{a}h^{\\beta})^{2}\\right]d t}\\\\ {\\leq}&{2K\\exp\\left(-\\frac{1}{2}n_{l}\\tilde{\\Delta}_{l}^{2}\\right)(\\tilde{\\Delta}_{l}+2L_{a}h\\beta_{e}^{\\frac{\\alpha}{2}}\\!+\\!1)+2K\\sqrt{\\frac{2\\pi}{n_{l}}}\\exp\\left(-\\frac{1}{2}n_{l}\\tilde{\\Delta}_{l}^{2}\\right)}\\\\ {=}&{\\left[2K\\left(\\tilde{\\Delta}_{l}+2L_{a}h\\beta_{e}^{\\alpha}\\!+\\!1\\right)+2K\\sqrt{\\frac{2\\pi}{n_{l}}}\\right]\\exp\\left(-\\frac{1}{2}n_{l}\\tilde{\\Delta}_{l}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "540 Take expectation over $\\mathbf{X}_{1:N}$ , we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{B_{l}}({\\eta}^{*}({\\bf x})-{\\mathbb E}[\\eta_{c_{l}}({\\bf x})])f({\\bf x})d{\\bf x}\\le p(B_{l})e^{-\\frac{1}{2}(1-\\ln2)N p(B_{l})}\\mathrm{~}}\\\\ {\\displaystyle+2K p(B_{l})\\left(\\Delta_{l}+2L_{d}h^{\\beta}+\\frac{e^{\\frac{\\epsilon}{2}}+1}{e^{\\frac{\\epsilon}{2}}-1}\\sqrt{\\frac{2\\pi}{N p(B_{l})}}\\right)\\exp\\left[-\\frac{1}{2}N p(B_{l})\\Delta_{l}^{2}\\left(\\frac{e^{\\frac{\\epsilon}{2}}-1}{e^{\\frac{\\epsilon}{2}}+1}\\right)^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "541 Define ", "page_idx": 16}, {"type": "equation", "text": "$$\na_{l}=\\left[3\\frac{e^{\\frac{\\epsilon}{2}}+1}{e^{\\frac{\\epsilon}{2}}-1}\\sqrt{\\frac{2\\ln(2K)}{c N h^{d}}}+2L_{d}h^{\\beta}\\right]p(B_{l}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "542 and ", "page_idx": 16}, {"type": "equation", "text": "$$\nb_{l}=2K p(B_{l})\\left(\\Delta_{l}+2L_{d}h^{\\beta}+\\frac{e^{\\frac{\\epsilon}{2}}+1}{e^{\\frac{\\epsilon}{2}}-1}\\sqrt{\\frac{2\\pi}{c N h^{d}}}\\right)\\exp\\left[-\\frac{1}{2}c N h^{d}\\Delta_{l}^{2}\\left(\\frac{e^{\\frac{\\epsilon}{2}}-1}{e^{\\frac{\\epsilon}{2}}+1}\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "543 From Assumption 1(c), $p(B_{l})\\geq c N h^{d}$ . Therefore, from (67) and (71) ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{R-R^{*}}}&{{\\leq}}&{{\\displaystyle\\sum_{l=1}^{G}\\Big[p(B_{l})e^{-\\frac{1}{2}(1-\\ln2)N p(B_{l})}+\\operatorname*{min}\\{a_{l},b_{l}\\}\\Big]}}\\\\ {{}}&{{\\leq}}&{{\\displaystyle e^{-\\frac{1}{2}(1-\\ln2)c N h^{d}}+\\sum_{l=1}^{G}\\operatorname*{min}\\{a_{l},b_{l}\\}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "544 It remains to bound $\\begin{array}{r}{\\sum_{l=1}^{G}\\operatorname*{min}\\{a_{l},b_{l}\\}}\\end{array}$ . Note that for all $\\textbf{x}\\in\\~B_{l}$ $B_{l},\\,\\eta^{\\ast}(\\mathbf{x})-\\eta_{s}(\\mathbf{x})\\leq\\,\\Delta_{l}+2L_{d}h^{\\beta}$   \n545 Thus ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{l:\\Delta_{l}\\leq u}p(B_{l})\\leq\\operatorname{P}\\left(\\eta^{*}(\\mathbf{X})-\\eta_{s}(\\mathbf{X})\\leq u+2L_{d}h^{\\beta}\\right)\\leq M(u+2L_{d}h^{\\beta})^{\\gamma}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "546 Let ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Delta_{0}=\\frac{e^{\\frac{\\epsilon}{2}}+1}{e^{\\frac{\\epsilon}{2}}-1}\\sqrt{\\frac{2\\ln(2K)}{c N h^{d}}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "547 and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l c l}{{{\\cal I}_{0}}}&{{=}}&{{\\{l|\\Delta_{l}\\leq\\Delta_{0}\\},}}\\\\ {{{\\cal I}_{k}}}&{{=}}&{{\\{l|2^{k-1}\\Delta_{0}<\\Delta_{l}\\leq2^{k}\\Delta_{0}\\},k=1,2,\\ldots.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "548 Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\operatorname*{min}_{l\\in{\\cal I}_{0}}\\{a_{l},b_{l}\\}}&{\\le}&{\\displaystyle\\sum_{l\\in{\\cal I}_{0}}a_{l}}\\\\ &{\\le}&{\\displaystyle\\left(\\sum_{l:\\Delta_{l}\\le\\Delta_{0}}p(B_{l})\\right)\\left[3\\frac{e^{\\frac{c^{\\frac{3}{2}}}{2}}+1}{e^{\\frac{-1}{2}}-1}\\sqrt{\\frac{2\\ln(2K)}{c N h^{d}}}+2L_{d}h^{\\beta}\\right]}\\\\ &{\\le}&{M(\\Delta_{0}+2L_{d}h^{\\beta})^{\\gamma}\\left[3\\frac{e^{\\frac{c^{\\frac{3}{2}}}{2}}+1}{e^{\\frac{-1}{2}}-1}\\sqrt{\\frac{2\\ln(2K)}{c N h^{d}}}+2L_{d}h^{\\beta}\\right]}\\\\ &{\\lesssim}&{\\displaystyle\\left(\\frac{1}{e^{2}\\wedge1}\\frac{\\ln K}{N h^{d}}\\right)^{\\frac{\\gamma+1}{2}}+h^{\\beta(\\gamma+1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "549 For $I_{k}$ with $k\\geq1$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\underset{\\in{\\cal I}_{k}}{\\mathrm{sim}}\\{a_{l},b_{l}\\}}&{\\leq}&{\\sum_{l\\in{\\cal I}_{k}}b_{l}}\\\\ &{\\leq}&{\\Bigg(\\displaystyle\\sum_{l:\\Delta_{l}\\leq2^{k}\\Delta_{0}}p(B_{l})\\Bigg)\\cdot2K\\left(2^{k}\\Delta_{0}+2L_{d}h^{\\beta}+\\Delta_{0}\\right)\\exp\\left[-\\frac{1}{2}\\left(\\frac{e^{\\frac{\\epsilon}{2}}-1}{e^{\\frac{\\epsilon}{2}}+1}\\right)^{2}c N h^{d}2^{2k}\\right.}\\\\ &{\\leq}&{M(2^{k}\\Delta_{0}+2L_{d}h^{\\beta})^{\\gamma}\\left((2^{k}+1)\\Delta_{0}+2L_{d}h^{\\beta}\\right)(2K)^{-2^{2k-2}+1}}\\\\ &{\\leq}&{M(\\Delta_{0}+2L_{d}h^{\\beta})^{\\gamma+1}2^{k\\gamma+k-2^{2k-2}+2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "550 It is obvious that there exists a finite constant $C^{\\prime}<\\infty$ that depends on $\\gamma$ , such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{\\infty}2^{k\\gamma+k-2^{2k-2}+2}\\leq C^{\\prime}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "551 Therefore ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{\\infty}\\sum_{l\\in I_{k}}\\operatorname*{min}\\{a_{l},b_{l}\\}\\lesssim\\left(\\frac{1}{\\epsilon^{2}\\wedge1}\\frac{\\ln K}{N h^{d}}\\right)^{\\frac{\\gamma+1}{2}}+h^{\\beta(\\gamma+1)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "552 Combine (74), (79) and (82), ", "page_idx": 17}, {"type": "equation", "text": "$$\nR-R^{*}\\lesssim\\left(\\frac{1}{\\epsilon^{2}\\wedge1}\\frac{\\ln K}{N h^{d}}\\right)^{\\frac{\\gamma+1}{2}}+h^{\\beta(\\gamma+1)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "553 To minimize the overall excess risk, let ", "page_idx": 17}, {"type": "equation", "text": "$$\nh\\sim\\left(\\frac{N(\\epsilon^{2}\\wedge1)}{\\ln K}\\right)^{-\\frac{1}{2\\beta+d}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "554 then ", "page_idx": 17}, {"type": "equation", "text": "$$\nR-R^{*}\\lesssim\\left(\\frac{N(\\epsilon^{2}\\wedge1)}{\\ln K}\\right)^{-\\frac{\\beta(\\gamma+1)}{2\\beta+d}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "555 Compare to the simple random response method, the bin splitting avoids the polynomial decrease   \n556 over $K$ . ", "page_idx": 17}, {"type": "text", "text": "557 D Proof of Theorem 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "558 We still divide the support as the local label DP setting, except that the value of $h$ is different, which   \n559 will be specified later in this section. Note that (42) still holds here. Let $\\mathbf{V}$ takes values from   \n560 $\\{-1,1\\}^{\\bar{m}}$ randomly with equal probability, and $V_{k}$ is the $k$ -th element. Then $\\eta_{\\bf V}({\\bf x})$ is a random   \n561 function. The corresponding random output of hypothesis testing is denoted as $\\hat{V}_{k}$ , which is calculated   \n562 by (38). Then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\operatorname*{inf}_{A\\in A_{\\epsilon}(f,\\eta)\\in\\mathcal{F}_{c l s}}(R-R^{*})}&{\\ge}&{\\displaystyle\\frac{1}{2}c h^{\\beta+d}\\left\\|\\phi\\right\\|_{1}\\operatorname*{inf}_{A\\in A_{\\epsilon}\\,\\mathbf{v}\\in\\mathcal{V}}\\displaystyle\\operatorname*{max}_{k=1}^{m}\\mathbf{P}(\\hat{v}_{k}\\neq v_{k})}\\\\ &{\\ge}&{\\displaystyle\\frac{1}{2}h^{\\beta+d}\\left\\|\\phi\\right\\|_{1}\\operatorname*{inf}_{A\\in A_{\\epsilon}}\\displaystyle\\sum_{k=1}^{m}\\mathbf{P}(\\hat{V}_{k}\\neq V_{k})}\\\\ &{=}&{\\displaystyle\\frac{1}{2}h^{\\beta+d}\\left\\|\\phi\\right\\|_{1}\\sum_{k=1}^{m}\\displaystyle\\operatorname*{inf}_{A\\in A_{\\epsilon}}\\mathbf{P}(\\hat{V}_{k}\\neq V_{k}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "563 in which the last step holds since $\\hat{V}_{k}$ for different $k$ are calculated independently. ", "page_idx": 18}, {"type": "text", "text": "564 It remains to give a lower bound of $\\mathsf{P}(\\hat{V}_{k}\\neq V_{k})$ . Denote $n_{k}$ as the number of samples falling in $B_{k}$ ,   \n565 $\\bar{Y}_{k}$ as the average label values in $B_{k}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{n_{k}}&{:=}&{\\displaystyle\\sum_{i=1}^{N}\\mathbf{1}(\\mathbf{X}_{i}\\in B_{k}),}\\\\ {\\bar{Y}_{k}}&{:=}&{\\displaystyle\\frac{1}{n_{k}}\\sum_{i=1}^{N}Y_{i}\\mathbf{1}(X_{i}\\in B_{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "566 Moreover, define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{a_{k}}&{:=}&{\\displaystyle\\frac{1}{n_{k}}\\sum_{i=1}^{N}|\\eta(\\mathbf{X}_{i})|\\mathbf{1}(\\mathbf{X}_{i}\\in B_{k})}\\\\ &{=}&{\\displaystyle\\frac{h^{\\beta}}{n_{k}}\\sum_{i=1}^{N}\\phi\\left(\\frac{\\mathbf{X}_{i}-\\mathbf{c}_{k}}{h}\\right)\\mathbf{1}(\\mathbf{X}_{i}\\in B_{k}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "567 in which the last step comes from (36). Then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\bar{Y}_{k}|{\\bf X}_{1:N},V_{k}]=V_{k}a_{k},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "568 in which $\\mathbf{X}_{1:N}$ means $\\mathbf{X}_{1},\\ldots,\\mathbf{X}_{N}$ . We then show the following lemma. ", "page_idx": 18}, {"type": "text", "text": "569 Lemma 1. I $\\mathrm{~\\dot{~}{0}~}\\leq t\\leq\\ln{2}/\\big(\\epsilon n_{k}\\big)$ , and $n_{k}t$ is an integer, then ", "page_idx": 18}, {"type": "equation", "text": "$$\nP(\\hat{V}_{k}=1|\\mathbf{X}_{1:N},\\bar{Y}_{k}=-t)+P(\\hat{V}_{k}=-1|\\mathbf{X}_{1:N},\\bar{Y}_{k}=t)\\geq\\frac{2}{3}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "570 Proof. Construct $D^{\\prime}$ by changing the label values of $l=n_{k}t$ items from these $n_{k}$ samples falling in   \n571 $B_{k}$ , from $-1$ to 1. Then the average label values in $B_{k}$ is denoted as $\\bar{Y}_{k}^{\\prime}$ after such replacement. $\\hat{V}_{k}$   \n572 also becomes $\\hat{V}_{k}^{\\prime}$ . Then from the $\\epsilon$ -label DP requirement, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}(\\hat{V}_{k}=1|\\mathbf{X}_{1:N},\\bar{Y}_{k}=-t)}&{\\overset{(a)}\\geq}&{e^{-l\\epsilon}\\mathbf{P}\\left(\\hat{V}_{k}^{\\prime}=1|\\mathbf{X}_{1:N},\\bar{Y}_{k}^{\\prime}=-t+\\frac{2l}{n_{k}}\\right)}\\\\ &{\\overset{(b)}\\geq}&{e^{-l\\epsilon}\\mathbf{P}\\left(\\hat{V}_{k}=1|\\mathbf{X}_{1:N},\\bar{Y}_{k}=-t+\\frac{2l}{n_{k}}\\right)}\\\\ &{\\overset{\\quad>}0}&{e^{-n_{k}t\\epsilon}\\left[1-\\mathbb{P}\\left(\\hat{V}_{k}=-1|\\mathbf{X}_{1:N},\\bar{Y}_{k}=-t+\\frac{2l}{n_{k}}\\right)\\right]}\\\\ &{\\geq}&{\\frac{1}{2}\\left[1-\\mathbb{P}\\left(\\hat{V}_{k}=-1|\\mathbf{X}_{1:N},\\bar{Y}_{k}=t\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "573 in which (a) uses the group privacy property. The Hamming distance between $D$ and $D^{\\prime}$ is $l$ , thus the   \n574 ratio of probability between $D$ and $D^{\\prime}$ is within $[e^{-l\\epsilon},e^{l\\epsilon}]$ . (b) holds because the algorithm does not   \n575 change after changing $D$ to $D^{\\prime}$ . Similarly, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{P}(\\hat{V}_{k}=-1|\\mathbf{X}_{1:N},\\bar{Y}_{k}=t)\\geq\\frac{1}{2}\\left[1-\\mathrm{P}\\left(\\hat{V}_{k}=1|\\mathbf{X}_{1:N},\\bar{Y}_{k}=-t\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "576 Then (91) can be shown by adding up (92) and (93). ", "page_idx": 19}, {"type": "text", "text": "577 Now we use Lemma 1 to bound the excess risk. With sufficiently large $n_{k}$ , $\\hat{Y}_{k}$ will be close to   \n578 Gaussian distribution with mean $a_{k}$ . To be more rigorous, by Berry-Esseen theorem [?], for some   \n579 absolute constant $C_{E}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{P}\\left({\\bar{Y}}_{k}\\leq a_{k}|\\mathbf{X}_{1:N},V_{k}=1\\right)\\geq{\\frac{1}{2}}-{\\frac{C_{E}}{\\sqrt{n_{k}}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "580 Similarly, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{P}\\left({\\bar{Y}}_{k}\\geq-a_{k}|\\mathbf{X}_{1:N},V_{k}=-1\\right)\\geq{\\frac{1}{2}}-{\\frac{C_{E}}{\\sqrt{n_{k}}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "581 We first analyze cubes with ", "page_idx": 19}, {"type": "equation", "text": "$$\nn_{k}>16C_{E}^{2},a_{k}<\\frac{\\ln2}{\\epsilon n_{k}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "582 Under condition (96), the right hand side of (94) and (95) are at least $1/4$ . Therefore ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{P}(\\hat{V}_{k}\\neq V_{k}|\\mathbf{X}_{1:N})}&{=}&{\\frac{1}{2}\\mathbf{P}(\\hat{V}_{k}=1|\\mathbf{X}_{1:N},V_{k}=-1)+\\frac{1}{2}\\mathbf{P}(\\hat{V}_{k}=-1|\\mathbf{X}_{1:N},V_{k}=1)}\\\\ &{\\geq}&{\\frac{1}{8}\\mathbf{P}\\left(\\hat{V}_{k}=1|\\mathbf{X}_{1:N},\\bar{Y}_{k}\\geq-\\frac{\\ln2}{\\epsilon n_{k}}\\right)+\\frac{1}{8}\\mathbf{P}\\left(\\hat{V}_{k}=-1|\\mathbf{X}_{1:N},\\bar{Y}_{k}\\leq\\frac{\\ln2}{\\epsilon n_{k}}\\right)}\\\\ &{\\geq}&{\\frac{1}{12}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "583 From (86), ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{A\\in{\\mathcal A}_{\\epsilon}(f,\\eta)\\in\\mathcal{F}_{c l s}}(R-R^{*})\\geq\\frac{1}{2}h^{\\beta+d}\\left\\|\\phi\\right\\|_{1}\\sum_{k=1}^{m}\\frac{1}{12}\\mathbb{P}\\left(a_{k}<\\frac{\\ln2}{\\epsilon n_{k}},n_{k}>16C_{E}^{2}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "584 From (35), (89) and (87), $a_{k}\\le h^{\\beta}$ . Therefore ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{A\\in{\\mathcal A}_{\\epsilon}(f,\\eta)\\in{\\mathcal F}_{c l s}}\\!\\left(R-R^{*}\\right)\\geq\\frac{1}{24}h^{\\beta+d}\\left\\|\\phi\\right\\|_{1}\\sum_{k=1}^{m}{\\mathbb P}\\left(16C_{E}^{2}<n_{k}<\\frac{\\ln2}{\\epsilon h^{\\beta}}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "585 Recall that each cube has probability mass $c h^{d}$ . Select $h$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n2N c h^{d}=\\frac{\\ln2}{\\epsilon h^{\\beta}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "586 From Chernoff inequality, $16C_{E}^{2}<n_{k}<\\ln{2}/(\\epsilon h^{\\beta})$ holds with high probability. (100) yields ", "page_idx": 19}, {"type": "equation", "text": "$$\nh\\sim(\\epsilon N)^{-{\\frac{1}{d+\\beta}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "587 Recall the bound of $m$ in (37). Let $m\\sim h^{\\gamma\\beta-d}$ , then (99) becomes ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\operatorname*{inf}_{A\\in{\\mathcal{A}}_{\\epsilon}(f,\\eta)\\in{\\mathcal{F}}_{c l s}}(R-R^{*})}&{\\gtrsim}&{h^{\\beta(\\gamma+1)}}\\\\ &&{\\gtrsim}&{(\\epsilon N)^{-\\frac{\\beta(\\gamma+1)}{d+\\beta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "588 Moreover, the standard lower bound for classification [28] is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{A\\in{\\mathcal{A}}_{\\epsilon}}\\operatorname*{sup}_{(f,\\eta)\\in{\\mathcal{F}}_{c l s}}(R-R^{*})\\gtrsim N^{-\\frac{\\beta(\\gamma+1)}{2\\beta+d}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "589 Therefore ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{A\\in{\\mathcal A}_{\\epsilon}}\\operatorname*{sup}_{(f,\\eta)\\in{\\mathcal F}_{c l s}}(R-R^{*})\\;\\;\\;\\gtrsim\\;\\;\\;N^{-\\frac{\\beta(\\gamma+1)}{2\\beta+d}}+(\\epsilon N)^{-\\frac{\\beta(\\gamma+1)}{d+\\beta}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "590 E Proof of Theorem 4 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "591 Denote ", "page_idx": 20}, {"type": "text", "text": "592 ", "page_idx": 20}, {"type": "equation", "text": "$$\nn_{l}^{*}=\\operatorname*{max}_{j}n_{l j},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\nn_{l}:=\\sum_{j=1}^{K}n_{l j}=\\sum_{i=1}^{N}\\mathbf{1}(\\mathbf{X}_{i}\\in B_{l}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "593 For all $j$ such that $n_{l}^{*}-n_{l j}>t$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{P}(c_{l}=j|\\mathbf{X}_{1:N},Y_{1:N})}&{=}&{\\displaystyle\\frac{e^{\\epsilon n_{l j}/2}}{\\sum_{k=1}^{K}e^{\\epsilon n_{l k}/2}}}\\\\ &{\\leq}&{\\displaystyle\\frac{e^{\\epsilon n_{l}^{*}/2}}{\\sum_{k=1}^{K}e^{\\epsilon n_{l k}/2}}e^{-\\frac{1}{2}\\epsilon t}}\\\\ &{\\leq}&{e^{-\\frac{1}{2}\\epsilon t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "594 Therefore ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathsf{P}(n_{l}^{*}-n_{l c_{l}}>t)=\\sum_{j:n_{l}^{*}-n_{l j}>t}\\mathsf{P}(c_{l}=j|\\mathbf{X}_{1:N},Y_{1:N})\\le K e^{-\\frac{1}{2}\\epsilon t}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "595 Hence ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}[n_{l}^{*}-n_{l c_{l}}]}&{=}&{\\displaystyle\\int_{0}^{\\infty}\\mathbb{P}(n_{l}^{*}-n_{l j}>t)d t}\\\\ &{\\leq}&{\\displaystyle\\int_{0}^{2\\ln K/\\epsilon}1d t+\\int_{2\\ln K/\\epsilon}^{\\infty}K e^{-\\frac{1}{2}\\epsilon t}d t}\\\\ &{=}&{\\displaystyle\\frac{2}{\\epsilon}(\\ln K+1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "596 Define ", "page_idx": 20}, {"type": "equation", "text": "$$\nv_{l j}=\\frac{1}{n_{l}}\\sum_{i=1}^{N}\\mathbf{1}(\\mathbf{X}_{i}\\in B_{l})\\eta_{j}(\\mathbf{X}_{i}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "597 then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[n_{l j}|\\mathbf{X}_{1:N}]=n_{l}v_{l j}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "598 From Hoeffding\u2019s inequality, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{P}(|n_{l j}-n_{l}v_{l j}|>t)\\le2e^{-\\frac{1}{2n_{l}}t^{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "599 Thus ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\mathbb{E}\\left[\\operatorname*{max}_{j}|n_{l j}-n_{l}v_{l j}|\\right]}&{=}&{\\displaystyle\\int_{0}^{\\infty}\\mathbf{P}\\left(\\cup_{j=1}^{K}\\left\\{|n_{l j}-n_{l}v_{l j}|>t\\right\\}\\right)d t}\\\\ &{\\leq}&{\\displaystyle\\int_{0}^{\\infty}\\operatorname*{min}\\left(1,2K e^{-\\frac{1}{2n_{l}}t^{2}}\\right)d t}\\\\ &{=}&{\\sqrt{2n_{l}\\ln(2K)}+\\displaystyle\\int_{\\sqrt{2n_{l}\\ln(2K)}}^{\\infty}2K e^{-\\frac{1}{2n_{l}}t^{2}}d t}\\\\ &{<}&{2\\sqrt{2n_{l}\\ln(2K)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "600 in which the last step uses the inequalit $\\begin{array}{r}{\\int_{t}^{\\infty}e^{-u^{2}/(2\\sigma^{2})}d u\\le\\sqrt{2\\pi}\\sigma e^{-t^{2}/(2\\sigma^{2})}}\\end{array}$ . Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\mathbb{E}[\\boldsymbol{v}_{l}^{*}-\\boldsymbol{v}_{l c_{l}}|\\mathbf{X}_{1:N}]}&{=}&{\\displaystyle\\frac{1}{n_{l}}\\mathbb{E}[n_{l}\\boldsymbol{v}_{l}^{*}-n_{l}\\boldsymbol{v}_{l c_{l}}]}\\\\ &{=}&{\\displaystyle\\frac{1}{n_{l}}\\mathbb{E}\\left[n_{l}^{*}-n_{l c_{l}}+n_{l}\\boldsymbol{v}_{l}^{*}-n_{l}^{*}+n_{l c_{l}}-n_{l}\\boldsymbol{v}_{l c_{l}}\\right]}\\\\ &{\\le}&{\\displaystyle\\frac{1}{n_{l}}\\mathbb{E}[n_{l}^{*}-n_{l c_{l}}]+\\displaystyle\\frac{2}{n_{l}}\\mathbb{E}\\left[\\operatorname*{max}|n_{l j}-n_{l}\\boldsymbol{v}_{l j}|\\right]}\\\\ &{\\le}&{\\displaystyle\\frac{2}{\\epsilon n_{l}}(\\ln K+1)+4\\sqrt{\\frac{2\\ln(2K)}{n_{l}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "601 By H\u00f6lder continuity assumption (Assumption 1(a)), for $\\mathbf{x}\\in B_{l}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n|v_{l j}-\\eta_{j}(\\mathbf{x})|\\leq\\frac{1}{n_{l}}\\sum_{i=1}^{N}\\mathbf{1}(\\mathbf{X}_{i}\\in B_{l})|\\eta_{j}(\\mathbf{X}_{i})-\\eta_{j}(\\mathbf{x})|\\leq L_{d}h^{\\beta},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "602 in which $L_{d}=L{\\sqrt{d}}$ , $L$ is the constant in Assumption 1(a). Thus ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\eta^{*}(\\mathbf{x})-\\eta_{c l}(\\mathbf{x})\\vert\\mathbf{X}_{1:N}]\\leq\\frac{2}{\\epsilon n_{l}}(\\ln K+1)+4\\sqrt{\\frac{2\\ln(2K)}{n_{l}}}+2L_{d}h^{\\beta}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "603 Now take integration over $B_{l}$ . ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{B_{l}}\\left(\\eta^{*}(\\mathbf{x})-\\mathbb{E}[\\eta_{c_{l}}(\\mathbf{x})]\\right)f(\\mathbf{x})d\\mathbf{x}}\\\\ {\\leq}&{\\displaystyle\\mathbb{P}\\left(n_{l}<\\frac{1}{2}N p(B_{l})\\right)\\int_{B_{l}}\\left(\\eta^{*}(\\mathbf{x})-\\mathbb{E}\\left[\\eta_{c_{l}}(\\mathbf{x})|n_{l}<\\frac{1}{2}N p(B_{l})\\right]\\right)f(\\mathbf{x})d\\mathbf{x}}\\\\ &{\\displaystyle+\\int_{B_{l}}\\left(\\eta^{*}(\\mathbf{x})-\\mathbb{E}\\left[\\eta_{c_{l}}(\\mathbf{x})|n_{l}\\geq\\frac{1}{2}N p(B_{l})\\right]\\right)f(\\mathbf{x})d\\mathbf{x}}\\\\ {\\leq}&{p(B_{l})\\exp\\left[-\\frac{1}{2}(1-\\ln2)N p(B_{l})\\right]+\\left[\\frac{2(\\ln K+1)}{\\epsilon N p(B_{l})}+4\\sqrt{\\frac{2\\ln(2K)}{N p(B_{l})}}+2L_{d}h^{\\beta}\\right]p(B_{l}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "604 in which $\\begin{array}{r}{p(B_{l})=\\mathrm{P}(\\mathbf{X}\\in B_{l})=\\int_{B_{l}}f(\\mathbf{x})d\\mathbf{x}.}\\end{array}$ (117) is the central label DP counterpart of (67). The   \n605 remainder of the proof follows arguments of the local label DP. We omit detailed steps. The result is ", "page_idx": 21}, {"type": "equation", "text": "$$\nR-R^{*}\\lesssim\\left(\\frac{\\ln K}{\\epsilon N h^{d}}+\\sqrt{\\frac{\\ln K}{N h^{d}}}+h^{\\beta}\\right)^{\\gamma+1}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "606 Let ", "page_idx": 21}, {"type": "equation", "text": "$$\nh\\sim\\left(\\frac{\\ln K}{\\epsilon N}\\right)^{\\frac{1}{\\beta+d}}+\\left(\\frac{\\ln K}{N}\\right)^{\\frac{1}{2\\beta+d}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "607 then ", "page_idx": 21}, {"type": "equation", "text": "$$\nR-R^{*}\\lesssim\\left(\\frac{\\ln K}{\\epsilon N}\\right)^{\\frac{\\beta(\\gamma+1)}{\\beta+d}}+\\left(\\frac{\\ln K}{N}\\right)^{\\frac{\\beta(\\gamma+1)}{2\\beta+d}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "608 The proof is complete. ", "page_idx": 21}, {"type": "text", "text": "609 F Proof of Theorem 5 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "610 Find $G$ cubes in the support and the length of each cube is $h$ . Let $\\phi(\\mathbf{u})$ be the same as the classification   \n611 case shown in appendix B. For $\\mathbf{v}\\in\\bar{\\nu}:=\\{-1,1\\}^{G}$ , let ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\eta_{\\mathbf{v}}(\\mathbf{x})=\\sum_{k=1}^{K}v_{k}\\phi\\left({\\frac{\\mathbf{x}-\\mathbf{c}_{k}}{h}}\\right)h^{\\beta}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "612 Let $\\mathrm{P}(Y=1|\\mathbf{x})=(1+\\eta_{\\mathbf{v}}(\\mathbf{x}))/2,\\mathrm{P}(Y=-1|\\mathbf{x})=(1-\\eta_{\\mathbf{v}}(\\mathbf{x}))$ , then $\\eta(\\mathbf{x})=\\mathbb{E}[Y|\\mathbf{x}]=\\eta_{\\mathbf{v}}(\\mathbf{x})$ . ", "page_idx": 21}, {"type": "text", "text": "613 The overall volume of the support is bounded. Thus, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nG\\leq C_{G}h^{-d}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "614 for some constant $C_{G}$ ", "page_idx": 21}, {"type": "text", "text": "615 Denote ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{v}_{k}=\\mathrm{sign}\\left(\\int_{B_{k}}\\hat{\\eta}(\\mathbf{x})\\phi\\left(\\frac{\\mathbf{x}-\\mathbf{c}_{k}}{h}\\right)f(\\mathbf{x})d\\mathbf{x}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "616 then the excess risk is bounded by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r c l}{R}&{=}&{\\mathbb{E}\\left[\\left(\\hat{\\eta}(\\mathbf{X})-\\eta_{\\mathbf{v}}(\\mathbf{X})\\right)^{2}\\right]}\\\\ &{=}&{\\displaystyle\\sum_{k=1}^{K}\\int_{B_{k}}\\mathbb{E}\\left[(\\hat{\\eta}(\\mathbf{x})-\\eta_{\\mathbf{v}}(\\mathbf{x}))^{2}\\right]f(\\mathbf{x})d\\mathbf{x}.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "617 If $\\hat{v}_{k}\\neq v_{k}$ , from (123), ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\int_{B_{k}}\\left(\\hat{\\eta}(\\mathbf{x})-v_{k}\\phi\\left(\\frac{\\mathbf{x}-\\mathbf{c}_{k}}{h}\\right)h^{\\beta}\\right)^{2}f(\\mathbf{x})d\\mathbf{x}\\geq\\int_{B_{k}}\\left(\\hat{\\eta}(\\mathbf{x})+v_{k}\\phi\\left(\\frac{\\mathbf{x}-\\mathbf{c}_{k}}{h}\\right)h^{\\beta}\\right)^{2}f(\\mathbf{x})d\\mathbf{x}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "618 Therefore, if $\\hat{v}_{k}\\neq v_{k}$ , then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\int_{B_{k}}\\left(\\hat{\\eta}(\\mathbf{x})-\\eta_{\\mathbf{v}}(\\mathbf{x})\\right)^{2}d\\mathbf{x}\\ge\\frac{1}{2}\\int_{B_{k}}\\phi^{2}\\left(\\frac{\\mathbf{x}-\\mathbf{c}_{k}}{h}\\right)h^{2\\beta}f(\\mathbf{x})d\\mathbf{x}=\\frac{1}{2}c h^{2\\beta+d}\\left\\|\\phi\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "619 Therefore ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{R-R^{*}}}&{{\\geq}}&{{\\mathbb{E}\\left[\\frac{1}{2}c h^{2\\beta+d}\\left\\|\\phi\\right\\|_{2}^{2}{\\bf1}(\\hat{v}_{k}\\neq v_{k})\\right]}}\\\\ {{}}&{{=}}&{{\\frac{1}{2}c h^{2\\beta+d}\\left\\|\\phi\\right\\|_{2}^{2}\\mathbb{E}[\\rho_{H}(\\hat{\\bf v},{\\bf v})].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "620 Similar to the classification problem analyzed in Appendix B, let ", "page_idx": 22}, {"type": "equation", "text": "$$\nh\\sim\\left(N(\\epsilon\\wedge1)^{2}\\right)^{-\\frac{1}{2\\beta+d}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "621 then $\\delta\\lesssim1$ , and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{\\mathbf{v}}}\\ \\operatorname*{sup}_{M\\in\\mathcal{M}_{\\epsilon}}\\operatorname*{max}_{\\mathbf{v}\\in\\mathcal{V}}\\mathbb{E}\\big[\\rho_{H}(\\hat{\\mathbf{v}},\\mathbf{v})\\big]\\gtrsim G\\sim h^{-d}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "622 Thus ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{\\eta}}\\operatorname*{inf}_{M\\in\\mathcal{M}_{\\epsilon}P_{X,Y}\\in\\mathcal{F}_{r e g1}}R\\gtrsim h^{2\\eta+d}h^{-d}\\sim h^{2\\beta}\\sim(N(\\epsilon\\wedge1)^{2})^{-\\frac{2\\beta}{2\\beta+d}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "623 G Proof of Theorem 6 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "624 According to Assumption 2, $|Y|<T$ with probability 1, thus $\\mathrm{Var}[Y|\\mathbf{x}]\\le T^{2}$ for any $\\mathbf{x}$ . A Laplacian   \n625 distribution with parameter $\\lambda$ has variance $2\\lambda^{2}$ , thus ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname{Var}[W]=2\\lambda^{2}=2\\left({\\frac{2T}{\\epsilon}}\\right)^{2}={\\frac{8T^{2}}{\\epsilon^{2}}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "626 Hence ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname{Var}[Z]=\\operatorname{Var}[Y]+\\operatorname{Var}[W]\\leq T^{2}\\left(1+{\\frac{8}{\\epsilon^{2}}}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "627 Now we analyze the bias first. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\hat{\\eta}(\\mathbf{x})]=\\mathbb{E}\\left[\\frac{1}{k}\\sum_{i\\in\\mathcal{N}_{k}(\\mathbf{x})}Z_{i}\\right]=\\mathbb{E}\\left[\\frac{1}{k}\\sum_{i\\in\\mathcal{N}_{k}(\\mathbf{x})}\\eta(\\mathbf{X}_{i})\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "628 Thus ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}[\\hat{\\eta}(\\mathbf{x})]-\\eta(\\mathbf{x})\\Bigm|}&{\\leq\\ R\\left[\\frac{1}{k}\\sum_{i\\in\\mathcal{N}_{\\mathbf{A}}}\\left|\\eta(\\mathbf{X}_{i})-\\eta(\\mathbf{x})\\right|\\right]}\\\\ &{\\leq\\ R\\left[\\frac{1}{k}\\sum_{i\\in\\mathcal{N}_{\\mathbf{A}}}\\mathrm{min}\\left\\{L\\left\\|\\mathbf{X}_{i}-\\mathbf{x}\\right\\|^{\\beta},2\\pi T\\right\\}\\right]}\\\\ &{\\leq\\ R\\left[\\frac{1}{k}\\sum_{i\\in\\mathcal{N}_{\\mathbf{A}}}\\mathrm{min}\\left\\{L\\rho^{\\beta}(\\mathbf{x}),2T\\right\\}\\right]}\\\\ &{\\leq\\ 2T\\mathbb{P}(\\rho|\\mathbf{x})>r_{0})+L r_{0}^{\\beta}}\\\\ &{\\leq\\ 2T e^{-(1-\\ln2)k}+L\\left(\\frac{2k}{N\\pi\\alpha_{0}\\beta}\\right)^{\\frac{\\beta}{2}}}\\\\ &{\\leq\\ C_{1}\\left(\\frac{k}{N}\\right)^{\\frac{\\beta}{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "629 for some constant $C_{1}$ . ", "page_idx": 23}, {"type": "text", "text": "630 It remains to bound the variance. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{Var}[\\hat{\\eta}(\\mathbf{x})]=\\mathbb{E}\\left[\\mathrm{Var}\\left[\\hat{\\eta}(\\mathbf{x})|\\mathbf{X}_{1},\\ldots,\\mathbf{X}_{N}\\right]\\right]+\\mathrm{Var}[\\mathbb{E}[\\hat{\\eta}(\\mathbf{x})]|\\mathbf{X}_{1},\\ldots,\\mathbf{X}_{N}].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "631 For the first term in (135), ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l c l}{\\operatorname{Var}[\\hat{\\eta}(\\mathbf{x})|\\mathbf{X}_{1},\\dots,\\mathbf{X}_{N}]}&{=}&{\\operatorname{Var}\\left[\\displaystyle\\frac{1}{k}\\sum_{i\\in\\mathcal{N}_{k}(\\mathbf{x})}Z_{i}|\\mathbf{X}_{1},\\dots,\\mathbf{X}_{N}\\right]}\\\\ &{=}&{\\displaystyle\\frac{1}{k^{2}}\\sum_{i\\in\\mathcal{N}_{k}(\\mathbf{x})}\\operatorname{Var}[Z_{i}|\\mathbf{X}_{1},\\dots,\\mathbf{X}_{N}]}\\\\ &{\\leq}&{\\displaystyle\\frac{1}{k}T^{2}\\left(1+\\frac{8}{\\epsilon^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "632 For the second term in (135), ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathrm{Var}[\\mathbb{E}|\\hat{n}(\\mathbf{x})|\\mathbf{X}_{1},\\dots,\\mathbf{X}_{N}]\\,}&{=}&{\\mathrm{Var}\\left[\\frac{1}{k}\\sum_{i\\in\\mathcal{N}_{i}\\setminus\\{\\mathbf{x}\\}}\\eta(\\mathbf{X}_{i})\\right]}\\\\ &{\\le}&{\\mathbb{E}\\left[\\left(\\frac{1}{k}\\sum_{i\\in\\mathcal{N}_{i}\\setminus\\{\\mathbf{x}\\}}\\eta(\\mathbf{X}_{i})-\\eta(\\mathbf{x})\\right)^{2}\\right]}\\\\ &{=}&{\\frac{1}{k}\\sum_{i\\in\\mathcal{N}_{i}\\setminus\\{\\mathbf{x}\\}}\\mathbb{E}\\left[\\eta(\\mathbf{X}_{i})-\\eta(\\mathbf{x})^{2}\\right]}\\\\ &{\\le}&{\\frac{1}{k}\\sum_{i\\in\\mathcal{N}_{i}\\setminus\\{\\mathbf{x}\\}}\\mathrm{E}\\left[\\mathrm{min}\\left\\{L^{2}\\left\\lVert\\mathbf{X}_{i}-\\mathbf{x}\\right\\rVert^{2s},\\,4\\mathbb{T}^{2}\\right\\}\\right]}\\\\ &{\\le}&{4T^{2}e^{-(1-\\mathrm{i}-2\\mathrm{i})k}+L^{2}r_{0}^{2s}}\\\\ &{\\le}&{C_{1}^{2}\\left(\\frac{k}{N}\\right)^{\\frac{s_{1}}{2s}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "633 Therefore (135) becomes ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{Var}[\\hat{\\eta}({\\bf x})]\\leq\\frac{1}{k}T^{2}\\left(1+\\frac{8}{\\epsilon^{2}}\\right)+C_{1}^{2}\\left(\\frac{k}{N}\\right)^{\\frac{2\\beta}{d}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "634 Combine the analysis of bias and variance, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}[(\\hat{\\eta}(\\mathbf{x})-\\eta(\\mathbf{x}))^{2}]\\le\\frac{1}{k}T^{2}\\left(1+\\frac{8}{\\epsilon^{2}}\\right)+2C_{1}^{2}\\left(\\frac{k}{N}\\right)^{\\frac{2\\beta}{d}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "635 Therefore the overall risk is bounded by ", "page_idx": 24}, {"type": "equation", "text": "$$\nR=\\mathbb{E}[(\\widehat{\\eta}(\\mathbf{X})-\\eta(\\mathbf{X}))^{2}]\\lesssim\\frac{1}{k}T^{2}\\left(1+\\frac{8}{\\epsilon^{2}}\\right)+2C_{1}^{2}\\left(\\frac{k}{N}\\right)^{\\frac{2\\beta}{d}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "636 The optimal growth rate of $k$ over $N$ is ", "page_idx": 24}, {"type": "equation", "text": "$$\nk\\sim N^{\\frac{2\\beta}{d+2\\beta}}(\\epsilon\\wedge1)^{-\\frac{2d}{d+2\\beta}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "637 Then the convergence rate of the overall risk becomes ", "page_idx": 24}, {"type": "equation", "text": "$$\nR\\lesssim(N(\\epsilon\\wedge1)^{2})^{-\\frac{2\\beta}{d+2\\beta}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "638 H Proof of Theorem 7 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "639 From (127), ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{R-R^{*}}}&{{\\displaystyle\\geq}}&{{\\displaystyle\\frac{1}{2}c h^{2\\beta+d}\\left\\|\\phi\\right\\|_{2}^{2}\\mathbb{E}[\\rho_{H}(\\hat{\\mathbf{V}},\\mathbf{V})]}}\\\\ {{}}&{{\\displaystyle=}}&{{\\displaystyle\\frac{1}{2}c h^{2\\beta+d}\\left\\|\\phi\\right\\|_{2}^{2}\\sum_{k=1}^{G}\\mathbf{P}(\\hat{V}_{k}\\neq V_{k}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "640 Follow the analysis of lower bounds of classification in Appendix $\\mathrm{D}$ , let $h$ scales as (101), then   \n641 $\\mathsf{P}(\\hat{V}_{k}\\neq V_{k})\\gtrsim1$ . Moreover, $G\\sim h^{-d}$ . Hence ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{A\\in{\\cal A}_{\\epsilon}}\\operatorname*{sup}_{(f,\\eta)\\in{\\cal F}_{r e g1}}(R-R^{*})\\gtrsim h^{2\\beta}\\sim(\\epsilon N)^{-\\frac{2\\beta}{d+\\beta}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "642 Moreover, note that the non-private lower bound of regression is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{A\\in{\\mathcal{A}}_{\\epsilon}(f,\\eta)\\in{\\mathcal{F}}_{r e g1}}(R-R^{*})\\gtrsim N^{-\\frac{2\\beta}{2\\beta+d}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "643 Combine (144) and (145), ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\mathcal{A}\\in A_{\\epsilon}(f,\\eta)\\in\\mathcal{F}_{r e g1}}(R-R^{*})\\gtrsim N^{-\\frac{2\\beta}{2\\beta+d}}+(\\epsilon N)^{-\\frac{2\\beta}{d+\\beta}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "644 I Proof of Theorem 8 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "645 1) Analysis of bias. Note that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\hat{\\eta}_{l}|\\mathbf{X}_{1:N}]=\\mathbb{E}[Y|\\mathbf{X}\\in B_{l}]=\\frac{1}{p(B_{l})}\\int\\eta(\\mathbf{u})f(\\mathbf{u})d\\mathbf{u}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "646 Therefore, for all $\\mathbf{x}\\in B_{l}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{|\\mathbb{E}[\\hat{\\eta}_{l}|\\mathbf{X}_{1:N}]-\\eta(\\mathbf{x})|}&{\\leq}&{\\displaystyle\\frac{1}{p(B_{l})}\\int|\\eta(\\mathbf{u})-\\eta(\\mathbf{x})|f(\\mathbf{u})d\\mathbf{u}}\\\\ &{\\leq}&{L_{d}h^{\\beta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "647 Therefore for all $\\mathbf{x}\\in B_{l}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n|\\mathbb{E}[\\hat{\\eta}_{l}]-\\eta(\\mathbf{x})|\\leq L_{d}h^{\\beta}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "648 2) Analysis of variance. If $n_{l}>0$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{Var}\\left[\\frac{1}{n_{l}}\\sum_{i=1}^{N}\\mathbf{1}(\\mathbf{X}_{i}\\in B_{l})Y_{i}|\\mathbf{X}_{1:N}\\right]=\\frac{1}{n_{l}}\\,\\mathrm{Var}[Y|\\mathbf{X}\\in B_{l}]\\leq\\frac{1}{n_{l}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "649 Therefore ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l l}{\\mathrm{Var}\\left[\\frac{1}{n_{l}}\\displaystyle\\sum_{i=1}^{N}\\mathbf{1}(\\mathbf{X}_{i}\\in B_{l})Y_{i}\\right]}&{\\le}&{\\mathbb{P}\\left(n_{l}<\\frac{1}{2}N p(B_{l})\\right)+\\mathbb{P}\\left(n_{l}\\ge\\frac{1}{2}N p(B_{l})\\right)\\displaystyle\\frac{2}{N p(B_{l})}}\\\\ &{\\le}&{\\exp\\left[-\\frac{1}{2}(1-\\ln2)N p(B_{l})\\right]+\\displaystyle\\frac{2}{N c h^{d}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "650 Similarly, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathrm{Var}[W_{l}]}&{\\le}&{\\displaystyle\\mathbb{P}\\left(n_{l}<\\frac{1}{2}N p(B_{l})\\right)\\frac{1}{\\epsilon^{2}}+\\mathbb{P}\\left(n_{l}\\ge\\frac{1}{2}N p(B_{l})\\right)\\frac{8}{\\left(\\frac{1}{2}N p(B_{l})\\right)^{2}\\epsilon^{2}}}\\\\ &{\\lesssim}&{\\displaystyle\\frac{1}{N^{2}h^{2d}\\epsilon^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "651 The mean squared error can then be bounded by the bounds of bias and variance. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[(\\hat{\\eta}(\\mathbf{x})-\\eta(\\mathbf{x}))^{2}\\right]\\lesssim h^{2\\beta}+\\frac{1}{N h^{d}}+\\frac{1}{N^{2}h^{2d}\\epsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "652 Let ", "page_idx": 25}, {"type": "equation", "text": "$$\nh\\sim N^{-\\frac{1}{2\\beta+d}}+(\\epsilon N)^{-\\frac{1}{d+\\beta}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "653 Then ", "page_idx": 25}, {"type": "equation", "text": "$$\nR-R^{*}\\lesssim N^{-\\frac{2\\beta}{2\\beta+d}}+(\\epsilon N)^{-\\frac{2\\beta}{d+\\beta}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "654 J Proof of Theorem 9 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "655 Now we prove the minimax lower bound of nonparametric regression under label DP constraint. We   \n656 focus on the case in which $\\epsilon$ is small.   \n657 Similar to the steps of the proof of Theorem 5 in Appendix F, we find $B$ cubes in the support. The   \n658 definition of $\\eta_{\\mathbf{v}}$ , $\\hat{v}_{k}$ are also the same as (121) and (123). Compared with the case with bounded   \n659 noise, now $Y$ can take values in $\\mathbb{R}$ . ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "660 For given $\\mathbf{x}$ , let ", "page_idx": 25}, {"type": "equation", "text": "$$\nY=\\left\\{\\begin{array}{c c}{{T}}&{{\\mathrm{with~probability}}}&{{\\frac{1}{2}\\left(\\frac{M_{p}}{T^{p}}+\\frac{\\eta_{\\bf v}({\\bf x})}{T}\\right)}}\\\\ {{0}}&{{\\mathrm{with~probability}}}&{{1-\\frac{M_{p}}{T^{p}}}}\\\\ {{-T}}&{{\\mathrm{with~probability}}}&{{\\frac{1}{2}\\left(\\frac{M_{p}}{T^{p}}-\\frac{\\eta_{\\bf v}({\\bf x})}{T}\\right).}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "661 In Appendix $\\boldsymbol{\\mathrm{F}}$ about the case with bounded noise, $T$ is a fixed constant. However, here $T$ is not fixed   \n662 and will change over $N$ . It is straightforward to show that the distribution of $Y$ in (156) satisfies   \n663 Assumption 3: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[|Y|^{p}|\\mathbf{x}]=M_{p}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "664 Moreover, by taking expectation over $Y$ , it can be shown that $\\eta_{\\mathbf{v}}$ is still the regression function: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[Y|\\mathbf{x}]=\\eta_{\\mathbf{v}}(\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "665 Let ", "page_idx": 25}, {"type": "equation", "text": "$$\nT=\\left(\\frac{1}{2}M_{p}h^{-\\beta}\\right)^{\\frac{1}{p-1}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "666 Here we still define ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\delta=\\operatorname*{sup}_{M\\in\\mathcal{M}_{\\epsilon}{\\mathbf{v}},{\\mathbf{v}}^{\\prime}:\\rho_{H}({\\mathbf{v}},{\\mathbf{v}}^{\\prime})=1}D(P_{(X,Z)_{1:N}|{\\mathbf{v}}}||P_{(X,Z)_{1:N}|{\\mathbf{v}}^{\\prime}}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "667 Without loss of generality, suppose that $v_{1}=v_{1}^{\\prime}$ for $i\\neq1$ . Then ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle D(P_{(X,Z)_{1:N}|\\mathrm{v}}||P_{(X,Z)_{1:N}|\\mathrm{v}})}&{=}&{\\displaystyle N D(P_{X,Z|\\mathrm{v}}||P_{X,Z|\\mathrm{v}})}\\\\ &{=}&{\\displaystyle N\\int_{B_{1}}f(\\mathbf{x})D(P_{Z|\\mathrm{x}},||P_{Z|\\mathrm{X},\\mathrm{v}}|)d\\mathbf{x}}\\\\ &{\\le}&{\\displaystyle N\\int_{B_{1}}f(\\mathbf{x})(e^{\\epsilon}-1)^{2}\\mathbb{T}\\nabla^{2}\\left(P_{Z|X,\\mathrm{v}},P_{Z|X,\\mathrm{v}}\\right)d\\mathbf{x}}\\\\ &{=}&{\\displaystyle N\\int_{B_{1}}f(\\mathbf{x})(e^{\\epsilon}-1)^{2}\\eta_{\\mathrm{v}}^{2}(\\mathbf{x})\\frac{1}{T^{2}}d\\mathbf{x}}\\\\ &{=}&{\\displaystyle N(e^{\\epsilon}-1)^{2}\\frac{h^{2}\\beta^{2}}{T^{2}}\\int_{B_{1}}f(\\mathbf{x})\\phi^{2}\\left(\\frac{\\mathbf{x}-\\mathbf{c_{1}}}{h}\\right)d\\mathbf{x}}\\\\ &{=}&{\\displaystyle N(e^{\\epsilon}-1)^{2}h^{2}\\mathbb{A}^{2}\\lVert\\phi\\rVert_{\\mathbf{c}}^{2}T^{-2}}\\\\ &{=}&{\\displaystyle N(e^{\\epsilon}-1)^{2}\\lVert\\phi\\rVert_{2}^{2}\\left(\\frac{1}{2}M_{P}\\right)^{-\\frac{\\eta_{\\mathrm{v}}}{r}}h^{2\\beta+d\\frac{2\\delta}{r}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "668 Let ", "page_idx": 26}, {"type": "equation", "text": "$$\nh\\sim(N(e^{\\epsilon}-1)^{2})^{-\\frac{p-1}{2p\\beta+d(p-1)}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "669 then $\\delta\\lesssim1$ . Hence ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{\\eta}}\\operatorname*{inf}_{M\\in\\mathcal{M}_{\\epsilon}}\\operatorname*{sup}_{(f,\\eta)\\in\\mathcal{F}}R\\gtrsim h^{2\\beta}\\sim(N(e^{\\epsilon}-1)^{2})^{-\\frac{2\\beta(p-1)}{2p\\beta+d(p-1)}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "670 K Proof of Theorem 10 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "671 Define ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\eta_{T}(\\mathbf{x}):=\\mathbb{E}[Y_{T}|\\mathbf{x}].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "672 Then ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\hat{\\eta}(\\mathbf{x})-\\eta(\\mathbf{x})=\\eta_{T}(\\mathbf{x})-\\eta(\\mathbf{x})+\\mathbb{E}[\\hat{\\eta}(\\mathbf{x})]-\\eta_{T}(\\mathbf{x})+\\hat{\\eta}(\\mathbf{x})-\\mathbb{E}[\\hat{\\eta}(\\mathbf{x})].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "673 Therefore", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{E}\\left[(\\hat{\\eta}(\\mathbf{x})-\\eta(\\mathbf{x}))^{2}\\right]}&{\\leq}&{3(\\eta_{T}(\\mathbf{x})-\\eta(\\mathbf{x}))^{2}+3(\\mathbb{E}[\\hat{\\eta}(\\mathbf{x})]-\\eta_{T}(\\mathbf{x}))^{2}+3\\operatorname{Var}[\\hat{\\eta}(\\mathbf{x})]}\\\\ &{:=}&{3(I_{1}+I_{2}+I_{3}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "674 Now we bound $I_{1},I_{2}$ and $I_{3}$ separately. ", "page_idx": 26}, {"type": "text", "text": "675 Bound of $I_{1}$ . We show the following lemma (which will also be used later). Lemma 2. ", "page_idx": 26}, {"type": "equation", "text": "$$\n|\\eta_{T}(\\mathbf{x})-\\eta(\\mathbf{x})|\\leq\\frac{M_{p}}{p-1}T^{1-p}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "676 Proof. Firstly, we decompose $\\eta_{T}(\\mathbf{x})$ and $\\eta(\\mathbf{x})$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\eta_{T}(\\mathbf{x})=\\mathbb{E}[Y_{T}|\\mathbf{x}]=\\mathbb{E}[Y\\mathbf{1}(-T\\leq Y\\leq T)|\\mathbf{x}]+T\\mathbf{P}(Y>T|\\mathbf{x})-T P(Y<T|\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "677 ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\eta(\\mathbf{x})=\\mathbb{E}[Y|\\mathbf{x}]=\\mathbb{E}[Y\\mathbf{1}(-T\\leq Y\\leq T)|\\mathbf{x}]+\\mathbb{E}[Y\\mathbf{1}(Y>T)|\\mathbf{x}]-\\mathbb{E}[Y\\mathbf{1}(Y<T)|\\mathbf{x}].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "678 The first term is the same between (168) and (169). Therefore we only need to compare the second   \n679 and the third term. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\mathbb{E}[Y\\mathbf{1}(Y>T)|\\mathbf{x}]}&{=}&{\\displaystyle\\int_{0}^{T}\\mathbf{P}(Y>T|\\mathbf{x})d t+\\displaystyle\\int_{T}^{\\infty}\\mathbf{P}(Y>T|\\mathbf{x})d t}\\\\ &{\\le}&{T\\mathbf{P}(Y>T|\\mathbf{x})+\\displaystyle\\int_{T}^{\\infty}M_{p}t^{-p}d t}\\\\ &{=}&{T\\mathbf{P}(Y>T|\\mathbf{x})+\\displaystyle\\frac{M_{p}}{p-1}T^{1-p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "680 Therefore ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}[Y\\mathbf{1}(Y>T)|\\mathbf{x}]-T\\mathbf{P}(Y>T|\\mathbf{x})\\leq\\frac{M_{p}}{p-1}T^{1-p}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "681 Similarly, ", "page_idx": 27}, {"type": "equation", "text": "$$\nT P(Y<T|\\mathbf x)-\\mathbb{E}[Y\\mathbf{1}(Y<T)|\\mathbf x]\\leq\\frac{M_{p}}{p-1}T^{1-p}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "682 A Combination of these two inequalities yields the (167). ", "page_idx": 27}, {"type": "text", "text": "683 With Lemma 2, ", "page_idx": 27}, {"type": "equation", "text": "$$\nI_{1}\\leq\\frac{M_{p}^{2}}{(p-1)^{2}}T^{2(1-p)}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "684 Bound of $I_{2}$ . Follow the steps in (134), ", "page_idx": 27}, {"type": "equation", "text": "$$\nI_{2}\\leq C_{1}^{2}\\left(\\frac{k}{N}\\right)^{\\frac{2\\beta}{d}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "685 Bound of $I_{3}$ . We decompose $\\mathrm{Var}[\\hat{\\eta}(\\mathbf{x})]$ as following: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{Var}[\\hat{\\eta}(\\mathbf{x})]=\\mathbb{E}[\\mathrm{Var}[\\hat{\\eta}(\\mathbf{x})|\\mathbf{X}_{1},\\ldots,\\mathbf{X}_{N}]]+\\mathrm{Var}[\\mathbb{E}[\\hat{\\eta}(\\mathbf{x})|\\mathbf{X}_{1},\\ldots,\\mathbf{X}_{N}]].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "686 For the first term in (175), from Assumption $\\mathsf{i},\\mathbb{E}[|Y|^{p}|\\mathbf{x}]\\leq M_{p}$ . Since $p\\geq2$ , we have $\\mathbb{E}[Y^{2}|\\mathbf{x}]=$   \n687 $M_{p}^{\\frac{2}{p}}$ . Therefore ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname{Var}[Z_{i}|\\mathbf{X}_{1},\\dots,\\mathbf{X}_{N}]=\\operatorname{Var}[Y_{T}]+\\operatorname{Var}[W]\\leq M_{p}^{\\frac{2}{p}}+{\\frac{8T^{2}}{\\epsilon^{2}}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "688 Recall (20), we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{\\mathrm{Var}[\\hat{\\eta}({\\bf x})|\\mathbf{X}_{1},\\ldots,\\mathbf{X}_{N}]}&{=}&{\\displaystyle\\frac{1}{k^{2}}\\sum_{i\\in\\mathcal{N}_{k}({\\bf x})}\\mathrm{Var}[Z_{i}|\\mathbf{X}_{1},\\ldots,\\mathbf{X}_{N}]}\\\\ &{\\le}&{\\displaystyle\\frac{1}{k}\\left(M_{p}^{\\frac{2}{p}}+\\frac{8T^{2}}{\\epsilon^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "689 For the second term in (175), (137) still holds, thus ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{Var}[\\mathbb{E}[\\hat{\\eta}(\\mathbf{x})|\\mathbf{X}_{1},\\ldots,\\mathbf{X}_{N}]]\\leq C_{1}^{2}\\left(\\frac k N\\right)^{\\frac{2\\beta}d},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "690 and ", "page_idx": 27}, {"type": "equation", "text": "$$\nI_{3}\\leq\\frac{1}{k}\\left(M_{p}^{\\frac{2}{p}}+\\frac{8T^{2}}{\\epsilon^{2}}\\right)+C_{1}^{2}\\left(\\frac{k}{N}\\right)^{\\frac{2\\beta}{d}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "691 Plug (173), (174) and (179) into (166), and take expectations, we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{R}&{=}&{\\mathbb{E}[(\\hat{\\eta}(\\mathbf{X})-\\eta(\\mathbf{X}))^{2}]}\\\\ &&{\\lesssim}&{T^{2(1-p)}+\\displaystyle\\frac{1}{k}+\\frac{T^{2}}{k\\epsilon^{2}}+\\left(\\displaystyle\\frac{k}{N}\\right)^{\\frac{2\\beta}{d}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "692 Let ", "page_idx": 27}, {"type": "equation", "text": "$$\nT\\sim(k\\epsilon^{2})^{\\frac{1}{2p}},k\\sim(N\\epsilon^{2})^{\\frac{2p\\beta}{d(p-1)+2p\\beta}}\\,\\vee N^{\\frac{2\\beta}{2\\beta+d}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "693 then ", "page_idx": 27}, {"type": "equation", "text": "$$\nR\\lesssim(N\\epsilon^{2})^{-\\frac{2\\beta(p-1)}{d(p-1)+2p\\beta}}\\,\\vee\\,N^{-\\frac{2\\beta}{2\\beta+d}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "694 L Proof of Theorem 11 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "695 Let $Y$ be distributed as (156). Recall Lemma 1 for the problem of classification and regression with   \n696 bounded noise. ", "page_idx": 28}, {"type": "text", "text": "697 Now we show the corresponding lemma for regression with unbounded noise. ", "page_idx": 28}, {"type": "text", "text": "698 Lemma 3. If $0\\leq t\\leq T\\ln2/(\\epsilon n_{k}),$ , and $n_{k}t/T$ is an integer, then ", "page_idx": 28}, {"type": "equation", "text": "$$\nP(\\hat{V}_{k}=1|\\mathbf{X}_{1:N},\\bar{Y}_{k}=-t)+P(\\hat{V}_{k}=-1|\\mathbf{X}_{1:N},\\bar{Y}_{k}=t)\\geq\\frac{2}{3}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "699 Here we briefly explain the condition $n_{k}t$ is an integer. Recall the definition of $\\bar{Y}_{k}$ in (88). Now since   \n700 $Y$ take values in $\\{-T,0,T\\}$ , $n_{k}\\bar{Y}_{k}/T$ must be an integer. Therefore, in Lemma 3, we only need to   \n701 consider the case such that $n_{k}t/T$ is an integer.   \n702 Proof. The proof follows the proof of Lemma 1 closely. We provide the proof here for completeness.   \n703 Construct $D^{\\prime}$ by changing the label values of $l=n_{k}t/T$ items from these $n_{k}$ samples falling in $B_{k}$ ,   \n704 from $-T$ to $T$ . Then the average label values in $B_{k}$ is denoted as $\\bar{Y}_{k}^{\\prime}$ after such replacement. $\\hat{V}_{k}$ also   \n705 becomes $\\hat{V}_{k}^{\\prime}$ . Then from the $\\epsilon$ -label DP requirement, ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}(\\hat{V}_{k}=1|\\mathbf{X}_{1:N},\\bar{Y}_{k}=-t)}&{\\overset{(a)}\\geq}&{e^{-l\\epsilon}\\mathbf{P}\\left(\\hat{V}_{k}^{\\prime}=1|\\mathbf{X}_{1:N},\\bar{Y}_{k}^{\\prime}=-t+\\frac{2l}{n_{k}}\\right)}\\\\ &{\\overset{(b)}\\geq}&{e^{-l\\epsilon}\\mathbf{P}\\left(\\hat{V}_{k}=1|\\mathbf{X}_{1:N},\\bar{Y}_{k}=-t+\\frac{2l}{n_{k}}\\right)}\\\\ &{\\overset{\\quad>}0}&{e^{-n_{k}t\\epsilon}\\left[1-\\mathbb{P}\\left(\\hat{V}_{k}=-1|\\mathbf{X}_{1:N},\\bar{Y}_{k}=-t+\\frac{2l}{n_{k}}\\right)\\right]}\\\\ &{\\geq}&{\\frac{1}{2}\\left[1-\\mathbb{P}\\left(\\hat{V}_{k}=-1|\\mathbf{X}_{1:N},\\bar{Y}_{k}=t\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "706 in which (a) uses the group privacy property. The Hamming distance between $D$ and $D^{\\prime}$ is $l$ , thus the   \n707 ratio of probability between $D$ and $D^{\\prime}$ is within $[e^{-l\\epsilon},e^{l\\epsilon}]$ . (b) holds because the algorithm does not   \n708 change after changing $D$ to $D^{\\prime}$ . Similarly, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{P}(\\hat{V}_{k}=-1|\\mathbf{X}_{1:N},\\bar{Y}_{k}=t)\\geq\\frac{1}{2}\\left[1-\\mathrm{P}\\left(\\hat{V}_{k}=1|\\mathbf{X}_{1:N},\\bar{Y}_{k}=-t\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "709 Then (183) can be shown by adding up (184) and (185). ", "page_idx": 28}, {"type": "text", "text": "710 We then follow the proof of Theorem 3 in Appendix D. (101) becomes ", "page_idx": 28}, {"type": "equation", "text": "$$\nh\\sim\\left({\\frac{\\epsilon N}{T}}\\right)^{-{\\frac{1}{d+\\beta}}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "711 In (156), note that $\\operatorname{P}(Y=T)\\geq0$ and $\\mathsf{P}(Y=-T)\\,\\geq\\,0$ . Therefore $M_{p}/T^{p}\\,\\geq\\,\\eta_{\\mathbf{v}}(\\mathbf{x})/T$ . This   \n712 requires $h^{\\beta}T^{p-1}\\leq M_{p}$ . Let T \u223ch\u2212p\u22121 , then ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h\\sim(\\epsilon N)^{-\\frac{1}{d+\\beta}}h^{\\frac{\\beta}{(d+\\beta)(p-1)}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "713 i.e. ", "page_idx": 28}, {"type": "equation", "text": "$$\nh\\sim(\\epsilon N)^{-{\\frac{p-1}{p\\beta+d(p-1)}}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "714 Combine with standard minimax rate, the lower bound of regression with unbounded noise is ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{A\\in{\\mathcal A}_{\\epsilon}}\\operatorname*{sup}_{(f,\\eta)\\in{\\mathcal F}_{r e g2}}(R-R^{*})\\gtrsim N^{-\\frac{2\\beta}{2\\beta+d}}+(\\epsilon N)^{-\\frac{2\\beta(p-1)}{p\\beta+d(p-1)}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "715 M Proof of Theorem 12 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "716 1) Analysis of bias. Note that Lemma 2 still holds here. Moreover, recall (149). Therefore ", "page_idx": 29}, {"type": "equation", "text": "$$\n|\\mathbb{E}[\\hat{\\eta}_{l}]-\\eta(\\mathbf{x})|\\leq|\\mathbb{E}[\\hat{\\eta}_{l}-\\eta_{T}(\\mathbf{x})]|+|\\eta_{T}(\\mathbf{x})-\\eta(\\mathbf{x})|\\leq L_{d}h^{\\beta}+\\frac{M_{p}}{p-1}T^{1-p}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "717 2) Analysis of variance. Similar to (151), it can be shown that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathrm{Var}\\left[\\frac{1}{n_{l}}\\sum_{i=1}^{N}\\mathbf{1}(\\mathbf{X}_{i}\\in B_{l})Y_{i}\\right]\\lesssim\\frac{1}{N h^{d}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "718 Moreover, the noise variance can be bounded by ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathrm{Var}[W_{l}]\\lesssim\\frac{T^{2}}{N^{2}h^{2d}\\epsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "719 The mean squared error is then bounded by ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left(\\hat{\\eta}(\\mathbf{x})-\\eta(\\mathbf{x})\\right)^{2}\\right]\\lesssim h^{2\\beta}+T^{2(1-p)}+\\frac{T^{2}}{N^{2}h^{2d}\\epsilon^{2}}+\\frac{1}{N h^{d}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "720 Let $T\\sim(\\epsilon N h^{d})^{1/p}$ , then ", "page_idx": 29}, {"type": "equation", "text": "$$\nR-R^{*}=\\mathbb{E}\\left[(\\hat{\\eta}(\\mathbf{X})-\\eta(\\mathbf{X}))^{2}\\right]\\lesssim h^{2\\beta}+\\frac{1}{N h^{d}}+(\\epsilon N h^{d})^{-2(1-1/p)}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "721 To minimize (194), let ", "page_idx": 29}, {"type": "equation", "text": "$$\nh\\sim N^{-\\frac{1}{2\\beta+d}}+(\\epsilon N)^{-\\frac{p-1}{p\\beta+d(p-1)}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "722 then ", "page_idx": 29}, {"type": "equation", "text": "$$\nR-R^{*}\\lesssim N^{-\\frac{2\\beta}{2\\beta+d}}+(\\epsilon N)^{-\\frac{2\\beta(p-1)}{p\\beta+d(p-1)}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "723 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "724 1. Claims   \n725 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n726 paper\u2019s contributions and scope?   \n727 Answer: [Yes]   \n728 Justification: The main contribution (i.e. proposing a new Huber loss minimization approach   \n729 which is more suitable to realistic cases, and providing theoretical analysis) has been made   \n730 clear in the abstract and introduction.   \n731 Guidelines:   \n732 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n733 made in the paper.   \n734 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n735 contributions made in the paper and important assumptions and limitations. A No or   \n736 NA answer to this question will not be perceived well by the reviewers.   \n737 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n738 much the results can be expected to generalize to other settings.   \n739 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n740 are not attained by the paper.   \n741 2. Limitations   \n742 Question: Does the paper discuss the limitations of the work performed by the authors?   \n743 Answer: [Yes]   \n744 Justification: It is explained at the end of conclusion section.   \n745 Guidelines:   \n746 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n747 the paper has limitations, but those are not discussed in the paper.   \n748 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n749 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n750 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n751 model well-specification, asymptotic approximations only holding locally). The authors   \n752 should reflect on how these assumptions might be violated in practice and what the   \n753 implications would be.   \n754 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n755 only tested on a few datasets or with a few runs. In general, empirical results often   \n756 depend on implicit assumptions, which should be articulated.   \n757 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n758 For example, a facial recognition algorithm may perform poorly when image resolution   \n759 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n760 used reliably to provide closed captions for online lectures because it fails to handle   \n761 technical jargon.   \n762 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n763 and how they scale with dataset size.   \n764 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n765 address problems of privacy and fairness.   \n766 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n767 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n768 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n769 judgment and recognize that individual actions in favor of transparency play an impor  \n770 tant role in developing norms that preserve the integrity of the community. Reviewers   \n771 will be specifically instructed to not penalize honesty concerning limitations.   \n772 3. Theory Assumptions and Proofs   \n773 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n74 a complete (and correct) proof?   \n777 Guidelines:   \n778 \u2022 The answer NA means that the paper does not include theoretical results.   \n779 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n780 referenced.   \n781 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n782 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n783 they appear in the supplemental material, the authors are encouraged to provide a short   \n784 proof sketch to provide intuition.   \n785 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n786 by formal proofs provided in appendix or supplemental material.   \n787 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n789 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n790 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n791 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "Justification: This is a theoretical paper without experiments. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "826 5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "827 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n828 tions to faithfully reproduce the main experimental results, as described in supplemental   \n829 material?   \n830 Answer: [NA]   \n831 Justification: This is a theoretical paper without experiments.   \n832 Guidelines:   \n833 \u2022 The answer NA means that paper does not include experiments requiring code.   \n834 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n835 public/guides/CodeSubmissionPolicy) for more details.   \n836 \u2022 While we encourage the release of code and data, we understand that this might not be   \n837 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n838 including code, unless this is central to the contribution (e.g., for a new open-source   \n839 benchmark).   \n840 \u2022 The instructions should contain the exact command and environment needed to run to   \n841 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n842 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n843 \u2022 The authors should provide instructions on data access and preparation, including how   \n844 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n845 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n846 proposed method and baselines. If only a subset of experiments are reproducible, they   \n847 should state which ones are omitted from the script and why.   \n848 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n849 versions (if applicable).   \n850 \u2022 Providing as much information as possible in supplemental material (appended to the   \n851 paper) is recommended, but including URLs to data and code is permitted.   \n852 6. Experimental Setting/Details   \n853 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n854 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n855 results?   \n856 Answer: [NA]   \n857 Justification: This is a theoretical paper without experiments.   \n858 Guidelines:   \n859 \u2022 The answer NA means that the paper does not include experiments.   \n860 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n861 that is necessary to appreciate the results and make sense of them.   \n862 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n863 material.   \n864 7. Experiment Statistical Significance   \n865 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n866 information about the statistical significance of the experiments?   \n867 Answer: [NA]   \n868 Justification: No experiments.   \n869 Guidelines:   \n870 \u2022 The answer NA means that the paper does not include experiments.   \n871 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n872 dence intervals, or statistical significance tests, at least for the experiments that support   \n873 the main claims of the paper.   \n874 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n875 example, train/test split, initialization, random drawing of some parameter, or overall   \n876 run with given experimental conditions).   \n877 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n878 call to a library function, bootstrap, etc.)   \n879 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n880 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n881 of the mean.   \n882 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n883 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n884 of Normality of errors is not verified.   \n885 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n886 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n887 error rates).   \n888 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n889 they were calculated and reference the corresponding figures or tables in the text.   \n890 8. Experiments Compute Resources   \n891 Question: For each experiment, does the paper provide sufficient information on the com  \n892 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n893 the experiments?   \n894 Answer: [NA]   \n895 Justification: No experiments.   \n896 Guidelines:   \n897 \u2022 The answer NA means that the paper does not include experiments.   \n898 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n899 or cloud provider, including relevant memory and storage.   \n900 \u2022 The paper should provide the amount of compute required for each of the individual   \n901 experimental runs as well as estimate the total compute.   \n902 \u2022 The paper should disclose whether the full research project required more compute   \n903 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n904 didn\u2019t make it into the paper).   \n905 9. Code Of Ethics   \n906 Question: Does the research conducted in the paper conform, in every respect, with the   \n907 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n908 Answer: [Yes]   \n909 Justification: Our paper does not violate code of ethics.   \n910 Guidelines:   \n911 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n912 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n913 deviation from the Code of Ethics.   \n914 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n915 eration due to laws or regulations in their jurisdiction).   \n916 10. Broader Impacts   \n917 Question: Does the paper discuss both potential positive societal impacts and negative   \n918 societal impacts of the work performed?   \n919 Answer: [NA]   \n920 Justification: This paper is foundational and theoretical research and not tied to particular   \n921 applications.   \n922 Guidelines:   \n923 \u2022 The answer NA means that there is no societal impact of the work performed.   \n924 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n925 impact or why the paper does not address societal impact.   \n926 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n927 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n928 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n929 groups), privacy considerations, and security considerations.   \n930 \u2022 The conference expects that many papers will be foundational research and not tied   \n931 to particular applications, let alone deployments. However, if there is a direct path to   \n932 any negative applications, the authors should point it out. For example, it is legitimate   \n933 to point out that an improvement in the quality of generative models could be used to   \n934 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n935 that a generic algorithm for optimizing neural networks could enable people to train   \n936 models that generate Deepfakes faster.   \n937 \u2022 The authors should consider possible harms that could arise when the technology is   \n938 being used as intended and functioning correctly, harms that could arise when the   \n939 technology is being used as intended but gives incorrect results, and harms following   \n940 from (intentional or unintentional) misuse of the technology.   \n941 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n942 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n943 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n944 feedback over time, improving the efficiency and accessibility of ML).   \n945 11. Safeguards   \n946 Question: Does the paper describe safeguards that have been put in place for responsible   \n947 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n948 image generators, or scraped datasets)?   \n949 Answer: [NA]   \n950 Justification: This paper has no such risks.   \n951 Guidelines:   \n952 \u2022 The answer NA means that the paper poses no such risks.   \n953 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n954 necessary safeguards to allow for controlled use of the model, for example by requiring   \n955 that users adhere to usage guidelines or restrictions to access the model or implementing   \n956 safety filters.   \n957 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n958 should describe how they avoided releasing unsafe images.   \n959 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n960 not require this, but we encourage authors to take this into account and make a best   \n961 faith effort.   \n962 12. Licenses for existing assets   \n963 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n964 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n965 properly respected?   \n966 Answer: [NA]   \n967 Justification: This paper does not use existing assets.   \n968 Guidelines:   \n969 \u2022 The answer NA means that the paper does not use existing assets.   \n970 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n971 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n972 URL.   \n973 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n974 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n975 service of that source should be provided.   \n976 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n977 package should be provided. For popular datasets, paperswithcode.com/datasets   \n978 has curated licenses for some datasets. Their licensing guide can help determine the   \n979 license of a dataset.   \n980 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n981 the derived asset (if it has changed) should be provided.   \n983 the asset\u2019s creators.   \n984 13. New Assets   \n985 Question: Are new assets introduced in the paper well documented and is the documentation   \n986 provided alongside the assets?   \n987 Answer: [NA]   \n988 Justification: This paper does not release new assets   \n989 Guidelines:   \n990 \u2022 The answer NA means that the paper does not release new assets.   \n991 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n992 submissions via structured templates. This includes details about training, license,   \n993 limitations, etc.   \n994 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n995 asset is used.   \n996 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n997 create an anonymized URL or include an anonymized zip file.   \n998 14. Crowdsourcing and Research with Human Subjects   \n999 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n1000 include the full text of instructions given to participants and screenshots, if applicable, as   \n001 well as details about compensation (if any)?   \n002 Answer: [NA]   \n003 Justification: This paper does not involve crowdsourcing.   \n004 Guidelines:   \n005 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n006 human subjects.   \n007 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n1008 tion of the paper involves human subjects, then as much detail as possible should be   \n009 included in the main paper.   \n1010 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n1011 or other labor should be paid at least the minimum wage in the country of the data   \n012 collector.   \n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n013   \n14 Subjects   \n15 Question: Does the paper describe potential risks incurred by study participants, whether   \n16 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n17 approvals (or an equivalent approval/review based on the requirements of your country or   \n18 institution) were obtained?   \n19 Answer: [NA]   \n20 Justification: This paper does not involve crowdsourcing.   \n021 Guidelines:   \n22 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n23 human subjects.   \n24 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n25 may be required for any human subjects research. If you obtained IRB approval, you   \n26 should clearly state this in the paper.   \n27 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n28 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n29 guidelines for their institution.   \n30 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n31 applicable), such as the institution conducting the review. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 35}]