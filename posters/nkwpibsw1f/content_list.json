[{"type": "text", "text": "Dual-Personalizing Adapter for Federated Foundation Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yiyuan Yang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Guodong Long ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Australian AI Institute, Faculty of Engineering & IT University of Technology Sydney Yiyuan.Yang-1@student.uts.edu.au ", "page_idx": 0}, {"type": "text", "text": "Australian AI Institute, Faculty of Engineering & IT University of Technology Sydney Guodong.Long@uts.edu.au ", "page_idx": 0}, {"type": "text", "text": "Tao Shen ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jing Jiang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Australian AI Institute, Faculty of Engineering & IT University of Technology Sydney Tao.Sheng@uts.edu.au ", "page_idx": 0}, {"type": "text", "text": "Australian AI Institute, Faculty of Engineering & IT University of Technology Sydney Jing.Jiang@uts.edu.au ", "page_idx": 0}, {"type": "text", "text": "Michael Blumenstein Australian AI Institute, Faculty of Engineering & IT University of Technology Sydney Michael.Blumenstein@uts.edu.au ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, foundation models, particularly large language models (LLMs), have demonstrated an impressive ability to adapt to various tasks by fine-tuning diverse instruction data. Notably, federated foundation models (FedFM) emerge as a privacy preservation method to fine-tune models collaboratively under federated learning (FL) settings by leveraging many distributed datasets with non-IID data. To alleviate communication and computation overhead, parameter-efficient methods are introduced for efficiency, and some research adapted personalization methods to FedFM for better user preferences alignment. However, a critical gap in existing research is the neglect of test-time distribution shifts in real-world applications, and conventional methods for test-time distribution shifts in personalized FL are less effective for FedFM due to their failure to adapt to complex distribution shift scenarios and the requirement to train all parameters. To bridge this gap, we refine the setting in FedFM, termed test-time personalization, which aims to learn personalized federated foundation models on clients while effectively handling test-time distribution shifts simultaneously. To address challenges in this setting, we explore a simple yet effective solution, a Federated Dual-Personalizing Adapter (FedDPA) architecture. By co-working with a foundation model, a global adapter and a local adapter jointly tackle the test-time distribution shifts and client-specific personalization. Additionally, we introduce an instance-wise dynamic weighting mechanism that dynamically integrates the global and local adapters for each test instance during inference, facilitating effective test-time personalization. The effectiveness of the proposed method has been evaluated on benchmark datasets across different NLP tasks with released code. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Foundation models, especially the large language model (LLM) in natural language processing (NLP), have nearly exhausted public data sources for training. This necessitates alternative solutions to further improve these foundation models by leveraging private or protected data sources, such as business data in companies, smartphones, and so on. Federated foundation models (FedFM)[44, 41] offer a promising solution by integrating federated learning (FL) frameworks to enhance the foundation models in a decentralized manner. Built upon existing Parameter-efficient fine-tuning (PEFT) methods [33, 14, 13], FedFM is a collaboratively fine-tuning framework that leverages private datasets with privacy preservation and avoiding overfitting from client-specific fine-tuning. ", "page_idx": 1}, {"type": "text", "text": "Test-time distribution shift in FL [16, 29] presents a significant challenge in practical scenarios, as clients may encounter unseen learning tasks during the testing and model inference phases. For example, a client accustomed to writing emails in English may require translation assistance when working on a new project in Chinese. Therefore, it is imperative for the deployed machine learning model to be capable of tackling the test-time distribution shifts from the training data, and our paper addresses this critical issue of test-time distribution shifts within the FedFM scenario. Previous works in test-time FL[29, 16] predominantly utilize conventional deep learning models that are trained from scratch in federated settings. Recent FedFM methods mainly focus on addressing specific challenges related to data heterogeneity [2, 15] and communication overheads [35, 27]. However, none of these methods have discussed test-time distribution shifts in FedFM scenarios. ", "page_idx": 1}, {"type": "text", "text": "To fill this gap, we propose a novel FedFM framework that is robust to client-specific alignment and test-time distribution shifts simultaneously. With the support of a foundation model with PEFT methods, we first refine the federated setting, termed test-time personalization, which follows: 1) each client needs to train a personalized model using its own data from a target task, and 2) during testing, each client\u2019s personalized model needs to be robust to tackle the receiving new tasks (unseen in training) with different distributions (test-time distribution shift). Essentially, the proposed test-time personalization in FL could be simply viewed as an optimization task to seek a sweet trade-off between client-specific model personalization and model generalization to test data. ", "page_idx": 1}, {"type": "text", "text": "For test-time personalization in FedFM, two primary challenges\u2014test-time distribution shifts and personalization\u2014necessitate learning tailored to distinct objectives, and the training cost of foundation models also represents a significant concern. To address these issues, we explore a simple yet effective method, dubbed Federated Dual-Personalizing Adapter (FedDPA), where each client learns a global adapter to learn generic knowledge from the aggregation for test-time tasks and maintains a local adapter for targeted ability personalization. During the inference phase, the local and global adapters are dynamically integrated to facilitate prediction, where an instance-wise dynamic weighting mechanism is proposed to autonomously adjudicate the proportional contribution of the local and global adapters for each test instance. Experimental results demonstrate that our method achieves state-of-the-art performance on benchmarks and all data and code are released 1. Our main contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We are the first to explore the test-time distribution shifts problem in federated foundation models for practical application scenarios alignment. \u2022 We introduce a new method, namely dual-personalizing adapter, to emphasize learning both generic and personalized knowledge in the context of FedFM with test-time personalization. \u2022 We conduct an exhaustive analysis using heterogeneous FL benchmarks across diverse NLP tasks. The empirical outcomes reveal that our method attains state-of-the-art performance, underscoring its superior test-time personalization capabilities than existing methods. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Adapter-based PEFT Methods ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Given the substantial computational and storage burdens associated with directly fine-tuning foundation models, the community has shifted towards embracing parameter-efficient methods [33], with the adapter family [14] being a notable exemplar. According to different architectures, methods in the adapter family can be categorized into four types. The first one is prompt-based learning [18, 21], which is aimed at learning the continuous/soft prompt for discrete optimization. The second one is reparametrization-based methods [13, 9], achieving parameter efficiency by utilizing low-rank techniques to decompose the high-dimensional matrices. The third one is series Adapters [12], which introduce additional learnable modules in a sequential manner within specific sublayers. The last one is parallel Adapters [11], which focus on learning additional learnable modules in a parallel way with distinct sublayers. In this context, our exploration delves into the adapter-based PEFT methods of federated foundation models. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.2 Federated Foundation Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "With the advent of foundation models, there has been a burgeoning interest [44, 41, 26, 5] in integrating these models within the FL setting. Particularly, in light of the inherent computation and communication cost, recent work [17, 43, 6] endeavors have delved deeper into integrating adapterbased parameter-efficient tuning (PEFT) methods with federated foundation models. Building upon this, a multitude of studies have emerged to navigate the challenges of incorporating federated foundation models with adapter-based PEFT methods. The paper [42] stands at the forefront, initiating the integration of instruction tuning within federated LLM frameworks. Addressing data-related issues, the paper [2] introduced a data-driven initialization approach to mitigate the primary challenges associated with LoRA in highly heterogeneous data scenarios. In addition, the research presented in [15] proposed a method to annotate unlabeled client-side data by harnessing the prowess of large models to address data scarcity concerns. To further optimize the communication and computational overheads associated with federated foundation models, the works [35, 27, 34] emphasize advancing gradient-free optimization methods suitable for devices with limited memory and computing power. For personalization, paper [38] focused on designing a specific training paradigm for LoRA to achieve more effective personalization in visual model-heterogeneous scenarios. Diverging from these approaches, our work delves into the realm of personalization with adapters in federated foundation models, extending the scope of research in this area. ", "page_idx": 2}, {"type": "text", "text": "2.3 Personalized Federated Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To address the necessity of personalization for individual clients, personalized Federated Learning (PFL) [28], which aims at training to cater to individual client preferences and needs, is proposed. Broadly, existing PFL methods can be categorized into two primary types: fine-tuning the global model for personalization or learning additional personalized models. Research works [10, 7] in the first category fine-tuned the whole or part of the global model with each client\u2019s local dataset for personalization. While research works [19, 22] in the second category is to learn the additional personalized layers or model through local aggregation. Nonetheless, a prevalent limitation among these PFL approaches is their concentrated focus on a specifically targeted task, often at the expense of performance when encountering test-time distribution shifts. ", "page_idx": 2}, {"type": "text", "text": "To fill this gap, recent research has shifted focus towards exploring different test-time distribution shifts in PFL. In contrast to studies [39, 8] in federated continual/incremental learning possessing ample annotated data from different distributions for training to address shifts, test-time FL focuses on handling distribution shifts during testing without the availability of annotated data for further training. One strand of research [3, 32] concentrates on addressing test-time distribution shifts that occur when new clients are introduced during the testing phase by module/prior adaptation. Another line of studies[29, 16] aims to tackle distribution shifts in existing clients during testing by aligning test features with existing features. Our paper falls into the second type and differs from previous work by exploring this challenge within the framework of foundation models, which are characterized by extensive parameter scales and more complex test-time distribution shifts. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Test-time Personalization in FedFM ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Considering $M$ clients in an $\\mathrm{FL}$ system, each client possesses its distinct local training dataset $\\mathbb{D}_{t r a i n}^{m}$ and test dataset $\\mathbb{D}_{t e s t}^{m}$ , where $m$ indexes a client. One data pair in datasets is denoted as , where $\\textbf{\\em x}$ is the input data and $\\textit{\\textbf{y}}$ is its corresponding label. ", "page_idx": 2}, {"type": "image", "img_path": "nkwPiBSw1f/tmp/1d12b960661e8430228b012b4f48f8b02198eb78b99f04af9d96e1266c50d1ee.jpg", "img_caption": ["Figure 1: The overall framework of FedDPA. Each client contains a frozen LLM, a trainable global adapter (LoRA) and a trainable local adapter (LoRA) with a specific task, where the global adapter (LoRA) is for test-time tasks and the local adapter (LoRA) is for personalization. During the training, only the parameters of the global adapter (LoRA) are transmitted to the server for aggregation. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "In each client $m$ , we will introduce the training and testing phases separately for our test-time personalization setting. In the training phase, model training utilizes solely the local dataset $\\mathbb{D}_{t r a i n}^{m}$ , which is derived from the distribution $P_{s}^{m}$ . While in the testing phase, the test dataset $\\mathbb{D}_{t e s t}^{m}$ comprises two components: the test set $\\mathbb{D}_{s}^{m}$ driven from the same distribution $P_{s}^{m}$ as training data, and additional test sets $\\mathbb{D}_{t}^{m}$ under data distribution shifts $P_{s}^{m}({\\pmb x},{\\pmb y})\\neq P_{t}^{m}({\\pmb x},{\\pmb y})$ . Therefore, the test dataset is $\\mathbb{D}_{t e s t}^{m}=\\mathbb{D}_{s}^{m}\\cup\\mathbb{D}_{t}^{m}$ , and we call these datasets $\\mathbb{D}_{t}^{m}$ as test-time datasets. Unlike previous works[29, 16] in test-time $\\mathrm{FL}$ concentrating on either feature-level shifts $P_{s}^{m}(\\pmb{x})\\neq P_{t}^{m}(\\pmb{x})$ or label shifts $P_{s}^{m}(\\pmb{y})\\neq P_{t}^{m}(\\pmb{y})$ , we investigate a more complex scenario where various distribution shifts, including semantic shifts, domain shifts and others, exist simultaneously. This is aligned with the practical application of foundation models, which often encounter testing data originating from diverse domains, backgrounds, or populations. ", "page_idx": 3}, {"type": "text", "text": "Therefore, the objective of the model in each client should not only perform well on the test set $\\mathbb{D}_{s}^{m}$ (refer to personalization) but also have comparable results on the test-time dataset $\\mathbb{D}_{t}^{m}$ (refer to test-time performance). This objective is consistent with the practical scenarios, since users primarily focus on the abilities they often utilize (abundant data available for training) and occasionally also introduce new tasks (limited to test data). ", "page_idx": 3}, {"type": "text", "text": "3.2 Challenges ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The test-time personalization setting raises two pivotal considerations: personalization and test-time distribution shifts. Personalization is the primary focus, followed by optimizing test-time tasks. Our proposed method introduced in section 4 is designed to achieve personalization within FedFM while ensuring comparable results for test-time tasks, and its vital intuition is illustrated below. ", "page_idx": 3}, {"type": "text", "text": "Considering a foundation model, it comprises a main body $f(\\pmb\\theta)$ , which holds most of the parameters and processes input $\\textbf{\\em x}$ to produce output features $\\pmb{h}=f(\\pmb{x};\\pmb{\\theta})$ . Additionally, there is a tail $g(\\pmb\\theta_{t})$ that maps these features to the output space (e.g., vocabulary), resulting in the predicted result $\\hat{\\pmb y}=g(\\pmb{h};\\pmb{\\theta}_{t})$ . Typically, the focus in tuning and adaptations primarily lies on the main body $f(\\pmb\\theta)$ because the tail $g(\\pmb\\theta_{t})$ , usually a linear function, remains unchanged (frozen) during tuning [14]. ", "page_idx": 3}, {"type": "text", "text": "Discordance between Personalization and Test-time Tasks. The key to addressing test-time distribution shifts lies in learning generic features universally applicable across disparate distributions [1]. That is, learning a foundation model $f(\\pmb\\theta)$ to satisfy $\\bar{P_{s}}(\\bar{f}(\\mathbf{\\bar{x}};\\theta),y)=P_{t}(f(\\mathbf{\\bar{x}};\\theta),y)$ although ", "page_idx": 3}, {"type": "text", "text": "$P_{s}(x,{\\pmb y})\\neq P_{t}({\\pmb x},{\\pmb y})$ . FL is a methodology designed to learn generic features across diverse non-IID data (different distributions) through aggregation algorithms [5, 29]. Therefore, we tailor FL training for addressing test-time tasks with the objective $\\operatorname*{min}_{\\pmb{\\theta}}\\mathcal{L}_{P_{a l l}}(\\pmb{\\theta})$ , where $\\mathcal{L}_{P_{a l l}}$ represents the loss function designed for learning generic features towards all clients\u2019 distributions $P_{a l l}$ . However, personalization focuses on aligning the model with the specific distribution $P_{s}$ , which means learning a foundation model $f(\\pmb\\theta)$ with the objective min\u03b8 $\\mathcal{L}_{P_{s}}(\\bar{\\pmb{\\theta}})$ , where $\\mathcal{L}_{P_{s}}$ represents the loss function designed for learning personalized features towards the specific distribution. Therefore, the discordance between specific distribution alignment for personalization and generic feature learning for test-time tasks leads to inconsistent optimization objectives. ", "page_idx": 4}, {"type": "text", "text": "The above analysis motivates a dual model strategy\u2014one model for test-time tasks and one model for personalization\u2014to realize test-time personalization in FedFM. This strategy, together with our other techniques presented below for FedFM scenarios, constitutes the foundation of our method. ", "page_idx": 4}, {"type": "text", "text": "4 Proposed Method ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To align with the application scenarios, we consider the test-time personalization setting in FedFM. Following a similar assumption from a Mixture of Experts [23], any test-time task (distributions) to a client can be approximated as a mixture of training tasks seen by other clients in the federated learning system. Therefore, each client primarily personalizes its model based on its local training task, while also tackling unseen test-time tasks by leveraging insights gained from other clients in the federated learning system. Discussions of other scenarios can be found in Appendix C. ", "page_idx": 4}, {"type": "text", "text": "In test-time personalization, test-time distribution shifts and personalization are two main issues that need to be addressed, and their optimization objectives toward different distributions are inconsistent. To address these challenges and consider the efficient learning of FedFM, we propose a Federated Dual-Personalizing Adapter (FedDPA) system for each client, as shown in Fig 1. During training, a global adapter is employed to acquire generic features by FL training for test-time tasks (Sec. 4.1). Meanwhile, to address personalization, a local adapter is maintained locally to align with the client\u2019s specific distribution, and leverages generic knowledge from the global adapter for faster learning (Sec. 4.2). During the inference, the learned global and local adapters are dynamically combined using a weight generated by the instance-wise dynamic weighting mechanism for each input test instance, realizing test-time personalization (Sec. 4.3). ", "page_idx": 4}, {"type": "text", "text": "The Overall Objective. Considering the computation and communication cost of FedFM, we utilize the adapter-based PEFT methods, which only learn a small part of parameters $\\Delta\\pmb{\\theta}$ while keeping most of the parameters $\\pmb{\\theta}$ frozen. Our proposed FedDPA is to learn the global adapter $\\Delta\\pmb{\\theta}_{g}$ and local adapters $\\Delta\\pmb{\\theta}_{l}^{m}$ simultaneously across $M$ client to realize test-time personalization, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\underset{\\Delta\\theta_{g},\\{\\Delta\\theta_{l}^{m}\\}}{\\mathrm{min}}}&{\\underset{m=1}{\\overset{M}{\\sum}}[\\mathcal{L}_{(x,y)\\sim P_{s}^{m}}(\\theta;\\Delta\\theta_{g};\\Delta\\theta_{l}^{m})]}\\\\ &{s.t.}&{\\Delta\\theta_{g}^{*}\\in\\arg\\operatorname*{min}\\mathcal{L}_{(x,y)\\sim P_{a l l}}(\\theta;\\Delta\\theta_{g};\\Delta\\theta_{l}^{m})}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the first part is a standard personalized FL loss to find optimal personalized models by minimizing the sum of loss on local training tasks $\\mathcal{L}_{(\\pmb{x},\\pmb{y})\\sim P_{s_{.}}^{m}}(.)$ , and the second part is a constraint term to seek an optimal solution by minimizing the test-time loss $\\mathcal{L}_{(\\pmb{x},\\pmb{y})\\sim P_{\\alpha l l}(.)}$ . Because we assume that the test-time task is unseen to a client but observed by other clients, the test-time loss can be estimated using the Empirical Risk Minimization of all client\u2019s training tasks $\\begin{array}{r}{\\operatorname*{min}_{\\Delta\\pmb{\\theta}_{g}}\\sum_{m=1}^{M}r_{m}\\mathcal{L}_{m}(\\pmb{\\theta};\\Delta\\pmb{\\theta}_{g})}\\end{array}$ where $P_{a l l}$ denotes all distributions of tasks in all clients, $r_{m}$ denotes each client\u2019s weight for aggregation (e.g., in FedAvg, $r_{m}$ is the proportion of each client\u2019s data number to all clients\u2019 data number) and ${\\mathcal{L}}_{m}$ denotes the loss for each client over its local training dataset. Since the above objective cannot be solved directly, we propose to alternatively learn the global and local adapters in a sequential manner (FedDPA- $\\cdot F$ with local adapter fine-tuning) or iterative manner (FedDPA-T with local adapter training). Detailed algorithms of these two methods are in Appendix A.3. ", "page_idx": 4}, {"type": "text", "text": "Remark. To simplify the illustration, we use LLM as the backbone and adopt LoRA [13] as the adapter-based PEFT method in our framework. The overall framework is easy to adapt to other types of backbone and other adapter-based PEFT methods. LoRA decomposes the training weight into a frozen weight $\\pmb{\\theta}$ , and a trainable weight derived by the multiplication of two low-rank weights $\\Delta\\theta=\\Delta\\theta^{b}\\bar{\\Delta\\theta^{a}}$ . The data heterogeneity in FL with LLM primarily manifests as distribution shifts across various NLP tasks among different clients, driven by diverse backgrounds, topics, and other contextual factors, and local loss ${\\mathcal{L}}_{m}$ for all NLP tasks is a standard language modeling objective [4]. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4.1 Generic Learning of Global Model ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Addressing test-time distribution shifts requires the acquisition of generic knowledge that is applicable across various distributions [1]. The conventional federated learning process is inherently designed to aggregate this generic knowledge among different non-IID data. Consequently, we utilize the adapter trained within the FL context as the global adapter for addressing test-time tasks. To further enhance generic learning, our model aggregation strategy is based on the client number rather than the number of data by considering the potential biases stemming from different numbers of tasks. ", "page_idx": 5}, {"type": "text", "text": "At each client, there consists of a frozen LLM model $f(\\mathbf{\\boldsymbol{x}};\\mathbf{\\boldsymbol{\\theta}})$ with a global lightweight global adapter (LoRA) $\\Delta\\pmb{\\theta}_{g}\\,=\\,\\Delta\\pmb{\\theta}_{g}^{b}\\Delta\\pmb{\\theta}_{g}^{a}$ . This global adapter is used for aggregation by sending to the server. Notably, the server\u2019s role is limited to computing the aggregated adapter $\\Delta\\pmb{\\theta}_{g}$ , thus obviating the need for maintaining a large-scale model. Similar to the standard $\\mathrm{FL}$ process, for each client $m$ , the adapter weight $\\Delta\\pmb{\\theta}_{g}^{m}$ is learned locally and sent to the server. Upon receipt of the adapter weights from all clients, the server employs FedAvg [24] to aggregate them and sends $\\Delta\\bar{\\theta_{g}}$ back to each client as their initialized parameter in a new round. It can be formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta\\bar{\\theta}_{g}=\\sum_{m=1}^{M}\\frac{1}{M}\\Delta{\\theta}_{g}^{m},\\quad\\mathrm{{Client:}}\\ \\Delta{\\theta}_{g}^{m}=\\arg\\operatorname*{min}_{\\Delta\\theta_{g}}\\mathcal{L}_{m}(\\theta;\\Delta\\theta_{g}),\\ \\mathrm{initialized\\with}\\ \\Delta\\bar{\\theta}_{g}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark. Other federated algorithms like FedProx [20] can also be applied with LoRA tuning of this global model learning (in Appendix B.1). In this paper, we just take FedAvg as an example. ", "page_idx": 5}, {"type": "text", "text": "4.2 Personalization of Local Model ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The previously developed global model, which focuses on acquiring generic features across diverse datasets, faces challenges with personalization due to inconsistent optimization objectives. To address this, we integrate a local adapter to better align with each client\u2019s specific distribution. We explore two methods as shown in Fig 2, 1) Learning sequentially: after global adapter training, the local adapter is initialized by the learned global adapter and directly fine-tuned; 2) Learning iteratively: during each communication round of global adapter training, the local adapter is re-initialized from its last state, fine-tuned alongside the frozen global adapter, and maintained locally without communication. ", "page_idx": 5}, {"type": "image", "img_path": "nkwPiBSw1f/tmp/e6f37161587bb45b5627ec89ee16af182e93d8be0846067375a342cfbfa8fe8d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 2: Frameworks of two personalized methods for local adapter (LoRA) are shown on the left, with their overall learning processes on the right. ", "page_idx": 5}, {"type": "text", "text": "To be more specific, a local adapter (LoRA) $\\Delta\\pmb{\\theta}_{l}=\\Delta\\pmb{\\theta}_{l}^{b}\\Delta\\pmb{\\theta}_{l}^{a}$ is introduced. Thus, each client contains three components: a frozen LLM $\\pmb{\\theta}$ , a global adapter (LoRA) $\\Delta\\pmb{\\theta}_{g}$ and a local adapter (LoRA) $\\Delta\\pmb{\\theta}_{l}^{m}$ As delineated in $\\mathrm{Fig}\\,2$ (a), for the first method, after global training, the local adapter is first initialized by the global adapter denoted as $\\Delta\\theta_{l}^{m}=\\Delta\\theta_{g}$ , then fine-tuned on local data to get the final local adapter. As shown in Fig 2 (b), for the second method, during each communication round in training for each adapter layer, upon receiving an input $^h$ , it simultaneously traverses the frozen LLM, the frozen global adapter and the local adapter. The process entails an initial fusion of the outputs from both the local and global adapters with a predefined weighting factor of $\\alpha$ , followed by integration with the output of the LLM to yield the final result $\\boldsymbol{h^{\\prime}}=\\theta\\boldsymbol{h}+((1-\\alpha)\\cdot\\Delta\\theta_{g}\\boldsymbol{h}+\\alpha\\cdot\\Delta\\theta_{l}^{m}\\boldsymbol{h})$ . Therefore, the learning of the local adapter $\\Delta\\pmb{\\theta}_{l}^{m}$ for these two methods can be unified as: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Delta\\pmb{\\theta}_{l}^{m}=\\arg\\operatorname*{min}_{\\Delta\\pmb{\\theta}_{l}^{m}}\\mathcal{L}_{m}(\\pmb{\\theta};\\Delta\\pmb{\\theta}_{g};\\Delta\\pmb{\\theta}_{l}^{m}),\\quad\\mathrm{initialized~with~}\\Delta\\pmb{\\theta}_{g}\\mathrm{~or~previous~}\\Delta\\pmb{\\theta}_{l}^{m}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4.3 LLM-enhanced Instance-wise Dynamic Weighting Mechanism ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As discussed in previous test-time FL methods [16], a dynamic combination of global components and personalized components can improve generalization while reducing the cost of hyper-parameter tuning in the deployment stage. Considering the disparate data distributions that characterize test-time tasks and local tasks and the wealth of training instances of local tasks available to each client, we propose an instance-wise dynamic weighting mechanism to calculate the similarity between the input instance and local instances, using this metric to determine the appropriate weight balance for the global and local adapter combination. To facilitate this, the representation of each input instance is essential. Leveraging the robust capability of pre-trained LLMs to abstract input sentences, we utilize the hidden states from the final layer of the LLM as the representation. Given that the LLM is decoder-based, with tokens attending only to preceding tokens, the embedding of the final token is considered representative of the entire input for similarity evaluation. Furthermore, to enhance the representation quality, the global adapter, which embodies generic knowledge, is incorporated into this embedding process. ", "page_idx": 6}, {"type": "text", "text": "More specifically, during the inference stage, for each input instance $\\textbf{\\em x}$ in a client, we randomly sample $S$ instances $\\{x_{0},x_{1},...,x_{s}\\}$ from the local training dataset. These instances are then fed into the LLM, augmented with the global adapter, to obtain the last token\u2019s embeddings from the final layer, denoted as $\\pmb{w}_{x}$ and $\\{\\pmb{\\omega}_{x_{0}},\\pmb{w}_{x_{1}},...,\\pmb{w}_{x_{s}}\\}$ respectively. Subsequently, we calculate the similarity between the input representation $\\pmb{w}_{x}$ and each sampled local representation in $\\{\\pmb{\\omega}_{x_{0}},\\pmb{w}_{x_{1}},...,\\pmb{w}_{x_{s}}\\}$ , resulting in a score range of $[0,1]$ . Finally, we average all scores to obtain the final result, represented aasn $\\begin{array}{r}{\\alpha_{t}=\\lambda\\cdot\\sum_{i=0}^{S}\\frac{1}{S}\\operatorname{Sim}(\\dot{\\pmb{w_{x}}},\\dot{\\pmb{w_{x}}_{i}})}\\end{array}$ ,t rwicht etrhee  Smiamx irempurems esintmsi ltahreit fy usnccotrieo n( etsop eccailaclullya tfeo rt hFee dsiDmPiAla-rTit)y., $\\lambda$ $(0,1]$ ", "page_idx": 6}, {"type": "text", "text": "Through this method, the balancing of weights between the global and local adapters is dynamically adjusted for each test instance, ensuring the model not only tailors to the individual client\u2019s specific needs but also benefits from the aggregated model\u2019s generic knowledge across test-time tasks. ", "page_idx": 6}, {"type": "text", "text": "5 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Experiment Setting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We construct two federated datasets from Flan [31], which is a collection of various NLP tasks from over 60 datasets for instruction turning. In order to be better suitable for FL settings, we randomly select 8 NLP tasks from different datasets for each federated dataset and downsample the original datasets with more details in Appendix A.1. ROGUE-1 is taken as a metric. ", "page_idx": 6}, {"type": "text", "text": "Baselines and Implementation. We compare our methods with four baselines based on the same model architecture: centralized model, Local-finetuned model, FedIT [42] and FedLoRA [38]. The centralized model is trained on all data of tasks in one center. The local-finetuned model infers that only local data are used to train the model without any communication with other clients or the server. ", "page_idx": 6}, {"type": "text", "text": "We distribute data between clients based on the NLP task for data heterogeneity, where different NLP tasks generated from different contextual factors inherently suffer from various complex distribution shifts. Since we select 8 NLP tasks, corresponding to $M\\,=\\,8$ clients.For each client, the local task serves as the primary focus for personalization, while the tasks from other clients are taken as test-time tasks. To better evaluate the effectiveness of methods, we assume that all clients are activated for every communication round and set the communication round $K=20$ . The alpaca-LoRA2 is adapted as the base model initialized with LLaMA-7B.3 The updating weight of local LoRA training (FedDPA-T) is $\\alpha=0.5$ $\\lambda=0.5)$ ) for federated dataset 1 and $\\alpha=0.3$ $\\lambda=0.3)$ ) for federated dataset ", "page_idx": 6}, {"type": "text", "text": "2. We set $S=5$ and choose cosine similarity for instance-wise dynamic weighting mechanism. More details are in Appendix A.2. ", "page_idx": 7}, {"type": "table", "img_path": "nkwPiBSw1f/tmp/c8d89e553b6356f47c23aee32fc70c481539874678ef161e8b5c4ee0264ee20c.jpg", "table_caption": ["Table 1: Personalization and test-time personalization results of different models on federated dataset 1. FedDPA-F represents the model with the local fine-tuning adapter and FedDPA-T represents the model with the local training adapter. Linguistic represents the linguistic acceptability task, Word Dis represents the word disambiguation task, and Question CLS represents question classification task. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.2 Main Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare FedDPA with other baselines on two main evaluation facets: personalization (scores on targeted local tasks) and test-time personalization (average scores on all tasks including test-time tasks). As evidenced in Table 1 and Table 2, our proposed dual-personalizing adapter methods (both fine-tuning and training) exhibit superior performance in personalization compared to other baseline models, which demonstrates the effectiveness of local adapter maintenance for enhancing performance on the targeted local task. For test-time personalization, the FedDPA-F method stands out as the most effective among all personalized models, which suggests that incorporating learning from the global adapter can be instrumental in adapting to test-time distribution shifts for a more comprehensive model achievement. Additionally, given that the global adapter aggregated on different distributions matin certain generalization capabilities, the local adapter of FedDPA-F has better generalization performance than that of FedDPA-T, which leads to better performance on most test-time tasks. More importantly, it is noteworthy that while centralized or global models may yield higher average performances across all tasks, they fall short in excelling at specific tasks for personalization, aligning with the conclusions of the previous study [30]. ", "page_idx": 7}, {"type": "table", "img_path": "nkwPiBSw1f/tmp/e951d606026826ce300274fb9a951e4327d59d04e66913cdd5e818019cf48d45.jpg", "table_caption": ["Table 2: Personalization and test-time personalization results of different models on federated dataset 2. FedDPA-F represents the model with the local fine-tuning adapter and FedDPA-T represents the model with the local training adapter. Reading Com represents the reading comprehension task. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6 Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "6.1 Convergence Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We present the convergence analysis of our methods in Figure 3. Figure 3 (a) compares our methods with other baselines for personalization, with the results showcasing the average performance on target local tasks across all clients. Notably, our methods exhibit a more rapid convergence compared to FedIT and achieve notable performance enhancements after five communication rounds. Despite sharing similar trends with FedLoRA, our approaches, particularly the FedDPA-T, ultimately outperform in personalization. For a more granular insight into test-time personalization convergence, Figure 3 (b) compares average performance on all tasks, including each client\u2019s targeted local and test-time tasks. The results substantiate that our approaches demonstrate faster convergence rates, further bolstering the efficacy of our methods. ", "page_idx": 8}, {"type": "text", "text": "Table 3: Ablation study of instance-wise dynamic weighting mechanism (Auto). P represents personalization, and TTP represents testtime personalization. ", "page_idx": 8}, {"type": "image", "img_path": "nkwPiBSw1f/tmp/2bcf8f07094af044f39d8859f201e46b477b8579397fd806956de659f0616eff.jpg", "img_caption": ["Figure 3: Average accuracy varies as communication rounds. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 4: Ablation study of updating weight. P represents personalization, and TTP represents test-time personalization. ", "page_idx": 8}, {"type": "table", "img_path": "nkwPiBSw1f/tmp/d7317830a694ab9f7a2332af3ed63aa64ce4197b3a49cbb455a6baa32aabf9bb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "nkwPiBSw1f/tmp/4eba05308a0fc4900b90b3ebddcc7a4605a103df4b61bb17afd5e0020ad2c987.jpg", "img_caption": ["Figure 4: Average accuracy varies as different client participation numbers. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6.2 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Impact of Instance-Wise Dynamic Weighting Mechanism. To explore the impact of the instancewise dynamic weighting mechanism, we implemented experiments with FedDPA methods on different datasets. As shown in Table 3, the incorporation of an instance-wise dynamic weighting mechanism contributes significantly to enhancing performance in both personalization and test-time personalization scenarios. More ablation studies are in Appendix B.2. ", "page_idx": 8}, {"type": "text", "text": "Impact of Updating Weight $\\alpha$ . In this study, we investigated the influence of the updating weight $\\alpha$ during FedDPA-T training with its value $\\alpha\\in\\{03,0.5,0.7\\}$ . As can be seen in Table 4, for test-time personalization, increasing updating weight $\\alpha$ will decrease the performance due to the increased proportion of the local adapter in the model, while for personalization, different updating weights $\\alpha$ are required for different datasets to achieve their optimal results. ", "page_idx": 8}, {"type": "text", "text": "Impact of Client Number. To better align with the FL setting in practical application, we scaled up clients to 40 and implemented experiments with sample rate $\\left\\lbrace0.2,0.4,0.\\bar{6},0.8,1\\right\\rbrace$ . For each communication round, the server will select clients from each task based on the sample rate (more details in Appendix A.1). As shown in Figure 4, as the client participant rates increase, model accuracy also increases as more participating clients provide more data for knowledge learning. Besides, ", "page_idx": 8}, {"type": "text", "text": "FedDPA-F outperforms all baselines, whereas FedDPA-T exhibits somewhat inferior performance, potentially due to overfitting issues when handling a small dataset. ", "page_idx": 9}, {"type": "text", "text": "More experiments and analyses of scalability and efficiency can be found in Appendix B.3 and B.4. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Federated Foundation Model (FedFM) is a promising direction to enhance existing Foundation Models, e.g. LLM, by leveraging private data sources. Test-time distribution shift is a critically important problem to ensure the practicability of the FedFM system. This work is the first to propose the test-time FedFM setting. To tackle this challenging scenario, we propose a novel dualpersonalizing adapter for the FedFM framework. The method is evaluated on public NLP tasks that are adapted to mimic the test-time FedFM setting. This work is the first step towards this direction. We focus on defining a new learning scenario, proposing a basic learning framework, and setting up the benchmark datasets. Our future works will be in two directions: the first is to rethink this problem from a theoretical perspective, and the second is to enhance the benchmark setting with more datasets in real applications. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Martin Arjovsky. Out of distribution generalization in machine learning. PhD thesis, New York University, 2020.   \n[2] Sara Babakniya, Ahmed Roushdy Elkordy, Yahya H Ezzeldin, Qingfeng Liu, Kee-Bong Song, Mostafa El-Khamy, and Salman Avestimehr. Slora: Federated parameter efficient fine-tuning of language models. arXiv preprint arXiv:2308.06522, 2023.   \n[3] Wenxuan Bao, Tianxin Wei, Haohan Wang, and Jingrui He. Adaptive test-time personalization for federated learning. Advances in Neural Information Processing Systems, 36, 2024. [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. [5] Zachary Charles, Nicole Mitchell, Krishna Pillutla, Michael Reneer, and Zachary Garrett. Towards federated foundation models: Scalable dataset pipelines for group-structured learning. Advances in Neural Information Processing Systems, 36, 2024. [6] Haokun Chen, Yao Zhang, Denis Krompass, Jindong Gu, and Volker Tresp. Feddat: An approach for foundation model finetuning in multi-modal heterogeneous federated learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 11285\u201311293, 2024.   \n[7] Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representations for personalized federated learning. In International conference on machine learning, pages 2089\u20132099. PMLR, 2021.   \n[8] Jiahua Dong, Lixu Wang, Zhen Fang, Gan Sun, Shichao Xu, Xiao Wang, and Qi Zhu. Federated class-incremental learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10164\u201310173, 2022. [9] Ali Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Partovi Nia, James J Clark, and Mehdi Rezagholizadeh. Krona: Parameter efficient tuning with kronecker adapter. arXiv preprint arXiv:2212.10650, 2022.   \n[10] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach. Advances in Neural Information Processing Systems, 33:3557\u20133568, 2020.   \n[11] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366, 2021.   \n[12] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pages 2790\u20132799. PMLR, 2019.   \n[13] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \n[14] Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee, Lidong Bing, and Soujanya Poria. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933, 2023.   \n[15] Jingang Jiang, Xiangyang Liu, and Chenyou Fan. Low-parameter federated learning with large language models. arXiv preprint arXiv:2307.13896, 2023.   \n[16] Liangze Jiang and Tao Lin. Test-time robust personalization for federated learning. In ICLR, 2023.   \n[17] Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen Pan, Yuexiang Xie, Yaliang Li, Bolin Ding, and Jingren Zhou. Federatedscope-llm: A comprehensive package for fine-tuning large language models in federated learning. arXiv preprint arXiv:2309.00363, 2023.   \n[18] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.   \n[19] Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning through personalization. In International Conference on Machine Learning, pages 6357\u20136368. PMLR, 2021.   \n[20] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine learning and systems, 2:429\u2013450, 2020.   \n[21] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021.   \n[22] Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Federated learning on non-iid features via local batch normalization. arXiv preprint arXiv:2102.07623, 2021.   \n[23] Saeed Masoudnia and Reza Ebrahimpour. Mixture of experts: a literature survey. Artificial Intelligence Review, 42:275\u2013293, 2014.   \n[24] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages 1273\u20131282. PMLR, 2017.   \n[25] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[26] Chao Ren, Han Yu, Hongyi Peng, Xiaoli Tang, Anran Li, Yulan Gao, Alysa Ziying Tan, Bo Zhao, Xiaoxiao Li, Zengxiang Li, et al. Advances and open challenges in federated learning with foundation models. arXiv preprint arXiv:2404.15381, 2024.   \n[27] Jingwei Sun, Ziyue Xu, Hongxu Yin, Dong Yang, Daguang Xu, Yiran Chen, and Holger R Roth. Fedbpt: Efficient federated black-box prompt tuning for large language models. arXiv preprint arXiv:2310.01467, 2023.   \n[28] Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. Towards personalized federated learning. IEEE Transactions on Neural Networks and Learning Systems, 2022.   \n[29] Yue Tan, Chen Chen, Weiming Zhuang, Xin Dong, Lingjuan Lyu, and Guodong Long. Is heterogeneity notorious? taming heterogeneity to handle test-time shift in federated learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[30] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023.   \n[31] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.   \n[32] Jian Xu and Shao-Lun Huang. A joint training-calibration framework for test-time personalization with label shift in federated learning. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pages 4370\u20134374, 2023.   \n[33] Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and Fu Lee Wang. Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment. arXiv preprint arXiv:2312.12148, 2023.   \n[34] M Xu, D Cai, Y Wu, X Li, and S Wang. Fwdllm: Efficient fedllm using forward gradient. 2024.   \n[35] Mengwei Xu, Yaozong Wu, Dongqi Cai, Xiang Li, and Shangguang Wang. Federated finetuning of billion-sized language models across mobile devices. arXiv preprint arXiv:2308.13894, 2023.   \n[36] Xiangpeng Yang, Linchao Zhu, Hehe Fan, and Yi Yang. Eva: Zero-shot accurate attributes and multi-object video editing. arXiv preprint arXiv:2403.16111, 2024.   \n[37] Xiangpeng Yang, Linchao Zhu, Xiaohan Wang, and Yi Yang. Dgl: Dynamic global-local prompt tuning for text-video retrieval. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 6540\u20136548, 2024.   \n[38] Liping Yi, Han Yu, Gang Wang, and Xiaoguang Liu. Fedlora: Model-heterogeneous personalized federated learning with lora tuning. arXiv preprint arXiv:2310.13283, 2023.   \n[39] Jaehong Yoon, Wonyong Jeong, Giwoong Lee, Eunho Yang, and Sung Ju Hwang. Federated continual learning with weighted inter-client transfer. In International Conference on Machine Learning, pages 12073\u201312086. PMLR, 2021.   \n[40] Jun Yu, Yutong Dai, Xiaokang Liu, Jin Huang, Yishan Shen, Ke Zhang, Rong Zhou, Eashan Adhikarla, Wenxuan Ye, Yixin Liu, et al. Unleashing the power of multi-task learning: A comprehensive survey spanning traditional, deep, and pretrained foundation model eras. arXiv preprint arXiv:2404.18961, 2024.   \n[41] Sixing Yu, J Pablo Mu\u00f1oz, and Ali Jannesari. Federated foundation models: Privacy-preserving and collaborative learning for large models. arXiv preprint arXiv:2305.11414, 2023.   \n[42] Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, Guoyin Wang, and Yiran Chen. Towards building the federated gpt: Federated instruction tuning. arXiv preprint arXiv:2305.05644, 2023.   \n[43] Zhuo Zhang, Yuanhang Yang, Yong Dai, Qifan Wang, Yue Yu, Lizhen Qu, and Zenglin Xu. Fedpetuning: When federated learning meets the parameter-efficient tuning methods of pretrained language models. In Annual Meeting of the Association of Computational Linguistics 2023, pages 9963\u20139977. Association for Computational Linguistics (ACL), 2023.   \n[44] Weiming Zhuang, Chen Chen, and Lingjuan Lyu. When foundation model meets federated learning: Motivations, challenges, and future directions. arXiv preprint arXiv:2306.15546, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Implementation Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Datasets ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this paper, we have developed two federated datasets derived from the Flan [31], and details to construct these datasets are elucidated in this section. Flan encompasses a diverse array of NLP tasks, each comprising multiple datasets. These tasks, generated from different contextual factors, inherently experience various complex distribution shifts. To align with FL settings, we employed a stratified selection process, randomly choosing one dataset from each of the eight distinct tasks from Flan to form each federated dataset. In addition, to simulate client local data scarcity [24], we implemented a downsampling strategy, reducing the size of each selected local dataset to 300 training instances and 200 testing instances. Consequently, each constructed federated dataset encompasses eight distinct NLP tasks, culminating in a whole dataset comprising 2400 training examples and 1600 testing examples across all tasks. The specific tasks and datasets included in each federated dataset are cataloged in Table 5. ", "page_idx": 12}, {"type": "text", "text": "The NLP tasks within these datasets can be broadly divided into two types: generation tasks and classification tasks. To facilitate uniform processing by LLM, all tasks are converted into a generative format, employing distinct instructions for each dataset. Illustrative examples of these data for both classification and generation tasks are provided in Table 6. For the input of the LLM, we adopted a simple template, the details of which are delineated in Table 7. ", "page_idx": 12}, {"type": "text", "text": "Dataset Partitioning for Ablation Study. In our ablation study in section 6.2 examining the client number to align with FL settings, we divided each task in our constructed federated datasets into five subsets, each comprising an equal number of training data. Based on our assumption that each client is associated with a single task, this division results in a total of 40 clients, with each client possessing a local dataset of 60 training examples. To mimic real-world FL communication dynamics, we employed a randomized selection process for clients (subsets) within each task according to specified sample rates. Accordingly, for sample rates specified as $\\{0.2,0.4,0.6,0.8,1\\}$ , we selected 1,2,3,4, and 5 clients (subsets) per task, leading to 8, 16, 24, 32, and 40 clients participating in federated communications, respectively. The evaluation phase involves computing the average results across these selected clients for each specified sample rate, which provides a comprehensive analysis of how client numbers influence the performance of our method. ", "page_idx": 12}, {"type": "text", "text": "Different with Multi-Task Learning. Although both multi-task learning and our test-time personalization setting involve multiple tasks during training and testing, multi-task learning operates in a centralized setting, whereas our setting is based on federated learning, a distributed setting. More importantly, our setting accounts for test-time distribution shifts, a challenge that is typically overlooked in conventional multi-task learning. Additionally, fine-tuning on the combination of multi-task data from a centralized foundation model serves as a strong baseline for multi-task learning. In foundation models, all tasks are standardized into a uniform format, and the model benefits from task-agnostic token embeddings learned through extensive pre-training on diverse data. Thus, directly fine-tuning on this multi-task data represents the implementation of multi-task learning using foundation models [40]. We have included this baseline, referred to as \"Centralized,\" in our experimental comparisons. ", "page_idx": 12}, {"type": "text", "text": "A.2 Baselines and Implementation ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, detailed descriptions of the implementation of FedDPA and each baseline compared in this study will be provided: ", "page_idx": 12}, {"type": "text", "text": "\u2022 Centralized model: This model is formulated by aggregating all available data from various tasks at a single centralized center for training purposes, with 50 epochs to optimize. ", "page_idx": 12}, {"type": "text", "text": "\u2022 Local-finetuned model: This model trains independently without any external communication from other clients or a central server. It is specifically trained on data pertaining to a single task, dedicating 50 epochs to optimize for task-specific performance without the influence of external data. ", "page_idx": 12}, {"type": "text", "text": "Table 5: Tasks and datasets of constructed federated dataset 1 and federated dataset 2. ", "page_idx": 13}, {"type": "table", "img_path": "nkwPiBSw1f/tmp/4840b5246cee1fcf38a6884077f0af681f69b5ebbdd72f0f2500ef988738835b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Table 6: Examples of data in our constructed federated datasets. ", "page_idx": 13}, {"type": "table", "img_path": "nkwPiBSw1f/tmp/0e406677c75e6529d1f5b2123f3acc0a82ac06a241020f3e61c696974b908afe.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "\u2022 FedIT model [42]: The FedIT model is the final aggregated global model derived from diverse local client datasets after training. It embodies the essence of collaborative learning inherent to federated learning, assimilating knowledge from a multitude of client-specific data sources. ", "page_idx": 13}, {"type": "text", "text": "\u2022 FedLoRA model [38]: Here, we adapt the training paradigm in paper [38] to NLP tasks by focusing on training the lightweight LoRA for aggregation while keeping the majority of the LLM parameters frozen. Subsequently, a personalized adaptation process is employed, where the globally aggregated LoRA undergoes further local training on each local client\u2019s dataset to tailor the learning outcomes to individual client needs. ", "page_idx": 13}, {"type": "text", "text": "\u2022 FedDPA-F: FedDPA-F is the combination of the global adapter and the local fine-tuning adapter. During the inference, the scale factor is set to $\\lambda=1$ in the instance-wise dynamic weighting mechanism. ", "page_idx": 13}, {"type": "text", "text": "\u2022 FedDPA-T: FedDPA-T is the combination of the global adapter and the local training adapter. Since the global adapter contributes to the training of the local adapter for personalization, it is essential to restrict the similarity score during the inference. This adjustment is necessary to ensure that the integration of the global and local adapters achieves optimal personalization outcomes. Thus, the scale factor $\\lambda$ is set equal to the updating weight $\\alpha$ used in the local adapter training. ", "page_idx": 13}, {"type": "text", "text": "All models are implemented using LoRA to enhance learning efficiency, with the rank of LoRA set as $r=8$ and only applied to $W_{q}$ and $W_{v}$ . For FL methods, each client conducts 10 local epochs with a batch size of 32. We implement all the methods using PyTorch and conduct all experiments on NVIDIA Quadro RTX 6000 GPU. ", "page_idx": 13}, {"type": "table", "img_path": "nkwPiBSw1f/tmp/74f8c45cb8a8dfe6f5016874835e8b46b15ab6cbdb388aaa5cd72ba5cd81f68c.jpg", "table_caption": ["Table 7: Prompt Template. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A.3 Algorithm ", "text_level": 1, "page_idx": 14}, {"type": "table", "img_path": "nkwPiBSw1f/tmp/f06210490644cc5db550c2e407a2f2a0b766c07a990960379a01d36af43026b2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "nkwPiBSw1f/tmp/19ccb8360b8691292b5bfe275ddf053d4ab9b924b3e2ddbfdcc1d9c4a3f0e0ec.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Additional Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Adaptability Analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To enhance applicability across diverse non-IID environments, our method is meticulously designed with a high degree of flexibility for its adaption across various global learning frameworks, backbones and PEFT methods for different scenarios. This adaptability is simply achieved through the straightforward substitution of the FedAvg, LLM and LoRA with alternative aggregation methods, transformer-based foundation models and adapter-based PEFT methods during the training. In our experiment, we employ FedAvg, LLM and LoRA as representative examples, demonstrating our methods\u2019 superior performance compared to other baselines as indicated in Table 1 and Table 2. To further validate the effectiveness and versatility of our approach within different federated learning contexts, we adapt our methods to include the FedProx[20] framework and also implement other baselines (FedIT and FedLoRA) within FedProx to maintain a fair comparison. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Results presented in Table 8 indicate that our methods, both training and fine-tuning methods, outperform competing approaches. Specifically, FedDPA-T excels in personalization, while FedDPAF leads in test-time personalization, maintaining consistent performance with FedAvg. These findings underscore the robustness and consistent efficacy of our methods across various global learning paradigms for different non-IID scenarios. ", "page_idx": 15}, {"type": "text", "text": "Table 8: Personalization and test-time personalization results of different models with FedProx framework on federated dataset 1. FedDPA-F represents the model with the local fine-tuning adapter and FedDPA-T represents the model with the local training adapter. Linguistic represents the linguistic acceptability task, Word Dis represents the word disambiguation task, and Question CLS represents question classification task. ", "page_idx": 15}, {"type": "table", "img_path": "nkwPiBSw1f/tmp/89159b10de48f8ec8883294c4ee3835179f562b02a511be84a067cb16e96b204.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.2 Instance-Wise Dynamic Weighting Mechanism Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we further examine the impact of the instance-wise dynamic weighting mechanism, including the similarity metric, the selected local instance number and the type of instance representation. ", "page_idx": 15}, {"type": "text", "text": "Impact of Similarity Metric. In Section 4.3, we employ the similarity metric to calculate the average similarity scores of each input instance, which serves as the weight $\\alpha_{t}$ to dynamically balance the global and local adapters. For this purpose, cosine similarity is selected in our experiment due to its better robustness and normalization with high-dimensional vectors than other metrics, and its superiority has been demonstrated in many NLP/CV works[25, 37, 36]. Additionally, we conducted an ablation study comparing other metrics like L2-norm and Pearson correlation, and the results in Table 9 demonstrate that cosine similarity outperforms other similarity metrics. ", "page_idx": 15}, {"type": "text", "text": "Impact of Instance Number $S$ . In Section 4.3, the selection of $S$ , representing the number of local instances for similarity calculation, is pivotal. To comprehensively evaluate the effect of varying the number of these instances, we conduct a series of experiments employing distinct local instance numbers, specifically $S\\in\\{1,3,5,7,9\\}$ . The accuracy results, as depicted in Figure 5, illustrate the dependency of model performance on different instance numbers $S$ . As demonstrated in Figure 5 (a), in the context of personalization, it is observed that our models attain a plateau in accuracy when the instance number exceeds 5. This indicates a stabilization in model performance beyond this threshold of local instances. Furthermore, Figure 5 (b) delves into the realm of test-time personalization. The findings here reveal similar results, indicating that variations in the instance number do not markedly impact the model\u2019s performance in test-time personalization. ", "page_idx": 15}, {"type": "text", "text": "Impact of Instance Representation. In Section 4.3, our method entails utilizing the embedding of the final token from the last hidden layer of the LLM, denoted as \u2019LAST\u2019, as the input instance representation for the purpose of similarity calculation. In this exploration, we delve into another instance representation strategy, which involves employing the average embedding of all tokens from the final hidden layer of the LLM, herein referred to as \u2019AVG\u2019. The comparative analysis, as presented in Table 10, demonstrates that employing the embedding of the last token yields superior performance relative to the strategy of averaging the embeddings of all tokens. This observed difference in performance can be attributed to the decoder structure inherent to LLMs, wherein the final token is capable of attending to all preceding tokens, thereby encapsulating comprehensive sentence-level information. ", "page_idx": 15}, {"type": "image", "img_path": "nkwPiBSw1f/tmp/03484a5039b0ec5aae52c164918d10b108ab20e38499b1a9cd41b8e100f41927.jpg", "img_caption": ["Figure 5: Average accuracy varies as different instance numbers. TTP represents test-time personalization. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Table 9: Ablation study of similarity metric (Sim). P represents personalization, and TTP represents test-time personalization. -L2 represents using the L2-Norm as metric, Pearson represents using the Pearson correlation as metric, and Cosine represents using the cosine similarity as the metric. ", "page_idx": 16}, {"type": "table", "img_path": "nkwPiBSw1f/tmp/219b982b8a45546975fbcf2ad40491107c7d8b63818a4a8301dd49d9e0925a62.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 10: Ablation study of instance representations (Emb). P represents personalization, and TTP represents test-time personalization. LAST represents using the embedding of the final token from the final hidden layer of LLM as instance representation, and AVG represents using the average embedding of all tokens from the final hidden layer of LLM as instance representation. ", "page_idx": 16}, {"type": "table", "img_path": "nkwPiBSw1f/tmp/ac54a86b7e98f998c2ed7e5c6cef19dd2e89a73b8e510ce8ba4670993370ba61.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.3 Model Scalability Analysis ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In order to examine the effectiveness of model scalability, we conduct experiments based on a larger model, LLaMA-13B. The outcomes, as presented in Table 11, elucidate that larger models exhibit superior performance over their smaller counterparts across all personalization methods evaluated. Furthermore, it is noteworthy that FedDPA-T surpasses FedDPA-F in terms of personalization and achieves comparable results in test-time personalization. This analysis underscores the inherent advantages of larger models in enhancing model performance, alongside the advance of the FedDPA-T approach in the context of personalization and adaptability to test-time conditions. ", "page_idx": 16}, {"type": "text", "text": "Table 11: Ablation study of model size. P represents personalization, and TTP represents test-time personalization. ", "page_idx": 16}, {"type": "table", "img_path": "nkwPiBSw1f/tmp/88946983afaf234b7cf0f2e1a68ed3980ced9750b913e479d6ad65c316c17e14.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.4 Communication and Computation Analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we undertake a detailed examination of both the communication and computation overhead associated with our proposed model in comparison to other baseline models. The results, as detailed in Table 12, delineate the communication and computation burdens imposed by various models. Given that these models are all based on the LoRA framework and exclusively transmit LoRA weights for aggregation (with our methods specifically transmitting only the global LoRA weights), they inherently sustain a minimal communication overhead. Regarding the computation overhead, the LoRA architecture permits the training of both local and global LoRAs in parallel, resulting in a marginal increase in computational demands for FedDPA-T. Conversely, FedDPA-F learns the local LoRA through an additional fine-tuning phase, thereby not imposing any additional computational overhead during the training phase. ", "page_idx": 17}, {"type": "text", "text": "Additionally, we have conducted an analysis of the inference time associated with our models. This examination involved calculating the average inference time per instance for FedLoRA, FedDPA without the instance-wise dynamic weighting mechanism, and FedDPA. As illustrated in Table 13, it is observed that our methods incur slightly higher inference time compared to FedLoRA. This marginal increase in inference time underscores the efficiency of our proposed methods, demonstrating that the enhanced performance and capabilities are achieved with a minimal impact on computational efficiency during inference. ", "page_idx": 17}, {"type": "table", "img_path": "nkwPiBSw1f/tmp/81aeaa896d74420b701c1222f74c9b93325fb0b2be1416170ec2e13c3a168485.jpg", "table_caption": ["Table 12: The communication and computation overhead of FedDPA and other baselines on Federated Dataset 1. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "nkwPiBSw1f/tmp/df254f1b5d629a01b0a22154dc1f6f0fc67867941bfa806d524da4acef462dd3.jpg", "table_caption": ["Table 13: Average inference time per instance. Auto represents the instancewise dynamic weighting mechanism. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "C Discussions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Extend to Other Scenarios of Test-Time FL ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this paper, we primarily consider an ideal scenario for our proposed test-time personalization setting, where all tasks are included for all clients. In this section, we will discuss our methods under alternative scenarios. ", "page_idx": 17}, {"type": "text", "text": "In practical applications, it is possible that some tasks remain unseen by all clients during training and may only appear during the testing phase for certain clients. In this scenario, test-time distribution shifts arise from these unseen tasks. According to previous works [5, 29], generic features learned through FL are robust to distribution shifts, even those originating from unseen test-time tasks. Consequently, our method can be directly adapted to this scenario. We conducted experiments to evaluate our methods against other baselines on three unseen test-time tasks. All methods and baselines are trained on our constructed Federated Dataset 1 and tested on three tasks not included in Federated Dataset 1. For personalized methods, we report the average score across all clients and the maximum score among all clients to provide a comprehensive comparison. ", "page_idx": 17}, {"type": "text", "text": "As shown in Table 14, FedDPA-T outperforms all other models, indicating the effectiveness of our method on unseen test-time tasks. Additionally, FedDPA-F surpasses FedLoRA, suggesting that the generic features learned through FL across diverse data distributions are robust to various distribution shifts, consistent with the findings in [29]. Despite this, other techniques targeting these unseen test-time tasks could further enhance our proposed methods, which we leave for future work to explore. ", "page_idx": 17}, {"type": "text", "text": "Our methods are also applicable to scenarios involving the introduction of new clients. As previously analyzed, our methods are robust to different distribution shifts. By computing the similarity between instances from new clients and existing clients through our instance-wise dynamic weighting mechanism, we can identify the most similar existing client. The model of this identified client can ", "page_idx": 17}, {"type": "text", "text": "Table 14: Test-time performance on unseen tasks. All models are trained on Federated Dataset 1, and these unseen test-time tasks are not included in Federated Dataset 1. AVG represents the average score across all clients for this task, while MAX represents the highest score among these clients for this task. The best performance for AVG is bolded, and the best performance for MAX is underlined. ", "page_idx": 18}, {"type": "table", "img_path": "nkwPiBSw1f/tmp/34da87fa56537f169151c3b4fa7ccbaaef054a748358a95e5349f7445dec8ca3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "then be used as the initial model for the new clients, providing a more effective starting point for further training. ", "page_idx": 18}, {"type": "text", "text": "C.2 Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The proposed dual-personalizing adapter architecture is limited by 1) model scales: the proposed methods rely on the foundation model, presupposing that each client possesses sufficient memory capacity and computational resources to store and train the foundation model with PEFT methods. 2) secure issues: the framework operates under the assumption that all clients are trusted and legally entitled to access and utilize data stored on them, and the whole process does not suffer from any attacks. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The main claims have been made clearly in the abstract and introduction. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Please refer to Appendix C. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our paper does not include theoretical results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Please refer to the experiment section 5 and Appendix A ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Please refer to the supplementary material. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Please refer to Appendix A.1. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: All experiments are conducted in the same setting with stable results. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Please refer to Appendix A.2. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper conforms with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Although Federated Learning technology has positive societal impacts on privacy preservation, our paper aims to enhance federated learning in the application scenario with foundation models. We only use publicly available NLP datasets to evaluate the effectiveness of the proposed method. Therefore, we would like to rate our work\u2019s societal impacts as N/A. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Since our paper proposes a new framework for federated learning and the models/datasets we use are publicly available, there are no such risks. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Please refer to the section 5. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 23}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Please refer to the supplementary material. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]