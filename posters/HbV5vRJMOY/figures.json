[{"figure_path": "HbV5vRJMOY/figures/figures_1_1.jpg", "caption": "Figure 1: MoNE's learned token importance: From left to right, fewer image tokens are processed using the full model \u2013 to fit a compute budget \u2013 by an increasing threshold on MoNE's router logits.", "description": "This figure shows how MoNE learns to prioritize tokens based on their importance.  As the compute budget decreases (moving from left to right), fewer tokens are processed by the full, most computationally expensive model. The tokens deemed less important are processed by smaller, less expensive nested experts. This demonstrates MoNE's adaptive processing capability.", "section": "1 Introduction"}, {"figure_path": "HbV5vRJMOY/figures/figures_3_1.jpg", "caption": "Figure 2: (a) Nested model: Partial in- and out-projections in the SA and MLP layers create nested models. m controls the parameter count and the FLOPs of nested models. The self-attention information exchange happens at the full model dimension D, MLP dimension is set to 4D as in ViT. (b) Mixture of Nested Experts (MoNE): Each token x is routed to a nested network, denoted by different model dimension in the diagram. Here xi gets routed to a nested model with model dimension D/4, whereas xi+1 gets to the full model. The information exchange between these tokens of different dimension happens in the self-attention block, where they are always projected to the same dimension. The router weights are also multiplied with the features for proper flow of gradients. A lighter color in the weight matrix indicate a sliced matrix to construct the nestedness.", "description": "This figure shows the architecture of the Mixture of Nested Experts (MoNE) model. (a) illustrates how nested models are created by using partial projections in the self-attention (SA) and multi-layer perceptron (MLP) layers of a Vision Transformer. The nested models have varying model dimensions, allowing for different computational costs. (b) shows how MoNE dynamically routes tokens to these nested models using a router network. The router assigns tokens to experts based on their importance, allowing redundant tokens to be processed through cheaper nested models, which improves efficiency.", "section": "Methodology"}, {"figure_path": "HbV5vRJMOY/figures/figures_6_1.jpg", "caption": "Figure 3: Image classification: Performance comparison of MoNE with baselines on ImageNet-21k for different model sizes. MoNE performs significantly better than MatViT and Mixture-of-Depth (MoD) and even benefits from isoFLOPs training (see fig a).", "description": "This figure compares the performance of the proposed MoNE model against several baselines on the ImageNet-21k dataset for image classification. The comparison is done for three different model sizes (S/16, B/16, L/16), showing the precision@1 metric plotted against FLOPs (floating point operations). MoNE consistently outperforms the other models, particularly at lower FLOPs, demonstrating its efficiency.  The isoFLOPs training variation of MoNE is also included (for the smallest model), showing a further performance improvement over the standard MoNE training.", "section": "5 Results"}, {"figure_path": "HbV5vRJMOY/figures/figures_6_2.jpg", "caption": "Figure 4: Video classification: MoNE vs. baselines on video datasets. Finetuning with the isoFLOPs training regime leads to matching baseline with > 2\u00d7 FLOP improvement.", "description": "This figure shows the performance comparison of MoNE with baselines (MatViViT and ViViT) on two video datasets, Kinetics-400 and Something-Something-v2.  The x-axis represents the FLOPs (floating point operations), a measure of computational cost, and the y-axis represents the accuracy achieved.  The results demonstrate that MoNE achieves comparable accuracy to the baselines while using significantly fewer FLOPs (more than 2 times less).  The isoFLOPs training regime, where models are trained to the same FLOP budget as the baseline, further improves MoNE's performance.", "section": "5 Results"}, {"figure_path": "HbV5vRJMOY/figures/figures_7_1.jpg", "caption": "Figure 5: Capacity adaptation during inference: Performance changes when a model trained at a certain capacity (denoted as ec) is evaluated at other capacities. The \u201cTrain Adaptive\u201d plot for SSv2 denotes a single model evaluated at different inference-time budgets.", "description": "This figure demonstrates the model's adaptability to varying compute budgets during inference.  It shows how the accuracy changes when a model trained at a specific capacity is used with different capacities during inference.  The 'Train Adaptive' curve highlights a single model's robustness across various budgets.  The results are shown for both the ImageNet21k and Something-Something-v2 datasets, indicating that MoNE can adapt to a range of computational resource constraints without retraining.", "section": "5 Results"}, {"figure_path": "HbV5vRJMOY/figures/figures_8_1.jpg", "caption": "Figure 6: Wallclock realization of MoNE's computational savings with varying effective capacities, depicted on the Something-Something-v2 dataset.", "description": "This figure shows the relationship between FLOPs, latency, throughput, and accuracy for the MoNE and ViViT models on the Something-Something-v2 dataset.  It demonstrates that MoNE achieves significant reductions in latency and increased throughput while maintaining comparable accuracy to ViViT, showcasing its computational efficiency improvements.  The plots illustrate how these metrics vary across different effective capacities, highlighting the model's adaptability to varying computational budgets.", "section": "5 Results"}, {"figure_path": "HbV5vRJMOY/figures/figures_8_2.jpg", "caption": "Figure 7: Router Analysis: Effect of router placement and learning on Something-Something v2.", "description": "This figure analyzes the effect of router placement and the number of routers used in the MoNE model on the Something-Something-v2 dataset.  Panel (a) shows that placing the router at the first layer provides the best performance. Panel (b) demonstrates that increasing the number of routers decreases performance.  Panel (c) compares the performance of MoNE's learned router with a random router, showing that the learned router significantly outperforms the random router, especially at lower capacities.", "section": "6 Router Analysis"}, {"figure_path": "HbV5vRJMOY/figures/figures_9_1.jpg", "caption": "Figure 8: Tokens routed to the full model: Highlighted regions are the tokens sent to the full model, while rest of the tokens are sent to the smaller nested models. (a) shows examples on images and (b) shows an example on a video at multiple temporal indices. As we can see, the necessary and important tokens are sent to the full model.", "description": "This figure visualizes which tokens are processed by the full model (the most computationally expensive model) versus the smaller nested models.  The highlighted regions in the images and video frames show tokens deemed most important by MoNE and thus routed to the full model.  This demonstrates that MoNE effectively focuses its computational resources on the most informative parts of the image or video.", "section": "Visualizing Important Tokens"}, {"figure_path": "HbV5vRJMOY/figures/figures_13_1.jpg", "caption": "Figure 9: MatFormer Structure on Model Dimension", "description": "This figure illustrates the MatFormer structure focusing on the model dimension. It shows how the self-attention and feed-forward subroutines are modified to accommodate nested models with varying dimensions.  The nested structure is applied to the input and output projections of both the self-attention and feed-forward layers, allowing for varying levels of computational complexity for different parts of the model.  The diagram details the architecture, showing how partial projections are extracted and used, with the final output projected to the full model dimension.", "section": "3.1 Nested Models"}, {"figure_path": "HbV5vRJMOY/figures/figures_14_1.jpg", "caption": "Figure 10: Latency gains by using ToMe on top of MoNE, compared against ToMe over vanilla ViT. We use a ViT-Ti/16 model trained on ImageNet-1K for this comparison.", "description": "This figure compares the latency gains achieved by applying Token Merging (ToMe) on top of Mixture of Nested Experts (MoNE) against using ToMe with a vanilla Vision Transformer (ViT).  It shows that combining ToMe and MoNE leads to greater latency improvements than ToMe alone. A ViT-Ti/16 model trained on ImageNet-1k was used for this comparison. The x-axis represents GFLOPs (billions of floating point operations), and the y-axis shows the drop in Top-1 accuracy.  The plot indicates that MoNE, when used in conjunction with ToMe, provides substantial latency reduction while maintaining competitive accuracy.", "section": "A.2 Comparing MoNE with Other Adaptive Baselines"}, {"figure_path": "HbV5vRJMOY/figures/figures_15_1.jpg", "caption": "Figure 1: MoNE's learned token importance: From left to right, fewer image tokens are processed using the full model \u2013 to fit a compute budget \u2013 by an increasing threshold on MoNE's router logits.", "description": "This figure shows how MoNE learns to prioritize tokens based on their importance.  It visualizes the different subsets of image tokens processed with the full model versus less computationally expensive nested experts to meet a compute budget. As you move from left to right, fewer tokens are processed by the largest, most accurate model, showing that MoNE adapts its processing based on available resources.", "section": "1 Introduction"}]