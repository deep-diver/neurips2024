{"references": [{"fullname_first_author": "Shun-ichi Amari", "paper_title": "Natural Gradient Works Efficiently in Learning", "publication_date": "1998-02-01", "reason": "This paper introduces the natural gradient descent method, a foundational concept for the improved empirical Fisher approximation discussed in the main paper."}, {"fullname_first_author": "James Martens", "paper_title": "New Insights and Perspectives on the Natural Gradient Method", "publication_date": "2020-00-00", "reason": "This paper provides a comprehensive overview of the natural gradient descent method, which is crucial for understanding and improving the empirical Fisher approximation."}, {"fullname_first_author": "James Martens", "paper_title": "Optimizing Neural Networks with Kronecker-Factored Approximate Curvature", "publication_date": "2015-00-00", "reason": "This paper introduces the K-FAC method, a popular and efficient approximate natural gradient method, which is closely related to the improved empirical Fisher approximation."}, {"fullname_first_author": "Frederik Kunstner", "paper_title": "Limitations of the Empirical Fisher Approximation for Natural Gradient Descent", "publication_date": "2019-00-00", "reason": "This paper thoroughly analyzes the limitations of the empirical Fisher approximation, motivating the need for the improved method proposed in the main paper."}, {"fullname_first_author": "Yi Ren", "paper_title": "Efficient Subsampled Gauss-Newton and Natural Gradient Methods for Training Neural Networks", "publication_date": "2019-00-00", "reason": "This paper proposes efficient natural gradient methods based on subsampled Gauss-Newton, providing context for the improved empirical Fisher approximation."}]}