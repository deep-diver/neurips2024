{"references": [{"fullname_first_author": "A. Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-10-26", "reason": "This paper introduces Vision Transformers (ViTs), the core subject of the current research paper, establishing the foundation for further advancements and modifications in the field."}, {"fullname_first_author": "H. Touvron", "paper_title": "Training data-efficient image transformers & distillation through attention", "publication_date": "2021-07-01", "reason": "This work details DeiT, a crucial ViT variant used for validation and comparison in the study, influencing the methodology and results of the current paper."}, {"fullname_first_author": "J. Yu", "paper_title": "Universally slimmable networks and improved training techniques", "publication_date": "2019-10-27", "reason": "This study presents the concept of slimmable neural networks, directly relevant to the goal of creating flexible ViT inference, informing the current paper's approach to network scaling."}, {"fullname_first_author": "K. He", "paper_title": "Masked autoencoders are scalable vision learners", "publication_date": "2022-06-15", "reason": "This work introduces a significant ViT improvement, Masked Autoencoders (MAE), which provides a context of scalable learning architectures for the study of slimmable ViTs."}, {"fullname_first_author": "K. Li", "paper_title": "Uniformer: Unifying convolution and self-attention for visual recognition", "publication_date": "2022-01-20", "reason": "This research introduces Uniformer, a hybrid CNN-ViT architecture used for comparative analysis in the study, allowing insights into slimmable representation across various network architectures."}]}